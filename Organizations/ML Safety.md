---
tags: Organization
href: https://www.mlsafety.org/
---

- [[Intro to ML Safety]]
- [Resources](https://www.mlsafety.org/resources) (See below)
- [Competitions](https://safe.ai/competitions)
- [Scholars Program](https://forum.effectivealtruism.org/posts/9RYvJu2iNJMXgWCBn/introducing-the-ml-safety-scholars-program)


---

# Introductory

-   [Unsolved Problems in ML Safety](https://arxiv.org/abs/2109.13916)
-   [Is Power-Seeking AI an Existential Risk?](https://arxiv.org/abs/2206.13353)
-   [X-Risk Analysis for AI Research](https://arxiv.org/abs/2206.05862)

# Robustness


### Adversarial Robustness

-   [_Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples_](https://arxiv.org/abs/1802.00420)
-   [_Towards Deep Learning Models Resistant to Adversarial Attacks_](https://arxiv.org/abs/1706.06083)
-   [Universal Adversarial Triggers for Attacking and Analyzing NLP](https://arxiv.org/abs/1908.07125)
-   [Data Augmentation Can Improve Robustness](https://arxiv.org/abs/2111.05328)
-   [Adversarial Examples for Evaluating Reading Comprehension Systems](https://arxiv.org/abs/1707.07328)
-   [BERT-ATTACK: Adversarial Attack Against BERT Using BERT](https://arxiv.org/abs/2004.09984) ([GitHub](https://github.com/chbrian/awesome-adversarial-examples-dl))
-   [Gradient-based Adversarial Attacks against Text Transformers](https://arxiv.org/abs/2104.13733)
-   [Adversarial Examples for Evaluating Reading Comprehension Systems](https://arxiv.org/abs/1707.07328)
-   [Smooth Adversarial Training](https://arxiv.org/abs/2006.14536)
-   [Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks](https://arxiv.org/abs/2003.01690) ([website](https://robustbench.github.io/))
-   [Certified Adversarial Robustness via Randomized Smoothing](https://arxiv.org/abs/1902.02918)
-   [Adversarial Examples Are a Natural Consequence of Test Error in Noise](https://arxiv.org/abs/1901.10513)
-   [Using Pre-Training Can Improve Model Robustness and Uncertainty](https://arxiv.org/abs/1901.09960)
-   [Motivating the Rules of the Game for Adversarial Example Research](https://arxiv.org/abs/1807.06732)
-   [Certified Defenses against Adversarial Examples](https://arxiv.org/abs/1801.09344)
-   [Towards Evaluating the Robustness of Neural Networks](https://arxiv.org/abs/1608.04644)

### Long Tails and Distribution Shift

-   [_The Many Faces of Robustness: A Critical Analysis of Out-of-Distribution Generalization_](https://arxiv.org/abs/2006.16241)
-   [_Benchmarking Neural Network Robustness to Common Corruptions and Perturbations_](https://arxiv.org/abs/1903.12261)
-   [PixMix: Dreamlike Pictures Comprehensively Improve Safety Measures](https://arxiv.org/abs/2112.05135)
-   [WILDS: A Benchmark of in-the-Wild Distribution Shifts](https://arxiv.org/abs/2012.07421)
-   [ObjectNet: A large-scale bias-controlled dataset for pushing the limits of object recognition models](https://papers.nips.cc/paper/2019/hash/97af07a14cacba681feacf3012730892-Abstract.html)
-   [Adversarial NLI: A New Benchmark for Natural Language Understanding](https://arxiv.org/abs/1910.14599)
-   [Natural Adversarial Examples](https://arxiv.org/abs/1907.07174)
-   [ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness](https://arxiv.org/abs/1811.12231)

‍

# Monitoring


### OOD and Malicious Behavior Detection

-   [_Deep Anomaly Detection with Outlier Exposure_](https://arxiv.org/abs/1812.04606)
-   [_A Baseline for Detecting Misclassified and Out-of-Distribution Examples in Neural Networks_](https://arxiv.org/abs/1610.02136)
-   [ViM: Out-Of-Distribution with Virtual-logit Matching](https://arxiv.org/abs/2203.10807)
-   [VOS: Learning What You Don’t Know by Virtual Outlier Synthesis](https://arxiv.org/abs/2202.01197)
-   [Scaling Out-of-Distribution Detection for Real-World Settings](https://arxiv.org/abs/1911.11132)
-   [A Simple Unified Framework for Detecting Out-of-Distribution Samples and Adversarial Attacks](https://arxiv.org/abs/1807.03888)

### Interpretable Uncertainty

-   [_On Calibration of Modern Neural Networks_](https://arxiv.org/abs/1706.04599)
-   [_Can You Trust Your Model’s Uncertainty? Evaluating Predictive Uncertainty Under Dataset Shift_](https://arxiv.org/abs/1906.02530)
-   [PixMix: Dreamlike Pictures Comprehensively Improve Safety Measures](https://arxiv.org/abs/2112.05135)
-   [Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles](https://arxiv.org/abs/1612.01474)
-   [Posterior calibration and exploratory analysis for natural language processing models](https://arxiv.org/abs/1508.05154)
-   [Accurate Uncertainties for Deep Learning Using Calibrated Regression](https://arxiv.org/abs/1807.00263)

### Transparency

-   [_The Mythos of Model Interpretability_](https://arxiv.org/abs/1606.03490)
-   [Sanity Checks for Saliency Maps](https://arxiv.org/abs/1810.03292)
-   [Interpretable Explanations of Black Boxes by Meaningful Perturbation](https://arxiv.org/abs/1704.03296)
-   [Locating and Editing Factual Knowledge in GPT](https://arxiv.org/abs/2202.05262)
-   [Acquisition of Chess Knowledge in AlphaZero](https://arxiv.org/abs/2111.09259)
-   [Feature Visualizations](https://distill.pub/2017/feature-visualization/) and [OpenAI Microscope](https://microscope.openai.com/)
-   [Exemplary Natural Images Explain CNN Activations Better than State-of-the-Art Feature Visualization](https://arxiv.org/abs/2010.12606)
-   [Network Dissection: Quantifying Interpretability of Deep Visual Representations](https://arxiv.org/abs/1704.05796)
-   [Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead](https://arxiv.org/abs/1811.10154)
-   [Convergent Learning: Do different neural networks learn the same representations?](https://arxiv.org/abs/1511.07543)

### Trojans

-   [_Poisoning and Backdooring Contrastive Learning_](https://arxiv.org/abs/2106.09667)
-   [_Universal Litmus Patterns: Revealing Backdoor Attacks in CNNs_](https://arxiv.org/abs/1906.10842)
-   [_Neural Cleanse: Identifying and Mitigating Backdoor Attacks in Neural Networks_](https://people.cs.uchicago.edu/~ravenben/publications/pdf/backdoor-sp19.pdf)
-   [TrojAI](https://pages.nist.gov/trojai/)
-   [Detecting AI Trojans Using Meta Neural Analysis](https://arxiv.org/abs/1910.03137)
-   [STRIP: A Defence Against Trojan Attacks on Deep Neural Networks](https://arxiv.org/abs/1902.06531)
-   [Targeted Backdoor Attacks on Deep Learning Systems Using Data Poisoning](https://arxiv.org/abs/1712.05526)
-   [BadNets: Identifying Vulnerabilities in the Machine Learning Model Supply Chain](https://arxiv.org/abs/1708.06733)

### Detecting and Forecasting Emergent Behavior

-   [_The Effects of Reward Misspecification: Mapping and Mitigating Misaligned Models_](https://arxiv.org/abs/2201.03544)
-   [_The Basic AI Drives_](https://selfawaresystems.files.wordpress.com/2008/01/ai_drives_final.pdf)
-   [Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets](https://arxiv.org/abs/2201.02177)
-   [Optimal Policies Tend to Seek Power](https://arxiv.org/abs/1912.01683)
-   [The Off-Switch Game](https://arxiv.org/abs/1611.08219)

# Alignment


### Honest AI

-   [TruthfulQA: Measuring How Models Mimic Human Falsehoods](https://arxiv.org/abs/2109.07958)
-   [Truthful AI: Developing and governing AI that does not lie](https://arxiv.org/abs/2110.06674)

### Machine Ethics

-   [_What Would Jiminy Cricket Do? Towards Agents That Behave Morally_](https://arxiv.org/abs/2110.13136)
-   [_Ethics Background (Introduction through “Absolute Rights or Prima Facie Duties”)_](https://www.youtube.com/playlist?list=PLKtXFotbf7fOg7zbQ3565EnpzzKlYaVVI)
-   [Aligning AI With Shared Human Values](https://arxiv.org/abs/2008.02275)
-   [Avoiding Side Effects in Complex Environments](https://arxiv.org/abs/2006.06547)
-   [Conservative Agency via Attainable Utility Preservation](https://arxiv.org/abs/1902.09725)
-   [The Structure of Normative Ethics](https://cpb-us-west-2-juc1ugur1qwqqqo4.stackpathdns.com/campuspress.yale.edu/dist/7/724/files/2016/01/The-Structure-of-Normative-Ethics-2gw2akt.pdf)

‍

# Systemic Safety


### Forecasting

-   [Forecasting Future World Events with Neural Networks](https://arxiv.org/abs/2206.15474)
-   [On Single Point Forecasts for Fat-Tailed Variables](https://arxiv.org/abs/2007.16096)
-   [On the Difference between Binary Prediction and True Exposure With Implications For Forecasting Tournaments and Decision Making Research](https://www.stat.berkeley.edu/~aldous/157/Papers/taleb_tetlock.pdf)
-   [Superforecasting – Philip Tetlock](https://youtu.be/pedNak4S9IE?t=440)

### ML for Cyberdefense

-   [Asleep at the Keyboard? Assessing the Security of GitHub Copilot’s Code Contributions](https://arxiv.org/pdf/2108.09293.pdf?nylayout=pc)

### Cooperative AI

-   [Uehiro Lectures 2022](https://www.practicalethics.ox.ac.uk/uehiro-lectures-2022)
-   [Open Problems in Cooperative AI](https://arxiv.org/abs/2012.08630)
-   [Cooperation, Conflict, and Transformative Artificial Intelligence: A Research Agenda](https://longtermrisk.org/files/Cooperation-Conflict-and-Transformative-Artificial-Intelligence-A-Research-Agenda.pdf)