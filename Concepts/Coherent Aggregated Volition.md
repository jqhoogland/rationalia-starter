---
tags: ['LessWrong', 'Portal', 'Concept']
src: https://lesswrong.com/tag/coherent-aggregated-volition
---

Coherent Aggregated Volition is one of [[Ben Goertzel]]'s responses to [[Eliezer Yudkowsky]]'s [[Coherent Extrapolated Volition]], the other being [[Coherent Blended Volition]]. CAV would be a combination of the goals and beliefs of humanity at the present time.

The author considers the "extrapolation" aspect of CEV as distorting the concept of volition and to be highly uncertain. He considers that if the person whose volition is being extrapolated has some inconsistent aspects (which is typically human), then there could be a great variety of possible extrapolations. The problem would then be which version of this extrapolated human to choose, or how to aggregate them, which would be very difficult to achieve.

Coherent Aggregated Volition is presented as simpler than his interpretation of CEV, and intended to be easier to formalize and prototype in the foreseeable future (with the help of platforms such as [OpenCog](http://opencog.org). CAV is not, however, intended to answer the question of provably [Friendly AI](https://wiki.lesswrong.com/wiki/Friendly_AI), although Goertzel claims CEV may not answer that question either.

## The concept
The author starts by arguing that we must treat goals and beliefs together, as a single concept, which he calls *gobs* (and *gobses* for the plural). Each agent thus can have several *gobs*, logically consistent or not. As a way of measuring the distance of these *gobs* from each other, the term *gobs metric* is used - the persons or AGI could agree, with various degrees, on several metrics, but it seems probable that this individual's metrics would differ less than their *gobses*.

Then, given a population of intelligent agents with different *gobses*, we could then try to find a single *gobs* that maximizes logical consistency, compactness, similarity to the different *gobses* in the population and amount of evidence supporting these beliefs. This "multi-extremal optimization algorithm" is what he calls Coherent Aggregated Volition. The term expresses the attempt to achieve both coherence and an aggregation of the population volitions.

CAV has some free parameters, like the averaging method, the measure of compactness, consistency evaluation and so on, but these are seen as features rather than limitations and do not taint the simplicity of the idea. At the same time, it is possible to refine some of the criteria stated before without changing the nature of the method.

## CEV vs CAV
Although CEV is seen as possibly giving a feasible solution, Goertzel states there's no guarantee of this, and that Yudkowsky's method can generate solutions very far from the population's *gobses*.

In some experiments with iteratively repairing inconsistent beliefs within a probabilistic reasoning system, the author claims that it seems like we can reach a set of beliefs very different from the one we started - which may be a problem with CEV. That is, the iterative refinement of the agents' goals and beliefs might not always be a good way to turn inconsistent values into similar consistent ones.

Finally, Goertzel feels that it seems like CEV bypasses an essential aspect of being human, by not allowing humans to resolve their inconsistencies themselves. CAV tries to summarize this process, respecting and not replacing it, even if this leads to more "bad" aspects of humanity being retained.

## See also
- [Friendly AI](https://wiki.lesswrong.com/wiki/Friendly_AI)
- [[Coherent Extrapolated Volition]]
- [Utility Functions](https://www.lesswrong.com/tag/utility-functions?showPostCount=true&useTagName=true)

## References
- Goertzel, Ben (2010) 
- [Coherent Aggregated Volition: A Method for Deriving Goal System Content for Advanced, Beneficial AGIs](http://multiverseaccordingtoben.blogspot.com/2010/03/coherent-aggregated-volition-toward.html)
-  
- [Consistency Measurement](http://www.aquar-system.com/catalog/pulp-consistency-measurement-and-control/)



---

