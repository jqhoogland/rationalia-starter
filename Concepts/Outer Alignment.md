---
tags: ['LessWrong', 'Concept']
src: https://www.lesswrong.com/tag/outer-alignment
---

# Outer Alignment
Outer Alignment in the context of machine learning is the property where the specified loss function is aligned with the intended goal of its designers. This is an intuitive notion, in part because human intentions are themselves not well-understood. This is what is typically discussed as the 'value alignment' problem. It is contrasted with [[Inner Alignment|inner alignment]], which discusses if an optimizer is the production of an outer aligned system, then whether that optimizer is itself aligned.See also: 

