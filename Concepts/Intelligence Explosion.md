---
tags: ['LessWrong', 'Concept']
src: https://www.lesswrong.com/tag/intelligence-explosion
---

# Intelligence Explosion
An intelligence explosion is theoretical scenario in which an intelligent agent analyzes the processes that produce its intelligence, improves upon them, and creates a successor which does the same. This process repeats in a positive feedback loop– each successive agent more intelligent than the last and thus more able to increase the intelligence of its successor – until some limit is reached. This limit is conjectured to be much, much higher than human intelligence.

A strong version of this idea suggests that once the positive feedback starts to play a role, it will lead to a very dramatic leap in capability very quickly. This is known as a “hard takeoff.” In this scenario, technological progress drops into the characteristic timescale of transistors rather than human neurons, and the ascent rapidly surges upward and creates superintelligence (a mind orders of magnitude more powerful than a human's) before it hits physical limits. A hard takeoff is distinguished from a "soft takeoff" only by the speed with which said limits are reached.

## Published arguments
Philosopher David Chalmers published a [significant analysis of the Singularity](http://consc.net/papers/singularity.pdf), focusing on intelligence explosions, in *Journal of Consciousness Studies*. [His analysis](https://wiki.lesswrong.com/wiki/Singularity#Chalmers.27_analysis) of how they could occur defends the likelihood of an intelligence explosion. He performed a very careful analysis of the main premises and arguments for the existence of the a singularity from an intelligence explosion. According to him, the main argument is:"...[(Read More)]()

