---
tags: ['LessWrong', 'Concept']
src: https://www.lesswrong.com/tag/oracle-ai
---

# Oracle AI
An Oracle AI is a regularly proposed solution to the problem of developing [Friendly AI](https://wiki.lesswrong.com/wiki/Friendly_AI). It is conceptualized as a super-intelligent system which is designed for only answering questions, and has no ability to act in the world. The name was first suggested by [Nick Bostrom](https://www.lesswrong.com/tag/nick-bostrom).

## See also
# Safety
The question of whether Oracles – or just [keeping an AGI forcibly confined](https://www.lesswrong.com/tag/ai-boxing-containment) - are safer than fully free AGIs has been the subject of debate for a long time. Armstrong, Sandberg and Bostrom discuss Oracle safety at length in their [Thinking inside the box: using and controlling an Oracle AI](http://www.aleph.se/papers/oracleAI.pdf). In the paper, the authors review various methods which might be used to measure an Oracle's accuracy. They also try to shed some light on some weaknesses and dangers that can emerge on the human side, such as psychological vulnerabilities which can be exploited by the Oracle through social engineering. The paper discusses ideas for physical security (“boxing”), as well as problems involved with trying to program the AI to only answer questions. In the end, the paper reaches the cautious conclusion of Oracle AIs probably being safer than free AGIs.

In a related work, [Dreams of Friendliness](http://lesswrong.com/lw/tj/dreams_of_friendliness/), [Eliezer Yudkowsky](https://www.lesswrong.com/tag/eliezer-yudkowsky) gives an informal argument stating that all oracles will be agent-like, that is, driven by its own goals. He rests on the idea that anything considered "intelligent" must choose the correct course of action among all actions avaliable. That means that the Oracle will have many possible things to believe, although very few of them are correct. Therefore believing the correct thing means some method was used to select the correct belief from the many incorrect beliefs. By definition, this is an [optimization process](https://www.lesswrong.com/tag/optimization) which has a goal of selecting correct beliefs....[(Read More)]()

