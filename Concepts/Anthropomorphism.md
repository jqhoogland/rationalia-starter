---
tags: ['LessWrong', 'Portal', 'Concept']
src: https://www.lesswrong.com/tag/anthropomorphism
---

Anthropomorphism is the error of attributing distinctly human characteristics to nonhuman processes. As creatures who evolved in a social context, we all have adaptations for making predictions about other humans by [Empathic Inference|empathic inference](https://www.lesswrong.com/tag/empathic-inference). When trying to understand the behavior of other humans, it oftentimes is a helpful (and [bias-correcting](https://wiki.lesswrong.com/wiki/Fundamental_attribution_error)) heuristic to ask, "Well, what would *I* do in such a situation?" and let that be your prediction. This mode of prediction simply won't do, however, for things (and in this wide universe there are many) that don't share the detailed structure bequeathed on the human brain by evolution, although it is oftentimes tempting.

*Related tags: *Mind Projection Fallacy, Typical Mind Fallacy, [Alien Values|Alien values](https://www.lesswrong.com/tag/alien-values), [Paperclip Maximizer|Paperclip maximizer](https://www.lesswrong.com/tag/paperclip-maximizer)

## Blog posts
- [The Tragedy of Group Selectionism](http://lesswrong.com/lw/kw/the_tragedy_of_group_selectionism/)
-  - A tale of how some pre-1960s biologists were led astray by expecting evolution to do smart, nice things like they would do themselves.
- [When Anthropomorphism Became Stupid](http://lesswrong.com/lw/t5/when_anthropomorphism_became_stupid/)
- [Anthropomorphic Optimism](http://lesswrong.com/lw/st/anthropomorphic_optimism/)
-  - You shouldn't bother coming up with clever, persuasive arguments for why evolution will do things the way you prefer. It really isn't listening.
- [Humans in Funny Suits](http://lesswrong.com/lw/so/humans_in_funny_suits/)
- [Anthropomorphic Optimism](http://lesswrong.com/lw/st/anthropomorphic_optimism/)

## 
- [Evolution As Alien God|Evolution as alien god](https://www.lesswrong.com/tag/evolution-as-alien-god)
- [Unsupervised Universe|Unsupervised universe](https://www.lesswrong.com/tag/unsupervised-universe)
- [Alien Values|Alien values](https://www.lesswrong.com/tag/alien-values)
- , 
- [Paperclip Maximizer|Paperclip maximizer](https://www.lesswrong.com/tag/paperclip-maximizer)
- [Mind Design Space|Mind design space](https://www.lesswrong.com/tag/mind-design-space)
- , 
- [Really Powerful Optimization Process|Really powerful optimization process](https://www.lesswrong.com/tag/really-powerful-optimization-process)



---

