---
_id: JmPALrDPiz5pegkKw
title: Alignment tax
href: https://forum.effectivealtruism.org/tag/alignment-tax
type: tag
tags:
  - LessWrong
  - Topic
  - Tag
synchedAt: '2022-09-11T14:33:49.029Z'
---
# Alignment tax

An **alignment tax** (sometimes called a **safety tax**) is the additional cost of making [[AI alignment|AI aligned]], relative to unaligned AI.

Approaches to the alignment tax
-------------------------------

[[Paul Christiano]] distinguishes two main approaches for dealing with the alignment tax.^[\[1\]](#fn23n8pxr8hei)^^[\[2\]](#fnhm2j5l8lqoq)^  One approach seeks to find ways to pay the tax, such as persuading individual actors to pay it or facilitating coordination of the sort that would allow groups to pay it. The other approach tries to reduce the tax, by differentially advancing existing alignable algorithms or by making existing algorithms more alignable.

Further reading
---------------

Askell, Amanda *et al.* (2021) [A general language assistant as a laboratory for alignment](http://arxiv.org/abs/2112.00861), arXiv:2112.00861 \[Cs\].

Xu, Mark & Carl Shulman (2021) [Rogue AGI embodies valuable intellectual property](https://www.lesswrong.com/posts/FM49gHBrs5GTx7wFf/rogue-agi-embodies-valuable-intellectual-property), *LessWrong*, June 3.

Yudkowsky, Eliezer (2017) [Aligning an AGI adds significant development time](https://arbital.com/p/aligning_adds_time/), *Arbital*, February 22.

Related entries
---------------

[[AI alignment]] | [[AI governance]] | [[AI forecasting]] | [[Differential progress|differential progress]]

1.  ^**[^](#fnref23n8pxr8hei)**^
    
    Christiano, Paul (2020) [Current work in AI alignment](https://forum.effectivealtruism.org/posts/63stBTw3WAW6k45dY/paul-christiano-current-work-in-ai-alignment), *Effective Altruism Global*, April 3.
    
2.  ^**[^](#fnrefhm2j5l8lqoq)**^
    
    For a summary, see Rohin Shah (2020) [A framework for thinking about how to make AI go well](https://www.lesswrong.com/posts/9et86yPRk6RinJNt3/an-95-a-framework-for-thinking-about-how-to-make-ai-go-well), *LessWrong*, April 15.