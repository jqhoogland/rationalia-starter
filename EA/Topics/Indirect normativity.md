---
_id: zZbc2Mvp7BRGxF2yP
title: Indirect normativity
href: https://forum.effectivealtruism.org/tag/indirect-normativity
type: tag
tags:
  - LessWrong
  - Topic
  - Tag
synchedAt: '2022-09-11T14:34:59.753Z'
---
# Indirect normativity

**Indirect normativity** is an approach to the [[AI alignment]] problem that attempts to specify AI values indirectly, such as by reference to what a rational agent would value under idealized conditions, rather than via direct specification.

Further reading
---------------

Bostrom, Nick (2014) [*Superintelligence: Paths, Dangers, Strategies*](https://en.wikipedia.org/wiki/Special:BookSources/9780199678112), Oxford: Oxford University Press, ch. 13.

Christiano, Paul (2012) [A formalization of indirect normativity](https://ordinaryideas.wordpress.com/2012/04/21/indirect-normativity-write-up/), *Ordinary Ideas*, April 21.

Yudkowsky, Eliezer (2013) [Five theses, two lemmas, and a couple of strategic implications](https://intelligence.org/2013/05/05/five-theses-two-lemmas-and-a-couple-of-strategic-implications/), *Machine Intelligence Research Institute's Blog*, May 5.

Related links
-------------

[[AI alignment]] | [[Motivation selection method|motivation selection method]]