---
_id: EJb2KZYkRs8rtNG7f
title: Center on Long-Term Risk
href: https://forum.effectivealtruism.org/tag/center-on-long-term-risk
type: tag
tags:
  - LessWrong
  - Topic
  - Tag
synchedAt: '2022-09-11T14:35:20.478Z'
aliases:
  - CLR
---

# Center on Long-Term Risk

The **Center on Long-Term Risk** (**CLR**) is a research institute that aims to mitigate [[S-risk|s-risks]] from advanced AI. Its research agenda focuses on encouraging cooperative behavior in and avoiding conflict between [[Transformative artificial intelligence|transformative AI systems]].^[\[1\]](#fnhzxpzna8km9)^

History
-------

CLR was founded in July 2013 as the **Foundational Research Institute**;^[\[2\]](#fnadclyyn92gt)^ it adopted its current name in March 2020.^[\[3\]](#fncvpov2dkx7n)^ CLR is part of the [[Effective Altruism Foundation]].

Funding
-------

As of June 2022, CLR has received over $1.2 million in funding from the [[Survival and Flourishing]] Fund.^[\[4\]](#fn3qx0d0u2h3n)^ 

Further reading
---------------

Rice, Issa (2018) [Timeline of Foundational Research Institute](https://timelines.issarice.com/wiki/Timeline_of_Foundational_Research_Institute), *Timelines Wiki*.

Torges, Stefan (2022) [CLR’s annual report 2021](https://forum.effectivealtruism.org/posts/BJk3TrEzsdSiuJTKa/clr-s-annual-report-2021), *Effective Altruism Forum*, February 26.

External links
--------------

[Center on Long-Term Risk](https://longtermrisk.org/). Official website.

[Apply for a job](https://longtermrisk.org/work-with-us/).

Related entries
---------------

[[AI risk]] | [[Effective Altruism Foundation]] | [[S-risk|s-risk]]

1. ^**[^](#fnrefhzxpzna8km9)**^

    Clifton, Jesse (2020) [Cooperation, Conflict, and Transformative Artificial Intelligence: A Research Agenda](https://longtermrisk.org/research-agenda), *Center on Long-Term Risk*.

2. ^**[^](#fnrefadclyyn92gt)**^

    Center on Long-Term Risk (2020) [Transparency](https://longtermrisk.org/transparency), *Center on Long-Term Risk*, November.

3. ^**[^](#fnrefcvpov2dkx7n)**^

    Vollmer, Jonas (2020) [EAF/FRI are now the Center on Long-Term Risk (CLR)](https://ea-foundation.org/blog/eaf-fri-are-now-clr/), *Effective Altruism Foundation*, March 6.

4. ^**[^](#fnref3qx0d0u2h3n)**^

    Survival and Flourishing Fund (2020) [SFF-2021-H1 S-process recommendations announcement](https://survivalandflourishing.fund/sff-2021-h1-recommendations), *Survival and Flourishing Fund*.

---

# [Research Agenda](https://longtermrisk.org/research-agenda#1_Introduction)

**Cooperation failures**: "potentially-catastrophic inefficiencies in interactions among TAI-enabled actors" (e.g., destructive conflict, coercion, social dilemmas)

**Transformative AI**: "AI that precipitates a transition comparable to (or more significant than) the agricultural or industrial revolution" (Karnofsky 2016).

A **Social Dilemma** (Macy & Flache 2002) is a 2-player normal-form game where payoffs satisfy:
	- $R > P$ (Mutual cooperation > mutual defection)
	- $R > S$ (Mutual cooperation > cooperating while opponent defects)
	- $2R > T + S$ (Mutual cooperation > random strategy)
	- $\text{greed} := T-R > 0$ and $\text{fear} := P-S > 0$

![[Pasted image 20220918092336.png]]

According to Fearon 1995, rational agents go to war because:
- **Credibility**: The agents cannot credibly commit to the terms of a peaceful settlement;
- **Incomplete information**: The agents have differing private information related to their chances of winning a conflict, and incentives to misrepresent that information (see Sanchez-Pages (2012) for a review of the literature on bargaining and conflict under incomplete information);
- **Indivisible stakes**: Conflict cannot be resolved by dividing the stakes, side payments, etc.


%%
START
type: Cloze
A 2-player normal-form game is a {{c1::**social dilemma**}}, when:
	- {{c2::$R > P$ (Mutual cooperation > mutual defection)}}
	- {{c2::$R > S$ (Mutual cooperation > cooperating while opponent defects)}}
	- {{c2::$2R > T + S$ (Mutual cooperation > random strategy)}}
	- {{c3::$\text{greed} := T-R > 0$ and $\text{fear} := P-S > 0$}}

![[Pasted image 20220918092539.png]]

Tags: LessWrong
END

START
type: Cloze

According to Fearon 1995, **rational agents may go to war** because:
- {{c1::**Credibility**: The agents cannot credibly commit to the terms of a peaceful settlement;}}
- {{c2::**Incomplete information**: The agents have differing private information related to their chances of winning a conflict, and incentives to misrepresent that information (see Sanchez-Pages (2012) for a review of the literature on bargaining and conflict under incomplete information);}}
- {{c3::**Indivisible stakes**: Conflict cannot be resolved by dividing the stakes, side payments, etc.}} 
Tags: LessWrong
END


%%