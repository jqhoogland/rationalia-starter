---
_id: 7Y5hYpbAGFrDrPfZM
title: Moral patienthood
href: https://forum.effectivealtruism.org/tag/moral-patienthood
type: tag
tags:
  - LessWrong
  - Topic
  - Tag
synchedAt: '2022-09-11T14:35:01.900Z'
---
# Moral patienthood

**Moral patienthood** is the condition of deserving moral consideration. A **moral patient** is an entity that possesses moral patienthood.

While it is normally agreed that typical humans are moral patients, there is debate about the patienthood of many other types of beings, including human embryos, [[Animal welfare|non-human animals]], [[Population ethics|future people]], and [[Artificial sentience|digital sentients]].

Moral patienthood should not be confused with moral agency.^[\[1\]](#fnrnoqqfllm4h)^ For example, we might think that a baby lacks moral agency - it lacks the ability to judge right from wrong, and to act on the basis of reasons - but that it is still a moral patient, in the sense that those with moral agency should care about their well-being.

If we assume a [[Welfarism|welfarist]] theory of the good, the question of patienthood can be divided into two sub-questions: *Which entities can have well-being?* and *Whose well-being is morally relevant?*  Each question can in turn be broken down into the question of which characteristics or capacities are relevant and the question of which beings have those capacities.

First, which entities can have well-being? A majority of scientists now agree that many non-human animals, including mammals, birds, and fish, are [[Consciousness research|conscious]] and capable of feeling [[Pain and suffering|pain]],^[\[2\]](#fnt60bk5cujqs)^ but this claim is more contentious in philosophy.^[\[3\]](#fne1y75hdul44)^ This question is vital for assessing the value of interventions aimed at improving [[Farmed animal welfare|farm]] and/or [[Wild Animal Initiative|wild animal welfare]]. A smaller but growing field of study considers whether artificial intelligences might be conscious in morally relevant ways.^[\[4\]](#fn2rxiodkjlc)^

Second, whose well-being do we care about? Some have argued that future beings have less value, even though they will be just as conscious as today’s beings are now. This reduction could be assessed in the form of a [[Temporal discounting|discount rate]] on future value, so that experiences occurring one year from now are worth, say, 3% less than they do at present. Alternatively, it could be assessed by valuing individuals who do not yet exist less than current beings, for reasons related to the non-identity problem^[\[5\]](#fnrgwe2i34tle)^ (see also [[Population ethics|population ethics]]). It is contentious whether these approaches are correct. Moreover, in light of the astronomical number of individuals who could potentially exist in the future, assigning some value to future people implies that virtually all value—at least for welfarist theories—will reside in the far future^[\[6\]](#fny11tsvcv4d)^ (see also [[EA/Topics/Longtermism|longtermism]]).

Further reading
---------------

Animal Ethics (2017) [*The relevance of sentience*](https://www.animal-ethics.org/sentience-section/relevance-of-sentience/), *Animal Ethics*, September.

Bostrom, Nick & Eliezer Yudkowsky (2014) [The ethics of artificial intelligence](https://doi.org/10.1017/CBO9781139046855.020), in Keith Frankish & William M. Ramsey (eds.) *The Cambridge Handbook of Artificial Intelligence*, Cambridge: Cambridge University Press, pp. 316–334.

Kagan, Shelly (2019) [*How to Count Animals, More or Less*](https://en.wikipedia.org/wiki/Special:BookSources/9780191868177), Oxford: Oxford University Press.

MacAskill, W. & Meissner, D. (2020) [The expanding moral circle](https://www.utilitarianism.net/utilitarianism-and-practical-ethics#the-expanding-moral-circle), in [*Introduction to Utilitarianism*](https://www.utilitarianism.net/).

Muehlhauser, Luke (2017) [2017 report on consciousness and moral patienthood](https://www.openphilanthropy.org/2017-report-consciousness-and-moral-patienthood), *Open Philanthropy*, June.

Tomasik, Brian (2014) [Do artificial reinforcement-learning agents matter morally?](https://foundational-research.org/files/do-artificial-reinforcement-learning-agents-matter-morally.pdf), arXiv:1410.8233.

Related entries
---------------

[[Axiology|axiology]] | [[Consciousness research|consciousness research]] | [[Moral circle expansion|moral circle expansion]] | [[Moral weight|moral weight]] | [[Speciesism|speciesism]] | [[Valence|valence]]

1.  ^**[^](#fnrefrnoqqfllm4h)**^
    
    Wikipedia (2004) [Distinction between moral agency and moral patienthood](https://en.wikipedia.org/wiki/Moral_agency#Distinction_between_moral_agency_and_moral_patienthood), in 'Moral agency', *Wikipedia*, September 25 (updated 14 November 2020‎).
    
2.  ^**[^](#fnreft60bk5cujqs)**^
    
    Low, Philip *et al.* (2012) [The Cambridge declaration on consciousness](http://fcmconference.org/img/CambridgeDeclarationOnConsciousness.pdf), *Francis Crick Memorial Conference*, July 7.
    
3.  ^**[^](#fnrefe1y75hdul44)**^
    
    Allen, Colin & Michael Trestman (2016) [Animal consciousness](http://plato.stanford.edu/entries/consciousness-animal/#summary), in Edward Zalta (ed.), *Stanford Encyclopedia of Philosophy*.
    
4.  ^**[^](#fnref2rxiodkjlc)**^
    
    Wikipedia (2003) [Artificial consciousness](https://en.wikipedia.org/wiki/Artificial_consciousness), *Wikipedia*, March 13 (updated 24 April 2021‎).
    
5.  ^**[^](#fnrefrgwe2i34tle)**^
    
    Roberts, M. A. (2019) [The nonidentity problem](http://plato.stanford.edu/entries/nonidentity-problem/), in Edward Zalta (ed.), *Stanford Encyclopedia of Philosophy*.
    
6.  ^**[^](#fnrefy11tsvcv4d)**^
    
    Bostrom, Nick (2009) [Astronomical waste: the opportunity cost of delayed technological development](http://intelligence.org/files/AstronomicalWaste.pdf), *Utilitas* 15(3), pp. 308-314.