---
_id: Bm7a3oPcZWpfyQkTn
title: Crux
href: https://forum.effectivealtruism.org/tag/crux
type: tag
tags:
  - LessWrong
  - Topic
  - Tag
synchedAt: '2022-09-11T14:33:34.653Z'
---
# Crux

A person's **crux** for holding a view on some topic is any belief on which that view depends, such that if the person ceased to have that belief, they would no longer hold the view. For example, a crux for someone who works in [[AI safety]] might be the belief that [[Transformative artificial intelligence|transformative artificial intelligence]] will arrive within the next few decades: ceasing to have this belief will cause the person to work on other causes or problems.^[\[1\]](#fnyw6bqo1xnx)^

A similar concept is that of a **true rejection**, or a person's crux for rejecting a view. Often, what is raised as an objection is not a true rejection, because the critic would still reject the view even if the objection was fully addressed.^[\[2\]](#fnqqhvhm141v)^

Another related concept is that of a [[Crucial consideration|crucial consideration]], or a consideration that warrants a major reassessment of a cause or intervention. A crucial consideration may be regarded as a  reason for rejecting a belief which is currently a crux for some cause or intervention, especially one considered to be high-priority.

Further reading
---------------

Sabien, Duncan (2021) [*Participant Handbook*](https://www.rationality.org/files/CFAR_Handbook_2021-01.pdf), Center for Applied Rationality, pp. 91–99.

Related entries
---------------

[[Crucial consideration|crucial consideration]] | [[Epistemology|epistemology]]

1.  ^**[^](#fnrefyw6bqo1xnx)**^
    
    Shlegeris, Buck (2020) [My personal cruxes for working on AI safety](https://forum.effectivealtruism.org/posts/Ayu5im98u8FeMWoBZ/my-personal-cruxes-for-working-on-ai-safety), *Effective Altruism Forum*, February 13.
    
2.  ^**[^](#fnrefqqhvhm141v)**^
    
    Yudkowsky, Eliezer (2008) [Is that your true rejection?](https://www.lesswrong.com/posts/TGux5Fhcd7GmTfNGC/is-that-your-true-rejection), *LessWrong*, December 6.