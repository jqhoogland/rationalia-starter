---
_id: yPshgJDmMhXmjESiA
title: State vs. step risk
href: https://forum.effectivealtruism.org/tag/state-vs-step-risk
type: tag
tags:
  - LessWrong
  - Topic
  - Tag
synchedAt: '2022-09-11T14:34:45.739Z'
---
# State vs. step risk

A **state risk** is a risk associated with being in a particular state, whereas a **step risk** (also called a **transition risk**) is a risk arising from transitioning to a new state. [[Nick Bostrom]] appears to have originated the distinction in his book [[Superintelligence (book)|*Superintelligence*]], although some mentions of it predate the book's publication.^[\[1\]](#fnc3n6aotkfn)^

The cumulative state risk associated with being in some state grows as a function of the time spent in that state. [[Natural existential risk|Natural existential risks]] are typically state risks. For example, absent deflection efforts, the risk that an [[Asteroids|asteroids]] of a certain size collides with Earth by 2030 is higher than the risk that it does so by 2029. The longer humanity exposes itself to a state risk, the higher its probability of succumbing to the associated catastrophe. For this reason, the are *pro tanto* reasons for reducing state risks as soon as possible.

Things are different with step risks. Here the threat arises only when the transition to the new state begins, and the overall risk incurred during this transition is not generally a function of its total duration. Thus, with step risks there is no presumption in favor or against either *postponing* or *prolonging* the transition; what is appropriate will vary depending on characteristics specific to each risk. Some [[Anthropogenic existential risk|anthropogenic existential risks]] are plausibly viewed as step risks, with [[AI risk]] being perhaps the clearest example.

Since state risks are correlated with natural existential risks, and step risks with anthropogenic existential risks, the latter's much greater share of [[Total existential risk|total existential risk]] suggests that most of this risk is posed by transitioning to new states, rather than by remaining in a given state.^[\[2\]](#fncykji9ilky)^Â This finding has important implications for the strategic management of existential risk.

Further reading
---------------

Bostrom, Nick (2014) [*Superintelligence: Paths, Dangers, Strategies*](https://en.wikipedia.org/wiki/Special:BookSources/9780199678112), Oxford: Oxford University Press, ch. 14.

Ord, Toby (2020) [*The Precipice: Existential Risk and the Future of Humanity*](https://en.wikipedia.org/wiki/Special:BookSources/1526600218), London: Bloomsbury Publishing, ch. 7.

Sandberg, Anders (2017) [Survivorship curves and existential risk](http://aleph.se/andart2/statistics/survivorship-curves-and-existential-risk/), *Andart II*, February 8.

Related entries
---------------

[[Anthropogenic existential risk|anthropogenic existential risk]] | [[EA/Topics/Existential risk|existential risk]] | [[Natural existential risk|natural existential risk]] | [[Total existential risk|total existential risk]]

1.  ^**[^](#fnrefc3n6aotkfn)**^
    
    For example Beckstead, Nick (2013) [How to compare broad and targeted attempts to shape the far future](http://intelligence.org/wp-content/uploads/2013/07/Beckstead-Evaluating-Options-Using-Far-Future-Standards.pdf), July 13.
    
2.  ^**[^](#fnrefcykji9ilky)**^
    
    Ord, Toby (2020) [*The Precipice: Existential Risk and the Future of Humanity*](https://en.wikipedia.org/wiki/Special:BookSources/1526600218), London: Bloomsbury Publishing.