---
_id: 8rQysEeqxtuBqr5Ya
title: Ethics of existential risk
href: https://forum.effectivealtruism.org/tag/ethics-of-existential-risk
type: tag
tags:
  - LessWrong
  - Topic
  - Tag
synchedAt: '2022-09-11T14:34:18.642Z'
---
# Ethics of existential risk

The **ethics of existential risk** is the study of the ethical issues related to [existential risk](/tag/existential-risk), including questions of how bad an existential catastrophe would be, how good it is to reduce existential risk, why those things are as bad or good as they are, and how this differs between different specific existential risks. There is a range of different perspectives on these questions, and these questions have implications for how much to [[Cause prioritization|prioritise]] reducing existential risk in general and which specific risks to prioritise reducing.

In [[The Precipice|*The Precipice*]], [[Toby Ord]] discusses five different "moral foundations" for assessing the value of existential risk reduction, depending on whether emphasis is placed on *the* *future*, *the* *present*, *the* *past*, *civilizational virtues* or *cosmic significance*.^[\[1\]](#fnjqfr1hk9l2)^

The future
----------

In one of the earliest discussions of the topic, [[Derek Parfit]] offers the following thought experiment:^[\[2\]](#fn5jhmxrywwz5)^

> I believe that if we destroy mankind, as we now can, this outcome will be much worse than most people think. Compare three outcomes:
> 
> 1.  Peace.
> 2.  A nuclear war that kills 99% of the world's existing population.
> 3.  A nuclear war that kills 100%.
> 
> (2) would be worse than (1), and (3) would be worse than (2). Which is the greater of these two differences? Most people believe that the greater difference is between (1) and (2). I believe that the difference between (2) and (3) is very much greater.

The scale of what is lost in an existential catastrophe is determined by humanity's long-term potential—all the value that would be realized if our species survived indefinitely. The [[Universe's resources|universe's resources]] could sustain a total of around \\(10^{35}\\) biological human beings, or around \\(10^{58}\\) digital human minds.^[\[3\]](#fng849b68vph4)^ And this may not exhaust all the relevant potential, if value supervenes on other things besides human or sentient minds, as some moral theories hold. 

In the effective altruism community, this is probably the ethical perspective most associated with existential risk reduction: existential risks are often seen as a pressing problem because of the astronomical amounts of value or disvalue potentially at stake over the course of the [[Long-term future|long-term future]].

The present
-----------

Some philosophers have defended views on which future or contingent people do not matter morally.^[\[4\]](#fngkpkn43j7sj)^ Even on such views, however, an existential catastrophe could be among the worst things imaginable: it would cut short the lives of every living [[Moral patienthood|moral patient]], destroying all of what makes their lives valuable, and most likely subjecting many of them to profound suffering. So even setting aside the value of future generations, a case for reducing existential risk could grounded in concern for presently existing beings.

This present-focused moral foundation could also be discussed as a "near-termist" or "[[Person-affecting views|person-affecting]]" argument for existential risk reduction.^[\[5\]](#fncu9upgvnfpp)^ In the effective altruism community, it appears to be the most commonly discussed non-longtermist ethical argument for existential risk reduction.

The past
--------

Humanity can be considered as a vast intergenerational partnership, engaged in the task of gradually increasing its stock of art, culture, wealth, science and technology. In Edmund Burke's words, "As the ends of such a partnership cannot be obtained except in many generations, it becomes a partnership not only between those who are living, but between those who are living, those who are dead, and those who are to be born."^[\[6\]](#fn1imvz2a4xvz)^ On this view, a generation that allowed an existential catastrophe to occur may be regarded as failing to discharge a moral duty owed to all previous generations.^[\[7\]](#fnw0c0ti890m)^ 

Civilizational virtues
----------------------

Instead of focusing on the impacts of individual human action, one can consider the dispositions and character traits displayed by humanity as a whole, which Ord calls *civilizational virtues*.^[\[8\]](#fn6ge6c7wksq7)^ An ethical framework that attached intrinsic moral significance to the cultivation and exercise of virtue would regard the neglect of existential risks as showing "a staggering deficiency of patience, prudence, and wisdom."^[\[9\]](#fnb6yuvdxscbw)^

Cosmic significance
-------------------

At the beginning of *On What Matters*, Parfit writes that "We are the animals that can both understand and respond to reasons. \[...\] We may be the only rational beings in the Universe."^[\[10\]](#fnfgclwd428uv)^ If this is so, then, as Ord writes, "responsibility for the history of the universe is entirely *on us*: this is the only chance ever to shape the universe toward what is right, what is just, what is best for all."^[\[11\]](#fnvdtrojctljo)^ In addition, it may be the only chance for the universe to understand itself.

Evaluating and prioritizing existential risk reduction
------------------------------------------------------

It is important to distinguish between the question of whether a given ethical perspective would see existential risk reduction as net positive and the question of whether that ethical perspective would *prioritise* existential risk reduction, and this distinction is not always made.^[\[12\]](#fn3sxta69061k)^ One reason this matters is that existential risk reduction may be much [[ITN framework|less tractable and perhaps less neglected]] than some other cause areas (e.g., near-term [[Farmed animal welfare|farmed animal welfare]]), but with that being made up for by far greater importance from a longtermist perspective. Therefore, if one adopts an ethical perspective that just sees existential risk reduction as similarly important to other major global issues, existential risk reduction may no longer seem worth prioritising.

Further reading
---------------

Aird, Michael (2021) [Why I think *The Precipice* might understate the significance of population ethics](https://forum.effectivealtruism.org/posts/EMKf4Gyee7BsY2RP8/michaela-s-shortform?commentId=mmoBoot3x8xBKWfgS), *Effective Altruism Forum*, January 5.

Ord, Toby (2020) [*The Precipice: Existential Risk and the Future of Humanity*](https://en.wikipedia.org/wiki/Special:BookSources/1526600218), London: Bloomsbury Publishing, ch. 2.

Related entries
---------------

[astronomical waste](/tag/astronomical-waste) | [existential risk](/tag/existential-risk) | [longtermism](/tag/longtermism) | [moral philosophy](/tag/moral-philosophy) | [moral uncertainty](/tag/moral-uncertainty) | [person-affecting views](/tag/person-affecting-views) | [population ethics](/tag/population-ethics) | [prioritarianism](/tag/prioritarianism) | [s-risk](/tag/s-risk) | [suffering-focused ethics ](/tag/suffering-focused-ethics)

1.  ^**[^](#fnrefjqfr1hk9l2)**^
    
    Ord, Toby (2020) [*The Precipice: Existential Risk and the Future of Humanity*](https://en.wikipedia.org/wiki/Special:BookSources/1526600218), London: Bloomsbury Publishing.
    
2.  ^**[^](#fnref5jhmxrywwz5)**^
    
    Parfit, Derek (1984) [*Reasons and Persons*](https://en.wikipedia.org/wiki/Special:BookSources/0-19-824908-X), Oxford: Clarendon press, pp. 453–454.
    
3.  ^**[^](#fnrefg849b68vph4)**^
    
    Bostrom, Nick, Allan Dafoe & Carrick Flynn (2020) [*Public Policy and Superintelligent AI*](http://doi.org/10.1093/oso/9780190905033.003.0011), In S. Matthew Liao (ed.), *Ethics of Artificial Intelligence*, Oxford: Oxford University Press, p. 319.
    
4.  ^**[^](#fnrefgkpkn43j7sj)**^
    
    Narveson, Jan (1973) [Moral problems of population](https://doi.org/10.5840/monist197357134), *Monist*, vol. 57, pp. 62–86.
    
5.  ^**[^](#fnrefcu9upgvnfpp)**^
    
    Lewis, Gregory (2018) [The person-affecting value of existential risk reduction](https://forum.effectivealtruism.org/posts/dfiKak8ZPa46N7Np6/the-person-affecting-value-of-existential-risk-reduction), *Effective Altruism Forum*, April 13.
    
6.  ^**[^](#fnref1imvz2a4xvz)**^
    
    Burke, Edmund (1790) *Reflections on the Revolution in France*, London: J. Dodsley, p. 193.
    
7.  ^**[^](#fnrefw0c0ti890m)**^
    
    Ord (2020) [*The Precipice*](https://en.wikipedia.org/wiki/Special:BookSources/1526600218), pp. 49–53.
    
8.  ^**[^](#fnref6ge6c7wksq7)**^
    
    Ord (2020) [*The Precipice*](https://en.wikipedia.org/wiki/Special:BookSources/1526600218), p. 53.
    
9.  ^**[^](#fnrefb6yuvdxscbw)**^
    
    Grimes, Barry (2020) [Toby Ord: Fireside chat and Q&A](https://forum.effectivealtruism.org/posts/QHxjRx8zpqL4xxsXT/toby-ord-fireside-chat-and-q-and-a), *Effective Altruism Global*, March 21.
    
10.  ^**[^](#fnreffgclwd428uv)**^
    
    Parfit, Derek (2011) [*On What Matters*](https://en.wikipedia.org/wiki/Special:BookSources/978-0-19-926592-3), vol. 1, Oxford: Oxford University Press, p. 31.
    
11.  ^**[^](#fnrefvdtrojctljo)**^
    
    Ord (2020) [*The Precipice*](https://en.wikipedia.org/wiki/Special:BookSources/1526600218), pp. 53 and 55.
    
12.  ^**[^](#fnref3sxta69061k)**^
    
    See Daniel, Max (2020) [Comment on 'What are the leading critiques of longtermism and related concepts'](https://forum.effectivealtruism.org/posts/jiwgT3WvMkwpWP4BC/what-are-the-leading-critiques-of-longtermism-and-related?commentId=pzT6AS2FBsAcZHpBp), *Effective Altruism Forum*, June 4.