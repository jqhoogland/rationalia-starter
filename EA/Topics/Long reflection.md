---
_id: LY5WnHuF8QjvhksGM
title: Long reflection
href: https://forum.effectivealtruism.org/tag/long-reflection
type: tag
tags:
  - LessWrong
  - Topic
  - Tag
synchedAt: '2022-09-11T14:35:36.983Z'
---
# Long reflection

The **long reflection** is a hypothesized period of time during which humanity works out how best to realize its long-term potential.

Some effective altruists, including [[Toby Ord]] and [[William MacAskill]], have argued that, if humanity succeeds in eliminating [[EA/Topics/Existential risk|existential risk]] or reducing it to acceptable levels, it should not immediately embark on an ambitious and potentially irreversible project of arranging the [[Universe's resources|universe's resources]] in accordance to its values, but ought instead to spend considerable time— "centuries (or more)";^[\[1\]](#fnipthilteu8)^ "perhaps tens of thousands of years";^[\[2\]](#fnw82ait03vnf)^ "thousands or millions of years";^[\[3\]](#fnr3gb671cl2)^ "\[p\]erhaps... a million years"^[\[4\]](#fn7t0vu6w86yd)^—figuring out what is in fact of value. The long reflection may thus be seen as an intermediate stage in a rational long-term human developmental trajectory, following an initial stage of [[Existential security|existential security]] when existential risk is drastically reduced and followed by a final stage when humanity's potential is fully realized.^[\[1\]](#fnipthilteu8)^

Criticism
---------

The idea of a long reflection has been criticized on the grounds that virtually eliminating all existential risk will almost certainly require taking a variety of large-scale, irreversible decisions—related to [[Space colonization|space colonization]], [[Global governance|global governance]], [[Cognitive enhancement|cognitive enhancement]], and so on—which are precisely the decisions meant to be discussed during the long reflection.^[\[5\]](#fnd395d9czg1k)^^[\[6\]](#fn8t05x5yy03)^ Since there are pervasive and inescapable tradeoffs between reducing existential risk and retaining moral option value, it may be argued that it does not make sense to frame humanity's long-term strategic picture as one consisting of two distinct stages, with one taking precedence over the other.

Further reading
---------------

Aird, Michael (2020) [Collection of sources that are highly relevant to the idea of the Long Reflection](https://forum.effectivealtruism.org/posts/H2zno3ggRJaph9P6c/quotes-about-the-long-reflection?commentId=z2ybSC353mPHpCjbn), *Effective Altruism Forum*, June 20.  
*Many additional resources on this topic.*

Wiblin, Robert & Keiran Harris (2018) [Our descendants will probably see us as moral monsters. what should we do about that?](https://80000hours.org/podcast/episodes/will-macaskill-moral-philosophy/), *80,000 Hours*, January 19.  
*Interview with William MacAskill about the long reflection and other topics.*

Related entries
---------------

[[Dystopia|dystopia]] | [[EA/Topics/Existential risk|existential risk]] | [[Existential security|existential security]] | [[Long-term future|long-term future]] | [[EA/Topics/Longtermism|longtermism]] | [[Longtermist institutional reform|longtermist institutional reform]] | [[Moral uncertainty|moral uncertainty]] | [[Normative ethics|normative ethics]] | [[Value lock-in|value lock-in]]

1.  ^**[^](#fnrefipthilteu8)**^
    
    Ord, Toby (2020) [*The Precipice: Existential Risk and the Future of Humanity*](https://en.wikipedia.org/wiki/Special:BookSources/1526600218), London: Bloomsbury Publishing.
    
2.  ^**[^](#fnrefw82ait03vnf)**^
    
    Greaves, Hilary *et al.* (2019) [A research agenda for the Global Priorities Institute](https://globalprioritiesinstitute.org/wp-content/uploads/2017/12/gpi-research-agenda.pdf), Oxford.
    
3.  ^**[^](#fnrefr3gb671cl2)**^
    
    Dai, Wei (2019) [The argument from philosophical difficulty](https://www.lesswrong.com/posts/w6d7XBCegc96kz4n3/the-argument-from-philosophical-difficulty), *LessWrong*, February 9.
    
4.  ^**[^](#fnref7t0vu6w86yd)**^
    
    William MacAskill, in Perry, Lucas (2018) [AI alignment podcast: moral uncertainty and the path to AI alignment with William MacAskill](https://futureoflife.org/2018/09/17/moral-uncertainty-and-the-path-to-ai-alignment-with-william-macaskill/), *AI Alignment podcast*, September 17.
    
5.  ^**[^](#fnrefd395d9czg1k)**^
    
    Stocker, Felix (2020) [Reflecting on the long reflection](https://www.felixstocker.com/blog/reflecting-on-the-long-reflection), *Felix Stocker’s Blog*, August 14.
    
6.  ^**[^](#fnref8t05x5yy03)**^
    
    Hanson, Robin (2021) [‘Long reflection’ is crazy bad idea](https://www.overcomingbias.com/2021/10/long-reflection-is-crazy-bad-idea.html), *Overcoming Bias*, October 20.