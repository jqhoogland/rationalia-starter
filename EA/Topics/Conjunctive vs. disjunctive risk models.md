---
_id: XyJasPR53z83aiRxE
title: Conjunctive vs. disjunctive risk models
href: https://forum.effectivealtruism.org/tag/conjunctive-vs-disjunctive-risk-models
type: tag
tags:
  - LessWrong
  - Topic
  - Tag
synchedAt: '2022-09-11T14:33:30.347Z'
---
# Conjunctive vs. disjunctive risk models

Models of [[Global catastrophic risk|catastrophic risks]] can be conjunctive or disjunctive. A **conjunctive risk model** is one in which the disaster is caused by the co-occurrence of multiple conditions (\\(D = C\_1 \\cap \\ldots \\cap C\_k\\)). In a conjunctive model, the probability of the disaster is *less than or equal to* the probabilities of the individual conditions. By contrast, a **disjunctive risk model** is one in which the disaster occurs as a result of *any* of several conditions holding (\\(D = C\_1 \\cup \\ldots \\cup C\_k\\)). In a disjunctive model, the probability of the disaster is *greater than or equal to* the probabilities of the individual conditions.

Examples of conjunctive and disjunctive risk models of [[AI risk]]:

*   Joseph Carlsmith's models existential risk from power-seeking AI conjunctively, i.e. as the intersection of six conditions, all of which must be true for the existential catastrophe to occur.^[\[1\]](#fnuuyfwmuig9)^
*   By contrast, Nate Soares's models AGI risk disjunctively, i.e. as the union of multiple conditions, any of which can cause existential catastrophe.^[\[2\]](#fnwqodvw1cjhs)^

Both types of models are simplifying assumptions. In reality, a disaster can be caused by multiple conditions that interact conjunctively *and* disjunctively. For example, a disaster \\(D\\) could occur if conditions \\(C_1\\) and \\(C_2\\) are true, or if condition \\(C_3\\) is true: \\(D = (C\_1 \\cap C\_2) \\cup C_3\\).

Further reading
---------------

Soares, Nate (2021) [Comments on Carlsmith’s “Is power-seeking AI an existential risk?”](https://www.lesswrong.com/posts/cCMihiwtZx7kdcKgt/comments-on-carlsmith-s-is-power-seeking-ai-an-existential), *LessWrong*, November 13. 

Related entries
---------------

[[Compound existential risk|compound existential risk]] | [[EA/Topics/Existential risk|existential risk]] | [[Existential risk factor|existential risk factor]] | [[Global catastrophic risk|global catastrophic risk]]

1.  ^**[^](#fnrefuuyfwmuig9)**^
    
    Carlsmith, Joseph (2021) [Draft report on existential risk from power-seeking AI](https://forum.effectivealtruism.org/posts/78NoGoRitPzeT8nga/draft-report-on-existential-risk-from-power-seeking-ai), *Effective Altruism Forum*, April 28. 
    
2.  ^**[^](#fnrefwqodvw1cjhs)**^
    
    Soares, Nate (2022) [AGI ruin scenarios are likely (and disjunctive)](https://forum.effectivealtruism.org/posts/vC6v2iTafkydBvnz7/agi-ruin-scenarios-are-likely-and-disjunctive), *Effective Altruism Forum*, July 27.