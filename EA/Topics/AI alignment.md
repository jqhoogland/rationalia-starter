---
_id: 9GQf4Ec6ckqvnPBSw
title: AI alignment
href: https://forum.effectivealtruism.org/tag/ai-alignment
type: tag
tags:
  - LessWrong
  - Topic
  - Tag
synchedAt: '2022-09-11T14:35:45.094Z'
---
# AI alignment

**AI alignment** is research on how to align [[Artificial intelligence|AI systems]] with human or moral goals.

Evaluation
----------

[[80,000 Hours]] rates AI alignment a "highest priority area": a problem at the top of their ranking of global issues assessed by [[ITN framework|importance, tractability and neglectedness]].^[\[1\]](#fnorjzl17i9vi)^

Further reading
---------------

Christiano, Paul (2020) [Current work in AI alignment](https://forum.effectivealtruism.org/posts/63stBTw3WAW6k45dY/paul-christiano-current-work-in-ai-alignment), *Effective Altruism Forum*, April 3.

Shah, Rohin (2020) [What’s been happening in AI alignment?](https://forum.effectivealtruism.org/posts/nqTdRNngCGDD54owu/rohin-shah-what-s-been-happening-in-ai-alignment), *Effective Altruism Forum*, July 29.

External links
--------------

[AI Alignment Forum](https://alignmentforum.org/).

Related entries
---------------

[[AI governance]] | [[AI forecasting]] | [[Alignment tax|alignment tax]] | [[Center for Human-Compatible Artificial Intelligence]] | [Machine Intelligence Research Institute](/tag/machine-intelligence-research-institute) | [[Rationality community|rationality community]]

1.  ^**[^](#fnreforjzl17i9vi)**^
    
    80,000 Hours (2021) [Our current list of the most important world problems](https://80000hours.org/problem-profiles/), *80,000 Hours*.