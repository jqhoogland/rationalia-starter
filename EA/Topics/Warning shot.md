---
_id: xSZDegEMcbexn84K5
title: Warning shot
href: https://forum.effectivealtruism.org/tag/warning-shot
type: tag
tags:
  - LessWrong
  - Topic
  - Tag
synchedAt: '2022-09-11T14:34:41.567Z'
---
# Warning shot

A **warning shot** is a [[Global catastrophic risk|global catastrophe]] that indirectly reduces [[EA/Topics/Existential risk|existential risk]] by increasing concern about future catastrophes. 

Terminology
-----------

The expression **warning sign** is sometimes used to describe any event that increases concern about a particular category of existential risk, regardless of whether the event itself constitutes a global catastrophe. For example, plausible candidates for an AI warning sign include not only a catastrophic failure by an AI system but also public outreach campaigns or the publication of an exceptionally persuasive book on AI safety.^[\[1\]](#fn3vg8vj5l8a8)^ 

A related notion is that of a **fire alarm**, a warning sign that creates *common knowledge* that some technology—typically [[Artificial intelligence|avanced artificial intelligence]]—actually poses an existential risk.^[\[2\]](#fnk1kxvv8081)^

Note, however, that both "warning shot" and "fire alarm" are sometimes used as synonyms for "warning sign".^[\[3\]](#fnipfdul9zh8q)^^[\[4\]](#fnwq9pdwhdojb)^

Further reading
---------------

Beckstead, Nick (2015) [The long-term significance of reducing global catastrophic risks](https://www.openphilanthropy.org/blog/long-term-significance-reducing-global-catastrophic-risks), *Open Philanthropy*, August 13.

Carlsmith, Joseph (2021) [Is power-seeking AI an existential risk?](https://docs.google.com/document/d/1smaI1lagHHcrhoi6ohdq3TYIZv0eNWWZMPEy8C8byYg/edit?usp=embed_facebook), *Open Philanthropy*, April, section 6.2.

Related entries
---------------

[[Artificial intelligence|artificial intelligence]] | [[EA/Topics/Existential risk|existential risk]] | [[Global catastrophic risk|global catastrophic risk]]

1.  ^**[^](#fnref3vg8vj5l8a8)**^
    
    Hobson, Donald (2020) [Comment on ‘What are the most plausible “AI Safety warning shot” scenarios?’](https://www.alignmentforum.org/posts/hLKKH9CM6NDiJBabC/what-are-the-most-plausible-ai-safety-warning-shot-scenarios), *AI Alignment Forum*, March 26.
    
2.  ^**[^](#fnrefk1kxvv8081)**^
    
    Grace, Katja (2021) [Beyond fire alarms: freeing the groupstruck](https://aiimpacts.org/beyond-fire-alarms-freeing-the-groupstruck/), *AI Impacts*, September 26.
    
3.  ^**[^](#fnrefipfdul9zh8q)**^
    
    Kokotajlo, Daniel (2020) [What are the most plausible ‘AI Safety warning shot’ scenarios?](https://www.alignmentforum.org/posts/hLKKH9CM6NDiJBabC/what-are-the-most-plausible-ai-safety-warning-shot-scenarios), *AI Alignment Forum*, March 26.
    
4.  ^**[^](#fnrefwq9pdwhdojb)**^
    
    McCluskey, Peter (2021) [AI fire alarm scenarios](http://www.bayesianinvestor.com/blog/index.php/2021/12/23/ai-fire-alarm-scenarios/), *Bayesian Investor*, December 23.