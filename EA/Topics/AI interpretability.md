---
_id: 5HfPzYvjYyXMKtspa
title: AI interpretability
href: https://forum.effectivealtruism.org/tag/ai-interpretability
type: tag
tags:
  - LessWrong
  - Topic
  - Tag
synchedAt: '2022-09-11T14:33:37.717Z'
---
# AI interpretability

**Interpretability** is the ability for the decision processes and inner workings of AI and machine learning systems to be understood by humans or other outside observers.^[\[1\]](#fnsg4kuuvihg)^

Present-day machine learning systems are typically not very transparent or interpretable. You can use a model's output, but the model can't tell you why it made that output. This makes it hard to determine the cause of biases in ML models.^[\[1\]](#fnsg4kuuvihg)^

Interpretability is a focus of Chris Olah and [[Anthropic]]'s work, though most AI alignment organisations work on interpretability to some extent, such as [[Redwood Research]]^[\[2\]](#fnwvmtlgzzbfc)^.

Related entries
---------------

[[AI risk]] | [[AI safety]] |Â [[Artificial intelligence]]

1.  ^**[^](#fnrefsg4kuuvihg)**^
    
    Multicore (2020) [Transparency / Interpretability (ML & AI)](https://www.alignmentforum.org/tag/transparency-interpretability-ml-and-ai), *AI Alignment Forum*, August 1.
    
2.  ^**[^](#fnrefwvmtlgzzbfc)**^
    
    Shlegeris, Buck (2022) [Answer to 'How might a herd of interns help with AI or biosecurity research tasks/questions?'](https://forum.effectivealtruism.org/posts/HZacQkvLLeLKT3a6j/how-might-a-herd-of-interns-help-with-ai-or-biosecurity?commentId=XcYk3Pux9WmgEcaoi), *Effective Altruism Forum*, March 21.