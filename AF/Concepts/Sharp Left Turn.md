---
type: jargon
---

[[A central AI alignment problem— capabilities generalization, and the sharp left turn]]

From [Pragmatic AI Safety #2](https://www.lesswrong.com/s/FaEBwhhe3otzYKGQt/p/AtfQFj8umeyBBkkxa):

> [AlphaZero](https://arxiv.org/pdf/2111.09259.pdf) experienced a phase transition where internal representations changed dramatically and capabilities altered significantly at about ~32,000 steps, when the system learned concepts like “king safety, threats, and mobility” suddenly. This can be seen by looking at the system’s preferred opening moves, the distribution of which dramatically changes at 32,000 steps.

![](https://lh3.googleusercontent.com/mhneYp8N23ZUC8VVKwXc757ZT39wAK3ph2SQ8I5a35FJCmD7meJHh0bTRUqIm9PC7JF4yxC1eU0nR5FRn7s9aorb3wwNFfvnmm8TkzCGHwTYJuvuZKFXdbJQd3aMqAkBLdW9GRFM_XLUbKIsYA)