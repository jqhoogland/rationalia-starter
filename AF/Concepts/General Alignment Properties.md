---
_id: woeLa6nvmhroCh3Fi
title: General Alignment Properties
href: https://www.lesswrong.com/tag/general-alignment-properties
slug: general-alignment-properties
type: tag
tags:
  - LessWrong
  - Concept
  - Tag
synchedAt: '2022-09-01T09:42:42.437Z'
status: todo
---

# General Alignment Properties

The AI alignment properties of agents which would be interesting to a range of principals trying to solve AI alignment. For example:

- Does the AI "care" about reality, or just about its sensory observations?
- Does the AI properly navigate [ontological shifts](https://arbital.com/p/ontology_identification/)?
- Does the AI reason about itself as embedded in its environment?


%%

% START
Basic (and reversed card)
What is **General Alignment Properties**?
Back: {TODO}
Tags: LessWrong
END
<!--ID: 1663157029051-->


%%
	
