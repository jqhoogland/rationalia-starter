---
_id: BisjoDrd3oNatDu7X
title: Outer Alignment
href: https://www.lesswrong.com/tag/outer-alignment
slug: outer-alignment
type: tag
tags:
  - LessWrong
  - Concept
  - Tag
synchedAt: '2022-09-01T09:42:32.319Z'
status: done
---

# Outer Alignment

**Outer Alignment** in the context of machine learning is the property where the specified loss function is aligned with the intended goal of its designers. This is an intuitive notion, in part because human intentions are themselves not well-understood. This is what is typically discussed as the 'value alignment' problem. It is contrasted with [[Inner Alignment|inner alignment]], which discusses if an optimizer is the production of an outer aligned system, then whether that optimizer is itself aligned.

*See also:*


