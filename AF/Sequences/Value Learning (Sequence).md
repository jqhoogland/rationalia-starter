---
_id: 4dHMdK5TLN6xcqtyc
type: sequence
title: Value Learning
curatedOrder: 2
href: https://www.lesswrong.com/s/4dHMdK5TLN6xcqtyc
synchedAt: '2022-09-15T19:21:48.895Z'
tags:
  - Sequence
status: todo
---
# Value Learning

This is a sequence investigating the feasibility of one approach to AI alignment: value learning.

# Value Learning


- [[Preface to the sequence on value learning]]

# Ambitious Value Learning


- [[What is ambitious value learning?]]
- [[The easy goal inference problem is still hard]]
- [[Humans can be assigned any values whatsoeverâ€¦]]
- [[Latent Variables and Model Mis-Specification]]
- [[Model Mis-specification and Inverse Reinforcement Learning]]
- [[Future directions for ambitious value learning]]

# Goals vs Utility Functions

Ambitious value learning aims to give the AI the correct utility function to avoid catastrophe. Given its difficulty, we revisit the arguments for utility functions in the first place.

- [[Intuitions about goal-directed behavior]]
- [[Coherence arguments do not entail goal-directed behavior]]
- [[Will humans build goal-directed agents?]]
- [[AI safety without goal-directed behavior]]

# Narrow Value Learning



- [[What is narrow value learning?]]
- [[Ambitious vs. narrow value learning]]
- [[Human-AI Interaction]]
- [[Reward uncertainty]]
- [[The human side of interaction]]
- [[Following human norms]]
- [[Future directions for narrow value learning]]
- [[Conclusion to the sequence on value learning]]