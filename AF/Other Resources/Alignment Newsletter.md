---
type: sequence
href: https://www.alignmentforum.org/s/dT7CKGXwq9vt76CeX
---

# Alignment Newsletter



- [[The Alignment Newsletter 1— 04-09-18]]
- [[The Alignment Newsletter 2— 04-16-18]]
- [[The Alignment Newsletter 3— 04-23-18]]
- [[The Alignment Newsletter 4— 04-30-18]]
- [[The Alignment Newsletter 5— 05-07-18]]
- [[The Alignment Newsletter 6— 05-14-18]]
- [[The Alignment Newsletter 7— 05-21-18]]
- [[The Alignment Newsletter 8— 05-28-18]]
- [[The Alignment Newsletter 9— 06-04-18]]
- [[The Alignment Newsletter 10— 06-11-18]]
- [[The Alignment Newsletter 11— 06-18-18]]
- [[The Alignment Newsletter 12— 06-25-18]]
- [[Alignment Newsletter 13— 07-02-18]]
- [[Alignment Newsletter 14]]
- [[Alignment Newsletter 15— 07-16-18]]
- [[Alignment Newsletter 16— 07-23-18]]
- [[Alignment Newsletter 17]]
- [[Alignment Newsletter 18]]
- [[Alignment Newsletter 19]]
- [[Alignment Newsletter 20]]
- [[Alignment Newsletter 21]]
- [[Alignment Newsletter 22]]
- [[Alignment Newsletter 23]]
- [[Alignment Newsletter 24]]
- [[Alignment Newsletter 25]]
- [[Alignment Newsletter 26]]
- [[Alignment Newsletter 27]]
- [[Alignment Newsletter 28]]
- [[Alignment Newsletter 29]]
- [[Alignment Newsletter 30]]
- [[Alignment Newsletter 31]]
- [[Alignment Newsletter 32]]
- [[Alignment Newsletter 33]]
- [[Alignment Newsletter 34]]
- [[Alignment Newsletter 35]]
- [[Alignment Newsletter 36]]
- [[Alignment Newsletter 37]]
- [[Alignment Newsletter 38]]
- [[Alignment Newsletter 39]]
- [[Alignment Newsletter 40]]
- [[Alignment Newsletter 41]]
- [[Alignment Newsletter 42]]
- [[Alignment Newsletter 43]]
- [[Alignment Newsletter 44]]
- [[Alignment Newsletter 45]]
- [[Alignment Newsletter 46]]
- [[Alignment Newsletter 47]]
- [[Alignment Newsletter 48]]
- [[Alignment Newsletter 49]]
- [[Alignment Newsletter 50]]
- [[Alignment Newsletter 51]]
- [[Alignment Newsletter 52]]
- [[Alignment Newsletter One Year Retrospective]]
- [[Alignment Newsletter 53]]
- [[[AN 54] Boxing a finite-horizon AI system to keep it unambitious]]
- [[[AN 55] Regulatory markets and international standards as a means of ensuring beneficial AI]]
- [[[AN 56] Should ML researchers stop running experiments before making hypotheses?]]
- [[[AN 57] Why we should focus on robustness in AI safety, and the analogous problems in programming]]
- [[[AN 58] Mesa optimization— what it is, and why we should care]]
- [[[AN 59] How arguments for AI risk have changed over time]]
- [[[AN 60] A new AI challenge— Minecraft agents that assist human players in creative mode]]
- [[[AN 61] AI policy and governance, from two people in the field]]
- [[[AN 62] Are adversarial examples caused by real but imperceptible features?]]
- [[[AN 63] How architecture search, meta learning, and environment design could lead to general intelligence]]
- [[[AN 64]— Using Deep RL and Reward Uncertainty to Incentivize Preference Learning]]
- [[[AN 65]— Learning useful skills by watching humans “play”]]
- [[[AN 66]— Decomposing robustness into capability robustness and alignment robustness]]
- [[[AN 67]— Creating environments in which to study inner alignment failures]]
- [[[AN 68]— The attainable utility theory of impact]]
- [[[AN 69] Stuart Russell's new book on why we need to replace the standard model of AI]]
- [[[AN 70]— Agents that help humans who are still learning about their own preferences]]
- [[[AN 71]— Avoiding reward tampering through current-RF optimization]]
- [[[AN 72]— Alignment, robustness, methodology, and system building as research priorities for AI safety]]
- [[[AN 73]— Detecting catastrophic failures by learning how agents tend to break]]
- [[[AN 74]— Separating beneficial AI into competence, alignment, and coping with impacts]]
- [[[AN 75]— Solving Atari and Go with learned game models, and thoughts from a MIRI employee]]
- [[[AN 76]— How dataset size affects robustness, and benchmarking safe exploration by measuring constraint violations]]
- [[[AN 77]— Double descent— a unification of statistical theory and modern ML practice]]
- [[[AN 78] Formalizing power and instrumental convergence, and the end-of-year AI safety charity comparison]]
- [[[AN 79]— Recursive reward modeling as an alignment technique integrated with deep RL]]
- [[[AN 80]— Why AI risk might be solved without additional intervention from longtermists]]
- [[[AN 81]— Universality as a potential solution to conceptual difficulties in intent alignment]]
- [[[AN 82]— How OpenAI Five distributed their training computation]]
- [[[AN 83]— Sample-efficient deep learning with ReMixMatch]]
- [[[AN 84] Reviewing AI alignment work in 2018-19]]
- [[[AN 85]— The normative questions we should be asking for AI alignment, and a surprisingly good chatbot]]
- [[[AN 86]— Improving debate and factored cognition through human experiments]]
- [[[AN 87]— What might happen as deep learning scales even further?]]
- [[[AN 88]— How the principal-agent literature relates to AI risk]]
- [[[AN 89]— A unifying formalism for preference learning algorithms]]
- [[[AN 90]— How search landscapes can contain self-reinforcing feedback loops]]
- [[[AN 91]— Concepts, implementations, problems, and a benchmark for impact measurement]]
- [[[AN 92]— Learning good representations with contrastive predictive coding]]
- [[[AN 93]— The Precipice we’re standing at, and how we can back away from it]]
- [[[AN 94]— AI alignment as translation between humans and machines]]
- [[[AN 95]— A framework for thinking about how to make AI go well]]
- [[[AN 96]— Buck and I discuss, argue about AI Alignment]]
- [[[AN 97]— Are there historical examples of large, robust discontinuities?]]
- [[[AN 98]— Understanding neural net training by seeing which gradients were helpful]]
- [[[AN 99]— Doubling times for the efficiency of AI algorithms]]
- [[[AN 100]— What might go wrong if you learn a reward function while acting]]
- [[[AN 101]— Why we should rigorously measure and forecast AI progress]]
- [[[AN 102]— Meta learning by GPT-3, and a list of full proposals for AI alignment]]
- [[[AN 103]— ARCHES— an agenda for existential safety, and combining natural language with deep RL]]
- [[[AN 104]— The perils of inaccessible information, and what we can learn about AI alignment from COVID]]
- [[[AN 105]— The economic trajectory of humanity, and what we might mean by optimization]]
- [[[AN 106]— Evaluating generalization ability of learned reward models]]
- [[[AN 107]— The convergent instrumental subgoals of goal-directed agents]]
- [[[AN 108]— Why we should scrutinize arguments for AI risk]]
- [[[AN 109]— Teaching neural nets to generalize the way humans would]]
- [[[AN 110]— Learning features from human feedback to enable reward learning]]
- [[[AN 111]— The Circuits hypotheses for deep learning]]
- [[[AN 112]— Engineering a Safer World]]
- [[[AN 113]— Checking the ethical intuitions of large language models]]
- [[[AN 114]— Theory-inspired safety solutions for powerful Bayesian RL agents]]
- [[[AN 115]— AI safety research problems in the AI-GA framework]]
- [[[AN 116]— How to make explanations of neurons compositional]]
- [[[AN 117]— How neural nets would fare under the TEVV framework]]
- [[[AN 118]— Risks, solutions, and prioritization in a world with many AI systems]]
- [[[AN 119]— AI safety when agents are shaped by environments, not rewards]]
- [[[AN 120]— Tracing the intellectual roots of AI and AI alignment]]
- [[[AN 121]— Forecasting transformative AI timelines using biological anchors]]
- [[[AN 122]— Arguing for AGI-driven existential risk from first principles]]
- [[[AN 123]— Inferring what is valuable in order to align recommender systems]]
- [[[AN 124]— Provably safe exploration through shielding]]
- [[[AN 125]— Neural network scaling laws across multiple modalities]]
- [[[AN 126]— Avoiding wireheading by decoupling action feedback from action effects]]
- [[[AN 127]— Rethinking agency— Cartesian frames as a formalization of ways to carve up the world into an agent and its environment]]
- [[[AN 128]— Prioritizing research on AI existential safety based on its application to governance demands]]
- [[[AN 129]— Explaining double descent by measuring bias and variance]]
- [[[AN 130]— A new AI x-risk podcast, and reviews of the field]]
- [[[AN 131]— Formalizing the argument of ignored attributes in a utility function]]
- [[[AN 132]— Complex and subtly incorrect arguments as an obstacle to debate]]
- [[[AN 133]— Building machines that can cooperate (with humans, institutions, or other machines)]]
- [[[AN 134]— Underspecification as a cause of fragility to distribution shift]]
- [[[AN 135]— Five properties of goal-directed systems]]
- [[[AN 136]— How well will GPT-N perform on downstream tasks?]]
- [[[AN 137]— Quantifying the benefits of pretraining on downstream task performance]]
- [[[AN 138]— Why AI governance should find problems rather than just solving them]]
- [[[AN 139]— How the simplicity of reality explains the success of neural nets]]
- [[[AN 140]— Theoretical models that predict scaling laws]]
- [[[AN 141]— The case for practicing alignment work on GPT-3 and other large models]]
- [[[AN 142]— The quest to understand a network well enough to reimplement it by hand]]
- [[[AN 143]— How to make embedded agents that reason probabilistically about their environments]]
- [[[AN 144]— How language models can also be finetuned for non-language tasks]]
- [[Alignment Newsletter Three Year Retrospective]]
- [[[AN 145]— Our three year anniversary!]]
- [[[AN 146]— Plausible stories of how we might fail to avert an existential catastrophe]]
- [[[AN 147]— An overview of the interpretability landscape]]
- [[[AN 148]— Analyzing generalization across more axes than just accuracy or loss]]
- [[[AN 149]— The newsletter's editorial policy]]
- [[[AN 150]— The subtypes of Cooperative AI research]]
- [[[AN 151]— How sparsity in the final layer makes a neural net debuggable]]
- [[[AN 152]— How we’ve overestimated few-shot learning capabilities]]
- [[[AN 153]— Experiments that demonstrate failures of objective robustness]]
- [[[AN 154]— What economic growth theory has to say about transformative AI]]
- [[[AN 155]— A Minecraft benchmark for algorithms that learn without reward functions]]
- [[[AN 156]— The scaling hypothesis— a plan for building AGI]]
- [[[AN 157]— Measuring misalignment in the technology underlying Copilot]]
- [[[AN 158]— Should we be optimistic about generalization?]]
- [[[AN 159]— Building agents that know how to experiment, by training on procedurally generated games]]
- [[[AN 160]— Building AIs that learn and think like people]]
- [[[AN 161]— Creating generalizable reward functions for multiple tasks by learning a model of functional similarity]]
- [[[AN 162]— Foundation models— a paradigm shift within AI]]
- [[[AN 163]— Using finite factored sets for causal and temporal inference]]
- [[[AN 164]— How well can language models write code?]]
- [[[AN 165]— When large models are more likely to lie]]
- [[[AN 166]— Is it crazy to claim we're in the most important century?]]
- [[[AN 167]— Concrete ML safety problems and their relevance to x-risk]]
- [[[AN 168]— Four technical topics for which Open Phil is soliciting grant proposals]]
- [[[AN 169]— Collaborating with humans without human data]]
- [[[AN 170]— Analyzing the argument for risk from power-seeking AI]]
- [[[AN 171]— Disagreements between alignment "optimists" and "pessimists"]]
- [[[AN 172] Sorry for the long hiatus!]]
- [[[AN 173] Recent language model results from DeepMind]]