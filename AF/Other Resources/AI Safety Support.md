---
tags:
 - Organization
href: https://www.aisafetysupport.org/home
---

- [Coaching](https://www.aisafetysupport.org/resources/career-coaching

# [Suggested Links](https://www.aisafetysupport.org/resources/lots-of-links)

## Highlights
- [[AGI Safety Fundamentals]]
- [Slack](https://www.google.com/url?q=https%3A%2F%2Fjoin.slack.com%2Ft%2Fai-alignment%2Fshared_invite%2Fzt-1eldahql0-yKOZEdV_mSy31GPQ2joC3Q&sa=D&sntz=1&usg=AOvVaw3x8mlvOmDdtuyRG8Zz5O7F) ^6b970f
- [80k AI Safety group](https://groups.google.com/g/david-kruegers-80k-people?pli=1)
- [Newsletter](https://www.aisafetysupport.org/newsletter)
- [Long-term future fund](https://funds.effectivealtruism.org/funds/far-future)
- [[How to pursue a career in technical AI alignment]]
- [80k Job board](https://80000hours.org/job-board/ai-safety-policy/)

## Fellowships, Internships, Training etc

  

-   [Summer Research Fellowship (Center on Long-Term Risk)](https://www.google.com/url?q=https%3A%2F%2Flongtermrisk.org%2Fwork-with-us%2F&sa=D&sntz=1&usg=AOvVaw2gGQqoqPhEPMlm6gRn_5QD)
    
-   [Summer Research Fellowship (SERI)](https://www.google.com/url?q=https%3A%2F%2Fcisac.fsi.stanford.edu%2Fstanford-existential-risks-initiative%2Fcontent%2Fstanford-existential-risks-initiative&sa=D&sntz=1&usg=AOvVaw0_ysw90EkqzG0uUi0sNYpX)
    
-   [Summer Research Fellowship (CHERI)](https://www.google.com/url?q=https%3A%2F%2Feffectivealtruism.ch%2Fswiss-existential-risk-initiative&sa=D&sntz=1&usg=AOvVaw0jyKnKDWWfHOdk1vjfRNGL)
    
-   [Summer Research Fellowship (CERI)](https://www.google.com/url?q=https%3A%2F%2Fcamxrisk.org%2F%23fellowships&sa=D&sntz=1&usg=AOvVaw1IEcUOsvWSd6_n0trYdvK9)
    
-   [Internship (CHAI)](https://www.google.com/url?q=https%3A%2F%2Fhumancompatible.ai%2Fjobs%23chai-internships&sa=D&sntz=1&usg=AOvVaw3mMoJYeagIAWL9GR00MJBA)
    
-   [ML Alignment Theory Program (SERI MATS)](https://www.google.com/url?q=https%3A%2F%2Fwww.serimats.org%2F&sa=D&sntz=1&usg=AOvVaw2kfSWwMqF75vWCxhc7NEMj)
    
-   [Research Scholars Programme (FHI)](https://www.google.com/url?q=https%3A%2F%2Fwww.fhi.ox.ac.uk%2Frsp%2F&sa=D&sntz=1&usg=AOvVaw1leRGRImrZL7QfcZDut4hm)
    
-   [AI Safety Camp (AI Safety Support)](https://www.google.com/url?q=https%3A%2F%2Faisafety.camp%2F&sa=D&sntz=1&usg=AOvVaw2-xpjjKWKlywwM2gROIqi6)
    
-   [AI Risk for Computer Scientists (MIRI)](https://www.google.com/url?q=https%3A%2F%2Fintelligence.org%2Fai-risk-for-computer-scientists%2F&sa=D&sntz=1&usg=AOvVaw0P3oyLnZIKCQF9KzHx3awW)
    
-   [Summer and Winter Fellowships (GovAI)](https://www.google.com/url?q=https%3A%2F%2Fwww.governance.ai%2Fopportunities%2Ffellowships&sa=D&sntz=1&usg=AOvVaw3VgXglRNWlB8uRGQpl2yeo)
    
-   [OpenAI Residency (OpenAI)](https://www.google.com/url?q=https%3A%2F%2Fopenai.com%2Fblog%2Fopenai-residency%2F&sa=D&sntz=1&usg=AOvVaw2GDUHkqaQieJlSgbupgZng)
    
-   [AGI Safety Fundamentals (EA Cambridge)](https://www.google.com/url?q=https%3A%2F%2Fwww.eacambridge.org%2Ftechnical-alignment-curriculum&sa=D&sntz=1&usg=AOvVaw2iaYy8HAYVvQyIxE9zBFY4)
    
-   [Mentorship (WANBAM)](https://www.google.com/url?q=https%3A%2F%2Fwww.wanbam.com%2F&sa=D&sntz=1&usg=AOvVaw04u3mDDb4yI0BoIIB58p58)
    
-   [Human-aligned AI Summer School (EA Prague)](http://www.google.com/url?q=http%3A%2F%2Fhumanaligned.ai%2F&sa=D&sntz=1&usg=AOvVaw1ufKy44MUcWpkiquJ2NlzC)
    
-   [PIBBSS summer research fellowship (PIBBSS)](https://www.google.com/url?q=https%3A%2F%2Fwww.pibbss.ai%2F&sa=D&sntz=1&usg=AOvVaw04pFb5Jfbh3QqiogyBJ4XL)
    
-   [Summer Research Fellowship (Legal Priorities Project)](https://www.google.com/url?q=https%3A%2F%2Fwww.legalpriorities.org%2Ffellowship.html&sa=D&sntz=1&usg=AOvVaw3i9nYADPGxp5-NiB1Oyd1e)
    
-   [AI Safety Unconference (AI Safety Support)](https://www.google.com/url?q=https%3A%2F%2Fwww.aisu.io%2F&sa=D&sntz=1&usg=AOvVaw369F35fVn4xkzXEalSp3F4)
    

  

## News and Community

**Email**

-   [80k's AI Safety group](https://groups.google.com/g/david-kruegers-80k-people)
    
-   [Alignment Newsletter](https://www.google.com/url?q=https%3A%2F%2Frohinshah.com%2Falignment-newsletter%2F&sa=D&sntz=1&usg=AOvVaw14UTQ4f5384i-YhwTwswE4) by Rohin Shah
    
-   [AI Safety Support Newsletter](https://www.aisafetysupport.org/newsletter)
    
-   [ML Safety Newsletter](https://www.google.com/url?q=https%3A%2F%2Fsubstack.com%2Fprofile%2F1681894-dan-hendrycks&sa=D&sntz=1&usg=AOvVaw3SgZESUzAEaSL_8c7NrIOo) by Dan Hendrycks
    

**Facebook**

-   [AI Safety Discussion](https://www.google.com/url?q=https%3A%2F%2Fwww.facebook.com%2Fgroups%2Faisafety%2F&sa=D&sntz=1&usg=AOvVaw3fueXiChDb37L-xXclPuGS) - "This group is primarily for people who have experience in AI/ML and/or are familiar with AI safety. We encourage beginners to join the [AI Safety Discussion (Open)](https://www.google.com/url?q=https%3A%2F%2Fwww.facebook.com%2Fgroups%2Faisafetyopen%2F&sa=D&sntz=1&usg=AOvVaw0KAVYJdbTcZ9gmI-XeBE9K) group."
    
-   [AI Safety Discussion (Open)](https://www.google.com/url?q=https%3A%2F%2Fwww.facebook.com%2Fgroups%2Faisafetyopen%2F&sa=D&sntz=1&usg=AOvVaw0KAVYJdbTcZ9gmI-XeBE9K)
    
-   [Careers in AI Safety](https://www.google.com/url?q=https%3A%2F%2Fwww.facebook.com%2Fgroups%2FAISafetyCareers%2F&sa=D&sntz=1&usg=AOvVaw1rR8I3WMVVGA5LDiC1CrD4)
    

**Twitter**

-   [AI Safety Core](https://www.google.com/url?q=https%3A%2F%2Ftwitter.com%2Fi%2Flists%2F1185207859728076800&sa=D&sntz=1&usg=AOvVaw3QHdZyxARjbNnpuwBtlBC1) by JJ Balisanyuka-Smith
    

**Other**

-   [AI Alignment Forum](https://www.google.com/url?q=https%3A%2F%2Fwww.alignmentforum.org%2F&sa=D&sntz=1&usg=AOvVaw0p7arI0K4tWnowapvKQUcU)
    
-   [AI Alignment Slack group](https://www.google.com/url?q=https%3A%2F%2Fjoin.slack.com%2Ft%2Fai-alignment%2Fshared_invite%2Fzt-1eldahql0-yKOZEdV_mSy31GPQ2joC3Q&sa=D&sntz=1&usg=AOvVaw3x8mlvOmDdtuyRG8Zz5O7F)
    
-   [Local Study Groups](https://www.aisafetysupport.org/friends)
    
-   [AI Existential Safety Community](https://www.google.com/url?q=https%3A%2F%2Fgrants.futureoflife.org%2Fprog%2Fmembership%2F&sa=D&sntz=1&usg=AOvVaw1wRtSxAUz8SuZOmjIeNIWH) from FLI
    

## Career Advice and Job Search

**General Career Advice**

-   [How to pursue a career in technical AI alignment](https://www.google.com/url?q=https%3A%2F%2Fforum.effectivealtruism.org%2Fposts%2F7WXPkpqKGKewAymJf%2Fhow-to-pursue-a-career-in-technical-ai-alignment&sa=D&sntz=1&usg=AOvVaw1NnIRpX5EavP7BSW6ckAZH) By Charlie Rogers-Smith
    
-   [FAQ: Advice for AI alignment researchers](https://www.google.com/url?q=https%3A%2F%2Frohinshah.com%2Ffaq-career-advice-for-ai-alignment-researchers%2F&sa=D&sntz=1&usg=AOvVaw10lcxHFaqAyVkTltpS917r) by Rohin Shah
    
-   [Beneficial AI Research Career Advice](https://docs.google.com/document/d/1RFo7_9JVmt0z8RPwUjB-mUMgCMoUQmsaj2CM5aHvxCw/edit) by Adam Gleave
    
-   [Hamming, "You and Your Research" (June 6, 1995)](https://www.youtube.com/watch?v=a1zDuOPkMSw)
    

**PhD Advice**

-   [Should you do a PhD?](https://www.google.com/url?q=https%3A%2F%2Fforum.effectivealtruism.org%2Fposts%2FxfN3zwaxuf6uaycv7%2Fshould-you-do-a-phd&sa=D&sntz=1&usg=AOvVaw2mwifFVqIwrWRNgXHSk-b8) by Linda
    
-   [Leveraging Academia](http://www.google.com/url?q=http%3A%2F%2Facritch.com%2Fleveraging-academia%2F&sa=D&sntz=1&usg=AOvVaw3eMJaVC7gBPOuE9LF06G1j) and [Deliberate Grad School](http://www.google.com/url?q=http%3A%2F%2Facritch.com%2Fdeliberate-grad-school%2F&sa=D&sntz=1&usg=AOvVaw1bQbeoYS_k6odruqUN9nLT) by Andrew Critch
    
-   [A Survival Guide to a PhD](https://www.google.com/url?q=https%3A%2F%2Fkarpathy.github.io%2F2016%2F09%2F07%2Fphd%2F&sa=D&sntz=1&usg=AOvVaw2kpnewy49iWo6b_lNpiVA5) by Andrej Karpathy
    
-   There are more non-public resources for finding an AI Safety (or AI Safety friendly) PhD position. [Contact Linda](mailto:linda.linsefors@gmail.com) for more info.
    
-   [Getting into CS graduate school in the USA](https://www.google.com/url?q=https%3A%2F%2Fmarkdcorner.com%2Ftips-and-tricks%2Fgetting-into-cs-graduate-school-in-the-usa%2F&sa=D&sntz=1&usg=AOvVaw3Rc2tgJAL-yqBjKY19lBAx)
    
-   [How to Write an Email to a Potential Ph.D. Advisor/Professor](https://www.google.com/url?q=https%3A%2F%2Ftheprofessorisin.com%2F2011%2F07%2F25%2Fhow-to-write-an-email-to-a-potential-ph-d-advisor%2F&sa=D&sntz=1&usg=AOvVaw3fOmx_orPjVcSubr0PyvlK)
    

**Jobs**

-   [80k's Job Board](https://www.google.com/url?q=https%3A%2F%2F80000hours.org%2Fjob-board%2Fai-safety-policy%2F&sa=D&sntz=1&usg=AOvVaw2BjbewyYjnPhvf5soGsi2K)
    
-   [EA Work Club](https://www.google.com/url?q=https%3A%2F%2Fwww.eawork.club%2Fcategories%2F3&sa=D&sntz=1&usg=AOvVaw38U1-xzg7RQ467KWBCW8DD)
    
-   [#EAjobs](https://www.google.com/url?q=https%3A%2F%2Ftwitter.com%2Fhashtag%2FEAjobs%3Fsrc%3Dhashtag_click&sa=D&sntz=1&usg=AOvVaw1qqBELjnFw-ffgmFo5Ubo2) on Twitter
    

**Other**

-   [20 AI and impact opportunities](https://www.google.com/url?q=https%3A%2F%2Fforum.effectivealtruism.org%2Fposts%2FzRHgLQ4TNvxy68jds%2F20-ai-alignment-opportunities-for-thoughtful-eas&sa=D&sntz=1&usg=AOvVaw3jyhW_JR9u3bjNeW6R75VK)
    

## Study Guides

**Technical AI safety**

-   [CHAI's Bibliography](https://www.google.com/url?q=https%3A%2F%2Fhumancompatible.ai%2Fbibliography&sa=D&sntz=1&usg=AOvVaw2gCd5eumh28yaOXBlXqnx9)
    
-   [80k's AI safety syllabus](https://www.google.com/url?q=https%3A%2F%2F80000hours.org%2Farticles%2Fai-safety-syllabus%2F&sa=D&sntz=1&usg=AOvVaw2evWomp-ZGJecy0fVbBNrx)
    
-   [MIRI’s Research Guide](https://www.google.com/url?q=https%3A%2F%2Fintelligence.org%2Fresearch-guide%2F&sa=D&sntz=1&usg=AOvVaw1XBWZ00Az6QQQ6yf9KnjbZ) (despite the name it is actually more of a study guide)
    
-   [Open AI’s Spinning Up in Deep RL](https://www.google.com/url?q=https%3A%2F%2Fspinningup.openai.com%2Fen%2Flatest%2Findex.html&sa=D&sntz=1&usg=AOvVaw3L_H3B2K3lvBcb4VkBlImi)
    
-   [Study Guide](https://www.google.com/url?q=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fbjjbp5i5G8bekJuxv%2Fstudy-guide&sa=D&sntz=1&usg=AOvVaw35sealO-ZIfOMqB4j1bFG1) by John Wentsworth
    
-   [List of AI](https://www.google.com/url?q=https%3A%2F%2Fforum.effectivealtruism.org%2Fposts%2FXvWWfq9iqFj8x7Eu8%2Flist-of-ai-safety-courses-and-resources&sa=D&sntz=1&usg=AOvVaw26b80G1TmkqLlc6H2WLHp_) [S](https://www.google.com/url?q=https%3A%2F%2Fforum.effectivealtruism.org%2Fposts%2FXvWWfq9iqFj8x7Eu8%2Flist-of-ai-safety-courses-and-resources&sa=D&sntz=1&usg=AOvVaw26b80G1TmkqLlc6H2WLHp_)[afety](https://www.google.com/url?q=https%3A%2F%2Fforum.effectivealtruism.org%2Fposts%2FXvWWfq9iqFj8x7Eu8%2Flist-of-ai-safety-courses-and-resources&sa=D&sntz=1&usg=AOvVaw26b80G1TmkqLlc6H2WLHp_) [T](https://www.google.com/url?q=https%3A%2F%2Fforum.effectivealtruism.org%2Fposts%2FXvWWfq9iqFj8x7Eu8%2Flist-of-ai-safety-courses-and-resources&sa=D&sntz=1&usg=AOvVaw26b80G1TmkqLlc6H2WLHp_)[echnical](https://www.google.com/url?q=https%3A%2F%2Fforum.effectivealtruism.org%2Fposts%2FXvWWfq9iqFj8x7Eu8%2Flist-of-ai-safety-courses-and-resources&sa=D&sntz=1&usg=AOvVaw26b80G1TmkqLlc6H2WLHp_) [C](https://www.google.com/url?q=https%3A%2F%2Fforum.effectivealtruism.org%2Fposts%2FXvWWfq9iqFj8x7Eu8%2Flist-of-ai-safety-courses-and-resources&sa=D&sntz=1&usg=AOvVaw26b80G1TmkqLlc6H2WLHp_)[ourses,](https://www.google.com/url?q=https%3A%2F%2Fforum.effectivealtruism.org%2Fposts%2FXvWWfq9iqFj8x7Eu8%2Flist-of-ai-safety-courses-and-resources&sa=D&sntz=1&usg=AOvVaw26b80G1TmkqLlc6H2WLHp_) [R](https://www.google.com/url?q=https%3A%2F%2Fforum.effectivealtruism.org%2Fposts%2FXvWWfq9iqFj8x7Eu8%2Flist-of-ai-safety-courses-and-resources&sa=D&sntz=1&usg=AOvVaw26b80G1TmkqLlc6H2WLHp_)[eading](https://www.google.com/url?q=https%3A%2F%2Fforum.effectivealtruism.org%2Fposts%2FXvWWfq9iqFj8x7Eu8%2Flist-of-ai-safety-courses-and-resources&sa=D&sntz=1&usg=AOvVaw26b80G1TmkqLlc6H2WLHp_) [L](https://www.google.com/url?q=https%3A%2F%2Fforum.effectivealtruism.org%2Fposts%2FXvWWfq9iqFj8x7Eu8%2Flist-of-ai-safety-courses-and-resources&sa=D&sntz=1&usg=AOvVaw26b80G1TmkqLlc6H2WLHp_)[ists, and](https://www.google.com/url?q=https%3A%2F%2Fforum.effectivealtruism.org%2Fposts%2FXvWWfq9iqFj8x7Eu8%2Flist-of-ai-safety-courses-and-resources&sa=D&sntz=1&usg=AOvVaw26b80G1TmkqLlc6H2WLHp_) [C](https://www.google.com/url?q=https%3A%2F%2Fforum.effectivealtruism.org%2Fposts%2FXvWWfq9iqFj8x7Eu8%2Flist-of-ai-safety-courses-and-resources&sa=D&sntz=1&usg=AOvVaw26b80G1TmkqLlc6H2WLHp_)[urriculums](https://www.google.com/url?q=https%3A%2F%2Fforum.effectivealtruism.org%2Fposts%2FXvWWfq9iqFj8x7Eu8%2Flist-of-ai-safety-courses-and-resources&sa=D&sntz=1&usg=AOvVaw26b80G1TmkqLlc6H2WLHp_) from Nonlinear
    

**AI governance**

-   [Reading Guide for the Global Politics of Artificial Intelligence](https://www.google.com/url?q=https%3A%2F%2Fwww.allandafoe.com%2Faireadings&sa=D&sntz=1&usg=AOvVaw0frd56U69QfQTWnZrDQ2Kx)
    

## Research Maps and Reviews

-   [FLI's AI Safety Research Landscape](https://www.google.com/url?q=https%3A%2F%2Ffutureoflife.org%2Flandscape%2F&sa=D&sntz=1&usg=AOvVaw1yUFxnlWKlYFqc8WMlirtA)
    
-   [Mapping the Conceptual Territory in AI Existential Safety and Alignment](https://www.google.com/url?q=https%3A%2F%2Fjbkjr.com%2Fposts%2F2020%2F12%2Fmapping_conceptual_territory_AI_safety_alignment%2F&sa=D&sntz=1&usg=AOvVaw38uJDDVCmu-d7kJ1eK8jtQ) by Jack Koch
    
-   [AI Alignment 2018-19 Review](https://www.google.com/url?q=https%3A%2F%2Fwww.alignmentforum.org%2Fposts%2FdKxX76SCfCvceJXHv%2Fai-alignment-2018-19-review&sa=D&sntz=1&usg=AOvVaw01wnyHIJel7yz8GH_6qTwu) by Rohin Shah
    
-   [2020 AI Alignment Literature Review and Charity Comparison](https://www.google.com/url?q=https%3A%2F%2Fwww.alignmentforum.org%2Fposts%2FpTYDdcag9pTzFQ7vw%2F2020-ai-alignment-literature-review-and-charity-comparison&sa=D&sntz=1&usg=AOvVaw3fHIvt1cYrkE2l1KaS_HpJ) by Larks
    

## Research Agendas

**Technical AI safety**

-   [MIRI](https://www.google.com/url?q=https%3A%2F%2Fintelligence.org%2F&sa=D&sntz=1&usg=AOvVaw23uLRq1LjOHqs7TWseh-kJ): [](https://www.google.com/url?q=https%3A%2F%2Fintelligence.org%2Ffiles%2FTechnicalAgenda.pdf&sa=D&sntz=1&usg=AOvVaw0aJl9PSaToeUcJaZ-vhYfU) [Agent Foundations for Aligning Machine Intelligence with Human Interests](https://www.google.com/url?q=https%3A%2F%2Fintelligence.org%2Ffiles%2FTechnicalAgenda.pdf&sa=D&sntz=1&usg=AOvVaw0aJl9PSaToeUcJaZ-vhYfU) (2017) and [](https://www.google.com/url?q=https%3A%2F%2Fintelligence.org%2Ffiles%2FAlignmentMachineLearning.pdf&sa=D&sntz=1&usg=AOvVaw3Et4uSaGu4QP6GODfre_aB) [Alignment for Advanced Machine Learning Systems](https://www.google.com/url?q=https%3A%2F%2Fintelligence.org%2Ffiles%2FAlignmentMachineLearning.pdf&sa=D&sntz=1&usg=AOvVaw3Et4uSaGu4QP6GODfre_aB) research agendas
    
-   [CLR](https://www.google.com/url?q=https%3A%2F%2Flongtermrisk.org%2F&sa=D&sntz=1&usg=AOvVaw1M39LSHSmgP61cub5WzHor): [](https://www.google.com/url?q=https%3A%2F%2Fwww.alignmentforum.org%2Fs%2Fp947tK8CoBbdpPtyK&sa=D&sntz=1&usg=AOvVaw2JnnWK_oLIKd7Q1NGgIFJb) [Cooperation, Conflict, and Transformative Artificial Intelligence: A Research Agenda](https://www.google.com/url?q=https%3A%2F%2Fwww.alignmentforum.org%2Fs%2Fp947tK8CoBbdpPtyK&sa=D&sntz=1&usg=AOvVaw2JnnWK_oLIKd7Q1NGgIFJb) (+ includes some questions related to AI governance)
    
-   Paul Christiano’s [](https://www.google.com/url?q=https%3A%2F%2Fai-alignment.com%2Fiterated-distillation-and-amplification-157debfd1616&sa=D&sntz=1&usg=AOvVaw0qSKiEz06DU5uPoV_43KXa) [research agenda summary](https://www.google.com/url?q=https%3A%2F%2Fai-alignment.com%2Fiterated-distillation-and-amplification-157debfd1616&sa=D&sntz=1&usg=AOvVaw0qSKiEz06DU5uPoV_43KXa) (and [](https://www.google.com/url?q=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDjs38EWYZG8o7JMWY%2Fpaul-s-research-agenda-faq&sa=D&sntz=1&usg=AOvVaw0NNONtLAxFkltqdZD-IHTi) [FAQ](https://www.google.com/url?q=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDjs38EWYZG8o7JMWY%2Fpaul-s-research-agenda-faq&sa=D&sntz=1&usg=AOvVaw0NNONtLAxFkltqdZD-IHTi) and [](https://www.google.com/url?q=https%3A%2F%2Fai-alignment.com%2Fai-alignment-landscape-d3773c37ae38&sa=D&sntz=1&usg=AOvVaw1fxl0FF0YE24uv90deo01n) [talk](https://www.google.com/url?q=https%3A%2F%2Fai-alignment.com%2Fai-alignment-landscape-d3773c37ae38&sa=D&sntz=1&usg=AOvVaw1fxl0FF0YE24uv90deo01n)) (2018)
    
-   [Synthesising a human's preferences into a utility function](https://www.google.com/url?q=https%3A%2F%2Fwww.alignmentforum.org%2Fposts%2FCSEdLLEkap2pubjof%2Fresearch-agenda-v0-9-synthesising-a-human-s-preferences-into&sa=D&sntz=1&usg=AOvVaw1DP-gf2PLCIS7bG9Ii5mF9) ([example use](https://www.google.com/url?q=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhcrFxeYYfbFrkKQEJ%2Ffull-toy-model-for-preference-learning&sa=D&sntz=1&usg=AOvVaw2_qofMzWdLwYnKH-Iby9Ci) and [](https://www.youtube.com/watch?v=1M9CvESSeVc) [talk](https://www.youtube.com/watch?v=1M9CvESSeVc)), Stuart Armstrong, (2019),[](https://www.google.com/url?q=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhcrFxeYYfbFrkKQEJ%2Ffull-toy-model-for-preference-learning&sa=D&sntz=1&usg=AOvVaw2_qofMzWdLwYnKH-Iby9Ci)
    
-   [The Learning-Theoretic AI Alignment Research Agenda](https://www.google.com/url?q=https%3A%2F%2Fwww.alignmentforum.org%2Fposts%2F5bd75cc58225bf0670375575%2Fthe-learning-theoretic-ai-alignment-research-agenda-1&sa=D&sntz=1&usg=AOvVaw0p25DTlq42Q0LxpQsVxm-g), Vanessa Kosoy, (2018)
    
-   [Research Priorities for Robust and Beneficial Artificial Intelligence](https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F1602.03506&sa=D&sntz=1&usg=AOvVaw2DKw7A5uFm9NpaTVITNGLQ), Stuart Russell, Daniel Dewey, Max Tegmark, (2016)
    
-   [Concrete problems in AI Safety](https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F1606.06565&sa=D&sntz=1&usg=AOvVaw11kpByDUW1ux8FlY5aF-rP), Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, Dan Mané, (2016)
    
-   [AGI Safety Literature Review](https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F1805.01109&sa=D&sntz=1&usg=AOvVaw1RT53f8w3SMVG3IwCI4P5f), Tom Everitt, Gary Lea, Marcus Hutter, (2018)
    
-   [AI Services as a Research Paradigm](https://www.google.com/url?q=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fz2ofM2oZQwmcWFt8N%2Fai-services-as-a-research-paradigm&sa=D&sntz=1&usg=AOvVaw0mD9VB-D6sqFM_-yCDvoTd), Vojta Kovarik, (2020)
    
-   [Avoiding Negative Side Effects due to Incomplete Knowledge of AI Systems](https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F2008.12146&sa=D&sntz=1&usg=AOvVaw1115HSOMHN8EOaC0id5s53), Sandhya Saisubramanian, Shlomo Zilberstein, Ece Kamar, (2020)
    
-   [AI Research Considerations for Human Existential Safety (ARCHES)](https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F2006.04948&sa=D&sntz=1&usg=AOvVaw1xs1w9W2WrysM4ii34qgjS), Andrew Critch, David Krueger, (2020)
    
-   [How do we become confident in the safety of a machine learning system?](https://www.google.com/url?q=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFDJnZt8Ks2djouQTZ%2Fhow-do-we-become-confident-in-the-safety-of-a-machine&sa=D&sntz=1&usg=AOvVaw3Aoqv3Vi41iXBFuYWEZV0V) By Evan Hubinger
    
-   [Research Agenda: Using Neuroscience to Achieve Safe and Beneficial AGI](https://www.google.com/url?q=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDkfGaZTgwsE7XZq9k%2Fresearch-agenda-update%231_5_Understand_human_social_instincts_well_enough_to_implement_them_in_code&sa=D&sntz=1&usg=AOvVaw0kAhsNbAL_dJmiwk17yU61) by Steve Brynes
    
-   [Unsolved Problems in ML Safety](https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fpdf%2F2109.13916.pdf&sa=D&sntz=1&usg=AOvVaw3snk8qEQDpIwpK8TWFxyJL) By Dan Hendrycks, Nicholas Carlini, J. Schulman, J. Steinhardt
    

**AI governance**

-   [AI Impacts](https://www.google.com/url?q=https%3A%2F%2Faiimpacts.org%2F&sa=D&sntz=1&usg=AOvVaw3ze6IPl2jg5VjQ9pe5HxTq): [](https://www.google.com/url?q=https%3A%2F%2Faiimpacts.org%2Fpromising-research-projects%2F&sa=D&sntz=1&usg=AOvVaw3U5wqp8eUSLLSaoIBMVwmz) [promising research projects](https://www.google.com/url?q=https%3A%2F%2Faiimpacts.org%2Fpromising-research-projects%2F&sa=D&sntz=1&usg=AOvVaw3U5wqp8eUSLLSaoIBMVwmz) and [](https://www.google.com/url?q=https%3A%2F%2Faiimpacts.org%2Fpossible-investigations%2F&sa=D&sntz=1&usg=AOvVaw2HyhQBsWfCz1GzRztaKaCJ) [possible empirical investigations](https://www.google.com/url?q=https%3A%2F%2Faiimpacts.org%2Fpossible-investigations%2F&sa=D&sntz=1&usg=AOvVaw2HyhQBsWfCz1GzRztaKaCJ)
    
-   [Governance of AI program](https://www.google.com/url?q=https%3A%2F%2Fwww.fhi.ox.ac.uk%2Fgovernance-ai-program%2F&sa=D&sntz=1&usg=AOvVaw2SaxoCJhFyk7qnk5qzQEZ0) at [FHI](https://www.google.com/url?q=https%3A%2F%2Fwww.fhi.ox.ac.uk%2F&sa=D&sntz=1&usg=AOvVaw3JR8bP_Dlh1REqN-ROaf9S): Alan Dafoe's [](https://www.google.com/url?q=https%3A%2F%2Fwww.fhi.ox.ac.uk%2Fwp-content%2Fuploads%2FGovAIAgenda.pdf&sa=D&sntz=1&usg=AOvVaw0eY5jRd-9zH-u8HJkq11rX) [AI governance research agenda](https://www.google.com/url?q=https%3A%2F%2Fwww.fhi.ox.ac.uk%2Fwp-content%2Fuploads%2FGovAIAgenda.pdf&sa=D&sntz=1&usg=AOvVaw0eY5jRd-9zH-u8HJkq11rX)
    
-   [Center for a New American Security](https://www.google.com/url?q=https%3A%2F%2Fwww.cnas.org%2Fartificial-intelligence-and-global-security-initiative-research-agenda&sa=D&sntz=1&usg=AOvVaw3CQT67Yc4c0HNflE1wgaZI): Artificial Intelligence and Global Security Initiative [](https://www.google.com/url?q=https%3A%2F%2Fwww.cnas.org%2Fartificial-intelligence-and-global-security-initiative-research-agenda&sa=D&sntz=1&usg=AOvVaw3CQT67Yc4c0HNflE1wgaZI) [Research Agenda](https://www.google.com/url?q=https%3A%2F%2Fwww.cnas.org%2Fartificial-intelligence-and-global-security-initiative-research-agenda&sa=D&sntz=1&usg=AOvVaw3CQT67Yc4c0HNflE1wgaZI)
    
-   [FLI](https://www.google.com/url?q=https%3A%2F%2Ffutureoflife.org%2F&sa=D&sntz=1&usg=AOvVaw1PbVePT07I647ghX-0AeIL): [](https://www.google.com/url?q=https%3A%2F%2Ffutureoflife.org%2Fdata%2Fdocuments%2Fresearch_survey.pdf&sa=D&sntz=1&usg=AOvVaw3Gftfe8Y05CLwNU1FYetr7) [A survey of research questions for robust and beneficial AI](https://www.google.com/url?q=https%3A%2F%2Ffutureoflife.org%2Fdata%2Fdocuments%2Fresearch_survey.pdf&sa=D&sntz=1&usg=AOvVaw3Gftfe8Y05CLwNU1FYetr7) (+ some aspects also fall into technical AI safety)
    
-   Luke Muehlhauser’s [](http://www.google.com/url?q=http%3A%2F%2Flukemuehlhauser.com%2Fsome-studies-which-could-improve-our-strategic-picture-of-superintelligence%2F&sa=D&sntz=1&usg=AOvVaw0YAlBLmjjPrCIGrELZW4jD) [list of research questions to improve our strategic picture of superintelligence](http://www.google.com/url?q=http%3A%2F%2Flukemuehlhauser.com%2Fsome-studies-which-could-improve-our-strategic-picture-of-superintelligence%2F&sa=D&sntz=1&usg=AOvVaw0YAlBLmjjPrCIGrELZW4jD) (2014)
    

## Books, papers, podcasts, videos

(Non exhaustive list of AI Safety material)

**Books**

-   [The Alignment Problem](https://www.google.com/url?q=https%3A%2F%2Fbrianchristian.org%2Fthe-alignment-problem%2F&sa=D&sntz=1&usg=AOvVaw22VV_OvYY795_v-NRN6OSq) by Brian Christian, 2020
    
-   [Human Compatible](https://www.google.com/url?q=https%3A%2F%2Fpeople.eecs.berkeley.edu%2F~russell%2Fhc.html&sa=D&sntz=1&usg=AOvVaw02DB3Lr816bcp7rB3KQUZk) by Stuart Russell, 2019
    
-   [Reframing Superintelligence](https://www.google.com/url?q=https%3A%2F%2Fwww.fhi.ox.ac.uk%2Freframing%2F&sa=D&sntz=1&usg=AOvVaw2Ab-7qU07FxfaEUUbs4ZzN) by Eric Drexler, 2019
    
-   [Artificial Intelligence Safety and Security](https://www.google.com/url?q=https%3A%2F%2Fwww.routledge.com%2FArtificial-Intelligence-Safety-and-Security%2FYampolskiy%2Fp%2Fbook%2F9780815369820&sa=D&sntz=1&usg=AOvVaw3UknuOJJRWwnfpuG45sj6K), By Roman Yampolskiy, 2018
    
-   [Superintelligence](http://www.google.com/url?q=http%3A%2F%2Fwww.amazon.com%2Fgp%2Fproduct%2F0199678111&sa=D&sntz=1&usg=AOvVaw06yB6in8AqHV_Ie21MujIj) by Nick Bostrom, 2014
    

**Other reading**

-   [AI Alignment Forum Sequences](https://drive.google.com/open?id=1qnBEfb-TvnvVlJ4cx81cAXg_XcQbG02rcPb1r6T28Kk)
    
-   [Victoria Krakovna's AI safety resources](https://www.google.com/url?q=https%3A%2F%2Fvkrakovna.wordpress.com%2Fai-safety-resources%2F&sa=D&sntz=1&usg=AOvVaw1Blxf8SVgq6ysZ9UsObY9j) (contains a list of motivational resources and key papers for some AI Safety sub fields)
    
-   [List of AI safety courses and resources](https://www.google.com/url?q=https%3A%2F%2Fforum.effectivealtruism.org%2Fposts%2FXvWWfq9iqFj8x7Eu8%2Flist-of-ai-safety-courses-and-resources&sa=D&sntz=1&usg=AOvVaw26b80G1TmkqLlc6H2WLHp_) from Nonlinear
    
-   [Pragmatic AI Safety](https://www.google.com/url?q=https%3A%2F%2Fforum.effectivealtruism.org%2Fs%2F8EqNwueP6iw2BQpNo&sa=D&sntz=1&usg=AOvVaw1FK8bwcEipdiWz0_BPDyJ5) by ThomasWoodside
    
-   [X-Risk Analysis for AI Research](https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F2206.05862&sa=D&sntz=1&usg=AOvVaw2mQSSawfnIIwZoMS4i45gh) by Dan Hendrycks, Mantas Mazeika
    

**Podcasts**

-   [FLI's AI Alignment Podcast](https://www.google.com/url?q=https%3A%2F%2Ffutureoflife.org%2Fai-alignment-podcast%2F&sa=D&sntz=1&usg=AOvVaw2uEXiDwmwFZQC8SV8DawwH)
    
-   [Alignment Newsletter Podcast](https://www.google.com/url?q=https%3A%2F%2Falignment-newsletter.libsyn.com%2F&sa=D&sntz=1&usg=AOvVaw1dp-SUH1e0VzzP-ZTET1vj) (Robert Miles reads the Alignment Newsletter)
    
-   [80k's Podcast](https://www.google.com/url?q=https%3A%2F%2F80000hours.org%2Fpodcast%2F&sa=D&sntz=1&usg=AOvVaw09omamR5OAPkC7NpDmwH38) (Effective Altruism podcast with some AI Safety episodes)
    
    -   AI Safety episodes include: [#44](https://www.google.com/url?q=https%3A%2F%2F80000hours.org%2Fpodcast%2Fepisodes%2Fpaul-christiano-ai-alignment-solutions%2F&sa=D&sntz=1&usg=AOvVaw1bkkQG8Z8r_8DFhmqdii0s), [#23](https://www.google.com/url?q=https%3A%2F%2F80000hours.org%2Fpodcast%2Fepisodes%2Fjan-leike-ml-alignment%2F&sa=D&sntz=1&usg=AOvVaw0bvLxuVLcNoYSx1Nw01E99), [#47](https://www.google.com/url?q=https%3A%2F%2F80000hours.org%2Fpodcast%2Fepisodes%2Folsson-and-ziegler-ml-engineering-and-safety%2F&sa=D&sntz=1&usg=AOvVaw29QgkepRyXOudqgXqA4WBx), [#80](https://www.google.com/url?q=https%3A%2F%2F80000hours.org%2Fpodcast%2Fepisodes%2Fstuart-russell-human-compatible-ai%2F&sa=D&sntz=1&usg=AOvVaw0qL_ho16zrkbNCOMj67kfk), [#78](https://www.google.com/url?q=https%3A%2F%2F80000hours.org%2Fpodcast%2Fepisodes%2Fdanny-hernandez-forecasting-ai-progress%2F&sa=D&sntz=1&usg=AOvVaw0Vsomcm-TB6wmQrVdSyd8h).
        
-   [AXRP - the AI X-risk Research Podcast](https://www.google.com/url?q=https%3A%2F%2Faxrp.net%2F&sa=D&sntz=1&usg=AOvVaw2eAsFfklzD7z4qd2Uv_O8M)
    
-   Quinn’s [Technical AI Safety Podcast](https://www.google.com/url?q=https%3A%2F%2Ftechnical-ai-safety.libsyn.com%2F&sa=D&sntz=1&usg=AOvVaw2y9NLrwz5mzMuxYQCifGwK)
    
-   [The Inside View](https://www.youtube.com/channel/UCb9F9_uV24PGj6x63PhXEVw)
    
-   [The Nonlinear Library](https://podcasts.google.com/feed/aHR0cHM6Ly9hdWRpby5iZXlvbmR3b3Jkcy5pby9mLzkxOTUvMzAyNjUvcmVhZF84NjE3ZDNhZWU1M2YzYWI4NDRhMzA5ZDM3ODk1YzE0Mw) - a repository of text-to-speech content from the EA Forum, Alignment Forum, LessWrong, and other EA blogs
    

**YouTube**

(Most of these channels are a mix of AI Safety content and other content)

-   [Robert Miles discuss AI on Computerphile](https://www.youtube.com/watch?v=tlS5Y2vm02c&list=PLzH6n4zXuckquVnQ0KlMDxyT5YE-sA8Ps) and [Robert Miles's own YouTube channel](https://www.youtube.com/channel/UCLB7AzTwc6VFZrBsO2ucBMg)
    
-   [AI Safety talks from EA Global conferences](https://www.youtube.com/watch?v=dbMp4pFVwnU&list=PLwp9xeoX5p8P0TuCSdTExdeCKr97DqXWZ)
    
-   [CEA's Existential Risk and the Far Future playlist](https://www.youtube.com/watch?v=l6yAylvzEXo&list=PLwp9xeoX5p8NjWAeGnbe5tQwoXm3oMY3H)
    
-   [MIRI's YouTube channel](https://www.youtube.com/c/IntelligenceOrg/)
    
-   [OpenAI's YouTube channel](https://www.youtube.com/c/OpenAI/)
    
-   [CSER's YouTube channel](https://www.youtube.com/c/CSERCambridge/)
    
-   [DeepMind's YouTube channel](https://www.youtube.com/channel/UCP7jMXSY2xbc3KCAE0MHQ-A)
    
-   [CLR's YouTube channel](https://www.youtube.com/channel/UCNPqscTt41xxJ-8RCN_3vFA)
    
-   [SlateStarCodex Meetups](https://www.youtube.com/watch?v=Wn2vgQGNI_c&list=PLFDYxsqlH6uhSghWfsuEAiKDfZNVZhUOX) (recorded talks)
    

**Other Videos**

-   [Recordings from past AISS events](https://www.google.com/url?q=https%3A%2F%2Fwww.aisafetysupport.org%2Fevents%2Fdiscussion-days%23h.hxkmsqyrl6yk&sa=D&sntz=1&usg=AOvVaw1GvMrC39hs4jQtRMQs2gtJ)
    
-   [Victoria Krakovna's Talks](https://www.google.com/url?q=https%3A%2F%2Fvkrakovna.wordpress.com%2Ftalks%2F&sa=D&sntz=1&usg=AOvVaw1YJNGHFCzHB7ADvMU_nq8W)
    

## AI Safety Research Groups

(Many of these groups do a combination of AI Safety and other X-risks.)

There are many academic and independent researchers, who are interested in AI Safety, and who are **not** covered by this list. We are not going to list specific individuals publicly, so please [contact us](https://www.google.com/url?q=https%3A%2F%2Fwww.aisafetysupport.org%2Fhome%23h.h9ofdjva2lw9&sa=D&sntz=1&usg=AOvVaw22QNven-E_GPVEV1q0Wh9l) if you want to find more AI Safety researchers.

**Technical AI safety**

-   [Machine Research Intelligence Institute (MIRI)](https://www.google.com/url?q=https%3A%2F%2Fintelligence.org%2F&sa=D&sntz=1&usg=AOvVaw23uLRq1LjOHqs7TWseh-kJ), Berkeley
    
-   [Center for Human-Compatible Artificial Intelligence (CHAI)](https://www.google.com/url?q=https%3A%2F%2Fhumancompatible.ai%2F&sa=D&sntz=1&usg=AOvVaw0KGFpr3xIdYaa3u-OPpxDt), University of California, Berkeley
    
-   [Future of Humanity Institute (FHI)](https://www.google.com/url?q=https%3A%2F%2Fwww.fhi.ox.ac.uk%2F&sa=D&sntz=1&usg=AOvVaw3JR8bP_Dlh1REqN-ROaf9S), University of Oxford
    
-   [Center on Long-Term Risk (CLR)](https://www.google.com/url?q=https%3A%2F%2Flongtermrisk.org%2F&sa=D&sntz=1&usg=AOvVaw1M39LSHSmgP61cub5WzHor), London
    
-   [Ought](https://www.google.com/url?q=https%3A%2F%2Fought.org%2F&sa=D&sntz=1&usg=AOvVaw3IBDMqxddXH110f1o3_Vuk), San Francisco
    
-   [Alignment Research Center](https://www.google.com/url?q=https%3A%2F%2Falignmentresearchcenter.org%2F&sa=D&sntz=1&usg=AOvVaw16ecypyx3egpy3YLCBplzm)
    
-   [Redwood Research](https://www.google.com/url?q=https%3A%2F%2Fwww.redwoodresearch.org%2F&sa=D&sntz=1&usg=AOvVaw1dmZhJnGSXn557TGDSHmI1), Berkeley
    
-   [Anthropic](https://www.google.com/url?q=https%3A%2F%2Fwww.anthropic.com%2F&sa=D&sntz=1&usg=AOvVaw0H4G5pwh_QKS9BhtrJdjUE), San Francisco
    
-   [AISafety.com](https://www.google.com/url?q=https%3A%2F%2Fwww.dropbox.com%2Fs%2Flk0g0mwdv0kvjrx%2FAISafety.com.pdf%3Fdl%3D0&sa=D&sntz=1&usg=AOvVaw3pRvAgC6Gxx3AmaCljn9xO) - A Startup for Aligning Narrowly Superhuman Models
    

**AI governance**

-   [The Center for the Study of Existential Risk (CSER)](https://www.google.com/url?q=https%3A%2F%2Fwww.cser.ac.uk%2F&sa=D&sntz=1&usg=AOvVaw29L7Xssl7bqh816Ktmb2el), University of Cambridge
    
-   [Future of Humanity Institute (FHI)](https://www.google.com/url?q=https%3A%2F%2Fwww.fhi.ox.ac.uk%2F&sa=D&sntz=1&usg=AOvVaw3JR8bP_Dlh1REqN-ROaf9S), University of Oxford
    
-   [Global Catastrophic Risk Institute (GCRI)](http://www.google.com/url?q=http%3A%2F%2Fgcrinstitute.org%2Fai%2F&sa=D&sntz=1&usg=AOvVaw2aTc5MOMEp-IjrDQ6FpjHL), [various locations](http://www.google.com/url?q=http%3A%2F%2Fgcrinstitute.org%2Fpeople%2F&sa=D&sntz=1&usg=AOvVaw1apyINHa1dqjXM_HWM0FFF)
    
-   [Median Group](http://www.google.com/url?q=http%3A%2F%2Fmediangroup.org%2F&sa=D&sntz=1&usg=AOvVaw3Vpv-jw0_Rh1K8cgsewRoS), Berkeley
    
-   [Center for Security and Emerging Technology (CSET)](https://www.google.com/url?q=https%3A%2F%2Fcset.georgetown.edu%2F&sa=D&sntz=1&usg=AOvVaw3EZ9h_9fN3SVFDXdoCqpQB), Washington
    

**AI companies which also does some safety work, both technical and governance**

-   [OpenAI](https://www.google.com/url?q=https%3A%2F%2Fopenai.com%2F&sa=D&sntz=1&usg=AOvVaw18C0xbKxaYFuyaOKCa9oiR), San Francisco
    
-   [DeepMind](https://www.deepmind.com/), London
    
-   [Cohere AI](https://www.google.com/url?q=https%3A%2F%2Fcohere.ai%2F&sa=D&sntz=1&usg=AOvVaw0hzaNXXIKtRoH1cghVzqZa), Toronto
    

**Meta, forecast, and strategy**

-   [Future of Humanity Institute (FHI)](https://www.google.com/url?q=https%3A%2F%2Fwww.fhi.ox.ac.uk%2F&sa=D&sntz=1&usg=AOvVaw3JR8bP_Dlh1REqN-ROaf9S), University of Oxford
    
-   [Convergence Analysis](https://www.google.com/url?q=https%3A%2F%2Fwww.convergenceanalysis.org%2F&sa=D&sntz=1&usg=AOvVaw2SP_GFFqMZnx1FMzKVc_zK), moving around
    
-   [Leverhulme Center for the Future of Intelligence (CFI)](http://www.google.com/url?q=http%3A%2F%2Flcfi.ac.uk%2F&sa=D&sntz=1&usg=AOvVaw05-djb0jqFjpvcHcS0e1Jq), University of Cambridge
    
-   [AI Impacts](https://www.google.com/url?q=https%3A%2F%2Faiimpacts.org%2F&sa=D&sntz=1&usg=AOvVaw3ze6IPl2jg5VjQ9pe5HxTq)
    
-   [Metaculus](https://www.google.com/url?q=https%3A%2F%2Fwww.metaculus.com%2Fquestions%2F%3Fsearch%3Dcat%3Acomp-sci--ai-and-machinelearning&sa=D&sntz=1&usg=AOvVaw2ke_QPspE-5aDUF3Xf-zsu)
    

## Other AI Safety Orgs and Initiatives

-   AI Safety Support (AISS) <- That's us!
    
-   [AI Safety Camp (AISC)](https://www.google.com/url?q=https%3A%2F%2Faisafety.camp&sa=D&sntz=1&usg=AOvVaw3cs8quA5vIRR0KF6jFLPmL), collaborate with a research mentor during intensive co-working sprints
    
-   [ML for Alignment Bootcamp (MLAB)](https://www.google.com/url?q=https%3A%2F%2Fforum.effectivealtruism.org%2Fposts%2FiwTr8S8QkutyYroGy%2Fapply-to-the-ml-for-alignment-bootcamp-mlab-in-berkeley-jan&sa=D&sntz=1&usg=AOvVaw3yaUJq1hywxNiQG3N8v7a-), three weeks of intense learning in Berkeley
    
-   [Future of Life Institute (FLI)](https://www.google.com/url?q=https%3A%2F%2Ffutureoflife.org&sa=D&sntz=1&usg=AOvVaw2GEhK-scMc38J0eYtJ41Tk), outreach, podcast and conferences
    
-   [Berkeley Existential Risk Initiative (BERI)](https://www.google.com/url?q=https%3A%2F%2Fexistence.org%2F&sa=D&sntz=1&usg=AOvVaw2BdqzalNHHKYzKFsnBI3jE), supports university groups
    
-   [Stanford Existential Risk Initiative (SERI)](https://www.google.com/url?q=https%3A%2F%2Fcisac.fsi.stanford.edu%2Fstanford-existential-risks-initiative%2Fcontent%2Fstanford-existential-risks-initiative&sa=D&sntz=1&usg=AOvVaw0_ysw90EkqzG0uUi0sNYpX), student and faculty group focused on reducing x-risks
    
-   [Swiss Existential Risk Initiative (CHER)](https://www.google.com/url?q=https%3A%2F%2Feffectivealtruism.ch%2Fswiss-existential-risk-initiative&sa=D&sntz=1&usg=AOvVaw0jyKnKDWWfHOdk1vjfRNGL), focused on mitigating global catastrophic risk
    
-   [AI Safety Landscape](https://www.google.com/url?q=https%3A%2F%2Fwww.ai-safety.org%2F&sa=D&sntz=1&usg=AOvVaw3HhemmyORiBh50RcEtt8ap), outreach, coordination and academic workshops
    
-   [AGI4ALL](https://www.google.com/url?q=https%3A%2F%2Fagi4all.com%2F&sa=D&sntz=1&usg=AOvVaw05SczM8jBWncFiHs-U4580), support for AGI experts
    
-   [Nonlinear](https://www.google.com/url?q=https%3A%2F%2Fwww.nonlinear.org%2F&sa=D&sntz=1&usg=AOvVaw1XpuuiVKlHmvk8Y2IyKe8Q), search for and support high impact strategies to reduce existential and suffering risks
    
-   [Existential Risk Observatory](https://www.google.com/url?q=https%3A%2F%2Fwww.existentialriskobservatory.org%2F&sa=D&sntz=1&usg=AOvVaw0GpoXVU2ELOaSSCUFKkbhP), collects and spreads information about existential risks
    
-   [Lightcone Infrastructure](https://www.google.com/url?q=https%3A%2F%2Fwww.lightconeinfrastructure.com%2F&sa=D&sntz=1&usg=AOvVaw3mYN8Th3fotOdi4EWXr0fj) building tech, infrastructure and community
    

## Funding

-   [The Center on Long-Term Risk Fund (CLR Fund)](https://www.google.com/url?q=https%3A%2F%2Flongtermrisk.org%2Fgrantmaking%2F&sa=D&sntz=1&usg=AOvVaw10iGQL1Jj9PQxwiDn_L4Bj)
    
-   [Long-Term Future Fund (LTFF)](https://www.google.com/url?q=https%3A%2F%2Fapp.effectivealtruism.org%2Ffunds%2Ffar-future&sa=D&sntz=1&usg=AOvVaw01SFqlv8m3kGyJACJejhFc)
    
-   [Survival and Flourishing Fund (SFF)](http://www.google.com/url?q=http%3A%2F%2Fsurvivalandflourishing.fund%2F&sa=D&sntz=1&usg=AOvVaw2lEYqd39HMK0PXZieSXOPO)
    
    -   awards and facilitates grants to existing charities.
        
-   [Survival and Flourishing (SAF)](http://www.google.com/url?q=http%3A%2F%2Fsurvivalandflourishing.org&sa=D&sntz=1&usg=AOvVaw1wUAEsAOh78JP-pVfoAtP0)
    
    -   awards small grants and service contracts for long-termist projects that don't yet have an institutional home.
        
-   [Open Phil's](https://www.google.com/url?q=https%3A%2F%2Fwww.openphilanthropy.org%2Ffocus%2Fother-areas%2Fearly-career-funding-individuals-interested-improving-long-term-future&sa=D&sntz=1&usg=AOvVaw0zxjoTasI2KYGxef9c7cZ4) [Early-career funding for individuals interested in improving the long-term future](https://www.google.com/url?q=https%3A%2F%2Fwww.openphilanthropy.org%2Ffocus%2Fother-areas%2Fearly-career-funding-individuals-interested-improving-long-term-future&sa=D&sntz=1&usg=AOvVaw0zxjoTasI2KYGxef9c7cZ4)
    
-   [Open Philanthropy undergraduate scholarship](https://www.google.com/url?q=https%3A%2F%2Fwww.openphilanthropy.org%2Ffocus%2Fother-areas%2Fundergraduate-scholarship&sa=D&sntz=1&usg=AOvVaw23otPRk0cZbSrOKkGm_kSD)
    
-   [Open Phil AI Fellowship](https://www.google.com/url?q=https%3A%2F%2Fwww.openphilanthropy.org%2Ffocus%2Fglobal-catastrophic-risks%2Fpotential-risks-advanced-artificial-intelligence%2Fthe-open-phil-ai-fellowship&sa=D&sntz=1&usg=AOvVaw0EsGgMkprouSreIlcYzO0V)
    
-   [Future of Life Institute - Grants](https://www.google.com/url?q=https%3A%2F%2Fgrants.futureoflife.org%2F&sa=D&sntz=1&usg=AOvVaw3t9-QdGOre4HKO1ALaK3I1)
    
-   [ACX Grants](https://www.google.com/url?q=https%3A%2F%2Fastralcodexten.substack.com%2Fp%2Fapply-for-an-acx-grant&sa=D&sntz=1&usg=AOvVaw0nvAFNhFRxzBRZOqFUrHwd) by Slate Star Codex
    
-   [FTX Future Fund](https://www.google.com/url?q=https%3A%2F%2Fftxfuturefund.org%2F&sa=D&sntz=1&usg=AOvVaw2-RGAvR1TbHkhmwIuFysGt)
    
-   [Future Funding List](https://www.google.com/url?q=https%3A%2F%2Fwww.futurefundinglist.com%2F&sa=D&sntz=1&usg=AOvVaw1DKFtTXjBThv_Fz91aM4VD)
    

Community building related

-   [CEA Group Support Funding](https://www.google.com/url?q=https%3A%2F%2Fwww.centreforeffectivealtruism.org%2Fgroup-support-funding&sa=D&sntz=1&usg=AOvVaw3YI3bU9k4kcGmhbhyDZ47q)
    
-   [CGP Fast Funding](https://www.google.com/url?q=https%3A%2F%2Fwww.globalchallengesproject.org%2F&sa=D&sntz=1&usg=AOvVaw39GHX3f-4KgeBLruoRSOf1)
    

Housing

-   [CEEALAR / EA Hotel](https://www.google.com/url?q=https%3A%2F%2Fceealar.org%2F&sa=D&sntz=1&usg=AOvVaw0l8Z1LZ26Dx0Lft-6nFD26) is group house in Blackpool, UK which provides free food and housing for people working on Effective Altruist projects (including AI Safety), for up to two years
    
-   [EA Housing by Nonlinear](https://www.google.com/url?q=https%3A%2F%2Fforum.effectivealtruism.org%2Fposts%2F4zHWQNzCusaTfD7jz%2Fea-houses-live-or-stay-with-eas-around-the-world&sa=D&sntz=1&usg=AOvVaw3qvv-eodXYf4BV1SmCOqTO)