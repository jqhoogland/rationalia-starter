---
type: post
_id: 4nZRzoGTqg8xy5rr8
title: 'The reward engineering problem '
slug: the-reward-engineering-problem
href: >-
  https://www.lesswrong.com/posts/4nZRzoGTqg8xy5rr8/the-reward-engineering-problem
synchedAt: '2022-10-08T14:58:31.311Z'
tags:
  - Post
  - AI
  - Iterated_Amplification_
  - Reward_Functions
sequence: Iterated Amplification
author: paulfchristiano
status: todo
---

> how do we carry out the evaluation [of a transcript produced by an agent A], so that the optimal strategy for **A** is to also make “good” decisions?



# Related

- [[AI]]
- [[Iterated Amplification ]]
- [[Reward Functions]]
- "[Following Daniel Dewey](https://medium.com/@paulfchristiano/30285c779450)"
- "[semi-supervised RL](https://medium.com/ai-control/semi-supervised-reinforcement-learning-cf7d5375197f#.rm5dypm4a)"
- "[learning with catastrophes](https://medium.com/ai-control/learning-with-catastrophes-59387b55cc30#.vctnbxafg)"
- "[compare two behaviors](https://medium.com/ai-control/optimizing-with-comparisons-c02b8c0d7877#.jx6i2cxxu)"
- "[**H** needs to be better-informed than **A** about what is “good.”](https://medium.com/ai-control/adequate-oversight-25fadf1edce9#.1aj4sfjiu)"
- "[meeting halfway](https://medium.com/ai-control/mimicry-maximization-and-meeting-halfway-c149dd23fc17#.l513cw5l0)"
- "[may still be a useful technique for the resulting RL problem](https://medium.com/ai-control/learn-policies-or-goals-348add76b8eb#.rvsv6syfl)"
- "[these lines](https://medium.com/ai-control/alba-an-explicit-proposal-for-aligned-ai-17a55f60bbcf)"
- "[Future of Life Institute](http://futureoflife.org/)"
- "[here](https://ai-alignment.com/the-reward-engineering-problem-30285c779450)"