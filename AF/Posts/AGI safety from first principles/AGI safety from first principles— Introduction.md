---
_id: 8xRSjC76HasLnMGSf
title: 'AGI safety from first principles: Introduction'
author: null
url: null
slug: agi-safety-from-first-principles-introduction
type: post
tags:
  - LessWrong
  - Concept
  - Post
  - AI
  - AI_Safety Public Materials
  - AI_Safety_Public_Materials
href: >-
  https://www.lesswrong.com/posts/8xRSjC76HasLnMGSf/agi-safety-from-first-principles-introduction
sequence: Alignment & Agency
chapter: null
synchedAt: '2022-09-01T09:35:03.864Z'
status: todo
collection: Best of LessWrong
aliases: 
- Second species argument
next: [[AGI safety from first principles— Superintelligence]]
---

# AGI Safety From First Principles: Introduction

## "Second species" Argument

> The key concern motivating technical AGI safety research is that we might build autonomous artificially intelligent agents which are much more intelligent than humans, and which pursue goals that conflict with our own.

We will no longer be the most powerful "species" and lose our say in the future.

## Thesis

The default outcome is likely:

1. **"Superintelligence"**: AIs that are much more intelligent than humans.
2. "**Agency**": AIs that are autonomous agents & that pursue large-scale goals.
3. "**Misalignment**": AIs with goals that be undesirable for us.
4. "**Disempowerment**": AIs will gain control of humanity’s future.

This does not require AIs to emerge from the deep learning paradigm.

# Related

- [[AI]]
- [[AI Safety Public Materials]]
