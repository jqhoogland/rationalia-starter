---
type: post
_id: eGihD5jnD6LFzgDZA
title: 'AGI safety from first principles: Control'
slug: agi-safety-from-first-principles-control
href: >-
  https://www.lesswrong.com/posts/eGihD5jnD6LFzgDZA/agi-safety-from-first-principles-control
synchedAt: '2022-09-04T07:50:25.283Z'
tags:
  - LessWrong
  - Post
  - AI
sequence: AGI safety from first principles
author: Richard_Ngo
status: done
---

# AGI Safety From First Principles: Control

[[Takeoff & Takeover (Sequence)|Takeover]] is not guaranteed. But it seems likely.

- More intelligence = greater ability to acquire power (in coordination & technology)
- If humans + aligned AGIs are less capable than misaligned AGIs, the misaligned AGIs will gain more power.

Disaster scenarios

- [[What failure looks like#Out with a whimper]]. See also [[The Age of Em]]
- [[What failure looks like#Out with a bang]] ([[Singularity]], [[Intelligence Explosion]])

[[AI Timelines]]

- [[Takeoff & Takeover (Sequence)|Takeoff]]. Ngo expects fast growth but no discontinuous jump.
	- Competition in AGI likely trims low-hanging fruit
	- Compute availability grows continuously
	- Continuous technological progress is more common than discontinuous progress %% I have a hard time evaluating this claim %%

[[Relaxed adversarial training for inner alignment|Broad approaches]]

- [[Transparency, Interpretability (ML & AI)|Interpretability tools]]
- Incentives towards transparency
- More interpretable algorithms & architectures (model-based systems)

# Related

- [[AI]]
- [[What failure looks like]]
- "[lays out a scenario](https://ageofem.com/)"
- "[nanotechnology](http://intelligence.org/files/AIPosNegFactor.pdf)"
- "[biotechnology](https://80000hours.org/problem-profiles/positively-shaping-artificial-intelligence/)"
- "[making threats to force concessions from us](https://www.lesswrong.com/s/p947tK8CoBbdpPtyK)"
- "[Yudkowsky argues](https://intelligence.org/files/IEM.pdf)"
- "[particular](https://aiimpacts.org/likelihood-of-discontinuous-progress-around-the-development-of-agi/)"
- "[pushback](https://sideways-view.com/2018/02/24/takeoff-speeds/)"
- "[on some views](http://www.incompleteideas.net/IncIdeas/BitterLesson.html)"
- "[discontinuous progress](https://aiimpacts.org/discontinuous-progress-investigation/)"
- "[steady and predictable](https://aiimpacts.org/historic-trends-in-chess-ai/)"
- "[Hubinger lists](https://www.lesswrong.com/posts/9Dy5YRaoCxH9zuJqa/relaxed-adversarial-training-for-inner-alignment#The_core_problem__transparency)"
- "[some hypotheses](https://en.wikipedia.org/wiki/Cooperative_eye_hypothesis)"
- "[cross-examination in Debate](https://www.alignmentforum.org/posts/Br4xDbYu4Frwrb64a/writeup-progress-on-ai-safety-via-debate-1)"
- "[deploying AGIs in more constrained ways](https://www.alignmentforum.org/posts/Fji2nHBaB6SjdSscr/safer-sandboxing-via-population-separation)"
- "[Hansonâ€™s _Age of Em_](https://ageofem.com/)"
