---
type: post
_id: oiuZjPfknKsSc5waC
title: Commentary on AGI Safety from First Principles
slug: commentary-on-agi-safety-from-first-principles
href: >-
  https://www.lesswrong.com/posts/oiuZjPfknKsSc5waC/commentary-on-agi-safety-from-first-principles
synchedAt: '2022-09-04T07:50:27.993Z'
tags:
  - LessWrong
  - Post
  - AI
sequence: AGI safety from first principles
author: Richard_Ngo
status: in progress
---

# Commentary

[[Will McAskill]] identifies two claims:

**Claim 1** ("uncontroversial")

> At some point in the future, assuming continued tech progress, history will have primarily become the story of AI systems doing things. The goals of those AI systems, or the emergent path that results from interactions among these systems, will probably not be what you reading this document want to happen.

**Claim 2** ("controversial")

> Claim 1 is true, and the point in time at which the transition from a human-driven world to an AI-driven world is in our lifetime, and the transition will be fast, and we can meaningfully affect how this transition goes with very long-lasting impacts, and (on the classic formulations at least) the transition will be to a single AI agent with more power than all other agents combined, and what we should try to do in response to all this is ensure that the AI systems that get built have goals that are the same as the goals of those who design the AI systems.

==A sufficiently high-dimensional utility function is impossible for the agent to grasp in full. This seems to put explicit pressure towards reversible outcomes that prevent the agent from constraining probability space too tightly. Would this manifest as not-caring (too much) about goals?==

# Related

- [[AI]]
- "[now online here](https://www.alignmentforum.org/s/mzgtmmTKKn5MuCzFJ)"
- "[practically any behaviour](https://www.alignmentforum.org/posts/vphFJzK3mWA4PJKAg/coherent-behaviour-in-the-real-world-is-an-incoherent)"
- [[Coherent decisions imply consistent utilities]]
- "[https://intelligence.org/2016/12/28/ai-alignment-why-its-hard-and-where-to-start/](https://intelligence.org/2016/12/28/ai-alignment-why-its-hard-and-where-to-start/)"
- "[Ortega et al.](https://medium.com/@deepmindsafetyresearch/building-safe-artificial-intelligence-52f5f75058f1)"
- "[this comment](https://www.alignmentforum.org/posts/BKM8uQS6QdJPZLqCr/towards-a-mechanistic-understanding-of-corrigibility#qyQNLHgFfCLKaTuaH)"
- "[longer thoughts I wrote up on this at one point.](https://docs.google.com/document/d/1mMvsiavUbHsJt2dMcQr6yEwr3Qjfj5qeb38QlCmU3Nc/edit?usp=sharing)"
- "[here](https://slatestarcodex.com/2020/05/12/studies-on-slack/)"
- "[run 56 times faster than humans](https://nofilmschool.com/2014/01/adam-magyar-high-speed-camera-film-series-stainless)"
- [[Evolving to Extinction]]
