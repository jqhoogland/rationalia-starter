---
type: post
_id: oiuZjPfknKsSc5waC
title: Commentary on AGI Safety from First Principles
slug: commentary-on-agi-safety-from-first-principles
href: >-
  https://www.lesswrong.com/posts/oiuZjPfknKsSc5waC/commentary-on-agi-safety-from-first-principles
synchedAt: '2022-09-04T07:50:27.993Z'
tags:
  - LessWrong
  - Post
  - AI
sequence: AGI safety from first principles
author: Richard_Ngo
status: todo
---

# Related

- [[AI]]
- "[now online here](https://www.alignmentforum.org/s/mzgtmmTKKn5MuCzFJ)"
- "[practically any behaviour](https://www.alignmentforum.org/posts/vphFJzK3mWA4PJKAg/coherent-behaviour-in-the-real-world-is-an-incoherent)"
- [[Coherent decisions imply consistent utilities]]
- "[https://intelligence.org/2016/12/28/ai-alignment-why-its-hard-and-where-to-start/](https://intelligence.org/2016/12/28/ai-alignment-why-its-hard-and-where-to-start/)"
- "[Ortega et al.](https://medium.com/@deepmindsafetyresearch/building-safe-artificial-intelligence-52f5f75058f1)"
- "[this comment](https://www.alignmentforum.org/posts/BKM8uQS6QdJPZLqCr/towards-a-mechanistic-understanding-of-corrigibility#qyQNLHgFfCLKaTuaH)"
- "[longer thoughts I wrote up on this at one point.](https://docs.google.com/document/d/1mMvsiavUbHsJt2dMcQr6yEwr3Qjfj5qeb38QlCmU3Nc/edit?usp=sharing)"
- "[here](https://slatestarcodex.com/2020/05/12/studies-on-slack/)"
- "[run 56 times faster than humans](https://nofilmschool.com/2014/01/adam-magyar-high-speed-camera-film-series-stainless)"
- [[Evolving to Extinction]]
