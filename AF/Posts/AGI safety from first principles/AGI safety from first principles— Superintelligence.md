---
type: post
_id: eG3WhHS8CLNxuH6rT
title: 'AGI safety from first principles: Superintelligence'
slug: agi-safety-from-first-principles-superintelligence
href: >-
  https://www.lesswrong.com/posts/eG3WhHS8CLNxuH6rT/agi-safety-from-first-principles-superintelligence
synchedAt: '2022-09-04T07:49:39.885Z'
tags:
  - LessWrong
  - Post
  - AI
sequence: AGI safety from first principles
author: null
status: todo
---

# Related

- [[AI]]
- "[the ability to do well on a broad range of cognitive tasks](https://arxiv.org/abs/0712.3329)"
- "[_Reframing Superintelligence_](https://www.fhi.ox.ac.uk/wp-content/uploads/Reframing_Superintelligence_FHI-TR-2019-1.1-1.pdf)"
- "[Iâ€™m skeptical of this claim](https://www.alignmentforum.org/posts/HvNAmkXPTSoA4dvzv/comments-on-cais)"
- "[state of the art results](https://openai.com/blog/better-language-models/)"
- "[a range of even more impressive behaviour](https://twitter.com/xuenay/status/1283312640199196673?s=20)"
- "[specific features of the ancestral environment](https://www.alignmentforum.org/posts/vqpEC3MPioHX7bv4t/environments-as-a-bottleneck-in-agi-development)"
- "[some have hypothesised](http://archives.evergreen.edu/webpages/curricular/2006-2007/languageofpolitics/files/languageofpolitics/Evol_Anthrop_6.pdf)"
- "[Many AI researchers expect](https://arxiv.org/abs/1705.08807)"
- "[Bostrom defines a superintelligence](https://en.wikipedia.org/wiki/Superintelligence)"
- "[many factors](https://intelligenceexplosion.com/2011/plenty-of-room-above-us/)"
- "[_collective AGI_](https://www.alignmentforum.org/posts/HekjhtWesBWTQW5eF/agis-as-populations)"
- "[rewriting its own source code](http://intelligence.org/files/LOGI.pdf)"
- "[the classic view of recursive self-improvement](https://intelligence.org/files/IEM.pdf)"
