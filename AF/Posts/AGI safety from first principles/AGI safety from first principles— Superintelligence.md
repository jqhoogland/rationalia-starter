---
type: post
_id: eG3WhHS8CLNxuH6rT
title: 'AGI safety from first principles: Superintelligence'
slug: agi-safety-from-first-principles-superintelligence
href: >-
  https://www.lesswrong.com/posts/eG3WhHS8CLNxuH6rT/agi-safety-from-first-principles-superintelligence
synchedAt: '2022-09-04T07:49:39.885Z'
tags:
  - LessWrong
  - Post
  - AI
sequence: AGI safety from first principles
author: Richard Ngo
status: todo
---

# AGI safety from first principles: Superintelligence

==What is **intelligence**?==
![[General Intelligence#^db4a54|Intelligence]] 

Ngo distinguishes two types of agents/approaches:
1. **Task-based approach**: %% "Narrow" %% AIs that are specially optimized for each task.
	- E.g.: RL algorithms like AlphaGo & AlphaStar.
    - [[Reframing Superintelligence— Comprehensive AI Services as General Intelligence|Drexler claims]] this approach will skill up to superintelligence. Ngo is skeptical.
    - Ngo expects this to get us very far in areas where we can gather lots of data (e.g., self-driving, even professions like medicine, law, and mathematics)
2. **Generalization-based approach**: %% "General" %% AIs that can understand new tasks with little task-specific training.
	- E.g.: GPT-3. Predicting the next word in a corpus gave GPT-3 totally novel results in other tasks; Modern human activities are very different from those practiced in our [[Environment of Evolutionary Adaptedness]]
	- A potential obstacle is that general intelligence required "[specific features of the ancestral environment](https://www.alignmentforum.org/posts/vqpEC3MPioHX7bv4t/environments-as-a-bottleneck-in-agi-development)" and even - "[social arms race](http://archives.evergreen.edu/webpages/curricular/2006-2007/languageofpolitics/files/languageofpolitics/Evol_Anthrop_6.pdf)"

==What is **superintelligence**?==
![[Superintelligence#^6cd1f4]]

Ngo adds the additional requirement of doing better than "all of humanity could if we coordinated globally (unaided by other advanced AI)."

[[Plenty of room above us|There's no reason to think superintelligence impossible or even unlikely.]].

Ngo won't focus much on [[AI Takeoff|takeoff speed]]. 

==What are the drivers of a possible [[Intelligence Explosion]]?==
- **Replication** ("Collective AGI"): It's substantially cheaper to run instances of trained models than it is to train them. Cloning AIs is cheaper than cloning humans.
- **Cultural learning**: AGIs are likely to share knowledge with each other. After all, we did it.
- **Recursive improvement**. This doesn't even have to be *self*-improvement. 
	- "Collective AGI" makes the individual AGI unimportant.
	- Directly modifying one's own weights is likely more difficult than training a new model.
	- It's not useful to require humans being entirely out of the loop.


# Related

- [[AI]]
- "[I’m skeptical of this claim](https://www.alignmentforum.org/posts/HvNAmkXPTSoA4dvzv/comments-on-cais)"
- "[state of the art results](https://openai.com/blog/better-language-models/)"
- "[a range of even more impressive behaviour](https://twitter.com/xuenay/status/1283312640199196673?s=20)"
- "[Many AI researchers expect](https://arxiv.org/abs/1705.08807)"
- "[_collective AGI_](https://www.alignmentforum.org/posts/HekjhtWesBWTQW5eF/agis-as-populations)"
- "[rewriting its own source code](http://intelligence.org/files/LOGI.pdf)"
- "[the classic view of recursive self-improvement](https://intelligence.org/files/IEM.pdf)"
