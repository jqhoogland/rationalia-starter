---
type: post
_id: Ni8ocGupB2kGG2fA7
title: 'AGI safety from first principles: Conclusion'
slug: agi-safety-from-first-principles-conclusion
href: >-
  https://www.lesswrong.com/posts/Ni8ocGupB2kGG2fA7/agi-safety-from-first-principles-conclusion
synchedAt: '2022-09-04T07:50:24.200Z'
tags:
  - LessWrong
  - Post
  - AI
sequence: AGI safety from first principles
author: Richard_Ngo
status: done
---

# AGI Safety From First Principles: Conclusion

1. We'll build AIs much more [[General Intelligence|intelligent]] than humans. 
2. Those AGIs will likely have [[agency]] â€” they will pursue goals because this is reinforced during training.
	1. Those goals are likely to be "long-term" and "large-scale" because we expect goals to generalize (as capabilities do)
3. These goals will by default be misaligned. Desires are complex & our existing tools for shaping goals are inadequate.
4. Autonomous, misaligned AGIs will gain control of humanity's future due to their intelligence.

Ngo questions his reliance on [[Anthropomorphism]].

# Related

- [[AI]]
- "[misuse concerns](https://maliciousaireport.com/)"
- "[undesirable structural changes](https://www.lawfareblog.com/thinking-about-risks-ai-accidents-misuse-and-structure)"
- "[shift the offense-defence balance](https://www.tandfonline.com/doi/full/10.1080/01402390.2019.1631810)"
- [[What failure looks like]]
