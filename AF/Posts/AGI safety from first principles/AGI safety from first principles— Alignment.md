---
type: post
_id: PvA2gFMAaHCHfMXrw
title: 'AGI safety from first principles: Alignment'
slug: agi-safety-from-first-principles-alignment
href: >-
  https://www.lesswrong.com/posts/PvA2gFMAaHCHfMXrw/agi-safety-from-first-principles-alignment
synchedAt: '2022-09-04T07:50:15.192Z'
tags:
  - LessWrong
  - Post
  - AI
sequence: AGI safety from first principles
author: Richard_Ngo
status: todo
---


# Related

- [[AI]]
- "[Following Gabriel](https://arxiv.org/abs/2001.09768)"
- "[Christiano](https://ai-alignment.com/ambitious-vs-narrow-value-learning-99bd0c59847e)"
- "[Christiano’s concept of intent alignment](https://ai-alignment.com/clarifying-ai-alignment-cec47cd69dd6)"
- "[orthogonality thesis](https://www.nickbostrom.com/superintelligentwill.pdf)"
- "[Goodhart’s law](https://arxiv.org/abs/1803.04585)"
- "[specification gaming behaviours](https://vkrakovna.wordpress.com/2018/04/02/specification-gaming-examples-in-ai/)"
- "[reward modelling](https://arxiv.org/abs/1706.03741)"
- "[Debate](https://openai.com/blog/debate/)"
- "[Recursive Reward Modelling](https://medium.com/@deepmindsafetyresearch/scalable-agent-alignment-via-reward-modeling-bf4ab06dfd84)"
- "[Iterated Amplification](https://openai.com/blog/amplifying-ai-training/)"
- "[the robot hand example here](https://openai.com/blog/deep-reinforcement-learning-from-human-preferences/)"
- "[as I’ve argued here,](https://www.alignmentforum.org/posts/GqxuDtZvfgL2bEQ5v/arguments-against-myopic-training)"
- "[can be modified](https://arxiv.org/abs/1505.07818)"
- "[this post by Sanjeev Arora](http://www.offconvex.org/2019/06/03/trajectories/)"
- "[deceptive alignment](https://www.alignmentforum.org/s/r9tYkB2a8Fp4DN8yB/p/zthDPAjh9w6Ytbeks)"
- [[Open question: are minimal circuits daemon-free?]]
- "[Risks from Learned Optimisation in Advanced Machine Learning Systems](https://arxiv.org/abs/1906.01820)"
- [[Gradient hacking]]