---
type: post
_id: bz5GdmCWj8o48726N
title: 'AGI safety from first principles: Goals and Agency'
slug: agi-safety-from-first-principles-goals-and-agency
href: >-
  https://www.lesswrong.com/posts/bz5GdmCWj8o48726N/agi-safety-from-first-principles-goals-and-agency
synchedAt: '2022-09-04T07:50:07.085Z'
tags:
  - LessWrong
  - Post
  - AI
sequence: AGI safety from first principles
author: Richard_Ngo
status: todo
---

# Related

- [[AI]]
- [[What failure looks like]]
- "[instrumental convergence thesis](https://www.nickbostrom.com/superintelligentwill.pdf)"
- "[_design objectives_](https://medium.com/@deepmindsafetyresearch/building-safe-artificial-intelligence-52f5f75058f1)"
- "[_expected utility maximisation_](https://en.wikipedia.org/wiki/Von_Neumann%E2%80%93Morgenstern_utility_theorem)"
- "[_intentional stance_](https://en.wikipedia.org/wiki/Intentional_stance)"
- "[_mesa-optimisation_](https://arxiv.org/abs/1906.01820)"
- "[practically any behaviour](https://www.alignmentforum.org/posts/vphFJzK3mWA4PJKAg/coherent-behaviour-in-the-real-world-is-an-incoherent)"
- "[_Risks from Learned Optimisation in Advanced ML systems_](https://arxiv.org/abs/1906.01820)"
- "[sphexish](http://www.personalityresearch.org/evolutionary/sphexishness.html)"
- "[planning using an implicit model](https://arxiv.org/abs/1901.03559)"
- "[Moravec’s paradox](https://en.wikipedia.org/wiki/Moravec%27s_paradox)"
- "[open-ended environment](https://arxiv.org/abs/2006.07495)"
- "[quantilisation](http://intelligence.org/files/QuantilizersSaferAlternative.pdf)"
- "[good reasons](https://arxiv.org/abs/1903.00742)"
- "[this type of specialisation](https://www.alignmentforum.org/posts/7jNveWML34EsjCD4c/safety-via-selection-for-obedience)"
- "[to deploy subsets of the collective](https://www.alignmentforum.org/posts/Fji2nHBaB6SjdSscr/safer-sandboxing-via-population-separation)"
- "[Hubinger et al’s paper](https://arxiv.org/abs/1906.01820)"
- [[Paul's research agenda FAQ]]
