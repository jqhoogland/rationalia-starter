---
type: post
_id: h9DesGT3WT9u2k7Hrh9DesGT3WT9u2k7Hr
title: The easy goal inference problem is still hard
slug: the-easy-goal-inference-problem-is-still-hard
href: >-
  https://www.lesswrong.com/posts/h9DesGT3WT9u2k7Hr/the-easy-goal-inference-problem-is-still-hard
synchedAt: '2022-09-15T19:23:59.924Z'
tags:
  - Post
  - Value_Learning
  - AI
sequence: Value Learning
author: paulfchristiano
status: todo
---

[[What is ambitious value learning?|Ambitious value learning]] "rests on an optimistic assumption that it's possible to model a human as an imperfect rational agent."

**The easy goal inference problem**. 
>  Given no algorithmic limitations and access to the complete human policy — a lookup table of what a human would do after making any sequence of observations — find any reasonable representation of any reasonable approximation to what that human wants.
>  
>  I think that this problem remains wide open[.]

Modeling mistakes is difficult — how to separate "good" part of human decisions from the "bad part"?

# Related

- [[Value Learning]]
- [[AI]]
- "[Value Learning](https://www.alignmentforum.org/s/4dHMdK5TLN6xcqtyc)"
- "[here)](https://ai-alignment.com/the-easy-goal-inference-problem-is-still-hard-fad030e0a876)"