---
type: post
_id: zthDPAjh9w6Ytbeks
title: Deceptive Alignment
slug: deceptive-alignment
href: https://www.lesswrong.com/posts/zthDPAjh9w6Ytbeks/deceptive-alignment
synchedAt: '2022-09-04T18:59:59.688Z'
tags:
  - LessWrong
  - Post
  - Mesa-Optimization
  - AI
  - AI_Risk
  - Deceptive_Alignment
sequence: Risks from Learned Optimization
author: evhub
status: todo
---

# Deceptive Alignment

A form of [[misalignment]] in which models try to fool the training process into thinking that theyâ€™re doing the right thing.

# Related

- [[Mesa-Optimization]]
- [[AI]]
- [[AI Risk]]
- [[Deceptive Alignment]]
- [[Risks from Learned Optimization]]
- "[Risks from Learned Optimization in Advanced Machine Learning Systems](https://arxiv.org/abs/1906.01820)"
- "[(22)](https://intelligence.org/learned-optimization#bibliography)"
- "[(23)](https://intelligence.org/learned-optimization#bibliography)"
- "[(24)](https://intelligence.org/learned-optimization#bibliography)"
- "[(25)](https://intelligence.org/learned-optimization#bibliography)"
- "[the third post](https://www.alignmentforum.org/posts/pL56xPoniLvtMDQ4J/the-inner-alignment-problem)"
- "[(26)](https://intelligence.org/learned-optimization#bibliography)"
- "[(27)](https://intelligence.org/learned-optimization#bibliography)"
- "[here](https://www.alignmentforum.org/posts/4XPa3xa44jAWiCkmy/risks-from-learned-optimization-conclusion-and-related-work)"
- "[Glossary](https://intelligence.org/learned-optimization/#glossary)"
- "[Bibliography](https://intelligence.org/learned-optimization/#bibliography)"
