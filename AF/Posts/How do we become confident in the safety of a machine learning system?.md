---
_id: FDJnZt8Ks2djouQTZ
type: post
title: How do we become confident in the safety of a machine learning system?
slug: how-do-we-become-confident-in-the-safety-of-a-machine
href: >-
  https://www.lesswrong.com/posts/FDJnZt8Ks2djouQTZ/how-do-we-become-confident-in-the-safety-of-a-machine
synchedAt: '2022-09-04T18:47:44.993Z'
tags:
  - LessWrong
  - Post
  - AI
author: null
status: in progress
aliases:
  - Training stories
---

# How do we become confident in the safety of a machine learning system?

## Training stories
>  a story of how you think training is going to go and what sort of model you think you’re going to get at the end, as a way of explaining how you’re planning on dealing with that very fundamental question of how your model is going to learn to accomplish the task that you give it.

We want *mechanistic* stories in terms of what algorithm you end up getting. These should be falsifiable.

### Components
- **Training goal**: the algorithm you hope your model will learn (specification) & why it's good (desirability)
- **Training rationale**: why (not just what) you believe the training set up will lead to that sort of algorithm. Mention constraints & nudges


# Related

- [[AI]]
- [[An overview of 11 proposals for building safe advanced AI]]
- [[Mesa-Optimization]]
- [[Objective misgeneralization]]

- "[Adversarial Examples Are Not Bugs, They Are Features](https://arxiv.org/abs/1905.02175)"
- "[how Paul describes corrigibility](https://ai-alignment.com/corrigibility-3039e668638)"
- "[corrigible](https://intelligence.org/files/Corrigibility.pdf)"
- "[article on Arbital](https://arbital.com/p/corrigibility/)"
- "[Teaching ML to answer questions honestly instead of predicting human answers](https://www.alignmentforum.org/posts/QqwZ7cwEA2cxFEAun/teaching-ml-to-answer-questions-honestly-instead-of)"
- "[objective robustness failure](https://arxiv.org/pdf/2105.14111.pdf)"
- "[capability generalization without objective generalization](https://www.alignmentforum.org/posts/2mhFMgtAjFJesaSYR/2-d-robustness)"
- "[Towards an empirical investigation of inner alignment](https://www.alignmentforum.org/posts/2GycxikGnepJbxfHT/towards-an-empirical-investigation-of-inner-alignment)"
- "[Objective Robustness in Deep Reinforcement Learning](https://arxiv.org/abs/2105.14111#)"
- [[Risks from Learned Optimization— Introduction]]
- "[unintended mesa-optimization](https://www.alignmentforum.org/s/r9tYkB2a8Fp4DN8yB/p/FkgsxrGf3QxhfLWHG#1_4__Mesa_optimization_as_a_safety_problem)"
- "[Outer alignment](https://www.alignmentforum.org/posts/33EKjmAdKFn3pbKPJ/outer-alignment-and-imitative-amplification)"
- "[Deceptive alignment](https://www.alignmentforum.org/s/r9tYkB2a8Fp4DN8yB/p/zthDPAjh9w6Ytbeks)"
- "[Does SGD Produce Deceptive Alignment?](https://www.alignmentforum.org/posts/ocWqg2Pf2br4jMmKA/does-sgd-produce-deceptive-alignment)"
- [[The Parable of Predict-O-Matic]]
- [[Chris Olah’s views on AGI safety]]
- "[automated transparency](https://www.alignmentforum.org/posts/cQwT8asti3kyA62zc/automating-auditing-an-ambitious-concrete-technical-research)"
- "[wirehead](https://www.lesswrong.com/posts/vXzM5L6njDZSf4Ftk/defining-ai-wireheading)"
- "[ambitious value learning](https://www.alignmentforum.org/posts/5eX8ko7GCxwR5N9mN/what-is-ambitious-value-learning)"
- "[Paul’s definition of corrigibility](https://ai-alignment.com/corrigibility-3039e668638)"
- "[approval-directed agent](https://ai-alignment.com/model-free-decisions-6e6609f5d99e)"
- "[there are some potential issues](http://lesswrong.com/posts/BKM8uQS6QdJPZLqCr/towards-a-mechanistic-understanding-of-corrigibility#Act_Based_Corrigibility)"
- "[act-based agents](https://ai-alignment.com/act-based-agents-8ec926c79e9c)"
- "[LCDT agent](https://www.alignmentforum.org/posts/Y76durQHrfqwgwM5o/lcdt-a-myopic-decision-theory)"
- "[HCH](https://ai-alignment.com/strong-hch-bedb0dc08d4e)"
- "[AlphaFold](https://deepmind.com/blog/article/alphafold-a-solution-to-a-50-year-old-grand-challenge-in-biology)"
- "[LCDT](https://www.alignmentforum.org/posts/Y76durQHrfqwgwM5o/lcdt-a-myopic-decision-theory)"
- "[Teaching ML to answer questions honestly instead of predicting human answers](https://www.alignmentforum.org/posts/QqwZ7cwEA2cxFEAun/teaching-ml-to-answer-questions-honestly-instead-of#The_problem)"
- "[optimization algorithms](https://www.alignmentforum.org/s/r9tYkB2a8Fp4DN8yB)"
- "[deception](https://www.alignmentforum.org/s/r9tYkB2a8Fp4DN8yB)"
- "[current large language models seem to be capable of understanding how to deceive humans](https://www.alignmentforum.org/posts/22GrdspteQc8EonMn/a-very-crude-deception-eval-is-already-passed)"
- "[Answering questions honestly instead of predicting human answers: lots of problems and some solutions](https://www.alignmentforum.org/posts/gEw8ig38mCGjia7dj/answering-questions-honestly-instead-of-predicting-human)"
- [[Understanding “Deep Double Descent”]]
- "[lottery tickets](https://arxiv.org/abs/1803.03635)"
- "[scaling laws](https://arxiv.org/abs/2001.08361)"
- "[grokking](https://mathai-iclr.github.io/papers/papers/MATHAI_29_paper.pdf)"
- "[distributional generalization](https://arxiv.org/abs/2009.08092)"
- "[worst-case transparency](https://www.alignmentforum.org/posts/cQwT8asti3kyA62zc/automating-auditing-an-ambitious-concrete-technical-research#More_general_thoughts)"
- "[Relaxed adversarial training for inner alignment](https://www.alignmentforum.org/posts/9Dy5YRaoCxH9zuJqa/relaxed-adversarial-training-for-inner-alignment)"
- "[Causal Analysis of Agent Behavior for AI Safety](https://deepmindsafetyresearch.medium.com/what-mechanisms-drive-agent-behaviour-e7b8d9aee88)"
- "[a deceptive model that’s actively trying to trick its training process](https://www.alignmentforum.org/posts/ocWqg2Pf2br4jMmKA/does-sgd-produce-deceptive-alignment)"
- "[Shaping safer goals](https://www.alignmentforum.org/s/boLPsyNwd6teK5key)"
- "[deceptive alignment](https://www.alignmentforum.org/s/r9tYkB2a8Fp4DN8yB)"
- "[Sensitivity analysis](https://en.wikipedia.org/wiki/Sensitivity_analysis)"
- "[catastrophically](https://www.alignmentforum.org/s/r9tYkB2a8Fp4DN8yB)"
- "[red-team](https://en.wikipedia.org/wiki/Red_team)"
- "[Open Problems with Myopia](https://www.alignmentforum.org/posts/LCLBnmwdxkkz5fNvH/open-problems-with-myopia)"