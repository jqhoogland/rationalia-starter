---
href: https://openai.com/blog/ai-and-efficiency/
---

![[Pasted image 20220924221518.png]]

> Efficiency trends can be compared across domains like DNA sequencing[17](https://openai.com/blog/ai-and-efficiency/#rf17) (10-month doubling), solar energy[18](https://openai.com/blog/ai-and-efficiency/#rf18) (6-year doubling), and transistor density[3](https://openai.com/blog/ai-and-efficiency/#rf3) (2-year doubling).

> We saw a similar rate of training efficiency improvement for ResNet-50 level performance on ImageNet (17-month doubling time).[7](https://openai.com/blog/ai-and-efficiency/#rf7)[16](https://openai.com/blog/ai-and-efficiency/#rf16) We saw faster rates of improvement over shorter timescales in Translation, Go, and Dota 2:

> 1.  Within translation, the Transformer[22](https://openai.com/blog/ai-and-efficiency/#rf22) surpassed seq2seq[23](https://openai.com/blog/ai-and-efficiency/#rf23) performance on English to French translation on WMT’14 with 61x less training compute 3 years later.
> 2.  We estimate AlphaZero[24](https://openai.com/blog/ai-and-efficiency/#rf24) took 8x less compute to get to AlphaGoZero[25](https://openai.com/blog/ai-and-efficiency/#rf25) level performance 1 year later.
> 3.  OpenAI Five Rerun required 5x less training compute to surpass OpenAI Five[26](https://openai.com/blog/ai-and-efficiency/#rf26) (which beat the world champions, [OG](https://liquipedia.net/dota2/OG)) 3 months later.


Comparison to "tick tock" model of semiconductors: initial tick is more expensive, the subsequent tock makes it much more efficient.