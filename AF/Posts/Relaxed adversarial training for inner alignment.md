---
type: post
_id: 9Dy5YRaoCxH9zuJqa
title: Relaxed adversarial training for inner alignment
slug: relaxed-adversarial-training-for-inner-alignment
href: >-
  https://www.lesswrong.com/posts/9Dy5YRaoCxH9zuJqa/relaxed-adversarial-training-for-inner-alignment
synchedAt: '2022-09-04T17:56:58.331Z'
tags:
  - LessWrong
  - Post
  - Inner_Alignment
  - Transparency_/_Interpretability_(ML_&_AI)
  - AI
  - Iterated_Amplification_
  - AI_Risk
author: null
status: todo
---


# Related

- [[Inner Alignment]]
- [[Transparency / Interpretability (ML & AI)]]
- [[AI]]
- [[Iterated Amplification ]]
- [[AI Risk]]
- "[inner alignment](https://arxiv.org/abs/1906.01820)"
- "[follow Paul Christiano](https://ai-alignment.com/training-robust-corrigibility-ce0e0a3b9b4d)"
- "[corrigibility](https://ai-alignment.com/corrigibility-3039e668638)"
- "[non-deception](https://www.alignmentforum.org/s/r9tYkB2a8Fp4DN8yB/p/zthDPAjh9w6Ytbeks)"
- "[Towards a mechanistic understanding of corrigibility](https://www.alignmentforum.org/posts/BKM8uQS6QdJPZLqCr/towards-a-mechanistic-understanding-of-corrigibility)"
- "[Risks from Learned Optimization](https://arxiv.org/abs/1906.01820)"
- "[Ortega et al. recently presented in a DeepMind Technical Report](https://arxiv.org/abs/1905.03030)"
- "[experimental evidence](https://www.alignmentforum.org/posts/uSdPa9nrSgmXCtdKN/concrete-experiments-in-inner-alignment)"
- "[capability generalization without objective generalization](https://www.alignmentforum.org/posts/2mhFMgtAjFJesaSYR/2-d-robustness)"
- "[One framework Paul has proposed](https://ai-alignment.com/training-robust-corrigibility-ce0e0a3b9b4d)"
- "[Paul's proposed solution](https://ai-alignment.com/training-robust-corrigibility-ce0e0a3b9b4d)"
- "[as per Paul's proposal](https://ai-alignment.com/training-robust-corrigibility-ce0e0a3b9b4d)"
- "[iterative black-box optimization](https://github.com/evhub/iternash)"
- "[deceptive](https://www.alignmentforum.org/posts/zthDPAjh9w6Ytbeks/deceptive-alignment)"
- "[Paul provides the example](https://ai-alignment.com/training-robust-corrigibility-ce0e0a3b9b4d)"
- "[RSA-2048](https://en.wikipedia.org/wiki/RSA_numbers#RSA-2048)"
- "[RSA Factoring Challenge](https://en.wikipedia.org/wiki/RSA_Factoring_Challenge)"
- "[My counterexample to the safety of minimal circuits](https://www.alignmentforum.org/posts/fM5ZWGDbnjb7ThNKJ/are-minimal-circuits-deceptive)"
- "[Paul noted](https://ai-alignment.com/training-robust-corrigibility-ce0e0a3b9b4d)"
- "[ascription universality](https://ai-alignment.com/towards-formalizing-universality-409ab893a456)"
- "[future research](https://www.alignmentforum.org/posts/Zj2PgP5A8vY2G3gYw/optimization-provenance)"
- "[inspecting the trained model](https://distill.pub/2018/building-blocks/)"
- "[models implementing highly coherent optimization processes are the most competitive](https://www.alignmentforum.org/s/r9tYkB2a8Fp4DN8yB/p/q2rCMHNXazALgQpGH)"
- "[Attainable Utility Preservation](https://arxiv.org/abs/1902.09725)"
- "[Relative Reachability](https://arxiv.org/abs/1806.01186)"
- "[Activation Atlases](https://distill.pub/2019/activation-atlas/)"