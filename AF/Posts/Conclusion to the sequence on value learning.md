---
type: post
_id: TE5nJ882s5dCMkBB8
title: Conclusion to the sequence on value learning
slug: conclusion-to-the-sequence-on-value-learning
href: >-
  https://www.lesswrong.com/posts/TE5nJ882s5dCMkBB8/conclusion-to-the-sequence-on-value-learning
synchedAt: '2022-09-25T14:37:24.538Z'
tags:
  - Post
  - Value_Learning
sequence: Value Learning
author: rohinmshah
status: todo
---

> I am *not* claiming that we don’t need to worry about AI safety since AIs won’t be expected utility maximizers. First of all, you *can* model them as expected utility maximizers, it’s just not useful. Second, if we build an AI system whose internal reasoning consisted of maximizing the expectation of some simple utility function, I think all of the classic concerns apply. Third, it does seem likely that [humans will build AI systems that are “trying to pursue a goal”](https://www.alignmentforum.org/s/4dHMdK5TLN6xcqtyc/p/9zpT9dikrrebdq3Jf), and that can have all of the standard [convergent instrumental subgoals](https://selfawaresystems.files.wordpress.com/2008/01/ai_drives_final.pdf). I propose that we describe these systems as [goal-directed](https://www.alignmentforum.org/s/4dHMdK5TLN6xcqtyc/p/DfcywmqRSkBaCB6Ma) rather than expected utility maximizers, since the latter is vacuous and implies a level of formalization that we have not yet reached.

^3fb196

# Related

- [[Value Learning]]
- "[impressions, not my beliefs](http://www.overcomingbias.com/2008/04/naming-beliefs.html)"
- "[should not be exploitable](https://arbital.com/p/optimized_agent_appears_coherent/)"
- "[expected utility maximizer](https://arbital.com/p/expected_utility_formalism/?l=7hh)"
- "[Goodhart’s Law](https://www.lesswrong.com/posts/EbFABnst8LsidYs5Y/goodhart-taxonomy)"
- [[Value is Fragile]]
- "[get the right utility function](https://www.alignmentforum.org/s/4dHMdK5TLN6xcqtyc/p/5eX8ko7GCxwR5N9mN)"
- "[adequate](https://www.alignmentforum.org/posts/Y2LhX3925RodndwpC/resolving-human-values-completely-and-adequately)"
- "[full access to the entire human policy](https://www.alignmentforum.org/s/4dHMdK5TLN6xcqtyc/p/h9DesGT3WT9u2k7Hr)"
- "[cannot infer their values](https://www.alignmentforum.org/s/4dHMdK5TLN6xcqtyc/p/ANupXf8XfZo2EJxGv)"
- "[misspecification](https://www.alignmentforum.org/s/4dHMdK5TLN6xcqtyc/p/gnvrixhDfG7S2TpNL)"
- "[bad inferences](https://www.alignmentforum.org/s/4dHMdK5TLN6xcqtyc/p/cnC2RMWEGiGpJv8go)"
- "[feedback from humans](https://www.alignmentforum.org/s/4dHMdK5TLN6xcqtyc/p/4783ufKpx8xvLMPc6)"
- "[potential avenues](https://www.alignmentforum.org/s/4dHMdK5TLN6xcqtyc/p/EhNCnCkmu7MwrQ7yz)"
- "[otherwise we could Dutch book it](https://arbital.com/p/optimized_agent_appears_coherent/)"
- [[Coherence arguments do not entail goal-directed behavior]]
- "[Optimized agent appears coherent](https://arbital.com/p/optimized_agent_appears_coherent/)"
- "[humans will build AI systems that are “trying to pursue a goal”](https://www.alignmentforum.org/s/4dHMdK5TLN6xcqtyc/p/9zpT9dikrrebdq3Jf)"
- "[convergent instrumental subgoals](https://selfawaresystems.files.wordpress.com/2008/01/ai_drives_final.pdf)"
- "[goal-directed](https://www.alignmentforum.org/s/4dHMdK5TLN6xcqtyc/p/DfcywmqRSkBaCB6Ma)"
- "[ambitious value learning](https://www.alignmentforum.org/s/4dHMdK5TLN6xcqtyc/p/5eX8ko7GCxwR5N9mN)"
- "[AI that is not goal-directed](https://www.alignmentforum.org/s/4dHMdK5TLN6xcqtyc/p/tHxXdAn8Yuiy9y2pZ)"
- "[corrigible](https://www.alignmentforum.org/posts/fkLYhTQteAu5SinAc/corrigibility)"
- "[_trying_ to do what we want](https://www.alignmentforum.org/posts/ZeE7EKHTFMBs8eMxn/clarifying-ai-alignment)"
- "[learn human norms](https://www.alignmentforum.org/s/4dHMdK5TLN6xcqtyc/p/eBd6WvzhuqduCkYv3)"
- [[Reframing Superintelligence: Comprehensive AI Services as General Intelligence]]
- "[how to do what we want in particular domains](https://www.alignmentforum.org/s/4dHMdK5TLN6xcqtyc/p/vX7KirQwHsBaSEdfK)"
- "[learning our instrumental goals and values](https://www.alignmentforum.org/s/4dHMdK5TLN6xcqtyc/p/SvuLhtREMy8wRBzpC)"
- "[human policy](https://www.alignmentforum.org/s/4dHMdK5TLN6xcqtyc/p/eD9T4kiwB6MHpySGE)"
- "[Human-AI Interaction](https://www.alignmentforum.org/s/4dHMdK5TLN6xcqtyc/p/4783ufKpx8xvLMPc6)"
- "[assume perfect information about the human](https://www.alignmentforum.org/s/4dHMdK5TLN6xcqtyc/p/h9DesGT3WT9u2k7Hr)"
