---
type: post
href: https://bounded-regret.ghost.io/thought-experiments-provide-a-third-anchor/
---

What anchors do we have for understanding AIs?
- Current ML systems ([[Future ML Systems will be qualitatively different|which suffers the limitation of "more is different"]])
- Humans ([[Anthropomorphism|which suffers the limitations of anthropomorphising]])
- **"The optimization anchor"** or what happens in the limiting case of "ideal optimizers". E.g.: [[Paperclip Maximizer]]
- Evolution, the economy, complex systems more generally.

The **optimization anchor** is not without faults:
> Neural networks often generalize in "natural" ways, that we can introspect on network representations, and ... training dynamics are smooth and continuous. Researchers focused on the optimization anchor don't entirely ignore these facts, but I think they tend to underemphasize them and are overly pessimistic as a result.

# Thought Experiments

E.g.: _What happens if most of an agent's learning occurs not during gradient descent, but through in-context learning?_
- Maybe GD becomes less important for understanding these behaviors?