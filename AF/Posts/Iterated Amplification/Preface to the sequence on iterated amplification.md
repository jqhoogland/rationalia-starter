---
type: post
_id: HCv2uwgDGf5dyX5y6
title: Preface to the sequence on iterated amplification
slug: preface-to-the-sequence-on-iterated-amplification
href: >-
  https://www.lesswrong.com/posts/HCv2uwgDGf5dyX5y6/preface-to-the-sequence-on-iterated-amplification
synchedAt: '2022-10-08T10:23:31.509Z'
tags:
  - Post
  - Iterated_Amplification_
sequence: Iterated Amplification
author: paulfchristiano
status: todo
---

**Underlying "hopes"**:
-   If you have an overseer who is smarter than the agent you are trying to train, you can safely use that overseer’s judgment as an objective.
-   We can train an RL system using very sparse feedback, so it’s OK if that overseer is very computationally expensive.
-   A team of aligned agents may be smarter than any individual agent, while remaining aligned.

**The idea**:
- Let a team of the smartest agents you've trained so far be the overseer for a slightly smarter, aligned successor. Alignment by induction from an aligned human.

**Outline**
1. What is the problem IA is trying to solve?
2. Supporting motivations & intuitions.
3. The "core": how to implement IA?
4. Making sense of remaining black boxes
5. Miscellaneous
6. FAQ


# Related

- [[Iterated Amplification ]]
- "[ai-alignment.com](https://ai-alignment.com/)"
- "[Benign model-free RL](https://ai-alignment.com/benign-model-free-rl-4aae8c97e385)"
- "[Value Learning](https://www.alignmentforum.org/s/4dHMdK5TLN6xcqtyc)"