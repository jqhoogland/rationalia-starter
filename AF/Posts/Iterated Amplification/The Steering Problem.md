---
type: post
_id: 4iPBctHSeHx8AkS6Z
title: The Steering Problem
slug: the-steering-problem
href: https://www.lesswrong.com/posts/4iPBctHSeHx8AkS6Z/the-steering-problem
synchedAt: '2022-10-08T10:28:17.726Z'
tags:
  - Post
  - AI
  - Outer_Alignment
sequence: Iterated Amplification
author: paulfchristiano
status: todo
---

**The steering problem**: Using black-box access to human-level cognitive abilities, can we write a program that is as useful as a well-motivated human with those abilities?

"Be as useful as possible" is harder to reason about than "predict/plan/reason well."
- Exact specification is hard.
- Imitating humans limits potential capabilities.
- You get what you measure.

**Subdefinitions**:
- "Usefulness": A program **P** is more useful than **H** for X if, for every project using **H** to accomplish X, we can efficiently transform it into a new project which uses **P** to accomplish X. %% Maybe polynomial transformation? %%
- "Well-motivated":
	- Universally: for _any_ human Hugh, if we run our program using Hugh-level black boxes, it should be as useful as Hugh.
	- Intuitively: "motivated to help the user’s project succeed."


# Related

- [[AI]]
- [[Outer Alignment]]
- "[this document](https://docs.google.com/document/d/1_ggFw8KbvW77Z3gCQUDyz3_IrR3pVyFZ2wkuBgMvoVU/edit?usp=sharing)"
- "[Embedded Agency](https://www.alignmentforum.org/s/Rm6oQRJJmhGCcLvxh)"