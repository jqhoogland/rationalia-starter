---
type: post
_id: DfcywmqRSkBaCB6Ma
title: Intuitions about goal-directed behavior
slug: intuitions-about-goal-directed-behavior
href: >-
  https://www.lesswrong.com/posts/DfcywmqRSkBaCB6Ma/intuitions-about-goal-directed-behavior
synchedAt: '2022-09-25T13:04:15.871Z'
tags:
  - Post
  - Value_Learning
  - AI_Risk
  - Goal-Directedness
sequence: Value Learning
author: rohinmshah
status: todo
---

[[Joy in the Merely Real]]
> an agent is considered intelligent if we do not know how to achieve the outcomes it does using the resources that it has
> 

>  I am suggesting as a possibility that the Misspecified Goal argument relies on us incorrectly equating superintelligence with “pursuing a goal” because we use “pursuing a goal” as a default model for anything that can do interesting things, even if that is not the best model to be using.

# Related

- [[Value Learning]]
- [[AI Risk]]
- [[Goal-Directedness]]
- "[comment](https://www.alignmentforum.org/posts/9zpT9dikrrebdq3Jf/will-humans-build-goal-directed-agents#FdQsD6Q78SZQeXa64)"
- "[optimization](https://www.lesswrong.com/posts/D7EcMhL26zFNbJ3ED/optimization)"
- "[minimax algorithm](https://en.wikipedia.org/wiki/Minimax#Minimax_algorithm_with_alternate_moves)"
- "[Risks from Learned Optimization](https://arxiv.org/abs/1906.01820)"
- "[no longer considered AI](https://www.zdnet.com/article/ai-tends-to-lose-its-definition-once-it-becomes-commonplace-sas/)"
- "[mathematics](https://en.wikipedia.org/wiki/Price_equation)"
- "[how they did it](http://mathforum.org/dr.math/faq/faq.calendar.html)"