---
type: post
_id: q2rCMHNXazALgQpGH
title: Conditions for Mesa-Optimization
slug: conditions-for-mesa-optimization
href: >-
  https://www.lesswrong.com/posts/q2rCMHNXazALgQpGH/conditions-for-mesa-optimization
synchedAt: '2022-09-04T19:01:10.391Z'
tags:
  - LessWrong
  - Post
  - Mesa-Optimization
  - AI
  - AI_Risk
sequence: Risks from Learned Optimization
author: evhub
status: todo
---

# Conditions for Mesa-Optimization

%%

"Humans as mesaoptimizers selected for by the evolution base optimizer" misses an important point. Humans aren't actually optimizers. Or, at least, [[Coherence arguments do not entail goal-directed behavior|not optimizers in any useful sense]].

%%

**Optimization Power**
> measured in terms of the number of times the optimizer is able to divide the search space in half—that is, the number of bits of information provided.

**Optimization** = search in policy space

 **Algorithmic range** = the size of the set of algorithms that the base optimizer can find.

**Inductive biases**
- **Simplicity bias** may result from explicit regularization, model architecture, model size.
- **Information funnels** (e.g.: DNA compressing brain structure) may favor compressed policies such as mesa-optimizers.

> The more a base optimizer is biased towards simple solutions, the more it will be incentivized to find a compressed policy such as a mesa-optimizer.

The authors speculate:
> neither a minimal-depth nor minimal-size boolean circuit that solves a problem can be a mesa-optimizer

**Statefulness** "could make it easier for a learned algorithm to implement a complex optimization process."
- It "decreas[es] the implicit penalty on time complexity imposed by enforcing all computations to end when the learned algorithm produces its output"




# Related

- [[Mesa-Optimization]]
- [[AI]]
- [[AI Risk]]
- [[Risks from Learned Optimization]]
- "[Risks from Learned Optimization in Advanced Machine Learning Systems](https://arxiv.org/abs/1906.01820)"
- [[Risks from Learned Optimization— Introduction]]
- "[(9)](https://intelligence.org/learned-optimization#bibliography)"
- "[(10)](https://intelligence.org/learned-optimization#bibliography)"
- "[(11)](https://intelligence.org/learned-optimization#bibliography)"
- "[(12)](https://intelligence.org/learned-optimization#bibliography)"
- "[(13)](https://intelligence.org/learned-optimization#bibliography)"
- "[(14)](https://intelligence.org/learned-optimization#bibliography)"
- "[(15)](https://intelligence.org/learned-optimization#bibliography)"
- "[(16)](https://intelligence.org/learned-optimization#bibliography)"
- "[here](https://www.alignmentforum.org/s/r9tYkB2a8Fp4DN8yB/p/pL56xPoniLvtMDQ4J)"
- "[Glossary](https://intelligence.org/learned-optimization/#glossary)"
- "[Bibliography](https://intelligence.org/learned-optimization/#bibliography)"