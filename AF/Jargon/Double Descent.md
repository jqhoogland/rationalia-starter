---
type: jargon
aliases: 
  - Grokking
---

The phenomenon in which Neural networks achieve perfect generalization past the point of overfitting. The name comes from the fact that the testing error first declines, then rises as the network overfits, then declines again as the network begins to generalize. 

See also:
- [[[AN 77]— Double descent— a unification of statistical theory and modern ML practice]]
- [[Understanding “Deep Double Descent”]]
- [[[AN 129]— Explaining double descent by measuring bias and variance]]
- [Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets](https://arxiv.org/abs/2201.02177)

%%

Could this be related to [Opening the Black Box of Deep Neural Networks via Information](https://arxiv.org/abs/1703.00810)

%%