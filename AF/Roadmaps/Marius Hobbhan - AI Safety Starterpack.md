---
_id: pbiGHk6AjRxdBPoD8
type: post
href: https://forum.effectivealtruism.org/posts/pbiGHk6AjRxdBPoD8/ai-safety-starter-pack
---

# Motivation & Mental Bottlenecks

## Insights

- **AI safety is still in its infancy.**
- **AI safety is multi-disciplinary.**
- **You can contribute relatively quickly.**

## Tips & Tricks

- **Find others & get help**
- **Build habits**
- **Don't overload yourself**
- **Choose your speed**

# Resources

- [[The Alignment Problem (Book)]]
- [x] [[AGI safety from first principles]]
- [ ] [[Alignment Newsletter]]
- [ ] [[AGI Safety Fundamentals]]
- [ ] [[Robert Miles's Channel]]
- [[The AI Alignment Forum]]
- [[AI Safety Support]]
	- [[Getting started independently in AI Safety]]
	- The application is not the applicant
- Podcasts
	- [[80,000 Hours Podcast]]
		- 108 - Chris Olah on working at top AI labs without an undergrad degree
		- 107 - Chris Olah on what the hell is going on inside neural networks
		- 92 - Brian Christian on the alignment problem
		- 90 - Ajeya Cotra on worldview diversification and how big the future could be
		- 62 - Paul Christiano on messaging the future, increasing compute, & how CO2 impacts your brain
		- 44 - Dr Paul Christiano on how OpenAI is developing real solutions to the 'AI alignment problem', and his vision of how humanity will progressively hand over decision-making to AI systems
	- [[AXRP]]: Start with [[12 - AI Existential Risk with Paul Christiano|the episode with Paul Christiano]]
- [[AI Safety Camp]]
- [[ML Safety]]
- Slack channels & other groups (e.g., [[AI Safety Support#^6b970f]])
- Write posts on [[The AI Alignment Forum]], [[LessWrong]], [[The Effective Altruism Forum]]
- For engineers: AI Safety Needs Great Engineers, 47 - Catherine Olsson & Daniel Ziegler on the fast path into high-impact ML engineering roles
- [[80,000 Hours#^e36e15]]
- [[AI Safety Ideas]]

# Funding

- [Open Philanthropy undergraduate scholarship](https://www.openphilanthropy.org/focus/other-areas/undergraduate-scholarship)
- [Open Philanthropy early-career funding](https://www.openphilanthropy.org/focus/other-areas/early-career-funding-individuals-interested-improving-long-term-future)
- [Long-term future fund](https://funds.effectivealtruism.org/funds/far-future)
- [FTX Future Fund](https://ftxfuturefund.org/)
- [Survival and flourishing fund](https://survivalandflourishing.fund/)
- [The Center on Long-Term Risk Fund (CLR Fund)](https://longtermrisk.org/grantmaking/)
- [Future of Life grants](https://futureoflife.org/grant-programs/)
- [Open Philanthropy AI Ph.D. scholarship](https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/the-open-phil-ai-fellowship#:~:text=Fellows%20receive%20a%20%2440%2C000%20stipend,out%20before%20March%2031%2C%202022.)
