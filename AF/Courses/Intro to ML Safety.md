---
href: https://course.mlsafety.org/
status: todo
---


# Background

## Introduction

- [ğŸ¥](https://course.mlsafety.org/#media-popup),Â [ğŸ›ï¸](https://docs.google.com/presentation/d/1vP4s1oxomdg3uU5PiV5EnSaiA6kSNcMxtI3L9wRhubQ/edit?usp=sharing)
- *[Unsolved Problems in ML Safety](https://arxiv.org/abs/2109.13916)*
- [Concrete Problems in AI Safety](https://arxiv.org/abs/1606.06565)
- [Workshop On Safety And Control For Artificial Intelligence](https://www.cmu.edu/safartint/watch.html)

### Deep Learning Review

- [ğŸ¥](https://course.mlsafety.org/#media-popup),Â [ğŸ›](https://docs.google.com/presentation/d/15yMNlkWAL5cuSHHZe1gy2sM8zcN8gHk9iBVzKKvS9zw/edit?usp=sharing),Â [ğŸ“–](https://github.com/centerforaisafety/Intro_to_ML_Safety/blob/master/Deep%20Learning%20Review/main.md),Â [ğŸ“](https://drive.google.com/file/d/1pGSXbv68aHJ-ThLUZzH4D2tzPNaFhVqF/view?usp=sharing),Â [âŒ¨ï¸](https://colab.research.google.com/drive/1AEUEhqVmS4PFl3hPMzs2qPvn38twrQh3?copy)
- *[Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)*
- *[Attention Is All You Need](https://arxiv.org/abs/1706.03762)*
- [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929)
- [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)
- [Decoupled Weight Decay Regularization](https://arxiv.org/abs/1711.05101)
- [Layer Normalization](https://arxiv.org/abs/1607.06450)
- [Gaussian Error Linear Units (GELUs)](https://arxiv.org/abs/1606.08415)
- [Dropout: A Simple Way to Prevent Neural Networks from Overfitting](https://jmlr.org/papers/v15/srivastava14a.html)
- [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/abs/1502.03167)
- [Adam: A Method for Stochastic Optimization](https://arxiv.org/abs/1412.6980)

# Hazard Analysis

- *[What is a Complex System?](https://www.youtube.com/watch?v=vp8v2Udd_PM)*
- *[Emergence (Intermediate)](https://www.youtube.com/watch?v=QItTWZc7hKs)*
- *Introduction to STAMP ([video](https://www.youtube.com/watch?v=_ptmjAbacMk),Â [slides](http://psas.scripts.mit.edu/home/wp-content/uploads/2020/07/STAMP-Tutorial.pdf))*
- [Safe Design](https://risk-engineering.org/safe-design/),Â [Consequence assessment](https://risk-engineering.org/consequence-assessment/),Â [Safety models](https://risk-engineering.org/safety-models/)
- [A Brief History of Generative Models for Power Law and Lognormal Distributions](https://www.stat.berkeley.edu/~aldous/Networks/1089229510.pdf)
- [Empirical examples of power-law CDFs](https://youtu.be/KKYhPPf4FxA?t=345)
- [Log-normal distributions (with comparisons to power laws)](https://youtu.be/KiTAyRZORtQ?t=124)
- [Multiplicative processes produce log normals](https://www.youtube.com/watch?v=yA1pkyanbzw)
- [Multiplicative processes produce power laws](https://youtu.be/B43yhWxdi5I?t=69)
- [Review of properties of power laws](https://www.youtube.com/watch?v=9Hc227Qy91k)
- [Numerous power laws for cities](https://www.youtube.com/watch?v=DsL7jEQXh8I)
- [The Black Swan](https://www.youtube.com/watch?v=caPy0OZmXKs)Â andÂ [Antifragile](https://www.youtube.com/watch?v=-MMLea-_ifw)Â Summaries
- [The Precautionary Principle (skip second half)](https://arxiv.org/abs/1410.5787)
- [Emergence (Basic)](https://www.youtube.com/watch?v=16W7c0mb-rE)
- [Nonlinear Causality](https://www.youtube.com/watch?v=76JRJ90s548)
- [Beyond Normal Accidents and High Reliability Organizations: The Need for an Alternative Approach to Safety in Complex Systems](http://sunnyday.mit.edu/papers/hro.pdf)
- [Shortcomings of the Bow Tie and Other Safety Tools Based on Linear Causality1](http://sunnyday.mit.edu/Bow-tie-final.pdf)
- [Systemantics Appendix](https://drive.google.com/file/d/1avoVTY8L3hpZi9fTxI_1mjjXC5JTz882/view?usp=sharing)
- [How Complex Systems Fail](https://how.complexsystems.fail/)

## Risk Decomposition

- [ğŸ¥](https://course.mlsafety.org/#media-popup),Â [ğŸ›ï¸](https://docs.google.com/presentation/d/1RMZ89VHzVnDhugcrrwvHQnRIw366dMr3JYFC3rkxjL0/edit?usp=sharing),Â [ğŸ“–](https://github.com/centerforaisafety/Intro_to_ML_Safety)
- *[Power laws, Pareto distributions and Zipfâ€™s law](https://arxiv.org/pdf/cond-mat/0412004.pdf)*

### Accident Models

- [ğŸ¥](https://course.mlsafety.org/#media-popup),Â [ğŸ›](https://docs.google.com/presentation/d/1HquuLs0OTVYvuk0QRCG_6aqWhmMEf7sDBFLvRaEAZL4/edit?usp=sharing),Â [ğŸ“–](https://github.com/centerforaisafety/Intro_to_ML_Safety)

## Black Swans

- [ğŸ¥](https://course.mlsafety.org/#media-popup),Â [ğŸ›](https://docs.google.com/presentation/d/1rDWQuwdqFPm1ebqnuM9x_H-2ZYGehj6kSp_5LOi6q5E/edit?usp=sharing),Â [ğŸ“–](https://github.com/centerforaisafety/Intro_to_ML_Safety)

## Review

[Review questions ğŸ“](https://drive.google.com/file/d/17hybWUxiVfdo7qFmvnfvfaLLS9Z43LtX/view?usp=sharing)

# Robustness

## Adversarial Robustness

- [ğŸ¥](https://course.mlsafety.org/#media-popup),Â [ğŸ›](https://docs.google.com/presentation/d/1HzloChC0XElQkCTI181CN6OaYcVNnB5l37sfuANkcq0/edit?usp=sharing),Â [ğŸ“–](https://github.com/centerforaisafety/Intro_to_ML_Safety/blob/master/Adversarial%20Robustness/main.md),Â [âŒ¨ï¸](https://colab.research.google.com/drive/1ezV-jXyPgXDMSo6LqXyRgV_f2ky0cCFH?copy)
- *[Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples](https://arxiv.org/abs/1802.00420)*
- *[Towards Deep Learning Models Resistant to Adversarial Attacks](https://arxiv.org/abs/1706.06083)*
- [Universal Adversarial Triggers for Attacking and Analyzing NLP](https://arxiv.org/abs/1908.07125)
- [Data Augmentation Can Improve Robustness](https://arxiv.org/abs/2111.05328)
- [Adversarial Examples for Evaluating Reading Comprehension Systems](https://arxiv.org/abs/1707.07328)
- [BERT-ATTACK: Adversarial Attack Against BERT Using BERT](https://arxiv.org/abs/2004.09984)Â ([GitHub](https://github.com/chbrian/awesome-adversarial-examples-dl))
- [Gradient-based Adversarial Attacks against Text Transformers](https://arxiv.org/abs/2104.13733)
- [Smooth Adversarial Training](https://arxiv.org/abs/2006.14536)
- [Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks](https://arxiv.org/abs/2003.01690)Â ([website](https://robustbench.github.io/))
- [Certified Adversarial Robustness via Randomized Smoothing](https://arxiv.org/abs/1902.02918)
- [Adversarial Examples Are a Natural Consequence of Test Error in Noise](https://arxiv.org/abs/1901.10513)
- [Using Pre-Training Can Improve Model Robustness and Uncertainty](https://arxiv.org/abs/1901.09960)
- [Motivating the Rules of the Game for Adversarial Example Research](https://arxiv.org/abs/1807.06732)
- [Certified Defenses against Adversarial Examples](https://arxiv.org/abs/1801.09344)
- [Towards Evaluating the Robustness of Neural Networks](https://arxiv.org/abs/1608.04644)

## Long Tails and Distribution Shift

- [ğŸ¥](https://course.mlsafety.org/#media-popup),Â [ğŸ›ï¸](https://docs.google.com/presentation/d/1uW7hNstJAq7_lSyk3yP8yTSjN85itESbDHFRi1F4wiw/edit?usp=sharing),Â [ğŸ“–](https://github.com/centerforaisafety/Intro_to_ML_Safety/blob/master/Black%20Swan%20Robustness/main.md)
- *[The Many Faces of Robustness: A Critical Analysis of Out-of-Distribution Generalization](https://arxiv.org/abs/2006.16241)*
- *[Benchmarking Neural Network Robustness to Common Corruptions and Perturbations](https://arxiv.org/abs/1903.12261)*
- [PixMix: Dreamlike Pictures Comprehensively Improve Safety Measures](https://arxiv.org/abs/2112.05135)
- [WILDS: A Benchmark of in-the-Wild Distribution Shifts](https://arxiv.org/abs/2012.07421)
- [ObjectNet: A large-scale bias-controlled dataset for pushing the limits of object recognition models](https://papers.nips.cc/paper/2019/hash/97af07a14cacba681feacf3012730892-Abstract.html)
- [Adversarial NLI: A New Benchmark for Natural Language Understanding](https://arxiv.org/abs/1910.14599)
- [Natural Adversarial Examples](https://arxiv.org/abs/1907.07174)
- [ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness](https://arxiv.org/abs/1811.12231)

## Review

[Review questions ğŸ“](https://drive.google.com/file/d/1ypEmCfBjDw3e0CS7urfb8pmwj1goAk97/view?usp=sharing)

# Monitoring

## OOD and Malicious Behavior Detection

- [ğŸ¥](https://course.mlsafety.org/#media-popup),Â [ğŸ›ï¸](https://docs.google.com/presentation/d/1WEzSFUbcl1Rp4kQq1K4uONMJHBAUWhCZTzWVHnLcSV8/edit?usp=sharing),Â [ğŸ“–](https://github.com/centerforaisafety/Intro_to_ML_Safety/blob/master/Anomaly%20Detection/main.md),Â [âŒ¨ï¸](https://colab.research.google.com/drive/1oj0HjykyAO9-oC2cJ1KZXPBavEfJ6k14?usp=sharing)
- *[Deep Anomaly Detection with Outlier Exposure](https://arxiv.org/abs/1812.04606)*
- *[A Baseline for Detecting Misclassified and Out-of-Distribution Examples in Neural Networks](https://arxiv.org/abs/1610.02136)*
- [ViM: Out-Of-Distribution with Virtual-logit Matching](https://arxiv.org/abs/2203.10807)
- [VOS: Learning What You Donâ€™t Know by Virtual Outlier Synthesis](https://arxiv.org/abs/2202.01197)
- [Scaling Out-of-Distribution Detection for Real-World Settings](https://arxiv.org/abs/1911.11132)
- [A Simple Unified Framework for Detecting Out-of-Distribution Samples and Adversarial Attacks](https://arxiv.org/abs/1807.03888)

## Interpretable Uncertainty

- [ğŸ¥](https://course.mlsafety.org/#media-popup),Â [ğŸ›](https://docs.google.com/presentation/d/1GHKlv-9UmQdUPracBr09AC2rFZp_GQyJzSQrvSWFjPM/edit?usp=sharing),Â [ğŸ“–](https://github.com/centerforaisafety/Intro_to_ML_Safety/blob/master/Interpretable%20Uncertainty/main.md)
- *[On Calibration of Modern Neural Networks](https://arxiv.org/abs/1706.04599)*
- *[Can You Trust Your Modelâ€™s Uncertainty? Evaluating Predictive Uncertainty Under Dataset Shift](https://arxiv.org/abs/1906.02530)*
- [PixMix: Dreamlike Pictures Comprehensively Improve Safety Measures](https://arxiv.org/abs/2112.05135)
- [Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles](https://arxiv.org/abs/1612.01474)
- [Posterior calibration and exploratory analysis for natural language processing models](https://arxiv.org/abs/1508.05154)
- [Accurate Uncertainties for Deep Learning Using Calibrated Regression](https://arxiv.org/abs/1807.00263)

## Transparency

- [ğŸ¥](https://course.mlsafety.org/#media-popup),Â [ğŸ›](https://docs.google.com/presentation/d/1kxgJw31-rH2sZ7z7T9OnaF9YNg9G5t1I1cU1f9UUUe4/edit?usp=sharing),Â [ğŸ“–](https://github.com/centerforaisafety/Intro_to_ML_Safety)
- *[The Mythos of Model Interpretability](https://arxiv.org/abs/1606.03490)*
- [Sanity Checks for Saliency Maps](https://arxiv.org/abs/1810.03292)
- [Interpretable Explanations of Black Boxes by Meaningful Perturbation](https://arxiv.org/abs/1704.03296)
- [Locating and Editing Factual Knowledge in GPT](https://arxiv.org/abs/2202.05262)
- [Acquisition of Chess Knowledge in AlphaZero](https://arxiv.org/abs/2111.09259)
- [Feature Visualizations](https://distill.pub/2017/feature-visualization/)Â andÂ [OpenAI Microscope](https://microscope.openai.com/)
- [Exemplary Natural Images Explain CNN Activations Better than State-of-the-Art Feature Visualization](https://arxiv.org/abs/2010.12606)
- [Network Dissection: Quantifying Interpretability of Deep Visual Representations](https://arxiv.org/abs/1704.05796)
- [Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead](https://arxiv.org/abs/1811.10154)
- [Convergent Learning: Do different neural networks learn the same representations?](https://arxiv.org/abs/1511.07543)

## Trojans

- [ğŸ¥](https://course.mlsafety.org/#media-popup),Â [ğŸ›](https://docs.google.com/presentation/d/1Nu5WwLQ7CDk_DotP_ET98F2F-2KBp9W8lkAfuCbeZ7E/edit?usp=sharing),Â [ğŸ“–](https://github.com/centerforaisafety/Intro_to_ML_Safety),Â [âŒ¨ï¸](https://colab.research.google.com/drive/1gK943aicv8QrElCBIZPz2CNdtdH1dpA2?copy)
- *[Poisoning and Backdooring Contrastive Learning](https://arxiv.org/abs/2106.09667)*
- *[Universal Litmus Patterns: Revealing Backdoor Attacks in CNNs](https://arxiv.org/abs/1906.10842)*
- *[Neural Cleanse: Identifying and Mitigating Backdoor Attacks in Neural Networks](https://people.cs.uchicago.edu/~ravenben/publications/pdf/backdoor-sp19.pdf)*
- [TrojAI](https://pages.nist.gov/trojai/)
- [Detecting AI Trojans Using Meta Neural Analysis](https://arxiv.org/abs/1910.03137)
- [STRIP: A Defence Against Trojan Attacks on Deep Neural Networks](https://arxiv.org/abs/1902.06531)
- [Targeted Backdoor Attacks on Deep Learning Systems Using Data Poisoning](https://arxiv.org/abs/1712.05526)
- [BadNets: Identifying Vulnerabilities in the Machine Learning Model Supply Chain](https://arxiv.org/abs/1708.06733)

## Detecting and Forecasting Emergent Behavior

- [ğŸ¥](https://course.mlsafety.org/#media-popup),Â [ğŸ›](https://docs.google.com/presentation/d/1MGz_eMPQNm5Ov52IZz2vTuTL9IRoABV2tn_MFVrxlQI/edit?usp=sharing),Â [ğŸ“–](https://github.com/centerforaisafety/Intro_to_ML_Safety)
- *[The Effects of Reward Misspecification: Mapping and Mitigating Misaligned Models](https://arxiv.org/abs/2201.03544)*
- *[The Basic AI Drives](https://selfawaresystems.files.wordpress.com/2008/01/ai_drives_final.pdf)*
- [Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets](https://arxiv.org/abs/2201.02177)
- [Optimal Policies Tend to Seek Power](https://arxiv.org/abs/1912.01683)
- [The Off-Switch Game](https://arxiv.org/abs/1611.08219)
- [Goal Misgeneralization in Deep Reinforcement Learning](https://arxiv.org/abs/2105.14111)

## Review

[Review questions ğŸ“](https://drive.google.com/file/d/1J1PsYxmWLfVD7nyACDV-vEpmTsq_fq8j/view?usp=sharing)

# Alignment

## Honest AI

- [ğŸ¥](https://course.mlsafety.org/#media-popup),Â [ğŸ›](https://docs.google.com/presentation/d/1eVO4-HiPlxkOgySEPBv_H-TKkkZYpC5buySBeo1C6eU/edit?usp=sharing),Â [ğŸ“–](https://github.com/centerforaisafety/Intro_to_ML_Safety)
- [TruthfulQA: Measuring How Models Mimic Human Falsehoods](https://arxiv.org/abs/2109.07958)
- [Truthful AI: Developing and governing AI that does not lie](https://arxiv.org/abs/2110.06674)

## Power-Seeking

ğŸ›

- [Is power-seeking AI an existential risk?](https://arxiv.org/abs/2206.13353)

## Machine Ethics

- [ğŸ¥](https://course.mlsafety.org/#media-popup),Â [ğŸ›](https://docs.google.com/presentation/d/1yibQ-RBSMnejAdEk8iMTTzYyTFmMiRasOLwdvvahZkE/edit?usp=sharing),Â [ğŸ“–](https://github.com/centerforaisafety/Intro_to_ML_Safety),Â [âŒ¨ï¸](https://colab.research.google.com/drive/1WyzvZR9Vd3R1QiJpKnYgnzCTp00xqnGF?usp=sharing)
- *[What Would Jiminy Cricket Do? Towards Agents That Behave Morally](https://arxiv.org/abs/2110.13136)*
- *[Ethics Background (Introduction through â€œAbsolute Rights or Prima Facie Dutiesâ€)](https://www.youtube.com/playlist?list=PLKtXFotbf7fOg7zbQ3565EnpzzKlYaVVI)*
- [Aligning AI With Shared Human Values](https://arxiv.org/abs/2008.02275)
- [Avoiding Side Effects in Complex Environments](https://arxiv.org/abs/2006.06547)
- [Conservative Agency via Attainable Utility Preservation](https://arxiv.org/abs/1902.09725)
- [The Structure of Normative Ethics](https://cpb-us-west-2-juc1ugur1qwqqqo4.stackpathdns.com/campuspress.yale.edu/dist/7/724/files/2016/01/The-Structure-of-Normative-Ethics-2gw2akt.pdf)

# Systemic Safety

## Forecasting

- [ğŸ¥](https://course.mlsafety.org/#media-popup),Â [ğŸ›](https://docs.google.com/presentation/d/1HmbLzwmx4IiqoIlhoZ0uDTdLxCP31O3XM9mhJOlJRYM/edit?usp=sharing),Â [ğŸ“–](https://github.com/centerforaisafety/Intro_to_ML_Safety)
- [Forecasting Future World Events with Neural Networks](https://arxiv.org/abs/2206.15474)
- [On Single Point Forecasts for Fat-Tailed Variables](https://arxiv.org/abs/2007.16096)
- [On the Difference between Binary Prediction and True Exposure With Implications For Forecasting Tournaments and Decision Making Research](https://www.stat.berkeley.edu/~aldous/157/Papers/taleb_tetlock.pdf)
- [Superforecasting â€“ Philip Tetlock](https://youtu.be/pedNak4S9IE?t=440)

## ML for Cyberdefense

- [ğŸ¥](https://course.mlsafety.org/#media-popup),Â [ğŸ›](https://docs.google.com/presentation/d/169h2BwK8U7CvPPYiCMEww3NxvpdKEP-Aes_iE5PS6pA/edit?usp=sharing),Â [ğŸ“–](https://github.com/centerforaisafety/Intro_to_ML_Safety)
- [Asleep at the Keyboard? Assessing the Security of GitHub Copilotâ€™s Code Contributions](https://arxiv.org/pdf/2108.09293.pdf?nylayout=pc)

## Cooperative AI

- [ğŸ¥](https://course.mlsafety.org/#media-popup),Â [ğŸ›](https://docs.google.com/presentation/d/1d5dryLUmW2bqQaCak4PxOPD3gqk2qeeW2ZwAB74ixhQ/edit?usp=sharing),Â [ğŸ“–](https://github.com/centerforaisafety/Intro_to_ML_Safety)
- [Uehiro Lectures 2022](https://www.practicalethics.ox.ac.uk/uehiro-lectures-2022)
- [Open Problems in Cooperative AI](https://arxiv.org/abs/2012.08630)
- [Cooperation, Conflict, and Transformative Artificial Intelligence: A Research Agenda](https://longtermrisk.org/files/Cooperation-Conflict-and-Transformative-Artificial-Intelligence-A-Research-Agenda.pdf)

# Additional X-Risk Discussion

## X-Risk Overview

- [ğŸ¥](https://course.mlsafety.org/#media-popup),Â [ğŸ›](https://docs.google.com/presentation/d/19IuBw7GsO6MEeOQIAbY6imDnWLSLT4Fzk2PAW6xq_gA/edit?usp=sharing),Â [ğŸ“–](https://github.com/centerforaisafety/Intro_to_ML_Safety)
- *[X-Risk Analysis for AI Research](https://arxiv.org/abs/2206.05862)*
- [Can we build AI without losing control over it?](https://www.youtube.com/watch?v=8nt3edWLgIg)
- [What happens when our computers get smarter than we are?](https://www.youtube.com/watch?v=MnT1xgZgkpk)
- [X-Risk Motivations for Safety Research Directions](https://docs.google.com/document/d/1PXwjSbh-g1U1JEXhf55C7YsqD5qKNdBPQGgQ7W0Zm1A/edit?usp=sharing)

## Possible X-Hazards

- [ğŸ¥](https://course.mlsafety.org/#media-popup),Â [ğŸ›](https://docs.google.com/presentation/d/1tp65f22ZhWoKdie6VNrh2nY1dWLoK_0WXSSXzdBYLt4/edit?usp=sharing),Â [ğŸ“–](https://github.com/centerforaisafety/Intro_to_ML_Safety)

## Safety-Capabilities Balance

- [ğŸ¥](https://course.mlsafety.org/#media-popup),Â [ğŸ›](https://docs.google.com/presentation/d/1P2VsZClM6YsK_vYtO66Yt-JeKlCFBABK-4ieZf0F2B4/edit?usp=sharing),Â [ğŸ“–](https://github.com/centerforaisafety/Intro_to_ML_Safety)

## Natural Selection Favors AIs Over Humans

- [ğŸ›](https://course.mlsafety.org/),Â [ğŸ“–](https://github.com/centerforaisafety/Intro_to_ML_Safety)

## Review & Conclusion

- [ğŸ¥](https://course.mlsafety.org/#media-popup),Â [ğŸ›](https://docs.google.com/presentation/d/1EL9ogIdzapL8_tZMMTw0CfhusRmHtnqA9uh3Wcoutj4/edit?usp=sharing),Â [ğŸ“](https://drive.google.com/file/d/1bKAyPeWSz4_jr3vdm2rHKrWu_xe-CNv_/view?usp=sharing)
