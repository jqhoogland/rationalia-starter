---
type: course
href: 
- https://docs.google.com/document/d/1mTm_sT2YQx3mRXQD6J2xD2QJG1c3kHyvX8kQc_IQ0ns/edit
- https://www.agisafetyfundamentals.com/
author: Richard Ngo
---

# Week 0 (optional): Introduction to Machine Learning

## Core Readings

1. [A short introduction to machine learning (Ngo, 2021)](https://www.alignmentforum.org/posts/qE73pqxAZmeACsAdF/a-short-introduction-to-machine-learning) (20 mins)
2. [But what is a neural network? (3Blue1Brown, 2017a)](https://www.youtube.com/watch?v=aircAruvnKk&t=0s) (20 mins)
3. [Gradient descent, how neural networks learn (3Blue1Brown, 2017b)](https://www.youtube.com/watch?v=IHZwWFHWa-w) (20 mins)
4. [Creating a Space Game with OpenAI Codex (OpenAI, 2021)](https://www.youtube.com/watch?v=Zm9B-DvwOgw) (10 mins)
5. [Introduction to reinforcement learning (von Hasselt, 2021)](https://www.youtube.com/watch?v=TCCjZe0y4Qc&list=PLqYmG7hTraZDVH599EItlEWsUOsJbAodm) (from 2:00 to 1:02:10, ending at the beginning of the section titled Inside the Agent: Models) (60 mins)

## Further Readings

1. [What is backpropagation really doing? (3Blue1Brown, 2017c)](https://www.youtube.com/watch?v=Ilg3gGewQ5U) (15 mins)
2. [The spelled-out intro to neural networks and backpropagation: building micrograd (Karpathy, 2022)](https://youtu.be/VMj-3S1tku0) (150 mins)
3. [A (long) peek into reinforcement learning (Weng, 2018)](https://lilianweng.github.io/posts/2018-02-19-rl-overview/) (35 mins)
4. [Machine learning for humans (Maini and Sabri, 2017)](https://medium.com/machine-learning-for-humans/why-machine-learning-matters-6164faf1df12)
5. [Collection of GPT-3 results (Sotala, 2020)](https://twitter.com/xuenay/status/1283312640199196673)
6. [AlphaStar: mastering the real-time strategy game StarCraft II (Vinyals et al., 2019)](https://deepmind.com/blog/article/alphastar-mastering-real-time-strategy-game-starcraft-ii) (20 mins)
7. [Generally capable agents emerge from open-ended play (DeepMind, 2021)](https://deepmind.com/blog/article/generally-capable-agents-emerge-from-open-ended-play) (25 mins)
8. Spinning up deep RL: [part 1](https://spinningup.openai.com/en/latest/spinningup/rl_intro.html) and [part 2](https://spinningup.openai.com/en/latest/spinningup/rl_intro2.html) (OpenAI, 2018) (40 mins)
9. [The unreasonable effectiveness of recurrent neural networks (Karpathy, 2015)](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) (40 mins)
10. [Machine learning glossary (Google, 2017)](https://developers.google.com/machine-learning/glossary)

## Exercises

1. What are the main similarities and differences between the process of fitting a linear regression to some data, and the process of training a neural network on the same data?
2. Explain why the “nonlinearity” in an artificial neuron (e.g. the sigmoid or RELU function) is so important. What would happen if we removed all the nonlinearities in a deep neural network? (Hint: try writing out explicit equations for a neural network with only one hidden layer between the input and output layers, and see what happens if you remove the nonlinearity.)

## Live Session:

1. [Recorded lecture](https://youtu.be/v5c2Ll9129k) and [lecture slides](https://docs.google.com/presentation/d/1vy193pcqe0nmLpTGBwP6O2Nlv7s0pq6oSzo-P-Kw4tM/edit?usp=sharing)
2. [Group exercises](https://docs.google.com/document/d/1ChHiwLCDWpkwNDL77iRBc3D8tydXonJgaK2BvVYI3oE/edit?usp=sharing) and [answers for all exercises](https://docs.google.com/document/d/19AIWZKPqRoKYul2Pj_rF7wnT_3zLpBTebSiR-1ogtYE/edit?usp=sharing)

# Week 1: Artificial General Intelligence

## Core Readings

1. [Four background claims (Soares, 2015)](https://intelligence.org/2015/07/24/four-background-claims/) (15 mins) (note that claims #3 and #4 are covered in more detail in the following two weeks)
2. [AGI safety from first principles (Ngo, 2020)](https://drive.google.com/file/d/1uK7NhdSKprQKZnRjU58X7NLA1auXlWHt/view) (only sections 1 and 2) (20 mins)
3. More is different for AI (Steinhardt, 2022) (only [introduction](https://bounded-regret.ghost.io/more-is-different-for-ai/), [second post](https://bounded-regret.ghost.io/future-ml-systems-will-be-qualitatively-different/), [third post](https://bounded-regret.ghost.io/thought-experiments-provide-a-third-anchor/)) (20 mins)
4. [“Most important century” series summary (Karnofsky, 2021a)](https://www.cold-takes.com/most-important-century/#Summary) (15 mins)
5. [Forecasting transformative AI: the “biological anchors” method in a nutshell (Karnofsky, 2021b)](https://www.cold-takes.com/forecasting-transformative-ai-the-biological-anchors-method-in-a-nutshell/) (30 mins)

## Further Readings

1. [AI: racing towards the brink (Harris and Yudkowsky, 2018)](https://intelligence.org/2018/02/28/sam-harris-and-eliezer-yudkowsky/) (110 mins) ([audio here](https://youtu.be/JuvonhJrzQ0))
3. [General intelligence (Yudkowsky, 2017)](https://arbital.com/p/general_intelligence/) and [The power of intelligence (Yudkowsky, 2007)](https://intelligence.org/2007/07/10/the-power-of-intelligence/) (35 mins)
5. [Three Impacts of Machine Intelligence (Christiano, 2014)](https://www.effectivealtruism.org/articles/three-impacts-of-machine-intelligence-paul-christiano/)
6. [The Bitter Lesson (Sutton, 2019)](http://www.incompleteideas.net/IncIdeas/BitterLesson.html) (15 mins)
8. [Understanding human intelligence through human limitations (Griffiths, 2020)](https://arxiv.org/abs/2009.14050) (40 mins)
10. [AI and compute: how much longer can computing power drive AI progress? (Lohn and Musser, 2022)](https://cset.georgetown.edu/wp-content/uploads/AI-and-Compute-How-Much-Longer-Can-Computing-Power-Drive-Artificial-Intelligence-Progress.pdf) (30 mins)
12. [AI and efficiency (Hernandez and Brown, 2020)](https://openai.com/blog/ai-and-efficiency/) (15 mins)
14. [When will AI exceed human performance? Evidence from AI experts (Grace et al., 2017)](https://arxiv.org/pdf/1705.08807.pdf) (15 mins)
16. [Predictability and surprise in large generative models (Ganguli et al., 2022)](https://dl.acm.org/doi/abs/10.1145/3531146.3533229)

## Exercises

1. As discussed in Ngo (2020), Legg and Hutter define intelligence as “an agent’s ability to achieve goals in a wide range of environments”: a definition of intelligence in terms of the outcomes it leads to. An alternative approach is to define intelligence in terms of the cognitive skills (memory, planning, etc) which intelligent agents used to achieve their desired outcomes. What are the key cognitive skills which should feature in such a definition of intelligence?
2. A crucial feature of AGI is that it will possess cognitive skills which are useful across a range of tasks, rather than just the tasks it was trained to perform. Which cognitive skills did humans evolve because they were useful in our ancestral environments, which have remained useful in our modern environment? Which have become less useful?
3. Some have argued that [“no free lunch” theorems](https://en.wikipedia.org/wiki/No_free_lunch_theorem) make the concept of general intelligence a meaningless one. However, we should interpret the concept of "general intelligence" not as requiring strong performance on all possible problems, but only strong performance on problems which are plausible in a universe like ours. Identify three ways in which the latter category of problems is more restrictive than the former.

## Discussion Prompts

1. Here’s yet another way of defining general intelligence: whatever mental skills humans have that allow us to build technology and civilization (in contrast to other animals). What do you think about this definition?
2. One intuition for how to think about very smart AIs: imagine speeding up human intellectual development by a factor of X. If an AI could do the same quality of research as a top scientist, but 10 or 100 times faster, and with the ability to make thousands of copies, how would you use it?
3. How frequently do humans build technologies where some of the details of why they work aren’t understood by anyone? What, if anything, makes AI different from other domains? Would it be very surprising if we built AGI without understanding very much about how its thinking process works?
4. Consider the following argument: if aliens with far more advanced technology than ours arrived on earth, we’d expect that the outcome of that meeting would be primarily determined by what their goals were, rather than what ours are. This is analogous to AGI in the sense that AGI will be very alien, and very powerful; but it’s disanalogous because we can try to shape the goals of AGIs to make them more compatible with ours. Therefore it’s very important to ensure that we have techniques for effectively doing so. How persuasive do you find this argument?
5. What are the most plausible ways for the hypothesis “we will eventually build AGIs which have transformative impacts on the world” to be false? How likely are they?

# Week 2: Goals and Misalignment

## Core Readings

1. [Specification gaming: the flip side of AI ingenuity (Krakovna et al., 2020)](https://www.deepmind.com/blog/specification-gaming-the-flip-side-of-ai-ingenuity) (15 mins)
2. [Goal misgeneralization in deep reinforcement learning (Langosco et al., 2022)](https://arxiv.org/abs/2105.14111) (ending after section 3.3) (25 mins)
3. Those with less background in reinforcement learning can skip the parts of section 2.1 focused on formal definitions.
4. [Superintelligence, Chapter 7: The superintelligent will (Bostrom, 2014)](https://drive.google.com/file/d/1FVl9W2gW5_8ODYNZJ4nuFg79Z-_xkHkJ/view?usp=sharing) (35 mins)
5. [The alignment problem from a deep learning perspective (Ngo, 2022)](https://www.alignmentforum.org/posts/KbyRPCAsWv5GtfrbG/the-alignment-problem-from-a-deep-learning-perspective#Realistic_training_processes_lead_to_the_development_of_misaligned_goals) (only the section titled Realistic training processes lead to the development of misaligned goals, including phases 1, 2 and 3) (30 mins)

## Further Readings

1. [Optimal policies tend to seek power (Turner et al., 2021)](https://neurips.cc/virtual/2021/poster/28400) (15 mins)
3. [What is inner misalignment? (Leike, 2022)](https://aligned.substack.com/p/inner-alignment) (10 mins)
5. [Clarifying “AI alignment” (Christiano, 2018)](https://ai-alignment.com/clarifying-ai-alignment-cec47cd69dd6) (10 mins)
7. [Is power-seeking AI an existential risk? (Carlsmith, 2021)](https://docs.google.com/document/d/1smaI1lagHHcrhoi6ohdq3TYIZv0eNWWZMPEy8C8byYg/edit#heading=h.lvsab2uecgk4) (only sections 2: Timelines and 3: Incentives) (25 mins)
8. [Risks from Learned Optimization (Hubinger et al., 2019)](https://arxiv.org/abs/1906.01820) ([introduction](https://www.alignmentforum.org/posts/FkgsxrGf3QxhfLWHG/risks-from-learned-optimization-introduction) and section on [the Inner Alignment Problem](https://www.alignmentforum.org/s/r9tYkB2a8Fp4DN8yB/p/pL56xPoniLvtMDQ4J)) (55 mins)
10. [Why alignment could be hard with modern deep learning (Cotra, 2021)](https://www.cold-takes.com/why-ai-alignment-could-be-hard-with-modern-deep-learning/) (25 mins)
12. [The other alignment problem: mesa-optimisers and inner alignment (Miles, 2021)](https://youtu.be/bJLcIBixGj8) (25 mins)
14. [A central AI alignment problem: capabilities generalization, and the sharp left turn (Soares, 2022)](https://www.alignmentforum.org/posts/GNhMPAWcfBCASy8e6/a-central-ai-alignment-problem-capabilities-generalization) (15 mins)

## Exercises

1. Why is it not appropriate to describe the agents discussed by Krakovna et al. as displaying goal misgeneralization? Describe a toy experiment similar to those from Langosco et al. which you expect could demonstrate goal misgeneralization.
2. Goal misgeneralization is most easily described in the context of reinforcement learning, but it could also arise in (self-)supervised learning or imitation learning contexts. Describe what it might look like for a large language model like GPT-3 to have learned a goal which then generalizes in undesirable ways in novel environments. How might you test whether that has happened?
3. [Guez et al. (2019)](https://arxiv.org/abs/1901.03559) devised a test for whether a recurrent network is doing planning: by seeing whether its performance improves when given more time to “think” before it can act. Think of some other test that we could run that would give us evidence about the extent to which a neural network is internally doing planning.

## Discussion Prompts

1. Christiano (2018) defined alignment as follows: “an AI A is aligned with an operator H if A is trying to do what H wants it to do”. Some questions about this:
1. What’s the most natural way to interpret “what the human wants” - what they say, or what they think, or what they would think if they thought about it for much longer?
2. How should we define an AI being aligned to a group of humans, rather than an individual?
3. Did Bostrom miss any important convergent instrumental goals? (His current list: self-preservation, goal-content integrity, cognitive enhancement, technological perfection, resource acquisition.) One way of thinking about this might be to consider which goals humans regularly pursue and why.
4. To what extent are humans inner misaligned with respect to evolution? How can you tell, and what might similar indicators look like in AGIs?
5. Suppose that we want to build a highly intelligent AGI that is myopic, in the sense that it only cares about what happens over the next (short) bounded timeframe. Would such an agent still have convergent instrumental goals? What factors might make it easier or harder to train a myopic AGI than a non-myopic AGI?
6. By some definitions, a chess AI has the goal of winning. When is it useful to describe it that way? What are the key differences between human goals and the “goals” of a chess AI?
1. The same questions, but for corporations and countries instead of chess AIs. Does it matter that these consist of many different people, or can we treat them as agents with goals in a similar way to individual humans?

# Week 3: Threat Models and Types of Solutions

## Core Readings

1. [Intelligence explosion: evidence and import (Muehlhauser and Salamon, 2012)](https://drive.google.com/file/d/1QxMuScnYvyq-XmxYeqBRHKz7cZoOosHr/view?usp=sharing) (only pages 10-15) (15 mins)
2. [What failure looks like (Christiano, 2019)](https://www.alignmentforum.org/posts/HBxe6wdjxK239zajf/what-failure-looks-like) (20 mins)
3. [ML systems will have weird failure modes (Steinhardt, 2022)](https://bounded-regret.ghost.io/ml-systems-will-have-weird-failure-modes-2/) (15 mins)
4. [AGI safety from first principles (Ngo, 2020)](https://drive.google.com/file/d/1uK7NhdSKprQKZnRjU58X7NLA1auXlWHt/view) (only section 5: Control) (15 mins)
5. [AI alignment landscape (Christiano, 2020)](https://forum.effectivealtruism.org/posts/63stBTw3WAW6k45dY/paul-christiano-current-work-in-ai-alignment) (35 mins)

## Further Readings

1. [Risks from Learned Optimisation: Deceptive alignment (Hubinger et al., 2019)](https://www.alignmentforum.org/posts/zthDPAjh9w6Ytbeks/deceptive-alignment) (30 mins)
3. [Unsolved problems in ML safety (Hendrycks et al., 2021)](https://arxiv.org/abs/2109.13916) (50 mins)
5. [Takeoff speeds (Christiano, 2018)](https://sideways-view.com/2018/02/24/takeoff-speeds/) (35 mins)
7. [Yudkowsky Contra Christiano On AI Takeoff Speeds (2022)](https://astralcodexten.substack.com/p/yudkowsky-contra-christiano-on-ai?s=r) (40 mins)
9. [X-risk analysis for AI research (Hendrycks and Mazeika, 2022)](https://arxiv.org/abs/2206.05862)
10. [Clarifying “What failure looks like” (part 1) (Clarke, 2020)](https://www.lesswrong.com/posts/v6Q7T335KCMxujhZu/clarifying-what-failure-looks-like-part-1) (30 mins)
12. [What multipolar failure looks like (Critch, 2021)](https://www.lesswrong.com/posts/LpM3EAakwYdS6aRKf/what-multipolar-failure-looks-like-and-robust-agent-agnostic) (45 mins)
14. [Another outer alignment failure story (Christiano, 2021)](https://www.alignmentforum.org/posts/AyNHoTWWAJ5eb99ji/another-outer-alignment-failure-story) (20 mins)
16. [Modeling the human trajectory (Roodman, 2020)](https://www.openphilanthropy.org/research/modeling-the-human-trajectory/) (35 mins)
18. [Long-term growth as a sequence of exponential modes (Hanson, 2000)](https://mason.gmu.edu/~rhanson/longgrow.pdf) (40 mins)

## Exercises

1. The possibility of deceptive alignment, as discussed by Steinhardt (2022), is an example of goal misgeneralization where a policy learns a goal that generalizes beyond the bounds of any given training episode. What factors might make this type of misgeneralization likely or unlikely?
2. Christiano’s “influence-seeking systems” threat model in What Failure Looks Like is in some ways analogous to profit-seeking companies. What are the most important mechanisms preventing companies from catastrophic misbehavior? Which of those would and wouldn’t apply to influence-seeking AIs?
3. Ask the [OpenAI API](https://openai.com/api/) what steps it would perform to achieve some large-scale goal. Then recursively ask it how it’d perform each of those steps, until it reaches a point where its answers don’t make sense. What’s the hardest task you can find for which the API can not only generate a plan, but also perform each of the steps in that plan?
4. What are the individual tasks involved in machine learning research (or some other type of research important for technological progress)? Identify the parts of the process which have already been automated, the parts of the process which seem like they could plausibly soon be automated, and the parts of the process which seem hardest to automate.

## Discussion Prompts

1. What are the biggest vulnerabilities in human civilisation that might be exploited by misaligned AGIs? To what extent do they depend on the development of other technologies more powerful than those which exist today?
2. Does the distinction between “paying the alignment tax” and “reducing the alignment tax” make sense to you? Give a concrete example of each case. Are there activities which fall into both of these categories, or are ambiguous between them?
3. Most of the readings so far have been framed in the current paradigm of deep learning. Is this reasonable? To what extent are they undermined by the possibility of future paradigm shifts in AI?

# Week 4: Learning From Humans

## Core Readings

1. [Imitation learning lecture: part 1 (Levine, 2021a)](https://youtu.be/kGc8jOy5_zY) (20 mins)
2. Read all four of the following blog posts, plus the full paper for whichever you found most interesting (if you’re undecided, default to the critiques paper):
1. [Deep RL from human preferences: blog post (Christiano et al., 2017)](https://openai.com/blog/deep-reinforcement-learning-from-human-preferences/) (10 mins)
2. [Aligning language models to follow instructions: blog post (Ouyang et al,, 2022)](https://openai.com/blog/instruction-following/) (10 mins)
3. [AI-written critiques help humans notice flaws: blog post (Saunders et al., 2022)](https://openai.com/blog/critiques/) (10 mins)
4. [Red-teaming language models with language models (Perez et al., 2022)](https://www.deepmind.com/publications/red-teaming-language-models-with-language-models) (10 mins)
4. [The easy goal inference problem is still hard (Christiano, 2015)](https://www.alignmentforum.org/s/4dHMdK5TLN6xcqtyc/p/h9DesGT3WT9u2k7Hr) (10 mins)

## Further Readings

1. [Learning to summarize with human feedback: blog post (Stiennon et al., 2020)](https://openai.com/blog/learning-to-summarize-with-human-feedback/) (20 mins)
2. [RL with KL penalties is best seen as Bayesian inference (Korbak and Perez, 2022)](https://www.alignmentforum.org/posts/eoHbneGvqDu25Hasc/rl-with-kl-penalties-is-better-seen-as-bayesian-inference)
3. [Adversarial training for high-stakes reliability (Ziegler et al., 2022)](https://arxiv.org/abs/2205.01663)
4. [Training language models with language feedback (Scheurer et al., 2022)](https://arxiv.org/abs/2204.14146)
5. [Reward-rational (implicit) choice: a unifying formalism for reward learning (Jeon et al., 2020)](https://arxiv.org/abs/2002.04833) (60 mins)
7. [Humans can be assigned any values whatsoever (Armstrong, 2018)](https://www.alignmentforum.org/posts/ANupXf8XfZo2EJxGv/humans-can-be-assigned-any-values-whatsoever) (15 mins)
9. [Learning the preferences of bounded agents (Evans et al., 2015)](https://stuhlmueller.org/papers/preferences-nipsworkshop2015.pdf) (25 mins)
11. [An EPIC way to evaluate reward functions (Gleave et al., 2021)](https://deepmindsafetyresearch.medium.com/an-epic-way-to-evaluate-reward-functions-c2c6d41b61cc) (15 mins) (see also [a recorded presentation](https://slideslive.com/38953511/quantifying-differences-in-reward-functions))
13. [A general language assistant as a laboratory for alignment (Askell et al., 2021)](https://arxiv.org/abs/2112.00861) (sections 1 and 2) (40 mins)
15. [The MineRL BASALT Competition on Learning from Human Feedback (Shah et al. 2021)](https://arxiv.org/abs/2107.01969) (only section 1) (25 mins)
17. [Cooperative inverse reinforcement learning (Hadfield-Menell et al., 2016)](https://arxiv.org/abs/1606.03137) (40 mins)

## Exercises

1. Autoregressive language modeling involves training a network to predict the next word in a sentence, given the previous words. (Since the correct answer for each prediction can be generated automatically from existing training data, this is known as [self-supervised learning](https://amitness.com/2020/05/self-supervised-learning-nlp/), and is the key technique for training cutting-edge language models.) In what ways is this the same as, or different from, behavioral cloning?
2. Imagine using RHLF, as described in the middle three core readings for this week, to train an AI to perform a complex task like building a castle in Minecraft. What sort of problems would you encounter?
3. In the first further reading, Stiennon et al. (2020) note that “​​optimizing our reward model eventually leads to sample quality degradation”. Look at the corresponding graph, and explain why the curves are shaped that way. How could we prevent performance from decreasing so much?
4. If you’re familiar with CIRL (discussed in the last further reading): does it help us address the problems discussed in Christiano (2015)? Why or why not?
5. Read the further reading by Armstrong about how humans can be assigned any values, then explain: why does reward learning ever work in practice?

## Discussion Prompts

1. What are the key similarities and differences between behavioral cloning, RL, and RLHF? What types of human preferences can these techniques most easily learn? What types would be hardest to learn?
2. Should we expect RLHF to be necessary for building AGI (independent of safety concerns)?
3. How might using RLHF lead to misaligned AGIs?

# Week 5: Decomposing Tasks for Outer Alignment

## Core Readings

1. [Summarizing books with human feedback: blog post (Wu et al., 2021)](https://openai.com/blog/summarizing-books/) (5 mins)
2. Factored cognition (Ought, 2019) ([introduction](https://ought.org/research/factored-cognition) and [scalability section](https://ought.org/research/factored-cognition/scalability)) (20 mins)
3. [Supervising strong learners by amplifying weak experts (Christiano et al., 2018)](https://arxiv.org/abs/1810.08575) (35 mins)
4. [AI safety via debate (Irving et al., 2018)](https://arxiv.org/abs/1805.00899) (ending after section 3) (35 mins)
1. Those without a background in complexity theory can skip section 2.2.
6. [Debate update: obfuscated arguments problem (Barnes and Christiano, 2020)](https://www.alignmentforum.org/posts/PJLABqQ962hZEqhdB/debate-update-obfuscated-arguments-problem) (15 mins)

## Further Readings

1. [Humans consulting HCH (Christiano, 2016a)](https://ai-alignment.com/humans-consulting-hch-f893f6051455) and [Strong HCH (Christiano, 2016b)](https://ai-alignment.com/strong-hch-bedb0dc08d4e) (15 mins)
3. [Least-to-most prompting enables complex reasoning in large language models (Zhou et al., 2022)](https://arxiv.org/abs/2205.10625) (35 mins)
5. [Chain of thought imitation with procedure cloning (Yang et al., 2022)](https://arxiv.org/abs/2205.10816)
7. [WebGPT (Nakano et al., 2022)](https://openai.com/blog/webgpt/) and [GopherCite (Menick et al., 2022)](https://www.deepmind.com/publications/gophercite-teaching-language-models-to-support-answers-with-verified-quotes)
9. [Iterated Distillation and Amplification (Cotra, 2018)](https://ai-alignment.com/iterated-distillation-and-amplification-157debfd1616) (20 mins)

## Exercises

1. Identify another mechanism which could be added to the debate protocol and might improve its performance. (It may be helpful to think about ways in which AI debaters are disanalogous to humans.)
2. Think of a complex question which you know a lot about. How would you argue for the dishonest side if doing a debate on that question? How would you rebut that line of argument if you were the honest debater?
3. A complex task like running a factory can be broken down into subtasks in a fairly straightforward way, allowing a large team of workers to perform much better than even an exceptionally talented individual. Describe a task where teams have much less of an advantage over the best individuals. Why doesn’t your task benefit as much from being broken down into subtasks? How might we change that?
4. Read Christiano’s posts on HCH from the further readings. Why might even an ideal implementation of HCH not be aligned? What assumptions could change that?

## Discussion Prompts

1. Wu et al. (2021) use a combination of behavioral cloning and reinforcement learning to train a summarization model; this combination was also used to train AlphaGo and AlphaStar. What are the advantages of this approach over using either technique by itself?
2. Different types of iterated amplification can use different techniques for learning from the amplified training signal. One type, imitative amplification, uses behavioral cloning; we could also use supervised learning or reinforcement learning. How should we expect these to differ?
3. Debate is limited to training agents to answer questions correctly. How important do you expect this limitation to be for training economically competitive agents?

# Week 6: Towards a Principled Understanding of AI Cognition

## Core Readings

1. [Feature visualization (Olah et al, 2017)](https://distill.pub/2017/feature-visualization/) (20 mins)
2. [Zoom In: an introduction to circuits (Olah et al., 2020)](https://distill.pub/2020/circuits/zoom-in/) (35 mins)
3. [Acquisition of chess knowledge in AlphaZero (McGrath et al., 2021)](https://arxiv.org/abs/2111.09259) (only up to the end of section 2.1) (20 mins)
4. [Embedded agents, part 1 (Demski and Garrabrant, 2018)](https://intelligence.org/2018/10/29/embedded-agents/) (15 mins)
5. Read one of the following three blog posts, which give brief descriptions of work on agent foundations.
1. [Logical induction: blog post (Garrabrant et al., 2016)](https://intelligence.org/2016/09/12/new-paper-logical-induction/) (10 mins)
2. Finite factored sets: talk transcript (Garrbarant, 2021) (only sections [2m: the Pearlian paradigm](https://www.alignmentforum.org/posts/N5Jm6Nj4HkNKySA5Z/finite-factored-sets#2m__The_Pearlian_Paradigm) and [2t: we can do better](https://www.alignmentforum.org/posts/N5Jm6Nj4HkNKySA5Z/finite-factored-sets#2t__We_Can_Do_Better)) (10 mins)
3. [Progress on causal influence diagrams: blog post (Everitt et al., 2021)](https://deepmindsafetyresearch.medium.com/progress-on-causal-influence-diagrams-a7a32180b0d1) (15 mins)

## Further Readings

### Related to Interpretability

1. [Mechanistic interpretability, variables, and the importance of interpretable bases (Olah, 2022)](https://www.transformer-circuits.pub/2022/mech-interp-essay/index.html)
2. [Thread: Circuits (Cammarata et al., 2020)](https://distill.pub/2020/circuits/)
4. [A mathematical framework for transformer circuits (Elhage et al., 2021)](https://transformer-circuits.pub/2021/framework/index.html) (90 mins)
6. [Chris Olah’s views on AGI safety (Hubinger, 2019)](https://www.alignmentforum.org/posts/X2i9dQQK3gETCyqh2/chris-olah-s-views-on-agi-safety) (20 mins)
7. [Rewriting a deep generative model (Bau et al., 2020)](https://rewriting.csail.mit.edu/) and [Locating and Editing Factual Associations in GPT (Meng et al., 2022)](https://rome.baulab.info/) (40 mins)
9. Intro to brain-like AGI safety (Byrnes, 2022) ([part 3: two subsystems](https://www.lesswrong.com/s/HzcM2dkCq7fwXBej8/p/hE56gYi5d68uux9oM), [part 6: big picture](https://www.lesswrong.com/s/HzcM2dkCq7fwXBej8/p/qNZSBqLEh4qLRqgWW), [part 7: worked example](https://www.lesswrong.com/s/HzcM2dkCq7fwXBej8/p/zXibERtEWpKuG5XAC))
11. [Compositional explanations of neurons (Mu and Andreas, 2021)](https://arxiv.org/abs/2006.14032)
12. [Interpretability beyond feature attribution: quantitative testing with concept attribution vectors (Kim et al., 2018)](https://arxiv.org/abs/1711.11279) (35 mins)
14. [Eliciting latent knowledge (Christiano et al., 2021)](https://docs.google.com/document/d/1WwsnJQstPq91_Yh-Ch2XRL8H_EpsnjrC1dwZXR37PC8/edit#) (up to the end of the Ontology Identification section on page 38) (60 mins)

### Related to Agent Foundations

10. [MIRI’s approach (Soares, 2015)](https://intelligence.org/2015/07/27/miris-approach/) (25 mins)
12. [Cheating Death in Damascus: blog post (Soares and Levenstein, 2017)](https://intelligence.org/2017/03/18/new-paper-cheating-death-in-damascus/) (10 mins)
14. [Parametric Bounded Löb's Theorem and Robust Cooperation of Bounded Agents (Critch, 2016)](https://arxiv.org/abs/1602.04184) (40 mins)
16. [Infra-bayesianism unwrapped (Shimi, 2021)](https://www.alignmentforum.org/posts/Zi7nmuSmBFbQWgFBa/infra-bayesianism-unwrapped) (45 mins)
18. [Causal inference in statistics: a primer (Pearl et al., 2016)](https://www.datascienceassn.org/sites/default/files/CAUSAL%20INFERENCE%20IN%20STATISTICS.pdf)

## Exercises

1. Given sufficient progress in interpretability, we might be able to supervise not just an agents’ behavior but also its thoughts (i.e. its neural activations). One concern with such proposals is that if we train a network to avoid any particular cognitive trait, that cognition will instead just be distributed across the network in a way that we can’t detect. Describe a toy example of a cognitive trait that we can currently detect automatically. Design an experiment to determine whether, after training to remove that trait, the network has learned to implement an equivalent trait in a less-easily-detectable way.
2. Interpretability work on artificial neural networks is closely related to interpretability work on biological neural networks (aka brains). Describe two ways in which the former is easier than the latter, and two ways in which it’s harder.
3. What are the most important disanalogies between [POMDP](https://en.wikipedia.org/wiki/Partially_observable_Markov_decision_process)s and the real world?

## Discussion Prompts

1. What are the best examples throughout history of scientists discovering mathematical formalisms that allowed them to deeply understand a phenomenon that they were previously confused about? (The easiest examples are from physics; how about others from outside physics?) To what extent should these make us optimistic about agent foundations research developing a better mathematical understanding of intelligence?
2. The third core reading discusses how Chris Olah wants machine learning to be “a field which focuses on deliberate design where understanding models is prioritized and the way that people make progress is through deeply understanding their systems”. How plausible/feasible is this? What might it look like to deeply understand the development of neural networks, apart from the sort of interpretability work discussed this week?
3. Were you surprised by the results and claims in Zoom In? Do you believe the Circuits hypothesis? If true, what are its most important implications? How might it be false?

# Week 7: AI Governance, and Careers in Alignment Research

## Core Readings

1. [Careers in alignment (Ngo, 2022)](https://docs.google.com/document/d/1iFszDulgpu1aZcq_aYFG7Nmcr5zgOhaeSwavOMk1akw/edit?usp=sharing) (30 mins)
2. [AI Governance: Opportunity and Theory of Impact (Dafoe, 2020)](https://forum.effectivealtruism.org/posts/42reWndoTEhFqu6T8/ai-governance-opportunity-and-theory-of-impact) (25 mins)
3. [Cooperation, conflict and transformative AI: sections 1 & 2 (Clifton, 2019)](https://www.alignmentforum.org/s/p947tK8CoBbdpPtyK/p/KMocAf9jnAKc2jXri) (25 mins)
4. [The semiconductor supply chain (Khan, 2021)](https://cset.georgetown.edu/publication/the-semiconductor-supply-chain/) (up to page 15) (15 mins)
5. [The global AI talent tracker (Macro Polo, 2020)](https://macropolo.org/digital-projects/the-global-ai-talent-tracker/) (5 mins)
6. [Sharing powerful AI models (Shevlane, 2022)](https://www.governance.ai/post/sharing-powerful-ai-models) (10 mins)

## Further Readings

### On Strategic AI Governance Considerations

1. [Deciphering China’s AI dream (Ding, 2018)](https://www.fhi.ox.ac.uk/wp-content/uploads/Deciphering_Chinas_AI-Dream.pdf) (95 mins) ([see also his podcast on this topic](https://80000hours.org/podcast/episodes/jeffrey-ding-china-ai-dream/))
3. [Leo Szilard and the danger of nuclear weapons: a case study in risk mitigation (Grace, 2015)](https://intelligence.org/files/SzilardNuclearWeapons.pdf) (60 mins)
5. [AI, the space race, and prestige (Barnhart, 2021)](https://docs.google.com/document/d/1T4FIbV32pHow72pd_aOrGPJuV8YZ8PwP5Mhwjt6ppy4/edit#) (all except section II: Exacerbating Conditions) (40 mins)
7. [The vulnerable world hypothesis (Bostrom, 2019)](https://www.nickbostrom.com/papers/vulnerable.pdf) (ending at the start of the section on ‘Preventive policing’) (60 mins)
9. [Sharing the world with digital minds (Shulman and Bostrom, 2020)](https://nickbostrom.com/papers/digital-minds.pdf) (50 mins)
11. [AI Governance: a research agenda (Dafoe, 2018)](https://www.fhi.ox.ac.uk/wp-content/uploads/GovAI-Agenda.pdf) (120 mins)

### On Useful Approaches to AI Governance

7. [Cooperative AI: machines must learn to find common ground (Dafoe et al., 2021)](https://www.nature.com/articles/d41586-021-01170-0) (15 mins)
9. [Truthful AI: Developing and governing AI that does not lie (Evans et al., 2021)](https://www.alignmentforum.org/posts/aBixCPqSnTsPsTJBQ/truthful-ai-developing-and-governing-ai-that-does-not-lie) (20 mins)
11. [Some AI governance research ideas (Anderljung and Carlier, 2021)](https://docs.google.com/document/d/13LJhP3ksrcEBKxYFG5GkJaC2UoxHKUYAHCRdRlpePEc/edit) (60 mins)
13. [Our AI governance grantmaking so far (Muehlhauser, 2020)](https://www.openphilanthropy.org/blog/ai-governance-grantmaking) (15 mins)
15. [The longtermist AI governance landscape: a basic overview (Clarke, 2022)](https://forum.effectivealtruism.org/posts/ydpo7LcJWhrr2GJrx/the-longtermist-ai-governance-landscape-a-basic-overview) (15 mins)

## Exercises

1. Explain the importance of the ability to make credible commitments for Clifton’s (2019) game-theoretic analysis of cooperation failures.
2. In what ways has humanity’s response to large-scale risks other than AI (e.g. nuclear weapons, pandemics) been better than we would have expected beforehand? In what ways has it been worse? What can we learn from this?

## Discussion Prompts

1. How worried are you about misuse vs structural vs accident risk?
2. Do you expect AGI to be developed by a government or corporation (or something else)? What are the key ways that this difference would affect AI governance?
3. What are the main ways in which technical work could make AI governance easier or harder?
4. What are the biggest ways you expect AI to impact the world in the next 10 years? How will these affect policy responses aimed at the decade after that?
5. How likely do you think it is that we build and deploy many AIs that have net negative conscious experiences (as discussed in Shulman and Bostrom (2020))?
6. It seems important for regulators and policy-makers to have a good technical understanding of AI and its implications. In what cases should people with technical AI backgrounds consider entering these fields?

# Week 8 (four Weeks later): Projects

## Some Project Ideas for Each Category

### Technical Upskilling

1. (For those with no ML experience): Train a neural network on some standard datasets. For help, see the [fast.ai course](https://course.fast.ai/) or the [PyTorch tutorials](https://pytorch.org/tutorials/).
2. (For those with some ML experience): Do some of the exercises from [Jacob Hilton’s deep learning curriculum](https://github.com/jacobhilton/deep_learning_curriculum).
3. (For those with extensive ML and RL experience): Replicate the [TREX paper](https://arxiv.org/abs/1904.06387) (easier) or the [Deep Reinforcement Learning from Human Preferences paper](https://arxiv.org/pdf/1706.03741.pdf) (harder) in a simpler environment (e.g. [cartpole](http://gym.openai.com/envs/CartPole-v1/)). See if you can train the agent to do something in that environment which you can’t write an explicit reward function for.
4. (For those interested in agent foundations research): Do some of Garrabrant’s [fixed point exercises](https://www.lesswrong.com/s/5WF3wmwvxX9TEbFXf/p/mojJ6Hpri8rfzY78b) (on [topology](https://www.lesswrong.com/s/5WF3wmwvxX9TEbFXf/p/svE3S6NKdPYoGepzq), [diagonalization](https://www.lesswrong.com/s/5WF3wmwvxX9TEbFXf/p/FZkLa3GRLW97fpknG), or [iteration](https://www.lesswrong.com/s/5WF3wmwvxX9TEbFXf/p/9a2asxypuNjCmga3p)).
5. (Meta): If you’re considering a career in alignment, put together a career plan, with a particular focus on the most important skills for you to acquire, and how you’ll do that.

### Distilling Understanding

6. Pick a reading you found interesting from the curriculum, and summarize or critique it. (Here are some examples of this being done well for [Iterated Amplification](https://www.lesswrong.com/posts/PT8vSxsusqWuN7JXp/my-understanding-of-paul-christiano-s-iterated-amplification) and [Inner Alignment](https://www.lesswrong.com/s/BRrcuwiF2SYWTQfqA/p/AHhCrJ2KpTjsCSwbt), although we don’t expect projects to be as comprehensive as these.)
7. Pick an exercise from this curriculum which is suitable to be fleshed out into a longer project, and solve it more thoroughly.
8. Make a set of forecasts about the future of AI, in a way that’s concrete enough that you will be able to judge whether you were right or not, and predict what would significantly change your mind.
9. Pick a key underlying belief which would impact your AGI alignment research interests, or whether to research alignment at all. Review the literature around this question, and write up a post giving your overall views on it, and the strongest arguments and evidence on the topic.

## Novel Exploration

10. Do a [black-box investigation](https://www.alignmentforum.org/posts/yGaw4NqRha8hgx5ny/the-case-for-becoming-a-black-box-investigator-of-language) trying to discover an interesting novel property of language models (e.g. [GPT-3](https://openai.com/api/)). Some more specific possibilities:
1. Submit an entry to the [inverse scaling prize](https://www.alignmentforum.org/posts/eqxqgFxymP8hXDTt5/announcing-the-inverse-scaling-prize-usd250k-prize-pool) (e.g. using the different model sizes available via the OpenAI API).
2. Search for alignment failures - cases where the model is capable of doing what you intend, but doesn’t. As one example (discussed in section 7.2 of [this paper](https://arxiv.org/abs/2107.03374)), when the user gives it a prompt containing a subtle bug, the Codex language model may “deliberately” introduce further bugs into the code it writes, in order to match the style of the user prompt.
3. Try to discover a novel capability or property of large language models - e.g. a type of prompting more effective than previous ones (like [this paper discovers](https://arxiv.org/abs/2205.11916)).
12. Read Christiano’s Eliciting Latent Knowledge proposal (the final further reading in week 6), then try producing a proposal for [the contest](https://www.lesswrong.com/posts/QEYWkRoCn4fZxXQAY/prizes-for-elk-proposals).
13. Identify a set of capabilities which would be necessary for an AI to be an existential risk, and produce some tests for which of those capabilities current systems do or don’t have.
See also [this longer list of project ideas](https://docs.google.com/spreadsheets/d/1Co45Bd_rOc2JZ3jqK7kw1tMptcI0J66Y_8khK4u30_w/), and [this list of conceptual research projects](https://www.alignmentforum.org/posts/27AWRKbKyXuzQoaSk/some-conceptual-alignment-research-projects).

# Further Resources

## ML Courses (free online)

- [Fast.ai](https://www.fast.ai/) courses (most recommended)

- [Jacob Hilton’s deep learning curriculum](https://github.com/jacobhilton/deep_learning_curriculum)

- [Google’s machine learning crash course](https://developers.google.com/machine-learning/crash-course/ml-intro)

- [Stanford computer vision course (2017)](http://cs231n.stanford.edu/2017/syllabus.html)

- [NYU deep learning course](https://atcold.github.io/pytorch-Deep-Learning/)

- [Spinning up in deep RL](https://spinningup.openai.com/en/latest/index.html)

- [Delta Academy courses](https://delta-academy.xyz/#courses)

## ML Textbooks (all Free Online Except the first)

- [Grokking deep learning (Trask)](https://www.manning.com/books/grokking-deep-learning)

- [Neural networks and deep learning (Nielsen)](http://neuralnetworksanddeeplearning.com/)

- [Deep Learning (Goodfellow, Bengio and Courville)](https://www.deeplearningbook.org/)

- [Reinforcement learning: an introduction (Sutton and Barto, 2nd edition)](http://incompleteideas.net/book/the-book-2nd.html)

- [Mathematics for machine learning (Deisenroth, Faisal and Ongg)](https://mml-book.github.io/)

- [Notes on contemporary machine learning for physicists (Kaplan)](https://sites.krieger.jhu.edu/jared-kaplan/files/2019/04/ContemporaryMLforPhysicists.pdf)

## AI Safety Resources

- [Annotated Bibliography of Recommended Materials](https://humancompatible.ai/bibliography) - UC Berkeley Center for Human-Compatible AI (CHAI)

- [Alignment forum curated sequences](https://www.alignmentforum.org/library)

- [Alignment newsletter](https://rohinshah.com/alignment-newsletter/) - Rohin Shah

- [AI safety videos](https://www.youtube.com/channel/UCLB7AzTwc6VFZrBsO2ucBMg) - Rob Miles

- [Lots of Links](https://www.aisafetysupport.org/resources/lots-of-links) - AI Safety Support

- [Longer list of further resources](https://www.eacambridge.org/agi-further-resources) (including financial support, career resources, etc)

- [List of readings removed from earlier versions of this curriculum](https://docs.google.com/document/d/1-KeJrYQwN-n_HVnRNORx9_IAQw0-8TzfwQF6RcnC73Q/edit?usp=sharing)

## Other

- [GB Hamming, "You and Your Research" (June 6, 1995)](https://www.youtube.com/watch?v=a1zDuOPkMSw)
