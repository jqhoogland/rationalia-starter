---
href: https://www.agisafetyfundamentals.com/alignment-201-curriculum
---

### **Syllabus**

[Week 1: Further understanding the problem](https://www.agisafetyfundamentals.com/alignment-201-curriculum#tabpanel-blog-post-title-one-7mj4n)

Assumed background:

1.  [The alignment problem from a deep learning perspective (Ngo, 2022)](https://www.alignmentforum.org/posts/KbyRPCAsWv5GtfrbG/what-misalignment-looks-like-as-capabilities-scale#Realistic_training_processes_lead_to_the_development_of_misaligned_goals) (40 mins)
    

Core readings:

1.  [AGI ruin: a list of lethalities (Yudkowsky, 2022)](https://www.alignmentforum.org/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities) (50 mins)
2.  [Where I agree and disagree with Eliezer (Christiano, 2022)](https://www.alignmentforum.org/posts/CoZhXrhpQxpy9xw9y/where-i-agree-and-disagree-with-eliezer) (30 mins)
3.  [Worst-case thinking in AI alignment (Shlegeris, 2021)](https://www.lesswrong.com/posts/yTvBSFrXhZfL8vr5a/worst-case-thinking-in-ai-alignment) (15 mins)
4.  [Optimal policies tend to seek power (Turner et al., 2021)](https://neurips.cc/virtual/2021/poster/28400) (video, 15 mins)
    

Further readings:

1.  [A central AI alignment problem: capabilities generalization, and the sharp left turn (Soares, 2022)](https://www.alignmentforum.org/posts/GNhMPAWcfBCASy8e6/a-central-ai-alignment-problem-capabilities-generalization) (20 mins)
2.  [Does SGD produce deceptive alignment? (Xu, 2020)Does SGD produce deceptive alignment? (Xu, 2020)](https://www.alignmentforum.org/posts/ocWqg2Pf2br4jMmKA/does-sgd-produce-deceptive-alignment) (25 mins)
3.  [Advanced artificial agents intervene in the provision of reward (Cohen et al., 2022)](https://onlinelibrary.wiley.com/doi/full/10.1002/aaai.12064) (45 mins)
4.  [Risks from Learned Optimization (Hubinger et al., 2019)](https://arxiv.org/abs/1906.01820) ([introduction](https://www.alignmentforum.org/posts/FkgsxrGf3QxhfLWHG/risks-from-learned-optimization-introduction) and section on [the Inner Alignment Problem](https://www.alignmentforum.org/s/r9tYkB2a8Fp4DN8yB/p/pL56xPoniLvtMDQ4J)) (55 mins)
5.  [The theory-practice gap (Shlegeris, 2021)](https://www.alignmentforum.org/posts/xRyLxfytmLFZ6qz5s/the-theory-practice-gap) (10 mins)    
6.  [Yudkowsky contra Ngo on agents (Alexander, 2022)](https://astralcodexten.substack.com/p/practically-a-book-review-yudkowsky) (30 mins)
7.  [How do we become confident in the safety of a machine learning system? (Hubinger, 2021)](https://www.alignmentforum.org/posts/FDJnZt8Ks2djouQTZ/how-do-we-become-confident-in-the-safety-of-a-machine) (50 mins)
    

Exercises:

1.  Compare the definition of power given by [Turner et al.](https://arxiv.org/pdf/1912.01683.pdf) with the definition of empowerment from [Klyubin et al.](https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.101.9018&rep=rep1&type=pdf) Which is more relevant to the goals which real-world policies might learn, and why? (For one answer, see Turner et al.’s appendix.)
    

[Week 2: Decomposing tasks for better supervision](https://www.agisafetyfundamentals.com/alignment-201-curriculum#tabpanel-blog-post-title-two-p2w9b)

**Problem:** We won’t be able to supervise the long-term behaviour of models which are more intelligent than us.

**Proposed solution:** Use models to supervise each other.

Assumed background:

1.  [Self-critiquing models for assisting human evaluators: blog post (Saunders et al., 2022)](https://openai.com/blog/critiques/) (10 mins)
    
2.  [Debate update: obfuscated arguments problem (Barnes and Christiano, 2020)](https://www.alignmentforum.org/posts/PJLABqQ962hZEqhdB/debate-update-obfuscated-arguments-problem) (15 mins)
    
    -   Those who haven’t already read this post should read the full Debate paper (from this week’s core readings) first.
        

Core readings:

1.  [Low-stakes alignment (Christiano, 2021)](https://www.alignmentforum.org/posts/TPan9sQFuPP6jgEJo/low-stakes-alignment) (10 mins)
    
2.  [AI safety via debate (Irving et al., 2018)](https://arxiv.org/abs/1805.00899) (60 mins)
    
    -   The first half of this paper was a core reading for the Alignment Fundamentals course; for this curriculum the second half is also assigned.
        
3.  [Humans consulting HCH (Christiano, 2016)](https://ai-alignment.com/humans-consulting-hch-f893f6051455) (5 mins) and [Strong HCH (Christiano, 2016)](https://ai-alignment.com/strong-hch-bedb0dc08d4e) (10 mins)
    
4.  [Least-to-most prompting enables complex reasoning in large language models (Zhou et al., 2022)](https://arxiv.org/abs/2205.10625) (35 mins)
    
    -   This paper provides an example of non-task-specific decomposition steps which allow amplification of a model beyond its baseline capabilities.
        

Further readings:

1.  [Training language models with language feedback (Scheurer et al., 2022)](https://arxiv.org/abs/2204.14146) (30 mins)
    
2.  [RL with KL penalties is better seen as Bayesian inference (Korbak, Perez and Buckley, 2022)](https://arxiv.org/pdf/2205.11275.pdf) (15 mins)
    
3.  [Imitative generalization (Barnes, 2021)](https://alignmentforum.org/posts/JKj5Krff5oKMb8TjT/imitative-generalisation-aka-learning-the-prior-1?_ga=2.92818205.1029020769.1659917223-1478508551.1655741465) (15 mins)
    
4.  [AI safety via market making (Hubinger, 2020)](https://www.alignmentforum.org/posts/YWwzccGbcHMJMpT45/ai-safety-via-market-making) (15 mins)
    

Exercises:

1.  [Korbak, Perez and Buckley](https://arxiv.org/pdf/2205.11275.pdf) claim that “KL-regularised RL is equivalent to variational inference”. Read up to equation 7, then prove the claim in the following paragraph (which they derive in the appendix).
    
2.  Work through some of the exercises in the [factored cognition primer](https://primer.ought.org/).
    

[Week 3: Training on unrestricted adversarial examples](https://www.agisafetyfundamentals.com/alignment-201-curriculum#tabpanel-blog-post-title-four-fssys)

**Problem:** we won’t be able to train models to generalize their goals to new situations in desirable ways; instead, they’ll learn to be [deceptively aligned](https://bounded-regret.ghost.io/ml-systems-will-have-weird-failure-modes-2/).

**Proposed solution:** generate novel inputs which we expect to lead to misaligned behavior, and train models to behave well even on those.

Assumed background:

1.  [Goal misgeneralization in deep reinforcement learning (Langosco et al., 2022)](https://arxiv.org/abs/2105.14111) (35 mins)
    
2.  [ML systems will have weird failure modes (Steinhardt, 2022)](https://bounded-regret.ghost.io/ml-systems-will-have-weird-failure-modes-2/) (15 mins)
    

Core readings:

1.  [Training robust corrigibility (Christiano, 2019)](https://ai-alignment.com/training-robust-corrigibility-ce0e0a3b9b4d) (20 mins)
    
2.  [Constructing unrestricted adversarial examples with generative models (Song et al., 2018)](https://proceedings.neurips.cc/paper/2018/hash/8cea559c47e4fbdb73b23e0223d04e79-Abstract.html) (25 mins)
    
3.  [Red-teaming language models with language models: blog post (Perez et al., 2022)](https://www.deepmind.com/publications/red-teaming-language-models-with-language-models) (5 mins)
    
4.  [The prototypical catastrophic AI action is getting root access to its datacenter (Shlegeris, 2022)](https://www.alignmentforum.org/posts/BAzCGCys4BkzGDCWR/the-prototypical-catastrophic-ai-action-is-getting-root) (5 mins)
    
5.  High-stakes alignment via adversarial training blog posts ([part one](https://www.alignmentforum.org/posts/A9tJFJY7DsGTFKKkh/high-stakes-alignment-via-adversarial-training-redwood), [part two](https://www.alignmentforum.org/posts/n3LAgnHg6ashQK3fF/takeaways-from-our-robust-injury-classifier-project-redwood)) (Ziegler et al., 2022) (25 mins)
    

Further readings:

1.  [Adversarial training for high-stakes reliability (Ziegler et al., 2022)](https://arxiv.org/pdf/2205.01663.pdf) (75 mins)
    
2.  [Goal misgeneralization: why correct specifications aren’t enough for correct goals (Shah et al., 2022)](https://arxiv.org/abs/2210.01790) (65 mins)
    
3.  [Unrestricted adversarial examples via semantic manipulation (Bhattad et al., 2020)](https://arxiv.org/abs/1904.06347) (45 mins)
    
4.  [2-D Robustness (Mikulik, 2019)](https://www.alignmentforum.org/posts/2mhFMgtAjFJesaSYR/2-d-robustness) (5 mins)
    
5.  [Towards deep learning models resistant to adversarial attacks (Madry et al., 2018)](https://openreview.net/forum?id=rJzIBfZAb) (55 mins)
    
6.  [Relaxed adversarial training for inner alignment (Hubinger, 2019)](https://www.alignmentforum.org/posts/9Dy5YRaoCxH9zuJqa/relaxed-adversarial-training-for-inner-alignment) (45 mins)
    
7.  [Adversarial examples for evaluating reading comprehension systems (Jia et al.,2017)](https://arxiv.org/abs/1707.07328) (35 mins)
    

Exercises:

1.  Use [the tool mentioned by Ziegler et al.](https://rr-data.herokuapp.com/talk-to-filtered-transformer) to find a violent completion which is misclassified by their model.
    

[Week 4: Interpretability](https://www.agisafetyfundamentals.com/alignment-201-curriculum#tabpanel-ysce6veotr9zkw9sr0xne31c7zv6s8)

**Problem:** Misaligned models may reason about how to perform in desirable ways during adversarial training (e.g. because we can’t generate compelling unrestricted adversarial inputs).

**Proposed solution:** Leverage interpretability techniques to detect and penalize undesirable reasoning. (One useful framing: how could interpretability tools be used to extend Debate so that debaters can make arguments about each others’ thought processes?)

Assumed background:

1.  [Feature visualization (Olah et al, 2017)](https://distill.pub/2017/feature-visualization/) (20 mins)
    
2.  [Zoom In: an introduction to circuits (Olah et al., 2020)](https://distill.pub/2020/circuits/zoom-in/) (35 mins)
    
3.  [Mechanistic interpretability, variables, and the importance of interpretable bases (Olah, 2022)](https://www.transformer-circuits.pub/2022/mech-interp-essay/index.html) (10 mins)
    

Core readings:

1.  [Toy models of superposition (Elhage et al., 2022)](https://transformer-circuits.pub/2022/toy_model/index.html) (60 mins) (everything up to the beginning of the Superposition and Learning Dynamics section, plus the [Strategic Picture of Superposition](https://transformer-circuits.pub/2022/toy_model/index.html#strategic) section)
    
2.  [Discovering latent knowledge in language models without supervision](https://openreview.net/pdf?id=ETKGuby0hcs) **(only sections 1 and 2)** (15 mins)
    
3.  [Locating and Editing Factual Associations in GPT (Meng et al., 2022)](https://arxiv.org/pdf/2202.05262.pdf) (40 mins) (see also [paper website](https://rome.baulab.info/))
    

Further readings:

1.  [A mathematical framework for transformer circuits (Elhage et al., 2021)](https://transformer-circuits.pub/2021/framework/index.html) (60 mins)
    
2.  [Polysemanticity and capacity in neural networks (Scherlis et al., 2022)](https://arxiv.org/abs/2210.01892) (50 mins)
    
3.  [A mechanistic interpretability analysis of grokking (Nanda, 2022)](https://www.alignmentforum.org/posts/N6WM6hs7RQMKDhYjB/a-mechanistic-interpretability-analysis-of-grokking#Future_Directions) (50 mins)
    
4.  [Convergent Learning: Do different neural networks learn the same representations? (Li et al., 2016)](https://arxiv.org/abs/1511.07543) (40 mins)
    
5.  [Rewriting a deep generative model: blog post (Bau et al., 2020)](https://rewriting.csail.mit.edu/) (10 mins)
    

Exercises:

1.  Train a two layer MLP to do addition mod 97 (input format is a 2*97 dimensional vector of each input one-hot encoded and concatenated together, output is a 97 dimensional vector of logits). Give it 30% of the data as training, and use weight decay of 1. Pick a neuron, and plot it as a heatmap over the two inputs, and see if it looks periodic.
    
    -   _**Bonus:**_ play around with the hyper-parameters and see how dramatically you can get it to grok.
        
    -   _**Bonus:**_ do this for a transformer, replicate Nanda (2022)’s analysis, then apply it to modular subtraction.
        
2.  Replicate the experiment in the Demonstrating Superposition section of Elhage et al. (2022): training a model such that ReLU(W^TWx) is approximately x. Show that as you vary sparsity, the model goes from having one feature per dimension to using superposition.
    
    -   _**Bonus:**_ Pick your favorite other experiment from the paper and try replicating that (e.g. can you get your network to form a tetrahedron?)
        

[Week 5: Reasoning about Reasoning](https://www.agisafetyfundamentals.com/alignment-201-curriculum#tabpanel-04p1gww3saf0jksxhikro8h8aktfwh)

**Problem:** models may very robustly learn to reason in misaligned ways, without us being able to study this in advance (because those reasoning skills will only develop once they reach general intelligence).

The readings for this week focus on some more speculative ideas which may help us understand and deal with this possibility.

Assumed background:

1.  [Superintelligence, Chapter 7: The superintelligent will (Bostrom, 2014)](https://drive.google.com/file/d/1FVl9W2gW5_8ODYNZJ4nuFg79Z-_xkHkJ/view?usp=sharing) (35 mins)
    

Core readings:

1.  [An investigation of model-free planning (Guez et al., 2019)](https://arxiv.org/abs/1901.03559) **(only abstract and introduction)** (5 mins)
    
2.  [Gradient hacking: definitions and examples (Ngo, 2022)](https://www.alignmentforum.org/posts/EeAgytDZbDjRznPMA/gradient-hacking-definitions-and-examples) (10 mins)
    
3.  Heuristic argument report (forthcoming) (35 mins)
    
4.  Intro to brain-like AGI safety (Byrnes, 2022) ([part 3: two subsystems](https://www.lesswrong.com/s/HzcM2dkCq7fwXBej8/p/hE56gYi5d68uux9oM), [part 6: big picture](https://www.lesswrong.com/s/HzcM2dkCq7fwXBej8/p/qNZSBqLEh4qLRqgWW), [part 7: worked example](https://www.lesswrong.com/s/HzcM2dkCq7fwXBej8/p/zXibERtEWpKuG5XAC)) (50 mins)
    

Further readings:

1.  [Towards an integration of deep learning and neuroscience (Marblestone et al., 2016)](https://arxiv.org/pdf/1606.03813.pdf) (125 mins)
    
2.  [The natural abstraction hypothesis](https://www.lesswrong.com/posts/Fut8dtFsBYRz8atFF/the-natural-abstraction-hypothesis-implications-and-evidence) (McDouglas, 2021) (25 mins)
    
3.  [Book review: surfing uncertainty](https://slatestarcodex.com/2017/09/05/book-review-surfing-uncertainty/) (Alexander, 2017) (35 mins)
    

Exercises:

1.  Guez et al. (2019) give three sources of evidence that a network might be doing implicit planning. What other evidence could you imagine getting about whether or not this is happening in a given network?
    

[Weeks 6 & 7 (Track 1): Eliciting Latent Knowledge](https://www.agisafetyfundamentals.com/alignment-201-curriculum#tabpanel-fatnresm4bxjh5lpmxrofzbi6j8gw4)

Core Readings:

1.  Read [Eliciting latent knowledge (Christiano et al., 2021)](https://docs.google.com/document/d/1WwsnJQstPq91_Yh-Ch2XRL8H_EpsnjrC1dwZXR37PC8/edit#) (up to the end of the Ontology Identification section on page 38) (60 mins)
    
2.  Spend some time writing up a proposal for the [ELK competition](https://www.lesswrong.com/posts/QEYWkRoCn4fZxXQAY/prizes-for-elk-proposals#Counterexample_features) (Christiano, 2022)
    
3.  Read [Prizes for ELK proposals](https://www.lesswrong.com/posts/QEYWkRoCn4fZxXQAY/prizes-for-elk-proposals), and try to predict the counterexamples for each strategy (Christiano, 2022) (15 mins)
    
4.  Try to develop an algorithm which solves the problems outlined in the heuristic arguments report.
    

[Weeks 6 & 7 (Track 2): Agent Foundations](https://www.agisafetyfundamentals.com/alignment-201-curriculum#tabpanel-9hhxuf3xe7x5crf0h44nk0ygmpv3ua)

Core Readings:

1.  [AIXI](https://en.wikipedia.org/wiki/AIXI) is an idealized reinforcement learning agent which considers every computable hypothesis, and rules out the ones which don’t predict its actual sensory evidence. One key goal of agent foundations research is to discover an idealized model of bounded cognition which (unlike [AIXI-tl](https://arbital.com/p/aixitl/)) helps us understand the cognition of real-world bounded agents, like humans. What properties would this type of idealized model have which AIXI lacks? Brainstorm some possibilities, then read [Embedded Agency (full version) (Garrabrant and Demski, 2018)](https://www.alignmentforum.org/posts/i3BTagvt3HbPMx6PN/embedded-agency-full-text-version) (80 mins).
    
2.  Some work which tries to understand differences between AIXI and humans:
    
    -   We often can’t calculate the implications of our hypotheses: [Logical induction: blog post (Garrabrant et al., 2016)](https://intelligence.org/2016/09/12/new-paper-logical-induction/) (10 mins)
        
    -   We often have hypotheses only about part of the world, not all of it: [Infra-bayesianism unwrapped (Shimi, 2021)](https://www.alignmentforum.org/posts/Zi7nmuSmBFbQWgFBa/infra-bayesianism-unwrapped) (40 mins)
        
    -   We treat hypotheses as having internal structure, rather than being black boxes: [Finite factored sets: talk transcript (Garrbarant, 2021)](https://www.alignmentforum.org/posts/N5Jm6Nj4HkNKySA5Z/finite-factored-sets) (40 mins)
        

_Do a first-pass read of all of the work linked above, then pick whichever you find most interesting, and try to fully understand that._

Further readings:

1.  [Introduction to infra-bayesianism (Diffractor and Kosoy, 2020)](https://www.lesswrong.com/posts/zB4f7QqKhBHa5b37a/introduction-to-the-infra-bayesianism-sequence) (25 mins)
    
2.  [Selection theorems: a program for understanding agents (Wentworth, 2021)](https://www.alignmentforum.org/posts/G2Lne2Fi7Qra5Lbuf/selection-theorems-a-program-for-understanding-agents) (15 mins)
    

[Weeks 6 & 7 (Track 3): Science of Deep Learning](https://www.agisafetyfundamentals.com/alignment-201-curriculum#tabpanel-gsj03ymbm3sfn6ihl638anoe5ihh0o)

Core readings:

1.  [Chinchilla’s wild implications](https://www.lesswrong.com/posts/6Fpvch8RR29qLEWNH/chinchilla-s-wild-implications) (Nostalgebraist, 2022) (20 mins)
    
2.  [Scaling laws from the data manifold dimension (Sharma and Kaplan, 2022)](https://jmlr.org/papers/v23/20-1111.html) (40 mins)
    
3.  [Understanding deep learning requires rethinking generalization (Zhang et al., 2016)](https://arxiv.org/abs/1611.03530) (35 mins)
    
4.  [Deep double descent: blog post](https://openai.com/blog/deep-double-descent/) (Nakkiran et al., 2019) (5 mins)
    
5.  [Grokking grokking (Millidge, 2022)](https://www.beren.io/2022-01-11-Grokking-Grokking/) (20 mins)
    
6.  [In-context learning and induction heads (Olsson et al., 2022)](https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html) **(only up to the end of Argument 1)** (30 mins)
    
    -   See also the [associated videos](https://transformer-circuits.pub/2021/videos/index.html)
        

Further readings:

1.  [Explaining neural scaling laws (Bahri et al., 2021)](https://arxiv.org/abs/2102.06701) (60 mins)
    
2.  [Omnigrok: grokking beyond algorithmic data (Liu, Michaud and Tegmark, 2022)](https://arxiv.org/abs/2210.01117) (30 mins)
    
3.  [Neural tangent kernel: convergence and generalization in neural networks (Jacot, Gabriel and Hongler, 2020)](https://arxiv.org/abs/1806.07572) (40 mins)
    
4.  [Equivalence between policy gradients and soft q-learning (Shulman et al., 2017)](https://arxiv.org/abs/1704.06440) (35 mins)
    
5.  [The lottery ticket hypothesis](https://arxiv.org/abs/1803.03635) (Frankle and Carbin, 2019) (70 mins)
    
6.  [A Review of Select Results from PDLT (Lin, 2022)](https://www.dropbox.com/s/0fq3u4wjhkeyb48/Review%20of%20PDLT%20v2.pdf) (20 mins), which summarizes conclusions from [Principles of deep learning theory (Roberts and Yaida, 2021)](https://arxiv.org/abs/2106.10165)
    
7.  [Scaling laws for transfer (Hernandez et al., 2021)](https://arxiv.org/abs/2102.01293) (45 mins)
    
8.  [Scaling laws and interpretability of learning from repeated data](https://arxiv.org/abs/2205.10487) (Hernandez et al., 2022) (55 mins)
    
9.  [Training compute-optimal large language models (Hoffman et al., 2022)](https://arxiv.org/abs/2203.15556) (70 mins)
    

Then do one of the following exercises:

1.  Do some of the exercises from [Jacob Hilton’s deep learning curriculum](https://github.com/jacobhilton/deep_learning_curriculum).
    
2.  Submit an entry to [Perez et al.’s (2022) inverse scaling prize](https://www.alignmentforum.org/posts/eqxqgFxymP8hXDTt5/announcing-the-inverse-scaling-prize-usd250k-prize-pool) (e.g. using the different model sizes available via the OpenAI API).
    
3.  Search for alignment failures - cases where the model is _capable_ of doing what you intend, but doesn’t. As one example (discussed in section 7.2 of [Chen et al., 2022](https://arxiv.org/abs/2107.03374)), when the user gives it a prompt containing a subtle bug, the Codex language model may “deliberately” introduce further bugs into the code it writes, in order to match the style of the user prompt.
    
4.  Do general [black-box investigation of a large language model](https://www.alignmentforum.org/posts/yGaw4NqRha8hgx5ny/the-case-for-becoming-a-black-box-investigator-of-language) to figure out what novel properties it might have.
    

[Weeks 8 & 9: Literature Review or Project Proposal](https://www.agisafetyfundamentals.com/alignment-201-curriculum#tabpanel-lbo2h1xbw8c3wi2gltnm37wdj6nqiw)

1.  Pick a topic that was discussed over the last 6 weeks and do a literature review of ideas and existing work that might be relevant.
    
2.  Alternatively, outline a project that you think might be valuable in this domain, in sufficient detail that you’d know how you’d go about doing it (including references to prior literature). Some relevant possibilities:
    
    -   This [list of conceptual alignment projects](https://www.alignmentforum.org/posts/27AWRKbKyXuzQoaSk/some-conceptual-alignment-research-projects)
        
    -   **Entries for** [**one of the competitions listed here**](https://www.cais.ai/competitions)