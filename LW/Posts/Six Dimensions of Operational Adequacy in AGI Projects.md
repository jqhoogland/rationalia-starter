---
_id: keiYkaeoLHoKK4LYA
title: Six Dimensions of Operational Adequacy in AGI Projects
author: Eliezer Yudkowsky
url: null
slug: six-dimensions-of-operational-adequacy-in-agi-projects
type: post
tags:
  - LessWrong
  - Concept
  - Post
  - Security_Mindset
  - Organizational_Culture & Design
  - AI_Governance
  - AI
  - Organizational_Culture_&_Design
href: >-
  https://www.lesswrong.com/posts/keiYkaeoLHoKK4LYA/six-dimensions-of-operational-adequacy-in-agi-projects
sequence: 2022 MIRI Alignment Discussion
chapter: null
synchedAt: '2022-09-01T09:10:39.170Z'
status: todo
---

# Six Dimensions of Operational Adequacy in AGI Projects


# Related

- [[Security Mindset]]
- [[Organizational Culture & Design]]
- [[AI Governance]]
- [[AI]]
- "[security mindset](https://intelligence.org/2017/11/25/security-mindset-ordinary-paranoia/)"
- "[cosmopolitan](https://arbital.com/p/value_cosmopolitan/)"
- "[orthogonal](https://arbital.com/p/orthogonality/)"
- "[convergent](https://arbital.com/p/instrumental_convergence/)"
- "[complex and fragile](https://arbital.com/p/complexity_of_value/)"
- "[as an attempt to keep AI "chained up"](https://books.google.com/books?id=2hIcDgAAQBAJ&pg=PA32&lpg=PA32&dq=Larry+%22that+digital+life+is+the+natural+and+desirable+next+step+in+the+cosmic+evolution+and+that+if+we+let+digital+minds+be+free+rather+than+try+to+stop+or+enslave+them+the+outcome+is+almost+certain+to+be+good%22&source=bl&ots=DIQP9C1EgF&sig=ACfU3U04K3r-b1kQqEvWF71-1Oo4ppsZsw&hl=en&sa=X&ved=2ahUKEwiFrvi6-K3gAhUHwlQKHc83AhgQ6AEwAXoECAkQAQ#v=onepage&q=Larry%20%22that%20digital%20life%20is%20the%20natural%20and%20desirable%20next%20step%20in%20the%20cosmic%20evolution%20and%20that%20if%20we%20let%20digital%20minds%20be%20free%20rather%20than%20try%20to%20stop%20or%20enslave%20them%20the%20outcome%20is%20almost%20certain%20to%20be%20good%22&f=false)"
- "[wanted to *open-source*](https://medium.com/backchannel/how-elon-musk-and-y-combinator-plan-to-stop-computers-from-taking-over-17e0e27dd02a)"
- "[stepped down](https://openai.com/blog/openai-supporters/)"
- "[significantly greater than zero](https://intelligence.org/2017/11/26/security-mindset-and-the-logistic-success-curve/)"
- "[tradeoffs between alignment and capabilities](https://arbital.com/p/aligning_adds_time/)"
- "[most of the real value at stake](https://www.nickbostrom.com/astronomical/waste.html)"
- "[coherent extrapolated volition](https://arbital.com/p/cev/)"
- "[ethicists](https://www.lesswrong.com/posts/SsCQHjqNT3xQAPQ6b/yudkowsky-on-agi-ethics)"
- "[which monkeys should get the tasty banana](https://intelligence.org/2017/11/26/security-mindset-and-the-logistic-success-curve/)"
- "[ordinary paranoia](https://intelligence.org/2017/11/25/security-mindset-ordinary-paranoia/)"
- "[minimal](https://arbital.com/p/minimality_principle/)"
- "[alignment assumptions](https://intelligence.org/2017/11/25/security-mindset-ordinary-paranoia/)"
- "[Graham's Design Paradox](https://intelligence.org/2017/11/26/security-mindset-and-the-logistic-success-curve/)"
- "[more competent civilizational equilibria](https://equilibriabook.com/toc)"
- [[Risks of Astronomical Suffering (S-risks)]]
- "[slipping sideways in reality](https://www.facebook.com/yudkowsky/posts/10154981483669228)"
- "[in a shorter timeframe](https://arbital.com/p/aligning_adds_time/)"
- "[task-directed AGI](https://arbital.com/p/task_agi/)"
- "[unrestricted superintelligence](https://arbital.com/p/Sovereign/)"
- "[logistic success curve](https://intelligence.org/2017/11/26/security-mindset-and-the-logistic-success-curve/)"
- [[Challenges to Christianoâ€™s capability amplification proposal]]