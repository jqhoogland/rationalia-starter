---
_id: X2i9dQQK3gETCyqh2
title: Chris Olah’s views on AGI safety
author: null
url: null
slug: chris-olah-s-views-on-agi-safety
type: post
tags:
  - LessWrong
  - Concept
  - Post
  - AI
  - Transparency_/ Interpretability (ML & AI)
  - OpenAI
  - Transparency_/_Interpretability_(ML_&_AI)
href: >-
  https://www.lesswrong.com/posts/X2i9dQQK3gETCyqh2/chris-olah-s-views-on-agi-safety
sequence: Trust
chapter: null
synchedAt: '2022-09-01T09:32:48.748Z'
status: todo
collection: Best of LessWrong
book: The Engines of Cognition
---

# Chris Olah’s Views on AGI Safety

1.  Train a predictive model on some set of data that you want to understand while using transparency tools to verify that the model isn’t performing any optimization.
2.  Use transparency tools to understand what the model learned about the data and use that understanding to guide human decision-making.


# Related

- [[AI]]
- [[Transparency / Interpretability (ML & AI)]]
- [[OpenAI]]
- "[Activation Atlases](https://distill.pub/2019/activation-atlas/)"
- "[Building Blocks of Interpretability](https://distill.pub/2018/building-blocks/)"
- "[Feature Visualization](https://distill.pub/2017/feature-visualization/)"
- "[DeepDream](https://ai.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html)"
- "[Concrete Problems in AI Safety](https://arxiv.org/abs/1606.06565)"
- "[deceptive alignment](https://www.alignmentforum.org/s/r9tYkB2a8Fp4DN8yB/p/zthDPAjh9w6Ytbeks)"
- "[proxy alignment](https://www.alignmentforum.org/s/r9tYkB2a8Fp4DN8yB/p/pL56xPoniLvtMDQ4J)"
- "[informed oversight](https://ai-alignment.com/informed-oversight-18fcb5d3d1e1)"
- "[relaxed adversarial training](https://www.alignmentforum.org/posts/9Dy5YRaoCxH9zuJqa/relaxed-adversarial-training-for-inner-alignment)"
- [[Gradient hacking (Post)]]
- "[Why Tool AIs Want to Be Agent AIs](https://www.gwern.net/Tool-AI)"
- "[mesa-optimization](https://arxiv.org/abs/1906.01820)"
- "[Reframing Superintelligence: Comprehensive AI Services as General Intelligence](https://www.fhi.ox.ac.uk/wp-content/uploads/Reframing_Superintelligence_FHI-TR-2019-1.1-1.pdf)"
- "[Visualizing Representations: Deep Learning and Human Beings](https://colah.github.io/posts/2015-01-Visualizing-Representations/)"
- "[Artificial Intelligence Augmentation](https://distill.pub/2017/aia/)"
- "[Distill](https://distill.pub/about/)"
- "[cited](https://scholar.google.com/scholar?q=site%3Adistill.pub)"
- "[Circuits Thread](https://distill.pub/2020/circuits/)"