---
_id: 6DuJxY8X45Sco4bS2
title: Seeking Power is Often Convergently Instrumental in MDPs
author: TurnTrout
url: https://arxiv.org/abs/1912.01683
slug: seeking-power-is-often-convergently-instrumental-in-mdps
type: post
tags:
  - LessWrong
  - Concept
  - Post
  - AI
  - Instrumental_Convergence
  - Myopia
href: >-
  https://www.lesswrong.com/posts/6DuJxY8X45Sco4bS2/seeking-power-is-often-convergently-instrumental-in-mdps
sequence: Incentives
chapter: null
synchedAt: '2022-09-01T09:11:25.185Z'
status: todo
collection: Best of LessWrong
book: The Engines of Cognition
---

# Seeking Power is Often Convergently Instrumental in MDPs


# Related

- [[AI]]
- [[Instrumental Convergence]]
- [[Myopia]]
- "[The Basic AI Drives](https://selfawaresystems.files.wordpress.com/2008/01/ai_drives_final.pdf)"
- "[toy models](https://intelligence.org/2015/11/26/new-paper-formalizing-convergent-instrumental-goals/)"
- "[debated by well-known AI researchers](https://www.lesswrong.com/posts/WxW6Gc6f2z3mzmqKs/debate-on-instrumental-convergence-between-lecun-russell)"
- "[*Optimal Policies Tend to Seek Power*](https://arxiv.org/abs/1912.01683)"
- "[*Formalizing Convergent Instrumental Goals*](https://intelligence.org/files/FormalizingConvergentGoals.pdf)"
- "[You can test this out for yourself](https://trinket.io/python/4b35f9be1c)"
- "[*Environmental Structure Can Cause Instrumental Convergence*](https://www.lesswrong.com/posts/b6jJddSvWMdZHJHh3/environmental-structure-can-cause-instrumental-convergence)"
- "[arXiv](https://arxiv.org/abs/1912.01683v6)"
- "[v6 of the paper](https://arxiv.org/abs/1912.01683v6)"
- "[What](https://www.lesswrong.com/s/Rm6oQRJJmhGCcLvxh)"
- "[agents](https://www.lesswrong.com/posts/26eupx3Byc8swRS7f/bottle-caps-aren-t-optimisers)"
- "[Do people even have "values"](https://www.lesswrong.com/posts/GermiEmcS6xuZ2gBh/what-ai-safety-researchers-have-written-about-the-nature-of)"
- "[should we try to get the AI to learn them?](https://www.lesswrong.com/posts/oH8KMnXHnw964QyS6/preface-to-the-sequence-on-value-learning)"
- "[What does it mean](https://www.lesswrong.com/posts/BKM8uQS6QdJPZLqCr/towards-a-mechanistic-understanding-of-corrigibility)"
- "[corrigible](https://arbital.com/p/corrigibility/)"
- "[deceptive](https://www.lesswrong.com/posts/zthDPAjh9w6Ytbeks/deceptive-alignment)"
- [[Chris Olahâ€™s views on AGI safety]]
- "[Goodhart's law](https://www.lesswrong.com/posts/EbFABnst8LsidYs5Y/goodhart-taxonomy)"
- "[I suspect that](https://www.lesswrong.com/s/7CdoznhJaLEKHwvJW/p/w6BtMqKRLxG9bNLMr)"
- "[more benign alternatives](https://www.fhi.ox.ac.uk/wp-content/uploads/Reframing_Superintelligence_FHI-TR-2019-1.1-1.pdf)"
- [[The strategy-stealing assumption]]
- "[noted](https://www.lesswrong.com/posts/jGB7Pd5q8ivBor8Ee/impact-measurement-and-value-neutrality-verification-1)"
- "[mesa optimizers](https://arxiv.org/abs/1906.01820)"
- "[bad for other agents in the environment](https://www.lesswrong.com/s/7CdoznhJaLEKHwvJW/p/w6BtMqKRLxG9bNLMr)"
- "[elriggs](https://www.lesswrong.com/users/elriggs)"