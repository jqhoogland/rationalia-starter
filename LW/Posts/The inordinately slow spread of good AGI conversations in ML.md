---
_id: Rkxj7TFxhbm59AKJh
title: The inordinately slow spread of good AGI conversations in ML
author: Rob Bensinger
url: null
slug: the-inordinately-slow-spread-of-good-agi-conversations-in-ml
type: post
tags:
  - LessWrong
  - Concept
  - Post
  - AI
  - Machine_Learning  (ML)
  - AI_Alignment Fieldbuilding
  - Machine_Learning__(ML)
  - AI_Alignment_Fieldbuilding
href: >-
  https://www.lesswrong.com/posts/Rkxj7TFxhbm59AKJh/the-inordinately-slow-spread-of-good-agi-conversations-in-ml
sequence: 2022 MIRI Alignment Discussion
chapter: null
synchedAt: '2022-09-01T09:08:36.445Z'
status: todo
---

# The Inordinately Slow Spread of Good AGI Conversations in ML


# Related

- [[AI]]
- [[Machine Learning  (ML)]]
- [[AI Alignment Fieldbuilding]]
- "[on Twitter](https://twitter.com/SpencrGreenberg/status/1536760598486196225)"
- "[reply](https://twitter.com/robbensinger/status/1537708636989816832)"
- "[exploratory](https://en.wikipedia.org/wiki/Exploratory_engineering)"
- "[engineering](https://intelligence.org/files/ExploratoryEngineeringAI.pdf)"
- "[Asilomar](https://intelligence.org/files/TheAsilomarConference.pdf)"
- [[Science and Rationality]]
- "[dakka](https://thezvi.wordpress.com/2017/12/02/more-dakka/)"
- [[Moloch's Toolbox (2/2)]]
- "[essay](http://www.huffingtonpost.com/stephen-hawking/artificial-intelligence_b_5174265.html)"
- "[tweeted](http://www.theverge.com/2014/8/3/5965099/elon-musk-compares-artificial-intelligence-to-nukes)"
- "[summoning the demon](http://www.independent.co.uk/life-style/gadgets-and-tech/news/tesla-boss-elon-musk-warns-artificial-intelligence-development-is-summoning-the-demon-9819760.html)"
- "[commented](https://twitter.com/SturnioloSimone/status/1537738980288040960)"
- [[Motivated Stopping and Motivated Continuation]]
- "[semantics](https://twitter.com/robbensinger/status/1515479401034489856)"
- "[Rodney Brooks' writing](https://www.technologyreview.com/2017/10/06/241837/the-seven-deadly-sins-of-ai-predictions/)"
- "[Russell 2014](https://www.edge.org/conversation/the-myth-of-ai#26015)"
- "[Urban 2015](https://waitbutwhy.com/2015/01/artificial-intelligence-revolution-1.html)"
- "[Muehlhauser 2015](https://lukemuehlhauser.com/a-reply-to-wait-but-why-on-machine-superintelligence/)"
- "[Yudkowsky 2016a](https://www.econlib.org/archives/2016/03/so_far_unfriend.html)"
- "[Yudkowsky 2017](https://intelligence.org/2017/11/25/security-mindset-ordinary-paranoia/)"
- "[Piper 2018](https://www.vox.com/future-perfect/2018/12/21/18126576/ai-artificial-intelligence-machine-learning-safety-alignment)"
- [[A central AI alignment problem: capabilities generalization, and the sharp left turn]]
- "[Bostrom 2014a](https://www.amazon.com/Superintelligence-Dangers-Strategies-Nick-Bostrom-dp-0198739834/dp/0198739834/)"
- "[Bostrom 2014b](https://www.youtube.com/watch?v=pywF6ZzsghI)"
- "[Yudkowsky 2016b](https://intelligence.org/2016/12/28/ai-alignment-why-its-hard-and-where-to-start/)"
- "[Soares 2017](https://intelligence.org/2017/04/12/ensuring/)"
- "[Hubinger et al. 2019](https://www.alignmentforum.org/s/r9tYkB2a8Fp4DN8yB)"
- "[Ngo 2020](https://www.lesswrong.com/s/mzgtmmTKKn5MuCzFJ)"
- "[Cotra 2021](https://www.cold-takes.com/why-ai-alignment-could-be-hard-with-modern-deep-learning/)"
- [[AGI Ruin: A List of Lethalities]]
- "[Arbital's listing](https://arbital.com/explore/ai_alignment/)"
- "[Omohundro 2008](https://selfawaresystems.files.wordpress.com/2008/01/ai_drives_final.pdf)"
- "[Yudkowsky 2008](https://intelligence.org/files/AIPosNegFactor.pdf)"
- "[Yudkowsky 2011](https://intelligence.org/files/ComplexValues.pdf)"
- "[Muehlhauser 2013](https://intelligenceexplosion.com/)"
- "[Yudkowsky 2013](https://intelligence.org/2013/05/05/five-theses-two-lemmas-and-a-couple-of-strategic-implications/)"
- "[Armstrong 2014](https://smarterthan.us/)"
- "[Dewey 2014](https://www.danieldewey.net/fast-takeoff-strategies.html)"
- "[Krakovna 2015](https://futureoflife.org/2015/11/30/risks-from-general-artificial-intelligence-without-an-intelligence-explosion/)"
- "[Open Philanthropy 2015](https://www.openphilanthropy.org/research/potential-risks-from-advanced-artificial-intelligence/)"
- "[Russell 2015](https://www.youtube.com/watch?v=GYQrNfSmQ0M)"
- "[Soares 2015a](https://intelligence.org/2015/07/24/four-background-claims/)"
- "[Soares 2015b](https://intelligence.org/files/ValueLearningProblem.pdf)"
- "[Steinhardt 2015](https://jsteinhardt.wordpress.com/2015/06/24/long-term-and-short-term-challenges-to-ensuring-the-safety-of-ai-systems/)"
- "[Alexander 2016](https://www.lesswrong.com/posts/LTtNXM9shNM9AC2mp/superintelligence-faq)"
- "[Amodei et al. 2016](https://arxiv.org/pdf/1606.06565.pdf)"
- "[Open Philanthropy 2016](https://www.openphilanthropy.org/research/potential-risks-from-advanced-artificial-intelligence-the-philanthropic-opportunity/)"
- "[Taylor et. al 2016](https://intelligence.org/2016/07/27/alignment-machine-learning/)"
- "[Taylor 2017](https://intelligence.org/2017/02/28/using-machine-learning/)"
- "[Wiblin 2017](https://80000hours.org/problem-profiles/positively-shaping-artificial-intelligence/)"
- "[Yudkowsky 2017](https://intelligence.org/2017/10/13/fire-alarm/)"
- "[Garrabrant and Demski 2018](https://www.lesswrong.com/posts/i3BTagvt3HbPMx6PN/embedded-agency-full-text-version)"
- "[Harris and Yudkowsky 2018](https://intelligence.org/2018/02/28/sam-harris-and-eliezer-yudkowsky/)"
- [[What failure looks like]]
- "[Christiano 2019b](https://forum.effectivealtruism.org/posts/63stBTw3WAW6k45dY/paul-christiano-current-work-in-ai-alignment)"
- "[Piper 2019](https://www.vox.com/future-perfect/2019/10/26/20932289/ai-stuart-russell-human-compatible)"
- "[Russell 2019](https://www.amazon.com/Human-Compatible-Artificial-Intelligence-Problem-ebook/dp/B07N5J5FTS)"
- "[Shlegeris 2020](https://forum.effectivealtruism.org/posts/Ayu5im98u8FeMWoBZ/my-personal-cruxes-for-working-on-ai-safety)"
- "[Carlsmith 2021](https://docs.google.com/document/d/1smaI1lagHHcrhoi6ohdq3TYIZv0eNWWZMPEy8C8byYg/edit)"
- "[Dewey 2021](https://www.danieldewey.net/risk/case.html)"
- "[Miles 2021](https://www.youtube.com/watch?v=pYXy-A4siMw)"
- "[Turner 2021](https://www.lesswrong.com/s/fSMbebQyR4wheRrvk/p/hzeLSQ9nwDkPc4KNt)"
- "[Steinhardt 2022](https://bounded-regret.ghost.io/more-is-different-for-ai/)"
- "[*Creating Friendly AI 1.0*](https://intelligence.org/files/CFAI.pdf)"