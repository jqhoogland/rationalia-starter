---
_id: uMQ3cqWDPHhjtiesc
title: 'AGI Ruin: A List of Lethalities'
author: Eliezer Yudkowsky
url: null
slug: agi-ruin-a-list-of-lethalities
type: post
tags:
  - LessWrong
  - Concept
  - Post
  - AI_Risk
  - AI
href: >-
  https://www.lesswrong.com/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities
sequence: 2022 MIRI Alignment Discussion
chapter: null
synchedAt: '2022-09-01T09:35:15.213Z'
status: todo
---

# AGI Ruin: A List of Lethalities


# Related

- [[AI Risk]]
- [[AI]]
- "[Section B](#Section_B_)"
- "[dignified](https://www.lesswrong.com/posts/j9Q8bRmwCgXRYAgcJ/miri-announces-new-death-with-dignity-strategy)"
- "[orthogonality](https://arbital.com/p/orthogonality/)"
- "[instrumental convergence](https://arbital.com/p/instrumental_convergence/)"
- "[if you can get a powerful AGI that carries out some pivotal superhuman engineering task, with a less than fifty percent change of killing more than one billion people](https://twitter.com/ESYudkowsky/status/1070095112791715846)"
- "[navigate its way out of the causal systems that are humans](https://www.yudkowsky.net/singularity/aibox)"
- "[low-entropy high-structure environments](https://intelligence.org/2017/12/06/chollet/)"
- "[tried and failed](https://www.alignmentforum.org/posts/5bd75cc58225bf0670374f04/forum-digest-corrigibility-utility-indifference-and-related-control-ideas)"
- "[**null string**](https://twitter.com/ESYudkowsky/status/1500863629490544645)"
- "[security mindset](https://intelligence.org/2017/11/25/security-mindset-ordinary-paranoia/)"
- "[modest](https://equilibriabook.com/toc/)"