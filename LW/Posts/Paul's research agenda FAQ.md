---
_id: Djs38EWYZG8o7JMWY
title: Paul's research agenda FAQ
author: zhukeepa
url: null
slug: paul-s-research-agenda-faq
type: post
tags:
  - LessWrong
  - Concept
  - Post
  - Research_Agendas
  - 'Iterated_Amplification '
  - Humans_Consulting HCH
  - Q&A_(format)
  - Iterated_Amplification_
  - Humans_Consulting_HCH
href: https://www.lesswrong.com/posts/Djs38EWYZG8o7JMWY/paul-s-research-agenda-faq
sequence: Alignment
chapter: null
synchedAt: '2022-09-01T09:25:44.661Z'
status: todo
collection: Best of LessWrong
book: A Map That Reflects the Territory
---

# Paul's Research Agenda FAQ


# Related

- [[Research Agendas]]
- [[Iterated Amplification ]]
- [[Humans Consulting HCH]]
- [[Q&A (format)]]
- "[Ajeya Cotra’s summary](https://ai-alignment.com/iterated-distillation-and-amplification-157debfd1616)"
- [[Challenges to Christiano’s capability amplification proposal]]
- "[here](https://agentfoundations.org/item?id=1534)"
- "[here](https://agentfoundations.org/item?id=1129)"
- "[posts](https://www.lesswrong.com/posts/o22kP33tumooBtia3/can-corrigibility-be-learned-safely)"
- "[and](https://www.lesswrong.com/posts/yxzrKb2vFXRkwndQ4/understanding-iterated-distillation-and-amplification-claims)"
- "[discussions](https://www.lesswrong.com/posts/LbJHizyfAsDYeETBq/amplification-discussion-notes)"
- "[on](https://www.lesswrong.com/posts/mSYR46GZZPMmX7q93/corrigible-but-misaligned-a-superintelligent-messiah)"
- "[LessWrong](https://www.lesswrong.com/posts/CCgvJHpbvc7Lm8ZS8/metaphilosophical-competence-can-t-be-disentangled-from)"
- "[Wei Dai’s comments on Paul’s blog](https://medium.com/@weidai)"
- "[ai-alignment.com](http://ai-alignment.com)"
- "[Clarifying "AI Alignment](https://ai-alignment.com/clarifying-ai-alignment-cec47cd69dd6)"
- "[Directions and desiderata for AI alignment](https://ai-alignment.com/directions-and-desiderata-for-ai-control-b60fca0da8f4)"
- "[Clarifying "AI alignment"](https://ai-alignment.com/clarifying-ai-alignment-cec47cd69dd6)"
- "[Approval-directed agents](https://ai-alignment.com/model-free-decisions-6e6609f5d99e)"
- "[Act-based agents](https://ai-alignment.com/act-based-agents-8ec926c79e9c)"
- "[The informed oversight problem](https://ai-alignment.com/the-informed-oversight-problem-1b51b4f66b35)"
- "[Learning representations](https://ai-alignment.com/learning-representations-c330b7d12c76)"
- "[Approval-maximising representations](https://ai-alignment.com/approval-maximizing-representations-56ee6a6a1fe6)"
- "[Capability amplification](https://ai-alignment.com/policy-amplification-6a70cbee4f34)"
- "[Reliability amplification](https://ai-alignment.com/reliability-amplification-a96efa115687)"
- "[Security amplification](https://ai-alignment.com/security-amplification-f4931419f903)"
- "[Universality and security amplification](https://ai-alignment.com/universality-and-security-amplification-551b314a3bab)"
- "[Two guarantees](https://ai-alignment.com/two-guarantees-c4c03a6b434f)"
- "[RL+Imitation](https://ai-alignment.com/imitation-rl-613d70146409)"
- "[Benign model-free RL](https://ai-alignment.com/benign-model-free-rl-4aae8c97e385)"
- "[Semi-supervised reinforcement learning](https://ai-alignment.com/semi-supervised-reinforcement-learning-cf7d5375197f)"
- "[Techniques for optimisizing worst-case performance](https://ai-alignment.com/techniques-for-optimizing-worst-case-performance-39eafec74b99)"
- "[The reward engineering problem](https://ai-alignment.com/the-reward-engineering-problem-30285c779450#.pmofowr9x)"
- "[Improbable Oversight, An Attempt at Informed Oversight](https://william-r-s.github.io/improbable-oversight.html)"
- "[daemon](https://arbital.com/p/daemons/)"
- "[Corrigibility](https://ai-alignment.com/corrigibility-3039e668638)"
- "[Automated assistants](https://ai-alignment.com/safe-automated-assistants-aa69edd46a57)"
- "[Techniques for optimizing worst-case performance](https://ai-alignment.com/techniques-for-optimizing-worst-case-performance-39eafec74b99)"
- "[Humans consulting HCH](https://ai-alignment.com/humans-consulting-hch-f893f6051455)"
- "[Strong HCH](https://ai-alignment.com/strong-hch-bedb0dc08d4e)"
- "[Of humans and universality thresholds](https://ai-alignment.com/of-humans-and-universality-thresholds-24b473e0c898)"
- "[approach to AI safety via debate](https://blog.openai.com/debate/)"
- "[comment](https://www.lesswrong.com/posts/o22kP33tumooBtia3/can-corrigibility-be-learned-safely#XajT7y4n9ubsK2cwt)"
- "[response](https://medium.com/@paulfchristiano/the-intention-is-to-completely-re-implement-translation-though-1-the-hope-is-to-accomplish-it-508733d98b77)"
- "[Ought](https://ought.org/)"
- "[Understanding Iterated Distillation and Amplification: Claims and Oversight](https://www.lesswrong.com/posts/yxzrKb2vFXRkwndQ4/understanding-iterated-distillation-and-amplification-claims)"