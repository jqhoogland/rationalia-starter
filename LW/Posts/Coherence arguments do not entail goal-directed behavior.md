---
_id: NxF5G6CJiof6cemTw
title: Coherence arguments do not entail goal-directed behavior
author: rohinmshah
url: null
slug: coherence-arguments-do-not-entail-goal-directed-behavior
type: post
tags:
  - LessWrong
  - Concept
  - Post
  - AI
  - Decision_Theory
  - Utility_Functions
  - Value_Learning
  - Goal-Directedness
  - Coherence_Arguments
href: >-
  https://www.lesswrong.com/posts/NxF5G6CJiof6cemTw/coherence-arguments-do-not-entail-goal-directed-behavior
sequence: Alignment
chapter: null
synchedAt: '2022-09-01T09:32:42.969Z'
status: todo
collection: Best of LessWrong
book: A Map That Reflects the Territory
---

# Coherence Arguments Do Not Entail Goal-directed Behavior

### ==Why model superintelligent agents as EU maximizers?==
1.  By hypothesis, we will have superintelligent agents.
2.  A superintelligent agent will follow principles of rationality, and thus will satisfy the VNM axioms.
3.  Therefore it can be modeled as an EU maximizer.
4.  Therefore it pursues convergent instrumental subgoals and kill us all.

### Maximization over history vs. state
It's not necessarily informative to describe behavior as EU maximization.
- You can always conjure a utility function that is maximized for exactly the actions chosen by that agent.

> What then is the purpose of the VNM theorem? It tells you how to behave _if you have probabilistic beliefs about the world_, as well as a _complete and consistent preference ordering over outcomes_

This is uninteresting when "outcomes" = "universe-histories". It's more interesting when "outcomes" = "world states/snapshots".

### Coherence does not imply goal-directedness

Examples that can be modeled as EU maximizers (but aren't goal-directed):
-   A robot that constantly twitches
-   The agent that always chooses the action that starts with the letter “A”
-   The agent that follows the policy \<policy\> where for every history the corresponding action in \<policy\> is generated randomly.

### Coherence does not require preferences
### Other
- Instrumental convergence requires the assumption of goal-directedness.
- Goodhart's Law is about goal-directedness.
- Wireheading is about explicit reward maximization.
- 

# Related

- [[AI]]
- [[Decision Theory]]
- [[Utility Functions]]
- [[AF/Concepts/Engineering Alignment/Value Learning]]
- [[Goal-Directedness]]
- [[Coherence Arguments]]
- "[here](https://arbital.com/p/expected_utility_formalism/?l=7hh)"
- "[powerful agents appear coherent to us](https://arbital.com/p/optimized_agent_appears_coherent/)"
- "[goal-directed behavior](https://www.alignmentforum.org/s/4dHMdK5TLN6xcqtyc/p/DfcywmqRSkBaCB6Ma)"
- "[these](https://www.lesswrong.com/posts/DkcdXsP56g9kXyBdq/coherence-arguments-imply-a-force-for-goal-directed-behavior?commentId=LvmHoLEhKLJzBrgrZ)"
- "[comments](https://www.alignmentforum.org/posts/vphFJzK3mWA4PJKAg/coherent-behaviour-in-the-real-world-is-an-incoherent#Gu4syyKBsRSQkktkJ)"
- "[this paper](https://arxiv.org/abs/1811.07871)"
- "[goal-directed](https://www.alignmentforum.org/s/4dHMdK5TLN6xcqtyc/p/DfcywmqRSkBaCB6Ma)"
- "[The Basic AI Drives](https://selfawaresystems.files.wordpress.com/2008/01/ai_drives_final.pdf)"
- [[Value is Fragile]]
- "[Goodhart’s Law](https://en.wikipedia.org/wiki/Goodhart%27s_law)"
- "[a](https://intelligence.org/files/LearningValue.pdf)"
- "[few](http://people.idsia.ch/~ring/AGI-2011/Paper-B.pdf)"
- "[papers](https://arxiv.org/abs/1605.03142)"
- "[Learning What to Value](https://intelligence.org/files/LearningValue.pdf)"
- [# What do coherence arguments imply about the behavior of advanced AI?](https://aiimpacts.org/what-do-coherence-arguments-imply-about-the-behavior-of-advanced-ai/)