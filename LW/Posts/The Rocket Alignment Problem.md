---
_id: Gg9a4y8reWKtLe3Tn
title: The Rocket Alignment Problem
author: Eliezer_Yudkowsky
url: null
slug: the-rocket-alignment-problem
type: post
tags:
  - LessWrong
  - Concept
  - Post
  - AI
  - Fiction
  - Machine_Intelligence Research Institute (MIRI)
  - Dialogue_(format)
  - Agent_Foundations
  - Machine_Intelligence_Research_Institute_(MIRI)
href: https://www.lesswrong.com/posts/Gg9a4y8reWKtLe3Tn/the-rocket-alignment-problem
sequence: Alignment
chapter: null
synchedAt: '2022-09-01T09:09:04.262Z'
status: todo
collection: Best of LessWrong
book: A Map That Reflects the Territory
---

# The Rocket Alignment Problem


# Related

- [[AI]]
- [[Fiction]]
- [[Machine Intelligence Research Institute (MIRI)]]
- [[Dialogue (format)]]
- [[Agent Foundations]]
- "[AI Alignment: Why It’s Hard, and Where to Start](https://intelligence.org/2016/12/28/ai-alignment-why-its-hard-and-where-to-start/)"
- "[might not be a very straight path](https://airandspace.si.edu/webimages/highres/5317h.jpg)"
- "[tiling positions](https://intelligence.org/files/TilingAgentsDraft.pdf)"
- "[how to fire a cannonball from a cannon](https://en.wikipedia.org/wiki/Newton%27s_cannonball)"
- "[logical undiscreteness](https://intelligence.org/files/QuestionsLogicalUncertainty.pdf)"
- "[Cromwell’s Rule](https://en.wikipedia.org/wiki/Cromwell%27s_rule)"
- "[assumptions](https://intelligence.org/2017/11/25/security-mindset-ordinary-paranoia/)"
- "[standard steering fins may stop working once the rocket gets high enough](https://intelligence.org/files/Corrigibility.pdf)"