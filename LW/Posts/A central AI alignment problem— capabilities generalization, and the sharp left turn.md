---
_id: GNhMPAWcfBCASy8e6
title: >-
  A central AI alignment problem: capabilities generalization, and the sharp
  left turn
author: So8res
url: null
slug: a-central-ai-alignment-problem-capabilities-generalization
type: post
tags:
  - LessWrong
  - Concept
  - Post
  - AI
  - Threat_Models
href: >-
  https://www.lesswrong.com/posts/GNhMPAWcfBCASy8e6/a-central-ai-alignment-problem-capabilities-generalization
sequence: 2022 MIRI Alignment Discussion
chapter: null
synchedAt: '2022-09-01T09:35:16.180Z'
status: todo
---

# A Central AI Alignment Problem: Capabilities Generalization, and the Sharp Left Turn


# Related

- [[AI]]
- [[Threat Models]]
- "[strawberry problem](https://www.lesswrong.com/posts/SsCQHjqNT3xQAPQ6b/yudkowsky-on-agi-ethics)"
- [[Sorting Pebbles Into Correct Heaps]]
- [[On how various plans miss the hard bits of the alignment challenge]]
- "[follow Eliezer](https://www.lesswrong.com/posts/mmXEk675etTKpkgTx/agi-ruin-a-poorly-organized-list-of-lethalities)"