---
_id: GNhMPAWcfBCASy8e6
title: >-
  A central AI alignment problem: capabilities generalization, and the sharp
  left turn
author: So8res
url: null
slug: a-central-ai-alignment-problem-capabilities-generalization
type: post
tags:
  - LessWrong
  - Concept
  - Post
  - AI
  - Threat_Models
href: >-
  https://www.lesswrong.com/posts/GNhMPAWcfBCASy8e6/a-central-ai-alignment-problem-capabilities-generalization
sequence: 2022 MIRI Alignment Discussion
chapter: null
synchedAt: '2022-09-01T09:35:16.180Z'
status: todo
---

# A Central AI Alignment Problem: Capabilities Generalization, and the Sharp Left Turn
> Some sub-problems look harder to me than others. For instance, people are still regularly surprised when I tell them that I think the hard bits are much more technical than moral: it looks to me like figuring out how to aim an AGI at all is harder than figuring out where to aim it.[\[1\]](https://www.alignmentforum.org/posts/GNhMPAWcfBCASy8e6/a-central-ai-alignment-problem-capabilities-generalization#fn895zjba6xtk)

> What undermines my hope is that nobody seems to be working on the hard bits, and I don't currently expect most people to become convinced that they need to solve those hard bits until it's too late.

**The Problems**
- Capabilities generalize better than alignment.

> The central analogy here is that optimizing apes for inclusive genetic fitness (IGF) doesn't make the resulting humans optimize mentally for IGF.

- Sudden, discontinuous jumps — a "sharp left turn" — are likely. 

> In the one real example of intelligence being developed we have to look at, continuous application of natural selection in fact found _Homo sapiens sapiens_, and the capability-gain curves of the ecosystem for various measurables were in fact sharply kinked by this new species (e.g., using machines, we sharply outperform other animals on well-established metrics such as “airspeed”, “altitude”, and “cargo carrying capacity”).

This is because:
- "[G]ood capabilities form something like an attractor well"
- That attractor well has a free parameter — the optimization target. 


- Downstream challenges include:
	1.  Directing a capable AGI towards an objective of your choosing.
	2.  Ensuring that the AGI is low-impact, conservative, shutdownable, and otherwise corrigible.

# Related

- [[AI]]
- [[Threat Models]]
- "[strawberry problem](https://www.lesswrong.com/posts/SsCQHjqNT3xQAPQ6b/yudkowsky-on-agi-ethics)"
- [[Sorting Pebbles Into Correct Heaps]]
- [[On how various plans miss the hard bits of the alignment challenge]]
- "[follow Eliezer](https://www.lesswrong.com/posts/mmXEk675etTKpkgTx/agi-ruin-a-poorly-organized-list-of-lethalities)"