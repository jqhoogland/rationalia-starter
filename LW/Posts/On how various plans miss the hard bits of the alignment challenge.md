---
_id: 3pinFH3jerMzAvmza
title: On how various plans miss the hard bits of the alignment challenge
author: So8res
url: null
slug: on-how-various-plans-miss-the-hard-bits-of-the-alignment
type: post
tags:
  - LessWrong
  - Concept
  - Post
  - AI_Risk
  - AI
  - Research_Agendas
href: >-
  https://www.lesswrong.com/posts/3pinFH3jerMzAvmza/on-how-various-plans-miss-the-hard-bits-of-the-alignment
sequence: 2022 MIRI Alignment Discussion
chapter: null
synchedAt: '2022-09-01T09:25:52.718Z'
status: todo
---

# On How Various Plans Miss the Hard Bits of the Alignment Challenge


# Related

- [[AI Risk]]
- [[AI]]
- [[Research Agendas]]
- "[*Spotify*](https://open.spotify.com/episode/6cDwctrmpWW6UNEKTT3Vvf?si=7T7SaHNCQ9axzL4D-crPQw)"
- "[*Apple Podcasts*](https://podcasts.apple.com/us/podcast/on-how-various-plans-miss-the-hard-bits-of/id1630783021?i=1000570244508)"
- "[*Libsyn*](https://sites.libsyn.com/421877/on-how-various-plans-miss-the-hard-bits-of-the-alignment-challenge-by-nate-soares)"
- [[A central AI alignment problem: capabilities generalization, and the sharp left turn]]
- "[ Copenhagen interpretation of ethics](https://blog.jaibot.com/the-copenhagen-interpretation-of-ethics/)"
- "[Truthful AI](https://arxiv.org/abs/2110.06674)"
- "[ puts two identical strawberries on a plate and does nothing else](https://www.lesswrong.com/posts/SsCQHjqNT3xQAPQ6b/yudkowsky-on-agi-ethics)"
- "[ wandered off and invented birth control](https://arbital.com/p/daemons/)"
- "[ Eliciting Latent Knowledge](https://docs.google.com/document/d/1WwsnJQstPq91_Yh-Ch2XRL8H_EpsnjrC1dwZXR37PC8/edit)"
- "[ AI Services](https://www.fhi.ox.ac.uk/wp-content/uploads/Reframing_Superintelligence_FHI-TR-2019-1.1-1.pdf)"
- "[ Natural Abstractions](https://www.lesswrong.com/posts/cy3BhHrGinZCp3LXE/testing-the-natural-abstraction-hypothesis-project-intro)"
- "[ Language for Intelligent Machines](https://www.fhi.ox.ac.uk/wp-content/uploads/2021/08/QNRs_FHI-TR-2021-3.0.pdf)"
- "[ here](https://www.fhi.ox.ac.uk/wp-content/uploads/2021/11/Language-for-Intelligent-Machines-A-Prospectus.pdf)"
- "[ Theories of Impact for Interpretability](https://www.lesswrong.com/posts/uK6sQCNMw8WKzJeCQ/a-longlist-of-theories-of-impact-for-interpretability)"
- "[ Concept Extrapolation](https://www.alignmentforum.org/s/u9uawicHx7Ng7vwxA)"
- "[**in a blog post**](https://www.lesswrong.com/posts/vBoq5yd7qbYoGKCZK/why-i-m-co-founding-aligned-ai)"
- "[likely near](https://intelligence.org/2017/10/13/fire-alarm/)"
- "[**Twitter**](https://twitter.com/Heighn/status/1515711401901400072)"
- "[Rohin’s summary](https://www.lesswrong.com/posts/k54rgSg7GcjtXnMHX/model-splintering-moving-from-one-imperfect-model-to-another-1?commentId=ZYFxoacy9WKtHL54R)"
- "[benchmark we published](https://www.lesswrong.com/posts/DiEWbwrChuzuhJhGr/benchmark-for-successful-concept-extrapolation-avoiding-goal)"
- "[confusing](https://intelligence.org/2018/11/22/2018-update-our-new-research-directions/#section2)"
- "[ideological Turing Tests](https://www.econlib.org/archives/2011/06/the_ideological.html)"t](https://www.econlib.org/archives/2011/06/the_ideolog//arbital.com/p/Sovereign/)"
- "[^](#fnrefqstucmz31g)"
- "[^](#fnreff06rzor5nj8)"