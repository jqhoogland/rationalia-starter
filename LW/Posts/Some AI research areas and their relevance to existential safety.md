---
_id: hvGoYXi2kgnS3vxqb
title: Some AI research areas and their relevance to existential safety
author: null
url: null
slug: some-ai-research-areas-and-their-relevance-to-existential-1
type: post
tags:
  - LessWrong
  - Concept
  - Post
  - AI
  - Existential_Risk
  - Academic_Papers
  - World_Optimization
  - Agent_Foundations
href: >-
  https://www.lesswrong.com/posts/hvGoYXi2kgnS3vxqb/some-ai-research-areas-and-their-relevance-to-existential-1
sequence: Takeoff & Takeover
chapter: null
synchedAt: '2022-09-01T09:10:47.893Z'
status: todo
collection: Best of LessWrong
---

# Some AI Research Areas and Their Relevance to Existential Safety


# Related

- [[AI]]
- [[EA/Topics/Existential risk]]
- [[Academic Papers]]
- [[World Optimization]]
- [[Agent Foundations]]
- "[*What Multipolar Failure Looks Like, and Robust Agent-Agnostic Processes (RAAPs)*](https://www.alignmentforum.org/posts/LpM3EAakwYdS6aRKf/what-multipolar-failure-looks-like-and-robust-agent-agnostic)"
- "[ARCHES](https://arxiv.org/abs/2006.04948)"
- "[AI Research Considerations for Human Existential Safety (ARCHES)](http://acritch.com/arches/)"
- "[dictionary.com/browse/safety](http://dictionary.com/browse/safety)"
- "[dictionary.com/browse/alignment](http://dictionary.com/browse/alignment)"
- "[User/Agent Value Alignment](https://www.aaai.org/Papers/Symposia/Spring/2002/SS-02-07/SS02-07-002.pdf)"
- "[Aligning Superintelligence with Human Interests: A Technical Research Agenda](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.675.9314&rep=rep1&type=pdf)"
- "[Clarifying AI Alignment](https://ai-alignment.com/clarifying-ai-alignment-cec47cd69dd6)"
- "[https://acm-fca.org/2018/03/29/negativeimpacts/](https://acm-fca.org/2018/03/29/negativeimpacts/)"
- "[civic integrity policy](https://help.twitter.com/en/rules-and-policies/election-integrity-policy)"
- "[*Robust artificial intelligence and robust human organizations*](https://arxiv.org/pdf/1811.10840.pdf)"
- "[8 citations](https://scholar.google.com/scholar?cites=241619937926090053&as_sdt=2005&sciodt=0,5&hl=en)"
- "[*Doubly robust off-policy value evaluation for reinforcement learning*](http://proceedings.mlr.press/v48/jiang16.pdf)"
- "[*A baseline for detecting misclassified and out-of-distribution examples in neural networks*](https://arxiv.org/pdf/1610.02136.pdf)"
- "[*Enhancing the reliability of out-of-distribution image detection in neural networks*](https://arxiv.org/pdf/1706.02690.pdf)"
- "[*Training confidence-calibrated classifiers for detecting out-of-distribution samples*](https://arxiv.org/pdf/1711.09325.pdf)"
- "[*Learning confidence for out-of-distribution detection in neural networks*](https://arxiv.org/pdf/1802.04865.pdf)"
- "[*Translucent players: Explaining cooperative behavior in social dilemmas*](http://www.tark.org/proceedings/tark_jun4_15/TARK2015.9.pdf)"
- "[*Logical induction*](https://arxiv.org/pdf/1609.03543.pdf)"
- "[*Reflective oracles: A foundation for game theory in artificial intelligence*](https://intelligence.org/files/ReflectiveOraclesAI.pdf)"
- "[*Functional decision theory: A new theory of instrumental rationality*](https://arxiv.org/pdf/1710.05060.pdf)"
- "[*Disarmament games*](https://www.aaai.org/ocs/index.php/AAAI/AAAI17/paper/viewFile/14537/13800)"
- "[*Game theory with translucent players*](https://link.springer.com/content/pdf/10.1007/s00182-018-0626-x.pdf)"
- "[*Embedded agency*](https://arxiv.org/pdf/1902.09469.pdf)"
- "[*A parametric, resource-bounded generalization of loebâ€™s theorem, and a robust cooperation criterion for open-source game theory*](https://www.cambridge.org/core/journals/journal-of-symbolic-logic/article/parametric-resourcebounded-generalization-of-lobs-theorem-and-a-robust-cooperation-criterion-for-opensource-game-theory/16063EA7BFFEE89438631B141E556E79)"
- "[*Risks from Learned Optimization in Advanced Machine Learning Systems*](https://arxiv.org/pdf/1906.01820.pdf)"
- "[*Cooperating with unknown teammates in complex domains: A robot soccer case study of ad hoc teamwork*](http://www.cs.utexas.edu/~pstone/Papers/bib2html-links/AAAI15-Barrett.pd)"
- "[*Learning to communicate with deep multi-agent reinforcement learning*](http://papers.nips.cc/paper/6042-learning-to-communicate-with-deep-multi-agent-reinforcement-learning.pdf)"
- "[*Emergent complexity via multi-agent competition*](https://arxiv.org/pdf/1710.03748.pdf)"
- "[*Making friends on the fly: Cooperating with new teammates*](http://legacydirs.umiacs.umd.edu/~sarit/data/articles/sbarrett_aij_2016.pdf)"
- "[*Multi-agent actor-critic for mixed cooperative-competitive environments*](https://papers.nips.cc/paper/7217-multi-agent-actor-critic-for-mixed-cooperative-competitive-environments.pdf)"
- "[*Multiagent cooperation and competition with deep reinforcement learning*](https://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0172395&type=printable)"
- "[*Stabilising experience replay for deep multi-agent reinforcement learning*](https://arxiv.org/pdf/1702.08887.pdf)"
- "[*Counterfactual multi-agent policy gradients*](https://arxiv.org/pdf/1705.08926.pdf)"
- "[*Learning with opponent-learning awareness*](https://arxiv.org/pdf/1709.04326.pdf)"
- "[*Autonomous agents modelling other agents: A comprehensive survey and open problems*](https://arxiv.org/pdf/1709.08071.pdf)"
- "[*Learning to share and hide intentions using information regularization*](https://papers.nips.cc/paper/8227-learning-to-share-and-hide-intentions-using-information-regularization.pdf)"
- "[*Inequity aversion improves cooperation in intertemporal social dilemmas*](https://papers.nips.cc/paper/7593-inequity-aversion-improves-cooperation-in-intertemporal-social-dilemmas.pdf)"
- "[*Social influence as intrinsic motivation for multi-agent deep reinforcement learning*](http://proceedings.mlr.press/v97/jaques19a/jaques19a.pdf)"
- "[*Policy-gradient algorithms have no guarantees of convergence in continuous action and state multi-agent settings*](https://arxiv.org/pdf/1907.03712.pdf)"
- "[*Deep reinforcement learning from human preferences*](https://papers.nips.cc/paper/7017-deep-reinforcement-learning-from-human-preferences.pdf)"
- "[*Reward learning from human preferences and demonstrations in Atari*](https://papers.nips.cc/paper/8025-reward-learning-from-human-preferences-and-demonstrations-in-atari.pdf)"
- "[*The alignment problem for Bayesian history-based reinforcement learners*](https://www.tomeveritt.se/papers/alignment.pdf)"
- "[*Learning human objectives by evaluating hypothetical behavior*](https://arxiv.org/pdf/1912.05652.pdf)"
- "[*On the feasibility of learning, rather than assuming, human biases for reward inference*](https://arxiv.org/pdf/1906.09624.pdf)"
- "[*Reward-rational (implicit) choice: A unifying formalism for reward learning*](https://arxiv.org/pdf/2002.04833.pdf)"
- "[*Shared autonomy via hindsight optimization*](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6329599/pdf/nihms801800.pdf)"
- "[*Learning preferences for manipulation tasks from online coactive feedback*](https://arxiv.org/pdf/1601.00741.pdf)"
- "[*Cooperative inverse reinforcement learning*](http://papers.nips.cc/paper/6420-cooperative-inverse-reinforcement-learning.pdf)"
- "[*Planning for autonomous cars that leverage effects on human actions.*](https://pdfs.semanticscholar.org/baef/0a1859fc0216c89289c69da88d6dc8399fc7.pdf)"
- "[*Should robots be obedient?*](https://arxiv.org/pdf/1705.09990.pdf)"
- "[*Where do you think you're going?: Inferring beliefs about dynamics from behavior*](https://papers.nips.cc/paper/7419-where-do-you-think-youre-going-inferring-beliefs-about-dynamics-from-behavior.pdf)"
- "[*Literal or Pedagogic Human? Analyzing Human Model Misspecification in Objective Learning*](https://arxiv.org/pdf/1903.03877.pdf)"
- "[*Hierarchical game-theoretic planning for autonomous vehicles*](http://iliad.stanford.edu/pdfs/publications/fisac2019hierarchical.pdf)"
- "[*Pragmatic-pedagogic value alignment*](https://arxiv.org/pdf/1707.06354.pdf)"
- "[*Penalizing side effects using stepwise relative reachability*](https://arxiv.org/pdf/1806.01186.pdf)"
- "[*Safelife 1.0: Exploring side effects in complex environments*](https://arxiv.org/pdf/1912.01217.pdf)"
- "[*Preferences Implicit in the State of the World*](https://arxiv.org/pdf/1902.04198.pdf)"
- "[*Conservative agency via attainable utility preservation*](https://dl.acm.org/doi/pdf/10.1145/3375627.3375851)"
- "[*Interpretable classifiers using rules and bayesian analysis: Building a better stroke prediction model*](https://projecteuclid.org/download/pdfview_1/euclid.aoas/1446488742)"
- "[*Towards a rigorous science of interpretable machine learning*](https://arxiv.org/pdf/1702.08608.pdf)"
- "[*The mythos of model interpretability*](https://dl.acm.org/doi/pdf/10.1145/3236386.3241340)"
- "[*The building blocks of interpretability*](https://distill.pub/2018/building-blocks/)"
- "[*Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead*](https://arxiv.org/pdf/1811.10154.pdf)"
- "[*This looks like that: deep learning for interpretable image recognition*](http://papers.nips.cc/paper/9095-this-looks-like-that-deep-learning-for-interpretable-image-recognition.pdf)"
- "[*A study in Rashomon curves and volumes: A new perspective on generalization and model simplicity in machine learning*](https://arxiv.org/pdf/1908.01755.pdf)"
- "[*Inherent trade-offs in the fair determination of risk scores*](https://arxiv.org/pdf/1609.05807.pdf)"
- "[*On fairness and calibration*](http://papers.nips.cc/paper/7151-on-fairness-and-calibration.pdf)"
- "[*Fairness and accountability design needs for algorithmic support in high-stakes public sector decision-making*](https://arxiv.org/pdf/1802.01029.pdf)"
- "[*Delayed impact of fair machine learning*](https://arxiv.org/pdf/1803.04383.pdf)"
- "[*Fairness definitions explained*](http://www.ece.ubc.ca/~mjulia/publications/Fairness_Definitions_Explained_2018.pdf)"
- "[*Fairness and abstraction in sociotechnical systems*](http://amulyayadav.com/spring19/pdf/SSRN-id3265913.pdf)"
- "[social choice theory](https://en.wikipedia.org/wiki/Social_choice_theory)"
- "[*Society-in-the-loop: programming the algorithmic social contract*](https://link.springer.com/content/pdf/10.1007/s10676-017-9430-8.pdf)"
- "[*Social choice ethics in artificial intelligence*](https://link.springer.com/content/pdf/10.1007/s00146-017-0760-1.pdf)"
- "[*Dynamic social choice with evolving preferences*](https://dash.harvard.edu/bitstream/handle/1/30782198/1969871.pdf?sequence=1)"
- "[*Handbook of computational social choice*](https://books.google.com/books?hl=en&lr=&id=nMHgCwAAQBAJ&oi=fnd&pg=PR11&dq=%22Vincent+Conitzer%22&ots=f0ok1EA65o&sig=5Ci-ljHKpfEIrAjiXkK2fZQUlTs#v=onepage&q=%22Vincent%20Conitzer%22&f=false)"
- "[*The revelation principle for mechanism design with reporting costs*](https://dl.acm.org/doi/pdf/10.1145/2940716.2940795?casa_token=TIma9hwPZowAAAAA:NIxNsWuEwTi3dP95DpScfq45G0uHDKncPkjQ1LhnU7UlIUPaWCZl-NwCWUmzq_prDeRKvr30ch8)"
- "[*Barriers to Manipulation in Voting*](https://www.cse.unsw.edu.au/~tw/cwcomsoc2016.pdf)"
- "[*Proportional justified representation*](https://arxiv.org/pdf/1611.09928.pdf)"
- "[*Fair public decision making*](https://dl.acm.org/doi/pdf/10.1145/3033274.3085125?casa_token=eX0SYTKRi68AAAAA:WZgg3E1THtPXfIxyVQyVx6ZVCexkBr3_HQbb00T8bueisfk76Ii0DM4oZR7qEc1sRDIqDTfXez0)"
- "[*Fair social choice in dynamic settings*](https://www.irit.fr/COMSOC-2016/proceedings/FreemanEtAlCOMSOC2016.pdf)"
- "[*Justified representation in approval-based committee voting*](https://idp.springer.com/authorize/casa?redirect_uri=https://link.springer.com/content/pdf/10.1007/s00355-016-1019-3.pdf&casa_token=hxHki7TDQpcAAAAA:O8eLogr_dv0cFe0LOgiSputqBYZ2Fs2twR9iHJLTTh5KgSY5xmzlAlxO-0pBn-exm2iiaTNnUS8_hF4)"
- "[*Preference elicitation for participatory budgeting*](http://www.andrew.cmu.edu/user/jbenade/files/19_pbfull_new.pdf)"
- "[*Almost envy-freeness with general valuations*](https://epubs.siam.org/doi/pdf/10.1137/19M124397X?casa_token=iDkyBB47__oAAAAA:3Yo_eMgzNycUl3LtrPAS7EYNc7p9r-PUh54xLFb7DAuvLQsKUBcN8Be_1tH1bAappaz60Nt_jg)"
- "[*Accountability in algorithmic decision making*](http://merrill.umd.edu/wp-content/uploads/2016/01/p56-diakopoulos.pdf)"
- "[*Accountability of AI under the law: The role of explanation*](https://arxiv.org/pdf/1711.01134.pdf)"
- "[*It's time to do something: Mitigating the negative impacts of computing through a change to the peer review process*](https://acm-fca.org/2018/03/29/negativeimpacts/)"
- "[*NeurIPS 2020 FAQ*](https://nips.cc/Conferences/2020/PaperInformation/NeurIPS-FAQ)"
- "[*Hecht et al (2018)*](https://brenthecht.com/papers/FCADIscussions_NegativeImpactsPost_032918.pdf)"
- "[*Hecht (2020)*](https://medium.com/@BrentH/suggestions-for-writing-neurips-2020-broader-impacts-statements-121da1b765bf)"
- "[*GovAI (2020)*](https://medium.com/@GovAI/a-guide-to-writing-the-neurips-impact-statement-4293b723f832)"
- "[*NSF Americaâ€™s Seed Fund Technology Topic: AI*](https://web.archive.org/web/20201018115905/https://seedfund.nsf.gov/topics/artificial-intelligence/)"
- "[*The Social Dilemma*](https://www.netflix.com/title/81254224)"
- "[CCPA](https://oag.ca.gov/privacy/ccpa)"
- "[*Value Alignment or Misalignment--What Will Keep Systems Accountable?*](https://hrilab.tufts.edu/publications/aaai17-alignment.pdf)"
- "[*Towards formal definitions of blameworthiness, intention, and moral responsibility*](https://www.cs.cornell.edu/home/halpern/papers/moralresp.pdf)"
- "[*Trends and trajectories for explainable, accountable and intelligible systems: An HCI research agenda*](https://www.cs.ubc.ca/~conati/522/532b-2019/papers/LinExplanationSurveyCHI2018Survey.pdf)"
- "[*Policy certificates: Towards accountable reinforcement learning*](http://proceedings.mlr.press/v97/dann19a/dann19a.pdf)"
- "[*intent alignment*](https://ai-alignment.com/clarifying-ai-alignment-cec47cd69dd6)"