---
_id: qAvbtzdG2A2RBn7in
title: Effective Altruism
href: https://lesswrong.com/tag/effective-altruism
slug: effective-altruism
type: tag
tags:
  - LessWrong
  - Concept
  - Tag
synchedAt: '2022-08-29T11:04:00.166Z'
status: todo
---

# Effective Altruism

**Effective Altruism** (EA) is a movement trying to invest time and money in causes that do the most possible good per unit investment. EA was at one point called **optimal philanthropy**.

The basic concept behind EA is that you would really struggle to donate 100 times more money or time to charity than you currently do--but (assuming you are approximately utilitarian), spending a little time researching who to donate to *could* have an impact on roughly this order of magnitude. The same argument works for doing good with your career or volunteer hours.

## Concepts

Despite a broad diversity of ideas within the EA community on which areas are most pressing, there are a handful of criteria that are generally agreed make an area potentially impactful to work on (either directly or through donation). These are:

- The area is generally **neglected,** that is, it has capacity for more support either financially or in terms of skills
- The area has the potential for **large impact,** either in human lives saved, animal or human suffering alleviated, catastrophic crises averted, etc. Sometimes this is called "scale"
- The area is **tractable**\-\- it is a solvable problem, or is solvable with minimal resource investment (relative to other 
    problem areas)

A fourth semi-area is:

- Does the individual have good **personal fit**? Do they have unique skills which will make them more effective in an area.

From this, we can see a vast number of charities do not meet all or indeed any of these criteria. A major issue with EA is that some areas are much easier to track progress in than others (think tracking the cost per life saved of malaria nets vs existential [[AI]] risk, for instance). What is clear, however, is that some of the more effective charities (of those which *are* easy to track) have [far more benefit over the average charity than people think](https://80000hours.org/2017/05/most-people-report-believing-its-incredibly-cheap-to-save-lives-in-the-developing-world/)\-\- perhaps as much as 10,000% as effective.

A large portion of the EA community are by and large, **longtermist**. This refers to the idea that, if there are many future generations (100s, 1000s or more), and their lives are as valuable as ours, then even very small impacts on all of their lives-- or things like moving good changes forwards in time or bad ones back-- far outweigh impacts on people who are currently alive. Because this concept is less broadly-accepted than charity for currently-alive people, longtermist solutions are also generally considered to be neglected. Longtermist interventions generally focus on [[Risks of Astronomical Suffering (S-risks)|S-risks]] or [[LW/Concepts/Existential Risk|X-risks]].

Examples of longtermist interventions include [[AI]] safety, pandemic preparedness, and [[Nanotechnology|nanotechnology]] security. Examples of other popular EA interventions include global poverty alleviation, malaria treatments, and vitamin supplementation in sub-saharan Africa.
 

The **Effective Altruism movement** also has its own forum,   [**The EA Forum**](https://forum.effectivealtruism.org/). It runs on the same software as LessWrong.

## Notable Posts

- [Efficient charity: do unto others...](https://lessestwrong.com/lw/3gj/efficient_charity_do_unto_others/)
- [Optimal philanthropy for human beings](https://lessestwrong.com/lw/6py/optimal_philanthropy_for_human_beings/)
- [Why we can't take expected value estimates literally (even when they're unbiased)](https://lessestwrong.com/lw/745/why_we_cant_take_expected_value_estimates/)

## Total Resources and How They Are Split

\[effectivealtruismdata.com\]

\[How much money 80k has an article on this\]

\[include graph\]

## Impact

### Global Health and Economic Development

\[estimated total amount of lives saved\]

The [Against Malaria Foundation](https://www.againstmalaria.com/) has distributed more than 70 million bednets to protect people (mostly children) from a debilitating parasite. ([Source](https://www.againstmalaria.com/Distributions.aspx)) \[number of lives saved\]

[GiveDirectly](https://givedirectly.org/) has facilitated more than $100 million in direct cash transfers to families living in extreme poverty, who determine for themselves how best to spend the money. ([Source](https://www.givedirectly.org/financials/)) \[number of lives saved\]

The [Schistosomiasis Control Initiative](https://www.imperial.ac.uk/schistosomiasis-control-initiative) and [Deworm the World Initiative](http://www.evidenceaction.org/dewormtheworld/) invests in people's health and future well-being by treating preventable diseases that often get little attention. They have given out hundreds of millions of deworming treatments to fight intestinal parasites, which may help people earn higher incomes later in life. (Sources for [SCI](https://schistosomiasiscontrolinitiative.org/reach) and [DWI](https://www.evidenceaction.org/dewormtheworld-2/))

### Animal Welfare

\[how much animal welfare in some reasonable metric\]

[The Humane League](https://thehumaneleague.org/) and [Mercy for Animals](https://mercyforanimals.org/), alongside many other organizations, have orchestrated corporate campaigns and legal reforms to fight the use of battery cages. Because of this work, more than 100 million hens that would have been caged instead live cage-free. (This includes all cage-free reform work, of which a sizable fraction was funded by EA-aligned donors.)

[The Good Food Institute](https://gfi.org/) works with scientists, entrepreneurs, and investors to develop and promote meat alternatives that don't require the suffering of farmed animals.

### Existential Risk and the Long-term Future

\[how much lower higher? risk of existential catastrphe as a result\]

Organizations like the [Future of Humanity Institute](https://www.fhi.ox.ac.uk/) and the [Centre for the Study of Existential Risk](https://www.cser.ac.uk/) work on research and policy related to some of the biggest threats facing humanity, from pandemics and climate change to nuclear war and superintelligent AI systems.

Some organizations in this space, like the [Center for Human-Compatible AI](https://humancompatible.ai/) and the [Machine Intelligence Research Institute](https://intelligence.org/), focus entirely on solving issues posed by advances in artificial intelligence. AI systems of the future could be very powerful and difficult to control --- a dangerous combination.

[Sherlock Biosciences](https://sherlock.bio/) is developing a diagnostic platform that could reduce threats from viral pandemics. (They are a private company, but much of their capital comes from a [grant](https://www.openphilanthropy.org/focus/scientific-research/sherlock-biosciences-research-viral-diagnostics) made by Open Philanthropy, an EA-aligned grantmaker.)

## Key Ideas

## Criticisms

- EA is incoherent. Consequentialism applies to one's whole life, but many EAs don’t take it this seriously
    - This argument applies to virtue ethics too, but no one criticises it - “why aren’t you constantly seeking to always do the virtuous action”. People in practice seem to take statements from consequentialist philosophies more seriously than they do from others
    - It is more intellectually honest to surface incoherence in your worldview - "I use 80% of my time as effectively as possible" 
- EA frames all value in terms of impact creation and this makes members sad^[\[1\]](#fnipt32j7op0s)^
    - How widespread is this?
    - Many EAs don't feel this way
    - Some people control orders of magnitude more resources than others. They could use their time and money to improve the lives of many other people. Whether they should is a different question, but it doesn't avoid the fact that this is true.
- EA supports a culture of guilt \[Kerry thread\]
    - How does EA compare in terms of mental wellbeing to other communities centred around "doing good" eg "Protestant Work Ethic" and "Catholic Guilt"?
    - If you struggle with this, consider reading[ Replacing Guilt](https://forum.effectivealtruism.org/s/a2LBRPLhvwB83DSGq), which is one of only 3 sequences with a permanent place sidebar of the EA Forum.
- EA is spending too much money
    - EA is spending *more* money but it's not immediately obvious it is spending too much. It might be spending too little. \[Will MacAskill article\]

### Criticisms to Add

[Stefan Shubert's criticisms and responses](https://stefanfschubert.com/blog/2020/12/30/five-common-ea-self-criticisms-i-disagree-with)

Kuhn, Ben (2013) [A critique of effective altruism](https://www.benkuhn.net/ea-critique/), *Ben Kuhn’s Blog*, December 2.

McMahan, Jeff (2016) [Philosophical critiques of effective altruism](https://doi.org/10.5840/tpm20167379), *The Philosophers’ Magazine*, vol. 73, pp. 92–99.

Nielsen, Michael (2022) [Notes on effective altruism](https://michaelnotebook.com/eanotes/), *Michael’s Notebook*, June 2.

Rowe, Abraham (2022) [Critiques of EA that I want to read](https://forum.effectivealtruism.org/posts/n3WwTz4dbktYwNQ2j/critiques-of-ea-that-i-want-to-read), *Effective Altruism Forum*, June 19.

Wiblin, Robert & Keiran Harris (2019) [Vitalik Buterin on effective altruism, better ways to fund public goods, the blockchain’s problems so far, and how it could yet change the world](https://80000hours.org/podcast/episodes/vitalik-buterin-new-ways-to-fund-public-goods/), *80,000 Hours*, September 3.

Zhang, Linchuan (2021) [The motivated reasoning critique of effective altruism](https://forum.effectivealtruism.org/posts/pxALB46SEkwNbfiNS/the-motivated-reasoning-critique-of-effective-altruism), *Effective Altruism Forum*, September 14.

## Related

- [[Altruism]] (`= [[Altruism]].status`)
- [[Cause Prioritization]] (`= [[Cause Prioritization]].status`)
- [Utilitarianism](https://lessestwrong.com/tag/utilitarianism)
- [[Risks of Astronomical Suffering (S-risks)|S-risk]] (`= [[Risks of Astronomical Suffering (S-risks)|S-risk]].status`)
- [[LW/Concepts/Existential Risk|X-risk]] (`= [[LW/Concepts/Existential Risk|X-risk]].status`)

## External Resources

- [80,000 Hours](https://80000hours.org/), who offer advice for how to have a maximally globally impactful career
- [Effective Altruism,](https://www.effectivealtruism.org/) who offer support for local EA groups, as well as articles and advice surrounding EA
- [GiveWell,](https://www.givewell.org/) a charity doing research into the effectiveness of other charities to provide information for donors
- [The Life You Can Save](https://www.thelifeyoucansave.org/the-book/?gclid=CjwKCAjwjqT5BRAPEiwAJlBuBXb3m1FKunezyfsYzYkjmgzSCHScRgZpzMH097cbAAGC5lmHUP-J3BoCcnAQAvD_BwE), a free eBook outlining reasons for donating more and more effectively
- [[Center on Long-Term Risk (CLR)|Center on Long-Term Risk]] (`= [[Center on Long-Term Risk (CLR)|Center on Long-Term Risk]].status`)

1. ^**[^](#fnrefipt32j7op0s)**^


    <https://twitter.com/KerryLVaughan/status/1545063368695898112?s=20&t=xgaSuh22V6y44Wkcebo22Q>
