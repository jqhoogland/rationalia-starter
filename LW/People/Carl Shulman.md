---
_id: 5f5c37ee1b5cdee568cfb1e5
title: Carl Shulman
href: https://lesswrong.com/tag/carl-shulman
slug: carl-shulman
type: tag
tags:
  - LessWrong
  - Concept
  - Tag
synchedAt: '2022-08-29T11:07:20.008Z'
status: todo
---

# Carl Shulman

**Carl Shulman** is a Research Fellow at the [[Machine Intelligence Research Institute (MIRI)|Machine Intelligence Research Institute]] who has authored and co-authored several papers on AI risk, including:

- “How Hard is Artificial Intelligence? Evolutionary Arguments and Selection Effects” [1](http://www.nickbostrom.com/aievolution.pdf), a analysis of the implications of the [[Observation Selection Effect|Observation selection effect]] on the [[Evolutionary Argument For Human-Level AI|Evolutionary argument for human-level AI]]
- ”Whole Brain Emulation and the Evolution of Superorganisms”[2](http://intelligence.org/files/WBE-Superorgs.pdf), argues for the existence of pressures favoring the emergence of increased coordination between [[Whole Brain Emulation|emulated brains]], in the form of superorganisms.
- ”Implications of a Software-Limited Singularity”[3](http://intelligence.org/files/SoftwareLimited.pdf), argues for the high probability of a human-level AI before 2060.

Previously, he worked at Clarium Capital Management, a global macro hedge fund, and at the law firm Reed Smith LLP. He attended New York University School of Law and holds a BA in philosophy from Harvard University.

## See Also

- [Timeline of Carl Shulman publications](http://lesswrong.com/lw/7ob/timeline_of_carl_shulman_publications/)
    - [More up-to-date and comprehensive timeline of publications](https://timelines.issarice.com/wiki/Timeline_of_Carl_Shulman_publications)
- [80,000 Hours Carl Shulman’s profile](http://80000hours.org/members/carl-shulman)
