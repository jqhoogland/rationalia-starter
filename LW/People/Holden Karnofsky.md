---
_id: 5f5c37ee1b5cdee568cfb316
title: Holden Karnofsky
href: https://lesswrong.com/tag/holden-karnofsky
slug: holden-karnofsky
type: tag
tags:
  - LessWrong
  - Concept
  - Tag
synchedAt: '2022-08-29T11:09:26.481Z'
status: todo
---

# Holden Karnofsky

Holden Karnofsky is a co-founder of [[Effective Altruism|effective altruism]] pioneers [[GiveWell]].

In May 2012, Karnofsky posted [Thoughts on the Singularity Institute (SI)](http://lesswrong.com/lw/cbs/thoughts_on_the_singularity_institute_si/), which became the most-upvoted article ever on Less Wrong. It offered a detailed critique of what is now the [[Machine Intelligence Research Institute (MIRI)|Machine Intelligence Research Institute]], and spawned a great deal of discussion.

MIRI staff posted two replies:

- [[Eliezer Yudkowsky]], [Reply to Holden on 'Tool AI'](http://lesswrong.com/lw/cze/reply_to_holden_on_tool_ai/)
- [[Luke Muehlhauser]], [Reply to Holden on The Singularity Institute](http://lesswrong.com/lw/di4/reply_to_holden_on_the_singularity_institute/)

Paul Crowley ("ciphergoth") posted discussion articles for each point raised:

- [Objection 1: it seems to me that any AGI that was set to maximize a "Friendly" utility function would be extraordinarily dangerous.](http://lesswrong.com/lw/cck/holden_karnofskys_singularity_institute_objection/)
- [Objection 2: SI appears to neglect the potentially important distinction between "tool" and "agent" AI.](http://lesswrong.com/lw/ccl/holden_karnofskys_singularity_institute_objection/)
- [Objection 3: SI's envisioned scenario is far more specific and conjunctive than it appears at first glance, and I believe this scenario to be highly unlikely.](http://lesswrong.com/lw/ccm/holden_karnofskys_singularity_institute_objection/)
- [Is SI the kind of organization we want to bet on?](http://lesswrong.com/lw/cco/holden_karnofskys_singularity_institute_critique/)
- [Other objections to SI's views](http://lesswrong.com/lw/ccn/holden_karnofskys_singularity_institute_critique/)

Other discussion:

- Phil Goetz, [Holden's Objection 1: Friendliness is dangerous](http://lesswrong.com/lw/chk/holdens_objection_1_friendliness_is_dangerous/)

## Related Pages

- [[Tool AI]]
