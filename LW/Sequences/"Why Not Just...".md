---
_id: TLSzP4xP42PPBctgw
title: '"Why Not Just..."'
type: sequence
tags:
  - LessWrong
  - Sequence
synchedAt: '2022-08-29T11:15:19.700Z'
---
# "Why Not Just..."

A compendium of rants about alignment proposals, of varying charitability.

# Chapters

## "Why Not Just..."

- [[Deep Learning Systems Are Not Less Interpretable Than Logic, Probability, Etc]]
- [[Godzilla Strategies]]
- [[Rant on Problem Factorization for Alignment]]
- [[Interpretability, Tool-ness, Alignment, Corrigibility are not Composable]]
- [[How To Go From Interpretability To Alignment— Just Retarget The Search]]
- [[Oversight Misses 100% of Thoughts The AI Does Not Think]]
- [[Human Mimicry Mainly Works When We’re Already Close]]