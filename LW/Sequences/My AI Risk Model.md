---
title: My AI Risk Model
type: sequence
tags:
  - LessWrong
  - Sequence
---

This sequence is an attempt to think concretely about where the danger from misaligned AI comes from. Why might AI systems develop objectives? Why might these objectives not be compatible with our human values? When does this misalignment seem particularly dangerous?

This work was done as part of the first iteration of the SERI MATS program.

# Chapters

## My AI Risk Model

- [[Why I'm Worried About AI]]
- [[A Story of AI Riskâ€” InstructGPT-N]]
- [[Confusions in My Model of AI Risk]]