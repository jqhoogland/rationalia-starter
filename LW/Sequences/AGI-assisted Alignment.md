---
_id: jiqxd9ZmSSocs5Qcz
title: AGI-assisted Alignment
type: sequence
tags:
  - LessWrong
  - Sequence
synchedAt: '2022-08-29T11:15:19.704Z'
---
# AGI-assisted Alignment

Can we start out with an unaligned superintelligent AGI, and end up with an aligned AGI-system? I argue *maybe*, and discuss principles, techniques and strategies that may enable us to do so.  
  
One reason for exploring such strategies is contingency planning (what if we haven’t solved alignment by the time the first superintelligent AGI arrives?). Another reason is that additional layers of security could be beneficial (even if we *think* we have solved alignment, are there ways to relatively quickly add additional layers of alignment-assurance?).

This is an ongoing series.

# Chapters

## AGI-assisted Alignment

- [[Getting from an unaligned AGI to an aligned AGI?]]
- [[Making it harder for an AGI to "trick" us, with STVs]]