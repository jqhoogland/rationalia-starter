---
_id: n945eovrA3oDueqtq
title: 2021 MIRI Conversations
curatedOrder: 400
type: sequence
tags:
  - LessWrong
  - Sequence
aliases:
  - 2021 MIRI Conversations
synchedAt: '2022-08-29T17:16:42.660Z'
---

# 2021 MIRI Conversations

This sequence is a (chronological) series of chatroom conversation logs about artificial general intelligence. A large number of topics are covered, beginning with conversations related to alignment difficulty.

Short summaries of each post, and links to audio versions, are available [here](https://intelligence.org/late-2021-miri-conversations/). There are also two related posts released shortly before this sequence:

- [Discussion with Eliezer Yudkowsky on AGI Interventions](https://www.lesswrong.com/posts/CpvyhFy9WvCNsifkY/discussion-with-eliezer-yudkowsky-on-agi-interventions)
- [Comments \[by Nate Soares\] on Joe Carlsmith's "Is power-seeking AI an existential risk?"](https://www.lesswrong.com/posts/cCMihiwtZx7kdcKgt/comments-on-carlsmith-s-is-power-seeking-ai-an-existential)

Rob Bensinger edited and posted this sequence, and Matthew Graves helped with much of the formatting.

⠀

## Chapters

### 2021 MIRI Conversations

- [[Ngo and Yudkowsky on alignment difficulty]]
- [[Ngo and Yudkowsky on AI capability gains]]
- [[Yudkowsky and Christiano discuss "Takeoff Speeds"]]
- [[Soares, Tallinn, and Yudkowsky discuss AGI cognition]]

### 2021 MIRI Conversations

- [[Christiano, Cotra, and Yudkowsky on AI progress]]
- [[Biology-Inspired AGI Timelines— The Trick That Never Works]]
- [[Reply to Eliezer on Biological Anchors]]
- [[Shulman and Yudkowsky on AI progress]]
- [[More Christiano, Cotra, and Yudkowsky on AI progress]]
- [[Conversation on technology forecasting and gradualism]]

### 2021 MIRI Conversations

- [[Ngo's view on alignment difficulty]]
- [[Ngo and Yudkowsky on scientific reasoning and pivotal acts]]
- [[Christiano and Yudkowsky on AI predictions and human intelligence]]
- [[Shah and Yudkowsky on alignment failures]]
- [[Late 2021 MIRI Conversations— AMA, Discussion]]
