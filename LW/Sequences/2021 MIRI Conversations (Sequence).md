---
_id: n945eovrA3oDueqtq
title: 2021 MIRI Conversations
curatedOrder: 400
type: sequence
tags:
  - LessWrong
  - Sequence
aliases:
  - 2021 MIRI Conversations
synchedAt: '2022-08-29T17:16:42.660Z'
status: todo
---

# 2021 MIRI Conversations

This sequence is a (chronological) series of chatroom conversation logs about artificial general intelligence. A large number of topics are covered, beginning with conversations related to alignment difficulty.

Short summaries of each post, and links to audio versions, are available [here](https://intelligence.org/late-2021-miri-conversations/). There are also two related posts released shortly before this sequence:

- [Discussion with Eliezer Yudkowsky on AGI Interventions](https://www.lesswrong.com/posts/CpvyhFy9WvCNsifkY/discussion-with-eliezer-yudkowsky-on-agi-interventions)
- [Comments \[by Nate Soares\] on Joe Carlsmith's "Is power-seeking AI an existential risk?"](https://www.lesswrong.com/posts/cCMihiwtZx7kdcKgt/comments-on-carlsmith-s-is-power-seeking-ai-an-existential)

Rob Bensinger edited and posted this sequence, and Matthew Graves helped with much of the formatting.

⠀

## Chapters

### 2021 MIRI Conversations

- [[Ngo and Yudkowsky on alignment difficulty]] (`= [[Ngo and Yudkowsky on alignment difficulty]].status`)
- [[Ngo and Yudkowsky on AI capability gains]] (`= [[Ngo and Yudkowsky on AI capability gains]].status`)
- [[Yudkowsky and Christiano discuss "Takeoff Speeds"]] (`= [[Yudkowsky and Christiano discuss "Takeoff Speeds"]].status`)
- [[Soares, Tallinn, and Yudkowsky discuss AGI cognition]] (`= [[Soares, Tallinn, and Yudkowsky discuss AGI cognition]].status`)

### 2021 MIRI Conversations

- [[Christiano, Cotra, and Yudkowsky on AI progress]] (`= [[Christiano, Cotra, and Yudkowsky on AI progress]].status`)
- [[Biology-Inspired AGI Timelines— The Trick That Never Works]] (`= [[Biology-Inspired AGI Timelines— The Trick That Never Works]].status`)
- [[Reply to Eliezer on Biological Anchors]] (`= [[Reply to Eliezer on Biological Anchors]].status`)
- [[Shulman and Yudkowsky on AI progress]] (`= [[Shulman and Yudkowsky on AI progress]].status`)
- [[More Christiano, Cotra, and Yudkowsky on AI progress]] (`= [[More Christiano, Cotra, and Yudkowsky on AI progress]].status`)
- [[Conversation on technology forecasting and gradualism]] (`= [[Conversation on technology forecasting and gradualism]].status`)

### 2021 MIRI Conversations

- [[Ngo's view on alignment difficulty]] (`= [[Ngo's view on alignment difficulty]].status`)
- [[Ngo and Yudkowsky on scientific reasoning and pivotal acts]] (`= [[Ngo and Yudkowsky on scientific reasoning and pivotal acts]].status`)
- [[Christiano and Yudkowsky on AI predictions and human intelligence]] (`= [[Christiano and Yudkowsky on AI predictions and human intelligence]].status`)
- [[Shah and Yudkowsky on alignment failures]] (`= [[Shah and Yudkowsky on alignment failures]].status`)
- [[Late 2021 MIRI Conversations— AMA, Discussion]] (`= [[Late 2021 MIRI Conversations— AMA, Discussion]].status`)
