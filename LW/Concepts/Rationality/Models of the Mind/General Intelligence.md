---
_id: ac84EpK6mZbPLzmqj
title: General Intelligence
href: https://www.lesswrong.com/tag/general-intelligence
slug: general-intelligence
type: tag
tags:
  - LessWrong
  - Concept
  - Tag
synchedAt: '2022-09-01T09:42:42.427Z'
status: done
aliases:
  - Intelligence
  - intelligent
  - Universal intelligence
---

# General Intelligence

**General Intelligence** or **Universal Intelligence** is the ability to efficiently achieve goals in a wide range of domains.  ^db4a54

This tag is specifically for discussing intelligence in the broad sense: for discussion of IQ testing and psychometric intelligence, see [[IQ and g-factor|IQ / g-factor]]; for discussion about e.g. specific results in artificial intelligence, see [[AI]]. These tags may overlap with this one to the extent that they discuss the nature of general intelligence.

Examples of posts that fall under this tag include [The Power of Intelligence](https://www.lesswrong.com/posts/aiQabnugDhcrFtr9n/the-power-of-intelligence), [Measuring Optimization Power](https://www.lesswrong.com/posts/Q4hLMDrFd8fbteeZ8/measuring-optimization-power), [Adaption-Executers not Fitness Maximizers](https://www.lesswrong.com/posts/XPErvb8m9FapXCjhA/adaptation-executers-not-fitness-maximizers), [Distinctions in Types of Thought](https://www.lesswrong.com/posts/FbQ9Y9pBif5xZ7w2f/distinctions-in-types-of-thought), [The Octopus, the Dolphin and Us: a Great Filter tale](https://www.lesswrong.com/posts/GMqZ2ofMnxwhoa7fD/the-octopus-the-dolphin-and-us-a-great-filter-tale).

On the difference between psychometric intelligence (IQ) and general intelligence:

> But the word “intelligence” commonly evokes pictures of the starving professor with an IQ of 160 and the billionaire CEO with an IQ of merely 120. Indeed there are differences of individual ability apart from “book smarts” which contribute to relative success in the human world: enthusiasm, social skills, education, musical talent, rationality. Note that each factor I listed is cognitive. Social skills reside in the brain, not the liver. And jokes aside, you will not find many CEOs, nor yet professors of academia, who are chimpanzees. You will not find many acclaimed rationalists, nor artists, nor poets, nor leaders, nor engineers, nor skilled networkers, nor martial artists, nor musical composers who are mice. Intelligence is the foundation of human power, the strength that fuels our other arts.

> \-\- Eliezer Yudkowsky, [Artificial Intelligence as a Positive and Negative Factor in Global Risk](https://intelligence.org/files/AIPosNegFactor.pdf)

## Definitions of General Intelligence

After reviewing extensive literature on the subject, [[@legg2007|Legg and Hutter]] summarizes the many possible valuable definitions in the informal statement “Intelligence measures an agent’s ability to achieve goals in a wide range of environments.” They then show this definition can be mathematically formalized given reasonable mathematical definitions of its terms. They use [Solomonoff induction](https://lessestwrong.com/tag/solomonoff-induction) \- a formalization of [Occam's razor](https://lessestwrong.com/tag/occam-s-razor) \- to construct an [universal artificial intelligence](https://lessestwrong.com/tag/aixi) with a embedded [utility function](https://lessestwrong.com/tag/utility-functions) which assigns less [utility](https://lessestwrong.com/tag/expected-utility) to those actions based on theories with higher [complexity](https://wiki.lesswrong.com/wiki/Kolmogorov_complexity). They argue this final formalization is a valid, meaningful, informative, general, unbiased, fundamental, objective, universal and practical definition of intelligence.

We can relate Legg and Hutter's definition with the concept of [optimization](https://lessestwrong.com/tag/optimization). According to [Eliezer Yudkowsky](https://lessestwrong.com/tag/eliezer-yudkowsky) intelligence is [efficient cross-domain optimization](https://lessestwrong.com/lw/vb/efficient_crossdomain_optimization/). It measures an agent's capacity for efficient cross-domain optimization of the world according to the agent’s preferences.^[\[2\]](#fn7hbpdfpe6x3)^ Optimization measures not only the capacity to achieve the desired goal but also is inversely proportional to the amount of resources used. It’s the ability to steer the future so it hits that small target of desired outcomes in the large space of all possible outcomes, using fewer resources as possible. For example, when Deep Blue defeated Kasparov, it was able to hit that small possible outcome where it made the right order of moves given Kasparov’s moves from the very large set of all possible moves. In that domain, it was more optimal than Kasparov. However, Kasparov would have defeated Deep Blue in almost any other relevant domain, and hence, he is considered more intelligent.

One could cast this definition in a possible world vocabulary, intelligence is:

1. the ability to precisely realize one of the members of a small set of possible future worlds that have a higher preference over the vast set of all other possible worlds with lower preference; while
2. using fewer resources than the other alternatives paths for getting there; and in the
3. most diverse domains as possible.

How many more worlds have a higher preference then the one realized by the agent, less intelligent he is. How many more worlds have a lower preference than the one realized by the agent, more intelligent he is. (Or: How much smaller is the set of worlds at least as preferable as the one realized, more intelligent the agent is). How much less paths for realizing the desired world using fewer resources than those spent by the agent, more intelligent he is. And finally, in how many more domains the agent can be more efficiently optimal, more intelligent he is. Restating it, the intelligence of an agent is directly proportional to:

- (a) the numbers of worlds with lower preference than the one realized,
- (b) how much smaller is the set of paths more efficient than the one taken by the agent and
- (c) how more wider are the domains where the agent can effectively realize his preferences;

and it is, accordingly, inversely proportional to:

- (d) the numbers of world with higher preference than the one realized,
- (e) how much bigger is the set of paths more efficient than the one taken by the agent and
- (f) how much more narrow are the domains where the agent can efficiently realize his preferences.

This definition avoids several problems common in many others definitions, especially it avoids [anthropomorphizing](https://lessestwrong.com/tag/anthropomorphism) intelligence.

## See Also

- [Optimization process](https://lessestwrong.com/tag/optimization)
- [Decision theory](https://lessestwrong.com/tag/decision-theory)
- [Rationality](https://lessestwrong.com/tag/rationality)
- [[@legg2007|Legg and Hutter paper “Universal Intelligence: A Deﬁnition of Machine Intelligence"]]
- <http://intelligence.org/files/IE-EI.pdf>

---

## [Arbital](https://arbital.com/p/general_intelligence/)
> A bee is born with the ability to construct hives; a beaver is born with an instinct for building dams; a human looks at both and imagines a gigantic dam with a honeycomb structure of internal reinforcement.

### AGI != human-level AI
Our intelligence comes from an iterative (inefficient) evolutionary process.

> We might at some point consistently see an infrahuman general intelligence that is not like a disabled human, but rather like some previously unobserved and unimagined form of weaker but still highly general intelligence.

### IQ does not matter for AGI

> That is: If you're trying to build the first mechanical heavier-than-air flying machine, you ought to be thinking "How do birds fly? How do they stay up in the air, at all?" Rather than, "Is there a central Fly-Q factor that can be inferred from the variation in many different measures of how well individual pigeons fly, which lets us predict the individual variation in a pigeon's speed or turning radius better than any single observation about one factor of that pigeon's flying ability?"

That said, g may constitute evidence in favor of "general intelligence".

We probably underestimate the commonality of cognitive tasks across superficially different tasks.

Two extreme straw positions:
- "I see no reason why ability at driving blue cars has to be so strongly correlated with driving red cars; they look pretty different to me"
- Maybe Tic Tac Toe isn't possible without full-blown AGI

> It seems like in practice, different beliefs about 'general intelligence' may account for a lot of the disagreement about "Can we have an AI that X-es without that AI being 30 seconds away from being capable of Y-ing?" In particular, different beliefs about:
> -   To what degree most interesting/relevant domain problems, decompose well into a similar class of deep cognitive subproblems;
> -   To what degree whacking on an interesting/relevant problem with general intelligence is a good or natural way to solve it, compared to developing specialized algorithms (that can't just be developed _by_ a general intelligence (without that AGI paying pragmatically very-difficult-to-pay costs in computation or sample complexity)).

%%

START

Basic

Informally, what is **General (or Universal) Intelligence** according to Legg & Hutter (2007)?

Back: "Intelligence measures an agent’s ability to achieve goals in a wide range of environments."

Tags: LessWrong

END

START

Basic

Who proposed the following informal definition of (general / universal) intelligence?

"Intelligence measures an agent’s ability to achieve goals in a wide range of environments."

Back: Legg & Hutter (2007)

Tags: LessWrong

END

START

Basic

Informally, what is **intelligence** according to Yudkowsky?

Back: "**[(2)] efficient [(3)] cross-domain [(1)] optimization**", i.e.:

1. the ability to precisely realize one of the members of a small set of possible future worlds that have a higher preference over the vast set of all other possible worlds with lower preference; while
2. using fewer resources than the other alternatives paths for getting there; and in the
3. most diverse domains as possible.

Tags: LessWrong

END

START

Basic

Informally, what is **General or Universal Intelligence** according to Yudkowsky?

Back: "**[(2)] efficient [(3)] cross-domain [(1)] optimization**", i.e.:

1. the ability to precisely realize one of the members of a small set of possible future worlds that have a higher preference over the vast set of all other possible worlds with lower preference; while
2. using fewer resources than the other alternatives paths for getting there; and in the
3. most diverse domains as possible.

Tags: LessWrong

END

START

Basic

Formally, what is **Universal Intelligence** according to Legg & Hutter (2007)?

Given an agent $\pi$ that produces actions $a_i$ given a history of observations $o_i$, rewards $r_i$, and actions.

Back:

An agent $\pi$'s expected utility $V_\mu^\pi$ over environments $\mu \in E$ weighted by prefix-free Kolmogorov complexity $K(\mu)$:

$$\Upsilon(\pi) := \sum_{\mu\in E} 2^{-K(\mu)}V_{\mu}^\pi,$$
where the expected utility is defined as:

$$V_\mu^\pi := \mathbb E\left(\sum_{i=1}^\infty r_i\right) \leq 1,$$

and $r_i$ is the reward at timestep $i$.

Tags: LessWrong

END

%%
