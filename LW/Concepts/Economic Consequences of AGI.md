---
_id: 5f5c37ee1b5cdee568cfb2ac
title: Economic Consequences of AGI
href: https://lesswrong.com/tag/economic-consequences-of-agi
type: tag
tags:
  - LessWrong
  - Concept
  - Tag
synchedAt: '2022-08-29T10:47:39.152Z'
---
# Economic Consequences of AGI

The **economic consequences of** **[[Artificial General Intelligence|artificial general intelligence]]** arise from their fundamentally new properties compared to the human brains currently driving the economy. Once such digital minds become generally intelligent enough to perform a wide range of economic functions, they are likely to bring radical changes, creating great wealth, but also displacing humans out of more and more types of job.

An important aspect of the question is that of economic growth. The invention of AGI or WBE could cause a sudden increase in growth by adding machine intelligence to the pool of human innovators. Machine intelligence could be much cheaper to produce, faster, and qualitatively smarter than human talent. A [[Intelligence Explosion|feedback loop]] from better machine intelligence technology, to more and better machine researchers, back to better machine intelligence technology could ensue.

[[Robin Hanson]] has written much about the economics of whole brain emulation. In [his view](http://hanson.gmu.edu/uploads.html), the unrestricted creation of additional uploads will cause a [[Malthusian Scenarios|Malthusian scenario]], where upload wages fall to subsistence levels. He sees the transition to whole brain emulation as a [jump to a new "growth mode"](http://hanson.gmu.edu/longgrow.pdf) with higher exponential growth rates, similar to the transitions to agriculture and industry.

In ["The Future of Human Evolution"](http://www.nickbostrom.com/fut/evolution.pdf), [[Nick Bostrom]] argues that in an emulated-brain society with individuals living at subsistence levels, entities that possess a large set of features we care about – which he calls flamboyant displays, or culture in general – will be outcompeted by more efficient ones that lack inefficient humans’ cultural aspects. This will lead to elimination of all forms of being that we care about. He proposes that only a [[Singleton]] could ensure strict control in order to prevent the elimination of culture through outcompetition.

Others predict that growth will blow up even more suddenly (up to the point where physical limits become relevant), and that growth will be concentrated in a smaller and more coherent set of agents, so that instead of continued free market competition, we will see a [[Singleton|singleton]] emerge.

## Blog posts

- [The Hanson-Yudkowsky Foom Debate](http://wiki.lesswrong.com/wiki/The_Hanson-Yudkowsky_AI-Foom_Debate)
- [Overcoming Bias posts tagged "Future"](http://www.overcomingbias.com/tag/future)
- [Is The City-ularity Near?](http://www.overcomingbias.com/2010/02/is-the-city-ularity-near.html)

## External links

- [Bostrom's paper on elimination of culture](http://www.nickbostrom.com/fut/evolution.pdf)
- [Economic growth given machine intelligence](http://hanson.gmu.edu/aigrow.pdf)
- [Economic implications of software minds](http://intelligence.org/files/EconomicImplications.pdf)
- [Long-term growth as a sequence of exponential modes](http://hanson.gmu.edu/longgrow.pdf)
- [Is a singularity just around the corner? What it takes to get explosive economic growth](http://hanson.gmu.edu/fastgrow.html)

## See also

- [Technological singularity](https://wiki.lesswrong.com/wiki/Technological_singularity)
- [Hard takeoff](https://wiki.lesswrong.com/wiki/Hard_takeoff), [Soft takeoff](https://wiki.lesswrong.com/wiki/Soft_takeoff)
- [[Artificial General Intelligence|Artificial general intelligence]], [[Whole Brain Emulation]]
- [[Technological Forecasting|Technological forecasting]]