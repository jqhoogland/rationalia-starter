---
_id: ESkc5kecgkLF235dj
title: Acute Risk Period
href: https://www.lesswrong.com/tag/acute-risk-period
slug: acute-risk-period
type: tag
tags:
  - LessWrong
  - Concept
  - Tag
synchedAt: '2022-09-01T09:42:58.213Z'
status: todo
---

# Acute Risk Period

The Acute Risk Period is the period of human history wherein it is possible to destroy all of civilization. (i.e. we destroy ourselves in nuclear war, or build an AGI that quickly bootstraps to be much more powerful than the rest of humanity combined).

It's an important strategic consideration in the field of [[EA/Topics/Existential risk]] â€“ it's important to steer humanity safely through this period to a point where we are technologically and civilizationally mature, and have spread across the stars such that no one act could destroy everyone.

See also [[EA/Topics/Existential risk]], [[AI Risk]], and the [Most Important Century](https://www.lesswrong.com/sequences/yYxggfHYRrqnJXuRx).

*This is a stub wiki post I hope gets fleshed out soon. I thought probably this was already written up on Arbital or something but I can't find it.*


%%

% START
Basic (and reversed card)
What is **Acute Risk Period**?
Back: {TODO}
Tags: LessWrong
END
<!--ID: 1663156956466-->


%%
	
