---
_id: 5f5c37ee1b5cdee568cfb2d0
title: Emulation Argument For Human-Level AI
href: https://lesswrong.com/tag/emulation-argument-for-human-level-ai
slug: emulation-argument-for-human-level-ai
type: tag
tags:
  - LessWrong
  - Concept
  - Tag
synchedAt: '2022-08-29T11:07:15.325Z'
---

# Emulation Argument For Human-Level AI

The **Emulation argument for human-level** [**AI**](https://wiki.lesswrong.com/wiki/AGI) argues that since [[Whole Brain Emulation|whole brain emulation]] seems feasible then human-level [AI](https://wiki.lesswrong.com/wiki/AGI) must also be feasible. There are many underlying assumptions in the argument, most of them are explored by Chalmers (2010)[^1^](#fn1). Perhaps the most debated premise is holding that a brain emulation could have a consciousness mind or that consciousness isn’t fundamental to human intelligence. Chalmers [^2^](#fn2) formalized the argument as follows:”

- (i) The human brain is a machine.
- (ii) We will have the capacity to emulate this machine (before long).
- (iii) If we emulate this machine, there will be AI.
- (iv) Absent defeaters, there will be AI (before long)”

## References

1. CHALMERS, David. (2010) "The Singularity: A Philosophical Analysis, Journal of Consciousness Studies", 17 (9-10), pp. 7-65.[↩](#fnref1)
