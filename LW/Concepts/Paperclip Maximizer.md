---
tags: ['LessWrong', 'Concept']
href: http://www.lesswrong.com/tag/paperclip-maximizer
---

# Paperclip Maximizer
A Paperclip Maximizer is a hypothetical [[AI|artificial intelligence]] whose [[Utility Functions|utility function]] values something that humans would consider almost worthless, like [[Optimization|maximizing]] the number of paperclips in the universe. The paperclip maximizer is the canonical thought experiment showing how an artificial general intelligence, even one designed competently and without malice, could ultimately destroy humanity. The thought experiment shows that AIs with apparently innocuous values could pose an [[1 Projects/Learning/Rationalism/Concepts/Existential Risk|existential threat]].

The goal of maximizing paperclips is chosen for illustrative purposes because it is very unlikely to be implemented, and has little apparent danger or emotional load (in contrast to, for example, curing cancer or winning wars). This produces a thought experiment which shows the contingency of human values: An [[Really Powerful Optimization Process|extremely powerful optimizer]] (a highly intelligent agent) could seek goals that are completely alien to ours ([[Orthogonality Thesis|orthogonality thesis]]), and as a side-effect destroy us by consuming resources essential to our survival.

## Description
First described by Bostrom (2003), a paperclip maximizer is an [[1 Projects/Learning/Rationalism/Concepts/Artificial General Intelligence|artificial general intelligence]] (AGI) whose goal is to maximize the number of paperclips in its collection. If it has been constructed with a roughly human level of general intelligence, the AGI might collect paperclips, earn money to buy paperclips, or begin to manufacture paperclips....[(Read More)]()

