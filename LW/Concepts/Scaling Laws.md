---
title: Scaling Laws
href: https://lesswrong.com/tags/scaling-laws
type: tag
tags:
  - LessWrong
  - Concept
  - Tag
---

**Scaling Laws** refer to the observed trend of some machine learning architectures (notably [transformers](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model))) to scale their performance on predictable power law when given more compute, data, or parameters (model size), assuming they are not bottlenecked on one of the other resources. This has been observed as highly consistent over more than six orders of magnitude.

![](https://i.imgur.com/7lhHT8n.png)

Scaling laws graph from [Scaling Laws for Neural Language Models](https://arxiv.org/pdf/2001.08361.pdf)