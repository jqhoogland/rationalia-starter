---
_id: nzvHaqwdXtvWkbonG
title: Risks of Astronomical Suffering (S-risks)
href: https://lesswrong.com/tag/risks-of-astronomical-suffering-s-risks
type: tag
tags:
  - LessWrong
  - Concept
  - Tag
synchedAt: '2022-08-29T11:03:20.428Z'
---
# Risks of Astronomical Suffering (S-risks)

**(Astronomical) suffering risks**, also known as **s-risks**, are risks of the creation of intense suffering in the far future on an astronomical scale, vastly exceeding all suffering that has existed on Earth so far.

S-risks are an example of [[Existential Risk|existential risk]] (also known as *x-risks*) according to Nick Bostrom's original definition, as they threaten to "permanently and drastically curtail \[Earth-originating intelligent life's\] potential". Most existential risks are of the form "event E happens which drastically reduces the number of conscious experiences in the future". S-risks therefore serve as a useful reminder that some x-risks are scary because they cause *bad* experiences, and not just because they prevent good ones.

Within the space of x-risks, we can distinguish x-risks that are s-risks, x-risks involving human extinction, x-risks that involve immense suffering *and* human extinction, and x-risks that involve neither. For example:

<table><tbody><tr><td>&nbsp;</td><td><strong>extinction risk</strong></td><td><strong>non-extinction risk</strong></td></tr><tr><td><strong>suffering risk</strong></td><td>Misaligned AGI wipes out humans, simulates many suffering alien civilizations.</td><td>Misaligned AGI tiles the universe with experiences of severe suffering.</td></tr><tr><td><strong>non-suffering risk</strong></td><td>Misaligned AGI wipes out humans.</td><td>Misaligned AGI keeps humans as "pets," limiting growth but not causing immense suffering.</td></tr></tbody></table>

A related concept is [**hyperexistential risk**](https://arbital.com/p/hyperexistential_separation/), the risk of "fates worse than death" on an astronomical scale. It is not clear whether all hyperexistential risks are s-risks per se. But arguably all s-risks are hyperexistential, since "tiling the universe with experiences of severe suffering" would likely be worse than death.

There are two [EA](https://wiki.lesswrong.com/wiki/EA) organizations with s-risk prevention research as their primary focus: the [[Center on Long-Term Risk (CLR)|Center on Long-Term Risk]] (CLR) and the [Center for Reducing Suffering](https://centerforreducingsuffering.org/). Much of CLR's work is on suffering-focused [AI safety](https://wiki.lesswrong.com/wiki/AI_safety) and [[Crucial Considerations|crucial considerations]]. Although to a much lesser extent, the [[Machine Intelligence Research Institute (MIRI)|Machine Intelligence Research Institute]] and [[Future of Humanity Institute (FHI)|Future of Humanity Institute]] have investigated strategies to prevent s-risks too.Â 

Another approach to reducing s-risk is to "expand the moral circle" [*together*](https://magnusvinding.com/2018/09/04/moral-circle-expansion-might-increase-future-suffering/) with raising concern for suffering, so that future (post)human civilizations and AI are less likely to [[Instrumental Value|instrumentally]] cause suffering to non-human minds such as animals or digital sentience. [Sentience Institute](http://www.sentienceinstitute.org/) works on this value-spreading problem.

## See also

*   [[Center on Long-Term Risk (CLR)|Center on Long-Term Risk]]
*   [[Existential Risk|Existential risk]]
*   [[Abolitionism]]
*   [Mind crime](https://wiki.lesswrong.com/wiki/Mind_crime)
*   [[Utilitarianism]], [[Hedonism]]

## External links

*   [Reducing Risks of Astronomical Suffering: A Neglected Global Priority (FRI)](https://foundational-research.org/reducing-risks-of-astronomical-suffering-a-neglected-global-priority/)
*   [Introductory talk on s-risks (FRI)](https://foundational-research.org/s-risks-talk-eag-boston-2017/)
*   [Risks of Astronomical Future Suffering (FRI)](https://foundational-research.org/risks-of-astronomical-future-suffering/)
*   [Suffering-focused AI safety: Why "fail-safe" measures might be our top intervention PDF (FRI)](https://foundational-research.org/files/suffering-focused-ai-safety.pdf)
*   [Artificial Intelligence and Its Implications for Future Suffering (FRI)](https://foundational-research.org/artificial-intelligence-and-its-implications-for-future-suffering)
*   [Expanding our moral circle to reduce suffering in the far future (Sentience Politics)](https://sentience-politics.org/expanding-moral-circle-reduce-suffering-far-future/)
*   [The Importance of the Far Future (Sentience Politics)](https://sentience-politics.org/philosophy/the-importance-of-the-future/)