---
_id: 5f5c37ee1b5cdee568cfb2a7
title: Fai-Complete
href: https://lesswrong.com/tag/fai-complete
slug: fai-complete
type: tag
tags:
  - LessWrong
  - Concept
  - Tag
synchedAt: '2022-08-29T11:05:48.380Z'
status: todo
---

# Fai-Complete

A problem is **Friendly AI-complete** if solving it is equivalent to creating [Friendly AI](https://wiki.lesswrong.com/wiki/Friendly_AI).

The [[Machine Intelligence Research Institute (MIRI)|Machine Intelligence Research Institute]] argues that any safe superintelligence architecture is FAI-complete. For example, the following have been proposed as hypothetical safe AGI designs:

- [[Oracle AI]] \- an AGI which takes no action besides answering questions.
- [[Tool AI]] \- an AGI which isn't an independent decision-maker at all, but is rather "just a calculator".
- [[Nanny AI]] \- an AGI of limited superintelligence, restricted to preventing more advanced AGIs from arising until safer AGIs are developed.

In ["Dreams of Friendliness"](http://lesswrong.com/lw/tj/dreams_of_friendliness/) Eliezer Yudkowsky argues that if you have an Oracle AI, then you can ask it, "What should I do?" If it can answer this question correctly, then it is FAI-complete.

Similarly, if you have a tool AI, it must make extremely complex decisions about how many resources it can use, how to display answers in human understandable yet accurate form, *et cetera*. The many ways in which it could choose catastrophically require it to be FAI-complete. Note that this does not imply that an agent-like, fully free FAI is easier to create than any of the other proposals.

Goertzel proposed a "Nanny AI"([Should humanity build a global AI nanny to delay the singularity until it’s better understood?](http://commonsenseatheism.com/wp-content/uploads/2012/03/Goertzel-Should-Humanity-Build-a-Global-AI-Nanny-to-Delay-the-Singularity-Until-its-Better-Understood.pdf)) with moderate superhuman intelligence, able to forestall Singularity eternally, or to delay it. However, it has been argued by Luke Muehlhauser and Anna Salamon ([Intelligence Explosion: Evidence and Import](http://intelligence.org/files/IE-EI.pdf)) that a Nanny AI is FAI-complete. They claim that building it could require solving all the problems required to build [[Friendly Artificial Intelligence]].

## Blog Posts

- [Dreams of Friendliness](http://lesswrong.com/lw/tj/dreams_of_friendliness/)
- [Reply to Holden on 'Tool AI'](http://lesswrong.com/lw/cze/reply_to_holden_on_tool_ai/)
- [A Taxonomy of Oracle AIs](http://lesswrong.com/lw/any/a_taxonomy_of_oracle_ais/)

## References

- [Should humanity build a global AI nanny to delay the singularity until it’s better understood?](http://commonsenseatheism.com/wp-content/uploads/2012/03/Goertzel-Should-Humanity-Build-a-Global-AI-Nanny-to-Delay-the-Singularity-Until-its-Better-Understood.pdf) by Ben Goertzel
- [Intelligence Explosion: Evidence and Import](http://intelligence.org/files/IE-EI.pdf) by Luke Muehlhauser and Anna Salamon

## See Also

- [FAI](https://wiki.lesswrong.com/wiki/FAI)
- [[AI-Complete|AI-complete]] (`= [[AI-Complete|AI-complete]].status`)
