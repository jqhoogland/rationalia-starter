---
_id: BXL4riEJvJJHoydjG
title: Orthogonality Thesis
href: https://lesswrong.com/tag/orthogonality-thesis
slug: orthogonality-thesis
type: tag
tags:
  - LessWrong
  - Concept
  - Tag
synchedAt: '2022-08-29T11:05:29.246Z'
---

# Orthogonality Thesis

The **Orthogonality Thesis** states that an agent can have any combination of intelligence level and final goal, that is, its [[Utility Functions|final goals]] and [[General Intelligence|intelligence levels]] can vary independently of each other. This is in contrast to the belief that, because of their intelligence, AIs will all converge to a common goal.

The thesis was originally defined by [Nick Bostrom](https://lessestwrong.com/tag/nick-bostrom) in the paper "[Superintelligent Will](https://nickbostrom.com/superintelligentwill.pdf)", (along with the [instrumental convergence thesis](https://wiki.lesswrong.com/wiki/instrumental_convergence_thesis)). For his purposes, Bostrom defines intelligence to be [instrumental rationality](https://wiki.lesswrong.com/wiki/instrumental_rationality).

*Related:* *[[Complexity of Value|Complexity of Value]]**,* *[[Decision Theory|Decision Theory]]**,* *[[General Intelligence|General Intelligence]]**,* *[[Utility Functions|Utility Functions]]*

## Defense of the Thesis

It has been pointed out that the orthogonality thesis is the default position, and that the burden of proof is on claims that limit possible AIs. Stuart Armstrong writes that,

One reason many researchers assume superintelligent agents to converge to the same goals may be because [most humans](https://lessestwrong.com/tag/human-universal) have similar values. Furthermore, many philosophies hold that there is a rationally correct morality, which implies that a sufficiently rational AI will acquire this morality and begin to act according to it. Armstrong points out that for formalizations of AI such as [AIXI](https://lessestwrong.com/tag/aixi) and [Gödel machines](https://lessestwrong.com/tag/g%C3%B6del-machine), the thesis is known to be true. Furthermore, if the thesis was false, then [Oracle AIs](https://lessestwrong.com/tag/oracle-ai) would be impossible to build, and all sufficiently intelligent AIs would be impossible to control.

## Pathological Cases

There are some pairings of intelligence and goals which cannot exist. For instance, an AI may have the goal of using as little resources as possible, or simply of being as unintelligent as possible. These goals will inherently limit the degree of intelligence of the AI.

## See Also

- [Instrumental Convergence](https://lessestwrong.com/tag/instrumental-convergence)

## External Links

- Definition of the orthogonality thesis from Bostrom's [Superintelligent Will](http://www.nickbostrom.com/superintelligentwill.pdf)
- [Arbital orthogonality thesis article ](https://arbital.com/p/orthogonality/)
- [Critique](http://philosophicaldisquisitions.blogspot.com/2012/04/bostrom-on-superintelligence-and.html) of the thesis by John Danaher
- Superintelligent Will paper by Nick Bostrom
