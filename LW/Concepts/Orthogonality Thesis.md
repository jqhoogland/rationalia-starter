---
tags: ['LessWrong', 'Portal', 'Concept']
href: https://www.lesswrong.com/tag/orthogonality-thesis/
---

The Orthogonality Thesis states that an artificial intelligence can have any combination of intelligence level and goal, that is, its [final goals](https://www.lesswrong.com/tag/utility-functions?showPostCount=true&useTagName=true) and [intelligence levels](https://www.lesswrong.com/tag/general-intelligence?showPostCount=true&useTagName=true) can vary independently of each other. This is in contrast to the belief that, because of their intelligence, AIs will all converge to a common goal. The thesis was originally defined by [Nick Bostrom](https://lessestwrong.com/tag/nick-bostrom) in the paper "Superintelligent Will", (along with the [instrumental convergence thesis](https://wiki.lesswrong.com/wiki/instrumental_convergence_thesis)). For his purposes, Bostrom defines intelligence to be [instrumental rationality](https://wiki.lesswrong.com/wiki/instrumental_rationality).

*Related: *[*Complexity of Value*](https://www.lesswrong.com/tag/complexity-of-value?showPostCount=true&useTagName=true)*, *[*Decision Theory*](https://www.lesswrong.com/tag/decision-theory?showPostCount=true&useTagName=true)*, *[*General Intelligence*](https://www.lesswrong.com/tag/general-intelligence?showPostCount=true&useTagName=true)*, *[*Utility Functions*](https://www.lesswrong.com/tag/utility-functions?showPostCount=true&useTagName=true)

## Defense of the thesis
It has been pointed out that the orthogonality thesis is the default position, and that the burden of proof is on claims that limit possible AIs. Stuart Armstrong writes that,...[(Read More)]()



---

