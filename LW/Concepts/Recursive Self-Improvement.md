---
tags: ['LessWrong', 'Concept']
href: https://www.lesswrong.com/tag/recursive-self-improvement
---

# Recursive Self-Improvement
Recursive self-improvement refers to the property of making improvements on one's own ability of making self-improvements. It is an approach to [[1 Projects/Learning/Rationalism/Concepts/Artificial General Intelligence]] that allows a system to make adjustments to its own functionality resulting in improved performance. The system could then feedback on itself with each cycle reaching ever higher levels of intelligence resulting in either a hard or soft [[AI Takeoff|AI takeoff]].

An agent can self-improve and get a linear succession of improvements, however if it is able to improve its ability of making self-improvements, then each step will yield exponentially more improvements then the next one.

## 
Recursively self-improving AI is considered to be the push behind the [[Intelligence Explosion|intelligence explosion]]. While any sufficiently intelligent AI will be able to improve itself,[[Seed AI|Seed AIs]] are specifically designed to use recursive self-improvement as their primary method of gaining intelligence. Architectures that had not been designed with this goal in mind, such as neural networks or large "hand-coded" projects like [[Cyc]], would have a harder time self-improving....[(Read More)]()

