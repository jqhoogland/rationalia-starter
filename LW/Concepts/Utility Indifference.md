---
_id: 5f5c37ee1b5cdee568cfb29b
title: Utility Indifference
href: https://www.lesswrong.com/tag/utility-indifference
slug: utility-indifference
type: tag
tags:
  - LessWrong
  - Concept
  - Tag
synchedAt: '2022-09-01T09:42:19.219Z'
status: todo
---

# Utility Indifference

**Utility indifference** is a type of defense against an AI becoming [unfriendly](https://wiki.lesswrong.com/wiki/Unfriendly_AI).

In creating an [AGI](https://wiki.lesswrong.com/wiki/AGI), a [[Utility Functions|utility function]] is explicitly or implicitly chosen. Imagine we set up a safeguard against the AGI acting against our intentions. Perhaps we surround the computer with explosives, so that we may destroy the AGI if it misbehaves. A sufficiently advanced AGI will realize this, and will quickly act to disarm the explosives. One way to prevent this would be to design its utility function so that it was indifferent to the explosives going off. That is, in any situation, the utility of the explosives going off would be equal to the utility if they did not.

Researcher Stuart Armstrong, of the [[Future of Humanity Institute (FHI)|Future of Humanity Institute]], has published mathematical models of this idea.

## Blog Posts

- [AI indifference through utility manipulation](http://lesswrong.com/lw/2nw/ai_indifference_through_utility_manipulation/)
- [Trapping AIs via utility indifference](http://lesswrong.com/lw/ae5/trapping_ais_via_utility_indifference/)

## External Links

- [Utility Indifference](http://www.fhi.ox.ac.uk/reports/2010-1.pdf) by Stuart Armstrong


%%

% START
Basic (and reversed card)
What is **Utility Indifference**?
Back: {TODO}
Tags: LessWrong
END

%%
	
