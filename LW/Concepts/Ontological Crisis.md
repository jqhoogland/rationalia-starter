---
_id: 5f5c37ee1b5cdee568cfb2c2
title: Ontological Crisis
href: https://lesswrong.com/tag/ontological-crisis
type: tag
tags:
  - LessWrong
  - Concept
  - Tag
synchedAt: '2022-08-29T10:47:36.565Z'
---
# Ontological Crisis

**Ontological crisis** is a term coined to describe the crisis an agent, human or not, goes through when its model - its ontology - of reality changes.

In the human context, a clear example of an ontological crisis is a believer’s loss of faith in God. Their motivations and goals, coming from a very specific view of life suddenly become obsolete and maybe even nonsense in the face of this new configuration. The person will then experience a deep crisis and go through the psychological task of reconstructing its set of preferences according the new world view.

When dealing with artificial agents, we, as their creators, are directly interested in their goals. That is, as Peter de Blanc puts it, when we create something we want it to be useful. As such we will have to define the artificial agent’s ontology – but since a fixed ontology severely limits its usefulness we have to think about adaptability. In his 2011 paper, the author then proposes a method to map old ontologies into new ones, thus adapting the agent’s utility functions and avoiding a crisis.

This crisis, in the context of an [AGI](https://wiki.lesswrong.com/wiki/AGI), could in the worst case pose an [[Existential Risk|existential risk]] when old preferences and goals continue to be used. Another possibility is that the AGI loses all ability to comprehend the world, and would pose no threat at all. If an AGI reevaluates its preferences after its ontological crisis, for example in the way mentioned above, very [[Unfriendly Artificial Intelligence|unfriendly]] behaviors could arise. Depending on the extent of the reevaluations, the AGI's changes may be detected and safely fixed. On the other hand, it could go undetected until they go wrong - which shows how it is of our interest to deeply explore ontological adaptation methods when designing AI.

## Further Reading & References

- [Ontological Crises in Artificial Agents' Value Systems](http://arxiv.org/abs/1105.3821) by Peter de Blanc

## Notable Posts

- [AI ontology crises: an informal typology](http://lesswrong.com/r/discussion/lw/827/ai_ontology_crises_an_informal_typology/) by Stuart Armstrong
- [Eutopia is Scary](http://lesswrong.com/lw/xl/eutopia_is_scary/) by Eliezer Yudkowsky
- [Ontological Crisis in Humans](http://lesswrong.com/lw/fyb/ontological_crisis_in_humans/) by Wei Dai

## See also

- [[Evolution]]
- [Adaptation executers](https://wiki.lesswrong.com/wiki/Adaptation_executers)