---
_id: KmgkrftQuX7jmjjp5
title: Language Models
href: https://lesswrong.com/tag/language-models
slug: language-models
type: tag
tags:
  - LessWrong
  - Concept
  - Tag
synchedAt: '2022-08-29T10:47:20.501Z'
---
# Language Models

**Language Models** are a class of [[AI]] trained on text, usually to predict the next word or a word which has been obscured. They have the ability to generate novel prose or code based on an initial prompt, which gives rise to a kind of natural language programming called prompt engineering. The most popular architecture for very large language models is called a [transformer](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)), which follows consistent [[Scaling Laws|scaling laws]] with respect to the size of the model being trained, meaning that a larger model trained with the same amount of compute will produce results which are better by a predictable amount (when measured by the 'perplexity', or how surprised the AI is by a test set of human-generated text).

### See also

- [[GPT]] \- A family of large language models created by [[OpenAI]]