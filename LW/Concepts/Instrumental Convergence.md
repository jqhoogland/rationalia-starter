---
_id: xHTXnyp65X8YX6ahT
title: Instrumental Convergence
href: https://lesswrong.com/tag/instrumental-convergence
slug: instrumental-convergence
type: tag
tags:
  - LessWrong
  - Concept
  - Tag
synchedAt: '2022-08-29T11:05:20.369Z'
status: todo
---

# Instrumental Convergence

**Instrumental convergence** or **convergent instrumental values** is the theorized tendency for most sufficiently intelligent agents to pursue potentially unbounded instrumental goals such as self-preservation and resource acquisition \[[1](https://en.wikipedia.org/wiki/Instrumental_convergence)\]. This concept has also been discussed under the term *basic drives.*

The idea was first explored by [Steve Omohundro](https://en.wikipedia.org/wiki/Steve_Omohundro). He argued that sufficiently advanced AI systems would all naturally discover similar instrumental subgoals. The view that there are important basic AI drives was subsequently defended by [Nick Bostrom](https://lessestwrong.com/tag/nick-bostrom) as the *instrumental convergence thesis*, or the *convergent instrumental goals thesis*. On this view, a few goals are [instrumental](https://lessestwrong.com/tag/instrumental-value) to almost all possible [final](https://lessestwrong.com/tag/terminal-value) goals. Therefore, all advanced AIs will pursue these instrumental goals. Omohundro uses microeconomic theory by von Neumann to support this idea.

## Omohundro’s Drives

Omohundro presents two sets of values, one for self-improving artificial intelligences [1](http://selfawaresystems.files.wordpress.com/2008/01/nature_of_self_improving_ai.pdf) and another he says will emerge in any sufficiently advanced AGI system [2](http://selfawaresystems.files.wordpress.com/2008/01/ai_drives_final.pdf). The former set is composed of four main drives:

- **Self-preservation**: A sufficiently advanced AI will probably be the best entity to achieve its goals. Therefore it must continue existing in order to maximize goal fulfillment. Similarly, if its goal system were modified, then it would likely begin pursuing different ends. Since this is not desirable to the current AI, it will act to preserve the content of its goal system.
- **Efficiency**: At any time, the AI will have finite resources of time, space, matter, energy and computational power. Using these more efficiently will increase its utility. This will lead the AI to do things like implement more efficient algorithms, physical embodiments, and particular mechanisms. It will also lead the AI to replace desired physical events with computational simulations as much as possible, to expend fewer resources.
- **Acquisition**: Resources like matter and energy are indispensable for action. The more resources the AI can control, the more actions it can perform to achieve its goals. The AI's physical capabilities are determined by its level of technology. For instance, if the AI could invent nanotechnology, it would vastly increase the actions it could take to achieve its goals.
- **Creativity**: The AI's operations will depend on its ability to come up with new, more efficient ideas. It will be driven to acquire more computational power for raw searching ability, and it will also be driven to search for better search algorithms. Omohundro argues that the drive for creativity is critical for the AI to display the richness and diversity that is valued by humanity. He discusses [signaling](https://lessestwrong.com/tag/signaling) goals as particularly rich sources of creativity.

## Bostrom’s Drives

Bostrom argues for an [orthogonality thesis](https://lessestwrong.com/tag/orthogonality-thesis): But he also argues that, despite the fact that values and intelligence are independent, any recursively self-improving intelligence would likely possess a particular set of instrumental values that are useful for achieving any kind of [terminal value](https://lessestwrong.com/tag/terminal-value).[3](http://www.nickbostrom.com/superintelligentwill.pdf) On his view, those values are:

- **Self-preservation**: A superintelligence will value its continuing existence as a means to to continuing to take actions that promote its values.
- **Goal-content integrity**: The superintelligence will value retaining the same preferences over time. Modifications to its future values through swapping memories, downloading skills, and altering its cognitive architecture and personalities would result in its transformation into an agent that no longer optimizes for the same things.
- **Cognitive enhancement**: Improvements in cognitive capacity, intelligence and rationality will help the superintelligence make better decisions, furthering its goals more in the long run.
- **Technological perfection**: Increases in hardware power and algorithm efficiency will deliver increases in its cognitive capacities. Also, better engineering will enable the creation of a wider set of physical structures using fewer resources (e.g., [nanotechnology](https://lessestwrong.com/tag/nanotechnology)).
- **Resource acquisition**: In addition to guaranteeing the superintelligence's continued existence, basic resources such as time, space, matter and free energy could be processed to serve almost any goal, in the form of extended hardware, backups and protection.

## Relevance

Both Bostrom and Omohundro argue these values should be used in trying to predict a superintelligence's behavior, since they are likely to be the only set of values shared by most superintelligences. They also note that these values are consistent with safe and beneficial AIs as well as unsafe ones.

Bostrom emphasizes, however, that our ability to predict a superintelligence's behavior may be very limited even if it shares most intelligences' instrumental goals.

Yudkowsky echoes Omohundro's point that the convergence thesis is consistent with the possibility of Friendly AI. However, he also notes that the convergence thesis implies that most AIs will be extremely dangerous, merely by being indifferent to one or more human values:[4](http://intelligence.org/2013/05/05/five-theses-two-lemmas-and-a-couple-of-strategic-implications/)

## Pathological Cases

In some rarer cases, AIs may not pursue these goals. For instance, if there are two AIs with the same goals, the less capable AI may determine that it should destroy itself to allow the stronger AI to control the universe. Or an AI may have the goal of using as few resources as possible, or of being as unintelligent as possible. These relatively specific goals will limit the growth and power of the AI.

## See Also

- [Convergent instrumental strategies](https://arbital.com/p/convergent_strategies/) ([Arbital](https://wiki.lesswrong.com/wiki/Arbital))
- [Instrumental convergence](https://arbital.com/p/instrumental_convergence/) ([Arbital](https://wiki.lesswrong.com/wiki/Arbital))
- [Orthogonality thesis](https://lessestwrong.com/tag/orthogonality-thesis)
- [Cox's theorem](https://wiki.lesswrong.com/wiki/Cox's_theorem)
- [Unfriendly AI](https://wiki.lesswrong.com/wiki/Unfriendly_AI), [Paperclip maximizer](https://lessestwrong.com/tag/paperclip-maximizer), [Oracle AI](https://lessestwrong.com/tag/oracle-ai)
- [Instrumental values](https://wiki.lesswrong.com/wiki/Instrumental_values)

## References

- Omohundro, S. (2007). [*The Nature of Self-Improving Artificial Intelligence*](http://selfawaresystems.files.wordpress.com/2008/01/nature_of_self_improving_ai.pdf).
- Omohundro, S. (2008). "[The Basic AI Drives](http://selfawaresystems.com/2007/11/30/paper-on-the-basic-ai-drives/)". *Proceedings of the First AGI Conference*.
- Omohundro, S. (2012). [*Rational Artificial Intelligence for the Greater Good*](http://selfawaresystems.files.wordpress.com/2012/03/rational_ai_greater_good.pdf).
- Bostrom, N. (2012). "[The Superintelligent Will: Motivation and Instrumental Rationality in Advanced Artificial Agents](http://www.nickbostrom.com/superintelligentwill.pdf)". *Minds and Machines*.
- Shulman, C. (2010). [*Omohundro's "Basic AI Drives" and Catastrophic Risks*](http://intelligence.org/files/BasicAIDrives.pdf).
