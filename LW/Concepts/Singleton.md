---
tags: ['LessWrong', 'Portal', 'Concept']
href: https://www.lesswrong.com/tag/singleton
---

A singleton refers to a world order in which there is a single decision-making agency at the highest level, capable of exerting effective control over its domain and preventing internal or external threats to its supremacy. An [[1 Projects/Learning/Rationalism/Concepts/Artificial General Intelligence|artificial general intelligence]] having undergone an [[Intelligence Explosion|intelligence explosion]] could form a singleton, as could a world government armed with mind control and social surveillance technologies. A singleton doesn't have to support a civilization, and in fact may obliterate it upon coming to power.

A singleton need not directly micromanage everything in its domain; it could allow diverse forms of organization within itself, albeit guaranteed to function within strict limits. Notably, a suitable singleton could solve world coordination problems that would not otherwise be solvable, opening up otherwise unavailable developmental trajectories for civilization. For example, such a singleton could hold Darwinian evolutionary pressures in check.

Like any technological advancement, a singleton could be good or bad, depending on what it is used for. It would provide strong stability, but could also restrict personal freedoms. It would nullify an arms race, but it would remove choice between ruling systems. It could be an [UFAI](https://wiki.lesswrong.com/wiki/UFAI), or it could be a [FAI](https://wiki.lesswrong.com/wiki/FAI) which prevents further UFAIs.

Bostrom postulates the singleton hypothesis; Earth-originating life will eventually form a singleton. He notes that this follows the historical trend of increasingly high levels of organization in the history of life and humanity. Also in favor of the hypothesis is the rise of technologies capable of aiding the creation of a singleton, such as world-wide communication, and potentially [AGI](https://wiki.lesswrong.com/wiki/AGI).

## See also
-[[1 Projects/Learning/Rationalism/Concepts/Existential Risk|Existential risk]]
-[[Friendly Artificial Intelligence|Friendly]]
-  and 
-[[Unfriendly Artificial Intelligence|Unfriendly]]
-  artificial intelligence

## References
- Nick Bostrom (2006). "
- [What is a Singleton?](http://www.nickbostrom.com/fut/singleton.html)
- ". 
- *Linguistic and Philosophical Investigations*
-  
- 5
-  (2): 48-54.
- Nick Bostrom (2004). "
- [The Future of Human Evolution](http://www.nickbostrom.com/fut/evolution.html)
- ". 
- *Death and Anti-Death: Two Hundred Years After Kant, Fifty Years After Turing*
- : 339-371.



---

