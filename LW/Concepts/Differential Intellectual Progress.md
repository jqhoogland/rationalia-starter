---
_id: 5f5c37ee1b5cdee568cfb2dd
title: Differential Intellectual Progress
href: https://lesswrong.com/tag/differential-intellectual-progress
type: tag
tags:
  - LessWrong
  - Concept
  - Tag
synchedAt: '2022-08-29T11:05:40.159Z'
---
# Differential Intellectual Progress

**Differential intellectual progress** was defined by [Luke Muehlhauser and Anna Salamon](http://web.archive.org/web/20190430130748/http://intelligence.org/files/IE-EI.pdf) as "prioritizing risk-reducing intellectual progress over risk-increasing intellectual progress". They discuss differential intellectual progress in relation to [[Artificial General Intelligence]] (AGI) development (which will also be the focus of this article):

> As applied to AI risks in particular, a plan of differential intellectual progress would recommend that our progress on the philosophical, scientific, and technological problems of AI _safety_ outpace our progress on the problems of AI _capability_ such that we develop _safe_ superhuman AIs before we develop arbitrary superhuman AIs.

Muehlhauser and Salamon also note that [differential technological development](https://en.wikipedia.org/wiki/Differential_technological_development) can be seen as a special case of this concept.

## Risk-increasing Progress

Technological advances — without corresponding development of safety mechanisms — simultaneously increase the capacity for both [friendly](https://wiki.lesswrong.com/wiki/Friendly_AI) and [unfriendly](https://wiki.lesswrong.com/wiki/Unfriendly_AI) AGI development. Presently, most AGI research is concerned with increasing its _capacity_ rather than its _safety_ and thus, most progress increases the risk for a [[Existential Risk|widespread negative effect]].

*   _Increased computing power._ Computing power continues to rise in step with [Moore's Law](http://www.intel.com/content/www/us/en/silicon-innovations/moores-law-technology.html), providing the raw capacity for smarter AGIs. This allows for more ['brute-force'](http://dictionary.reference.com/browse/brute+force) programming, increasing the probability of someone creating an AGI without properly understanding it. Such an AGI would also be harder to control.

*   _More efficient algorithms._ Mathematical advances can produce [substantial reductions](http://users.ece.gatech.edu/~mrichard/Richards%26Shaw_Algorithms01204.pdf) in computing time, allowing an AGI to be more efficient within its current operating capacity. The ability to carry about a larger number of computations with the same amount of hardware would have the net effect of making the AGI smarter.

*   _Extensive datasets._ Living in the ['Information Age'](http://en.wikipedia.org/wiki/Information_Age) has produced immense amounts of data. As [data storage capacity](http://www.scientificamerican.com/article.cfm?id=kryders-law) has increased, so has the amount of information that is collected and stored, allowing an AGI immediate access to massive amounts of knowledge.

*   _Advanced neuroscience._ Cognitive scientists have discovered several algorithms used by the human brain which contribute to our intelligence, leading to a field called ['Computational Cognitive Neuroscience.'](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3153062/) This has led to developments such as brain implants that have helped restore [memory](http://www.nytimes.com/2011/06/17/science/17memory.html) and [motor learning](http://www.popsci.com/science/article/2011-09/israeli-researchers-build-rat-cyborg-packing-digitally-derived-cerebellum) in animals, algorithms which might conceivably contribute to AGI development.

The above developments could also help in the creation of [Friendly AI](https://wiki.lesswrong.com/wiki/Friendly_AI). However, Friendliness requires the development of both AGI and Friendliness theory, while an [Unfriendly Artificial Intelligence](https://wiki.lesswrong.com/wiki/Unfriendly_AI) might be created by AGI efforts alone. Thus developments that bring AGI closer or make it more powerful will increase risk, at least if not combined with work on Friendliness.

## Risk-reducing Progress

There are several areas which, when more developed, will provide a means to produce AGIs that are friendly to humanity. These areas of research should be prioritized to prevent possible disasters.

*   _Computer security._ One way by which AGIs might grow rapidly more powerful is by taking over poorly-protected computers on the Internet. Hardening computers and networks against such attacks would help reduce this risk.

*   _AGI confinement._ Incorporating physical mechanisms which limit the AGI can prevent it from inflicting damage. Physical isolation has already been developed (such as [[AI Boxing (Containment)|AI Boxing]]) as well as embedded solutions which shut down parts of the system under certain conditions.

*   _Friendly AGI goals._ Embedding an AGI with friendly [[Terminal Value|terminal values]] reduces the risk that it will take action that is harmful to humanity. [Development](http://lukeprog.com/SaveTheWorld.html#goals) in this area has lead to many questions about what _should_ be implemented. However, precise methodologies which, when executed within an AGI, would prevent it from harming humanity have not yet materialized.

## See Also

*   [AGI](https://wiki.lesswrong.com/wiki/AGI)
*   [Friendly AI](https://wiki.lesswrong.com/wiki/Friendly_AI)
*   [Unfriendly AI](https://wiki.lesswrong.com/wiki/Unfriendly_AI)
*   [[AI Boxing (Containment)|AI Boxing]]

## References

*   [Intelligence Explosion: Evidence and Import](http://web.archive.org/web/20190430130748/http://intelligence.org/files/IE-EI.pdf) by Luke Muehlhauser and Anna Salamon
*   [Why We Need Friendly AI](http://www.preventingskynet.com/why-we-need-friendly-ai/) by Eliezer Yudkowsky
*   [Safety Engineering for Artificial General Intelligence](http://intelligence.org/files/SafetyEngineering.pdf) by Roman Yampolskiy and Joshua Fox
*   [Leakproofing the Singularity](http://cecs.louisville.edu/ry/LeakproofingtheSingularity.pdf) by Roman Yampolskiy