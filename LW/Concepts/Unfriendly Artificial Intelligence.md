---
_id: 5f5c37ee1b5cdee568cfb119
title: Unfriendly Artificial Intelligence
href: https://lesswrong.com/tag/unfriendly-artificial-intelligence
type: tag
tags:
  - LessWrong
  - Concept
  - Tag
synchedAt: '2022-08-29T11:05:44.922Z'
---
# Unfriendly Artificial Intelligence

An **Unfriendly artificial intelligence** (or **UFAI**) is an [[Artificial General Intelligence|artificial general intelligence]] capable of causing [[Existential Risk|great harm]] to humanity, and having goals that [make it useful](https://wiki.lesswrong.com/wiki/Instrumental_values) for the AI to do so. The AI's goals don't need to be antagonistic to humanity's goals for it to be Unfriendly; there are [[Instrumental Convergence|strong reasons]] to expect that almost any powerful AGI not explicitly programmed to be benevolent to humans is lethal. A [[Paperclip Maximizer|paperclip maximizer]] is often imagined as an illustrative example of an unFriendly AI indifferent to humanity. An AGI specifically designed to have a positive effect on humanity is called a [Friendly AI](https://wiki.lesswrong.com/wiki/Friendly_AI).

See also
--------

*   [[Mind Design Space|Mind design space]], [[Magical Categories|magical categories]]
*   [[Really Powerful Optimization Process|Really powerful optimization process]]
*   [[Instrumental Convergence|Basic AI drives]]
*   [[Paperclip Maximizer|Paperclip maximizer]]
*   [[Existential Risk|Existential risk]]
*   [Friendly AI](https://wiki.lesswrong.com/wiki/Friendly_AI)

References
----------

*   Eliezer S. Yudkowsky (2008). "[Artificial Intelligence as a Positive and Negative Factor in Global Risk](https://yudkowsky.net/singularity/ai-risk/)". Global Catastrophic Risks. Oxford University Press. ([PDF](http://intelligence.org/files/AIPosNegFactor.pdf))
*   Stephen M. Omohundro (2008). "[The Basic AI Drives](https://selfawaresystems.com/2007/11/30/paper-on-the-basic-ai-drives/)". Frontiers in Artificial Intelligence and Applications (IOS Press). ([PDF](http://selfawaresystems.files.wordpress.com/2008/01/ai_drives_final.pdf))