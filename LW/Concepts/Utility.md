---
_id: 5f5c37ee1b5cdee568cfb197
title: Utility
href: https://lesswrong.com/tag/utility
slug: utility
type: tag
tags:
  - LessWrong
  - Concept
  - Tag
synchedAt: '2022-08-29T11:08:25.073Z'
status: todo
---

# Utility

**Utility** is how much a certain outcome satisfies an agent’s preferences. Its unit – the **util** or **utilon** – is an abstract arbitrary measure that assumes a concrete value only when the agent’s preferences have been determined through a [[Utility Functions|utility function]].

The concept of utility stems from economics and [[Game Theory|game theory]], where it measures how much a certain commodity increases welfare. One of the clearest examples is money: the price that a person is willing to pay for something can be considered a measure of the strength of his or her preference for it. Thus, a willingness to pay a high sum for something implies that the person has a strong desire for it, i.e. it has a high utility for him or her.

Although it has been argued that utility is hard to quantify in the case of humans - mainly due to the complexity of the causal roles played by preferences and motivations – utility-based agents are quite common in AI systems. Examples include [navigation systems](http://u.cs.biu.ac.il/~meshulr1/meshulam05.pdf) or [automated resources allocation models](http://www.diee.unica.it/biomed05/pdf/W22-104.pdf), where the agent has to choose the best action based on its expected utility.

Some people prefer to keep [a distinction between two types of utility](https://www.lesswrong.com/posts/DQ4pyHoAKpYutXwSr/underappreciated-points-about-utility-functions-of-both): *utility as in decision theory* refers to the theoretical construct which represents a single agent's preferences, as defined by the [[VNM Theorem]] or other decision-theoretic representation theorems (such as [Savage](https://www.lesswrong.com/posts/5J34FAKyEmqKaT7jt/a-summary-of-savage-s-foundations-for-probability-and) or [Jeffrey-Bolker](https://plato.stanford.edu/entries/decision-theory/#JefThe)), and *utility as in utilitarianism*, a cross-agent notion of welfare intended to capture ethical reasoning. One reason for keeping the two distinct is that [utility functions are not comparable](https://www.lesswrong.com/posts/cYsGrWEzjb324Zpjx/comparing-utilities), which means it is unclear how to use single-agent utility as a cross-agent concept. Another reason to keep the two concepts separate is that a utilitarian may have a concept of welfare of an agent which differs from an agent's own preferences. For example, hedonic utilitarians may say that an agent would be better off if it were happier, even if that agent prefers to be sad.

## Further Reading & References

- [Mistakes in Choice-Based Welfare Analysis](http://elsa.berkeley.edu/~botond/mistakeschicago.pdf) by Botond Köszegi and Matthew Rabin
- Russell, Stuart J.; Norvig, Peter (2003), Artificial Intelligence: A Modern Approach (2nd ed.), Upper Saddle River, New Jersey: Prentice Hall, ISBN 0-13-790395-2

## Notable Posts

- [Purchase Fuzzies and Utilons Separately](http://lesswrong.com/lw/6z/purchase_fuzzies_and_utilons_separately/)
- [Post your Utility Function](http://lesswrong.com/lw/zv/post_your_utility_function/)
- [Applying utility functions to humans considered harmful](http://lesswrong.com/lw/1qk/applying_utility_functions_to_humans_considered/)
- [Do Humans Want Things?](http://lesswrong.com/lw/6da/do_humans_want_things/)
- [Money: The Unit of Caring](http://lesswrong.com/lw/65/money_the_unit_of_caring/)
- [Pinpointing Utility](https://www.lesswrong.com/posts/CQkGJ2t5Rw8GcZKJm/pinpointing-utility)

## See Also

- [[Utilitarianism]]
- [[Utility Functions|Utility function]]
- [[Utility Extraction|Utility extraction]]
- [[Expected Utility|Expected utility]]
- [[The Utility Function Is Not Up For Grabs|The utility function is not up for grabs]]
- [[Preference]]
- [[Game Theory|Game theory]]
