---
title: AI Alignment Fieldbuilding
href: https://lesswrong.com/tag/ai-alignment-fieldbuilding
type: tag
tags:
  - LessWrong
  - Concept
  - Tag
---

**AI Alignment Fieldbuilding** is the effort to improve the alignment ecosystem. Some priorities include introducing new people to the importance of AI risk, on-boarding them by connecting them with key resources and ideas, educating them on existing literature and methods for generating new and valuable research, supporting people who are contributing, and maintaining and improving the funding systems.

There is an invite-only Slack for people working on the alignment ecosystem. If you'd like to join message [plex](https://www.lesswrong.com/users/ete) with an overview of your involvement.