---
title: General Alignment Properties
href: https://lesswrong.com/tags/general-alignment-properties
type: tag
tags:
  - LessWrong
  - Concept
  - Tag
---

The AI alignment properties of agents which would be interesting to a range of principals trying to solve AI alignment. For example:

*   Does the AI "care" about reality, or just about its sensory observations?
*   Does the AI properly navigate [ontological shifts](https://arbital.com/p/ontology_identification/)?
*   Does the AI reason about itself as embedded in its environment?