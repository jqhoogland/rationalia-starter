---
_id: woeLa6nvmhroCh3Fi
title: General Alignment Properties
href: https://lesswrong.com/tag/general-alignment-properties
type: tag
tags:
  - LessWrong
  - Concept
  - Tag
synchedAt: '2022-08-29T10:47:13.954Z'
---
# General Alignment Properties

The AI alignment properties of agents which would be interesting to a range of principals trying to solve AI alignment. For example:

- Does the AI "care" about reality, or just about its sensory observations?
- Does the AI properly navigate [ontological shifts](https://arbital.com/p/ontology_identification/)?
- Does the AI reason about itself as embedded in its environment?