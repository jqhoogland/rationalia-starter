---
_id: sYm3HiWcfZvrGu3ui
title: AI
href: https://www.lesswrong.com/tag/ai
slug: ai
type: tag
tags:
  - LessWrong
  - Concept
  - Tag
core: true
synchedAt: '2022-09-01T09:42:58.580Z'
status: todo
---

# AI

**Artificial Intelligence** is the study of creating intelligence in algorithms. On LessWrong, the primary focus of AI discussion is to ensure that as humanity builds increasingly powerful AI systems, the outcome will be good. The central concern is that a powerful enough AI, if not designed and implemented with sufficient understanding, would optimize something unintended by its creators and pose an existential threat to the future of humanity. This is known as the *AI alignment* problem.

Common terms in this space are *superintelligence, AI Alignment, AI Safety, Friendly AI, Transformative AI, human-level-intelligence, AI Governance, and Beneficial AI.* This entry and the associated tag roughly encompass all of these topics: anything part of the broad cluster of understanding AI and its future impacts on our civilization deserves this tag.

**AI Alignment**

There are narrow conceptions of alignment, where you’re trying to get it to do something like cure Alzheimer’s disease without destroying the rest of the world. And there’s much more ambitious notions of alignment, where you’re trying to get it to do the right thing and achieve a happy intergalactic civilization.

But both the narrow and the ambitious alignment have in common that you’re trying to have the AI do that thing rather than making a lot of paperclips.

See also [[General Intelligence]].

<table style="background-color:rgb(255, 255, 255);border:10px solid #f8f8f8"><tbody><tr><td style="border:1px solid hsl(0, 0%, 100%);padding:10px;vertical-align:top;width:33.33%" rowspan="2"><p><strong>Basic Alignment Theory</strong></p><p><a href="https://www.lesswrong.com/tag/aixi?showPostCount=true&amp;useTagName=true">AIXI</a><br><a href="http://www.lesswrong.com/tag/coherent-extrapolated-volition?showPostCount=true&amp;useTagName=true">Coherent Extrapolated Volition</a><br><a href="https://www.lesswrong.com/tag/complexity-of-value?showPostCount=true&amp;useTagName=true">Complexity of Value</a><br><a href="https://www.lesswrong.com/tag/corrigibility?showPostCount=true&amp;useTagName=true">Corrigibility</a><br><a href="https://www.lesswrong.com/tag/decision-theory?showPostCount=true&amp;useTagName=true">Decision Theory</a><br><a href="https://www.lesswrong.com/tag/embedded-agency?showPostCount=true&amp;useTagName=true">Embedded Agency</a><br><a href="https://www.lesswrong.com/tag/fixed-point-theorems?showPostCount=true&amp;useTagName=true">Fixed Point Theorems</a><br><a href="https://www.lesswrong.com/tag/goodhart-s-law?showPostCount=true&amp;useTagName=true">Goodhart's Law</a><br><a href="https://www.lesswrong.com/tag/goal-directedness?showPostCount=true&amp;useTagName=true">Goal-Directedness</a><br><a href="http://www.lesswrong.com/tag/infra-bayesianism?showPostCount=true&amp;useTagName=true">Infra-Bayesianism</a><br><a href="https://www.lesswrong.com/tag/inner-alignment?showPostCount=true&amp;useTagName=true">Inner Alignment</a><br><a href="https://www.lesswrong.com/tag/instrumental-convergence?showPostCount=true&amp;useTagName=true">Instrumental Convergence</a><br><a href="https://www.lesswrong.com/tag/intelligence-explosion?showPostCount=true&amp;useTagName=true">Intelligence Explosion</a><br><a href="https://www.lesswrong.com/tag/logical-induction?showPostCount=true&amp;useTagName=true">Logical Induction</a><br><a href="http://www.lesswrong.com/tag/logical-uncertainty?showPostCount=true&amp;useTagName=true">Logical Uncertainty</a><br><a href="https://www.lesswrong.com/tag/mesa-optimization?showPostCount=true&amp;useTagName=true">Mesa-Optimization</a><br><a href="https://www.lesswrong.com/tag/myopia?showPostCount=true&amp;useTagName=true">Myopia</a><br><a href="https://www.lesswrong.com/tag/newcomb-s-problem?showPostCount=true&amp;useTagName=true">Newcomb's Problem</a><br><a href="https://www.lesswrong.com/tag/optimization?showPostCount=true&amp;useTagName=true">Optimization</a><br><a href="https://www.lesswrong.com/tag/orthogonality-thesis?showPostCount=true&amp;useTagName=true">Orthogonality Thesis</a><br><a href="https://www.lesswrong.com/tag/outer-alignment?showPostCount=true&amp;useTagName=true">Outer Alignment</a><br><a href="http://www.lesswrong.com/tag/paperclip-maximizer?showPostCount=true&amp;useTagName=true">Paperclip Maximizer</a><br><a href="https://www.lesswrong.com/tag/recursive-self-improvement?showPostCount=true&amp;useTagName=true">Recursive Self-Improvement</a><br><a href="https://www.lesswrong.com/tag/solomonoff-induction?showPostCount=true&amp;useTagName=true">Solomonoff Induction</a><br><a href="https://www.lesswrong.com/tag/treacherous-turn?showPostCount=true&amp;useTagName=true">Treacherous Turn</a><br><a href="https://www.lesswrong.com/tag/utility-functions?showPostCount=true&amp;useTagName=true">Utility Functions</a></p></td><td style="border-color:hsl(0, 0%, 100%);border-style:solid;padding:10px;vertical-align:top;width:33.33%" rowspan="2"><p><strong>Engineering Alignment</strong></p><p><a href="https://www.lesswrong.com/tag/ai-boxing-containment?showPostCount=true&amp;useTagName=true">AI Boxing (Containment)</a><br><a href="https://www.lesswrong.com/tag/conservatism-ai?showPostCount=true&amp;useTagName=true">Conservatism (AI)</a><br><a href="https://www.lesswrong.com/tag/ai-safety-via-debate?showPostCount=true&amp;useTagName=true">Debate (AI safety technique)</a><br><a href="https://www.lesswrong.com/tag/factored-cognition?showPostCount=true&amp;useTagName=true">Factored Cognition</a><br><a href="https://www.lesswrong.com/tag/hch?showPostCount=true&amp;useTagName=true">Humans Consulting HCH</a><br><a href="https://www.lesswrong.com/tag/impact-measures?showPostCount=true&amp;useTagName=true">Impact Measures</a><br><a href="https://www.lesswrong.com/tag/inverse-reinforcement-learning?showPostCount=true&amp;useTagName=true">Inverse Reinforcement Learning</a><br><a href="https://www.lesswrong.com/tag/iterated-amplification?showPostCount=true&amp;useTagName=true">Iterated Amplification</a><br><a href="http://www.lesswrong.com/tag/mild-optimization?showPostCount=true&amp;useTagName=true">Mild Optimization</a><br><a href="https://www.lesswrong.com/tag/oracle-ai?showPostCount=true&amp;useTagName=true">Oracle AI</a><br><a href="https://www.lesswrong.com/tag/reward-functions?showPostCount=true&amp;useTagName=true">Reward Functions</a><br><a href="http://www.lesswrong.com/tag/tool-ai?showPostCount=true&amp;useTagName=true">Tool AI</a><br><a href="https://www.lesswrong.com/tag/transparency-interpretability-ml-and-ai?showPostCount=true">Transparency / Interpretability</a><br><a href="https://www.lesswrong.com/tag/tripwire?showPostCount=true&amp;useTagName=true">Tripwire</a><br><a href="https://www.lesswrong.com/tag/value-learning?showPostCount=true&amp;useTagName=true">Value Learning</a></p><p>&nbsp;</p><p><strong>Strategy</strong></p><p><a href="http://www.lesswrong.com/tag/ai-governance?showPostCount=true&amp;useTagName=true">AI Governance</a><br><a href="https://www.lesswrong.com/tag/ai-risk?showPostCount=true&amp;useTagName=true">AI Risk</a><br><a href="http://www.lesswrong.com/tag/ai-services-cais?showPostCount=true&amp;useTagName=true"><u>AI Services (CAIS)</u></a><br><a href="https://www.lesswrong.com/tag/ai-takeoff?showPostCount=true&amp;useTagName=true">AI Takeoff</a><br><a href="https://www.lesswrong.com/tag/ai-timelines?showPostCount=true&amp;useTagName=true">AI Timelines</a><br><a href="https://www.lesswrong.com/tag/computing-overhang?showPostCount=true&amp;useTagName=true">Computing Overhang</a><br><a href="https://www.lesswrong.com/tag/regulation-and-ai-risk?showPostCount=true&amp;useTagName=true">Regulation and AI Risk</a><br><a href="https://www.lesswrong.com/tag/transformative-ai?showPostCount=true&amp;useTagName=true">Transformative AI</a></p></td><td style="border-color:hsl(0, 0%, 100%);border-style:solid;padding:10px;vertical-align:top;width:33.33%"><p><strong>Organizations</strong></p><p><a href="https://www.lesswrong.com/tag/ai-safety-camp?showPostCount=true&amp;useTagName=true">AI Safety Camp</a><br><a href="https://www.lesswrong.com/tag/centre-for-human-compatible-ai?showPostCount=true&amp;useTagName=true">Centre for Human-Compatible AI</a><br><a href="https://www.lesswrong.com/tag/alpha-algorithm-family?showPostCount=true&amp;useTagName=true">DeepMind</a><br><a href="https://www.lesswrong.com/tag/future-of-humanity-institute?showPostCount=true&amp;useTagName=true">Future of Humanity Institute</a><br><a href="https://www.lesswrong.com/tag/future-of-life-institute-fli?showPostCount=true&amp;useTagName=true">Future of Life Institute</a><br><a href="https://www.lesswrong.com/tag/machine-intelligence-research-institute-miri?showPostCount=true">Machine Intelligence Research Institute</a><br><a href="https://www.lesswrong.com/tag/openai?showPostCount=true&amp;useTagName=true">OpenAI</a><br><a href="https://www.lesswrong.com/tag/ought?showPostCount=true&amp;useTagName=true">Ought</a></p><p>&nbsp;</p><p><strong>Other</strong></p><p><a href="https://www.lesswrong.com/tag/ai-capabilities?showPostCount=true&amp;useTagName=true">AI Capabilities</a><br><a href="https://www.lesswrong.com/tag/gpt?showPostCount=true&amp;useTagName=true">GPT</a><br><a href="https://www.lesswrong.com/tag/language-models?showPostCount=true&amp;useTagName=true">Language Models</a><br><a href="https://www.lesswrong.com/tag/machine-learning?showPostCount=true&amp;useTagName=true">Machine Learning</a><br><a href="https://www.lesswrong.com/tag/narrow-ai?showPostCount=true&amp;useTagName=true">Narrow AI</a><br><a href="https://www.lesswrong.com/tag/neuromorphic-ai?showPostCount=true&amp;useTagName=true">Neuromorphic AI</a><br><a href="https://www.lesswrong.com/tag/reinforcement-learning?showPostCount=true&amp;useTagName=true">Reinforcement Learning</a><br><a href="https://www.lesswrong.com/tag/research-agendas?showPostCount=true&amp;useTagName=true">Research Agendas</a>&nbsp;<br><a href="https://www.lesswrong.com/tag/superintelligence?showPostCount=true&amp;useTagName=true">Superintelligence</a><br><a href="https://www.lesswrong.com/tag/whole-brain-emulation?showPostCount=true&amp;useTagName=true">Whole Brain Emulation</a></p></td></tr><tr><td style="border-color:hsl(0, 0%, 100%);border-style:solid;padding:0px;vertical-align:top">&nbsp;</td></tr></tbody></table>


%%

% START
Basic (and reversed card)
What is **AI**?
Back: {TODO}
Tags: LessWrong
END

%%
	
