---
_id: 5f5c37ee1b5cdee568cfb2a5
title: Event Horizon Thesis
href: https://lesswrong.com/tag/event-horizon-thesis
slug: event-horizon-thesis
type: tag
tags:
  - LessWrong
  - Concept
  - Tag
synchedAt: '2022-08-29T11:09:40.541Z'
status: todo
---

# Event Horizon Thesis

The **event horizon thesis** states that, once [[Superintelligence|superintelligence]] arises, the result will be alien and unpredictable in a way qualitatively different from the results of other technological advances. In this view, we cannot see beyond the [[Singularity|singularity]], just as we cannot see beyond a black hole's event horizon.

[[Eliezer Yudkowsky]] names this idea as one of the [three singularity schools](http://yudkowsky.net/singularity/schools), attributing it to [Vernor Vinge](https://wiki.lesswrong.com/wiki/Vernor_Vinge), who [describes](http://www-rohan.sdsu.edu/faculty/vinge/misc/singularity.html) the singularity as "a point where our models must be discarded and a new reality rules".

An argument in favor of such unpredictability goes as follows. Suppose you could always predict what a superintelligence would do. Then you, yourself, would be a superintelligence â€” but you are not a superintelligence.

However, this argument does not rule out all predictions. In particular, if we can predict what a superintelligence's goals will be, we can predict that it will probably achieve those goals, even if we don't know by what method. The predictions involved in [Friendly AI](https://wiki.lesswrong.com/wiki/Friendly_AI) tend to be of this nature.

## External Links

- [Knowability of FAI](http://sl4.org/wiki/KnowabilityOfFAI), by Eliezer Yudkowsky
- [The Coming Technological Singularity](http://www-rohan.sdsu.edu/faculty/vinge/misc/singularity.html), by Vernor Vinge
- [Three Major Singularity Schools](http://yudkowsky.net/singularity/schools), by Eliezer Yudkowsky

## See Also

- [Technological singularity](https://wiki.lesswrong.com/wiki/Technological_singularity)
