---
tags: ['LessWrong', 'Concept']
href: https://www.lesswrong.com/tag/superintelligence
cite: '[@bostrom2014]'
---

# Superintelligence

> "An intellect that is much smarter than the best human brains in practically every field, including scientific creativity, general wisdom and social skills."

A Superintelligence is a being with superhuman intelligence, and a focus of the [[MIRI|Machine Intelligence Research Institute]]'s research. Specifically, Nick Bostrom (1997) defined it as

The [[MIRI|Machine Intelligence Research Institute]] is dedicated to ensuring humanity's safety and prosperity by preparing for the development of an [[1 Projects/Learning/Rationalism/Concepts/Artificial General Intelligence]] with superintelligence. Given its intelligence, it is likely to be [[AI Boxing (Containment)|incapable of being controlled]] by humanity. It is important to prepare early for the development of [[Friendly Artificial Intelligence|friendly artificial intelligence]], as there may be an [[AI Arms Race|AI arms race]]. A strong superintelligence is a term describing a superintelligence which is not designed with the same architecture as the human brainâ€¦.[(Read More)]()

[[Intelligence]]

[[MuZero]]

[[AI Timelines]]

- [[Fast takeoff]]
