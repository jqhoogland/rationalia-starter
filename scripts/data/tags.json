[
  {
    "_id": "tcrM545tcazjWsrxp",
    "name": "EA Radio",
    "core": false,
    "slug": "ea-radio",
    "oldSlugs": null,
    "postCount": 4,
    "description": {
      "markdown": "A podcast that mostly ports EA videos - especially EA Global talks - to podcast form for easy listening on the go."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "Ah6iawbFLqi97h6cj",
    "name": "History and Philosophy of Science (HPS) ",
    "core": false,
    "slug": "history-and-philosophy-of-science-hps",
    "oldSlugs": null,
    "postCount": 6,
    "description": {
      "markdown": ""
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "XWgogixr8HkFwBxtL",
    "name": "Friedrich Nietzsche",
    "core": false,
    "slug": "friedrich-nietzsche",
    "oldSlugs": null,
    "postCount": 1,
    "description": {
      "markdown": ""
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "cKsTofAvGKA5e99ug",
    "name": "Bernard Williams",
    "core": false,
    "slug": "bernard-williams",
    "oldSlugs": null,
    "postCount": 1,
    "description": {
      "markdown": ""
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "mKNdfyhoHQdCRMDzq",
    "name": "GiveWell Change Our Mind Contest",
    "core": false,
    "slug": "givewell-change-our-mind-contest",
    "oldSlugs": null,
    "postCount": 1,
    "description": {
      "markdown": "Read More\n\nhttps://forum.effectivealtruism.org/posts/6wwK6Qduxt7mMmj8k/announcing-the-change-our-mind-contest-for-critiques-of-our"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "SzGJ7egdaQ6mJhjs9",
    "name": "Weekly Summaries",
    "core": false,
    "slug": "weekly-summaries",
    "oldSlugs": null,
    "postCount": 2,
    "description": {
      "markdown": ""
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "XQBQoi4YdMoKT4Pdg",
    "name": "Meta-crisis",
    "core": false,
    "slug": "meta-crisis",
    "oldSlugs": null,
    "postCount": 1,
    "description": {
      "markdown": ""
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "FZu54ufK6oFfL9EDm",
    "name": "Complexity",
    "core": false,
    "slug": "complexity",
    "oldSlugs": null,
    "postCount": 1,
    "description": {
      "markdown": ""
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "fD8ANYL2SPqiug2Fq",
    "name": "Pascal's Wager",
    "core": false,
    "slug": "pascal-s-wager",
    "oldSlugs": null,
    "postCount": 1,
    "description": {
      "markdown": "Pascal's Wager is a thought experiment involving arbitrarily small probabilities for infinite utility."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "p8ygFEviwJiCWowSr",
    "name": "Eastern Europe",
    "core": false,
    "slug": "eastern-europe",
    "oldSlugs": null,
    "postCount": 1,
    "description": {
      "markdown": ""
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "LuoziLeK6Funq7aB5",
    "name": "Deutsch/ German",
    "core": false,
    "slug": "deutsch-german",
    "oldSlugs": null,
    "postCount": 2,
    "description": {
      "markdown": "Auf Deutsch verfasste Forumposts."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "hJd3Pb9SAbzPDBKLp",
    "name": "Goodhart's Law",
    "core": false,
    "slug": "goodhart-s-law",
    "oldSlugs": null,
    "postCount": 4,
    "description": {
      "markdown": ""
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "7MDBDhPJezCqvgMg2",
    "name": "SPC framework",
    "core": false,
    "slug": "spc-framework",
    "oldSlugs": null,
    "postCount": 6,
    "description": {
      "markdown": "The **significance, persistence and contingency framework**, or **SPC framework** for short, is a framework for estimating the instrumental value of historical or future events.\n\nFurther reading\n---------------\n\nMacAskill, William, Teruji Thomas & Aron Vallinder (2022) [The significance, persistence, contingency framework](https://drive.google.com/file/d/1Lapv64IYsvUnaYWDFmZBoDWe_x5_zkr7/view), *What We Owe the Future: Supplementary Materials*.\n\nRelated entries\n---------------\n\n[ITN framework](https://forum.effectivealtruism.org/topics/itn-framework)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "ARCicHJHsws2HMdbX",
    "name": "Squiggle",
    "core": false,
    "slug": "squiggle",
    "oldSlugs": null,
    "postCount": 12,
    "description": {
      "markdown": "**Squiggle** is a special-purpose programming language for probabilistic estimation, developed by the [Quantified Uncertainty Research Institute](https://forum.effectivealtruism.org/topics/quantified-uncertainty-research-institute).\n\nExternal links\n--------------\n\n[Squiggle](https://www.squiggle-language.com/). Official website."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "bwEuFKB2vk7tg574L",
    "name": "Offense-defense balance",
    "core": false,
    "slug": "offense-defense-balance",
    "oldSlugs": null,
    "postCount": 2,
    "description": {
      "markdown": "The **offense-defense balance** is the relative ease of carrying out and defending against attacks.\n\nFurther reading\n---------------\n\nGarfinkel, Ben & Allan Dafoe (2019) [How does the offense-defense balance scale?](https://doi.org/10.1080/01402390.2019.1631810), *Journal of Strategic Studies*, vol. 42, pp. 736–763.\n\nRelated entries\n---------------\n\n[differential progress](https://forum.effectivealtruism.org/topics/differential-progress) | [international relations](https://forum.effectivealtruism.org/topics/international-relations)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "oqdHhqKeuZMTqdhq4",
    "name": "Mind Enhancement",
    "core": false,
    "slug": "mind-enhancement",
    "oldSlugs": null,
    "postCount": 1,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "mdCuDzZ9gBAepdXiZ",
    "name": "Cognitive decline",
    "core": false,
    "slug": "cognitive-decline",
    "oldSlugs": [
      "cognitive-decline"
    ],
    "postCount": 6,
    "description": {
      "markdown": "Related entries\n---------------\n\n[cognitive enhancement](https://forum.effectivealtruism.org/topics/cognitive-enhancement)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "EEMpm8z9vAyPyYuCs",
    "name": "Schwarzman Scholars",
    "core": false,
    "slug": "schwarzman-scholars",
    "oldSlugs": null,
    "postCount": 1,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "ChFyDNCHirzMLw6cY",
    "name": "Neurotechnology",
    "core": false,
    "slug": "neurotechnology",
    "oldSlugs": null,
    "postCount": 2,
    "description": {
      "markdown": "**Neurotechnology** is any tool that directly, exogenously observes or manipulates the state of biological nervous systems, especially the human brain.\n\nFamiliar examples include electrode-based [brain-computer interfaces](https://forum.effectivealtruism.org/topics/brain-computer-interfaces) (BCIs), antidepressant drugs, or magnetic resonance imaging (MRI)."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "mEmcswc5mSmcB58DG",
    "name": "Repugnant conclusion",
    "core": false,
    "slug": "repugnant-conclusion",
    "oldSlugs": null,
    "postCount": 21,
    "description": {
      "markdown": "The **Repugnant Conclusion** is the implication, generated by a number of theories in [population ethics](https://forum.effectivealtruism.org/topics/population-ethics), that an outcome with sufficiently many people with lives just barely worth living is better than an outcome with arbitrarily many people each arbitrarily well off. [Derek Parfit](https://forum.effectivealtruism.org/topics/derek-parfit), who first brought the Repugnant Conclusion to the attention of contemporary philosophers, stated it informally as follows: \"For any possible population of at least ten billion people, all with a very high quality of life, there must be some much larger imaginable population whose existence, if other things are equal, would be better even though its members have lives that are barely worth living.\"^[\\[1\\]](#fn2v9m1coctmx)^\n\nFurther reading\n---------------\n\nArrhenius, Gustaf, Jesper Ryberg & Torbjörn Tännsjö (2006) [The repugnant conclusion](https://plato.stanford.edu/archives/spr2017/entries/repugnant-conclusion/), in Edward N. Zalta (ed.) *The Stanford Encyclopedia of Philosophy*, March (updated March 2017).\n\nBlackorby, Charles, Walter Bossert & David Donaldson (2003) [The axiomatic approach to population ethics](https://doi.org/10.1177/1470594X030023004), *Politics, Philosophy & Economics*, vol. 2, pp. 342–381.\n\nParfit, Derek (1984) [*Reasons and Persons*](https://en.wikipedia.org/wiki/Special:BookSources/0-19-824908-X), Oxford: Clarendon Press, ch. 17.\n\nSpears, Dean & Mark Budolfson (2021) [Repugnant conclusions](https://doi.org/10.1007/s00355-021-01321-2), *Social Choice and Welfare*., vol. 57, pp. 567–588.\n\nZuber, Stéphane *et al.* (2021) [What should we agree on about the repugnant conclusion?](https://doi.org/10.1017/S095382082100011X), *Utilitas*, pp. 1–5.\n\nRelated entries\n---------------\n\n[population ethics](https://forum.effectivealtruism.org/topics/population-ethics) | [total view](https://forum.effectivealtruism.org/topics/total-view)\n\n1.  ^**[^](#fnref2v9m1coctmx)**^\n    \n    Parfit, Derek (1984) [*Reasons and Persons*](https://en.wikipedia.org/wiki/Special:BookSources/0-19-824908-X), Oxford: Clarendon Press, p. 388."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "uhAc72cRZyRDdts73",
    "name": "Inequality",
    "core": false,
    "slug": "inequality",
    "oldSlugs": null,
    "postCount": 2,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "4RD5bKYke368bih3y",
    "name": "Impostor syndrome",
    "core": false,
    "slug": "impostor-syndrome",
    "oldSlugs": [
      "imposter-syndrome"
    ],
    "postCount": 9,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "RtNiFnxyWs2ALnnYF",
    "name": "Carl Sagan",
    "core": false,
    "slug": "carl-sagan",
    "oldSlugs": null,
    "postCount": null,
    "description": {
      "markdown": "**Carl Edward Sagan** (9 November 1934 – 20 December 1996) was an [American](https://forum.effectivealtruism.org/topics/united-states) astronomer and science writer.\n\nFurther reading\n---------------\n\nPoundstone, William (1999) [*Carl Sagan: A Life in the Cosmos*](https://en.wikipedia.org/wiki/Special:BookSources/978-0-8050-5766-9), New York: Henry Holt.\n\nRelated entries\n---------------\n\n[extraterrestrial intelligence](https://forum.effectivealtruism.org/topics/extraterrestrial-intelligence) | [nuclear disarmament movement](https://forum.effectivealtruism.org/topics/nuclear-disarmament-movement) | [nuclear winter](https://forum.effectivealtruism.org/topics/nuclear-winter) | [time of perils](https://forum.effectivealtruism.org/topics/time-of-perils)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "PiYrGwmx5NNhtyffg",
    "name": "Product management",
    "core": false,
    "slug": "product-management",
    "oldSlugs": null,
    "postCount": 2,
    "description": {
      "markdown": "Related entries\n===============\n\n[career choice](https://forum.effectivealtruism.org/topics/career-choice) | [data science](https://forum.effectivealtruism.org/topics/data-science) | [entrepreneurship](https://forum.effectivealtruism.org/topics/entrepreneurship) | [public interest technology](https://forum.effectivealtruism.org/topics/public-interest-technology) | [software engineering](https://forum.effectivealtruism.org/topics/software-engineering)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "M2hjfDHTwXuJkSpvn",
    "name": "AI Safety Public Materials",
    "core": false,
    "slug": "ai-safety-public-materials",
    "oldSlugs": null,
    "postCount": 4,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "3MrtjWQRPDdgniMtz",
    "name": "Public relations",
    "core": false,
    "slug": "public-relations",
    "oldSlugs": [
      "public-relations"
    ],
    "postCount": 8,
    "description": {
      "markdown": "**Public relations** (**PR**) is \"the practice of managing and disseminating information from an individual or an organization \\[. . .\\] to the public in order to affect their public perception. Public relations and publicity differ in that PR is controlled internally, whereas publicity is not controlled and contributed by external parties\".^[\\[1\\]](#fnh8yy4ylu8p)^\n\n1.  ^**[^](#fnrefh8yy4ylu8p)**^\n    \n    Wikipedia (2001) [Public relations](https://en.wikipedia.org/w/index.php?title=Public_relations&oldid=1101423043), *Wikipedia*, October 15 (updated 30 July 2022‎)."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "ct6dCPfJK9jkJnyAc",
    "name": "AI Safety Field Building",
    "core": false,
    "slug": "ai-safety-field-building",
    "oldSlugs": [
      "ai-safety-community-building"
    ],
    "postCount": 10,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "bbvQiecrr2HF8drgd",
    "name": "Carrick Flynn",
    "core": false,
    "slug": "carrick-flynn",
    "oldSlugs": null,
    "postCount": 10,
    "description": {
      "markdown": "**Carrick Flynn** (born 1986) is an American researcher, advisor and politician. Flynn is a research affiliate with the [Future of Humanity Institute](https://forum.effectivealtruism.org/topics/future-of-humanity-institute) (focusing on AI strategy, policy, and [governance](https://forum.effectivealtruism.org/topics/ai-governance)) and the [Centre for the Governance of AI](https://forum.effectivealtruism.org/topics/centre-for-the-governance-of-ai). In addition to [AI safety](https://forum.effectivealtruism.org/topics/ai-safety), he also works on [pandemic preparedness](https://forum.effectivealtruism.org/topics/pandemic-preparedness) and [biosecurity](https://forum.effectivealtruism.org/topics/biosecurity). He was admitted to practice law in the state of New York.^[\\[1\\]](#fnsg4kuuvihg)^\n\nFlynn contested in the Democratic Primary for Oregon's 6th Congressional District.^[\\[2\\]](#fnwvmtlgzzbfc)^ The race was the third most expensive democratic primary in 2022,  with total spending reaching $18.3 million.^[\\[3\\]](#fnvvryj51611b)^ Flynn came second, with 13,052 votes,  and lost to Andrea Salinas (26,101 votes)^[\\[4\\]](#fnzqb24z5jydm)^.\n\nBackground\n----------\n\nFlynn attended the University of Oregon on a Ford Family Foundation scholarship,^[\\[5\\]](#fnn4rk7x8tfdr)^ earning a Bachelor of Arts  in Economics and International Studies, and Yale Law School, earning his Juris Doctor.^[\\[6\\]](#fno3l8newyc0r)^\n\nOregon's 6th Congressional District\n-----------------------------------\n\nFlynn announced that he was running for Congress on 1 February 2022.^[\\[7\\]](#fnyg81j38zowf)^ Among other issues, his campaign focused on fixing congress, building a green economy and preventing pandemics.^[\\[2\\]](#fnwvmtlgzzbfc)^ The vast majority of funding for Carrick Flynn's campaign came from [Sam Bankman-Fried](https://forum.effectivealtruism.org/topics/sam-bankman-fried), which drew criticism.^[\\[8\\]](#fnbouerapp55)^\n\nUpon losing to Andrea Salinas in the Democratic primary, Flynn congratulated her and offered his full support to her campaign for the general election on 8 November 2022.^[\\[9\\]](#fncq06hb2n3lp)^\n\nExternal links\n--------------\n\n [Carrick Flynn](https://forum.effectivealtruism.org/users/carrickflynn). [Effective Altruism Forum](https://forum.effectivealtruism.org/topics/effective-altruism-forum-1) account.\n\n1.  ^**[^](#fnrefsg4kuuvihg)**^\n    \n    LinkedIn (2022) [Carrick Flynn](https://www.linkedin.com/in/carrickflynn), *LinkedIn*.\n    \n2.  ^**[^](#fnrefwvmtlgzzbfc)**^\n    \n    See the homepage of [Flynn for Oregon](https://web.archive.org/web/20220301032647/http://carrickflynnfororegon.com/), the campaign website (retrieved 1 March 2022).\n    \n3.  ^**[^](#fnrefvvryj51611b)**^\n    \n    Grisales, Claudia (2022) [Bitter feuds and crypto ties: Inside one of the most expensive Democratic primaries](https://www.npr.org/2022/05/11/1097691538/bitter-feuds-and-crypto-ties-inside-one-of-the-most-expensive-democratic-primari), *NPR*, May 11.\n    \n4.  ^**[^](#fnrefzqb24z5jydm)**^\n    \n    Fagan, Shemia (2022) [Unofficial primary election: US representative, 6th district - democrat](https://results.oregonvotes.gov/resultsSW.aspx?type=FED&map=CTY#:~:text=DISTRICT%20%2D%20REPUBLICAN%20contest-,US%20REPRESENTATIVE%2C%206TH%20DISTRICT%20%2D%20DEMOCRAT,-Follow%20this%20contest), May 17. \n    \n5.  ^**[^](#fnrefn4rk7x8tfdr)**^\n    \n    Flynn for Oregon (2022) [Meet Carrick](https://web.archive.org/web/20220425230509/https://www.carrickflynnfororegon.com/meetcarrick), *Flynn for Oregon*, April 25.\n    \n6.  ^**[^](#fnrefo3l8newyc0r)**^\n    \n    Future of Humanity Institute (2022) [Carrick Flynn](https://www.fhi.ox.ac.uk/team/carrick-flynn/), *Future of Humanity Institute*.\n    \n7.  ^**[^](#fnrefyg81j38zowf)**^\n    \n    Flynn, Carrick (2022) [I know what it means to have one moment change a family’s whole trajectory](https://twitter.com/CarrickFlynnOR/status/1488544019802112003), *Twitter*, February 1.\n    \n8.  ^**[^](#fnrefbouerapp55)**^\n    \n    Dixon-Luinenburg, Miranda & Dylan Matthews (2022) [Carrick Flynn may be 2022’s unlikeliest congressional candidate. Here’s why he’s running](https://www.vox.com/23066877/carrick-flynn-effective-altruism-sam-bankman-fried-congress-house-election-2022), *Vox*, May 14.  \n    \n9.  ^**[^](#fnrefcq06hb2n3lp)**^\n    \n    Flynn, Carrick (2022) [Thank you to my supporters](https://twitter.com/CarrickFlynnOR/status/1526784842473480192), *Twitter*, February 1."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "XyJasPR53z83aiRxE",
    "name": "Conjunctive vs. disjunctive risk models",
    "core": false,
    "slug": "conjunctive-vs-disjunctive-risk-models",
    "oldSlugs": null,
    "postCount": 3,
    "description": {
      "markdown": "Models of [catastrophic risks](https://forum.effectivealtruism.org/topics/global-catastrophic-risk) can be conjunctive or disjunctive. A **conjunctive risk model** is one in which the disaster is caused by the co-occurrence of multiple conditions (\\\\(D = C\\_1 \\\\cap \\\\ldots \\\\cap C\\_k\\\\)). In a conjunctive model, the probability of the disaster is *less than or equal to* the probabilities of the individual conditions. By contrast, a **disjunctive risk model** is one in which the disaster occurs as a result of *any* of several conditions holding (\\\\(D = C\\_1 \\\\cup \\\\ldots \\\\cup C\\_k\\\\)). In a disjunctive model, the probability of the disaster is *greater than or equal to* the probabilities of the individual conditions.\n\nExamples of conjunctive and disjunctive risk models of [AI risk](https://forum.effectivealtruism.org/topics/ai-risk):\n\n*   Joseph Carlsmith's models existential risk from power-seeking AI conjunctively, i.e. as the intersection of six conditions, all of which must be true for the existential catastrophe to occur.^[\\[1\\]](#fnuuyfwmuig9)^\n*   By contrast, Nate Soares's models AGI risk disjunctively, i.e. as the union of multiple conditions, any of which can cause existential catastrophe.^[\\[2\\]](#fnwqodvw1cjhs)^\n\nBoth types of models are simplifying assumptions. In reality, a disaster can be caused by multiple conditions that interact conjunctively *and* disjunctively. For example, a disaster \\\\(D\\\\) could occur if conditions \\\\(C_1\\\\) and \\\\(C_2\\\\) are true, or if condition \\\\(C_3\\\\) is true: \\\\(D = (C\\_1 \\\\cap C\\_2) \\\\cup C_3\\\\).\n\nFurther reading\n---------------\n\nSoares, Nate (2021) [Comments on Carlsmith’s “Is power-seeking AI an existential risk?”](https://www.lesswrong.com/posts/cCMihiwtZx7kdcKgt/comments-on-carlsmith-s-is-power-seeking-ai-an-existential), *LessWrong*, November 13. \n\nRelated entries\n---------------\n\n[compound existential risk](https://forum.effectivealtruism.org/topics/compound-existential-risk) | [existential risk](https://forum.effectivealtruism.org/topics/existential-risk) | [existential risk factor](https://forum.effectivealtruism.org/topics/existential-risk-factor) | [global catastrophic risk](https://forum.effectivealtruism.org/topics/global-catastrophic-risk)\n\n1.  ^**[^](#fnrefuuyfwmuig9)**^\n    \n    Carlsmith, Joseph (2021) [Draft report on existential risk from power-seeking AI](https://forum.effectivealtruism.org/posts/78NoGoRitPzeT8nga/draft-report-on-existential-risk-from-power-seeking-ai), *Effective Altruism Forum*, April 28. \n    \n2.  ^**[^](#fnrefwqodvw1cjhs)**^\n    \n    Soares, Nate (2022) [AGI ruin scenarios are likely (and disjunctive)](https://forum.effectivealtruism.org/posts/vC6v2iTafkydBvnz7/agi-ruin-scenarios-are-likely-and-disjunctive), *Effective Altruism Forum*, July 27."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "n89A3KAZXYLESGD5r",
    "name": "Invincible Wellbeing",
    "core": false,
    "slug": "invincible-wellbeing",
    "oldSlugs": null,
    "postCount": 1,
    "description": {
      "markdown": "**Invincible Wellbeing** (IW) is a non-profit organization focused on foundational research and applied work targeting the biological substrates of [suffering](https://forum.effectivealtruism.org/topics/pain-and-suffering). IW also hosts a podcast under the same name, where Jacob Shwartz-Lucas (IW’s Executive Director) interviews scientists on topics relevant to IW’s prioritization and research.\n\nIW also manages the Joshua Cefalu Fund, a fund that “seeks to advance understanding of the biological basis for suffering, model its global distribution, and most effectively minimize it”.^[\\[1\\]](#fnc22sqk6jrkk)^ The fund offers grants to research projects that further the fund's goal. The fund was created in partnership with the Joshua Cefalu Foundation and Henry George School of San Francisco.^[\\[1\\]](#fnc22sqk6jrkk)^\n\nIW’s Director of [Bioethics](https://forum.effectivealtruism.org/topics/bioethics) is philosopher [David Pearce](https://forum.effectivealtruism.org/topics/david-pearce-1).^[\\[2\\]](#fn3xhohvoygtl)^\n\nExternal links\n--------------\n\n[Invincible Wellbeing](https://www.invinciblewellbeing.com/). Official website.\n\n[Invincible Wellbeing](https://www.invinciblewellbeing.com/podcast). Invincible Wellbeing's podcast.\n\n[The Joshua Cefalu Fund](https://www.invinciblewellbeing.com/joshuacefalufund). Invincible Wellbeing’s research fund.\n\n[Volunteer](https://www.invinciblewellbeing.com/volunteer).\n\nRelated entries\n---------------\n\n[animal welfare](https://forum.effectivealtruism.org/topics/animal-welfare-1) | [biotechnology](https://forum.effectivealtruism.org/topics/biotechnology) | [David Pearce](https://forum.effectivealtruism.org/topics/david-pearce-1) | [hellish existential catastrophe](https://forum.effectivealtruism.org/topics/hellish-existential-catastrophe) | [invertebrate welfare](https://forum.effectivealtruism.org/topics/invertebrate-welfare) | [mental health](https://forum.effectivealtruism.org/topics/mental-health) | [podcasts](https://forum.effectivealtruism.org/topics/podcasts) | [sentience](https://forum.effectivealtruism.org/topics/sentience-1) | [suffering and pain](https://forum.effectivealtruism.org/topics/pain-and-suffering) | [welfare biology](https://forum.effectivealtruism.org/topics/welfare-biology) | [wild animal welfare](https://forum.effectivealtruism.org/topics/wild-animal-welfare)\n\n1.  ^**[^](#fnrefc22sqk6jrkk)**^\n    \n    Invincible Wellbeing (2021) [The Joshua Cefalu Fund](https://www.invinciblewellbeing.com/joshuacefalufund), *Invincible Wellbeing.*\n    \n2.  ^**[^](#fnref3xhohvoygtl)**^\n    \n    Invincible Wellbeing (2022) [Meet the team](https://www.invinciblewellbeing.com/staff), *Invincible Wellbeing.*"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "mxyrHDrmiYq3MR6oD",
    "name": "Writing advice",
    "core": false,
    "slug": "writing-advice",
    "oldSlugs": null,
    "postCount": 14,
    "description": {
      "markdown": "This tag is for posts about **how to write better**."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "iqn7WExNjMoCpysMY",
    "name": "Operations research",
    "core": false,
    "slug": "operations-research",
    "oldSlugs": null,
    "postCount": 3,
    "description": {
      "markdown": "**Operations research (OR)** is \"the field of applying advanced analytics to make better decisions. That is, making the best possible decision under constraints and uncertainty.\"^[\\[1\\]](#fn7mfb1moguwg)^\n\nNot to be confused with [operations](https://forum.effectivealtruism.org/topics/operations).\n\nRelated entries\n---------------\n\n[artificial intelligence](https://forum.effectivealtruism.org/topics/artificial-intelligence) | [data science](https://forum.effectivealtruism.org/topics/data-science) | [mechanism design](https://forum.effectivealtruism.org/topics/mechanism-design) | [public interest technology](https://forum.effectivealtruism.org/topics/public-interest-technology) | [software engineering](https://forum.effectivealtruism.org/topics/software-engineering)\n\n1.  ^**[^](#fnref7mfb1moguwg)**^\n    \n    Gurnee, Wes (2022) [Why EA needs operations research: the science of decision making](https://forum.effectivealtruism.org/posts/kZ77cifvvNcwG56sH/why-ea-needs-operations-research-the-science-of-decision), *Effective Altruism Forum*, July 21."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "9HCih8LkqgwKATwX7",
    "name": "Conjecture",
    "core": false,
    "slug": "conjecture",
    "oldSlugs": null,
    "postCount": 2,
    "description": {
      "markdown": "**Conjecture** is an [AI alignment](https://forum.effectivealtruism.org/topics/ai-alignment) for-profit research organization.\n\nHistory\n-------\n\nConjecture was founded in 2022 by Connor Leahy, Sid Black and Gabriel Alfour. Its founders and early staff are mostly former members of EleutherAI, a decentralized collective of volunteer researchers focused on building and open sourcing large language models.\n\nFunding\n-------\n\nConjecture has received VC funding from Nat Friedman, Daniel Gross, Patrick and John Collison, Arthur Breitman, Andrej Karpathy, and [Sam Bankman-Fried](https://forum.effectivealtruism.org/topics/sam-bankman-fried). It is headquartered in London.\n\nFurther reading\n---------------\n\nLeahy, Connor (2022) [We are Conjecture, a new alignment research startup](https://www.alignmentforum.org/posts/jfq2BH5kfQqu2vYv3/we-are-conjecture-a-new-alignment-research-startup), *AI Alignment Forum*, April 8.\n\nExternal links\n--------------\n\n[Conjecture](https://www.conjecture.dev/). Official website.\n\nRelated entries\n---------------\n\n[AI alignment](https://forum.effectivealtruism.org/topics/ai-alignment)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "7ELFfirnfj3Svqxea",
    "name": "Equal consideration of interests",
    "core": false,
    "slug": "equal-consideration-of-interests",
    "oldSlugs": null,
    "postCount": 2,
    "description": {
      "markdown": "**Equal consideration of interests** is the view according to which the interests of every [moral patient](https://forum.effectivealtruism.org/topics/moral-patient) matter equally. An early proponent of equal consideration of interests was [Jeremy Bentham](https://forum.effectivealtruism.org/topics/jeremy-bentham), who popularized the *dictum*, \"everybody to count for one, nobody for more than one\".^[\\[1\\]](#fnrmrfi914yvj)^\n\nThe equal consideration of interests view can be contrasted with views that accord greater weight to the interests of some moral patients over others. Examples include *partialism*, which gives extra weight to the agent's own interests and those of their family members or fellow nationals, and [*prioritarianism*](https://forum.effectivealtruism.org/topics/prioritarianism), which gives extra weight to the interests of the worse off.\n\nFurther reading\n---------------\n\nGuidi, Marco E. L. (2008) [“Everybody to count for one, nobody for more than one”: The Principle of Equal Consideration of Interests from Bentham to Pigou](https://doi.org/10.4000/etudes-benthamiennes.182), *Revue d’études Benthamiennes*, vol. 4.\n\nMacAskill, William, Darius Meissner & Richard Yetter Chappell (2022) [Elements and types of utilitarianism](https://www.utilitarianism.net/types-of-utilitarianism), *Utilitarianism.net*, section 3.\n\nSinger, Peter (2011) [*Practical Ethics*](https://en.wikipedia.org/wiki/Special:BookSources/978-0-521-70768-8), 3rd ed., Cambridge: Cambridge University Press, ch. 2.\n\nRelated entries\n---------------\n\n[prioritarianism](https://forum.effectivealtruism.org/topics/prioritarianism) | [speciesism](https://forum.effectivealtruism.org/topics/speciesism) | [utilitarianism](https://forum.effectivealtruism.org/topics/utilitarianism)\n\n1.  ^**[^](#fnrefrmrfi914yvj)**^\n    \n    Mill, John Stuart (1861) [*Utilitarianism*](https://en.wikipedia.org/wiki/Special:BookSources/978-0-86597-650-4), in John B. Robson (ed.) *Collected Works of John Stuart Mill*, vol. 10, Toronto: University of Toronto Press, 1969, p. 257."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "88XcyShz4uFg2P9WG",
    "name": "Metagenomics",
    "core": false,
    "slug": "metagenomics",
    "oldSlugs": null,
    "postCount": 1,
    "description": {
      "markdown": "**Metagenomics** is the analysis of genetic material obtained from an environmental sample.\n\nFurther reading\n---------------\n\nMoorhouse, Fin & Luca Righetti (2022) [Ajay Karpur on metagenomic sequencing](https://hearthisidea.com/episodes/karpur), *Hear This Idea*, June 13.\n\nRelated entries\n---------------\n\n[biosecurity](https://forum.effectivealtruism.org/topics/biosecurity) | [biosurveillance](https://forum.effectivealtruism.org/topics/biosurveillance)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "LPrJwBBfmAAagvuHt",
    "name": "Reasoning transparency",
    "core": false,
    "slug": "reasoning-transparency",
    "oldSlugs": null,
    "postCount": 5,
    "description": {
      "markdown": "**Reasoning transparency** is a form of [transparency](https://forum.effectivealtruism.org/topics/transparency) that prioritizes the sharing of information about underlying general thinking processes and philosophy and the communication of this information in ways that make it easier for the recipient to determine what updates to make in response to it.\n\nFurther reading\n---------------\n\nMuehlhauser, Luke (2017) [Reasoning transparency](https://www.openphilanthropy.org/reasoning-transparency), *Open Philanthropy*, December.\n\nRelated entries\n---------------\n\n[transparency](https://forum.effectivealtruism.org/topics/transparency)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "aA4tCJrkoy9aeXvfN",
    "name": "Technology race",
    "core": false,
    "slug": "technology-race",
    "oldSlugs": null,
    "postCount": 8,
    "description": {
      "markdown": "A **technology race** is a competitive dynamic between rival teams to first develop a powerful technology, such as [weapons of mass destruction](https://forum.effectivealtruism.org/topics/weapons-of-mass-destruction), advanced [artificial intelligence](https://forum.effectivealtruism.org/topics/artificial-intelligence), or spaceflight capability.\n\nTerminology\n-----------\n\nThe expression **arms race** is often used to describe technology races, though the expression is also used to describe a specific type of technology race involving military technology. A technology race involving the development of AI is known as an [AI race](https://forum.effectivealtruism.org/topics/ai-race).\n\nFurther reading\n---------------\n\nBelfield, Haydn & Christian Ruhl (2022) [Why policy makers should beware claims of new “arms races”](https://thebulletin.org/2022/07/why-policy-makers-should-beware-claims-of-new-arms-races/), *Bulletin of the Atomic Scientists*, July 14.\n\nRelated entries\n---------------\n\n[AI race](https://forum.effectivealtruism.org/topics/ai-race) | [great power conflict](https://forum.effectivealtruism.org/topics/great-power-conflict) | [weapons of mass destruction](https://forum.effectivealtruism.org/topics/weapons-of-mass-destruction)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "mYetvJak4JxuCoJgv",
    "name": "Online effective altruism communities",
    "core": false,
    "slug": "online-effective-altruism-communities",
    "oldSlugs": null,
    "postCount": 15,
    "description": {
      "markdown": "This tag is about **online communities** related to effective altruism.\n\nRelated entries\n---------------\n\n[effective altruism groups](https://forum.effectivealtruism.org/topics/effective-altruism-groups)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "LZBWjXrn6dBy9vm3d",
    "name": "Research Institute for Future Design",
    "core": false,
    "slug": "research-institute-for-future-design",
    "oldSlugs": null,
    "postCount": 3,
    "description": {
      "markdown": "**Research Institute for Future Design** is an interdisciplinary research group that conducts research on ways to make democracy and markets more responsive to the needs of future generations.\n\nEvaluation\n----------\n\nResearch Institute for Future Design is one of the two organizations focused on [longtermist institutional reform](https://forum.effectivealtruism.org/topics/longtermist-institutional-reform) recommended by [Founders Pledge](https://forum.effectivealtruism.org/topics/founders-pledge).^[\\[1\\]](#fngz0lg834g5)^\n\nFurther reading\n---------------\n\nLerner, Matt (2021) [Research Institute for Future Design](https://www.founderspledge.com/stories/research-institute-for-future-design), *Founders Pledge*, December 13.\n\nExternal links\n--------------\n\n[Research Institute for Future Design](http://www.souken.kochi-tech.ac.jp/seido/en/index.html). Official website.\n\nRelated entries\n---------------\n\n[longtermist institutional reform](https://forum.effectivealtruism.org/topics/longtermist-institutional-reform)\n\n1.  ^**[^](#fnrefgz0lg834g5)**^\n    \n    Lerner, Matt (2021) [Research Institute for Future Design](https://www.founderspledge.com/stories/research-institute-for-future-design), *Founders Pledge*, December 13."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "uxoyvActFbMwEcGqF",
    "name": "Low- and middle-income countries",
    "core": false,
    "slug": "low-and-middle-income-countries",
    "oldSlugs": [
      "low-and-middle-income-countries"
    ],
    "postCount": 13,
    "description": {
      "markdown": "**Low- and middle-income countries** (**LMICs**) are all countries not considered to be high-income. Although there is no universally agreed-upon definition, the World Bank defines high-income countries as those with a gross national income per capita of $12,696 or more in 2020. Countries with a gross national income per capita below that line include upper-middle, lower-middle, and low-income countries, all of which are classified as LMICs.\n\nExternal links\n--------------\n\n[Low- and middle-income countries](https://data.worldbank.org/country/XO). List of countries currently classified as low- and middle-income by the World Bank."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "MbJx4xe2AtQPW6NE8",
    "name": "Center for Space Governance",
    "core": false,
    "slug": "center-for-space-governance",
    "oldSlugs": null,
    "postCount": 1,
    "description": {
      "markdown": "The **Center for Space Governance** is a non-profit research organization dedicated to exploring issues in [space governance](https://forum.effectivealtruism.org/topics/space-governance).\n\nExternal links\n--------------\n\n[Center for Space Governance](https://governance.space/). Official website.\n\nRelated entries\n---------------\n\n[space colonization](https://forum.effectivealtruism.org/topics/space-colonization) | [space governance](https://forum.effectivealtruism.org/topics/space-governance)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "SMqtvDKoZFFdau8MT",
    "name": "Research agendas, questions, and project lists",
    "core": false,
    "slug": "research-agendas-questions-and-project-lists",
    "oldSlugs": null,
    "postCount": 8,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "APT6di9bDPYGBqPLv",
    "name": "Total view",
    "core": false,
    "slug": "total-view",
    "oldSlugs": null,
    "postCount": 10,
    "description": {
      "markdown": "The **total view** is a view in [population ethics](https://forum.effectivealtruism.org/topics/population-ethics) that regards one outcome as better than another if and only if it contains greater total [wellbeing](https://forum.effectivealtruism.org/topics/wellbeing), regardless of whether it does so because people are better off or because there are more well-off people. It may be contrasted with [person-affecting views](https://forum.effectivealtruism.org/topics/person-affecting-views).\n\nFurther reading\n---------------\n\nMacAskill, William, Richard Yetter Chappell & Darius Meissner (2022) [Population ethics: the total view, average view & other theories](https://www.utilitarianism.net/population-ethics), *Utilitarianism.Net*.\n\nRelated entries\n---------------\n\n[person-affecting views](https://forum.effectivealtruism.org/topics/person-affecting-views) | [population ethics](https://forum.effectivealtruism.org/topics/population-ethics) | [repugnant conclusion](https://forum.effectivealtruism.org/topics/repugnant-conclusion) | [utilitarianism](https://forum.effectivealtruism.org/topics/utilitarianism)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "rfoiQXtn6zFqRzfrP",
    "name": "Poverty trap",
    "core": false,
    "slug": "poverty-trap",
    "oldSlugs": null,
    "postCount": 3,
    "description": {
      "markdown": "A **poverty trap** is a set of hypothesized self-reinforcement mechanisms that cause poor countries to remain poor, such that present poverty directly causes future poverty. A commonly postulated mechanism is that, when resources can at most only meet the basic needs of the population, the lack of resources available for investment will constrain economic development, perpetuating poverty.^[\\[1\\]](#fni35zb85t8b)^\n\nFurther reading\n---------------\n\nKraay, Aart & David McKenzie (2014) [Do poverty traps exist? Assessing the evidence](https://doi.org/10.1257/jep.28.3.127), *Journal of Economic Perspectives*, vol. 28, pp. 127–148.\n\nRelated entries\n---------------\n\n[economic growth](https://forum.effectivealtruism.org/topics/economic-growth) | [foreign aid skepticism](https://forum.effectivealtruism.org/topics/foreign-aid-skepticism) | [global health and development](https://forum.effectivealtruism.org/topics/global-health-and-development) | [global poverty](https://forum.effectivealtruism.org/topics/global-poverty)\n\n1.  ^**[^](#fnrefi35zb85t8b)**^\n    \n    Hashimzade, Nigar, Gareth Myles & John Black (eds.) (2017) [*A Dictionary of Economics*](https://en.wikipedia.org/wiki/Special:BookSources/978-0-19-875943-0), 5th ed., New York: Oxford University Press."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "bvCS3g4keyD7KsgyC",
    "name": "University groups",
    "core": false,
    "slug": "university-groups",
    "oldSlugs": [
      "university-groups"
    ],
    "postCount": 15,
    "description": {
      "markdown": ""
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "Wurqm8Gnm9tYqBYbP",
    "name": "Hackathons",
    "core": false,
    "slug": "hackathons",
    "oldSlugs": null,
    "postCount": 1,
    "description": {
      "markdown": "Pressure cooking events for generating/implementing new ideas."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "PxzzBueY8hk8Th87K",
    "name": "Fund for Alignment Research",
    "core": false,
    "slug": "fund-for-alignment-research",
    "oldSlugs": null,
    "postCount": 2,
    "description": {
      "markdown": "The **Fund for Alignment Research** (**FAR**) is an organization that helps [AI safety](https://forum.effectivealtruism.org/topics/ai-safety) researchers pursue high-impact research by hiring contractors.\n\nExternal links\n--------------\n\n[Fund for Alignment Research](https://alignmentfund.org/). Official website.\n\n[Apply for a job](https://forum.effectivealtruism.org/posts/gNHjEmLeKM47FDdqM/introducing-the-fund-for-alignment-research-we-re-hiring-1).\n\nRelated entries\n---------------\n\n[AI safety](https://forum.effectivealtruism.org/topics/ai-safety)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "zvREBuHHKFwxgQ9qb",
    "name": "UK policy",
    "core": false,
    "slug": "uk-policy",
    "oldSlugs": null,
    "postCount": 15,
    "description": {
      "markdown": "The **UK policy** tag is used for posts that are about working in or improving the government policy of the United Kingdom.\n\nRelated entries\n---------------\n\n[Policy](https://forum.effectivealtruism.org/topics/policy) | [United Kingdom](https://forum.effectivealtruism.org/topics/united-kingdom)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "tYPXXENLbJxuMAaxq",
    "name": "US policy",
    "core": false,
    "slug": "us-policy",
    "oldSlugs": null,
    "postCount": 30,
    "description": {
      "markdown": "The **US policy** tag is used for posts that are about working in or improving the government policy of the United States.\n\nRelated entries\n---------------\n\n[Policy](https://forum.effectivealtruism.org/topics/policy) | [United States](https://forum.effectivealtruism.org/topics/united-states)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "HX22bnMHL4WNRgeFB",
    "name": "Association for Long Term Existence and Resilience",
    "core": false,
    "slug": "association-for-long-term-existence-and-resilience",
    "oldSlugs": null,
    "postCount": 2,
    "description": {
      "markdown": "**Association for Long Term Existence and Resilience** (**ALTER**) is an academic research and advocacy organization based in Israel focused on improving the short- and long-term future.\n\nFunding\n-------\n\nAs of July 2022, ALTER has received $400,000 in funding from the [Survival and Flourishing](https://forum.effectivealtruism.org/topics/survival-and-flourishing) Fund,^[\\[1\\]](#fnhya3b9k032j)^ $320,000 from the [Future Fund](https://forum.effectivealtruism.org/topics/future-fund),^[\\[2\\]](#fn4pq18hl02df)^ and over $150,000 from [Effective Altruism Funds](https://forum.effectivealtruism.org/topics/effective-altruism-funds).^[\\[3\\]](#fn8t254i9td85)^^[\\[4\\]](#fnayx2ri28lmf)^^[\\[5\\]](#fnf6224ek5rj5)^^[\\[6\\]](#fn172j5wj4wxvi)^\n\nExternal links\n--------------\n\n[Association for Long Term Existence and Resilience](https://alter.org.il/). Official website.\n\n1.  ^**[^](#fnrefhya3b9k032j)**^\n    \n    Survival and Flourishing Fund (2021) [SFF-2022-H1 S-Process recommendations announcement](https://survivalandflourishing.fund/sff-2022-h1-recommendations), *Survival and Flourishing Fund*.\n    \n2.  ^**[^](#fnref4pq18hl02df)**^\n    \n    Future Fund (2022) [Our grants and investments: Association for Long Term Existence and Resilience](https://ftxfuturefund.org/our-grants/?_organization_name=association-for-long-term-existence-and-resilience), *Future Fund*.\n    \n3.  ^**[^](#fnref8t254i9td85)**^\n    \n    Effective Altruism Infrastructure Fund (2021) [May-August 2021: EA Infrastructure Fund grants](https://funds.effectivealtruism.org/funds/payouts/may-august-2021-ea-infrastructure-fund-grants), *Effective Altruism Funds*, August.\n    \n4.  ^**[^](#fnrefayx2ri28lmf)**^\n    \n    Long-Term Future Fund (2021) [May 2021: Long-Term Future Fund grants](https://funds.effectivealtruism.org/funds/payouts/may-2021-long-term-future-fund-grants), *Effective Altruism Funds*, May.\n    \n5.  ^**[^](#fnreff6224ek5rj5)**^\n    \n    Long-Term Future Fund (2022) [December 2021: Long-Term Future Fund grants](https://funds.effectivealtruism.org/funds/payouts/december-2021-long-term-future-fund-grants), *Effective Altruism Funds*, August.\n    \n6.  ^**[^](#fnref172j5wj4wxvi)**^\n    \n    Manheim, David (2022) [Funding](https://forum.effectivealtruism.org/posts/azoDjaiSPNT9sccnR/alter-israel-mid-year-2022-update#Funding), in 'ALTER Israel - Mid-year 2022 update', *Effective Altruism Forum*, June 12."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "pJGDYyKce6FYZDw3o",
    "name": "Atlas Fellowship",
    "core": false,
    "slug": "atlas-fellowship",
    "oldSlugs": null,
    "postCount": null,
    "description": {
      "markdown": "The **Atlas Fellowship** is a summer program in California open to talented high school students globally aimed at helping these students pursue [high-impact careers](https://forum.effectivealtruism.org/topics/career-choice).\n\nFunding\n-------\n\nAs of July 2022, the Atlas Fellowship has received $5 million in funding from [Open Philanthropy](https://forum.effectivealtruism.org/topics/open-philanthropy)^[\\[1\\]](#fn22y74inpdnn)^ and another $5 million from the [Future Fund](https://forum.effectivealtruism.org/topics/future-fund).^[\\[2\\]](#fnllcimhuac)^\n\nExternal links\n--------------\n\n[Atlas Fellowship](https://www.atlasfellowship.org/). Official website.\n\n1.  ^**[^](#fnref22y74inpdnn)**^\n    \n    Open Philanthropy (2022) [Grants database: Atlas Fellowship](https://www.openphilanthropy.org/grants/?q=&organization-name=atlas-fellowship), *Open Philanthropy*.\n    \n2.  ^**[^](#fnrefllcimhuac)**^\n    \n    Future Fund (2022) [Our grants and investments: Atlas Fellowship](https://ftxfuturefund.org/all-grants/?_search=atlas%20fellowship), *Future Fund*."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "9KQSwicqspasGxmMY",
    "name": "Apollo Academic Surveys",
    "core": false,
    "slug": "apollo-academic-surveys",
    "oldSlugs": null,
    "postCount": 1,
    "description": {
      "markdown": "**Apollo Academic Surveys** is a nonprofit that aggregates the views of academic experts in a wide variety of fields and makes this information publicly available.\n\nFunding\n-------\n\nAs of July 2022, Apollo Academic Surveys has received $250,000 in funding from the [Future Fund](https://forum.effectivealtruism.org/topics/future-fund).^[\\[1\\]](#fnmdit10gm8m)^\n\nExternal links\n--------------\n\n[Apollo Academic Surveys](https://www.apollosurveys.org/). Official website.\n\nRelated entries\n---------------\n\n[academia](https://forum.effectivealtruism.org/topics/academia-1) | [expertise](https://forum.effectivealtruism.org/topics/expertise) | [surveys](https://forum.effectivealtruism.org/topics/surveys)\n\n1.  ^**[^](#fnrefmdit10gm8m)**^\n    \n    Future Fund (2022) [Our grants and investments: Apollo Academic Surveys](https://ftxfuturefund.org/all-grants/?_organization_name=apollo-academic-surveys), *Future Fund*."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "eCajhwPoGmyJc8eYB",
    "name": "Schmidt Futures",
    "core": false,
    "slug": "schmidt-futures",
    "oldSlugs": null,
    "postCount": 2,
    "description": {
      "markdown": "**Schmidt Futures** is a philanthropic venture based in New York, with offices in Washington, D.C., and London.\n\nHistory\n-------\n\nSchmidt Futures was founded by Eric Schmidt and Wendy Schmidt in 2017.\n\nActivities\n----------\n\nSchmidt Futures has funded a number of organizations in the [effective altruism](https://forum.effectivealtruism.org/topics/effective-altruism) community, including the [Lead Exposure Elimination Project](https://forum.effectivealtruism.org/topics/lead-exposure-elimination-project), [Institute for Progress](https://forum.effectivealtruism.org/topics/institute-for-progress), [1Day Sooner](https://forum.effectivealtruism.org/topics/1day-sooner) and [Metaculus](https://forum.effectivealtruism.org/topics/metaculus).\n\nIn 2019, Schmidt Futures committed $1 billion to offer scholarships, mentorship, and other types of support to exceptionally talented youth.^[\\[1\\]](#fn9n79qa57zvi)^ More recently, it funded a $125 million program to support [artificial intelligence](https://forum.effectivealtruism.org/topics/artificial-intelligence) researchers,^[\\[2\\]](#fno4s98qvdyie)^ and a program to help people make high-impact [career shifts](https://forum.effectivealtruism.org/topics/career-choice).^[\\[3\\]](#fndm966f6dkwo)^\n\nExternal links\n--------------\n\n[Schmidt Futures](https://www.schmidtfutures.com/). Official website.\n\n1.  ^**[^](#fnref9n79qa57zvi)**^\n    \n    Schmidt Futures (2022) [Rise](https://www.schmidtfutures.com/our-work/rise/), *Schmidt Futures*.\n    \n2.  ^**[^](#fnrefo4s98qvdyie)**^\n    \n    Medina, Gabe (2022) [Schmidt Futures launches AI2050 to protect our human future in the age of artificial intelligence](https://www.schmidtfutures.com/schmidt-futures-launches-ai2050-to-protect-our-human-future-in-the-age-of-artificial-intelligence/), *Schmidt Futures*, February 16\n    \n3.  ^**[^](#fnrefdm966f6dkwo)**^\n    \n    Savage, James (2022) [Call for early-career journalists and researchers](https://forum.effectivealtruism.org/posts/vrmt84oT6SDdLeTSj/call-for-early-career-journalists-and-researchers), *Effective Altruism Forum*, March 8."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "w5ZRxDK4cMoe8SKoo",
    "name": "Manifold Markets",
    "core": false,
    "slug": "manifold-markets",
    "oldSlugs": null,
    "postCount": 8,
    "description": {
      "markdown": "**Manifold Markets** is a play-money [prediction market](https://forum.effectivealtruism.org/topics/prediction-markets).\n\nFunding\n-------\n\nAs of July 2022, Manifold Markets has received $1.5 million in funding from the [Future Fund](https://forum.effectivealtruism.org/topics/future-fund),^[\\[1\\]](#fny40446uwl9)^ and over $340,000 from the [Survival and Flourishing](https://forum.effectivealtruism.org/topics/survival-and-flourishing) Fund.^[\\[2\\]](#fnk8ys9psia7e)^\n\nFurther reading\n---------------\n\nPatel, Dwarkesh (2022) [27: Stephen Grugett (Manifold Markets Founder) - Predictions markets & better governance](https://www.dwarkeshpatel.com/p/stephen-grugett), *Lunar Society*, May 5.\n\nExternal links\n--------------\n\n[Manifold Markets](https://manifold.markets/). Official website.\n\n[Above the Fold](https://news.manifold.markets/). Blog.\n\nRelated entries\n---------------\n\n[forecasting](https://forum.effectivealtruism.org/topics/forecasting) | [prediction markets](https://forum.effectivealtruism.org/topics/prediction-markets)\n\n1.  ^**[^](#fnrefy40446uwl9)**^\n    \n    Future Fund (2022) [Our grants and investments: Manifold Markets](https://ftxfuturefund.org/all-grants/?_organization_name=manifold-markets), *Future Fund*.\n    \n2.  ^**[^](#fnrefk8ys9psia7e)**^\n    \n    Survival and Flourishing Fund (2021) [SFF-2022-H1 S-Process recommendations announcement](https://survivalandflourishing.fund/sff-2022-h1-recommendations), *Survival and Flourishing Fund*."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "jLyyGzTHyRpftib7q",
    "name": "Swift Centre for Applied Forecasting",
    "core": false,
    "slug": "swift-centre-for-applied-forecasting",
    "oldSlugs": [
      "switft-centre"
    ],
    "postCount": null,
    "description": {
      "markdown": "The **Swift Centre for Applied Forecasting** (commonly known as the **Swift Centre**) is a nonprofit organization that publishes [forecasts](https://forum.effectivealtruism.org/topics/forecasting) from expert forecasters and professionals from the financial industry.\n\nActivities\n----------\n\nThe Swift Centre plans to forecast trends from [Our World in Data](https://forum.effectivealtruism.org/topics/our-world-in-data) and other topics relevant from a [longtermist](https://forum.effectivealtruism.org/topics/longtermism) perspective, and to articulate the reasons behind those forecasts.^[\\[1\\]](#fnh157u3zw6w)^\n\nFunding\n-------\n\nAs of July 2022, the Swift Centre has received $2 million in funding from the [Future Fund](https://forum.effectivealtruism.org/topics/future-fund).^[\\[2\\]](#fnqzdk48mk6a8)^\n\nExternal links\n--------------\n\n[Swift Centre](https://www.swiftcentre.org/). Official website.\n\nRelated entries\n---------------\n\n[forecasting](https://forum.effectivealtruism.org/topics/forecasting) | [Our World in Data](https://forum.effectivealtruism.org/topics/our-world-in-data)\n\n1.  ^**[^](#fnrefh157u3zw6w)**^\n    \n    Beckstead, Nick *et al.* (2022) [Future Fund June 2022 update](https://ftxfuturefund.org/future-fund-june-2022-update/), *Future Fund*, June 30.\n    \n2.  ^**[^](#fnrefqzdk48mk6a8)**^\n    \n    Future Fund (2022) [Our grants and investments: Swift Centre for Applied Forecasting](https://ftxfuturefund.org/all-grants/?_organization_name=swift-centre-for-applied-forecasting), *Future Fund*."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "biEu68vAJfwthJ4vQ",
    "name": "Nonprofit governance",
    "core": false,
    "slug": "nonprofit-governance",
    "oldSlugs": [
      "non-profit-governance"
    ],
    "postCount": 3,
    "description": {
      "markdown": "**Nonprofit governance** is the form in which nonprofit organizations are governed.\n\nTopics related to nonprofit governance include:\n\n*   how do their boards work\n*   how can they run effectively\n*   what kind of structures should they follow\n*   what kind of protocols help people do good work"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "9X5ZYQmmtH8EBYC4Q",
    "name": "Estimate elicitation",
    "core": false,
    "slug": "estimate-elicitation",
    "oldSlugs": null,
    "postCount": 1,
    "description": {
      "markdown": "Posts about the use of crowdsourcing or other means of eliciting estimates from other people in order to achieve better estimates."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "9jotJcCFLJMDkdBd7",
    "name": "Ready Research",
    "core": false,
    "slug": "ready-research",
    "oldSlugs": null,
    "postCount": 1,
    "description": {
      "markdown": "**Ready Research** is a group of volunteers who conduct behaviour science research to help address the world's most pressing problems.\n\nExternal links\n--------------\n\n[Ready Research](https://www.readyresearch.org/). Official website.\n\nRelated entries\n---------------\n\n[Effective Altruism Behavioral Science Newsletter](https://forum.effectivealtruism.org/topics/effective-altruism-behavioral-science-newsletter)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "MAuGrb3iCWA66kmnB",
    "name": "Michael Huemer",
    "core": false,
    "slug": "michael-huemer",
    "oldSlugs": null,
    "postCount": 5,
    "description": {
      "markdown": "**Michael Huemer** (born 27 December 1969) is an [American](https://forum.effectivealtruism.org/topics/united-states) [philosopher](https://forum.effectivealtruism.org/topics/philosophy). He is a professor of philosophy at the University of Colorado, Boulder and the author of several books, including *Ethical Intuitionism*,^[\\[1\\]](#fnhtzia658cg)^*The Problem of Political Authority: An Examination of the Right to Coerce and the Duty to Obey*,^[\\[2\\]](#fn6cfg57p7wma)^ *Approaching Infinity* ,^[\\[3\\]](#fnfwj8ypm2fe4)^*Dialogues Concerning Vegetarianism: The Ethics of Eating Meat*,^[\\[4\\]](#fnglfvdzx5ch6)^ and *Knowledge, Reality, and Value: A Mostly Common Sense Guide to Philosophy* .^[\\[5\\]](#fnturja8l82vf)^\n\nFurther reading\n---------------\n\nSosis, Chris (2021) [Michael Huemer](http://www.whatisitliketobeaphilosopher.com/michael-huemer), *What Is It Like to Be a Philosopher?*, October 14.\n\nExternal links\n--------------\n\n[Michael Huemer](https://www.owl232.net/). Official website.\n\n[Fake Nous](https://fakenous.substack.com/). Huemer's blog.\n\nRelated entries\n---------------\n\n[Bryan Caplan](https://forum.effectivealtruism.org/topics/bryan-caplan)\n\n1.  ^**[^](#fnrefhtzia658cg)**^\n    \n    Huemer, Michael (2005) [*Ethical Intuitionism*](https://en.wikipedia.org/wiki/Special:BookSources/978-1-4039-8968-0), New York: Palgrave Macmillan.\n    \n2.  ^**[^](#fnref6cfg57p7wma)**^\n    \n    Huemer, Michael (2013) [*The Problem of Political Authority: An Examination of the Right to Coerce and the Duty to Obey*](https://doi.org/10.1057/9781137281661), New York: Palgrave Macmillan.\n    \n3.  ^**[^](#fnreffwj8ypm2fe4)**^\n    \n    Huemer, Michael (2016) [*Approaching Infinity*](https://en.wikipedia.org/wiki/Special:BookSources/978-1-137-56085-8), New York: Palgrave Macmillan.\n    \n4.  ^**[^](#fnrefglfvdzx5ch6)**^\n    \n    Huemer, Michael (2019) [*Dialogues Concerning Vegetarianism: The Ethics of Eating Meat*](https://en.wikipedia.org/wiki/Special:BookSources/978-1-138-32828-0), New York: Routledge.\n    \n5.  ^**[^](#fnrefturja8l82vf)**^\n    \n    Huemer, Michael (2021) [*Knowledge, Reality, and Value: A Mostly Common Sense Guide to Philosophy*](https://en.wikipedia.org/wiki/Special:BookSources/9798729007028), Amazon Digital Services."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "8vBvmQkD3DcmTccrn",
    "name": "Workplace groups",
    "core": false,
    "slug": "workplace-groups",
    "oldSlugs": null,
    "postCount": 13,
    "description": {
      "markdown": "**Workplace groups** collects posts related to workplace and professional [effective altruism groups](https://forum.effectivealtruism.org/tag/effective-altruism-groups) engaged in [workplace advocacy](https://forum.effectivealtruism.org/tag/workplace-advocacy) as well as other relevant activities.\n\nRelated entries\n---------------\n\n[workplace advocacy](https://forum.effectivealtruism.org/tag/workplace-advocacy)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "ruZhRNf4E4aNjPKmg",
    "name": "Blogging",
    "core": false,
    "slug": "blogging",
    "oldSlugs": null,
    "postCount": 7,
    "description": {
      "markdown": "Relating to the creation of blogs."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "zKmkPzbJiW9TEzgmA",
    "name": "Effective altruism in French",
    "core": false,
    "slug": "effective-altruism-in-french",
    "oldSlugs": null,
    "postCount": 4,
    "description": {
      "markdown": "**Effective altruism in French** collects posts written in the French language.\n\nFurther reading\n---------------\n\nAdamczewski, Tom (2016) [Qu’est-ce que l’altruisme efficace ?](https://fragile-credences.github.io/quest-ce-que-laltruisme-efficace/), *Fragile Credences*, July 6.\n\nExternal links\n--------------\n\n[Altruisme Efficace France](https://www.altruismeefficacefrance.org/).\n\nRelated entries\n---------------\n\n[effective altruism in Spanish](https://forum.effectivealtruism.org/tag/effective-altruism-in-spanish)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "o6SGQyrxGCFtWDFrP",
    "name": "Greater New York City area",
    "core": false,
    "slug": "greater-new-york-city-area",
    "oldSlugs": null,
    "postCount": 3,
    "description": {
      "markdown": "This tag is for effective altruism events and programs in the **greater New York City area**."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "MkkEn3Rzr4cxKFTTe",
    "name": "Cooperative AI",
    "core": false,
    "slug": "cooperative-ai-1",
    "oldSlugs": null,
    "postCount": 5,
    "description": {
      "markdown": "**Cooperative AI** is a subfield of [artificial intelligence](https://forum.effectivealtruism.org/topics/artificial-intelligence) that aims to improve the ability of AI systems to engender [cooperation](https://forum.effectivealtruism.org/topics/moral-cooperation) between humans, machines, and institutions. It can therefore also be seen as part of the [AI safety](https://forum.effectivealtruism.org/topics/ai-safety) landscape, aimed at preventing [AI risks](https://forum.effectivealtruism.org/topics/ai-risk) and [existential risks](https://forum.effectivealtruism.org/topics/existential-risk) resulting from cooperation failure.\n\nFurther Reading\n---------------\n\nBaumann, Tobias (2022) [Cooperative artificial intelligence](https://doi.org/10.48550/arXiv.2202.09859), arXiv:2202.09859.\n\nDafoe, Allan *et al.* (2020) [Open problems in cooperative AI](http://arxiv.org/abs/2012.08630), arXiv:2012.08630.\n\nDafoe, Allan *et al.* (2021) [Cooperative AI: machines must learn to find common ground](http://doi.org/10.1038/d41586-021-01170-0), *Nature*, vol. 593, pp. 33–36.\n\nRelated Entries\n---------------\n\n[AI safety](https://forum.effectivealtruism.org/topics/ai-safety) | [Cooperative AI Foundation](https://forum.effectivealtruism.org/topics/cooperative-ai-foundation)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "yq5yEhdHxS5LFmDpv",
    "name": "Epoch",
    "core": false,
    "slug": "epoch",
    "oldSlugs": null,
    "postCount": 4,
    "description": {
      "markdown": "**Epoch** is a research initiative working on investigating trends in machine learning and [forecasting](https://forum.effectivealtruism.org/topics/ai-forecasting) the development of [transformative artificial intelligence](https://forum.effectivealtruism.org/topics/transformative-artificial-intelligence).\n\nFurther reading\n---------------\n\nSevilla, Jaime *et al.* (2022) [Announcing Epoch: A research initiative investigating the road to transformative AI](https://epochai.org/blog/announcing-epoch), *Epoch*, June 23.\n\nExternal links\n--------------\n\n[Epoch](https://epochai.org/). Official website.\n\nRelated entries\n---------------\n\n[AI forecasting](https://forum.effectivealtruism.org/topics/ai-forecasting) | [transformative artificial intelligence](https://forum.effectivealtruism.org/topics/transformative-artificial-intelligence)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "zFow4MEQohsnv6cvx",
    "name": "Intellectual property",
    "core": false,
    "slug": "intellectual-property",
    "oldSlugs": [
      "intellectual-property"
    ],
    "postCount": 5,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "i3yarKsx6qW9jrvLo",
    "name": "California effect",
    "core": false,
    "slug": "california-effect",
    "oldSlugs": null,
    "postCount": 11,
    "description": {
      "markdown": "The **California effect** is the shift of [regulation](https://forum.effectivealtruism.org/topics/standards-and-regulation)—such as [antitrust](https://forum.effectivealtruism.org/topics/antitrust-law), environmental, data privacy, and [artificial intelligence](https://forum.effectivealtruism.org/topics/artificial-intelligence) regulations—toward political jurisdictions with stricter regulatory standards.^[\\[1\\]](#fn597mtv38hc4)^^[\\[2\\]](#fn0j128dmyrv24)^\n\nTerminology\n-----------\n\nSometimes the expression \"California effect\" is used to describe the shift of regulation toward regulation introduced in California, which typically involves stricter regulatory standards. The expression \"**Brussels effect**\" is used in a similar sense, to describe the shift of regulation toward regulation introduced by the [European Union](https://forum.effectivealtruism.org/tag/european-union).^[\\[3\\]](#fn0g6i138hmyvp)^\n\nA distinction is sometimes made between *de jure* and *de facto* versions of the Brussels effect. EU regulation may cause countries outside the EU to adopt similar standards, and these standards may in turn influence firms operating in these non-EU jurisdictions. This is an example of *de jure* Brussels effect, because the firms are legally required to comply with these new standards. In other cases, however, the firms may voluntarily decide to comply with EU law even in the absence of similar regulation outside the European Union, typically because doing so simplifies their business processes. An example is firms that modify their websites to comply with General Data Protection Regulation (GDPR) not just in the EU but globally, even when not legally required to do so.^[\\[4\\]](#fnspmjqyyu1sa)^\n\nFurther reading\n---------------\n\nBradford, Anu (2021) [*The Brussels Effect: How the European Union Rules the World*](https://doi.org/10.1093/oso/9780190088583.001.0001), New York: Oxford University Press.\n\nVogel, David (1995) [*Trading up: Consumer and Environmental Regulation in a Global Economy*](https://en.wikipedia.org/wiki/Special:BookSources/978-0-674-90083-7), Cambridge, Massachusetts: Harvard University Press, ch. 8.\n\nRelated entries\n---------------\n\n[AI governance](https://forum.effectivealtruism.org/topics/ai-governance) | [European Union](https://forum.effectivealtruism.org/topics/european-union) | [global governance](https://forum.effectivealtruism.org/topics/global-governance) | [international relations](https://forum.effectivealtruism.org/topics/international-relations) | [law](https://forum.effectivealtruism.org/topics/law) | [policy](https://forum.effectivealtruism.org/topics/policy-change) | [standards and regulation](https://forum.effectivealtruism.org/topics/standards-and-regulation)\n\n1.  ^**[^](#fnref597mtv38hc4)**^\n    \n    Vogel, David (1995) [*Trading up: Consumer and Environmental Regulation in a Global Economy*](https://en.wikipedia.org/wiki/Special:BookSources/978-0-674-90083-7), Cambridge, Massachusetts: Harvard University Press, p. 259.\n    \n2.  ^**[^](#fnref0j128dmyrv24)**^\n    \n    Princen, Sebastiaan (1999) [The California effect in the EC’s external relations: a comparison of the leghold trap and beef-hormone issues between the EC and the U.S. and Canada](http://aei.pitt.edu/2367/), *ECSA Sixth Biennial International*, p. 1.\n    \n3.  ^**[^](#fnref0g6i138hmyvp)**^\n    \n    Bradford, Anu (2021) [*The Brussels Effect: How the European Union Rules the World*](https://doi.org/10.1093/oso/9780190088583.001.0001), New York: Oxford University Press.\n    \n4.  ^**[^](#fnrefspmjqyyu1sa)**^\n    \n    Engler, Alex (2022) [The EU AI Act will have global impact, but a limited Brussels effect](https://www.brookings.edu/research/the-eu-ai-act-will-have-global-impact-but-a-limited-brussels-effect/), *Brookings*, June 8."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "QBcQD59hhPEhdotQL",
    "name": "Tabletop exercises",
    "core": false,
    "slug": "tabletop-exercises",
    "oldSlugs": null,
    "postCount": 9,
    "description": {
      "markdown": "**Tabletop exercises** are informal meetings in which members of an emergency team discuss, usually with the help of a facilitator, their individual roles and appropriate responses during an emergency situation.^[\\[1\\]](#fnkv4pp2o50p)^^[\\[2\\]](#fn46pigu09xby)^^[\\[3\\]](#fn0a27papse56w)^^[\\[4\\]](#fnl5mzh7vg58m)^\n\nFurther reading\n---------------\n\nBlough, Ryan (2022) [Wargaming AGI development](https://www.lesswrong.com/posts/ouFnZoYaKqicC6jH8/wargaming-agi-development), *LessWrong*, March 19.\n\nPolice Department (2012) [What is a tabletop exercise?](https://uwpd.wisc.edu/content/uploads/2014/01/What_is_a_tabletop_exercise.pdf), *University of Wisconsins-Madison*, May 9.\n\nExternal links\n--------------\n\n[Intelligence Rising](https://intelligencerising.org/).  A strategic role-playing game meant to explore possible AI futures.\n\nRelated entries\n---------------\n\n[AI forecasting](https://forum.effectivealtruism.org/topics/ai-forecasting) | [biosecurity](https://forum.effectivealtruism.org/topics/biosecurity) | [forecasting](https://forum.effectivealtruism.org/topics/forecasting) | [improving institutional decision-making](https://forum.effectivealtruism.org/topics/improving-institutional-decision-making) | [policy](https://forum.effectivealtruism.org/topics/policy-change) | [red teaming](https://forum.effectivealtruism.org/topics/red-teaming)\n\n1.  ^**[^](#fnrefkv4pp2o50p)**^\n    \n    Ready.gov (2021) [Exercises](https://www.ready.gov/exercises), *Ready.Gov*.\n    \n2.  ^**[^](#fnref46pigu09xby)**^\n    \n    Ackerman, Gary & Douglas Clifford (2021) [Red teaming and crisis preparedness](https://doi.org/10.1093/acrefore/9780190228637.013.1969), in *Oxford Research Encyclopedia of Politics*, Oxford: Oxford University Press.\n    \n3.  ^**[^](#fnref0a27papse56w)**^\n    \n    Police Department (2012) [What is a tabletop exercise?](https://uwpd.wisc.edu/content/uploads/2014/01/What_is_a_tabletop_exercise.pdf), *University of Wisconsins-Madison*, May 9.\n    \n4.  ^**[^](#fnrefl5mzh7vg58m)**^\n    \n    Johnson, Leighton (2020) [*Security Component Fundamentals for Assessment*](https://doi.org/10.1016/C2018-0-03706-8), 2nd ed., Amsterdam: Elsevier, p. 493."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "yDEmZqDHqJfLyRrkr",
    "name": "Quantum computing",
    "core": false,
    "slug": "quantum-computing",
    "oldSlugs": null,
    "postCount": 6,
    "description": {
      "markdown": "**Quantum computing** is the use of quantum mechanical physical processes to perform computation in ways unavailable to classical computers.\n\nFurther reading\n---------------\n\nSevilla, Jaime (2021) [Forecasting quantum computing](https://forum.effectivealtruism.org/s/eAfxC2EooihZsgo2T), *Effective Altruism Forum*, April 10.\n\nExternal links\n--------------\n\n[Quantum Country](https://quantum.country/). A free introduction to quantum computing and quantum mechanics using [spaced repetition](https://forum.effectivealtruism.org/tag/spaced-repetition), by Andy Matuschak and Michael Nielsen.\n\nRelated entries\n---------------\n\n[AI forecasting](https://forum.effectivealtruism.org/topics/ai-forecasting) | [AI governance](https://forum.effectivealtruism.org/topics/ai-governance) | [AI safety](https://forum.effectivealtruism.org/topics/ai-safety) | [compute governance](https://forum.effectivealtruism.org/topics/compute-governance)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "dA9ZNkgd3F8CgMEsC",
    "name": "Antitrust law",
    "core": false,
    "slug": "antitrust-law",
    "oldSlugs": null,
    "postCount": 7,
    "description": {
      "markdown": "**Antitrust law** (often called **competition law** outside the US) is the body of law intended to prevent monopolization and cartelization of commercial markets. \n\nAntitrust is particularly relevant to [AI governance](https://forum.effectivealtruism.org/topics/ai-governance), as it may in some cases constrain safety cooperation between competitors.\n\nFurther reading\n---------------\n\nBrundage, Miles, Shahar Avin, Jasmine Wang, Haydn Belfield, Gretchen Kreuger *et al.* (2020) [Toward trustworthy AI development: Mechanisms for supporting verifiable claims](http://arxiv.org/abs/2004.07213), arXiv:2004.07213.  \n*Appendix V on p. 70 discusses antitrust as a potential concern for AI safety collaborations between competitors.*\n\nFischer, Sophie-Charlotte *et al.* (2021) [AI policy levers: A review of the U.S. government’s tools to shape AI research, development, and deployment](https://www.governance.ai/research-paper/ai-policy-levers-a-review-of-the-u-s-governments-tools-to-shape-ai-research-development-and-deployment), Future of Humanity Institute, University of Oxford.  \n*The section on pp. 41–44 discusses antitrust enforcement as a means of influencing AI research, development and deployment.*\n\nHua, Shin-Shin & Haydn Belfield (2021) [AI & antitrust: Reconciling tensions between competition law and cooperative AI development](https://yjolt.org/ai-antitrust-reconciling-tensions-between-competition-law-and-cooperative-ai-development), *Yale Journal of Law & Technology*, vol. 23, pp. 415–550.\n\nO'Keefe, Cullen (2020) [How will national security considerations affect antitrust decisions in AI? An examination of historical precedents](https://www.fhi.ox.ac.uk/wp-content/uploads/How-Will-National-Security-Considerations-Affect-Antitrust-Decisions-in-AI-Cullen-OKeefe.pdf), Future of Humanity Institute, University of Oxford.\n\nO'Keefe, Cullen (2021) [Antitrust-compliant AI industry self-regulation](http://doi.org/10.2139/ssrn.3933677), *SSRN Electronic Journal*, Legal Priorities Project working paper no. 6.\n\nRelated entries\n---------------\n\n[AI governance](https://forum.effectivealtruism.org/topics/ai-governance) | [law](https://forum.effectivealtruism.org/topics/law) | [policy](https://forum.effectivealtruism.org/topics/policy-change) | [Windfall Clause](https://forum.effectivealtruism.org/topics/windfall-clause)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "ftyDfKF45Gig2u4sx",
    "name": "Standards and regulation",
    "core": false,
    "slug": "standards-and-regulation",
    "oldSlugs": null,
    "postCount": 17,
    "description": {
      "markdown": "Further reading\n---------------\n\nCihon, Peter (2019) [Standards for AI governance: International standards to enable global coordination in AI research & development](https://www.fhi.ox.ac.uk/wp-content/uploads/Standards_-FHI-Technical-Report.pdf), Future of Humanity Institute, University of Oxford.\n\nRelated entries\n---------------\n\n[AI governance](https://forum.effectivealtruism.org/topics/ai-governance) | [California effect](https://forum.effectivealtruism.org/topics/california-effect) | [corporate governance](https://forum.effectivealtruism.org/topics/corporate-governance) | [law](https://forum.effectivealtruism.org/topics/law) | [policy](https://forum.effectivealtruism.org/topics/policy-change)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "GQiD9puGQ5fWYLWS8",
    "name": "Practical",
    "core": false,
    "slug": "practical",
    "oldSlugs": null,
    "postCount": 72,
    "description": {
      "markdown": "**Practical** collects posts that offer practical advice, tips, recommendations, or are otherwise meant to be practically useful.\n\nRelated entries\n---------------\n\n[college advice](https://forum.effectivealtruism.org/tag/college-advice) | [personal development](https://forum.effectivealtruism.org/tag/personal-development)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "fCcrMpyRbozMfwYPF",
    "name": "Application announcements",
    "core": false,
    "slug": "application-announcements",
    "oldSlugs": null,
    "postCount": 79,
    "description": {
      "markdown": "**Application announcements** collects posts announcing applications for jobs, grants, events, or other activities."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "MXALuBaehAd3pqQ3g",
    "name": "Twitter",
    "core": false,
    "slug": "twitter",
    "oldSlugs": null,
    "postCount": 12,
    "description": {
      "markdown": "**Twitter** collects posts related to the microblogging and social networking service.\n\nExternal links\n--------------\n\n[Effective Altruism](https://twitter.com/i/lists/1504834614485032960). List of Twitter accounts that regularly tweet EA-relevant content."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "JpjRa7Ht3nLjncarx",
    "name": "Neutrality",
    "core": false,
    "slug": "neutrality",
    "oldSlugs": null,
    "postCount": 14,
    "description": {
      "markdown": "**Neutrality** is the notion that we ought to be neutral with respect to some feature. For instance, advocates of cause neutrality argue that we should be neutral regarding what cause to prioritize, and instead choose cause based on impartial assessments of impact. Neutrality categories include:\n\n*   [Cause neutrality](https://forum.effectivealtruism.org/tag/cause-neutrality)\n*   [Means neutrality](https://forum.effectivealtruism.org/tag/means-neutrality)\n*   [Risk neutrality](https://forum.effectivealtruism.org/topics/risk-aversion)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "bFyyNcLQNK96HWGgH",
    "name": "Means neutrality",
    "core": false,
    "slug": "means-neutrality",
    "oldSlugs": null,
    "postCount": 1,
    "description": {
      "markdown": "**Means neutrality** (sometimes called **means impartiality**^[\\[1\\]](#fnr85q9odhdl)^) is the view that we should select whichever means best promote our chosen ends.\n\nFurther reading\n---------------\n\nSchubert, Stefan (2017) [Understanding cause-neutrality](https://www.centreforeffectivealtruism.org/blog/understanding-cause-neutrality), *Centre For Effective Altruism*, March 10 (updated 30 December 2020).\n\nRelated entries\n---------------\n\n[cause neutrality](https://forum.effectivealtruism.org/tag/cause-neutrality) | [neutrality](https://forum.effectivealtruism.org/topics/neutrality)\n\n1.  ^**[^](#fnrefr85q9odhdl)**^\n    \n    Schubert, Stefan (2017) [Understanding cause-neutrality](https://www.centreforeffectivealtruism.org/blog/understanding-cause-neutrality), *Centre For Effective Altruism*, March 10 (updated 30 December 2020)."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "Bm7a3oPcZWpfyQkTn",
    "name": "Crux",
    "core": false,
    "slug": "crux",
    "oldSlugs": null,
    "postCount": 8,
    "description": {
      "markdown": "A person's **crux** for holding a view on some topic is any belief on which that view depends, such that if the person ceased to have that belief, they would no longer hold the view. For example, a crux for someone who works in [AI safety](https://forum.effectivealtruism.org/tag/ai-safety) might be the belief that [transformative artificial intelligence](https://forum.effectivealtruism.org/tag/transformative-artificial-intelligence) will arrive within the next few decades: ceasing to have this belief will cause the person to work on other causes or problems.^[\\[1\\]](#fnyw6bqo1xnx)^\n\nA similar concept is that of a **true rejection**, or a person's crux for rejecting a view. Often, what is raised as an objection is not a true rejection, because the critic would still reject the view even if the objection was fully addressed.^[\\[2\\]](#fnqqhvhm141v)^\n\nAnother related concept is that of a [crucial consideration](https://forum.effectivealtruism.org/topics/crucial-consideration), or a consideration that warrants a major reassessment of a cause or intervention. A crucial consideration may be regarded as a  reason for rejecting a belief which is currently a crux for some cause or intervention, especially one considered to be high-priority.\n\nFurther reading\n---------------\n\nSabien, Duncan (2021) [*Participant Handbook*](https://www.rationality.org/files/CFAR_Handbook_2021-01.pdf), Center for Applied Rationality, pp. 91–99.\n\nRelated entries\n---------------\n\n[crucial consideration](https://forum.effectivealtruism.org/tag/crucial-consideration) | [epistemology](https://forum.effectivealtruism.org/tag/epistemology)\n\n1.  ^**[^](#fnrefyw6bqo1xnx)**^\n    \n    Shlegeris, Buck (2020) [My personal cruxes for working on AI safety](https://forum.effectivealtruism.org/posts/Ayu5im98u8FeMWoBZ/my-personal-cruxes-for-working-on-ai-safety), *Effective Altruism Forum*, February 13.\n    \n2.  ^**[^](#fnrefqqhvhm141v)**^\n    \n    Yudkowsky, Eliezer (2008) [Is that your true rejection?](https://www.lesswrong.com/posts/TGux5Fhcd7GmTfNGC/is-that-your-true-rejection), *LessWrong*, December 6."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "PQiqG7pJoCa8imrfn",
    "name": "Waitlist Zero",
    "core": false,
    "slug": "waitlist-zero",
    "oldSlugs": null,
    "postCount": 4,
    "description": {
      "markdown": "**Waitlist Zero** is an advocacy group dedicated to promoting [living kidney transplantation](https://forum.effectivealtruism.org/topics/kidney-donation).\n\nFunding\n-------\n\nAs of July 2022, Waitlist Zero has received $700,000 in grants from [Open Philanthropy](https://forum.effectivealtruism.org/tag/open-philanthropy).^[\\[1\\]](#fnci814wtrlvh)^\n\nFurther reading\n---------------\n\nBerger, Alexander (2015) [A conversation with Josh Morrison](https://www.openphilanthropy.org/sites/default/files/Josh_Morrison_09-15-15_%28public%29.pdf), *Open Philanthropy*, September 15.\n\nBerger, Alexander (2016) [A conversation with Josh Morrison](https://www.openphilanthropy.org/sites/default/files/Josh_Morrison_02-05-2016_%28public%29.pdf), *Open Philanthropy*, February 5.\n\nBerger, Alexander (2016) [A conversation with Josh Morrison](https://www.openphilanthropy.org/sites/default/files/Josh_Morrison_04-25-16_%28public%29.pdf), *Open Philanthropy*, April 25.\n\nBerger, Alexander (2017) [A conversation with Josh Morrison](https://www.openphilanthropy.org/sites/default/files/Josh_Morrison_08-25-17_%28public%29.pdf), *Open Philanthropy*, August 25.\n\nBerger, Alexander & Nicole Ross (2016) [A conversation with Josh Morrison](https://www.openphilanthropy.org/files/Conversations/Josh_Morrison_10-20-16_(public).pdf), *Open Philanthropy*, October 20.\n\nExternal links\n--------------\n\n[Waitlist Zero](http://waitlistzero.org/). Official website.\n\nRelated entries\n---------------\n\n[kidney donation](https://forum.effectivealtruism.org/tag/kidney-donation)\n\n1.  ^**[^](#fnrefci814wtrlvh)**^\n    \n    Open Philanthropy (2014) [Grants database: Waitlist Zero](https://www.openphilanthropy.org/grants/?q=&organization-name=waitlist-zero), *Open Philanthropy*, September."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "TszgAx7tNBpzh67C6",
    "name": "Welfare Footprint Project",
    "core": false,
    "slug": "welfare-footprint-project",
    "oldSlugs": null,
    "postCount": 2,
    "description": {
      "markdown": "The **Welfare Footprint Project** (previously known as the **Center for Welfare Metrics**) is a multidisciplinary research effort to quantify the impact of animal products on the [negative affective experiences](https://forum.effectivealtruism.org/topics/pain-and-suffering) of [farmed animals](https://forum.effectivealtruism.org/topics/farmed-animal-welfare).\n\nFunding\n-------\n\nAs of July 2022, the Welfare Footprint Project and its principal investigators have received over $980,000 in funding from [Open Philanthropy](https://forum.effectivealtruism.org/tag/open-philanthropy).^[\\[1\\]](#fn4xw79ui19zq)^\n\nFurther reading\n---------------\n\nSchuck-Paim, Cynthia & Wladimir J. Alonso (2021) [*Quantifying Pain Laying Hens: A Blueprint for the Comparative Analysis of Welfare in Animals*](https://welfarefootprint.org/book-laying-hens/).\n\nSchuck-Paim, Cynthia & Wladimir J. Alonso (2022) [*Quantifying Pain in Broiler Chickens: Impact of the Better Chicken Commitment and Adoption of Slower-Growing Breeds on Broiler Welfare*](https://welfarefootprint.org/broilers/).\n\nSt. Jules, Michael (2021) [Welfare Footprint Project - a blueprint for quantifying animal pain](https://forum.effectivealtruism.org/posts/c5HbABYR2bbCnP4Rg/welfare-footprint-project-a-blueprint-for-quantifying-animal), *Effective Altruism Forum*, June 26.\n\nExternal links\n--------------\n\n[Welfare Footprint Project](https://welfarefootprint.org/). Official website.\n\n[Apply for a job](https://welfarefootprint.org/get-involved/jobs/).\n\nRelated entries\n---------------\n\n[consumption of animal products](https://forum.effectivealtruism.org/tag/consumption-of-animal-products) | [farmed animal welfare](https://forum.effectivealtruism.org/tag/farmed-animal-welfare) | [pain and suffering](https://forum.effectivealtruism.org/tag/pain-and-suffering)\n\n1.  ^**[^](#fnref4xw79ui19zq)**^\n    \n    Open Philanthropy (2022) [Grants database: Center for Welfare Metrics, Cynthia Schuck & Wladimir Alonso](https://www.openphilanthropy.org/grants/?q=&organization-name=center-for-welfare-metrics&organization-name=cynthia-schuck-wladimir-alonso), *Open Philanthropy*."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "gP4rpeK3CoE7mfrxK",
    "name": "Criticism of longtermism and existential risk studies",
    "core": false,
    "slug": "criticism-of-longtermism-and-existential-risk-studies",
    "oldSlugs": [
      "criticisms-of-longtermism-and-existential-risk-studies"
    ],
    "postCount": 40,
    "description": {
      "markdown": "**Criticism of longtermism and existential risk studies** collects critical discussions of [longtermism](https://forum.effectivealtruism.org/tag/longtermism) and [existential risk](https://forum.effectivealtruism.org/tag/existential-risk) studies.\n\nFurther reading\n---------------\n\nMelchin, Denise (2021) [Why I am probably not a longtermist](https://forum.effectivealtruism.org/posts/Jxfq6xCP9ZoTBFewA/why-i-am-probably-not-a-longtermist), *Effective Altruism Forum*, September 23.\n\nTomasik, Brian (2015) [Should altruists focus on reducing short-term or far-future suffering?](https://reducing-suffering.org/altruists-focus-reducing-short-term-far-future-suffering/), *Essays on Reducing Suffering*, February 6 (updated 17 March 2015).\n\nWebb, Duncan (2021) [Formalising the “washing out hypothesis”](https://forum.effectivealtruism.org/posts/z2DkdXgPitqf98AvY/formalising-the-washing-out-hypothesis), *Effective Altruism Forum*, March 25.\n\nRelated entries\n---------------\n\n[cluelessness](https://forum.effectivealtruism.org/tag/cluelessness) | [criticism of effective altruism](https://forum.effectivealtruism.org/tag/criticism-of-effective-altruism) | [criticism of effective altruist causes](https://forum.effectivealtruism.org/tag/criticism-of-effective-altruist-causes) | [existential risk](https://forum.effectivealtruism.org/tag/existential-risk) | [global health and wellbeing](https://forum.effectivealtruism.org/topics/global-health-and-wellbeing) | [longtermism](https://forum.effectivealtruism.org/tag/longtermism)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "xEnFbH3FjEzXhinGc",
    "name": "News relevant to effective altruism",
    "core": false,
    "slug": "news-relevant-to-effective-altruism",
    "oldSlugs": [
      "in-the-news"
    ],
    "postCount": 12,
    "description": {
      "markdown": "**News relevant to effective altruism** collects news stories relevant from an [effective altruism](https://forum.effectivealtruism.org/tag/effective-altruism) perspective. This is different from \"[effective altruism in the media](https://forum.effectivealtruism.org/topics/effective-altruism-in-the-media),\" which is about media coverage of effective altruism. \n\nRelated entries\n---------------\n\n[Future Perfect](https://forum.effectivealtruism.org/tag/future-perfect) | [journalism](https://forum.effectivealtruism.org/tag/journalism) | [in the media](https://forum.effectivealtruism.org/topics/effective-altruism-in-the-media)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "N3i6zieyCpDjTWC3C",
    "name": "EAecon",
    "core": false,
    "slug": "eaecon",
    "oldSlugs": [
      "eaecon"
    ],
    "postCount": 2,
    "description": {
      "markdown": "**EAecon** is a project to [build effective altruism](https://forum.effectivealtruism.org/topics/building-effective-altruism-1) among [economists](https://forum.effectivealtruism.org/tag/economics).\n\nFurther reading\n---------------\n\nJabarian, Brian (2022) [Introducing EAecon: Community-building project](https://forum.effectivealtruism.org/posts/9gLtXR6KkZEYie8Au/introducing-eaecon-community-building-project), *Effective Altruism Forum*, May 30.\n\nRelated entries\n---------------\n\n[building effective altruism](https://forum.effectivealtruism.org/tag/building-effective-altruism) | [economics](https://forum.effectivealtruism.org/tag/economics)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "pdbetNaMJCebi4Mmc",
    "name": "Non-trivial",
    "core": false,
    "slug": "non-trivial",
    "oldSlugs": [
      "non-trivial-pursuits"
    ],
    "postCount": 2,
    "description": {
      "markdown": "**Non-trivial** is an online platform aimed at introducing core ideas in [effective altruism](https://forum.effectivealtruism.org/topics/effective-altruism) to talented teenagers.\n\nFunding\n-------\n\nAs of July 2022, Non-trivial has received $1 million in funding the [Future Fund](https://forum.effectivealtruism.org/topics/future-fund).^[\\[1\\]](#fnic6zprt800r)^\n\nExternal links\n--------------\n\n[Non-trivial](https://non-trivial.org/). Official website.\n\nRelated entries\n---------------\n\n[80,000 Hours](https://forum.effectivealtruism.org/tag/80-000-hours) | [Probably Good](https://forum.effectivealtruism.org/tag/probably-good)\n\n1.  ^**[^](#fnrefic6zprt800r)**^\n    \n    Future Fund (2022) [Our grants and investments: Non-trivial Pursuits](https://ftxfuturefund.org/all-grants/?_organization_name=non-trivial-pursuits), *Future Fund*."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "AeD3QMnawJg4veEeR",
    "name": "Engineering",
    "core": false,
    "slug": "engineering",
    "oldSlugs": null,
    "postCount": 5,
    "description": {
      "markdown": "**Engineering** is the application of science to the optimum conversion of the resources of nature to the uses of humankind.\n\nExternal links\n--------------\n\n[High Impact Engineers](https://www.highimpactengineers.org/) \\- an organisation helping to increase the quantity of impactful work done by engineers.  [Slack server](https://bit.ly/HIEng-slack) for discussions about engineering within the [effective altruism](https://forum.effectivealtruism.org/tag/effective-altruism) community.\n\nRelated entries\n---------------\n\n[climate engineering](https://forum.effectivealtruism.org/tag/climate-engineering) | [software engineering](https://forum.effectivealtruism.org/tag/software-engineering)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "2aCWGwb9cJFQSxzzG",
    "name": "Asterisk",
    "core": false,
    "slug": "asterisk",
    "oldSlugs": null,
    "postCount": 1,
    "description": {
      "markdown": "**Asterisk** is a quarterly journal of ideas related to [effective altruism](https://forum.effectivealtruism.org/tag/effective-altruism).\n\nFurther reading\n---------------\n\nCollier, Clara (2022) [Introducing Asterisk](https://forum.effectivealtruism.org/posts/Mts84Mv5cFHRYBBA8/introducing-asterisk), *Effective Altruism Forum*, May 26."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "Wj5wb2JnAwBJj2zj4",
    "name": "Giving What We Can Newsletter",
    "core": false,
    "slug": "giving-what-we-can-newsletter",
    "oldSlugs": null,
    "postCount": 17,
    "description": {
      "markdown": "The **Giving What We Can Newsletter** is a monthly [newsletter](https://forum.effectivealtruism.org/tag/newsletters) by [Giving What We Can](https://forum.effectivealtruism.org/tag/giving-what-we-can).\n\nExternal links\n--------------\n\n[Giving What We Can Newsletter](https://www.givingwhatwecan.org/newsletter/). Official website.\n\nRelated entries\n---------------\n\n[Giving What We Can](https://forum.effectivealtruism.org/tag/giving-what-we-can) | [newsletters](https://forum.effectivealtruism.org/tag/newsletters)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "CKr5QriGiF4CoDAMs",
    "name": "Sequence vs. cluster thinking",
    "core": false,
    "slug": "sequence-vs-cluster-thinking",
    "oldSlugs": null,
    "postCount": 4,
    "description": {
      "markdown": "**Sequence thinking** and **cluster thinking** are two contrasting approaches to thinking about decisions. [Holden Karnofsky](https://forum.effectivealtruism.org/tag/holden-karnofsky), who introduced the distinction, characterizes the contrast as follows:^[\\[1\\]](#fn0loxuwhuqodr)^\n\n> **Sequence thinking** involves making a decision based on a single model of the world: breaking down the decision into a set of key questions, taking one’s best guess on each question, and accepting the conclusion that is implied by the set of best guesses (an excellent example of this sort of thinking is Robin Hanson’s discussion of cryonics). It has the form: “A, and B, and C … and N; therefore X.” Sequence thinking has the advantage of making one’s assumptions and beliefs highly transparent, and as such it is often associated with finding ways to make counterintuitive comparisons.\n> \n> **Cluster thinking** – generally the more common kind of thinking – involves approaching a decision from multiple perspectives (which might also be called “mental models”), observing which decision would be implied by each perspective, and weighing the perspectives in order to arrive at a final decision. Cluster thinking has the form: “Perspective 1 implies X; perspective 2 implies not-X; perspective 3 implies X; … therefore, weighing these different perspectives and taking into account how much uncertainty I have about each, X.” Each perspective might represent a relatively crude or limited pattern-match (e.g., “This plan seems similar to other plans that have had bad results”), or a highly complex model; the different perspectives are combined by weighing their conclusions against each other, rather than by constructing a single unified model that tries to account for all available information.\n\nFurther reading\n---------------\n\nKarnofsky, Holden (2014) [Sequence thinking vs. cluster thinking](https://blog.givewell.org/2014/06/10/sequence-thinking-vs-cluster-thinking/), *The GiveWell Blog*, June 10 (updated 25 July 2016).\n\nRelated entries\n---------------\n\n[decision theory](https://forum.effectivealtruism.org/topics/decision-theory) | [epistemology](https://forum.effectivealtruism.org/tag/epistemology) | [model uncertainty](https://forum.effectivealtruism.org/tag/model-uncertainty)\n\n1.  ^**[^](#fnref0loxuwhuqodr)**^\n    \n    Karnofsky, Holden (2014) [Sequence thinking vs. cluster thinking](https://blog.givewell.org/2014/06/10/sequence-thinking-vs-cluster-thinking/), *The GiveWell Blog*, June 10 (updated 25 July 2016)."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "hswjjhoS6RF9DrKEu",
    "name": "Human challenge trials",
    "core": false,
    "slug": "human-challenge-trials",
    "oldSlugs": null,
    "postCount": 8,
    "description": {
      "markdown": "**Human challenge trials** (also called **human challenge studies** and **human infection studies**) are clinical trials that involve deliberately exposing willing volunteers to an infectious disease. Human challenge trials have been used for several diseases, including influenza, typhoid fever, cholera, [malaria](https://forum.effectivealtruism.org/topics/malaria) and, more recently, [COVID-19](https://forum.effectivealtruism.org/topics/covid-19-pandemic).\n\nFurther reading\n---------------\n\nWellcome (2018) [What are human infection studies and why do we need them?](https://wellcome.org/news/what-are-human-infection-studies-and-why-do-we-need-them-covid-19), *Wellcome*, December (updated 28 June 2021).\n\nRelated entries\n---------------\n\n[1Day Sooner](https://forum.effectivealtruism.org/tag/1day-sooner) | [COVID-19 pandemic](https://forum.effectivealtruism.org/tag/covid-19-pandemic)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "xPgDmQ8nPZhtva9k8",
    "name": "Cause Exploration Prizes",
    "core": false,
    "slug": "cause-exploration-prizes",
    "oldSlugs": null,
    "postCount": 169,
    "description": {
      "markdown": "Use this tag when you publish a submission for the [**Cause Exploration Prizes**](https://www.causeexplorationprizes.com/).\n\nIf you're reading this in August and you feel like there are way too many submissions on the Forum, remember that you can [filter a tag](https://forum.effectivealtruism.org/posts/88cCS6Bympb9sRov6/forum-update-tags-are-live-go-use-them#New_filtering_options) to hide or de-emphasize posts that use it!"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "4eyeLKC64Yvznzt6Z",
    "name": "Philosophy",
    "core": false,
    "slug": "philosophy",
    "oldSlugs": null,
    "postCount": 17,
    "description": {
      "markdown": "**Philosophy** is the systematized investigation of the most general and abstract features of the world.\n\nPhilosophy as a career\n----------------------\n\n[80,000 Hours](https://forum.effectivealtruism.org/tag/80-000-hours)' shallow profile rates philosophy a \"sometimes recommended\" career, especially for its potential to do research in a variety of [neglected](https://forum.effectivealtruism.org/topics/neglectedness) areas and its potential for advocacy through teaching or public engagement.^[\\[1\\]](#fn0l5oqns4bsx8)^\n\nFurther reading\n---------------\n\nMacAskill, William (2012a) [How to be a high impact philosopher](https://80000hours.org/2012/05/how-to-be-a-high-impact-philosopher/), *80,000 Hours*, May 8.\n\nMacAskill, William (2012b) [How to be a high impact philosopher, part II](https://80000hours.org/2012/09/how-to-be-a-high-impact-philosopher-part-ii/), *80,000 Hours*, September 27.\n\nNagel, Thomas (1987) [*What Does It All Mean? A Very Short Introduction to Philosophy*](https://en.wikipedia.org/wiki/Special:BookSources/0-19-505292-7), Oxford: Oxford University Press.\n\nRelated entries\n---------------\n\n[moral philosophy](https://forum.effectivealtruism.org/tag/moral-philosophy) | [philosophy of effective altruism](https://forum.effectivealtruism.org/tag/philosophy-of-effective-altruism) | [philosophy of mind](https://forum.effectivealtruism.org/tag/philosophy-of-mind)\n\n1.  ^**[^](#fnref0l5oqns4bsx8)**^\n    \n    Koehler, Arden & William MacAskill (2019) [Philosophy academia](https://80000hours.org/career-reviews/philosophy-academia/), *80,000 Hours*, August."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "rFzSdxvrz3a79aLuu",
    "name": "Environmental science",
    "core": false,
    "slug": "environmental-science",
    "oldSlugs": [
      "environment"
    ],
    "postCount": 7,
    "description": {
      "markdown": "**Environmental science** is an interdisciplinary academic field that studies environmental problems and human impacts on the environment.\n\nRecommendations\n---------------\n\nIn [*The Precipice: Existential Risk and the Future of Humanity*](https://forum.effectivealtruism.org/tag/the-precipice), [Toby Ord](https://forum.effectivealtruism.org/tag/toby-ord) offers several [policy](https://forum.effectivealtruism.org/tag/policy) and [research](https://forum.effectivealtruism.org/tag/research) recommendations for handling risks from environmental damage:^[\\[2\\]](#fnchcrqxsxdus)^\n\n*   Improve our understanding of whether any kind of resource depletion currently poses an [existential risk](https://forum.effectivealtruism.org/topics/existential-risk).\n*   Improve our understanding of current [biodiversity loss](https://forum.effectivealtruism.org/topics/biodiversity-loss) (both regional and global) and how it compares to that of past extinction events.\n*   Create a database of existing biological diversity to preserve the genetic material of threatened species.\n\nRelated entries\n---------------\n\n[biodiversity loss](https://forum.effectivealtruism.org/tag/biodiversity-loss) | [climate change](https://forum.effectivealtruism.org/tag/climate-change)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "idxzdxPtX3wconfu5",
    "name": "Effective altruism in Spanish",
    "core": false,
    "slug": "effective-altruism-in-spanish",
    "oldSlugs": [
      "spanish-language"
    ],
    "postCount": 5,
    "description": {
      "markdown": "**Effective altruism in Spanish** collects posts written in the Spanish language.\n\nExternal links\n--------------\n\n[Riesgos Catastróficos Globales](https://riesgoscatastroficosglobales.com/). Web page introducing [Global catastrophic risk](https://forum.effectivealtruism.org/tag/global-catastrophic-risk) to a Spanish-speaking audience.\n\n[Altruismo Eficaz y Racionalidad](https://altruismo-eficaz.slack.com/join/shared_invite/zt-9dcv7eki-jrN6GerS0NAI~97RH4dB2A#/shared-invite/email). Slack group for the Spanish-speaking [effective altruism](https://forum.effectivealtruism.org/topics/effective-altruism) and [rationality](https://forum.effectivealtruism.org/topics/rationality-community) communities.\n\nRelated entries\n---------------\n\n[Ayuda Efectiva](https://forum.effectivealtruism.org/tag/ayuda-efectiva)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "RLLZFJJfMggLAAXpf",
    "name": "Explore-exploit tradeoff",
    "core": false,
    "slug": "explore-exploit-tradeoff",
    "oldSlugs": null,
    "postCount": 3,
    "description": {
      "markdown": "The **explore-exploit tradeoff** (or **exploration-exploitation trade-off**) is the choice between seeking new information (\"explore\") or acting on the information already available (\"exploit\").\n\nFurther reading\n---------------\n\nConceptually (2018) [Explore-exploit tradeoff](https://conceptually.org/concepts/explore-or-exploit), *Conceptually*.\n\nKuhn, Ben (2013) [Exploration and exploitation](https://www.benkuhn.net/exploration/), *Ben Kuhn’s Blog*, June.\n\nRelated entries\n---------------\n\n[value of information](https://forum.effectivealtruism.org/tag/value-of-information)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "HTzkTP8LsPCDpyrjP",
    "name": "Windfall Clause",
    "core": false,
    "slug": "windfall-clause",
    "oldSlugs": null,
    "postCount": 6,
    "description": {
      "markdown": "The **Windfall Clause** is a proposed mechanism by which AI firms could commit to donate a significant amount of any eventual extremely large profits. The idea was first proposed by [Nick Bostrom](https://forum.effectivealtruism.org/topics/nick-bostrom),^[\\[1\\]](#fn5eepeewx6al)^ and subsequently developed  by the [Centre for the Governance of AI](https://forum.effectivealtruism.org/tag/centre-for-the-governance-of-ai). \n\nFurther reading\n---------------\n\nO’Keefe, Cullen (2019) [Cullen O’Keefe: The Windfall Clause — sharing the benefits of advanced AI](https://forum.effectivealtruism.org/posts/eCihFiTmg748Mnoac/cullen-o-keefe-the-windfall-clause-sharing-the-benefits-of), *Effective Altruism Global*, June 23.\n\nO'Keefe, Cullen *et al.* (2020) [The Windfall Clause: Distributing the benefits of AI for the common good](https://www.fhi.ox.ac.uk/wp-content/uploads/Windfall-Clause-Report.pdf), Centre for the Governance of AI, Future of Humanity Institute, University of Oxford.\n\nO’Keefe, Cullen *et al.* (2020) [The Windfall Clause: Distributing the benefits of AI for the common good](https://doi.org/10.1145/3375627.3375842), *Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society*, pp. 327–331.\n\nRelated entries\n---------------\n\n[AI governance](https://forum.effectivealtruism.org/topics/ai-governance) | [antitrust law](https://forum.effectivealtruism.org/topics/antitrust-law) | [law](https://forum.effectivealtruism.org/topics/law) | [*Superintelligence*](https://forum.effectivealtruism.org/topics/superintelligence-book)  | [transformative artificial intelligence](https://forum.effectivealtruism.org/tag/transformative-artificial-intelligence)\n\n1.  ^**[^](#fnref5eepeewx6al)**^\n    \n    Bostrom, Nick (2014) [*Superintelligence: Paths, Dangers, Strategies*](https://en.wikipedia.org/wiki/Special:BookSources/978-0-19-967811-2), Oxford: Oxford University Press, p. 254."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "mpAppihEusrfX8ojX",
    "name": "Architecture",
    "core": false,
    "slug": "architecture",
    "oldSlugs": null,
    "postCount": 2,
    "description": {
      "markdown": "Related entries\n---------------\n\n[career choice](https://forum.effectivealtruism.org/topics/career-choice)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5gN3evxkrEJqSTbGS",
    "name": "Theory of change",
    "core": false,
    "slug": "theory-of-change",
    "oldSlugs": null,
    "postCount": 16,
    "description": {
      "markdown": "A **theory of change** is a set of hypotheses about how a project—such as an intervention, an organization,  or a movement—will accomplish its goals.\n\nIt is often created by starting with the goals, and then identifying the penultimate step that leads to the goals. The step that leads to the penultimate step is then identified, and so on, until the first steps are known. This method is called backward induction, backwards mapping or backchaining.\n\nFurther reading\n---------------\n\nSiegmann, Charlotte (2022) [Collection of resources about theories of change](https://forum.effectivealtruism.org/posts/7kjXFpj7oGFShzFNH/charlotte-s-shortform?commentId=tRubEJspXkrLD6dni), *Effective Altruism Forum*, May 15.\n\nRelated entries\n---------------\n\n[org strategy](https://forum.effectivealtruism.org/tag/org-strategy) | [research methods](https://forum.effectivealtruism.org/tag/research-methods)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "xL7bAZBiMqQyBfmyX",
    "name": "Healthier Hens",
    "core": false,
    "slug": "healthier-hens",
    "oldSlugs": null,
    "postCount": 6,
    "description": {
      "markdown": "**Healthier Hens** is an organization focused on addressing [micronutrient deficiencies](https://forum.effectivealtruism.org/topics/micronutrient-deficiency) in egg-laying hens through feed fortification.\n\nHistory\n-------\n\nHealthier Hens was launched in October 2021 with a $100,000 seed grant from [Charity Entrepreneurship](https://forum.effectivealtruism.org/tag/charity-entrepreneurship)'s incubation program.^[\\[1\\]](#fns1zfk6drqmh)^\n\nFurther reading\n---------------\n\nEsparza, Isaac & Lukas Jasiunas (2021) [Introducing Healthier Hens](https://forum.effectivealtruism.org/posts/4QGhyXjXM4yJBvNap/introducing-healthier-hens), *Effective Altruism Forum*, October 25.\n\nExternal links\n--------------\n\n[Healthier Hens](https://www.healthierhens.com/). Official website.\n\n[Apply for a job](https://www.healthierhens.com/careers).\n\nRelated entries\n---------------\n\n[farmed animal welfare](https://forum.effectivealtruism.org/tag/farmed-animal-welfare) | [micronutrient deficiency](https://forum.effectivealtruism.org/topics/micronutrient-deficiency)\n\n1.  ^**[^](#fnrefs1zfk6drqmh)**^\n    \n    Savoie, Joey (2021) [Presenting: 2021 incubated charities (Charity Entrepreneurship)](https://forum.effectivealtruism.org/posts/dAqrQQK42PpjHYoZs/presenting-2021-incubated-charities-charity-entrepreneurship), *Effective Altruism Forum*, October 7."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "ZirQDpKCag4j3Tihu",
    "name": "Student projects",
    "core": false,
    "slug": "student-projects",
    "oldSlugs": [
      "student-project"
    ],
    "postCount": 4,
    "description": {
      "markdown": "**Student projects** collects projects that can be undertaken by students with little to no prior knowledge or experience. Student projects can range from focusing on direct impact, to focusing more on internal development, knowledge transfer and career planning. Examples of projects are: fundraising, organizing retreats, skilling up in AI, preparing lightning talks about EA related topics, and going through the 8-week career planning program by 80,000 Hours. \n\nRelated entries\n---------------\n\n[community projects](https://forum.effectivealtruism.org/topics/community-projects)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5HfPzYvjYyXMKtspa",
    "name": "AI interpretability",
    "core": false,
    "slug": "ai-interpretability",
    "oldSlugs": [
      "ai-interpretability"
    ],
    "postCount": 5,
    "description": {
      "markdown": "**Interpretability** is the ability for the decision processes and inner workings of AI and machine learning systems to be understood by humans or other outside observers.^[\\[1\\]](#fnsg4kuuvihg)^\n\nPresent-day machine learning systems are typically not very transparent or interpretable. You can use a model's output, but the model can't tell you why it made that output. This makes it hard to determine the cause of biases in ML models.^[\\[1\\]](#fnsg4kuuvihg)^\n\nInterpretability is a focus of Chris Olah and [Anthropic](https://forum.effectivealtruism.org/topics/anthropic)'s work, though most AI alignment organisations work on interpretability to some extent, such as [Redwood Research](https://forum.effectivealtruism.org/topics/redwood-research)^[\\[2\\]](#fnwvmtlgzzbfc)^.\n\nRelated entries\n---------------\n\n[AI risk](https://forum.effectivealtruism.org/topics/ai-risk) | [AI safety](https://forum.effectivealtruism.org/topics/ai-safety) | [Artificial intelligence](https://forum.effectivealtruism.org/topics/artificial-intelligence)\n\n1.  ^**[^](#fnrefsg4kuuvihg)**^\n    \n    Multicore (2020) [Transparency / Interpretability (ML & AI)](https://www.alignmentforum.org/tag/transparency-interpretability-ml-and-ai), *AI Alignment Forum*, August 1.\n    \n2.  ^**[^](#fnrefwvmtlgzzbfc)**^\n    \n    Shlegeris, Buck (2022) [Answer to 'How might a herd of interns help with AI or biosecurity research tasks/questions?'](https://forum.effectivealtruism.org/posts/HZacQkvLLeLKT3a6j/how-might-a-herd-of-interns-help-with-ai-or-biosecurity?commentId=XcYk3Pux9WmgEcaoi), *Effective Altruism Forum*, March 21."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "ovqupjaS2zjpc5msX",
    "name": "Antibiotic resistance",
    "core": false,
    "slug": "antibiotic-resistance",
    "oldSlugs": [
      "antibiotic-resistance"
    ],
    "postCount": 8,
    "description": {
      "markdown": "**Antibiotic resistance** is the acquisition of unresponsiveness to an antibiotic by a microorganism that was previously adversely affected by it."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "entupKnnBByQCMBe7",
    "name": "THINK",
    "core": false,
    "slug": "think",
    "oldSlugs": null,
    "postCount": 2,
    "description": {
      "markdown": "**THINK** (**The High Impact Network**) was an early effective altruist organization. It launched in August 2012.^[\\[1\\]](#fn52nxeqiv4x)^\n\nHistory\n-------\n\nDuring its first two years or so, the organization had a volunteer network of around 25 people from the effective altruism and [rationality](https://forum.effectivealtruism.org/tag/rationality-community) communities, with a presence on about a dozen US college campuses. [Kelsey Piper](https://forum.effectivealtruism.org/tag/kelsey-piper) was an early member. Afterward, activity declined, and the chapters were transferred to [LEAN](https://forum.effectivealtruism.org/tag/lean).^[\\[2\\]](#fn56dsptkvask)^\n\nFurther reading\n---------------\n\nArnold, Raymond (2012) [The High Impact Network (THINK) - Launching now](https://www.lesswrong.com/posts/KEcWJSzxFzcYvyLsW/the-high-impact-network-think-launching-now), *LessWrong*, August 7.\n\nExternal links\n--------------\n\n[THINK](http://web.archive.org/web/20210301072422/http://www.thehighimpactnetwork.org). Official website, archived from the original.\n\nRelated entries\n---------------\n\n[history of effective altruism](https://forum.effectivealtruism.org/tag/history-of-effective-altruism)\n\n1.  ^**[^](#fnref52nxeqiv4x)**^\n    \n    Arnold, Raymond (2012) [The High Impact Network (THINK) - Launching now](https://www.lesswrong.com/posts/KEcWJSzxFzcYvyLsW/the-high-impact-network-think-launching-now), *LessWrong*, August 7.\n    \n2.  ^**[^](#fnref56dsptkvask)**^\n    \n    Mark Lee, private communication."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "K7DZskCyxX7wdvq4D",
    "name": "Biodiversity loss",
    "core": false,
    "slug": "biodiversity-loss",
    "oldSlugs": [
      "biodiversity-loss"
    ],
    "postCount": 12,
    "description": {
      "markdown": "**Biodiversity loss** is a decrease in biodiversity within a species, an ecosystem, a given geographic area, or Earth as a whole.^[\\[1\\]](#fnjfvwmb9ryzq)^ **Biodiversity** refers to all aspects of biological diversity, especially including species richness, ecosystem complexity, and genetic variation.^[\\[2\\]](#fnv2nwvuvh7fl)^\n\nRelated entries\n---------------\n\n[climate change](https://forum.effectivealtruism.org/topics/climate-change) | [conservation](https://forum.effectivealtruism.org/topics/conservation) | [environmental science](https://forum.effectivealtruism.org/topics/environmental-science)\n\n1.  ^**[^](#fnrefjfvwmb9ryzq)**^\n    \n    Rafferty, John P. (2019) [Biodiversity loss](https://www.britannica.com/science/biodiversity-loss), *Encyclopedia Britannica*, April 19 (updated 14 June 2019). \n    \n2.  ^**[^](#fnrefv2nwvuvh7fl)**^\n    \n    Allaby, Michael (ed.) (2019) [*A Dictionary of Plant Sciences*](https://doi.org/10.1093/acref/9780198833338.001.0001), 4th ed., Oxford: Oxford University Press."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "2vjDFiDtEH2dDsWPr",
    "name": "World Health Organization",
    "core": false,
    "slug": "world-health-organization",
    "oldSlugs": null,
    "postCount": 2,
    "description": {
      "markdown": "The **World Health Organization** is a specialized agency of the [United Nations](https://forum.effectivealtruism.org/tag/united-nations) established in 1948 to further international cooperation for improved [global health](https://forum.effectivealtruism.org/topics/global-health-and-development).\n\nFurther reading\n---------------\n\nSchneider, Carmen Huckel (2009) [World Health Organization (WHO)](https://en.wikipedia.org/wiki/Special:BookSources/978-0-19-533688-7), in David P. Forsythe (ed.) *Encyclopedia of Human Rights*, vol. 5, Oxford: Oxford University Press, pp. 388–394.\n\nExternal links\n--------------\n\n[World Health Organization](https://www.who.int/). Official website.\n\n[Apply for a job](https://www.who.int/careers).\n\nRelated entries\n---------------\n\n[global health and development](https://forum.effectivealtruism.org/topics/global-health-and-development) | [international organization](https://forum.effectivealtruism.org/tag/international-organization) | [United Nations](https://forum.effectivealtruism.org/topics/united-nations-1)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "iooCtnM4DuSHjsahK",
    "name": "Nucleic Acid Observatory",
    "core": false,
    "slug": "nucleic-acid-observatory",
    "oldSlugs": null,
    "postCount": 2,
    "description": {
      "markdown": "The **Nucleic Acid Observatory** is an organization that aims to prevent [global catastrophic biological risks](https://forum.effectivealtruism.org/tag/global-catastrophic-biological-risk) by detecting novel agents spreading in the human population or environment. Their goal is to achieve reliable early detection of any biological threat by collecting wastewater and other environmental samples at airports and other sites, performing unbiased [metagenomic sequencing](https://forum.effectivealtruism.org/topics/metagenomics), and analyzing the resulting data with novel pathogen-agnostic detection algorithms. It is part of the Sculpting Evolution group at the MIT Media Lab.\n\nFurther reading\n---------------\n\nBradshaw, William, Michael McLaren & Anjali Gopal (2022) [Announcing the Nucleic Acid Observatory project for early detection of catastrophic biothreats](https://forum.effectivealtruism.org/posts/gLPEAFicFBW8BKCnr/announcing-the-nucleic-acid-observatory-project-for-early), *Effective Altruism Forum*, April 29.\n\nThe Nucleic Acid Observatory Consortium (2021) [A global nucleic acid observatory for biodefense and planetary health](http://arxiv.org/abs/2108.02678), arXiv:2108.02678.\n\nExternal links\n--------------\n\n[Nucleic Acid Observatory](https://www.naobservatory.org/). Official website.\n\n[Apply for a job](https://www.naobservatory.org/home#work-with-us).\n\nRelated entries\n---------------\n\n[biosecurity](https://forum.effectivealtruism.org/tag/biosecurity) | [biosurveillance](https://forum.effectivealtruism.org/tag/biosurveillance) | [global catastrophic biological risk](https://forum.effectivealtruism.org/tag/global-catastrophic-biological-risk)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "ugC4gupXHqjvsRhFv",
    "name": "Snakebite",
    "core": false,
    "slug": "snakebite",
    "oldSlugs": null,
    "postCount": 1,
    "description": {
      "markdown": "A **snakebite** is an injury caused by the bite of a snake. **Snakebite envenoming** is a [neglected tropical disease](https://forum.effectivealtruism.org/topics/neglected-tropical-diseases) that results in around 100,000 deaths and  400,000 permanent disabilities every year, according to the [World Health Organization](https://forum.effectivealtruism.org/tag/world-health-organization).^[\\[1\\]](#fnyd74rg5l89n)^\n\nA [cost-effectiveness analysis](https://forum.effectivealtruism.org/tag/cost-effectiveness-analysis) of antivenom for snakebite in 16 West African countries estimated a cost per death averted ranging from $1,997 in Guinea Bissau to $6,205 in Liberia and Sierra Leone, and a cost per [disability-adjusted life-year](https://forum.effectivealtruism.org/topics/adjusted-life-year) (DALY) ranging from $83 for Benin to $281 for Sierra-Leone.^[\\[2\\]](#fnfmtbjakh6v)^\n\nFurther reading\n---------------\n\nBonde, Mathias Kirk (2022) [Snakebites kill 100,000 people every year, here’s what you should know](https://forum.effectivealtruism.org/posts/WyqGircJgCBG9ivNL/snakebites-kill-100-000-people-every-year-here-s-what-you), *Effective Altruism Forum*, April 27.\n\nRelated entries\n---------------\n\n[cause candidates](https://forum.effectivealtruism.org/tag/cause-candidates) | [global health and wellbeing](https://forum.effectivealtruism.org/tag/global-health-and-wellbeing) | [neglected tropical diseases](https://forum.effectivealtruism.org/topics/neglected-tropical-diseases)\n\n1.  ^**[^](#fnrefyd74rg5l89n)**^\n    \n    World Health Organization (2019) [*Snakebite Envenoming: A Strategy for Prevention and Control*](https://en.wikipedia.org/wiki/Special:BookSources/978-92-4-151564-1), Geneva: World Health Organization.\n    \n2.  ^**[^](#fnreffmtbjakh6v)**^\n    \n     Hamza, Muhammad *et al.* (2016) [Cost-effectiveness of antivenoms for snakebite envenoming in 16 countries in West Africa](https://doi.org/10.1371/journal.pntd.0004568), *PLOS Neglected Tropical Diseases*, vol. 10."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "YTpGBSTcELYh76o2u",
    "name": "Supererogation",
    "core": false,
    "slug": "supererogation",
    "oldSlugs": null,
    "postCount": 1,
    "description": {
      "markdown": "In [moral philosophy](https://forum.effectivealtruism.org/tag/moral-philosophy), **supererogation** is the performance of more than what [morality demands](https://forum.effectivealtruism.org/topics/demandingness-of-morality). A **supererogatory** act is morally good, but not morally required.\n\nFurther reading\n---------------\n\nHeyd, David (2002) [Supererogation](https://plato.stanford.edu/entries/supererogation/), *Stanford Encyclopedia of Philosophy*, November 4 (updated 5 November 2019).\n\nRelated entries\n---------------\n\n[deontology](https://forum.effectivealtruism.org/tag/deontology) | [demandingness of morality](https://forum.effectivealtruism.org/tag/demandingness-of-morality) | [moral philosophy](https://forum.effectivealtruism.org/tag/moral-philosophy) | [normative ethics](https://forum.effectivealtruism.org/tag/normative-ethics)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "AnACRjh5AqL3mcKSQ",
    "name": "Corporate governance",
    "core": false,
    "slug": "corporate-governance",
    "oldSlugs": null,
    "postCount": 6,
    "description": {
      "markdown": "Further reading\n---------------\n\nKarnofsky, Holden (2022) [Ideal governance (for companies, countries and more)](https://forum.effectivealtruism.org/posts/hxTFAetiiSL7dZmyb/ideal-governance-for-companies-countries-and-more), *Effective Altruism Forum*, April 7.\n\nRelated entries\n---------------\n\n[AI governance](https://forum.effectivealtruism.org/topics/ai-governance) | [law](https://forum.effectivealtruism.org/topics/law) | [policy](https://forum.effectivealtruism.org/topics/policy-change) | [prediction markets](https://forum.effectivealtruism.org/topics/prediction-markets) | [standards and regulation](https://forum.effectivealtruism.org/topics/standards-and-regulation)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "4dikNJgjJRYB66Eck",
    "name": "Productivity",
    "core": false,
    "slug": "productivity",
    "oldSlugs": null,
    "postCount": 33,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "JDqJmFK9nqb7HSiXj",
    "name": "Grabby aliens",
    "core": false,
    "slug": "grabby-aliens",
    "oldSlugs": [
      "grabby-aliens"
    ],
    "postCount": 5,
    "description": {
      "markdown": "**Grabby aliens** is a model of alien civilizations developed by [Robin Hanson](https://forum.effectivealtruism.org/tag/robin-hanson), Daniel Martin, Calvin McCarter and Jonathan Paulson.\n\nFurther reading\n---------------\n\nAaronson, Scott (2021) [Once we can see them, it’s too late](https://www.scottaaronson.com/blog/?p=5253), *Shtetl-Optimized*, January 30.\n\nHanson, Robin (2021) [A simple model of grabby aliens](https://www.youtube.com/watch?v=0lKliaFllPA), *Foresight Institute*, February 9.\n\nHanson, Robin *et al.* (2021) [If loud aliens explain human earliness, quiet aliens are also rare](https://doi.org/10.3847/1538-4357/ac2369), *The Astrophysical Journal*, vol. 922.\n\nOlson, S. Jay & Toby Ord (2021) [Implications of a search for intergalactic civilizations on prior estimates of human survival and travel speed](http://arxiv.org/abs/2106.13348), arXiv:2106.13348.\n\nExternal links\n--------------\n\n[Grabby Aliens](https://grabbyaliens.com/). A collection of resources.\n\nRelated entries\n---------------\n\n[anthropics](https://forum.effectivealtruism.org/tag/anthropics) | [doomsday argument](https://forum.effectivealtruism.org/tag/doomsday-argument) | [extraterrestrial intelligence](https://forum.effectivealtruism.org/topics/extraterrestrial-intelligence) | [Fermi paradox](https://forum.effectivealtruism.org/topics/fermi-paradox) | [Great Filter](https://forum.effectivealtruism.org/tag/great-filter)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "zceotSS4xhFPoGgY3",
    "name": "Urgent Longtermism",
    "core": false,
    "slug": "urgent-longtermism",
    "oldSlugs": null,
    "postCount": 2,
    "description": {
      "markdown": "**Urgent longtermism** is the position that now is an unusually good time to have an outsized impact on the future (i.e., [the hinge of history](https://forum.effectivealtruism.org/tag/hinge-of-history) hypothesis). In particular, it claims that the current time is particularly pivotal and that we have some scope to change what happens. Ben Todd creates a distinction between two kinds of urgent longtermism: broad and targeted. \n\n**Broad urgent longtermism** posits that we are in a pivotal time, and have influence over the future, but may not be sure what the specific influential events in this time are. Toby Ord advocates for this position in The Precipice. A broad longtermist might focus on reducing [existential risk factors](https://forum.effectivealtruism.org/tag/existential-risk-factor) like great power conflict. \n\n**Targeted urgent longtermism** presumes that we know what the specific influential moments are. Within targeted urgent longtermism there are two main tracks: reducing specific existential risks and creating other path or trajectory changes. \n\nFurther reading\n---------------\n\nKoehler, Arden & Keiran Harris (2020) [Benjamin Todd on varieties of longtermism and things 80,000 Hours might be getting wrong](https://80000hours.org/podcast/episodes/ben-todd-on-varieties-of-longtermism/), *80,000 Hours*, September 1.\n\nRelated entries\n---------------\n\n[broad vs narrow interventions](https://forum.effectivealtruism.org/tag/broad-vs-narrow-interventions) | [hinge of history](https://forum.effectivealtruism.org/tag/hinge-of-history) | [patient altruism](https://forum.effectivealtruism.org/tag/patient-altruism)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "623FBGnads5qSwWcd",
    "name": "High Impact Professionals",
    "core": false,
    "slug": "high-impact-professionals",
    "oldSlugs": null,
    "postCount": 11,
    "description": {
      "markdown": "**High Impact Professionals** (**HIP**) is an organization that focuses on increasing the impact of working professionals in the [effective altruism](https://forum.effectivealtruism.org/tag/effective-altruism) community.\n\nHistory\n-------\n\nHIP was launched in 2021, with a $100,000 seed grant from [Charity Entrepreneurship](https://forum.effectivealtruism.org/tag/charity-entrepreneurship)'s incubation program.^[\\[1\\]](#fn352t6i8enha)^ In May 2022, HIP received a grant of $320,000 from the [Future Fund](https://forum.effectivealtruism.org/topics/future-fund).^[\\[2\\]](#fnqfbdmk2ep9)^\n\nFurther reading\n---------------\n\nFritz, Devon & Federico Speziali (2021) [High Impact Professionals](https://drive.google.com/file/d/1Ai1GBQhq92lUJuprbdfKMnwRhtvg7xDX/view).\n\nExternal links\n--------------\n\n[High Impact Professionals](https://www.highimpactprofessionals.org/). Official website.\n\nRelated entries\n---------------\n\n[earning to give](https://forum.effectivealtruism.org/tag/earning-to-give) | [workplace advocacy](https://forum.effectivealtruism.org/topics/workplace-advocacy) | [workplace groups](https://forum.effectivealtruism.org/topics/workplace-groups)\n\n1.  ^**[^](#fnref352t6i8enha)**^\n    \n    Savoie, Joey (2021) [Presenting: 2021 incubated charities (Charity Entrepreneurship)](https://forum.effectivealtruism.org/posts/dAqrQQK42PpjHYoZs/presenting-2021-incubated-charities-charity-entrepreneurship), *Effective Altruism Forum*, October 7.\n    \n2.  ^**[^](#fnrefqfbdmk2ep9)**^\n    \n    Future Fund (2022) [Our grants and investments: High Impact Professionals](https://ftxfuturefund.org/all-grants/?_organization_name=high-impact-professionals), *Future Fund*."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "33HtuxWcS8ppcihWr",
    "name": "EAGx",
    "core": false,
    "slug": "eagx",
    "oldSlugs": null,
    "postCount": 42,
    "description": {
      "markdown": "**EAGx** conferences are locally-organised conferences for people interested in [effective altruism](https://forum.effectivealtruism.org/tag/effective-altruism). An EAGx event takes place over 1-2 days and will typically involve talks and workshops, opportunities to meet and share advice with other effective altruists, and social events.\n\nExternal links\n--------------\n\n[Effective Altruism Global: EAGx](https://www.eaglobal.org/eagxhome/). Official website.\n\nRelated entries\n---------------\n\n[Effective Altruism Global](https://forum.effectivealtruism.org/tag/effective-altruism-global) | [conferences](https://forum.effectivealtruism.org/tag/conferences)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "BmAxkAtXXLvREtYHh",
    "name": "Community housing",
    "core": false,
    "slug": "community-housing",
    "oldSlugs": [
      "housing"
    ],
    "postCount": 10,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "pBwErD6FGycKomFrA",
    "name": "Exercises",
    "core": false,
    "slug": "exercises",
    "oldSlugs": null,
    "postCount": 7,
    "description": {
      "markdown": "**Exercises** collects posts that discuss activities that individuals or groups can do, that:\n\n1.  Can be fit into a discrete time window\n2.  May be helpful for practice or learning or thinking things through"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "TkHixijxWP2ZmdyiE",
    "name": "Population decline",
    "core": false,
    "slug": "population-decline",
    "oldSlugs": null,
    "postCount": 7,
    "description": {
      "markdown": "Human **population decline** is a reduction in population size. The world population growth rate peaked in 1963 at 2.2%,^[\\[1\\]](#fnrhmc9ptxeye)^ and has since halved. Currently, just about half of the world's population is below replacement.^[\\[2\\]](#fnla2ojb2uq9a)^\n\nFurther reading\n---------------\n\nBricker, Darrell Jay & John Ibbitson (2019) [*Empty Planet: The Shock of Global Population Decline*](https://en.wikipedia.org/wiki/Special:BookSources/978-1-984823-22-9), New York: Crown.\n\nPiper, Kelsey (2019) [We’ve worried about overpopulation for centuries. And we’ve always been wrong.](https://www.vox.com/future-perfect/2019/8/20/20802413/overpopulation-demographic-transition-population-explained), *Vox*, August 20.\n\nRelated entries\n---------------\n\n[economic growth](https://forum.effectivealtruism.org/tag/economic-growth)\n\n1.  ^**[^](#fnrefrhmc9ptxeye)**^\n    \n    Roser, Max, Hannah Ritchie & Esteban Ortiz-Ospina (2013) [World population growth](https://ourworldindata.org/world-population-growth), *Our World in Data*, May 9. \n    \n2.  ^**[^](#fnrefla2ojb2uq9a)**^\n    \n    Gietel-Basten, Stuart & Sergei Scherbov (2019) [Is half the world’s population really below ‘replacement-rate’?](https://doi.org/10.1371/journal.pone.0224985), *PLOS ONE*, vol. 14."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "yembakxdaCMhjum3L",
    "name": "Assistants",
    "core": false,
    "slug": "assistants",
    "oldSlugs": [
      "personal-assistants"
    ],
    "postCount": 6,
    "description": {
      "markdown": "A tag for posts related to personal, executive, research, and other types of **assistants**.\n\nRelated entries\n---------------\n\n[Pineapple Operations](https://forum.effectivealtruism.org/topics/pineapple-operations)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "EpysAgCEJXSXTrnRt",
    "name": "Spillover effects",
    "core": false,
    "slug": "spillover-effects",
    "oldSlugs": null,
    "postCount": 3,
    "description": {
      "markdown": "**Spillover effects** are effects of an activity that spread further than originally intended.\n\nRelated entries\n---------------\n\n[accidental harm](https://forum.effectivealtruism.org/tag/accidental-harm) | [indirect long-term effects](https://forum.effectivealtruism.org/tag/indirect-long-term-effects)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "J7gQeKxPCbALNAuYh",
    "name": "Podcasts",
    "core": false,
    "slug": "podcasts",
    "oldSlugs": null,
    "postCount": 22,
    "description": {
      "markdown": "Further reading\n---------------\n\nStafforini, Pablo (2019) [Effective Altruism podcasts](https://www.listennotes.com/playlists/effective-altruism-podcasts-DEUpnbYFt1P/podcasts/), *Listen Notes*, November 27 (updated 8 April 2022).\n\nRelated entries\n---------------\n\n[80,000 Hours Podcast](https://forum.effectivealtruism.org/tag/80-000-hours-podcast) | [Hear This Idea](https://forum.effectivealtruism.org/tag/hear-this-idea) | [Making Sense](https://forum.effectivealtruism.org/tag/sam-harris#Making_Sense_podcast) | [Rationally Speaking](https://forum.effectivealtruism.org/tag/julia-galef#Rationally_Speaking)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "KnGNLDLyc7R2nwqwm",
    "name": "Documentaries",
    "core": false,
    "slug": "documentaries",
    "oldSlugs": null,
    "postCount": 9,
    "description": {
      "markdown": "Related entries\n---------------\n\n[effective altruism in the media](https://forum.effectivealtruism.org/tag/effective-altruism-in-the-media) | [effective altruism messaging](https://forum.effectivealtruism.org/tag/effective-altruism-messaging/)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "A4qGL4YhJemN5ihGS",
    "name": "Needs funding",
    "core": false,
    "slug": "needs-funding",
    "oldSlugs": null,
    "postCount": 2,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "jbN5dLQgewB7uRZ9u",
    "name": "Needs founders",
    "core": false,
    "slug": "needs-founders",
    "oldSlugs": null,
    "postCount": null,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "jKptoA9Qc8sjkEPPY",
    "name": "Existential risk fiction",
    "core": false,
    "slug": "existential-risk-fiction",
    "oldSlugs": [
      "existential-risk-fiction-non-fiction"
    ],
    "postCount": 23,
    "description": {
      "markdown": "Posts regarding works of **existential risk fiction** (e.g., *Don’t Look Up*, *The Man Who Saved the World*, *Terminator*), either currently in existence or proposed.\n\nRelated entries\n---------------\n\n[effective altruism art and fiction](https://forum.effectivealtruism.org/tag/effective-altruism-art-and-fiction) |  [effective altruism in the media](https://forum.effectivealtruism.org/tag/effective-altruism-in-the-media)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "h35zYKrRrPs6unk4x",
    "name": "Coworking spaces",
    "core": false,
    "slug": "coworking-spaces",
    "oldSlugs": null,
    "postCount": 43,
    "description": {
      "markdown": "The EA community has created multiple **coworking spaces** in cities around the world. Benefits of coworking spaces for individuals and organizations include being able to work more flexibly, easier collaboration with people with similar interests, and economies of scale on office management and resources.\n\nRelated entries\n---------------\n\n[building effective altruism](https://forum.effectivealtruism.org/tag/building-effective-altruism-1) | [Centre for Enabling EA Learning & Research](https://forum.effectivealtruism.org/tag/centre-for-enabling-ea-learning-and-research)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "n4mR2dCWGPguCLzaP",
    "name": "Credo AI",
    "core": false,
    "slug": "credo-ai",
    "oldSlugs": null,
    "postCount": 3,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "tCPXWN5vysYTqi8Cb",
    "name": "Compute governance",
    "core": false,
    "slug": "compute-governance",
    "oldSlugs": null,
    "postCount": 8,
    "description": {
      "markdown": "**Compute governance** is the subfield of [AI governance](https://forum.effectivealtruism.org/topics/ai-governance) concerned with controlling and governing access to computational resources.^[\\[1\\]](#fnhcj3i8o475a)^\n\nFurther reading\n---------------\n\nHeim, Lennart (2021) [Compute governance and conclusions](https://forum.effectivealtruism.org/posts/g6cwjcKMZba4RimJk/compute-governance-and-conclusions-transformative-ai-and), *Effective Altruism Forum*, October 14.\n\nKirchner, Jan Hendrik (2022) [Compute governance: the role of commodity hardware](https://www.lesswrong.com/posts/z8BF9GwcCjeXShC4q/compute-governance-the-role-of-commodity-hardware), *LessWrong*, March 26.\n\nRelated entries\n---------------\n\n[AI governance](https://forum.effectivealtruism.org/topics/ai-governance) | [quantum computing](https://forum.effectivealtruism.org/topics/quantum-computing) | [semiconductors](https://forum.effectivealtruism.org/tag/semiconductors)\n\n1.  ^**[^](#fnrefhcj3i8o475a)**^\n    \n    Kirchner, Jan Hendrik (2022) [Compute governance: the role of commodity hardware](https://www.lesswrong.com/posts/z8BF9GwcCjeXShC4q/compute-governance-the-role-of-commodity-hardware), *LessWrong*, March 26."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "mPDquzDnkBkgi2iKR",
    "name": "Marketing",
    "core": false,
    "slug": "marketing",
    "oldSlugs": null,
    "postCount": 14,
    "description": {
      "markdown": "**Marketing** is the set of activities undertaken to promote the buying or selling of a product or service.\n\nMarketing as a career\n---------------------\n\n[80,000 Hours](https://forum.effectivealtruism.org/tag/80-000-hours)' shallow profile rates marketing a \"sometimes recommended\" career, especially for skill-building and [earning to give](https://forum.effectivealtruism.org/tag/earning-to-give).^[\\[1\\]](#fnqep4ocffxlk)^\n\nFurther reading\n---------------\n\nDuda, Roman (2015) [If you want to make the world a better place should you work in marketing?](https://80000hours.org/career-reviews/work-in-marketing/), *80,000 Hours*, June (updated July 2015).\n\nRelated entries\n---------------\n\n[building effective altruism](https://forum.effectivealtruism.org/tag/building-effective-altruism-1) | [career choice](https://forum.effectivealtruism.org/tag/career-choice) | [effective altruism messaging](https://forum.effectivealtruism.org/tag/effective-altruism-messaging)\n\n1.  ^**[^](#fnrefqep4ocffxlk)**^\n    \n    Duda, Roman (2015) [If you want to make the world a better place should you work in marketing?](https://80000hours.org/career-reviews/work-in-marketing/), *80,000 Hours*, June (updated July 2015)."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "xGGyQvqoisJw7hhqi",
    "name": "Emergency response",
    "core": false,
    "slug": "emergency-response",
    "oldSlugs": [
      "emergency-response-teams"
    ],
    "postCount": 6,
    "description": {
      "markdown": "**Emergency response** is the set of actions taken during and immediately following a disaster.\n\nRelated entries\n---------------\n\n[existential risk factor](https://forum.effectivealtruism.org/tag/existential-risk-factor) | [global catastrophic risk](https://forum.effectivealtruism.org/tag/global-catastrophic-risk)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "69wrutbRBMjiypNmE",
    "name": "Retreats",
    "core": false,
    "slug": "retreats",
    "oldSlugs": null,
    "postCount": 30,
    "description": {
      "markdown": "Further reading\n---------------\n\nGlobal Challenges Project (2022) [Guide to running a retreat/summit](https://handbook.globalchallengesproject.org/packaged-programs/guide-to-running-a-retreatsummit), *Global Challenges Project*.\n\nMcCurdy, Jessica (2019) [Yale retreat handover doc](https://forum.effectivealtruism.org/posts/YGdkLSToQ6rw5QqDP/yale-retreat-handover-doc), *Effective Altruism Forum*, November 7.\n\nExternal links\n--------------\n\n[Canopy Retreats](https://www.canopyretreats.org/). An organization that helps members of the [effective altruism](https://forum.effectivealtruism.org/tag/effective-altruism) community in the planning and running of mid-sized, multi-day retreats.\n\nRelated entries\n---------------\n\n[building effective altruism](https://forum.effectivealtruism.org/tag/building-effective-altruism-1) | [effective altruism groups](https://forum.effectivealtruism.org/tag/effective-altruism-groups) | [fellowships and internships](https://forum.effectivealtruism.org/tag/fellowships-and-internships)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "6TYK9QffFqkSZGFN8",
    "name": "United Nations",
    "core": false,
    "slug": "united-nations-1",
    "oldSlugs": null,
    "postCount": 6,
    "description": {
      "markdown": "The **United Nations** is an intergovernmental organization that \"maintain\\[s\\] international peace and security \\[including by\\] develop\\[ing\\] friendly relations among nations based on respect for the principle of equal rights and self-determination of peoples \\[and that\\] achieve\\[s\\] international co-operation in solving international problems of an economic, social, cultural, or humanitarian character and in promoting and encouraging respect for human rights and for fundamental freedoms for all without distinction as to race, sex, language, or religion \\[and is\\] a centre for harmonizing the actions of nations in the attainment of these common ends.\"^[\\[1\\]](#fnb9z54jrd3es)^\n\nExternal links\n--------------\n\n[United Nations](https://www.un.org/en/). Official website.\n\nRelated entries\n---------------\n\n[European Union](https://forum.effectivealtruism.org/tag/european-union) | [global governance](https://forum.effectivealtruism.org/tag/global-governance) | [international organization](https://forum.effectivealtruism.org/tag/international-organization) | [World Health Organization](https://forum.effectivealtruism.org/topics/world-health-organization)\n\n1.  ^**[^](#fnrefb9z54jrd3es)**^\n    \n    United Nations (1945) [Chapter I: Purposes and Principles](https://treaties.un.org/doc/Publication/CTC/uncharter.pdf), in *Charter of the United Nations and Statute of the International Court of Justice.*"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "kQ4ZANEHFMSz7ZrSk",
    "name": "Pineapple Operations",
    "core": false,
    "slug": "pineapple-operations",
    "oldSlugs": null,
    "postCount": 1,
    "description": {
      "markdown": "**Pineapple Operations** is a free service focused on helping people doing [longtermism](https://forum.effectivealtruism.org/tag/longtermism) work find personal and executive assistants.\n\nExternal links\n--------------\n\n[Pineapple Operations](https://pineappleoperations.org/). Official website.\n\nRelated entries\n---------------\n\n [assistants](https://forum.effectivealtruism.org/topics/assistants) | [community infrastructure](https://forum.effectivealtruism.org/tag/community-infrastructure) | [longtermism](https://forum.effectivealtruism.org/tag/longtermism) | [operations](https://forum.effectivealtruism.org/tag/operations)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "X8jpxbG3xDQXCwLos",
    "name": "Newsletters",
    "core": false,
    "slug": "newsletters",
    "oldSlugs": null,
    "postCount": 26,
    "description": {
      "markdown": "Further reading\n---------------\n\nGertler, Aaron (2019) [List of EA-related email newsletters](https://forum.effectivealtruism.org/posts/vNhfvYWNiejQQps7N/list-of-ea-related-email-newsletters), *Effective Altruism Forum*, October 9.  \n*A list of newsletters related to effective altruism. See the comments for additions and updates.*\n\nRelated entries\n---------------\n\n[Alignment Newsletter](https://forum.effectivealtruism.org/tag/alignment-newsletter) | [Effective Altruism Newsletter](https://forum.effectivealtruism.org/tag/effective-altruism-newsletter) | [Forecasting Newsletter](https://forum.effectivealtruism.org/tag/forecasting-newsletter) | [Future Matters](https://forum.effectivealtruism.org/tag/future-matters) | [Giving What We Can Newsletter](https://forum.effectivealtruism.org/topics/giving-what-we-can-newsletter?sortedBy=new) | [Monthly Overload of Effective Altruism](https://forum.effectivealtruism.org/topics/monthly-overload-of-effective-altruism)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "CuoDsPLQ7YvhSskLr",
    "name": "Innovation in Government Initiative",
    "core": false,
    "slug": "innovation-in-government-initiative",
    "oldSlugs": null,
    "postCount": 1,
    "description": {
      "markdown": "The **Innovation in Government Initiative** (**IGI**, formerly known as **Government Partnership Initiative**) is a [grantmaking](https://forum.effectivealtruism.org/tag/grantmaking) entity that funds research on evidence-based programs to reduce [global poverty](https://forum.effectivealtruism.org/tag/global-poverty) and efforts to pilot and scale those programs. IGI sits within the [Abdul Latif Jameel Poverty Action Lab](https://forum.effectivealtruism.org/tag/abdul-latif-jameel-poverty-action-lab).\n\nEvaluation\n----------\n\nIn 2019, IGI received a $1 million grant from the [Global Health and Development Fund](https://forum.effectivealtruism.org/tag/global-health-and-development-fund).^[\\[1\\]](#fnuvi9quheri)^ The grant was recommended by [GiveWell](https://forum.effectivealtruism.org/tag/givewell).^[\\[2\\]](#fn8rold5a81i5)^\n\n[Founders Pledge](https://forum.effectivealtruism.org/tag/founders-pledge) estimates that IGI is roughly three times as cost-effective as direct cash transfers ^[\\[3\\]](#fnueusgb3awn)^ and recommends IGI as the most promising organization that they have identified in the area of evidence-based [policy](https://forum.effectivealtruism.org/topics/policy).^[\\[4\\]](#fn6oq000whect)^\n\nFurther reading\n---------------\n\nGiveWell (2019) [Innovation in Government Initiative — general support](https://www.givewell.org/research/incubation-grants/innovation-in-government-initiative/december-2018-grant), *GiveWell*, January.\n\nRuhl, Christian (2022) [Innovation in Government Initiative (IGI)](https://founderspledge.com/stories/innovation-in-government-initiative-igi), *Founders Pledge*, April 8.\n\nExternal links\n--------------\n\n[Innovation in Government Initiative](https://www.povertyactionlab.org/initiative/innovation-government-initiative). Official website.\n\nRelated entries\n---------------\n\n[global health and development](https://forum.effectivealtruism.org/tag/global-health-and-development) | [Abdul Latif Jameel Poverty Action Lab](https://forum.effectivealtruism.org/tag/abdul-latif-jameel-poverty-action-lab)\n\n1.  ^**[^](#fnrefuvi9quheri)**^\n    \n    Global Health and Development Fund (2019) [January 2019: J-PAL’s Innovation in Government Initiative](https://funds.effectivealtruism.org/funds/payouts/january-2019-j-pals-innovation-in-government-initiative), *Effective Altruism Funds*, January.\n    \n2.  ^**[^](#fnref8rold5a81i5)**^\n    \n    GiveWell (2019) [Innovation in Government Initiative — general support](https://www.givewell.org/research/incubation-grants/innovation-in-government-initiative/december-2018-grant), *GiveWell*, January.\n    \n3.  ^**[^](#fnrefueusgb3awn)**^\n    \n    Ruhl, Christian (2022) [Innovation in Government Initiative (IGI)](https://founderspledge.com/stories/innovation-in-government-initiative-igi), *Founders Pledge*, April 8.\n    \n4.  ^**[^](#fnref6oq000whect)**^\n    \n    Capriati, Marinella & Christian Ruhl (2022) [Evidence based policy cause area report](https://founderspledge.com/stories/evidence-based-policy-executive-summary), *Founders Pledge*, April 8."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "4saLTjJHsbduczFti",
    "name": "April Fools' Day",
    "core": false,
    "slug": "april-fools-day-1",
    "oldSlugs": [
      "april-1st"
    ],
    "postCount": 27,
    "description": {
      "markdown": "A tag  (or topic page) for collecting posts that participate in the [April Fools' Day](https://en.wikipedia.org/wiki/April_Fools%27_Day) tradition of playing practical jokes and setting up hoaxes.\n\nRelated entries\n---------------\n\n[humor](https://forum.effectivealtruism.org/tag/humor-1)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5xoQz7ZNnXiC2Bhmp",
    "name": "What We Owe the Future",
    "core": false,
    "slug": "what-we-owe-the-future",
    "oldSlugs": [
      "what-we-owe-the-future"
    ],
    "postCount": 42,
    "description": {
      "markdown": "***What We Owe the Future*** is a 2022 book by [William MacAskill](https://forum.effectivealtruism.org/tag/william-macaskill). The book makes the case for [longtermism](https://forum.effectivealtruism.org/tag/longtermism)—defined as the view that positively affecting the [long-run future](https://forum.effectivealtruism.org/tag/long-term-future) is a key moral priority of our time—and explores what follows from that view.\n\nSummary\n=======\n\nThe book makes the case for longtermism and proposes that we can make the future better in two ways: \"by averting permanent catastrophes, thereby ensuring civilisation’s survival; or by changing civilisation’s trajectory to make it better while it lasts...Broadly, ensuring survival increases the quantity of future life; trajectory changes increase its quality\".^:35–36 ^\n\nPart 1: Longtermism\n-------------------\n\nPart 1 introduces and advocates for *longtermism*, which MacAskill defines as \"the idea that positively influencing the long-term future is a key moral priority of our time.\"^: 4^ This part of the book also describes how we, the current generation, can shape the future through our actions.^: 29 ^\n\nMacAskill's argument for longtermism has three parts. First, future people count morally as much as the people alive today, which he supports by drawing the analogy that \"distance in time is like distance in space. People matter even if they live thousands of miles away. Likewise, they matter even if they live thousands of years hence\".^: 10^\n\nSecond, the future is immensely big since humanity may survive for a very long time, and there may be many more people alive at any given time. MacAskill points out that the \"future of civilisation could...be extremely long. The earth will remain habitable for hundreds of millions of years...And if humanity ultimately takes to the stars, the timescales become literally astronomical\".^: 14^\n\nThird, the future could be very good or very bad, and our actions may affect what it will be. It could be very good if technological and moral progress continue to improve the quality of life into the future, just as they have greatly improved our lives compared to our ancestors; yet, the future could also be very bad if technology were to allow [a totalitarian regime](https://forum.effectivealtruism.org/topics/totalitarianism) to control the world or a [world war](https://forum.effectivealtruism.org/topics/great-power-conflict) to completely destroy civilisation.^: 19–21^ MacAskill notes that our present time is highly unusual in that \"we live in an era that involves an extraordinary amount of change\"^: 26^—both relative to the past (where rates of economic and technological progress were very slow) and to the future (since current growth rates cannot continue for long before hitting physical limits).^\\[1\\]: 26–28^ From this he concludes that we live at a pivotal moment in human history, where \"the world’s long-run fate depends in part on the choices we make in our lifetimes\"^: 6^ since \"society has not yet settled down into a stable state, and we are able to influence which stable state we end up in\".^: 28 ^\n\nPart 1 ends with a chapter on how individuals can shape the course of history. MacAskill introduces a three-part framework for thinking about the future, which states that the long-term value of an outcome we may bring about depends on its *significance*, *persistence*, and *contingency*.^: 31–33^ He explains that significance \"is the average value added by bringing about a certain state of affairs\", persistence means \"how long that state of affairs lasts, once it has been brought about\", and contingency \"refers to the extent to which the state of affairs depends on an individual’s action\".^: 32 ^\n\nPart 2: Trajectory changes\n--------------------------\n\nPart 2 investigates how moral change and value lock-in may constitute *trajectory changes*, affecting the long-run value of future civilisation. MacAskill argues that \"we are living through a period of plasticity, that the moral views that shape society are like molten glass that can be blown into many different shapes. But the glass is cooling, and at some point, perhaps in the not-too-distant future, it might set\".^ :102 ^\n\nMacAskill suggests that moral and cultural values are malleable, contingent, and potentially long-lived—if history were to be rerun, the dominant global values may be very different from those in our world. For example, he argues that the abolition of slavery may not have been morally or economically inevitable.^\\[1\\]: 70 ^Abolition may thus have been a turning point in the entirety of human history, supporting the idea that improving society's values may positively influence the long-run future.\n\nMacAskill warns of a potential *value lock-in*, \"an event that causes a single value system, or set of value systems, to persist for an extremely long time\".^ 78 ^He notes that if \"value lock-in occurred globally, then how well or poorly the future goes would be determined in significant part by the nature of those locked-in values\".^78 ^Various past rulers sought to lock in their values—some with more success, like the Han dynasty in ancient China entrenching Confucianism for over a millenium,^: 78 ^and some with less success, like Hitler's proclaimed \"Thousand-Year Reich\"^: 92 ^. MacAskill states that the \"key issue is which values will guide the future. Those values could be narrow-minded, parochial, and unreflective. Or they could be open-minded, ecumenical, and morally exploratory\".^: 88 ^\n\nValue lock-in result from certain technological advances, according to MacAskill. In particular, he argues that the development of [artificial general intelligence](https://forum.effectivealtruism.org/topics/artificial-intelligence) (AGI)—an AI system \"capable of learning as wide an array of tasks as human beings can and performing them to at least the same level as human beings\"^: 80 ^—could result in the permanent lock-in of the values of those who control or have programmed the AGI.^: 80–86^ This may occur because AGI systems may be both enormously powerful and potentially immortal since they \"could replicate themselves as many times as they wanted, just as easily as we can replicate software today\".^: 86^ MacAskill concludes that \"if this happened, then the ruling ideology could in principle persist as long as civilisation does. And there would no longer be competing value systems that could dislodge the status quo\".^: 86 ^\n\nPart 3: Safeguarding civilisation\n---------------------------------\n\nPart 3 explores how to protect humanity from risks of [*extinction*](https://forum.effectivealtruism.org/topics/existential-risk), *unrecoverable* [*civilisational collapse*](https://forum.effectivealtruism.org/topics/civilizational-collapse), and long-run *technological stagnation*.\n\nMacAskill discusses several risks of human extinction, focusing on [engineered pathogens](https://forum.effectivealtruism.org/topics/biosecurity), [misaligned artificial general intelligence](https://forum.effectivealtruism.org/topics/ai-risk) (AGI), and great power war. He points to the rapid progress in biotechnology and states that \"engineered pathogens could be much more destructive than natural pathogens because they can be modified to have dangerous new properties\", such as a pathogen \"with the lethality of Ebola and the contagiousness of measles\".^: 108^ MacAskill points to other scholars who \"put the probability of an extinction-level engineered pandemic this century at around 1 percent\" and references his colleague [Toby Ord](https://en.wikipedia.org/wiki/Toby_Ord), who estimates the probability at 3 percent in his 2020 book [*The Precipice: Existential Risk and the Future of Humanity*](https://forum.effectivealtruism.org/topics/the-precipice).^: 113^ Ensuring humanity's survival by reducing extinction risks may significantly improve the long-term future by increasing the number of flourishing future lives.^: 35–36^\n\nThe next chapter discusses the risk of civilisational collapse, referring to events \"in which society loses the ability to create most industrial and postindustrial technology\".^\\[1\\]: 124 ^He discusses several potential causes of civilisational collapse—including extreme [climate change](https://forum.effectivealtruism.org/topics/climate-change), fossil fuel depletion, and [nuclear winter](https://forum.effectivealtruism.org/topics/nuclear-winter) caused by [nuclear war](https://forum.effectivealtruism.org/topics/nuclear-warfare-1)—concluding that civilisation appears very resilient, with recovery after a collapse being likely.^: 127–142^ Yet, he believes that the \"lingering uncertainty is more than enough to make the risk of unrecovered collapse a key longtermist priority\".^: 142^\n\nMacAskill next considers the risk of long-lasting technological and economic stagnation. While he considers indefinite stagnation unlikely, \"it seems entirely plausible that we could stagnate for hundreds or thousands of years\".^: 144^ This matters for longtermism for two reasons: first, \"if society stagnates technologically, it could remain stuck in a period of high catastrophic risk for such a long time that extinction or collapse would be all but inevitable\".^: 142^ Second, the society emerging after the period of stagnation may be guided by worse values than society today.^: 144^\n\nPart 4: Assessing the end of the world\n--------------------------------------\n\nPart 4 discusses how bad the end of humanity would be, which depends on whether it is morally good for happy people to be born and whether the future will be good or bad. The answers to these questions, according to MacAskill, \"determine whether we should focus on trajectory changes or on ensuring survival, or on both\".^: 163^\n\nWhether making happy people improves the world is a key question in [population ethics](https://forum.effectivealtruism.org/topics/population-ethics), which concerns \"the evaluation of actions that might change who is born, how many people are born, and what their quality of life will be\".^: 168^ Answering this question determines whether we should \"care about the loss of those future people who will never be born if humanity goes extinct in the next few centuries\".^: 188^ After discussing several population ethical theories—including the total view, the average view, critical-level theories, and person-affecting views—MacAskill concludes that \"it is a loss if future people are prevented from coming into existence—as long as their lives would be good enough. So the early extinction of the human race would be a truly enormous tragedy\".^: 189^\n\nOn whether the future will be good or bad, MacAskill notes that the \"more optimistic we are, the more important it is to avoid permanent collapse or extinction; the less optimistic we are, the stronger the case for focusing instead on improving values or other trajectory changes\".^: 192^ To answer the question, MacAskill compares how the quality of life of humans and nonhuman animals has changed over time and how both groups should be weighted numerically.^: 194–213^ While arguing that the billions of animals [suffering in factory farms](https://forum.effectivealtruism.org/topics/farmed-animal-welfare) likely have negative well-being—they would have been better off never having been born—MacAskill concludes optimistically that \"we should expect the future to be positive on balance\".^: 193^ He justifies this optimism in several ways, most crucially by pointing to \"an asymmetry in the motivation of future people—namely, people sometimes produce good things just because the things are good, but people rarely produce bad things just because they are bad\".^: 218 ^\n\nPart 5: Taking action\n---------------------\n\nPart 5 details what readers can do to take action based on the book's arguments.\n\nMacAskill emphasises the significance of professional work, writing that \"by far the most important decision you will make, in terms of your lifetime impact, is your choice of career\".^:234^ He points the reader to the nonprofit [80,000 Hours](https://forum.effectivealtruism.org/topics/80-000-hours) that he helped cofound, which conducts research and provides advice on which careers have the largest positive social impact, especially from a longtermist perspective. One career opportunity he highlights is movement-building work—to \"convince others to care about future generations...and to act to positively influence the long term\".^:243^\n\nHe makes a case that the common emphasis on personal behaviour and consumption, \"though understandable, is a major strategic blunder for those of us who want to make the world better\".^: 243^ Instead, he argues that donations to effective causes and organisations are much more impactful than changing our personal consumption.^: 232^ Beyond donations, he elaborates on three other impactful personal decisions, including political activism, spreading good ideas, and having children.^: 2 33 ^\n\nMacAskill acknowledges the pervasive uncertainty, both moral and empirical, that surrounds longtermism and offers four lessons to help guide attempts to improve the long-term future: taking robustly good actions, building up options, learning more, and avoiding causing harm.^226,240^\n\nFurther reading\n---------------\n\nMacAskill, William (2022) [*What We Owe the Future*](https://en.wikipedia.org/wiki/Special:BookSources/978-1-5416-1862-6), New York: Basic Books.\n\nExternal links\n--------------\n\n[What We Owe the Future](https://www.whatweowethefuture.com/). Official website.\n\nRelated entries\n---------------\n\n[longtermism](https://forum.effectivealtruism.org/tag/longtermism) | [*The Precipice*](https://forum.effectivealtruism.org/tag/the-precipice) | [William MacAskill](https://forum.effectivealtruism.org/tag/william-macaskill)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "KdxaNG2Fwkgh4ywFm",
    "name": "Nuclear security",
    "core": false,
    "slug": "nuclear-security",
    "oldSlugs": null,
    "postCount": 89,
    "description": {
      "markdown": "**Nuclear security** is the set of procedures, practices or other measures used to manage risks from nuclear [weapons](https://forum.effectivealtruism.org/tag/weapons-of-mass-destruction) and other nuclear materials.\n\nEvaluation\n----------\n\n[80,000 Hours](https://forum.effectivealtruism.org/tag/80-000-hours) rates nuclear security a \"second-highest priority area\": an unusually pressing global problem ranked slightly below their four highest priority areas.^[\\[1\\]](#fnqhws52z16v7)^\n\nRecommendations\n---------------\n\nIn [*The Precipice: Existential Risk and the Future of Humanity*](https://forum.effectivealtruism.org/tag/the-precipice), [Toby Ord](https://forum.effectivealtruism.org/tag/toby-ord) offers several [policy](https://forum.effectivealtruism.org/tag/policy) and [research](https://forum.effectivealtruism.org/tag/research) recommendations for handling risks from nuclear weapons:^[\\[2\\]](#fnu9nacjazuu)^\n\n*   Restart the Intermediate-Range Nuclear Forces Treaty (INF).\n*   Take US ICBMs off hair-trigger alert (officially called Launch on Warning).\n*   Increase the capacity of the International Atomic Energy Agency (IAEA) to verify nations are complying with safeguards agreements.\n*   Work on resolving the key uncertainties in nuclear winter modeling.\n*   Characterize the remaining uncertainties then use Monte Carlo techniques to show the distribution of outcome possibilities, with a special focus on the worst-case possibilities compatible with our current understanding.\n*   Investigate which parts of the world appear most robust to the effects of nuclear winter and how likely civilization is to continue there.\n\nFurther reading\n---------------\n\nMcIntyre, Peter (2016) [Nuclear security](https://80000hours.org/problem-profiles/nuclear-security/), *80,000 Hours*, April.\n\nOpen Philanthropy (2013) [Nuclear security](https://www.openphilanthropy.org/research/cause-reports/nuclear-security/july-2013), *Open Philanthropy*, July 18.\n\nOpen Philanthropy (2015) [Nuclear weapons policy](https://www.openphilanthropy.org/research/cause-reports/nuclear-weapons-policy), *Open Philanthropy*, September.\n\nRelated entries\n---------------\n\n[nuclear disarmament movement](https://forum.effectivealtruism.org/tag/nuclear-disarmament-movement) | [nuclear warfare](https://forum.effectivealtruism.org/tag/nuclear-warfare-1) | [nuclear winter](https://forum.effectivealtruism.org/tag/nuclear-winter) | [weapons of mass destruction](https://forum.effectivealtruism.org/topics/weapons-of-mass-destruction)\n\n1.  ^**[^](#fnrefqhws52z16v7)**^\n    \n    80,000 Hours (2021) [Our current list of the most important world problems](https://80000hours.org/problem-profiles/), *80,000 Hours*.\n    \n2.  ^**[^](#fnrefu9nacjazuu)**^\n    \n    Ord, Toby (2020) [*The Precipice: Existential Risk and the Future of Humanity*](https://en.wikipedia.org/wiki/Special:BookSources/1-5266-0021-8), London: Bloomsbury Publishing, p. 278"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "S8ALjn3WaQPh8EXqz",
    "name": "Emergent Ventures",
    "core": false,
    "slug": "emergent-ventures",
    "oldSlugs": null,
    "postCount": 1,
    "description": {
      "markdown": "**Emergent Ventures** is a fellowship and grant program that seeks to fund high-risk, high-reward ideas that advance prosperity, opportunity, and wellbeing.^[\\[1\\]](#fniian1g7zq1)^\n\nIn April 2022, [Tyler Cowen](https://forum.effectivealtruism.org/tag/tyler-cowen) announced a special new tranche of the Emergent Ventures fund focused on finding talented [artificial intelligence](https://forum.effectivealtruism.org/tag/artificial-intelligence) researchers in emerging economies.^[\\[2\\]](#fn69nxei05fv5)^\n\nFurther reading\n---------------\n\nCowen, Tyler (2018) [Emergent Ventures, a new project to help foment enlightenment](https://marginalrevolution.com/marginalrevolution/2018/09/emergent-ventures-new-project-help-foment-enlightenment.html), *Marginal Revolution*, September 13.\n\nExternal links\n--------------\n\n[Emergent Ventures](https://www.mercatus.org/emergent-ventures). Official website.\n\n[Apply for funding](https://mercatuscenter.formstack.com/forms/emergent_ventures).\n\nRelated entries\n---------------\n\n[funding opportunities](https://forum.effectivealtruism.org/tag/funding-opportunities)\n\n1.  ^**[^](#fnrefiian1g7zq1)**^\n    \n    Mercatus Center (2018) [Mercatus Center launches Emergent Ventures](https://www.mercatus.org/features/emergent-ventures), *Mercatus Center*, July 18.\n    \n2.  ^**[^](#fnref69nxei05fv5)**^\n    \n    Cowen, Tyler (2022) [AI for Emergent Ventures](https://marginalrevolution.com/marginalrevolution/2022/04/ai-for-emergent-ventures.html), *Marginal Revolution*, April 8."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "be4pBryMKxLhkmgvE",
    "name": "Funding opportunities",
    "core": false,
    "slug": "funding-opportunities",
    "oldSlugs": [
      "funding-opportunity"
    ],
    "postCount": 20,
    "description": {
      "markdown": "There are a number of ways to get funding for projects motivated by ideas and goals from effective altruism: \n\n*   [Open Philanthropy](https://forum.effectivealtruism.org/topics/open-philanthropy)\n    *   [Opportunities for funding](https://www.openphilanthropy.org/how-to-apply-for-funding/#section-2)\n        *   [The Century Fellowship](https://www.openphilanthropy.org/how-to-apply-for-funding/#section-2)\n        *   [The University Organizer Fellowship](https://www.openphilanthropy.org/how-to-apply-for-funding/#section-2)\n        *   [Early-career funding for individuals interested in improving the long-term future](https://www.openphilanthropy.org/how-to-apply-for-funding/#section-2)\n        *   [Request for proposals to grow the community of people motivated to improve the long-term future](https://www.openphilanthropy.org/how-to-apply-for-funding/#section-2)\n        *   [Course development grants](https://www.openphilanthropy.org/how-to-apply-for-funding/#section-2)\n        *   [Undergraduate scholarships](https://www.openphilanthropy.org/how-to-apply-for-funding/#section-2)\n        *   [Cause Exploration Prizes](https://www.openphilanthropy.org/how-to-apply-for-funding/#section-2)\n        *   [Technology Policy Fellowship](https://www.openphilanthropy.org/how-to-apply-for-funding/#section-2)\n*   [EA Funds](https://forum.effectivealtruism.org/tag/effective-altruism-funds)\n    *   [Global Health and Development Fund](https://forum.effectivealtruism.org/tag/global-health-and-development-fund)\n    *   [Long-Term Future Fund](https://forum.effectivealtruism.org/tag/long-term-future-fund)\n    *   [Effective Altruism Infrastructure Fund](https://forum.effectivealtruism.org/tag/effective-altruism-infrastructure-fund)\n    *   [Animal Welfare Fund](https://forum.effectivealtruism.org/tag/animal-welfare-fund)\n*   [Survival and Flourishing Projects](https://forum.effectivealtruism.org/tag/survival-and-flourishing) (SFP)\n    *   for small grants and service contracts for long-termist projects that don't yet have an institutional home\n*   [Survival and Flourishing Fund](https://forum.effectivealtruism.org/tag/survival-and-flourishing) (SFF)\n*   [GiveWell](https://www.givewell.org/apply-for-consideration)\n*   [Emergent Ventures](https://forum.effectivealtruism.org/tag/emergent-ventures)\n\nPlaces you could find funding:\n\n*   [Assorted open bounties and prizes](https://forum.effectivealtruism.org/tag/bounty-open)\n*   [Job listings (open)](https://forum.effectivealtruism.org/tag/job-listing-open)\n*   [Take action](https://forum.effectivealtruism.org/tag/take-action) page on the Forum\n\nExternal links\n--------------\n\n[Funding opportunities](https://airtable.com/shrAhiA5DZqkgpwh1/tblXkLQ2WqPI8au1M). A list of funding opportunities maintained by [Effective Thesis](https://forum.effectivealtruism.org/topics/effective-thesis).\n\nRelated entries\n---------------\n\n[collections and resources](https://forum.effectivealtruism.org/tag/collections-and-resources) | [effective altruism funding](https://forum.effectivealtruism.org/tag/effective-altruism-funding) | [job listings (open)](https://forum.effectivealtruism.org/tag/job-listing-open) | [take action](https://forum.effectivealtruism.org/tag/take-action)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "DM6dRqsYRpeiLEHou",
    "name": "Public goods",
    "core": false,
    "slug": "public-goods",
    "oldSlugs": null,
    "postCount": 6,
    "description": {
      "markdown": "**Public goods** are goods or services that are open to use by all members of society if they are provided at all. Public goods are both *non-excludable* and *non-rivalrous*, i.e. they are goods no society member can be excluded from consuming if they are at all supplied, and for which consumption by one member does not reduce the quantity available to others.^[\\[1\\]](#fn0gxn5ryg8qap)^ **Global public goods** are public goods for which the pool of beneficiaries spans the entire globe. And **Intergenerational global public goods**, in turn, are global public goods for which the pool of beneficiaries spans multiple generations. An example of an intergenerational global public good is protection from [existential risk](https://forum.effectivealtruism.org/tag/existential-risk).^[\\[2\\]](#fnmui382hq6k)^\n\nEvaluation\n----------\n\n[80,000 Hours](https://forum.effectivealtruism.org/tag/80-000-hours) rates global public goods a \"potential highest priority area\": an issue that, if more thoroughly examined, could rank as a top global challenge.^[\\[3\\]](#fnfitqcnzc6c8)^\n\nFurther reading\n---------------\n\nButerin, Vitalik, Zoë Hitzig & E. Glen Weyl (2019) [A flexible design for funding public goods](https://doi.org/10.1287/mnsc.2019.3337), *Management Science*, vol. 65, pp. 5171–5187.\n\nSandmo, Agnar (2018) [Public goods](http://doi.org/10.1057/978-1-349-95189-5_1696), in Garett Jones (ed.) *The New Palgrave Dictionary of Economics*, 3rd ed., London: Palgrave Macmillan, pp. 10973–10984.\n\nRelated entries\n---------------\n\n[economics](https://forum.effectivealtruism.org/tag/economics) | [quadratic payments](https://forum.effectivealtruism.org/tag/quadratic-payments)\n\n1.  ^**[^](#fnref0gxn5ryg8qap)**^\n    \n    Black, John (1997) [*A Dictionary of Economics*](https://en.wikipedia.org/wiki/Special:BookSources/0-19-280018-3), Oxford: Oxford University Press.\n    \n2.  ^**[^](#fnrefmui382hq6k)**^\n    \n    Ord, Toby (2020) [*The Precipice: Existential Risk and the Future of Humanity*](https://en.wikipedia.org/wiki/Special:BookSources/1-5266-0021-8), London: Bloomsbury Publishing, ch. 2\n    \n3.  ^**[^](#fnreffitqcnzc6c8)**^\n    \n    80,000 Hours (2022) [Our current list of pressing world problems](https://80000hours.org/problem-profiles/), *80,000 Hours*."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "EvJeqMg2ZEqJqjA9d",
    "name": "Vitalik Buterin",
    "core": false,
    "slug": "vitalik-buterin",
    "oldSlugs": null,
    "postCount": 7,
    "description": {
      "markdown": "**Vitaly Dmitriyevich \"Vitalik\" Buterin** (born 31 January 1994) is a [Russian](https://forum.effectivealtruism.org/tag/russia)-born Canadian writer and programmer, and the co-founder of the Ethereum [blockchain](https://forum.effectivealtruism.org/topics/blockchain-technology).\n\nFurther reading\n---------------\n\nBankman-Fried, Sam *et al.* (2020) [Effective altruism: crypto farmers give](https://www.youtube.com/watch?v=sBYOqwE70PE), *FTX Podcast*, December 22.\n\nWiblin, Robert & Keiran Harris (2019) [Vitalik Buterin on effective altruism, better ways to fund public goods, the blockchain’s problems so far, and how it could yet change the world](https://80000hours.org/podcast/episodes/vitalik-buterin-new-ways-to-fund-public-goods/), *80,000 Hours*, September 3.\n\nExternal links\n--------------\n\n[Vitalik Buterin](https://vitalik.ca/). Official website.\n\nRelated entries\n---------------\n\n[blockchain  technology](https://forum.effectivealtruism.org/topics/blockchain-technology) | [certificate of impact](https://forum.effectivealtruism.org/tag/certificate-of-impact) | [public goods](https://forum.effectivealtruism.org/tag/public-goods) | [quadratic voting](https://forum.effectivealtruism.org/tag/quadratic-voting)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "aEfhhqymeFsdhDQLk",
    "name": "Criticism and Red Teaming Contest",
    "core": false,
    "slug": "criticism-and-red-teaming-contest",
    "oldSlugs": [
      "critiques-and-red-teaming-contest",
      "critiques-and-red-teaming-contest"
    ],
    "postCount": 275,
    "description": {
      "markdown": "The **Criticism and Red Teaming Contest** is a contest on the [Effective Altruism Forum](https://forum.effectivealtruism.org/tag/effective-altruism-forum-1) for writing that critically engages with theory or work from the [effective altruism](https://forum.effectivealtruism.org/tag/effective-altruism) community.\n\nFurther reading\n---------------\n\nVaintrob, Lizka, Fin Moorhouse & Joshua Teperowski Monrad (2022) [Announcing a contest: EA Criticism and Red Teaming](https://forum.effectivealtruism.org/posts/8hvmvrgcxJJ2pYR4X/announcing-a-contest-ea-criticism-and-red-teaming), *Effective Altruism Forum*, June 1.\n\nRelated entries\n---------------\n\n[criticism of effective altruism](https://forum.effectivealtruism.org/tag/criticism-of-effective-altruism) | [criticism of effective altruist causes](https://forum.effectivealtruism.org/tag/criticism-of-effective-altruist-causes) | [criticism of effective altruist organizations](https://forum.effectivealtruism.org/tag/criticism-of-effective-altruist-organizations) | [criticism of the effective altruism community](https://forum.effectivealtruism.org/tag/criticism-of-the-effective-altruism-community) | [red teaming](https://forum.effectivealtruism.org/tag/red-teaming)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "cHbsJPEqzD65u7dMb",
    "name": "Future Matters",
    "core": false,
    "slug": "future-matters",
    "oldSlugs": null,
    "postCount": 5,
    "description": {
      "markdown": "**Future Matters** is a monthly [newsletter](https://forum.effectivealtruism.org/tag/newsletters) about [longtermism](https://forum.effectivealtruism.org/tag/longtermism).\n\nExternal links\n--------------\n\n[Future Matters](http://www.futurematters.news/). Official website.\n\nRelated entries\n---------------\n\n[Alignment Newsletter](https://forum.effectivealtruism.org/tag/alignment-newsletter) | [Effective Altruism Newsletter](https://forum.effectivealtruism.org/tag/effective-altruism-newsletter) | [Forecasting Newsletter](https://forum.effectivealtruism.org/tag/forecasting-newsletter) | [newsletters](https://forum.effectivealtruism.org/tag/newsletters)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "qrGWKaGq5oMJrDwf5",
    "name": "Vegan Outreach",
    "core": false,
    "slug": "vegan-outreach",
    "oldSlugs": null,
    "postCount": null,
    "description": {
      "markdown": "**Vegan Outreach** is a nonprofit organization that focuses on [farmed animal](https://forum.effectivealtruism.org/tag/farmed-animal-welfare) advocacy. It was founded by Matt Ball and Jack Norris in 1993.\n\nEvaluation\n----------\n\nVegan Outreach was rated a Standout Charity by [Animal Charity Evaluators](https://forum.effectivealtruism.org/tag/animal-charity-evaluators) between May 2014 to November 2018.^[\\[1\\]](#fnjbqu6nkxb4)^\n\nFurther reading\n---------------\n\nAnimal Charity Evaluators (2018) [Vegan Outreach review](https://animalcharityevaluators.org/charity-review/vegan-outreach/), *Animal Charity Evaluators*, November.\n\nBall, Matt (2014) [*The Accidental Activist: Stories, Speeches, Articles, and Interviews by Vegan Outreach's Co-Founder*](https://en.wikipedia.org/wiki/Special:BookSources/978-1-59056-454-7), New York: Lantern Books.\n\nTomasik, Brian (2006) [How much is a dollar worth? The case of Vegan Outreach](https://reducing-suffering.org/wp-content/uploads/2014/10/dollar-worth.pdf), *Essays on Reducing Suffering*, July 2.\n\nExternal links\n--------------\n\n[Vegan Outreach](https://veganoutreach.org/). Official website.\n\n[Apply for a job](https://veganoutreach.org/job-openings/).\n\nRelated entries\n---------------\n\n[dietary change](https://forum.effectivealtruism.org/tag/dietary-change) | [effective animal advocacy](https://forum.effectivealtruism.org/tag/effective-animal-advocacy) | [farmed animal welfare](https://forum.effectivealtruism.org/tag/farmed-animal-welfare)\n\n1.  ^**[^](#fnrefjbqu6nkxb4)**^\n    \n    Animal Charity Evaluators (2018) [Vegan Outreach review](https://animalcharityevaluators.org/charity-review/vegan-outreach/), *Animal Charity Evaluators*, November."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "id4pCbci2ANqWjfEp",
    "name": "Arb",
    "core": false,
    "slug": "arb",
    "oldSlugs": null,
    "postCount": 1,
    "description": {
      "markdown": "**Arb** is a research [consultancy](https://forum.effectivealtruism.org/topics/consultancy) that conducts original research, evidence reviews, and large-scale data pipelines primarily in [forecasting](https://forum.effectivealtruism.org/topics/forecasting), [artificial intelligence](https://forum.effectivealtruism.org/topics/artificial-intelligence), and [pandemic preparedness](https://forum.effectivealtruism.org/topics/pandemic-preparedness).\n\nHistory\n-------\n\nArb was founded in 2022 by Misha Yagudin and Gavin Leech.\n\nFurther reading\n---------------\n\nKarnofsky, Holden (2022) [The track record of futurists seems … fine](https://www.cold-takes.com/the-track-record-of-futurists-seems-fine/), *Cold Takes*, June 30.\n\nLeech, Gavin (2022) [Comparing top forecasters and domain experts](https://forum.effectivealtruism.org/posts/qZqvBLvR5hX9sEkjR/comparing-top-forecasters-and-domain-experts), *Effective Altruism Forum*, March 6.\n\nExternal links\n--------------\n\n[Arb](https://arbresearch.com/). Official website.\n\n[Arb](https://forum.effectivealtruism.org/users/arb). [Effective Altruism Forum](https://forum.effectivealtruism.org/topics/effective-altruism-forum-1) account.\n\n[Apply for a job](https://forum.effectivealtruism.org/posts/DXcg6N6CGvRA2vrCk/who-s-hiring-may-september-2022?commentId=aDWmbHCqAqSJbTkTK).\n\nRelated entries\n---------------\n\n[consultancy](https://forum.effectivealtruism.org/topics/consultancy) | [forecasting](https://forum.effectivealtruism.org/topics/forecasting)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "pKm8aYtnhvCEzBZBA",
    "name": "Quadratic voting",
    "core": false,
    "slug": "quadratic-voting",
    "oldSlugs": null,
    "postCount": 4,
    "description": {
      "markdown": "**Quadratic voting** is a collective decision-making procedure in which each participant is given an equal budget of voting credits, which they can use to vote for or against a proposal (such as a candidate or issue). Participants can cast as many votes for a given proposal as their budget allows, but the costs increase quadratically, i.e. it costs  \\\\(n^2\\\\) credits to cast \\\\(n\\\\) votes for the proposal.\n\nFurther reading\n---------------\n\nButerin, Vitalik (2019) [Quadratic payments: a primer](https://vitalik.ca/general/2019/12/07/quadratic.html), *Vitalik Buterin’s Website*, December 7.\n\nLalley, Steven P. & E. Glen Weyl (2018) [Quadratic voting: how mechanism design can radicalize democracy](https://doi.org/10.1257/pandp.20181002), *AEA Papers and Proceedings*, vol. 108, pp. 33–37.\n\nRelated entries\n---------------\n\n[electoral reform](https://forum.effectivealtruism.org/tag/electoral-reform) | [mechanism design](https://forum.effectivealtruism.org/tag/mechanism-design)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "rEcyevPTw3MJggGBt",
    "name": "Community Builder Writing Contest",
    "core": false,
    "slug": "community-builder-writing-contest",
    "oldSlugs": null,
    "postCount": 34,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "aoer2ckLPANNxamtG",
    "name": "Obituary",
    "core": false,
    "slug": "obituary",
    "oldSlugs": [
      "obituary"
    ],
    "postCount": 8,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "HwzJDPKPqJyHDRyJh",
    "name": "Future Fund",
    "core": false,
    "slug": "future-fund",
    "oldSlugs": null,
    "postCount": 15,
    "description": {
      "markdown": "The  **Future Fund** is a philanthropic fund by the [FTX Foundation](https://forum.effectivealtruism.org/tag/ftx-foundation) that makes grants and investments to [ambitious](https://forum.effectivealtruism.org/tag/ambition) projects focused on improving humanity's [long-term prospects](https://forum.effectivealtruism.org/tag/long-term-future).\n\nThe Fund was launched in February 2022. It plans to distribute at least $100 million, potentially up to $1 billion, by the end of 2022.^[\\[1\\]](#fnd40g5orka0t)^ The Fund's team consists of [Nick Beckstead](https://forum.effectivealtruism.org/tag/nick-beckstead) (CEO of the FTX Foundation), Leopold Aschenbrenner, Avital Balwit, [William MacAskill](https://forum.effectivealtruism.org/tag/william-macaskill) and Ketan Ramakrishnan.^[\\[2\\]](#fn0x4vjk1tdrkq)^\n\nFurther reading\n---------------\n\nBeckstead, Nick *et al.* (2022) [Announcing the Future Fund](https://ftxfuturefund.org/announcing-the-future-fund/), *Future Fund*.\n\nExternal links\n--------------\n\n[Future Fund](https://ftxfuturefund.org/). Official website.\n\n[Apply for funding](https://ftxfuturefund.org/apply/).\n\nRelated entries\n---------------\n\n[FTX Foundation](https://forum.effectivealtruism.org/tag/ftx-foundation) | [funding opportunities](https://forum.effectivealtruism.org/tag/funding-opportunity)\n\n1.  ^**[^](#fnrefd40g5orka0t)**^\n    \n    Beckstead, Nick *et al.* (2022) [Announcing the Future Fund](https://ftxfuturefund.org/announcing-the-future-fund/), *Future Fund*.\n    \n2.  ^**[^](#fnref0x4vjk1tdrkq)**^\n    \n    Future Fund (2022) [About](https://ftxfuturefund.org/about/), *Future Fund*."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "7GKgiZPxMq2Mwdhy9",
    "name": "Momentum",
    "core": false,
    "slug": "momentum",
    "oldSlugs": null,
    "postCount": 1,
    "description": {
      "markdown": "**Momentum** (formerly **Sparrow**) is a venture-backed startup that aims to increase [effective giving](https://forum.effectivealtruism.org/tag/effective-giving-1).\n\nExternal links\n--------------\n\n[Momentum](https://www.givemomentum.com/). Official website.\n\n[Apply for a job](https://givemomentum.com/careers/).\n\nRelated entries\n---------------\n\n[effective giving](https://forum.effectivealtruism.org/tag/effective-giving-1)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "fcX54pGGdqhGARfLb",
    "name": "Ukraine",
    "core": false,
    "slug": "ukraine",
    "oldSlugs": null,
    "postCount": 30,
    "description": {
      "markdown": "**Ukraine** is a country in Eastern Europe. It is, after [Russia](https://forum.effectivealtruism.org/tag/russia), the second-largest country by area in Europe.\n\nFurther reading\n---------------\n\nPinsker, Joe (2022) [How can individual people most help Ukraine?](https://www.theatlantic.com/family/archive/2022/04/help-ukraine-donations-effective-altruism/629453/), *The Atlantic*, April 1.\n\nPiper, Kelsey (2022) [How you can help Ukrainians](https://www.vox.com/future-perfect/22955885/donate-ukraine), *Vox*, March 1.\n\nWalsh, Bryan (2022) [The war in Ukraine could portend the end of the ‘long peace’](https://www.vox.com/2022/2/26/22951016/russia-ukraine-long-peace-nuclear-weapons-global-development), *Vox*, February 26.\n\nExternal links\n--------------\n\n[War in Ukraine](https://ourworldindata.org/ukraine-war). Collection of resources by [Our World in Data](https://forum.effectivealtruism.org/tag/our-world-in-data).\n\nRelated entries\n---------------\n\n[China](https://forum.effectivealtruism.org/tag/china) | [Russia](https://forum.effectivealtruism.org/tag/russia)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "CRgJaRs5PqzxCd2Pp",
    "name": "Aligned AI",
    "core": false,
    "slug": "aligned-ai",
    "oldSlugs": null,
    "postCount": 4,
    "description": {
      "markdown": "**Aligned AI** is a benefit corporation focused on reducing [existential risk](https://forum.effectivealtruism.org/tag/existential-risk) from [artificial intelligence](https://forum.effectivealtruism.org/tag/artificial-intelligence) via value extrapolation. It launched in February 2022.\n\nFurther reading\n---------------\n\nArmstrong, Stuart (2022) [Why I’m co-founding Aligned AI](https://www.alignmentforum.org/posts/vBoq5yd7qbYoGKCZK/why-i-m-co-founding-aligned-ai), *AI Alignment Forum*, February 17.\n\nExternal links\n--------------\n\n[Aligned](https://www.aligned-ai.com/) [AI](https://buildaligned.ai/). Official website.\n\n[Apply for a job](https://airtable.com/shrqEPuijRUJ94tGS).\n\nRelated entries\n---------------\n\n[AI alignment](https://forum.effectivealtruism.org/tag/ai-alignment) | [artificial intelligence](https://forum.effectivealtruism.org/tag/artificial-intelligence) | [existential risk](https://forum.effectivealtruism.org/tag/existential-risk)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "HWm5Xo4SxucjdyPru",
    "name": "Alvea",
    "core": false,
    "slug": "alvea",
    "oldSlugs": null,
    "postCount": 1,
    "description": {
      "markdown": "**Alvea** is a startup biotechnology company focused on building a streamlined platform for developing and deploying DNA vaccines against [COVID-19](https://forum.effectivealtruism.org/tag/covid-19-pandemic) variants and other pathogens. It launched in February 2022.^[\\[1\\]](#fndqqj8ja1lw8)^\n\nFurther reading\n---------------\n\nFish, Kyle (2022) [Announcing Alvea—An EA COVID vaccine project](https://forum.effectivealtruism.org/posts/67awq5ozeYSjsYchk/announcing-alvea-an-ea-covid-vaccine-project), *Effective Altruism Forum*, February 16.\n\nExternal links\n--------------\n\n[Alvea](https://www.alvea.bio/). Official website.\n\n[Apply for a job](https://jobs.lever.co/telis-bio).\n\nRelated entries\n---------------\n\n[biosecurity](https://forum.effectivealtruism.org/tag/biosecurity) | [COVID-19 pandemic](https://forum.effectivealtruism.org/tag/covid-19-pandemic) | [pandemic preparedness](https://forum.effectivealtruism.org/tag/pandemic-preparedness) | [vaccines](https://forum.effectivealtruism.org/tag/vaccines)\n\n1.  ^**[^](#fnrefdqqj8ja1lw8)**^\n    \n    PR Newswire (2022) [Alvea launches scalable, shelf- stable DNA vaccine development against new SARS-CoV-2 variants](https://www.prnewswire.com/news-releases/alvea-launches-scalable-shelf--stable-dna-vaccine-development-against-new-sars-cov-2-variants-301483557.html), *PR Newswire*, February 16."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "GamgBCvDSWhCQJgC8",
    "name": "Hear This Idea",
    "core": false,
    "slug": "hear-this-idea",
    "oldSlugs": null,
    "postCount": 19,
    "description": {
      "markdown": "**Hear This Idea** is a podcast about [effective altruism](https://forum.effectivealtruism.org/tag/effective-altruism), [philosophy](https://forum.effectivealtruism.org/topics/philosophy), and the [social sciences](https://forum.effectivealtruism.org/topics/social-science-1), co-hosted by Luca Righetti and Fin Moorhouse.\n\nThe podcast was launched in December 2019. Past guests of the show include [Bryan Caplan](https://forum.effectivealtruism.org/tag/bryan-caplan),^[\\[1\\]](#fnrmq6e4t1f4k)^ Bruce Friedrich,^[\\[2\\]](#fn8te0kcys15u)^ [Anders Sandberg](https://forum.effectivealtruism.org/tag/anders-sandberg),^[\\[3\\]](#fnmwvqnq3jwz)^ [Peter Singer](https://forum.effectivealtruism.org/tag/peter-singer),^[\\[4\\]](#fncfwxujtv5l)^ Phil Trammell,^[\\[5\\]](#fn8epng1wh1d8)^ Eva Vivalt,^[\\[6\\]](#fn3ashps0vi7l)^ and others.\n\nFurther reading\n---------------\n\nMoorhouse, Fin (2021) [Advice for starting a podcast](https://www.finmoorhouse.com/writing/podcast-advice), *Fin Moorhouse’s Website*, September 15.\n\nRighetti, Luca & Fin Moorhouse (2019) [What is Hear This Idea?](https://hearthisidea.com/episodes/introduction), *Hear This Idea*, December 2.\n\nExternal links\n--------------\n\n[Hear This Idea](https://hearthisidea.com/). Official website.\n\nRelated entries\n---------------\n\n[80,000 Hours Podcast](https://forum.effectivealtruism.org/tag/80-000-hours-podcast) | [Making Sense](https://forum.effectivealtruism.org/tag/sam-harris#Making_Sense_podcast) | [podcasts](https://forum.effectivealtruism.org/tag/podcasts) | [Rationally Speaking](https://forum.effectivealtruism.org/tag/julia-galef#Rationally_Speaking)\n\n1.  ^**[^](#fnrefrmq6e4t1f4k)**^\n    \n    Righetti, Luca & Fin Moorhouse (2021) [Bryan Caplan on causes of poverty and the case for open borders](https://hearthisidea.com/episodes/bryan), *Hear This Idea*, August 23.\n    \n2.  ^**[^](#fnref8te0kcys15u)**^\n    \n    Righetti, Luca & Fin Moorhouse (2021) [Bruce Friedrich on protein alternatives and the Good Food Institute](https://hearthisidea.com/episodes/bruce), *Hear This Idea*, January 4.\n    \n3.  ^**[^](#fnrefmwvqnq3jwz)**^\n    \n    Righetti, Luca & Fin Moorhouse (2021) [Anders Sandberg on the Fermi Paradox, transhumanism, and so much more](https://hearthisidea.com/episodes/anders), *Hear This Idea*, August 2.\n    \n4.  ^**[^](#fnrefcfwxujtv5l)**^\n    \n    Righetti, Luca & Fin Moorhouse (2020) [Peter Singer on speciesism, lockdown ethics, and controversial ideas](https://hearthisidea.com/episodes/peter), *Hear This Idea*, December 6.\n    \n5.  ^**[^](#fnref8epng1wh1d8)**^\n    \n    Righetti, Luca & Fin Moorhouse (2021) [Phil Trammell on economic growth under transformative AI](https://hearthisidea.com/episodes/phil), *Hear This Idea*, May 12.\n    \n6.  ^**[^](#fnref3ashps0vi7l)**^\n    \n    Righetti, Luca & Fin Moorhouse (2021) [Eva Vivalt on Evidence-Based Policy and Forecasting Social Science](https://hearthisidea.com/episodes/eva), *Hear This Idea*, April 12."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "FkNX3LcJey8cNFXn2",
    "name": "Wave",
    "core": false,
    "slug": "wave",
    "oldSlugs": null,
    "postCount": 5,
    "description": {
      "markdown": "**Wave** is a mobile service provider that allows unbanked people in Africa to access financial services such as saving and transferring money. Although Wave is a for-profit company, its founders—long-time members of the [effective altruism](https://forum.effectivealtruism.org/tag/effective-altruism) community—launched it primarily with the goal of reducing [global poverty](https://forum.effectivealtruism.org/tag/global-poverty).\n\nWave was founded in 2018 and quickly became the largest mobile money service provider in Senegal.^[\\[1\\]](#fnryktgyied4)^  Ben Kuhn, Wave's CTO, estimates that the company saves people in Senegal over $200 million every year—or around 1% of that country's GDP.^[\\[2\\]](#fn68fpq3yuhl5)^\n\nFurther reading\n---------------\n\nKosloff, Marshall (2022) [Mobile money with Wave’s CTO Ben Kuhn](https://ideas.beondeck.com/mobile-money-with-waves-cto-ben-kuhn/), *The Deep End*, June 9.\n\nKuhn, Ben (2019) [Why and how to start a startup serving emerging markets](https://www.benkuhn.net/emco/), *Ben Kuhn’s Blog*, November 5.\n\nQuirk, Lincoln & Ben Kuhn (2020) [We’re Lincoln Quirk & Ben Kuhn from Wave, AMA!](https://forum.effectivealtruism.org/posts/kBSgtcrbBXEwLyYRD/we-re-lincoln-quirk-and-ben-kuhn-from-wave-ama), *Effective Altruism Forum*, October 27.\n\nExternal links\n--------------\n\n[Wave](https://www.wave.com/en/). Official website.\n\n[Apply for a job](https://www.wave.com/en/careers/).\n\nRelated entries\n---------------\n\n[entrepreneurship](https://forum.effectivealtruism.org/tag/entrepreneurship)\n\n1.  ^**[^](#fnrefryktgyied4)**^\n    \n    Finextra (2021) [Wave closes largest Series A round for an African fintech with $200 million](https://www.finextra.com/newsarticle/38780/wave-closes-largest-series-a-round-for-an-african-fintech-with-200-million), *Finextra*, September 7.\n    \n2.  ^**[^](#fnref68fpq3yuhl5)**^\n    \n    Kuhn, Ben (2021) [Working at Wave is an extremely effective way to improve the world](https://www.wave.com/en/blog/world/), *Wave’s Blog*, July 8."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "Jkqzfvm4RDN3Fd4zb",
    "name": "Great Filter",
    "core": false,
    "slug": "great-filter",
    "oldSlugs": null,
    "postCount": 3,
    "description": {
      "markdown": "The **Great Filter** is the mechanism that explains why insentient matter in the universe does not frequently evolve into technologically advanced intelligence.\n\nThe hypothetical process from primitive matter to technological maturity may be regarded as a series of critical transitions. Our failure to observe [extraterrestrial intelligence](https://forum.effectivealtruism.org/tag/extraterrestrial-intelligence)—sometimes called the \"Great Silence\"—implies the existence of a Great Filter somewhere along the series. Great Filter theorists, such as [Robin Hanson](https://forum.effectivealtruism.org/tag/robin-hanson), emphasize that the location of the Filter along that series has implications for human prospects of long-term survival. In particular, if the Great Filter is still in the future, our species will almost certainly fail to [colonize](https://forum.effectivealtruism.org/tag/space-colonization) much of the universe. In Hanson's words, \"The easier it was for life to evolve to our stage, the bleaker our future chances probably are.\"^[\\[1\\]](#fnr2y8m480ses)^\n\nCandidate Filters\n-----------------\n\nSuggested candidate Great Filters include:^[\\[1\\]](#fnr2y8m480ses)^\n\n*   The right star system (including organics)\n*   Reproductive something (e.g. RNA)\n*   Simple (prokaryotic) single-cell life\n*   Complex (archaeatic & eukaryotic) single-cell life\n*   Sexual reproduction\n*   Multi-cell life\n*   Tool-using animals with big brains\n*   Where we are now\n*   Colonization explosion\n\nNote that the filters in the list may not be jointly exhaustive. Nor are they mutually exclusive—there may be multiple Great Filters.\n\nImplications for cause prioritization\n-------------------------------------\n\nDiscoveries bearing on the location of the Great Filter have several distinct implications for [cause prioritization](https://forum.effectivealtruism.org/tag/cause-prioritization), especially in connection with [existential risk](https://forum.effectivealtruism.org/tag/existential-risk) reduction.\n\n### Prioritization of general efforts to reduce existential risk\n\nConcluding that the Great Filter is in the future may suggest that efforts to reduce existential risk should be increased. As Hanson writes, \"The larger the remaining ﬁlter we face, the more carefully humanity should try to avoid negative scenarios.\"^[\\[1\\]](#fnr2y8m480ses)^ Alternatively, reducing existential risks posed by the Great Filter may be regarded as intractable, given that by assumption no civilizations so far managed to avoid succumbing to it. In [Nick Bostrom](https://forum.effectivealtruism.org/topics/nick-bostrom)'s words, \"If the Great Filter is ahead of us, we must relinquish all hope of ever colonizing the galaxy; and we must fear that our adventure will end soon, or at any rate that it will end prematurely.\"^[\\[2\\]](#fnwhee5rkhtt)^ What would be surprising is an update towards a future Great Filter—and a commensurate increase in the probability assigned to an existential catastrophe—that warranted neither prioritizing nor deprioritizing marginal investments in securing our species' long-term potential.\n\n### Prioritization of reduction of some types of existential risk over others\n\nSuppose we assign some credence to the hypothesis that the Great Filter is ahead of us. This assignment should make us prioritize not only existential risk reduction, but also the reduction of some existential risks over others. This is because not all potential causes of an existential catastrophe are equally good Great Filter candidates.\n\nFirst, *the posited future Great Filter must be powerful enough to kill not only actual humans, but also all other civilizations reaching the relevant stage of development.* As Bostrom writes, \"random natural disasters such as asteroid hits and supervolcanic eruptions are poor Great Filter candidates, because even if they destroyed a significant number of civilizations, we would expect some civilizations to get lucky; and some of these civilizations could then go on to colonize the universe. Perhaps the existential risks that are most likely to constitute a Great Filter are those that arise from technological discovery.\"^[\\[2\\]](#fnwhee5rkhtt)^^[\\[3\\]](#fn6pd18gduy2v)^\n\nSecond, *the posited future Great Filter must be compatible with observational evidence.* A catastrophic event expected to leave traces of itself visible from space is unlikely to be a future Great Filter, since we do not appear to be observing any such traces. Thus, it seems that an [intelligence explosion](https://forum.effectivealtruism.org/topics/intelligence-explosion) is probably not the Great Filter, since the universe (excluding Earth) appears to be devoid not only of intelligent life, but also of the signs of a prior explosion that could have killed such life.^[\\[4\\]](#fnrk073iobx8c)^\n\nThird, *the posited future Great Filter has to be compatible with the existence of human observers*. The Great Filter has to be powerful enough to prevent any intelligent species like us from turning into a spacefaring civilization. But it cannot be so powerful as to prevent our own existence, since we are here. As Katja Grace notes, \"any disaster that would destroy everyone in the observable universe at once, or destroy space itself, is out.\"^[\\[5\\]](#fnzavzlcfi60d)^\n\n### Prioritization of some risk-reducing strategies over others\n\nThere is a second respect in which the Great Filter should cause us to be especially concerned with certain existential catastrophes. Hanson writes that \"without such findings we must consider the possibility that we have yet to pass through a substantial part of the Great Filter. If so, then our prospects are bleak, but *knowing this fact may at least help us improve our chances*.\"^[\\[6\\]](#fnyj5fwdjttmm)^ Mere awareness of the fact that the Great Filter is ahead, however, cannot substantially increase our chances of surviving it, since a sizeable fraction of civilizations like us likely also developed this awareness yet failed to survive. In general, the more common one expects a risk-reducing strategy to be across civilizations, the more pessimistic one should be about its chances of succeeding. So priority should be given to strategies expected to be very uncommon. As [Carl Shulman](https://forum.effectivealtruism.org/tag/carl-shulman) writes, \"the mere fact that we adopt any purported Filter-avoiding strategy S is strong evidence that S won’t work... To expect S to work we would have to be very confident that we were highly unusual in adopting S (or any strategy as good as S), in addition to thinking S very good on the merits. This burden might be met if it was only through some bizarre fluke that S became possible, and a strategy might improve our chances even though we would remain almost certain to fail, but common features, such as awareness of the Great Filter, would not suffice to avoid future filters.\"^[\\[7\\]](#fndpae7x6d9pl)^\n\n### Prioritization of research that could help us better locate the Great Filter\n\nA final implication of the Great Filter for cause prioritization relates to prioritization of different types of research. In particular, it seems that this consideration favors prioritizing research with the potential to better locate the Great Filter, given the high [information value](https://forum.effectivealtruism.org/topics/value-of-information) of this type of research. Two obvious types of research are (1) research that helps us define the space of candidate Great Filters and (2) research that helps us estimate the probability that any of those candidates is in fact the Great Filter. More broadly, this consideration seems to raise the value of disciplines like astrobiology and projects like SETI.^[\\[1\\]](#fnr2y8m480ses)^\n\nFurther reading\n---------------\n\nBostrom, Nick (2008) [Where are they?](https://www.technologyreview.com/2008/04/22/220999/where-are-they/), *MIT Technology Review*, April 22, pp. 72–77.\n\nHanson, Robin (1998) [The Great Filter—are we almost past it?](http://mason.gmu.edu/~rhanson/greatfilter.html), *Robin Hanson’s Website*, September 15.\n\nRelated entries\n---------------\n\n[existential risk](https://forum.effectivealtruism.org/tag/existential-risk) | [extraterrestrial intelligence](https://forum.effectivealtruism.org/tag/extraterrestrial-intelligence) | [Fermi paradox](https://forum.effectivealtruism.org/tag/fermi-paradox) | [grabby aliens](https://forum.effectivealtruism.org/topics/grabby-aliens) | [human extinction](https://forum.effectivealtruism.org/tag/human-extinction) | [space colonization](https://forum.effectivealtruism.org/tag/space-colonization)\n\n1.  ^**[^](#fnrefr2y8m480ses)**^\n    \n    Hanson, Robin (1998) [The Great Filter—are we almost past it?](http://mason.gmu.edu/~rhanson/greatfilter.html), *Robin Hanson’s Website*, September 15.\n    \n2.  ^**[^](#fnrefwhee5rkhtt)**^\n    \n    Bostrom, Nick (2008) [Where are they?](https://www.technologyreview.com/2008/04/22/220999/where-are-they/), *MIT Technology Review*, April 22, pp. 72–77.\n    \n3.  ^**[^](#fnref6pd18gduy2v)**^\n    \n    See also Scott Alexander (2014) [Don’t fear the filter](https://slatestarcodex.com/2014/05/28/dont-fear-the-filter/), *Slate Star Codex*, May 28: \"In other words, there’s no way global warming kills 999,999,999 in every billion civilizations. Maybe it kills 100,000,000. Maybe it kills 900,000,000. But occasionally one manages to make it to space before frying their home planet. That means it can’t be the Great Filter, or else we would have run into the aliens who passed their Kyoto Protocols.\"\n    \n4.  ^**[^](#fnrefrk073iobx8c)**^\n    \n    This point is often overlooked; even Bostrom includes AGI in his list of candidate Great Filters.^[\\[2\\]](#fnwhee5rkhtt)^ It appears that this was first noted by Katja Grace in 2010;^[\\[5\\]](#fnzavzlcfi60d)^ see also Robn Hanson (2010) [Beware future filters](http://www.overcomingbias.com/2010/11/beware-future-filters.html), *Overcoming bias*, November 4 and Gregory Lewis (2015) [Unfriendly AI cannot be the great filter](https://medium.com/@gjlewis/unfriendly-ai-cannot-be-the-great-filter-8d609af39b6c), *Gregory Lewis's website*, January 16.\n    \n5.  ^**[^](#fnrefzavzlcfi60d)**^\n    \n     Grace, Katja (2010) [Light cone eating AI explosions are not filters](https://meteuphoric.com/2010/11/05/light-cone-eating-ai-explosions-are-not-filters/), *Meteuphoric*, November 5.\n    \n6.  ^**[^](#fnrefyj5fwdjttmm)**^\n    \n    Hanson,  [The Great Filter](http://mason.gmu.edu/~rhanson/greatfilter.html) (emphasis added).\n    \n7.  ^**[^](#fnrefdpae7x6d9pl)**^\n    \n    Shulman, Carl (2012) [Future filter fatalism](http://www.overcomingbias.com/2012/12/future-filter-fatalism.html), *Overcoming bias*, December 22."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "tXxetAAaYGXM8KoyP",
    "name": "Distribution of cost-effectiveness",
    "core": false,
    "slug": "distribution-of-cost-effectiveness",
    "oldSlugs": null,
    "postCount": 14,
    "description": {
      "markdown": "Not all causes are equally [cost-effective](https://forum.effectivealtruism.org/tag/cost-effectiveness). In fact, the **distribution of cost-effectiveness** is heavy-tailed: some causes are thousands of times more cost-effective than others.\n\nSuch variation in cost-effectiveness in part follows from the existence of immense [global inequality](https://forum.effectivealtruism.org/tag/global-poverty), which implies that many people in the world suffer from problems that could be solved with only small amounts of money. Consider, for instance, a developing world charity that spends $20 per person on a surgery to prevent blindness, compared to a charity that spends $40,000 per person to provide guide dogs to blind people in the United States. But even charities that [think globally](https://forum.effectivealtruism.org/tag/cosmopolitanism) are far from equal in their abilities to turn the donations they receive into real improvements in people’s lives.\n\nWithin a given focus area, we can understand variation in cost-effectiveness as arising from both underlying variation in the impact of the interventions that charities carry out and variation in how much charities spend to carry them out. For instance, the cost-effectiveness of a charity that combats malaria will depend both on whether it distributes medication or bednets and on how much wasteful spending it engages in. People in the effective altruism community tend to believe that the former source of variation is [more significant than the latter](https://forum.effectivealtruism.org/tag/relationship-between-overheads-and-effectiveness).\n\nOne general finding is that, in the long run, the cost-effectiveness of additional donations to many charities will [diminish as the amount they have already received grows](https://forum.effectivealtruism.org/tag/diminishing-returns). Because of this phenomenon, we would expect the cost-effectiveness of charities to become more and more equal in [a world where donations were based on cost-effectiveness](https://forum.effectivealtruism.org/tag/market-efficiency-of-philanthropy).\n\nFurther reading\n---------------\n\nCaviola, Lucius *et al.* (2020) [Donors vastly underestimate differences in charities' effectiveness](http://journal.sjdm.org/20/200504/jdm200504.pdf), *Judgment and Decision Making* , vol. 15, pp. 509–516.  \n*Shows that lay people estimate that the variation in charity cost-effectiveness is muted, whereas experts estimate them to be much larger.*\n\nOrd, Toby (2019) [The moral imperative toward cost-effectiveness in global health](http://doi.org/10.1093/oso/9780198841364.003.0002), in Hilary Greaves & Theron Pummer (eds.) *Effective Altruism: Philosophical Issues*, Oxford: Oxford University Press, pp. 29–36.  \n*Discusses the moral relevance of variations in cost-effectiveness.*\n\nTomasik, Brian (2014) [Why charities usually don't differ astronomically in expected cost-effectiveness](https://reducing-suffering.org/why-charities-dont-differ-astronomically-in-cost-effectiveness/), *Essays on Reducing Suffering*, January 5 (updated 16 September 2017).  \n*Argues that the variation in cost-effectiveness of the charity sector isn't as significant as some people believe.*\n\nRelated entries\n---------------\n\n[cause prioritization](https://forum.effectivealtruism.org/tag/cause-prioritization) | [cost-effectiveness](https://forum.effectivealtruism.org/tag/cost-effectiveness) | [impact assessment](https://forum.effectivealtruism.org/tag/impact-assessment) | [intervention evaluation](https://forum.effectivealtruism.org/tag/intervention-evaluation) | [ITN framework](https://forum.effectivealtruism.org/tag/itn-framework-1)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "94XkYtZy9pvt63fCu",
    "name": "Elon Musk",
    "core": false,
    "slug": "elon-musk",
    "oldSlugs": null,
    "postCount": 12,
    "description": {
      "markdown": "**Elon Reeve Musk** (born 28 June 1971) is an entrepreneur and business magnate. He is the founder and CEO of SpaceX, the co-founder and CEO of Tesla, and the co-founder of [OpenAI](https://forum.effectivealtruism.org/tag/openai) and Neuralink.\n\nPhilanthropic activity\n----------------------\n\nIn 2015, Musk donated $10 million to the [Future of Life Institute](https://forum.effectivealtruism.org/tag/future-of-life-institute) to fund an [AI safety](https://forum.effectivealtruism.org/tag/ai-safety) research program.^[\\[1\\]](#fn1jfedqn3myq)^\n\nIn 2021, he committed $100 million to fund a carbon removal [prize](https://forum.effectivealtruism.org/tag/prize) to reduce [climate change](https://forum.effectivealtruism.org/tag/climate-change).^[\\[2\\]](#fn39aiqr5s7ib)^\n\nMusk intends to donate about half his money to [settle Mars](https://forum.effectivealtruism.org/tag/space-colonization) and reduce [existential risk](https://forum.effectivealtruism.org/tag/existential-risk):^[\\[3\\]](#fn3wryedes4ij)^\n\n> About half my money is intended to help problems on Earth & half to help establish a self-sustaining city on Mars to ensure continuation of life (of all species) in case Earth gets hit by a meteor like the dinosaurs or WW3 happens & we destroy ourselves\".\n\nFurther reading\n---------------\n\nPiper, Kelsey (2018) [Why Elon Musk fears artificial intelligence](https://www.vox.com/future-perfect/2018/11/2/18053418/elon-musk-artificial-intelligence-google-deepmind-openai), *Vox*, November 2.\n\nExternal links\n--------------\n\n[Elon Musk](https://twitter.com/elonmusk). Twitter account.\n\n[Musk Foundation](http://www.muskfoundation.org/). Official website.\n\n1.  ^**[^](#fnref1jfedqn3myq)**^\n    \n    Future of Life Institute (2015) [Elon Musk donates $10M to our research program](https://futureoflife.org/2015/01/22/elon-musk-donates-10m-to-our-research-program/), *Future of Life Institute*, October 12.\n    \n2.  ^**[^](#fnref39aiqr5s7ib)**^\n    \n    XPRIZE (2021) [$100M XPRIZE for carbon removal funded by Elon Musk to fight climate change](https://www.xprize.org/prizes/elonmusk/articles/100m-xprize-for-carbon-removal-funded-by-elon-musk-to-fight-climate-change), *XPRIZE Foundation*, February 8.\n    \n3.  ^**[^](#fnref3wryedes4ij)**^\n    \n    Musk, Elon (2018) [About half my money is intended to help problems on Earth & half to help establish a self-sustaining city on Mars](https://twitter.com/elonmusk/status/1050812486226599936), *Twitter*, October 13."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "ZMdMmDKd4Qnsr7GG7",
    "name": "History of existential risk",
    "core": false,
    "slug": "history-of-existential-risk",
    "oldSlugs": null,
    "postCount": 7,
    "description": {
      "markdown": "Further reading\n---------------\n\nMoynihan, Thomas (2020) [*X-Risk: How Humanity Discovered Its Own Extinction*](https://en.wikipedia.org/wiki/Special:BookSources/978-1-913029-84-5), Falmouth: Urbanomic.\n\nRighetti, Luca & Fin Moorhouse (2021) [Thomas Moynihan on the history of existential risk](https://hearthisidea.com/episodes/thomas), *Hear This Idea*, March 22.\n\nSánchez, Sebastián (2022) [Timeline of existential risk](https://timelines.issarice.com/wiki/Timeline_of_existential_risk), *Timelines Wiki*, August 5.\n\nRelated entries\n---------------\n\n[history](https://forum.effectivealtruism.org/tag/history) | [history of effective altruism](https://forum.effectivealtruism.org/tag/history-of-effective-altruism)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "eyEoe2jXua6Wq4cXd",
    "name": "LessWrong for EA",
    "core": false,
    "slug": "lesswrong-for-ea",
    "oldSlugs": [
      "lcllbc",
      "lclwbc",
      "low-commitment-less-wrong-book-club",
      "less-wrong-for-ea"
    ],
    "postCount": 25,
    "description": {
      "markdown": "Use the **LessWrong for EA** (or **LW4EA**) tag for any EA-relevant [LessWrong](https://forum.effectivealtruism.org/tag/lesswrong) post, including posts from the weekly LessWrong repost & low-commitment discussion group."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "iovGG2wTaAQZD8sDj",
    "name": "Dual-use",
    "core": false,
    "slug": "dual-use",
    "oldSlugs": null,
    "postCount": 9,
    "description": {
      "markdown": "Technology, research or artifacts are **dual-use** if they have both upside and downside potential. Sometimes the term is used more narrowly to describe technology, research or artifacts that can be used for both civilian and military purposes.^[\\[1\\]](#fnta73fx6zobk)^\n\nExamples\n--------\n\nDual-use research is particularly relevant in the field of [biosecurity](https://forum.effectivealtruism.org/tag/biosecurity). A salient example is so-called \"gain-of-function\" research, which \"might help identify the most dangerous strains of flu in nature, create targets for vaccine development, and alert the world to the possibility that H5N1 could become airborne\"^[\\[2\\]](#fnzpbww67jlsn)^ but could also cause the accidental deployment of a biological agent and provide malicious actors with information that enables them to create particularly dangerous pathogens.\n\nConcerns about dual-use come up in other contexts, too, including other [existential risks](https://forum.effectivealtruism.org/topics/existential-risk) besides [global catastrophic biological risks](https://forum.effectivealtruism.org/tag/global-catastrophic-biological-risk). One example relates to risks from [asteroids](https://forum.effectivealtruism.org/tag/asteroids): technology that helps move asteroids away from Earth can be used to move asteroids *towards* Earth, either accidentally or intentionally.^[\\[3\\]](#fnn382f5mu8j)^\n\nFurther reading\n---------------\n\nForge, John (2010) [A note on the definition of “dual use”](https://doi.org/10.1007/s11948-009-9159-9), *Science and Engineering Ethics*, vol. 16, pp. 111–118.\n\nMellon, William S. (2016) [Dual use research](https://doi.org/10.1016/B978-0-12-801885-9.00006-8), in Carole R. Baskin & Alan P. Zelicoff (eds.) *Ensuring National Biosecurity: Institutional Biosafety Committees*, Amsterdam: Elsevier, pp. 93–115.\n\nReading, Further & Peter Dale (2006) [Dual-use technology](https://en.wikipedia.org/wiki/Special:BookSources/978-0-7619-2927-7), in Richard J. Samuels (ed.) *Encyclopedia of United States National Security*, Thousand Oaks, California: Sage Publications, pp. 220–222.\n\n1.  ^**[^](#fnrefta73fx6zobk)**^\n    \n    See e.g. Jürgen Altmann (2010) [Dual use](https://doi.org/10.4135/9781412972093.n97), in David Guston (ed.) *Encyclopedia of Nanoscience and Society*, Thousand Oaks, California: Sage Publications, pp. 171–173. See also p. 402, footnote † in Sebastian Farquhar, Owen Cotton-Barratt & Andrew Snyder-Beattie (2017) [Pricing externalities to balance public risks and benefits of research](https://doi.org/10.1089/hs.2016.0118), *Health Security*, vol. 15, pp. 401–408.\n    \n2.  ^**[^](#fnrefzpbww67jlsn)**^\n    \n    Garrett, Laurie (2013) [Biology’s brave new world](https://www.foreignaffairs.com/articles/2013-11-01/biologys-brave-new-world), *Foreign Affairs*, vol. 92, pp. 28–46.\n    \n3.  ^**[^](#fnrefn382f5mu8j)**^\n    \n    Ord, Toby (2020) [*The Precipice: Existential Risk and the Future of Humanity*](https://en.wikipedia.org/wiki/Special:BookSources/1-5266-0021-8), London: Bloomsbury Publishing, p. 73."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "tRqH8tbgwDNpcMyGP",
    "name": "Red teaming",
    "core": false,
    "slug": "red-teaming",
    "oldSlugs": null,
    "postCount": 44,
    "description": {
      "markdown": "A **red team** is an independent group that challenges an organization or movement in order to improve it. **Red teaming** is the practice of using red teams.\n\nHistory of the term\n-------------------\n\nThe term \"red teaming\" appears to originate in the United States military. A common exercise was to pitch an offensive \"red team\", representing the enemy, against a defensive \"blue team\", representing the U.S. The purpose of the exercise was to identify vulnerabilities and develop effective countermeasures.^[\\[1\\]](#fn1bt59b23sc2)^ The term was later extended to cover related practices in other fields, including [information security](https://forum.effectivealtruism.org/tag/information-security) and intelligence analysis.\n\nRed teaming in effective altruism\n---------------------------------\n\nWithin [effective altruism](https://forum.effectivealtruism.org/tag/effective-altruism), \"red teaming\" refers to attempts to identify problems or errors in popular or prestigious views held by members of this community, such as views about the value of different causes or organizations.^[\\[2\\]](#fn6q3xj5z40b7)^\n\nRelated concepts include *minimal-trust investigations*,^[\\[3\\]](#fnv6hony7jd28)^ *epistemic spot-checks*,^[\\[4\\]](#fnx9lppxl4apk)^ and *hypothetical apostasy*.^[\\[5\\]](#fn1odprbpu5jp)^\n\nFurther reading\n---------------\n\nRäuker, Max *et al.* (2022) [Idea: Red-teaming fellowships](https://forum.effectivealtruism.org/posts/obHA95otPtDNSD6MD/idea-red-teaming-fellowships), *Effective Altruism Forum*, February 2.\n\nVaintrob, Lizka & Fin Moorhouse (2022) [Resource for criticisms and red teaming](https://forum.effectivealtruism.org/posts/uuQDgiJJaswEyyzan/resource-for-criticisms-and-red-teaming), *Effective Altruism Forum*, June 1.\n\nZhang, Linchuan (2021) [Red teaming papers as an EA training exercise?](https://forum.effectivealtruism.org/posts/myp9Y9qJnpEEWhJF9/linch-s-shortform), *Effective Altruism Forum*, June 22.\n\nRelated entries\n---------------\n\n[criticism of effective altruism](https://forum.effectivealtruism.org/tag/criticism-of-effective-altruism) | [epistemology](https://forum.effectivealtruism.org/tag/epistemology) | [epistemic deference](https://forum.effectivealtruism.org/topics/epistemic-deference) | [tabletop exercises](https://forum.effectivealtruism.org/topics/tabletop-exercises)\n\n1.  ^**[^](#fnref1bt59b23sc2)**^\n    \n    Johnson, Rowland (2015) [How your red team penetration testers can help improve your blue team](https://web.archive.org/web/20160530230034/http://www.scmagazineuk.com/how-your-red-team-penetration-testers-can-help-improve-your-blue-team/article/431023/), *SC Magazine*, August 18.\n    \n2.  ^**[^](#fnref6q3xj5z40b7)**^\n    \n    Räuker, Max *et al.* (2022) [Idea: Red-teaming fellowships](https://forum.effectivealtruism.org/posts/obHA95otPtDNSD6MD/idea-red-teaming-fellowships), *Effective Altruism Forum*, February 2.\n    \n3.  ^**[^](#fnrefv6hony7jd28)**^\n    \n    Karnofsky, Holden (2021) [Minimal-trust investigations](https://forum.effectivealtruism.org/posts/8RcFQPiza2rvicNqw/minimal-trust-investigations), *Effective Altruism Forum*, November 23.\n    \n4.  ^**[^](#fnrefx9lppxl4apk)**^\n    \n    Ravid, Yoav (2020) [Epistemic spot check](https://www.lesswrong.com/tag/epistemic-spot-check), *LessWrong Wiki*, August 7.\n    \n5.  ^**[^](#fnref1odprbpu5jp)**^\n    \n    Bostrom, Nick (2009) [Write your hypothetical apostasy](https://www.overcomingbias.com/2009/02/write-your-hypothetical-apostasy.html), *Overcoming Bias*, February 21."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "7i3ufdLEYhCSp7qxX",
    "name": "PlayPump",
    "core": false,
    "slug": "playpump",
    "oldSlugs": null,
    "postCount": null,
    "description": {
      "markdown": "A **PlayPump™** is a water pump powered by a playground roundabout or merry-go-round. It was conceived as a means of supplying water for domestic use in developing-world villages by taking advantage of the energy produced by children playing at it. The general failure of the idea has provided what is perhaps the best-known example of ineffective altruism.\n\nMechanism and basic idea\n------------------------\n\nThe rotational movement of the roundabout is converted into the reciprocal vertical motion of a pump,^[\\[1\\]](#fnscclfpkh1sq)^ by the work of which water is pumped up into an elevated tank, typically bearing advertising billboards, from where it is carried by gravity to a tap stand. System maintenance was supposed to be covered by the revenue generated from advertisers.^[\\[2\\]](#fnvg5qwcd3lak)^\n\nHistory\n-------\n\nThe roundabout pump was invented by Ronnie Stuiver, a South African engineer. In 1989, Trevor Field, a British advertising professional, noticed a model of it in an agriculture fair in Johannesburg and bought the patent from Stuiver. Field decided to focus full-time on the project and founded Roundabout Outdoor, which won the World Bank's Development Marketplace competition in 2000,^[\\[3\\]](#fnct3nf4ei93)^^[\\[4\\]](#fnv9v1rshb69j)^ and subsequently received international attention and increasing funding. In 2006, with backing from the Case Foundation, PlayPumps International, based in the US, was set up.^[\\[5\\]](#fn5gdfaqkvrr4)^ By 2009, Field and his charity had installed eighteen hundred PlayPumps across South Africa.^[\\[6\\]](#fn2xc0ggms1gd)^\n\nAfter a series of criticism, the Case Foundation acknowledged the failure of the program, and PlayPumps International was shut down. Despite this major setback, Roundabout Outdoor kept on functioning and receiving funding from other sources.^[\\[7\\]](#fnue0ekt3spn)^ As of February 2022, Field continues to install the same model of PlayPump in African communities.\n\nCriticism\n---------\n\nNo negative review was written before 2007, when UNICEF issued a report.^[\\[8\\]](#fnbqyzn10bifn)^ It was the first of a series of critical evaluations that went on to reveal multiple problems arising from the PlayPumps.^[\\[9\\]](#fnkzh714wci9n)^ Among the most serious of these are the following:\n\n*   The extra force needed to pump water made the merry-go-round less attractive to children, who quickly got tired. As a result, the pumps were operated primarily by women, who found the task exhausting and undignified.^[\\[10\\]](#fnjxs39ezb0k)^\n*   The communities do not appear to have been taken into account. In Mozambique, for example, there were \"no signs that communities had been consulted prior to installation or had a say in choosing the pump type of their choice\",^[\\[11\\]](#fnxxvmnp8nedm)^ and the same situation was also found in many other places.^[\\[12\\]](#fni1ye1pilh0k)^\n*   The cost of the PlayPump is $14.000, four times the cost of a standard handpump,^[\\[13\\]](#fneoofawmu8l8)^ but handpumps are significantly more effective than PlayPumps, \"pumping 3 to 5 times as fast\".^[\\[14\\]](#fnlktt85rvtge)^\n*   The tank is elevated 7 meters above the ground, with the only purpose of making advertising billboards visible from afar. That being so, unnecessary extra work is needed to raise the water to the tank, from where it goes down again until it reaches the tap.^[\\[15\\]](#fnbzyihawwmk7)^ Furthermore, billboards turned out to be an unviable source of revenue, because companies were not interested in paying for advertising in remote rural areas.^[\\[10\\]](#fnjxs39ezb0k)^^[\\[16\\]](#fnnbhu2rjrntq)^\n\nFurther reading\n---------------\n\nMacAskill, William (2013) [An example of do-gooding done wrong](https://forum.effectivealtruism.org/posts/YdLaiL7ao6zwtDhzu/an-example-of-do-gooding-done-wrong), *Effective Altruism Forum*, May 15.\n\nPremiere Speakers Bureau (2022) [Trevor Field bio](https://premierespeakers.com/trevor-field/bio), *Premiere Speakers Bureau*.\n\nRelated entries\n---------------\n\n[Scared Straight](https://forum.effectivealtruism.org/tag/scared-straight)\n\n1.  ^**[^](#fnrefscclfpkh1sq)**^\n    \n    Stellman, Andrew (2009) 'Saving lives: An interview with Trevor Field', in Andrew Stellman & Jennifer Greene (eds.) (2009) [*Beautiful Teams: Inspiring and Cautionary Tales from Veteran Team Leaders*](https://en.wikipedia.org/wiki/Special:BookSources/978-0-596-51802-8), Sebastopol, CA: O’Reilly, p. 171.\n    \n2.  ^**[^](#fnrefvg5qwcd3lak)**^\n    \n    MacAskill, William (2015) [*Doing Good Better: How Effective Altruism Can Help You Make a Difference*](https://en.wikipedia.org/wiki/Special:BookSources/978-1-59240-966-2), New York: Random House, p. 2.\n    \n3.  ^**[^](#fnrefct3nf4ei93)**^\n    \n    Stellman, Andrew & Jennifer Greene (eds.) (2009) [*Beautiful Teams: Inspiring and Cautionary Tales from Veteran Team Leaders*](https://en.wikipedia.org/wiki/Special:BookSources/978-0-596-51802-8), Sebastopol, CA: O’Reilly, p. 467.\n    \n4.  ^**[^](#fnrefv9v1rshb69j)**^\n    \n    World Bank (2002) [South Africa: the Roundabout Outdoor Playpump](https://openknowledge.worldbank.org/bitstream/handle/10986/9749/multi0page.pdf?sequence=1&isAllowed=y), Findings report no. 218, World Bank.\n    \n5.  ^**[^](#fnref5gdfaqkvrr4)**^\n    \n    Borland, Ralph (2011) [*Radical Plumbers and PlayPumps*](http://hdl.handle.net/2262/89797), PhD thesis, Trinity College, p. 26.\n    \n6.  ^**[^](#fnref2xc0ggms1gd)**^\n    \n    MacAskill, [*Doing Good Better*](https://en.wikipedia.org/wiki/Special:BookSources/978-1-59240-966-2), p. 3.\n    \n7.  ^**[^](#fnrefue0ekt3spn)**^\n    \n    Borland, [*Radical Plumbers and PlayPumps*](http://hdl.handle.net/2262/89797), p. 196.\n    \n8.  ^**[^](#fnrefbqyzn10bifn)**^\n    \n    Brocklehurst, Clarissa & Peter Harvey (2007) 'An evaluation of the PlayPump® water system as an appropriate technology for water, sanitation and hygiene programmes', UNICEF. Seemingly this report was issued *erroneously* (see MacAskill, William (2017) [Errata](https://www.williammacaskill.com/errata/), *William MacAskill's Website*).\n    \n9.  ^**[^](#fnrefkzh714wci9n)**^\n    \n    For a detailed list of these early works see Borland, [*Radical Plumbers and PlayPumps*](http://hdl.handle.net/2262/89797), pp. 160-2.\n    \n10.  ^**[^](#fnrefjxs39ezb0k)**^\n    \n    MacAskill, [*Doing Good Better*](https://en.wikipedia.org/wiki/Special:BookSources/978-1-59240-966-2), p. 4.\n    \n11.  ^**[^](#fnrefxxvmnp8nedm)**^\n    \n    Obiols, Ana Lucia & Karl Erpf (2008) [Mission report on the evaluation of the PlayPumps installed in Mozambique](https://www-tc.pbs.org/frontlineworld/stories/southernafrica904/flash/pdf/mozambique_report.pdf), The Swiss Resource Centre and Consultancies for Development, p. 31.\n    \n12.  ^**[^](#fnrefi1ye1pilh0k)**^\n    \n    Borland, [*Radical Plumbers and PlayPumps*](http://hdl.handle.net/2262/89797), pp. 179.\n    \n13.  ^**[^](#fnrefeoofawmu8l8)**^\n    \n    MacAskill, [*Doing Good Better*](https://en.wikipedia.org/wiki/Special:BookSources/978-1-59240-966-2), p. 5.\n    \n14.  ^**[^](#fnreflktt85rvtge)**^\n    \n    Borland, [*Radical Plumbers and PlayPumps*](http://hdl.handle.net/2262/89797), p.  171.\n    \n15.  ^**[^](#fnrefbzyihawwmk7)**^\n    \n    Borland, [*Radical Plumbers and PlayPumps*](http://hdl.handle.net/2262/89797), pp. 174-5.\n    \n16.  ^**[^](#fnrefnbhu2rjrntq)**^\n    \n    Borland, [*Radical Plumbers and PlayPumps*](http://hdl.handle.net/2262/89797), pp. 176-7."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "zKmrf2yxuH3wsmDWQ",
    "name": "Bounty (closed)",
    "core": false,
    "slug": "bounty-closed",
    "oldSlugs": null,
    "postCount": 15,
    "description": {
      "markdown": "Use the bounty (closed) tag for posts that offered a prize, but no longer do (because the prize was claimed, expired, etc.)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "vgT4Fiybt4qjHLoBv",
    "name": "Bounty (open)",
    "core": false,
    "slug": "bounty-open",
    "oldSlugs": [
      "bounty"
    ],
    "postCount": 24,
    "description": {
      "markdown": "Use the **bounty (open)** tag for posts that offer a prize (monetary or otherwise).\n\nAfter the bounty is claimed or closed, remove this tag and add the [bounty (closed)](https://forum.effectivealtruism.org/tag/bounty-closed) tag.  \n  \n[job listing (open)](https://forum.effectivealtruism.org/tag/job-listing-open) | [megaprojects](https://forum.effectivealtruism.org/topics/megaprojects) | [requests (open)](https://forum.effectivealtruism.org/tag/requests-open) | [take action](https://forum.effectivealtruism.org/tag/take-action)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "byzc268Sr8K9wjh8z",
    "name": "Institute for Progress",
    "core": false,
    "slug": "institute-for-progress",
    "oldSlugs": null,
    "postCount": 1,
    "description": {
      "markdown": "The **Institute for Progress** (**IFP**) is a [think tank](https://forum.effectivealtruism.org/tag/think-tanks) dedicated to accelerating scientific, technological, and industrial progress while [safeguarding humanity's future](https://forum.effectivealtruism.org/tag/longtermism).\n\nFunding\n-------\n\nAs of July 2022, IFP has received a $1.6 million grant from [Open Philanthropy](https://forum.effectivealtruism.org/tag/open-philanthropy),^[\\[1\\]](#fn3dux994ex9a)^ and a $480,000 grant from the [Future Fund](https://forum.effectivealtruism.org/topics/future-fund).^[\\[2\\]](#fnsa0ff4uhg9)^ It has also received additional funding from [Emergent Ventures](https://forum.effectivealtruism.org/tag/emergent-ventures), [Sam Bankman-Fried](https://forum.effectivealtruism.org/tag/sam-bankman-fried), and Patrick and John Collison.^[\\[3\\]](#fnsxil5aa0qx9)^\n\nFurther reading\n---------------\n\nStapp, Alec & Caleb Watney (2022) [Progress is a policy choice](https://progress.institute/progress-is-a-policy-choice/), *Institute for Progress*, January 20.\n\nExternal links\n--------------\n\n[Institute for Progress](https://progress.institute/). Official website.\n\n1.  ^**[^](#fnref3dux994ex9a)**^\n    \n    Open Philanthropy (2022) [Grants database: Institute for Progress](https://www.openphilanthropy.org/grants/?q=&organization-name=institute-for-progress), *Open Philanthropy.*\n    \n2.  ^**[^](#fnrefsa0ff4uhg9)**^\n    \n    Future Fund (2022) [Our grants and investments: Institute for Progress](https://ftxfuturefund.org/all-grants/?_organization_name=institute-for-progress), *Future Fund*.\n    \n3.  ^**[^](#fnrefsxil5aa0qx9)**^\n    \n    Institute for Progress (2022) [About](https://progress.institute/about/), *Institute for Progress*."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "hADucjAHM5FWx8hgA",
    "name": "Ambition",
    "core": false,
    "slug": "ambition",
    "oldSlugs": null,
    "postCount": 19,
    "description": {
      "markdown": "Further reading\n---------------\n\nTodd, Benjamin (2021) [The growth of effective altruism: what does it mean for our priorities and level of ambition?](https://80000hours.org/2021/11/growth-of-effective-altruism/), *80,000 Hours*, November 11.\n\nTodd, Benjamin (2021) [Be more ambitious: a rational case for dreaming big (if you want to do good)](https://80000hours.org/articles/be-more-ambitious/), *80,000 Hours*, November 12.\n\nRelated entries\n---------------\n\n[megaprojects](https://forum.effectivealtruism.org/topics/megaprojects)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "3oqEfwcBThAHajeQn",
    "name": "Animal Empathy Philippines",
    "core": false,
    "slug": "animal-empathy-philippines",
    "oldSlugs": [
      "animalempathyphilippines"
    ],
    "postCount": 1,
    "description": {
      "markdown": "**Animal Empathy Philippines** is an organization dedicated to community building work on [effective animal advocacy](https://forum.effectivealtruism.org/tag/effective-animal-advocacy) in the [Philippines](https://forum.effectivealtruism.org/tag/philippines).\n\nFunding\n-------\n\nAs of June 2022, Animal Empathy Philippines has received over $60,000 in funding from [Effective Altruism Funds](https://forum.effectivealtruism.org/topics/effective-altruism-funds).^[\\[1\\]](#fnrz06v61prec)^\n\nFurther reading\n---------------\n\nGeronimo, Ging (2022) [Introducing Animal Empathy Philippines](https://forum.effectivealtruism.org/posts/gqAzR4BjbrNjzgcDf/introducing-animal-empathy-philippines), *Effective Altruism Forum*, January 25.\n\nExternal links\n--------------\n\n[Animal Empathy Philippines](https://www.facebook.com/AnimalEmpathyPH). Official website.\n\nRelated entries\n---------------\n\n[effective animal advocacy](https://forum.effectivealtruism.org/tag/effective-animal-advocacy) | [Philippines](https://forum.effectivealtruism.org/tag/philippines)\n\n1.  ^**[^](#fnrefrz06v61prec)**^\n    \n    Animal Welfare Fund (2021) [November 2021: Animal Welfare Fund grants](https://funds.effectivealtruism.org/funds/payouts/november-2021-animal-welfare-fund-grants), *Effective Altruism Funds*, November."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "Zg2WKoocjbHsnKiFK",
    "name": "Training for Good",
    "core": false,
    "slug": "training-for-good",
    "oldSlugs": null,
    "postCount": 10,
    "description": {
      "markdown": ""
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "qwmzx7NwCKH5MGuLo",
    "name": "EA Librarian (project inactive)",
    "core": false,
    "slug": "ea-librarian-project-inactive",
    "oldSlugs": [
      "ea-librarian"
    ],
    "postCount": 13,
    "description": {
      "markdown": "(This project is no longer running, but feel free to submit questions using the regular forum question feature.)  \n  \n**EA Librarian** was a project of the [Centre for Effective Altruism](https://forum.effectivealtruism.org/tag/centre-for-effective-altruism-1) that provided answers to user-submitted questions related to [effective altruism](https://forum.effectivealtruism.org/tag/effective-altruism). Questions were submitted by filling out a form or by posting them directly to the [Effective Altruism Forum](https://forum.effectivealtruism.org/tag/effective-altruism-forum-1) with the [EA Librarian](https://forum.effectivealtruism.org/tag/ea-librarian) tag.\n\nFurther reading\n---------------\n\nParikh, Caleb (2022) [EA Librarian: CEA wants to help answer your EA questions!](https://forum.effectivealtruism.org/posts/Y5pja6CrRmPPFkMKj/ea-librarian-cea-wants-to-help-answer-your-ea-questions), *Effective Altruism Forum*, January 17.\n\nRelated entries\n---------------\n\n[building effective altruism](https://forum.effectivealtruism.org/tag/building-effective-altruism-1) | [Centre for Effective Altruism](https://forum.effectivealtruism.org/tag/centre-for-effective-altruism-1) | [community infrastructure](https://forum.effectivealtruism.org/tag/community-infrastructure)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "8mdFr2SzXFe4HThuS",
    "name": "Automation",
    "core": false,
    "slug": "automation",
    "oldSlugs": null,
    "postCount": 3,
    "description": {
      "markdown": "**Automation** is the application of machines to tasks once performed by humans.\n\nFurther reading\n---------------\n\nAcemoglu, Daron & Pascual Restrepo (2019) [Automation and new tasks: how technology displaces and reinstates labor](https://doi.org/10.1257/jep.33.2.3), *Journal of Economic Perspectives*, vol. 33, pp. 3–30.\n\nOrr, Peter (2015) [Which careers are most likely to be automated?](https://80000hours.org/2015/02/which-careers-will-be-automated/), *80,000 Hours*, February 4.\n\nRelated entries\n---------------\n\n[economic growth](https://forum.effectivealtruism.org/tag/economic-growth) | [economics of artificial intelligence](https://forum.effectivealtruism.org/tag/economics-of-artificial-intelligence) | [universal basic income](https://forum.effectivealtruism.org/tag/universal-basic-income)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "aFJtBqxDkRX4Dj3m4",
    "name": "Community epistemic health",
    "core": false,
    "slug": "community-epistemic-health",
    "oldSlugs": [
      "community-epistemic-health"
    ],
    "postCount": 27,
    "description": {
      "markdown": "The **community epistemic health** tag applies to posts addressing the current state of epistemic health in the effective altruism community, as well as posts discussing how we might improve the epistemic health of the community.\n\nEffective altruism is about working out how to do as much good as we can using evidence and reason and then taking action on that basis.  Posts here are particularly concerned with how well the movement is able to use evidence and reason, norms that affect discourse between individuals and how individuals form beliefs.  \n  \nIf you have ideas for improving or retaining epistemic norms within the EA community please email Nicole Ross (community health team manager at CEA) at nicole\\[dot\\]ross\\[at\\]centreforeffectivealtruism.org ."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "h4jS7ZpjjKtEpKNQA",
    "name": "Air pollution",
    "core": false,
    "slug": "air-pollution",
    "oldSlugs": [
      "air-quality",
      "air-quality"
    ],
    "postCount": 10,
    "description": {
      "markdown": "**Air pollution** is the contamination of air due to the presence of substances in the atmosphere that are harmful to the health of humans and other living beings. I**ndoor** (or **household**) **air pollution** is caused by burning solid fuel sources such as firewood, crop waste, and dung for cooking and heating, while **outdoor air pollution** is primarily caused by combustion processes from motor vehicles, solid fuel burning, and industry.\n\nFurther reading\n---------------\n\nBennitt, F. B. *et al.* (2021) [Estimating disease burden attributable to household air pollution: new methods within the Global Burden of Disease Study](https://doi.org/10.1016/S2214-109X(21)00126-1), *The Lancet Global Health*, vol. 9, p. S18.\n\nMatthews, Dylan (2021) [How humans could live two years longer](https://www.vox.com/future-perfect/22691558/air-pollution-deaths-mortality-pm-25-soot-particulate), *Vox*, December 27.\n\nRitchie, Hannah & Max Roser (2017) [Air pollution](https://ourworldindata.org/air-pollution), *Our World in Data*.\n\nRelated entries\n---------------\n\n[burden of disease](https://forum.effectivealtruism.org/tag/burden-of-disease) | [climate change](https://forum.effectivealtruism.org/tag/climate-change) |  [global health and development](https://forum.effectivealtruism.org/tag/global-health-and-development)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "weTXETx5WLsezBvKG",
    "name": "Sam Harris",
    "core": false,
    "slug": "sam-harris",
    "oldSlugs": null,
    "postCount": null,
    "description": {
      "markdown": "**Samuel Benjamin Harris** (9 April 1967) is an American philosopher, neuroscientist, and author.\n\n*Making Sense*\n--------------\n\nHarris hosts *Making Sense*, a popular podcast about \"the human mind, society, and current events.\" Past guests of the podcast include [Sam Bankman-Fried](https://forum.effectivealtruism.org/tag/sam-bankman-fried),^[\\[1\\]](#fnm1g1tb1rgii)^[ Nick Bostrom](https://forum.effectivealtruism.org/tag/nick-bostrom),^[\\[2\\]](#fnaa6qsfca9om)^[ David Chalmers](https://forum.effectivealtruism.org/tag/david-chalmers),^[\\[3\\]](#fnuy7mb9c29da)^[ Robin Hanson](https://forum.effectivealtruism.org/tag/robin-hanson),^[\\[4\\]](#fnsxtdwgj0oy)^[ William MacAskill](https://forum.effectivealtruism.org/tag/william-macaskill),^[\\[5\\]](#fn2zklb4srpyr)^^[\\[6\\]](#fn8f4gqbpuip)^[ Jeff McMahan](https://forum.effectivealtruism.org/tag/jeff-mcmahan),^[\\[7\\]](#fn6c230h5ytk)^ [Toby Ord](https://forum.effectivealtruism.org/tag/toby-ord),^[\\[8\\]](#fnpgnd7arwz0g)^[ Peter Singer](https://forum.effectivealtruism.org/tag/peter-singer)^[\\[7\\]](#fn6c230h5ytk)^^[\\[9\\]](#fnbtbnyteb7r6)^ and [Eliezer Yudkowsky](https://forum.effectivealtruism.org/tag/eliezer-yudkowsky).^[\\[10\\]](#fnt3wjf6x5og)^ The conversations with MacAskill prompted so many people—including Harris himself—to take the [Giving What We Can](https://forum.effectivealtruism.org/tag/giving-what-we-can) pledge that they have been described as the most successful effective altruism podcast episodes of all time.^[\\[11\\]](#fnsyt4jyhrs3)^\n\nExternal links\n--------------\n\n[Sam Harris](https://www.samharris.org/). Official website.\n\n1.  ^**[^](#fnrefm1g1tb1rgii)**^\n    \n    Harris, Sam (2021) [Earning to give: A conversation with Sam Bankman-Fried](https://www.samharris.org/podcasts/making-sense-episodes/271-earning-to-give), *Making Sense*, December 24.\n    \n2.  ^**[^](#fnrefaa6qsfca9om)**^\n    \n    Harris, Sam (2019) [Will we destroy the future? A conversation with Nick Bostrom](https://www.samharris.org/podcasts/making-sense-episodes/151-will-destroy-future), *Making Sense*, March 18.\n    \n3.  ^**[^](#fnrefuy7mb9c29da)**^\n    \n    Harris, Sam (2016) [The light of the mind: A conversation with David Chalmers](https://www.samharris.org/podcasts/making-sense-episodes/the-light-of-the-mind), *Making Sense*, April 18.\n    \n4.  ^**[^](#fnrefsxtdwgj0oy)**^\n    \n    Harris, Sam (2018) [Hidden motives: A conversation with Robin Hanson](https://www.samharris.org/podcasts/making-sense-episodes/119-hidden-motives), *Making Sense*, March 12.\n    \n5.  ^**[^](#fnref2zklb4srpyr)**^\n    \n    Harris, Sam (2016) [Being good and doing good: A conversation with William MacAskill](https://www.samharris.org/podcasts/making-sense-episodes/being-good-and-doing-good), *Making Sense*, August 29.\n    \n6.  ^**[^](#fnref8f4gqbpuip)**^\n    \n    Harris, Sam (2021) [Doing good: A conversation with Will MacAskill](https://www.samharris.org/podcasts/making-sense-episodes/228-doing-good), *Making Sense*, December 14.\n    \n7.  ^**[^](#fnref6c230h5ytk)**^\n    \n    Harris, Sam (2021) [Can we talk about scary ideas? A conversation with Peter Singer, Francesca Minerva, Jeff McMahan](https://www.samharris.org/podcasts/making-sense-episodes/245-can-talk-scary-ideas), *Making Sense*, April 12.\n    \n8.  ^**[^](#fnrefpgnd7arwz0g)**^\n    \n    Harris, Sam (2020) [Existential risk: A conversation with Toby Ord](https://www.samharris.org/podcasts/making-sense-episodes/208-existential-risk), *Making Sense*, June 23.\n    \n9.  ^**[^](#fnrefbtbnyteb7r6)**^\n    \n    Harris, Sam (2016) [What is moral progress? A conversation with Peter Singer](https://www.samharris.org/podcasts/making-sense-episodes/what-is-moral-progress), *Making Sense*, October 21.\n    \n10.  ^**[^](#fnreft3wjf6x5og)**^\n    \n    Harris, Sam (2018) [AI: racing toward the brink: A conversation with Eliezer Yudkowsky](https://www.samharris.org/podcasts/making-sense-episodes/116-ai-racing-toward-brink), *Making Sense*, February 6.\n    \n11.  ^**[^](#fnrefsyt4jyhrs3)**^\n    \n    Gertler, Aaron (2021) [The most successful EA podcast of all time: Sam Harris and Will MacAskill (2020)](https://forum.effectivealtruism.org/posts/hd2LeisjAmzdJdGb3/the-most-successful-ea-podcast-of-all-time-sam-harris-and), *Effective Altruism Forum*, July 3."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "TdEje2LRrCFFvAk94",
    "name": "California YIMBY",
    "core": false,
    "slug": "california-yimby",
    "oldSlugs": null,
    "postCount": 1,
    "description": {
      "markdown": "**California YIMBY** is an organization that advocates for [land use reform](https://forum.effectivealtruism.org/tag/land-use-reform) in California.\n\nHistory\n-------\n\nCalifornia YIMBY was founded in 2017. As of August 2022, the organization has over 80,000 members and 20 local teams across California.^[\\[1\\]](#fnz6uebfacyxr)^\n\nEvaluation\n----------\n\n[Open Philanthropy](https://forum.effectivealtruism.org/tag/open-philanthropy) considers California a promising jurisdiction for land use reform advocacy because the state accounts for 12% of the United States population and around half of the most expensive areas in the country; because California has very high rates of rent-inclusive poverty and high rates of out-migration by low-income residents; because state-level advocacy appears to be more tractable than city-level advocacy; and because of other reasons.^[\\[2\\]](#fn2imin3pbjz6)^\n\nOpen Philanthropy estimates the social value of a new housing unit in coastal California to be in the low hundreds of thousands of dollars.^[\\[3\\]](#fntlipn7eljic)^ Combining this estimate with the consensus view that California should build millions of new houses suggests that efforts that increase the probability of more housing being built have very high expected value.^[\\[2\\]](#fn2imin3pbjz6)^\n\nAs of August 2022, California YIMBY has received over $5.4 million in funding from Open Philanthropy,^[\\[4\\]](#fnes3ub6r0gss)^ as well as $1 million from Stripe co-founders John and Patrick Collison.^[\\[5\\]](#fnor1586c1xmp)^ [Alexander Berger](https://forum.effectivealtruism.org/tag/alexander-berger) has recommended California YIMBY as an attractive funding opportunity for individual donors.^[\\[6\\]](#fni362a23wdqh)^^[\\[7\\]](#fnti65j3jl8x)^\n\nFurther reading\n---------------\n\nBerger, Alexander (2018) [California YIMBY — General Support (April 2018)](https://www.openphilanthropy.org/focus/us-policy/land-use-reform/california-yimby-general-support), *Open Philanthropy*, April.\n\nExternal links\n--------------\n\n[California YIMBY](https://cayimby.org/). Official website.\n\n[Apply for a job](https://cayimby.org/careers/).\n\n[Donate to California YIMBY](https://secure.actblue.com/donate/cayimby/).\n\nRelated entries\n---------------\n\n[land use reform](https://forum.effectivealtruism.org/tag/land-use-reform) | [policy](https://forum.effectivealtruism.org/topics/policy) | [United States](https://forum.effectivealtruism.org/topics/united-states)\n\n1.  ^**[^](#fnrefz6uebfacyxr)**^\n    \n    California YIMBY (2022) [About](https://cayimby.org/about/), *California YIMBY*.\n    \n2.  ^**[^](#fnref2imin3pbjz6)**^\n    \n    Berger, Alexander (2019) [California YIMBY — General Support (2019)](https://www.openphilanthropy.org/focus/us-policy/land-use-reform/california-yimby-general-support-2019), *Open Philanthropy*, April.\n    \n3.  ^**[^](#fnreftlipn7eljic)**^\n    \n    Berger,  [California YIMBY — General Support (2019)](https://www.openphilanthropy.org/focus/us-policy/land-use-reform/california-yimby-general-support-2019), fn. 7.\n    \n4.  ^**[^](#fnrefes3ub6r0gss)**^\n    \n    Open Philanthropy (2022) [Grants database: California YIMBY](https://www.openphilanthropy.org/grants/?q=&organization-name=california-yimby), *Open Philanthropy*.\n    \n5.  ^**[^](#fnrefor1586c1xmp)**^\n    \n    Kendall, Marisa (2018) [Stripe gives $1 million to pro-development YIMBY group tackling Bay Area housing shortage](https://www.mercurynews.com/2018/05/03/pro-development-yimby-group-scores-1-million-from-stripe-tackle-housing-shortage/), *The Mercury News*, May 3.\n    \n6.  ^**[^](#fnrefi362a23wdqh)**^\n    \n    Karnofsky, Holden (2019) [Suggestions for individual donors from Open Philanthropy Staff - 2019](https://www.openphilanthropy.org/blog/suggestions-individual-donors-open-philanthropy-staff-2019), *Open Philanthropy*, December 18, section 3.3.\n    \n7.  ^**[^](#fnrefti65j3jl8x)**^\n    \n    Karnofsky, Holden (2020) [Suggestions for individual donors from Open Philanthropy Staff - 2020](https://www.openphilanthropy.org/blog/suggestions-individual-donors-open-philanthropy-staff-2020), *Open Philanthropy*, November 25, section 4.2."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "8SphLXgkRSa867beP",
    "name": "Journalism",
    "core": false,
    "slug": "journalism",
    "oldSlugs": null,
    "postCount": 8,
    "description": {
      "markdown": "Further reading\n---------------\n\nDuda, Roman (2015) [Journalism](https://80000hours.org/career-reviews/journalism/), *80,000 Hours*, June.\n\nKoehler, Arden (2021) [Journalism](https://80000hours.org/career-reviews/become-a-journalist/), *80,000 Hours*, October 21.\n\nRelated entries\n---------------\n\n[Future Perfect](https://forum.effectivealtruism.org/topics/future-perfect) | [news relevant to effective altruism](https://forum.effectivealtruism.org/topics/news-relevant-to-effective-altruism)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "9iwDjQHCNg4JXNfPj",
    "name": "External praise for effective altruism",
    "core": false,
    "slug": "external-praise-for-effective-altruism",
    "oldSlugs": [
      "external-praise-for-ea"
    ],
    "postCount": 4,
    "description": {
      "markdown": "The **external praise for effective altruism** tag covers positive reviews of EA from people who don't consider themselves to be members of the movement. Think of it as a counterpart to the various \"criticism of EA\" tags."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "zMevJFMi5EHTLitXC",
    "name": "Biotechnology",
    "core": false,
    "slug": "biotechnology",
    "oldSlugs": null,
    "postCount": 12,
    "description": {
      "markdown": "**Biotechnology** is the use of living organisms and their derivatives to develop products in medicine, industry, and other areas.\n\nFurther reading\n---------------\n\nNouri, Ali & Christopher F. Chyba (2008) [Biotechnology and biosecurity](https://en.wikipedia.org/wiki/Special:BookSources/9780198570509), in Nick Bostrom & Milan M. Ćirković (eds.) *Global Catastrophic Risks*, Oxford: Oxford University Press, pp. 450–480.\n\nPearce, David (2017) [*Can Biotechnology Abolish Suffering?*](https://www.davidpearce.com/paradise-engineering.html), edited by Magnus Vinding, The Neuroethics Foundation.\n\nSnyder-Beattie, Andrew (2017) [Biotechnology and existential risk](https://forum.effectivealtruism.org/posts/an3LQNYMyxKTioKyn/andrew-snyder-beattie-biotechnology-and-existential-risk), *Effective Altruism Global*.\n\nRelated entries\n---------------\n\n[biosecurity](https://forum.effectivealtruism.org/tag/biosecurity) | [cultivated meat](https://forum.effectivealtruism.org/topics/cultivated-meat) | [gene drives](https://forum.effectivealtruism.org/topics/gene-drives) | [global catastrophic biological risk](https://forum.effectivealtruism.org/tag/global-catastrophic-biological-risk/) | [Invincible Wellbeing](https://forum.effectivealtruism.org/topics/invincible-wellbeing) | [life sciences](https://forum.effectivealtruism.org/topics/life-sciences)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "J2FJM4q22gf8djBtp",
    "name": "Refuges",
    "core": false,
    "slug": "refuges",
    "oldSlugs": null,
    "postCount": 15,
    "description": {
      "markdown": "**Refuges** (also known as **bunkers** or **shelters**) are structures designed to help humanity survive a [global catastrophe](https://forum.effectivealtruism.org/tag/global-catastrophic-risk).\n\nFurther reading\n---------------\n\nBaum, Seth D. (2015) [Confronting the threat of nuclear winter](https://doi.org/10.1016/j.futures.2015.03.004), *Futures*, vol. 72, pp. 69–79.  \n*Contains a brief discussion of refuges in section 4.2 (p. 7).*\n\nBaum, Seth D., David C. Denkenberger & Jacob Haqq-Misra (2015) [Isolated refuges for surviving global catastrophes](https://doi.org/10.1016/j.futures.2015.03.009), *Futures*, vol. 72, pp. 45–56.\n\nBeckstead, Nick (2015) [How much could refuges help us recover from a global catastrophe?](https://doi.org/10.1016/j.futures.2014.11.003), *Futures*, vol. 72, pp. 36–44.\n\nBoyd, Matt & Nick Wilson (2020) [The prioritization of island nations as refuges from extreme pandemics](https://doi.org/10.1111/risa.13398), *Risk Analysis*, vol. 40, pp. 227–239.\n\nChurch, George (2022) [SafeHomx (Home Sweet Biome): Biosecurity, refuges and/or ‘space colonies on earth’](https://arep.med.harvard.edu/gmc/safehomx.html), *George Church’s Website*.\n\nHanson, Robin (2008) [Catastrophe, social collapse, and human extinction](https://doi.org/10.1093/oso/9780198570509.003.0023), in Nick Bostrom & Milan M. Ćirković (eds.) *Global Catastrophic Risks*, Oxford: Oxford University Press, pp. 363–377.\n\nTurchin, Alexey & Brian Patrick Green (2017) [Aquatic refuges for surviving a global catastrophe](https://doi.org/10.1016/j.futures.2017.03.010), *Futures*, vol. 89, pp. 26–37.\n\nTurchin, Alexey & Brian Patrick Green (2019) [Islands as refuges for surviving global catastrophes](https://doi.org/10.1108/FS-04-2018-0031), *Foresight*, vol. 21, pp. 100–117.\n\nRelated entries\n---------------\n\n[civilizational collapse](https://forum.effectivealtruism.org/tag/civilizational-collapse) | [existential risk](https://forum.effectivealtruism.org/tag/existential-risk) | [global catastrophic risk](https://forum.effectivealtruism.org/tag/global-catastrophic-risk) | [nuclear winter](https://forum.effectivealtruism.org/tag/nuclear-winter) | [resilient food](https://forum.effectivealtruism.org/tag/resilient-food)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "qTohT7cn2S8prSrGk",
    "name": "Bioethics",
    "core": false,
    "slug": "bioethics",
    "oldSlugs": null,
    "postCount": 6,
    "description": {
      "markdown": "**Bioethics** is the branch of [applied ethics](https://forum.effectivealtruism.org/tag/applied-ethics) that studies the ethical issues arising from [medicine](https://forum.effectivealtruism.org/tag/medicine) and the [life sciences](https://forum.effectivealtruism.org/tag/life-sciences).\n\nFurther reading\n---------------\n\nBaron, Jonathan (2006) [*Against Bioethics*](https://en.wikipedia.org/wiki/Special:BookSources/978-0-262-02596-6), Cambridge, Massachusetts: The MIT Press.\n\nSchüklenk, Udo & Peter Singer (eds.) (2021) [*Bioethics: An Anthology*](https://en.wikipedia.org/wiki/Special:BookSources/978-1-119-63515-4), 4th ed., Hoboken, New Jersey: Wiley-Blackwell.\n\nRelated entries\n---------------\n\n[applied ethics](https://forum.effectivealtruism.org/tag/applied-ethics) | [moral philosophy](https://forum.effectivealtruism.org/tag/moral-philosophy)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "veupCrTGb8GMGs9xg",
    "name": "Alexander Berger",
    "core": false,
    "slug": "alexander-berger",
    "oldSlugs": null,
    "postCount": 2,
    "description": {
      "markdown": "**Alexander Berger** is an American researcher and grantmaker, and the co-founder and co-CEO of [Open Philanthropy](https://forum.effectivealtruism.org/tag/open-philanthropy). He leads Open Philanthropy’s [global health and wellbeing](https://forum.effectivealtruism.org/tag/global-health-and-wellbeing) program.^[\\[1\\]](#fn4es4dfsqlfp)^\n\nBackground\n----------\n\nBerger has a BA in philosophy and an MA in education from Stanford University. After reading a book by [Peter Singer](https://forum.effectivealtruism.org/tag/peter-singer) as a college student, he decided to take time off his studies and live in India for a while. During his stay there, he discovered [GiveWell](https://forum.effectivealtruism.org/tag/givewell). Upon returning to the United States, Berger contacted Elie Hassenfeld, and ended up volunteering during a summer break. He joined GiveWell in July 2011 after completing his studies, as that organization's fifth employee.^[\\[2\\]](#fnwy3gpejb7e)^\n\nIn 2011, Berger [donated a kidney](https://forum.effectivealtruism.org/tag/kidney-donation) to a stranger, and later wrote a *New York Times* editorial arguing for a regulated legal market for kidneys.^[\\[3\\]](#fn7h4wycz84sw)^\n\nFurther reading\n---------------\n\nWiblin, Robert & Keiran Harris (2021) [Alexander Berger on improving global health and wellbeing in clear and direct ways](https://80000hours.org/podcast/episodes/alexander-berger-improving-global-health-wellbeing-clear-direct-ways/), *80,000 Hours*, July 12.\n\nExternal links\n--------------\n\n[Alexander Berger](https://forum.effectivealtruism.org/users/alexander_berger). [Effective Altruism Forum](https://forum.effectivealtruism.org/tag/effective-altruism-forum-1) account.\n\nRelated entries\n---------------\n\n[GiveWell](https://forum.effectivealtruism.org/tag/givewell) | [Holden Karnofsky](https://forum.effectivealtruism.org/tag/holden-karnofsky) | [Open Philanthropy](https://forum.effectivealtruism.org/tag/open-philanthropy)\n\n1.  ^**[^](#fnref4es4dfsqlfp)**^\n    \n    Karnofsky, Holden (2021) [Open Philanthropy’s new co-CEO](https://www.openphilanthropy.org/blog/open-philanthropys-new-co-ceo), *Open Philanthropy*, June 16.\n    \n2.  ^**[^](#fnrefwy3gpejb7e)**^\n    \n    Wiblin, Robert & Keiran Harris (2021) [Alexander Berger on improving global health and wellbeing in clear and direct ways](https://80000hours.org/podcast/episodes/alexander-berger-improving-global-health-wellbeing-clear-direct-ways/), *80,000 Hours*, July 12.\n    \n3.  ^**[^](#fnref7h4wycz84sw)**^\n    \n    Berger, Alexander (2011) [Why selling kidneys should be legal](https://www.nytimes.com/2011/12/06/opinion/why-selling-kidneys-should-be-legal.html), *The New York Times*, December 6."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "QeyqB4hor6faJyvGG",
    "name": "Global health and wellbeing",
    "core": false,
    "slug": "global-health-and-wellbeing",
    "oldSlugs": null,
    "postCount": 31,
    "description": {
      "markdown": "**Global health and wellbeing** is a name that has been proposed by [Open Philanthropy](https://forum.effectivealtruism.org/topics/open-philanthropy) for the broad approach to [cause prioritization](https://forum.effectivealtruism.org/tag/cause-prioritization) within [effective altruism](https://forum.effectivealtruism.org/tag/effective-altruism) that focuses on doing good in the near term*.*^[\\[1\\]](#fn2mai76sf85l)^^[\\[2\\]](#fnx3td3il12u)^Other names for this approach include **short-termism**, **neartermism**, **non-longtermism**, and **global wellbeing**. Some consider these designations to be inaccurate or misleading, because they overemphasize [temporal discounting](https://forum.effectivealtruism.org/tag/temporal-discounting) and overlook its other defining characteristics, such as tight feedback loops, non-[fanaticism](https://forum.effectivealtruism.org/tag/fanaticism), and tangible impact.^[\\[1\\]](#fn2mai76sf85l)^^[\\[2\\]](#fnx3td3il12u)^^[\\[3\\]](#fn4ljbtpmoqad)^\n\nGlobal health and wellbeing may be contrasted to [longtermism](https://forum.effectivealtruism.org/tag/longtermism), which evaluates causes based on their expected effects on the [long-term future](https://forum.effectivealtruism.org/tag/long-term-future). \n\nProminent causes within global health and wellbeing include [scientific research](https://forum.effectivealtruism.org/topics/meta-science), [policy advocacy](https://forum.effectivealtruism.org/tag/policy), [farmed animal welfare](https://forum.effectivealtruism.org/tag/farmed-animal-welfare), and [global development](https://forum.effectivealtruism.org/tag/global-health-and-development). \n\nFurther reading\n---------------\n\nWiblin, Robert & Keiran Harris (2021) [Alexander Berger on improving global health and wellbeing in clear and direct ways](https://80000hours.org/podcast/episodes/alexander-berger-improving-global-health-wellbeing-clear-direct-ways/), *80,000 Hours*, July 12.\n\nRelated entries\n---------------\n\n[criticisms of longtermism and existential risk studies](https://forum.effectivealtruism.org/tag/criticisms-of-longtermism-and-existential-risk-studies) | [farmed animal welfare](https://forum.effectivealtruism.org/tag/farmed-animal-welfare) | [global health and development](https://forum.effectivealtruism.org/tag/global-health-and-development) | [longtermism](https://forum.effectivealtruism.org/tag/longtermism) | [metascience](https://forum.effectivealtruism.org/topics/metascience) | [policy](https://forum.effectivealtruism.org/tag/policy)\n\n1.  ^**[^](#fnref2mai76sf85l)**^\n    \n    Wiblin, Robert & Keiran Harris (2021) [Alexander Berger on improving global health and wellbeing in clear and direct ways](https://80000hours.org/podcast/episodes/alexander-berger-improving-global-health-wellbeing-clear-direct-ways/), *80,000 Hours*, July 12.\n    \n2.  ^**[^](#fnrefx3td3il12u)**^\n    \n    Karnofsky, Holden (2021) [Open Philanthropy’s new co-CEO](https://www.openphilanthropy.org/blog/open-philanthropys-new-co-ceo), *Open Philanthropy*, June 16.\n    \n3.  ^**[^](#fnref4ljbtpmoqad)**^\n    \n    Reinstein, David (2022) [Can we agree on a better name than ‘near-termist’? “Not-longermist”? “Not-full-longtermist”?](https://forum.effectivealtruism.org/posts/ySP5ZhXsxWXv7suv2/can-we-agree-on-a-better-name-than-near-termist-not), *Effective Altruism Forum*, April 19."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "WGzhhnemDALraKkDh",
    "name": "Coordination Forum",
    "core": false,
    "slug": "coordination-forum",
    "oldSlugs": [
      "ea-leaders-forum",
      "leaders-forum"
    ],
    "postCount": 2,
    "description": {
      "markdown": "The **Coordination Forum** (formerly known as the Leaders Forum) is an annual event for a few dozen core members of the [effective altruism](https://forum.effectivealtruism.org/tag/effective-altruism) community, run by the [Centre for Effective Altruism](https://forum.effectivealtruism.org/tag/centre-for-effective-altruism-1). The event seeks to improve coordination among thought leaders, organization founders and major donors, by building relationships, hosting strategy discussions and collaborating on various projects.\n\nIn 2020, the Coordination Forum was replaced by the **Virtual Coordination Forum**.^[\\[1\\]](#fniyb7s1h71re)^\n\nLeaders Forum survey\n--------------------\n\n[Surveys](https://forum.effectivealtruism.org/tag/surveys) of Coordination Forum attendees have been run at each Coordination Forum event. Results and analyses of some of these surveys have been published by [80,000 Hours](https://forum.effectivealtruism.org/tag/80-000-hours)^[\\[2\\]](#fnzxifbp5433)^^[\\[3\\]](#fnkymv9ang1i)^^[\\[4\\]](#fnnae22ncyay)^ and on the [Effective Altruism Forum](https://forum.effectivealtruism.org/tag/effective-altruism-forum-1).^[\\[5\\]](#fnchimeezfzk)^\n\nRelated entries\n---------------\n\n[Effective Altruism Global](https://forum.effectivealtruism.org/tag/effective-altruism-global)\n\n1.  ^**[^](#fnrefiyb7s1h71re)**^\n    \n    Dalton, Max (2020) [CEA’s 2020 Annual Review](https://forum.effectivealtruism.org/posts/SppupBEiPCAYA5nLW/cea-s-2020-annual-review), *Effective Altruism Forum*, December 10.\n    \n2.  ^**[^](#fnrefzxifbp5433)**^\n    \n    McIntyre, Peter (2017) [What skills are effective altruist organisations short of? Results from our survey](https://80000hours.org/2017/03/what-skills-are-effective-altruist-organisations-missing/), *80,000 Hours*, March 19.\n    \n3.  ^**[^](#fnrefkymv9ang1i)**^\n    \n    Wiblin, Robert (2017) [What are the most important talent gaps in the effective altruism community?](https://80000hours.org/2017/11/talent-gaps-survey-2017/), *80,000 Hours*, November 3.\n    \n4.  ^**[^](#fnrefnae22ncyay)**^\n    \n    Todd, Benjamin & Howie Lempel (2018) [What skills and experience are most needed in professional effective altruism in 2018? – 80,000 Hours leaders survey](https://80000hours.org/2018/10/2018-talent-gaps-survey/), *80,000 Hours*, October 8.\n    \n5.  ^**[^](#fnrefchimeezfzk)**^\n    \n    Gertler, Aaron (2019) [EA Leaders Forum: Survey on EA priorities (data and analysis)](https://forum.effectivealtruism.org/posts/TpoeJ9A2G5Sipxfit/ea-leaders-forum-survey-on-ea-priorities-data-and-analysis), *Effective Altruism Forum*, November 11."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "NXc4QiFBodzEuMmFs",
    "name": "People for the Ethical Treatment of Reinforcement Learners",
    "core": false,
    "slug": "people-for-the-ethical-treatment-of-reinforcement-learners",
    "oldSlugs": null,
    "postCount": 2,
    "description": {
      "markdown": "**People for the Ethical Treatment of Reinforcement Learners** (**PETRL**) is an organization focused on the welfare of AI agents.\n\nHistory\n-------\n\nPETRL was founded by Daniel Filan, Buck Shlegeris, Jan Leike, Mayank Daswani, and others. A paper by [Brian Tomasik](https://forum.effectivealtruism.org/tag/brian-tomasik) on the ethics of reinforcement-learning agents inspired the creation of PETRL.^[\\[1\\]](#fndpymqljz8hi)^\n\nPETRL was most active from 2014 to 2015.\n\nFurther reading\n---------------\n\nBostrom, Nick (2014) [*Superintelligence: Paths, Dangers, Strategies*](https://en.wikipedia.org/wiki/Special:BookSources/978-0-19-967811-2), Oxford: Oxford University Press.\n\nDaswani, Mayank & Jan Leike (2015) [A definition of happiness for reinforcement learning agents](http://arxiv.org/abs/1505.04497), arXiv:1505.04497 \\[Cs\\].\n\nSchwitzgebel, Eric & Mara Garza (2015) [A defense of the rights of artificial intelligences: defense of the rights of artificial intelligences](https://doi.org/10.1111/misp.12032), *Midwest Studies In Philosophy*, vol. 39, pp. 98–119.\n\nvan der Merwe, Matthew (2019) [What ever happened to PETRL (People for the Ethical Treatment of Reinforcement Learners)?](https://forum.effectivealtruism.org/posts/gsJn5BpDLQu4bbKpX/what-ever-happened-to-petrl-people-for-the-ethical-treatment), *Effective Altruism Forum*, December 30.\n\nExternal links\n--------------\n\n[People for the Ethical Treatment of Reinforcement Learners](http://petrl.org/). Official website.\n\nRelated entries\n---------------\n\n[artificial intelligence](https://forum.effectivealtruism.org/tag/artificial-intelligence) | [artificial sentience](https://forum.effectivealtruism.org/tag/artificial-sentience) | [mind crime](https://forum.effectivealtruism.org/tag/mind-crime)\n\n1.  ^**[^](#fnrefdpymqljz8hi)**^\n    \n    Tomasik, Brian (2014) [Do artificial reinforcement-learning agents matter morally?](http://arxiv.org/abs/1410.8233), arXiv:1410.8233 \\[Cs\\]*,* p. 23."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "JmPALrDPiz5pegkKw",
    "name": "Alignment tax",
    "core": false,
    "slug": "alignment-tax",
    "oldSlugs": null,
    "postCount": 3,
    "description": {
      "markdown": "An **alignment tax** (sometimes called a **safety tax**) is the additional cost of making [AI aligned](https://forum.effectivealtruism.org/tag/ai-alignment), relative to unaligned AI.\n\nApproaches to the alignment tax\n-------------------------------\n\n[Paul Christiano](https://forum.effectivealtruism.org/tag/paul-christiano) distinguishes two main approaches for dealing with the alignment tax.^[\\[1\\]](#fn23n8pxr8hei)^^[\\[2\\]](#fnhm2j5l8lqoq)^  One approach seeks to find ways to pay the tax, such as persuading individual actors to pay it or facilitating coordination of the sort that would allow groups to pay it. The other approach tries to reduce the tax, by differentially advancing existing alignable algorithms or by making existing algorithms more alignable.\n\nFurther reading\n---------------\n\nAskell, Amanda *et al.* (2021) [A general language assistant as a laboratory for alignment](http://arxiv.org/abs/2112.00861), arXiv:2112.00861 \\[Cs\\].\n\nXu, Mark & Carl Shulman (2021) [Rogue AGI embodies valuable intellectual property](https://www.lesswrong.com/posts/FM49gHBrs5GTx7wFf/rogue-agi-embodies-valuable-intellectual-property), *LessWrong*, June 3.\n\nYudkowsky, Eliezer (2017) [Aligning an AGI adds significant development time](https://arbital.com/p/aligning_adds_time/), *Arbital*, February 22.\n\nRelated entries\n---------------\n\n[AI alignment](https://forum.effectivealtruism.org/tag/ai-alignment) | [AI governance](https://forum.effectivealtruism.org/topics/ai-governance) | [AI forecasting](https://forum.effectivealtruism.org/tag/ai-forecasting) | [differential progress](https://forum.effectivealtruism.org/tag/differential-progress)\n\n1.  ^**[^](#fnref23n8pxr8hei)**^\n    \n    Christiano, Paul (2020) [Current work in AI alignment](https://forum.effectivealtruism.org/posts/63stBTw3WAW6k45dY/paul-christiano-current-work-in-ai-alignment), *Effective Altruism Global*, April 3.\n    \n2.  ^**[^](#fnrefhm2j5l8lqoq)**^\n    \n    For a summary, see Rohin Shah (2020) [A framework for thinking about how to make AI go well](https://www.lesswrong.com/posts/9et86yPRk6RinJNt3/an-95-a-framework-for-thinking-about-how-to-make-ai-go-well), *LessWrong*, April 15."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "JoRQv39rbpzvw5NAq",
    "name": "Megaprojects",
    "core": false,
    "slug": "megaprojects",
    "oldSlugs": [
      "effective-altruism-megaprojects"
    ],
    "postCount": 19,
    "description": {
      "markdown": "Within the [effective altruism](https://forum.effectivealtruism.org/topics/effective-altruism) community, **megaprojects** are projects that could deploy $100 million or more in funding per year.\n\nFurther reading\n---------------\n\nTodd, Benjamin (2021) [Effective altruism needs more \"megaprojects\"](https://twitter.com/ben_j_todd/status/1423318852801290248), *Twitter*, August 5.\n\nYoung, Nathan (2021) [What EA projects could grow to become megaprojects, eventually spending $100m per year?](https://forum.effectivealtruism.org/posts/ckcoSe3CS2n3BW3aT/what-ea-projects-could-grow-to-become-megaprojects), *Effective Altruism Forum*, August 6.\n\nRelated entries\n---------------\n\n[ambition](https://forum.effectivealtruism.org/tag/ambition) | [community projects](https://forum.effectivealtruism.org/tag/community-projects) | [scalably using labour](https://forum.effectivealtruism.org/tag/scalably-using-labour)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "ZfCuaYAhXLfhYe3eG",
    "name": "Cambridge Existential Risks Initiative",
    "core": false,
    "slug": "cambridge-existential-risks-initiative",
    "oldSlugs": [
      "cambridge-existential-risk-initiative"
    ],
    "postCount": 10,
    "description": {
      "markdown": "The **Cambridge Existential Risks Initiative** (**CERI**, pronounced /ˈkɛri/) is a network of academics and students at the University of Cambridge focused on [existential risk](https://forum.effectivealtruism.org/tag/existential-risk) reduction.\n\nThe **CERI Fellowship** provides aspiring researchers with an in-person, paid, 10-week summer research fellowship in Cambridge to do research and become immersed in the community developing ways to mitigate extreme threats to humanity.\n\nFunding\n-------\n\nAs of July 2022, CERI has received nearly $750,000 in funding from [Open Philanthropy](https://forum.effectivealtruism.org/topics/open-philanthropy) to support the CERI Fellowship.^[\\[1\\]](#fnmqwh7id96o)^\n\nFurther reading\n---------------\n\nAldred, Will (2022) [CERI SRF ’22](https://forum.effectivealtruism.org/s/c42cHpNCLPN8cLtje), *Effective Altruism Forum*, March 3  \n*Collection of posts from the 2022 Summer Research Fellowship.*\n\nExternal links\n--------------\n\n[Cambridge Existential Risks Initiative](https://camxrisk.org/). Official website. \n\n[CERI Fellowship](https://www.cerifellowship.org/). Official website.\n\nRelated entries\n---------------\n\n[fellowships & internships](https://forum.effectivealtruism.org/tag/fellowships-and-internships) | [field-building](https://forum.effectivealtruism.org/tag/field-building) | [research training programs](https://forum.effectivealtruism.org/tag/research-training-programs) | [Stanford Existential Risks Initiative](https://forum.effectivealtruism.org/tag/stanford-existential-risks-initiative)\n\n1.  ^**[^](#fnrefmqwh7id96o)**^\n    \n    Open Philanthropy (2022) [Grants database: Cambridge Existential Risk Initiative](https://www.openphilanthropy.org/grants/?q=&organization-name=cambridge-existential-risk-initiative), *Open Philanthropy*."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "24CZyPmaQqehDEAFw",
    "name": "Nick Beckstead",
    "core": false,
    "slug": "nick-beckstead",
    "oldSlugs": null,
    "postCount": 3,
    "description": {
      "markdown": "**Nicholas Beckstead** (born 1985) is an American philosopher and the Chief Executive Officer of the [FTX Foundation](https://forum.effectivealtruism.org/tag/ftx-foundation).\n\nBackground\n----------\n\nBeckstead majored in mathematics and philosophy at the University of Minnesota and obtained a PhD in philosophy from Rutgers University. As a graduate student, he co-founded the first US chapter of [Giving What We Can](https://forum.effectivealtruism.org/tag/giving-what-we-can), pledging to donate half of his post-tax income until his retirement to the most cost-effective organizations fighting [global poverty](https://forum.effectivealtruism.org/tag/global-poverty) in the developing world.^[\\[1\\]](#fnuwpb5lozswj)^^[\\[2\\]](#fnbkgm5o33uuw)^\n\nAfter completing his studies, Beckstead became a Research Fellow at the [Future of Humanity Institute](https://forum.effectivealtruism.org/tag/future-of-humanity-institute) and then a Program Officer for [Open Philanthropy](https://forum.effectivealtruism.org/tag/open-philanthropy), where he oversaw much of that organization's research and grantmaking related to [global catastrophic risk](https://forum.effectivealtruism.org/tag/global-catastrophic-risk) reduction. He joined the FTX Foundation in November 2021.^[\\[3\\]](#fnsxqgct91aoe)^\n\nResearch\n--------\n\nBeckstead's research focuses on topics related to the [long-term future](https://forum.effectivealtruism.org/tag/long-term-future) and its normative implications, including [existential risk](https://forum.effectivealtruism.org/tag/existential-risk),^[\\[4\\]](#fnv3r7ehxyuar)^^[\\[5\\]](#fn1aimmpe0llu)^ [population ethics](https://forum.effectivealtruism.org/tag/population-ethics),^[\\[6\\]](#fnafe0bzgl5iu)^ [space colonization](https://forum.effectivealtruism.org/tag/space-colonization),^[\\[7\\]](#fn3geuv6g7gkv)^ and [differential progress](https://forum.effectivealtruism.org/tag/differential-progress).^[\\[8\\]](#fnm7j5e831vm8)^ His doctoral dissertation, which combines some of these interests, is often credited as an important early contribution to [longtermism](https://forum.effectivealtruism.org/tag/longtermism).^[\\[9\\]](#fn0zlpp1vkujl)^\n\nFurther reading\n---------------\n\nBeckstead, Nick (2020) [Existential risks: fundamentals, overview and intervention points](https://www.youtube.com/watch?v=JfpTywk6chw), *World Universities Debating Championship Distinguished Lecture Series*, November 5.\n\nMuehlhauser, Luke (2011) [Nick Beckstead - Morality and global catastrophic risks](https://www.listennotes.com/podcasts/conversations-from/087-nick-beckstead-morality-W7nH5Zf4-Ba/), *Conversations from the Pale Blue Dot*, January 30.\n\nWiblin, Robert (2017) [You want to do as much good as possible and have billions of dollars. What do you do?](https://80000hours.org/podcast/episodes/nick-beckstead-giving-billions/), *80,000 Hours*, October 11.\n\nExternal links\n--------------\n\n[Nick Beckstead](https://www.nickbeckstead.com/). Official website.\n\n[Nick Beckstead](https://forum.effectivealtruism.org/users/nick_beckstead). [Effective Altruism Forum](https://forum.effectivealtruism.org/tag/effective-altruism-forum-1) account.\n\n1.  ^**[^](#fnrefuwpb5lozswj)**^\n    \n    Eng, James (2010) [Ordinary people, extraordinary giving](https://www.nbcnews.com/id/wbna40533741), *NBC News*, December 21.\n    \n2.  ^**[^](#fnrefbkgm5o33uuw)**^\n    \n    Muehlhauser, Luke (2011) [Nick Beckstead - Morality and global catastrophic risks](https://www.listennotes.com/podcasts/conversations-from/087-nick-beckstead-morality-W7nH5Zf4-Ba/), *Conversations from the Pale Blue Dot*, January 30.\n    \n3.  ^**[^](#fnrefsxqgct91aoe)**^\n    \n    LinkedIn (2022) [Nick Beckstead](https://www.linkedin.com/in/nick-beckstead-7aa54374/), *LinkedIn*.\n    \n4.  ^**[^](#fnrefv3r7ehxyuar)**^\n    \n    Beckstead, Nick & Toby Ord (2014) [Managing existential risk from emerging technologies](https://www.fhi.ox.ac.uk/wp-content/uploads/Managing-existential-risks-from-Emerging-Technologies.pdf), in Mark Walport (ed.) *Annual Report of the Government Chief Scientific Advisor 2014. Innovation: Managing Risk, Not Avoiding It. Evidence and Case Studies*, London: Government Office for Science, pp. 115–120.\n    \n5.  ^**[^](#fnref1aimmpe0llu)**^\n    \n    Beckstead, Nick (2015) [The long-term significance of reducing global catastrophic risks](https://www.openphilanthropy.org/blog/long-term-significance-reducing-global-catastrophic-risks), *Open Philanthropy*, August 13.\n    \n6.  ^**[^](#fnrefafe0bzgl5iu)**^\n    \n    Beckstead, Nick (2019) [A brief argument for the overwhelming importance of shaping the far future](https://doi.org/10.1093/oso/9780198841364.003.0006), in Hilary Greaves & Theron Pummer (eds.) *Effective Altruism: Philosophical Issues*, Oxford: Oxford University Press, pp. 80–98.\n    \n7.  ^**[^](#fnref3geuv6g7gkv)**^\n    \n    Beckstead, Nick (2014) [Will we eventually be able to colonize other stars? Notes from a preliminary review](http://globalprioritiesproject.org/2014/06/will-we-eventually-be-able-to-colonize-other-stars-notes-from-a-preliminary-review/), *Global Priorities Project*, June 22.\n    \n8.  ^**[^](#fnrefm7j5e831vm8)**^\n    \n    Beckstead, Nick (2015) [Differential technological development: some early thinking](https://blog.givewell.org/2015/09/30/differential-technological-development-some-early-thinking/), *The GiveWell Blog*, September 30.\n    \n9.  ^**[^](#fnref0zlpp1vkujl)**^\n    \n    Beckstead, Nick (2013) [*On the Overwhelming Importance of Shaping the Far Future*](https://doi.org/10.7282/T35M649T), PhD thesis, Rutgers University."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "vQW8K9NseFGxjSJqX",
    "name": "Project voting",
    "core": false,
    "slug": "project-voting",
    "oldSlugs": null,
    "postCount": 6,
    "description": {
      "markdown": "A voting category for simpler projects that EA communities and individuals can take on to help further the cause of Effective Altruism. Write PV: to show the intention of it being part of the voting system."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "biPnaPsij8n2cir8s",
    "name": "Economics of artificial intelligence",
    "core": false,
    "slug": "economics-of-artificial-intelligence",
    "oldSlugs": [
      "economics-of-ai"
    ],
    "postCount": 9,
    "description": {
      "markdown": "The **economics of artificial intelligence** studies the implications for [economic growth](https://forum.effectivealtruism.org/topics/economic-growth) and labor markets of increasingly powerful AI systems. The economics of AI also covers the study of short-term and long-term economic dynamics that affect the development of AI, including increased production of computing resources; the relationship between access to human capital and speed of AI research; and competition dynamics among AI firms. The field draws heavily from the study of technical change and innovation in economics, and their effect on economic growth and firm dynamics.\n\nFurther reading\n---------------\n\nHanson, Robin (2001) [Economic growth given machine intelligence](https://mason.gmu.edu/~rhanson/aigrow.pdf), unpublished.\n\nTrammell, Philip & Anton Korinek (2021) [Economic growth under transformative AI: A guide to the vast range of possibilities for output growth, wages, and the labor share](https://globalprioritiesinstitute.org/wp-content/uploads/Philip-Trammell-and-Anton-Korinek_economic-growth-under-transformative-ai.pdf), Global Priorities Institute, University of Oxford.\n\nYudkowsky, Eliezer (2013) [Intelligence explosion microeconomics](https://intelligence.org/files/IEM.pdf), technical report 2013-1, Machine Intelligence Research Institute.\n\nExternal links\n--------------\n\n[The economics of AI](https://www.coursera.org/learn/economics-of-ai). An online course by Prof. Anton Korinek.\n\nRelated entries\n---------------\n\n[AI governance](https://forum.effectivealtruism.org/topics/ai-governance) | [artificial intelligence](https://forum.effectivealtruism.org/tag/ai-governance) | [economics](https://forum.effectivealtruism.org/tag/economics) | [economic growth](https://forum.effectivealtruism.org/tag/economic-growth) | [transformative artificial intelligence](https://forum.effectivealtruism.org/tag/transformative-artificial-intelligence)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "F9XvnXFkxC68DdC5b",
    "name": "Jaan Tallinn",
    "core": false,
    "slug": "jaan-tallinn",
    "oldSlugs": null,
    "postCount": 4,
    "description": {
      "markdown": "**Jaan Tallinn** (born 14 February 1972) is an Estonian investor and philanthropist. He is the co-founder of the [Future of Life Institute](https://forum.effectivealtruism.org/tag/future-of-life-institute) and of the [Centre for the Study of Existential Risk](https://forum.effectivealtruism.org/tag/centre-for-the-study-of-existential-risk)."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "qxgzzC3o2LH7eyvyX",
    "name": "David Pearce",
    "core": false,
    "slug": "david-pearce-1",
    "oldSlugs": null,
    "postCount": 8,
    "description": {
      "markdown": "**David Pearce** (born 1959) is a philosopher and writer best known for his 1995 manifesto 'The hedonistic imperative'^[\\[1\\]](#fnagolrd3bwsr)^ and the associated ideas about abolishing [suffering](https://forum.effectivealtruism.org/tag/pain-and-suffering) for all sentient life using [biotechnology](https://forum.effectivealtruism.org/tag/biotechnology) and other technologies.^[\\[2\\]](#fn9qq7dvjqrqu)^\n\nBackground\n----------\n\nPearce studied philosophy at Brasenose College, Oxford.^[\\[3\\]](#fnfdidv34vwba)^ In 1998, together with [Nick Bostrom](https://forum.effectivealtruism.org/tag/nick-bostrom), Pearce co-founded the World Transhumanist Association, today known as Humanity+.\n\nPearce is the director of bioethics of [Invincible Wellbeing](https://forum.effectivealtruism.org/topics/invincible-wellbeing) and is on the advisory boards of the [Center on Long-Term Risk](https://forum.effectivealtruism.org/tag/center-on-long-term-risk), the [Organisation for the Prevention of Intense Suffering](https://forum.effectivealtruism.org/tag/organisation-for-the-prevention-of-intense-suffering) and the [Qualia Research Institute](https://forum.effectivealtruism.org/tag/qualia-research-institute). He is also a fellow of the Institute for Ethics and Emerging Technologies and is on the futurist advisory board of the Lifeboat Foundation.\n\nAbolition of suffering\n----------------------\n\nPearce argues that it is technically feasible and ethically rational to abolish suffering on the planet by replacing Darwinian suffering-based motivational systems with minds motivated exclusively by pleasant experience. He stresses that this \"abolitionist project\" is compatible with a diverse set of values and \"intentional objects\".^[\\[2\\]](#fn9qq7dvjqrqu)^\n\nOther interests\n---------------\n\nPearce has also written on the [intelligence explosion](https://forum.effectivealtruism.org/tag/intelligence-explosion),^[\\[4\\]](#fndr3ys55vo0i)^[ wild animal welfare](https://forum.effectivealtruism.org/tag/wild-animal-welfare),^[\\[5\\]](#fneduspahmtkq)^[ philosophy of mind](https://forum.effectivealtruism.org/tag/philosophy-of-mind),^[\\[6\\]](#fnmvuodjbqqsb)^ affective enhancement,^[\\[7\\]](#fnu5z9tn6bte)^ and other topics at the intersection of [transhumanism](https://forum.effectivealtruism.org/tag/transhumanism) and [effective altruism](https://forum.effectivealtruism.org/tag/effective-altruism).\n\nFurther reading\n---------------\n\nPearce, David (2007) [The abolitionist project](https://www.hedweb.com/abolitionist-project/index.html), *HedWeb*.\n\nPearce, David (1995) [The hedonistic imperative](https://www.hedweb.com/hedethic/tabconhi.htm), *HedWeb*.\n\nPerry, Lucas (2018) [The metaethics of joy, suffering, and artificial intelligence with Brian Tomasik and David Pearce](https://futureoflife.org/2018/08/16/ai-alignment-podcast-metaethics-of-joy-suffering-with-brian-tomasik-and-david-pearce/), *AI Alignment Podcast*, August 16.\n\nQualia Research Institute (2022) [David Pearce and Andrés Gómez Emilsson chat about the nature of reality](https://www.youtube.com/watch?v=0GE4AdG1Z2g), *YouTube*, September 7.\n\nTomasik, Brian (2016) [Why I don’t focus on the Hedonistic Imperative](https://reducing-suffering.org/dont-focus-hedonistic-imperative/), *Essays on Reducing Suffering*, October 5 (updated 20 July 2017).\n\nExternal links\n--------------\n\n[HedWeb](https://www.hedweb.com/confile.htm). Official website.\n\n[Invincible Wellbeing](https://www.invinciblewellbeing.com/). Official website.\n\nRelated entries\n---------------\n\n[flourishing futures](https://forum.effectivealtruism.org/tag/flourishing-futures) | [gene drives](https://forum.effectivealtruism.org/tag/gene-drives) | [Invincible Wellbeing](https://forum.effectivealtruism.org/topics/invincible-wellbeing) | [pain and suffering](https://forum.effectivealtruism.org/tag/pain-and-suffering) | [philosophy of mind](https://forum.effectivealtruism.org/topics/philosophy-of-mind) | [suffering-focused ethics](https://forum.effectivealtruism.org/tag/suffering-focused-ethics) | [transhumanism](https://forum.effectivealtruism.org/tag/transhumanism) | [welfare biology](https://forum.effectivealtruism.org/topics/welfare-biology) | [wild animal welfare](https://forum.effectivealtruism.org/tag/wild-animal-welfare)\n\n1.  ^**[^](#fnrefagolrd3bwsr)**^\n    \n    Pearce, David (1995) [The hedonistic imperative](https://www.hedweb.com/hedethic/tabconhi.htm), *HedWeb*.\n    \n2.  ^**[^](#fnref9qq7dvjqrqu)**^\n    \n    Pearce, David (2007) [The abolitionist project](https://www.hedweb.com/abolitionist-project/index.html), *HedWeb*.\n    \n3.  ^**[^](#fnreffdidv34vwba)**^\n    \n    Brasenose College (2021) [Notable alumni](https://www.bnc.ox.ac.uk/alumni/notable-alumni), *Brasenose College*.\n    \n4.  ^**[^](#fnrefdr3ys55vo0i)**^\n    \n    Pearce, David (2012) [The biointelligence explosion](https://doi.org/10.1007/978-3-642-32560-1_11), in Amnon H. Eden *et al.* (eds.) *Singularity Hypotheses*, Berlin, Heidelberg: Springer Berlin Heidelberg, pp. 199–238.\n    \n5.  ^**[^](#fnrefeduspahmtkq)**^\n    \n    Pearce, David (2012) [A welfare state for elephants? Costs and practicalities of comprehensive healthcare for free-living African elephants](https://www.hedweb.com/abolitionist-project/elephantcare.html), *HedWeb* (updated 2015).\n    \n6.  ^**[^](#fnrefmvuodjbqqsb)**^\n    \n    Pearce, David (2014) [Does physicalism entail monistic idealism? An experimentally testable conjecture](https://www.hedweb.com/physicalism/index.html), *HedWeb*, September (updated October 2014).\n    \n7.  ^**[^](#fnrefu5z9tn6bte)**^\n    \n    Pearce, David (2002) [Utopian Pharmacology: MDMA / Ecstasy and beyond](https://www.hedweb.com/ecstasy/index.html), *HedWeb* (updated 2020)."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "66XKrj8MecBoYRDrm",
    "name": "Optimizer's curse",
    "core": false,
    "slug": "optimizer-s-curse",
    "oldSlugs": null,
    "postCount": 8,
    "description": {
      "markdown": "The **optimizer's curse** is the phenomenon that causes an agent optimizing over a set set of options with uncertain value to systematically overestimate the value of the chosen option.\n\nIllustration\n------------\n\nThe authors of the paper that originally introduced the optimizer's curse provide a simple illustration of the phenomenon:^[\\[1\\]](#fnyaousqfl47j)^\n\n> suppose that we evaluate three alternatives that all have true values \\\\(\\\\mu_i\\\\) of exactly zero. The value of each alternative is estimated and the estimates \\\\(V_i\\\\) are independent and normally distributed with mean equal to the true value of zero (they are conditionally unbiased) and a standard deviation of one. Selecting the highest value estimate then amounts to selecting the maximum of three draws from a standard normal distribution. The distribution of this maximal value estimate is easily determined by simulation or using results from order statistics and is displayed in Figure 1. The mean of this distribution is 0.85, so in this case, the expected disappointment,\\\\(E\\[V_{i*}-\\\\mu_{i*}\\]\\\\), is 0.85.\n\n![](https://39669.cdn.cke-cs.com/cgyAlfpLFBBiEjoXacnz/images/6f2628fb2638b7279b06ba60e059257ac0a8b591743e74bd.png)\n\nFrom Smith, James E. & Robert L. Winkler (2006) [The optimizer’s curse: skepticism and postdecision surprise in decision analysis](https://doi.org/10.1287/mnsc.1050.0451), *Management Science*, vol. 52, p. 312.\n\nFurther reading\n---------------\n\nMuehlhauser, Luke (2011) [The optimizer’s curse and how to beat it](https://www.lesswrong.com/posts/5gQLrJr2yhPzMCcni/the-optimizer-s-curse-and-how-to-beat-it), *LessWrong*, September 15.\n\nSmith, James E. & Robert L. Winkler (2006) [The optimizer’s curse: skepticism and postdecision surprise in decision analysis](https://doi.org/10.1287/mnsc.1050.0451), *Management Science*, vol. 52, pp. 311–322.\n\nRelated entries\n---------------\n\n[expected value](https://forum.effectivealtruism.org/tag/expected-value) | [unilateralist's curse](https://forum.effectivealtruism.org/topics/unilateralist-s-curse)\n\n1.  ^**[^](#fnrefyaousqfl47j)**^\n    \n    Smith, James E. & Robert L. Winkler (2006) [The optimizer’s curse: skepticism and postdecision surprise in decision analysis](https://doi.org/10.1287/mnsc.1050.0451), *Management Science*, vol. 52, pp. 312."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "xcJfJSSCigZibT3Sc",
    "name": "Corruption",
    "core": false,
    "slug": "corruption",
    "oldSlugs": null,
    "postCount": 10,
    "description": {
      "markdown": "**Corruption** is improper and usually unlawful conduct intended to secure a benefit for oneself or another.\n\nFurther reading\n---------------\n\nMiller, Seumas (2005) [Corruption](https://plato.stanford.edu/archives/win2018/entries/corruption/), *The Stanford Encyclopedia of Philosophy*, September 14 (updated 21 September 2018).\n\nRelated entries\n---------------\n\n[economics](https://forum.effectivealtruism.org/tag/economics) | [improving institutional decision-making](https://forum.effectivealtruism.org/topics/improving-institutional-decision-making) | [policy](https://forum.effectivealtruism.org/tag/policy) | [transparency](https://forum.effectivealtruism.org/tag/transparency)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "WWdbuEmxRy5w7CqDx",
    "name": "Biosurveillance",
    "core": false,
    "slug": "biosurveillance",
    "oldSlugs": null,
    "postCount": 7,
    "description": {
      "markdown": "**Biosurveillance**, in the context of [biosecurity](https://forum.effectivealtruism.org/tag/biosecurity), is the detection and monitoring of biological agents to prevent biological threats. This is most commonly associated and achieved via [metagenomic sequencing](https://forum.effectivealtruism.org/topics/metagenomics), the readout of the totality of nucleic acids (DNA and RNA) present in an environmental sample.\n\nExamples\n--------\n\nA prominent biosurveillance implementation is BioWatch, a US federal program designed to detect the release of known pathogens in 21 American cities. It mainly focuses on detecting airborne releases and subsequent PCR-based testing against a list of candidate pathogens. BioWatch has, however, been criticised for failing to respond to known threats while sounding false-positive alarms.\n\nAn unbiased, metagenomic sequencing-based proposal is the [Nucleic Acid Observatory](https://forum.effectivealtruism.org/topics/nucleic-acid-observatory). It aims to detect any changes in the presence and composition of nucleic acids in major waterways and sewage, thus recognising emerging pathogens as early as possible.\n\nFurther reading\n---------------\n\nFox, Ben (2021) [Audit finds major gaps in US bio weapons detection system](https://apnews.com/article/bioterrorism-terrorism-united-states-biological-weapons-19da38a1af8b1f3b1c41573d6cbdff7a), *AP News*, March 4.\n\nNieuwenhuijse, David F. *et al.* (2020) [Setting a baseline for global urban virome surveillance in sewage](http://doi.org/10.1038/s41598-020-69869-0), *Scientific Reports*, vol. 10, 13748.\n\nShea, Dana A. & Sarah A. Lister (2003) [The Biowatch Program: Detection of bioterrorism](https://crsreports.congress.gov/product/pdf/RL/RL32152/3), Report No. RL32152, Congressional Research Service.\n\nThe Nucleic Acid Observatory Consortium (2021) [A global nucleic acid observatory for biodefense and planetary health](http://arxiv.org/abs/2108.02678), arXiv:2108.02678.\n\nRelated entries\n---------------\n\n[biosecurity](https://forum.effectivealtruism.org/tag/biosecurity) | [COVID-19 pandemic](https://forum.effectivealtruism.org/tag/covid-19-pandemic) | [global catastrophic biological risk](https://forum.effectivealtruism.org/tag/global-catastrophic-biological-risk) | [metagenomics](https://forum.effectivealtruism.org/topics/metagenomics) | [Nucleic Acid Observatory](https://forum.effectivealtruism.org/topics/nucleic-acid-observatory)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "doAxFNMmB7qMbJsDR",
    "name": "Criticism of effective altruism culture",
    "core": false,
    "slug": "criticism-of-effective-altruism-culture",
    "oldSlugs": [
      "criticism-of-the-effective-altruism-community"
    ],
    "postCount": 80,
    "description": {
      "markdown": "The **criticism of effective altruism culture** tag covers criticism of behaviors, norms, and dynamics in the effective altruism community, as opposed to [criticism of effective altruist causes](https://forum.effectivealtruism.org/topics/criticism-of-effective-altruist-causes), [organizations](https://forum.effectivealtruism.org/topics/criticism-of-effective-altruist-organizations), or [philosophy](https://forum.effectivealtruism.org/topics/criticism-of-effective-altruism).\n\nRelated entries\n---------------\n\n[building effective altruism](https://forum.effectivealtruism.org/tag/building-effective-altruism-1) | [community](https://forum.effectivealtruism.org/tag/community) | [criticism of effective altruism](https://forum.effectivealtruism.org/tag/criticism-of-effective-altruism) | [criticism of effective altruist causes](https://forum.effectivealtruism.org/tag/criticism-of-effective-altruist-causes) | [criticism of effective altruist organizations](https://forum.effectivealtruism.org/tag/criticism-of-effective-altruist-organizations) | [effective altruism culture](https://forum.effectivealtruism.org/tag/effective-altruism-culture)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "JL82PqCp4bKbrrQuT",
    "name": "Food security",
    "core": false,
    "slug": "food-security",
    "oldSlugs": null,
    "postCount": 15,
    "description": {
      "markdown": "**Food security** is a measure of the degree to which individuals have access to food.\n\nFurther reading\n---------------\n\nEC-FAO Food Security Programme (2008) [An introduction to the basic concepts of food security](https://www.fao.org/documents/card/en/c/2357d07c-b359-55d8-930a-13060cedd3e3/), *Food and Agriculture Organization of the United NationsFood and Agriculture Organization of the United Nations*.\n\nRelated entries\n---------------\n\n[global poverty](https://forum.effectivealtruism.org/topics/global-poverty) | [resilient food](https://forum.effectivealtruism.org/topics/resilient-food)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "ZoJKZcJspE9rhwvy5",
    "name": "Volcanic winter",
    "core": false,
    "slug": "volcanic-winter",
    "oldSlugs": null,
    "postCount": 3,
    "description": {
      "markdown": "A **volcanic winter** is the prolonged global temperature decrease after large volumes of particulate matter are thrown into earth's atmosphere by a [severe volcanic eruption](https://forum.effectivealtruism.org/tag/supervolcano)."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "DZaNsdyL5kaE4jTYE",
    "name": "High Impact Medicine",
    "core": false,
    "slug": "high-impact-medicine",
    "oldSlugs": null,
    "postCount": 6,
    "description": {
      "markdown": "**High Impact Medicine** is an informal group that spreads [effective altruism](https://forum.effectivealtruism.org/tag/effective-altruism) ideas among medical students and doctors.\n\nExternal links\n--------------\n\n[High Impact Medicine](https://www.highimpactmedicine.org/). Official website.\n\n[High Impact Medicine](https://forum.effectivealtruism.org/users/high-impact-medicine). [Effective Altruism Forum](https://forum.effectivealtruism.org/topics/effective-altruism-forum-1) account.\n\nRelated entries\n---------------\n\n[High Impact Athletes](https://forum.effectivealtruism.org/tag/high-impact-athletes) | [Raising for Effective Giving](https://forum.effectivealtruism.org/tag/raising-for-effective-giving)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "Nru7n4CCPpChDqB7K",
    "name": "Medicine",
    "core": false,
    "slug": "medicine",
    "oldSlugs": null,
    "postCount": 9,
    "description": {
      "markdown": "Further reading\n---------------\n\nCarey, Ryan (2014) [Increasing your earnings as a doctor](https://80000hours.org/2014/06/increasing-your-earnings-as-a-doctor/), *80,000 Hours*, June 17.\n\nLewis, Gregory (2015) [Medical careers](https://80000hours.org/career-reviews/medical-careers/), *80,000 Hours*, July.\n\nRelated entries\n---------------\n\n[bioethics](https://forum.effectivealtruism.org/tag/bioethics) | [life sciences](https://forum.effectivealtruism.org/tag/life-sciences)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "iPdzz64knZ3KbYaRs",
    "name": "Teaching materials",
    "core": false,
    "slug": "teaching-materials",
    "oldSlugs": null,
    "postCount": 20,
    "description": {
      "markdown": "**Teaching materials** include syllabi and other course contents related to effective altruism.\n\nFurther reading\n---------------\n\nStafforini, Pablo (2021) [Courses on longtermism](https://www.stafforini.com/blog/), *Pablo’s Miscellany*, June 25 (updated 4 December 2021).\n\nWise, Julia (2018) [EA syllabi and teaching materials](https://forum.effectivealtruism.org/posts/Y8mBXCKmkS9eBokhG/ea-syllabi-and-teaching-materials), *Effective Altruism Forum*, September 5.\n\nRelated entries\n---------------\n\n[effective altruism education](https://forum.effectivealtruism.org/tag/effective-altruism-education) | [effective altruism outreach in schools](https://forum.effectivealtruism.org/tag/effective-altruism-outreach-in-schools)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "fa87M47hGk973eT7u",
    "name": "Forum Review",
    "core": false,
    "slug": "forum-review",
    "oldSlugs": null,
    "postCount": 5,
    "description": {
      "markdown": "Use the **Forum Review** tag for posts related to our Forum Review events (e.g. the [Decade Review](https://forum.effectivealtruism.org/posts/jB7Ten8qmDszRMTho/forum-review-the-best-of-ea-2011-2020))."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "2c65rs2sJFSzBLAvG",
    "name": "Effective Altruism Behavioral Science Newsletter",
    "core": false,
    "slug": "effective-altruism-behavioral-science-newsletter",
    "oldSlugs": [
      "ea-behavioral-science-newsletter"
    ],
    "postCount": 5,
    "description": {
      "markdown": "The **Effective Altruism Behavioral Science Newsletter** is a [newsletter](https://forum.effectivealtruism.org/tag/newsletters) that curates resources relevant to the effective altruism behavioral research community.\n\nExternal links\n--------------\n\n[Effective Altruism Behavioral Science Newsletter](https://docs.google.com/forms/d/e/1FAIpQLSe5IbNdg7XaZ_w2NEyBNddgK-nlyc4xwMdhkdUQzTl3IFRcGg/viewform). Official website.\n\nRelated entries\n---------------\n\n[Alignment Newsletter](https://forum.effectivealtruism.org/tag/alignment-newsletter) | [Effective Altruism Newsletter](https://forum.effectivealtruism.org/tag/effective-altruism-newsletter) | [Forecasting Newsletter](https://forum.effectivealtruism.org/tag/forecasting-newsletter) | [Future Matters](https://forum.effectivealtruism.org/tag/future-matters) | [newsletters](https://forum.effectivealtruism.org/tag/newsletters) | [Ready Research](https://forum.effectivealtruism.org/topics/ready-research)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "eWH3tWCBbTnABLpxq",
    "name": "Review crosspost",
    "core": false,
    "slug": "review-crosspost",
    "oldSlugs": null,
    "postCount": 22,
    "description": {
      "markdown": "Use the **review crosspost** tag for posts that you cross-posted in order to make them eligible for the [Decade Review](https://forum.effectivealtruism.org/posts/jB7Ten8qmDszRMTho/effective-altruism-the-first-decade-forum-review)."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "44v5xESuEnBdYTqJw",
    "name": "Shrimp Welfare Project",
    "core": false,
    "slug": "shrimp-welfare-project",
    "oldSlugs": null,
    "postCount": 3,
    "description": {
      "markdown": "**Shrimp Welfare Project** (**SWP**) is an organization focused on improving farmed shrimp welfare in Southeast Asia.\n\nHistory\n-------\n\nSWP was launched in September 2021, with a seed grant from [Charity Entrepreneurship](https://forum.effectivealtruism.org/tag/charity-entrepreneurship)'s incubation program.^[\\[1\\]](#fn0hng9ygkgvpf)^\n\nFurther reading\n---------------\n\nBoddy, Aaron & Andrés Jiménez (2021) [Introducing Shrimp Welfare Project](https://forum.effectivealtruism.org/posts/z79ycP5jCDks4LPxA/introducing-shrimp-welfare-project), *Effective Altruism Forum*, November 30.\n\nWiblin, Robert & Keiran Harris (2022) [Andrés Jiménez Zorrilla on the Shrimp Welfare Project](https://80000hours.org/after-hours-podcast/episodes/andres-jimenez-zorrilla-shrimp-welfare-project/), *80k After Hours Podcast*, September 5.\n\nExternal links\n--------------\n\n[Shrimp Welfare Project](https://www.shrimpwelfareproject.org/). Official website.\n\n[Apply for a job](https://www.shrimpwelfareproject.org/work-with-us).\n\nRelated entries\n---------------\n\n[Crustacean Compassion](https://forum.effectivealtruism.org/tag/crustacean-compassion) | [crustacean welfare](https://forum.effectivealtruism.org/tag/crustacean-welfare) | [farmed animal welfare](https://forum.effectivealtruism.org/tag/farmed-animal-welfare)\n\n1.  ^**[^](#fnref0hng9ygkgvpf)**^\n    \n    Boddy, Aaron & Andrés Jiménez (2021) [Introducing Shrimp Welfare Project](https://forum.effectivealtruism.org/posts/z79ycP5jCDks4LPxA/introducing-shrimp-welfare-project), *Effective Altruism Forum*, November 30."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "nFqbcCDWXXjASxD9R",
    "name": "Meditation",
    "core": false,
    "slug": "meditation",
    "oldSlugs": null,
    "postCount": 7,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "zcZebXAHGfKt5JhR9",
    "name": "Effective Altruism Newsletter",
    "core": false,
    "slug": "effective-altruism-newsletter",
    "oldSlugs": null,
    "postCount": 15,
    "description": {
      "markdown": "The **Effective Altruism Newsletter** is a monthly [newsletter](https://forum.effectivealtruism.org/tag/newsletters) that provides news and updates about the [effective altruism](https://forum.effectivealtruism.org/tag/effective-altruism) community.\n\nThe Effective Altruism Newsletter is currently published by the [Centre for Effective Altruism](https://forum.effectivealtruism.org/tag/centre-for-effective-altruism-1). In the past, it received support from [Rethink Charity](https://forum.effectivealtruism.org/tag/rethink-charity) and the [Effective Altruism Hub](https://forum.effectivealtruism.org/tag/effective-altruism-hub).\n\nExternal links\n--------------\n\n[Effective Altruism Newsletter](https://www.effectivealtruism.org/ea-newsletter-archives/). Archive of all past newsletters.\n\nRelated entries\n---------------\n\n[Alignment Newsletter](https://forum.effectivealtruism.org/tag/alignment-newsletter) | [EA Organization Updates](https://forum.effectivealtruism.org/tag/ea-organization-updates-monthly-series) | [Forecasting Newsletter](https://forum.effectivealtruism.org/tag/forecasting-newsletter) | [Future Matters](https://forum.effectivealtruism.org/tag/future-matters) | [Monthly Overload of Effective Altruism](https://forum.effectivealtruism.org/topics/monthly-overload-of-effective-altruism) | [newsletters](https://forum.effectivealtruism.org/tag/newsletters)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "n5Cav8npKxzqKyduj",
    "name": "EA Organization Updates (monthly series)",
    "core": false,
    "slug": "ea-organization-updates-monthly-series",
    "oldSlugs": [
      "ea-organization-updates",
      "ea-organization-updates-monthly-series",
      "ea-organization-updates"
    ],
    "postCount": 37,
    "description": {
      "markdown": "*This entry is for the monthly post series. For posts with updates about effective altruism organizations, see* [*organization updates*](https://forum.effectivealtruism.org/topics/organization-updates)*.*\n\n**EA Organization Updates** is a monthly post series containing updates about organizations within the effective altruism community.\n\nHistory\n-------\n\nEA Organization Updates originated as the \"Updates\" section of the [Effective Altruism Newsletter](https://forum.effectivealtruism.org/tag/effective-altruism-newsletter). Starting in July 2019, the section was moved out of the Newsletter and published separately on the [Effective Altruism Forum](https://forum.effectivealtruism.org/tag/effective-altruism-forum-1).^[\\[1\\]](#fnu5n4dpya3w9)^\n\nRelated entries\n---------------\n\n[Effective Altruism Newsletter](https://forum.effectivealtruism.org/tag/effective-altruism-newsletter) | [Monthly Overload of Effective Altruism](https://forum.effectivealtruism.org/topics/monthly-overload-of-effective-altruism) | [newsletters](https://forum.effectivealtruism.org/tag/newsletters) | [organization updates](https://forum.effectivealtruism.org/topics/organization-updates)\n\n1.  ^**[^](#fnrefu5n4dpya3w9)**^\n    \n    Gertler, Aaron (2019) [EA Organization Updates: July 2019](https://forum.effectivealtruism.org/posts/3RpCB4rhpPNoCH7mL/ea-organization-updates-july-2019), *Effective Altruism Forum*, August 7."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "hvSAuXockimYDiAx2",
    "name": "Monthly Overload of Effective Altruism",
    "core": false,
    "slug": "monthly-overload-of-effective-altruism",
    "oldSlugs": [
      "ea-updates",
      "ea-updates-monthly-series"
    ],
    "postCount": 49,
    "description": {
      "markdown": "**Monthly Overload of Effective Altruism** (previously **EA Updates**) is a monthly [newsletter](https://forum.effectivealtruism.org/topics/newsletters) about research and events in the [effective altruism](https://forum.effectivealtruism.org/topics/effective-altruism) community, published by David Nash from [Effective Altruism London](https://forum.effectivealtruism.org/tag/effective-altruism-london).\n\nExternal links\n--------------\n\n[Monthly Overload of Effective Altruism](https://moea.substack.com). Substack website.\n\nRelated entries\n---------------\n\n[EA Organization Updates](https://forum.effectivealtruism.org/tag/ea-organization-updates-monthly-series) | [Effective Altruism London](https://forum.effectivealtruism.org/tag/effective-altruism-london) | [Effective Altruism Newsletter](https://forum.effectivealtruism.org/tag/effective-altruism-newsletter) | [newsletters](https://forum.effectivealtruism.org/tag/newsletters) | [organization updates](https://forum.effectivealtruism.org/topics/organization-updates)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "4zqqjcphDToti9WRF",
    "name": "Brain-computer interfaces",
    "core": false,
    "slug": "brain-computer-interfaces",
    "oldSlugs": null,
    "postCount": 1,
    "description": {
      "markdown": "**Brain-computer interfaces** (**BCIs**) are technologies that allow the brain to interface directly with an external computing or robotic device.\n\nIt has been argued that BCIs could make [robust totalitarianism](https://forum.effectivealtruism.org/tag/totalitarianism) significantly more likely, and that such devices should for that reason be regarded as an [existential risk factor](https://forum.effectivealtruism.org/tag/existential-risk-factor).^[\\[1\\]](#fnw68cbexbfm)^\n\nFurther reading\n---------------\n\nHill, N. J. & J. R. Wolpaw (2016) [Brain–computer interface](https://doi.org/10.1016/B978-0-12-801238-3.99322-X), in *Reference Module in Biomedical Sciences*, Amsterdam: Elsevier.\n\nHannas, William *et al.* (2020) [China AI-brain research: brain-inspired AI, connectomics, brain-computer interfaces](https://cset.georgetown.edu/publication/china-ai-brain-research/), Center for Security and Emerging Technology.\n\nLessWrong (2021) [Brain-computer interfaces](https://www.lesswrong.com/tag/brain-computer-interfaces), *LessWrong Wiki*.\n\nTullis, Paul (2020) [The brain-computer interface is coming, and we are so not ready for it](https://thebulletin.org/2020/09/the-brain-computer-interface-is-coming-and-we-are-so-not-ready-for-it/), *Bulletin of the Atomic Scientists*, September 15.\n\n1.  ^**[^](#fnrefw68cbexbfm)**^\n    \n    Rafferty, Jack (2020) [A new x-risk factor: brain-computer interfaces](https://forum.effectivealtruism.org/posts/qfDeCGxBTFhJANAWm/a-new-x-risk-factor-brain-computer-interfaces-1), *Effective Altruism Forum*, August 10."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "vFsKuY4Adb6sYETdd",
    "name": "Compassion in World Farming",
    "core": false,
    "slug": "compassion-in-world-farming",
    "oldSlugs": null,
    "postCount": 7,
    "description": {
      "markdown": "**Compassion in World Farming** (**CIWF**) is an organization that campaigns to end [factory farming](https://forum.effectivealtruism.org/tag/farmed-animal-welfare).\n\nActivities\n----------\n\nCIWF activities include engaging in political campaigning to encode animal welfare protections into law; working with companies to improve welfare standards for farmed animals; and engaging in outreach efforts to reduce consumption of animal products.\n\nEvaluation\n----------\n\nMost evaluations of CIWF have focused on **Compassion in World Farming USA** (**CIWF USA**, or **Compassion USA**), the US branch of CIWF.\n\n[Animal Charity Evaluators](https://forum.effectivealtruism.org/tag/animal-charity-evaluators) considers CIFW USA \"an excellent giving opportunity because of their strong programs focused on improving the welfare of animals in the U.S.\"^[\\[1\\]](#fngo4u2y1lpif)^ CIFW USA is also recommended by [Founders Pledge](https://forum.effectivealtruism.org/tag/founders-pledge) as \"one of the most cost-effective animal advocacy organisations in the world.\"^[\\[2\\]](#fnrnth44xsep)^\n\nAs of August 2022, [Open Philanthropy](https://forum.effectivealtruism.org/tag/open-philanthropy) has granted over $14.7 million to CIFW^[\\[3\\]](#fngpq2szytlbf)^ and over $3.2 million to CIFW USA.^[\\[4\\]](#fnrjnt2tvjffq)^ Both organizations have also received comparatively small donations from [Effective Altruism Funds](https://forum.effectivealtruism.org/topics/effective-altruism-funds).^[\\[5\\]](#fni3p7ygivtpr)^^[\\[6\\]](#fn91yennwqub)^^[\\[7\\]](#fn9igshmo0ner)^\n\nFurther reading\n---------------\n\nAnimal Charity Evaluators (2021) [Compassion In World Farming USA](https://animalcharityevaluators.org/charity-review/compassion-in-world-farming-usa-ciwf/), *Animal Charity Evaluators*, November.\n\nClare, Stephen (2020) [Compassion in World Farming USA](https://founderspledge.com/stories/compassion-in-world-farming-usa-high-impact-funding-opportunity), *Founders Pledge*, November 3.\n\nExternal links\n--------------\n\n[Compassion in World Farming](https://www.ciwf.org.uk/). Official website.\n\n[Apply for a job](https://www.ciwf.org.uk/get-involved/jobs-and-volunteering/).\n\n[Donate to Compassion in World Farming](https://donate.ciwf.org.uk/page/81196/donate/1?supporter.appealCode=LAWE_UK1018c).\n\n1.  ^**[^](#fnrefgo4u2y1lpif)**^\n    \n    Animal Charity Evaluators (2021) [Compassion In World Farming USA](https://animalcharityevaluators.org/charity-review/compassion-in-world-farming-usa-ciwf/), *Animal Charity Evaluators*, November.\n    \n2.  ^**[^](#fnrefrnth44xsep)**^\n    \n    Clare, Stephen (2020) [Compassion in World Farming USA](https://founderspledge.com/stories/compassion-in-world-farming-usa-high-impact-funding-opportunity), *Founders Pledge*, November 3.\n    \n3.  ^**[^](#fnrefgpq2szytlbf)**^\n    \n    Open Philanthropy (2022) [Grants database: Compassion in World Farming](https://www.openphilanthropy.org/grants/?q=&organization-name=compassion-in-world-farming), *Open Philanthropy*.\n    \n4.  ^**[^](#fnrefrjnt2tvjffq)**^\n    \n    Open Philanthropy (2022) [Grants database: Compassion in World Farming USA](https://www.openphilanthropy.org/grants/?q=&organization-name=compassion-in-world-farming-usa), *Open Philanthropy*.\n    \n5.  ^**[^](#fnrefi3p7ygivtpr)**^\n    \n    Animal Welfare Fund (2017) [November 2017: Animal Welfare Fund grants](https://funds.effectivealtruism.org/funds/payouts/november-2017-animal-welfare-fund-grants), *Effective Altruism Funds*.\n    \n6.  ^**[^](#fnref91yennwqub)**^\n    \n    Animal Welfare Fund (2017) [April 2017: Animal Welfare Fund grants](https://funds.effectivealtruism.org/funds/payouts/april-2017-animal-welfare-fund-grants), *Effective Altruism Funds*, April.\n    \n7.  ^**[^](#fnref9igshmo0ner)**^\n    \n    Animal Welfare Fund (2019) [July 2019: Animal Welfare Fund grants](https://funds.effectivealtruism.org/funds/payouts/july-2019-animal-welfare-fund-grants), *Effective Altruism Funds*, July."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "v3SyquD5Nky4Ednnv",
    "name": "Meme",
    "core": false,
    "slug": "meme",
    "oldSlugs": null,
    "postCount": 7,
    "description": {
      "markdown": "Images or short humorous snippets meant to convey a single EA idea."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "rSF94ws2yqcpecHNn",
    "name": "Vaccines",
    "core": false,
    "slug": "vaccines",
    "oldSlugs": null,
    "postCount": 33,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "kBLnKWiJnsbCtWzu9",
    "name": "Tuberculosis",
    "core": false,
    "slug": "tuberculosis",
    "oldSlugs": null,
    "postCount": 1,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "SPiJDaKCYaEt6HNTA",
    "name": "HIV/AIDS",
    "core": false,
    "slug": "hiv-aids",
    "oldSlugs": null,
    "postCount": 3,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "Gux6FLJKJkXquEzoa",
    "name": "Just Impact",
    "core": false,
    "slug": "just-impact",
    "oldSlugs": null,
    "postCount": 2,
    "description": {
      "markdown": "**Just Impact** is \"a [criminal justice reform](https://forum.effectivealtruism.org/tag/criminal-justice-reform) advisory group and fund that is focused on building the power and influence of highly strategic, directly-impacted leaders and their allies to create [transformative change](https://forum.effectivealtruism.org/tag/systemic-change) from the ground up.\"^[\\[1\\]](#fn8kuq3ciw088)^ \n\nHistory\n-------\n\nJust Impact was spun out of [Open Philanthropy](https://forum.effectivealtruism.org/tag/open-philanthropy) in December 2021, when it received a grant of $50 million to support work for the next 3.5 years.^[\\[1\\]](#fn8kuq3ciw088)^^[\\[2\\]](#fn2k9som0mvy6)^\n\nFurther reading\n---------------\n\nRobinson, Zachary & Alexander Berger (2021) [Our criminal justice reform program is now an independent organization: Just Impact](https://www.openphilanthropy.org/blog/our-criminal-justice-reform-program-now-independent-organization-just-impact), *Open Philanthropy*, November 16.\n\nExternal links\n--------------\n\n[Just Impact](https://justimpactadvisors.org/). Official website\n\nRelated entries\n---------------\n\n[criminal justice reform](https://forum.effectivealtruism.org/tag/criminal-justice-reform) | [Open Philanthropy](https://forum.effectivealtruism.org/tag/open-philanthropy)\n\n1.  ^**[^](#fnref8kuq3ciw088)**^\n    \n    Robinson, Zachary & Alexander Berger (2021) [Our criminal justice reform program is now an independent organization: Just Impact](https://www.openphilanthropy.org/blog/our-criminal-justice-reform-program-now-independent-organization-just-impact), *Open Philanthropy*, November 16.\n    \n2.  ^**[^](#fnref2k9som0mvy6)**^\n    \n    Open Philanthropy (2022) [Just Impact — Safely reducing incarceration](https://www.openphilanthropy.org/grants/just-impact-safely-reducing-incarceration/), *Open Philanthropy*."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "HbeGXnCbY34JjF8aq",
    "name": "Crustacean welfare",
    "core": false,
    "slug": "crustacean-welfare",
    "oldSlugs": null,
    "postCount": 5,
    "description": {
      "markdown": "**Crustacean welfare** is the study of interventions aimed at improving the welfare of crustaceans.\n\nFurther reading\n---------------\n\nBirch, Jonathan *et al.* (2021) [Review of the evidence of sentience in cephalopod molluscs and decapod crustaceans](https://www.lse.ac.uk/News/News-Assets/PDFs/2021/Sentience-in-Cephalopod-Molluscs-and-Decapod-Crustaceans-Final-Report-November-2021.pdf), London School of Economics.\n\nConte, Francesca *et al.* (2021) [Humane slaughter of edible decapod crustaceans](https://doi.org/10.3390/ani11041089), *Animals*, vol. 11, p. 1089.\n\nRelated entries\n---------------\n\n[Crustacean Compassion](https://forum.effectivealtruism.org/tag/crustacean-compassion) | [fish welfare](https://forum.effectivealtruism.org/tag/fish-welfare) | [invertebrate welfare](https://forum.effectivealtruism.org/tag/invertebrate-welfare) | [wild animal welfare](https://forum.effectivealtruism.org/tag/wild-animal-welfare)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "PP9hnptiErAHmPEKL",
    "name": "AI race",
    "core": false,
    "slug": "ai-race",
    "oldSlugs": null,
    "postCount": 9,
    "description": {
      "markdown": "An **AI race** is a competition between rival teams to first develop advanced [artificial intelligence](https://forum.effectivealtruism.org/tag/artificial-intelligence).\n\nTerminology\n-----------\n\nThe expression \"AI race\" may be used in a somewhat narrower sense to describe a competition to attain military superiority via AI. The expressions **AI arms race**,^[\\[1\\]](#fnp4glye3l62)^ **arms race for AI**,^[\\[2\\]](#fnaqtrxmgy4vk)^ and **military AI arms race**^[\\[3\\]](#fnwfngajtf3k)^ are sometimes employed to refer to this specific type of AI race.\n\nAI races and technological races\n--------------------------------\n\nAn AI race is an example of the broader phenomenon of a [technology race](https://forum.effectivealtruism.org/topics/technology-race), characterized by a \"winner-take-all\" structure where the team that first develops the technology gets all (or most) of its benefits. This could happen because of various types of feedback loops that magnify the associated benefits. In the case of AI, it is generally believed that these benefits are very large, perhaps sufficient to confer the winning team a [decisive strategic advantage](https://forum.effectivealtruism.org/tag/decisive-strategic-advantage).\n\nSignificance of AI races\n------------------------\n\nAI races are significant primarily because of their effects on [AI risk](https://forum.effectivealtruism.org/tag/ai-risk): a team can plausibly improve its chances of winning the race by relaxing safety precautions, and the payoffs from winning the race are great enough to provide strong incentives for that relaxation. In addition, a race that unfolds between national governments—rather than between private firms—could increase global instability and make [great power conflicts](https://forum.effectivealtruism.org/tag/great-power-conflict) more probable.\n\nA model of AI races\n-------------------\n\nStuart Armstrong, [Nick Bostrom](https://forum.effectivealtruism.org/tag/nick-bostrom) and [Carl Shulman](https://forum.effectivealtruism.org/tag/carl-shulman) have developed a model of AI races.^[\\[4\\]](#fnvdnxbct8u9)^  (Although the model is focused on artificial intelligence, it is applicable to any technology where the first team to develop it gets a disproportionate share of its benefits and each team can speed up its development by relaxing the safety precautions needed to reduce the dangers associated with the technology.)\n\nThe model involves *n* different teams racing to first build AI. Each team has a given AI-building capability *c*, as well as a chosen AI safety level *s* ranging from 0 (no precautions) to 1 (maximum precaution). The team for which *c* – *s* is highest wins the race, and the probability of AI disaster is 1 – *s*.\n\nUtility is normalized so that, for each team, 0 utility corresponds to an AI disaster and 1 corresponds to winning the AI race. In addition, each team has a degree of enmity *e* towards the other teams, ranging from 0 to 1, such that it gets utility 1 – *e* if another team wins the race. The model assumes a constant value of *e* for all teams.\n\nEach team's capability is drawn randomly from a uniform distribution ranging over the interval \\[0, *μ*\\], for a single given *μ*, with lower values representing lower capability.\n\nFrom this model, a number of implications follow:\n\n*   As *μ* increases, capability becomes increasingly important relative to safety in determining the outcome of the race, and teams become correspondingly less inclined to skimp on safety precautions. Conversely, lower values of *μ* are associated with fewer precautions; at the limiting case of *μ* = 0, teams will take no precautions at all.\n*   As enmity increases, the cost to each team of losing the race increases, and so teams become more inclined to skimp on safety precautions. But whereas the relative importance of capability is largely determined by technology, and is as such mostly intractable, there are various interventions reasonably expected to decrease enmity, such as \"building trust between nations and groups, sharing technologies or discoveries, merging into joint projects or agreeing to common aims.\"^[\\[5\\]](#fnm9lwu2dikao)^\n*   A less intuitive finding of the model concerns how capability and enmity relate to scenarios involving (1) no information; (2) private information (each teams knows its own capability); and (3) public information (each team knows the capability of every team). No information is always safer than either private or private information. But while public information can decrease risk, relative to private information, when both capability and enmity are low, the reverse is the case for sufficiently high levels of capability or enmity.\n*   Another surprising finding concerns the impact of the number of teams under different informational scenarios. When there is either no information or public information, risk strictly increases with the number of teams. But although this effect is also observed for private information when capability is low, as capability grows the effect eventually reverses.\n\nAI races and information hazards\n--------------------------------\n\nAI races are sometimes cited as an example of an [information hazard](https://forum.effectivealtruism.org/tag/information-hazard), i.e. a risk arising from the spread of true information. There are in fact a number of different hazards associated with AI races. One such hazard is the risk identified by Armstrong, Bostrom and Shulman: moving from a situation of no information to one of either private or public information increases risks. Another, more subtle information hazard concerns the sharing of information about the model itself: widespread awareness that no information is safer might encourage teams to adopt a culture of secrecy which might constitute an impediment to the building of trust among rival teams.^[\\[6\\]](#fnej3p2lxsept)^ More generally, the framing of AI development as involving a winner-take-all dynamic in discussions by public leaders and intellectuals may itself be considered hazardous, insofar as it is likely to hinder cooperation and exacerbate conflict.^[\\[7\\]](#fntxtwxnw12g)^^[\\[8\\]](#fnqygd046ptql)^^[\\[9\\]](#fnb1f41j0k15g)^\n\nFurther reading\n---------------\n\nArmstrong, Stuart, Nick Bostrom & Carl Shulman (2016) [Racing to the precipice: a model of artificial intelligence development](https://doi.org/10.1007/s00146-015-0590-y), *AI and Society*, vol. 31, pp. 201–206.\n\nRelated entries\n---------------\n\n[AI risk](https://forum.effectivealtruism.org/tag/ai-risk) | [artificial intelligence](https://forum.effectivealtruism.org/tag/artificial-intelligence) | [great power conflict](https://forum.effectivealtruism.org/tag/great-power-conflict) | [technology race](https://forum.effectivealtruism.org/topics/technology-race)\n\n1.  ^**[^](#fnrefp4glye3l62)**^\n    \n    Barnes, Julian E. & Josh Chin (2018) [The new arms race in AI](https://www.wsj.com/articles/the-new-arms-race-in-ai-1520009261), *Wall Street Journal*, March 2.\n    \n2.  ^**[^](#fnrefaqtrxmgy4vk)**^\n    \n    Fedasiuk, Ryan (2021) [We spent a year investigating what the Chinese army is buying. Here’s what we learned](https://www.politico.com/news/magazine/2021/11/10/chinese-army-ai-defense-contracts-520445), *Politico*, November 10.\n    \n3.  ^**[^](#fnrefwfngajtf3k)**^\n    \n    Cave, Stephen & Seán ÓhÉigeartaigh (2018) [An AI race for strategic advantage: Rhetoric and risks](https://doi.org/10.1145/3278721.3278780), in Jason Furman *et al.* (eds.) *Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society*, New York: Association for Computing Machinery, pp. 36–40, p. 37.\n    \n4.  ^**[^](#fnrefvdnxbct8u9)**^\n    \n    Armstrong, Stuart, Nick Bostrom & Carl Shulman (2016) [Racing to the precipice: a model of artificial intelligence development](https://doi.org/10.1007/s00146-015-0590-y), *AI and Society*, vol. 31, pp. 201–206.\n    \n5.  ^**[^](#fnrefm9lwu2dikao)**^\n    \n    Armstrong, Bostrom & Shulman, [Racing to the precipice](https://doi.org/10.1007/s00146-015-0590-y), p. 204.\n    \n6.  ^**[^](#fnrefej3p2lxsept)**^\n    \n    Armstrong, Bostrom & Shulman, [Racing to the precipice](https://doi.org/10.1007/s00146-015-0590-y), p. 205, fn. 7.\n    \n7.  ^**[^](#fnreftxtwxnw12g)**^\n    \n    Tomasik, Brian (2013) [International cooperation vs. AI arms race](https://longtermrisk.org/international-cooperation-vs-ai-arms-race/), *Center on Long-Term Risk*, December 5 (updated 29 February 2016).\n    \n8.  ^**[^](#fnrefqygd046ptql)**^\n    \n    Baum, Seth D. (2017) [On the promotion of safe and socially beneficial artificial intelligence](https://doi.org/10.1007/s00146-016-0677-0), *AI and Society*, vol. 32, pp. 543–551.\n    \n9.  ^**[^](#fnrefb1f41j0k15g)**^\n    \n    Cave, Stephen & Seán ÓhÉigeartaigh,  [An AI race for strategic advantage](https://doi.org/10.1145/3278721.3278780)."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "vfZK5YCYFZwHFk4qq",
    "name": "Resource tradeoffs",
    "core": false,
    "slug": "resource-tradeoffs",
    "oldSlugs": [
      "time-money-tradeoffs"
    ],
    "postCount": 8,
    "description": {
      "markdown": "**Resource tradeoffs** are exchanges between resources, such as money, time, and energy.\n\nMost people trade significant amounts of their time for money on a daily basis (e.g., working to earn income), and conversely money can be spent to free one's time (e.g., paying someone else to spend their time on a task). To make effective time-money tradeoffs, it's important to be able to estimate the value of a marginal hour of one's time.\n\nFurther reading\n---------------\n\nBye, Lynette (2019) [How much is your time worth?](https://www.lesswrong.com/posts/y5RoNDPcfJqm3vfQA/how-much-is-your-time-worth), *LessWrong*, September 2. \n\nCotton-Barratt, Owen (2015) [Neutral hours: a tool for valuing time and energy](http://globalprioritiesproject.org/wp-content/uploads/2015/03/NeutralHours.pdf), *Global Priorities Project*, March."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "vChjSEoothYNxh9Xd",
    "name": "North Korea",
    "core": false,
    "slug": "north-korea",
    "oldSlugs": null,
    "postCount": 7,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "oXxa8GBoWhi299LXD",
    "name": "Effective Altruism for Jews",
    "core": false,
    "slug": "effective-altruism-for-jews",
    "oldSlugs": null,
    "postCount": 6,
    "description": {
      "markdown": "**Effective altruism (EA) for Jews** is a group of Jews of all backgrounds and denominations, secular and religious, committed to sharing the ideas of [effective altruism](https://forum.effectivealtruism.org/tag/effective-altruism) in the Jewish community and building a community of Jewish effective altruists.\n\nFurther reading\n---------------\n\nManheim, David (2022) [Ancient and modern conceptions of charity: orthodox Judaism and effective altruism](https://doi.org/10.5771/9783748925361), in Dominic Roser, Stefan Riedener & Markus Huppenbauer (eds.) *Effective Altruism and Religion: Synergies, Tensions, Dialogue*, Baden-Baden: Nomos, pp. 77–95.\n\nExternal links\n--------------\n\n[Effective altruism for Jews: official website](https://eaforjews.org/).  \n[Effective altruism for Jews: facebook group](https://www.facebook.com/groups/eaforjews).  \n[Effective altruism for Jews: facebook page](https://www.facebook.com/eaforjews).\n\nRelated entries\n---------------\n\n[religion](https://forum.effectivealtruism.org/tag/religion)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "vRGcwRw8pWjSozAbJ",
    "name": "College advice",
    "core": false,
    "slug": "college-advice",
    "oldSlugs": [
      "for-students-college-university"
    ],
    "postCount": 21,
    "description": {
      "markdown": "College [career advice](https://forum.effectivealtruism.org/tag/career-advising) and other advice regarding decisions relating to undergraduate studies.\n\nRelated entries\n---------------\n\n[academia](https://forum.effectivealtruism.org/tag/academia-1) | [career choice](https://forum.effectivealtruism.org/tag/career-choice) | [practical](https://forum.effectivealtruism.org/topics/practical) | [research careers](https://forum.effectivealtruism.org/tag/research-careers)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "xoZfSjT2oXLYnDfmN",
    "name": "FTX Foundation",
    "core": false,
    "slug": "ftx-foundation",
    "oldSlugs": null,
    "postCount": 16,
    "description": {
      "markdown": "The **FTX Foundation** is a non-profit organization funded by FTX, a cryptocurrency derivatives exchange founded by [Sam Bankman-Fried](https://forum.effectivealtruism.org/tag/sam-bankman-fried).\n\nProjects\n--------\n\nIn July 2021, FTX Foundation launched **FTX Climate**, an initiative to make FTX carbon-neutral by the end of 2021; to fund promising research on [climate change](https://forum.effectivealtruism.org/tag/climate-change); and to support various other climate-related projects.^[\\[1\\]](#fnb6hvylgw564)^^[\\[2\\]](#fnduic5d1arxa)^ In late October 2021, FTX Foundation announced a [fellowship program](https://forum.effectivealtruism.org/tag/fellowships-and-internships) to support individuals working on high-impact projects and to [build an effective altruism community](https://forum.effectivealtruism.org/tag/building-effective-altruism-1) in the Bahamas, where FTX is headquartered.^[\\[3\\]](#fnwlhd5mtszwd)^\n\nThe [Future Fund](https://forum.effectivealtruism.org/tag/future-fund), a philanthropic fund that makes grants and investments to ambitious projects focused on improving humanity's long-term prospects, was launched in late February 2022. That month, the FTX Foundation also launched **FTX Community**, focused on [global poverty](https://forum.effectivealtruism.org/tag/global-poverty), [animal welfare](https://forum.effectivealtruism.org/tag/animal-welfare-1), and [community outreach](https://forum.effectivealtruism.org/tag/global-outreach).\n\nFurther reading\n---------------\n\nBankman-Fried, Sam (2021) [The FTX Foundation for charitable giving](https://blog.ftx.com/blog/ftx-foundation/), *FTX Research*, February 8.\n\nExternal links\n--------------\n\n[FTX Foundation](https://ftxfoundation.org/). Official website.\n\n[FTX Climate](https://www.ftx-climate.com/). Official website.\n\n[FTX Future Fund](https://ftxfuturefund.org/). Official website.\n\nRelated entries\n---------------\n\n[Future Fund](https://forum.effectivealtruism.org/tag/future-fund) | [grantmaking](https://forum.effectivealtruism.org/tag/grantmaking) | [Sam Bankman-Fried](https://forum.effectivealtruism.org/tag/sam-bankman-fried)\n\n1.  ^**[^](#fnrefb6hvylgw564)**^\n    \n    Bankman-Fried, Sam (2021) [The FTX Foundation for charitable giving](https://blog.ftx.com/blog/ftx-foundation/), *FTX Research*, February 8.\n    \n2.  ^**[^](#fnrefduic5d1arxa)**^\n    \n    PRNewswire (2021) [The FTX Foundation Group launches the FTX Climate program](https://www.prnewswire.com/news-releases/the-ftx-foundation-group-launches-the-ftx-climate-program-301342380.html), *PRNewswire*, July 27.\n    \n3.  ^**[^](#fnrefwlhd5mtszwd)**^\n    \n    FTX Foundation (2021) [FTX EA Fellowships](https://forum.effectivealtruism.org/posts/sdjcH7KAxgB328RAb/ftx-ea-fellowships), *Effective Altruism Forum*, October 25."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "ebw3NkFduzdugvMb4",
    "name": "Psychotherapy",
    "core": false,
    "slug": "psychotherapy",
    "oldSlugs": null,
    "postCount": 4,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "9hCXdw2Ges3DtKMrG",
    "name": "StrongMinds",
    "core": false,
    "slug": "strongminds",
    "oldSlugs": null,
    "postCount": 3,
    "description": {
      "markdown": "**StrongMinds** is a non-profit that treats depression in women and adolescents in low-income communities in sub-Saharan Africa.\n\nEvaluation\n----------\n\nIn 2021, the [Happier Lives Institute](https://forum.effectivealtruism.org/tag/happier-lives-institute) published an analysis of StrongMinds, estimating that the organization was 12 times more cost-effective than [GiveDirectly](https://forum.effectivealtruism.org/tag/givedirectly), as measured by [subjective wellbeing](https://forum.effectivealtruism.org/tag/subjective-wellbeing).^[\\[1\\]](#fnkqefquybbu)^ In a follow-up, prompted by criticism from [Alexander Berger](https://forum.effectivealtruism.org/tag/alexander-berger), ^[\\[2\\]](#fnm58apu7am8d)^ HLI revised the figure, estimating that Strong Minds was 9 times more cost-effective than GiveDirectly.^[\\[3\\]](#fne9f4obzjaqt)^\n\nExternal links\n--------------\n\n[StrongMinds](https://strongminds.org/). Official website.\n\n[Apply for a job](https://strongminds.org/strongminds-history/job-vacancies/).\n\nRelated entries\n---------------\n\n[global health and wellbeing](https://forum.effectivealtruism.org/tag/global-health-and-wellbeing) | [mental health](https://forum.effectivealtruism.org/topics/mental-health) | [subjective wellbeing](https://forum.effectivealtruism.org/tag/subjective-wellbeing)\n\n1.  ^**[^](#fnrefkqefquybbu)**^\n    \n    Plant, Michael & Joel McGuire (2021) [Donating money, buying happiness: new meta-analyses comparing the cost-effectiveness of cash transfers and psychotherapy in terms of subjective well-being](https://forum.effectivealtruism.org/posts/mY4pZSwvFCDsjorJX/donating-money-buying-happiness-new-meta-analyses-comparing), *Effective Altruism Forum*, October 25.\n    \n2.  ^**[^](#fnrefm58apu7am8d)**^\n    \n    Berger, Alexander (2021) [Discussion with Joel McGuire and Michael Plant](http://web.archive.org/web/20211028135434/https://twitter.com/albrgr/status/1453721539652718605), *Twitter*, October 26.\n    \n3.  ^**[^](#fnrefe9f4obzjaqt)**^\n    \n    McGuire, Joel & Michael Plant (2022) [Happiness for the whole household: accounting for household spillovers when comparing the cost-effectiveness of psychotherapy to cash transfers](https://forum.effectivealtruism.org/posts/zCD98wpPt3km8aRGo/happiness-for-the-whole-household-accounting-for-household), *Effective Altruism Forum*, April 14."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "sLQYE2fgCLwmdRBFH",
    "name": "Surveys",
    "core": false,
    "slug": "surveys",
    "oldSlugs": null,
    "postCount": 146,
    "description": {
      "markdown": "The **surveys** tag is for posts that discuss results from surveys; promote particular surveys; and/or discuss pros, cons, and best practices for running surveys or making use of their results (either in general or in relation to particular topics, methodologies, etc.).\n\nSurveys of the effective altruism community include the [Effective Altruism Survey](https://forum.effectivealtruism.org/tag/effective-altruism-survey), the [Effective Altruism Group Organisers' Survey](https://forum.effectivealtruism.org/tag/effective-altruism-group-organisers-survey#), the [Leaders Forum survey](https://forum.effectivealtruism.org/tag/leaders-forum#Leaders_Forum_survey).\n\nRelated entries\n---------------\n\n[data (EA community)](https://forum.effectivealtruism.org/tag/data-ea-community-1) | [data science](https://forum.effectivealtruism.org/tag/data-science) | [Effective Altruism Survey](https://forum.effectivealtruism.org/tag/effective-altruism-survey) | [Effective Altruism Group Organisers' Survey](https://forum.effectivealtruism.org/tag/effective-altruism-group-organisers-survey) | [forecasting](https://forum.effectivealtruism.org/tag/forecasting) | [impact assessment](https://forum.effectivealtruism.org/tag/impact-assessment) | [psychology research](https://forum.effectivealtruism.org/tag/psychology-research) | [randomized controlled trials](https://forum.effectivealtruism.org/tag/randomized-controlled-trials-in-social-science) | [research](https://forum.effectivealtruism.org/tag/research) | [research methods](https://forum.effectivealtruism.org/tag/research-methods) | [social science](https://forum.effectivealtruism.org/tag/social-science-1)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "MPa7u5iTd8mszHpwP",
    "name": "Coaching",
    "core": false,
    "slug": "coaching",
    "oldSlugs": null,
    "postCount": 25,
    "description": {
      "markdown": "Further reading\n---------------\n\nWissemann, Anne *et al.* (2018) [Coaches and therapists in the effective altruism & rationality sphere](https://docs.google.com/document/d/1q0NUPXpTOz6xygf4UMT-CsNMC187AHdwAWv55HyBodQ/edit#), unpublished.\n\nRelated entries\n---------------\n\n[career advising](https://forum.effectivealtruism.org/tag/career-advising) | [Effective Altruism Coaching](https://forum.effectivealtruism.org/tag/effective-altruism-coaching) | [effective altruism lifestyle](https://forum.effectivealtruism.org/tag/effective-altruism-lifestyle) | [personal development](https://forum.effectivealtruism.org/tag/personal-development) | [management & mentoring](https://forum.effectivealtruism.org/tag/management-and-mentoring) | [self-care](https://forum.effectivealtruism.org/tag/self-care)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "tpvP2TL2E4dQRQpiJ",
    "name": "Pascal's mugging",
    "core": false,
    "slug": "pascal-s-mugging",
    "oldSlugs": null,
    "postCount": 17,
    "description": {
      "markdown": "**Pascal's mugging** is a thought experiment intended to raise a problem for [expected value](https://forum.effectivealtruism.org/tag/expected-value) theory. Unlike Pascal's wager, Pascal's mugging does not involve infinite utilities or probabilities, so the problem it raises is separate from any of the known paradoxes of [infinity](https://forum.effectivealtruism.org/tag/infinite-ethics).\n\nThe thought experiment\n----------------------\n\nThe thought experiment and its name first appeared in a blog post by [Eliezer Yudkowsky](https://forum.effectivealtruism.org/tag/eliezer-yudkowsky).^[\\[1\\]](#fnyax64nzhlt)^ [Nick Bostrom](https://forum.effectivealtruism.org/tag/nick-bostrom) later elaborated it in the form of a fictional dialogue.^[\\[2\\]](#fnc5ofd0wq9zv)^\n\nIn Yudkowsky's original formulation, a person is approached by a mugger who threatens to kill an astronomical number of people unless the person agrees to give them five dollars. Even a tiny probability assigned to the hypothesis that the mugger will deliver on his promise seems sufficient to make the prospect of giving the mugger five dollars better than the alternative, in expectation. The minuscule chance that the mugger is willing and able to save astronomically many people is more than compensated by the enormous value of what is at stake. (If one thinks the probability too low, the number of lives the mugger threatens to kill could be arbitrarily increased.) The thought experiment supposedly raises a problem for expected value theory because it seems intuitively absurd that we should give money to the mugger, yet this is what the theory apparently implies.\n\nResponses\n---------\n\nA variety of responses have been developed. One common response is to revise or reject expected value theory. A frequent revision is to ignore scenarios whose probability is below a certain threshold.\n\nThis response, however, has a number of problems. One problem is that the threshold seems arbitrary, regardless of where it is set. A critic could always say: \"Why do you set the threshold at that value, rather than e.g. one order of magnitude higher or lower?\" A more fundamental problem is that it seems that whether a scenario falls below or above a certain threshold is contingent on how one carves up the space of possibilities. For example, an existential risk of 1-in-100 per century can be redescribed as an existential risk of 1-in-5.2 billion per minute. If the threshold is set to a value between those two numbers, whether one should or should not ignore the risk will depend merely on how one decides to describe it.\n\nAnother response is to adopt a prior that penalizes hypotheses in proportion to the number of people they imply we can affect. That is, one could adopt a view on which there is roughly a 1 in 10^n^ chance that someone will have the power to affect 10^n^ people. Given this penalty, the mugger can no longer resort to the trick of increasing the number of people they threaten to kill in order to make the offer sufficiently attractive. As the number of people increases, the probability that they will be killed by the mugger decreases correspondingly, and the expected value of their successive proposals remains the same.\n\nA final response is to just \"bite the bullet\" and accept that if the mugger's proposal is better in expectation, one should indeed give them the five dollars. This approach becomes more plausible when combined with a [debunking explanation](https://forum.effectivealtruism.org/tag/debunking-argument) of the intuition that paying the mugger would be absurd. For example, one can argue that human brains cannot adequately represent very large or very small numbers, and that therefore intuitions triggered by thought experiments making use of such quantities are unreliable and should not be given much evidential weight.\n\nImplications\n------------\n\nRegardless of how one responds to Pascal's mugging, it is important to note that it does not appear to affect the value assigned to \"high-stakes\" causes or interventions prioritized within the effective altruism community, such as [AI safety](https://forum.effectivealtruism.org/tag/ai-safety) research or other forms of [existential risk](https://forum.effectivealtruism.org/tag/existential-risk) mitigation. The case for working on these causes is not fundamentally different from more mundane arguments which we do not take to fall under the scope of Pascal's mugging, such as voting in an election.^[\\[3\\]](#fnbioy53c5fjb)^^[\\[4\\]](#fn4mfyng7sa6k)^\n\nIt is also worth stressing that Pascal's mugging involves both very high stakes and very small probabilities, but the term is sometimes incorrectly applied to cases involving high stakes, regardless of their probability.^[\\[5\\]](#fna2fb0yq3iig)^\n\nFurther reading\n---------------\n\nBostrom, Nick (2009) [Pascal’s mugging](https://doi.org/10.1093/analys/anp062), *Analysis*, vol. 69, pp. 443–445.\n\nYudkowsky, Eliezer (2007) [Pascal’s mugging: tiny probabilities of vast utilities](https://www.lesswrong.com/posts/a5JAiTdytou3Jg749/pascal-s-mugging-tiny-probabilities-of-vast-utilities), *LessWrong*, October 19.\n\nRelated entries\n---------------\n\n[alternatives to expected value theory](https://forum.effectivealtruism.org/tag/alternatives-to-expected-value-theory) | [altruistic wager](https://forum.effectivealtruism.org/tag/altruistic-wager) | [decision theory](https://forum.effectivealtruism.org/tag/decision-theory) | [fanaticism](https://forum.effectivealtruism.org/tag/fanaticism) | [risk aversion](https://forum.effectivealtruism.org/tag/risk-aversion)\n\n1.  ^**[^](#fnrefyax64nzhlt)**^\n    \n    Yudkowsky, Eliezer (2007) [Pascal’s mugging: tiny probabilities of vast utilities](https://www.lesswrong.com/posts/a5JAiTdytou3Jg749/pascal-s-mugging-tiny-probabilities-of-vast-utilities), *LessWrong*, October 19.\n    \n2.  ^**[^](#fnrefc5ofd0wq9zv)**^\n    \n    Bostrom, Nick (2009) [Pascal’s mugging](https://doi.org/10.1093/analys/anp062), *Analysis*, vol. 69, pp. 443–445.\n    \n3.  ^**[^](#fnrefbioy53c5fjb)**^\n    \n    Wiblin, Robert (2012) [If elections aren’t a Pascal’s mugging, existential risk shouldn’t be either](https://www.overcomingbias.com/2012/09/if-elections-arent-a-pascals-mugging-existential-risk-shouldnt-be-either.html), *Overcoming Bias*, September 27.\n    \n4.  ^**[^](#fnref4mfyng7sa6k)**^\n    \n    Wiblin, Robert (2015) [Saying ‘AI safety research is a Pascal’s Mugging’ isn’t a strong response](https://forum.effectivealtruism.org/posts/vYb2qEyqv76L62izD/saying-ai-safety-research-is-a-pascal-s-mugging-isn-t-a), *Effective Altruism Forum*, December 15.\n    \n5.  ^**[^](#fnrefa2fb0yq3iig)**^\n    \n    For discussion of a parallel misapplication of Pascal's wager, see Yudkowsky, Eliezer (2009) [The Pascal’s wager fallacy fallacy](https://www.overcomingbias.com/2009/03/pascals-wager-metafallacy.html), *Overcoming Bias*, March 17."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "mKNnvQ3fwMqcGzeXx",
    "name": "Robin Hanson",
    "core": false,
    "slug": "robin-hanson",
    "oldSlugs": null,
    "postCount": 12,
    "description": {
      "markdown": "**Robin Dale Hanson** (born August 28, 1959) is an associate professor of economics at George Mason University and a research associate at the [Future of Humanity Institute](https://forum.effectivealtruism.org/tag/future-of-humanity-institute).\n\nFurther reading\n---------------\n\nBergal, Asya & Robert Long (2019) [Conversation with Robin Hanson](https://aiimpacts.org/conversation-with-robin-hanson/), *AI Impacts*, November 13.\n\nBerger, Alexander (2014) [A conversation with Robin Hanson](https://www.openphilanthropy.org/sites/default/files/Hanson%201-15-14%20%28public%29%20.pdf), *Open Philanthropy*, January 15.\n\nGalef, Julia (2015) [Most human behavior is signaling (Robin Hanson)](http://rationallyspeakingpodcast.org/135-most-human-behavior-is-signaling-robin-hanson/), *Rationally Speaking*, May 31.\n\nMuehlhauser, Luke (2013) [Robin Hanson on serious futurism](https://intelligence.org/2013/11/01/robin-hanson/), *Machine Intelligence Research Institute’s Blog*, November 1.\n\nRadhakrishnan, Anshuman (2020) [Less is more: how Robin Hanson’s ideas elegantly frame human behavior](https://thepolitic.org/less-is-more-how-robin-hansons-ideas-elegantly-frame-human-behavior/), *The Politic*, June 15.\n\nWiblin, Robert & Keiran Harris (2018) [Why we have to lie to ourselves about why we do what we do, according to Prof Robin Hanson](https://80000hours.org/podcast/episodes/robin-hanson-on-lying-to-ourselves/), *80,000 Hours*, March 28.\n\nExternal links\n--------------\n\n[Robin Hanson](https://mason.gmu.edu/~rhanson/home.html). Hanson's website.\n\n[Overcoming Bias](https://www.overcomingbias.com/). Hanson's blog.\n\nRelated entries\n---------------\n\n[digital person](https://forum.effectivealtruism.org/tag/digital-person) | [economic growth](https://forum.effectivealtruism.org/tag/economic-growth) | [grabby aliens](https://forum.effectivealtruism.org/topics/grabby-aliens) | [Great Filter](https://forum.effectivealtruism.org/topics/great-filter) | [marginal charity](https://forum.effectivealtruism.org/tag/marginal-charity) | [prediction markets](https://forum.effectivealtruism.org/tag/prediction-markets) | [whole brain emulation](https://forum.effectivealtruism.org/tag/whole-brain-emulation)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "CS7sWGfzkf24wR9fs",
    "name": "Lightcone Infrastructure",
    "core": false,
    "slug": "lightcone-infrastructure",
    "oldSlugs": null,
    "postCount": 3,
    "description": {
      "markdown": "**Lightcone Infrastructure** is an organization that builds community infrastructure projects expected to help safeguard humanity's [long-term future](https://forum.effectivealtruism.org/tag/long-term-future).\n\nLightcone Infrastructure built the version of [LessWrong](https://forum.effectivealtruism.org/tag/lesswrong) launched in 2017 as well as the [AI Alignment Forum](https://forum.effectivealtruism.org/tag/ai-alignment-forum); both projects continue to be in active development.\n\nFurther reading\n---------------\n\nHabryka, Oliver (2021) [The LessWrong Team is now Lightcone Infrastructure, come work with us!](https://www.lesswrong.com/posts/eR7Su77N2nK3e5YRZ/the-lesswrong-team-is-now-lightcone-infrastructure-come-work-3), *LessWrong*, September 30.\n\nExternal links\n--------------\n\n[Lightcone Infrastructure](https://www.lightconeinfrastructure.com/). Official website.\n\nRelated entries\n---------------\n\n[AI Alignment Forum](https://forum.effectivealtruism.org/tag/ai-alignment-forum) | [LessWrong](https://forum.effectivealtruism.org/tag/lesswrong)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "hryFtiuLFmC5TSsaq",
    "name": "Introductory fellowship",
    "core": false,
    "slug": "introductory-fellowship",
    "oldSlugs": [
      "intro-fellowship"
    ],
    "postCount": 30,
    "description": {
      "markdown": "An **introductory fellowship** (sometimes called **introductory program**, **seminar program**, or **scholars program)** is a program in which a dedicated group meets several times to learn about [effective altruism](https://forum.effectivealtruism.org/tag/effective-altruism). Typically, introductory fellowships consist of *weekly readings and discussion groups* for a period of 6–10 weeks, in groups of 3–10 people. Fellowships are a high-intensity and high-fidelity way of learning about effective altruism, and many local effective altruism groups run an intro fellowship as an early/middle part of their [funnel](https://www.centreforeffectivealtruism.org/the-funnel-model/). \n\nYou can learn more about intro fellowships [**here**](https://resources.eahub.org/events/fellowships/)**.**  If you're interested in running your own intro fellowship, see [**this post**](https://forum.effectivealtruism.org/posts/tn328tcnrKiEMb7Lr/should-you-organise-your-own-introductory-ea-program-or). \n\n[**EA Virtual Programs**](https://www.effectivealtruism.org/virtual-programs/) runs regular intro fellowships; anyone can apply to participate."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "F5NzCnE3gkZq4Gjs3",
    "name": "Petrov Day",
    "core": false,
    "slug": "petrov-day",
    "oldSlugs": null,
    "postCount": 9,
    "description": {
      "markdown": "**Petrov Day** is a tradition celebrating Soviet military officer [Stanislav Petrov](https://forum.effectivealtruism.org/tag/stanislav-petrov), who played a key role in preventing a nuclear attack during a false alarm incident on September 26, 1983.\n\nFurther reading\n---------------\n\nYudkowsky, Eliezer (2007) [9/26 is Petrov Day](https://www.lesswrong.com/posts/QtyKq4BDyuJ3tysoK/9-26-is-petrov-day), *LessWrong*, September 26.\n\nRelated entries\n---------------\n\n[ human extinction](https://forum.effectivealtruism.org/tag/human-extinction) | [nuclear warfare](https://forum.effectivealtruism.org/tag/nuclear-warfare-1) | [Stanislav Petrov](https://forum.effectivealtruism.org/tag/stanislav-petrov) | [Vasili Arkhipov](https://forum.effectivealtruism.org/tag/vasili-arkhipov)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "PstBTzt8sEfocKudP",
    "name": "Risk assessment",
    "core": false,
    "slug": "risk-assessment",
    "oldSlugs": null,
    "postCount": 6,
    "description": {
      "markdown": "**Risk assessment** is the estimation of the probability of an adverse outcome from exposure to a particular hazard. Risk assessment is part of the broader field of **risk management**, which seeks to identify, evaluate, monitor and mitigate risk."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "aapHL82yyPhNTZyye",
    "name": "Independent impression",
    "core": false,
    "slug": "independent-impression",
    "oldSlugs": [
      "beliefs-vs-impressions",
      "independent-impressions"
    ],
    "postCount": 5,
    "description": {
      "markdown": "An **independent impression** is a belief formed through a process that excludes [epistemic deference](https://forum.effectivealtruism.org/tag/epistemic-deference) to the beliefs of others. Independent impressions may be contrasted with **all-things-considered beliefs**, which are beliefs that do allow for such deference.\n\nHistory of the concept\n----------------------\n\nThe concept of an *independent impression* appears to have originated in a comment by user Rolf Nelson on the *Overcoming Bias* blog, back when it was still a group blog prior to the formation of [LessWrong](https://forum.effectivealtruism.org/tag/lesswrong). Nelson lamented the \"lack of generally-accepted terminology\" to express the concept of \"the opinion of each person, before they factored in the opinions of the others.\"^[\\[1\\]](#fnmmgh2extg7)^ Hal Finney then published a post on that blog recognizing the importance of the concept identified by Nelson—which Finney paraphrased as \"beliefs we would have had if we didn’t choose to be persuaded by the fact that everyone else believes differently\"—and invited readers to suggest a name for it.^[\\[2\\]](#fn4vr0bf354v3)^ [Eliezer Yudkowsky](https://forum.effectivealtruism.org/tag/eliezer-yudkowsky) suggested \"independent impression\" and Anna Salamon and others seconded that suggestion, though no clear consensus emerged; other proposals included \"independent belief\" (Hanson), \"first impression\" (Hanson), \"initial thought\" (Yudkowsky) and \"uncorrected impression\" (Yudkowsky).\n\nIn 2009, Salamon and Steve Rayhawk published a post on *LessWrong* discussing possible epistemic practices conducive to the spread of accurate beliefs within the [rationality community](https://forum.effectivealtruism.org/tag/rationality-community). One of the practices proposed was \"to explicitly separate 'individual impressions' (impressions based only on evidence we’ve ourselves verified) from 'beliefs' (which include evidence from others’ impressions).\"^[\\[3\\]](#fnvb5kzli8kwc)^ In a comment, [Robin Hanson](https://forum.effectivealtruism.org/tag/robin-hanson) singled out the value of this particular proposal, but raised the worry that \"we actually have a whole range of indirection \\[…\\] If I actually looked something up recently in an encyclopedia, while someone else just vaguely remembers looking it up sometime long ago, is that my impression or my belief?\"^[\\[4\\]](#fnoh77855netq)^\n\nIn the following years, there was very limited use or discussion of the concept of an independent impression. Activity appears to have resumed only around 2017, after Katja Grace published a number of posts applying the distinction.^[\\[5\\]](#fn596p0ts5hzd)^^[\\[6\\]](#fn4o4y13ys966)^ At around that same time, some authors independently came up with equivalent distinctions. For example, in a post about epistemic modesty, Gregory Lewis distinguished between \"credence by my lights\" and \"credence all things considered\", which as noted by Claire Zabel corresponds to \"independent impressions\" and \"all-things-considered beliefs\", respectively.^[\\[7\\]](#fney4d53ja3tf)^^[\\[8\\]](#fnrrxkcuou2v7)^\n\nFurther reading\n---------------\n\nAird, Michael (2021) [Independent impressions](https://forum.effectivealtruism.org/posts/2WS3i7eY4CdLH99eg/independent-impressions), *Effective Altruism Forum*, September 26.\n\nRelated entries\n---------------\n\n[discussion norms](https://forum.effectivealtruism.org/tag/discussion-norms) | [epistemic deference](https://forum.effectivealtruism.org/tag/epistemic-deference) | [inside vs. outside view](https://forum.effectivealtruism.org/tag/inside-vs-outside-view)\n\n1.  ^**[^](#fnrefmmgh2extg7)**^\n    \n    Nelson, Rolf (2008) [Comment on ‘Conformity questions’](https://www.overcomingbias.com/2008/04/conformity-ques.html#comment-518259239), *Overcoming Bias*, April 12.\n    \n2.  ^**[^](#fnref4vr0bf354v3)**^\n    \n    Finney, Hal (2008) [Naming beliefs](https://www.overcomingbias.com/2008/04/naming-beliefs.html), *Overcoming Bias*, April 12.\n    \n3.  ^**[^](#fnrefvb5kzli8kwc)**^\n    \n    Salamon, Anna & Steve Rayhawk (2009) [The ethic of hand-washing and community epistemic practice](https://www.lesswrong.com/posts/ZP2om2oWHPhvWP2Q3/the-ethic-of-hand-washing-and-community-epistemic-practice), *LessWrong*, March 5.\n    \n4.  ^**[^](#fnrefoh77855netq)**^\n    \n    Hanson, Robin (2009) [Comment on ‘The ethic of hand-washing and community epistemic practice’](https://www.lesswrong.com/posts/ZP2om2oWHPhvWP2Q3/the-ethic-of-hand-washing-and-community-epistemic-practice), *LessWrong*, March 5.\n    \n5.  ^**[^](#fnref596p0ts5hzd)**^\n    \n    Grace, Katja (2017) [Impression track records](https://meteuphoric.com/2017/09/23/impression-track-records/), *Meteuphoric*, September 24.\n    \n6.  ^**[^](#fnref4o4y13ys966)**^\n    \n    Grace, Katja (2017) [Want like want want](https://meteuphoric.com/2017/01/09/want-like-want-want/), *Meteuphoric*, January 9.\n    \n7.  ^**[^](#fnrefey4d53ja3tf)**^\n    \n    Lewis, Gregory (2017) [In defence of epistemic modesty](https://forum.effectivealtruism.org/posts/WKPd79PESRGZHQ5GY/in-defence-of-epistemic-modesty), *Effective Altruism Forum*, October 29.\n    \n8.  ^**[^](#fnrefrrxkcuou2v7)**^\n    \n    Zabel, Claire (2017) [Comment on ‘In defence of epistemic modesty’](https://forum.effectivealtruism.org/posts/WKPd79PESRGZHQ5GY/in-defence-of-epistemic-modesty), *Effective Altruism Forum*, October 29."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "bi9YoAPtH2pTXQTsj",
    "name": "Social science",
    "core": false,
    "slug": "social-science-1",
    "oldSlugs": null,
    "postCount": 15,
    "description": {
      "markdown": "Further reading\n---------------\n\nAskell, Amanda (2019) [AI safety needs social scientists](https://www.effectivealtruism.org/articles/ea-global-2018-ai-safety-needs-social-scientists/), *EAGlobal*, March 4.\n\nIrving, Geoffrey & Amanda Askell (2019) [Ai safety needs social scientists](http://doi.org/10.23915/distill.00014), *Distill*, vol. 4.\n\nMauricio (2020) [Social sciences & existential risk - syllabus](https://docs.google.com/document/d/1hdvJLNL8vI7rGPPJHGrJme8LwvJWYz01o3yEVDdym2s/edit#heading=h.8qmg70qszqbl), unpublished.\n\nRelated entries\n---------------\n\n[economics](https://forum.effectivealtruism.org/tag/economics) | [history](https://forum.effectivealtruism.org/tag/history) | [international relations](https://forum.effectivealtruism.org/tag/international-relations) | [psychology](https://forum.effectivealtruism.org/tag/psychology-research) | [surveys](https://forum.effectivealtruism.org/tag/surveys)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "np3gJZwzfmFJzkGK8",
    "name": "Pugwash Conferences on Science and World Affairs",
    "core": false,
    "slug": "pugwash-conferences-on-science-and-world-affairs",
    "oldSlugs": null,
    "postCount": 1,
    "description": {
      "markdown": "The **Pugwash Conferences on Science and World Affairs** (often abbreviated **Pugwash**) is an international organization focused on reducing [global catastrophic risks](https://forum.effectivealtruism.org/tag/global-catastrophic-risk) posed by [nuclear weapons](https://forum.effectivealtruism.org/tag/nuclear-warfare-1) and other [weapons of mass destruction](https://forum.effectivealtruism.org/tag/weapons-of-mass-destruction).\n\nHistory\n-------\n\nPugwash was founded in 1957 by [Bertrand Russell](https://forum.effectivealtruism.org/tag/bertrand-russell) and Joseph Rotblat, following the release of the [Russell-Einstein Manifesto](https://forum.effectivealtruism.org/tag/russell-einstein-manifesto) two years earlier. That document expressed the opinion that \"scientists should assemble in conference to appraise the perils that have arisen as a result of the development of weapons of mass destruction\", and Pugwash was an attempt to create just such a forum, to facilitate communication between East and West by bringing together eminent scientists from both sides of the Iron Curtain.\n\nIn 1995, Pugwash and Rotblat were jointly awarded the Nobel Peace Prize \"for their efforts to diminish the part played by nuclear arms in international politics and, in the longer run, to eliminate such arms.\"^[\\[1\\]](#fnj9vvh8jldjf)^\n\nEvaluation\n----------\n\nA comprehensive literature review by the Urban Institute commissioned by [Open Philanthropy](https://forum.effectivealtruism.org/tag/open-philanthropy) found relatively strong evidence, from both primary and secondary sources, that Pugwash played a key role in enabling the 1963 Partial Test Ban Treaty (PTBT) and the 1972 Anti-Ballistic Missile Treaty (ABT).^[\\[2\\]](#fn5kpv4hht9vw)^ Furthermore, in the 1980s Pugwash had a significant influence on Soviet policy: Mikhail Gorbachev reportedly told Rotblat that \"Pugwash scientists were crucial in shaping his views against nuclear weapons.\"^[\\[3\\]](#fnr47mhipha9e)^\n\nDespite these and other achievements, Pugwash faced recurrent financial difficulties after severing its ties to Cyrus Eaton, the wealthy industrialist who provided initial funding for the conference series. Rotblat noted that, for many years, Pugwash \"was run on a shoestring\",^[\\[4\\]](#fnwgp5oreefn)^ and the author of the review concludes that \"Pugwash seems to have been almost constantly in danger of insolvency.\"^[\\[5\\]](#fnox7kcrhrr1r)^\n\nFurther reading\n---------------\n\nEvangelista, Matthew (2002) [*Unarmed Forces: The Transnational Movement to End the Cold War*](https://en.wikipedia.org/wiki/Special:BookSources/978-0-8014-8784-2), Ithaca, New York: Cornell University Press.   \n*According to Paul Rubinson's literature review, this book is the\"most groundbreaking work on Pugwash\".*\n\nKarnofsky, Holden (2019) [History of philanthropy literature review: Pugwash Conferences on Science and World Affairs](https://www.openphilanthropy.org/blog/history-philanthropy-literature-review-pugwash-conferences-science-and-world-affairs), *Open Philanthropy*, April 8.\n\nKraft, Alison, Holger Nehring & Carola Sachse (2018) [The Pugwash Conferences and the global cold war: scientists, transnational networks, and the complexity of nuclear histories](https://doi.org/10.1162/jcws_e_00799), *Journal of Cold War Studies*, vol. 20, pp. 4–30.\n\nLenz, John R. (1996) [Pugwash and Russell’s legacy](https://users.drew.edu/~jlenz/pugwash.html), *The Bertrand Russell Society Quarterly*, vol. 89, pp. 18–24.\n\nRubinson, Paul (2019) [Pugwash literature review](https://www.urban.org/sites/default/files/publication/100070/pugwash_literature_review_0.pdf), Urban Institute.\n\nExternal links\n--------------\n\n[Pugwash Conferences on Science and World Affairs](https://pugwash.org/). Official website.\n\nRelated entries\n---------------\n\n[Bertrand Russell](https://forum.effectivealtruism.org/tag/bertrand-russell) | [global catastrophic risk](https://forum.effectivealtruism.org/tag/global-catastrophic-risk) | [nuclear disarmament movement](https://forum.effectivealtruism.org/tag/nuclear-disarmament-movement) | [nuclear warfare](https://forum.effectivealtruism.org/tag/nuclear-warfare-1) | [Russell-Einstein Manifesto](https://forum.effectivealtruism.org/tag/russell-einstein-manifesto) | [weapons of mass destruction](https://forum.effectivealtruism.org/tag/weapons-of-mass-destruction)\n\n1.  ^**[^](#fnrefj9vvh8jldjf)**^\n    \n    The Royal Swedish Academy of Sciences (1995) [The Nobel Peace Prize 1995](https://www.nobelprize.org/prizes/peace/1995/summary/), *The Nobel Prize*, October 13.\n    \n2.  ^**[^](#fnref5kpv4hht9vw)**^\n    \n    Rubinson, Paul (2019) [Pugwash literature review](https://www.urban.org/sites/default/files/publication/100070/pugwash_literature_review_0.pdf), Urban Institute.\n    \n3.  ^**[^](#fnrefr47mhipha9e)**^\n    \n    Rubinson, [Pugwash literature review](https://www.urban.org/sites/default/files/publication/100070/pugwash_literature_review_0.pdf), p. 10.\n    \n4.  ^**[^](#fnrefwgp5oreefn)**^\n    \n    Rotblat, Joseph (1972) [*Scientists in the Quest for Peace: A History of the Pugwash Conferences*](https://en.wikipedia.org/wiki/Special:BookSources/0-262-18054-5), Cambridge, Massachusetts: The MIT Press, p. 14.\n    \n5.  ^**[^](#fnrefox7kcrhrr1r)**^\n    \n    Rubinson, [Pugwash literature review](https://www.urban.org/sites/default/files/publication/100070/pugwash_literature_review_0.pdf), p. 18.\n    \n6.  ^**[^](#fnrefg510toc1l)**^\n    \n    Rubinson, [Pugwash literature review](https://www.urban.org/sites/default/files/publication/100070/pugwash_literature_review_0.pdf), p. 6."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "FJWS6W5zzzbFQCF77",
    "name": "Global Food Partners",
    "core": false,
    "slug": "global-food-partners",
    "oldSlugs": null,
    "postCount": 2,
    "description": {
      "markdown": "**Global Food Partners** (**GFP**) is a Singapore-based multinational [consultancy](https://forum.effectivealtruism.org/tag/consultancy) that works with food businesses and farmers in Asia to improve [farmed animal welfare](https://forum.effectivealtruism.org/tag/farmed-animal-welfare) policies and practices.\n\nEvaluation\n----------\n\nGFP is one of the five organizations focused on animal welfare recommended by [Founders Pledge](https://forum.effectivealtruism.org/tag/founders-pledge).^[\\[1\\]](#fnwnxvkfz2kb)^^[\\[2\\]](#fnxjo8aa4uxsh)^ As of August 2022, GFP has received over $4.6 million in funding from [Open Philanthropy](https://forum.effectivealtruism.org/tag/open-philanthropy),^[\\[3\\]](#fn91fkjlfs0yu)^ and over $230,000 from the [Animal Welfare Fund](https://forum.effectivealtruism.org/tag/animal-welfare-fund).^[\\[4\\]](#fnw2l6cv55m7h)^^[\\[5\\]](#fn6bl4380stm)^^[\\[6\\]](#fn1y5wdmoet4k)^^[\\[7\\]](#fnw3rmi377awo)^\n\nExternal links\n--------------\n\n[Global Food Partners](https://globalfoodpartners.com/). Official website.\n\n[Apply for a job](https://globalfoodpartners.com/careers).\n\n1.  ^**[^](#fnrefwnxvkfz2kb)**^\n    \n    Clare, Stephen (2020) [Animal welfare cause report](https://founderspledge.com/stories/animal-welfare-cause-report), *Founders Pledge*, November 2.\n    \n2.  ^**[^](#fnrefxjo8aa4uxsh)**^\n    \n    Clare, Stephen (2020) [Global Food Partners - high-impact funding opportunity](https://founderspledge.com/stories/global-food-partners-high-impact-funding-opportunity), *Founders Pledge*, November 3.\n    \n3.  ^**[^](#fnref91fkjlfs0yu)**^\n    \n    Open Philanthropy (2022) [Grants database: Global Food Partners](https://www.openphilanthropy.org/grants/?q=&organization-name=global-food-partners), *Open Philanthropy*.\n    \n4.  ^**[^](#fnrefw2l6cv55m7h)**^\n    \n    Animal Welfare Fund (2020) [July 2020: Animal Welfare Fund grants](https://funds.effectivealtruism.org/funds/payouts/july-2020-animal-welfare-fund-grants), *Effective Altruism Funds*, July. \n    \n5.  ^**[^](#fnref6bl4380stm)**^\n    \n    Animal Welfare Fund (2020) [November 2020: Animal Welfare Fund grants](https://funds.effectivealtruism.org/funds/payouts/november-2020-animal-welfare-fund-grants), *Effective Altruism Funds*, November. \n    \n6.  ^**[^](#fnref1y5wdmoet4k)**^\n    \n    Animal Welfare Fund (2021) [May 2021: Animal Welfare Fund grants](https://funds.effectivealtruism.org/funds/payouts/may-2021-animal-welfare-fund-grants), *Effective Altruism Funds*, May. \n    \n7.  ^**[^](#fnrefw3rmi377awo)**^\n    \n    Animal Welfare Fund (2021) [July 2021: Animal Welfare Fund grants](https://funds.effectivealtruism.org/funds/payouts/july-2021-animal-welfare-fund-grants), *Effective Altruism Funds*, July."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "gpNhZNH2seiEcmvzh",
    "name": "Nuclear energy",
    "core": false,
    "slug": "nuclear-energy",
    "oldSlugs": [
      "nuclear-energy"
    ],
    "postCount": 10,
    "description": {
      "markdown": "**Nuclear energy** is power obtained from nuclear reactions to produce electricity.\n\nRelated entries\n---------------\n\n[climate change](https://forum.effectivealtruism.org/tag/climate-change)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "qudNsPyYu6Q3LWq8D",
    "name": "Redwood Research",
    "core": false,
    "slug": "redwood-research",
    "oldSlugs": null,
    "postCount": 8,
    "description": {
      "markdown": "**Redwood Research** is a nonprofit organization focused on applied [AI alignment](https://forum.effectivealtruism.org/tag/ai-alignment) research.  \n  \nTheir current project started in early August 2021.^[\\[1\\]](#fnkhjbvwhxumr)^\n\nFunding\n-------\n\nAs of July 2022, Redwood Research has received over $9.4 million in funding from [Open Philanthropy](https://forum.effectivealtruism.org/topics/open-philanthropy)^[\\[2\\]](#fnspn6ad5ee3g)^ and nearly $1.3 million from the [Survival and Flourishing](https://forum.effectivealtruism.org/topics/survival-and-flourishing) Fund.^[\\[3\\]](#fnkvg80rmiu2f)^\n\nExternal links\n--------------\n\n[Redwood Research](https://www.redwoodresearch.org/). Official website.\n\n[Apply for a job](https://www.redwoodresearch.org/jobs).\n\n1.  ^**[^](#fnrefkhjbvwhxumr)**^\n    \n    Shlegeris, Buck (2021) [Redwood Research’s current project](https://www.alignmentforum.org/posts/k7oxdbNaGATZbtEg3/redwood-research-s-current-project), *AI Alignment Forum*, September 21.\n    \n2.  ^**[^](#fnrefspn6ad5ee3g)**^\n    \n    Open Philanthropy (2022) [Grants database: Redwood Research](https://www.openphilanthropy.org/grants/?q=&organization-name=redwood-research), *Open Philanthropy*.\n    \n3.  ^**[^](#fnrefkvg80rmiu2f)**^\n    \n    Survival and Flourishing Fund (2021) [SFF-2022-H1 S-Process recommendations announcement](https://survivalandflourishing.fund/sff-2022-h1-recommendations), *Survival and Flourishing Fund*."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "CCeRmSG9xoKs4CCdt",
    "name": "Fair trade",
    "core": false,
    "slug": "fair-trade",
    "oldSlugs": [
      "fairtrade",
      "fairt-rade"
    ],
    "postCount": 4,
    "description": {
      "markdown": "**Fair trade** is trade that meets certain social, economic, or environmental standards relating to wages and conditions for those involved in the production or distribution of goods, as well as a [social movement](https://forum.effectivealtruism.org/tag/social-and-intellectual-movements) dedicated to ensuring that those standards are met.\n\nFurther reading\n---------------\n\nGriffiths, Peter (2012) [Ethical objections to fairtrade](https://doi.org/10.1007/s10551-011-0972-0), *Journal of Business Ethics*, vol. 105, pp. 357–373.\n\nRelated entries\n---------------\n\n[ethics of personal consumption](https://forum.effectivealtruism.org/tag/ethics-of-personal-consumption) | [international trade](https://forum.effectivealtruism.org/tag/international-trade) | [social and intellectual movements](https://forum.effectivealtruism.org/tag/social-and-intellectual-movements)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "CMcC63gTKMPGCFnpS",
    "name": "Creative Writing Contest",
    "core": false,
    "slug": "creative-writing-contest",
    "oldSlugs": [
      "creative-writing-contest",
      "creative-writing-contest-dying-good-when-dying-s-bad-1058"
    ],
    "postCount": 146,
    "description": {
      "markdown": "Use this tag to post entries to the Creative Writing Contest."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "oniiXetJqb2qsJ7YT",
    "name": "Think tanks",
    "core": false,
    "slug": "think-tanks",
    "oldSlugs": null,
    "postCount": 18,
    "description": {
      "markdown": "**Think tanks** (sometimes spelled **think-tanks**) are nonprofit organizations that conduct research aimed at providing policy advice and analysis to policymakers.\n\nWhat is a think tank?\n---------------------\n\nIn general, there is great diversity in the think tank ecosystem and experts often note that there is no such thing as a prototypical think tank: these organizations differ from one another across several important dimensions, such as \"in how they are funded, the roles that they play, their attitudes toward 'neutral expertise', their recruitment of staff, and their 'product lines.'\"^[\\[1\\]](#fngpx92rp0wng)^ Moreover, the boundaries between think tanks and other entities with a mandate to supply policy advice, such as pressure groups, private foundations, academic institutes, policy schools, government agencies, and non-government organizations are sometimes blurry.\n\nA stylized distinction may be drawn between \"advocacy\" and \"research\" think tanks, depending on whether the primary goal is to provide \"ammunition\" or \"enlightenment\",^[\\[1\\]](#fngpx92rp0wng)^ although these are probably best regarded as two limit cases in opposite ends of a continuum. Advocacy think tanks (sometimes called \"ideological tanks\" or \"think and do tanks\") often self-identify with a specific political ideology and offer their services to clients in a relatively well-defined range of the political spectrum. Examples include the Heritage Foundation and the American Enterprise Institute (conservative), the Center for American Progress and the Economic Policy Institute (liberal), and Cato Institute and the Reason Foundation (libertarian).\n\nBy contrast, research think tanks (sometimes called \"ink tanks\") are not antecedently committed—at least not explicitly—to policy proposals with a particular ideological bent and focus primarily on generating novel policy insights. A subset of these think tanks resemble academic institutions in many respects, and are as such sometimes referred to as \"universities without students\": organizations such as the Brookings Institution, the Russell Sage Foundation, and the Peterson Institute for International Economics, to name some examples, are staffed mainly by researchers holding doctoral degrees and publish their research in scholarly books or monographs, although these think tanks are usually  (but not always) organizationally independent of academia and have much closer contact with policy activists and a heavier emphasis on practical applicability. Other research think tanks, by contrast, operate in a manner more similar to [consultancies](https://forum.effectivealtruism.org/tag/consultancy): most work by the RAND Corporation, for instance, is focused on program evaluations requested and funded by government agencies.\n\nThe impact of think tanks on policy\n-----------------------------------\n\nThink tanks do not only differ significantly in their structure; there is also great diversity in the type and extent of their impact. Andrew Rich and Kent Weaver summarize:^[\\[2\\]](#fnmg7solffwzb)^\n\n> First, \\[think tanks\\] can provide basic research on policy problems and policy solutions—for example, outlining the causes and consequences of skills deficits or slow economic growth. Second, think tanks can provide advice on immediate policy concerns through many points of entry into the US policy-making process. These include testifying before congressional committees, writing opinion pieces for newspapers and new media outlets, and writing policy briefs that are increasingly distributed in both print and Web-based formats. Informal consultations and dialogues are another vehicle for advice in immediate policy debates. Third, think tanks can act as evaluators of government programs, usually on a contractual basis. Fourth, think tank staff can be called upon to provide commentary on current events, both for the national and regional press and through new media outlets such as Web commentaries and blog posts. Finally, think tanks can supply personnel for government, given the relatively porous nature of the personnel system and in particular the substantial turnover of high-level, policy-making personnel that takes place at the beginning of presidential and gubernatorial terms.\n\n[Open Philanthropy](https://forum.effectivealtruism.org/tag/open-philanthropy) has given grants to several think tanks working in a variety of areas, including the [Center for Security and Emerging Technology](https://forum.effectivealtruism.org/tag/center-for-security-and-emerging-technology) ([artificial intelligence](https://forum.effectivealtruism.org/tag/artificial-intelligence)), the [Johns Hopkins Center for Health Security](https://forum.effectivealtruism.org/tag/johns-hopkins-center-for-health-security) ([biosecurity](https://forum.effectivealtruism.org/tag/biosecurity) and [pandemic preparedness](https://forum.effectivealtruism.org/tag/pandemic-preparedness)), Dezernat Zukunft ([macroeconomic policy](https://forum.effectivealtruism.org/tag/macroeconomic-policy)), the Center for Global Development ([immigration reform](https://forum.effectivealtruism.org/tag/immigration-reform)), the [Good Food Institute](https://forum.effectivealtruism.org/tag/good-food-institute) ([animal product alternatives](https://forum.effectivealtruism.org/tag/animal-product-alternatives)), [Nuclear Threat Initiative](https://forum.effectivealtruism.org/tag/nuclear-threat-initiative) (biosecurity and [global catastrophic biological risks](https://forum.effectivealtruism.org/tag/global-catastrophic-biological-risk)), the Center on Budget and Policy Priorities (macroeconomic policy), and the Sightline Institute ([land use reform](https://forum.effectivealtruism.org/tag/land-use-reform)).\n\nIn the United Kingdom, a think tank often cited as having had a major—though not necessarily positive—impact is the Institute of Economic Affairs (IEA), described by an author as \"arguably \\[the\\] most influential think tank in British history\"^[\\[3\\]](#fnplg9grwvbvd)^ and frequently discussed as having played a significant role in the rise of \"neoliberal\" ideas in the 1970s and 1980s.^[\\[4\\]](#fnrh0lz8rffxi)^ Some have even claimed that IEA was causally implicated in key geopolitical developments beyond British borders: Sir Oliver Letwin, a conservative MP, once wrote: \"without the IEA and its clones, no Thatcher and quite possibly no Reagan; without Reagan, no Star Wars; without Star Wars, no economic collapse of the Soviet Union.\"^[\\[5\\]](#fnl09t4jxkczh)^\n\nA comprehensive report focused on American think tanks by a pseudonymous group of authors with familiarity and personal experience in the Washington D.C. think tank ecosystem summarizes: \"Whereas the *potential* for impact is widely accepted, the *average* level of think tank impact is far more uncertain.\"^[\\[6\\]](#fne2qm9uk4i74)^\n\nThink tanks and career choice\n-----------------------------\n\n[80,000 Hours](https://forum.effectivealtruism.org/tag/80-000-hours) rates working at a think tank highly in terms of impact potential, [career capital](https://forum.effectivealtruism.org/tag/career-capital) and [job satisfaction](https://forum.effectivealtruism.org/tag/job-satisfaction), and recommends this path for candidates early in their careers and suited for it.\n\nThe report cited in the previous section considers working at a think tank to be usually more valuable for its career capital—in particular, for growing one's professional network, improving one's understanding of the policy world, gaining policy-relevant skills, and becoming a recognized domain expert—than for its direct impact.^[\\[6\\]](#fne2qm9uk4i74)^ The report also notes that the types of career capital think tanks help build vary considerably across different think tanks and roles within them.\n\nNote that, generally, think tank work is considered to be a stage in a broader career in policy rather than a career in itself: \"Almost nobody has a 'think tank career'—instead, see a think tank job as one possible part of a 'policy career'.\"^[\\[6\\]](#fne2qm9uk4i74)^\n\nFurther reading\n---------------\n\nLocke_USA (2021) [Working at a (DC) policy think tank: Why you might want to do it, what it’s like, and how to get a job](https://forum.effectivealtruism.org/posts/dZnLssXGoHDs9kSPu/working-at-a-dc-policy-think-tank-why-you-might-want-to-do), *Effective Altruism Forum*, August 31.  \n*A comprehensive overview of the Washington DC think tank ecosystem. Appendix A lists a number of relevant books, articles, podcasts and websites.*\n\nStone, Diane (2015) [Think tanks](https://doi.org/10.1016/B978-0-08-097086-8.75052-5), in James D. Wright (ed.) *International Encyclopedia of the Social & Behavioral Sciences*, 2nd ed., Amsterdam: Elsevier, pp. 294–299.\n\nTroy, Tevi (2012) [Devaluing the think tank](https://www.nationalaffairs.com/publications/detail/devaluing-the-think-tank), *National Affairs*, vol. 10.\n\nWeaver, R. Kent (1989) [The changing world of think tanks](https://doi.org/10.2307/419623), *PS: Political Science and Politics*, vol. 22, pp. 563–578.\n\nWiblin, Robert (2015) [Think tank research](https://80000hours.org/career-reviews/think-tank-research/), *80,000 Hours*, July.\n\nRelated entries\n---------------\n\n[academia](https://forum.effectivealtruism.org/tag/academia-1) | [policy](https://forum.effectivealtruism.org/tag/policy-change) | [research careers](https://forum.effectivealtruism.org/tag/research-careers) | [United Kingdom](https://forum.effectivealtruism.org/topics/united-kingdom) | [United States](https://forum.effectivealtruism.org/topics/united-states)\n\n1.  ^**[^](#fnrefgpx92rp0wng)**^\n    \n    Rich, Andrew & R. Kent Weaver (2012) [Think tanks](https://doi.org/10.1093/acref/9780199764310.001.0001), in David Coates (ed.) *The Oxford Companion to American Politics*, Oxford: Oxford University Press, pp. 363–369.\n    \n2.  ^**[^](#fnrefmg7solffwzb)**^\n    \n    Rich & Weaver, [Think tanks](https://doi.org/10.1093/acref/9780199764310.001.0001), p. 364.\n    \n3.  ^**[^](#fnrefplg9grwvbvd)**^\n    \n    Plehwe, Dieter (2011) [Who cares about excellence? Social sciences under think tank pressure](https://en.wikipedia.org/wiki/Special:BookSources/978-1-4438-3439-1), in Tor Halvorsen & Attle Nyhagen (eds.) *Academic Identities, Academic Challenges? American and European Experiences of the Transformation of Higher Education and Research*, Newcastle upon Tyne: Cambridge Scholars Publishing, pp. 159–193, p. 173.\n    \n4.  ^**[^](#fnrefrh0lz8rffxi)**^\n    \n    Vaughan, Kerry (2016) [What the EA community can learn from the rise of the neoliberals](https://www.effectivealtruism.org/articles/ea-neoliberal/), *Effective Altruism*, December 5.\n    \n5.  ^**[^](#fnrefl09t4jxkczh)**^\n    \n    Letwin, Oliver (1994) 'Wising up the stupid party', *The Times*, May 26, p. 40.\n    \n6.  ^**[^](#fnrefe2qm9uk4i74)**^\n    \n    Locke_USA (2021) [Working at a (DC) policy think tank: Why you might want to do it, what it’s like, and how to get a job](https://forum.effectivealtruism.org/posts/dZnLssXGoHDs9kSPu/working-at-a-dc-policy-think-tank-why-you-might-want-to-do), *Effective Altruism Forum*, August 31."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "yTyKr6WCeJGSqLtt3",
    "name": "Management & mentoring",
    "core": false,
    "slug": "management-and-mentoring",
    "oldSlugs": null,
    "postCount": 50,
    "description": {
      "markdown": "Further reading\n---------------\n\nAird, Michael (2021) [Collection of collections of resources relevant to (research) management, mentorship, training, etc.](https://docs.google.com/document/d/1LvJWwqBHAvl268_HVyNRAmx4-PAjbE8Y_i2eL4nbRGk/edit), unpublished.\n\nKoehler, Arden & Keiran Harris (2020) [Benjamin Todd on what the effective altruism community most needs](https://80000hours.org/podcast/episodes/ben-todd-on-what-effective-altruism-most-needs/), *80,000 Hours*, November 12.\n\nTodd, Benjamin & Roman Duda (2018) [Why operations management is one of the biggest bottlenecks in effective altruism](https://80000hours.org/articles/operations-management/), *80,000 Hours*, March.\n\nWhittlestone, Jess (2013) [Bringing it all together: high impact research management](https://80000hours.org/2013/02/bringing-it-all-together-high-impact-research-management/), *80,000 Hours*, February 18.\n\nRelated entries\n---------------\n\n[coaching](https://forum.effectivealtruism.org/tag/coaching) | [constraints on effective altruism](https://forum.effectivealtruism.org/tag/constraints-on-effective-altruism) | [hiring](https://forum.effectivealtruism.org/tag/hiring) | [operations](https://forum.effectivealtruism.org/tag/operations) | [org strategy](https://forum.effectivealtruism.org/tag/org-strategy) | [research training programs](https://forum.effectivealtruism.org/tag/research-training-programs) |  [scalably using labour](https://forum.effectivealtruism.org/tag/scalably-using-labour) | [WANBAM](https://forum.effectivealtruism.org/tag/wanbam)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "iSwQSEgqvpK24aswy",
    "name": "Spaced repetition",
    "core": false,
    "slug": "spaced-repetition",
    "oldSlugs": null,
    "postCount": 22,
    "description": {
      "markdown": "**Spaced repetition** is a memorization technique that involves the scheduling of study sessions according to an algorithm optimized for efficient long-term retention of the learned material.\n\nFurther reading\n---------------\n\nBranwen, Gwern (2009) [Spaced repetition for efficient learning](https://www.gwern.net/Spaced-repetition), *Gwern.net*, March 11 (updated 17 May 2019).\n\nNielsen, Michael (2018) [Augmenting long-term memory](http://augmentingcognition.com/ltm.html), *Augmenting Cognition*, July."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "DFuiCaTCxwWWy2gmZ",
    "name": "The Roots of Progress",
    "core": false,
    "slug": "the-roots-of-progress",
    "oldSlugs": null,
    "postCount": 1,
    "description": {
      "markdown": "**The Roots of Progress** is a nonprofit organization focused on [progress studies](https://forum.effectivealtruism.org/tag/progress-studies).\n\nHistory\n-------\n\nThe Roots of Progress  started as a blog in 2017. It launched as a nonprofit in August 2021.^[\\[1\\]](#fn7tecxvi5i4q)^\n\nEvaluation\n----------\n\nThe Roots of Progress has received funding from [Survival and Flourishing Fund](https://forum.effectivealtruism.org/tag/survival-and-flourishing-fund), [Long-Term Future Fund](https://forum.effectivealtruism.org/tag/long-term-future-fund), [Open Philanthropy](https://forum.effectivealtruism.org/tag/open-philanthropy), and [Emergent Ventures](https://forum.effectivealtruism.org/tag/emergent-ventures).^[\\[1\\]](#fn7tecxvi5i4q)^\n\nFurther reading\n---------------\n\nCrawford, Jason (2020) [AMA: Jason Crawford, The Roots of Progress](https://forum.effectivealtruism.org/posts/ZLJgvkXwgRyoxixHx/ama-jason-crawford-the-roots-of-progress), *Effective Altruism Forum*, December 3.\n\nCrawford, Jason (2021) [Wanted: Chief of Staff](https://rootsofprogress.org/wanted-chief-of-staff), *The Roots of Progress*, September 7.\n\nPiper, Kelsey (2021) [How does progress happen?](https://www.vox.com/future-perfect/22652782/roots-of-progress-jason-crawford), *Vox*, September 25.  \n*An interview with Jason Crawford, CEO of The Roots of Progress.*\n\nExternal links\n--------------\n\n[The Roots of Progress](https://rootsofprogress.org/). Official website.\n\n[Donate to The Root of Progress](https://rootsofprogress.org/support). \n\nRelated entries\n---------------\n\n[progress studies](https://forum.effectivealtruism.org/tag/progress-studies)\n\n1.  ^**[^](#fnref7tecxvi5i4q)**^\n    \n    The Roots of Progress (2021) [About](https://rootsofprogress.org/about), *The Roots of Progress*."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "yT7AzG3frSR6yYAss",
    "name": "1Day Sooner",
    "core": false,
    "slug": "1day-sooner",
    "oldSlugs": null,
    "postCount": 5,
    "description": {
      "markdown": "**1Day Sooner** is a nonprofit organization that advocates on behalf of volunteers willing to participate in high-risk, high-impact medical research.\n\nHistory\n-------\n\n1Day Sooner was founded in April 2020 to promote the use of [human challenge trials](https://forum.effectivealtruism.org/topics/human-challenge-trials) as a way to speed up the development of [COVID-19](https://forum.effectivealtruism.org/tag/covid-19-pandemic) vaccines. The organization has now expanded to work on other types of challenge trials, as well as other projects such as [pandemic preparedness](https://forum.effectivealtruism.org/tag/pandemic-preparedness) and [vaccine equity](https://forum.effectivealtruism.org/tag/vaccines).\n\nFunding \n--------\n\nAs of July 2022, 1Day Sooner has received a total of $3.1 million in grants from [Open Philanthropy](https://forum.effectivealtruism.org/tag/open-philanthropy),^[\\[1\\]](#fn0u9dmz0thj6f)^ $500,000 from the [Survival and Flourishing](https://forum.effectivealtruism.org/topics/survival-and-flourishing) Fund,^[\\[2\\]](#fn5zbl7ccyr5t)^ and $350,000 from the [Future Fund](https://forum.effectivealtruism.org/topics/future-fund).^[\\[3\\]](#fn9n48hu87rb)^\n\nFurther reading\n---------------\n\nMatthews, Dylan (2020) [How exposing healthy volunteers to Covid-19 for vaccine testing would work](https://www.vox.com/future-perfect/2020/5/20/21258725/covid-19-human-challenge-trials-vaccine-update-sars-cov-2), *Vox*, May 20.\n\nPiper, Kelsey (2021) [What kind of person signs up to be infected with the coronavirus on purpose?](https://www.vox.com/future-perfect/22340540/human-challenge-trials-coronavirus-vaccine-pandemic), *Vox*, March 31.\n\nExternal links\n--------------\n\n[1Day Sooner](https://www.1daysooner.org/). Official website.\n\n[Apply for a job](https://www.1daysooner.org/jobs).\n\n[Donate to 1Day Sooner](https://www.1daysooner.org/donate).\n\n[Volunteer for a human challenge trial](https://www.1daysooner.org/volunteer).\n\nRelated entries\n---------------\n\n[COVID-19 pandemic](https://forum.effectivealtruism.org/tag/covid-19-pandemic) | [human challenge trials](https://forum.effectivealtruism.org/topics/human-challenge-trials) | [pandemic preparedness](https://forum.effectivealtruism.org/tag/pandemic-preparedness) | [vaccines](https://forum.effectivealtruism.org/tag/vaccines)\n\n1.  ^**[^](#fnref0u9dmz0thj6f)**^\n    \n    Open Philanthropy (2022) [Grants database: 1Day Sooner](https://www.openphilanthropy.org/grants/?q=&organization-name=1day-sooner), *Open Philanthropy*.\n    \n2.  ^**[^](#fnref5zbl7ccyr5t)**^\n    \n    Survival and Flourishing Fund (2021) [SFF-2022-H1 S-Process recommendations announcement](https://survivalandflourishing.fund/sff-2022-h1-recommendations), *Survival and Flourishing Fund*.\n    \n3.  ^**[^](#fnref9n48hu87rb)**^\n    \n    Future Fund (2022) [Our grants and investments: 1Day Sooner](https://ftxfuturefund.org/our-grants/?_organization_name=1day-sooner), *Future Fund*."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "DyXj7q39RnY5eKdgh",
    "name": "Crustacean Compassion",
    "core": false,
    "slug": "crustacean-compassion",
    "oldSlugs": null,
    "postCount": 1,
    "description": {
      "markdown": "**Crustacean Compassion** is a nonprofit organization that campaigns for the humane treatment of crabs, lobsters and other decapod [crustaceans](https://forum.effectivealtruism.org/tag/crustacean-welfare) in the United Kingdom.\n\nActivities\n----------\n\nCrustacean Compassion's activities so far have focused on organizing a public petition to include crustaceans as a class of animals protected by the Animal Welfare (Sentience) Bill;^[\\[1\\]](#fnqe79esjn7m)^  releasing a report outlining the ethical, legal and scientific arguments for the protection of crustaceans;^[\\[2\\]](#fntzgs3ke75a)^ and drafting an open letter to the Department for Environment, Food and Rural Affairs, signed by over fifty animal welfare scientists and other relevant experts, asserting the sentience of crustaceans and calling for their protection.^[\\[3\\]](#fnob4mxg71sys)^\n\nIn November 2021, following the publication of a report by the London School of Economics on decapod and cephalopod sentience,^[\\[4\\]](#fn39agdk739xp)^ the Animal Welfare (Sentience) Bill was amended to recognize crabs, octopuses and lobsters as sentient beings.^[\\[5\\]](#fna4fy0ihwjtv)^^[\\[6\\]](#fnkjw0ve6rqq)^^[\\[7\\]](#fnnny86v2hm9)^\n\nFunding\n-------\n\nCrustacean Compassion was the recipient of four grants totaling $137,000 from the [Animal Welfare Fund](https://forum.effectivealtruism.org/tag/animal-welfare-fund),^[\\[8\\]](#fnkl1eghtvvab)^^[\\[9\\]](#fn9829vcygto)^^[\\[10\\]](#fnbtigeovkrj)^^[\\[11\\]](#fncb2x0fygsrc)^ and of a grant of over $780,000 from [Open Philanthropy](https://forum.effectivealtruism.org/tag/open-philanthropy) to support work to advance UK welfare reforms for decapod crustaceans.^[\\[12\\]](#fn43kj4t6x7u)^\n\nExternal links\n--------------\n\n[Crustacean Compassion](https://www.crustaceancompassion.org.uk/). Official website.\n\n[Apply for a job](https://www.crustaceancompassion.org.uk/work-with-us).\n\n[Donate to Crustacean Compassion](https://www.crustaceancompassion.org.uk/donate).\n\nRelated entries\n---------------\n\n[crustacean welfare](https://forum.effectivealtruism.org/tag/crustacean-welfare) | [farmed animal welfare](https://forum.effectivealtruism.org/tag/farmed-animal-welfare) | [Shrimp Welfare Project](https://forum.effectivealtruism.org/tag/shrimp-welfare-project)\n\n1.  ^**[^](#fnrefqe79esjn7m)**^\n    \n    Crustacean Compassion (2021) [Take action](https://www.crustaceancompassion.org.uk/take-action), *Crustacean Compassion*.\n    \n2.  ^**[^](#fnreftzgs3ke75a)**^\n    \n    Crustacean Compassion (2021) [The case for the legal protection of decapod crustaceans](https://8fd525d4-663c-4ad2-9429-d18b1bfe5cb1.filesusr.com/ugd/46ca59_8ae6fa8c7fa545e89b4827722e394bcd.pdf), *Crustacean Compassion*, July.\n    \n3.  ^**[^](#fnrefob4mxg71sys)**^\n    \n    Crustacean Compassion (2018) [Open letter to Rt Hon Michael Gove MP](https://www.crustaceancompassion.org.uk/open-letter), *Crustacean Compassion*, January 31.\n    \n4.  ^**[^](#fnref39agdk739xp)**^\n    \n    Birch, Jonathan *et al.* (2021) [Review of the evidence of sentience in cephalopod molluscs and decapod crustaceans](https://www.lse.ac.uk/News/News-Assets/PDFs/2021/Sentience-in-Cephalopod-Molluscs-and-Decapod-Crustaceans-Final-Report-November-2021.pdf), London School of Economics.\n    \n5.  ^**[^](#fnrefa4fy0ihwjtv)**^\n    \n    Crustacean Compassion (2021) [Uk government recognises crabs, lobsters & prawns as sentient and plans to protect them!](https://www.crustaceancompassion.org.uk/single-post/uk-government-recognises-crabs-lobsters-prawns-as-sentient-and-plans-to-protect-them), *Crustacean Compassion*, November 21.\n    \n6.  ^**[^](#fnrefkjw0ve6rqq)**^\n    \n    Department for Environment, Food & Rural Affairs (2021) [Lobsters, octopus and crabs recognised as sentient beings](https://www.gov.uk/government/news/lobsters-octopus-and-crabs-recognised-as-sentient-beings), *GOV.UK*, November 19.\n    \n7.  ^**[^](#fnrefnny86v2hm9)**^\n    \n    Benyon, Richard (2021) [Animal Welfare (Sentience) Bill \\[HL\\]](https://bills.parliament.uk/bills/2867), *UK Parliament*, May 13.\n    \n8.  ^**[^](#fnrefkl1eghtvvab)**^\n    \n    Animal Welfare Fund (2019) [November 2019: Animal Welfare Fund grants](https://funds.effectivealtruism.org/funds/payouts/november-2019-animal-welfare-fund-grants), *Effective Altruism Funds*, November.\n    \n9.  ^**[^](#fnref9829vcygto)**^\n    \n    Animal Welfare Fund (2020) [July 2020: Animal Welfare Fund grants](https://funds.effectivealtruism.org/funds/payouts/july-2020-animal-welfare-fund-grants), *Effective Altruism Funds*, July.\n    \n10.  ^**[^](#fnrefbtigeovkrj)**^\n    \n    Animal Welfare Fund (2020) [November 2020: Animal Welfare Fund grants](https://funds.effectivealtruism.org/funds/payouts/november-2020-animal-welfare-fund-grants), *Effective Altruism Funds*, November.\n    \n11.  ^**[^](#fnrefcb2x0fygsrc)**^\n    \n    Animal Welfare Fund (2021) [July 2021: Animal Welfare Fund grants](https://funds.effectivealtruism.org/funds/payouts/july-2021-animal-welfare-fund-grants), *Effective Altruism Funds*, July.\n    \n12.  ^**[^](#fnref43kj4t6x7u)**^\n    \n    Bollard, Lewis & Amanda Hungerford (2021) [Crustacean Compassion — General support](https://www.openphilanthropy.org/grants/crustacean-compassion-general-support/), *Open Philanthropy*, February 17."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "Ja4BNWoETLwGHZ8T9",
    "name": "Digital person",
    "core": false,
    "slug": "digital-person",
    "oldSlugs": null,
    "postCount": 11,
    "description": {
      "markdown": "A **digital person** is a person running on digital computing hardware.\n\nAs [Holden Karnofsky](https://forum.effectivealtruism.org/topics/holden-karnofsky) uses the term, \"digital person\" may refer to human [whole brain emulations](https://forum.effectivealtruism.org/tag/whole-brain-emulation) as well as digital \"descendants\" of humans.^[\\[1\\]](#fnomjxn8g7a6)^ According to Karnofsky, digital people would have the same moral standing as humans (cf. [moral patient](https://forum.effectivealtruism.org/tag/moral-patient)), would accelerate [economic growth](https://forum.effectivealtruism.org/topics/economic-growth) and [scientific progress](https://forum.effectivealtruism.org/topics/scientific-progress), and could cause a [\"lock-in\" of values](https://forum.effectivealtruism.org/topics/value-lock-in).  For this reason, Karnofsky considers the emergence of digital people a [transformative development](https://forum.effectivealtruism.org/topics/transformative-development), as well as one that, because of advances in [artificial intelligence](https://forum.effectivealtruism.org/topics/artificial-intelligence), could occur this century—making it potentially [the most important one in human history](https://forum.effectivealtruism.org/topics/hinge-of-history).^[\\[2\\]](#fnzn1rdhv7z1)^\n\nFurther reading\n---------------\n\nHanson, Robin (2016) [*The Age of Em: Work, Love, and Life When Robots Rule the Earth*](https://en.wikipedia.org/wiki/Special:BookSources/978-0-19-875462-6), Oxford: Oxford University Press.\n\nKarnofsky, Holden (2021) [Digital people FAQ](https://www.cold-takes.com/digital-people-faq/), *Cold Takes*, July 27.\n\nRelated entries\n---------------\n\n[artificial intelligence](https://forum.effectivealtruism.org/tag/artificial-intelligence) | [artificial sentience](https://forum.effectivealtruism.org/tag/artificial-sentience) | [human-level artificial intelligence](https://forum.effectivealtruism.org/tag/artificial-sentience) | [moral patienthood](https://forum.effectivealtruism.org/tag/moral-patienthood) | [transformative development](https://forum.effectivealtruism.org/tag/transformative-development) | [whole brain emulation](https://forum.effectivealtruism.org/tag/whole-brain-emulation)\n\n1.  ^**[^](#fnrefomjxn8g7a6)**^\n    \n    \"The central case I'll focus on is that of digital people just like us, perhaps created via mind uploading (simulating human brains). However, one could also imagine entities unlike us in many ways, but still properly thought of as \"descendants\" of humanity; those would be digital people as well.\" Karnofsky, Holden (2021) [Digital people would be an even bigger deal](https://www.cold-takes.com/how-digital-people-could-change-the-world/), *Cold Takes*, July 27.\n    \n2.  ^**[^](#fnrefzn1rdhv7z1)**^\n    \n    Karnofsky, Holden (2021) [Digital people FAQ](https://www.cold-takes.com/digital-people-faq/), *Cold Takes*, July 27."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "ueKcDqXGzgy7urHkA",
    "name": "United Kingdom",
    "core": false,
    "slug": "united-kingdom",
    "oldSlugs": [
      "united-kingdom-policy-and-politics"
    ],
    "postCount": 13,
    "description": {
      "markdown": "The **United Kingdom** tag is used for posts that are about the United Kingdom, including (but not restricted to)  [policy](https://forum.effectivealtruism.org/topics/policy),  [politics](https://forum.effectivealtruism.org/topics/electoral-politics), and [international relations](https://forum.effectivealtruism.org/topics/international-relations).\n\nRelated entries\n---------------\n\n[China](https://forum.effectivealtruism.org/topics/china) | [India](https://forum.effectivealtruism.org/topics/india) | [international relations](https://forum.effectivealtruism.org/topics/international-relations) | [electoral politics](https://forum.effectivealtruism.org/topics/electoral-politics) | [policy change](https://forum.effectivealtruism.org/topics/policy-change)| [Russia](https://forum.effectivealtruism.org/topics/russia) | [United States](https://forum.effectivealtruism.org/topics/united-states)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "beaSg7bEPx3aBq8ra",
    "name": "Classic repost",
    "core": false,
    "slug": "classic-repost",
    "oldSlugs": [
      "classic-repost"
    ],
    "postCount": 12,
    "description": {
      "markdown": "The EA Forum team sometimes shares older posts to the frontpage as \"classic reposts\", as a way to bring outstanding older content to the attention of those who haven't seen it. This tag collects those posts."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "desRRDiFE5snwMk77",
    "name": "Swiss Existential Risk Initiative",
    "core": false,
    "slug": "swiss-existential-risk-initiative",
    "oldSlugs": [
      "cheri-2021",
      "cheri",
      "cheri",
      "cheri"
    ],
    "postCount": 4,
    "description": {
      "markdown": "The **Swiss Existential Risk Initiative** (**CHERI**) is a project of Effective Altruism Switzerland that seeks \"to foster engagement from students, researchers and professors to produce meaningful work aiming at the mitigation of global catastrophic risks (GCRs) by providing skill, knowledge development, and a network.\"^[\\[1\\]](#fnurmjbzwxpfs)^\n\nRelated entries\n---------------\n\n[Stanford Existential Risks Initiative](https://forum.effectivealtruism.org/tag/stanford-existential-risks-initiative)\n\nExternal links\n--------------\n\n[Swiss Existential Risk Initiative](http://xrisk.ch/). Official website.\n\n1.  ^**[^](#fnrefurmjbzwxpfs)**^\n    \n    Swiss Existential Risk Initiative (2021) [Swiss Existential Risk Initiative](https://effectivealtruism.ch/swiss-existential-risk-initiative), *Effective Altruism Switzerland*."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "uZpXNGLgJX5BuHhcm",
    "name": "Option value",
    "core": false,
    "slug": "option-value",
    "oldSlugs": null,
    "postCount": null,
    "description": {
      "markdown": "**Option value** is the value of being able to delay a decision. Within effective altruism, the idea has been used to argue for the importance of reducing [existential risk](https://forum.effectivealtruism.org/tag/existential-risk), for deciding what choices effective altruists should make as a community, and for assessing [the value of different careers](https://forum.effectivealtruism.org/topics/career-choice). Option value may also be useful for looking at problems such as [differential progress](https://forum.effectivealtruism.org/tag/differential-progress) and [climate change](https://forum.effectivealtruism.org/tag/climate-change).\n\nThe idea of option value was adapted from finance, and is heavily used in environmental economics to measure the value of delaying an irreversible decision. It arises as a result of the uncertainty of future costs and benefits, the ability to gain information over time, and the ability to delay a decision. When done in the typical fashion, orthodox [cost-benefit analysis](https://forum.effectivealtruism.org/tag/cost-benefit-analysis) ignores the second and last of those three conditions. In more sophisticated applications of cost-benefit analysis, however, consideration of option value allows for a better-optimized decision-making process.\n\nA simple model\n--------------\n\nDixit and Pindyck gave the simple example of a firm that has the option to invest in a factory that would produce one \"widget\" per year starting on the year of investment.^[\\[1\\]](#fntbpj1viob7s)^ The current price of the widget is $200. Next year the price of the widget will either rise to $300 or go down to $100 and will stay at that level permanently. The chance of either outcome happening is 50%. The cost of the investment is $1600 and the interest rate is 10%. We find that the net present value (NPV) is equal to:\n\n\\\\(\\\\text{NPV} = -1600 + 200 + \\\\sum\\\\limits_{t=1}^\\\\infty \\\\frac{0.5 \\\\cdot 300+0.5 \\\\cdot 100}{1.1^t} = $600.\\\\)\n\nThe NPV is positive and a traditional cost-benefit analysis suggests the firm should invest. A more complex analysis, however, considers not only whether or not to invest, but also considers whether the investment would be better made now, or in the future. In this case, we can consider the option of waiting until next year when the price changes. Our traditional analysis now indicates that in a year, we would invest only if the price rises to $300:\n\n\\\\(\\\\text{NPV} = 0.5 \\\\cdot 0 + 0.5 \\\\cdot \\\\left\\[ \\\\frac{-1600}{1.1}+\\\\sum\\\\limits_{t=1}^\\\\infty \\\\frac{300}{1.1^t} \\\\right\\] = $773.\\\\)\n\nEven though we are now foregoing the current year sale at $200, we find that the NPV is higher if the firm delays the investment decision. Intuitively, we can understand that by ignoring the possibility of delaying the investment and of learning new information by waiting, the original NPV calculation boiled down to a \"now or never\" decision. In the second calculation, the firm waited and found the NPV to be higher. The option value of this investment decision is:\n\n\\\\(\\\\text{OV} = 773 - 600 = $173.\\\\)\n\nThat is, even if delaying the decision creates additional costs, the firm should be willing to pay up to $173 for the ability to delay the investment decision.\n\nWhich option value are we talking about?\n----------------------------------------\n\nThe term option value has been used in academic literature to describe three similar but distinct effects. To separate these different definitions, they are usually referred to as real option value, quasi-option value, and option value. This entry will focus on real option value and quasi option value. These two concepts are very similar, and at least one scholar has claimed that they are identical.^[\\[2\\]](#fns8stgyh7mr)^ The relationship between the two is explored further in the next section.\n\n**Real option value** \\- Real option value is defined as the difference between the expected value of an investment when the ability to wait and to resolve uncertainty over time is introduced, and the expected value of the investment according to NPV calculations without the possibility of waiting and investing later. The simple model presented above is an example of a real option value. Formulated by economists Avinash Dixit and Robert Pindyck, real option value is often denoted in the academic literature as OP\\\\(^{\\\\text{DP}}\\\\).^[\\[3\\]](#fnir9yrfs5aw)^\n\n**Quasi-Option value** \\- quasi-option value is the gain in value obtained by learning new information, conditional on delaying an irreversible development decision. For example, if a decision-maker is deciding whether to preserve or to develop a piece of land, where the future value of preservation is unknown and will be discovered at a later date, the quasi-option value is equal to the value of delaying the decision to develop until after the information pertaining to preservation is discovered. This concept was developed by Arrow and Fisher,^[\\[4\\]](#fnwchx23hrkaa)^ Henry,^[\\[5\\]](#fnvpfjkkei2u)^ and Hanemann,^[\\[6\\]](#fnztzqy9z0tvd)^ and is often referred to as OP\\\\(^{\\\\text{AFHH}}\\\\).\n\nAs expressed above, real option value and quasi-option value seem to be pointing to a very similar concept, approached from two different angles. We will examine the difference between them in the next section. First, however,  one last type of option value will be highlighted.\n\n**Option value** \\- Originally developed by B. A. Weisbrod^[\\[7\\]](#fnwdz1yfi9s6q)^ and expounded upon by Schmalensee,^[\\[8\\]](#fno6uzz3l4rue)^ Bishop,^[\\[9\\]](#fnto6gace80sm)^ and others. This type of option value is usually not given a prefix and is also significantly different than the definition of option value given here. Most notably it requires risk aversion, whereas real and quasi option value do not. Very briefly, this type of option value represents a willingness of consumers to pay a fee to preserve the option to use a service (such as a public park) even though they are likely to never use it.\n\nTo keep the introduction simple, this concept will not be addressed further. Still, readers should know that there are multiple different definitions with the name option value. Whil the term 'option value' has been used without a prefix to define the Weisbrod/Schmalensee approach, other authors often use it to describe the DP or AFHH option value. It is quite common for the term to be used without even being defined, sometimes leaving it unclear, or incorrectly assuming it does not matter which type of option value was intended. Because of this, it is prudent for readers to pay attention to what type of option value is being invoked, and for writers to ensure that they define the term before using it.\n\nThe difference between quasi and real option value\n--------------------------------------------------\n\nQuasi-Option value measures the gain in value achieved by taking into account the ability to delay an irreversible decision. This added value, however, does not necessarily mean that the decision should be deferred. Intuitively, the ability to delay the decision might strengthen the case to not invest now, but the investment might be so profitable that investing now is still the preferred option.\n\nOn the other hand, real option value **is** more clearly consolidates the economic analysis of the decision-maker's options. If the real option value is positive, then the decision-maker should delay the decision, while if it is negative, he should decide now.\n\nTo investigate this further, let's use some of AFHHs' notation.\n\nIn the AFHH model, The decision-maker can choose whether or not to undertake an irreversible investment decision. Let \\\\(d = 0\\\\) denote the decision not to invest, and \\\\(d = 1\\\\) denotes the decision to invest.\n\n\\\\(\\\\hat{V}(d)\\\\) is the expected value of the decision when given the option to delay it until more information is discovered.\n\n\\\\(V^*(d)\\\\) is the expected value of the decision when the decision-maker ignores the option of learning more information (similar to the original NPV estimate at the beginning of this article).\n\nThe decision tree according to the more basic decision rule \\\\(V^*(d)\\\\) is:\n\n![](https://i.postimg.cc/JhjhMDnQ/Prior-decision-tree.png)\n\nThe decision tree according to the decision rule that allows waiting \\\\(\\\\hat{V}(d)\\\\) is:\n\n![](https://i.postimg.cc/jdzKW0Ks/OV-decision-tree.png)\n\nQuasi-Option value is:\n\nOV\\\\(^{\\\\text{AFHH}} =\\\\hat{V}(0) - V^*(0)\\\\).\n\nIn other words, quasi-option value is equal to the added expected value of conservation by using the superior decision rule *conditional* on not developing (hence \\\\(d = 0\\\\) ). We can see this in the decision trees as the option of waiting on the complex decision tree minus the option of abandoning now on the simple decision tree.\n\nIt is important to note that the existence of quasi-option value doesn't imply that we should delay the investment decision. It is perfectly possible that, \\\\(\\\\hat{V}(1) > \\\\hat{V}(0)\\\\). In other words, the expected value of investing now may still be higher than the expected value of delaying the decision, even when taking into account the ability to learn more information.\n\nReal option value, when converted into the notation of AFHH is as follows,\n\n\\\\(\\\\text{OV}^{\\\\text{DP}} =\\\\hat{V}(0) - V^*(1).\\\\)\n\nIn this case, \\\\(\\\\hat{V}(0)\\\\) represents the value of keeping the option open to invest or not invest in the future (by not undertaking an irreversible decision now) whereas \\\\(V^*(1)\\\\) is the opportunity cost of this decision.\n\nIf the decision-maker chooses to undertake the irreversible investment, future information is of no value. Therefore,\n\n\\\\(\\\\hat{V}(1) = V^*(1)\\\\).\n\nFrom this, it follows that if the real option value is positive, then the decision-maker should delay the investment decision. As we saw, this is in opposition to quasi-option value.\n\n\\\\(\\\\text{OV}^{\\\\text{DP}} =  \\\\hat{V}(0) - V^*(1) = \\\\hat{V}(0) - \\\\hat{V}(1) > 0 \\\\Leftrightarrow \\\\hat{V}(0) > \\\\hat{V}(1)\\\\).\n\nWe see this on the complex decision tree as the top branch (waiting), minus the bottom branch (proceeding now).\n\nThe bottom line is that these terms are referring to the same analysis, but are different numbers. This is critical because reporting a positive option value is ambiguous until the terms are clarified. Real option value is the value of exercising the option; if it is positive, it is better to wait. Quasi-option value, as shown above, instead shows the value of the ability to wait as opposed to abandoning now \\\\((\\\\hat{V}(0) - V^*(0))\\\\). Even if this is positive, it may be better to develop now \\\\((\\\\hat{V}(1))\\\\), if the value of developing now is higher than the value of delaying \\\\((\\\\hat{V}(0))\\\\). This arguably makes real option value a clearer summary of the economic benefits and costs of delaying the investment decision.\n\nConnection to value of information\n----------------------------------\n\nOption value is closely related to the [value of information](https://forum.effectivealtruism.org/tag/value-of-information) (VoI). Value of information, stated simply, is how much a piece of information is worth to a decision-maker were he to discover it. Whereas option value looks at information that will be discovered passively, simply by waiting, value of information deals with information that can be discovered (relatively) instantly, but that one must actively expend resources to find.\n\nThe decision tree looks very similar to that of option value.\n\n![](https://i.postimg.cc/bNzgck6q/Vo-I-decision-tree.png)\n\nValue of information measures the value of obtaining information. VoI can equally be defined as the amount a rational decision-maker should be willing to pay for information before making a decision.\n\nOption value measures the value of the ability to wait and not make an irreversible decision. This value is derived (usually) from the information that is discovered by waiting. More specifically, real option value can be defined as the amount a rational decision-maker should be willing to pay for the option to delay an irreversible decision.\n\nFurther reading\n---------------\n\nPearce, David W., Susana Mourato & Giles Atkinson (2006) [(Quasi) option value](http://doi.org/10.1787/9789264010055-11-en), in *Cost-Benefit Analysis and the Environment*, Organisation for Economic Co-operation and Development, pp. 145–154.  \n*Pages 147 ff. give a useful example of option value.*\n\n1.  ^**[^](#fnreftbpj1viob7s)**^\n    \n    Dixit, Avinash K. & Robert S. Pindyck (1994) [*Investment under Uncertainty*](https://en.wikipedia.org/wiki/Special:BookSources/978-0-691-03410-2), Princeton: Princeton University Press, p. 27.\n    \n2.  ^**[^](#fnrefs8stgyh7mr)**^\n    \n    Fisher, Anthony C. (2000) [Investment under uncertainty and option value in environmental economics](http://doi.org/10.1016/S0928-7655(00)00025-7), *Resource and Energy Economics*, vol. 22, pp. 197–204.\n    \n3.  ^**[^](#fnrefir9yrfs5aw)**^\n    \n    Dixit & Pindyck, [*Investment under Uncertainty*](https://en.wikipedia.org/wiki/Special:BookSources/978-0-691-03410-2)*.*\n    \n4.  ^**[^](#fnrefwchx23hrkaa)**^\n    \n    Arrow, Kenneth J. & Anthony C. Fisher (1974) [Environmental preservation, uncertainty, and irreversibility](http://doi.org/10.2307/1883074), *The Quarterly Journal of Economics*, vol. 88, pp. 312–319.\n    \n5.  ^**[^](#fnrefvpfjkkei2u)**^\n    \n    Henry, Claude (1974) [Investment decisions under uncertainty: The “irreversibility effect”](https://www.jstor.org/stable/1815248), *American Economic Review*, vol. 64, pp. 1006–1012.\n    \n6.  ^**[^](#fnrefztzqy9z0tvd)**^\n    \n    Hanemann, W. Michael (1989) [Information and the concept of option value](http://doi.org/10.1016/0095-0696(89)90042-9), *Journal of Environmental Economics and Management*, vol. 16, pp. 23–37.\n    \n7.  ^**[^](#fnrefwdz1yfi9s6q)**^\n    \n    Weisbrod, Burton A. (1964) [Collective-consumption services of individual-consumption goods](http://doi.org/10.2307/1879478), *The Quarterly Journal of Economics*, vol. 78, p. 471.\n    \n8.  ^**[^](#fnrefo6uzz3l4rue)**^\n    \n    Schmalensee, Richard (1972) [Option demand and consumer’s surplus: Valuing price changes under uncertainty](https://www.jstor.org/stable/1815201), *American Economic Review*, vol. 62, pp. 813–824.\n    \n9.  ^**[^](#fnrefto6gace80sm)**^\n    \n    Bishop, Richard C. (1982) [Option value: An exposition and extension](http://doi.org/10.2307/3146073), *Land Economics*, vol. 58, pp. 1–15."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "cMnSqfwaA7un8tbAR",
    "name": "Effective Institutions Project",
    "core": false,
    "slug": "effective-institutions-project",
    "oldSlugs": [
      "effective-institutions-project"
    ],
    "postCount": 9,
    "description": {
      "markdown": "The **Effective Institutions Project** (formerly the **Improving Institutional Decision-Making Working Group**) is a working group focused on [improving institutional decision-making](https://forum.effectivealtruism.org/topics/improving-institutional-decision-making).\n\nFunding\n-------\n\nAs of July 2022, Effective Institutions Project  has received over $45,000 in grants from [Effective Altruism Funds](https://forum.effectivealtruism.org/topics/effective-altruism-funds).^[\\[1\\]](#fn80u27lez96h)^^[\\[2\\]](#fn6dmwipl975t)^\n\nFurther reading\n---------------\n\nMoss, Ian David, Laura Green & Vicky Clayton (2020) [Improving Institutional Decision-Making: a new working group](https://forum.effectivealtruism.org/posts/94QtuT4ss3RzrfH8A/improving-institutional-decision-making-a-new-working-group), *Effective Altruism Forum*, December 28.\n\nExternal links\n--------------\n\n[Effective Institutions Project](https://effectiveinstitutionsproject.org/). Official website.\n\nRelated entries\n---------------\n\n[improving institutional decision-making](https://forum.effectivealtruism.org/topics/improving-institutional-decision-making)\n\n1.  ^**[^](#fnref80u27lez96h)**^\n    \n    Effective Altruism Infrastructure Fund (2021) [May 2021: EA Infrastructure Fund grants](https://funds.effectivealtruism.org/funds/payouts/may-2021-ea-infrastructure-fund-grants), *Effective Altruism Funds*, May.\n    \n2.  ^**[^](#fnref6dmwipl975t)**^\n    \n    Effective Altruism Infrastructure Fund (2021) [May-August 2021: EA Infrastructure Fund grants](https://funds.effectivealtruism.org/funds/payouts/may-august-2021-ea-infrastructure-fund-grants), *Effective Altruism Funds*, August."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "RcuE86WhJQguD2whE",
    "name": "Communities adjacent to effective altruism",
    "core": false,
    "slug": "communities-adjacent-to-effective-altruism",
    "oldSlugs": null,
    "postCount": 8,
    "description": {
      "markdown": "**Communities adjacent to effective altruism** are groups related in one way or another to the [effective altruism community](https://forum.effectivealtruism.org/tag/community). Possible examples of such communities include the [rationality community](https://forum.effectivealtruism.org/tag/rationality-community), the [forecasting](https://forum.effectivealtruism.org/tag/forecasting) community, and the [progress studies](https://forum.effectivealtruism.org/tag/progress-studies) community;^[\\[1\\]](#fnq1p6j3s3gn)^ and, historically, the [Philosophic Radicals](https://forum.effectivealtruism.org/tag/philosophic-radicals), the [Society for the Diffusion of Useful Knowledge](https://forum.effectivealtruism.org/tag/society-for-the-diffusion-of-useful-knowledge), and [Fabianism](https://forum.effectivealtruism.org/tag/fabianism), among others.\n\nFurther reading\n---------------\n\nMac Aulay, Tara (2016) [The effective altruism ecosystem](https://www.youtube.com/watch?v=3S0SJgKytHE), *Effective Altruism Global*.\n\nRelated entries\n---------------\n\n[network building](https://forum.effectivealtruism.org/tag/network-building) | [social and intellectual movements](https://forum.effectivealtruism.org/tag/social-and-intellectual-movements)\n\n1.  ^**[^](#fnrefq1p6j3s3gn)**^\n    \n    Carey, Ryan (2021) [Name for the larger EA+adjacent ecosystem?](https://forum.effectivealtruism.org/posts/zA9Hr2xb7HszjtmMx/name-for-the-larger-ea-adjacent-ecosystem), *Effective Altruism Forum*, March 18."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "rC3ifbb2SChuhKPEm",
    "name": "Bayes' Theorem",
    "core": false,
    "slug": "bayes-theorem",
    "oldSlugs": null,
    "postCount": 2,
    "description": {
      "markdown": "**Bayes' Theorem** (also known as **Bayes' Rule** or **Bayes' Law**) is a law of probability that describes the proper way to incorporate new evidence into prior probabilities to form an updated probability estimate. It is commonly regarded as the foundation of consistent rational reasoning under uncertainty. Bayes' Theorem is named after Reverend Thomas Bayes, who proved the theorem in 1763.\n\nBayes' theorem commonly takes the form:\n\n\\\\\\[P(A|B)={\\\\frac{P(B|A)\\\\,P(A)}{P(B)}}\\\\\\]\n\nwhere A is the proposition of interest, B is the observed evidence, P(A) and P(B) are prior probabilities, and P(A|B) is the posterior probability of A.\n\nWith the posterior odds, the prior odds and the likelihood ratio written explicitly, the theorem reads:\n\n\\\\\\[\\\\frac{P(A|B)}{P(\\\\neg{} A|B)}=\\\\frac{P(A)}{P(\\\\neg{} A)}\\\\cdot\\\\frac{P(B|A)}{P(B|\\\\neg{} A)}\\\\\\]\n\nVisualization of Bayes' Rule\n----------------------------\n\n![](https://wiki.lesswrong.com/images/7/74/Bayes.png)\n\nFurther reading\n---------------\n\nAlexander Kruel (2010) [A guide to Bayes’ theorem – A few links](https://web.archive.org/web/20210820163156/http://kruel.co/2010/02/27/a-guide-to-bayes-theorem-a-few-links/), *Alexander Kruel’s Blog*, February 27.\n\nArbital (2021) [Bayes’ rule: Guide](https://arbital.com/p/bayes_rule_guide/), *Arbital*.\n\nBonilla, Oscar (2009) [Visualizing Bayes theorem](https://oscarbonilla.com/2009/05/visualizing-bayes-theorem/), *Oscar Bonilla’s Blog*, May 1.\n\nJoyce, James (2003) [Bayes’ theorem](https://plato.stanford.edu/entries/bayes-theorem/), *The Stanford Encyclopedia of Philosophy*, June 28 (updated 12 August 2021).\n\nOracle Aide (2012) [A Venn pie (using Venn pies to illustrate Bayes’ theorem)](https://oracleaide.wordpress.com/2012/12/26/a-venn-pie/), *Oracle Aide*, December 26.\n\nWikipedia (2002) [Bayes’ theorem](https://en.wikipedia.org/wiki/Bayes%27_theorem), *Wikipedia*, April 18 (updated 3 August 2021‎).\n\nWikipedia (2004) [Base rate fallacy](https://en.wikipedia.org/wiki/Base_rate_fallacy), *Wikipedia*, June 17 (updated 17 June 2021‎).\n\nYudkowsky, Eliezer S. (2003) [An intuitive explanation of Bayes’ theorem](https://www.yudkowsky.net/rational/bayes), *Eliezer S. Yudkowsky’s Website*, (updated 4 June 2006).\n\nRelated entries\n---------------\n\n[Bayesian epistemology](https://forum.effectivealtruism.org/tag/bayesian-epistemology) | [credence](https://forum.effectivealtruism.org/tag/credence) | [epistemology](https://forum.effectivealtruism.org/tag/epistemology)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "LB7a2ybtjqtLPdHPA",
    "name": "Singapore",
    "core": false,
    "slug": "singapore",
    "oldSlugs": null,
    "postCount": 1,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "AGS8ToNJpkDkTecCa",
    "name": "Macrostrategy",
    "core": false,
    "slug": "macrostrategy",
    "oldSlugs": null,
    "postCount": 9,
    "description": {
      "markdown": "**Macrostrategy** is the study of how present-day actions may influence the [long-term future](https://forum.effectivealtruism.org/tag/long-term-future) of humanity.^[\\[1\\]](#fn82uo5yn5q5b)^\n\nMacrostrategy as a field of research was pioneered by [Nick Bostrom](https://forum.effectivealtruism.org/tag/nick-bostrom), and it is a core focus area of the [Future of Humanity Institute](https://forum.effectivealtruism.org/tag/future-of-humanity-institute).^[\\[2\\]](#fny31bfvlj09i)^ Some authors distinguish between \"foundational\" and \"applied\" [global priorities research](https://forum.effectivealtruism.org/tag/global-priorities-research).^[\\[3\\]](#fngkke5k63u)^ On this distinction, macrostrategy may be regarded as closely related to the former. It is concerned with the assessment of general hypotheses such as the [hinge of history hypothesis](https://forum.effectivealtruism.org/tag/hinge-of-history-hypothesis), the [vulnerable world hypothesis](https://forum.effectivealtruism.org/tag/vulnerable-world-hypothesis) and the [technological completion conjecture](https://forum.effectivealtruism.org/tag/technological-completion-conjecture); the development of conceptual tools such as the concepts of [existential risk](https://forum.effectivealtruism.org/tag/existential-risk), of a [crucial consideration](https://forum.effectivealtruism.org/tag/crucial-consideration) and of [differential progress](https://forum.effectivealtruism.org/tag/differential-progress); and the analysis of the impacts and capabilities of future technologies such as [artificial general intelligence](https://forum.effectivealtruism.org/tag/human-level-artificial-intelligence), [whole brain emulation](https://forum.effectivealtruism.org/tag/whole-brain-emulation) and [atomically precise manufacturing](https://forum.effectivealtruism.org/tag/atomically-precise-manufacturing), but considered at a higher level of abstraction than is generally the case in [cause prioritization research](https://forum.effectivealtruism.org/tag/cause-prioritization).\n\nFurther reading\n---------------\n\nBostrom, Nick (2016) [Macrostrategy](https://www.youtube.com/watch?v=f9HvMLSD0jo), *Bank of England*, April 11.\n\nRelated entries\n---------------\n\n[crucial consideration](https://forum.effectivealtruism.org/tag/crucial-consideration) | [existential risk](https://forum.effectivealtruism.org/tag/existential-risk) | [global priorities research](https://forum.effectivealtruism.org/tag/global-priorities-research) |  [longtermism](https://forum.effectivealtruism.org/tag/longtermism) | [long-range forecasting](https://forum.effectivealtruism.org/tag/long-range-forecasting) | [long-term future](https://forum.effectivealtruism.org/tag/long-term-future) | [trajectory change](https://forum.effectivealtruism.org/tag/trajectory-changes)\n\n1.  ^**[^](#fnref82uo5yn5q5b)**^\n    \n    Bostrom, Nick (2021) [Home page](https://nickbostrom.com/), *Nick Bostrom’s Website*.\n    \n2.  ^**[^](#fnrefy31bfvlj09i)**^\n    \n    Future of Humanity Institute (2021) [Research areas](https://www.fhi.ox.ac.uk/research/research-areas/), *Future of Humanity Institute*.\n    \n3.  ^**[^](#fnrefgkke5k63u)**^\n    \n    Duda, Roman (2016) [Global priorities research](https://80000hours.org/problem-profiles/global-priorities-research/), *80,000 Hours*, April (updated July 2018)."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "M56s5yFziKvHFhNKk",
    "name": "Data science",
    "core": false,
    "slug": "data-science",
    "oldSlugs": null,
    "postCount": 32,
    "description": {
      "markdown": "**Data science** is \"an interdisciplinary field that uses scientific methods, processes, algorithms and systems to extract knowledge and insights from structured and unstructured data, and apply knowledge and actionable insights from data across a broad range of application domains.\"^[\\[1\\]](#fn6edqzxw99ck)^\n\nData science as a career\n------------------------\n\n[80,000 Hours](https://forum.effectivealtruism.org/tag/80-000-hours)' shallow profile rates data science a \"sometimes recommended\" career, especially for skill-building and [earning to give](https://forum.effectivealtruism.org/tag/earning-to-give).^[\\[2\\]](#fnmkwlu9m4zj)^\n\nFurther reading\n---------------\n\nDuda, Roman (2015) [Data science (for skill-building & earning to give)](https://80000hours.org/career-reviews/data-science/), *80,000 Hours*, June.\n\nRelated entries\n---------------\n\n[public interest technology](https://forum.effectivealtruism.org/tag/public-interest-technology) | [software engineering](https://forum.effectivealtruism.org/tag/software-engineering) | [statistical methods](https://forum.effectivealtruism.org/tag/statistical-methods)\n\n1.  ^**[^](#fnref6edqzxw99ck)**^\n    \n    Wikipedia (2012) [Data science](https://en.wikipedia.org/wiki/Data_science), *Wikipedia*, April 11 (updated 11 August 2021‎).\n    \n2.  ^**[^](#fnrefmkwlu9m4zj)**^\n    \n    Duda, Roman (2015) [Data science (for skill-building & earning to give)](https://80000hours.org/career-reviews/data-science/), *80,000 Hours*, June."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "muZkkRHHnmT6nuJEz",
    "name": "Charter Cities Institute",
    "core": false,
    "slug": "charter-cities-institute",
    "oldSlugs": null,
    "postCount": 1,
    "description": {
      "markdown": "The **Charter Cities Institute** (**CCI**, formerly the **Center for Innovative Governance Research**) is a nonprofit organization that promotes the creation of [charter cities](https://forum.effectivealtruism.org/tag/charter-cities). CCI's activities include producing popular content, publishing academic research, and providing technical assistance to governments and companies. CCI was founded in 2017.\n\nFunding\n-------\n\nAs of June 2022, CCI has received over $500,000 in funding from the [Survival and Flourishing](https://forum.effectivealtruism.org/topics/survival-and-flourishing) Fund.^[\\[1\\]](#fnaaaar46b4w)^^[\\[2\\]](#fn5y7jcx8g9cb)^^[\\[3\\]](#fnysxbwdcifim)^^[\\[4\\]](#fnxgmhvtfanjp)^ It has also received funding from [Tyler Cowen](https://forum.effectivealtruism.org/tag/tyler-cowen)'s [Emergent Ventures](https://forum.effectivealtruism.org/tag/emergent-ventures)^[\\[5\\]](#fnzaj8su8rile)^ and from [Vitalik Buterin](https://forum.effectivealtruism.org/tag/vitalik-buterin).^[\\[6\\]](#fnpd6wdqlh3pl)^\n\nFurther reading\n---------------\n\nWiblin, Robert & Keiran Harris (2019) [The team trying to end poverty by founding well-governed ‘Charter’ cities](https://80000hours.org/podcast/episodes/lutter-and-winter-chater-cities-innovative-governance/), *80,000 Hours*, March 31.\n\nExternal links\n--------------\n\n[Charter Cities Institute](https://www.chartercitiesinstitute.org/). Official website.\n\n[Apply for a job](https://chartercitiesinstitute.org/careers/).\n\n1.  ^**[^](#fnrefaaaar46b4w)**^\n    \n    Survival and Flourishing Fund (2018) [SFF-2019-Q4 S-process recommendations announcement](https://survivalandflourishing.fund/sff-2019-q4-recommendations), *Survival and Flourishing Fund*. \n    \n2.  ^**[^](#fnref5y7jcx8g9cb)**^\n    \n    Survival and Flourishing Fund (2019) [SFF-2020-H2 S-process recommendations announcement](https://survivalandflourishing.fund/sff-2020-h2-recommendations), *Survival and Flourishing Fund*.\n    \n3.  ^**[^](#fnrefysxbwdcifim)**^\n    \n    Survival and Flourishing Fund (2020) [SFF-2021-H1 S-process recommendations announcement](https://survivalandflourishing.fund/sff-2021-h1-recommendations), *Survival and Flourishing Fund*. \n    \n4.  ^**[^](#fnrefxgmhvtfanjp)**^\n    \n    Survival and Flourishing Fund (2021) [SFF-2022-H1 S-Process recommendations announcement](https://survivalandflourishing.fund/sff-2022-h1-recommendations), *Survival and Flourishing Fund*.\n    \n5.  ^**[^](#fnrefzaj8su8rile)**^\n    \n    Cowen, Tyler (2018) [Emergent Ventures grant recipients, the first cohort](https://marginalrevolution.com/marginalrevolution/2018/11/emergent-ventures-grant-recipients.html), *Marginal Revolution*, November 7.\n    \n6.  ^**[^](#fnrefpd6wdqlh3pl)**^\n    \n    Singh, Manish (2021) [Ethereum creator donates $1 billion worth of meme coins to India](https://social.techcrunch.com/2021/05/12/vitalik-buterin-donates-1-billion-worth-of-meme-coins-to-india-covid-relief-fund/), *TechCrunch*, May 12."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5dS2GzNmDpHbcKek7",
    "name": "Effective Altruism Anywhere",
    "core": false,
    "slug": "effective-altruism-anywhere",
    "oldSlugs": null,
    "postCount": 5,
    "description": {
      "markdown": "**Effective Altruism Anywhere** is a virtual group for members of the [effective altruism](https://forum.effectivealtruism.org/tag/effective-altruism) community. Although the group is open to anyone, it is primarily targeted at people who do not live in areas with active [effective altruism groups](https://forum.effectivealtruism.org/tag/effective-altruism-groups) or who cannot attend their meetings. It hosts several events every week, including discussion groups, social meetups, and co-working sessions.^[\\[1\\]](#fn00zyyc37x6q8e)^\n\nFurther reading\n---------------\n\nJurczyk, Marisa & Sami Mubarak (2021) [EA Anywhere: A year in review](https://forum.effectivealtruism.org/posts/5L5KPGprQAoBfmTHc/ea-anywhere-a-year-in-review), *Effective Altruism Forum*, August 8.\n\nExternal links\n--------------\n\n[Effective Altruism Anywhere](https://www.effectivealtruismanywhere.org/). Official website.\n\nRelated entries\n---------------\n\n[effective altruism groups](https://forum.effectivealtruism.org/tag/effective-altruism-groups)\n\n1.  ^**[^](#fnref00zyyc37x6q8e)**^\n    \n    Jurczyk, Marisa & Sami Mubarak (2021) [EA Anywhere: A year in review](https://forum.effectivealtruism.org/posts/5L5KPGprQAoBfmTHc/ea-anywhere-a-year-in-review), *Effective Altruism Forum*, August 8."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "zgcQYBGu4c4FQDw8d",
    "name": "Guarding Against Pandemics",
    "core": false,
    "slug": "guarding-against-pandemics",
    "oldSlugs": null,
    "postCount": 3,
    "description": {
      "markdown": "**Guarding Against Pandemics** (**GAP**) is a nonprofit organization that advocates for public investments in [pandemic preparedness](https://forum.effectivealtruism.org/tag/pandemic-preparedness).\n\nGAP was founded in July 2020. Its activities so far have included campaigning for U.S. President Joseph Biden's pandemic preparedness plan and funding a pandemic research [ballot initiative](https://forum.effectivealtruism.org/tag/ballot-initiative) in Denver, Colorado.^[\\[1\\]](#fndv29jbrs6zm)^\n\nFurther reading\n---------------\n\nMeyer, Robinson (2021) [Congress is slashing a $30 billion plan to fight the next pandemic](https://www.theatlantic.com/science/archive/2021/08/congress-slashing-plan-end-pandemics/619640/), *The Atlantic*, August 2.\n\nPiper, Kelsey (2021) [Does Congress know what it would take to stop the next pandemic?](https://www.vox.com/22589042/covid-pandemic-preparedness-vaccines-treatments-funding), *Vox*, July 24.\n\nExternal links\n--------------\n\n[Guarding Against Pandemics](https://www.againstpandemics.org/). Official website.\n\n1.  ^**[^](#fnrefdv29jbrs6zm)**^\n    \n    Guarding Against Pandemics (2021) [Guarding Against Pandemics](https://forum.effectivealtruism.org/posts/Btm562wDNEuWXj9Gk/guarding-against-pandemics), *Effective Altruism Forum,* September 18."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "S4WgtngcE652DQqjf",
    "name": "Pandemic preparedness",
    "core": false,
    "slug": "pandemic-preparedness",
    "oldSlugs": null,
    "postCount": 26,
    "description": {
      "markdown": "**Pandemic preparedness** is the set of measures to better prepare humanity for future pandemics.\n\nFurther reading\n---------------\n\nJamieson, Alice (2021) [Improving global pandemic preparedness by 2025](https://cms.wellcome.org/sites/default/files/2021-10/Wellcome-improving-global-pandemic-preparedness-2025.pdf), Wellcome.\n\nOrd, Toby (2020) [Why we need worst-case thinking to prevent pandemics](http://www.theguardian.com/science/2020/mar/06/worst-case-thinking-prevent-pandemics-coronavirus-existential-risk), *The Guardian*, March 6.\n\nVox Staff (2022) [Pandemic-Proof](https://www.vox.com/23001426/pandemic-proof), *Vox*, April 4.\n\nRelated entries\n---------------\n\n[biosecurity](https://forum.effectivealtruism.org/tag/biosecurity) | [COVID-19 pandemic](https://forum.effectivealtruism.org/tag/covid-19-pandemic) | [global catastrophic biological risk](https://forum.effectivealtruism.org/tag/global-catastrophic-biological-risk) | [vaccines](https://forum.effectivealtruism.org/tag/vaccines)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "SKLj2ayeEQPJXQRyj",
    "name": "80,000 Hours Podcast",
    "core": false,
    "slug": "80-000-hours-podcast",
    "oldSlugs": null,
    "postCount": 41,
    "description": {
      "markdown": "The **80,000 Hours Podcast** is a podcast by [80,000 Hours](https://forum.effectivealtruism.org/tag/80-000-hours). It is primarily hosted by Robert Wiblin, and produced by Keiran Harris. The first episode was released in June 2017.\n\nPast guests of the show include [Sam Bankman-Fried](https://forum.effectivealtruism.org/tag/sam-bankman-fried),^[\\[1\\]](#fnu4n2oq2zos)^ [Nick Beckstead](https://forum.effectivealtruism.org/tag/nick-beckstead),^[\\[2\\]](#fn47grgpwls3a)^ [Alexander Berger](https://forum.effectivealtruism.org/tag/alexander-berger),^[\\[3\\]](#fnh2rvh97gc7m)^ [Vitalik Buterin](https://forum.effectivealtruism.org/tag/vitalik-buterin),^[\\[4\\]](#fnjmcom6yrm9)^ [Bryan Caplan](https://forum.effectivealtruism.org/tag/bryan-caplan),^[\\[5\\]](#fnaas1k198r6b)^^[\\[6\\]](#fnzfa7ikfg4hr)^ [David Chalmers](https://forum.effectivealtruism.org/tag/david-chalmers),^[\\[7\\]](#fndaaglpuve5d)^ [Paul Christiano](https://forum.effectivealtruism.org/tag/paul-christiano),^[\\[8\\]](#fnpznus671oqa)^^[\\[9\\]](#fnrje144nn4wf)^ [Tyler Cowen](https://forum.effectivealtruism.org/tag/tyler-cowen),^[\\[10\\]](#fnwq9zh8ijm7)^ [Julia Galef](https://forum.effectivealtruism.org/tag/julia-galef),^[\\[11\\]](#fnjqfh428ej8c)^ [Hilary Greaves](https://forum.effectivealtruism.org/tag/hilary-greaves),^[\\[12\\]](#fn42bl0mefhmm)^^[\\[13\\]](#fnz5bh1uthvg)^ [Robin Hanson](https://forum.effectivealtruism.org/tag/robin-hanson),^[\\[14\\]](#fn5r7y21upzs)^ [Holden Karnofsky](https://forum.effectivealtruism.org/tag/holden-karnofsky),^[\\[15\\]](#fn0qfakaubr39n)^^[\\[16\\]](#fn85g39pz2zn7)^^[\\[17\\]](#fnwzwd4pgcvyg)^ [William MacAskill](https://forum.effectivealtruism.org/tag/william-macaskill),^[\\[18\\]](#fng9ufe23imbc)^^[\\[19\\]](#fn7vgxyoiqc5b)^ [Yew-Kwang Ng](https://forum.effectivealtruism.org/tag/yew-kwang-ng),^[\\[20\\]](#fnedpebti7j0i)^ [Toby Ord](https://forum.effectivealtruism.org/tag/toby-ord),^[\\[21\\]](#fnaqjlz06wy2q)^^[\\[22\\]](#fnz13my57t13)^ [Kelsey Piper](https://forum.effectivealtruism.org/tag/kelsey-piper),^[\\[23\\]](#fn9lt3pkkpiho)^ [Stuart Russell](https://forum.effectivealtruism.org/tag/stuart-russell),^[\\[24\\]](#fn1xwrr7k8yip)^ [Anders Sandberg](https://forum.effectivealtruism.org/tag/anders-sandberg),^[\\[25\\]](#fnqm83tqaoy5)^^[\\[26\\]](#fny22o0tbpya)^ [Carl Shulman](https://forum.effectivealtruism.org/tag/carl-shulman),^[\\[27\\]](#fnuvlfsgmbmdf)^ [Peter Singer](https://forum.effectivealtruism.org/tag/peter-singer),^[\\[28\\]](#fn6xiibvhm95d)^ [Philip Tetlock](https://forum.effectivealtruism.org/tag/philip-tetlock)^[\\[29\\]](#fnsnqduqbsfp)^^[\\[30\\]](#fn7goxys84qn8)^ and many other notable figures in the effective altruism community or [adjacent to it](https://forum.effectivealtruism.org/tag/communities-adjacent-to-effective-altruism).\n\nExternal links\n--------------\n\n[80,000 Hours Podcast](https://80000hours.org/podcast/). Official website.\n\nRelated entries\n---------------\n\n[80,000 Hours](https://forum.effectivealtruism.org/tag/80-000-hours) | [Hear This Idea](https://forum.effectivealtruism.org/tag/hear-this-idea) | [podcasts](https://forum.effectivealtruism.org/tag/podcasts)\n\n1.  ^**[^](#fnrefu4n2oq2zos)**^\n    \n    Wiblin, Robert & Keiran Harris (2022) [Sam Bankman-Fried on taking a high-risk approach to crypto and doing good](https://80000hours.org/podcast/episodes/sam-bankman-fried-high-risk-approach-to-crypto-and-doing-good/), *80,000 Hours*, April 14.\n    \n2.  ^**[^](#fnref47grgpwls3a)**^\n    \n    Wiblin, Robert (2017) [You want to do as much good as possible and have billions of dollars. What do you do?](https://80000hours.org/podcast/episodes/nick-beckstead-giving-billions/), *80,000 Hours*, October 11.\n    \n3.  ^**[^](#fnrefh2rvh97gc7m)**^\n    \n    Wiblin, Robert & Keiran Harris (2021) [Alexander Berger on improving global health and wellbeing in clear and direct ways](https://80000hours.org/podcast/episodes/alexander-berger-improving-global-health-wellbeing-clear-direct-ways/), *80,000 Hours*, July 12.\n    \n4.  ^**[^](#fnrefjmcom6yrm9)**^\n    \n    Wiblin, Robert & Keiran Harris (2019) [Vitalik Buterin on effective altruism, better ways to fund public goods, the blockchain’s problems so far, and how it could yet change the world](https://80000hours.org/podcast/episodes/vitalik-buterin-new-ways-to-fund-public-goods/), *80,000 Hours*, September 3.\n    \n5.  ^**[^](#fnrefaas1k198r6b)**^\n    \n    Wiblin, Robert & Keiran Harris (2018) [Economist Bryan Caplan thinks education is mostly pointless showing off. We test the strength of his case](https://80000hours.org/podcast/episodes/bryan-caplan-case-for-and-against-education/), *80,000 Hours*, May 22.\n    \n6.  ^**[^](#fnrefzfa7ikfg4hr)**^\n    \n    Wiblin, Robert & Keiran Harris (2022) [Bryan Caplan on why lazy parenting is actually OK](https://80000hours.org/podcast/episodes/bryan-caplan-parenting-workers-betting/), *80,000 Hours*, April 5.\n    \n7.  ^**[^](#fnrefdaaglpuve5d)**^\n    \n    Wiblin, Robert, Arden Koehler & Keiran Harris (2019) [David Chalmers on the nature and ethics of consciousness](https://80000hours.org/podcast/episodes/david-chalmers-nature-ethics-consciousness/), *80,000 Hours*, December 16.\n    \n8.  ^**[^](#fnrefpznus671oqa)**^\n    \n    Wiblin, Robert & Keiran Harris (2018) [Dr Paul Christiano on how OpenAI is developing real solutions to the “AI Alignment Problem”, and his vision of how humanity will progressively hand over decision-making to AI systems](https://80000hours.org/podcast/episodes/paul-christiano-ai-alignment-solutions/), *80,000 Hours*, October 2.\n    \n9.  ^**[^](#fnrefrje144nn4wf)**^\n    \n    Wiblin, Robert & Keiran Harris (2019) [Should we leave a helpful message for future civilizations, just in case humanity dies out?](https://80000hours.org/podcast/episodes/paul-christiano-a-message-for-the-future/), *80,000 Hours*, August 5.\n    \n10.  ^**[^](#fnrefwq9zh8ijm7)**^\n    \n    Wiblin, Robert & Keiran Harris (2018) [Economics prof Tyler Cowen says our overwhelming priorities should be maximising economic growth and making civilisation more stable. Is he right?](https://80000hours.org/podcast/episodes/tyler-cowen-stubborn-attachments/), *80,000 Hours*, October 17.\n    \n11.  ^**[^](#fnrefjqfh428ej8c)**^\n    \n    Wiblin, Robert (2017) [Julia Galef on making humanity more rational, what EA does wrong, and why Twitter isn’t all bad](https://80000hours.org/podcast/episodes/is-it-time-for-a-new-scientific-revolution-julia-galef-on-how-to-make-humans-smarter/), *80,000 Hours*, September 13.\n    \n12.  ^**[^](#fnref42bl0mefhmm)**^\n    \n    Wiblin, Robert & Keiran Harris (2018) [Philosopher Hilary Greaves on moral cluelessness, population ethics, probability within a multiverse, & harnessing the brainpower of academia to tackle the most important research questions](https://80000hours.org/podcast/episodes/hilary-greaves-global-priorities-institute/), *80,000 Hours*, October 23.\n    \n13.  ^**[^](#fnrefz5bh1uthvg)**^\n    \n    Koehler, Arden, Robert Wiblin & Keiran Harris (2020) [Hilary Greaves on Pascal’s mugging, strong longtermism, and whether existing can be good for us](https://80000hours.org/podcast/episodes/hilary-greaves-comparing-existence-and-non-existence/), *80,000 Hours*, October 21.\n    \n14.  ^**[^](#fnref5r7y21upzs)**^\n    \n    Wiblin, Robert & Keiran Harris (2018) [Why we have to lie to ourselves about why we do what we do, according to Prof Robin Hanson](https://80000hours.org/podcast/episodes/robin-hanson-on-lying-to-ourselves/), *80,000 Hours*, March 28.\n    \n15.  ^**[^](#fnref0qfakaubr39n)**^\n    \n    Wiblin, Robert & Keiran Harris (2018) [The world’s most intellectual foundation is hiring. Holden Karnofsky, founder of GiveWell, on how philanthropy can have maximum impact by taking big risks](https://80000hours.org/podcast/episodes/holden-karnofsky-open-philanthropy/), *80,000 Hours*, February 27.\n    \n16.  ^**[^](#fnref85g39pz2zn7)**^\n    \n    Wiblin, Robert & Keiran Harris (2021) [Holden Karnofsky on the most important century](https://80000hours.org/podcast/episodes/holden-karnofsky-most-important-century/), *80,000 Hours*, August 19.\n    \n17.  ^**[^](#fnrefwzwd4pgcvyg)**^\n    \n    Wiblin, Robert & Keiran Harris (2021) [Holden Karnofsky on building aptitudes and kicking ass](https://80000hours.org/podcast/episodes/holden-karnofsky-building-aptitudes-kicking-ass/), *80,000 Hours*, August 26.\n    \n18.  ^**[^](#fnrefg9ufe23imbc)**^\n    \n    Wiblin, Robert & Keiran Harris (2018) [Our descendants will probably see us as moral monsters. what should we do about that?](https://80000hours.org/podcast/episodes/will-macaskill-moral-philosophy/), *80,000 Hours*, January 19.\n    \n19.  ^**[^](#fnref7vgxyoiqc5b)**^\n    \n    Wiblin, Robert & Keiran Harris (2020) [Will MacAskill on the moral case against ever leaving the house, whether now is the hinge of history, and the culture of effective altruism](https://80000hours.org/podcast/episodes/will-macaskill-paralysis-and-hinge-of-history/), *80,000 Hours*, January 24.\n    \n20.  ^**[^](#fnrefedpebti7j0i)**^\n    \n    Wiblin, Robert & Keiran Harris (2018) [Prof Yew-Kwang Ng on ethics and how to create a much happier world](https://80000hours.org/podcast/episodes/yew-kwang-ng-anticipating-effective-altruism/), *80,000 Hours*, July 26.\n    \n21.  ^**[^](#fnrefaqjlz06wy2q)**^\n    \n    Wiblin, Robert (2017) [Toby Ord on why the long-term future of humanity matters more than anything else, and what we should do about it](https://80000hours.org/podcast/episodes/why-the-long-run-future-matters-more-than-anything-else-and-what-we-should-do-about-it/), *80,000 Hours*, September 6.\n    \n22.  ^**[^](#fnrefz13my57t13)**^\n    \n    Wiblin, Robert, Arden Koehler & Keiran Harris (2020) [Toby Ord on the precipice and humanity’s potential futures](https://80000hours.org/podcast/episodes/toby-ord-the-precipice-existential-risk-future-humanity/), *80,000 Hours*, March 7.\n    \n23.  ^**[^](#fnref9lt3pkkpiho)**^\n    \n    Wiblin, Robert & Keiran Harris (2019) [Can journalists still write about important things?](https://80000hours.org/podcast/episodes/kelsey-piper-important-advocacy-in-journalism/), *80,000 Hours*, February 27.\n    \n24.  ^**[^](#fnref1xwrr7k8yip)**^\n    \n    Wiblin, Robert & Keiran Harris (2020) [The flaws that make today’s AI architecture unsafe and a new approach that could fix it](https://80000hours.org/podcast/episodes/stuart-russell-human-compatible-ai/), *80,000 Hours*, June 22.\n    \n25.  ^**[^](#fnrefqm83tqaoy5)**^\n    \n    Wiblin, Robert & Keiran Harris (2018) [Where are the aliens? Three new resolutions to the fermi paradox. And how we could easily colonise the whole universe](https://80000hours.org/podcast/episodes/anders-sandberg-fermi-paradox/), *80,000 Hours*, May 8.\n    \n26.  ^**[^](#fnrefy22o0tbpya)**^\n    \n    Wiblin, Robert & Keiran Harris (2018) [Oxford university’s Dr Anders Sandberg on if dictators could live forever, the annual risk of nuclear war, solar flares, and more](https://80000hours.org/podcast/episodes/anders-sandberg-extending-life/), *80,000 Hours*, May 29.\n    \n27.  ^**[^](#fnrefuvlfsgmbmdf)**^\n    \n    Wiblin, Robert & Keiran Harris (2021) [Carl Shulman on the common-sense case for existential risk work and its practical implications](https://80000hours.org/podcast/episodes/carl-shulman-common-sense-case-existential-risks/), *80,000 Hours*, October 5.\n    \n28.  ^**[^](#fnref6xiibvhm95d)**^\n    \n    Wiblin, Robert, Arden Koehler & Keiran Harris (2019) [Peter Singer on being provocative, EA, how his moral views have changed, & rescuing children drowning in ponds](https://80000hours.org/podcast/episodes/peter-singer-advocacy-and-the-life-you-can-save/), *80,000 Hours*, December 5.\n    \n29.  ^**[^](#fnrefsnqduqbsfp)**^\n    \n    Wiblin, Robert (2017) [Prof Tetlock on predicting catastrophes, why keep your politics secret, and when experts know more than you](https://80000hours.org/podcast/episodes/prof-tetlock-predicting-the-future/), *80,000 Hours*, November 20.\n    \n30.  ^**[^](#fnref7goxys84qn8)**^\n    \n    Wiblin, Robert & Keiran Harris (2019) [Accurately predicting the future is central to absolutely everything. Professor Tetlock has spent 40 years studying how to do it better](https://80000hours.org/podcast/episodes/philip-tetlock-forecasting-research/), *80,000 Hours*, June 28."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "JsjbmcRdJheubi2F3",
    "name": "Risks from malevolent actors",
    "core": false,
    "slug": "risks-from-malevolent-actors",
    "oldSlugs": null,
    "postCount": 10,
    "description": {
      "markdown": "**Risks from malevolent actors** are risks posed by individuals with narcissistic, psychopathic, sadistic, or other socially harmful personality traits.^[\\[1\\]](#fnfexzlqdq47r)^\n\nMalevolence as a risk\n---------------------\n\nMalevolence may be operationalized as the general factor that accounts for the observed correlations between negative personality traits—the so-called \"dark core\" of personality.^[\\[2\\]](#fnpeolhr05nog)^ People who score unusually highly on this general factor could pose serious risks for the long-term future of humanity, including [existential risks](https://forum.effectivealtruism.org/tag/existential-risk) and [risks of astronomical suffering](https://forum.effectivealtruism.org/tag/s-risk). Such people appear to be more likely than average to attain positions of power in government and industry. And conditional on gaining this influence, they are also much more likely to cause major harm.\n\nThe types of risks malevolent actors in power might pose vary greatly. They include the spread of extremist and dangerous ideologies, the escalation of [great power conflicts](https://forum.effectivealtruism.org/tag/great-power-conflict), the formation of [totalitarian](https://forum.effectivealtruism.org/tag/great-power-conflict) regimes, and the undermining of public institutions and global [coordination](https://forum.effectivealtruism.org/tag/global-governance). Since malevolence has a [broadly](https://forum.effectivealtruism.org/tag/broad-vs-narrow-interventions) negative influence, it is probably best regarded as a [risk factor](https://forum.effectivealtruism.org/tag/existential-risk-factor) rather than as a distinct kind of risk.\n\nInterventions against malevolent actors\n---------------------------------------\n\nIn a pioneering essay on the topic, David Althaus and Tobias Baumann have proposed a number of interventions aimed at reducing risks from malevolent actors.^[\\[1\\]](#fnfexzlqdq47r)^ Their proposals include advancing the science of malevolence—by aligning constructs with the morally relevant forms of malevolence and by developing manipulation-proof measures of it—, and promoting political reforms to make the rise of malevolent actors less probable. Because malevolent actors likely pose the gravest risks in scenarios where they gain access to [transformative technology](https://forum.effectivealtruism.org/tag/transformative-development), the authors also propose shaping the development of those technologies so as to mitigate the harms posed by these individuals, such as by reducing the presence of malevolent traits in [AI](https://forum.effectivealtruism.org/tag/artificial-intelligence) training environments and by ensuring that candidates for [whole brain emulation](https://forum.effectivealtruism.org/tag/whole-brain-emulation) score low on measures of malevolence.\n\nEvaluation\n----------\n\n[80,000 Hours](https://forum.effectivealtruism.org/tag/80-000-hours) rates risks from malevolent actors a \"potential highest priority area\": an issue that, if more thoroughly examined, could rank as a top global challenge.^[\\[3\\]](#fnnee99omlg2)^\n\nFurther reading\n---------------\n\nAlthaus, David & Tobias Baumann (2020) [Reducing long-term risks from malevolent actors](https://forum.effectivealtruism.org/posts/LpkXtFXdsRd4rG8Kb/reducing-long-term-risks-from-malevolent-actors), *Effective Altruism Forum*, April 29.\n\nRelated entries\n---------------\n\n[existential risk](https://forum.effectivealtruism.org/tag/existential-risk) | [existential risk factor](https://forum.effectivealtruism.org/tag/existential-risk-factor) | [s-risk](https://forum.effectivealtruism.org/tag/s-risk) | [transformative development](https://forum.effectivealtruism.org/tag/transformative-development)\n\n1.  ^**[^](#fnreffexzlqdq47r)**^\n    \n    Althaus, David & Tobias Baumann (2020) [Reducing long-term risks from malevolent actors](https://forum.effectivealtruism.org/posts/LpkXtFXdsRd4rG8Kb/reducing-long-term-risks-from-malevolent-actors), *Effective Altruism Forum*, April 29.\n    \n2.  ^**[^](#fnrefpeolhr05nog)**^\n    \n    Moshagen, Morten, Benjamin E. Hilbig & Ingo Zettler (2018) [The dark core of personality](https://doi.org/10.1037/rev0000111), *Psychological Review*, vol. 125, pp. 656–688.\n    \n3.  ^**[^](#fnrefnee99omlg2)**^\n    \n    80,000 Hours (2022) [Our current list of pressing world problems](https://80000hours.org/problem-profiles/), *80,000 Hours*."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "PJhKREgw5K77hKXAp",
    "name": "Democracy",
    "core": false,
    "slug": "democracy",
    "oldSlugs": null,
    "postCount": 29,
    "description": {
      "markdown": "**Democracy** is a form of government in which power is vested in the people and exercised by them directly or through elected representatives.\n\nFurther reading\n---------------\n\nChristiano, Tom & Sameer Bajaj (2006) [Democracy](https://plato.stanford.edu/archives/spr2022/entries/democracy/), *The Stanford Encyclopedia of Philosophy*, July 27 (updated 3 March 2022).\n\nRelated entries\n---------------\n\n[ballot initiative](https://forum.effectivealtruism.org/tag/ballot-initiative) | [electoral reform](https://forum.effectivealtruism.org/tag/electoral-reform) | [improving institutional decision-making](https://forum.effectivealtruism.org/topics/improving-institutional-decision-making) | [safeguarding liberal democracy](https://forum.effectivealtruism.org/tag/safeguarding-liberal-democracy)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "ncGCrGbovWAq7R7c6",
    "name": "Safeguarding liberal democracy",
    "core": false,
    "slug": "safeguarding-liberal-democracy",
    "oldSlugs": null,
    "postCount": 18,
    "description": {
      "markdown": "**Safeguarding liberal democracy** refers to attempts to improve, promote or protect liberal forms of [democratic](https://forum.effectivealtruism.org/tag/democracy) government, characterized by universal suffrage, competitive elections, a market economy, separation of powers, the rule of law, and respect for human rights.\n\nEvaluation\n----------\n\nAccording to [80,000 Hours](https://forum.effectivealtruism.org/topics/80-000-hours), liberal democracies seem to foster [economic growth](https://forum.effectivealtruism.org/tag/economic-growth), [promote peace](https://forum.effectivealtruism.org/tag/peace-and-conflict-studies) and spearhead innovation to a greater degree than do other forms of government tried so far.^[\\[1\\]](#fn4nrwiy5zcl)^ Many alternatives to liberal democracy, on both ends of the political spectrum, have historically been associated with great human suffering: [totalitarian](https://forum.effectivealtruism.org/tag/totalitarianism) regimes, in particular, are estimated to have been responsible for the deaths of over 125 million people in the 20th century alone, mostly in the Soviet Union, Nazi Germany, and communist China.^[\\[2\\]](#fnn2luk9j65v)^ Furthermore, a report from [Rethink Priorities](https://forum.effectivealtruism.org/topics/rethink-priorities) argues that many of the attributes of liberal democracy are conducive to intermediate [longtermist](https://forum.effectivealtruism.org/tag/longtermism) goals, such as reduced [great power conflict](https://forum.effectivealtruism.org/tag/great-power-conflict), [moral circle expansion](https://forum.effectivealtruism.org/tag/moral-circle-expansion-1), and the [flourishing of effective altruism](https://forum.effectivealtruism.org/tag/building-effective-altruism-1).^[\\[3\\]](#fnaugo12rygyw)^ For these and other reasons, safeguarding liberal democracy is considered a promising cause by some members of the [effective altruism](https://forum.effectivealtruism.org/topics/effective-altruism) community; as of June 2022, 80,000 Hours rates it a \"potential highest priority\"—an issue that, if more thoroughly examined, could rank as a top global challenge.^[\\[4\\]](#fnrf98t0d7jw)^\n\nFurther reading\n---------------\n\nWiblin, Robert & Keiran Harris (2021) [Mike Berkowitz on keeping the U.S. a liberal democratic country](https://80000hours.org/podcast/episodes/mike-berkowitz-preserving-us-democracy/), *80,000 Hours*, April 20.\n\nRelated entries\n---------------\n\n[democracy](https://forum.effectivealtruism.org/tag/democracy) | [totalitarianism](https://forum.effectivealtruism.org/tag/totalitarianism)\n\n1.  ^**[^](#fnref4nrwiy5zcl)**^\n    \n    Koehler, Arden (2020) [Problem areas beyond 80,000 Hours’ current priorities](https://forum.effectivealtruism.org/posts/xoxbDsKGvHpkGfw9R/problem-areas-beyond-80-000-hours-current-priorities), *Effective Altruism Forum*, June 22.\n    \n2.  ^**[^](#fnrefn2luk9j65v)**^\n    \n    Bernholz, Peter (2000) [Totalitarianism](https://doi.org/10.1007/978-0-306-47828-4_201), in Charles K. Rowley & Friedrich Schneider (eds.) *The Encyclopedia of Public Choice*, Boston: Springer, pp. 565–569, p. 568.\n    \n3.  ^**[^](#fnrefaugo12rygyw)**^\n    \n    Barnes, Tom & Marie Buhl (2021) [Towards a longtermist framework for evaluating democracy-related interventions](https://forum.effectivealtruism.org/posts/f8Cc4XikFGMdrZJAa/towards-a-longtermist-framework-for-evaluating-democracy-1), *Effective Altruism Forum*, July 28.\n    \n4.  ^**[^](#fnrefrf98t0d7jw)**^\n    \n    80,000 Hours (2021) [Our current list of the most important world problems](https://80000hours.org/problem-profiles/), *80,000 Hours*."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "wSxFZMMhoo8zKrBHM",
    "name": "Sam Bankman-Fried",
    "core": false,
    "slug": "sam-bankman-fried",
    "oldSlugs": null,
    "postCount": 11,
    "description": {
      "markdown": "**Sam Bankman-Fried** (born 6 March 1992) is an American trader, entrepreneur, and philanthropist. He is the founder and CEO of FTX, a cryptocurrency derivatives exchange valued at $32 billion,^[\\[1\\]](#fnrs6xuabvvxk)^ and of Alameda Research, a quantitative trading firm. \n\nBackground\n----------\n\nBankman-Fried was born and raised in Stanford, California. He and his younger brother Gabe were introduced to moral philosophy at a young age by his parents, both  [consequentialists](https://forum.effectivealtruism.org/tag/consequentialism) and professors at Stanford Law School. The brothers eventually became \"take-no-prisoners utilitarians\".^[\\[4\\]](#fn26m4ru4vxh4j)^\n\nAs a high school student, Bankman-Fried attended Canada/USA Mathcamp, a summer program for mathematically talented youth. There he met his future business partners Sam Trabucco (co-CEO of Alameda)^[\\[5\\]](#fnd9sq1vpiaua)^ and Gary Wang (co-founder and CTO of FTX).^[\\[6\\]](#fnpbm7ng87t7c)^\n\nDuring his third year as a physics major at the Massachusetts Institute of Technology, Bankman-Fried was exposed to effective altruism at a talk by [William MacAskill](https://forum.effectivealtruism.org/tag/william-macaskill) on the ethics of [career choice](https://forum.effectivealtruism.org/tag/career-choice).^[\\[7\\]](#fnaafef847jok)^ Back then, he was considering doing [direct work](https://forum.effectivealtruism.org/tag/direct-work) for an organization focused on improving [farmed animal welfare](https://forum.effectivealtruism.org/tag/farmed-animal-welfare). However, he was persuaded by the argument that, for someone like him, [earning to give](https://forum.effectivealtruism.org/tag/earning-to-give) was a career with a much higher degree of [personal fit](https://forum.effectivealtruism.org/tag/personal-fit). By working in finance, he expected to donate enough money to pay for the salaries of several charity employees, each of whom roughly as impactful as he would himself be in that [role](https://forum.effectivealtruism.org/tag/role-impact). As he reasoned, \"I would probably make enough money that the [Humane League](https://forum.effectivealtruism.org/tag/the-humane-league) could hire many people and accomplish much more than it would if I went to work for them and started handing out leaflets.\"^[\\[8\\]](#fn8il778sv0bf)^\n\nUpon graduating from MIT, Bankman-Fried joined Jane Street Capital, a quantitative trading firm and liquidity provider, where he worked as a trader and designed the firm's automated off-exchange trading system. After serving briefly as director of development at the [Centre for Effective Altruism](https://forum.effectivealtruism.org/tag/centre-for-effective-altruism-1), he founded Alameda Research in 2017 and FTX two years later.\n\nPhilanthropic work\n------------------\n\nBankman-Fried's fortune has been described as \"the product of a long-nurtured utilitarian worldview\",^[\\[9\\]](#fnvozdw2fin7)^ and Bankman-Fried himself has been nicknamed \"the Bentham of crypto\".^[\\[10\\]](#fn1g2gjgazsjc)^\n\nWhile working at Jane Street, Bankman-Fried gave about half of his salary to charity, primarily to animal causes.^[\\[11\\]](#fnnwg6ic35xie)^ In 2020, he donated $5.2 million to a committee supporting Joseph Biden, becoming one of the president's top donors.^[\\[9\\]](#fnvozdw2fin7)^ More recently, he has contributed to organizations working in [global poverty](https://forum.effectivealtruism.org/tag/global-poverty), [climate change](https://forum.effectivealtruism.org/tag/climate-change), [AI safety](https://forum.effectivealtruism.org/tag/ai-safety), [biosecurity](https://forum.effectivealtruism.org/tag/biosecurity) and [pandemic preparedness](https://forum.effectivealtruism.org/tag/pandemic-preparedness).^[\\[12\\]](#fnu3csydsz6un)^ As of February 2022, Bankman-Fried has given away between $50 and 100 million,^[\\[13\\]](#fn0qgz0khd25d8)^ and has stated that he intends to donate the vast majority of his fortune to [longtermist](https://forum.effectivealtruism.org/tag/longtermism) causes eventually.^[\\[13\\]](#fn0qgz0khd25d8)^^[\\[14\\]](#fnr0z0h6fxy5f)^^[\\[15\\]](#fnyfv7ov2397)^^[\\[11\\]](#fnnwg6ic35xie)^\n\nThe [FTX Foundation](https://forum.effectivealtruism.org/tag/ftx-foundation) donates 1% of all fees collected by FTX, and has funded projects to mitigate [climate change](https://forum.effectivealtruism.org/tag/climate-change) and [promote effective altruism](https://forum.effectivealtruism.org/tag/building-effective-altruism-1). Alameda Research originally required all its employees to donate at least 50% of their salaries to effective charities, although the policy has since been revised to help attract more talent.^[\\[5\\]](#fnd9sq1vpiaua)^\n\nIn February 2022, the FTX Foundation launched the [Future Fund](https://forum.effectivealtruism.org/tag/future-fund), a philanthropic fund that makes grants and investments to [ambitious](https://forum.effectivealtruism.org/tag/ambition) projects focused on improving humanity's [long-term prospects](https://forum.effectivealtruism.org/tag/long-term-future). The Fund plans to distribute at least $100 million, and potentially up to $1 billion, by the end of 2022.^[\\[16\\]](#fn0pl44hj3s45o)^\n\nBankman-Fried is a [vegan](https://forum.effectivealtruism.org/tag/dietary-change), and has been a member of [Giving What We Can](https://forum.effectivealtruism.org/tag/giving-what-we-can) since August 2016^[\\[17\\]](#fndkue1s281nh)^ and a signatory to the [Giving Pledge](https://forum.effectivealtruism.org/topics/giving-pledge) since June 2022.^[\\[18\\]](#fnza733b65e39)^^[\\[19\\]](#fnvn9de92agtj)^\n\nFurther reading\n---------------\n\n80,000 Hours (2021) [Sam Bankman-Fried](https://80000hours.org/stories/sam-bankman-fried/), *80,000 Hours*.\n\nCowen, Tyler (2022) [Sam Bankman-Fried on arbitrage and altruism](https://conversationswithtyler.com/episodes/sam-bankman-fried/), *Conversations with Tyler*, March 10.\n\nEhrlich, Steven (2021) [Meet the world's richest 29-year-old: How Sam Bankman-Fried made a record fortune in the crypto frenzy](https://www.forbes.com/sites/stevenehrlich/2021/10/06/the-richest-under-30-in-the-world-all-thanks-to-crypto/), *Forbes*, October 10.\n\nGabriele, Mario (2021) [FTX trilogy, part 1: The prince of risk](https://www.readthegeneralist.com/briefing/ftx-1), *The Generalist*, August 1.\n\nHarris, Sam (2021) [Earning to give: a conversation with Sam Bankman-Fried](https://www.samharris.org/podcasts/making-sense-episodes/271-earning-to-give), *Making Sense*, December 24.\n\nWiblin, Robert & Keiran Harris (2022) [Sam Bankman-Fried on taking a high-risk approach to crypto and doing good](https://80000hours.org/podcast/episodes/sam-bankman-fried-high-risk-approach-to-crypto-and-doing-good/), *80,000 Hours*, April 14.\n\nRelated entries\n---------------\n\n[blockchain technology](https://forum.effectivealtruism.org/topics/blockchain-technology) | [classical utilitarianism](https://forum.effectivealtruism.org/tag/classical-utilitarianism) | [earning to give](https://forum.effectivealtruism.org/tag/earning-to-give) | [FTX Foundation](https://forum.effectivealtruism.org/tag/ftx-foundation) | [Future Fund](https://forum.effectivealtruism.org/tag/future-fund)\n\n1.  ^**[^](#fnrefrs6xuabvvxk)**^\n    \n    Szalay, Eva (2022) [FTX valued at $32bn as blue-chip investors pile into crypto groups](https://www.ft.com/content/a8714df8-a408-48de-a2a9-c5295332a0be), *Financial Times*, January 31.\n    \n2.  ^**[^](#fnref5wirhxmsdad)**^\n    \n    Forbes (2022) [Sam Bankman-Fried](https://www.forbes.com/profile/sam-bankman-fried/), *Forbes*, February 18.\n    \n3.  ^**[^](#fnref1t2g5tre713)**^\n    \n    Parloff, Roger (2021) [Portrait of a 29-year-old billionaire: Can Sam Bankman-Fried make his risky crypto business work?](https://finance.yahoo.com/news/ftx-ceo-sam-bankman-fried-profile-085444366.html), *Yahoo Finance*, August 12.\n    \n4.  ^**[^](#fnref26m4ru4vxh4j)**^\n    \n    Fried, Barbara H. (2020) [*Facing up to Scarcity: The Logic and Limits of Nonconsequentialist Thought*](https://en.wikipedia.org/wiki/Special:BookSources/978-0-19-884787-8), Oxford: Oxford University Press, p. xv.\n    \n5.  ^**[^](#fnrefd9sq1vpiaua)**^\n    \n    Huang, Kari McMahon, Vicky Ge (2021) [Here are 10 of the most surprising little-known facts we learned about the 29-year-old crypto billionaire Sam Bankman-Fried after months speaking to his closest friends, family, and colleagues](https://www.businessinsider.com/crypto-trading-billionaire-sam-bankman-fried-ftx-alameda-surprising-facts-2021-12), *Business Insider*, December 28.\n    \n6.  ^**[^](#fnrefpbm7ng87t7c)**^\n    \n    Bankman-Fried, Sam (2020) [Raising the bar](https://blog.ftx.com/blog/raising-the-bar/), *FTX Research*, May 17.\n    \n7.  ^**[^](#fnrefaafef847jok)**^\n    \n    Gabriele, Mario (2021) [FTX trilogy, part 1: The prince of risk](https://www.readthegeneralist.com/briefing/ftx-1), *The Generalist*, August 1.\n    \n8.  ^**[^](#fnref8il778sv0bf)**^\n    \n    Gose, Ben (2013) [A new donor movement seeks to put data ahead of passion](https://www.philanthropy.com/article/a-new-donor-movement-seeks-to-put-data-ahead-of-passion/), *The Chronicle of Philanthropy*, November 3.\n    \n9.  ^**[^](#fnrefvozdw2fin7)**^\n    \n    Wallace, Benjamin (2021) [The mysterious cryptocurrency magnate who became one of Biden's biggest donors](https://nymag.com/intelligencer/2021/02/sam-bankman-fried-biden-donor.html), *Intelligencer*, February 2.\n    \n10.  ^**[^](#fnref1g2gjgazsjc)**^\n    \n    Doherty, Brendan (2021) [Icon: The untold story of crypto billionaire Sam Bankman-Fried](https://www.forbes.com/sites/bdoherty/2021/05/16/icon-the-untold-story-of-crypto-billionaire-sam-bankman-fried/), *Forbes*, May 16.\n    \n11.  ^**[^](#fnrefnwg6ic35xie)**^\n    \n    Faux, Zeke (2022) [A 30-year-old crypto billionaire wants to give his fortune away](https://www.bloomberg.com/news/features/2022-04-03/sam-bankman-fried-ftx-s-crypto-billionaire-who-wants-to-give-his-fortune-away), *Bloomberg.com*, April 3.\n    \n12.  ^**[^](#fnrefu3csydsz6un)**^\n    \n    Schleifer, Theodore (2022) [The notorious S.B.F.](https://puck.news/the-notorious-s-b-f/), *Puck*, February 16.\n    \n13.  ^**[^](#fnref0qgz0khd25d8)**^\n    \n    James, David (2022) [The founder of the FTX platform wants to donate his fortune during his lifetime](https://blazetrends.com/the-founder-of-the-ftx-platform-wants-to-donate-his-fortune-during-his-lifetime/), *Blaze Trends*, February 13.\n    \n14.  ^**[^](#fnrefr0z0h6fxy5f)**^\n    \n    Kavate, Michael (2021) [A booming crypto platform makes its first foray into climate funding](https://www.insidephilanthropy.com/home/2021/8/31/a-booming-crypto-platform-makes-its-first-foray-into-climate-philanthropy-heres-what-its-backing-so-far), *Inside Philanthropy*, August 31.\n    \n15.  ^**[^](#fnrefyfv7ov2397)**^\n    \n    Chatterley, Julia (2021) [\"It's incumbent upon us to give back\"](https://twitter.com/jchatterleyCNN/status/1471572604284715011), *CNN*, December 16.\n    \n16.  ^**[^](#fnref0pl44hj3s45o)**^\n    \n    Beckstead, Nick *et al.* (2022) [Announcing the Future Fund](https://ftxfuturefund.org/announcing-the-future-fund/), *Future Fund*.\n    \n17.  ^**[^](#fnrefdkue1s281nh)**^\n    \n    Giving What We Can (2022) [Our members](https://www.givingwhatwecan.org/about-us/members/), *Giving What We Can*.\n    \n18.  ^**[^](#fnrefza733b65e39)**^\n    \n    Bankman-Fried, Sam (2022) [I’m excited and honored to sign the Giving Pledge](https://twitter.com/SBF_FTX/status/1532072177415372801), *Twitter*, June 1. \n    \n19.  ^**[^](#fnrefvn9de92agtj)**^\n    \n    Bankman-Fried, Sam (2022) [Sam Bankman-Fried](https://givingpledge.org/pledger?pledgerId=445), *Giving Pledge*, June 1.\n    \n20.  ^**[^](#fnrefmi1mye3r9a)**^\n    \n    Chan, Michelle (2021) [Hong Kong's 29-year-old crypto billionaire: FTX's Sam Bankman-Fried](https://www.ft.com/content/5dc2a9c3-7a30-4256-b291-d7ffcf978bc7), *Financial Times*, July 4.\n    \n21.  ^**[^](#fnref2glpz6aoq7n)**^\n    \n    Bambysheva, Nina (2021) [Bitcoin alert: biggest private crypto deal ever is closed](https://www.forbes.com/sites/ninabambysheva/2021/07/20/bitcoin-exchange-led-by-worlds-richest-crypto-billionaire-raises-record-900-million/), *Forbes*, July 20."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "qfhG5EdhTr5FNRXoi",
    "name": "Job listing (closed)",
    "core": false,
    "slug": "job-listing-closed",
    "oldSlugs": null,
    "postCount": 113,
    "description": {
      "markdown": "Use the **job listing (closed)** tag for posts about job listings that are no longer open.\n\nRelated entries\n---------------\n\n[Job listing (open)](https://forum.effectivealtruism.org/tag/job-listing-open)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "jgF5zh8BuEr3XuGKS",
    "name": "Job profile",
    "core": false,
    "slug": "job-profile",
    "oldSlugs": null,
    "postCount": 35,
    "description": {
      "markdown": "**Job profile** posts involve people writing about their jobs — how they got the job, what they do all day, and other notes that might be valuable to people interested in similar work.\n\nFurther reading\n---------------\n\nGertler, Aaron (2021) [You should write about your job](https://forum.effectivealtruism.org/posts/nf72oiJddwDhoJ4QH/you-should-write-about-your-job), *Effective Altruism Forum*, July 19.  \n*More information on this project.*"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "op8zZnCBphniNztyh",
    "name": "Tobacco control",
    "core": false,
    "slug": "tobacco-control",
    "oldSlugs": null,
    "postCount": 7,
    "description": {
      "markdown": "**Tobacco control** projects aim to reduce rates of tobacco use through a variety of strategies. These might include:\n\n*   Monitoring tobacco use and prevention policies\n*   Protecting people from tobacco use\n*   Offering help to quit tobacco use\n*   Warning about the dangers of tobacco\n*   Enforcing bans on tobacco advertising, promotion and sponsorship\n*   Raising taxes on tobacco\n\nFurther reading\n---------------\n\nCharity Entrepreneurship (2016) [Tobacco taxation](https://www.charityentrepreneurship.com/blog/tobacco-taxation), *Charity Entrepreneurship*, April 19.\n\nMilam, Per-Erik (2015) [Is tobacco control a “best-buy” for the developing world?](https://www.givingwhatwecan.org/post/2015/09/tobacco-control-best-buy-developing-world/), *Giving What We Can*, September 7.\n\nOpen Philanthropy (2013) [Tobacco control in low- and middle-income countries](https://www.openphilanthropy.org/research/cause-reports/tobacco-control), *Open Philanthropy*, November."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "ifia9octjLeEz5Yfc",
    "name": "Defense in depth",
    "core": false,
    "slug": "defense-in-depth",
    "oldSlugs": null,
    "postCount": 6,
    "description": {
      "markdown": "**Defense in depth** (**DiD**) (sometimes spelled **defense-in-depth** and **defense in-depth**) is the use of multiple defense layers to protect against an external threat. The concept was originally developed in the context of military strategy, but has since been applied to many other domains.^[\\[1\\]](#fngv08foee9f)^\n\nDefense in depth against human extinction\n-----------------------------------------\n\nA recent paper by Owen Cotton-Barratt, Max Daniel and [Anders Sandberg](https://forum.effectivealtruism.org/tag/anders-sandberg) proposes a defense in depth approach for handling risks of [human extinction](https://forum.effectivealtruism.org/tag/human-extinction).^[\\[2\\]](#fnpd6kt06eieb)^ Such risks may be analyzed as involving three successive stages in the unfolding of a terminal event: the origination of the catastrophe, its attainment of global scale, and the destruction of everyone alive. To each of these three threats (*origin*, *scaling* and *endgame*) corresponds a particular type of defense layer: *prevention*, *response* and *resilience*. Besides serving to clarify the nature of the risks involved, this analysis can assist efforts to prioritize the allocation of resources among the different defense layers. In particular, if the threats that the layers protect against interact multiplicatively with one another, the impact on overall risk reduction of strengthening a particular layer will depend on the *relative* size of the change to the probability that the associated threat will get past it: halving the risk posed by a threat reduces total risk by the same degree regardless of the absolute magnitude of the change.\n\nFurther reading\n---------------\n\nCotton-Barratt, Owen, Max Daniel & Anders Sandberg (2020) [Defence in depth against human extinction: Prevention, response, resilience, and why they all matter](https://doi.org/10.1111/1758-5899.12786), *Global Policy*, vol. 11, pp. 271–282.\n\nRelated entries\n---------------\n\n[estimation of existential risk](https://forum.effectivealtruism.org/tag/estimation-of-existential-risk) | [existential risk](https://forum.effectivealtruism.org/tag/existential-risk) | [existential risk factor](https://forum.effectivealtruism.org/tag/existential-risk-factor) | [existential security](https://forum.effectivealtruism.org/tag/existential-security) | [global catastrophic risk](https://forum.effectivealtruism.org/tag/global-catastrophic-risk) | [unknown existential risk](https://forum.effectivealtruism.org/tag/unknown-existential-risk) | [unprecedented risks](https://forum.effectivealtruism.org/tag/unprecedented-risks)\n\n1.  ^**[^](#fnrefgv08foee9f)**^\n    \n    Muehlhauser, Luke (2021) [A personal take on longtermist AI governance](https://forum.effectivealtruism.org/posts/M2SBwctwC6vBqAmZW/a-personal-take-on-longtermist-ai-governance), *Effective Altruism Forum*, July 21, fn. 19.\n    \n2.  ^**[^](#fnrefpd6kt06eieb)**^\n    \n    Cotton-Barratt, Owen, Max Daniel & Anders Sandberg (2020) [Defence in depth against human extinction: Prevention, response, resilience, and why they all matter](https://doi.org/10.1111/1758-5899.12786), *Global Policy*, vol. 11, pp. 271–282."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "pNzHWdpePMeTWBJbZ",
    "name": "Semiconductors",
    "core": false,
    "slug": "semiconductors",
    "oldSlugs": null,
    "postCount": 10,
    "description": {
      "markdown": "Further reading\n---------------\n\nCenter for Security and Emerging Technology (2021) [Tag archive: Semiconductors](https://cset.georgetown.edu/article/tag/semiconductors/), *Center for Security and Emerging Technology*.\n\nRelated entries\n---------------\n\n[AI alignment](https://forum.effectivealtruism.org/tag/ai-alignment) | [AI governance](https://forum.effectivealtruism.org/topics/ai-governance) | [AI forecasting](https://forum.effectivealtruism.org/tag/ai-forecasting) | [AI risk](https://forum.effectivealtruism.org/tag/ai-risk) | [Center for Security and Emerging Technology](https://forum.effectivealtruism.org/tag/center-for-security-and-emerging-technology) | [China](https://forum.effectivealtruism.org/tag/china) | [scientific progress](https://forum.effectivealtruism.org/tag/scientific-progress) | [United States](https://forum.effectivealtruism.org/topics/united-states)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "YopStk3kZ3TcwCYCG",
    "name": "Request for proposal",
    "core": false,
    "slug": "request-for-proposal",
    "oldSlugs": [
      "request-for-proposals"
    ],
    "postCount": 7,
    "description": {
      "markdown": "A **request for proposal** (**RFP**) is a written statement in which an organization solicits, usually through a bidding process, proposals from prospective suppliers by specifying the good or service that it intends to purchase.\n\nRFPs have been contrasted with [consultancies](https://forum.effectivealtruism.org/tag/consultancy) as two alternative models for how organizations in the effective altruism community can fund external projects.^[\\[1\\]](#fnizaaz0m5jlj)^\n\nFurther reading\n---------------\n\nOpen Philanthropy (2017) [April 2017 update on grant to the Future of Life Institute for Artificial Intelligence RFP](https://www.openphilanthropy.org/research/april-2017-update-on-grant-to-the-future-of-life-institute-for-artificial-intelligence-rfp/), *Open Philanthropy*, April.\n\nOpen Philanthropy (2017) [Scientific research grants from the 2016 NIH Transformative Research Award RFP](https://www.openphilanthropy.org/research/scientific-research-grants-from-the-2016-nih-transformative-research-award-rfp/), *Open Philanthropy*, October.\n\nOpen Philanthropy (2020) [Foundation for Food and Agriculture Research — farm animal welfare research (2020)](https://www.openphilanthropy.org/focus/us-policy/farm-animal-welfare/foundation-food-and-agriculture-research-farm-animal-welfare-research-2020), *Open Philanthropy*, April.\n\nRelated entries\n---------------\n\n[effective altruism funding](https://forum.effectivealtruism.org/tag/effective-altruism-funding) | [take action](https://forum.effectivealtruism.org/tag/take-action) | [grantmaking](https://forum.effectivealtruism.org/tag/grantmaking) | [prize](https://forum.effectivealtruism.org/tag/prize) | [requests (open)](https://forum.effectivealtruism.org/tag/requests-open)\n\n1.  ^**[^](#fnrefizaaz0m5jlj)**^\n    \n    Muehlhauser, Luke (2021) [EA needs consultancies](https://forum.effectivealtruism.org/posts/CwFyTacABbWuzdYwB/ea-needs-consultancies), *Effective Altruism Forum*, June 28."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "mMecqGKjsurhiSKHa",
    "name": "Disentanglement research",
    "core": false,
    "slug": "disentanglement-research",
    "oldSlugs": null,
    "postCount": 18,
    "description": {
      "markdown": "**Disentanglement research** refers to \"a type of research that involves disentangling ideas and questions in a “pre-paradigmatic” area where the core concepts, questions, and methodologies are under-defined.\" When coining the term, [Carrick Flynn](https://forum.effectivealtruism.org/topics/carrick-flynn) gave [Nick Bostrom](https://forum.effectivealtruism.org/tag/nick-bostrom) as an example of someone skilled at this type of research.^[\\[1\\]](#fnvhwg2gixn2o)^\n\nRelated entries\n---------------\n\n[research methods](https://forum.effectivealtruism.org/tag/research-methods) | [scalably using labour](https://forum.effectivealtruism.org/tag/scalably-using-labour)\n\n1.  ^**[^](#fnrefvhwg2gixn2o)**^\n    \n    Flynn, Carrick (2017) [Personal thoughts on careers in AI policy and strategy](https://web.archive.org/web/20210622160148/https://forum.effectivealtruism.org/posts/RCvetzfDnBNFX7pLH/personal-thoughts-on-careers-in-ai-policy-and-strategy), *Effective Altruism Forum*, September 27."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "diqCxjodiFupyAN8z",
    "name": "Sanku",
    "core": false,
    "slug": "sanku",
    "oldSlugs": null,
    "postCount": null,
    "description": {
      "markdown": "**Sanku - Project Healthy Children** (formerly **Project Healthy Children**) is a nonprofit organization that works to prevent [micronutrient deficiencies](https://forum.effectivealtruism.org/topics/micronutrient-deficiency) by helping countries design and implement micronutrient fortification programs.\n\nEvaluation\n----------\n\n[GiveWell](https://forum.effectivealtruism.org/tag/givewell) tentatively estimates that effective fortification programs may be in the cost-effectiveness range of their priority programs. Because of insufficient information about Sanku at the time of evaluation, GiveWell is uncertain about the cost-effectiveness of Sanku's work specifically.^[\\[1\\]](#fncdh93nqgwxi)^ Sanku was a GiveWell [standout charity](https://forum.effectivealtruism.org/tag/givewell#Standout_charities) from 2016  to 2021, when that designation was discontinued\n\nAs of May 2022, Sanku has received $600,000 in funding from [Open Philanthropy](https://forum.effectivealtruism.org/tag/open-philanthropy),^[\\[2\\]](#fnivkeyz22um)^ and is also featured in [The Life You Can Save](https://forum.effectivealtruism.org/tag/the-life-you-can-save)'s list of \"best charities\".^[\\[3\\]](#fnaixq3whyvkf)^\n\nExternal links\n--------------\n\n[Sanku - Project Healthy Children](https://projecthealthychildren.org/). Official website.\n\n[Donate to Sanku](https://donate.projecthealthychildren.org/give/106741/#!/donation/checkout).\n\nRelated entries\n---------------\n\n[global health and development](https://forum.effectivealtruism.org/topics/global-health-and-development) | [micronutrient deficiency](https://forum.effectivealtruism.org/topics/micronutrient-deficiency)\n\n1.  ^**[^](#fnrefcdh93nqgwxi)**^\n    \n    GiveWell (2016) [Project Healthy Children](https://www.givewell.org/charities/project-healthy-children), *GiveWell*, September (updated March 2017).\n    \n2.  ^**[^](#fnrefivkeyz22um)**^\n    \n    Open Philanthropy (2022) [Grants database: Project Healthy Children](https://www.openphilanthropy.org/grants/?q=&organization-name=project-healthy-children), *Open Philanthropy*. \n    \n3.  ^**[^](#fnrefaixq3whyvkf)**^\n    \n    The Life You Can Save (2021) [Best charities](https://www.thelifeyoucansave.org/best-charities/), *The Life You Can Save*."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "jBbz7yEbCcNEWr4Ek",
    "name": "Living Goods",
    "core": false,
    "slug": "living-goods",
    "oldSlugs": null,
    "postCount": 1,
    "description": {
      "markdown": "**Living Goods** is a nonprofit organization that runs a network of micro-entrepreneurs, known as \"community health providers\" (CHPs), who sell health and household goods door-to-door in Uganda and Kenya and provide basic health counseling.\n\nEvaluation\n----------\n\nA cluster [randomized controlled trial](https://forum.effectivealtruism.org/tag/randomized-controlled-trials) found a 27% mortality decrease among young children in Ugandan villages visited by CHPs.^[\\[1\\]](#fndunwqch63x5)^ Partly on the basis of these findings, in 2014 [GiveWell](https://forum.effectivealtruism.org/tag/givewell) estimated Living Goods' cost per life saved to be in the range of $10,000,^[\\[2\\]](#fnd8korq50g58)^ and rated it a [standout charity](https://forum.effectivealtruism.org/tag/givewell#Standout_charities) every year until 2021, when that designation was discontinued.^[\\[2\\]](#fnd8korq50g58)^ An update published in 2016 noted a few developments in the intervening period, but did not revise that estimate or alter its rating.^[\\[3\\]](#fn3x3q74msno6)^\n\nAs of July 2022, Living Goods has received $1.1 in funding from [Open Philanthropy](https://forum.effectivealtruism.org/tag/open-philanthropy),^[\\[4\\]](#fnqaatziguqs)^ and is also featured in [The Life You Can Save](https://forum.effectivealtruism.org/tag/the-life-you-can-save)'s list of \"best charities\".^[\\[5\\]](#fn98xzojhjxo7)^\n\nFurther reading\n---------------\n\nCapriati, Marinella (2019) [A conversation with Martina Björkman Nyqvist](https://docs.google.com/document/d/1SxD4pMQbh_0eAsr-4oMoDNFTTnAJT4ShkjOsPtvEH-o/edit?usp=embed_facebook), *GiveWell*, August 19.\n\nExternal links\n--------------\n\n[Living Goods](https://livinggoods.org/). Official website.\n\n[Apply for a job](https://livinggoods.applytojob.com/apply/).\n\n[Donate to Living Goods](https://livinggoods.org/give/).\n\n1.  ^**[^](#fnrefdunwqch63x5)**^\n    \n    Björkman Nyqvist, Martina *et al.* (2019) [Reducing child mortality in the last mile: Experimental evidence on community health promoters in Uganda](https://doi.org/10.1257/app.20170201), *American Economic Journal: Applied Economics*, vol. 11, pp. 155–192.\n    \n2.  ^**[^](#fnrefd8korq50g58)**^\n    \n    GiveWell (2014) [Living Goods](https://www.givewell.org/charities/living-goods), *GiveWell*, November.\n    \n3.  ^**[^](#fnref3x3q74msno6)**^\n    \n    GiveWell (2016) [Living Goods – Mid-2016 update](https://www.givewell.org/charities/living-goods/all-content/2016-update), *GiveWell*, September.\n    \n4.  ^**[^](#fnrefqaatziguqs)**^\n    \n    Open Philanthropy (2022) [Grants database: Living Goods](https://www.openphilanthropy.org/grants/?q=&organization-name=living-goods), *Open Philanthropy*.\n    \n5.  ^**[^](#fnref98xzojhjxo7)**^\n    \n    The Life You Can Save (2021) [Living Goods](https://www.thelifeyoucansave.org/best-charities/living-goods/), *The Life You Can Save*."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "mWb44yE6Lv2pTuBv3",
    "name": "Iodine Global Network",
    "core": false,
    "slug": "iodine-global-network",
    "oldSlugs": null,
    "postCount": null,
    "description": {
      "markdown": "**Iodine Global Network** (**IGN**, formerly **ICCIDD**) is a nonprofit organization focused on reducing iodine deficiency by advocating salt iodization programs at the national and global levels.\n\nEvaluation\n----------\n\n[GiveWell](https://forum.effectivealtruism.org/tag/givewell) rated IGN a [standout charity](https://forum.effectivealtruism.org/tag/givewell#Standout_charities) from 2014 to 2021, when that designation was discontinued.^[\\[1\\]](#fn3plvwn6mu28)^ As of July 2022, IGN has received $1.1 million in funding from [Open Philanthropy](https://forum.effectivealtruism.org/tag/open-philanthropy),^[\\[2\\]](#fnah00hlgypzf)^ and is also featured in [The Life You Can Save](https://forum.effectivealtruism.org/tag/the-life-you-can-save)'s list of \"best charities\".^[\\[3\\]](#fne9pq1ovujdu)^\n\nFurther reading\n---------------\n\nGiveWell (2014) [Iodine Global Network (IGN)](https://www.givewell.org/charities/IGN), *GiveWell*, December.\n\nExternal links\n--------------\n\n[Iodine Global Network](https://www.ign.org/). Official website.\n\n[Donate to the Iodine Global Network](https://www.ign.org/Donation).\n\n1.  ^**[^](#fnref3plvwn6mu28)**^\n    \n    GiveWell (2014) [Iodine Global Network (IGN)](https://www.givewell.org/charities/IGN), *GiveWell*, December.\n    \n2.  ^**[^](#fnrefah00hlgypzf)**^\n    \n    Open Philanthropy (2022) [Grants database: Iodine Global Network](https://www.openphilanthropy.org/grants/?q=&organization-name=iodine-global-network), *Open Philanthropy.*\n    \n3.  ^**[^](#fnrefe9pq1ovujdu)**^\n    \n    The Life You Can Save (2021) [Iodine Global Network](https://www.thelifeyoucansave.org/best-charities/iodine-global-network/), *The Life You Can Save*."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "Tw7N5fMyqDAZCHqvo",
    "name": "Zusha!",
    "core": false,
    "slug": "zusha",
    "oldSlugs": null,
    "postCount": null,
    "description": {
      "markdown": "**Zusha!** (the Swahili expression for \"protest!\") is a road safety campaign launched in Kenya in 2015 by Georgetown University's Initiative on Innovation, Development, and Evaluation in partnership with local authorities. The program aims to reduce road accidents through stickers placed inside of public service vehicles with messages encouraging passengers to complain directly to reckless drivers.\n\nEvaluation\n----------\n\n[GiveWell](https://forum.effectivealtruism.org/tag/givewell) rated Zusha! a  [standout charity](https://forum.effectivealtruism.org/tag/givewell#Standout_charities) from 2018 to 2021, when that designation was discontinued.^[\\[1\\]](#fn7e3h8t0d4d6)^ The organization is also featured in [The Life You Can Save](https://forum.effectivealtruism.org/tag/the-life-you-can-save)'s list of \"best charities\".^[\\[2\\]](#fn2aeqb3apo59)^\n\nFurther reading\n---------------\n\nGeorgetown University Initiative on Innovation, Development and Evaluation (2015) [*gui2de* launches Zusha! road safety campaign nationwide in Kenya](https://web.archive.org/web/20200811102610/https://gui2de.georgetown.edu/announcement/zusha/), *Georgetown University Initiative on Innovation, Development and Evaluation*, May 8.\n\nExternal links\n--------------\n\n[Donate to Zusha!](https://tlycs.networkforgood.com/projects/53356-zusha-zusha)\n\n1.  ^**[^](#fnref7e3h8t0d4d6)**^\n    \n    Rosenberg, Josh (2018) [Announcing Zusha! as a standout charity](https://blog.givewell.org/2018/06/21/announcing-zusha-as-a-standout-charity/), *The GiveWell Blog*, June 21.\n    \n2.  ^**[^](#fnref2aeqb3apo59)**^\n    \n    The Life You Can Save (2021) [Zusha!](https://www.thelifeyoucansave.org/best-charities/zusha/), *The Life You Can Save*."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "eijJMGKyTrmHSwv5R",
    "name": "Global Alliance for Improved Nutrition",
    "core": false,
    "slug": "global-alliance-for-improved-nutrition",
    "oldSlugs": null,
    "postCount": null,
    "description": {
      "markdown": "The **Global Alliance for Improved Nutrition** (**GAIN**) is a foundation based in Switzerland focused on reducing world malnutrition.\n\nEvaluation\n----------\n\nGAIN implements a wide range of programs, only one of which—its universal salt iodization (USI) program—has been evaluated by [GiveWell](https://forum.effectivealtruism.org/tag/givewell). In GiveWell's assessment, the cost-effectiveness range of salt iodization is very close to that of their priority programs,^[\\[1\\]](#fn03e4o3kcezu)^ though they are less certain about the cost-effectiveness of GAIN's USI program.^[\\[2\\]](#fny7avpfwl12r)^\n\n USI was a GiveWell [standout charity](https://forum.effectivealtruism.org/tag/givewell#Standout_charities) from 2014 to 2021, when that designation was discontinued. USI is also one of the three organizations focused on [education](https://forum.effectivealtruism.org/topics/education) recommended by [Founders Pledge](https://forum.effectivealtruism.org/tag/founders-pledge)^[\\[3\\]](#fnxrx8e5bwuee)^ and is featured by [The Life You Can Save](https://forum.effectivealtruism.org/tag/the-life-you-can-save) in their list of \"best charities\".^[\\[4\\]](#fn72exhrvvsfn)^\n\nAs of July 2022, GAIN has received $1.1 million in grants from [Open Philanthropy](https://forum.effectivealtruism.org/topics/open-philanthropy).^[\\[5\\]](#fnkavzmmyfsc)^\n\nExternal links\n--------------\n\n[Global Alliance for Improved Nutrition](https://www.gainhealth.org/). Official website.\n\n[Apply for a job](https://www.gainhealth.org/careers/jobs).\n\n[Donate to GAIN](https://www.gainhealth.org/financials/support-us).\n\n1.  ^**[^](#fnref03e4o3kcezu)**^\n    \n    GiveWell (2014) [Salt iodization](https://www.givewell.org/international/technical/programs/salt-iodization), *GiveWell*, December (updated March 2021).\n    \n2.  ^**[^](#fnrefy7avpfwl12r)**^\n    \n    GiveWell (2016) [Global Alliance for Improved Nutrition (GAIN)—Universal Salt Iodization (USI)](https://www.givewell.org/charities/GAIN), *GiveWell*, November (updated May 2017).\n    \n3.  ^**[^](#fnrefxrx8e5bwuee)**^\n    \n    Calvert, Callum (2019) [Cause area report: education](https://founderspledge.com/research/fp-education), Founders Pledge.\n    \n4.  ^**[^](#fnref72exhrvvsfn)**^\n    \n    The Life You Can Save (2021) [Best charities](https://www.thelifeyoucansave.org/best-charities/), *The Life You Can Save*.\n    \n5.  ^**[^](#fnrefkavzmmyfsc)**^\n    \n    Open Philanthropy (2022) [Grants database: Global Alliance for Improved Nutrition](https://www.openphilanthropy.org/grants/?q=&organization-name=global-alliance-for-improved-nutrition), *Open Philanthropy*."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "tJ8bEazCqvDAP6nbe",
    "name": "Dispensers for Safe Water",
    "core": false,
    "slug": "dispensers-for-safe-water",
    "oldSlugs": null,
    "postCount": 2,
    "description": {
      "markdown": "**Dispensers for Safe Water** is a program run by [Evidence Action](https://forum.effectivealtruism.org/tag/evidence-action) that provides chlorine dispensers for decontamination of drinking water to reduce diarrhea and prevent associated deaths in young children.\n\nEvaluation\n----------\n\nA review by [GiveWell](https://forum.effectivealtruism.org/tag/givewell) found strong evidence for the effects of chlorination in reducing most diarrhea-causing microorganisms, but weaker evidence for a causal link between water chlorination programs and reductions in diarrhea and death among young children.^[\\[1\\]](#fnpxakivosrk)^  GiveWell estimates that the Dispensers for Safe Water program is in the same cost-effectiveness range as unconditional [cash transfer](https://forum.effectivealtruism.org/tag/cash-transfers) programs.^[\\[1\\]](#fnpxakivosrk)^ Dispensers for Safe Water was a GiveWell [standout charity](https://forum.effectivealtruism.org/tag/givewell#Standout_charities) from 2017 to 2021, when that designation was discontinued.\n\nIn 2022, GiveWell made a major update in their assessment of water quality interventions. The update was prompted by preliminary results from a meta-analysis of mortality data from [randomized controlled trials](https://forum.effectivealtruism.org/tag/randomized-controlled-trials) of water quality interventions conducted by [Michael Kremer](https://forum.effectivealtruism.org/tag/michael-kremer)'s team. Based on these results, GiveWell now estimates the mortality reduction in young children attributable to those interventions to be 14%, an increase of 11 percentage points from their previous estimate. In line with this update, GiveWell now considers Dispensers for Safe Water to be four to eight times as cost-effective as unconditional cash transfers. The re-evaluation also prompted GiveWell to recommend a grant of up to $64.7 million for the organization in January 2022. GiveWell also speculates that Dispensers for Safe Water or similar programs could use $350 million per year globally at equal cost-effectiveness.^[\\[2\\]](#fnwxntrxrkp6f)^\n\nFurther reading\n---------------\n\nGiveWell (2017) [Evidence Action’s Dispensers for Safe Water program](https://www.givewell.org/charities/dispensers-for-safe-water), *GiveWell*, November.\n\nKaplan, Miranda (2022) [A major update in our assessment of water quality interventions](https://blog.givewell.org/2022/04/06/water-quality-overview/), *The GiveWell Blog*, April 6.\n\nTabart, Chelsea (2018) [A conversation with Arjun Pant and Grace Morgan](https://files.givewell.org/files/conversations/Arjun_Pant_Grace_Morgan_06-20-18_(public).pdf), *GiveWell*, June 20.\n\nExternal links\n--------------\n\n[Dispensers for Safe Water](https://www.evidenceaction.org/dispensersforsafewater/). Official website.\n\n1.  ^**[^](#fnrefpxakivosrk)**^\n    \n    GiveWell (2017) [Evidence Action’s Dispensers for Safe Water program](https://www.givewell.org/charities/dispensers-for-safe-water), *GiveWell*, November.\n    \n2.  ^**[^](#fnrefwxntrxrkp6f)**^\n    \n    Kaplan, Miranda (2022) [A major update in our assessment of water quality interventions](https://blog.givewell.org/2022/04/06/water-quality-overview/), *The GiveWell Blog*, April 6."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "RCrAqWbGZPXfhzz4d",
    "name": "Food Fortification Initiative",
    "core": false,
    "slug": "food-fortification-initiative",
    "oldSlugs": null,
    "postCount": null,
    "description": {
      "markdown": "**Food Fortification Initiative** (**FFI**) is an organization that focuses on reducing [micronutrient deficiencies](https://forum.effectivealtruism.org/topics/micronutrient-deficiency) by helping country leaders plan, implement and monitor fortification programs.\n\nEvaluation\n----------\n\nA review by [GiveWell](https://forum.effectivealtruism.org/tag/givewell) concluded that effective micronutrient fortification programs could be as cost-effective as their top-rated programs. Due to difficulties in assessing FFI's causal role in implementing their programs, however, GiveWell has not attempted an explicit cost-effectiveness analysis of this organization's impact.\n\nBecause of these uncertainties, GiveWell rated FFI a [standout charity](https://forum.effectivealtruism.org/tag/givewell#Standout_charities)—an organization with exceptional potential for impact which does not fully meet all of the criteria to be rated a top charity. As GiveWell believes that further research is unlikely to resolve these uncertainties, it does not plan to conduct additional investigations on FFI for the time being.^[\\[1\\]](#fnirhowuwevw)^ FFI was a standout charity from 2016 to 2021, when that designation was discontinued. \n\nAs of July 2022, FFI has received $600,000 in grants from [Open Philanthropy](https://forum.effectivealtruism.org/topics/open-philanthropy).^[\\[2\\]](#fn0032jpsisrobc)^\n\nFurther reading\n---------------\n\nGiveWell (2016) [Food Fortification Initiative](https://www.givewell.org/charities/food-fortification-initiative), *GiveWell*, September (updated March 2017).\n\nExternal links\n--------------\n\n[Food Fortification Initiative](https://www.ffinetwork.org/). Official website.\n\n[Donate to Food Fortification Initiative](https://www.ffinetwork.org/ffidonate).\n\nRelated entries\n---------------\n\n[global health and development](https://forum.effectivealtruism.org/topics/global-health-and-development) | [micronutrient deficiency](https://forum.effectivealtruism.org/topics/micronutrient-deficiency)\n\n1.  ^**[^](#fnrefirhowuwevw)**^\n    \n    GiveWell (2016) [Food Fortification Initiative](https://www.givewell.org/charities/food-fortification-initiative), *GiveWell*, September (updated March 2017).\n    \n2.  ^**[^](#fnref0032jpsisrobc)**^\n    \n    Open Philanthropy (2022) [Grants database: Food Fortification Initiative](https://www.openphilanthropy.org/grants/?q=&organization-name=food-fortification-initiative), *Open Philanthropy*."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "Eowrw7xJWx5ANj8RE",
    "name": "Precision Development",
    "core": false,
    "slug": "precision-development",
    "oldSlugs": null,
    "postCount": null,
    "description": {
      "markdown": "**Precision Development** (**PxD**), previously **Precision Agriculture for Development** (**PAD**), is a nonprofit organization that provides personalized advice through mobile phone notifications to farmers living in extreme [poverty](https://forum.effectivealtruism.org/tag/global-poverty).\n\nEvaluation\n----------\n\nA preliminary report by [GiveWell](https://forum.effectivealtruism.org/tag/givewell) concluded that it costs PxD around $1.80 per year to reach a household with its messaging. GiveWell rated PxD a [standout charity](https://forum.effectivealtruism.org/tag/givewell#Standout_charities) from 2020 to  2021, when that designation was discontinued.^[\\[1\\]](#fnrxeniynzeas)^\n\nFurther reading\n---------------\n\nGiveWell (2020) [Precision Development](https://www.givewell.org/charities/precision-agriculture-for-development-November-2020-version), *GiveWell*, November.\n\nGiveWell (2020) [Precision Development (mobile-based agricultural advice)](https://www.givewell.org/international/technical/programs/precision-agriculture-for-development), *GiveWell*, November.\n\nPrecision Development (2021) [X marks the spot! Introducing Precision Development](https://precisionag.org/introducing-precision-development/), *Precision Development*, May 21.\n\nExternal links\n--------------\n\n[Precision Development](https://precisionag.org/). Official website.\n\n[Apply for a job](https://precisiondev.org/who-we-are/job-openings/).\n\n1.  ^**[^](#fnrefrxeniynzeas)**^\n    \n    GiveWell (2020) [Precision Development (mobile-based agricultural advice)](https://www.givewell.org/international/technical/programs/precision-agriculture-for-development), *GiveWell*, November."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "TeppgEYmJoE5hoq3S",
    "name": "Giving and happiness",
    "core": false,
    "slug": "giving-and-happiness-1",
    "oldSlugs": null,
    "postCount": 10,
    "description": {
      "markdown": "**Giving and happiness** refers to the apparent causal relationship that exists between prosocial spending and subjective wellbeing.\n\nWhen a person donates money that they could have spent on themselves, the resulting change in wellbeing depends not only on the welfare loss from forgone self-interested spending, but also on possible welfare gains associated with the perception that the spending will help other people or promote a worthy cause. The net causal impact of giving on happiness can thus be analyzed as the sum of these two effects.\n\nWithin the [effective altruism](https://forum.effectivealtruism.org/tag/effective-altruism) community, the relationship between giving and happiness has attracted some attention because it bears on a number of debates of interest, such as debates concerning [altruistic motivation](https://forum.effectivealtruism.org/tag/altruistic-motivation) and the [demandingness of morality](https://forum.effectivealtruism.org/tag/altruistic-motivation).\n\nFurther reading\n---------------\n\nDalton, Max (2020) [Some extremely rough research on giving and happiness](https://forum.effectivealtruism.org/posts/iyFD6iLHYqhzjqHZs/some-extremely-rough-research-on-giving-and-happiness), *Effective Altruism Forum*, September 9.\n\nMacAskill, William, Andreas Mogensen & Toby Ord (2018) [Giving isn’t demanding](https://doi.org/10.1093/oso/9780190648879.003.0007), in Paul Woodruff (ed.) *The Ethics of Giving: Philosophers’ Perspectives on Philanthropy*, New York: Oxford University Press, pp. 178–203.\n\nPlant, Michael & Julian Hazell (2021) [Can money buy happiness? A review of new data](https://www.givingwhatwecan.org/post/2021/06/can-money-buy-happiness-a-review-of-new-data/), *Giving What We Can*, June 23."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "ufTv7KohrLmtAh2wC",
    "name": "Humane Slaughter Association",
    "core": false,
    "slug": "humane-slaughter-association",
    "oldSlugs": null,
    "postCount": null,
    "description": {
      "markdown": "The **Humane Slaughter Association** (**HSA**) is a British nonprofit that supports research, development and training to improve the welfare of [farmed animals](https://forum.effectivealtruism.org/tag/farmed-animal-welfare) during transportation and slaughter.\n\nEvaluation\n----------\n\nHSA is one of the two organizations with a focus on animal welfare recommended by [Raising for Effective Giving](https://forum.effectivealtruism.org/tag/raising-for-effective-giving).^[\\[1\\]](#fncenh1jwjxi9)^ It is also one of the two organizations within this cause area recommended by [Brian Tomasik](https://forum.effectivealtruism.org/tag/brian-tomasik).^[\\[2\\]](#fnud8owcrls0s)^\n\nAs of July 2022, HSA has received over $3.5 million in funding from [Open Philanthropy](https://forum.effectivealtruism.org/topics/open-philanthropy).^[\\[3\\]](#fnz17ejh0wks)^\n\nFurther reading\n---------------\n\nRaising for Effective Giving (2020) [Humane Slaughter Association](https://reg-charity.org/recommended-charities/humane-slaughter-association/), *Raising for Effective Giving*.\n\nExternal links\n--------------\n\n[Humane Slaughter Association](https://www.hsa.org.uk/). Official website.\n\n[Apply for a job](https://www.hsa.org.uk/about/careers).\n\n[Donate to the Humane Slaughter Association](https://www.hsa.org.uk/support/donate-on-line).\n\n1.  ^**[^](#fnrefcenh1jwjxi9)**^\n    \n    Raising for Effective Giving (2021) [Animal welfare](https://reg-charity.org/recommended-charities/animal-welfare/), *Raising for Effective Giving*.\n    \n2.  ^**[^](#fnrefud8owcrls0s)**^\n    \n    Tomasik, Brian (2015) [Why I support the Humane Slaughter Association](https://reducing-suffering.org/why-i-support-the-humane-slaughter-association/), *Essays on Reducing Suffering*, February 24 (updated 19 December 2018).\n    \n3.  ^**[^](#fnrefz17ejh0wks)**^\n    \n    Open Philanthropy (2022) [Grants database: Humane Slaughter Association](https://www.openphilanthropy.org/grants/?q=&organization-name=humane-slaughter-association), *Open Philanthropy*."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "HmgSZhvc3mfyiCb4X",
    "name": "Good Growth",
    "core": false,
    "slug": "good-growth",
    "oldSlugs": null,
    "postCount": 2,
    "description": {
      "markdown": "**Good Growth** is a research and design [consultancy](https://forum.effectivealtruism.org/tag/consultancy) that helps organizations use evidence to increase their social impact.\n\nExternal links\n--------------\n\n[Good Growth](https://www.goodgrowth.io/). Official website.\n\n[Apply for a job](https://www.goodgrowth.io/#team).\n\nRelated entries\n---------------\n\n[consultancy](https://forum.effectivealtruism.org/tag/consultancy)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "nPtxAWdzFhXtP3QFy",
    "name": "Consultancy",
    "core": false,
    "slug": "consultancy",
    "oldSlugs": null,
    "postCount": 18,
    "description": {
      "markdown": "A **consultancy** is a professional practice that provides expert advice in a particular field or subject.\n\nEffective altruism organizations are often constrained by the difficulty of accessing talented specialists when their needs fluctuate too rapidly to justify a new hire. Effective altruism consultancies have been proposed as a partial solution to this problem.^[\\[1\\]](#fna6yleyi2nj6)^ People capable of doing temporary work for a variety of organizations could be permanently employed by such consultancies. Possible examples of effective altruism consultancies include [Rethink Priorities](https://forum.effectivealtruism.org/tag/rethink-priorities), [Berkeley Existential Risk Initiative](https://forum.effectivealtruism.org/tag/berkeley-existential-risk-initiative) and [Good Growth](https://forum.effectivealtruism.org/tag/good-growth).\n\nFurther reading\n---------------\n\n80,000 Hours (2021) [All content tagged \"consulting\"](https://80000hours.org/topic/careers/consulting/), *80,000 Hours*.\n\nBeckstead, Nick (2015) [Management consulting (for skill-building & earning to give)](https://80000hours.org/career-reviews/management-consulting/), *80,000 Hours*, March (updated April 2015).\n\nMuehlhauser, Luke (2021) [EA needs consultancies](https://forum.effectivealtruism.org/posts/CwFyTacABbWuzdYwB/ea-needs-consultancies), *Effective Altruism Forum*, June 28.\n\nTodd, Benjamin (2019) [Before committing to management consulting, consider directly entering priority paths, policy, startups, and other options](https://80000hours.org/articles/alternatives-to-consulting/), *80,000 Hours*, November.\n\nRelated entries\n---------------\n\n[career choice](https://forum.effectivealtruism.org/tag/career-choice) | [Effective Altruism and Consulting Network](https://forum.effectivealtruism.org/tag/effective-altruism-and-consulting-network) | [org strategy](https://forum.effectivealtruism.org/tag/org-strategy) | [scalably using labour](https://forum.effectivealtruism.org/tag/scalably-using-labour) | [working at EA vs. non-EA orgs](https://forum.effectivealtruism.org/tag/working-at-ea-vs-non-ea-orgs)\n\n1.  ^**[^](#fnrefa6yleyi2nj6)**^\n    \n    Muehlhauser, Luke (2021) [EA needs consultancies](https://forum.effectivealtruism.org/posts/CwFyTacABbWuzdYwB/ea-needs-consultancies), *Effective Altruism Forum*, June 28."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "uhJeftevWYofuogcC",
    "name": "Kidney donation",
    "core": false,
    "slug": "kidney-donation",
    "oldSlugs": null,
    "postCount": 13,
    "description": {
      "markdown": "**Kidney donation** is the process of a person allowing a kidney to be removed from their body and transplanted to another person, usually to treat kidney failure. Kidney donation has been argued to be a highly effective way to increase human health.\n\nA number of individuals involved with the effective altruism community or [adjacent to it](https://forum.effectivealtruism.org/tag/communities-adjacent-to-effective-altruism) have donated a kidney, including [Alexander Berger](https://forum.effectivealtruism.org/tag/alexander-berger),^[\\[1\\]](#fn22xndchk6mu)^ Jeremiah Johnson,^[\\[2\\]](#fnqu22vt244lb)^ Zell Kravinsky,^[\\[3\\]](#fnfe90nk9mtdt)^ [Dylan Matthews](https://forum.effectivealtruism.org/tag/dylan-matthews),^[\\[4\\]](#fn4ouux68uenh)^ Michael Page,^[\\[5\\]](#fn1r397tugsp9j)^ and others.^[\\[6\\]](#fnhjet9qnefyl)^\n\nFurther reading\n---------------\n\nGalef, Julia (2017) [The science and ethics of kidney donation (Dylan Matthews)](http://rationallyspeakingpodcast.org/177-the-science-and-ethics-of-kidney-donation-dylan-matthews/), *Rationally Speaking*, February 5.\n\nParker, Ian (2004) [The gift](https://www.newyorker.com/magazine/2004/08/02/the-gift-ian-parker), *The New Yorker*, July 25.\n\nRelated entries\n---------------\n\n[Waitlist Zero](https://forum.effectivealtruism.org/topics/waitlist-zero)\n\n1.  ^**[^](#fnref22xndchk6mu)**^\n    \n    Berger, Alexander (2011) [Why selling kidneys should be legal](https://www.nytimes.com/2011/12/06/opinion/why-selling-kidneys-should-be-legal.html), *The New York Times*, December 5.\n    \n2.  ^**[^](#fnrefqu22vt244lb)**^\n    \n    Johnson, Jeremiah (2019) [Last week I donated my left kidney anonymously to a total stranger on the kidney waitlist. AMA!](https://www.reddit.com/r/IAmA/comments/c7u0lx/last_week_i_donated_my_left_kidney_anonymously_to/), *Reddit*, July 1.\n    \n3.  ^**[^](#fnreffe90nk9mtdt)**^\n    \n    Parker, Ian (2004) [The gift](https://www.newyorker.com/magazine/2004/08/02/the-gift-ian-parker), *The New Yorker*, July 25.\n    \n4.  ^**[^](#fnref4ouux68uenh)**^\n    \n    Matthews, Dylan (2017) [Why I gave my kidney to a stranger — and why you should consider doing it too](https://www.vox.com/science-and-health/2017/4/11/12716978/kidney-donation-dylan-matthews), *Vox*, April 11.\n    \n5.  ^**[^](#fnref1r397tugsp9j)**^\n    \n    Beard, Matt, Molly Daniels & Carl Smith (2017) [Would you donate your kidney to a stranger?](https://www.abc.net.au/radio/programs/shortandcurly/s5ep3/8941404), *Short & Curly*, September 13.\n    \n6.  ^**[^](#fnrefhjet9qnefyl)**^\n    \n    Singer, Peter (2015) [*The Most Good You Can Do: How Effective Altruism Is Changing Ideas about Living Ethically*](https://en.wikipedia.org/wiki/Special:BookSources/9780300180275), New Haven: Yale University Press, ch. 6."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "bTuCPLohLYPAeorzD",
    "name": "Resilient food",
    "core": false,
    "slug": "resilient-food",
    "oldSlugs": [
      "alternative-foods",
      "alternative-food"
    ],
    "postCount": 22,
    "description": {
      "markdown": "**Resilient food** (sometimes called **alternative food, ** **alternate food** or **emergency food**) and **resilient food solutions** are those foods, food production methods or interventions that would allow for significant food availability in the face of a global catastrophic food shock (GCFS). These solutions should be well-suited for contributing to an adequate food supply for the greatest number of people even in the worst scenarios, for example by being scalable and amenable to rapid production ramp-up.^[\\[1\\]](#fn24sheiuq7po)^ \n\nSuch food could be produced even if a [global catastrophe](https://forum.effectivealtruism.org/tag/global-catastrophic-risk) such as a [nuclear war](https://forum.effectivealtruism.org/tag/nuclear-warfare-1), [supervolcano](https://forum.effectivealtruism.org/tag/supervolcano) eruption, or [asteroid](https://forum.effectivealtruism.org/tag/asteroids) impact significantly reduces the amount of sunlight reaching the Earth's surface by releasing large quantities of sulfate aerosols, smoke or ash into the atmosphere.^[\\[1\\]](#fn24sheiuq7po)^  Examples of resilient foods for these abrupt sunlight reduction scenarios (ASRS) include seaweed and single-cell protein grown from natural gas.^[\\[2\\]](#fnfexj9bhaz6)^ Other examples of ASRS-resilient food solutions could be: relocation of cool tolerant crops to more adequate climates, rapid deployment of greenhouses,^[\\[3\\]](#fntpy5yy72c2)^ and systems or policies for a rapid emergency phaseout of systems with a net consumption of food such as first-generation biofuels or most animal agriculture.^[\\[1\\]](#fn24sheiuq7po)^ \n\nResearch and development of resilient foods for GCFS scenarios can reduce [global catastrophic risk](https://forum.effectivealtruism.org/tag/global-catastrophic-risk), increase global [food security](https://forum.effectivealtruism.org/tag/food-security), and reduce the risk of [civilizational collapse](https://forum.effectivealtruism.org/tag/civilizational-collapse) from [nuclear war](https://forum.effectivealtruism.org/tag/nuclear-warfare-1). One organisation that focuses mostly on resilient foods is the nonprofit [ALLFED](https://forum.effectivealtruism.org/tag/allfed).\n\nFurther reading\n---------------\n\nBaum, Seth D., David C. Denkenberger & Joshua M. Pearce (2016) [Alternative foods as a solution to global food supply catastrophes](https://hal.archives-ouvertes.fr/hal-02113500/document), *Solutions*, vol. 7, pp. 31–35.\n\nDenkenberger, David *et al.* (2021) [Long term cost-effectiveness of resilient foods for global catastrophes compared to artificial general intelligence safety](https://doi.org/10.31219/osf.io/vrmpf), *OSF Preprints*.\n\nOpen Philanthropy (2020) [Penn State University — Research on Emergency Food Resilience (Charles Anderson) (2020)](https://www.openphilanthropy.org/focus/scientific-research/miscellaneous/penn-state-university-research-on-emergency-food-resilience-2020), *Open Philanthropy*.\n\nRelated entries\n---------------\n\n[ALLFED](https://forum.effectivealtruism.org/tag/allfed) | [asteroids](https://forum.effectivealtruism.org/tag/asteroids) | [civilizational collapse](https://forum.effectivealtruism.org/tag/civilizational-collapse) | [existential risk](https://forum.effectivealtruism.org/tag/existential-risk) | [global catastrophic risk](https://forum.effectivealtruism.org/tag/global-catastrophic-risk) | [nuclear winter](https://forum.effectivealtruism.org/tag/nuclear-winter) | [refuges](https://forum.effectivealtruism.org/tag/refuges) | [supervolcano](https://forum.effectivealtruism.org/tag/supervolcano)\n\n1.  ^**[^](#fnref24sheiuq7po)**^\n    \n    Pham, Alix *et al.* (2022) [Nutrition in abrupt sunlight reduction scenarios: Envisioning feasible balanced diets on resilient foods](http://doi.org/10.3390/nu14030492), *Nutrients*, vol. 14, p. 492.\n    \n2.  ^**[^](#fnreffexj9bhaz6)**^\n    \n    García Martínez, Juan Bartolomé *et al.* (2022) [Methane single cell protein: Securing protein supply during global food catastrophes](http://doi.org/10.31219/osf.io/94mkg), ALLFED.\n    \n3.  ^**[^](#fnreftpy5yy72c2)**^\n    \n    Alvarado, Kyle A. *et al.* (2020) [Scaling of greenhouse crop production in low sunlight scenarios](http://doi.org/10.1016/j.scitotenv.2019.136012), *Science of The Total Environment*, vol. 707, 136012."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "cQtA4kMGaYj8t8Fga",
    "name": "Surveillance",
    "core": false,
    "slug": "surveillance",
    "oldSlugs": null,
    "postCount": 9,
    "description": {
      "markdown": "**Surveillance** is the systematic monitoring of human activity for social protection or control.\n\nEvaluation\n----------\n\n[80,000 Hours](https://forum.effectivealtruism.org/tag/80-000-hours) rates surveillance a \"potential highest priority area\": an issue that, if more thoroughly examined, could rank as a top global challenge.^[\\[1\\]](#fniw1klc5sqvn)^\n\nFurther reading\n---------------\n\nBostrom, Nick (2019) [The vulnerable world hypothesis](http://doi.org/10.1111/1758-5899.12718), *Global Policy*, vol. 10, pp. 455–476.\n\nChristiano, Paul (2018) [Surveil things, not people](https://sideways-view.com/2018/02/02/surveil-things-not-people/), *The Sideways View*, February 2.\n\nGarfinkel, Ben (2018) [The future of surveillance](https://www.effectivealtruism.org/articles/ea-global-2018-the-future-of-surveillance/), *Effective Altruism*, October 12.\n\nSegal, Aaron, Joan Feigenbaum & Bryan Ford (2016) [Open, privacy-preserving protocols for lawful surveillance](http://arxiv.org/abs/1607.03659), arXiv:1607.03659.\n\nTrask, Andrew (2017) [Safe crime detection](https://iamtrask.github.io/2017/06/05/homomorphic-surveillance/), *I am Trask*, June 5.\n\nRelated entries\n---------------\n\n[AI governance](https://forum.effectivealtruism.org/topics/ai-governance) | [China](https://forum.effectivealtruism.org/tag/china) | [existential risk](https://forum.effectivealtruism.org/tag/existential-risk) | [global catastrophic biological risk](https://forum.effectivealtruism.org/tag/global-catastrophic-biological-risk) | [totalitarianism](https://forum.effectivealtruism.org/tag/totalitarianism) | [vulnerable world hypothesis](https://forum.effectivealtruism.org/tag/vulnerable-world-hypothesis)\n\n1.  ^**[^](#fnrefiw1klc5sqvn)**^\n    \n    80,000 Hours (2022) [Our current list of pressing world problems](https://80000hours.org/problem-profiles/), *80,000 Hours*."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "r8SkWRCSpnAtpY85i",
    "name": "Quotes",
    "core": false,
    "slug": "quotes",
    "oldSlugs": null,
    "postCount": 5,
    "description": {
      "markdown": "The **quotes** tag collects posts where people share quotes related to effective altruism."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "HAExnopvTTd4Peev9",
    "name": "Effective Altruism and Consulting Network",
    "core": false,
    "slug": "effective-altruism-and-consulting-network",
    "oldSlugs": [
      "effective-altruism-and-consulting-network",
      "effective-altruism-and-consulting-network"
    ],
    "postCount": 5,
    "description": {
      "markdown": "The **Effective Altruism and Consulting Network** (**EACN**) is a [networking](https://forum.effectivealtruism.org/tag/network-building) organization for management [consultants](https://forum.effectivealtruism.org/tag/consultancy) interested in effective altruism.  \n  \nEACN seeks to facilitate its members' engagement with effective altruism ideas and discussion of how to apply them to career and donation decisions, both during their time in consulting and afterwards. (Many consultants stay in consulting only 2-5 years before working in industry, entrepreneurship, nonprofits, or the public sector.)  \n  \nEACN has members in each of the largest consulting firms, including Accenture, Bain & Company, Boston Consulting Group, and McKinsey & Company. \n\nFunding\n-------\n\nAs of July 2022, EACN has received $120,000 in funding from [Effective Altruism Funds](https://forum.effectivealtruism.org/topics/effective-altruism-funds).^[\\[1\\]](#fne057r5q4le)^\n\nExternal links\n--------------\n\n[Effective Altruism and Consulting Network](https://www.eac-network.com/). Official website.\n\n[Apply for a job](https://www.eac-network.com/careers).\n\nRelated entries\n---------------\n\n[consultancy](https://forum.effectivealtruism.org/tag/consultancy) | [effective giving](https://forum.effectivealtruism.org/tag/effective-giving) | [network building](https://forum.effectivealtruism.org/tag/network-building)\n\n1.  ^**[^](#fnrefe057r5q4le)**^\n    \n    Effective Altruism Infrastructure Fund (2021) [May-August 2021: EA Infrastructure Fund grants](https://funds.effectivealtruism.org/funds/payouts/may-august-2021-ea-infrastructure-fund-grants), *Effective Altruism Funds*, August."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "GKeFgBJnBSuGePbgh",
    "name": "Transformative artificial intelligence",
    "core": false,
    "slug": "transformative-artificial-intelligence",
    "oldSlugs": null,
    "postCount": 46,
    "description": {
      "markdown": "**Transformative artificial intelligence** (**TAI**) is [artificial intelligence](https://forum.effectivealtruism.org/topics/artificial-intelligence) that constitutes a [transformative development](https://forum.effectivealtruism.org/tag/transformative-development), i.e. a development at least as significant as the agricultural or industrial revolutions.^[\\[1\\]](#fn6hltgjabli8)^\n\nFurther reading\n---------------\n\nGruetzemacher, Ross & Jess Whittlestone (2022) [The transformative potential of artificial intelligence](https://doi.org/10.1016/j.futures.2021.102884), *Futures*, vol. 135.\n\nKarnofsky, Holden (2016) [Some background on our views regarding advanced artificial intelligence](https://www.openphilanthropy.org/blog/some-background-our-views-regarding-advanced-artificial-intelligence), *Open Philanthropy*, May 6, section 1.\n\nRelated entries\n---------------\n\n[AI governance](https://forum.effectivealtruism.org/topics/ai-governance) | [AI safety](https://forum.effectivealtruism.org/tag/ai-safety) | [economics of artificial intelligence](https://forum.effectivealtruism.org/tag/economics-of-artificial-intelligence) | [human-level general intelligence](https://forum.effectivealtruism.org/tag/human-level-artificial-intelligence) | [superintelligence](https://forum.effectivealtruism.org/tag/superintelligence) | [transformative development](https://forum.effectivealtruism.org/tag/transformative-development)\n\n1.  ^**[^](#fnref6hltgjabli8)**^\n    \n    For a brief review of existing definitions in the literature, see Gruetzemacher, Ross & Jess Whittlestone (2022) [The transformative potential of artificial intelligence](https://doi.org/10.1016/j.futures.2021.102884), *Futures*, vol. 135, pp. 2–3."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "FtscHgKMASeoT5sPu",
    "name": "Moral psychology",
    "core": false,
    "slug": "moral-psychology",
    "oldSlugs": null,
    "postCount": 57,
    "description": {
      "markdown": "**Moral psychology** is a field of study at the intersection of [philosophy](https://forum.effectivealtruism.org/tag/moral-philosophy) and [psychology](https://forum.effectivealtruism.org/tag/psychology-research). It  covers areas such as reasoning, moral development, moral motivation, and the evolutionary origins of morality.\n\nEffective altruists have taken a special interest in several topics moral psychology, including the psychology of [effective giving](https://forum.effectivealtruism.org/tag/effective-giving-1);^[\\[1\\]](#fncf5nk931d4v)^^[\\[2\\]](#fn055adcje9hjr)^^[\\[3\\]](#fnful4bc4pl28)^^[\\[4\\]](#fnviozlu81ufh)^^[\\[5\\]](#fnzue8vb75y9s)^^[\\[6\\]](#fnv5i4zmtoed)^^[\\[7\\]](#fn2fd29mvkto2)^ the psychology of [existential risk](https://forum.effectivealtruism.org/tag/existential-risk);^[\\[8\\]](#fnv0thm4njep)^ the psychology of [population ethics](https://forum.effectivealtruism.org/tag/population-ethics);^[\\[9\\]](#fnse0zcspifk)^  the psychology of the future;^[\\[10\\]](#fnh1f33gprr0p)^ the psychology of [speciesism](https://forum.effectivealtruism.org/tag/speciesism);^[\\[11\\]](#fnwuj11n49mmo)^^[\\[12\\]](#fnh30ng4rs136)^^[\\[13\\]](#fn8aii9dkr2nd)^ the psychology of [utilitarianism](https://forum.effectivealtruism.org/tag/utilitarianism);^[\\[14\\]](#fn5j4xvnq2uv7)^^[\\[15\\]](#fngf4ied1n4ei)^ the link between [giving and happiness](https://forum.effectivealtruism.org/tag/giving-and-happiness-1);^[\\[16\\]](#fn2y7padyz0i7)^^[\\[17\\]](#fng3w0e1g3klp)^ the personality traits of effective altruists;^[\\[18\\]](#fnicfgv2q0dhs)^ and the psychology of [altruistic motivation](https://forum.effectivealtruism.org/tag/altruistic-motivation).^[\\[19\\]](#fnf9qmfkfax8)^\n\nFurther reading\n---------------\n\nGreenberg, Spencer (2021) [Episode 046: EA efficacy and community norms with Stefan Schubert](https://clearerthinkingpodcast.com/?ep=046), *Clearer Thinking*, May 29.\n\nRelated entries\n---------------\n\n[cognitive bias](https://forum.effectivealtruism.org/tag/cognitive-bias) | [effective altruism messaging](https://forum.effectivealtruism.org/tag/effective-altruism-messaging) | [effective giving](https://forum.effectivealtruism.org/tag/effective-giving-1) | [existential risk](https://forum.effectivealtruism.org/tag/existential-risk) | [population ethics](https://forum.effectivealtruism.org/tag/population-ethics) | [psychology](https://forum.effectivealtruism.org/tag/psychology) | [rationality](https://forum.effectivealtruism.org/tag/rationality) | [scope neglect](https://forum.effectivealtruism.org/tag/scope-neglect) | [speciesism](https://forum.effectivealtruism.org/tag/speciesism) | [temporal discounting](https://forum.effectivealtruism.org/tag/temporal-discounting) | [utilitarianism](https://forum.effectivealtruism.org/tag/utilitarianism)\n\n1.  ^**[^](#fnrefcf5nk931d4v)**^\n    \n    Karlan, Dean, John A. List & Eldar Shafir (2011) [Small matches and charitable giving: evidence from a natural field experiment](http://doi.org/10.1016/j.jpubeco.2010.11.024), *Journal of Public Economics*, vol. 95, pp. 344–350.\n    \n2.  ^**[^](#fnref055adcje9hjr)**^\n    \n    Caviola, Lucius *et al.* (2014) [The evaluability bias in charitable giving: saving administration costs or saving lives?](http://doi.org/10.13140/2.1.1028.9287), *Judgment and Decision Making*, vol. 9, pp. 303–315.\n    \n3.  ^**[^](#fnrefful4bc4pl28)**^\n    \n    Caviola, Lucius, Stefan Schubert & Jason Nemirow (2020) [The many obstacles to effective giving](http://doi.org/10.31234/osf.io/3z7hj), *Judgment and Decision Making*, vol. 15, pp. 159–172.\n    \n4.  ^**[^](#fnrefviozlu81ufh)**^\n    \n    Burum, Bethany, Martin A. Nowak & Moshe Hoffman (2020) [An evolutionary explanation for ineffective altruism](http://doi.org/10.1038/s41562-020-00950-4), *Nature Human Behaviour*, vol. 4, pp. 1245–1257.\n    \n5.  ^**[^](#fnrefzue8vb75y9s)**^\n    \n    Caviola, Lucius, Stefan Schubert & Joshua D. Greene (2021) [The psychology of (in)effective altruism](http://doi.org/10.1016/j.tics.2021.03.015), *Trends in Cognitive Sciences*, vol. 25, pp. 596–607.\n    \n6.  ^**[^](#fnrefv5i4zmtoed)**^\n    \n    Schubert, Stefan (2018) [Why aren’t people donating more effectively?](https://www.youtube.com/watch?v=QyvzbW0XKmY), *Effective Altruism Global*, June 8.\n    \n7.  ^**[^](#fnref2fd29mvkto2)**^\n    \n    Berman, Jonathan Z. *et al.* (2018) [Impediments to effective altruism: The role of subjective preferences in charitable giving](http://doi.org/10.1177/0956797617747648), *Psychological Science*, vol. 29, pp. 834–844.\n    \n8.  ^**[^](#fnrefv0thm4njep)**^\n    \n    Schubert, Stefan, Lucius Caviola & Nadira S. Faber (2019) [The psychology of existential risk: moral judgments about human extinction](http://doi.org/10.1038/s41598-019-50145-9), *Scientific Reports*, vol. 9, pp. 1–8.\n    \n9.  ^**[^](#fnrefse0zcspifk)**^\n    \n    Caviola, Lucius *et al.* (2022) [Population ethical intuitions](https://doi.org/10.1016/j.cognition.2021.104941), *Cognition*, vol. 218.\n    \n10.  ^**[^](#fnrefh1f33gprr0p)**^\n    \n    Vallinder, Aron (2019) [Psychology of the future: Bibliography](https://docs.google.com/document/d/1s_bc9e-vGf7N2gixcoAeniMegYwgojE87eNIvsHljeo/edit#), unpublished.\n    \n11.  ^**[^](#fnrefwuj11n49mmo)**^\n    \n    Caviola, Lucius (2019) [*How We Value Animals: The Psychology of Speciesism*](http://files.luciuscaviola.com/Caviola-DPhil-thesis-July-8.pdf), PhD thesis, University of Oxford.\n    \n12.  ^**[^](#fnrefh30ng4rs136)**^\n    \n    Caviola, Lucius, Jim A. C. Everett & Nadira S. Faber (2019) [The moral standing of animals: towards a psychology of speciesism](http://doi.org/10.1037/pspp0000182), *Journal of Personality and Social Psychology*, vol. 116, pp. 1011–1029.\n    \n13.  ^**[^](#fnref8aii9dkr2nd)**^\n    \n    Caviola, Lucius & Valerio Capraro (2020) [Liking but devaluing animals: emotional and deliberative paths to speciesism](http://doi.org/10.1177/1948550619893959), *Social Psychological and Personality Science*, vol. 11.\n    \n14.  ^**[^](#fnref5j4xvnq2uv7)**^\n    \n    Kahane, Guy *et al.* (2018) [Beyond sacrificial harm: a two-dimensional model of utilitarian psychology](http://doi.org/10.1037/rev0000093), *Psychological Review*, vol. 125, pp. 131–164.\n    \n15.  ^**[^](#fnrefgf4ied1n4ei)**^\n    \n    Everett, Jim A. C. & Guy Kahane (2020) [Switching tracks? Towards a multidimensional model of utilitarian psychology](http://doi.org/10.1016/j.tics.2019.11.012), *Trends in Cognitive Sciences*, vol. 24, pp. 124–134.\n    \n16.  ^**[^](#fnref2y7padyz0i7)**^\n    \n    MacAskill, William, Andreas Mogensen & Toby Ord (2018) [Giving isn’t demanding](http://doi.org/10.1093/oso/9780190648879.003.0007), in Paul Woodruff (ed.) *The Ethics of Giving: Philosophers’ Perspectives on Philanthropy*, New York: Oxford University Press, pp. 178–203.\n    \n17.  ^**[^](#fnrefg3w0e1g3klp)**^\n    \n    Dalton, Max (2020) [Some extremely rough research on giving and happiness](https://forum.effectivealtruism.org/posts/iyFD6iLHYqhzjqHZs/some-extremely-rough-research-on-giving-and-happiness), *Effective Altruism Forum*, September 9.\n    \n18.  ^**[^](#fnreficfgv2q0dhs)**^\n    \n    E., Elizabeth (2020) [Correlations between cause prioritization and the big five personality traits](https://forum.effectivealtruism.org/posts/F6gMCRwAzxNL4cwjv/correlations-between-cause-prioritization-and-the-big-five), *Effective Altruism Forum*, September 24.\n    \n19.  ^**[^](#fnreff9qmfkfax8)**^\n    \n    Law, Kyle Fiore, Dylan Campbell & Brendan Gaesser (2021) [Biased benevolence: The perceived morality of effective altruism across social distance](http://doi.org/10.1177/01461672211002773), *Personality and Social Psychology Bulletin*, vol. 53.\n    \n20.  ^**[^](#fnrefa6l97mlzm)**^\n    \n    Desvousges, William *et al.* (2010) [*Measuring Nonuse Damages Using Contingent Valuation: An Experimental Evaluation of Accuracy*](http://doi.org/10.3768/rtipress.2009.bk.0001.1009), 2nd ed., Research Triangle Park, North Carolina: Research Triangle Institute."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "hai6pftgCCF7EJKcK",
    "name": "Mission-correlated investing",
    "core": false,
    "slug": "mission-correlated-investing",
    "oldSlugs": [
      "mission-hedging"
    ],
    "postCount": 17,
    "description": {
      "markdown": "**Mission-correlated investing** is any investment strategy that produces more money in worlds where money is relatively more valuable. That is, investments with returns that are correlated with the relative cost-effectiveness of future giving opportunities.\n\nMission-correlated investing increases the expected amount of good done by someone who is 'investing to give'**.** This is because the contribution of an altruistic investor to the total good can be thought of as the amount of money they donate multiplied by the impact per dollar of the opportunities they fund. All else equal, increasing the correlation between the amount of money and the impact per future dollar increases the investor's expected contribution.\n\nSome of these strategies are 'mission hedging', though strictly speaking these strategies will only hedge (reduce the variance of outcomes) in certain situations. It is also possible that a mission-correlated investing increases the variance of future outcomes. \n\nThere are many specific varieties including 'investing in evil to do good', investing in companies developing new risky technologies like AI, and investing in a way that has low or negative correlation with other altruists. The average investor's aversion to market risk can be viewed as coming from the mission-correlation between the market and their mission to consume more."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "7zBJLdkj9REcfuZwh",
    "name": "Career advising",
    "core": false,
    "slug": "career-advising",
    "oldSlugs": null,
    "postCount": 71,
    "description": {
      "markdown": "For posts that offer career advising, discuss better (and worse) types of career advising, etc.\n\nTypes of career advice research\n-------------------------------\n\nOne proposed typology of research on career advice distinguishes between career *choice* research (\"Which career paths are especially impactful?\"), career *success* research (\"What strategies are best for succeeding in a career?\"), and career *intervention* research (\"What can be done to help people pursue impactful careers?\"). Within career success research, a further distinction may be drawn between *generic* (or movement-level) and *specific* (or individual-level) research, depending on whether the research aims to identify the most impactful careers in general or whether it seeks to determine whether a career is a good fit for a particular person or group of people.^[\\[1\\]](#fnv3netmmua8c)^^[\\[2\\]](#fnwn9drmvw7ml)^\n\nOrganizations within the [effective altruism](https://forum.effectivealtruism.org/tag/effective-altruism) community providing career advice include [80,000 Hours](https://forum.effectivealtruism.org/tag/80-000-hours), [Animal Advocacy Careers](https://forum.effectivealtruism.org/tag/animal-advocacy-careers) and [Probably Good](https://forum.effectivealtruism.org/tag/probably-good).\n\nRelated entries\n---------------\n\n[80,000 Hours](https://forum.effectivealtruism.org/tag/80-000-hours) | [Animal Advocacy Careers](https://forum.effectivealtruism.org/tag/animal-advocacy-careers) | [building effective altruism](https://forum.effectivealtruism.org/tag/building-effective-altruism-1) | [career choice](https://forum.effectivealtruism.org/tag/career-choice) | [career framework](https://forum.effectivealtruism.org/tag/career-framework) | [intervention evaluation](https://forum.effectivealtruism.org/tag/intervention-evaluation) | [Probably Good](https://forum.effectivealtruism.org/tag/probably-good)\n\n1.  ^**[^](#fnrefv3netmmua8c)**^\n    \n    Agarwalla, Vaidehi (2021) [What is meta effective altruism?](https://forum.effectivealtruism.org/posts/Wx7LuMHbhABrtYrv9/what-is-meta-effective-altruism), *Effective Altruism Forum*, June 2.\n    \n2.  ^**[^](#fnrefwn9drmvw7ml)**^\n    \n    Aird, Michael (2021) [Comment on ‘What is meta Effective Altruism?’](https://forum.effectivealtruism.org/posts/Wx7LuMHbhABrtYrv9/what-is-meta-effective-altruism), *Effective Altruism Forum*, June 12."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "uNBgAb3rKz8bm2BLE",
    "name": "Charter cities",
    "core": false,
    "slug": "charter-cities",
    "oldSlugs": null,
    "postCount": 9,
    "description": {
      "markdown": "**Charter cities** (sometimes called **model cities**) are semi-autonomous political units within an existing state with significant administrative and regulatory authority.\n\nEvaluation\n----------\n\nA comprehensive report by [Rethink Priorities](https://forum.effectivealtruism.org/tag/rethink-priorities) identified three main types of benefits charter cities could potentially deliver:^[\\[1\\]](#fn2ppopuprqju)^\n\n1.  *Direct benefits*: increased income for those living in the city or its surroundings.\n2.  *Indirect local benefits*: scaling up successful charter city policies across the host country.\n3.  *Indirect global benefits*: facilitating experimentation with novel forms of governance.\n\nThe authors of the report do not believe that allocating resources to charter cities projects is cost-effective—relative to [GiveWell](https://forum.effectivealtruism.org/tag/givewell)'s priority programs—based on their direct benefits. The authors speculate that the indirect benefits may exceed these direct benefits, but note that (1) such indirect benefits are very hard to estimate and (2) regardless of their magnitude, they can likely be produced more cost-effectively by special economic zones (SEZs) than by charter cities. While noting the potential value of additional research on the indirect benefits of charter cities, the authors conclude that \"charter cities require more evidence in their favor before we could recommend them as a promising intervention to pursue.\"^[\\[1\\]](#fn2ppopuprqju)^ \n\nIn a series of replies, Mark Lutter, from the [Charter Cities Institute](https://forum.effectivealtruism.org/tag/charter-cities-institute), disputed the report's conclusions as overly pessimistic, arguing that charter cities constitute the only attempt to replicate the unprecedented economic growth of [China](https://forum.effectivealtruism.org/tag/china), which lifted 850 million people out of [poverty](https://forum.effectivealtruism.org/tag/global-poverty); that SEZs differ from charter cities in several important respects and are for that reason unlikely to deliver comparable economic benefits; and that, given their low budget, the apparent failures in Madagascar and Honduras provide evidence that charter cities are more, not less, tractable than it may otherwise appear.^[\\[2\\]](#fn3g6qnnx3yuf)^^[\\[3\\]](#fnls3whekkqu)^\n\nFurther reading\n---------------\n\nBernard, David & Jason Schukraft (2021) [Intervention report: Charter cities](https://forum.effectivealtruism.org/posts/EpaSZWQkAy9apupoD/intervention-report-charter-cities), *Effective Altruism Forum*, June 12.\n\nMason, Jeffrey (2019) [The case for charter cities within the effective altruist framework](https://assets.website-files.com/5d253237e31f051057dc0a2b/5d88effe42420361c5e0c3c2_The%20Case%20for%20Charter%20Cities%20Within%20the%20Effective%20Altruist%20Framework.pdf), *Charter Cities Institute*, September.\n\nWiblin, Robert & Keiran Harris (2019) [The team trying to end poverty by founding well-governed ‘charter’ cities](https://80000hours.org/podcast/episodes/lutter-and-winter-chater-cities-innovative-governance/), *80,000 Hours*, March 31.  \n*An interview with Mark Lutter and Tamara Winter from the Charter Cities Institute.*\n\nExternal links\n--------------\n\n[Charter Cities](https://web.archive.org/web/20110813112644/http://chartercities.org/). A defunct site with considerable information about charter cities as originally conceived by Paul Romer.\n\nRelated entries\n---------------\n\n[Charter Cities Institute](https://forum.effectivealtruism.org/tag/charter-cities-institute) | [economic growth](https://forum.effectivealtruism.org/tag/economic-growth) | [global health and development](https://forum.effectivealtruism.org/tag/global-health-and-development) |  [policy change](https://forum.effectivealtruism.org/tag/policy-change)\n\n1.  ^**[^](#fnref2ppopuprqju)**^\n    \n    Bernard, David & Jason Schukraft (2021) [Intervention report: Charter cities](https://forum.effectivealtruism.org/posts/EpaSZWQkAy9apupoD/intervention-report-charter-cities), *Effective Altruism Forum*, June 12.\n    \n2.  ^**[^](#fnref3g6qnnx3yuf)**^\n    \n    Lutter, Mark (2021) [Comments on ‘Intervention report: Charter cities’](https://forum.effectivealtruism.org/posts/EpaSZWQkAy9apupoD/intervention-report-charter-cities?commentId=aZeCDYQ3TkaSHPonB), *Effective Altruism Forum*, June 13.\n    \n3.  ^**[^](#fnrefls3whekkqu)**^\n    \n    Lutter, Mark (2021) [Further thoughts on charter cities and effective altruism](https://forum.effectivealtruism.org/posts/9oyHip2JCyjdLPhEs/further-thoughts-on-charter-cities-and-effective-altruism), *Effective Altruism Forum*, July 20."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "Gedmfw35JZLHM63oM",
    "name": "Existential catastrophe",
    "core": false,
    "slug": "existential-catastrophe-1",
    "oldSlugs": [
      "existential-catastrophe-1"
    ],
    "postCount": null,
    "description": {
      "markdown": "An **existential catastrophe** is an event that destroys humanity's long-term potential.\n\nClarifications\n--------------\n\nThe concept of an existential catastrophe may be distinguished from two other concepts with which it is sometimes conflated. First, existential catastrophes differ from [human extinction](https://forum.effectivealtruism.org/tag/human-extinction). An existential catastrophe need not involve the extinction of all humans, since humanity's potential can fail to be fully realized even if our species survives indefinitely.\n\nSecond, existential catastrophes differ from catastrophes involving what is loosely referred to as \"the end of the world\". By this expression it is generally meant some form of [civilizational collapse](https://forum.effectivealtruism.org/tag/civilizational-collapse). To amount to proper existential catastrophes, however, such catastrophes need to meet two additional conditions: first, the collapse must be *global*, and secondly, it must be *irreversible*. (If the first, but not the second, condition is satisfied, the scenario falls under the category of a [global catastrophe](https://forum.effectivealtruism.org/tag/global-catastrophic-risk).)\n\nTypes of catastrophe\n--------------------\n\nA typology of existential catastrophes introduced by [Toby Ord](https://forum.effectivealtruism.org/tag/toby-ord) classifies existential catastrophes into three main types:^[\\[1\\]](#fnpjafgw34d3)^\n\n1.  [human extinction](https://forum.effectivealtruism.org/tag/human-extinction)\n2.  unrecoverable [civilizational collapse](https://forum.effectivealtruism.org/tag/civilizational-collapse)\n3.  unrecoverable [dystopia](https://forum.effectivealtruism.org/tag/dystopia)\n\nFurther reading\n---------------\n\nOrd, Toby (2020) [*The Precipice: Existential Risk and the Future of Humanity*](https://en.wikipedia.org/wiki/Special:BookSources/1526600218), London: Bloomsbury Publishing.\n\nRelated entries\n---------------\n\n[civilizational collapse](https://forum.effectivealtruism.org/tag/civilizational-collapse) | [dystopia](https://forum.effectivealtruism.org/tag/dystopia) | [existential risk](https://forum.effectivealtruism.org/tag/existential-risk) | [global catastrophic risk](https://forum.effectivealtruism.org/tag/global-catastrophic-risk) | [Great Filter](https://forum.effectivealtruism.org/tag/great-filter) | [human extinction](https://forum.effectivealtruism.org/tag/human-extinction)\n\n1.  ^**[^](#fnrefpjafgw34d3)**^\n    \n    Ord, Toby (2020) [*The Precipice: Existential Risk and the Future of Humanity*](https://en.wikipedia.org/wiki/Special:BookSources/1526600218), London: Bloomsbury Publishing, fig. 5.2."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "wWPDwxR4ZhjWwno4w",
    "name": "Grantmaking",
    "core": false,
    "slug": "grantmaking",
    "oldSlugs": null,
    "postCount": 41,
    "description": {
      "markdown": "**Grantmaking** is the process by which a government or an organization gives non-repayable funds or products for a particular purpose or project.\n\nGrantmaking focused on top cause areas is one of [80,000 hours](https://forum.effectivealtruism.org/tag/80-000-hours) \"priority paths\"—the most promising career opportunities the organization is currently aware of.^[\\[1\\]](#fnxm3pldt6wec)^ Their medium-depth profile rates grantmaking an option with high [career capital](https://forum.effectivealtruism.org/tag/career-capital), [job satisfaction](https://forum.effectivealtruism.org/tag/job-satisfaction), direct impact and advocacy potential.^[\\[2\\]](#fn6ey3jjjy5em)^\n\nFurther reading\n---------------\n\nAlexander, Scott (2022) [So you want to run a microgrants program](https://astralcodexten.substack.com/p/so-you-want-to-run-a-microgrants), *Astral Codex Ten*, February 9.\n\nKarnofsky, Holden (2013) [Challenges of passive funding](https://blog.givewell.org/2013/04/18/challenges-of-passive-funding/), *The GiveWell Blog*, April 18.^[\\[3\\]](#fndgquuyrpf7q)^  \n\nProbaby Good (2021) [Grantmaking](https://www.probablygood.org/profile-grantmaking), *Probably Good.*\n\nWiblin, Robert (2017) [You want to do as much good as possible and have billions of dollars. What do you do?](https://80000hours.org/podcast/episodes/nick-beckstead-giving-billions/), *80,000 Hours*, October 11.\n\nWiblin, Robert & Keiran Harris (2018) [The world’s most intellectual foundation is hiring. Holden Karnofsky, founder of GiveWell, on how philanthropy can have maximum impact by taking big risks](https://80000hours.org/podcast/episodes/holden-karnofsky-open-philanthropy/), *80,000 Hours*, February 27.\n\nRelated entries\n---------------\n\n[donation choice](https://forum.effectivealtruism.org/tag/donation-choice) | [donor lotteries](https://forum.effectivealtruism.org/tag/donor-lotteries) | [effective altruism funding](https://forum.effectivealtruism.org/tag/effective-altruism-funding) | [effective giving](https://forum.effectivealtruism.org/tag/effective-giving-1)\n\n1.  ^**[^](#fnrefxm3pldt6wec)**^\n    \n    80,000 Hours (2021) [Our list of high-impact careers](https://80000hours.org/career-reviews/), *80,000 Hours*.\n    \n2.  ^**[^](#fnref6ey3jjjy5em)**^\n    \n    Duda, Roman (2015) [Foundation grantmaker](https://80000hours.org/career-reviews/foundation-program-manager/), *80,000 Hours*, July.\n    \n3.  ^**[^](#fnrefdgquuyrpf7q)**^\n    \n    There are many additional relevant posts in [Open Philanthropy's](https://www.openphilanthropy.org/blog) and [GiveWell's](https://blog.givewell.org/) blogs."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "FHE3J3E8qd6oqGZ8a",
    "name": "Software engineering",
    "core": false,
    "slug": "software-engineering",
    "oldSlugs": null,
    "postCount": 64,
    "description": {
      "markdown": "**Software engineering** comprises the entire range of activities used to design and develop software.\n\nSoftware engineering as a career\n--------------------------------\n\n[80,000 Hours](https://forum.effectivealtruism.org/tag/80-000-hours)' in-depth profile rates software engineering a \"sometimes recommended\" career.^[\\[1\\]](#fn9pjoa3pknl)^ The profile finds that a software engineer's most promising path to impact involves doing direct work on [AI safety](https://forum.effectivealtruism.org/tag/ai-safety) or in highly impactful organizations, and that the value of this work is likely higher than via [earning to give](https://forum.effectivealtruism.org/tag/earning-to-give). A career in software engineering can also be a good way to gain [career capital](https://forum.effectivealtruism.org/tag/career-capital).\n\nFurther reading\n---------------\n\nHilton, Benjamin (2022) [Software engineering](https://80000hours.org/career-reviews/software-engineering/), *80,000 Hours*, February 4.\n\nJones, Andy (2021) [AI safety needs great engineers](https://www.alignmentforum.org/posts/YDF7XhMThhNfHfim9/ai-safety-needs-great-engineers), *AI Alignment Forum*, November 23.\n\nRelated entries\n---------------\n\n[artificial intelligence](https://forum.effectivealtruism.org/tag/artificial-intelligence) | [data science](https://forum.effectivealtruism.org/tag/data-science) | [engineering](https://forum.effectivealtruism.org/topics/engineering) | [public interest technology](https://forum.effectivealtruism.org/tag/public-interest-technology) | [SparkWave](https://forum.effectivealtruism.org/tag/sparkwave)\n\n1.  ^**[^](#fnref9pjoa3pknl)**^\n    \n    Hilton, Benjamin (2022) [Software engineering](https://80000hours.org/career-reviews/software-engineering/), *80,000 Hours*, February 4."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "voyq7LkNhsFfYqddL",
    "name": "Transparency",
    "core": false,
    "slug": "transparency",
    "oldSlugs": null,
    "postCount": 11,
    "description": {
      "markdown": "**Transparency** is the extent to which outsiders can obtain information about the activities of government or private entities.\n\nSome organizations in the effective altruism community have a strong commitment to transparency. [GiveWell](https://forum.effectivealtruism.org/tag/givewell) publishes full details of their [charity evaluation](https://forum.effectivealtruism.org/tag/charity-evaluation) process, makes available records of all board meetings, and shares information about their operations.^[\\[1\\]](#fnfthzucwi1lr)^ GiveWell has also pioneered the practice of publishing a \"mistakes\" page, which other organizations and individuals—such as the [Centre for Effective Altruism](https://forum.effectivealtruism.org/tag/centre-for-effective-altruism-1)^[\\[2\\]](#fn8x9767gtb73)^ and [Scott Alexander](https://forum.effectivealtruism.org/tag/scott-alexander)^[\\[3\\]](#fnbsjjo98qv55)^–have since adopted.\n\n[Open Philanthropy](https://forum.effectivealtruism.org/tag/open-philanthropy) has argued that GiveWell's commitment to transparency makes sense given the charity evaluator's goal of making donation recommendations to the public: these recommendations are credible in part because the process that produced them is open to public examination. According to Open Philanthropy, other forms and degrees of transparency may be more appropriate to organizations that do not share GiveWell's mission, especially given the costs and risks of increased transparency. For this reason, Open Philanthropy does not place a high priority on explaining their individual grantmaking decisions to the public. Instead, it prioritizes the sharing of information about their general thinking processes and philosophy^[\\[4\\]](#fnh98ayzm6r1)^ and the communication of this information in ways that make it easier for the recipient to determine what updates to make in response to it (known as [reasoning transparency](https://forum.effectivealtruism.org/topics/reasoning-transparency)).^[\\[5\\]](#fnnccyd2fuyyn)^\n\nFurther reading\n---------------\n\nGiveWell (2021) [Our approach to transparency](https://www.givewell.org/how-we-work/transparency), *GiveWell*, April.\n\nMuehlhauser, Luke (2017) [Reasoning transparency](https://www.openphilanthropy.org/reasoning-transparency), *Open Philanthropy*, December.\n\nRelated entries\n---------------\n\n[corruption](https://forum.effectivealtruism.org/tag/corruption) | [reasoning transparency](https://forum.effectivealtruism.org/topics/reasoning-transparency)\n\n1.  ^**[^](#fnreffthzucwi1lr)**^\n    \n    GiveWell (2021) [Our approach to transparency](https://www.givewell.org/how-we-work/transparency), *GiveWell*, April.\n    \n2.  ^**[^](#fnref8x9767gtb73)**^\n    \n    Centre for Effective Altruism (2021) [Our mistakes](http://doi.org/10.1016/S0140-6736(02)14449-7), *Centre for Effective Altruism*, May.\n    \n3.  ^**[^](#fnrefbsjjo98qv55)**^\n    \n    Alexander, Scott (2021) [Mistakes](https://astralcodexten.substack.com/p/mistakes), *Astral Codex Ten*, April 25.\n    \n4.  ^**[^](#fnrefh98ayzm6r1)**^\n    \n    Karnofsky, Holden (2016) [Update on how we’re thinking about openness and information sharing](https://www.openphilanthropy.org/blog/update-how-were-thinking-about-openness-and-information-sharing), *Open Philanthropy*, September.\n    \n5.  ^**[^](#fnrefnccyd2fuyyn)**^\n    \n    Muehlhauser, Luke (2017) [Reasoning transparency](https://www.openphilanthropy.org/reasoning-transparency), *Open Philanthropy*, December."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "qX4dg4wEZy9sHorYi",
    "name": "Adjusted life year",
    "core": false,
    "slug": "adjusted-life-year",
    "oldSlugs": null,
    "postCount": 20,
    "description": {
      "markdown": "An **adjusted life year** is a measure of the value of a year of life adjusted in ways relevant for moral or medical valuation.\n\nThe most commonly used types of adjusted life year are **quality-adjusted life years** (**QALYs**) and **disability-adjusted life years** (**DALYs**). Both QALYs and DALYs fall under the broader category of **health-adjusted life years** (**HALYs**), which rely on measures of health to make the relevant adjustments. HALYs are commonly used in public health to quantify the [burden of disease](https://forum.effectivealtruism.org/tag/burden-of-disease) and to compare the [cost-effectiveness](https://forum.effectivealtruism.org/tag/cost-effectiveness) of different health interventions. By contrast, **wellbeing-adjusted life years** (**WALYs**) adjust based on measures of [wellbeing](https://forum.effectivealtruism.org/tag/wellbeing), such as hedonic experience or preference satisfaction. One type of WALY is the WELLBY, which measures wellbeing on a scale ranging from 0 to 10, representing the answers \"not at all\" and \"completely\", respectively, to the question \"Overall, how satisfied are you with your life nowadays?\"^[\\[1\\]](#fn2lapu1y0bll)^ Within the [effective altruism](https://forum.effectivealtruism.org/tag/effective-altruism) community, WALYs are generally preferred over HALYs, because things other than health can contribute to a person's wellbeing, and because health itself, while often a good proxy for wellbeing, is not generally regarded as intrinsically valuable.^[\\[2\\]](#fnrr0ltkcv9p)^\n\nFurther reading\n---------------\n\nBroome, John (1993) [Qalys](http://doi.org/10.1016/0047-2727(93)90047-W), *Journal of Public Economics*, vol. 50, pp. 149–167.\n\nFrijters, Paul & Christian Krekel (2021) [*A Handbook for Wellbeing Policy-Making: History, Theory, Measurement, Implementation, and Examples*](http://doi.org/10.1093/oxfordhb/9780199325818.001.0001), Oxford: Oxford University Press.\n\nGold, Marthe R., David Stevenson & Dennis G. Fryback (2002) [HALYs and QALYs and DALYs, oh my: Similarities and differences in summary measures of population health](http://doi.org/10.1146/annurev.publhealth.23.100901.140513), *Annual Review of Public Health*, vol. 23, pp. 115–134.\n\nRelated entries\n---------------\n\n[burden of disease](https://forum.effectivealtruism.org/tag/burden-of-disease) | [wellbeing](https://forum.effectivealtruism.org/tag/wellbeing)\n\n1.  ^**[^](#fnref2lapu1y0bll)**^\n    \n    Frijters, Paul & Christian Krekel (2021) [*A Handbook for Wellbeing Policy-Making: History, Theory, Measurement, Implementation, and Examples*](http://doi.org/10.1093/oxfordhb/9780199325818.001.0001), Oxford: Oxford University Press.\n    \n2.  ^**[^](#fnrefrr0ltkcv9p)**^\n    \n    Todd, Benjamin (2015) [We care about WALYs not QALYs](https://forum.effectivealtruism.org/posts/nevDBjuCPMCuaoMYT/we-care-about-walys-not-qalys), *Effective Altruism Forum*, November 13."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "w4Wz6AmL5d4HzXwra",
    "name": "Effective altruism outreach in schools",
    "core": false,
    "slug": "effective-altruism-outreach-in-schools",
    "oldSlugs": null,
    "postCount": 27,
    "description": {
      "markdown": "Related entries\n---------------\n\n[building effective altruism](https://forum.effectivealtruism.org/tag/building-effective-altruism-1) | [effective altruism education](https://forum.effectivealtruism.org/tag/effective-altruism-education) | [effective altruism messaging](https://forum.effectivealtruism.org/tag/effective-altruism-messaging)| [teaching materials](https://forum.effectivealtruism.org/tag/teaching-materials)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "Gk6vKenDrDLpwunHb",
    "name": "Charity evaluation",
    "core": false,
    "slug": "charity-evaluation",
    "oldSlugs": null,
    "postCount": 22,
    "description": {
      "markdown": "**Charity evaluation** (or **charity assessment**) is the investigation of charities to determine whether they should receive funding. It often includes a mixture of quantitative [cost-effectiveness](https://forum.effectivealtruism.org/tag/cost-effectiveness) calculations and other considerations, such as the strength of the charity's team, their [theory of change](https://forum.effectivealtruism.org/topics/theory-of-change), and their [room for more funding](https://forum.effectivealtruism.org/tag/room-for-more-funding).\n\n**Charity evaluators** are organisations that have as one of their core activities conducting charity evaluations. Examples include [GiveWell](https://forum.effectivealtruism.org/tag/givewell), [Animal Charity Evaluators](https://forum.effectivealtruism.org/tag/animal-charity-evaluators), [The Life You Can Save](https://forum.effectivealtruism.org/tag/the-life-you-can-save), [Giving Green](https://forum.effectivealtruism.org/tag/giving-green), and [Founders Pledge](https://forum.effectivealtruism.org/tag/founders-pledge). But charity evaluation can also be done by other organisations with less focus on that or by individuals, either as part of their own [donation choice](https://forum.effectivealtruism.org/tag/donation-choice) or to help inform other people's donation choices.\n\nThere are also other types of funding opportunities that can be evaluated in similar ways, such as individuals, for-profits, academic institutes, think tanks, or other projects or organisations that are not formally registered as charities.^[\\[1\\]](#fnf5z4wxnmkig)^\n\nRelated entries\n---------------\n\n[cause prioritization](https://forum.effectivealtruism.org/tag/cause-prioritization) | [cost-effectiveness](https://forum.effectivealtruism.org/tag/cost-effectiveness) | [donation choice](https://forum.effectivealtruism.org/tag/donation-choice) | [donation writeup](https://forum.effectivealtruism.org/tag/donation-writeup) | [effective giving](https://forum.effectivealtruism.org/tag/effective-giving-1) | [impact assessment](https://forum.effectivealtruism.org/tag/impact-assessment) | [intervention evaluation](https://forum.effectivealtruism.org/tag/intervention-evaluation)\n\n1.  ^**[^](#fnreff5z4wxnmkig)**^\n    \n    Khan, Anu & Rachel Baxter (2021) [Why we say ‘funding opportunity’ instead of ‘charity’](https://founderspledge.com/stories/why-we-say-funding-opportunity-instead-of-charity), *Founders Pledge*, February 22."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "CZsxDHkmqxiTGDjBX",
    "name": "Tyler Cowen",
    "core": false,
    "slug": "tyler-cowen",
    "oldSlugs": null,
    "postCount": 4,
    "description": {
      "markdown": "**Tyler Cowen** (born 21 January 1962) is an American economist.\n\nCowen has made contributions to [population ethics](https://forum.effectivealtruism.org/tag/population-ethics),^[\\[1\\]](#fnrmqyty8kuyn)^^[\\[2\\]](#fnfyu8ul67nu)^^[\\[3\\]](#fntqab5wyv87h)^ [temporal discounting](https://forum.effectivealtruism.org/tag/temporal-discounting),^[\\[4\\]](#fnfj5lqybqaru)^^[\\[5\\]](#fnlxlfeydhage)^ [cluelessness](https://forum.effectivealtruism.org/tag/cluelessness),^[\\[6\\]](#fnj57i0syz5et)^ [wild animal welfare](https://forum.effectivealtruism.org/tag/wild-animal-welfare),^[\\[7\\]](#fnnbzekxf7288)^ [farmed animal welfare](https://forum.effectivealtruism.org/tag/farmed-animal-welfare),^[\\[8\\]](#fn0712sykp4j4)^  the [long-term future](https://forum.effectivealtruism.org/tag/long-term-future),^[\\[9\\]](#fnevsla33xotu)^ the long-run significance of [economic growth](https://forum.effectivealtruism.org/tag/economic-growth),^[\\[10\\]](#fnuu4ubyeizfe)^ and other topics.\n\nFurther reading\n---------------\n\nFrank, Daniel (2021) [My favourite Tyler Cowen posts and ideas](https://danfrank.ca/my-favourite-tyler-cowen-posts-and-ideas/), *Frankly Speaking*, May 14.\n\nGalef, Julia (2019) [Defending big business against its critics (Tyler Cowen)](http://rationallyspeakingpodcast.org/232-defending-big-business-against-its-critics-tyler-cowen/), *Rationally Speaking*, April 29.\n\nKulesa, Tony (2021) [Tyler Cowen is the best curator of talent in the world](https://kulesa.substack.com/p/tyler-cowen-is-the-best-curator-of), *Beginnings*, September 1.\n\nWiblin, Robert & Keiran Harris (2018) [Economics Prof Tyler Cowen says our overwhelming priorities should be maximising economic growth and making civilisation more stable. Is he right?](https://80000hours.org/podcast/episodes/tyler-cowen-stubborn-attachments/), *80,000 Hours*, October 17.\n\nExternal links\n--------------\n\n[Tyler Cowen](https://tylercowen.com/). Official website.\n\n[Marginal Revolution](https://marginalrevolution.com/). Cowen's blog.\n\n[Conversations with Tyler](https://conversationswithtyler.com/). Cowen's podcast.\n\nRelated entries\n---------------\n\n[Bryan Caplan](https://forum.effectivealtruism.org/tag/bryan-caplan) | [Robin Hanson](https://forum.effectivealtruism.org/tag/robin-hanson)\n\n1.  ^**[^](#fnrefrmqyty8kuyn)**^\n    \n    Cowen, Tyler (1989) [Normative population theory](https://doi.org/10.1007/BF00433361), *Social Choice and Welfare*, vol. 6, pp. 33–43.\n    \n2.  ^**[^](#fnreffyu8ul67nu)**^\n    \n    Cowen, Tyler (1996) [What do we learn from the repugnant conclusion?](https://doi.org/10.1086/233671), *Ethics*, vol. 106, pp. 754–775.\n    \n3.  ^**[^](#fnreftqab5wyv87h)**^\n    \n    Cowen, Tyler (2004) [Resolving the repugnant conclusion](https://en.wikipedia.org/wiki/Special:BookSources/1-4020-2472-X), in Jesper Ryberg & Torbjörn Tännsjö (eds.) *The Repugnant Conclusion: Essays on Population Ethics*, Dordrecht: Kluwer, pp. 81–97.\n    \n4.  ^**[^](#fnreffj5lqybqaru)**^\n    \n    Cowen, Tyler & Derek Parfit (1992) [Against the social discount rate](https://doi.org/10.1017/CBO9781107415324.004), in Peter Laslett & James Fishkin (eds.) *Justice Between Age Groups and Generations*, New Haven: Yale University Press, pp. 144–168.\n    \n5.  ^**[^](#fnreflxlfeydhage)**^\n    \n    Cowen, Tyler (2004) [Policy implications of zero discounting: An exploration in politics and morality](https://doi.org/10.1017/S0265052504211062), *Social Philosophy and Policy*, vol. 21, pp. 121–140.\n    \n6.  ^**[^](#fnrefj57i0syz5et)**^\n    \n    Cowen, Tyler (2006) [The epistemic problem does not refute consequentialism](https://doi.org/10.1017/S0953820806002172), *Utilitas*, vol. 18, pp. 383–399.\n    \n7.  ^**[^](#fnrefnbzekxf7288)**^\n    \n    Cowen, Tyler (2003) [Policing nature](https://doi.org/10.5840/enviroethics200325231), *Environmental Ethics*, vol. 25, pp. 169–182.\n    \n8.  ^**[^](#fnref0712sykp4j4)**^\n    \n    Cowen, Tyler (2006) [Market failure for the treatment of animals](https://doi.org/10.1007/BF02687369), *Society*, vol. 43, pp. 39–44.\n    \n9.  ^**[^](#fnrefevsla33xotu)**^\n    \n    Cowen, Tyler (2007) [Caring about the distant future: why it matters and what it means](https://en.wikipedia.org/wiki/Special:BookSources/0041-9494), *University of Chicago Law Review*, vol. 74, pp. 5–40.\n    \n10.  ^**[^](#fnrefuu4ubyeizfe)**^\n    \n    Cowen, Tyler (2018) [*Stubborn Attachments: A Vision for a Society of Free, Prosperous, and Responsible Individuals*](https://en.wikipedia.org/wiki/Special:BookSources/978-1-73226-513-4), San Francisco: Stripe Press."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "EEWbG3ovWsxmKceMn",
    "name": "Farmed Animal Funders",
    "core": false,
    "slug": "farmed-animal-funders",
    "oldSlugs": null,
    "postCount": 3,
    "description": {
      "markdown": "**Farmed Animal Funders** (**FAF**) is a decentralized group of funders supporting charities focused on ending [factory farming](https://forum.effectivealtruism.org/tag/farmed-animal-welfare) and moving towards a more sustainable food system. The group is open to all individuals and foundations giving more than $250,000 per year.\n\nFurther reading\n---------------\n\nFarmed Animal Funders (2021) [About](https://farmedanimalfunders.org/about/), *Farmed Animal Funders*.\n\nExternal links\n--------------\n\n[Farmed Animal Funders](https://farmedanimalfunders.org/). Official website."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "PyAFcMkKMgvNfx7kP",
    "name": "Animal Advocacy Africa",
    "core": false,
    "slug": "animal-advocacy-africa",
    "oldSlugs": null,
    "postCount": 2,
    "description": {
      "markdown": "**Animal Advocacy Africa** (**AAA**) is a program of Credence Institute that aims to develop the [effective animal advocacy](https://forum.effectivealtruism.org/tag/effective-animal-advocacy) movement in Africa.\n\nFunding\n-------\n\nAs of June 2022, AAA has received over $90,000 in funding from [Effective Altruism Funds](https://forum.effectivealtruism.org/topics/effective-altruism-funds).^[\\[1\\]](#fnh2nouyc7n9w)^^[\\[2\\]](#fnzac57yzb3w)^\n\nFurther reading\n---------------\n\nAnimal Advocacy Africa (2021) [Six month update](https://mailchi.mp/c1201741a25c/hello-from-aaa?e=8da91f7b9e), *Animal Advocacy Africa Newsletter*, August 10.\n\nExternal links\n--------------\n\n[Animal Advocacy Africa](https://www.animaladvocacyafrica.org/). Official website.\n\n1.  ^**[^](#fnrefh2nouyc7n9w)**^\n    \n    Animal Welfare Fund (2021) [July 2021: Animal Welfare Fund grants](https://funds.effectivealtruism.org/funds/payouts/july-2021-animal-welfare-fund-grants), *Effective Altruism Funds*, July.\n    \n2.  ^**[^](#fnrefzac57yzb3w)**^\n    \n    Animal Welfare Fund (2021) [November 2021: Animal Welfare Fund grants](https://funds.effectivealtruism.org/funds/payouts/november-2021-animal-welfare-fund-grants), *Effective Altruism Funds*, November."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "rn5NjbucPojvcYNZs",
    "name": "Effective Altruism for Christians",
    "core": false,
    "slug": "effective-altruism-for-christians",
    "oldSlugs": [
      "ea-for-christians"
    ],
    "postCount": 8,
    "description": {
      "markdown": "**Effective Altruism for Christians** is a global community group of Christians within [effective altruism](https://forum.effectivealtruism.org/tag/effective-altruism).\n\nFurther reading\n---------------\n\nOrd, Toby (2012) [Global poverty and the demands of morality](https://doi.org/10.1017/CBO9781107279629.013), in John Perry (ed.) *God, the Good, and Utilitarianism*, Cambridge: Cambridge University Press, pp. 177–191.\n\nRiedener, Stefan (2022) [Human extinction from a Thomist perspective](https://doi.org/10.5771/9783748925361), in Dominic Roser, Stefan Riedener & Markus Huppenbauer (eds.) *Effective Altruism and Religion: Synergies, Tensions, Dialogue*, Baden-Baden: Nomos, pp. 187–209.\n\nRoser, Dominic (2022) [Effective altruism as Egyptian gold for Christians](https://doi.org/10.5771/9783748925361), in Dominic Roser, Stefan Riedener & Markus Huppenbauer (eds.) *Effective Altruism and Religion: Synergies, Tensions, Dialogue*, Baden-Baden: Nomos, pp. 47–76.\n\nExternal links\n--------------\n\n[Effective Altruism for Christians](https://www.eaforchristians.org/). Official website.\n\nRelated entries\n---------------\n\n[religion](https://forum.effectivealtruism.org/tag/religion)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "fok5NSeijwXcTLNvC",
    "name": "The END Fund",
    "core": false,
    "slug": "the-end-fund",
    "oldSlugs": [
      "end-fund"
    ],
    "postCount": null,
    "description": {
      "markdown": "**The END Fund** is a private philanthropic initiative dedicated to fighting the most common [neglected tropical diseases](https://forum.effectivealtruism.org/tag/neglected-tropical-diseases). \n\nEvaluation\n----------\n\nAs of December 2021, [GiveWell](https://forum.effectivealtruism.org/tag/givewell) estimates that The END Fund can deworm a child at a cost of around $1.^[\\[1\\]](#fn0gv1rfeptpb)^^[\\[2\\]](#fnsgce5goa5k)^ The END Fund's [deworming](https://forum.effectivealtruism.org/tag/deworming) program was a [GiveWell](https://forum.effectivealtruism.org/tag/givewell) top-rated charity between 2016 and 2022.^[\\[1\\]](#fn0gv1rfeptpb)^\n\nAs of July 2022, The END Fund has received $13.5 million in funding from [Open Philanthropy](https://forum.effectivealtruism.org/tag/open-philanthropy).^[\\[3\\]](#fnt0fb3q4o1hs)^\n\nFurther reading\n---------------\n\nGiveWell (2020) [The END Fund’s deworming program](https://www.givewell.org/charities/end-fund), *GiveWell*, November.\n\nExternal links\n--------------\n\n[The END Fund](https://end.org/). Official website.\n\n[Apply for a job](https://jobs.lever.co/end).\n\n[Donate to The End Fund](https://end.org/engage/donate/). \n\n1.  ^**[^](#fnref0gv1rfeptpb)**^\n    \n    GiveWell (2020) [The END Fund’s deworming program](https://www.givewell.org/charities/end-fund), *GiveWell*, November.\n    \n2.  ^**[^](#fnrefsgce5goa5k)**^\n    \n    GiveWell (2021) [Our top charities](https://www.givewell.org/charities/top-charities), *GiveWell*, November.\n    \n3.  ^**[^](#fnreft0fb3q4o1hs)**^\n    \n    Open Philanthropy (2022) [Grants database: The END Fund](https://www.openphilanthropy.org/grants/?q=&organization-name=the-end-fund), *Open Philanthropy*."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "CZHt3YMoy4mcTS6xa",
    "name": "Nonlinear Fund",
    "core": false,
    "slug": "nonlinear-fund",
    "oldSlugs": null,
    "postCount": 8,
    "description": {
      "markdown": "The **Nonlinear Fund** is an organization that aims to research, fund, and seed [AI safety](https://forum.effectivealtruism.org/tag/ai-safety) interventions.\n\nFunding\n-------\n\nAs of July 2022, the Nonlinear Fund has received nearly $600,000 in funding from the [Survival and Flourishing](https://forum.effectivealtruism.org/topics/survival-and-flourishing) Fund,^[\\[1\\]](#fn5wdawo2dh57)^ and $250,000 from the [Future Fund](https://forum.effectivealtruism.org/topics/future-fund).^[\\[2\\]](#fnwewjmk6s18n)^\n\nFurther reading\n---------------\n\nWoods, Kat (2021) [Introducing The Nonlinear Fund: AI Safety research, incubation, and funding](https://forum.effectivealtruism.org/posts/fX8JsabQyRSd7zWiD/introducing-the-nonlinear-fund-ai-safety-research-incubation), *Effective Altruism Forum*, March 18.\n\nExternal links\n--------------\n\n[Nonlinear Fund](https://www.nonlinear.org/). Official website.\n\n[Apply for a job](https://www.nonlinear.org/jobs.html).\n\n1.  ^**[^](#fnref5wdawo2dh57)**^\n    \n    Survival and Flourishing Fund (2021) [SFF-2022-H1 S-Process recommendations announcement](https://survivalandflourishing.fund/sff-2022-h1-recommendations), *Survival and Flourishing Fund*.\n    \n2.  ^**[^](#fnrefwewjmk6s18n)**^\n    \n    Future Fund (2022) [Our grants and investments: Nonlinear](https://ftxfuturefund.org/all-grants/?_organization_name=nonlinear), *Future Fund*."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "rnvJp5pSdAcoLkmnF",
    "name": "Let's Fund",
    "core": false,
    "slug": "let-s-fund",
    "oldSlugs": null,
    "postCount": null,
    "description": {
      "markdown": "**Let's Fund** is a charity evaluator that aims to direct funds especially to small projects focused on research, policy and advocacy. It allows people to donate to high-risk projects that could have an enormous impact if successful.\n\nFunding\n-------\n\nAs of July 2022, Let's Fund has received $80,000 from [Effective Altruism Funds](https://forum.effectivealtruism.org/topics/effective-altruism-funds).^[\\[1\\]](#fnuw3j2b7ja49)^^[\\[2\\]](#fn0mt74lbv4pnj)^\n\nExternal links\n--------------\n\n[Let's Fund](https://lets-fund.org/). Official website.\n\n1.  ^**[^](#fnrefuw3j2b7ja49)**^\n    \n    Effective Altruism Infrastructure Fund (2018) [November 2018: EA Meta Fund grants](https://funds.effectivealtruism.org/funds/payouts/november-2018-ea-meta-fund-grants), *Effective Altruism Funds*, November. \n    \n2.  ^**[^](#fnref0mt74lbv4pnj)**^\n    \n    Effective Altruism Infrastructure Fund (2020) [March 2020: EA Meta Fund Grants](https://funds.effectivealtruism.org/funds/payouts/march-2020-ea-meta-fund-grants), *Effective Altruism Funds*, March."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "iJNW2gaHSFdHz724R",
    "name": "Giving Multiplier",
    "core": false,
    "slug": "giving-multiplier",
    "oldSlugs": null,
    "postCount": 4,
    "description": {
      "markdown": "**Giving Multiplier** is a website that adds 3% to 30% on top of donations to any charity on condition that donors give a percentage (at least 10%) to an effective charity from their list of expert recommendations. It was created by the psychologists Lucius Caviola and Joshua Greene at Harvard University based on their research^[\\[1\\]](#fn33w2nqlkuam)^ and is funded by [Effective Altruism Funds](https://forum.effectivealtruism.org/tag/effective-altruism-funds).\n\nFurther reading\n---------------\n\nCaviola, Lucius & Joshua D. Greene (2021) [Boosting effective giving with bundling and donor coordination](https://doi.org/10.31234/osf.io/65fmr), PsyArXiv.\n\nExternal links\n--------------\n\n[Giving Multiplier](https://givingmultiplier.org/). Official website.\n\nRelated entries\n---------------\n\n[donation matching](https://forum.effectivealtruism.org/tag/donation-matching) | [effective giving](https://forum.effectivealtruism.org/tag/effective-giving) | [philanthropic coordination](https://forum.effectivealtruism.org/tag/philanthropic-coordination)\n\n1.  ^**[^](#fnref33w2nqlkuam)**^\n    \n    Caviola, Lucius & Joshua D. Greene (2021) [Boosting effective giving with bundling and donor coordination](https://doi.org/10.31234/osf.io/65fmr), PsyArXiv."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "qHMwjwHZ3D39S2DMB",
    "name": "GiEffektivt.no",
    "core": false,
    "slug": "gieffektivt-no",
    "oldSlugs": null,
    "postCount": null,
    "description": {
      "markdown": "**GiEffektivt.no** (\"give effectively\") is a donation portal based in Norway that [fundraises](https://forum.effectivealtruism.org/tag/fundraising) for [GiveWell](https://forum.effectivealtruism.org/tag/givewell)’s top charities. It makes donations easier by lowering transaction costs and offering tax refunds to Norwegian donors.\n\nFunding\n-------\n\nAs of July 2022, GiEffektivt.no has received over $300,000 from [Effective Altruism Funds](https://forum.effectivealtruism.org/topics/effective-altruism-funds).^[\\[1\\]](#fnwn4tr6f8t5q)^^[\\[2\\]](#fnzziadnbk2y)^\n\nExternal links\n--------------\n\n[GiEffektivt.no](https://gieffektivt.no/). Official website.\n\nRelated entries\n---------------\n\n[effective giving](https://forum.effectivealtruism.org/tag/effective-giving) | [fundraising](https://forum.effectivealtruism.org/tag/fundraising)\n\n1.  ^**[^](#fnrefwn4tr6f8t5q)**^\n    \n    Effective Altruism Infrastructure Fund (2020) [July 2020: EA Meta Fund grants](https://funds.effectivealtruism.org/funds/payouts/july-2020-ea-meta-fund-grants), *Effective Altruism Funds*, July. \n    \n2.  ^**[^](#fnrefzziadnbk2y)**^\n    \n    Effective Altruism Infrastructure Fund (2022) [September-December 2021: EA Infrastructure Fund](https://funds.effectivealtruism.org/funds/payouts/september-december-2021-ea-infrastructure-fund), *Effective Altruism Funds*, July."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "xfxemDkeEsfeaoZk9",
    "name": "Effektiv-Spenden.org",
    "core": false,
    "slug": "effektiv-spenden-org",
    "oldSlugs": null,
    "postCount": 5,
    "description": {
      "markdown": "**Effektiv-Spenden.org** (\"donate effectively\") is a non-profit donation platform based in Germany that was founded in coordination with the [Effective Altruism Foundation](https://forum.effectivealtruism.org/tag/effective-altruism-foundation) in early 2019. It aims to attract donors by offering a set of evaluated charities focused on broadly appealing cause areas such as [global health and development](https://forum.effectivealtruism.org/tag/global-health-and-development), [climate change](https://forum.effectivealtruism.org/tag/climate-change), and [animal welfare](https://forum.effectivealtruism.org/tag/animal-welfare-1). \n\nFunding\n-------\n\nAs of July 2022, Effektiv-Spenden.org has received nearly $250,000 from [Effective Altruism Funds](https://forum.effectivealtruism.org/topics/effective-altruism-funds).^[\\[1\\]](#fn9g70obi907h)^^[\\[2\\]](#fne2f304u5wjo)^\n\nExternal links\n--------------\n\n[Effektiv-Spenden.org](https://www.effektiv-spenden.org/). Official website.\n\n1.  ^**[^](#fnref9g70obi907h)**^\n    \n    Effective Altruism Infrastructure Fund (2020) [November 2020: EA Infrastructure Fund grants](https://funds.effectivealtruism.org/funds/payouts/november-2020-ea-infrastructure-fund-grants), *Effective Altruism Funds*, November.\n    \n2.  ^**[^](#fnrefe2f304u5wjo)**^\n    \n    Effective Altruism Infrastructure Fund (2022) [September-December 2021: EA Infrastructure Fund](https://funds.effectivealtruism.org/funds/payouts/september-december-2021-ea-infrastructure-fund), *Effective Altruism Funds*, July."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "DvuobLHGEg6ojJ8ib",
    "name": "Effective Giving (organization)",
    "core": false,
    "slug": "effective-giving-organization",
    "oldSlugs": [
      "effective-giving-2"
    ],
    "postCount": null,
    "description": {
      "markdown": "**Effective Giving** is a non-profit based in Amsterdam that aims to help donors to explore the best giving strategies backed by evidence and research in order to maximize the impact of their donations.\n\nExternal links\n--------------\n\n[Effective Giving](https://www.effectivegiving.org/). Official website.\n\nRelated entries\n---------------\n\n[effective giving](https://forum.effectivealtruism.org/tag/effective-giving-1)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "Z86LkckNEKQ8vFT84",
    "name": "Donational",
    "core": false,
    "slug": "donational",
    "oldSlugs": null,
    "postCount": 1,
    "description": {
      "markdown": "**Donational** is a donation platform which evaluates and recommends effective charities, and helps donors to allocate a proportion of their income to those charities.\n\nExternal links\n--------------\n\n[Donational](https://donational.org/). Official website."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "PyxpB4csyPdjkRpBt",
    "name": "Doebem",
    "core": false,
    "slug": "doebem",
    "oldSlugs": null,
    "postCount": 1,
    "description": {
      "markdown": "**Doebem** (\"give well\") is a Brazilian-based donation platform which evaluates and recommends charities based on impact and transparency.\n\nExternal links\n--------------\n\n[Doebem](https://www.doebem.org.br/). Official website."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "R8drqcprwGGA6cQkD",
    "name": "Ayuda Efectiva",
    "core": false,
    "slug": "ayuda-efectiva",
    "oldSlugs": null,
    "postCount": 2,
    "description": {
      "markdown": "**Ayuda Efectiva** is a non-profit that promotes [effective giving](https://forum.effectivealtruism.org/tag/effective-giving-1) in Spain. Their Global Health Fund routes donations to a selection of [GiveWell](https://forum.effectivealtruism.org/tag/givewell)’s recommended charities, providing tax deductibility for Spanish donors.\n\nFunding\n-------\n\nAs of July 2022, Ayuda Efectiva has received $85,000 in funding from [Effective Altruism Funds](https://forum.effectivealtruism.org/topics/effective-altruism-funds).^[\\[1\\]](#fnzui83ak74k)^\n\nFurther reading\n---------------\n\nVita, Ana Muñoz (2020) [El altruismo eficaz llega a España](https://cincodias.elpais.com/cincodias/2020/09/02/fortunas/1599068812_670968.html), *Cinco Días*, September 3.\n\nExternal links\n--------------\n\n[Ayuda Efectiva](https://ayudaefectiva.org/). Official website.\n\nRelated entries\n---------------\n\n[effective giving](https://forum.effectivealtruism.org/tag/effective-giving-1)\n\n1.  ^**[^](#fnrefzui83ak74k)**^\n    \n    Effective Altruism Infrastructure Fund (2022) [September-December 2021: EA Infrastructure Fund](https://funds.effectivealtruism.org/funds/payouts/september-december-2021-ea-infrastructure-fund), *Effective Altruism Funds*, July."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "DHLEK6cPkJ4Anfw9F",
    "name": "Spark Wave",
    "core": false,
    "slug": "spark-wave",
    "oldSlugs": [
      "sparkwave"
    ],
    "postCount": 7,
    "description": {
      "markdown": "**Spark Wave** is a company dedicated to creating socially beneficial software startups. Its products include Clearer Thinking (a website for improving decision making and accessing science-based tools), UpLift (an app for treating depression), and Positly (a platform for fast recruitment of study participants). Spark Wave was founded by Spencer Greenberg in 2016.\n\nExternal links\n--------------\n\n[Spark Wave](https://www.sparkwave.tech/). Official website.\n\nRelated entries\n---------------\n\n[mental health](https://forum.effectivealtruism.org/tag/mental-health) | [metascience](https://forum.effectivealtruism.org/tag/meta-science) | [software engineering](https://forum.effectivealtruism.org/tag/software-engineering)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "2x9X6nPQ8CMJYuBPr",
    "name": "Sentience Politics",
    "core": false,
    "slug": "sentience-politics",
    "oldSlugs": null,
    "postCount": 1,
    "description": {
      "markdown": "**Sentience Politics** is an anti-speciesist organization working primarily on political campaigning in Switzerland with the aim of reducing the suffering of non-human animals. It was started as a project of the Giordano Bruno Foundation.\n\nFunding\n-------\n\nAs of July 2022, Sentience Politics has received over $120,000 in funding from [Effective Altruism Funds](https://forum.effectivealtruism.org/topics/effective-altruism-funds).^[\\[1\\]](#fnl6gtdyn5z9)^\n\nExternal links\n--------------\n\n[Sentience Politics](https://sentience.ch/en/). Official website.\n\n[Apply for a job](https://sentience.ch/en/jobs/).\n\n1.  ^**[^](#fnrefl6gtdyn5z9)**^\n    \n    Animal Welfare Fund (2021) [July 2021: Animal Welfare Fund grants](https://funds.effectivealtruism.org/funds/payouts/july-2021-animal-welfare-fund-grants), *Effective Altruism Funds*, July."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "rJXCoxw6AMsGfkPKF",
    "name": "Our World in Data",
    "core": false,
    "slug": "our-world-in-data",
    "oldSlugs": null,
    "postCount": 18,
    "description": {
      "markdown": "**Our World in Data** (**OWID**) is a non-profit that aims to popularize research and data on how to make progress against [poverty](https://forum.effectivealtruism.org/tag/global-poverty), [disease](https://forum.effectivealtruism.org/tag/global-health-and-development), [climate change](https://forum.effectivealtruism.org/tag/climate-change), [war](https://forum.effectivealtruism.org/tag/global-health-and-development), [existential risk](https://forum.effectivealtruism.org/tag/existential-risk), and other high-priority problems.\n\nFurther reading\n---------------\n\nRoser, Max (2018) [The world is much better; The world is awful; The world can be much better](https://ourworldindata.org/much-better-awful-can-be-better), *Our World in Data*, October 31.\n\nRoser, Max (2021) [Why do we need to know about progress if we are concerned about the world’s large problems? The mission of Our World in Data](https://ourworldindata.org/problems-and-progress), *Our World in Data*, June 7.\n\nWiblin, Robert & Keiran Harris (2021) [Max Roser on building the world’s first great source of COVID-19 data at Our World in Data](https://80000hours.org/podcast/episodes/max-roser-our-world-in-data/), *80,000 Hours*, June 21.\n\nExternal links\n--------------\n\n[Our World in Data](https://ourworldindata.org/). Official website.\n\n[Apply for a job](https://ourworldindata.org/jobs).\n\nRelated entries\n---------------\n\n[research](https://forum.effectivealtruism.org/topics/research) | [Swift Centre for Applied Forecasting](https://forum.effectivealtruism.org/topics/swift-centre-for-applied-forecasting)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "LJzGDLsFMvGP7aBpS",
    "name": "Impactful Government Careers",
    "core": false,
    "slug": "impactful-government-careers",
    "oldSlugs": [
      "high-impact-careers-in-government"
    ],
    "postCount": 7,
    "description": {
      "markdown": "**Impactful Government Careers** (formerly **High Impact Careers in Government**) is an organization that helps civil servants in the UK to do the most good they can by running workshops and events, doing research, and providing one-on-one career coaching.\n\nExternal links\n--------------\n\n[Impactful Government Careers](https://www.impactfulgovcareers.org/). Official website."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "iC8So3aRmfXcgiDK6",
    "name": "Giving Green",
    "core": false,
    "slug": "giving-green",
    "oldSlugs": null,
    "postCount": 10,
    "description": {
      "markdown": "**Giving Green** is a [charity evaluator](https://forum.effectivealtruism.org/tag/charity-evaluation) focused on [climate change](https://forum.effectivealtruism.org/tag/climate-change). It was incubated by [Charity Entrepreneurship](https://forum.effectivealtruism.org/tag/charity-entrepreneurship).^[\\[1\\]](#fni4b5al9m2z)^\n\nFunding\n-------\n\nAs of July 2022, Giving Green has received $50,000 in funding from [Effective Altruism Funds](https://forum.effectivealtruism.org/topics/effective-altruism-funds).^[\\[2\\]](#fntttvl1ast4)^\n\nExternal links\n--------------\n\n[Giving Green](https://www.givinggreen.earth/). Official website.\n\n[Apply for a job](https://www.givinggreen.earth/careers).\n\nRelated links\n-------------\n\n[charity evaluation](https://forum.effectivealtruism.org/tag/charity-evaluation) | [climate change](https://forum.effectivealtruism.org/tag/climate-change)\n\n1.  ^**[^](#fnrefi4b5al9m2z)**^\n    \n    Charity Entrepreneurship (2020) [Presenting: 2020 incubated charities](https://www.charityentrepreneurship.com/post/presenting-2020-incubated-charities), *Charity Entrepreneurship*, October 13 (updated 8 December 2021).\n    \n2.  ^**[^](#fnreftttvl1ast4)**^\n    \n    Effective Altruism Infrastructure Fund (2021) [May 2021: EA Infrastructure Fund grants](https://funds.effectivealtruism.org/funds/payouts/may-2021-ea-infrastructure-fund-grants), *Effective Altruism Funds*, May."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "FyMr7so883Lt6Yymb",
    "name": "Projekt Framtid",
    "core": false,
    "slug": "projekt-framtid",
    "oldSlugs": null,
    "postCount": null,
    "description": {
      "markdown": "**Projekt Framtid** (\"Project Future\") is a project of Effective Altruism Sweden that promotes the adoption by the Swedish Parliament of  policies and [institutional reforms](https://forum.effectivealtruism.org/topics/longtermist-institutional-reform) designed to better represent the interests of future generations.\n\nExternal links\n--------------\n\n[Projekt Framtid](https://www.projektframtid.nu/). Official website."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "QJNdDqiK6MbXWiqbL",
    "name": "Centre for Long-Term Resilience",
    "core": false,
    "slug": "centre-for-long-term-resilience",
    "oldSlugs": null,
    "postCount": 5,
    "description": {
      "markdown": "The **Centre for Long-Term Resilience** (**CLTR**; previously known as **Alpenglow**) is a UK-based non-profit with current focuses on [artificial intelligence](https://forum.effectivealtruism.org/tag/artificial-intelligence), [biosecurity](https://forum.effectivealtruism.org/tag/biosecurity), and [climate change](https://forum.effectivealtruism.org/tag/climate-change).\n\nFunding\n-------\n\nAs of June 2022, the Centre for Long-Term Resilience has received over $2.8 million in funding from the [Survival and Flourishing](https://forum.effectivealtruism.org/topics/survival-and-flourishing) Fund,^[\\[1\\]](#fn1bx7twr07nk)^^[\\[2\\]](#fna91rkdbsh1w)^^[\\[3\\]](#fnxffbzuqfz2)^ and $100,000 from [Effective Altruism Funds](https://forum.effectivealtruism.org/topics/effective-altruism-funds).^[\\[4\\]](#fnm3un8uqwlgd)^\n\nFurther reading\n---------------\n\nOrd, Toby, Angus Mercer & Sophie Dannreuther (2021) [Future proof: The opportunity to transform the UK’s resilience to extreme risks](https://drive.google.com/file/d/1LHn3nzxF2p68SfhwiPLCb5FMaMLq1dk6/view), June, The Centre for Long-Term Resilience.\n\nExternal links\n--------------\n\n[Centre for Long-Term Resilience](https://www.longtermresilience.org/). Official website.\n\n1.  ^**[^](#fnref1bx7twr07nk)**^\n    \n    Survival and Flourishing Fund (2020) [SFF-2021-H1 S-process recommendations announcement](https://survivalandflourishing.fund/sff-2021-h1-recommendations), *Survival and Flourishing Fund*.\n    \n2.  ^**[^](#fnrefa91rkdbsh1w)**^\n    \n     Survival and Flourishing Fund (2020) [SFF-2021-H2 S-process recommendations announcement](https://survivalandflourishing.fund/sff-2021-h2-recommendations), *Survival and Flourishing Fund*.\n    \n3.  ^**[^](#fnrefxffbzuqfz2)**^\n    \n    Survival and Flourishing Fund (2021) [SFF-2022-H1 S-Process recommendations announcement](https://survivalandflourishing.fund/sff-2022-h1-recommendations), *Survival and Flourishing Fund*.\n    \n4.  ^**[^](#fnrefm3un8uqwlgd)**^\n    \n    Effective Altruism Infrastructure Fund (2021) [May 2021: EA Infrastructure Fund grants](https://funds.effectivealtruism.org/funds/payouts/may-2021-ea-infrastructure-fund-grants), *Effective Altruism Funds*, May."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "CrxYgvS7Wgak45te9",
    "name": "Suvita",
    "core": false,
    "slug": "suvita",
    "oldSlugs": null,
    "postCount": 6,
    "description": {
      "markdown": "**Suvita** is an organisation incubated by [Charity Entrepreneurship](https://forum.effectivealtruism.org/tag/charity-entrepreneurship) and originally focused on running an immunization ambassadors program. In 2020 it merged with Charity Science Health, which had been working to send SMS reminders about immunisations to caregivers, in order to join forces to better increase uptake of routine vaccinations across India.\n\nExternal links\n--------------\n\n[Suvita](https://www.suvita.org/). Official website.\n\n[Apply for a job](https://www.suvita.org/careers)."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "gkFyDdthDJbJQGKte",
    "name": "Sightsavers",
    "core": false,
    "slug": "sightsavers",
    "oldSlugs": null,
    "postCount": null,
    "description": {
      "markdown": "**Sightsavers** is an organization with multiple program areas that focuses on preventing avoidable blindness, supporting people with disabilities, and distributing treatments for [neglected tropical diseases](https://forum.effectivealtruism.org/tag/neglected-tropical-diseases). It works with governments and other organizations in Asia and Africa.\n\nEvaluation\n----------\n\nSightsavers's [deworming](https://forum.effectivealtruism.org/tag/deworming) program was a [GiveWell](https://forum.effectivealtruism.org/tag/givewell) top-rated charity between 2016 and 2022.^[\\[1\\]](#fnf92yj5f2vog)^  GiveWell estimates that Sightsavers can deworm a child at a cost of around $1.^[\\[1\\]](#fnf92yj5f2vog)^^[\\[2\\]](#fnpkz9252zx5g)^\n\nAs of July 2022, Sightsavers has received over $25.2 million in funding from [Open Philanthropy](https://forum.effectivealtruism.org/tag/open-philanthropy).^[\\[3\\]](#fnnlhepyrgo3d)^\n\nFurther reading\n---------------\n\nGiveWell (2021) [Sightsavers’ deworming program](https://www.givewell.org/charities/sightsavers), *GiveWell*, November.\n\nExternal links\n--------------\n\n[Sightsavers](https://www.sightsavers.org/). Official website.\n\n[Apply for a job](https://jobs.sightsavers.org/).\n\n[Donate to Sightsavers](https://donate.sightsavers.org/countries/global/appeals/web2022gl).\n\n1.  ^**[^](#fnreff92yj5f2vog)**^\n    \n    GiveWell (2021) [Sightsavers’ Deworming Program](https://www.givewell.org/charities/sightsavers), *GiveWell*, November.\n    \n2.  ^**[^](#fnrefpkz9252zx5g)**^\n    \n    GiveWell (2021) [Our top charities](https://www.givewell.org/charities/top-charities), *GiveWell*, November.\n    \n3.  ^**[^](#fnrefnlhepyrgo3d)**^\n    \n    Open Philanthropy (2022) [Grants database: Sightsavers](https://www.openphilanthropy.org/grants/?q=&organization-name=sightsavers), *Open Philanthropy*."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "rYmeyPDSKdfcokr2J",
    "name": "Policy Entrepreneurship Network",
    "core": false,
    "slug": "policy-entrepreneurship-network",
    "oldSlugs": null,
    "postCount": 2,
    "description": {
      "markdown": "**Policy Entrepreneurship Network** is a charity operating in [low- and middle-income countries](https://forum.effectivealtruism.org/topics/low-and-middle-income-countries) dedicated to providing local initiatives working on neglected health areas with tailored ways of making use of public health research. It was incubated by [Charity Entrepreneurship](https://forum.effectivealtruism.org/tag/charity-entrepreneurship).\n\nAs of January 2022, the project appears to be no longer active."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "W7nCk5DbayguDq2ev",
    "name": "New Incentives",
    "core": false,
    "slug": "new-incentives",
    "oldSlugs": null,
    "postCount": 2,
    "description": {
      "markdown": "**New Incentives** is a charity that works in North West Nigeria to increase the uptake of routine immunizations through conditional [cash transfers](https://forum.effectivealtruism.org/tag/cash-transfers).\n\nEvaluation\n----------\n\nNew Incentives has been a [GiveWell](https://forum.effectivealtruism.org/tag/givewell) top-rated charity since 2020.^[\\[1\\]](#fnvmy64tkhmar)^ GiveWell estimates that New Incentives can vaccinate an infant at a cost of about $160, and that a donation to New Incentives has an average [cost-effectiveness](https://forum.effectivealtruism.org/tag/cost-effectiveness-analysis) of $5,000 per life saved.^[\\[2\\]](#fnqomn7qzh0oc)^^[\\[3\\]](#fnq9uhfg4vhkr)^^[\\[4\\]](#fnktj2avlzo5)^ (The cost of incentivizing a caregiver to complete a baby's immunization schedule is much lower than the cost of saving a life because a small fraction of children who receive immunization would otherwise have died from the diseases that immunization confers protection against, and because of other factors.)^[\\[3\\]](#fnq9uhfg4vhkr)^\n\nAs of July 2022, New Incentives has received nearly $42 million in funding from [Open Philanthropy](https://forum.effectivealtruism.org/tag/open-philanthropy),^[\\[5\\]](#fnh7uymy1s4vp)^ and is also featured in [The Life You Can Save](https://forum.effectivealtruism.org/tag/the-life-you-can-save)'s list of \"best charities\".^[\\[6\\]](#fnt2u7yg80p1c)^\n\nFurther reading\n---------------\n\nGiveWell (2020) [New Incentives](https://www.givewell.org/charities/new-incentives), *GiveWell*, November.\n\nExternal links\n--------------\n\n[New Incentives](https://www.newincentives.org/). Official website.\n\n[Apply for a job](https://new-incentives.breezy.hr/).\n\n[Donate to New Incentives](https://www.newincentives.org/donate). \n\n1.  ^**[^](#fnrefvmy64tkhmar)**^\n    \n    GiveWell (2020) [New Incentives](https://www.givewell.org/charities/new-incentives), *GiveWell*, November.\n    \n2.  ^**[^](#fnrefqomn7qzh0oc)**^\n    \n    GiveWell (2022) [Our top charities](https://www.givewell.org/charities/top-charities), *GiveWell*, July.\n    \n3.  ^**[^](#fnrefq9uhfg4vhkr)**^\n    \n    GiveWell (2022) [How we produce impact estimates](https://www.givewell.org/impact-estimates), *GiveWell*, July. \n    \n4.  ^**[^](#fnrefktj2avlzo5)**^\n    \n    GiveWell (2022) [GiveWell directed grants to top charities with impact information (2020 onward)](https://docs.google.com/spreadsheets/d/1z065ab9PPMu9i5KiQ4yLyQJPFQCfEzHSgtHulPiZeBo/edit#gid=1407352843), *GiveWell*, July.\n    \n5.  ^**[^](#fnrefh7uymy1s4vp)**^\n    \n    Open Philanthropy (2022) [Grants database: New Incentives](https://www.openphilanthropy.org/grants/?q=&organization-name=new-incentives), *Open Philanthropy*.\n    \n6.  ^**[^](#fnreft2u7yg80p1c)**^\n    \n    The Life You Can Save (2021) [Best charities](https://www.thelifeyoucansave.org/best-charities/), *The Life You Can Save*."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "WMGW3YRrHRcjkxvrp",
    "name": "Charity incubation",
    "core": false,
    "slug": "charity-incubation",
    "oldSlugs": null,
    "postCount": 19,
    "description": {
      "markdown": "**Charity incubation** is the provision of various types of support for the creation of a new charity. This support can involve things like training, advice, seed funding, connections, and operational support. It could also involve actively trying to work out what new types of new charities should be founded (e.g., which interventions it would be worth having a new charity working on) and then finding people to found those charities.\n\nOrganizations that focus primarily on charity incubation are known as **charity incubators**. Examples of charity incubators aligned with [effective altruism](https://forum.effectivealtruism.org/topics/effective-altruism) are [Charity Entrepreneurship](https://forum.effectivealtruism.org/tag/charity-entrepreneurship) and the [Longtermist Entrepreneurship Fellowship](https://forum.effectivealtruism.org/tag/longtermist-entrepreneurship-fellowship). There are also organizations that, despite not being charity incubators, engage in some charity incubation activities. Examples include [GiveWell](https://forum.effectivealtruism.org/tag/givewell) (via its incubation grants).^[\\[1\\]](#fnbzigrvt6v1b)^\n\nRelated entries\n---------------\n\n[Charity Entrepreneurship](https://forum.effectivealtruism.org/tag/charity-entrepreneurship) | [effective altruism funding](https://forum.effectivealtruism.org/tag/effective-altruism-funding) | [entrepreneurship](https://forum.effectivealtruism.org/tag/entrepreneurship) | [Longtermist Entrepreneurship Fellowship](https://forum.effectivealtruism.org/tag/longtermist-entrepreneurship-fellowship) | [operations ](https://forum.effectivealtruism.org/tag/operations)\n\n1.  ^**[^](#fnrefbzigrvt6v1b)**^\n    \n    GiveWell (2021) [GiveWell Incubation Grants](https://www.givewell.org/research/incubation-grants), *GiveWell*, April."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "jE52ujHaE9shykQeP",
    "name": "Lead Exposure Elimination Project",
    "core": false,
    "slug": "lead-exposure-elimination-project",
    "oldSlugs": null,
    "postCount": 10,
    "description": {
      "markdown": "The **Lead Exposure Elimination Project** (**LEEP**) is a nonprofit organization that advocates for regulation of lead paint in low-income countries.\n\nHistory\n-------\n\nLEEP launched in October 2020 with the help of a $60,000 incubation grant by [Charity Entrepreneurship](https://forum.effectivealtruism.org/tag/charity-entrepreneurship).^[\\[1\\]](#fnbrtbkn4wbrs)^\n\nActivities\n----------\n\nLEEP prioritizes work in countries where the problem of lead exposure is unusually tractable, neglected, and large-scale, and they currently focus on Malawi as the most promising country given those criteria. LEEP's activities so far have resulted in a commitment by the Malawi Bureau of Standards to implement a ban on lead paint.^[\\[2\\]](#fn0f829kd4wk1s)^\n\nImpact\n------\n\nA [cost-effectiveness analysis](https://forum.effectivealtruism.org/tag/cost-effectiveness-analysis) by LEEP estimates that their intervention to implement lead paint regulation in Malawi has a cost per [disability-adjusted life-year](https://forum.effectivealtruism.org/tag/adjusted-life-year) of approximately $14.^[\\[3\\]](#fn6dka7oxemim)^\n\nFurther reading\n---------------\n\nRafferty, Jack & Lucia Coulter (2020) [Introducing LEEP: Lead Exposure Elimination Project](https://forum.effectivealtruism.org/posts/fd96FtLFACeAshqJP/introducing-leep-lead-exposure-elimination-project), *Effective Altruism Forum*, October 6.\n\nRafferty, Jack & Lucia Coulter (2021) [Seven things that surprised us in our first year working in policy - Lead Exposure Elimination Project](https://forum.effectivealtruism.org/posts/ErKzbKWnQMwvzRX4m/seven-things-that-surprised-us-in-our-first-year-working-in), *Effective Altruism Forum*, May 14.\n\nExternal links\n--------------\n\n[Lead Exposure Elimination Project](https://leadelimination.org/). Official website.\n\n[Apply for a job](https://leadelimination.org/jobs/).\n\nRelated entries\n---------------\n\n[Charity Entrepreneurship](https://forum.effectivealtruism.org/tag/charity-entrepreneurship) **|** [global health and development](https://forum.effectivealtruism.org/tag/global-health-and-development) | [policy change](https://forum.effectivealtruism.org/tag/policy-change)\n\n1.  ^**[^](#fnrefbrtbkn4wbrs)**^\n    \n    Charity Entrepreneurship (2020) [Presenting: 2020 Incubated Charities](http://www.charityentrepreneurship.com/1/post/2020/10/presenting-2020-incubated-charities.html), *Charity Entrepreneurship Blog*, October 13.\n    \n2.  ^**[^](#fnref0f829kd4wk1s)**^\n    \n    Bernard, David & Jason Schukraft (2021) [Global lead exposure report](https://forum.effectivealtruism.org/posts/naTwu3xD3WFWu5fbp/global-lead-exposure-report), *Effective Altruism Forum*, May 29.\n    \n3.  ^**[^](#fnref6dka7oxemim)**^\n    \n    Hu, James (2022) [How cost-effective is LEEP’s Malawi program?](https://leadelimination.org/malawi_cost-effectiveness_intro/), *LEEP’s Blog*, January 13."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "JDRpnhC5khoNQjk5m",
    "name": "IDInsight",
    "core": false,
    "slug": "idinsight",
    "oldSlugs": null,
    "postCount": 6,
    "description": {
      "markdown": "**IDInsight** is a research organisation dedicated to providing effective advice by means of data analysis and evidence tools such as [randomized evaluations](https://forum.effectivealtruism.org/tag/randomized-controlled-trials) and [artificial intelligence](https://forum.effectivealtruism.org/tag/artificial-intelligence). It works with governments and other organizations in Asia and Africa.\n\nFunding\n-------\n\nAs of July 2022, IDInsight has received over $8.6 million in grants from [Open Philanthropy](https://forum.effectivealtruism.org/topics/open-philanthropy),^[\\[1\\]](#fnj9rn1ltkl2o)^  and over $1.2 million from [Effective Altruism Funds](https://forum.effectivealtruism.org/topics/effective-altruism-funds).^[\\[2\\]](#fnagwntp3d0si)^^[\\[3\\]](#fnpo4lowdjc0c)^\n\nExternal links\n--------------\n\n[IDInsight](https://www.idinsight.org/). Official website.\n\n[Donate to IDinsight](https://www.idinsight.org/give/).\n\n1.  ^**[^](#fnrefj9rn1ltkl2o)**^\n    \n    Open Philanthropy (2022) [Grants datab](https://www.openphilanthropy.org/grants/?q=&organization-name=the-center-for-election-science)[a](https://www.openphilanthropy.org/grants/?q=&organization-name=idinsight)[se: IDInsight](https://www.openphilanthropy.org/grants/?q=&organization-name=the-center-for-election-science), *Open Philanthropy*.\n    \n2.  ^**[^](#fnrefagwntp3d0si)**^\n    \n    Global Health and Development Fund (2020) [June 2020: IDinsight](https://funds.effectivealtruism.org/funds/payouts/june-2020-idinsight), *Effective Altruism Funds*, June. \n    \n3.  ^**[^](#fnrefpo4lowdjc0c)**^\n    \n    Global Health and Development Fund (2020) [September 2020: IDinsight](https://funds.effectivealtruism.org/funds/payouts/september-2020-idinsight), *Effective Altruism Funds*, September."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "LaxL2irAPrvNjmixQ",
    "name": "Helen Keller International",
    "core": false,
    "slug": "helen-keller-international",
    "oldSlugs": null,
    "postCount": 2,
    "description": {
      "markdown": "**Helen Keller International** (**HKI**) is a nonprofit organization focused on preventing blindness and reducing malnutrition.\n\nActivities\n----------\n\nHKI carries out over 120 programs in 21 countries around the world, including cataract surgery, vision correction, screening and treatment for diabetic retinopathy, distribution of treatments and cures for [neglected tropical diseases](https://forum.effectivealtruism.org/tag/neglected-tropical-diseases), maternal and child nutrition education, and community-based management of acute malnutrition .^[\\[1\\]](#fny98d000b4ug)^ Charity evaluators within the effective altruism community have largely focused on HKI's vitamin A supplementation (VAS) program, which distributes vitamin A supplements in sub-Saharan Africa.\n\nEvaluation\n----------\n\nAs of July 2022, [GiveWell](https://forum.effectivealtruism.org/tag/givewell) estimates that HKI can deliver a vitamin A supplement at a cost of about $1, and that a donation to HKI's VAS program has an average [cost-effectiveness](https://forum.effectivealtruism.org/tag/cost-effectiveness-analysis) of $3,500 per life saved.^[\\[2\\]](#fn1bk24hkiw5l)^^[\\[3\\]](#fnlmq152187zo)^^[\\[4\\]](#fnncxfo2wvtba)^ (The cost of distributing vitamin supplements is much lower than the cost of saving a life because a small fraction of people who receive supplementation would otherwise have died from conditions related to vitamin deficiency, and because of other factors.)^[\\[3\\]](#fnlmq152187zo)^\n\nHKI's VAS program has been a GiveWell top-rated charity since 2017, and its cost-effectiveness is estimated to be in the same range as GiveWell's other priority programs.^[\\[5\\]](#fn5slker0nr4i)^ As of July 2022, HKI has received over $31 million in funding from [Open Philanthropy](https://forum.effectivealtruism.org/tag/open-philanthropy),^[\\[6\\]](#fnac2yitiymqq)^ and is also featured in [The Life You Can Save](https://forum.effectivealtruism.org/tag/the-life-you-can-save)'s list of \"best charities\".^[\\[7\\]](#fnrlc1zvbr0t)^\n\nFurther reading\n---------------\n\nHelen Keller International (2021) [About us](https://www.hki.org/who-we-are/about-us/), *Helen Keller International*.\n\nSánchez, Sebastián (2021) [Timeline of Helen Keller International](https://timelines.issarice.com/wiki/Timeline_of_Helen_Keller_International), *Timelines Wiki*.\n\nExternal links\n--------------\n\n[Helen Keller International](https://www.hki.org/). Official website.\n\n[Apply for a job](https://www.hki.org/careers/).\n\n[Donate to Helen Keller International](https://donate.hki.org/).\n\nRelated entries\n---------------\n\n[global health and development](https://forum.effectivealtruism.org/tag/global-health-and-development) | [global poverty](https://forum.effectivealtruism.org/tag/global-poverty) | [micronutrient fortification](https://forum.effectivealtruism.org/tag/micronutrient-fortification)\n\n1.  ^**[^](#fnrefy98d000b4ug)**^\n    \n    Helen Keller International (2021) [2020 annual report](https://www.hki.org/wp-content/uploads/2021/06/HKIntl_Annual_Report_2020_web-2.pdf), *Helen Keller International*.\n    \n2.  ^**[^](#fnref1bk24hkiw5l)**^\n    \n    GiveWell (2022) [Our top charities](https://www.givewell.org/charities/top-charities), *GiveWell*, July.\n    \n3.  ^**[^](#fnreflmq152187zo)**^\n    \n    GiveWell (2022) [How we produce impact estimates](https://www.givewell.org/impact-estimates), *GiveWell*, July. \n    \n4.  ^**[^](#fnrefncxfo2wvtba)**^\n    \n    GiveWell (2022) [GiveWell directed grants to top charities with impact information (2020 onward)](https://docs.google.com/spreadsheets/d/1z065ab9PPMu9i5KiQ4yLyQJPFQCfEzHSgtHulPiZeBo/edit#gid=1407352843), *GiveWell*, July.\n    \n5.  ^**[^](#fnref5slker0nr4i)**^\n    \n    GiveWell (2021) [Helen Keller International’s Vitamin A Supplementation Program](https://www.givewell.org/charities/helen-keller-international), *GiveWell*, November.\n    \n6.  ^**[^](#fnrefac2yitiymqq)**^\n    \n    Open Philanthropy (2022) [Grants database: Helen Keller International](https://www.openphilanthropy.org/grants/?q=&organization-name=helen-keller-international&organization-name=hellen-keller-international), *Open Philanthropy*.\n    \n7.  ^**[^](#fnrefrlc1zvbr0t)**^\n    \n    The Life You Can Save (2021) [Best charities](https://www.thelifeyoucansave.org/best-charities/), *The Life You Can Save*."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "i3fLeT4brYJoRKvHP",
    "name": "Fortify Health",
    "core": false,
    "slug": "fortify-health",
    "oldSlugs": null,
    "postCount": 7,
    "description": {
      "markdown": "**Fortify Health** is an organization that aims to tackle anaemia and neural tube defects in [India](https://forum.effectivealtruism.org/tag/india) through iron [fortification](https://forum.effectivealtruism.org/topics/micronutrient-deficiency) of wheat flour.\n\nFunding\n-------\n\nAs of July 2022, [GiveWell](https://forum.effectivealtruism.org/tag/givewell) has made three incubation grants to Fortify Health for a total of $9.5 million.^[\\[1\\]](#fnuv6y2qew9go)^^[\\[2\\]](#fny7y63zzxptr)^^[\\[3\\]](#fnnv9nofdzacc)^ These grants were funded by the [Global Health and Development Fund](https://forum.effectivealtruism.org/topics/global-health-and-development-fund) and [Open Philanthropy](https://forum.effectivealtruism.org/topics/open-philanthropy).^[\\[4\\]](#fnlauvi26maqs)^^[\\[5\\]](#fn5ad4pdlz9lv)^  \n\nFurther reading\n---------------\n\nEappen, Brendan (2019) [Fortitude, collaboration, and humility: lessons from an EA-aligned charity startup](https://forum.effectivealtruism.org/posts/eMDBygj4DMaRGcZx7/brendan-eappen-lessons-from-an-ea-aligned-charity-startup), *Effective Altruism Global*, June 23.\n\nGiveWell (2022) [Fortify Health – Support for expansion (December 2021)](https://www.givewell.org/research/incubation-grants/Fortify-Health-expansion-December-2021?fbclid=IwAR1v0MucbsjAGl2eGaACY9fL9X_91JRhakv6o1CSdbFiL2_HbHSybE_X20o), *GiveWell*, April.\n\nExternal links\n--------------\n\n[Fortify Health](https://www.fortifyhealth.global/). Official website.\n\n[Donate to Fortify Health](https://www.fortifyhealth.global/donate.html).\n\nRelated entries\n---------------\n\n[global health and development](https://forum.effectivealtruism.org/tag/global-health-and-development) | [India](https://forum.effectivealtruism.org/tag/india) | [micronutrient deficiency](https://forum.effectivealtruism.org/topics/micronutrient-deficiency)\n\n1.  ^**[^](#fnrefuv6y2qew9go)**^\n    \n    GiveWell (2018) [Fortify Health — General support](https://www.givewell.org/research/incubation-grants/fortify-health/june-2018-grant), *GiveWell*, June.\n    \n2.  ^**[^](#fnrefy7y63zzxptr)**^\n    \n    GiveWell (2020) [Fortify Health — General support (2019)](https://www.givewell.org/research/incubation-grants/fortify-health/august-2019-grant), *GiveWell*, January.\n    \n3.  ^**[^](#fnrefnv9nofdzacc)**^\n    \n    GiveWell (2022) [Fortify Health – Support for expansion (December 2021)](https://www.givewell.org/research/incubation-grants/Fortify-Health-expansion-December-2021?fbclid=IwAR1v0MucbsjAGl2eGaACY9fL9X_91JRhakv6o1CSdbFiL2_HbHSybE_X20o), *GiveWell*, April.\n    \n4.  ^**[^](#fnreflauvi26maqs)**^\n    \n    Global Health and Development Fund (2019) [August 2019: Fortify Health](https://funds.effectivealtruism.org/funds/payouts/august-2019-fortify-health), *Effective Altruism Funds*, August.\n    \n5.  ^**[^](#fnref5ad4pdlz9lv)**^\n    \n    Open Philanthropy (2022) [Grants database: Fortify Health](https://www.openphilanthropy.org/grants/?q=&organization-name=fortify-health), *Open Philanthropy*."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "kxKBLEqCDvaPNzY8u",
    "name": "Family Empowerment Media",
    "core": false,
    "slug": "family-empowerment-media",
    "oldSlugs": null,
    "postCount": 5,
    "description": {
      "markdown": "**Family Empowerment Media** is a non-profit that aims to increase awareness and knowledge of modern forms of contraception through radio-based announcements and interactive programs, in order to encourage informed [family planning](https://forum.effectivealtruism.org/tag/family-planning) and birth spacing decisions. It was incubated by [Charity Entrepreneurship](https://forum.effectivealtruism.org/tag/charity-entrepreneurship).\n\nFurther reading\n---------------\n\nScheffler, Kenneth & Anna Christina Thorsheim (2020) [Introducing Family Empowerment Media](https://www.familyempowermentmedia.org/post/introducingfem), *Family Empowerment Media*, November 29 (updated 24 October 2021).\n\nExternal links\n--------------\n\n[Family Empowerment Media](https://www.familyempowermentmedia.org/). Official website.\n\nRelated entries\n---------------\n\n[family planning](https://forum.effectivealtruism.org/tag/family-planning)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "LbHg4gZkMdEm9ak2H",
    "name": "Longtermist Entrepreneurship Fellowship",
    "core": false,
    "slug": "longtermist-entrepreneurship-fellowship",
    "oldSlugs": null,
    "postCount": 4,
    "description": {
      "markdown": "The **Longtermist Entrepreneurship Fellowship** (also called the **Longtermist Entrepreneurship Program**^[\\[1\\]](#fn7j7kezof61x)^ and the **Longtermist Entrepreneurship Fellowship Program**^[\\[2\\]](#fn1ok87gu9ie6)^) was a [longtermist](https://forum.effectivealtruism.org/topics/longtermism) project [incubator](https://forum.effectivealtruism.org/topics/charity-incubation) that ran from April 2020 through May 2021.\n\nFurther reading\n---------------\n\nKagan, Rebecca, Jade Leung & Ben Clifford (2021) [What we learned from a year incubating longtermist entrepreneurship](https://forum.effectivealtruism.org/posts/z56YFpphrQDTSPLqi/what-we-learned-from-a-year-incubating-longtermist), *Effective Altruism Forum*, August 30.\n\nRelated entries\n---------------\n\n[charity incubation](https://forum.effectivealtruism.org/tag/charity-incubation) | [entrepreneurship](https://forum.effectivealtruism.org/tag/entrepreneurship) | [longtermism](https://forum.effectivealtruism.org/topics/longtermism)\n\n1.  ^**[^](#fnref7j7kezof61x)**^\n    \n    Kagan, Rebecca, Jade Leung & Ben Clifford (2021) [What we learned from a year incubating longtermist entrepreneurship](https://forum.effectivealtruism.org/posts/z56YFpphrQDTSPLqi/what-we-learned-from-a-year-incubating-longtermist), *Effective Altruism Forum*, August 30.\n    \n2.  ^**[^](#fnref1ok87gu9ie6)**^\n    \n    Bergal, Asya (2021) [Long-Term Future Fund: May 2021 grant recommendations](https://forum.effectivealtruism.org/posts/diZWNmLRgcbuwmYn4/long-term-future-fund-may-2021-grant-recommendations), *Effective Altruism Forum*, May 27."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "pLwWCPoSizr8pbLdy",
    "name": "Independent research",
    "core": false,
    "slug": "independent-research",
    "oldSlugs": null,
    "postCount": 28,
    "description": {
      "markdown": "**Independent research** is research conducted by an individual who is not employed by any organisation or institution, or who is employed but is conducting this research separately from that. This person may or may not have funding for this research (e.g., via grants). Research that is done by two or more people collaborating, but still separate from an organisation or institution, could arguably be considered independent research.\n\nThere are various advantages and disadvantages of independent research relative to research conducted as part of an organisation or institution.\n\nThis tag is intended for posts relevant to things like tips for doing independent research or advantages and disadvantages of independent research, not just for posts that are examples of independent research.\n\nRelated entries\n---------------\n\n[effective altruism funding](https://forum.effectivealtruism.org/tag/effective-altruism-funding) | [org strategy](https://forum.effectivealtruism.org/tag/org-strategy) | [research methods](https://forum.effectivealtruism.org/tag/research-methods) | [research training programs](https://forum.effectivealtruism.org/tag/research-training-programs) | [scalably using labour](https://forum.effectivealtruism.org/tag/scalably-using-labour)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "9AWwpyjfWmp5qhgMc",
    "name": "Cambridge Summer Programme in Applied Reasoning",
    "core": false,
    "slug": "cambridge-summer-programme-in-applied-reasoning",
    "oldSlugs": [
      "caspar"
    ],
    "postCount": null,
    "description": {
      "markdown": "The **Cambridge Summer Programme in Applied Reasoning** (**CaSPAR**) is \"an immersive one week programme for mathematically talented students with a desire to understand themselves and the world.\"^[\\[1\\]](#fn87xm5bzm5wf)^\n\nFunding\n-------\n\nAs of July 2022, CaSPAR has received nearly $60,000 in funding from [Effective Altruism Funds](https://forum.effectivealtruism.org/topics/effective-altruism-funds).^[\\[2\\]](#fnfjykrqnp0xg)^^[\\[3\\]](#fnjnmnxyxbdw)^\n\nExternal links\n--------------\n\n[CaSPAR](https://www.caspar-camp.uk/). Official website.\n\nRelated entries\n---------------\n\n[building effective altruism](https://forum.effectivealtruism.org/tag/building-effective-altruism-1) | [Center for Applied Rationality](https://forum.effectivealtruism.org/tag/center-for-applied-rationality) | [effective altruism education](https://forum.effectivealtruism.org/tag/ea-education) | [European Summer Program on Rationality](https://forum.effectivealtruism.org/tag/espr) | [rationality](https://forum.effectivealtruism.org/tag/rationality) | [rationality community](https://forum.effectivealtruism.org/tag/rationality-community) | [research training programs](https://forum.effectivealtruism.org/tag/research-training-programs) | [Summer Program on Applied Rationality and Cognition](https://forum.effectivealtruism.org/tag/sparc)\n\n1.  ^**[^](#fnref87xm5bzm5wf)**^\n    \n    Cambridge Summer Programme in Applied Reasoning (2021) [An intensive week-long workshop in applied reasoning for mathematicians](https://www.caspar-camp.uk/), *Cambridge Summer Programme in Applied Reasoning*.\n    \n2.  ^**[^](#fnreffjykrqnp0xg)**^\n    \n    Long-Term Future Fund (2020) [September 2020: Long-Term Future Fund grants](https://funds.effectivealtruism.org/funds/payouts/september-2020-long-term-future-fund-grants), *Effective Altruism Funds*, September.\n    \n3.  ^**[^](#fnrefjnmnxyxbdw)**^\n    \n    Long-Term Future Fund (2020) [November 2020: Long-Term Future Fund grants](https://funds.effectivealtruism.org/funds/payouts/november-2020-long-term-future-fund-grants), *Effective Altruism Funds*, November."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "e5k9q5PiRFBZ3CDTp",
    "name": "European Summer Program on Rationality",
    "core": false,
    "slug": "european-summer-program-on-rationality",
    "oldSlugs": [
      "espr"
    ],
    "postCount": 1,
    "description": {
      "markdown": "The **European Summer Program on Rationality** (**ESPR**) is \"an immersive summer workshop for mathematically talented students with a desire to understand themselves and the world. \\[...\\] Past participants include International Math Olympiad gold medalists, web entrepreneurs, and student researchers. ESPR orginated as EuroSPARC, which was modeled after its American equivalent, SPARC.\"^[\\[1\\]](#fnhkmsxgb8f3f)^\n\nFunding\n-------\n\nAs of July 2022, ESPR has received $510,000 in funding from [Open Philanthropy](https://forum.effectivealtruism.org/topics/open-philanthropy).^[\\[2\\]](#fnqjccajt02gi)^\n\nFurther reading\n---------------\n\nOpen Philanthropy (2017) [Center for Applied Rationality — European Summer Program on Rationality](https://www.openphilanthropy.org/giving/grants/center-applied-rationality-european-summer-program-rationality), *Open Philanthropy*, September.\n\nExternal links\n--------------\n\n[European Summer Program on Rationality](https://espr-camp.org/). Official website.\n\n[Apply for ESPR](https://espr-camp.org/apply-now).\n\nRelated entries\n---------------\n\n[building effective altruism](https://forum.effectivealtruism.org/tag/building-effective-altruism-1) | [CaSPAR](https://forum.effectivealtruism.org/tag/caspar) | [Center for Applied Rationality](https://forum.effectivealtruism.org/tag/center-for-applied-rationality) | [effective altruism education](https://forum.effectivealtruism.org/tag/effective-altruism-education) | [rationality](https://forum.effectivealtruism.org/tag/rationality) | [rationality community](https://forum.effectivealtruism.org/tag/rationality-community) | [research training programs](https://forum.effectivealtruism.org/tag/research-training-programs) | [SPARC](https://forum.effectivealtruism.org/tag/sparc)\n\n1.  ^**[^](#fnrefhkmsxgb8f3f)**^\n    \n    European Summer Program on Rationality (2021) [What is ESPR](https://espr-camp.org/), *ESPR*.\n    \n2.  ^**[^](#fnrefqjccajt02gi)**^\n    \n    Open Philanthropy (2022) [Grants database: European Summer Program on Rationality](https://www.openphilanthropy.org/grants/?q=&organization-name=european-summer-program-on-rationality), *Open Philanthropy*."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "qrCjfikBNuEuMywYS",
    "name": "Anthropic",
    "core": false,
    "slug": "anthropic",
    "oldSlugs": null,
    "postCount": 5,
    "description": {
      "markdown": "**Anthropic** is an [AI safety](https://forum.effectivealtruism.org/tag/ai-safety) and research company. It was founded in May 2021 by siblings Dario Amodei and Daniela Amodei, who serve as CEO and President, respectively.^[\\[1\\]](#fnb99fohfg1es)^^[\\[2\\]](#fnao6asdpk65h)^\n\nAnthropic raised $124 million in a series A founding round. The round was led by [Jaan Tallinn](https://forum.effectivealtruism.org/topics/jaan-tallinn), and included participation from James McClave, [Dustin Moskovitz](https://forum.effectivealtruism.org/tag/dustin-moskovitz), the Center for Emerging Risk Research (now [Polaris Ventures](https://forum.effectivealtruism.org/topics/polaris-ventures)), Eric Schmidt, and others.^[\\[3\\]](#fnjv5u06gbe9q)^^[\\[4\\]](#fnn7w02yx0nom)^^[\\[5\\]](#fntnjhms6b2g)^\n\nAnthropic raised a further $580 million in a series B founding round. The round was led by [Sam Bankman-Fried](https://forum.effectivealtruism.org/tag/sam-bankman-fried), and included participation from Caroline Ellison, Jim McClave, Nishad Singh, as well as Tallinn and CERR.^[\\[6\\]](#fnr6gct1aow8)^\n\nFurther reading\n---------------\n\nPerry, Lucas (2022) [Daniela and Dario Amodei on Anthropic](https://futureoflife.org/2022/03/04/daniela-and-dario-amodei-on-anthropic/), *Future of Life Institute*, March 4.\n\nExternal links\n--------------\n\n[Anthropic](https://www.anthropic.com/). Official website.\n\n[Apply for a job](https://www.anthropic.com/#careers).\n\nRelated entries\n---------------\n\n[AI safety](https://forum.effectivealtruism.org/tag/ai-safety) | [OpenAI](https://forum.effectivealtruism.org/tag/openai)\n\n1.  ^**[^](#fnrefb99fohfg1es)**^\n    \n    Amodei, Daniela (2021) [Excited to announce what we’ve been working on this year - @AnthropicAI, an AI safety and research company](https://twitter.com/DanielaAmodei/status/1398306463945158656), *Twitter*, May 28.\n    \n2.  ^**[^](#fnrefao6asdpk65h)**^\n    \n    Waters, Richard & Miles Kruppa (2021) [Rebel AI group raises record cash after machine learning schism](https://www.ft.com/content/8de92f3a-228e-4bb8-961f-96f2dce70ebb), *Financial Times*, May 28.\n    \n3.  ^**[^](#fnrefjv5u06gbe9q)**^\n    \n    Anthropic (2021) [Anthropic raises $124 million to build more reliable, general AI systems](https://www.anthropic.com/news/announcement), *Anthropic*, May 28.\n    \n4.  ^**[^](#fnrefn7w02yx0nom)**^\n    \n    Piper, Kelsey (2021) [Future Perfect Newsletter](https://voxcom.cmail19.com/t/ViewEmail/d/5E61BDB945C3771F2540EF23F30FEDED/7655520C49A52981BA4AF9908B8D85ED), *Vox*, May 28.\n    \n5.  ^**[^](#fnreftnjhms6b2g)**^\n    \n    Coldewey, Devin (2021) [Anthropic is the new AI research outfit from OpenAI’s Dario Amodei, and it has $124M to burn](https://techcrunch.com/2021/05/28/anthropic-is-the-new-ai-research-outfit-from-openais-dario-amodei-and-it-has-124m-to-burn/), *TechCrunch*, May 28.\n    \n6.  ^**[^](#fnrefr6gct1aow8)**^\n    \n    Coldewey, Devin (2022) [Anthropic’s quest for better, more explainable AI attracts $580M](https://techcrunch.com/2022/04/29/anthropics-quest-for-better-more-explainable-ai-attracts-580m/), *TechCrunch*, April 29."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "xFiRgKngDusLXyKy5",
    "name": "Summer Program on Applied Rationality and Cognition",
    "core": false,
    "slug": "summer-program-on-applied-rationality-and-cognition",
    "oldSlugs": [
      "sparc"
    ],
    "postCount": 15,
    "description": {
      "markdown": "The **Summer Program on Applied Rationality and Cognition** (**SPARC**) is a free program that aims at helping high school students develop quantitative thinking skills and improve mental habits by applying techniques from [decision theory](https://forum.effectivealtruism.org/tag/decision-theory), [behavioral economics](https://forum.effectivealtruism.org/tag/economics) and [statistics](https://forum.effectivealtruism.org/tag/statistical-methods). It takes place at California State University, East Bay, typically for two weeks in late July. Boarding and food are provided free of charge.\n\nFurther reading\n---------------\n\nOpen Philanthropy (2016) [Center for Applied Rationality — SPARC](https://www.openphilanthropy.org/giving/grants/center-applied-rationality-sparc), *Open Philanthropy*, July.\n\nExternal links\n--------------\n\n[SPARC](https://sparc-camp.org/). Official website.\n\nRelated entries\n---------------\n\n[building effective altruism](https://forum.effectivealtruism.org/tag/building-effective-altruism-1) | [Cambridge Summer Programme in Applied Reasoning](https://forum.effectivealtruism.org/tag/cambridge-summer-programme-in-applied-reasoning) | [Center for Applied Rationality](https://forum.effectivealtruism.org/tag/center-for-applied-rationality) | [effective altruism education](https://forum.effectivealtruism.org/tag/ea-education) | [European Summer Program on Rationality](https://forum.effectivealtruism.org/tag/european-summer-program-on-rationality) | [rationality](https://forum.effectivealtruism.org/tag/rationality) | [rationality community](https://forum.effectivealtruism.org/tag/rationality-community) | [research training programs](https://forum.effectivealtruism.org/tag/research-training-programs)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "s6XCioNwagv3P3DC3",
    "name": "AI Safety Support",
    "core": false,
    "slug": "ai-safety-support",
    "oldSlugs": null,
    "postCount": 7,
    "description": {
      "markdown": "**AI Safety Support** (**AISS**) is an organization focused on reducing [existential](https://forum.effectivealtruism.org/tag/existential-risk) and [catastrophic](https://forum.effectivealtruism.org/tag/global-catastrophic-risk) risk from [artificial intelligence](https://forum.effectivealtruism.org/tag/artificial-intelligence) by providing support to those interested in working in this cause area.\n\nFunding\n-------\n\nAs of July 2022, AISS has received $200,000 in funding from the [Survival and Flourishing](https://forum.effectivealtruism.org/topics/survival-and-flourishing) Fund,^[\\[1\\]](#fnvgjz05groym)^ $200,000 from the [Future Fund](https://forum.effectivealtruism.org/topics/future-fund),^[\\[2\\]](#fnozq5ng7cqh)^ and $25,000 from [Effective Altruism Funds](https://forum.effectivealtruism.org/topics/effective-altruism-funds).^[\\[3\\]](#fn3cq890ohsv8)^ \n\nExternal links\n--------------\n\n[AI Safety Support](https://www.aisafetysupport.org/). Official website.\n\nRelated entries\n---------------\n\n[AI safety](https://forum.effectivealtruism.org/tag/ai-safety) | [existential risk](https://forum.effectivealtruism.org/tag/existential-risk) | [global catastrophic risk](https://forum.effectivealtruism.org/tag/global-catastrophic-risk)\n\n1.  ^**[^](#fnrefvgjz05groym)**^\n    \n    Survival and Flourishing Fund (2020) [SFF-2021-H1 S-process recommendations announcement](https://survivalandflourishing.fund/sff-2021-h1-recommendations), *Survival and Flourishing Fund*.\n    \n2.  ^**[^](#fnrefozq5ng7cqh)**^\n    \n    Future Fund (2022) [Our grants and investments: AI Safety Support](https://ftxfuturefund.org/all-grants/?_organization_name=ai-safety-support), *Future Fund*.\n    \n3.  ^**[^](#fnref3cq890ohsv8)**^\n    \n    Long-Term Future Fund (2021) [July 2021: Long-Term Future Fund grants](https://funds.effectivealtruism.org/funds/payouts/july-2021-long-term-future-fund-grants), *Effective Altruism Funds*, July."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "AMEgA2KBBEXzjK5Cm",
    "name": "Weapons of mass destruction",
    "core": false,
    "slug": "weapons-of-mass-destruction",
    "oldSlugs": [
      "weapons-of-mass-destruction",
      "weapon-of-mass-destruction"
    ],
    "postCount": 5,
    "description": {
      "markdown": "**Weapons of mass destruction** (**WMD**) are weapons whose destructive capacity far exceeds that of conventional weaponry. Modern WMDs are nuclear, biological or chemical weapons, which are frequently referred to collectively as **NBC weapons**.\n\nRelated entries\n---------------\n\n[anthropogenic existential risk](https://forum.effectivealtruism.org/tag/anthropogenic-existential-risk) | [armed conflict](https://forum.effectivealtruism.org/tag/armed-conflict) | [biosecurity](https://forum.effectivealtruism.org/tag/biosecurity) | [global governance](https://forum.effectivealtruism.org/tag/global-governance) | [Nuclear Threat Initiative](https://forum.effectivealtruism.org/tag/nuclear-threat-initiative) | [nuclear warfare](https://forum.effectivealtruism.org/tag/nuclear-warfare-1) | [peace and conflict studies](https://forum.effectivealtruism.org/tag/peace-and-conflict-studies) | [technology race](https://forum.effectivealtruism.org/topics/technology-race) | [terrorism](https://forum.effectivealtruism.org/tag/terrorism)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "GfSwkCRufanoxajtG",
    "name": "Effective Altruism Israel",
    "core": false,
    "slug": "effective-altruism-israel",
    "oldSlugs": [
      "ea-israel"
    ],
    "postCount": 7,
    "description": {
      "markdown": "**Effective Altruism Israel** is an [effective altruism group](https://forum.effectivealtruism.org/tag/effective-altruism-groups) based in Israel.\n\nFurther reading\n---------------\n\nEffective Altruism Israel (2021) [Strategy document](https://docs.google.com/document/d/1UoAdSID0crufgE6S_vLWxu8huCj0L1snCnbYSZygPCk/edit#heading=h.umsov078gk96), *Effective Altruism Israel*, May 10.\n\nExternal links\n--------------\n\n[Effective Altruism Israel](https://effective-altruism.org.il/). Official website.\n\nRelated entries\n---------------\n\n[effective altruism groups](https://forum.effectivealtruism.org/tag/effective-altruism-groups)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "dftysM9XeTXQYMbiC",
    "name": "Cooperative AI Foundation",
    "core": false,
    "slug": "cooperative-ai-foundation",
    "oldSlugs": null,
    "postCount": 2,
    "description": {
      "markdown": "The **Cooperative AI Foundation** (**CAIF**) is a nonprofit organization that seeks to support research with the potential to \"improve the cooperative intelligence of advanced AI for the benefit of all humanity.\"^[\\[1\\]](#fnd9kwn016nc5)^\n\nCAIF was established with a $15 million grant from the Center for Emerging Risk Research (now [Polaris Ventures](https://forum.effectivealtruism.org/topics/polaris-ventures)).^[\\[2\\]](#fnrz638hue0i)^  It plans to use this endowment to make grants and scholarships, organize workshops and seminar series, and engage in other activities that contribute to its mission.^[\\[3\\]](#fnqnc2n2f3cz)^\n\nFurther reading\n---------------\n\nDafoe, Allan *et al.* (2021) [Cooperative AI: Machines must learn to find common ground](https://www.nature.com/articles/d41586-021-01170-0), *Nature*, May 4.\n\nExternal links\n--------------\n\n[Cooperative AI Foundation](https://www.cooperativeai.com/). Official website.\n\nRelated entries\n---------------\n\n[artificial intelligence](https://forum.effectivealtruism.org/tag/artificial-intelligence) | [Center for Emerging Risk Research](https://forum.effectivealtruism.org/tag/center-for-emerging-risk-research) | [cooperative AI](https://forum.effectivealtruism.org/topics/cooperative-ai-1) | [moral cooperation](https://forum.effectivealtruism.org/tag/moral-cooperation)\n\n1.  ^**[^](#fnrefd9kwn016nc5)**^\n    \n    Cooperative AI Foundation (2021) [Foundation](https://www.cooperativeai.com/foundation), *Cooperative AI Foundation*.\n    \n2.  ^**[^](#fnrefrz638hue0i)**^\n    \n    Center for Emerging Risk Research (2021) [Commitment to fund the Cooperative AI Foundation](https://emergingrisk.ch/cooperativeai/), *Center for Emerging Risk Research.* \n    \n3.  ^**[^](#fnrefqnc2n2f3cz)**^\n    \n    Dafoe, Allan *et al.* (2021) [Cooperative AI: Machines must learn to find common ground](https://www.nature.com/articles/d41586-021-01170-0), *Nature*, May 4."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "pMQ3uHMmzyWMaqH5E",
    "name": "Misinformation",
    "core": false,
    "slug": "misinformation-1",
    "oldSlugs": [
      "disinformation"
    ],
    "postCount": 10,
    "description": {
      "markdown": "**Misinformation** is the spread of false information. When false information is spread knowingly, the associated misinformation is generally referred to as **disinformation**.\n\nRelated entries\n---------------\n\nWiblin, Robert & Keiran Harris (2021) [Nina Schick on disinformation and the rise of synthetic media](https://80000hours.org/podcast/episodes/nina-schick-disinformation-synthetic-media/), *80,000 Hours*, April 6.\n\nRelated entries\n---------------\n\n[AI governance](https://forum.effectivealtruism.org/topics/ai-governance) | [AI risk](https://forum.effectivealtruism.org/tag/ai-risk) | [epistemology](https://forum.effectivealtruism.org/tag/epistemology) | [information hazard](https://forum.effectivealtruism.org/tag/information-hazard) | [psychology](https://forum.effectivealtruism.org/tag/psychology) | [rationality](https://forum.effectivealtruism.org/tag/rationality)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "rJwMNzqZEqCiies7L",
    "name": "International trade",
    "core": false,
    "slug": "international-trade",
    "oldSlugs": [
      "international-trade"
    ],
    "postCount": 6,
    "description": {
      "markdown": "**International trade** is the exchange of good and services between countries.\n\nFurther reading\n---------------\n\nCowen, Tyler & Alex Tabarrok (2020) [International trade](https://mru.org/international-trade), *Marginal Revolution University*.\n\nGopinath, Gita, Elhanan Helpman & Kenneth Rogoff (eds.) (2014) [*Handbook of International Economics*](http://doi.org/10.1016/b978-0-444-54314-1.00014-8), vol. 4, Amsterdam: Elsevier.\n\nWonnacott, Paul (1998) [International trade](https://www.britannica.com/topic/international-trade), *Encyclopedia Britannica*, July 20 (updated 11 November 2020).\n\nRelated entries\n---------------\n\n[economics](https://forum.effectivealtruism.org/tag/economics) | [economic growth](https://forum.effectivealtruism.org/tag/economic-growth) | [international organization](https://forum.effectivealtruism.org/tag/international-organization) | [international relations](https://forum.effectivealtruism.org/tag/international-relations)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "CsuhfxPYxZsLdWrr8",
    "name": "Universe's resources",
    "core": false,
    "slug": "universe-s-resources",
    "oldSlugs": null,
    "postCount": 3,
    "description": {
      "markdown": "The **universe's resources** (sometimes called **humanity's** **cosmic endowment**) is the stock of physical resources in the universe currently accessible to humanity.\n\nIf humanity does not go prematurely [extinct](https://forum.effectivealtruism.org/tag/human-extinction), the number of people—or [moral patients](https://forum.effectivealtruism.org/tag/moral-patienthood) generally—who will ultimately exist is potentially astronomical. These figures are useful for judging the value of work aimed at influencing the [long-term future](https://forum.effectivealtruism.org/tag/long-term-future), and in particular for estimating the [importance](https://forum.effectivealtruism.org/tag/importance) of avoiding [existential risks](https://forum.effectivealtruism.org/tag/existential-risk).\n\n[Nick Bostrom](https://forum.effectivealtruism.org/tag/nick-bostrom) argues that, barring disaster, Earth will be capable of sustaining life for approximately another billion years.^[\\[1\\]](#fnqq49bekfnys)^ This means that if Earth’s population were to remain fairly close to what is today, then, assuming hundred-year life-spans, the planet would ultimately host about \\\\(10^{16}\\\\) people.\n\nThis is not an upper-bound on the possible number of people, however. If humans are ultimately able to [colonize other star systems](https://forum.effectivealtruism.org/tag/space-colonization), then they will not be limited by Earth’s ability to sustain life. Given certain assumptions, Bostrom estimates that humanity could eventually reach \\\\(10^{18}\\\\) to \\\\(10^{20}\\\\) stars, which could sustain a total of around \\\\(10^{35}\\\\) biological human beings, or around \\\\(10^{58}\\\\) [digital human minds](https://forum.effectivealtruism.org/tag/digital-person).^[\\[2\\]](#fnwtdkclefh2f)^\n\nFurther reading\n---------------\n\nAdams, Fred C. (2008) [Long-term astrophysical processes](https://en.wikipedia.org/wiki/Special:BookSources/9780198570509), in Nick Bostrom & Milan M. Ćirković (eds.) *Global Catastrophic Risks*, Oxford: Oxford University Press, pp. 33–47.\n\nOrd, Toby (2021) [The edges of our universe](https://arxiv.org/abs/2104.01191), *ArXiv*, 2104.01191.\n\nRelated entries\n---------------\n\n[astronomical waste](https://forum.effectivealtruism.org/tag/astronomical-waste) | [computational power of the human brain](https://forum.effectivealtruism.org/tag/computational-power-of-the-human-brain) | [non-humans and the long-term future](https://forum.effectivealtruism.org/tag/non-humans-and-the-long-term-future) | [space colonization](https://forum.effectivealtruism.org/tag/space-colonization) | [whole brain emulation](https://forum.effectivealtruism.org/tag/whole-brain-emulation)\n\n1.  ^**[^](#fnrefqq49bekfnys)**^\n    \n    Bostrom, Nick (2013) [Existential risk prevention as global priority](http://doi.org/10.1111/1758-5899.12002), *Global Policy*, vol. 4, pp. 15–31.\n    \n2.  ^**[^](#fnrefwtdkclefh2f)**^\n    \n    Bostrom, Nick, Allan Dafoe & Carrick Flynn (2020) [Public policy and superintelligent AI](http://doi.org/10.1093/oso/9780190905033.003.0011), in S. Matthew Liao (ed.) *Ethics of Artificial Intelligence*, Oxford: Oxford University Press, pp. 293-326, p. 319."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "kWNnWtggZuzbatasx",
    "name": "Geomagnetic storms",
    "core": false,
    "slug": "geomagnetic-storms",
    "oldSlugs": null,
    "postCount": 5,
    "description": {
      "markdown": "**Geomagnetic storms** are temporary disturbances of Earth's magnetosphere. They are caused by solar wind shock waves or clouds of magnetic field interacting with the Earth's magnetic field. A severe geomagnetic storm could potentially shut down power grids on a continental scale for months.^[\\[1\\]](#fnff7jxqetx1k)^\n\nFurther reading\n---------------\n\nRoodman, David (2015a) [Geomagnetic storms: using extreme value theory to gauge the risk](https://www.openphilanthropy.org/blog/geomagnetic-storms-using-extreme-value-theory-gauge-risk), *Open Philanthropy*, July 13.\n\nRoodman, David (2015b) [Geomagnetic storms: an introduction to the risk](https://www.openphilanthropy.org/blog/geomagnetic-storms-introduction-risk), *Open Philanthropy*, June 29.\n\nRoodman, David (2015c) [Geomagnetic storms: history’s surprising, if tentative, reassurance](https://www.openphilanthropy.org/blog/geomagnetic-storms-historys-surprising-if-tentative-reassurance), *Open Philanthropy*, July 2.\n\nRoodman, David (2015d) [Coming down to earth: what if a big geomagnetic storm does hit?](https://www.openphilanthropy.org/blog/coming-down-earth-what-if-big-geomagnetic-storm-does-hit), *Open Philanthropy*, August 21.\n\nRelated entries\n---------------\n\n[global catastrophic risk](https://forum.effectivealtruism.org/tag/global-catastrophic-risk)\n\n1.  ^**[^](#fnrefff7jxqetx1k)**^\n    \n    Open Philanthropy (2015) [Geomagnetic storms](https://www.openphilanthropy.org/research/cause-reports/geomagnetic-storms), *Open Philanthropy*, July."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "qWJopNTvGXpCTh2ZH",
    "name": "Global Health and Development Fund",
    "core": false,
    "slug": "global-health-and-development-fund",
    "oldSlugs": null,
    "postCount": 4,
    "description": {
      "markdown": "The **Global Health and Development Fund** is an expert-managed fund focused on [global health and development](https://forum.effectivealtruism.org/tag/global-health-and-development) . It is one of the four funds run by [Effective Altruism Funds](https://forum.effectivealtruism.org/tag/effective-altruism-funds). As of May 2022, it has funded payouts for over $25.1 million.\n\nExternal links\n--------------\n\n[Global Health and Development Fund](https://funds.effectivealtruism.org/funds/global-development). Official website.\n\nRelated links\n-------------\n\n[Animal Welfare Fund](https://forum.effectivealtruism.org/tag/animal-welfare-fund) | [Effective Altruism Funds](https://forum.effectivealtruism.org/tag/effective-altruism-funds) | [Effective Altruism Infrastructure Fund](https://forum.effectivealtruism.org/tag/effective-altruism-infrastructure-fund) | [funding opportunities](https://forum.effectivealtruism.org/tag/funding-opportunity) | [Long-Term Future Fund](https://forum.effectivealtruism.org/tag/long-term-future-fund)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "abazKdGKYPX6B8yjy",
    "name": "Animal Welfare Fund",
    "core": false,
    "slug": "animal-welfare-fund",
    "oldSlugs": null,
    "postCount": 19,
    "description": {
      "markdown": "The **Animal Welfare Fund** is an expert-managed fund focused on improving [animal welfare](https://forum.effectivealtruism.org/tag/animal-welfare-1). It is one of the four funds run by [Effective Altruism Funds](https://forum.effectivealtruism.org/tag/effective-altruism-funds). As of May 2022, it has funded payouts for over $10.6 million.\n\nFurther reading\n---------------\n\nGreig, Kieran (2021) [Animal Welfare Fund: ask us anything!](https://forum.effectivealtruism.org/posts/pmr9tR2GYoxDMqbor/animal-welfare-fund-ask-us-anything), *Effective Altruism Forum*, May 7.\n\nExternal links\n--------------\n\n[Animal Welfare Fund](https://funds.effectivealtruism.org/funds/animal-welfare). Official website.\n\n[Apply for funding](https://av20jp3z.paperform.co/?fund=Animal%20Welfare%20Fund).\n\nRelated entries\n---------------\n\n[Effective Altruism Funds](https://forum.effectivealtruism.org/tag/effective-altruism-funds) | [Effective Altruism Infrastructure Fund](https://forum.effectivealtruism.org/tag/effective-altruism-infrastructure-fund) | [funding opportunities](https://forum.effectivealtruism.org/tag/funding-opportunity) | [Global Health and Development Fund](https://forum.effectivealtruism.org/tag/global-health-and-development-fund) | [Long-Term Future Fund](https://forum.effectivealtruism.org/tag/long-term-future-fund)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "LzqGj7rjpeQ5ojfuC",
    "name": "Effective Altruism Infrastructure Fund",
    "core": false,
    "slug": "effective-altruism-infrastructure-fund",
    "oldSlugs": null,
    "postCount": 31,
    "description": {
      "markdown": "The **Effective Altruism Infrastructure Fund** (previously the **Effective Altruism Meta Fund**) is an expert-managed fund focused on [building effective altruism](https://forum.effectivealtruism.org/tag/building-effective-altruism-1). It is one of the four funds run by [Effective Altruism Funds](https://forum.effectivealtruism.org/tag/effective-altruism-funds). As of August 2022, it has funded payouts for over $9.2 million.\n\nExternal links\n--------------\n\n[Effective Altruism Infrastructure Fund](https://funds.effectivealtruism.org/funds/ea-community). Official website.\n\n[Apply for funding](https://av20jp3z.paperform.co/?fund=EA%20Infrastructure%20Fund).\n\nRelated links\n-------------\n\n[Animal Welfare Fund](https://forum.effectivealtruism.org/tag/animal-welfare-fund) | [Effective Altruism Funds](https://forum.effectivealtruism.org/tag/effective-altruism-funds) | [funding opportunities](https://forum.effectivealtruism.org/tag/funding-opportunity) | [Global Health and Development Fund](https://forum.effectivealtruism.org/tag/global-health-and-development-fund) | [Long-Term Future Fund](https://forum.effectivealtruism.org/tag/long-term-future-fund)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "HGcfdjhKx57u8juwM",
    "name": "Long-Term Future Fund",
    "core": false,
    "slug": "long-term-future-fund",
    "oldSlugs": null,
    "postCount": 38,
    "description": {
      "markdown": "The **Long-Term Future Fund** (**LTFF**) is an expert-managed fund focused on mitigating [global catastrophic risks](https://forum.effectivealtruism.org/tag/global-catastrophic-risk) that threaten the [long-term future](https://forum.effectivealtruism.org/tag/long-term-future) of humanity, with a special focus on risks from [advanced artificial intelligence](https://forum.effectivealtruism.org/tag/human-level-artificial-intelligence) and on [global catastrophic biological risks](https://forum.effectivealtruism.org/tag/global-catastrophic-biological-risk). It is one of the four funds run by [Effective Altruism Funds](https://forum.effectivealtruism.org/tag/effective-altruism-funds). As of August 2022, it has funded payouts for over $8.9 million.\n\nExternal links\n--------------\n\n[Long-Term Future Fund](https://funds.effectivealtruism.org/funds/far-future). Official website.\n\n[Apply for funding](https://av20jp3z.paperform.co/?fund=Long-Term%20Future%20Fund).\n\nRelated links\n-------------\n\n[Animal Welfare Fund](https://forum.effectivealtruism.org/tag/animal-welfare-fund) | [Effective Altruism Funds](https://forum.effectivealtruism.org/tag/effective-altruism-funds) | [Effective Altruism Infrastructure Fund](https://forum.effectivealtruism.org/tag/effective-altruism-infrastructure-fund) | [funding opportunities](https://forum.effectivealtruism.org/tag/funding-opportunity) | [Global Health and Development Fund](https://forum.effectivealtruism.org/tag/global-health-and-development-fund)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "wzTibuandkPzWfTRj",
    "name": "Introduction to effective altruism",
    "core": false,
    "slug": "introduction-to-effective-altruism",
    "oldSlugs": null,
    "postCount": 18,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5J7yCwv6jnPJ3Jzrz",
    "name": "Longview Philanthropy",
    "core": false,
    "slug": "longview-philanthropy",
    "oldSlugs": null,
    "postCount": 4,
    "description": {
      "markdown": "**Longview Philanthropy** is a [longtermist](https://forum.effectivealtruism.org/tag/longtermism) grantmaking organization based in London.\n\nAs of July 2022, Longview Philanthropy has three core focus areas: [existential risk reduction](https://forum.effectivealtruism.org/tag/existential-risk), [global priorities research](https://forum.effectivealtruism.org/tag/global-priorities-research), and [longtermist institutional reform](https://forum.effectivealtruism.org/topics/longtermist-institutional-reform).\n\nFunding\n-------\n\nAs of July 2022, Longview Philanthropy has received $15 million in funding from the [Future Fund](https://forum.effectivealtruism.org/topics/future-fund),^[\\[1\\]](#fneivgk78pelp)^  and $500,000 from [Open Philanthropy](https://forum.effectivealtruism.org/topics/open-philanthropy).^[\\[2\\]](#fn133vdz4dfh9k)^\n\nJob opportunities\n-----------------\n\nLongview Philanthropy launched a nuclear security grantmaking program in December 2021. The organization is currently hiring a grantmaker to co-lead this program.^[\\[3\\]](#fn64q0bdgtvv7)^ They are also hiring a grantmaker to work on other existential risks.^[\\[4\\]](#fntt2bhogp0m)^\n\nExternal links\n--------------\n\n[Longview Philanthropy](https://www.longview.org/). Official website.\n\n[Apply for a job](https://www.longview.org/contact).\n\nRelated entries\n---------------\n\n[effective altruism funding](https://forum.effectivealtruism.org/tag/effective-altruism-funding) | [existential risk](https://forum.effectivealtruism.org/tag/existential-risk) | [global priorities research](https://forum.effectivealtruism.org/tag/global-priorities-research) | [longtermism](https://forum.effectivealtruism.org/tag/longtermism) | [longtermist institutional reform](https://forum.effectivealtruism.org/topics/longtermist-institutional-reform)\n\n1.  ^**[^](#fnrefeivgk78pelp)**^\n    \n    Future Fund (2022) [Our grants and investments: Longview](https://ftxfuturefund.org/all-grants/?_organization_name=longview), *Future Fund*.\n    \n2.  ^**[^](#fnref133vdz4dfh9k)**^\n    \n    Open Philanthropy (2022) [Grants database: Longview Philanthropy](https://www.openphilanthropy.org/grants/?q=&organization-name=longview-philanthropy), *Open Philanthropy*.\n    \n3.  ^**[^](#fnref64q0bdgtvv7)**^\n    \n    Longview Philanthropy (2022) [Nuclear security programme co-lead](https://www.longview.org/nuclear-security-programme-co-lead), *Longview Philanthropy*, March.\n    \n4.  ^**[^](#fnreftt2bhogp0m)**^\n    \n    Longview Philanthropy (2022) [Longtermist grantmaker](https://www.longview.org/longtermist-grantmaker), *Longview Philanthropy*, March."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "4neBBrvXansMnDdfi",
    "name": "Great power conflict",
    "core": false,
    "slug": "great-power-conflict",
    "oldSlugs": null,
    "postCount": 19,
    "description": {
      "markdown": "A **great power conflict** is a military confrontation between states capable of projecting their influence on a global scale.\n\nEvaluation\n----------\n\n[80,000 Hours](https://forum.effectivealtruism.org/tag/80-000-hours) rates great power conflict a \"potential highest priority area\": an issue that, if more thoroughly examined, could rank as a top global challenge.^[\\[1\\]](#fnm8j6ne24eu)^\n\nFurther reading\n---------------\n\nClare, Stephen (2021) [Great power conflict](https://founderspledge.com/stories/great-power-conflict), *Founders Pledge*, November 29.\n\nColucci, Lamont (2015) [Great power conflict: Will it return?](https://web.archive.org/web/20150124100136/http://www.worldaffairsjournal.org/article/great-power-conflict-will-it-return), *World Affairs*, vol. 177, pp. 44–53.\n\nHanson, Robin (2019) [Big war remains possible](https://www.overcomingbias.com/2019/07/big-war-remains-possible.html), *Overcoming Bias*, July 25.\n\nSmith, Noah (2016) [The real danger](https://noahpinionblog.blogspot.com/2016/11/the-real-danger.html), *Noahpinion*, November 14.\n\nTse, Brian (2019) [Risks from great power conflicts](https://www.effectivealtruism.org/articles/ea-global-2018-risks-from-great-power-conflicts/), *Effective Altruism*, March 11.\n\nRelated entries\n---------------\n\n[armed conflict](https://forum.effectivealtruism.org/tag/armed-conflict) | [global governance](https://forum.effectivealtruism.org/tag/global-governance) | [peace and conflict studies](https://forum.effectivealtruism.org/tag/peace-and-conflict-studies)  | [polarity](https://forum.effectivealtruism.org/tag/polarity) | [singleton](https://forum.effectivealtruism.org/tag/singleton) | [technology race](https://forum.effectivealtruism.org/topics/technology-race)\n\n1.  ^**[^](#fnrefm8j6ne24eu)**^\n    \n    80,000 Hours (2022) [Our current list of pressing world problems](https://80000hours.org/problem-profiles/), *80,000 Hours*."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "88LHxQCKhYyXvohEX",
    "name": "Normative uncertainty",
    "core": false,
    "slug": "normative-uncertainty-1",
    "oldSlugs": null,
    "postCount": 1,
    "description": {
      "markdown": "**Normative uncertainty** is uncertainty about how to act given lack of certainty in any one normative theory, as well as the study of how one ought to act given this uncertainty.\n\nTypes of uncertainty\n--------------------\n\nAt the most basic level, uncertainty can be either *descriptive* or *normative*. Normative uncertainty can itself be either *theoretical* or *practical*. Within theoretical uncertainty, a further subdivision can be made between *epistemological uncertainty* and *decision-theoretic uncertainty*. And practical uncertainty can be subdivided into [*moral uncertainty*](https://forum.effectivealtruism.org/tag/moral-uncertainty) and *prudential uncertainty*, while theoretical uncertainty can be subdivided into *epistemological uncertainty* and [*decision-theoretic uncertainty*](https://forum.effectivealtruism.org/tag/decision-theoretic-uncertainty).\n\n*   Uncertainty\n    *   Descriptive\n    *   Normative\n        *   Theoretical\n            *   Epistemological\n            *   Decision-theoretic\n        *   Practical\n            *   Moral\n            *   Prudential\n\nSome of these terms are not used consistently in the literature. In particular, what the taxonomy above calls \"practical uncertainty\" is referred to as \"normative uncertainty\" by some authors,^[\\[1\\]](#fnjkj7trniic)^ and as \"moral uncertainty\" by other (and sometimes even the same) authors.^[\\[2\\]](#fnd4tnnqdtz0o)^^[\\[3\\]](#fnxfqullm724)^^[\\[4\\]](#fnwswi0a5r7jc)^\n\nFurther reading\n---------------\n\nMacAskill, William, Krister Bykvist & Toby Ord (2020) [*Moral Uncertainty*](https://en.wikipedia.org/wiki/Special:BookSources/9780198722274), Oxford: Oxford University Press.\n\nRelated entries\n---------------\n\n[decision-theoretic uncertainty](https://forum.effectivealtruism.org/tag/decision-theoretic-uncertainty) | [moral uncertainty](https://forum.effectivealtruism.org/tag/moral-uncertainty)\n\n1.  ^**[^](#fnrefjkj7trniic)**^\n    \n    MacAskill, William & Toby Ord (2020) [Why maximize expected choice-worthiness?](http://doi.org/10.1111/nous.12264), *Noûs*, vol. 54, pp. 327–353, p. 328.\n    \n2.  ^**[^](#fnrefd4tnnqdtz0o)**^\n    \n    MacAskill, William, Krister Bykvist & Toby Ord (2020) [*Moral Uncertainty*](https://en.wikipedia.org/wiki/Special:BookSources/9780198722274), Oxford: Oxford University Press.\n    \n3.  ^**[^](#fnrefxfqullm724)**^\n    \n    Cf. Bykvist, Krister (2017) [Moral uncertainty](http://doi.org/10.1111/phc3.12408), *Philosophy Compass*, vol. 12, pp. 1–8.\n    \n4.  ^**[^](#fnrefwswi0a5r7jc)**^\n    \n    Podgorski, Abelard (2020) [Normative uncertainty and the dependence problem](http://doi.org/10.1093/mind/fzz048), *Mind*, vol. 129, pp. 43–70."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "FkDQ9S96jnuyfzAPd",
    "name": "Democracy Defense Fund",
    "core": false,
    "slug": "democracy-defense-fund",
    "oldSlugs": null,
    "postCount": null,
    "description": {
      "markdown": "The **Democracy Defense Fund** (**DDF**) was a project formed in March 2017 to match up donors interested in [safeguarding liberal democracy](https://forum.effectivealtruism.org/tag/safeguarding-liberal-democracy) in the United States with subject matter experts familiar with effective giving opportunities.\n\nDDF had two funds: a Democratic Participation Fund and a Free Press Fund. It awarded grants to three organizations: Repairers of the Breach, Forward Justice, and Reporters Committee for Freedom of the Press.\n\nDDF's team consisted of Barbara Fried, a Professor of Law at Stanford Law School, and Connor Flexman.\n\nFurther reading\n---------------\n\nFried, Barbara & Connor Flexman (2017) [Grants to Repairers of the Breach and Forward Justice](https://medium.com/democracy-defense-fund/grants-to-repairers-of-the-breach-and-forward-justice-8d23b77e7f09), *Democracy Defense Fund*, July 31.\n\nLalwani, Nikita, Alasdair Phillips-Robins & Chris Peak (2017) [Grant to Reporters Committee for Freedom of the Press](https://medium.com/democracy-defense-fund/grant-to-reporters-committee-for-freedom-of-the-press-d050c7871ad3), *Democracy Defense Fund*, July 31.\n\nExternal links\n--------------\n\n[Democracy Defense Fund](https://web.archive.org/web/20220407220855/http://www.democracydefensefund.net/). Official website, archived from the original.\n\nRelated entries\n---------------\n\n[democracy](https://forum.effectivealtruism.org/tag/democracy) | [electoral politics](https://forum.effectivealtruism.org/tag/electoral-politics) | [policy change](https://forum.effectivealtruism.org/tag/policy-change) |  [safeguarding liberal democracy](https://forum.effectivealtruism.org/tag/safeguarding-liberal-democracy) | [United States policy and politics](https://forum.effectivealtruism.org/tag/united-states-policy-and-politics)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "4KMQGocE3QYzjHhff",
    "name": "Definition of effective altruism",
    "core": false,
    "slug": "definition-of-effective-altruism",
    "oldSlugs": null,
    "postCount": 19,
    "description": {
      "markdown": "The [Centre for Effective Altruism](https://forum.effectivealtruism.org/tag/centre-for-effective-altruism-1), a key organization in the EA space, describes effective altruism as both \"an intellectual project, using evidence and reason to figure out how to benefit others as much as possible\" and \"a practical project: to take action based on the research, and build a better world\".^[\\[1\\]](#fn49duasviyjb)^\n\nFurther reading\n---------------\n\nMacAskill, William (2019) [The definition of effective altruism](https://doi.org/10.1093/oso/9780198841364.003.0001), in Hilary Greaves & Theron Pummer (eds.) *Effective Altruism: Philosophical Issues*, Oxford: Oxford University Press, pp. 10–28.\n\nRelated entries\n---------------\n\n[effective altruism](https://forum.effectivealtruism.org/tag/effective-altruism) | [effective altruism messaging](https://forum.effectivealtruism.org/tag/effective-altruism-messaging)\n\n1.  ^**[^](#fnref49duasviyjb)**^\n    \n    Centre for Effective Altruism (2021) [Home](https://www.centreforeffectivealtruism.org/), *Centre for Effective Altruism*."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "vhrCHyrCwM59ze9Se",
    "name": "Conflict theory vs. mistake theory",
    "core": false,
    "slug": "conflict-theory-vs-mistake-theory",
    "oldSlugs": null,
    "postCount": 1,
    "description": {
      "markdown": "**Conflict theory** and **mistake theory** are two contrasting frameworks for analyzing political disagreements. Roughly, mistake theorists treat politics as a problem to be solved, whereas conflict theories treat politics as a war to be won. These two frameworks were articulated by [Scott Alexander](https://forum.effectivealtruism.org/tag/scott-alexander) in a 2018 blog post,^[\\[1\\]](#fnrrx1o9m21n)^ though Alexander credits an earlier blog post for the distinction,^[\\[2\\]](#fn1my1czrbojo)^ which itself relates to the contrast in academic sociology between *conflict theory* and *structural functionalism*.^[\\[3\\]](#fnyr373motwa)^\n\nApplications\n------------\n\nThe conflict vs. mistake distinction has been applied to explain recent criticisms of billionaire philanthropy. While some mistake theorists have offered a number of plausible reasons why such criticisms are mistaken,^[\\[4\\]](#fnzvl0mu0zmhe)^ other mistake theorists note that the criticisms are made by conflict theorists, and that failure to acknowledge this fact will cause those criticisms to be misunderstood and to offer ineffective responses to them.^[\\[5\\]](#fnpxqb46khuue)^\n\nSome argue that, while at the current margin a mistake theory mindset is probably most conducive to political progress than a conflict theory mindset, for most human history the reverse was in fact the case.^[\\[6\\]](#fnmt8y9i06ew)^ For example, the \"tax the rich\" approach favored by conflict theorists may be currently inferior to the \"let's design a more rational tax code\" approach favored by mistake theorists, yet it is arguably the reason why progressive taxation exists at all (which both conflict and mistake theorists generally regard as desirable). Overall, the most obvious gains in any given area may come from overruling those who benefit from the *status quo*, and conflict theory is arguably much better positioned than mistake theory to accomplish this goal. (Mistake theorists might object that whether one should try to fight entrenched interests or instead adopt a less conflict-oriented solution is itself a question that conflict theory is ill-equipped to answer.)\n\nFurther reading\n---------------\n\nAlexander, Scott (2018) [Conflict vs. mistake](https://slatestarcodex.com/2018/01/24/conflict-vs-mistake/), *Slate Star Codex*, January 24.\n\nAlexander, Scott (2018) [Highlights from the comments on conflict vs. mistake](https://slatestarcodex.com/2018/01/29/highlights-from-the-comments-on-conflict-vs-mistake/), *Slate Star Codex*, January 29.\n\n1.  ^**[^](#fnrefrrx1o9m21n)**^\n    \n    Alexander, Scott (2018) [Conflict vs. mistake](https://slatestarcodex.com/2018/01/24/conflict-vs-mistake/), *Slate Star Codex*, January 24.\n    \n2.  ^**[^](#fnref1my1czrbojo)**^\n    \n    no\\_bear\\_so_low  (2017) [Socialism, communism and Marxism pt: 1, on trust and trust surveys](https://www.reddit.com/r/slatestarcodex/comments/74vpwm/socialism_communism_and_marxism_pt_1_on_trust_and/), *Reddit*, October 7.\n    \n3.  ^**[^](#fnrefyr373motwa)**^\n    \n    Marshall, Gordon (ed.) (1998) [*A Dictionary of Sociology*](https://en.wikipedia.org/wiki/Special:BookSources/0192800817), 2nd ed., Oxford: Oxford University Press, pp. 108-109.\n    \n4.  ^**[^](#fnrefzvl0mu0zmhe)**^\n    \n    Alexander, Scott (2019) [Against against billionaire philanthropy](https://slatestarcodex.com/2019/07/29/against-against-billionaire-philanthropy/), *Slate Star Codex*, July 29.\n    \n5.  ^**[^](#fnrefpxqb46khuue)**^\n    \n    Mowshowitz, Zvi (2019) [Mistake versus conflict theory of against billionaire philanthropy](https://www.lesswrong.com/posts/k3MhrQewcHziu3PHS/mistake-versus-conflict-theory-of-against-billionaire), *LessWrong*, August 1.\n    \n6.  ^**[^](#fnrefmt8y9i06ew)**^\n    \n    Ngo, Richard (2018) [In defence of conflict theory](https://thinkingcomplete.blogspot.com/2018/02/in-defence-of-conflict-theory.html), *Thinking Complete*, February 16."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "iBBa3jpeBDxjeZ9qR",
    "name": "Organisation for the Prevention of Intense Suffering",
    "core": false,
    "slug": "organisation-for-the-prevention-of-intense-suffering",
    "oldSlugs": [
      "organisation-for-the-prevention-of-intense-sufferin"
    ],
    "postCount": 7,
    "description": {
      "markdown": "The **Organisation for the Prevention of Intense Suffering** (**OPIS**) is an organization that advocates for the alleviation and ultimate elimination of intense forms of suffering. OPIS has so far mostly focused on campaigning for access to medication for especially painful conditions, including [cluster headaches](https://forum.effectivealtruism.org/tag/cluster-headache).\n\nExternal links\n--------------\n\n[Organisation for the Prevention of Intense Suffering](https://www.preventsuffering.org/) . Official website."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "kEBKfckAoQpXnGzZA",
    "name": "Academia",
    "core": false,
    "slug": "academia-1",
    "oldSlugs": null,
    "postCount": 70,
    "description": {
      "markdown": "The **academia** tag is for posts relevant to topics such as how (in)efficient academia is, what influence academia has had and could have, the best ways to leverage or direct academia, and whether people should go into academic or academia-related careers.\n\nFurther reading\n---------------\n\nWhittlestone, Jess (2018) [Academic research](https://80000hours.org/career-reviews/academic-research/), *80,000 Hours*\n\nRelated entries\n---------------\n\n[field-building](https://forum.effectivealtruism.org/tag/field-building) | [metascience](https://forum.effectivealtruism.org/topics/metascience) | [research methods](https://forum.effectivealtruism.org/tag/research-methods) | [research training programs](https://forum.effectivealtruism.org/tag/research-training-programs) | [scientific progress](https://forum.effectivealtruism.org/tag/scientific-progress)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "LNkYkfJNywxq88u5N",
    "name": "Johns Hopkins Center for Health Security",
    "core": false,
    "slug": "johns-hopkins-center-for-health-security",
    "oldSlugs": null,
    "postCount": 6,
    "description": {
      "markdown": "The **Johns Hopkins Center for Health Security** (**JHCHS**) (sometimes referred to as the **Center for Health Security** and abbreviated **CHS**) is a nonprofit organization that conducts policy research and development in [biosecurity](https://forum.effectivealtruism.org/tag/biosecurity) and pandemic preparedness. It is part of the Johns Hopkins Bloomberg School of Public Health.\n\nHistory\n-------\n\nJHCHS was founded in 1998 as the **Johns Hopkins Center for Civilian Biodefense Strategies** (**CCBS**).\n\nEvaluation\n----------\n\nAs of May 2022, JHCHS has received over $40 million in grants from [Open Philanthropy](https://forum.effectivealtruism.org/tag/open-philanthropy).^[\\[1\\]](#fn3x46iaprrq3)^ JHCHS is one of the four organizations recommended by [Founders Pledge](https://forum.effectivealtruism.org/tag/founders-pledge) in their cause report on safeguarding the [long-term future](https://forum.effectivealtruism.org/tag/long-term-future).^[\\[2\\]](#fno9yoz76hvh)^\n\nFurther reading\n---------------\n\nOpen Philanthropy (2017) [Johns Hopkins Center for Health Security — biosecurity, global health security, and global catastrophic risks (2017)](https://www.openphilanthropy.org/focus/global-catastrophic-risks/biosecurity/center-health-security-biosecurity-global-health-security-and-global-catastrophic), *Open Philanthropy*, February.\n\nExternal links\n--------------\n\n[Johns Hopkins Center for Health Security](https://www.centerforhealthsecurity.org/). Official website.\n\n[Apply for a job](https://www.centerforhealthsecurity.org/who-we-are/work-with-us/).\n\n[Donate to the Johns Hopkins Center for Health Security](https://www.centerforhealthsecurity.org/giving/).\n\nRelated entries\n---------------\n\n[biosecurity](https://forum.effectivealtruism.org/tag/biosecurity) | [global catastrophic biological risk](https://forum.effectivealtruism.org/tag/global-catastrophic-biological-risk) | [Nuclear Threat Initiative](https://forum.effectivealtruism.org/tag/nuclear-threat-initiative)\n\n1.  ^**[^](#fnref3x46iaprrq3)**^\n    \n    Open Philanthropy (2022) [Grants database: Johns Hopkins Center for Health Security](https://www.openphilanthropy.org/grants/?q=&organization-name=johns-hopkins-center-for-health-security), *Open Philanthropy*.\n    \n2.  ^**[^](#fnrefo9yoz76hvh)**^\n    \n    Halstead, John (2019) [Safeguarding the future cause area report](https://assets.ctfassets.net/x5sq5djrgbwu/5C1hNPO8RK2E3RzH9dj88M/1fd2c52ab1e534af95c25c5ebea92b49/Cause_Report_-_Safeguarding_the_Future.pdf), *Founders Pledge*, January (updated December 2020)."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "NdpMrADKqWj8AQaEk",
    "name": "AI Safety Camp",
    "core": false,
    "slug": "ai-safety-camp",
    "oldSlugs": null,
    "postCount": 11,
    "description": {
      "markdown": "**AI Safety Camp** (**AISC**) is a non-profit initiative that runs programs serving students and early-career researchers who want to work on reducing  [existential risk](https://forum.effectivealtruism.org/tag/existential-risk) from AI. \n\nFunding\n-------\n\nAs of  July 2022, AISC has received $290,000 in funding from the [Future Fund](https://forum.effectivealtruism.org/topics/future-fund),^[\\[1\\]](#fnt6osvmat05)^ $180,000 from [Effective Altruism Funds](https://forum.effectivealtruism.org/topics/effective-altruism-funds),^[\\[2\\]](#fn6ezqsuwkkoa)^^[\\[3\\]](#fndepvgxq68pi)^^[\\[4\\]](#fnri78mh72bw)^^[\\[5\\]](#fnh3frxloljnp)^ and $130,000 from the [Survival and Flourishing](https://forum.effectivealtruism.org/topics/survival-and-flourishing) Fund.^[\\[6\\]](#fn1qp52bhjnyp)^ \n\nExternal links\n--------------\n\n[AI Safety Camp](https://aisafety.camp/). Official website.\n\nRelated entries\n---------------\n\n[AI safety](https://forum.effectivealtruism.org/tag/ai-safety) | [existential risk](https://forum.effectivealtruism.org/tag/existential-risk)\n\n1.  ^**[^](#fnreft6osvmat05)**^\n    \n    Future Fund (2022) [Our grants and investments: AI Safety Camp](https://ftxfuturefund.org/all-grants/?_organization_name=ai-safety-camp), *Future Fund*.\n    \n2.  ^**[^](#fnref6ezqsuwkkoa)**^\n    \n    Long-Term Future Fund (2019) [April 2019: Long-Term Future Fund grants and recommendations](https://funds.effectivealtruism.org/funds/payouts/april-2019-long-term-future-fund-grants-and-recommendations), *Effective Altruism Funds*, April.\n    \n3.  ^**[^](#fnrefdepvgxq68pi)**^\n    \n    Long-Term Future Fund (2019) [August 2019: Long-Term Future Fund grants and recommendations](https://funds.effectivealtruism.org/funds/payouts/august-2019-long-term-future-fund-grants-and-recommendations), *Effective Altruism Funds*, August.\n    \n4.  ^**[^](#fnrefri78mh72bw)**^\n    \n    Long-Term Future Fund (2019) [November 2019: Long-Term Future Fund grants](https://funds.effectivealtruism.org/funds/payouts/november-2019-long-term-future-fund-grants), *Effective Altruism Funds*, November.\n    \n5.  ^**[^](#fnrefh3frxloljnp)**^\n    \n    Long-Term Future Fund (2021) [May 2021: Long-Term Future Fund grants](https://funds.effectivealtruism.org/funds/payouts/may-2021-long-term-future-fund-grants), *Effective Altruism Funds*, May.\n    \n6.  ^**[^](#fnref1qp52bhjnyp)**^\n    \n    Survival and Flourishing Fund (2020) [SFF-2021-H2 S-process recommendations announcement](https://survivalandflourishing.fund/sff-2021-h2-recommendations), *Survival and Flourishing Fund*."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "eDF9penvRvEoDbo8c",
    "name": "Leverhulme Center for the Future of Intelligence",
    "core": false,
    "slug": "leverhulme-center-for-the-future-of-intelligence",
    "oldSlugs": null,
    "postCount": 1,
    "description": {
      "markdown": "The **Leverhulme Centre for the Future of Intelligence** (**CFI**) is an interdisciplinary research centre within the University of Cambridge that studies the impact of [artificial intelligence](https://forum.effectivealtruism.org/tag/artificial-intelligence). CFI was founded in 2015 and is currently lead by Stephen Cave.\n\nFurther reading\n---------------\n\nUniversity of Cambridge (2015) [The future of intelligence: Cambridge University launches new centre to study AI and the future of humanity](https://www.cam.ac.uk/research/news/the-future-of-intelligence-cambridge-university-launches-new-centre-to-study-ai-and-the-future-of), *University of Cambridge*, December 3.\n\nExternal links\n--------------\n\n[Leverhulme Centre for the Future of Intelligence](http://lcfi.ac.uk/). Official website.\n\n[Apply for a job](http://lcfi.ac.uk/get-involved/vacancies/).\n\nRelated entries\n---------------\n\n[AI safety](https://forum.effectivealtruism.org/tag/ai-safety) | [Centre for the Study of Existential Risk](https://forum.effectivealtruism.org/tag/centre-for-the-study-of-existential-risk)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "yhjwSkP8ZAa8bcqED",
    "name": "Long-range forecasting",
    "core": false,
    "slug": "long-range-forecasting",
    "oldSlugs": null,
    "postCount": 29,
    "description": {
      "markdown": "**Long-range forecasting** is [forecasting](https://forum.effectivealtruism.org/topics/forecasting) involving long time horizons. Long-range forecasts are sometimes defined as involving events at least 10 years into the future,^[\\[1\\]](#fneopyxmbvly7)^ although there is no generally agreed-upon definition.\n\nFurther reading\n---------------\n\nGoth, Aidan, Stephen Clare & Christian Ruhl (2022) [Professor Philip Tetlock’s research on improving judgments of existential risk](https://founderspledge.com/stories/prof-philip-tetlocks-forecasting-research-high-impact-funding-opportunity), *Founders Pledge*, March 8.\n\nKarnofsky, Holden (2022) [The track record of futurists seems … fine](https://www.cold-takes.com/the-track-record-of-futurists-seems-fine/), *Cold Takes*, June 30.\n\nMuehlhauser, Luke (2019) [How feasible is long-range forecasting?](https://www.openphilanthropy.org/blog/how-feasible-long-range-forecasting), *Open Philanthropy*, October 10.\n\nSandberg, Anders (2021) [Popper vs macrohistory: what can we say about the long-run future?](https://www.youtube.com/watch?v=nhoKXBZTKSo), *Oxford Karl Popper Society*, January 25.\n\nRelated entries\n---------------\n\n[AI forecasting](/tag/ai-forecasting) | [cluelessness](https://forum.effectivealtruism.org/tag/cluelessness) | [estimation of existential risk](https://forum.effectivealtruism.org/tag/estimation-of-existential-risks) | [forecasting](/tag/forecasting) | [long-term future](https://forum.effectivealtruism.org/topics/long-term-future) | [longtermism](https://forum.effectivealtruism.org/tag/longtermism)\n\n1.  ^**[^](#fnrefeopyxmbvly7)**^\n    \n    Muehlhauser, Luke (2019) [How feasible is long-range forecasting?](https://www.openphilanthropy.org/blog/how-feasible-long-range-forecasting), *Open Philanthropy*, October 10."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "8rQysEeqxtuBqr5Ya",
    "name": "Ethics of existential risk",
    "core": false,
    "slug": "ethics-of-existential-risk",
    "oldSlugs": [
      "arguments-for-reducing-existential-risk",
      "moral-perspectives-on-existential-risk-reduction"
    ],
    "postCount": 21,
    "description": {
      "markdown": "The **ethics of existential risk** is the study of the ethical issues related to [existential risk](/tag/existential-risk), including questions of how bad an existential catastrophe would be, how good it is to reduce existential risk, why those things are as bad or good as they are, and how this differs between different specific existential risks. There is a range of different perspectives on these questions, and these questions have implications for how much to [prioritise](https://forum.effectivealtruism.org/tag/cause-prioritization) reducing existential risk in general and which specific risks to prioritise reducing.\n\nIn [*The Precipice*](https://forum.effectivealtruism.org/tag/the-precipice), [Toby Ord](https://forum.effectivealtruism.org/tag/toby-ord) discusses five different \"moral foundations\" for assessing the value of existential risk reduction, depending on whether emphasis is placed on *the* *future*, *the* *present*, *the* *past*, *civilizational virtues* or *cosmic significance*.^[\\[1\\]](#fnjqfr1hk9l2)^\n\nThe future\n----------\n\nIn one of the earliest discussions of the topic, [Derek Parfit](https://forum.effectivealtruism.org/tag/derek-parfit) offers the following thought experiment:^[\\[2\\]](#fn5jhmxrywwz5)^\n\n> I believe that if we destroy mankind, as we now can, this outcome will be much worse than most people think. Compare three outcomes:\n> \n> 1.  Peace.\n> 2.  A nuclear war that kills 99% of the world's existing population.\n> 3.  A nuclear war that kills 100%.\n> \n> (2) would be worse than (1), and (3) would be worse than (2). Which is the greater of these two differences? Most people believe that the greater difference is between (1) and (2). I believe that the difference between (2) and (3) is very much greater.\n\nThe scale of what is lost in an existential catastrophe is determined by humanity's long-term potential—all the value that would be realized if our species survived indefinitely. The [universe's resources](https://forum.effectivealtruism.org/tag/universe-s-resources) could sustain a total of around \\\\(10^{35}\\\\) biological human beings, or around \\\\(10^{58}\\\\) digital human minds.^[\\[3\\]](#fng849b68vph4)^ And this may not exhaust all the relevant potential, if value supervenes on other things besides human or sentient minds, as some moral theories hold. \n\nIn the effective altruism community, this is probably the ethical perspective most associated with existential risk reduction: existential risks are often seen as a pressing problem because of the astronomical amounts of value or disvalue potentially at stake over the course of the [long-term future](https://forum.effectivealtruism.org/tag/long-term-future).\n\nThe present\n-----------\n\nSome philosophers have defended views on which future or contingent people do not matter morally.^[\\[4\\]](#fngkpkn43j7sj)^ Even on such views, however, an existential catastrophe could be among the worst things imaginable: it would cut short the lives of every living [moral patient](https://forum.effectivealtruism.org/tag/moral-patienthood), destroying all of what makes their lives valuable, and most likely subjecting many of them to profound suffering. So even setting aside the value of future generations, a case for reducing existential risk could grounded in concern for presently existing beings.\n\nThis present-focused moral foundation could also be discussed as a \"near-termist\" or \"[person-affecting](https://forum.effectivealtruism.org/tag/person-affecting-views)\" argument for existential risk reduction.^[\\[5\\]](#fncu9upgvnfpp)^ In the effective altruism community, it appears to be the most commonly discussed non-longtermist ethical argument for existential risk reduction.\n\nThe past\n--------\n\nHumanity can be considered as a vast intergenerational partnership, engaged in the task of gradually increasing its stock of art, culture, wealth, science and technology. In Edmund Burke's words, \"As the ends of such a partnership cannot be obtained except in many generations, it becomes a partnership not only between those who are living, but between those who are living, those who are dead, and those who are to be born.\"^[\\[6\\]](#fn1imvz2a4xvz)^ On this view, a generation that allowed an existential catastrophe to occur may be regarded as failing to discharge a moral duty owed to all previous generations.^[\\[7\\]](#fnw0c0ti890m)^ \n\nCivilizational virtues\n----------------------\n\nInstead of focusing on the impacts of individual human action, one can consider the dispositions and character traits displayed by humanity as a whole, which Ord calls *civilizational virtues*.^[\\[8\\]](#fn6ge6c7wksq7)^ An ethical framework that attached intrinsic moral significance to the cultivation and exercise of virtue would regard the neglect of existential risks as showing \"a staggering deficiency of patience, prudence, and wisdom.\"^[\\[9\\]](#fnb6yuvdxscbw)^\n\nCosmic significance\n-------------------\n\nAt the beginning of *On What Matters*, Parfit writes that \"We are the animals that can both understand and respond to reasons. \\[...\\] We may be the only rational beings in the Universe.\"^[\\[10\\]](#fnfgclwd428uv)^ If this is so, then, as Ord writes, \"responsibility for the history of the universe is entirely *on us*: this is the only chance ever to shape the universe toward what is right, what is just, what is best for all.\"^[\\[11\\]](#fnvdtrojctljo)^ In addition, it may be the only chance for the universe to understand itself.\n\nEvaluating and prioritizing existential risk reduction\n------------------------------------------------------\n\nIt is important to distinguish between the question of whether a given ethical perspective would see existential risk reduction as net positive and the question of whether that ethical perspective would *prioritise* existential risk reduction, and this distinction is not always made.^[\\[12\\]](#fn3sxta69061k)^ One reason this matters is that existential risk reduction may be much [less tractable and perhaps less neglected](https://forum.effectivealtruism.org/tag/itn-framework-1) than some other cause areas (e.g., near-term [farmed animal welfare](https://forum.effectivealtruism.org/tag/farmed-animal-welfare)), but with that being made up for by far greater importance from a longtermist perspective. Therefore, if one adopts an ethical perspective that just sees existential risk reduction as similarly important to other major global issues, existential risk reduction may no longer seem worth prioritising.\n\nFurther reading\n---------------\n\nAird, Michael (2021) [Why I think *The Precipice* might understate the significance of population ethics](https://forum.effectivealtruism.org/posts/EMKf4Gyee7BsY2RP8/michaela-s-shortform?commentId=mmoBoot3x8xBKWfgS), *Effective Altruism Forum*, January 5.\n\nOrd, Toby (2020) [*The Precipice: Existential Risk and the Future of Humanity*](https://en.wikipedia.org/wiki/Special:BookSources/1526600218), London: Bloomsbury Publishing, ch. 2.\n\nRelated entries\n---------------\n\n[astronomical waste](/tag/astronomical-waste) | [existential risk](/tag/existential-risk) | [longtermism](/tag/longtermism) | [moral philosophy](/tag/moral-philosophy) | [moral uncertainty](/tag/moral-uncertainty) | [person-affecting views](/tag/person-affecting-views) | [population ethics](/tag/population-ethics) | [prioritarianism](/tag/prioritarianism) | [s-risk](/tag/s-risk) | [suffering-focused ethics ](/tag/suffering-focused-ethics)\n\n1.  ^**[^](#fnrefjqfr1hk9l2)**^\n    \n    Ord, Toby (2020) [*The Precipice: Existential Risk and the Future of Humanity*](https://en.wikipedia.org/wiki/Special:BookSources/1526600218), London: Bloomsbury Publishing.\n    \n2.  ^**[^](#fnref5jhmxrywwz5)**^\n    \n    Parfit, Derek (1984) [*Reasons and Persons*](https://en.wikipedia.org/wiki/Special:BookSources/0-19-824908-X), Oxford: Clarendon press, pp. 453–454.\n    \n3.  ^**[^](#fnrefg849b68vph4)**^\n    \n    Bostrom, Nick, Allan Dafoe & Carrick Flynn (2020) [*Public Policy and Superintelligent AI*](http://doi.org/10.1093/oso/9780190905033.003.0011), In S. Matthew Liao (ed.), *Ethics of Artificial Intelligence*, Oxford: Oxford University Press, p. 319.\n    \n4.  ^**[^](#fnrefgkpkn43j7sj)**^\n    \n    Narveson, Jan (1973) [Moral problems of population](https://doi.org/10.5840/monist197357134), *Monist*, vol. 57, pp. 62–86.\n    \n5.  ^**[^](#fnrefcu9upgvnfpp)**^\n    \n    Lewis, Gregory (2018) [The person-affecting value of existential risk reduction](https://forum.effectivealtruism.org/posts/dfiKak8ZPa46N7Np6/the-person-affecting-value-of-existential-risk-reduction), *Effective Altruism Forum*, April 13.\n    \n6.  ^**[^](#fnref1imvz2a4xvz)**^\n    \n    Burke, Edmund (1790) *Reflections on the Revolution in France*, London: J. Dodsley, p. 193.\n    \n7.  ^**[^](#fnrefw0c0ti890m)**^\n    \n    Ord (2020) [*The Precipice*](https://en.wikipedia.org/wiki/Special:BookSources/1526600218), pp. 49–53.\n    \n8.  ^**[^](#fnref6ge6c7wksq7)**^\n    \n    Ord (2020) [*The Precipice*](https://en.wikipedia.org/wiki/Special:BookSources/1526600218), p. 53.\n    \n9.  ^**[^](#fnrefb6yuvdxscbw)**^\n    \n    Grimes, Barry (2020) [Toby Ord: Fireside chat and Q&A](https://forum.effectivealtruism.org/posts/QHxjRx8zpqL4xxsXT/toby-ord-fireside-chat-and-q-and-a), *Effective Altruism Global*, March 21.\n    \n10.  ^**[^](#fnreffgclwd428uv)**^\n    \n    Parfit, Derek (2011) [*On What Matters*](https://en.wikipedia.org/wiki/Special:BookSources/978-0-19-926592-3), vol. 1, Oxford: Oxford University Press, p. 31.\n    \n11.  ^**[^](#fnrefvdtrojctljo)**^\n    \n    Ord (2020) [*The Precipice*](https://en.wikipedia.org/wiki/Special:BookSources/1526600218), pp. 53 and 55.\n    \n12.  ^**[^](#fnref3sxta69061k)**^\n    \n    See Daniel, Max (2020) [Comment on 'What are the leading critiques of longtermism and related concepts'](https://forum.effectivealtruism.org/posts/jiwgT3WvMkwpWP4BC/what-are-the-leading-critiques-of-longtermism-and-related?commentId=pzT6AS2FBsAcZHpBp), *Effective Altruism Forum*, June 4."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "nBiPDg9XitwzHhvGB",
    "name": "Terrorism",
    "core": false,
    "slug": "terrorism",
    "oldSlugs": null,
    "postCount": 10,
    "description": {
      "markdown": "Related entries\n---------------\n\n[armed conflict](https://forum.effectivealtruism.org/tag/armed-conflict) | [autonomous weapon](https://forum.effectivealtruism.org/tag/autonomous-weapon) | [biosecurity](https://forum.effectivealtruism.org/tag/biosecurity) | [global catastrophic risk](https://forum.effectivealtruism.org/tag/global-catastrophic-risk) | [nuclear warfare](https://forum.effectivealtruism.org/tag/nuclear-warfare-1) | [vulnerable world hypothesis](https://forum.effectivealtruism.org/tag/vulnerable-world-hypothesis) | [weapons of mass destruction](https://forum.effectivealtruism.org/tag/weapons-of-mass-destruction)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "6NbFfgj8bmzFhAGuH",
    "name": "Climate engineering",
    "core": false,
    "slug": "climate-engineering",
    "oldSlugs": [
      "geoengineering"
    ],
    "postCount": 20,
    "description": {
      "markdown": "**Climate engineering**, sometimes also referred to as **geoengineering** or **climate intervention**, is intentional, large-scale intervention in the Earth's climate system.^[\\[1\\]](#fnm0tz465ttek)^ One reason to engage in climate engineering would be to counter anthropogenic [climate change](https://forum.effectivealtruism.org/tag/climate-change). The main techniques that could be used for climate engineering are carbon dioxide removal and solar radiation management.\n\nFurther reading\n---------------\n\nConn, Ariel (2019) [Not Cool episode 6: Alan Robock on geoengineering](https://futureoflife.org/2019/09/17/not-cool-ep-6-alan-robock-on-geoengineering/?cn-reloaded=1), *Future of Life Institute*, September 17.\n\nOpen Philanthropy (2013) [Geoengineering research](https://www.openphilanthropy.org/research/cause-reports/geoengineering), *Open Philanthropy*, July.\n\nOpen Philanthropy (2015) [Governance of solar radiation management](https://www.openphilanthropy.org/research/cause-reports/SRM-governance), *Open Philanthropy*, October.\n\nPerry, Lucas (2020) [Kelly Wanser on climate change as a possible existential threat](https://futureoflife.org/2020/09/30/kelly-wanser-on-marine-cloud-brightening-for-mitigating-climate-change/), *Future of Life Institute*, September 30.\n\nWiblin, Robert (2021) [Kelly Wanser on whether to deliberately intervene in the climate](https://80000hours.org/podcast/episodes/kelly-wanser-climate-interventions), *80,000 Hours*, March 26.\n\nRelated entries\n---------------\n\n[climate change](https://forum.effectivealtruism.org/tag/climate-change) | [engineering](https://forum.effectivealtruism.org/topics/engineering) | [global catastrophic risk](/tag/global-catastrophic-risk)\n\n1.  ^**[^](#fnrefm0tz465ttek)**^\n    \n    Union of Concerned Scientists (2017) [What is climate engineering?](https://www.ucsusa.org/resources/what-climate-engineering), *Union of Concerned Scientists*, November 6."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "vLSh6ePux7eZcD3qc",
    "name": "Simon Institute for Longterm Governance",
    "core": false,
    "slug": "simon-institute-for-longterm-governance",
    "oldSlugs": null,
    "postCount": 3,
    "description": {
      "markdown": "The **Simon Institute for Longterm Governance** (**SI**) is a Swiss nonprofit organization that focuses on building the capacity of policy networks to mitigate [global catastrophic risks](https://forum.effectivealtruism.org/tag/global-catastrophic-risk). SI provides training programs and advice to policymakers, conducts research on long-term policy, and coordinates a network of longtermist actors and researchers. SI was founded in 2021.^[\\[1\\]](#fnba5911lx0ai)^\n\nFunding\n-------\n\nAs of July 2022, SI has received $820,000 in funding from the [Future Fund](https://forum.effectivealtruism.org/topics/future-fund),^[\\[2\\]](#fnl5lcmoth3)^ and over $380,000 from the [Survival and Flourishing](https://forum.effectivealtruism.org/topics/survival-and-flourishing) Fund.^[\\[3\\]](#fnn87e2i1ofg)^ \n\nFurther reading\n---------------\n\nHaas, Felix (2021) [Our theory of change](https://www.simoninstitute.ch/blog/post/our-theory-of-change/), *Simon Institute for Longterm Governance*, April 23.\n\nStauffer, Maxime & Konrad Seifert (2021) [Introducing the Simon Institute for Longterm Governance (SI)](https://forum.effectivealtruism.org/posts/eKn7TDxMSSsoHhcap/introducing-the-simon-institute-for-longterm-governance-si), *Effective Altruism Forum*, March 29.\n\nExternal links\n--------------\n\n[Simon Institute for Longterm Governance](https://www.simoninstitute.ch/). Official website.\n\n[Apply for a job](https://www.simoninstitute.ch/get-involved/#work-with-us). \n\nRelated entries\n---------------\n\n[global catastrophic risk](https://forum.effectivealtruism.org/tag/global-catastrophic-risk) | [improving institutional decision-making](https://forum.effectivealtruism.org/topics/improving-institutional-decision-making) | [longtermism](https://forum.effectivealtruism.org/tag/longtermism) | [longtermist institutional reform](https://forum.effectivealtruism.org/topics/longtermist-institutional-reform) | [policy change](https://forum.effectivealtruism.org/tag/policy-change)\n\n1.  ^**[^](#fnrefba5911lx0ai)**^\n    \n    Stauffer, Maxime & Konrad Seifert (2021) [Introducing the Simon Institute for Longterm Governance (SI)](https://forum.effectivealtruism.org/posts/eKn7TDxMSSsoHhcap/introducing-the-simon-institute-for-longterm-governance-si), *Effective Altruism Forum*, March 29.\n    \n2.  ^**[^](#fnrefl5lcmoth3)**^\n    \n    Future Fund (2022) [Our grants and investments: Simon Institute for Longterm Governance](https://ftxfuturefund.org/all-grants/?_organization_name=simon-institute-for-longterm-governance), *Future Fund*.\n    \n3.  ^**[^](#fnrefn87e2i1ofg)**^\n    \n    Survival and Flourishing Fund (2021) [SFF-2022-H1 S-Process recommendations announcement](https://survivalandflourishing.fund/sff-2022-h1-recommendations), *Survival and Flourishing Fund*."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "RwosYxj7wR88fpazt",
    "name": "Alignment Research Center",
    "core": false,
    "slug": "alignment-research-center",
    "oldSlugs": null,
    "postCount": 3,
    "description": {
      "markdown": "The **Alignment Research Center** (**ARC**) is a non-profit research organization focused on [AI alignment](https://forum.effectivealtruism.org/tag/ai-alignment). It was founded in 2021 by [Paul Christiano](https://forum.effectivealtruism.org/tag/paul-christiano).^[\\[1\\]](#fnspcuyhg3dn)^\n\nFunding\n-------\n\nAs of July 2022, ARC has received over $260,000 in funding from [Open Philanthropy](https://forum.effectivealtruism.org/topics/open-philanthropy).^[\\[2\\]](#fnne9cdpp8j7o)^ \n\nFurther reading\n---------------\n\nChristiano, Paul (2021) [Announcing the Alignment Research Center](https://ai-alignment.com/announcing-the-alignment-research-center-a9b07f77431b), *AI Alignment*, April 27.\n\nExternal links\n--------------\n\n[Alignment Research Center](https://alignmentresearchcenter.org/). Official website.\n\n[Apply for a job](https://alignment.org/hiring/).\n\n1.  ^**[^](#fnrefspcuyhg3dn)**^\n    \n    Christiano, Paul (2021) [Announcing the Alignment Research Center](https://ai-alignment.com/announcing-the-alignment-research-center-a9b07f77431b), *AI Alignment*, April 27.\n    \n2.  ^**[^](#fnrefne9cdpp8j7o)**^\n    \n    Open Philanthropy (2022) [Grants database: Alignment Research Center](https://www.openphilanthropy.org/grants/?q=&organization-name=alignment-research-center), *Open Philanthropy*."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "LRLb252ZaS6ty4MH6",
    "name": "Models",
    "core": false,
    "slug": "models",
    "oldSlugs": null,
    "postCount": 23,
    "description": {
      "markdown": "A **model** is a simplified representation of some aspect of the world, which can be more easily analyzed than the world itself. When assessing and assimilating evidence, it will often be necessary to construct models.\n\nFor instance, economists who are interested in predicting how people will behave often analyze models in which everyone acts perfectly in accordance with [decision theory](https://forum.effectivealtruism.org/tag/decision-theory). Although the existence of [cognitive biases](https://forum.effectivealtruism.org/tag/cognitive-biases) means that no real people actually act this way, economists are able to make mathematically precise conclusions about these models in a way they could not about the actual world. They then argue that their models are similar enough to reality to be a useful way to structure thinking about the relevant aspect of the world.\n\nSee [game theory](https://forum.effectivealtruism.org/tag/game-theory) and 80,000 Hours' [three-factor framework](https://forum.effectivealtruism.org/tag/itn-framework-1) for two examples of models being used within the context of effective altruism.\n\nPerhaps the main difficulty when building a model is dealing with uncertainty. First, in cases where there is substantial certainty about the relevant aspect of the world, it may be necessary to [incorporate uncertainty into the model](https://forum.effectivealtruism.org/tag/model-uncertainty). Second, there may be [uncertainty about whether the model is appropriate, or whether it diverges too substantially from reality to be instructive."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "kfcER7wfyQ7TYCE5S",
    "name": "Virtue ethics",
    "core": false,
    "slug": "virtue-ethics",
    "oldSlugs": null,
    "postCount": 7,
    "description": {
      "markdown": "**Virtue ethics** holds that the morality of our action stems from its relationship to the virtues, and our moral character.\n\nVirtue ethics attempts to reorient morality away from focusing on particular actions and toward the individual's character. This is contrasted with views that hold that the morality of an action stems from its relationship to a set of rules or duties ([deontology](https://forum.effectivealtruism.org/tag/deontology)) or its consequences ([consequentialism](https://forum.effectivealtruism.org/tag/consequentialism)).\n\nDifferent virtue ethicists have different ways of characterizing virtues, but most tie their views closely with those of Aristotle. Aristotle argued that virtues like courage or honesty are a middle ground between two different extreme character traits. Courage, for example, is a middle ground between cowardice and recklessness. The virtuous agent, though, possesses not just one virtue, but a host of virtues that need to be balanced with one another. Aristotle, and most other virtue ethicists, introduce the idea of a perfectly virtuous agent to address this. For any given action, the right action is not the one which merely instantiates one virtue; it is whatever action the ideal agent would do in those circumstances. Aristotle also held that the truly flourishing human life (*Eudaimonia*) is only attainable by having a virtuous character.\n\nFurther reading\n---------------\n\nHursthouse, Rosalind & Glen Pettigrove (2003) [Virtue ethics](https://plato.stanford.edu/entries/ethics-virtue/), *Stanford Encyclopedia of Philosophy*, July 18 (updated 9 December 2018).\n\nRelated entries\n---------------\n\n[consequentialism](https://forum.effectivealtruism.org/tag/consequentialism) | [deontology](https://forum.effectivealtruism.org/tag/deontology) | [normative ethics](https://forum.effectivealtruism.org/tag/normative-ethics) | [wellbeing](https://forum.effectivealtruism.org/tag/wellbeing)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "LFviDDNxpD7Mf3WzD",
    "name": "Value of movement growth",
    "core": false,
    "slug": "value-of-movement-growth",
    "oldSlugs": [
      "movement-growth-debate"
    ],
    "postCount": 22,
    "description": {
      "markdown": "People often debate the **value of** **movement growth** in effective altruism.\n\nIt may seem that, in order for the effective altruism movement to do as much good as possible, the movement should aim to grow as much as possible. However, there are risks to rapid growth that may be avoidable if we aim to grow more slowly and deliberately. For example, rapid growth could lead to a large influx of people with specific interests/priorities who slowly reorient the entire movement to focus on those interests/priorities.\n\nThere could also be downsides to growing larger even if that's done slowly and deliberately. For example, it may increase the difficulties of [altruistic coordination](https://forum.effectivealtruism.org/tag/altruistic-coordination).\n\nThis tag is meant for posts that discuss questions about *how large* the movement should grow, *how quickly*, and *why*, rather than posts that *only* cover strategies to bolster movement growth (for such posts, see the [building effective altruism](https://forum.effectivealtruism.org/tag/building-effective-altruism-1) tag). Posts that discuss growth strategies *in light of* potential downsides of growth could fit this tag.\n\nFurther reading\n---------------\n\nAnonymous (2020) [Anonymous contributors answer: Should the effective altruism community grow faster or slower? And should it be broader, or narrower?](https://80000hours.org/2020/02/anonymous-answers-effective-altruism-community-and-growth/), *80,000 Hours*, February 17.\n\nCotton-Barratt, Owen (2015) [How valuable is movement growth?](http://effective-altruism.com/ea/is/how_valuable_is_movement_growth/), *Effective Altruism Forum*, May 14.\n\nRelated entries\n---------------\n\n[building effective altruism](https://forum.effectivealtruism.org/tag/building-effective-altruism-1) | [global outreach](/tag/global-outreach) | [movement collapse](/tag/movement-collapse) | [network building](/tag/network-building)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "fzHn542Wn3nYatnpn",
    "name": "Religion",
    "core": false,
    "slug": "religion",
    "oldSlugs": null,
    "postCount": 42,
    "description": {
      "markdown": "The **religion** tag covers posts about the intersection between religion and effective altruism.\n\nFurther reading\n---------------\n\nCrimmins, James E. (2013) [Religious utilitarians](https://www.bloomsburycollections.com/book/the-bloomsbury-encyclopedia-of-utilitarianism), in James E. Crimmins (ed.) *The Bloomsbury Encyclopedia of Utilitarianism*, London: Bloomsbury Academic, pp. 475–478.\n\nPieters, Timo (2021) [Doing good better: Combining ‘effective altruism’ and Buddhist ethics](https://europeanacademyofreligionandsociety.com/weekly_comments/combining-effective-altruism-and-buddhist-ethics/), *European Academy on Religion and Society*, April 22.\n\nRoser, Dominic, Stefan Riedener & Markus Huppenbauer (eds.) (2022) [*Effective Altruism and Religion: Synergies, Tensions, Dialogue*](https://doi.org/10.5771/9783748925361), Baden-Baden: Nomos.\n\nRelated entries\n---------------\n\n[building effective altruism](https://forum.effectivealtruism.org/tag/building-effective-altruism-1) | [Effective Altruism for Christians](https://forum.effectivealtruism.org/tag/effective-altruism-for-christians) | [Effective Altruism for Jews](https://forum.effectivealtruism.org/tag/effective-altruism-for-jews) | [social and intellectual movements](https://forum.effectivealtruism.org/tag/social-and-intellectual-movements)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "e7Ar8cwuzyeqKNd4v",
    "name": "Fermi estimate",
    "core": false,
    "slug": "fermi-estimate",
    "oldSlugs": [
      "fermi-estimation"
    ],
    "postCount": 13,
    "description": {
      "markdown": "A **Fermi estimate** (or **back-of-the-envelope calculation**, often abbreviated **BOTEC**) is a rough calculation that aims to be right within about an order of magnitude, prioritizing getting an answer good enough to be useful without putting the large amounts of thought and research needed to attain greater accuracy. Fermi estimates typically approximate an answer by making various simplifying assumptions and decomposing the problem into smaller tractable units. [Open Philanthropy](https://forum.effectivealtruism.org/tag/open-philanthropy) and other organizations in the [effective altruism](https://forum.effectivealtruism.org/topics/effective-altruism) community routinely use BOTECs for [impact assessment](https://forum.effectivealtruism.org/tag/impact-assessment) and [cause prioritization](https://forum.effectivealtruism.org/tag/cause-prioritization).\n\nFurther reading\n---------------\n\nBerger, Alexander (2014) [Alexander’s back of the envelope “importance” calculations](https://docs.google.com/document/d/1DTl4TYaTPMAtwQTju9PZmxKhZTCh6nmi-Vh8cnSgYak/edit#heading=h.6kp52s82mn4s), *GiveWell*.\n\nMuehlhauser, Luke (2013) [Fermi estimates](https://www.lesswrong.com/posts/PsEppdvgRisz5xAHG/fermi-estimates), *LessWrong*, April 11.\n\nvon Baeyer, Hans Christian (1988) [How Fermi would have fixed it](http://doi.org/10.1002/j.2326-1951.1988.tb03037.x), *The Sciences*, vol. 28, pp. 2-4.\n\nRelated entries\n---------------\n\n[cause prioritization](https://forum.effectivealtruism.org/tag/cause-prioritization) | [expected value](https://forum.effectivealtruism.org/tag/expected-value) | [forecasting](https://forum.effectivealtruism.org/tag/forecasting) | [impact assessment](https://forum.effectivealtruism.org/tag/impact-assessment) | [model uncertainty](https://forum.effectivealtruism.org/tag/model-uncertainty) | [prediction markets](https://forum.effectivealtruism.org/tag/prediction-markets) | [value of information](https://forum.effectivealtruism.org/tag/value-of-information)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "Pu7vRAdmC5zsPJNZ5",
    "name": "Scope neglect",
    "core": false,
    "slug": "scope-neglect",
    "oldSlugs": null,
    "postCount": 12,
    "description": {
      "markdown": "**Scope neglect** (also known as **scope insensitivity**) is the [cognitive bias](https://forum.effectivealtruism.org/topics/cognitive-bias) that causes people to value an issue in ways that ignore or give insufficient weight to its size.\n\nFurther reading\n---------------\n\nAnimal Ethics (2020) [Scope insensitivity: failing to appreciate the numbers of those who need our help](https://www.animal-ethics.org/cognitive-biases-and-animals/scope-insensitivity-failing-to-appreciate-the-numbers-of-those-who-need-our-help/), *Animal Ethics*.\n\nKahneman, Daniel & Amos Tversky (eds.) (2000) [*Choices, Values, and Frames*](https://en.wikipedia.org/wiki/Special:BookSources/0-521-62749-4), Cambridge: Cambridge University Press, pp. 650–657.\n\nYudkowsky, Eliezer (2015) [Scope insensitivity](https://en.wikipedia.org/wiki/Special:BookSources/9781939311146), in *Rationality: From AI to Zombies*, Berkeley: Machine Intelligence Research Institute, pp. 1453–1455.\n\nRelated entries\n---------------\n\n[cognitive bias](https://forum.effectivealtruism.org/tag/cognitive-bias) | [debunking argument](https://forum.effectivealtruism.org/tag/debunking-argument) | [expected value](https://forum.effectivealtruism.org/tag/expected-value) | [rationality](https://forum.effectivealtruism.org/tag/rationality)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "Sag3zQWi6CxDxuTXb",
    "name": "Cognitive bias",
    "core": false,
    "slug": "cognitive-bias",
    "oldSlugs": [
      "cognitive-biases"
    ],
    "postCount": 29,
    "description": {
      "markdown": "A **cognitive bias** is a systematic deviation from ideal reasoning.\n\nFurther reading\n---------------\n\nYudkowsky, Eliezer (2008) [Cognitive biases potentially affecting judgement of global risks](http://doi.org/10.1093/oso/9780198570509.003.0009), in Nick Bostrom & Milan M. Čirkovič (eds.) *Global Catastrophic Risks*, Oxford: Oxford University Press, pp. 91–119.\n\nRelated entries\n---------------\n\n[debunking argument](https://forum.effectivealtruism.org/tag/debunking-argument) | [inside vs. outside view](https://forum.effectivealtruism.org/tag/inside-vs-outside-view) | [rationality](https://forum.effectivealtruism.org/tag/rationality) | [rationality community](https://forum.effectivealtruism.org/tag/rationality-community) | [scope neglect](https://forum.effectivealtruism.org/tag/scope-neglect) | [speciesism](https://forum.effectivealtruism.org/tag/speciesism) | [status quo bias](https://forum.effectivealtruism.org/tag/status-quo-bias)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "EAxuwYnyrRhJBKMoH",
    "name": "Instrumental vs. epistemic rationality",
    "core": false,
    "slug": "instrumental-vs-epistemic-rationality",
    "oldSlugs": null,
    "postCount": 2,
    "description": {
      "markdown": "**Instrumental rationality** (or **practical rationality**) is concerned with what to do, while **epistemic rationality** (or **theoretical rationality**) is concerned with what to believe.\n\nFurther reading\n---------------\n\nLessWrong (2021) [Instrumental vs epistemic rationality](https://www.lesswrong.com/tag/rationality#:~:text=Instrumental%20rationality%20is%20defined%20as%20being%20concerned%20with%20achieving%20goals.&text=Epistemic%20rationality%20is%20defined%20as,you%20believe%20what%20you%20believe.), in 'Rationality', *LessWrong Wiki*.\n\nRelated entries\n---------------\n\n[rationality](https://forum.effectivealtruism.org/tag/rationality)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "rdxSaRhkHxwjbNkD7",
    "name": "Cost-effectiveness analysis",
    "core": false,
    "slug": "cost-effectiveness-analysis",
    "oldSlugs": null,
    "postCount": 63,
    "description": {
      "markdown": "**Cost-effectiveness analysis** (**CEA**) is a kind of economic analysis that allows comparison of the relative costs and effects of rival interventions. Unlike [cost-benefit analysis](https://forum.effectivealtruism.org/tag/cost-benefit-analysis), CEA does not express these effects in monetary values.\n\nFurther reading\n---------------\n\nBhula, Radhika, Meghan Mahoney & Kyle Murphy (2020) [Conducting cost-effectiveness analysis (CEA)](https://www.povertyactionlab.org/resource/conducting-cost-effectiveness-analysis-cea), *The Abdul Latif Jameel Poverty Action Lab*, July.\n\nJamison, Dean T. *et al.* (eds.) (2006) [*Priorities in Health*](https://doi.org/10.1596/978-0-8213-6260-0), Washington, D.C.: The World Bank, ch. 3.\n\nRelated entries\n---------------\n\n[cost-benefit analysis](https://forum.effectivealtruism.org/tag/cost-benefit-analysis) | [cost-effectiveness](https://forum.effectivealtruism.org/tag/cost-effectiveness) | [distribution of cost-effectiveness](https://forum.effectivealtruism.org/tag/distribution-of-cost-effectiveness) | [intervention evaluation](https://forum.effectivealtruism.org/tag/intervention-evaluation) | [ITN framework](https://forum.effectivealtruism.org/tag/itn-framework-1) | [cause priotization](https://forum.effectivealtruism.org/tag/cause-prioritization)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "vifMwt29txiZT6pQs",
    "name": "RC Forward",
    "core": false,
    "slug": "rc-forward",
    "oldSlugs": null,
    "postCount": 5,
    "description": {
      "markdown": "**RC Forward** is an effective giving platform based in Canada that regrants to high-impact charities globally. It offers donors tax-efficient giving options for a wide range of high-impact charities across several cause areas, including [global health](https://forum.effectivealtruism.org/topics/global-health-and-development), [animal welfare](https://forum.effectivealtruism.org/topics/animal-welfare-1), [climate change](https://forum.effectivealtruism.org/topics/climate-change), and the [long-term future](https://forum.effectivealtruism.org/topics/long-term-future).  RC Forward is a project of [Rethink Charity](https://forum.effectivealtruism.org/tag/rethink-charity).\n\nExternal links\n--------------\n\n[RC Forward](https://rcforward.org/). Official website."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "2Awc6vPWsReSxiwbM",
    "name": "Competitive debating",
    "core": false,
    "slug": "competitive-debating",
    "oldSlugs": [
      "ea-and-competitive-debate"
    ],
    "postCount": 8,
    "description": {
      "markdown": "Posts that relate to EA in the competitive debate sphere, such as at the high school or collegiate level. This especially includes topics such as community outreach/education projects (e.g., trying to introduce EA to debaters).\n\nRelated entries\n---------------\n\n[building effective altruism](/tag/building-effective-altruism-1) | [discussion norms](https://forum.effectivealtruism.org/tag/discussion-norms) | [effective altruism education](https://forum.effectivealtruism.org/tag/ea-education)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "mRwN89WxaT6TH6SBd",
    "name": "Students for High-Impact Charity",
    "core": false,
    "slug": "students-for-high-impact-charity",
    "oldSlugs": null,
    "postCount": 11,
    "description": {
      "markdown": "**Students for High-Impact Charity** (**SHIC**) was a project that focused on educational outreach for high school students (primarily ages 16-18) through interactive content.\n\nIn 2017, SHIC teamed up with [.impact](https://forum.effectivealtruism.org/tag/impact) to form [Rethink Charity](https://forum.effectivealtruism.org/tag/rethink-charity).^[\\[1\\]](#fna7mewidx83)^ SHIC suspended outreach operations in 2019.^[\\[2\\]](#fn61za3jf9idu)^\n\nExternal links\n--------------\n\n[Students for High-Impact Charity](https://shicschools.org/). Official website.\n\nRelated entries\n---------------\n\n[building effective altruism](https://forum.effectivealtruism.org/tag/building-effective-altruism-1) | [effective altruism education](https://forum.effectivealtruism.org/tag/ea-education) | [.impact](https://forum.effectivealtruism.org/tag/impact) | [Rethink Charity](https://forum.effectivealtruism.org/tag/rethink-charity)\n\n1.  ^**[^](#fnrefa7mewidx83)**^\n    \n    Barnett, Tee (2017) [.impact is now Rethink Charity](https://forum.effectivealtruism.org/posts/a3sScjxdgNTCseBMJ/impact-is-now-rethink-charity), *Effective Altruism Forum*, May 30.\n    \n2.  ^**[^](#fnref61za3jf9idu)**^\n    \n    Bullock, Baxter & Catherine Low (2019) [SHIC will suspend outreach operations](https://forum.effectivealtruism.org/posts/3HaXa7dtu86NQNEZJ/shic-will-suspend-outreach-operations), *Effective Altruism Forum*, March 7."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "Es7euBt6Ky9icFtX8",
    "name": "Expertise",
    "core": false,
    "slug": "expertise",
    "oldSlugs": null,
    "postCount": 7,
    "description": {
      "markdown": "Posts that relate to \"expertise\" in the sense of what it is, how important it is (e.g., when to rely on it, how valuable it is to achieve/develop), how to evaluate it, etc.\n\nRelated entries\n---------------\n\n[academia](https://forum.effectivealtruism.org/tag/academia-1) | [epistemic deference](https://forum.effectivealtruism.org/tag/epistemic-deference) | [inside vs. outside view](https://forum.effectivealtruism.org/tag/inside-vs-outside-view)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "gttxR9Cnzo6MQwJ5c",
    "name": "Forecasting Newsletter",
    "core": false,
    "slug": "forecasting-newsletter",
    "oldSlugs": null,
    "postCount": 31,
    "description": {
      "markdown": "The **Forecasting Newsletter** is a monthly newsletter about [forecasting](https://forum.effectivealtruism.org/tag/forecasting), with a focus on experimental human judgemental forecasting.  It was launched in early 2020.\n\nFurther reading\n---------------\n\nSempere, Nuño (2020) [What is this about?](https://forecasting.substack.com/about), *Forecasting Newsletter*.\n\nSempere, Nuño (2021) [2020: Forecasting in review](https://forum.effectivealtruism.org/posts/8shCj2eoQygQvtoZP/2020-forecasting-in-review), *Effective Altruism Forum*, January 10.\n\nExternal links\n--------------\n\n[Forecasting Newsletter](https://forecasting.substack.com/). Official website.\n\nRelated entries\n---------------\n\n[Alignment Newsletter](https://forum.effectivealtruism.org/tag/alignment-newsletter) | [Effective Altruism Newsletter](https://forum.effectivealtruism.org/tag/effective-altruism-newsletter) | [forecasting](https://forum.effectivealtruism.org/tag/forecasting) | [Forecasting Newsletter](https://forum.effectivealtruism.org/tag/forecasting-newsletter) | [Future Matters](https://forum.effectivealtruism.org/tag/future-matters) | [newsletters](https://forum.effectivealtruism.org/tag/newsletters)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "AtQL8fbsC9H7X4xzM",
    "name": "International organization",
    "core": false,
    "slug": "international-organization",
    "oldSlugs": [
      "united-nations",
      "international-organisations",
      "international-organizations"
    ],
    "postCount": 8,
    "description": {
      "markdown": "An **international organization** is an \"institution drawing membership from at least three states, having activities in several states, and whose members are held together by a formal agreement\".^[\\[1\\]](#fnnr61yow1frf)^ Examples include the [United Nations](https://forum.effectivealtruism.org/tag/united-nations-1) and the [European Union](https://forum.effectivealtruism.org/tag/european-union). Two types of international organization are **intergovernmental organizations** and **supranational organizations**.\n\nFurther reading\n---------------\n\nMingst, Karen (1998) [International organization](https://www.britannica.com/topic/international-organization), *Encyclopedia Britannica*, July 20 (updated 21 May 2020).\n\nRelated entries\n---------------\n\n[European Union](/tag/european-union) | [global governance](/tag/global-governance) | [international relations](/tag/international-relations) | [law](/tag/law) | [United Nations](https://forum.effectivealtruism.org/tag/united-nations-1) | [vulnerable world hypothesis](/tag/vulnerable-world-hypothesis) | [World Health Organization](https://forum.effectivealtruism.org/topics/world-health-organization)\n\n1.  ^**[^](#fnrefnr61yow1frf)**^\n    \n    Mingst, Karen (1998) [International organization](https://www.britannica.com/topic/international-organization), *Encyclopedia Britannica*, July 20 (updated 21 May 2020)."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "mR7SeshNbRoKoCqTJ",
    "name": "Information security",
    "core": false,
    "slug": "information-security",
    "oldSlugs": [
      "cybersecurity-and-information-security"
    ],
    "postCount": 29,
    "description": {
      "markdown": "**Information security** is the set of methods aimed at protecting the confidentiality, availability and integrity of information.\n\nFurther reading\n---------------\n\nLadish, Jeffrey & Lennart Heim (2022) [Information security considerations for AI and the long term future](https://forum.effectivealtruism.org/posts/WqQDCCLWbYfFRwubf/information-security-considerations-for-ai-and-the-long-term), *Effective Altruism Forum*, May 2.\n\nWiblin, Robert & Keiran Harris (2022) [Nova DasSarma on why information security may be critical to the safe development of AI systems](https://80000hours.org/podcast/episodes/nova-dassarma-information-security-and-ai-systems/), *80,000 Hours*, June 14.\n\nZabel, Claire & Luke Muehlhaeuser (2019) [Information security careers for GCR reduction](https://forum.effectivealtruism.org/posts/ZJiCfwTy5dC4CoxqA/information-security-careers-for-gcr-reduction), *Effective Altruism Forum*, June 20.\n\nExternal links\n--------------\n\n[Information Security in Effective Altruism](https://www.facebook.com/groups/EAinfosec/). A group for members of the [effective altruism](https://forum.effectivealtruism.org/topics/effective-altruism) community interested in applying information security to [global catastrophic risk](https://forum.effectivealtruism.org/topics/global-catastrophic-risk) reduction or other areas of effective altruism.\n\nRelated entries\n---------------\n\n[AI governance](https://forum.effectivealtruism.org/topics/ai-governance) | [AI risk](https://forum.effectivealtruism.org/tag/ai-risk) | [armed conflict](https://forum.effectivealtruism.org/tag/armed-conflict) | [autonomous weapon](https://forum.effectivealtruism.org/tag/autonomous-weapon) | [existential risk factor](https://forum.effectivealtruism.org/tag/existential-risk-factor) | [nuclear warfare](https://forum.effectivealtruism.org/tag/nuclear-warfare-1) | [public interest technology](https://forum.effectivealtruism.org/tag/public-interest-technology)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "QPz9zjrK7i8gDvXxB",
    "name": "India",
    "core": false,
    "slug": "india",
    "oldSlugs": null,
    "postCount": 39,
    "description": {
      "markdown": "The **India** tag is for posts that are about India, that address how India is relevant to various issues people in the effective altruism community care about, or that are relevant to how one could have an impact by engaging with India.\n\nRelated entries\n---------------\n\n[China](https://forum.effectivealtruism.org/tag/china) | [global outreach](https://forum.effectivealtruism.org/tag/global-outreach) | [international relations](https://forum.effectivealtruism.org/tag/international-relations)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "3MaNTgoRt9iLHEcW3",
    "name": "Local priorities research",
    "core": false,
    "slug": "local-priorities-research-1",
    "oldSlugs": null,
    "postCount": 11,
    "description": {
      "markdown": "**Local priorities research** (**LPR**) is research aimed at identifying high priority problems within a local context, such as the context of a particular country.\n\nThis is separate from research that aims to help a narrow group of people; the idea is to leverage local opportunities, not to narrow the moral circle. \n\nRelated entries\n---------------\n\n[building effective altruism](https://forum.effectivealtruism.org/tag/building-effective-altruism-1) | [global priorities research](https://forum.effectivealtruism.org/tag/global-priorities-research)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "8XJ65HZ4Pp2BFhZzp",
    "name": "GiveDirectly",
    "core": false,
    "slug": "givedirectly",
    "oldSlugs": null,
    "postCount": 41,
    "description": {
      "markdown": "**GiveDirectly** is a charity that provides direct [cash transfers](https://forum.effectivealtruism.org/tag/cash-transfers) to people living in [extreme poverty](https://forum.effectivealtruism.org/tag/global-poverty) in many countries, especially Kenya and Uganda.\n\nHistory\n-------\n\nGiveDirectly was founded in 2009 by Paul Niehaus, Michael Faye, Rohit Wanchoo and Jeremy Shapiro.\n\nEvaluation\n----------\n\nGiveDirectly's programs have been the subject of several RCTs, and they have experimented with many ways to implement transfers, such as single lump-sum transfers, multiple transfers as \"basic income\", and transfers as a form of disaster relief.\n\nGiveDirectly was a [GiveWell](https://forum.effectivealtruism.org/tag/givewell) top-rated charity between 2012 and 2022.^[\\[1\\]](#fnn95i8k5xoe)^ GiveWell estimates that participants in GiveDirectly's programs receive $83 out of every $100 donated to that organization.^[\\[2\\]](#fnr5w10pyilj)^^[\\[3\\]](#fnltw5rs9bkz)^ GiveDirectly is also featured in [The Life You Can Save](https://forum.effectivealtruism.org/tag/the-life-you-can-save)'s list of \"best charities\".^[\\[4\\]](#fn00fi85d13fpo)^\n\nAs of July 2022, GiveDirectly has received over $58.2 million in funding from [Open Philanthropy](https://forum.effectivealtruism.org/tag/open-philanthropy).^[\\[5\\]](#fnar4ny1niqt9)^ \n\nFurther reading\n---------------\n\nGiveWell (2018) [GiveWell’s analysis of GiveDirectly financial summary through February 2018](https://docs.google.com/spreadsheets/d/1L03SQuAeRRfjyuxy20QIJByOx6PEzyJ-x4edz2fSiQ4/edit#gid=537899494), *GiveWell*.\n\nGiveWell (2020) [GiveDirectly](https://www.givewell.org/charities/give-directly), *GiveWell*, November.\n\nExternal links\n--------------\n\n[GiveDirectly](https://www.givedirectly.org/). Official website.\n\n[GiveDirectly](https://forum.effectivealtruism.org/users/givedirectly). [Effective Altruism Forum](https://forum.effectivealtruism.org/tag/effective-altruism-forum-1) account.\n\n[Apply for a job](https://givedirectly.hire.trakstar.com/).\n\n[Donate to GiveDirectly](https://donate.givedirectly.org/).\n\n1.  ^**[^](#fnrefn95i8k5xoe)**^\n    \n    Sánchez, Sebastián *et al.* (2019) [Timeline of GiveDirectly](https://timelines.issarice.com/wiki/Timeline_of_GiveDirectly), *Timelines Wiki*.\n    \n2.  ^**[^](#fnrefr5w10pyilj)**^\n    \n    GiveWell (2021) [Our top charities](https://www.givewell.org/charities/top-charities), *GiveWell*, November.\n    \n3.  ^**[^](#fnrefltw5rs9bkz)**^\n    \n    GiveWell (2021) [2021 GiveWell cost-effectiveness analysis — version 3](https://docs.google.com/spreadsheets/d/1B1fODKVbnGP4fejsZCVNvBm5zvI1jC7DhkaJpFk6zfo/edit#gid=1377543212), *GiveWell*, July 6.\n    \n4.  ^**[^](#fnref00fi85d13fpo)**^\n    \n    The Life You Can Save (2021) [GiveDirectly](https://www.thelifeyoucansave.org/best-charities/givedirectly/), *The Life You Can Save*.\n    \n5.  ^**[^](#fnrefar4ny1niqt9)**^\n    \n    Open Philanthropy (2022) [Grants database: GiveDirectly](https://www.openphilanthropy.org/grants/?q=&organization-name=givedirectly), *Open Philanthropy*."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "zrXZ8sezcSoND2z74",
    "name": "Canopie",
    "core": false,
    "slug": "canopie",
    "oldSlugs": null,
    "postCount": 1,
    "description": {
      "markdown": "**Canopie** is an organization that provides [mental health](https://forum.effectivealtruism.org/tag/mental-health) support for pre- and post-partum women through guided cognitive behavioral therapy. It was incubated by [Charity Entrepreneurship](https://forum.effectivealtruism.org/tag/charity-entrepreneurship).\n\nExternal links\n--------------\n\n[Canopie](https://www.canopie.org/). Official website.\n\n[Apply for a job](https://angel.co/company/canopie/jobs).\n\nRelated entries\n---------------\n\n[mental health](https://forum.effectivealtruism.org/tag/mental-health)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "8YskW4Rdo2Fqxs3n8",
    "name": "Bayesian epistemology",
    "core": false,
    "slug": "bayesian-epistemology",
    "oldSlugs": [
      "bayesian-reasoning"
    ],
    "postCount": 15,
    "description": {
      "markdown": "**Bayesian epistemology** (sometimes referred to as **Bayesianism**) is the broader [epistemology](https://forum.effectivealtruism.org/topics/epistemology) informed by [Bayes' theorem](https://forum.effectivealtruism.org/topics/bayes-theorem).\n\nFurther reading\n---------------\n\nLessWrong (2020) [Bayesianism](https://www.lesswrong.com/tag/bayesianism), *LessWrong Wiki*.\n\nRelated entries\n---------------\n\n[Bayes' theorem](https://forum.effectivealtruism.org/tag/bayes-theorem) | [decision theory](https://forum.effectivealtruism.org/tag/decision-theory) | [epistemic deference](https://forum.effectivealtruism.org/tag/epistemic-deference) | [epistemology](https://forum.effectivealtruism.org/tag/epistemology) | [rationality](https://forum.effectivealtruism.org/tag/rationality)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "jTr2TBadFnd9XkJEs",
    "name": "Ideological Turing test",
    "core": false,
    "slug": "ideological-turing-test",
    "oldSlugs": null,
    "postCount": 2,
    "description": {
      "markdown": "The **ideological Turing test** (sometimes called the **political Turing test**^[\\[1\\]](#fniuhpitd1cxl)^) is a test of a person's ability to state opposing views as clearly and persuasively as those views are stated by their proponents. The test was originally proposed by [Bryan Caplan](https://forum.effectivealtruism.org/tag/bryan-caplan),^[\\[2\\]](#fnf86iwfksflu)^ in analogy with Alan Turing's \"imitation game\"—more widely known as the Turing test—, which measures a machine's ability to exhibit intelligent behaviour indistinguishable from that of a human.\n\nFurther reading\n---------------\n\nBrin, David (2000) [Disputation arenas: Harnessing conflict and competitiveness for society’s benefit](https://heinonline.org/HOL/Page?handle=hein.journals/ohjdpr15&div=34&g_sent=1&casa_token=tgVElxn7wd0AAAAA:Dsq8nJVTPrab-XlB-XsmdHb6xhWjkrSojPvXOBzuzdoSIMV1_jeyr-OQ7XcSn93uacrXkZx6kPk&collection=journals), *Ohio State Journal on Dispute Resolution*, vol. 15, pp. 597–618.\n\nCaplan, Bryan (2011) [The ideological Turing test](https://www.econlib.org/archives/2011/06/the_ideological.html), *Econlog*, June 20.\n\nGalef, Julia (2021) [*The Scout Mindset: Why Some People See Things Clearly and Others Don’t*](https://en.wikipedia.org/wiki/Special:BookSources/9780735217553), New York: Portfolio.\n\nKling, Arnold (2013) [*The Three Languages of Politics: Talking across the Political Divides*](https://en.wikipedia.org/wiki/Special:BookSources/9781948647427), Washington: Cato Institute.\n\nRelated entries\n---------------\n\n[Bryan Caplan](https://forum.effectivealtruism.org/tag/bryan-caplan) | [epistemic deference](https://forum.effectivealtruism.org/tag/epistemic-deference)\n\n1.  ^**[^](#fnrefiuhpitd1cxl)**^\n    \n    Hannon, Michael (2020) [Empathetic understanding and deliberative democracy](https://doi.org/10.1111/phpr.12624), *Philosophy and Phenomenological Research*, vol. 101, pp. 591–611.\n    \n2.  ^**[^](#fnreff86iwfksflu)**^\n    \n    Caplan, Bryan (2011) [The ideological Turing test](https://www.econlib.org/archives/2011/06/the_ideological.html), *Econlog*, June 20."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "FDge6hLNoN2xz3QE9",
    "name": "Effective Altruism Coaching",
    "core": false,
    "slug": "effective-altruism-coaching",
    "oldSlugs": null,
    "postCount": 10,
    "description": {
      "markdown": "**Effective Altruism Coaching** (often spelled **EA Coaching**) is a nonprofit organization that provides [productivity](https://forum.effectivealtruism.org/topics/productivity) [coaching](https://forum.effectivealtruism.org/tag/coaching) to individuals working on [high-priority causes](https://forum.effectivealtruism.org/tag/cause-prioritization).\n\nFunding \n--------\n\nEffective Altruism Coaching has received $70,000 in grants from [Effective Altruism Funds](https://forum.effectivealtruism.org/topics/effective-altruism-funds).^[\\[1\\]](#fnrniit8fycjn)^^[\\[2\\]](#fntpo80ibbxuc)^^[\\[3\\]](#fn694232rxmln)^\n\nExternal links\n--------------\n\n[Effective Altruism Coaching](https://effectivealtruismcoaching.com/). Official website.\n\nRelated entries\n---------------\n\n[coaching](https://forum.effectivealtruism.org/tag/coaching) | [personal development](https://forum.effectivealtruism.org/tag/personal-development) | [management & mentoring](https://forum.effectivealtruism.org/tag/management-and-mentoring)\n\n1.  ^**[^](#fnrefrniit8fycjn)**^\n    \n    Effective Altruism Infrastructure Fund (2019) [July 2019: EA Meta Fund grants](https://funds.effectivealtruism.org/funds/payouts/july-2019-ea-meta-fund-grants), *Effective Altruism Funds*, July.\n    \n2.  ^**[^](#fnreftpo80ibbxuc)**^\n    \n    Long-Term Future Fund (2019) [August 2019: Long-Term Future Fund grants and recommendations](https://funds.effectivealtruism.org/funds/payouts/august-2019-long-term-future-fund-grants-and-recommendations), *Effective Altruism Funds*, August.\n    \n3.  ^**[^](#fnref694232rxmln)**^\n    \n    Effective Altruism Infrastructure Fund (2020) [March 2020: EA Meta Fund Grants](https://funds.effectivealtruism.org/funds/payouts/march-2020-ea-meta-fund-grants), *Effective Altruism Funds*, March."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "YiPo6ijmHxZ3LoLot",
    "name": "Naive vs. sophisticated consequentialism",
    "core": false,
    "slug": "naive-vs-sophisticated-consequentialism",
    "oldSlugs": [
      "naive-consequentialism-vs-sophisticated-consequentialism"
    ],
    "postCount": 10,
    "description": {
      "markdown": "**Naive consequentialism** is the view that, to comply with the requirements of [consequentialism](https://forum.effectivealtruism.org/topics/consequentialism), an agent should at all times be motivated to perform the act that consequentialism requires. By contrast, **sophisticated consequentialism** holds that a consequentialist agent should adopt whichever set of motivations will cause her to in fact act in ways required by consequentialism.\n\nTerminology\n-----------\n\nSometimes the terms \"sophisticated consequentialism\" and \"naive consequentialism\" are used to describe the contrast between applications of consequentialism that do and do not, respectively, consider less direct, less immediate, or otherwise less visible consequences into account.^[\\[1\\]](#fn7vra0f9q0iu)^\n\nAs for a concrete example, a naive conception of consequentialism may lead one to believe that it is right to break rules if it seems that the immediate effects on the world would be net-positive. Such rule-breaking normally has negative side-effects, however - e.g. it can lower the degree of trust in society, and for the rule-breaker’s group in particular - which means that sophisticated consequentialism tends to be more opposed to rule-breaking than naive consequentialism.\n\nFurther reading\n---------------\n\nCaviola, Lucius (2017) [Against naive effective altruism](https://www.youtube.com/watch?v=-2oRgxxafXk), EAGx Berlin, November 20.\n\nOrd, Toby (2009) [*Beyond Action: Applying Consequentialism to Decision Making and Motivation*](https://ethos.bl.uk/OrderDetails.do?uin=uk.bl.ethos.508590), Ph.D. thesis, University of Oxford.\n\nRelated entries\n---------------\n\n[accidental harm](https://forum.effectivealtruism.org/tag/accidental-harm) | [consequentialism](https://forum.effectivealtruism.org/tag/consequentialism) | [fanaticism](https://forum.effectivealtruism.org/tag/fanaticism) | [indirect long-term effects](https://forum.effectivealtruism.org/tag/indirect-long-term-effects)\n\n1.  ^**[^](#fnref7vra0f9q0iu)**^\n    \n    Cf. [80,000 Hours](https://forum.effectivealtruism.org/topics/80-000-hours)’ discussion of “simplistic” vs. “correct” [replaceability](https://forum.effectivealtruism.org/tag/replaceability) in Todd, Benjamin (2015) [‘Replaceability’ isn’t as important as you might think (or we’ve suggested)](https://80000hours.org/2015/07/replaceability-isnt-as-important-as-you-might-think-or-weve-suggested/), *80,000 Hours*, July 27."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "cbT27oq3XKren8MWB",
    "name": "Donation pledge",
    "core": false,
    "slug": "donation-pledge",
    "oldSlugs": null,
    "postCount": 22,
    "description": {
      "markdown": "A **donation pledge** is a commitment to give at least some specified amount of money over some specified time period to charities or other donation opportunities. Some pledges target specific types of people. For example, the [Giving What We Can](https://forum.effectivealtruism.org/tag/giving-what-we-can) Pledge is a commitment people make to donate at least 10% of their income per year over their lifetimes, the [Founders Pledge](https://forum.effectivealtruism.org/tag/founders-pledge) is a commitment entrepreneurs make to donate a percentage of their profits, and the [Giving Pledge](https://forum.effectivealtruism.org/tag/giving-pledge) is a commitment very wealthy individuals make to donate the majority of their wealth.\n\nOther examples of donation pledges include the pledges associated with the organisations [One for the World](https://forum.effectivealtruism.org/tag/one-for-the-world-1), [The Life You Can Save](https://forum.effectivealtruism.org/tag/the-life-you-can-save), [Generation Pledge](https://forum.effectivealtruism.org/tag/generation-pledge), [Raising for Effective Giving](https://forum.effectivealtruism.org/tag/raising-for-effective-giving), and [High-Impact Athletes](https://forum.effectivealtruism.org/tag/high-impact-athletes), as well as [Giving What We Can's Trial Pledge](https://www.givingwhatwecan.org/get-involved/trial-pledge/).\n\nThere are various reasons for or against taking, or promoting, a donation pledge. For example, donation pledges might help a person actually follow through on their altruistic intentions, rather than forgetting to do so or facing [value drift](https://forum.effectivealtruism.org/tag/value-drift). But it is also possible for a pledge to be *too* constraining, for example, if a person's financial situation changes, if they now want to take risks with their career and thus need more financial runway, or if the donation opportunities they want to give to are not within the scope of their pledge (e.g., if they are not registered charities).\n\nRelated entries\n---------------\n\n[donation choice](https://forum.effectivealtruism.org/tag/donation-choice) | [effective altruism funding](https://forum.effectivealtruism.org/tag/effective-altruism-funding) | [effective altruism lifestyle](https://forum.effectivealtruism.org/tag/effective-altruism-lifestyle) | [earning to give](https://forum.effectivealtruism.org/tag/earning-to-give) | [effective giving](https://forum.effectivealtruism.org/tag/effective-giving-1) | [The Life You Can Save](https://forum.effectivealtruism.org/tag/the-life-you-can-save)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "2BvgFyR85zX25osTT",
    "name": "Fellowships and internships",
    "core": false,
    "slug": "fellowships-and-internships",
    "oldSlugs": [
      "fellowships-and-internships",
      "fellowships-and-internships"
    ],
    "postCount": 63,
    "description": {
      "markdown": "The **fellowships and internships** tag is used for discussion of limited-term work or community experiences offered by particular organizations. These opportunities may be paid or unpaid, and are usually targeted at people early in their careers. Many [effective altruism groups](https://forum.effectivealtruism.org/tag/effective-altruism-groups) offer fellowships for their members.\n\nSee [research training programs](https://forum.effectivealtruism.org/tag/research-training-programs) for opportunities specific to research.\n\nExternal links\n--------------\n\n[EA Internship Board](https://ea-internships.pory.app/board). A list of internships.\n\n[EA Training Board](https://www.trainingforgood.com/training-board). A list of fellowships and other training programs.\n\nRelated entries\n---------------\n\n[research training programs](https://forum.effectivealtruism.org/tag/research-training-programs) | [job listing (open)](https://forum.effectivealtruism.org/tag/job-listing-open) | [take action](https://forum.effectivealtruism.org/tag/take-action)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "PfGnfNeHw6B4figcH",
    "name": "Stuart Russell",
    "core": false,
    "slug": "stuart-russell",
    "oldSlugs": null,
    "postCount": 4,
    "description": {
      "markdown": "**Stuart Jonathan Russell** (born 1962) is an English computer scientist, currently Professor of Computer Science at the University of California, Berkeley. Russell is the author of [*Human Compatible: Artificial Intelligence and the Problem of Control*](https://forum.effectivealtruism.org/tag/human-compatible) and the co-author of *Artificial Intelligence: A Modern Approach*.\n\nExternal links\n--------------\n\n[Stuart J. Russell](https://www2.eecs.berkeley.edu/Faculty/Homepages/russell.html). Berkeley EECS homepage.\n\nRelated entries\n---------------\n\n[Center for Human-Compatible Artificial Intelligence](https://forum.effectivealtruism.org/tag/center-for-human-compatible-artificial-intelligence)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5R7s7dweoeBvcb9Yk",
    "name": "Estimation of existential risk",
    "core": false,
    "slug": "estimation-of-existential-risk",
    "oldSlugs": [
      "estimating-existential-risks",
      "estimation-of-existential-risks"
    ],
    "postCount": 21,
    "description": {
      "markdown": "**Estimation of existential risk** is the set of methods for assessing the probability of [human extinction](https://forum.effectivealtruism.org/topics/human-extinction) and other [existential catastrophes](https://forum.effectivealtruism.org/topics/existential-catastrophe-1).\n\nFurther reading\n---------------\n\nBeard, Simon, Thomas Rowe & James Fox (2020) [An analysis and evaluation of methods currently used to quantify the likelihood of existential hazards](http://doi.org/10.1016/j.futures.2019.102469), *Futures*, vol. 115, pp. 1–14.\n\nBesiroglu, Tamay (2019) [Ragnarök Series — results so far](https://www.metaculus.com/questions/2568/ragnar%25C3%25B6k-seriesresults-so-far/), *Metaculus*, October 15.\n\nBostrom, Nick (2002) [Existential risks: Analyzing human extinction scenarios and related hazards](https://www.jetpress.org/volume9/risks.html), *Journal of evolution and technology*, vol. 9.  \n*The section “Assessing the Probability of Existential Risks” discusses methods of existential risk estimation.*\n\nGoth, Aidan, Stephen Clare & Christian Ruhl (2022) [Professor Philip Tetlock’s research on improving judgments of existential risk](https://founderspledge.com/stories/prof-philip-tetlocks-forecasting-research-high-impact-funding-opportunity), *Founders Pledge*, March 8.\n\nKarger, Ezra, Pavel D. Atanasov & Philip Tetlock (2022) [Improving judgments of existential risk: Better forecasts, questions, explanations, policies](http://doi.org/10.2139/ssrn.4001628), *SSRN Electronic Journal*.\n\nMuehlhauser, Luke (2019) [How feasible is long-range forecasting?](https://www.openphilanthropy.org/blog/how-feasible-long-range-forecasting), *Open Philanthropy*, October 10.\n\nSandberg, A. & Bostrom, N. (2008) [Global Catastrophic Risks Survey](https://www.fhi.ox.ac.uk/reports/2008-1.pdf), Technical Report #2008-1, Future of Humanity Institute, University of Oxford.\n\nTonn, Bruce & Dorian Stiefel (2013) [Evaluating methods for estimating existential risks](https://doi.org/10.1111/risa.12039), *Risk Analysis*, vol. 33, pp. 1772–1787.\n\nRelated entries\n---------------\n\n[AI forecasting](https://forum.effectivealtruism.org/tag/ai-forecasting) | [anthropic shadow](https://forum.effectivealtruism.org/tag/anthropic-shadow) | [existential risk](https://forum.effectivealtruism.org/tag/existential-risk) | [forecasting](https://forum.effectivealtruism.org/tag/forecasting) | [long-range forecasting](https://forum.effectivealtruism.org/tag/long-range-forecasting)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "rYR2oa6DZyPcnvRTM",
    "name": "Generation Pledge",
    "core": false,
    "slug": "generation-pledge",
    "oldSlugs": null,
    "postCount": 4,
    "description": {
      "markdown": "**Generation Pledge** is a nonprofit that encourages inheritors of large fortunes to [pledge to donate](https://forum.effectivealtruism.org/tag/donation-pledge) at least ten percent of their inheritance to [effective charities](https://forum.effectivealtruism.org/tag/effective-giving).\n\nFunding\n-------\n\nAs of July 2022, Generation Pledge has received nearly $400,000 in funding from the [Survival and Flourishing](https://forum.effectivealtruism.org/topics/survival-and-flourishing) Fund,^[\\[1\\]](#fn22qzyb7lwds)^^[\\[2\\]](#fn08kr0oozj2cr)^^[\\[3\\]](#fnkypdqxrfokn)^ and $30,000 from [Effective Altruism Funds](https://forum.effectivealtruism.org/topics/effective-altruism-funds).^[\\[4\\]](#fnso7r2st949i)^\n\nFurther reading\n---------------\n\nRathner, Natalie & Sunnie Huang (2021) [The world isn’t paying enough attention to inheritors. And vice versa](https://members.wingsweb.org/news/108186), *Wings*, October 28.\n\nExternal links\n--------------\n\n[Generation Pledge](https://www.generationpledge.org/). Official website.\n\n[Take the Generation Pledge](https://www.generationpledge.org/contact).\n\nRelated entries\n---------------\n\n[donation pledge](https://forum.effectivealtruism.org/tag/donation-pledge)\n\n1.  ^**[^](#fnref22qzyb7lwds)**^\n    \n    Survival and Flourishing Fund (2019) [SFF-2020-H2 S-process recommendations announcement](https://survivalandflourishing.fund/sff-2020-h2-recommendations), *Survival and Flourishing Fund*. \n    \n2.  ^**[^](#fnref08kr0oozj2cr)**^\n    \n    Survival and Flourishing Fund (2020) [SFF-2021-H1 S-process recommendations announcement](https://survivalandflourishing.fund/sff-2021-h1-recommendations), *Survival and Flourishing Fund*. \n    \n3.  ^**[^](#fnrefkypdqxrfokn)**^\n    \n    Survival and Flourishing Fund (2021) [SFF-2022-H1 S-Process recommendations announcement](https://survivalandflourishing.fund/sff-2022-h1-recommendations), *Survival and Flourishing Fund*.\n    \n4.  ^**[^](#fnrefso7r2st949i)**^\n    \n    Effective Altruism Infrastructure Fund (2019) [July 2019: EA Meta Fund grants](https://funds.effectivealtruism.org/funds/payouts/july-2019-ea-meta-fund-grants), *Effective Altruism Funds*, July."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "P92i38tXueadEodJ6",
    "name": "Probably Good",
    "core": false,
    "slug": "probably-good",
    "oldSlugs": null,
    "postCount": 8,
    "description": {
      "markdown": "**Probably Good** is nonprofit organization that helps individuals select [high-impact careers](https://forum.effectivealtruism.org/tag/career-choice).\n\nEvaluation\n----------\n\nAs of July 2022, Probably Good has received a total of $300,000 in funding from [Open Philanthropy](https://forum.effectivealtruism.org/tag/open-philanthropy).^[\\[1\\]](#fnbdt4ce7455a)^\n\nFurther reading\n---------------\n\nNevo, Omer & Sella Nevo (2020) [Introducing Probably Good: a new career guidance organization](https://forum.effectivealtruism.org/posts/3QufK3jjQ5aqksaJu/introducing-probably-good-a-new-career-guidance-organization), *Effective Altruism Forum*, November 6.\n\nExternal links\n--------------\n\n[Probably Good](https://www.probablygood.org/). Official website.\n\n[Apply for a job](https://www.probablygood.org/work-with-us).\n\nRelated entries\n---------------\n\n[80,000 Hours](https://forum.effectivealtruism.org/topics/80-000-hours) | [career advising](https://forum.effectivealtruism.org/tag/career-advising) | [career choice](https://forum.effectivealtruism.org/tag/career-choice) | [Non-trivial](https://forum.effectivealtruism.org/topics/non-trivial)\n\n1.  ^**[^](#fnrefbdt4ce7455a)**^\n    \n    Open Philanthropy (2022) [Grants database: Probably Good](https://www.openphilanthropy.org/grants/?q=&organization-name=probably-good), *Open Philanthropy*."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "A7Kgc9sMmRcXkWK7S",
    "name": "Field-building",
    "core": false,
    "slug": "field-building",
    "oldSlugs": [
      "field-building",
      "field-building"
    ],
    "postCount": 35,
    "description": {
      "markdown": "**Field-building** refers to influencing existing fields of research or advocacy or developing new ones, through advocacy, creating organisations,  or funding people to work in the field.\n\nExamples of field-building activities include scholarships and teaching buyouts (e.g. [Forethought Foundation](https://forum.effectivealtruism.org/tag/forethought-foundation), several grantmakers); [Charity Entrepreneurship](https://forum.effectivealtruism.org/tag/charity-entrepreneurship)’s Incubation Program;^[\\[1\\]](#fnuo356dk43i)^ [Future of Humanity Institute](https://forum.effectivealtruism.org/tag/future-of-humanity-institute)’s Research Scholars Programme;^[\\[2\\]](#fnv7ay4sl0fta)^ and [conferences](https://forum.effectivealtruism.org/tag/conferences) (such as [Effective Altruism Global](https://forum.effectivealtruism.org/tag/effective-altruism-global)).\n\nFurther reading\n---------------\n\nMuehlhauser, Luke (2017) [Some case studies in early field growth](https://www.openphilanthropy.org/research/history-of-philanthropy/some-case-studies-early-field-growth), *Open Philanthropy*, August.\n\nVaughan, Kerry (2016) [What the EA community can learn from the rise of the neoliberals](https://www.effectivealtruism.org/articles/ea-neoliberal/), *Effective Altruism*, December 5.\n\nRelated entries\n---------------\n\n[academia](https://forum.effectivealtruism.org/tag/academia-1/) | [network building](https://forum.effectivealtruism.org/tag/network-building) | [research training programs](https://forum.effectivealtruism.org/tag/research-training-programs)\n\n1.  ^**[^](#fnrefuo356dk43i)**^\n    \n    Charity Entrepreneurship (2021) [Incubation Program](https://www.charityentrepreneurship.com/incubation-program.html), *Charity Entrepreneurship*.\n    \n2.  ^**[^](#fnrefv7ay4sl0fta)**^\n    \n    Hadshar, Rose (2020) [What FHI’s Research Scholars Programme is like: views from scholars](https://forum.effectivealtruism.org/posts/e8CXMz3PZqSir4uaX/what-fhi-s-research-scholars-programme-is-like-views-from-1), *Effective Altruism Forum*, August 11, 2020."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "nzoQuy7Hj7cehMNeS",
    "name": "Intervention evaluation",
    "core": false,
    "slug": "intervention-evaluation",
    "oldSlugs": [
      "intervention-evaluation"
    ],
    "postCount": 30,
    "description": {
      "markdown": "**Intervention evaluation** is the comparison of different interventions, normally within a given cause area. However, some organizations may compare interventions across cause areas to decide where to give. Organizations that conduct intervention evaluations include [GiveWell](https://forum.effectivealtruism.org/tag/givewell), [Animal Charity Evaluators](https://forum.effectivealtruism.org/tag/animal-charity-evaluators), [Charity Entrepreneurship](https://forum.effectivealtruism.org/tag/charity-entrepreneurship), [Rethink Priorities](https://forum.effectivealtruism.org/tag/rethink-priorities), [Happier Lives Institute](https://forum.effectivealtruism.org/tag/happier-lives-institute), [Future of Humanity Institute](https://forum.effectivealtruism.org/tag/future-of-humanity-institute), and the [Center on Long-Term Risk](https://forum.effectivealtruism.org/tag/center-on-long-term-risk).\n\nRelated entries\n---------------\n\n[cause prioritization](https://forum.effectivealtruism.org/tag/cause-prioritization) | [distribution of cost-effectiveness](https://forum.effectivealtruism.org/tag/distribution-of-cost-effectiveness) | [global priorities research](https://forum.effectivealtruism.org/tag/global-priorities-research) | [impact assessment](https://forum.effectivealtruism.org/tag/impact-assessment) | [ITN framework](https://forum.effectivealtruism.org/tag/itn-framework-1)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "tqPxveBdDvk8g6WLM",
    "name": "Network building",
    "core": false,
    "slug": "network-building",
    "oldSlugs": [
      "network-building"
    ],
    "postCount": 27,
    "description": {
      "markdown": "**Network building** refers to developing the EA network to include non-EA actors, organisations, and communities.^[\\[1\\]](#fnclunyha97c6)^\n\nExamples of network building efforts include: Effective Altruism Consulting Network, [High Impact Athletes](https://forum.effectivealtruism.org/tag/high-impact-athletes), [Raising for Effective Giving](https://forum.effectivealtruism.org/tag/raising-for-effective-giving), [All-Party Parliamentary Group for Future Generations](https://forum.effectivealtruism.org/tag/all-party-parliamentary-group-for-future-generations) (London), promoting positive values,^[\\[2\\]](#fn2z5k81uon3l)^^[\\[3\\]](#fnpz7udru8vkb)^ EA Asia, and more.\n\nRelated entries\n---------------\n\n[building effective altruism](https://forum.effectivealtruism.org/tag/building-effective-altruism-1) | [communities adjacent to effective altruism](https://forum.effectivealtruism.org/tag/communities-adjacent-to-effective-altruism) | [field building](https://forum.effectivealtruism.org/tag/field-building) | [value of movement growth](https://forum.effectivealtruism.org/tag/value-of-movement-growth)\n\n1.  ^**[^](#fnrefclunyha97c6)**^\n    \n    For more information, see Nash, David (2019) [Community vs network](https://forum.effectivealtruism.org/posts/S6QHRyi7joCWN9dkv/community-vs-network), *Effective Altruism Forum*, December 12.\n    \n2.  ^**[^](#fnref2z5k81uon3l)**^\n    \n    80,000 Hours (2016) [Broadly promoting positive values](https://80000hours.org/problem-profiles/#promoting-positive-values), in 'Our current list of especially pressing world problems', *80,000 Hours*.\n    \n3.  ^**[^](#fnrefpz7udru8vkb)**^\n    \n    For example, Lipsitz, Daniel (2020) [EA for non-EA people: “External movement-building”](https://www.youtube.com/watch?v=w0AiIMeyxWk), *YouTube*, June 20."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "yMu9phtzZspY8pyyE",
    "name": "Effective Altruism Policy Analytics",
    "core": false,
    "slug": "effective-altruism-policy-analytics",
    "oldSlugs": null,
    "postCount": 6,
    "description": {
      "markdown": "**Effective Altruism Policy Analytics** (**EAPA**) was a project that sought to improve regulatory action in the United States. EAPA ran between June and September 2015.\n\nExternal links\n--------------\n\n[Effective Altruism Policy Analytics](https://eapolicy.wordpress.com/). Official website.\n\nRelated entries\n---------------\n\n[policy](https://forum.effectivealtruism.org/topics/policy) | [United States](https://forum.effectivealtruism.org/topics/united-states)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "LSPwJqPfC4irmPR7c",
    "name": "Human Compatible",
    "core": false,
    "slug": "human-compatible",
    "oldSlugs": [
      "human-compatible-book",
      "human-compatible"
    ],
    "postCount": 7,
    "description": {
      "markdown": "***Human Compatible: Artificial Intelligence and the Problem of Control*** is a book by [Stuart Russell](https://forum.effectivealtruism.org/tag/stuart-russell). It was published in October 2019.\n\nExternal links\n--------------\n\n[Stuart Russell](http://people.eecs.berkeley.edu/~russell/). Official website.\n\nRelated entries\n---------------\n\n[AI alignment](https://forum.effectivealtruism.org/tag/ai-alignment) | [Center for Human-Compatible Artificial Intelligence](https://forum.effectivealtruism.org/tag/center-for-human-compatible-artificial-intelligence) | [Stuart Russell](https://forum.effectivealtruism.org/tag/stuart-russell)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "rncYBHYvfM2eEdqXY",
    "name": "Transhumanism",
    "core": false,
    "slug": "transhumanism",
    "oldSlugs": null,
    "postCount": 19,
    "description": {
      "markdown": "**Transhumanism** is a social and intellectual movement that advocates for the research and development of robust human-enhancement technologies, especially technologies with the potential to radically [extend life](https://forum.effectivealtruism.org/tag/aging-research), improve wellbeing, and [increase intelligence](https://forum.effectivealtruism.org/tag/cognitive-enhancement).\n\nFurther reading\n---------------\n\nMore, Max & Natasha Vita-More (eds.) (2013) [*The Transhumanist Reader: Classical and Contemporary Essays on the Science, Technology, and Philosophy of the Human Future*](https://doi.org/10.1002/9781118555927), Malden, Massachusetts: Wiley.\n\nNgo, Richard (2020) [EA reading list: futurism and transhumanism](https://forum.effectivealtruism.org/posts/qM8Y4qcyEZJBQtF2m/ea-reading-list-futurism-and-transhumanism), *Effective Altruism Forum*, August 4.\n\nRegis, Ed (1990) [*Great Mambo Chicken and the Transhuman Condition: Science Slightly over the Edge*](https://en.wikipedia.org/wiki/Special:BookSources/978-0-201-09258-5), Reading: Addison-Wesley.\n\nRelated entries\n---------------\n\n[aging research](https://forum.effectivealtruism.org/tag/aging-research) | [biotechnology](https://forum.effectivealtruism.org/topics/biotechnology) | [cryonics](https://forum.effectivealtruism.org/tag/cryonics) | [cognitive enhancement](https://forum.effectivealtruism.org/tag/cognitive-enhancement/) | [flourishing futures](https://forum.effectivealtruism.org/tag/flourishing-futures)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5pdNux8dFm2uBDpHE",
    "name": "Flourishing futures",
    "core": false,
    "slug": "flourishing-futures",
    "oldSlugs": null,
    "postCount": 26,
    "description": {
      "markdown": "**Flourishing futures**, **utopias**, **ideal futures**, or simply **(highly) positive futures** are expressions used to describe the extremely good forms that the [long-term future](https://forum.effectivealtruism.org/tag/long-term-future) could assume.\n\nIt could be important to consider what types of flourishing future are possible, how good each would be, how likely each is, and what would make these futures more or less likely. Reasons why this might be important include the following:\n\n*   A better understanding of how positive the future might be or is likely to be is relevant to the question of how much to prioritise reducing [existential risks](https://forum.effectivealtruism.org/tag/existential-risk).\n*   A better understanding of how good and likely various flourishing futures are, and what would make them more or less likely, could aid in generating, prioritising among, and implementing [longtermist](https://forum.effectivealtruism.org/tag/longtermism) interventions.\n*   Having clearer pictures of how the future might go extremely well could aid in building support for work to reduce existential risks.\n*   A better understanding of what futures should be steered towards might aid in working out which scenarios might constitute unrecoverable [dystopias](https://forum.effectivealtruism.org/tag/dystopia) or unrecoverable [collapses](https://forum.effectivealtruism.org/tag/civilizational-collapse-and-recovery) (i.e., existential catastrophes other than extinction).\n\nFurther reading\n---------------\n\nBostrom, Nick (2008) [Letter from utopia](http://doi.org/10.2202/1941-6008.1025), *Studies in Ethics, Law, and Technology*, vol. 2.\n\nCotton-Barratt, Owen & Toby Ord (2015) [Existential risk and existential hope: Definitions](https://www.fhi.ox.ac.uk/xrisk-xhope-dfns/), Technical Report #2015-1, Future of Humanity Institute, University of Oxford.\n\nLessWrong (2009) [Fun theory](https://www.lesswrong.com/tag/fun-theory), *LessWrong Wiki*, June 25.\n\nOrd, Toby (2020) [*The Precipice: Existential Risk and the Future of Humanity*](https://en.wikipedia.org/wiki/Special:BookSources/1526600218), chapter 8, London: Bloomsbury Publishing.\n\nPearce, David (1995) [*The Hedonistic Imperative*](https://www.hedweb.com/hedab.htm), BLTC Research (updated 2007).\n\nSandberg, Anders (2020) [Post scarcity civilizations & cognitive enhancement](https://www.youtube.com/watch?v=DZfh3JRlc44), *Foresight Institute*, September 4.\n\nWiblin, Robert & Keiran Harris (2018) [The world’s most intellectual foundation is hiring. Holden Karnofsky, founder of GiveWell, on how philanthropy can have maximum impact by taking big risks](https://80000hours.org/podcast/episodes/holden-karnofsky-open-philanthropy/), *80,000 Hours*, February 27.\n\nRelated entries\n---------------\n\n[dystopia](https://forum.effectivealtruism.org/tag/dystopia) | [existential security](https://forum.effectivealtruism.org/tag/existential-security) | [Future of Humanity Institute](https://forum.effectivealtruism.org/topics/future-of-humanity-institute) | [Future of Life Institute](https://forum.effectivealtruism.org/topics/future-of-life-institute) | [hedonium](https://forum.effectivealtruism.org/tag/hedonium) | [hellish existential catastrophe](https://forum.effectivealtruism.org/tag/hellish-existential-catastrophe) | [Invincible Wellbeing](https://forum.effectivealtruism.org/topics/invincible-wellbeing) | [long reflection](https://forum.effectivealtruism.org/tag/long-reflection) | [long-term future](https://forum.effectivealtruism.org/tag/long-term-future) | [longtermism](https://forum.effectivealtruism.org/tag/longtermism) | [motivational](https://forum.effectivealtruism.org/tag/motivational) | [transhumanism](https://forum.effectivealtruism.org/tag/transhumanism) | [welfare biology](https://forum.effectivealtruism.org/topics/welfare-biology)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "8WjRtbxai7LdemwX8",
    "name": "Gene drives",
    "core": false,
    "slug": "gene-drives",
    "oldSlugs": [
      "genetics"
    ],
    "postCount": 17,
    "description": {
      "markdown": "**Gene drive** refers to the process by which a genetic element biases its own inheritance, and the genetic element itself. \n\nMany natural gene drives have been identified that operate through a range of mechanisms. The CRISPR nucleases have accelerated the development of synthetic gene drives by making it easier to genetically engineer non-model species and a CRISPR nuclease is frequently used as a core component of the synthetic inheritance biasing mechanism. \n\nGene drives can through higher-than-normal rates of inheritance spread a genetic change through a population. This can occur even if the modification has a fitness disadvantage for those carrying it. \n\nHoming endonuclease gene drives (HEGs) are likely the furthest developed form of synthetic gene drives.  In diploids, one chromosome is contributed by the father and one by the mother. HEGs operate by copying themselves from one homologous chromosome to the other (switching the cell from being heterozygotes to homozygous from the HEG). Thus, while a mutation spread through Mendelian inheritance can propagate to only 50% of descendants with each generation, a modification spread through gene drives (or \"super-Mendelian\" inheritance) is passed on to virtually all descendants within a single generation.^[\\[1\\]](#fnryshlf2elzo)^\n\nApplications to malaria control\n-------------------------------\n\nTwo applications of gene drives to the control of malaria have been proposed. One is to modify the relevant mosquito species to make it incapable of carrying the malaria parasite. The other is to significantly reduce the population of those mosquito species.^[\\[2\\]](#fnbkgvppzdhgd)^ Once the gene drives are released, the relevant mutation could be propagated through the entire population of interest in a period of just a few years. In 2016, a group of researchers at Imperial College and other universities genetically engineered the *Anopheles gambiae* mosquito—the primary mosquito species that spreads the malaria parasite—rendering it capable of passing the genetic modification to over 99% of offspring.^[\\[1\\]](#fn0s0rducam3sr)^\n\n[Target Malaria](https://forum.effectivealtruism.org/tag/target-malaria) is one organization currently working on these applications.\n\nOther applications\n------------------\n\nGene drives have also been suggested for reducing [wild-animal suffering](https://forum.effectivealtruism.org/tag/wild-animal-welfare). For example, [transhumanist](https://forum.effectivealtruism.org/tag/transhumanism) philosopher [David Pearce](https://forum.effectivealtruism.org/tag/david-pearce-1) proposes CRISPR-based gene drives for promoting low-[pain](https://forum.effectivealtruism.org/tag/pain-and-suffering) alleles in sexually reproducing wild animals.^[\\[2\\]](#fnwf74rnxbhnb)^ The leader of the MIT Media Lab’s Sculpting Evolution group Kevin M. Esvelt also has written favorably about engineered gene drives for reducing wild-animal suffering.^[\\[3\\]](#fnhgufntqxqlu)^\n\nFurther reading\n---------------\n\nWarmbrod, Kelsey Lane *et al.* (2020) [Gene drives: pursuing opportunities, minimizing risk](https://www.centerforhealthsecurity.org/our-work/publications/gene-drives-pursuing-opportunities-minimizing-risk), The Johns Hopkins Center for Health Security.\n\nExternal links\n--------------\n\n[Sculpting Evolution](http://www.sculptingevolution.org/). Many additional resources on this topic.\n\n1.  ^**[^](#fnref0s0rducam3sr)**^\n    \n    Hammond, Andrew et al. (2016) [A CRISPR-Cas9 gene drive system targeting female reproduction in the malaria mosquito vector Anopheles gambiae](http://doi.org/10.1038/nbt.3439), *Nature Biotechnology*, vol. 34, pp. 78–83.\n    \n2.  ^**[^](#fnrefwf74rnxbhnb)**^\n    \n    Pearce, David (2016) [Compassionate biology: How CRISPR-based “gene drives” could cheaply, rapidly and sustainably reduce suffering throughout the living world](https://www.hedweb.com/gene-drives/index.html), *BLTC Research* (updated 2021).\n    \n3.  ^**[^](#fnrefhgufntqxqlu)**^\n    \n    Esvelt, Kevin (2019) [When are we obligated to edit wild creatures?](https://leaps.org/when-are-we-obligated-to-edit-wild-creatures/), *Leaps.Org*, August 30."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "uPfmQH7oxkDHgpG8e",
    "name": "Legal Priorities Project",
    "core": false,
    "slug": "legal-priorities-project",
    "oldSlugs": null,
    "postCount": 8,
    "description": {
      "markdown": "The **Legal Priorities Project** is an organization that conducts and supports high-impact legal research.\n\nHistory\n-------\n\nThe Legal Priorities Project was founded in 2020. The idea for the project originated in an effective altruism group at Harvard Law School two years earlier.\n\nFunding\n-------\n\nAs of July 2022, the Legal Priorities Project has received nearly 1.2 million in funding from the [Future Fund](https://forum.effectivealtruism.org/topics/future-fund),^[\\[1\\]](#fn6dgnmjnlfze)^ over $260,000 from the [Survival and Flourishing](https://forum.effectivealtruism.org/topics/survival-and-flourishing) Fund, ^[\\[2\\]](#fnlvorw66ppx8)^ and over $130,000 from [Effective Altruism Funds](https://forum.effectivealtruism.org/topics/effective-altruism-funds).^[\\[3\\]](#fnbaee7a3apa)^\n\nFurther reading\n---------------\n\nRighetti, Luca & Fin Moorhouse (2021) [Christoph Winter on the Legal Priorities Project](https://hearthisidea.com/episodes/christoph), *Hear This Idea*, October 18.\n\nSchuett, Jonas (2020) [Introducing the Legal Priorities Project](https://forum.effectivealtruism.org/posts/PvBLDPkqKvdHQkKPn/introducing-the-legal-priorities-project), *Effective Altruism Forum*, August 30.\n\nWinter, Christoph *et al.* (2021) [Legal priorities research: a research agenda](https://www.legalpriorities.org/research_agenda.pdf), *Legal Priorities Project*, January.\n\nExternal links\n--------------\n\n[Legal Priorities Project](https://www.legalpriorities.org/). Official website.\n\n[Apply for a job](https://www.legalpriorities.org/open-positions.html).\n\nRelated entries\n---------------\n\n[global priorities research](https://forum.effectivealtruism.org/tag/global-priorities-research) | [law](https://forum.effectivealtruism.org/tag/law)\n\n1.  ^**[^](#fnref6dgnmjnlfze)**^\n    \n    Future Fund (2022) [Our grants and investments: Legal Priorities Project](https://ftxfuturefund.org/all-grants/?_organization_name=legal-priorities-project), *Future Fund*.\n    \n2.  ^**[^](#fnreflvorw66ppx8)**^\n    \n    Survival and Flourishing Fund (2020) [SFF-2021-H1 S-process recommendations announcement](https://survivalandflourishing.fund/sff-2021-h1-recommendations), *Survival and Flourishing Fund*.\n    \n3.  ^**[^](#fnrefbaee7a3apa)**^\n    \n    Long-Term Future Fund (2021) [May 2021: Long-Term Future Fund grants](https://funds.effectivealtruism.org/funds/payouts/may-2021-long-term-future-fund-grants), *Effective Altruism Funds*, May."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "jqCNhCb3cgaWPfhq8",
    "name": "SoGive",
    "core": false,
    "slug": "sogive",
    "oldSlugs": null,
    "postCount": 27,
    "description": {
      "markdown": "**SoGive** is a nonprofit [charity evaluator](https://forum.effectivealtruism.org/topics/charity-evaluation). They keep an online list of charities that have been analyzed and rated, allowing donors to compare the estimated impact of each one.\n\nFurther reading\n---------------\n\nRighetti, Luca & Fin Moorhouse (2020) [Sanjay Joshi on charity evaluation and nonprofit entrepreneurship](https://hearthisidea.com/episodes/sanjay), *Hear This Idea*, May 20.\n\nExternal links\n--------------\n\n[SoGive](https://sogive.org/). Official website.\n\n[Apply for a job](https://sogive.org/#careers)."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "3rvqEHfsAeLJqroAw",
    "name": "All-Party Parliamentary Group for Future Generations",
    "core": false,
    "slug": "all-party-parliamentary-group-for-future-generations",
    "oldSlugs": [
      "all-party-parliamentary-group-for-future-generations",
      "appg-on-future-generations"
    ],
    "postCount": 8,
    "description": {
      "markdown": "The **All-Party Parliamentary Group for Future Generations** (also known as the **APPG on Future Generations** or **APPGFG**) is an all-party parliamentary group in the United Kingdom which aims to increase and support the consideration of long-term issues in policy-making.\n\nFunding\n-------\n\nAs of June 2022, APPGFG has received over $250,000 in funding from the [Survival and Flourishing](https://forum.effectivealtruism.org/topics/survival-and-flourishing) Fund,^[\\[1\\]](#fnu4gwhk7i82r)^^[\\[2\\]](#fn6s7305otpts)^ and over $60,000 from [Effective Altruism Funds](https://forum.effectivealtruism.org/topics/effective-altruism-funds).^[\\[3\\]](#fn8hj9q1i52uj)^\n\nFurther reading\n---------------\n\nHilton, Sam (2021) [A little bit of funding goes a long way: the APPG for Future Generations](https://en.wikipedia.org/wiki/Special:BookSources/978-0-9957281-8-9), in Natalie Cargill & Tyler John (eds.) *The Long View: Essays on Policy, Philanthropy, and the Long-Term Future*, London: First, pp. 84–92.\n\nRighetti, Luca & Fin Moorhouse (2022) [Lord Bird on the Wellbeing of Future Generations Bill](https://hearthisidea.com/episodes/bird), *Hear This Idea*, April 5.\n\nExternal links\n--------------\n\n[All-Party Parliamentary Group for Future Generations](https://www.appgfuturegenerations.com/). Official website.\n\nRelated entries\n---------------\n\n[electoral reform](https://forum.effectivealtruism.org/tag/electoral-reform) | [improving institutional decision-making](https://forum.effectivealtruism.org/topics/improving-institutional-decision-making) | [longtermism](https://forum.effectivealtruism.org/tag/longtermism) | [longtermist institutional reform](https://forum.effectivealtruism.org/topics/longtermist-institutional-reform) | [policy change](https://forum.effectivealtruism.org/tag/policy-change)\n\n1.  ^**[^](#fnrefu4gwhk7i82r)**^\n    \n    Survival and Flourishing Fund (2018) [SFF-2019-Q4 S-process recommendations announcement](https://survivalandflourishing.fund/sff-2019-q4-recommendations), *Survival and Flourishing Fund*.\n    \n2.  ^**[^](#fnref6s7305otpts)**^\n    \n    Survival and Flourishing Fund (2020) [SFF-2021-H1 S-process recommendations announcement](https://survivalandflourishing.fund/sff-2021-h1-recommendations), *Survival and Flourishing Fund*.\n    \n3.  ^**[^](#fnref8hj9q1i52uj)**^\n    \n    Long-Term Future Fund (2019) [November 2019: Long-Term Future Fund grants](https://funds.effectivealtruism.org/funds/payouts/november-2019-long-term-future-fund-grants), *Effective Altruism Funds*, November."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "FApCfgvWARAZR7rxm",
    "name": "Survival and Flourishing",
    "core": false,
    "slug": "survival-and-flourishing",
    "oldSlugs": [
      "survival-and-flourishing",
      "survival-and-flourishing-fund"
    ],
    "postCount": 9,
    "description": {
      "markdown": " **Survival and Flourishing Fund** (**SFF**) and **Survival and Flourishing Projects** (**SFP**) are two related entities that facilitate grants to projects [aiming to improve the long-term future](https://forum.effectivealtruism.org/tag/longtermism).\n\nSFF and SAP share a common mission and leadership, and work closely with each other, but are not formally related and have different functions. SFF generally makes grants to existing charities, while SAP makes smaller grants to individuals.\n\nExternal links\n--------------\n\n[Survival and Flourishing Projects](http://survivalandflourishing.org/). Official website.\n\n[Survival and Flourishing Fund](http://survivalandflourishing.fund/). Official website.\n\n[Apply for funding](https://docs.google.com/forms/d/e/1FAIpQLSc4PRBIP8CNCviFiNkSSMKy6Ms3sUJBOi_ykE8IDvTeB5Nr_A/viewform).\n\nRelated entries\n---------------\n\n[funding opportunities](https://forum.effectivealtruism.org/tag/funding-opportunity)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "bbRb4H4S6BhK84x24",
    "name": "Axiology",
    "core": false,
    "slug": "axiology",
    "oldSlugs": null,
    "postCount": 22,
    "description": {
      "markdown": "**Axiology**, also known as **theory of the good** and **value theory** (in a narrow sense of that term), is a branch of [normative ethics](https://forum.effectivealtruism.org/tag/normative-ethics) concerned with what kind of things and outcomes are morally good, or [intrinsically valuable](https://forum.effectivealtruism.org/tag/intrinsic-value-and-instrumental-value). This is in contrast to the theory of the right, which is concerned with what people morally ought to do.\n\nThe theory of the good is typically aggregative—concerning some quantity which it is better to increase or decrease. However, in some cases people might also include principles (e.g. “an outcome is better the fewer rights violations it contains”).\n\nThere are many possible theories of value. Some hold that there is just one source of value, while others rely on multiple sources. Almost all theories agree that [wellbeing](https://forum.effectivealtruism.org/tag/wellbeing) (also known as “welfare”) has value, and some theories, known as [*welfarist*](https://forum.effectivealtruism.org/tag/welfarism) theories, hold that that it is the *only* thing which has intrinsic value. Non-welfarist theories recognize other [sources of value](https://forum.effectivealtruism.org/tag/non-wellbeing-sources-of-value), such as fairness, equality, or beauty.\n\nFurther reading\n---------------\n\nGreaves, Hilary (2017) [Population axiology](http://doi.org/10.1111/phc3.12442), *Philosophy Compass*, vol. 12, pp. 1–15.\n\nGreaves, Hilary & Toby Ord (2017) [Moral uncertainty about population axiology](http://doi.org/10.26556/jesp.v12i2.223), *Journal of Ethics and Social Philosophy*, vol. 12, pp. 135–167.\n\nSchroeder, Mark (2008) [Value theory](https://plato.stanford.edu/entries/value-theory/), *Stanford Encyclopedia of Philosophy*, February 5 (updated 4 March 2021).\n\nRelated entries\n---------------\n\n[consciousness research](https://forum.effectivealtruism.org/tag/consciousness-research) | [hedonism](https://forum.effectivealtruism.org/tag/hedonism) | [infinite ethics](https://forum.effectivealtruism.org/tag/infinite-ethics) | [intrinsic vs. instrumental value](https://forum.effectivealtruism.org/tag/intrinsic-vs-instrumental-value) | [moral patienthood](https://forum.effectivealtruism.org/tag/moral-patienthood) | [moral weight](https://forum.effectivealtruism.org/tag/moral-weight) | [normative ethics](https://forum.effectivealtruism.org/tag/normative-ethics) | [population ethics](https://forum.effectivealtruism.org/tag/population-ethics) | [suffering-focused ethics](https://forum.effectivealtruism.org/tag/suffering-focused-ethics) | [utilitarianism](https://forum.effectivealtruism.org/tag/utilitarianism) | [valence](https://forum.effectivealtruism.org/tag/valence) | [welfarism](https://forum.effectivealtruism.org/tag/welfarism)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "Gtv3Reg6kzMZsh5Gs",
    "name": "Electoral politics",
    "core": false,
    "slug": "electoral-politics",
    "oldSlugs": [
      "electoral-politics"
    ],
    "postCount": 52,
    "description": {
      "markdown": "The **electoral politics** tag is used to discuss elections, how to make voting decisions, or the general impact of participating in electoral politics (e.g. how impactful it is to vote or volunteer during elections). For content specific to the USA, please use [United States](https://forum.effectivealtruism.org/topics/united-states) instead.\n\nThis is distinct from [electoral reform](https://forum.effectivealtruism.org/tag/electoral-reform), which is about how to change the system of electoral politics, and from [ballot initiative](https://forum.effectivealtruism.org/tag/ballot-initiative), which is about voting directly on legislation or referenda.\n\nRelated entries\n---------------\n\n[ballot initiative](https://forum.effectivealtruism.org/tag/ballot-initiative) | [Democracy Defense Fund](https://forum.effectivealtruism.org/tag/democracy-defense-fund) | [electoral reform](https://forum.effectivealtruism.org/tag/electoral-reform) | [policy](https://forum.effectivealtruism.org/topics/policy) | [United States](https://forum.effectivealtruism.org/topics/united-states)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "7AJ94kru2pTbsQzkz",
    "name": "Microfinance",
    "core": false,
    "slug": "microfinance",
    "oldSlugs": null,
    "postCount": 1,
    "description": {
      "markdown": "**Microfinance** is a category of financial services—such as credit, insurance, payment systems, and savings and checking accounts—provided to low-income individuals or groups who are typically excluded from traditional banking.\n\nFurther reading\n---------------\n\nWykstra, Stephanie (2019) [Microcredit was a hugely hyped solution to global poverty. What happened?](https://www.vox.com/future-perfect/2019/1/15/18182167/microcredit-microfinance-poverty-grameen-bank-yunus), *Vox*, January 15.\n\nRelated entries\n---------------\n\n[cash transfers](https://forum.effectivealtruism.org/tag/cash-transfers)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "imoDutHs2nFJ8aSmX",
    "name": "Postmortems & retrospectives",
    "core": false,
    "slug": "postmortems-and-retrospectives",
    "oldSlugs": [
      "project-writeup",
      "project-writeup",
      "postmortems-and-restrospectives",
      "postmortems-and-restrospectives"
    ],
    "postCount": 43,
    "description": {
      "markdown": "A **postmortem** or **retrospective** is a reflection on past actions with an eye to what went well, what didn't, and the cause of any failures. These retrospectives may include meta-commentary such as lessons learned, data gathered, and whether (and how) similar projects should be run in the future. Sharing such accounts publicly is prosocial and allows others to learn from one's experience too.\n\nRelated entries\n---------------\n\n[donation writeup](https://forum.effectivealtruism.org/tag/donation-writeup) | [community projects](https://forum.effectivealtruism.org/tag/community-projects) | [community experiences](https://forum.effectivealtruism.org/tag/community-experiences) | [data (EA community)](https://forum.effectivealtruism.org/tag/data-ea-community-1)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "PfcTK4c9oynmQb4Ri",
    "name": "Leverage Research",
    "core": false,
    "slug": "leverage-research",
    "oldSlugs": null,
    "postCount": 13,
    "description": {
      "markdown": "**Leverage Research** is a nonprofit research institute that conducts research in early stage science.\n\nFunding\n-------\n\nAs of June 2022, Leverage Research has received $80,000 in funding from the [Survival and Flourishing](https://forum.effectivealtruism.org/topics/survival-and-flourishing) Fund.^[\\[1\\]](#fnnk36pwttg3p)^\n\nFurther reading\n---------------\n\nBayAreaHuman (2021) [Common knowledge facts about Leverage Research 1.0](https://www.lesswrong.com/posts/Kz9zMgWB5C27Pmdkh/common-knowledge-facts-about-leverage-research-1-0-1), *LessWrong*, September 24.\n\nRowe, Larissa (2021) [Updates from Leverage Research: history and recent progress](https://www.lesswrong.com/posts/3GgoJ2nCj8PiD4FSi/updates-from-leverage-research-history-and-recent-progress), *LessWrong*, September 27.\n\nExternal links\n--------------\n\n[Leverage Research](https://www.leverageresearch.org/). Official website.\n\n[Apply for a job](https://www.leverageresearch.org/hiring).\n\n1.  ^**[^](#fnrefnk36pwttg3p)**^\n    \n    Survival and Flourishing Fund (2019) [SFF-2020-H1 S-process recommendations announcement](https://survivalandflourishing.fund/sff-2020-h1-recommendations), *Survival and Flourishing Fund*."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "MD9hJR8LxyRHXTt6K",
    "name": "Magnify Mentoring",
    "core": false,
    "slug": "magnify-mentoring",
    "oldSlugs": [
      "wanbam"
    ],
    "postCount": 18,
    "description": {
      "markdown": "**Magnify Mentoring** (formerly **WANBAM**) is a nonprofit that aims at developing and supporting a global network of people interested in having a positive impact with their careers and their lives. Founded in 2019, it was initially focused on women, non binary and trans people. In the course of two years they have facilitated the mentoring of over 400 people and their community extended to thirty-five countries, with members including founders of organizations, leaders in animal-product alternatives, and researchers.\n\nFunding\n-------\n\nAs of July 2022, Magnify Mentoring has received nearly $127,000 in grants from [Effective Altruism Funds](https://forum.effectivealtruism.org/topics/effective-altruism-funds).^[\\[1\\]](#fn87pk4r40224)^^[\\[2\\]](#fnce9432p7dm)^^[\\[3\\]](#fntdd6cxo9kjk)^\n\nExternal links\n--------------\n\n[Magnify Mentoring](https://www.magnifymentoring.org/). Official website.\n\n1.  ^**[^](#fnref87pk4r40224)**^\n    \n    Effective Altruism Infrastructure Fund (2020) [March 2020: EA Meta Fund Grants](https://funds.effectivealtruism.org/funds/payouts/march-2020-ea-meta-fund-grants), *Effective Altruism Funds*, March. \n    \n2.  ^**[^](#fnrefce9432p7dm)**^\n    \n    Effective Altruism Infrastructure Fund (2020) [July 2020: EA Meta Fund grants](https://funds.effectivealtruism.org/funds/payouts/july-2020-ea-meta-fund-grants), *Effective Altruism Funds*, July.\n    \n3.  ^**[^](#fnreftdd6cxo9kjk)**^\n    \n    Animal Welfare Fund (2021) [May 2021: Animal Welfare Fund grants](https://funds.effectivealtruism.org/funds/payouts/july-2021-animal-welfare-fund-grants), *Effective Altruism Funds*, May."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "uXmu66GgDqJxjZNBc",
    "name": "Speeding up development",
    "core": false,
    "slug": "speeding-up-development",
    "oldSlugs": null,
    "postCount": 26,
    "description": {
      "markdown": "Some actions might **speed up development**, or might be aimed at doing so. For example, actions might speed up [economic growth](https://forum.effectivealtruism.org/tag/economic-growth), [scientific](https://forum.effectivealtruism.org/tag/scientific-progress) and technological progress, or expected future changes in values, laws, or political systems. This can also be referred to as **speeding up progress**, although that may be problematic in implying that the developments are necessarily good things that should be advanced.\n\nBeckstead describes speeding up development as one of three main types of benefits from attempts to improve the world, with the other two being [\"trajectory changes\"](https://forum.effectivealtruism.org/tag/trajectory-changes) (including [existential risk](https://forum.effectivealtruism.org/tag/existential-risk) reduction) and \"proximate benefits\" (meaning \"the fairly short-run, fairly predictable benefits that we ordinarily think about when we cure some child's blindness, save a life, or help an old lady cross the street\").^[\\[1\\]](#fn5qjkbd1guo5)^\n\nIt is possible that speeding up development in various ways would make trajectory changes more or less likely, and that this would be the most significant effect of speeding up development in those ways. For example, faster economic growth might decrease or increase existential risk, with this benefit or harm outweighing the other effects of that growth.\n\nSome people have argued that speeding up development is *in itself* the best way to improve the [long-term future](https://forum.effectivealtruism.org/tag/long-term-future). One argument that could be made for this position is that every delay to development causes [astronomical waste](https://forum.effectivealtruism.org/tag/astronomical-waste). However, others have argued that we should instead focus on trajectory changes because roughly \"where we end up\" matters more than \"how fast we get there\".^[\\[2\\]](#fnig7xau4ojgo)^^[\\[3\\]](#fnirzkdpfrd8p)^\n\nRelated entries\n---------------\n\n[differential progress](https://forum.effectivealtruism.org/tag/differential-progress) | [economic growth](https://forum.effectivealtruism.org/tag/economic-growth) | [moral advocacy](https://forum.effectivealtruism.org/tag/moral-advocacy) | [scientific progress](https://forum.effectivealtruism.org/tag/scientific-progress)\n\n1.  ^**[^](#fnref5qjkbd1guo5)**^\n    \n    Beckstead, Nick (2013) [A proposed adjustment to the astronomical waste argument](https://forum.effectivealtruism.org/posts/RXpJbWKDJ7WFWqEin/a-proposed-adjustment-to-the-astronomical-waste-argument), *Effective Altruism Forum*, May 27.\n    \n2.  ^**[^](#fnrefig7xau4ojgo)**^\n    \n    Todd, Benjamin (2017) [The case for reducing existential risks](https://80000hours.org/articles/existential-risks/#who-shouldnt-prioritise-safeguarding-the-future), *80,000 Hours*, October.\n    \n3.  ^**[^](#fnrefirzkdpfrd8p)**^\n    \n    See also Bostrom, Nick (2003) [Astronomical waste: the opportunity cost of delayed technological development](http://doi.org/10.1017/S0953820800004076), *Utilitas*, vol. 15, pp. 308–314."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "34ZAwrvmhcFAo9wan",
    "name": "Animal Ask",
    "core": false,
    "slug": "animal-ask",
    "oldSlugs": null,
    "postCount": 9,
    "description": {
      "markdown": "**Animal Ask** is a nonprofit that helps animal advocacy organizations deal with requests from industry or governments. It was incubated by [Charity Entrepreneurship](https://forum.effectivealtruism.org/tag/charity-entrepreneurship).\n\nFunding\n-------\n\nAs of July 2022, Animal Ask has received $130,000 in funding from [Open Philanthropy](https://forum.effectivealtruism.org/topics/open-philanthropy).^[\\[1\\]](#fnewoilu7ojzq)^\n\nFurther reading\n---------------\n\nAnimal Ask (2020) [Introducing Animal Ask](https://forum.effectivealtruism.org/posts/rq6rh7BfzGzKkb7MM/introducing-animal-ask), *Effective Altruism Forum*, November 9.\n\nExternal links\n--------------\n\n[Animal Ask](https://www.animalask.org/). Official website.\n\n[Apply for a job](https://www.animalask.org/jobs).\n\nRelated entries\n---------------\n\n[effective animal advocacy](https://forum.effectivealtruism.org/tag/effective-animal-advocacy)\n\n1.  ^**[^](#fnrefewoilu7ojzq)**^\n    \n     Open Philanthropy (2022) [Grants database: Animal Ask](https://www.openphilanthropy.org/grants/?q=&organization-name=animal-ask), *Open Philanthropy*."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "EtCjijubiHeNAbBbp",
    "name": "Alignment Newsletter",
    "core": false,
    "slug": "alignment-newsletter",
    "oldSlugs": null,
    "postCount": 1,
    "description": {
      "markdown": "The **Alignment Newsletter** provides weekly summaries and opinions on new research relevant to [AI alignment](https://forum.effectivealtruism.org/tag/ai-alignment). It was created in 2018 by Rohin Shah (then a PhD student at Berkeley, currently a researcher at [DeepMind](https://forum.effectivealtruism.org/tag/deepmind)) who continues to run it and writes most of the content.^[\\[1\\]](#fn7a6uaiohgog)^ As of May 2022, the newsletter has over 2600 subscribers.\n\nFurther reading\n---------------\n\nShah, Rohin (2021) [Alignment newsletter three year retrospective](https://www.alignmentforum.org/posts/L7yHdqRiHKd3FhQ7B/alignment-newsletter-three-year-retrospective), *AI Alignment Forum*, April 7.\n\nExternal links\n--------------\n\n[Alignment Newsletter](http://rohinshah.com/alignment-newsletter/). Official website.\n\nRelated entries\n---------------\n\n[AI Alignment Forum](https://forum.effectivealtruism.org/tag/ai-alignment-forum) | [Effective Altruism Newsletter](https://forum.effectivealtruism.org/tag/effective-altruism-newsletter) | [Forecasting Newsletter](https://forum.effectivealtruism.org/tag/forecasting-newsletter) | [Future Matters](https://forum.effectivealtruism.org/tag/future-matters) | [newsletters](https://forum.effectivealtruism.org/tag/newsletters)\n\n1.  ^**[^](#fnref7a6uaiohgog)**^\n    \n    Shah, Rohin (2019) [Alignment newsletter one year retrospective](https://forum.effectivealtruism.org/posts/Prxqvhr9JFj7JyJRX/alignment-newsletter-one-year-retrospective), *Effective Altruism Forum*, April 10."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "LfiaLyzM4e7vfQgZR",
    "name": "Cluster headache",
    "core": false,
    "slug": "cluster-headache",
    "oldSlugs": [
      "cluster-headaches"
    ],
    "postCount": 5,
    "description": {
      "markdown": "A **cluster headache** is a neurological disorder characterized by recurrent severe headaches on one side of the head. It has been described as \"probably one of the most painful conditions known to mankind\".^[\\[1\\]](#fnfn25kpe59du)^\n\nFurther reading\n---------------\n\nSharma, Sid, Clare Donaldson & Michael Plant (2020) [Problem area report: pain](https://web.archive.org/web/20201130121652/https://www.happierlivesinstitute.org/uploads/1/0/9/9/109970865/problem-area-pain.pdf), *Happier Lives Institute*, November, pp. 26–28.\n\nRelated entries\n---------------\n\n[pain and suffering](https://forum.effectivealtruism.org/tag/pain-and-suffering)\n\n1.  ^**[^](#fnreffn25kpe59du)**^\n    \n    Matharu, Manjit S. & Peter J. Goadsby (2004) [Cluster headache: focus on emerging therapies](https://doi.org/10.1586/14737175.4.5.895), *Expert Review of Neurotherapeutics*, vol. 4, pp. 895–907."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "KGgLo5i7YCrtfnaBR",
    "name": "Computational power of the human brain",
    "core": false,
    "slug": "computational-power-of-the-human-brain",
    "oldSlugs": null,
    "postCount": 3,
    "description": {
      "markdown": "The **computational power of the human brain** is the number of operations per unit time performed by the adult brain of a typical human.\n\nFurther reading\n---------------\n\nAguirre, Anthony (2019) [How many FLOPS for human-level AGI?](https://www.metaculus.com/questions/2646/what-will-the-necessary-computational-power-to-replicate-human-mental-capability-turn-out-to-be/), *Metaculus*, March 13.\n\nSandberg, Anders & Nick Bostrom (2008) [*Whole Brain Emulation: A Roadmap*](https://www.fhi.ox.ac.uk/brain-emulation-roadmap-report.pdf), Oxford: Future of Humanity Institute.\n\nCarlsmith, Joseph (2020a) [How much computational power does it take to match the human brain?](https://www.openphilanthropy.org/brain-computation-report), *Open Philanthropy*, September 11.\n\nCarlsmith, Joseph (2020b) [Discussions with Dr. Paul Christiano, Fall 2019-Spring 2020](https://www.openphilanthropy.org/files/Conversations/Discussions%20with%20Dr.%20Paul%20Christiano.pdf), *Open Philanthropy*.\n\nRelated entries\n---------------\n\n[AI forecasting](https://forum.effectivealtruism.org/tag/ai-forecasting) | [whole brain emulation](https://forum.effectivealtruism.org/tag/whole-brain-emulation)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "nC9tCPZhdCGTpbKco",
    "name": "Animal cognition",
    "core": false,
    "slug": "animal-cognition",
    "oldSlugs": null,
    "postCount": 3,
    "description": {
      "markdown": "**Animal cognition** is the study of the cognitive capacities of nonhuman animals.\n\nFurther reading\n---------------\n\nAndrews, Kristin & Susana Monsó (2021) [Animal cognition](https://plato.stanford.edu/archives/spr2021/entries/cognition-animal/), *The Stanford Encyclopedia of Philosophy*, March 8.\n\nShettleworth, Sara J. (2010) [*Cognition, Evolution, and Behavior*](https://en.wikipedia.org/wiki/Special:BookSources/978-0-19-531984-2), 2nd ed., Oxford: Oxford University Press.\n\nRelated entries\n---------------\n\n[animal sentience](https://forum.effectivealtruism.org/tag/animal-sentience)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "XAczFPPbefQLfBt67",
    "name": "Psychology",
    "core": false,
    "slug": "psychology",
    "oldSlugs": [
      "psychology-research"
    ],
    "postCount": 54,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "y28dsovumn9LQAtfa",
    "name": "Psychedelics",
    "core": false,
    "slug": "psychedelics",
    "oldSlugs": null,
    "postCount": 27,
    "description": {
      "markdown": " "
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "2ENWEuetHT6ZZzHap",
    "name": "Risk aversion",
    "core": false,
    "slug": "risk-aversion",
    "oldSlugs": null,
    "postCount": 25,
    "description": {
      "markdown": "**Risk aversion** is the preference for less risky payoffs over riskier payoffs with the same [expected value](https://forum.effectivealtruism.org/topics/expected-value).\n\nAn option is more risky if the value of its possible outcomes is more widely dispersed (higher variance). An agent is **risk averse** in a pure sense if they prefer safe options over risky ones, even when the riskier options (gambles) would give more of what they value [in expectation](https://forum.effectivealtruism.org/tag/expected-value). (Conversely, an agent is **risk neutral** if they are indifferent between options with the same expected payoffs, and they are **risk seeking** if they prefer gambles to equal-expected-payoff safe options.)\n\nAll payoffs in this definition of pure risk aversion are expressed in terms of what the agent values. As a consequence, risk aversion is on this definition quite distinct from the most common notion of risk aversion in economics, whereby diminishing marginal utility of money causes people to prefer low-variance, lower-expected-value monetary tradeoffs. Risk aversion is also related to, but distinct from, ambiguity aversion.^[\\[1\\]](#fnkjx1dclj4e)^\n\nThis form of pure risk aversion appears to be irrational under a variety of assumptions, as mentioned in [expected value theory](https://forum.effectivealtruism.org/tag/expected-value). Indeed, risk averse agents in this sense can be exploited in ways that seem to count against risk aversion.^[\\[2\\]](#fn1s89xp44a1b)^ However, some have defended it as rational.^[\\[3\\]](#fnubi7uruz3hb)^ If risk aversion is rational, some form of risk-averse decision theory might be appropriate.\n\nHowever, it seems that altruists should be close to risk-neutral in the economic sense. Though there may be some [diminishing returns](https://forum.effectivealtruism.org/tag/diminishing-returns) to altruistic effort, the returns diminish much more slowly than e.g. the marginal personal utility of money does. This means that the reasons for economic risk-aversion that apply in standard economic theory apply less strongly for altruists.\n\nRisk aversion is important to effective altruism because it informs how rational and altruistic people should make their decisions. Since it seems that altruists should be close to risk-neutral in economic terms, then unless we should use a decision theory that is risk neutral in the pure sense advocated for by Buchak, it may be best for altruists to be approximately risk-neutral overall. Risk-neutrality leads to some interesting and surprising conclusions, such as prioritizing risky careers,^[\\[4\\]](#fnp0ggxxyyzsq)^ taking unusual financial risks to increase an expected donation,^[\\[5\\]](#fnhurfbenp7ia)^ or donating to only one charity (although [there are also countervailing considerations](https://forum.effectivealtruism.org/tag/philanthropic-diversification)).^[\\[6\\]](#fns6klzedb3qf)^\n\nRelated entries\n---------------\n\n[alternatives to expected value theory](https://forum.effectivealtruism.org/tag/alternatives-to-expected-value-theory) | [altruistic wager](https://forum.effectivealtruism.org/tag/altruistic-wager) | [decision theory](https://forum.effectivealtruism.org/tag/decision-theory) | [expected value](https://forum.effectivealtruism.org/tag/expected-value) | [fanaticism](https://forum.effectivealtruism.org/tag/fanaticism) | [neutrality](https://forum.effectivealtruism.org/topics/neutrality) | [philanthropic diversification](https://forum.effectivealtruism.org/tag/philanthropic-diversification)\n\n1.  ^**[^](#fnrefkjx1dclj4e)**^\n    \n    Wikipedia (2006) [Ambiguity aversion](https://en.wikipedia.org/wiki/Ambiguity_aversion), *Wikipedia*, April 14 (updated 1 March 2021‎).\n    \n2.  ^**[^](#fnref1s89xp44a1b)**^\n    \n    Yudkowsky, Eliezer (2008) [The Allais paradox](https://www.lesswrong.com/posts/zJZvoiwydJ5zvzTHK/the-allais-paradox), *LessWrong*, January 19.\n    \n3.  ^**[^](#fnrefubi7uruz3hb)**^\n    \n    For example Buchak, Lara (2013) [*Risk and Rationality*](https://en.wikipedia.org/wiki/Special:BookSources/9780199672165), Oxford: Oxford University Press.\n    \n4.  ^**[^](#fnrefp0ggxxyyzsq)**^\n    \n    Shulman, Carl (2012) [Salary or startup? How do-gooders can gain more from risky careers](https://80000hours.org/2012/01/salary-or-startup-how-do-gooders-can-gain-more-from-risky-careers/), *80,000 Hours*, January 8.\n    \n5.  ^**[^](#fnrefhurfbenp7ia)**^\n    \n    Tomasik, Brian (2013) [When should altruists be financially risk-averse?](https://reducing-suffering.org/when-should-altruists-be-financially-risk-averse/), *Essays on Reducing Suffering*, November 20 (updated 30 January 2016).\n    \n6.  ^**[^](#fnrefs6klzedb3qf)**^\n    \n    Snowden, James (2015) [Does risk aversion give an agent with purely altruistic preferences a good reason to donate to multiple charities?](https://drive.google.com/file/d/0B551Ijx9v_RoMzZrOXZvbG9BMzA/view), September."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "85i64CfBc6wQ4NgZB",
    "name": "Relationship between overheads and effectiveness",
    "core": false,
    "slug": "relationship-between-overheads-and-effectiveness",
    "oldSlugs": null,
    "postCount": null,
    "description": {
      "markdown": "A charity’s overhead costs are their administrative and fundraising costs. It is relatively common to use the proportion of a charity’s budget that’s spent on non-overhead costs (often called “program costs”) as an indicator for how good (or [cost-effective](https://forum.effectivealtruism.org/tag/cost-effectiveness)) the charity is. For example, this a key criterion used in Charity Navigator's ratings.^[\\[1\\]](#fn7y93hmba1wi)^\n\nSome use overhead costs as a proxy for effectiveness on the following rationale: What we really care about is how much a charity helps others; it‘s the charity‘s program costs, not its administrative and fundraising costs, which help others; therefore the greater the proportion of its income a charity spends on others, relative to itself, the better.\n\nThe above rationale is misguided, however, for two reasons. First, overhead ratio doesn’t measure how much the charity actually helps others. For example, a charity could be spending all its money on program costs, but if the aim of the charity is to give donuts to police officers, it still won’t do much good.\n\nSecond, having a low overhead ratio may be actively bad for a charity. Charity fundraising often gets a very good rate of return, so putting money into fundraising may allow a charity to grow faster and in the long term spend more on its programs. Administration costs often include monitoring and evaluation to find out whether a program is working, and how it could be done better.\n\nFurther reading\n---------------\n\nKarnofsky, Holden (2009a) [Pitfalls of the overhead ratio?](https://blog.givewell.org/2009/05/21/pitfalls-of-the-overhead-ratio/), *The GiveWell Blog*, May 21 (updated 17 August 2011).\n\nKarnofsky, Holden (2009b) [The worst way to pick a charity](https://blog.givewell.org/2009/12/01/the-worst-way-to-pick-a-charity/), *The GiveWell Blog*, December 1 (updated 19 December 2012).\n\n1.  ^**[^](#fnref7y93hmba1wi)**^\n    \n    Charity Navigator (2016) [Financial efficiency performance metrics](https://www.charitynavigator.org/index.cfm?bay=content.view&cpid=35#Financial-Efficiency-Performance-Metrics), in 'How do we rate charities’ financial health?', *Charity Navigator*, June 1."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "ToyELgMZm9LDdtRFK",
    "name": "Other moral theories",
    "core": false,
    "slug": "other-moral-theories",
    "oldSlugs": null,
    "postCount": 5,
    "description": {
      "markdown": "While most contemporary ethicists subscribe to one of the three major theories in normative ethics—[consequentialism](https://forum.effectivealtruism.org/tag/consequentialism), [deontology](https://forum.effectivealtruism.org/tag/deontology), and [virtue ethics](https://forum.effectivealtruism.org/tag/virtue-ethics)— there are other notable views. Here we explore two: ethics of care and particularism.\n\nEthics of care\n--------------\n\nAccording to the ethics of care, morality is defined by a relationship of care between individuals in an interconnected social network.\n\nInstead of abstract principles like duty ([deontology](https://forum.effectivealtruism.org/tag/deontology)) or maximizing good outcomes ([consequentialism](https://forum.effectivealtruism.org/tag/consequentialism)) guiding our moral life, the ethics of care argues that the emotional and personal experience of caring for others should set the scope and depth of our moral obligations. This has different implications for different theorists. Some have argued that the ethics of care implies that we have a much stronger moral duty to those close to us, with whom we have a genuine caring relationship, than we do to individuals who live far away from us. Other theorists provide broader definitions of care that make the application of this theory more universal.\n\nParticularism\n-------------\n\nMoral particularism holds that when we make moral judgments, we should not use general moral principles, but should instead just apply our moral judgment to the case at hand.\n\nTake a particular case—an individual is murdered. The consequentialist will argue that this is wrong because it will have harmful long term consequences, while the deontologist will argue that this is wrong because it violates a moral prohibition against murder. The particularist, on the other hand, will argue that we have a clear moral judgment that murder is wrong, and it stands independently of our belief in any of these moral theories. We shouldn't rely on these principles both because no theory has adequately synthesized all of our judgments, and because these moral theories derive their plausibility from our moral judgments. Instead, particularists claim, we are better off appealing to these judements directly.\n\nFurther reading\n---------------\n\nDancy, Jonathan (2001) [Moral particularism](https://plato.stanford.edu/entries/moral-particularism/), *Stanford Encyclopedia of Philosophy*, June 6 (updated 22 September 2017).\n\nNorlock, Kathryn (2019) [Feminist ethics](https://plato.stanford.edu/entries/feminism-ethics/), *Stanford Encyclopedia of Philosophy*, May 27 (updated 15 June 2019).\n\nSander-Staudt, Maureen (2011) [Care ethics](https://iep.utm.edu/care-eth/), *Internet Encyclopedia of Philosophy*, March 18.\n\nTsu, Peter Shiu-Hwa (2013) [Moral particularism](http://doi.org/10.5840/tpm20011674), *Internet Encyclopedia of Philosophy*, January 27.\n\nWikipedia (2006) [Ethics of care](https://en.wikipedia.org/wiki/Ethics_of_care), *Wikipedia*, February 18 (updated 21 March 2021)."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "aoG4RzP4KCkckH2Gz",
    "name": "Cause neutrality",
    "core": false,
    "slug": "cause-neutrality",
    "oldSlugs": [
      "neutrality-in-focus-area-selection"
    ],
    "postCount": 6,
    "description": {
      "markdown": "**Cause neutrality** (sometimes called **cause impartiality**^[\\[1\\]](#fn4496lhu6ztv)^ or **strategic cause selection**^[\\[2\\]](#fnlmoi20qa9va)^) is the view that causes should be prioritized based on impartial assessments of impact rather than on other considerations, such as saliency or personal attachment. It is generally considered to be a core idea in [effective altruism](https://forum.effectivealtruism.org/tag/effective-altruism).\n\nSuppose you value the welfare of all humans equally and suppose that dementia research is not the most effective way to help humans. Then being cause-impartial means you should not fund dementia research, even if dementia has personal significance to you (e.g. because it affected a family member).\n\nNote that the implications of neutrality vary with the value system that one holds. If the person in the above example instead placed special weight on the welfare of those suffering from dementia, then supporting dementia research might actually be the best course of action.\n\nFurther reading\n---------------\n\nHutchinson, Michelle (2016) [Giving What We Can is cause neutral](https://forum.effectivealtruism.org/posts/tLdtftZakmpWq73kA/giving-what-we-can-is-cause-neutral), *Effective Altruism Forum*, April 22.  \n*A discussion of cause neutrality.*\n\nMacAskill, William & Darius Meissner (2020) [Cause Impartiality](https://www.utilitarianism.net/utilitarianism-and-practical-ethics#cause-impartiality), in 'Utilitarianism and practical ethics', *Utilitarianism*.\n\nSchubert, Stefan (2017) [Understanding cause-neutrality](https://www.centreforeffectivealtruism.org/blog/understanding-cause-neutrality), *Centre For Effective Altruism*, March 10 (updated 30 December 2020).\n\nSentience Politics (2016) [The benefits of cause-neutrality](https://web.archive.org/web/20160403060915/http://sentience-politics.org/en/philosophy/the-benefits-of-cause-neutrality/), *Sentience Politics*, April.  \n*A discussion of the practice and benefits.*\n\nRelated entries\n---------------\n\n[cause prioritization](https://forum.effectivealtruism.org/tag/cause-prioritization) | [means neutrality](https://forum.effectivealtruism.org/topics/means-neutrality) | [neutrality](https://forum.effectivealtruism.org/topics/neutrality)\n\n1.  ^**[^](#fnref4496lhu6ztv)**^\n    \n    Schubert, Stefan (2017) [Understanding cause-neutrality](https://www.centreforeffectivealtruism.org/blog/understanding-cause-neutrality), *Centre For Effective Altruism*, March 10 (updated 30 December 2020).\n    \n2.  ^**[^](#fnreflmoi20qa9va)**^\n    \n    Karnofsky, Holden (2012) [Strategic cause selection](https://blog.givewell.org/2012/05/02/strategic-cause-selection/), *The GiveWell Blog*, May 2 (updated 25 July 2016)."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "oXReA4D9eReeDwoR4",
    "name": "Micronutrient deficiency",
    "core": false,
    "slug": "micronutrient-deficiency",
    "oldSlugs": [
      "micronutrient-programs",
      "micronutrient-fortification"
    ],
    "postCount": 11,
    "description": {
      "markdown": "Micronutrients are vitamins and minerals needed in small amounts for normal growth and development. **Micronutrient deficiencies** affect over two billion humans worldwide,^[\\[1\\]](#fna8qof71x8cb)^ and have been shown to affect people’s physical health (e.g. iron deficiency causes anemia) and cognitive function (e.g. iodine deficiency negatively affects mental development and lowers IQ). Many [farm animals](https://forum.effectivealtruism.org/topics/farmed-animal-welfare) appear to also be deficient in some micronutrients, because their diets are optimized for food production rather than overall health.^[\\[2\\]](#fnm4ua3le6dnn)^\n\nThere are three ways to reduce micronutrient deficiencies: giving people micronutrient supplements directly (supplementation); enriching staple foods such as rice, flour, oil and salt with micronutrients (fortification); and breeding or genetically manipulating plants to increase their micronutrient content (biofortification).\n\nTreating micronutrient deficiencies in humans is a very [cost-effective](https://forum.effectivealtruism.org/tag/cost-effectiveness-analysis) intervention: it usually costs only a few cents per person per year to provide particular vitamins or minerals.^[\\[3\\]](#fncwp4tnvmf2)^ One of [Givewell](https://forum.effectivealtruism.org/tag/givewell)’s top-rated charities—[Helen Keller International](https://forum.effectivealtruism.org/tag/helen-keller-international)'s vitamin A supplementation program—focuses on reducing micronutrient deficiencies.^[\\[4\\]](#fnxo66373u35)^ In animals, the organization [Healthier Hens](https://forum.effectivealtruism.org/topics/healthier-hens) focuses on feed fortification for egg-laying hens.\n\nFurther reading\n---------------\n\nRitchie, Hannah & Max Roser (2017) [Micronutrient deficiency](https://ourworldindata.org/micronutrient-deficiency), *Our World in Data*.  \n*A comprehensive overview of the main types of micronutrient deficiencies in humans.*\n\nRelated entries\n---------------\n\n[farmed animal welfare](https://forum.effectivealtruism.org/topics/farmed-animal-welfare) | [global health and development](https://forum.effectivealtruism.org/tag/global-health-and-development)\n\n1.  ^**[^](#fnrefa8qof71x8cb)**^\n    \n    Ritchie, Hannah & Max Roser (2017) [Micronutrient deficiency](https://ourworldindata.org/micronutrient-deficiency), *Our World in Data*.\n    \n2.  ^**[^](#fnrefm4ua3le6dnn)**^\n    \n    Esparza, Isaac & Lukas Jasiunas (2021) [Introducing Healthier Hens](https://forum.effectivealtruism.org/posts/4QGhyXjXM4yJBvNap/introducing-healthier-hens), *Effective Altruism Forum*, October 25.\n    \n3.  ^**[^](#fnrefcwp4tnvmf2)**^\n    \n    Hillebrandt, Hauke & Mark Engelbert (2015) [Micronutrient fortification](https://www.givingwhatwecan.org/report/micronutrient-fortification/), *Giving What We Can*, October 29.\n    \n4.  ^**[^](#fnrefxo66373u35)**^\n    \n    GiveWell (2020) [Our top charities](https://www.givewell.org/charities/top-charities), *GiveWell*, November."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "zgz4DXga5rBCHQE6t",
    "name": "Measuring and comparing value",
    "core": false,
    "slug": "measuring-and-comparing-value",
    "oldSlugs": null,
    "postCount": 15,
    "description": {
      "markdown": "Measuring how much value interventions create can help us to evaluate their [cost-effectiveness](https://forum.effectivealtruism.org/tag/cost-effectiveness-analysis).\n\nIdeally, we would measure the things we value directly. Unfortunately, we often value things that are difficult to measure, so that we must instead measure other variables that are closely related to what we do value. For instance, we may try to measure the impact of an intervention on someone’s health, because improving health tends to improve [wellbeing](https://forum.effectivealtruism.org/tag/wellbeing). Although we ultimately care about wellbeing rather than health, the latter is a relatively good measure of the former, which is hard to measure directly.\n\nIt can also be worthwhile to assess the value of things that matter only instrumentally, such as the [value of information](https://forum.effectivealtruism.org/tag/value-of-information), and to consider how value may change over time, as represented by [discount rates](https://forum.effectivealtruism.org/tag/temporal-discounting).\n\nFurther reading\n---------------\n\nGrace, Katja (2014) [Apples and oranges? Some initial thoughts on comparing diverse benefits](https://www.givingwhatwecan.org/post/2014/05/apples-and-oranges-some-initial-thoughts-comparing-diverse-benefits/), *Giving What We Can*, May 22.  \n*A discussion of various means of measurement and comparison.*\n\nWikipedia (2007) [Happiness economics](https://en.wikipedia.org/w/index.php?title=Happiness_economics&dir=prev&action=history), *Wikipedia*, July 4 (updated 17 February 2021‎).  \n*A discussion of the measurement of happiness.*"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "JuYoMsErtC4W6QwqD",
    "name": "Deontology",
    "core": false,
    "slug": "deontology",
    "oldSlugs": null,
    "postCount": 7,
    "description": {
      "markdown": "**Deontology** refers to a group of moral views that focus on rules or prohibitions for action. Deontologists hold that these rules have moral importance that is independent of their effect on the good ([consequentialism](https://forum.effectivealtruism.org/tag/consequentialism)) or our character ([virtue ethics](https://forum.effectivealtruism.org/tag/virtue-ethics)). Different deontological theories focus on different concepts, but many focus on the intrinsic moral value of principles like justice, rights, and duties.\n\nIn contrast with deontologists, consequentialists hold that principles like justice, rights, and duties are only [instrumentally valuable](https://forum.effectivealtruism.org/tag/intrinsic-vs-instrumental-value), that is to say these are only morally important considerations if acting in accordance with them will lead to the best outcome. Thus the difference between deontology and consequentialism is most strongly felt in cases where we can violate a deontological rule, like “do not murder”, in order to bring about great benefits to other people. For example, would it be morally permissible for a doctor to painlessly, secretly kill a patient, in order to use their organs to save five other patients, who would otherwise die without the procedure? Most deontologists would argue that it is impermissible to kill the one to save the five, even if this would lead to a better outcome, because the doctor would be violating the moral prohibition against killing innocent people.\n\nNote that consequentialists may follow rules or guidelines instrumentally, in order to promote the good (see [naive consequentialism vs. sophisticated consequentialism](https://forum.effectivealtruism.org/tag/naive-vs-sophisticated-consequentialism)).\n\nFurther reading\n---------------\n\nAlexander, Larry & Michael Moore (2007) [Deontological ethics](https://plato.stanford.edu/entries/ethics-deontological/), *Stanford Encyclopedia of Philosophy*, November 21 (updated 30 October 2020).\n\nRelated entries\n---------------\n\n [consequentialism](https://forum.effectivealtruism.org/tag/consequentialism) | [normative ethics](https://forum.effectivealtruism.org/tag/normative-ethics) | [virtue ethics](https://forum.effectivealtruism.org/tag/virtue-ethics)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "iaRevNjskjxFvehQ8",
    "name": "Credal resilience",
    "core": false,
    "slug": "credal-resilience",
    "oldSlugs": null,
    "postCount": 9,
    "description": {
      "markdown": "**Credal resilience** is a measure of the degree to which a [credence](https://forum.effectivealtruism.org/topics/credence) is expected to change in response to new evidence.\n\nSuppose a person has two coins in front of them: coin A and coin B. They have flipped coin A thousands of times to confirm that it is unbiased, so it seems reasonable for the person to have a credence of 0.5 that if they flip coin A again, it will land heads. Coin B, on the other hand, might be biased or unbiased. However even if it is biased, the person has no evidence about which way it is biased, and so has no reason to think that the bias will favor either heads or tails. In these circumstances, it also seems reasonable for them to have a credence of 0.5 that if they flip coin B, it will land heads.\n\nA person can have the same credence in two different propositions, and yet think that one of those credences is more likely to move given new evidence. For example, suppose that coin B lands on heads each of the first four times they test it. Based on these observations, their credence that coin B will land on heads on the next toss will be higher than 0.5, because they now have evidence that the coin is biased. However, if they saw coin A land heads four times in a row, their credence that the next toss will be a head will still be close to 0.5, because they already have considerable evidence that the coin is fair.\n\nIn choice situations where we have low-resilience credences, the [value of information](https://forum.effectivealtruism.org/tag/value-of-information) will usually be higher, because new evidence is more likely to change our credences.\n\nFurther reading\n---------------\n\nEgan, Andy & Adam Elga (2005) [I can’t believe I’m stupid](http://doi.org/10.1111/j.1520-8583.2005.00054.x), *Philosophical Perspectives*, vol. 19, pp. 77–93.\n\nPopper, Karl (1959) *The logic of scientific discovery*, New York: Basic Books.\n\nSkyrms, Brian (1977) [Resiliency, propensities, and causal necessity](http://doi.org/10.2307/2025774), *The Journal of Philosophy*, vol. 74, pp. 704–713.\n\nRelated entries\n---------------\n\n[cluelessness](https://forum.effectivealtruism.org/tag/cluelessness) | [credence](https://forum.effectivealtruism.org/tag/credence) | [decision theory](https://forum.effectivealtruism.org/tag/decision-theory) | [expected value](https://forum.effectivealtruism.org/tag/expected-value) | [forecasting](https://forum.effectivealtruism.org/tag/forecasting) | [model uncertainty](https://forum.effectivealtruism.org/tag/model-uncertainty) | [value of information](https://forum.effectivealtruism.org/tag/value-of-information)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "G2LQtuW6qoBrsfKEv",
    "name": "Counterfactual reasoning",
    "core": false,
    "slug": "counterfactual-reasoning",
    "oldSlugs": null,
    "postCount": 10,
    "description": {
      "markdown": "**Counterfactual reasoning** involves scenarios that will occur if an agent chooses a certain action, or that would have occurred if an agent had chosen an action they did not. For instance, we can consider a counterfactual scenario in which the effective altruism community was called ‘effective giving’ rather than effective altruism.\n\nWhen we rank actions, we generally want to consider not just how good an action is, but how good it is relative to the alternatives. This is implicitly assumed by the framework of idealized decision-making, but it is useful to state it explicitly.\n\nOne related heuristic is [replaceability](https://forum.effectivealtruism.org/tag/replaceability): it may be the case, for instance, that if you do not take a certain action, then someone else will take it instead.\n\nUnfortunately, counterfactuals are often difficult to evaluate. Even after an action is taken, there will in many cases remain substantial uncertainty about what would have happened if one had acted otherwise. This means that we will often be unsure about whether we have acted in the best possible way.\n\nFurther reading\n---------------\n\nOrd, Toby (2014) [Drones, counterfactuals, and equilibria: Challenges in evaluating new military technologies](https://www.fhi.ox.ac.uk/publications/ord-t-drones-counterfactuals-and-equilibria-challenges-in-evaluating-new-military-technologies/), Future of Humanity Institute, University of Oxford.\n\nSempere, Nuño (2019) [Shapley values: Better than counterfactuals](https://forum.effectivealtruism.org/posts/XHZJ9i7QBtAJZ6byW/shapley-values-better-than-counterfactuals), *Effective Altruism Forum*, October 10.\n\nRelated entries\n---------------\n\n[impact assessment](https://forum.effectivealtruism.org/tag/impact-assessment) | [markets for altruism](https://forum.effectivealtruism.org/tag/markets-for-altruism)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "Ds75i66So34kfTd7X",
    "name": "Non-wellbeing sources of value",
    "core": false,
    "slug": "non-wellbeing-sources-of-value",
    "oldSlugs": null,
    "postCount": 6,
    "description": {
      "markdown": "Nearly all moral theories assign some value to [wellbeing](https://forum.effectivealtruism.org/tag/wellbeing). But while some theories hold that well-being is the only source of value (“welfarist theories”), other theories recognize other sources of value.\n\nHow value is defined tends to depend on the preferred moral theory. [Deontologists](https://forum.effectivealtruism.org/tag/deontology) tend to focus on principles which should guide actions, such as “treat others as you would want to be treated”, while [virtue ethicists](https://forum.effectivealtruism.org/tag/virtue-ethics) concentrate on specific virtues like wisdom or benevolence. Non-welfarist [consequentialists](https://forum.effectivealtruism.org/tag/consequentialism) are more likely to focus on non-welfare goods like knowledge, beauty, and diversity.\n\nTheories which recognize multiple sources of value also need some mechanism for dealing with cases where the different values conflict.\n\nFurther reading\n---------------\n\nMason, Elinor (2006) [Value pluralism](http://plato.stanford.edu/entries/value-pluralism/), *Stanford Encyclopedia of Philosophy*, June 20 (updated 7 Feb 2018)."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "Hh2HRXggFWRrwocab",
    "name": "Mechanism design",
    "core": false,
    "slug": "mechanism-design",
    "oldSlugs": null,
    "postCount": 28,
    "description": {
      "markdown": "**Mechanism design** is a field that applies [economics](https://forum.effectivealtruism.org/tag/economics) and [game theory](https://forum.effectivealtruism.org/tag/game-theory) to the design of incentive structures—or *mechanisms*—in order to achieve specific social objectives. Conventionally, mechanism design assumes that players have private information about their preferences and act rationally so as to maximize those preferences.\n\nThere are several types of mechanisms:\n\n*   Markets facilitate the provision and exchange of goods between buyers and sellers.\n    *   Auctions are a type of market in which buyers place bids and the prices and allocation of items are determined by the bids.\n    *   Matching markets are markets in which members of two sets of players are matched, or goods are allocated to players without prices.\n*   Voting systems are mechanisms that aggregate the preferences of individual players into a collective decision.\n\nMechanisms can be evaluated based on the following criteria:\n\n*   Pareto efficiency: no player can be made better off without making another player worse off.\n*   Truthfulness or strategyproofness: it is a dominant strategy for players to submit their true preferences.\n*   Stability: a matching is stable if no players can be made better off by changing their assignment without making any other players worse off.\n*   Social welfare: mechanisms may be evaluated based on whether they maximize a given social welfare function.\n\nMechanism design has been used to improve social outcomes in domains such as medical residency matching,^[\\[1\\]](#fnptfptlu3uwa)^ school choice,^[\\[2\\]](#fnlk4mb0nb0xs)^ organ donation,^[\\[3\\]](#fno2sops91dzq)^ and voting. Mechanisms have also been proposed to improve the provision of [public goods](https://forum.effectivealtruism.org/tag/public-goods).^[\\[4\\]](#fnpqpz89ijed)^\n\nFurther reading\n---------------\n\nPeters, Dominik (2019) [Economic design for effective altruism](https://doi.org/10.1007/978-3-030-18050-8), in Jean-François Laslier *et al.* (eds.) *The Future of Economic Design*, Cham: Springer, pp. 381–388.\n\nWiblin, Robert & Keiran Harris (2019) [Radical institutional reforms that make capitalism & democracy work better, and how to get them](https://80000hours.org/podcast/episodes/glen-weyl-radically-reforming-capitalism-and-democracy/), *80,000 Hours*, February 8.\n\nExternal links\n--------------\n\n[Mechanism Design for Social Good (MD4SG)](https://www.md4sg.com/). An initiative using techniques from algorithms, optimization, and mechanism design to improve access for marginalized groups.\n\nRelated entries\n---------------\n\n[altruistic coordination](https://forum.effectivealtruism.org/tag/altruistic-coordination) | [economics](https://forum.effectivealtruism.org/tag/economics) | [electoral reform](https://forum.effectivealtruism.org/tag/electoral-reform) | [game theory](https://forum.effectivealtruism.org/tag/game-theory) | [markets for altruism](https://forum.effectivealtruism.org/tag/markets-for-altruism) | [public interest technology](https://forum.effectivealtruism.org/tag/public-interest-technology)\n\n1.  ^**[^](#fnrefptfptlu3uwa)**^\n    \n    Wikipedia (2009) [National resident matching program](https://en.wikipedia.org/wiki/National_Resident_Matching_Program), *Wikipedia*, April 6 (updated 9 May 2021‎).\n    \n2.  ^**[^](#fnreflk4mb0nb0xs)**^\n    \n    Abdulkadiroğlu, Atila & Tayfun Sönmez (2003) [School choice: A mechanism design approach](http://doi.org/10.1257/000282803322157061), *American Economic Review*, vol. 93, pp. 729–747.\n    \n3.  ^**[^](#fnrefo2sops91dzq)**^\n    \n    Roth, A. E., T. Sonmez & M. U. Unver (2004) [Kidney exchange](http://doi.org/10.1162/0033553041382157), *The Quarterly Journal of Economics*, vol. 119, pp. 457–488.\n    \n4.  ^**[^](#fnrefpqpz89ijed)**^\n    \n    MikkW (2020) [The case for promoting/creating public goods markets as a cause area](https://forum.effectivealtruism.org/posts/ZjBnzzqkrB7JNLjTg/the-case-for-promoting-creating-public-goods-markets-as-a), *Effective Altruism Forum*, October 24."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "tshCaNbZdrWp8MJ7A",
    "name": "Trajectory change",
    "core": false,
    "slug": "trajectory-change",
    "oldSlugs": [
      "trajectory-changes"
    ],
    "postCount": 20,
    "description": {
      "markdown": "In [longtermism](https://forum.effectivealtruism.org/tag/longtermism), a **trajectory change** is a persistent change to total value at every point in the [long-term future](https://forum.effectivealtruism.org/tag/long-term-future). Trajectory changes have also been described as slight or significant changes to \"the world's *development trajectory*, or just trajectory for short\", with that referring to:^[\\[1\\]](#fnbg8kg4s89uv)^\n\n> a rough summary way the future will unfold over time. The summary includes various facts about the world that matter from a macro perspective, such as how rich people are, what technologies are available, how happy people are, how developed our science and culture is along various dimensions, and how well things are going all-things-considered at different points of time. It may help to think of the trajectory as a collection of graphs, where each graph in the collection has time on the x-axis and one of these other variables on the y-axis.\n> \n> \\[...\\] If we ever prevent an existential catastrophe, that would be an extreme example of a trajectory change. There may also be smaller trajectory changes. For example, if some species of dolphins that we really loved were destroyed, that would be a much smaller trajectory change.\n\nMost longtermist interventions focus on trajectory changes, including but not limited to [existential risk](https://forum.effectivealtruism.org/tag/existential-risk) reduction. However, some longtermist interventions focus on alternative objectives, such as [speeding up development](https://forum.effectivealtruism.org/tag/speeding-up-development).\n\nFurther reading\n---------------\n\nForethought Foundation (2018) [Longtermism: Potential research projects](https://www.forethought.org/longtermism), *Forethought Foundation*, November.\n\nKoehler, Arden, Benjamin Todd, Robert Wiblin & Keiran Harris (2020) [Benjamin Todd on varieties of longtermism and things 80,000 Hours might be getting wrong](https://80000hours.org/podcast/episodes/ben-todd-on-varieties-of-longtermism), *The 80,000 Hours Podcast*, September.\n\nRelated entries\n---------------\n\n[cultural persistence](https://forum.effectivealtruism.org/tag/cultural-persistence) | [existential risk](https://forum.effectivealtruism.org/tag/existential-risk) | [longtermism](https://forum.effectivealtruism.org/tag/longtermism)\n\n1.  ^**[^](#fnrefbg8kg4s89uv)**^\n    \n    Beckstead, Nick (2013) [A proposed adjustment to the astronomical waste argument](https://forum.effectivealtruism.org/posts/RXpJbWKDJ7WFWqEin/a-proposed-adjustment-to-the-astronomical-waste-argument), *Effective Altruism Forum*, May 27."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "FymMLwKiuBPp6AWey",
    "name": "Charity Science Foundation",
    "core": false,
    "slug": "charity-science-foundation",
    "oldSlugs": [
      "charity-science"
    ],
    "postCount": 20,
    "description": {
      "markdown": "The **Charity Science Foundation** is an umbrella organization comprising Charity Science Outreach, Charity Science Health, and [Charity Entrepreneurship](https://forum.effectivealtruism.org/tag/charity-entrepreneurship).\n\nFurther reading\n---------------\n\nKikauka, Xiomara (2015) 'Charity Science', in Ryan Carey (ed.), [*The Effective Altruism Handbook*](https://en.wikipedia.org/wiki/Special:BookSources/9781534935778), 1st ed., Oxford: The Centre for Effective Altruism, pp. 124-126.\n\nExternal links\n--------------\n\n[Charity Science Foundation](https://www.charityscience.com/). Official website.\n\n[Donate to the Charity Science Foundation](https://www.charityscience.com/donate.html).\n\nRelated links\n-------------\n\n[Charity Entrepreneurship](https://forum.effectivealtruism.org/tag/charity-entrepreneurship)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "2pYW47yQfGxDdawpj",
    "name": "Video",
    "core": false,
    "slug": "video-1",
    "oldSlugs": null,
    "postCount": 247,
    "description": {
      "markdown": "The **video** tag is used for posts that you can watch, or that are substantially about video content. This includes links to video talks and  collections of video content.\n\n## Related entries\n[audio](https://forum.effectivealtruism.org/tag/audio)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "RTrJsKeyDihucbtK6",
    "name": "Philosophy of mind",
    "core": false,
    "slug": "philosophy-of-mind",
    "oldSlugs": null,
    "postCount": 14,
    "description": {
      "markdown": "Related entries\n---------------\n\n[consciousness research](https://forum.effectivealtruism.org/topics/consciousness-research) |  [philosophy](https://forum.effectivealtruism.org/topics/philosophy)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "pjafFR2EwL8xAjgvB",
    "name": "Animal sentience",
    "core": false,
    "slug": "animal-sentience",
    "oldSlugs": null,
    "postCount": 53,
    "description": {
      "markdown": "*This entry discusses specifically the question of whether nonhuman animals are sentient. For discussion about the moral significance of sentience or its distribution, see* [*sentience*](https://forum.effectivealtruism.org/topics/sentience-1)*.*\n\n**Animal sentience** is the study of the affective capacities of nonhuman animals, i.e. their capacity to have subjective experiences.\n\nFurther reading\n---------------\n\nBrowning, Heather & Jonathan Birch (2022) [Animal sentience](https://compass.onlinelibrary.wiley.com/doi/10.1111/phc3.12822), *Philosophy Compass*.\n\nTye, Michael (2017) [*Tense Bees and Shell-Shocked Crabs: Are Animals Conscious?*](https://en.wikipedia.org/wiki/Special:BookSources/978-0-19-027801-4), New York: Oxford University Press.\n\nRelated entries\n---------------\n\n[animal cognition](https://forum.effectivealtruism.org/tag/animal-cognition) | [moral patienthood](https://forum.effectivealtruism.org/tag/moral-patienthood) | [sentience](https://forum.effectivealtruism.org/tag/sentience-1)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "BjdHNs5r3GDqve5Yo",
    "name": "Fish welfare",
    "core": false,
    "slug": "fish-welfare",
    "oldSlugs": null,
    "postCount": 24,
    "description": {
      "markdown": "**Fish welfare** is the study of interventions aimed at improving the welfare of fish.\n\nFurther reading\n---------------\n\nTorrella, Kenny (2021) [The next frontier for animal welfare: Fish](https://www.vox.com/future-perfect/22301931/fish-animal-welfare-plant-based), *Vox*, March 2.  \n*An informative popular overview of the main issues surrounding fish welfare.*\n\nExternal links\n--------------\n\n[Fish Count](http://fishcount.org.uk/). A website focused on increasing understanding and awareness of fish sentience.\n\nRelated entries\n---------------\n\n[animal welfare](https://forum.effectivealtruism.org/tag/animal-welfare-1) | [Aquatic Life Institute](https://forum.effectivealtruism.org/tag/aquatic-life-institute) | [crustacean welfare](https://forum.effectivealtruism.org/tag/crustacean-welfare) | [farmed animal welfare](https://forum.effectivealtruism.org/tag/farmed-animal-welfare) | [Fish Welfare Initiative](https://forum.effectivealtruism.org/tag/fish-welfare-initiative)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "wtSKh87mma5Qytp7t",
    "name": "Valence",
    "core": false,
    "slug": "valence",
    "oldSlugs": null,
    "postCount": 15,
    "description": {
      "markdown": "**Valence** is a core dimension shared by all affective states: it is what makes certain experiences feel good or bad to varying degrees.\n\nFurther reading\n---------------\n\nCarruthers, Peter (2018) [Valence and value](http://doi.org/10.1111/phpr.12395), *Philosophy and Phenomenological Research*, vol. 97, pp. 658–680.\n\nRelated entries\n---------------\n\n[axiology](https://forum.effectivealtruism.org/tag/axiology)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "8FxWuRZCwoxzQPkWM",
    "name": "United States",
    "core": false,
    "slug": "united-states",
    "oldSlugs": [
      "us-politics",
      "us-politics",
      "united-states-politics",
      "united-states-policy-and-politics"
    ],
    "postCount": 88,
    "description": {
      "markdown": "The **United States** tag is used for posts that are about the United States, including (but not restricted to)  [policy](https://forum.effectivealtruism.org/topics/policy),  [politics](https://forum.effectivealtruism.org/topics/electoral-politics), and [international relations](https://forum.effectivealtruism.org/topics/international-relations).\n\nRelated entries\n---------------\n\n[China](https://forum.effectivealtruism.org/topics/china) | [India](https://forum.effectivealtruism.org/topics/india) | [international relations](https://forum.effectivealtruism.org/topics/international-relations) | [electoral politics](https://forum.effectivealtruism.org/tag/electoral-politics) | [policy change](https://forum.effectivealtruism.org/tag/policy-change)| [Russia](https://forum.effectivealtruism.org/topics/russia) | [United Kingdom](https://forum.effectivealtruism.org/topics/united-kingdom)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "EpitmZxh6PFDR2PgD",
    "name": "Happier Lives Institute",
    "core": false,
    "slug": "happier-lives-institute",
    "oldSlugs": null,
    "postCount": 31,
    "description": {
      "markdown": "The **Happier Lives Institute** (**HLI**) is an organization that researches the best ways to increase global [wellbeing](https://forum.effectivealtruism.org/tag/subjective-wellbeing). This includes theoretical research on wellbeing measurement as well as practical research into effective interventions. HLI was incubated by [Charity Entrepreneurship](https://forum.effectivealtruism.org/tag/charity-entrepreneurship).\n\nFunding\n-------\n\nAs of July 2022, HLI has received $55,000 in funding from [Effective Altruism Funds](https://forum.effectivealtruism.org/topics/effective-altruism-funds).^[\\[1\\]](#fnezqqj44yq1d)^^[\\[2\\]](#fnegc9x3o2d3)^\n\nFurther reading\n---------------\n\nCohen, Alex (2021) [A conversation with the Happier Lives Institute](https://docs.google.com/document/d/16hDzQvdQ7dhOoFlewNK9cMuq5Kox3ZxR2CpCZwKUtJg/edit), *GiveWell*, February 12.\n\nPlant, Michael *et al.* (2021) [2021-22 Research agenda and context](https://www.happierlivesinstitute.org/research/research-agenda/), *Happier Lives Institute*, April.\n\nExternal links\n--------------\n\n[Happier Lives Institute](https://www.happierlivesinstitute.org/). Official website.\n\n[Apply for a job](https://www.happierlivesinstitute.org/about/work-with-us/).\n\nRelated entries\n---------------\n\n[adjusted life year](https://forum.effectivealtruism.org/tag/adjusted-life-year) | [mental health](https://forum.effectivealtruism.org/tag/mental-health) | [pain and suffering](https://forum.effectivealtruism.org/tag/pain-and-suffering) | [subjective wellbeing](https://forum.effectivealtruism.org/tag/subjective-wellbeing)\n\n1.  ^**[^](#fnrefezqqj44yq1d)**^\n    \n    Effective Altruism Infrastructure Fund (2019) [November 2019: EA Meta Fund grants](https://funds.effectivealtruism.org/funds/payouts/november-2019-ea-meta-fund-grants), *Effective Altruism Funds*, November. \n    \n2.  ^**[^](#fnrefegc9x3o2d3)**^\n    \n    Effective Altruism Infrastructure Fund (2020) [July 2020: EA Meta Fund grants](https://funds.effectivealtruism.org/funds/payouts/july-2020-ea-meta-fund-grants), *Effective Altruism Funds*, July."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "JjJwkAtCruTW7QeTN",
    "name": "Animal Advocacy Careers",
    "core": false,
    "slug": "animal-advocacy-careers",
    "oldSlugs": null,
    "postCount": 28,
    "description": {
      "markdown": "**Animal Advocacy Careers** (**AAC**) is an organization that seeks to address the career and talent bottlenecks in the [animal advocacy movement](https://forum.effectivealtruism.org/tag/effective-animal-advocacy), especially the [farmed animal](https://forum.effectivealtruism.org/tag/farmed-animal-welfare) movement, by providing career services and advice. AAC was incubated by [Charity Entrepreneurship](https://forum.effectivealtruism.org/tag/charity-entrepreneurship).\n\nFunding\n-------\n\nAs of June 2022, AAC has received over $430,000 in funding from from [Open Philanthropy](https://forum.effectivealtruism.org/topics/open-philanthropy),^[\\[1\\]](#fns5uk4iw51ph)^ and over $80,000 from [Effective Altruism Funds](https://forum.effectivealtruism.org/topics/effective-altruism-funds).^[\\[2\\]](#fn09s1xewqi28j)^^[\\[3\\]](#fncwy847vcuq8)^\n\nFurther reading\n---------------\n\nAnimal Advocacy Careers (2021) [2021 plans and 2020 review](https://www.animaladvocacycareers.org/post/2021-plans-and-2020-review), *Animal Advocacy Careers*, February 17.\n\nExternal links\n--------------\n\n[Animal Advocacy Careers](https://www.animaladvocacycareers.org/). Official website.\n\n[Apply for a job](https://www.animaladvocacycareers.org/get-involved).\n\n[Donate to Animal Advocacy Careers](https://www.animaladvocacycareers.org/donate/).\n\nRelated entries\n---------------\n\n[career advising](https://forum.effectivealtruism.org/tag/career-advising) | [farmed animal welfare](https://forum.effectivealtruism.org/tag/farmed-animal-welfare)\n\n1.  ^**[^](#fnrefs5uk4iw51ph)**^\n    \n    Open Philanthropy (2022) [Grants database: Animal Advocacy Careers](https://www.openphilanthropy.org/grants/?q=&organization-name=animal-advocacy-careers), *Open Philanthropy*.\n    \n2.  ^**[^](#fnref09s1xewqi28j)**^\n    \n    Animal Welfare Fund (2020) [March 2020: Animal Welfare Fund grants](https://funds.effectivealtruism.org/funds/payouts/march-2020-animal-welfare-fund-grants), *Effective Altruism Funds*, March.\n    \n3.  ^**[^](#fnrefcwy847vcuq8)**^\n    \n    Animal Welfare Fund (2021) [November 2021: Animal Welfare Fund grants](https://funds.effectivealtruism.org/funds/payouts/november-2021-animal-welfare-fund-grants), *Effective Altruism Funds*, November."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "rwXxjhRgp4ZKgYE78",
    "name": "Sentience Institute",
    "core": false,
    "slug": "sentience-institute",
    "oldSlugs": null,
    "postCount": 31,
    "description": {
      "markdown": "**Sentience Institute** is a nonprofit think tank promoting interdisciplinary research on long-term social and technological change, particularly on [moral circle expansion](https://forum.effectivealtruism.org/tag/moral-circle-expansion-1).\n\nFunding\n-------\n\nAs of July 2022, Sentience Institute has received $80,000 in funding from [Effective Altruism Funds](https://forum.effectivealtruism.org/topics/effective-altruism-funds),^[\\[1\\]](#fn8hla52a4ub3)^^[\\[2\\]](#fnh57yag81c7n)^ and $50,000 from the [Survival and Flourishing](https://forum.effectivealtruism.org/topics/survival-and-flourishing) Fund.^[\\[3\\]](#fnr72t485s2y)^ \n\nExternal links\n--------------\n\n[Sentience Institute](https://www.sentienceinstitute.org/). Official website.\n\n[Apply for a job](https://www.sentienceinstitute.org/get-involved).\n\n1.  ^**[^](#fnref8hla52a4ub3)**^\n    \n    Animal Welfare Fund (2017) [November 2017: Animal Welfare Fund grants](https://funds.effectivealtruism.org/funds/payouts/november-2017-animal-welfare-fund-grants), *Effective Altruism Funds*, November. \n    \n2.  ^**[^](#fnrefh57yag81c7n)**^\n    \n    Animal Welfare Fund (2018) [December 2018: Animal Welfare Fund grants](https://funds.effectivealtruism.org/funds/payouts/december-2018-animal-welfare-fund-grants), *Effective Altruism Funds*, December.\n    \n3.  ^**[^](#fnrefr72t485s2y)**^\n    \n    Survival and Flourishing Fund (2021) [SFF-2022-H1 S-Process recommendations announcement](https://survivalandflourishing.fund/sff-2022-h1-recommendations), *Survival and Flourishing Fund*."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "KwxhLvuK88Qak5n3u",
    "name": "Public interest technology",
    "core": false,
    "slug": "public-interest-technology",
    "oldSlugs": null,
    "postCount": 104,
    "description": {
      "markdown": "**Public interest technology** (**PIT**) is a field that combines expertise in technology, especially computing, with the common good. Public interest tech can be divided into two approaches:\n\n*   The application of computing and related technologies to serve the common good.\n*   Applying knowledge of computing and related technologies to public policy.\n\nThe word \"technology\" can refer broadly to any devices, processes, or skills used to solve practical problems. In the context of public interest tech, however, \"technology\" is often narrowly defined as \"digital skills required to build modern products and services\".^[\\[1\\]](#fngpj37qnj4do)^ This narrower definition of \"technology\" includes, but is not limited to, the following disciplines:\n\n*   [Software engineering](https://forum.effectivealtruism.org/tag/software-engineering)\n*   User experience (UX) design and research\n*   [Data science](https://forum.effectivealtruism.org/tag/data-science) and data engineering\n*   Product management\n*   [Operations research](https://forum.effectivealtruism.org/topics/operations-research)\n*   Computational sciences, such as computational biology and computational social science\n\nOrganizations that promote public interest tech include:\n\n*   Code for America\n*   Coding it Forward\n*   Ford Foundation\n*   Hack4Impact\n*   Impact Labs\n*   Mozilla Foundation\n*   TechShift\n\nPublic interest tech may be done with a focus on the most pressing causes identified by [effective altruism](https://forum.effectivealtruism.org/tag/effective-altruism). For example:\n\n*   The for-profit startup Sendwave is a mobile payments platform that allows users to send money to contacts in Africa and Asia.\n*   altLabs has developed computational models to identify the origin of genetically engineered material, as a way to improve biosecurity.\n*   The fields of [AI alignment](https://forum.effectivealtruism.org/tag/ai-alignment) and [AI governance](https://forum.effectivealtruism.org/topics/ai-governance) aim to mitigate global catastrophic risks from advanced artificial intelligence.\n\nFurther reading\n---------------\n\nFord Foundation (2021) [Public interest tech: About](https://www.fordfoundation.org/campaigns/public-interest-tech/about/), *Ford Foundation*.\n\nNew America (2021) [Public interest technology: About](http://newamerica.org/pit/), *New America*.\n\nExternal links\n--------------\n\n[Public-Interest Technology Resources](https://public-interest-tech.com/). Website maintained by Bruce Schneier.\n\nRelated entries\n---------------\n\n[AI governance](https://forum.effectivealtruism.org/topics/ai-governance) | [AI alignment](https://forum.effectivealtruism.org/tag/ai-alignment) | [data science](https://forum.effectivealtruism.org/tag/data-science) | [entrepreneurship](https://forum.effectivealtruism.org/tag/entrepreneurship) | [information security](https://forum.effectivealtruism.org/tag/information-security) | [mechanism design](https://forum.effectivealtruism.org/tag/mechanism-design) | [operations research](https://forum.effectivealtruism.org/topics/operations-research) | [software engineering](https://forum.effectivealtruism.org/tag/software-engineering)\n\n1.  ^**[^](#fnrefgpj37qnj4do)**^\n    \n    Code for America (2018) [Talent initiative](https://www.codeforamerica.org/programs/talent-initiative), *Code for America*."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "xSELbJPbFreHNTijq",
    "name": "Classical utilitarianism",
    "core": false,
    "slug": "classical-utilitarianism",
    "oldSlugs": null,
    "postCount": 7,
    "description": {
      "markdown": "**Classical utilitarianism** is the form of [utilitarianism](https://forum.effectivealtruism.org/tag/utilitarianism) espoused by the founding fathers of utilitarianism, especially [Jeremy Bentham](https://forum.effectivealtruism.org/tag/jeremy-bentham). It is generally understood to be a form of [total](https://forum.effectivealtruism.org/topics/total-view), [hedonistic](https://forum.effectivealtruism.org/tag/hedonism) act [consequentialism](https://forum.effectivealtruism.org/tag/consequentialism) that gives equal intrinsic weight to positive and negative hedonic states.\n\nFurther reading\n---------------\n\nMacAskill, William, Darius Meissner & Richard Yetter Chappell (2021) [Elements and types of utilitarianism](https://www.utilitarianism.net/types-of-utilitarianism), *Utilitarianism.Net*.\n\nRelated entries\n---------------\n\n[consequentialism](https://forum.effectivealtruism.org/tag/consequentialism) | [hedonism](https://forum.effectivealtruism.org/tag/hedonism) | [utilitarianism](https://forum.effectivealtruism.org/tag/utilitarianism) | [welfarism](https://forum.effectivealtruism.org/tag/welfarism)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "iB2dz4Q933zK4qmkm",
    "name": "Economics",
    "core": false,
    "slug": "economics",
    "oldSlugs": null,
    "postCount": 97,
    "description": {
      "markdown": "**Economics** is the social science of how economic actors interact with things they value, especially goods and services in an economy. Economic behavior is often studied using rational choice theory, which assumes that actors in a social system make decisions so as to maximize their individual utility functions; the subfield of behavioral economics provides evidence as to when, and to what extent, economic actors conform to the assumptions of rational choice theory.\n\nEconomic methods are often used to guide [cause prioritization](https://forum.effectivealtruism.org/tag/cause-prioritization) and decision-making in [effective altruism](https://forum.effectivealtruism.org/tag/effective-altruism).\n\nFurther reading\n---------------\n\nBernard, David Rhys (2020) [An introduction to global priorities research for economists](https://forum.effectivealtruism.org/posts/dia3NcGCqLXhWmsaX/an-introduction-to-global-priorities-research-for-economists), *Effective Altruism Forum*, August 28.\n\nDuda, Roman (2015) [Why an economics PhD might be the best grad degree](https://80000hours.org/career-reviews/economics-phd/), *80,000 Hours*, June.\n\nExternal links\n--------------\n\n[Economics](https://en.wikipedia.org/wiki/Economics). Wikipedia entry.\n\nRelated entries\n---------------\n\n[decision theory](https://forum.effectivealtruism.org/tag/decision-theory) | [economic growth](https://forum.effectivealtruism.org/tag/economic-growth) | [game theory](https://forum.effectivealtruism.org/tag/game-theory) | [global health and development](https://forum.effectivealtruism.org/tag/global-health-and-development) | [welfare economics](https://forum.effectivealtruism.org/tag/welfare-economics)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "78sSkbNRimvYJcCxh",
    "name": "Education",
    "core": false,
    "slug": "education",
    "oldSlugs": [
      "education-cause-area"
    ],
    "postCount": 35,
    "description": {
      "markdown": "Use this tag for discussions of **education** in developed or developing countries as a potential cause area. For posts related to teaching EA concepts, use [effective altruism education](https://forum.effectivealtruism.org/tag/effective-altruism-education). \n\nFurther reading\n---------------\n\nCalvert, Callum (2019) [Cause area report: education](https://founderspledge.com/research/fp-education), Founders Pledge.\n\nGiveWell (2018) [Education in developing countries](https://www.givewell.org/international/technical/programs/education), *GiveWell*, April.\n\nWiblin, Robert & Keiran Harris (2018) [Economist Bryan Caplan thinks education is mostly pointless showing off. We test the strength of his case](https://80000hours.org/podcast/episodes/bryan-caplan-case-for-and-against-education/), *80,000 Hours*, May 22."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "EHXNGqENrziYQJsox",
    "name": "AI forecasting",
    "core": false,
    "slug": "ai-forecasting",
    "oldSlugs": [
      "ai-forecasting"
    ],
    "postCount": 115,
    "description": {
      "markdown": "**AI forecasting** is the process of trying to predict a variety of outcomes related to AI, such as when human-level AI will emerge (**AI timelines**) and what impacts it will have.\n\nFurther reading\n---------------\n\nCotra, Ajeya (2020) [Draft report on AI timelines](https://www.lesswrong.com/posts/KrJfoZzpSDpnrv9va/draft-report-on-ai-timelines), *LessWrong*, September 18.\n\nDavidson, Tom (2021) [Semi-informative priors over AI timelines](https://www.openphilanthropy.org/semi-informative-priors), *Open Philanthropy*, March 25.\n\nGruetzemacher, Ross *et al.* (2020) [Forecasting AI progress: a research agenda](http://arxiv.org/abs/2008.01848), arXiv:2008.01848 \\[Cs\\].\n\nKoehler, Arden, Robert Wiblin & Keiran Harris (2020) [Danny Hernandez on forecasting and the drivers of AI progress](https://80000hours.org/podcast/episodes/danny-hernandez-forecasting-ai-progress/), *80,000 Hours*, May 22.\n\nWiblin, Robert & Keiran Harris (2018) [How well can we actually predict the future? Katja Grace on why expert opinion isn’t a great guide to AI’s impact and how to do better](https://80000hours.org/podcast/episodes/katja-grace-forecasting-technology/), *80,000 Hours*, August 21.\n\nExternal links\n--------------\n\n[AI Forecasting Dictionary](https://parallel-forecast.github.io/AI-dict/docs/dictionary.html). A set of standards and conventions for precisely interpreting AI and auxiliary terms.\n\n[AI timelines](https://aiimpacts.org/category/ai-timelines/). List of publications by [AI Impacts](https://forum.effectivealtruism.org/tag/ai-impacts).\n\nRelated entries\n---------------\n\n[AI governance](https://forum.effectivealtruism.org/topics/ai-governance) | [AI alignment](https://forum.effectivealtruism.org/tag/ai-alignment) | [AI Impacts](https://forum.effectivealtruism.org/tag/ai-impacts) | [estimation of existential risk](https://forum.effectivealtruism.org/tag/estimation-of-existential-risk) | [forecasting](https://forum.effectivealtruism.org/tag/forecasting) | [long-range forecasting](https://forum.effectivealtruism.org/tag/long-range-forecasting) | [quantum computing](https://forum.effectivealtruism.org/topics/quantum-computing) | [tabletop exercises](https://forum.effectivealtruism.org/topics/tabletop-exercises)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "ebQCN6HzQgndSAi86",
    "name": "Center for Election Science",
    "core": false,
    "slug": "center-for-election-science",
    "oldSlugs": null,
    "postCount": 16,
    "description": {
      "markdown": "The **Center for Election Science** (**CES**) is a nonprofit organization that campaigns for [electoral reform](https://forum.effectivealtruism.org/tag/electoral-reform) in the United States. It advocates cardinal voting methods, with an emphasis on approval voting.\n\nActivities\n----------\n\nCES was involved in successful campaigns to help pass approval voting in the cities of Fargo, North Dakota and St. Louis, Missouri.^[\\[1\\]](#fn7rg9q46fd7p)^^[\\[2\\]](#fnqefsena7wl)^ In November 2021, CES launched a campaign to get approval voting adopted in Seattle, Washington in the November 2022 election.^[\\[3\\]](#fnezmsyrvg9ha)^ As of June 2022, the campaign has gathered the signatures necessary to include a [ballot initiative](https://forum.effectivealtruism.org/tag/ballot-initiative) on the November ballot.^[\\[4\\]](#fn8niec1eiejm)^\n\nFunding\n-------\n\nAs of August 2022, CES has received over $2.4 million in grants from [Open Philanthropy](https://forum.effectivealtruism.org/tag/open-philanthropy),^[\\[5\\]](#fnq9d7r9odgpj)^ $300,000 from the [Future Fund](https://forum.effectivealtruism.org/topics/future-fund),^[\\[6\\]](#fndmeumzg6hn)^ $100,000 from [Effective Altruism Funds](https://forum.effectivealtruism.org/topics/effective-altruism-funds),^[\\[7\\]](#fnkfnjl8ee28)^^[\\[8\\]](#fnmpyzn1jv00a)^ and over $40,000 from the [Survival and Flourishing](https://forum.effectivealtruism.org/topics/survival-and-flourishing) Fund.^[\\[9\\]](#fnbpkqbkc2u9f)^\n\nFurther reading\n---------------\n\nWiblin, Robert & Keiran Harris (2018) [Politics is so much worse because we use an atrocious 18th century voting system. Aaron Hamlin has a viable plan to fix it](https://80000hours.org/podcast/episodes/aaron-hamlin-voting-reform/), *80,000 Hours*, May 31.\n\nExternal links\n--------------\n\n[Center for Election Science](https://electionscience.org/). Official Website.\n\n[Apply for a job](https://electionscience.org/about/careers-and-board/).\n\n[Donate to the Center for Election Science](https://electionscience.org/donate/).\n\nRelated entries\n---------------\n\n[electoral politics](https://forum.effectivealtruism.org/tag/electoral-politics) | [electoral reform](https://forum.effectivealtruism.org/tag/electoral-reform) | [United States](https://forum.effectivealtruism.org/tag/united-states)\n\n1.  ^**[^](#fnref7rg9q46fd7p)**^\n    \n    Piper, Kelsey (2018) [This city just approved a new election system never tried before in America](https://www.vox.com/future-perfect/2018/11/15/18092206/midterm-elections-vote-fargo-approval-voting-ranked-choice), *Vox*, November 15.\n    \n2.  ^**[^](#fnrefqefsena7wl)**^\n    \n    Rakich, Nathaniel (2021) [In St. Louis, voters will get to vote for as many candidates as they want](https://fivethirtyeight.com/features/in-st-louis-voters-will-get-to-vote-for-as-many-candidates-as-they-want/), *FiveThirtyEight*, March 1.\n    \n3.  ^**[^](#fnrefezmsyrvg9ha)**^\n    \n    Piel, Michael (2021) [The new frontier: Seattle approves launches a ballot initiative campaign](https://electionscience.org/ces-updates/the-new-frontier-seattle-approves-launches-a-ballot-initiative-campaign/), *The Center for Election Science*, November 17.\n    \n4.  ^**[^](#fnref8niec1eiejm)**^\n    \n    Raleigh, Chris (2022) [Congratulations Seattle Approves: Approval voting measure qualifies for city ballot this fall](https://electionscience.org/uncategorized/congratulations-seattle-approves-approval-voting-measure-qualifies-for-city-ballot-this-fall/), *The Center for Election Science*, June 16.\n    \n5.  ^**[^](#fnrefq9d7r9odgpj)**^\n    \n    Open Philanthropy (2022) [Grants database: The Center for Election Science](https://www.openphilanthropy.org/grants/?q=&organization-name=the-center-for-election-science), *Open Philanthropy*.\n    \n6.  ^**[^](#fnrefdmeumzg6hn)**^\n    \n    Future Fund (2022) [Our grants and investments: The Center for the Election Science](https://ftxfuturefund.org/our-grants/?_organization_name=the-center-for-election-science), *Future Fund*.\n    \n7.  ^**[^](#fnrefkfnjl8ee28)**^\n    \n    Long-Term Future Fund (2020) [September 2020: Long-Term Future Fund grants](https://funds.effectivealtruism.org/funds/payouts/september-2020-long-term-future-fund-grants), *Effective Altruism Funds*, September.\n    \n8.  ^**[^](#fnrefmpyzn1jv00a)**^\n    \n    Long-Term Future Fund (2022) [December 2021: Long-Term Future Fund grants](https://funds.effectivealtruism.org/funds/payouts/december-2021-long-term-future-fund-grants), *Effective Altruism Funds*, August.\n    \n9.  ^**[^](#fnrefbpkqbkc2u9f)**^\n    \n    Survival and Flourishing Fund (2019) [SFF-2020-H2 S-process recommendations announcement](https://survivalandflourishing.fund/sff-2020-h2-recommendations), *Survival and Flourishing Fund*."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "JtLxJQPbSJA9CCNj9",
    "name": "Style guide",
    "core": false,
    "slug": "style-guide",
    "oldSlugs": null,
    "postCount": 2,
    "description": {
      "markdown": "This ***Style Guide*** describes the stylistic rules to which all EA Wiki articles should conform. **We are definitely happy for you to begin contributing to the Wiki without having learned these rules.** We would much rather have good content we need to reformat than no content at all, and we are grateful for your contributions either way.\n\nThe document is heavily based on Wikipedia's [*Manual of Style*](https://en.wikipedia.org/wiki/Wikipedia:Manual_of_Style), and a few parts of it are directly copied from that article. However, it also deviates from it in a number of respects, and is considerably shorter. If you are broadly familiar with Wikipedia's conventions already, you may want to skim liberally and just focus on the sections that present you with new information.\n\nThe *Style Guide* is a work in progress, and it will be revised and expanded over time. If you have questions about matters of style not answered by this document, or answered inadequately, feel free to make an edit or leave a comment.\n\nOrganization of articles\n========================\n\nTitles\n------\n\nThe title of an article should be a precise, concise and recognizable name or description of the topic of the article. Use [sentence case](https://en.wikipedia.org/wiki/Letter_case#Sentence_case), and avoid articles at the beginning and punctuation marks at the end,  unless doing so would result in the removal of an inseparable part of the name.\n\n> History of effective altruism\n> \n> ~The history of effective altruism~\n> \n> *The Life You Can Save*\n> \n> *~Life You Can Save~*\n\nIf the title is a name that has an associated  acronym, use the acronym only if that is how the subject is primarily known.\n\n> ALLFED\n> \n> Animal Charity Evaluators\n> \n> ~ACE~\n\nIn general, give preference to the singular over the plural:\n\n> existential risk\n> \n> ~existential risks~\n\nIf the title involves a conjunction, use \"and\" rather than an ampersand (&).\n\nSections\n--------\n\nBegin every article with a *lead section*, or a summary of the subject, followed by a *body*.\n\nThe first sentence of the lead should contain a definition of the topic of the article. The title of the article should appear in this sentence, in boldface.\n\n> **GiveWell** is a nonprofit charity evaluator based in San Francisco.\n\nIf relevant, include alternative or former terms or spellings for the topic in parentheses. These variants should also be boldfaced.\n\n> **Open Philanthropy** (previously the **Open Philanthropy Project**) is a research and grantmaking foundation based in San Francisco.\n\nDivide the body, but never the lead itself, into sections. Each section may be further subdivided into subsections, as needed.\n\nThe sections of the body may be followed, when appropriate, with the sections *Further reading*, *External links* and *Related entries*, in that order. See [here](https://forum.effectivealtruism.org/tag/global-dystopia) and [here](https://forum.effectivealtruism.org/tag/80-000-hours) for examples.\n\nSection headings\n----------------\n\nEach section in the article, except the lead section, is associated with a *section heading*. When naming headings, follow the same rules as govern the naming of titles.\n\nTone\n====\n\nWrite in an encyclopedic tone. Avoid slang, colloquialisms, legalese or unnecessary jargon. Do not write from a first- or second-person perspective. Avoid bombastic wording, innuendo, humor, or irony.\n\nEntries should adopt a neutral point of view rather than advocate for a particular point of view. If controversy exists, do not take sides; rather, summarize the relevant views, and the evidence and arguments presented in their favor, fairly and accurately.\n\nAvoid stating as facts claims that would be disputed by someone endorsing reasonable epistemic standards. Conversely, avoid presenting a claim that would pass the above test as a mere subjective opinion.\n\nAvoid making value judgments. If contextually relevant, you may describe value judgments made by others, but never in a way that suggests endorsement.\n\nVarieties of English\n====================\n\nWhen differences in spelling, vocabulary or grammar exist between different national varieties of English, you are free to use whichever variant corresponds to the variety of English you speak, or that you prefer for any other reason. However, this is subject to the following qualifications:\n\n*   If a more universal variant exists, give preference to it, e.g. use \"glasses\" rather than \"spectacles\" (British English) or \"eyeglasses\" (American English).\n*   If an article is already written in a particular variety of English, use that variety throughout the article.\n\nAbbreviations\n=============\n\nWrite words in full when they first occur in the article, mentioning the abbreviation in parenthesis, unless an abbreviation is so familiar that it is used more often in full. \n\n> The BBC \n> \n> ~The British Broadcasting Corporation~\n\nUse the abbreviated form throughout the rest of the article, but also consider alternative ways of referring to the entity in question to avoid unnecessary proliferation of capital letters.\n\nThe terms 'effective altruism', 'existential risk' and other expressions commonly abbreviated in informal discussion should be spelled in full and—unless they occur as part of a name—in lowercase.\n\n> ~Criticism of EA~\n> \n> ~The Effective Altruism movement~\n> \n> ~anthropogenic x-risk~\n> \n> The Centre for Effective Altruism\n> \n> The Centre for the Study of Existential Risk\n\nEmphasis markers\n================\n\nAs noted in the *Organization of articles* section, the article's title, as well as variant spellings, should appear in boldface. Otherwise, you should use only italics—rather than boldface or capitals—for emphasis.\n\nThe use of italics for citing works is covered in *References*.\n\nQuotations\n==========\n\nEnclose short quotations in double quotation marks, and use [block quotations](https://en.wikipedia.org/wiki/Block_quotation) for longer quotations. We consider a quotation short if it consists of less than one full paragraph and at most 40 words, and longer otherwise.\n\nFor short quotations, the citation should follow the quotation.\n\n> In Jan Naverson's famous dictum, \"We are in favor of making people happy, but neutral about making happy people.\"^[\\[1\\]](#fnvm7n2wmtj0r)^\n\nFor long quotations, the citation should precede it.\n\n> Derek Parfit has expressed a version of this view :^[\\[2\\]](#fnh46e3s0602)^\n\n> >If we are the only rational beings in the Universe, as some recent evidence suggests, it matters even more whether we shall have descendants or successors during the billions of years in which that would be possible. Some of our successors might live lives and create worlds that, though failing to justify past suffering, would have given us all, including those who suffered most, reasons to be glad that the Universe exists.\n\nPunctuation\n===========\n\n*To keep the length of this document within reasonable boundaries, there are many punctuation rules we do not cover here. We just focus on the most important rules, or those where we expect most uncertainty to exist. Please refer to the* [*corresponding section*](https://en.wikipedia.org/wiki/Wikipedia:Manual_of_Style#Punctuation) *of Wikipedia's manual of style for further details.*\n\nUse [logical punctuation](https://en.wikipedia.org/wiki/Quotation_marks_in_English#Logical_quotation), always keeping periods and commas inside the quotation marks when they are meant to apply to the quoted material.\n\n> Steven Pinker says you should use logical punctuation \"if you write for Wikipedia or another tech-friendly platform\", or \"if you have a temperament that is both logical and rebellious\".\n\nWhen a quotation occurs inside another quotation enclosed in quotation marks, use single quotes for the inside quotation. Otherwise always use double quotes.\n\n> The EA Wiki *Style Guide* should really stop trying to be funny by using examples such as \"Steven Pinker encourages the use of logical punctuation \\[...\\] 'if you have a temperament that is both logical and rebellious'.\"\n\nAs illustrated by the above example, inessential parts of a quotation may be omitted by enclosing the ellipsis in square brackets.\n\nThe use of [serial commas](https://en.wikipedia.org/wiki/Serial_comma) is typically optional. However, serial commas should be either used or avoided when this is necessary to avoid syntactic ambiguity.\n\n> ~This book is dedicated to my parents, Ayn Rand and God.~\n> \n> This book is dedicated to my parents, Ayn Rand, and God.\n\nTo mark divisions within a sentence, either singly or in pairs (parenthetical dashes), use em dashes (—) with no spaces on either side.\n\n> you should use only italics—rather than boldface or capitals—for emphasis\n\nTo insert an em dash on MacOS, press `Option-Shift-Hyphen`; to do so on Windows, type `0151` while holding `Alt`. You can also use a shortcut application like [AutoControl Shortcut Manager](https://www.autocontrol.app/) or [AutoHotkey](https://www.autohotkey.com/) to make this punctuation easier to access.\n\nDates and time\n==============\n\nDates\n-----\n\nWrite dates in the formats \"1 January 2021\" (full date), \"January 1\" (month and day), and \"January 2021\" (month and year). Do not write \"1st\",  \"2nd\", and so on. Note that the separator is always a space and never a comma.\n\nFor decades, use \"the 1920s\" rather than \"the '20s\".\n\nFor the current date, use \"As of January 2021\" or \"As of 1 January 2021\", replacing the dates in the examples by the current date.\n\n> As of January 2021, Giving What We Can has 5,600 members.\n\nTime\n----\n\nUse either a [12-hour clock](https://en.wikipedia.org/wiki/12-hour_clock) or a [24-hour clock](https://en.wikipedia.org/wiki/24-hour_clock). Twelve-hour clock times are written in the form 8:15 am and 2:30 pm. Twenty-four-hour clock times are written in the form 08:15 and 14:30.\n\nIf the time zone is relevant, use [Coordinated Universal Time](https://en.wikipedia.org/wiki/Coordinated_Universal_Time). You may also specify the number of seconds, using a 24-hour clock.\n\n> The first nuclear device was detonated on 16 July 1945 at 11:29:45 UTC.\n\nNumbers\n=======\n\nSpell integers from zero to nine in words. Integers greater than nine expressible in one or two words may be spelled in either words or numerals.\n\n> There are four main cause areas in effective altruism.\n\n> Good Ventures has given over  $1.25 billion in grants.\n\nDo not abbreviate \"thousand\", \"million\", \"billion\" and \"trillion\".\n\nFor ranges—including date and page ranges—use an en dash (–). To insert an en dash on MacOS, press `Option-Hyphen`; to do so on Windows, type `0150` while holding `Alt`. You can also use a shortcut application like [AutoControl Shortcut Manager](https://www.autocontrol.app/) or [AutoHotkey](https://www.autohotkey.com/) to make this punctuation easier to access.\n\nFor large numbers, use scientific notation.\n\n> ~what hangs in the balance is at least 10,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000 human lives (though the true number is probably larger).~\n> \n> What hangs in the balance is at least 10^58^ human lives (though the true number is probably larger).\n\nCurrencies\n==========\n\nUnless the context requires the use of a different currency, express amounts of money in United States dollars, euros, or pounds sterling, and indicate the currency by the symbols $, € and £, respectively. Use only one symbol with ranges separated by a dash, and do not insert a space between the symbol and the number.\n\n> A donation of $3000–5000 to the Against Malaria Foundation can avert the death of a child under five.\n\nFor other currencies, use [ISO 4217](https://en.wikipedia.org/wiki/ISO_4217) codes. The code should precede the currency amount, with a space separating the two.\n\nUnits of measurement\n====================\n\nUse the metric system and, more generally, the [International System of Units](https://en.wikipedia.org/wiki/International_System_of_Units) and the [accepted Non-SI units](https://en.wikipedia.org/wiki/Non-SI_units_mentioned_in_the_SI). Unit names should be given in full if used only a few times, but symbols may be used when a unit (especially one with a long name) is used repeatedly after spelling out the first use.  Do not insert a period after the symbol, unless a period is required for other reasons. Use \"per\" when writing out a unit, rather than a slash.\n\nMathematical symbols\n====================\n\nFor mathematical symbols, consider using LaTeX. You can use\\\\(\\\\) `Ctrl-4` (Windows) or `Cmd-4` (Mac) to open a LaTeX prompt in the Forum’s editor. Otherwise\n\n*   for a negative sign or subtraction operator, use a minus sign (−).\n*   for multiplication, use a multiplication sign (×) or a dot operator (⋅).\n*   for exponentiation, use superscript.\n\nVocabulary\n==========\n\nAvoid contractions. Use \"is not\", \"cannot\", \"will not\" instead of \"isn't\", \"can't\", and \"won't\", etc.\n\nUse [gender-neutral language](https://en.wikipedia.org/wiki/Wikipedia:Gender-neutral_language). To avoid generic masculine and generic feminine pronouns, consider the following approaches:\n\n*   *Pluralization.* By turning a phrase from singular to plural, gender neutrality is insured since plural forms in English do not change with gender. However, for a number of different reasons, this option is not always available or advisable.\n*   *Disjunction*. Another approach is to use both singular forms, with the expression \"he or she\" or their cognates (\"Each politician is responsible for his or her constituency\"). However, such turns of phrase can be ungainly or tedious if repeated many times within a short space.\n*   *Avoidance*. As a third alternative, consider rewording the phrase in a way that makes no use of pronouns.\n\nSome Forum users have suggested using the [singular *they*](https://en.wikipedia.org/wiki/Singular_they). Reference works do not have a consistent position on this, so we tentatively leave the decision to the discretion of contributors. If you have suggestions concerning gender-neutral language, please leave them in the comments below.\n\nLinks\n=====\n\nInternal links\n--------------\n\nInternal links are links to other articles of the Wiki. As a general rule, if a term occurs for which there is a Wiki entry, or refers to the topic of an entry without explicitly mentioning it, you should link to this entry. However, you should not create an internal link if this would result in repeated links to the same article within the span of a few paragraphs or so. In these cases, readers are likely to have already been exposed to that link, so there is no need to add another one.\n\nExternal links\n--------------\n\nExternal links are all links other than internal links. They include links both to other websites, and to pages within [effectivealtruism.org](http://effectivealtruism.org) that are not Wiki articles.\n\nExternal links should only be used in the *Further reading* and *External links* sections of the article, and in footnotes, but never in the lead or body sections (see the *Organization of articles* section).\n\nIn the *External links* section, add links each in a separate line. The title of the link should be the name of the webpage to which the link points. Next to the title, add a brief description of that link; often, the description should just be \"official website\". See [here](https://forum.effectivealtruism.org/tag/80-000-hours) and [here](https://forum.effectivealtruism.org/tag/wild-animal-initiative) for examples.\n\nRelated entries\n===============\n\nThis section should list the entries most closely related to the topic of the article. If an entry is already linked to in the body of the article, you may still include it in this section, provided that it is sufficiently related.\n\nNames should be listed in lowercase, in alphabetical order, and separated by a vertical bar (|) with spaces on either side. Do not add a period after the final entry. See [here](https://forum.effectivealtruism.org/tag/global-dystopia) and [here](https://forum.effectivealtruism.org/tag/patient-altruism) for examples.\n\nCitations\n=========\n\nFormatting citations can be quite time-consuming. We do not require contributors to format citations properly: you are only asked to provide the minimal details necessary to identify the relevant work, and we will deal with the rest. Alternatively, you can add citations in the appropriate format, without having to format them manually, by using the Wiki's associated Citation Language File, as explained [here](https://forum.effectivealtruism.org/topics/ea-wiki-faq?sortedBy=#I_would_like_to_add_references__but_it_is_tedious_to_do_this_by_hand__I_m_also_confused_by_your_citation_format__Is_there_a_way_to_make_this_whole_thing_less_annoying_).\n\nCitations in a Wiki article have two parts: first, a *footnote number* next to the text that is being quoted or referenced; and, second, a *footnote* where the details of the work are provided in full.\n\nFootnote numbers\n----------------\n\nFootnote numbers should immediately follow the text for which the citation is provided. This is typically a clause, a sentence or a paragraph, or the elements of a range or an enumeration.\n\n> In its current form, the technology was first described by [Carl Shulman](https://forum.effectivealtruism.org/tag/carl-shulman) in 2009,^[\\[3\\]](#fnhmth9jitt7v)^ and the idea was further developed in a 2014 paper by Shulman and [Nick Bostrom](https://forum.effectivealtruism.org/tag/nick-bostrom).^[\\[4\\]](#fn7jbte1e8lqw)^\n\n> Public approval for preimplantation genetic diagnosis for intelligence has been found to range from 13%^[\\[5\\]](#fnlbu07jo5nd)^ to 19%^[\\[6\\]](#fn3ch5pgsse54)^ to 28%^[\\[7\\]](#fnuilbfnz8qmf)^.\n\nMultiple references should be separated using individual footnote numbers corresponding to each of  them.\n\n> At around that time, while preparing to transition from journalism to philanthropy, Tuna read [Peter Singer](https://forum.effectivealtruism.org/tag/peter-singer)'s [*The Life You Can Save*](https://forum.effectivealtruism.org/tag/the-life-you-can-save-book), which introduced her \"to the idea of not just trying to do some good with your giving, but doing as much good as you can.\"^[\\[8\\]](#fn91pxvphzs8w)^^[\\[9\\]](#fnylwwr619yga)^^[\\[10\\]](#fnvo25fg1n5qr)^\n\nFootnotes\n---------\n\nFootnotes should provide the details of the works cited. When possible, specify the relevant page, chapter, or section numbers. When referring to pages, use  abbreviations \"p.\" or \"pp.\" When chapters are being referred to, indicate that by using the abbreviations \"ch.\"/\"chs.\"\n\n> Just as one may wonder why undesired dystopias would exist, one may wonder why desired dystopias would be dystopian. Here a relevant example has been provided by [Nick Bostrom](https://forum.effectivealtruism.org/tag/nick-bostrom).^[\\[11\\]](#fnpr28okr4x6g)^^[\\[12\\]](#fnj8zhxg7sm8i)^\n\nSee the next section for more precisions about citation formatting.\n\nFurther reading\n---------------\n\nThis section includes works especially helpful for users interested in reading further. Below these suggested readings, you may include a sentence summarizing the work's contents. (To add such a line using the editor, without creating an extra space, press `Shift + Enter`.)\n\n> Selgelid, Michael J. (2016) [Gain-of-Function research: Ethical analysis](http://doi.org/10.1007/s11948-016-9810-1), *Science and Engineering Ethics*, vol. 22, pp. 923–964.  \n> *A paper outlining the main moral considerations surrounding gain-of-function research.*\n\nPosts from the EA Forum are eligible for inclusion, just like any other work, despite the fact that these posts would typically also be tagged and therefore show up below the article. The bar for tagging a post is lower than for adding a post to the article's list of recommended readings, so in general a small subset of posts tagged should appear in this section.\n\nIt is tedious to specify all the rules that govern how the different types of work should be cited. Below, we provide sufficient examples to allow contributors to infer the underlying rules, followed by a series of notes that make the most important rules explicit.\n\n### Books\n\n> Ord, Toby (2020) [*The Precipice: Existential Risk and the Future of Humanity*](https://en.wikipedia.org/wiki/Special:BookSources/1526600218), London: Bloomsbury Publishing.\n\n### Anthologies\n\n> Bostrom, Nick & Milan M. Ćirković (eds.) (2008) [*Global Catastrophic Risks*](https://en.wikipedia.org/wiki/Special:BookSources/9780198570509), Oxford: Oxford University Press.\n\n### Theses\n\n> Beckstead, Nick (2013) [*On the Overwhelming Importance of Shaping the Far Future*](http://doi.org/10.7282/T35M649T), PhD thesis, Rutgers University.\n\n### Papers\n\n> North, Ace R., Austin Burt & H. Charles J. Godfray (2019) [Modelling the potential of genetic control of malaria mosquitoes at national scale](http://doi.org/10.1186/s12915-019-0645-5), *BMC Biology*, vol. 17, pp. 1–12.\n\n### Blogs\n\n> Diabate, Abdoulaye (2019) [Target Malaria proceeded with a small-scale release of genetically modified sterile male mosquitoes in Bana, a village in Burkina Faso](https://targetmalaria.org/target-malaria-proceeded-with-a-small-scale-release-of-genetically-modified-sterile-male-mosquitoes-in-bana-a-village-in-burkina-faso/), *Target Malaria's Blog*, July 1.\n\n### Websites\n\n> Open Philanthropy (2016) [Center for global development — general support 2016](https://www.openphilanthropy.org/focus/global-health-and-development/miscellaneous/center-global-development-general-support-2016), *Open Philanthropy*, February.\n\n### Magazines\n\n> Tuna, Cari (2008) [Denzel charms Silliman students with ‘sexy smile’](https://yaledailynews.com/blog/2008/04/25/denzel-charms-silliman-students-with-sexy-smile/), *Yale Daily News*, April 25.\n\n### Newspapers\n\n> Vastag, Brian (2012) [‘Radical’ bill seeks to reduce cost of AIDS drugs by awarding prizes instead of patents](https://www.washingtonpost.com/national/health-science/radical-bill-seeks-to-reduce-cost-of-aids-drugs-by-awarding-prizes-instead-of-patents/2012/05/19/gIQAEGfabU_story.html), *The Washington Post*, May 19.\n\n### Book chapters\n\n> Jamison, Dean T. *et al.* (2013) [Infectious disease, injury, and reproductive health](http://doi.org/10.1017/CBO9781139600484.009), in Bjørn Lomborg (ed.) *Global Problems, Smart Solutions: Costs and Benefits*, Cambridge: Cambridge University Press, pp. 390–438.\n\n### Working papers\n\n> Wilkinson, Hayden (2020) [In defence of fanaticism](https://globalprioritiesinstitute.org/wp-content/uploads/Hayden-Wilkinson_In-defence-of-fanaticism.pdf), GPI working paper no. 4-2020, Global Priorities Institute, University of Oxford.\n\n### Reports\n\n> Sandberg, A. & Nick Bostrom (2008) [Global catastrophic risks survey](https://www.fhi.ox.ac.uk/reports/2008-1.pdf), technical report no. 2008-1, Future of Humanity Institute, University of Oxford.\n\n### Interviews\n\n> Koehler, Arden, Robert Wiblin & Keiran Harris (2020) [Hilary Greaves on Pascal’s mugging, strong longtermism, and whether existing can be good for us](https://80000hours.org/podcast/episodes/hilary-greaves-comparing-existence-and-non-existence/), *80,000 Hours*, October 21.\n\n### Conversations\n\nCrispin, Natalie, Teryn Maddox & Tom Adamczewski (2020) [A conversation with Dr. James Tibenderana, Helen Counihan, Maddy Marasciulo and Dr. Arantxa Roca](https://docs.google.com/document/d/1i_HC5C8Y310Z2nRYmx-pCQIozehCZWlrzgTxRNaQgDI/edit), *GiveWell*, May 11.\n\n### Videos\n\n> Dalton, Max & Jonas Volmer (2018) [How to avoid accidentally having a negative impact with your project](https://www.youtube.com/watch?v=RU168E9fLIM), *Effective Altruism Global*, October 27.\n\n### Comments\n\n> Rice, Issa (2019) [Comment on 'Cause X guide'](https://forum.effectivealtruism.org/posts/kFmFLcdSFKo2GFJkc/cause-x-guide?commentId=dqCy9FEuLDP25hw9a), *Effective Altruism Forum*, September 1.\n\n### Unpublished works\n\n> Arrhenius, Gustaf (2021) *Population Ethics*, Oxford: Oxford University Press, forthcoming.\n> \n> Parfit, Derek (1988) 'On giving priority to the worse off', unpublished.\n\nNotes\n-----\n\nUse italics for *major works* (books, journals, magazines, newspapers, and websites). Use simple quotation marks for *minor works* (chapters, papers, articles, posts, and web pages) if the title is not also a hyperlink, otherwise omit the quotations.\n\n> *Doing Good Better*\n> \n> [*Astral Codex Ten*](https://astralcodexten.substack.com/)\n> \n> 'Famine, affluence, and morality'\n> \n> [Beware surprising and suspicious convergence](https://forum.effectivealtruism.org/posts/omoZDu8ScNbot6kXS/beware-surprising-and-suspicious-convergence)\n\nUse *title case* for major works and *sentence case* for minor works.\n\nList each work in a separate line ending with a period and no extra line breaks.\n\nFor interviews (including GiveWell and Open Phil \"conversations\"), do not include the interviewers in the list of authors, since usually their names are mentioned in the title.\n\n> Koehler, Arden, Robert Wiblin & Keiran Harris (2020) [Hilary Greaves on Pascal’s mugging, strong longtermism, and whether existing can be good for us](https://80000hours.org/podcast/episodes/hilary-greaves-comparing-existence-and-non-existence), *80,000 Hours*, October 21.\n> \n> Crispin, Natalie, Teryn Maddox & Tom Adamczewski (2020) [A conversation with Dr. James Tibenderana, Helen Counihan, Maddy Marasciulo and Dr. Arantxa Roca, May 11, 2020](https://docs.google.com/document/d/1i_HC5C8Y310Z2nRYmx-pCQIozehCZWlrzgTxRNaQgDI/edit), *GiveWell*, May 11.\n\nList all authors of a work if the work has three or fewer authors, and otherwise only list the first author followed by \"*et al.*\" (in italics and with a period at the end). Use an ampersand (&) to separate the last two authors and otherwise use a comma. Only for the first author should the last name precede the first name.\n\nUse \"vol.\" and \"p.\" to indicate volume and page, respectively. For volume or page ranges, use \"vols.\" and \"pp.\", with the first and last volume or page in the range separated by an en dash (–). (As noted in the *Numbers* section, such dashes should be used for all numerical ranges.)\n\nIf a web page does not credit an author, list the name of the website.\n\nFor interviews, web pages, newspaper articles and magazine articles, provide the month and day of publication, when available.\n\nIf a work includes both the date of publication and the date it was most recently updated, cite it using the former, but append the date of update parenthetically, like this:\n\n> Tomasik, Brian (2009) [Do bugs feel pain?](https://reducing-suffering.org/do-bugs-feel-pain), *Essays on Reducing Suffering*, April 7 (updated 28 July 2017).\n\nProvide links to all works cited. These links should be attached to the entire title of the work, and should be constructed as follows:\n\n*   If the work has an associated digital object identifier (DOI), use a URL of the form `http://doi.org/number`\n*   If the work lacks a DOI but has an ISBN, use a URL of the form `https://en.wikipedia.org/wiki/Special:BookSources/ISBN`\n    *   If the work is a book chapter, use the ISBN of the book containing that chapter. The link should still be attached to the title of the work—in this case, the book chapter—rather than to the book itself.\n*   If the work lacks both a DOI and an ISBN but otherwise has an associated canonical URL, use that URL. (This will typically be the case with web pages, newspaper articles, and magazine articles.)\n    *   If the canonical URL no longer works, link to the version archived on the [Wayback Machine](http://web.archive.org/), if it exists. Otherwise do not include a link.\n*   If the work lacks a DOI, ISBN and canonical URL, do not include a link. (This will typically be the case with books published before 1967, when ISBNs were first issued.)\n\n1.  ^**[^](#fnrefvm7n2wmtj0r)**^\n    \n    Narveson, Jan (1973) [Moral problems of population](http://doi.org/10.5840/monist197357134), *Monist*, vol. 57, pp. 62–86, p. 80.\n    \n2.  ^**[^](#fnrefh46e3s0602)**^\n    \n    Parfit, Derek (2011) [*On What Matters*](https://en.wikipedia.org/wiki/Special:BookSources/978-0-19-926592-3), vol. 3, Oxford: Oxford University Press, pp. 436–437.\n    \n3.  ^**[^](#fnrefhmth9jitt7v)**^\n    \n    Shulman, Carl (2009) [What is multi-generational in vitro embryo selection?](http://theuncertainfuture.com/faq.html#7), *The Uncertain Future*.\n    \n4.  ^**[^](#fnref7jbte1e8lqw)**^\n    \n    Shulman, Carl & Nick Bostrom (2014) [Embryo selection for cognitive enhancement: curiosity or game-changer?](http://doi.org/10.1111/1758-5899.12123), *Global Policy*, vol. 5, pp. 85–92.\n    \n5.  ^**[^](#fnreflbu07jo5nd)**^\n    \n    Hathaway, Feighanne, Esther Burns & Harry Ostrer (2009) [Consumers’ desire towards current and prospective reproductive genetic testing](http://doi.org/10.1007/s10897-008-9199-3), *Journal of Genetic Counseling*, vol. 18, pp. 137–146, p. 140.\n    \n6.  ^**[^](#fnref3ch5pgsse54)**^\n    \n    Winkelman, William D. *et al.* (2015) [Public perspectives on the use of preimplantation genetic diagnosis](http://doi.org/10.1007/s10815-015-0456-8), *Journal of Assisted Reproduction and Genetics*, vol. 32, pp. 665–675, p. 668.\n    \n7.  ^**[^](#fnrefuilbfnz8qmf)**^\n    \n    Kalfoglou, A. *et al.* (2004) [Reproductive genetic testing: What America thinks](http://web.archive.org/web/20060620032617/http://www.dnapolicy.org/images/reportpdfs/ReproGenTestAmericaThinks.pdf), Genetics and Public Policy Center, p. 11.\n    \n8.  ^**[^](#fnref91pxvphzs8w)**^\n    \n    Tuna, Cari (2011) [Guest post from Cari Tuna](https://blog.givewell.org/2011/12/23/guest-post-from-cari-tuna/), *The GiveWell Blog*, December 23.\n    \n9.  ^**[^](#fnrefylwwr619yga)**^\n    \n    Preston, Caroline (2012) [Another Facebook co-founder gets philanthropic](https://www.philanthropy.com/article/another-facebook-co-founder-gets-philanthropic/), *The Chronicle of Philanthropy*, January 10.\n    \n10.  ^**[^](#fnrefvo25fg1n5qr)**^\n    \n    Gunther, Marc (2018) [Giving in the light of reason](https://ssir.org/articles/entry/giving_in_the_light_of_reason), *Stanford Social Innovation Review*.\n    \n11.  ^**[^](#fnrefpr28okr4x6g)**^\n    \n    Bostrom, Nick (2004) [The future of human evolution](https://en.wikipedia.org/wiki/Special:BookSources/9780974347226) in Charles Tandy (ed.) *Death and Anti-Death: Two Hundred Years after Kant, Fifty Years after Turing*, vol. 2, Palo Alto, California: Ria University Press, pp. 339–371.\n    \n12.  ^**[^](#fnrefj8zhxg7sm8i)**^\n    \n    Bostrom, Nick (2014) [*Superintelligence: Paths, Dangers, Strategies*](https://en.wikipedia.org/wiki/Special:BookSources/9780199678112), Oxford: Oxford University Press, pp. 172-173."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "GrahSGgWpSATFkCeQ",
    "name": "Queen's Lane Coffee House",
    "core": false,
    "slug": "queen-s-lane-coffee-house",
    "oldSlugs": null,
    "postCount": null,
    "description": {
      "markdown": "**Queen's Lane Coffee House** is a coffee house in Oxford, England. It was established in 1654 and is the oldest continually serving coffee house in Europe.\n\nCoffee houses in England first opened in Oxford in the 1650s.^[\\[1\\]](#fnsjwo4do8xp8)^ Queen's Lane Coffee House was one among several such establishments. Coffee houses enjoyed great popularity as soon as they appeared. They were also the target of harsh criticism. In a pamphlet from 1673, its anonymous author, introducing himself as \"a Lover of his country and well-wisher to the prosperity both of the King and kingdoms\", condemns \"these *Coffee* Houses \\[which have\\] done great mischiefs to the Nation, undone many of the Kings Subjects, for they being very great Enemies to Diligence and Industry, have been the ruine of many serious and hopeful young Gentlemen and Tradesmen.\"^[\\[2\\]](#fn5u035iargz)^\n\nYears later, antiquarian Anthony Wood—\"a deaf, bitter, and suspicious man\"^[\\[3\\]](#fn75odl29r3ic)^—asked: \"Why doth solid and serious learning decline, and few or none follow it now in the University? Answer, because of coffeea-houses, where they spend all their time.\"^[\\[4\\]](#fniammeogwbgq)^ And writing in 1744, the historian and biographer Roger North lamented the \"vast Loss of Time grown out of a pure Novelty. For who can apply close to a Subject with his Head full of the Din of a Coffee-house?\"^[\\[5\\]](#fnk7rju8y3lb)^\n\nIt was, however, at Queen's Lane Coffee House—called at the time **Harper's Coffee-House**—that [Jeremy Bentham](https://forum.effectivealtruism.org/tag/jeremy-bentham) first discovered [utilitarianism](https://forum.effectivealtruism.org/tag/utilitarianism). As he writes (using the third person) in an autobiographical article:^[\\[6\\]](#fn933y82ptpzj)^\n\n> Between the years 1762 and 1769 came out a pamphlet of Dr. Priestley’s, written as usual with him *currente calamo* and without any precise method predetermined, but containing at the close of it, it is believed in the very last page, in so many words the phrase ‘the greatest happiness of the greatest number’, and this was stated in the character of a principle constituting not only a rational foundation, but the only rational foundation, of all enactments in legislation and all rules and precepts destined for the direction of human conduct in private life.\n> \n> Somehow or other shortly after its publication a copy of this pamphlet found its way into the little circulating library belonging to a little coffee-house called Harper’s Coffee-house, attached as it were to Queen’s College Oxford, and deriving from the population of that College the whole of its subsistence. It was a corner house having one front towards the High Street, another towards a narrow lane which on that side skirts Queen’s College and loses itself in a lane issuing from one of the gates of New College. \\[…\\]\n> \n> \\[I\\]t was by that pamphlet and this phrase in it that his principles on the subject of morality, public and private together, were determined. It was from that pamphlet and that page of it that he drew that phrase, the words and import of which have by his writings been so widely diffused over the civilized world. At sight of it he cried out as it were in an inward ecstacy like Archimedes on the discovery of the fundamental principle of Hydrostatics, *Eureka!*\n\nPhilip Bliss, Registrar of the University of Oxford from 1824 to 1853, recounts a description, by a contemporary of Bentham's, of the coffee house scene in Oxford at around the time of Bentham's discovery:^[\\[7\\]](#fnhb21uwn28yf)^\n\n> The fashion of drinking coffee in public, prevailed in Oxford immediately upon its introduction into England, and continued to a late period. I am told by a venerable friend, now (Feb. 1848) in his 93rd year, that he well remembers the time when every academic of any fashion resorted to the coffee house during the afternoon: Tom's, nearly opposite the present market, being frequented by the most gay and expensive; Horseman's, also in the High Street, nearly opposite the house of the principal of Brasenose, received the members of Merton, All Souls, Corpus, and Orie; *Harper's, the corner house of the lane leading to Edmund hall, those of Queens and Magdalen*; Baggs's, the stone house… at the corner of Holywell, facing the King's Arms, used by New college, Hertford, and Wadham; and Malbon's, a diminutive tenement some feet below the present street at the north east corner of the Turl, was filled from trinity, and by the members of the neighbouring colleges.\n\nThe first [Effective Altruism Global](https://forum.effectivealtruism.org/tag/effective-altruism-global) conference in Oxford was held in November 2016 at the Examination Schools, just across Queen's Lane Coffee House. In his introductory presentation on the history of [effective altruism](https://forum.effectivealtruism.org/tag/effective-altruism), [William MacAskill](https://forum.effectivealtruism.org/tag/william-macaskill) highlighted the significance of that location in connection to the birthplace of utilitarianism. More recently, an online introductory textbook on utilitarianism, co-written by MacAskill and Darius Meißner, was launched from Queen's Lane Coffee House.^[\\[8\\]](#fn71qjgccg0x2)^\n\n1.  ^**[^](#fnrefsjwo4do8xp8)**^\n    \n    Standage, Tom (2013) [Social networking in the 1600s](https://www.nytimes.com/2013/06/23/opinion/sunday/social-networking-in-the-1600s.html), *The New York Times*, June 22.\n    \n2.  ^**[^](#fnref5u035iargz)**^\n    \n    Anonymous (1673) *The Grand Concern of England Explained; in Several Proposals Offered to the Consideration of the Parliament*, London.\n    \n3.  ^**[^](#fnref75odl29r3ic)**^\n    \n    The Editors of Encyclopædia Britannica (2019) [Anthony Wood](https://www.britannica.com/biography/Anthony-Wood), *Encyclopædia Britannica*.\n    \n4.  ^**[^](#fnrefiammeogwbgq)**^\n    \n    Wood, Anthony (1848) *Athenæ Oxonienses; An Exact History of Writers and Bishops Who Have Had Their Education in the University of Oxford*, vol. 1, Oxford: Ecclesiastical History Society, p. 201.\n    \n5.  ^**[^](#fnrefk7rju8y3lb)**^\n    \n    North, John (1744) *The Life of the Honourable Sir Dudley North, Knt., Commissioner of the Customs, and Afterwards of the Treasury to His Majesty King Charles the Second. And of the Honourable and Reverend Dr. John North, Master of Trinity College in Cambridge, and Greek Professor, Prebend of Westminster, and Sometime Clerk of the Closet to the Same King Charles the Second*, London: John Whiston.\n    \n6.  ^**[^](#fnref933y82ptpzj)**^\n    \n    Bentham, Jeremy (1829) [Article on utilitarianism: long version](https://en.wikipedia.org/wiki/Special:BookSources/9780198226093), in Amnon Goldworth (ed.) *Deontology Together with a Table of the Springs of Action and Article on Utilitarianism*, Oxford: Clarendon Press, 1983, pp. 291-292.\n    \n7.  ^**[^](#fnrefhb21uwn28yf)**^\n    \n    Wood, *Athenæ Oxonienses,* p. 48, fn. q; emphasis added.\n    \n8.  ^**[^](#fnref71qjgccg0x2)**^\n    \n    MacAskill, William & Darius Meißner (2020) [About us](https://www.utilitarianism.net/about), *Utilitarianism.net*."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "2rfqAu53iz9bbsZxH",
    "name": "Fanaticism",
    "core": false,
    "slug": "fanaticism",
    "oldSlugs": null,
    "postCount": 31,
    "description": {
      "markdown": "**Fanaticism** is the apparent problem faced by moral theories that rank a minuscule probability of an arbitrarily large value above a guaranteed modest amount of value.^[\\[1\\]](#fnz3tbarlnff)^^[\\[2\\]](#fnt8dxz0a2u6p)^ Some have argued that fanatical theories should be rejected and that this might undermine the case for certain philosophical positions, such as [longtermism](https://forum.effectivealtruism.org/tag/longtermism).\n\nSee also [Pascal's mugging](https://forum.effectivealtruism.org/tag/pascal-s-mugging).\n\nFurther reading\n---------------\n\nBeckstead, Nick & Teruji Thomas (2021) [A paradox for tiny probabilities and enormous values](https://globalprioritiesinstitute.org/nick-beckstead-and-teruji-thomas-a-paradox-for-tiny-probabilities-and-enormous-values/), Global Priorities Institute.\n\nWilkinson, Hayden (2022) [In defense of fanaticism](https://doi.org/10.1086/716869), *Ethics*, vol. 132, pp. 445–477.\n\nWiblin, Robert & Keiran Harris (2021) [Christian Tarsney on future bias and a possible solution to moral fanaticism](https://80000hours.org/podcast/episodes/christian-tarsney-future-bias-fanaticism/), *80,000 Hours*, May 5.\n\nRelated entries\n---------------\n\n[alternatives to expected value theory](https://forum.effectivealtruism.org/tag/alternatives-to-expected-value-theory) | [altruistic wager](https://forum.effectivealtruism.org/tag/altruistic-wager) | [decision theory](https://forum.effectivealtruism.org/tag/decision-theory) | [decision-theoretic uncertainty](https://forum.effectivealtruism.org/tag/decision-theoretic-uncertainty) | [expected value](https://forum.effectivealtruism.org/tag/expected-value) | [moral uncertainty](https://forum.effectivealtruism.org/tag/moral-uncertainty) | [naive vs. sophisticated consequentialism](https://forum.effectivealtruism.org/tag/naive-vs-sophisticated-consequentialism) | [risk aversion](https://forum.effectivealtruism.org/tag/risk-aversion)\n\n1.  ^**[^](#fnrefz3tbarlnff)**^\n    \n    Wilkinson, Hayden (2022) [In defense of fanaticism](https://doi.org/10.1086/716869), *Ethics*, vol. 132, pp. 445–477.\n    \n2.  ^**[^](#fnreft8dxz0a2u6p)**^\n    \n    Tarsney, Christian (2020) [The epistemic challenge to longtermism](https://www.dropbox.com/s/mf0lvkxxkglwhpf/Epistemic%20Challenge%20to%20Longtermism.pdf?dl=0), Global Priorities Institute, section 6.2."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "vZyM7Tk69Rcvp3TTF",
    "name": "Humor",
    "core": false,
    "slug": "humor-1",
    "oldSlugs": [
      "april-fool",
      "april-fools-day"
    ],
    "postCount": 39,
    "description": {
      "markdown": "Related entries\n---------------\n\n[April Fools' Day](https://forum.effectivealtruism.org/tag/april-fools-day-1)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "qcQXDXBW8gYHzYngc",
    "name": "Effective altruism culture",
    "core": false,
    "slug": "effective-altruism-culture",
    "oldSlugs": [
      "ea-culture"
    ],
    "postCount": 47,
    "description": {
      "markdown": "**Effective altruism culture** is the set of norms and beliefs of the effective altruism community.\n\nRelated entries\n---------------\n\n[building effective altruism](https://forum.effectivealtruism.org/tag/building-effective-altruism-1) | [criticism of the effective altruism community](https://forum.effectivealtruism.org/tag/criticism-of-the-effective-altruism-community) | [discussion norms](https://forum.effectivealtruism.org/tag/discussion-norms) | [philosophy of effective altruism](https://forum.effectivealtruism.org/topics/philosophy-of-effective-altruism)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "KJqf4FNc2eezNBeYv",
    "name": "Blockchain technology",
    "core": false,
    "slug": "blockchain-technology",
    "oldSlugs": [
      "blockchain"
    ],
    "postCount": 33,
    "description": {
      "markdown": "The **blockchain technology** tag is for posts relevant to blockchain, crytocurrencies, and related topics, either as investment opportunities or as tools that could be used for achieving altruistic objectives (e.g., via mechanism design or solving coordination problems)."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "vM9C2kQS5i45zmiBd",
    "name": "Non-humans and the long-term future",
    "core": false,
    "slug": "non-humans-and-the-long-term-future",
    "oldSlugs": [
      "non-humans-and-the-long-term-future"
    ],
    "postCount": 41,
    "description": {
      "markdown": "The **non-humans and the long-term future** tag is for posts relevant to questions such as:\n\n*   To what extent (if at all) is [longtermism](https://forum.effectivealtruism.org/tag/longtermism) focused only on humans?\n*   To what extent should longtermists focus on improving wellbeing (or other outcomes) for humans? For other animals? For [artificial sentiences](https://forum.effectivealtruism.org/tag/artificial-sentience)? For something else?\n*   Are [existential risks](https://forum.effectivealtruism.org/tag/existential-risk) just about humans?\n*   Will most [moral patients](https://forum.effectivealtruism.org/tag/moral-patienthood) in the long-term future be humans? Other animals? Something else? By how large a margin?\n\nFurther reading\n---------------\n\nAnthis, Jacy Reese & Eze Paez (2021) [Moral circle expansion: A promising strategy to impact the far future](http://doi.org/10.1016/j.futures.2021.102756), *Futures*, vol. 130.\n\nBaumann, Tobias (2020) [Longtermism and animal advocacy](https://centerforreducingsuffering.org/longtermism-and-animal-advocacy/), *Center for Reducing Suffering*, November 11.\n\nFeinberg, Joel (1983) [The rights of animals and unborn generations](https://en.wikipedia.org/wiki/Special:BookSources/978-0-8203-0343-7), in William T. Blackstone (ed.) *Philosophy and Environmental Crisis*, Athens, Atlanta: University of Georgia Press, pp. 43–68.\n\nFreitas-Groff, Zach (2021) [Longtermism in animal advocacy](https://www.youtube.com/watch?v=QHkLJwY3cj4), *Animal Charity Evaluators*, March 31.\n\nOwe, Andrea & Seth D. Baum (2021) [Moral consideration of nonhumans in the ethics of artificial intelligence](https://doi.org/10.1007/s43681-021-00065-0), *AI and Ethics*, vol. 1, pp. 517–528.\n\nRowe, Abraham (2020) [Should longtermists mostly think about animals?](https://forum.effectivealtruism.org/posts/W5AGTHm4pTd6TeEP3/should-longtermists-mostly-think-about-animals), *Effective Altruism Forum*, January 3.\n\nRelated entries\n---------------\n\n[artificial sentience](https://forum.effectivealtruism.org/tag/artificial-sentience) | [farmed animal welfare](https://forum.effectivealtruism.org/tag/farmed-animal-welfare) | [longtermism](https://forum.effectivealtruism.org/tag/longtermism) | [long-term future](https://forum.effectivealtruism.org/tag/long-term-future) | [moral circle expansion](https://forum.effectivealtruism.org/tag/moral-circle-expansion-1) | [moral patienthood](https://forum.effectivealtruism.org/tag/moral-patienthood) | [universe's resources](https://forum.effectivealtruism.org/tag/universe-s-resources) | [wild animal welfare](https://forum.effectivealtruism.org/tag/wild-animal-welfare) | [whole brain emulation](https://forum.effectivealtruism.org/tag/whole-brain-emulation)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "HrZ8whku9ov3dKvdX",
    "name": "Scalably using labour",
    "core": false,
    "slug": "scalably-using-labour",
    "oldSlugs": [
      "scalably-using-people",
      "scalably-involving-people",
      "scalably-involving-people"
    ],
    "postCount": 51,
    "description": {
      "markdown": "The EA movement's ability to **scalably use labour** refers to its ability to efficiently allocate many people to valuable work and to recommend valuable actions large segments of the population could take. There have been discussions within EA about the movement's strengths and weaknesses on those fronts, the consequences of that, how to improve, and how this all might change as EA grows.\n\nA related but narrower discussion centers on the concept of \"Task Y\":^[\\[1\\]](#fnbrelcxra2ve)^ a task which has one or more of the following properties:\n\n*   It be performed usefully by people who are not currently able to choose their career path entirely based on EA concerns\n*   It's clearly effective, and doesn't become much less effective the more people who are doing it\n*   Its positive effects are obvious to the person doing the task\n\nOther related ideas include the claims that EA is vetting-constrained or that some of the major bottlenecks the EA movement currently faces are related to organizational capacity, infrastructure, and management.^[\\[2\\]](#fn4ahfgn36chm)^\n\nFurther reading\n---------------\n\nEA person (2019) [EA is vetting-constrained](https://forum.effectivealtruism.org/posts/G2Pfpkcwv3bJNF8o9/ea-is-vetting-constrained), *Effective Altruism Forum*, March 9.\n\nRelated entries\n---------------\n\n[career choice](https://forum.effectivealtruism.org/tag/career-choice) | [community infrastructure](https://forum.effectivealtruism.org/tag/community-infrastructure) | [constraints on effective altruism](https://forum.effectivealtruism.org/tag/constraints-on-effective-altruism) | [criticism of effective altruism](https://forum.effectivealtruism.org/tag/criticism-of-effective-altruism) | [hiring](https://forum.effectivealtruism.org/tag/hiring) | [take action](https://forum.effectivealtruism.org/tag/take-action) | [markets for altruism](https://forum.effectivealtruism.org/tag/markets-for-altruism) | [research training programs](https://forum.effectivealtruism.org/tag/research-training-programs) | [working at EA vs non-EA orgs](https://forum.effectivealtruism.org/tag/working-at-ea-vs-non-ea-orgs)\n\n1.  ^**[^](#fnrefbrelcxra2ve)**^\n    \n    Lawsen, Alex (2019) [Can the EA community copy Teach for America? (Looking for Task Y)](https://forum.effectivealtruism.org/posts/uWWsiBdnHXcpr7kWm/can-the-ea-community-copy-teach-for-america-looking-for-task), *Effective Altruism Forum*, February 21.\n    \n2.  ^**[^](#fnref4ahfgn36chm)**^\n    \n    Koehler, Arden & Keiran Harris (2020) [Benjamin Todd on what the effective altruism community most needs](https://80000hours.org/podcast/episodes/ben-todd-on-what-effective-altruism-most-needs/), *80,000 Hours*, November 12."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "6DWibSGkovfWcW3QE",
    "name": "Working at EA vs. non-EA orgs",
    "core": false,
    "slug": "working-at-ea-vs-non-ea-orgs",
    "oldSlugs": [
      "working-at-ea-vs-non-ea-orgs",
      "working-at-ea-vs-non-ea-orgs"
    ],
    "postCount": 41,
    "description": {
      "markdown": "The **working at EA vs. non-EA orgs** tag is for posts that include arguments for or against pursuing jobs at organisations that explicitly identify with the \"effective altruism\" label, relative to jobs at other organisations. The tag is also for posts that include discussion of whether and why members of the EA community may be biased in one direction or the other on this question, and how to address that (e.g., how to raise the status - within the EA community - of high-impact work at non-EA orgs).\n\nThis tag is not intended for posts that discuss things like how the effectiveness of EA orgs tends to differ from that of non-EA orgs, except when those posts connect this to the question of where EAs should work.\n\nRelated entries\n---------------\n\n[career choice](https://forum.effectivealtruism.org/tag/career-choice) | [constraints on effective altruism](https://forum.effectivealtruism.org/tag/constraints-on-effective-altruism) | [criticism of effective altruism](https://forum.effectivealtruism.org/tag/criticism-of-effective-altruism) | [criticism of effective altruist organizations](https://forum.effectivealtruism.org/tag/criticism-of-effective-altruist-organizations) | [hiring](https://forum.effectivealtruism.org/tag/hiring) | [research training programs](https://forum.effectivealtruism.org/tag/research-training-programs) | [scalably using labour](https://forum.effectivealtruism.org/tag/scalably-using-labour)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "HmqkjPueHoEwmGwfB",
    "name": "Cultural evolution",
    "core": false,
    "slug": "cultural-evolution",
    "oldSlugs": [
      "cultural-evolution"
    ],
    "postCount": 9,
    "description": {
      "markdown": "**Cultural evolution** is the process through which non-genetic information is transmitted from one generation of humans to the next.\n\nRelated entries\n---------------\n\n[cultural lag](https://forum.effectivealtruism.org/tag/cultural-lag) | [cultural persistence](https://forum.effectivealtruism.org/tag/cultural-persistence) | [trajectory change](https://forum.effectivealtruism.org/tag/trajectory-change) | [value drift](https://forum.effectivealtruism.org/tag/value-drift)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "8ksiPjRESdQL35xnY",
    "name": "Audio",
    "core": false,
    "slug": "audio",
    "oldSlugs": null,
    "postCount": 99,
    "description": {
      "markdown": "The **audio** tag is for posts that you can listen to, or that are substantially about audio content. This can include links to [podcasts](https://forum.effectivealtruism.org/tag/podcasts), collections of audio content, or posts for which the author has recorded a narration (authors: please link your audio narration or talk at the top of your post).\n\nRelated entries\n---------------\n\n[video](https://forum.effectivealtruism.org/tag/video-1)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "DXfPYEx7G8wAbEZTc",
    "name": "Parenting",
    "core": false,
    "slug": "parenting",
    "oldSlugs": null,
    "postCount": 20,
    "description": {
      "markdown": "Further reading\n---------------\n\nHutchinson, Michelle (2020) [Parenting: Things I wish I could tell my past self](https://forum.effectivealtruism.org/posts/Pq9GRnyAbNLC4TPkW/parenting-things-i-wish-i-could-tell-my-past-self), *Effective Altruism Forum*, September 12.\n\nTomasik, Brian (2012) [The cost of kids](https://reducing-suffering.org/the-cost-of-kids/), *Essays on Reducing Suffering*, August 4 (updated 29 November 2017).\n\nWiblin, Robert & Keiran Harris (2022) [Bryan Caplan on why lazy parenting is actually OK](https://80000hours.org/podcast/episodes/bryan-caplan-parenting-workers-betting/), *80,000 Hours*, April 5.\n\nWise, Julia (2013) [Cheerfully](http://www.givinggladly.com/2013/06/cheerfully.html), *Giving Gladly*, June 8.\n\nYoung, Bernadette (2014) [Parenthood and effective altruism](https://forum.effectivealtruism.org/posts/bz2A2gRvtrpHAsToN/parenthood-and-effective-altruism), *Effective Altruism Forum*, April 13.\n\nRelated entries\n---------------\n\n[effective altruism lifestyle](https://forum.effectivealtruism.org/tag/effective-altruism-lifestyle)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "QSwqS4RqQqbrh5wRS",
    "name": "Cultural persistence",
    "core": false,
    "slug": "cultural-persistence",
    "oldSlugs": [
      "persistence-of-political-cultural-variables"
    ],
    "postCount": 14,
    "description": {
      "markdown": "**Cultural persistence** is the persistence of \"cultural traits\" like customs, beliefs, and behaviours over time. In some cases, such traits last for very long periods, while in other cases, they change more rapidly and dramatically.^[\\[1\\]](#fn08kcququdr6a)^ Relevant traits could include democratic or authoritarian norms and behaviours, concern for human rights, concern for animal welfare, and norms conducive to scientific progress or free markets.\n\nIt could be valuable to know how persistent cultural traits tend to be, how often they are *very* persistent, how this varies between traits and conditions, and what interventions could affect persistence. This could inform predictions about what the future will be like and views about the importance of intervening to change various traits or various conditions that affect persistence. For example, generally, the more persistent we expect a positive change in people's moral views to be, the more valuable causing that change may be. Cultural persistence is therefore relevant to ideas such as [trajectory change](https://forum.effectivealtruism.org/tag/trajectory-change) and [value lock-in](https://forum.effectivealtruism.org/tag/value-lock-in).^[\\[2\\]](#fne8n6cxgr4z5)^\n\nThis question of cultural persistence has received some academic attention,^[\\[1\\]](#fn08kcququdr6a)^^[\\[3\\]](#fngwie421027m)^ and is also related to the field of [cultural evolution](https://forum.effectivealtruism.org/tag/cultural-evolution).\n\nFurther reading\n---------------\n\nSevilla, Jaime (2021) [Persistence - A critical review](https://docs.google.com/document/d/14ULAaTofWiQbTCP1ekuaenQJ6saXEzjgiKMznIBrXvQ/edit?usp=embed_facebook), unpublished.\n\nRelated entries\n---------------\n\n[cluelessness](https://forum.effectivealtruism.org/tag/cluelessness) | [cultural evolution](https://forum.effectivealtruism.org/tag/cultural-evolution) | [cultural lag](https://forum.effectivealtruism.org/tag/cultural-lag) | [hinge of history](https://forum.effectivealtruism.org/tag/hinge-of-history) | [trajectory change](https://forum.effectivealtruism.org/tag/trajectory-change) | [value drift](https://forum.effectivealtruism.org/tag/value-drift) | [value lock-in](https://forum.effectivealtruism.org/tag/value-lock-in)\n\n1.  ^**[^](#fnref08kcququdr6a)**^\n    \n    Giuliano, Paola & Nathan Nunn (2020) [Understanding cultural persistence and change](https://doi.org/10.1093/restud/rdaa074), *The Review of Economic Studies*, vol. 88, pp. 1541–1581.\n    \n2.  ^**[^](#fnrefe8n6cxgr4z5)**^\n    \n    Beckstead, Nick (2015) [The long-term significance of reducing global catastrophic risks](https://www.openphilanthropy.org/blog/long-term-significance-reducing-global-catastrophic-risks), *The GiveWell Blog*, August 13 (updated 16 September 2015).\n    \n3.  ^**[^](#fnrefgwie421027m)**^\n    \n    Kelly, Morgan (2019) [The standard errors of persistence](https://doi.org/10.2139/ssrn.3398303), *SSRN Electronic Journal*."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "xG4aZPDrvtKRwP6mK",
    "name": "Neglected tropical diseases",
    "core": false,
    "slug": "neglected-tropical-diseases",
    "oldSlugs": [
      "research-into-neglected-tropical-diseases"
    ],
    "postCount": 3,
    "description": {
      "markdown": "**Neglected tropical diseases** (**NTDs**) are a collection of diseases which are prevalent in developing countries. The diseases included by the term varies, but the [World Health Organization](https://forum.effectivealtruism.org/topics/world-health-organization) recognizes 19 diseases, including viral, bacterial and parasitic worm infections (such as soil transmitted helminthiasis). These diseases are contrasted with the big three infectious diseases ([HIV/AIDS](https://forum.effectivealtruism.org/tag/hiv-aids), [malaria](https://forum.effectivealtruism.org/tag/malaria) and [tuberculosis](https://forum.effectivealtruism.org/tag/tuberculosis)), which are generally less [neglected](https://forum.effectivealtruism.org/tag/neglectedness). NTDs affect more than 1 billion people worldwide.^[\\[1\\]](#fnc61qy1vmml)^\n\nFor some NTDs, there are treatments but no vaccines: for example, praziquantel is an effective treatment for schistosomiasis. For other NTDs, such as dengue, there are neither treatments nor vaccines.\n\nSince there has been little assessment of this area, current cost-effectiveness estimates are very speculative. However, they indicate that research into NTDs may be several times more effective than other global health interventions.\n\nFurther reading\n---------------\n\nGiving What We Can (2015) [Medical research](https://www.givingwhatwecan.org/cause/medical-research/), *Giving What We Can*, September 21 (updated 25 April 2018).  \n*An investigation into the effectiveness of medical research into neglected tropical diseases.*\n\nKarnofsky, Holden (2015) [Investigating neglected goals in scientific research](https://www.openphilanthropy.org/blog/investigating-neglected-goals-scientific-research), *Open Philanthropy*, March 26.  \n*A discussion by the Open Philanthropy Project on tropical diseases as a neglected research area.*\n\n1.  ^**[^](#fnrefc61qy1vmml)**^\n    \n    World Health Organization (2012) [Neglected tropical diseases](https://www.who.int/news-room/q-a-detail/neglected-tropical-diseases), *World Health Organization*, January 17."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "aRLDYMuyWpq7r6pST",
    "name": "Life sciences",
    "core": false,
    "slug": "life-sciences",
    "oldSlugs": [
      "life-sciences-research"
    ],
    "postCount": 11,
    "description": {
      "markdown": "The **life sciences** comprise the various scientific disciplines concerned with the study of living organisms, including biology, biochemistry, and physiology. Research in these disciplines can result in major improvements in human health, and has led to valuable breakthroughs in other fields. As there is significant existing funding for life sciences research, one of the key criteria for identifying projects to fund in this area is finding research areas that are unduly [neglected](https://forum.effectivealtruism.org/tag/neglectedness). Examples of such areas include research into [neglected diseases](https://forum.effectivealtruism.org/tag/neglected-tropical-diseases), [aging research](https://forum.effectivealtruism.org/tag/aging-research), and research into [alternatives to animal products](https://forum.effectivealtruism.org/tag/animal-product-alternatives).\n\nFurther reading\n---------------\n\nGiving What We Can (2016) [Medical research](https://www.givingwhatwecan.org/cause/medical-research/), *Giving What We Can*.\n\nOpen Philanthropy (2021) [Scientific research](https://www.openphilanthropy.org/focus/scientific-research), *Open Philanthropy*.\n\nRelated entries\n---------------\n\n[bioethics](https://forum.effectivealtruism.org/tag/bioethics) | [medicine](https://forum.effectivealtruism.org/tag/medicine) | [metascience](https://forum.effectivealtruism.org/topics/metascience)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "vfJTB9XFun9JA6k5f",
    "name": "Macroeconomic policy",
    "core": false,
    "slug": "macroeconomic-policy",
    "oldSlugs": [
      "macroeconomic-stabilization"
    ],
    "postCount": 5,
    "description": {
      "markdown": "**Macroeconomic policy** is the set of government rules and regulations concerned with influencing the operation of the economy as a whole.\n\nThe workings of the national and global economy affect many people. Often the global economy is unstable, which can be costly: for instance, the Great Recession caused a rise in unemployment, and might have had other serious, but less visible, effects such as increasing cancer deaths worldwide.^[\\[1\\]](#fnxcl8oh1bx5)^\n\n[Open Philanthropy](https://forum.effectivealtruism.org/topics/open-philanthropy) believes that there are macroeconomic policies that could alleviate some of this instability, and reduce its associated costs. It supports work to advocate for such policies, and to research better policies.^[\\[2\\]](#fn8dvn70p3pkk)^\n\nFurther reading\n---------------\n\nOpen Philanthropy (2016) [Macroeconomic stabilization policy](https://www.openphilanthropy.org/focus/us-policy/macroeconomic-policy), *Open Philanthropy*.\n\nRelated entries\n---------------\n\n[economics](https://forum.effectivealtruism.org/tag/economics) | [economic growth](https://forum.effectivealtruism.org/topics/economic-growth) | [policy](https://forum.effectivealtruism.org/tag/policy)\n\n1.  ^**[^](#fnrefxcl8oh1bx5)**^\n    \n    Maruthappu, Mahiben *et al.* (2016) [Economic downturns, universal health coverage, and cancer mortality in high-income and middle-income countries, 1990–2010: A longitudinal analysis](http://doi.org/10.1016/S0140-6736(16)00577-8), *The Lancet*, vol. 388, pp. 684–695.\n    \n2.  ^**[^](#fnref8dvn70p3pkk)**^\n    \n    Open Philanthropy (2016) [Macroeconomic stabilization policy](https://www.openphilanthropy.org/focus/us-policy/macroeconomic-policy), *Open Philanthropy*."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "C4EvLFtbXpaX8dHsd",
    "name": "Immigration reform",
    "core": false,
    "slug": "immigration-reform",
    "oldSlugs": null,
    "postCount": 18,
    "description": {
      "markdown": "**Immigration reform** is the set of interventions aimed at improving immigration [policy](https://forum.effectivealtruism.org/topics/policy).\n\nSeveral studies indicate that increased labor mobility could bring about large economic benefits worldwide, and fully open borders could bring gains of between 67% and 122% of world GDP^[\\[1\\]](#fn6ynxhsalb4h)^ and dramatically reduce [global poverty](https://forum.effectivealtruism.org/tag/global-poverty).^[\\[2\\]](#fnxskn9mw9h0p)^ Much of this increase would come via substantially increasing the wages of migrants from poorer countries, allowing them to earn more, and possibly sending remittances that will help their families or communities out of poverty.\n\nHowever, there are some negative effects. There may be a “brain drain” from low income countries, that harms their economies in the long run. Additionally, domestic workers in high-income countries may face more competition which could lower their wages or increase unemployment rates for domestic workers. However, it is extremely unclear whether this effect is significant. Third, high levels of immigration may destabilize culture in damaging ways. Finally, political “backlash” against high levels of immigration may fuel the rise of anti-immigration political movements, who may revert policies, resulting in lower levels of immigration than before. They may also introduce other damaging policies.\n\nRegardless of the merit of the arguments, increasing labour mobility has proven politically difficult, since citizens from high income countries generally strongly oppose this type of reform.\n\nFurther reading\n---------------\n\nCaplan, Bryan & Vipul Naik (2015) [A radical case for open borders](https://doi.org/10.1093/acprof:oso/9780190258788.003.0008), in Benjamin Powell (ed.) *The Economics of Immigration*, Oxford: Oxford University Press, pp. 180–209.\n\nCaplan, Bryan (2022) [Open borders as ultra-effective altruism](https://betonit.substack.com/p/open-borders-as-ultra-effective-altruism), *Bet On It*, March 24.\n\nOpen Philanthropy (2013) [Labor mobility](https://www.openphilanthropy.org/research/cause-reports/labor-mobility), *Open Philanthropy*, May.  \n*OpenPhil's shallow investigation about labor mobility.*\n\nPritchett, Lant (2006) [*Let Their People Come: Breaking the Gridlock on International Labor Mobility*](https://en.wikipedia.org/wiki/Special:BookSources/9781933286105), Washington: Center for Global Development.  \n*A book-length examination of the economic effects of increased labor mobility.*\n\nRoodman, David (2014) [The domestic economic impacts of immigration](https://davidroodman.com/blog/2014/09/03/the-domestic-economic-impacts-of-immigration/), *David Roodman’s Blog*, September 3.  \n*A thorough overview of the evidence on the potential side effects of immigration.*\n\nExternal links\n--------------\n\n[Open Borders](http://openborders.info/). A comprehensive repository of information about open borders.\n\nRelated entries\n---------------\n\n[economic growth](https://forum.effectivealtruism.org/tag/economic-growth) | [global poverty](https://forum.effectivealtruism.org/tag/global-poverty)\n\n1.  ^**[^](#fnref6ynxhsalb4h)**^\n    \n    Clemens, Michael A. (2011) [Economics and emigration: Trillion-dollar bills on the sidewalk?](http://doi.org/10.1257/jep.25.3.83), *Journal of Economic Perspectives*, vol. 25, pp. 83–106.\n    \n2.  ^**[^](#fnrefxskn9mw9h0p)**^\n    \n    Shulman, Carl (2014) [How migration liberalization might eliminate most absolute poverty](http://reflectivedisequilibrium.blogspot.com/2014/05/how-migration-liberalization-might.html), *Reflective Disequilibrium*, May 27."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "6YinwPnz76XM5WKec",
    "name": "Burden of disease",
    "core": false,
    "slug": "burden-of-disease",
    "oldSlugs": [
      "the-global-burden-of-disease",
      "global-burden-of-disease-study"
    ],
    "postCount": 11,
    "description": {
      "markdown": "The **burden of disease** (also called **disease burden** and **global burden of disease**) is the total impact of disease in a population.\n\nHistory\n-------\n\nThe concept of burden of disease originates in a study commissioned by the World Bank in 1990 and in collaboration with Harvard University and the [World Health Organization](https://forum.effectivealtruism.org/topics/world-health-organization) (WHO). The study, published in the World Bank's 1993 *World Development Report* and known as the first **Global Burden of Disease** **study** (GBD 1990), was a comprehensive effort to quantify the healthy life years lost globally not only from premature death but also from nonfatal medical conditions.^[\\[1\\]](#fni597sis7xm)^^[\\[2\\]](#fn1cc8lulaat2)^ To measure this loss, GBD 1990 introduced the [Disability-Adjusted Life Year](https://forum.effectivealtruism.org/tag/adjusted-life-year) (DALY), a metric that has since become standard in health impact assessment and informed policy-making. GBD 1990 had considerable influence, both in stimulating other studies focused on various subpopulations and in contributing to the setting of global health priorities.^[\\[3\\]](#fnqlbxv40tol)^\n\nWHO carried out further GBD studies between 1998 and 2004. Since 2010, GBD studies are conducted by the Institute for Health Metrics and Evaluation (IHME). The most recent study was completed in 2019.\n\nApplications\n------------\n\nIn 2019, the world population lost 1.68 billion years of life due to premature death—calculated as the sum of the difference between each person's age of death and their life expectancy at that age–and another 863 million years of healthy life, measured in DALYs.^[\\[4\\]](#fnbbzrrjccci9)^  This loss corresponds to a burden of 33,073 DALYs per 100,000 people, or about four months of healthy life lost for every person alive. (The global burden of disease per person has been falling consistently for the last three decades. In 1990, when the first study was conducted, it was 48,595 DALYs per 100,000 people.)\n\nBurden of disease figures can be helpful not only to assess the current state of the world and make secular comparisons, but also to help with [cause prioritization](https://forum.effectivealtruism.org/tag/cause-prioritization). Different medical conditions, as well as different geographical regions, vary considerably in their contributions to the global burden of disease. An awareness of this variation can significantly assist efforts to allocate resources where they can have the most impact.\n\nFurther reading\n---------------\n\nMurray, Christopher J. L. & Alan D. Lopez (1996) [*Global Health Statistics: A Compendium of Incidence, Prevalence and Mortality Estimates for over 200 Conditions*](https://en.wikipedia.org/wiki/Special:BookSources/0-674-35449-4), Boston: Harvard School of Public Health.\n\nRoser, Max & Hannah Ritchie (2016) [Burden of disease](https://ourworldindata.org/burden-of-disease), *Our World in Data*.\n\nExternal links\n--------------\n\n[GBD Compare | IHME Viz Hub](http://vizhub.healthdata.org/gbd-compare/). An interactive tool to explore the results of the Global Burden of Disease Study.\n\nRelated entries\n---------------\n\n[adjusted life year](https://forum.effectivealtruism.org/tag/adjusted-life-year) | [global health and development](https://forum.effectivealtruism.org/tag/global-health-and-development)\n\n1.  ^**[^](#fnrefi597sis7xm)**^\n    \n    World Bank (1993) [*World Development Report 1993: Investing in Health*](http://doi.org/10.1596/0-1952-0890-0), Oxford: Oxford University Press.\n    \n2.  ^**[^](#fnref1cc8lulaat2)**^\n    \n    Murray, Christopher J. L. & Alan D. Lopez (eds.) (1996) [*The Global Burden of Disease: A Comprehensive Assessment of Mortality and Disability from Diseases, Injuries, and Risk Factors in 1990 and Projected to 2020*](https://en.wikipedia.org/wiki/Special:BookSources/978-0-674-35448-7), Cambridge, Massachusetts: Harvard University Press.\n    \n3.  ^**[^](#fnrefqlbxv40tol)**^\n    \n    Mathers, Colin (2017) [Global burden of disease](http://doi.org/10.1016/B978-0-12-803678-5.00175-2), in Stella R. Quah (ed.) *International Encyclopedia of Public Health*, 2nd ed., vol. 3, Amsterdam: Elsevier, pp. 256–267.\n    \n4.  ^**[^](#fnrefbbzrrjccci9)**^\n    \n    Global Health Data Exchange (2019) [GBD results tool](http://ghdx.healthdata.org/gbd-results-tool?params=gbd-api-2019-permalink/970372736105a1707fe5b09492a51add), Institute for Health Metrics and Evaluation."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "Mvm2cbyAHRxGHM7L3",
    "name": "Existential risks from fundamental physics research",
    "core": false,
    "slug": "existential-risks-from-fundamental-physics-research",
    "oldSlugs": null,
    "postCount": null,
    "description": {
      "markdown": "**Existential risks from fundamental physics research** are speculative risks of an [existential catastrophe](https://forum.effectivealtruism.org/topics/existential-catastrophe-1) potentially arising from certain types of research in particle physics.\n\nFundamental physics research appears very unlikely to pose an [existential risk](https://forum.effectivealtruism.org/tag/existential-risk). Physicists have extremely accurate models of the world, and are able to estimate the likely consequences of different experimental conditions. Even where they do not know precisely what will happen, they usually have a fairly good idea of roughly what will happen.\n\nHowever, one of the reasons to run experiments in the first place is to probe areas where the existing models of physics might be wrong. If the models are wrong, and the experiments are sufficiently exotic, there is some very small chance of accidental and unpredictable harm resulting. Some, for example, have concerns that strangelets created by particle accelerators might cause catastrophe. Such risks are extremely unlikely conditional on our phyiscal models being true. But the risks become more concerning given a non-negligible chance that [the models are mistaken](https://forum.effectivealtruism.org/tag/model-uncertainty).\n\nFurther reading\n---------------\n\nOrd, Toby, Rafaela Hillerbrand & Anders Sandberg (2010) [Probing the improbable: methodological challenges for risks with low probabilities and high stakes](http://doi.org/10.1080/13669870903126267), *Journal of risk research*, vol. 13, pp. 191–205.  \n*A technical paper investigating ways to estimate risks from model uncertainty.*\n\nRelated entries\n---------------\n\n[anthropogenic existential risk](https://forum.effectivealtruism.org/tag/anthropogenic-existential-risk) | [existential risk](https://forum.effectivealtruism.org/tag/existential-risk)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "sQL3e3FpPuPW7tpAX",
    "name": "Market efficiency of philanthropy",
    "core": false,
    "slug": "market-efficiency-of-philanthropy",
    "oldSlugs": null,
    "postCount": 2,
    "description": {
      "markdown": "The **market efficiency of philanthropy** is a measure of the difficulty of finding more [cost-effective](https://forum.effectivealtruism.org/topics/cost-effectiveness) giving opportunities than the philanthropic sector has identified so far.\n\nAccording to the efficient market hypothesis, if all investors have the same information and behave rationally, all assets will be priced “correctly”. Under these conditions, it becomes impossible to consistently beat the market.\n\nThe efficient-market hypothesis is usually discussed in financial contexts, but it can in principle apply—or fail to apply—in any market, including philanthropy. If all philanthropists are trying to help others the most and rely upon publicly available information, the efficient-market hypothesis implies that philanthropic resources will be optimally allocated. The more efficient the market for philanthropy, the harder it is to identify outstanding giving opportunities (which are likely to have been already snapped up by previous funders).\n\nThe efficient-market hypothesis applied to financial markets is controversial, and there are good reasons to think that the philanthropy market is less efficient than financial markets. Maximizing return on investment is the predominant motivation among investors, whereas philanthropists are often motivated by concerns other than doing the most good. Furthermore, different donors have different moral beliefs, so even if each was motivated purely by moral considerations, lower levels of efficiency should be expected in the philanthropy market.\n\nStill, it appears that the philanthropy market is at least partially efficient. For example, it is difficult to find funding opportunities to distribute vaccines: the evidence in favour of vaccine distribution is so overwhelming that distribution is generally funded by large institutional donors.\n\nOne heuristic for outperforming the philanthropy market is to search for donation opportunities that are currently underfunded, perhaps because they have a less compelling or harder-to-understand narrative. For example, protection against [global catastrophic risks](https://forum.effectivealtruism.org/tag/global-catastrophic-risk) may be underfunded because of scope insensitivity. As an increasingly larger proportion of philanthropists correctly identify underfunded opportunities—for instance, due to the [growth of the effective altruism movement](https://forum.effectivealtruism.org/tag/value-of-movement-growth)—the philanthropy market would become more efficient.\n\nFurther reading\n---------------\n\nChristiano, Paul (2013) [The efficiency of modern philanthropy](https://rationalaltruist.com/2013/05/05/the-efficiency-of-modern-philanthropy/), *Rational Altruist*, May 5.  \n*Argues that philanthropists neglect interventions which won’t reliably look like a good idea in retrospect.*\n\nKarnofsky, Holden (2013) [Broad market efficiency](https://blog.givewell.org/2013/05/02/broad-market-efficiency/), *The GiveWell Blog*, May 2 (updated 25 July 2016).  \n*Investigates the difficulties of identifying outstanding giving opportunities neglected by others.*"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "is8kynTqvpzQ5RtEh",
    "name": "Constraints on effective altruism",
    "core": false,
    "slug": "constraints-on-effective-altruism",
    "oldSlugs": [
      "talent-constraints-and-funding-constraints",
      "talent-vs-funding-constraints"
    ],
    "postCount": 33,
    "description": {
      "markdown": "The effective altruism community requires various resources that exist in limited supply, such as talent, funding, entrepreneurship, vetting, risk tolerance, and the ability to coordinate. These resources may be seen as **constraints** **on effective altruism**,  insofar as they limit the community's capacity to attain its goals.\n\nConsider two organizations, both of which have ten staff and would like to increase capacity:\n\n1.  **Organization A:** Has annual funding of $5m, so can fund more staff, and has been actively hiring for a year, but has been unable to find anyone suitable.\n2.  **Organization B:** Has annual funding under $1m, so can only just cover current costs. Several suitably talented individuals have made speculative applications to the organization, but the organization could not afford to hire them.\n\nOrganization A is more **talent constrained** than **funding constrained**, and vice versa for Organization B. One might also say that Organization A faces more of a **talent gap** and organization B faces more of a **funding gap**.^[\\[1\\]](#fna4gx68mglda)^ We can generalize these concepts:\n\n*   An organization is more talent constrained if the main factor limiting their work is access to skilled labour.\n*   An organization is more funding constrained if the main factor limiting their work is a lack of money.\n\nThese concepts are not precisely defined: rather, there tends to be a spectrum between the extremes that Organization A and Organization B occupy.\n\nOne of the key decisions people face when they want to support a cause is whether to [work directly](https://forum.effectivealtruism.org/tag/direct-work) on that cause, or whether to [earn to give](https://forum.effectivealtruism.org/tag/earning-to-give) in order to [fund that cause](https://forum.effectivealtruism.org/tag/effective-giving-1/). A key consideration relevant to this decision is whether the cause is talent constrained or funding constrained, since that will influence what kind of resource is most needed.\n\nHowever, this consideration is also simplistic in some ways, and has at times been misinterpreted or misused. Todd lists nine common misconceptions related to the idea of \"talent gaps\".^[\\[2\\]](#fn7ms6rfs6982)^ More recently, he has argued that the main bottlenecks for the effective altruism community now are neither general \"talent\" constraints nor funding constraints, but rather \"specific skills and capacity\", such as \"organizational capacity, infrastructure, and management to help train people up, as well as specialist skills that people can put to work now\"^[\\[3\\]](#fn8221pgqjrc)^ (see [scalably using labour](https://forum.effectivealtruism.org/tag/scalably-using-labour)). It has also been suggested that the next major bottleneck might be \"coordination — the ability to make sure people keep working efficiently and effectively together as the community grows\"^[\\[3\\]](#fn8221pgqjrc)^ (see also [altruistic coordination](https://forum.effectivealtruism.org/tag/altruistic-coordination)).\n\nFurther reading\n---------------\n\nAgarwalla, Vaidehi (2020) [Collection of constraints in EA](https://forum.effectivealtruism.org/posts/4SRj3KnRCh7iFoGK2/vaidehi_agarwalla-s-shortform?commentId=5XymYrET43cHT9bwg), *Effective Altruism Forum*, July 15.\n\nNaik, Vipul (2014) [On the concept of “talent-constrained” organizations](http://lesswrong.com/lw/jw8/on_the_concept_of_talentconstrained_organizations/), *LessWrong*, March 14.  \n*Suggests that organizations might simply not be paying enough for talent, causing talent constraints, although there are countervailing considerations.*\n\nRelated entries\n---------------\n\n[career choice](https://forum.effectivealtruism.org/tag/career-choice) | [direct work](https://forum.effectivealtruism.org/tag/direct-work) | [earning to give](https://forum.effectivealtruism.org/tag/earning-to-give) | [effective giving](https://forum.effectivealtruism.org/tag/effective-giving-1) | [get involved](https://forum.effectivealtruism.org/tag/get-involved) | [hiring](https://forum.effectivealtruism.org/tag/hiring) | [scalably using labour](https://forum.effectivealtruism.org/tag/scalably-using-labour) | [working at EA vs non-EA orgs](https://forum.effectivealtruism.org/tag/working-at-ea-vs-non-ea-orgs)\n\n1.  ^**[^](#fnrefa4gx68mglda)**^\n    \n    Todd, Benjamin (2015) [Why you should focus more on talent gaps, not funding gaps](https://80000hours.org/2015/11/why-you-should-focus-more-on-talent-gaps-not-funding-gaps/), *80,000 Hours*, November 27.\n    \n2.  ^**[^](#fnref7ms6rfs6982)**^\n    \n    Todd, Benjamin (2018) [Think twice before talking about 'talent gaps' – clarifying nine misconceptions](https://80000hours.org/2018/11/clarifying-talent-gaps/), *80,000 Hours*, November 12.\n    \n3.  ^**[^](#fnref8221pgqjrc)**^\n    \n    Koehler, Arden & Keiran Harris (2020) [Benjamin Todd on what the effective altruism community most needs](https://80000hours.org/podcast/episodes/ben-todd-on-what-effective-altruism-most-needs/), *80,000 Hours*, November 12."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "sL8QzAGJ6FLrdbwqR",
    "name": "Diminishing returns",
    "core": false,
    "slug": "diminishing-returns",
    "oldSlugs": null,
    "postCount": 9,
    "description": {
      "markdown": "**Diminishing returns** (or **diminishing returns to scale**) is the decrease in [marginal](https://forum.effectivealtruism.org/topics/thinking-at-the-margin) output of a production process as one factor of production is increased while the remaining factors are held constant.\n\nGiving money to an altruistic project (for example, a charity) increases the amount of useful work that can be done by that project—this is a social return on the money donated. If an area exhibits diminishing returns, then the social return on each additional dollar will continuously decline as more and more money is given to the project.\n\nThe *monetary* difference between a project receiving nothing and it receiving $500,000 is obviously the same as the monetary difference between it receiving $4.5m and it receiving $5m. However, the theory of diminishing marginal returns suggests that in terms of *the amount of good done*, the difference between a project receiving nothing and receiving $500,000 is probably bigger than the difference between it receiving $4.5m and it receiving $5m. So if there are diminishing marginal returns, a project will get more good done with their first few dollars than with their last ones.\n\nThis point does not only apply to charities and money: it can also apply to other kinds of funding opportunities (e.g., individuals, universities, scientific research programmes, think tanks, nonprofit organisations)^[\\[1\\]](#fnamvf14ddc0e)^ and other kinds of resources (e.g., time and labour). However, it is unclear to what extent diminishing returns apply in different areas. Owen Cotton-Barratt gives an overview of the variety of cases it seems to apply to.^[\\[2\\]](#fnoohqidgirtf)^ However, Michael Dickens, for instance, argues that there are no detectable diminishing marginal returns in animal advocacy or work on existential risk.^[\\[3\\]](#fn91zr41ihgim)^\n\nIf charities face diminishing marginal returns, it may make sense to only fund them to a certain level: beyond that we say they have no additional [room for more funding](https://forum.effectivealtruism.org/tag/room-for-more-funding). Given that organizations and areas may face different returns to additional labour than they do to additional capital, it may be worth considering whether they are more [talent- or funding- constrained](https://forum.effectivealtruism.org/tag/constraints-on-effective-altruism) (though there are also ways that framing could be misleading).^[\\[4\\]](#fnvdh1tdkrmef)^^[\\[5\\]](#fn2d737gtfjw8)^\n\nFurther reading\n---------------\n\nDalton, Max & Owen Cotton-Barratt (2017) [Selecting the appropriate model for diminishing returns](https://www.centreforeffectivealtruism.org/blog/selecting-the-appropriate-model-for-diminishing-returns/), *Centre for Effective Altruism*, May 23.  \n*Sets out some considerations for choosing between different models of diminishing returns.*\n\nRelated entries\n---------------\n\n[donation choice](https://forum.effectivealtruism.org/tag/donation-choice) | [effective altruism funding](https://forum.effectivealtruism.org/tag/effective-altruism-funding) | [effective giving](https://forum.effectivealtruism.org/tag/effective-giving-1) | [neglectedness](https://forum.effectivealtruism.org/tag/neglectedness) | [philanthropic coordination](https://forum.effectivealtruism.org/tag/philanthropic-coordination) | [philanthropic diversification](https://forum.effectivealtruism.org/tag/philanthropic-diversification) | [room for more funding](https://forum.effectivealtruism.org/tag/room-for-more-funding) | [thinking at the margin](https://forum.effectivealtruism.org/tag/thinking-at-the-margin)\n\n1.  ^**[^](#fnrefamvf14ddc0e)**^\n    \n    Khan, Anu & Rachel Baxter (2021) [Why we say ‘funding opportunity’ instead of ‘charity’](https://founderspledge.com/stories/why-we-say-funding-opportunity-instead-of-charity), *Founders Pledge*, February 22.\n    \n2.  ^**[^](#fnrefoohqidgirtf)**^\n    \n    Cotton-Barratt, Owen (2015) [Part 4: The law of logarithmic returns](http://globalprioritiesproject.org/2015/02/the-law-of-logarithmic-returns/), *Global Priorities Project*, February 6.\n    \n3.  ^**[^](#fnref91zr41ihgim)**^\n    \n    Dickens, Michael (2016) [How should a large donor prioritize cause areas?](https://mdickens.me/2016/04/25/how_should_the_open_philanthropy_project_prioritize_cause_areas%3F/), *Philosophical Multicore*, April 25.\n    \n4.  ^**[^](#fnrefvdh1tdkrmef)**^\n    \n    Benjamin Todd (2018) [Think twice before talking about ‘talent gaps’ – clarifying nine misconceptions](https://80000hours.org/2018/11/clarifying-talent-gaps/), *80,000 Hours*, November 12.\n    \n5.  ^**[^](#fnref2d737gtfjw8)**^\n    \n    Arden Koehler & Keiran Harris (2020) [Benjamin Todd on what the effective altruism community most needs](https://80000hours.org/podcast/episodes/ben-todd-on-what-effective-altruism-most-needs/), *80,000 Hours*, November 12."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "yQ6kFwaRR8daE97kn",
    "name": "Cost-effectiveness",
    "core": false,
    "slug": "cost-effectiveness",
    "oldSlugs": null,
    "postCount": 25,
    "description": {
      "markdown": "The **cost-effectiveness** of a cause or a charity is the benefit it provides per unit cost.\n\nFurther reading\n---------------\n\nOrd, Toby (2019) [The moral imperative toward cost-effectiveness in global health](https://doi.org/10.1093/oso/9780198841364.003.0002), in Hilary Greaves & Theron Pummer (eds.) *Effective Altruism: Philosophical Issues*, Oxford: Oxford University Press, pp. 29–36.\n\nRelated entries\n---------------\n\n[cause prioritization](https://forum.effectivealtruism.org/tag/cause-prioritization) | [distribution of cost-effectiveness](https://forum.effectivealtruism.org/tag/distribution-of-cost-effectiveness) | [impact assessment](https://forum.effectivealtruism.org/tag/impact-assessment) | [intervention evaluation](https://forum.effectivealtruism.org/tag/intervention-evaluation) | [ITN framework](https://forum.effectivealtruism.org/tag/itn-framework-1) | [cost-effectiveness analysis](https://forum.effectivealtruism.org/topics/cost-effectiveness-analysis)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "fpFhkuue2R53o4duh",
    "name": "Divestment",
    "core": false,
    "slug": "divestment",
    "oldSlugs": null,
    "postCount": 3,
    "description": {
      "markdown": "**Divestment** is the process of reducing investment in a company or industry. Although divestment can be motivated by several different reasons, the motives behind divestment campaigns are typically moral or political. A recent example of a divestment campaign is Fossil Free, which sought to divest from energy companies that rely on fossil fuels.^[\\[1\\]](#fnatwa68zqjo)^\n\nAdvocates of divestment campaigns sometimes argue that they reduce the funding available to firms, thus limiting their capabilities. However, this seems not to be backed up by empirical evidence: while some investors will divest from objectionable companies, others, focused only on maximizing returns, will make up the funding gap.\n\nWill Macaskill claims that divestment leads to higher returns ending up in the hands of the least scrupulous investors, who are likely to spend their returns in more harmful ways than their more socially conscious counterparts.^[\\[2\\]](#fnwj0l5f4fvjl)^ He notes that divestment might be more impactful if its primary purpose is to affect the brand of the firm, and that product boycotts are more likely to have an impact than divestment campaigns.\n\nFurther reading\n---------------\n\nMacaskill, William (2015) [Does divestment work?](http://www.newyorker.com/business/currency/does-divestment-work), *The New Yorker*, October 20.  \n*A critical piece on the effectiveness of divestment campaigns.*\n\nRelated entries\n---------------\n\n[investing](https://forum.effectivealtruism.org/topics/investing) | [marginal charity](https://forum.effectivealtruism.org/tag/marginal-charity)\n\n1.  ^**[^](#fnrefatwa68zqjo)**^\n    \n    Fossil Free (2016) [What is fossil fuel divestment?](http://gofossilfree.org/what-is-fossil-fuel-divestment/) , *Fossil Free*.\n    \n2.  ^**[^](#fnrefwj0l5f4fvjl)**^\n    \n    Macaskill, William (2015) [Does divestment work?](http://www.newyorker.com/business/currency/does-divestment-work), *The New Yorker*, October 20."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "cbEeL3YgyujNuQ57i",
    "name": "Impact investing",
    "core": false,
    "slug": "impact-investing-1",
    "oldSlugs": null,
    "postCount": 9,
    "description": {
      "markdown": "**Impact investing** is the practice of investing in for-profit companies with the intention of generating both financial returns and social impact. The theory is that impact investing can increase the capital available to a company, allowing it to generate more positive social outcomes than would otherwise have occurred.\n\nFor impact investing to do good, it has to provide a company with more capital than it would have otherwise received. This is often difficult unless the investment is *concessionary* (sacrifices some level of financial return against the market rate), or takes place in a private market which other investors are unable or unwilling to invest in.^[\\[1\\]](#fniqqnzp3p2lo)^\n\nImpact investing is one form of [socially responsible investing](https://forum.effectivealtruism.org/tag/socially-responsible-investing).\n\nFurther reading\n---------------\n\nHillebrandt, Hauke & John Halstead (2018) [Impact Investing Report](https://founderspledge.com/research/fp-impact-investing), *Founders Pledge*.\n\nKarnofsky, Holden (2009) [Acumen Fund and social enterprise investment](http://blog.givewell.org/2009/11/25/acumen-fund-and-social-enterprise-investment/), *GiveWell*, November 25.  \n*GiveWell’s shallow investigation into impact investing.*\n\nRelated entries\n---------------\n\n[funding high-impact for-profits](https://forum.effectivealtruism.org/tag/funding-high-impact-for-profits) | [investing](https://forum.effectivealtruism.org/tag/investing) | [markets for altruism](https://forum.effectivealtruism.org/tag/markets-for-altruism) | [socially responsible investing](https://forum.effectivealtruism.org/tag/socially-responsible-investing)\n\n1.  ^**[^](#fnrefiqqnzp3p2lo)**^\n    \n    Brest, Paul & Kelly Born (2013) [When can impact investing create real impact?](https://ssir.org/up_for_debate/article/impact_investing), *Stanford Social Innovation Review*, vol. 11, pp. 22–31."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "SNnjZM8g7rgFjRjYK",
    "name": "Socially responsible investing",
    "core": false,
    "slug": "socially-responsible-investing",
    "oldSlugs": null,
    "postCount": 29,
    "description": {
      "markdown": "**Socially responsible investing** is any investment strategy which considers both financial and social returns. This can include [impact investing](https://forum.effectivealtruism.org/tag/impact-investing-1) in for-profit companies with the intention of bringing about social good, [divesting](https://forum.effectivealtruism.org/tag/divestment) from companies which might generate harmful social outcomes, and influencing companies' decisions towards better social outcomes by means of shareholder advocacy.\n\nFurther reading\n---------------\n\nGastfriend, Eric (2015) [Does effective altruism encourage impact investing?](https://www.quora.com/Does-effective-altruism-encourage-impact-investing), *Quora*.  \n*A discussion of effective altruism’s relationship to impact investing.*\n\nRelated entries\n---------------\n\n[funding high-impact for-profits](https://forum.effectivealtruism.org/tag/funding-high-impact-for-profits) | [impact investing](https://forum.effectivealtruism.org/tag/impact-investing-1) | [investing](https://forum.effectivealtruism.org/tag/investing) | [markets for altruism](https://forum.effectivealtruism.org/tag/markets-for-altruism)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "288ECLqh924sJm5qh",
    "name": "Funding high-impact for-profits",
    "core": false,
    "slug": "funding-high-impact-for-profits",
    "oldSlugs": null,
    "postCount": 6,
    "description": {
      "markdown": "Funding for-profits that address a pressing social need can be a promising way to have a positive social impact.\n\nAn example of a social enterprise that is regarded as effective within the effective altruism community is [Living Goods](https://forum.effectivealtruism.org/topics/living-goods). It supports a network of community health promoters that sell health products door-to-door and provide basic health counseling in sub-Saharan Africa. [GiveWell](https://forum.effectivealtruism.org/tag/givewell) lists it under the “Other Charities Worthy of Special Recognition” category, but recommends donating to Living Goods rather than investing in them.^[\\[1\\]](#fn828p8rzn96f)^\n\nFurther reading\n---------------\n\nWiblin, Robert (2016) [Doing good through for-profits: Wave and financial tech](https://80000hours.org/2016/02/doing-good-through-for-profits-lincoln-quirk-and-wave/), *80,000 Hours*, February 15.  \n*A profile of the social startup Wave.*\n\nRelated entries\n---------------\n\n[impact investing](https://forum.effectivealtruism.org/tag/impact-investing-1) | [investing](https://forum.effectivealtruism.org/tag/investing) | [markets for altruism](https://forum.effectivealtruism.org/tag/markets-for-altruism) | [socially responsible investing](https://forum.effectivealtruism.org/tag/socially-responsible-investing)\n\n1.  ^**[^](#fnref828p8rzn96f)**^\n    \n    GiveWell (2014) [Living Goods](http://www.givewell.org/charities/living-goods), *GiveWell*, November. *An evaluation of the social enterprise Living Goods.*"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "GdS3poEwBnB9mZnPu",
    "name": "Philanthropic diversification",
    "core": false,
    "slug": "philanthropic-diversification",
    "oldSlugs": null,
    "postCount": 6,
    "description": {
      "markdown": "**Philanthropic diversification** is the practice of allocating donations to multiple charities rather than the single charity with the highest expected value.\n\nThere are *prima facie* strong reasons for a [risk-neutral](https://forum.effectivealtruism.org/tag/risk-aversion) donor seeking to maximize the expected value of their donation to donate to only one charity. It may seem as though, if the donor believes their first dollar will do the most good if donated to some charity, they should also donate the second dollar to that charity, and so on until they run out of dollars to donate.^[\\[1\\]](#fnpqafpn9i8pa)^\n\nHowever, there are a number of reasons why diversifying donations may sometimes be justified.\n\nFirst, new evidence may change a donor’s mind about the charity which can most effectively use their donation.\n\nSecond, given uncertainty surrounding the effectiveness of charities, donating to multiple charities reduces the risk of a donor’s donations having little or no impact. Even risk-neutral donors can become demoralized upon finding out that their charitable donations had much less impact than initially expected.\n\nThird, being an active donor to multiple charities may make it easier to [obtain more information](https://forum.effectivealtruism.org/tag/value-of-information) on each of them. This is useful for prioritizing future donation decisions, and spreading the word to others.\n\nFourth, when a donor gives a very large amount, or gives to a very small charity, the donation will experience significant [diminishing marginal returns](https://forum.effectivealtruism.org/tag/diminishing-returns), to the point where it is no longer the option with the highest [expected value](https://forum.effectivealtruism.org/tag/expected-value).\n\nFinally, if everyone simultaneously gives to the charity they believe is best, this may mean that the *group* donation is sufficiently large to result in diminishing marginal returns, again leading to a suboptimal outcome. This final reason highlights the importance of [donor coordination](https://forum.effectivealtruism.org/tag/philanthropic-coordination).\n\nFurther reading\n---------------\n\nKuhn, Ben (2014) [How many causes should you give to?](https://www.benkuhn.net/how-many-causes/), *Ben Kuhn’s Blog*, June.  \n*An article considering the arguments for and against diversifying a charitable portfolio.*\n\nSnowden, James (2019) [Should we give to more than one charity?](http://doi.org/10.1093/oso/9780198841364.003.0005), in Hilary Greaves & Theron Pummer (eds.) *Effective Altruism*, Oxford: Oxford University Press, pp. 69–79.\n\nRelated entries\n---------------\n\n[diminishing returns](https://forum.effectivealtruism.org/tag/diminishing-returns) | [donation choice](https://forum.effectivealtruism.org/tag/donation-choice) | [effective altruism funding](https://forum.effectivealtruism.org/tag/effective-altruism-funding) | [effective giving](https://forum.effectivealtruism.org/tag/effective-giving-1) | [philanthropic coordination](https://forum.effectivealtruism.org/tag/philanthropic-coordination) | [risk aversion](https://forum.effectivealtruism.org/tag/risk-aversion) | [room for more funding](https://forum.effectivealtruism.org/tag/room-for-more-funding)\n\n1.  ^**[^](#fnrefpqafpn9i8pa)**^\n    \n    Landsburg, Steven E. (1997) [Giving your all](https://slate.com/culture/1997/01/giving-your-all.html), *Slate*, January 11."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "RRvnjvmpFJDKiuNnj",
    "name": "Philanthropic coordination",
    "core": false,
    "slug": "philanthropic-coordination",
    "oldSlugs": null,
    "postCount": 10,
    "description": {
      "markdown": "**Philanthropic coordination** is the practice of donors' working together to make individual donations more effective.\n\nIllustration\n------------\n\nConsider two people who both like two charities. They think each should get $10,000 this year, because they believe that [diminishing marginal returns](https://forum.effectivealtruism.org/tag/diminishing-returns) mean that $10,000 is the limit of what each charity could spend productively at the moment (see also [room for more funding](https://forum.effectivealtruism.org/tag/room-for-more-funding)). Each of them wants to donate a total of $10,000. If they cannot coordinate and randomly choose one of the charities to receive all of their money, then there is a 50% chance that they will each give to the same charity. In that case, the other charity will be unfunded, and the funded charity will be unable to spend all of its money well.\n\nThis problem becomes even harder if donors are able to wait to see what others do first. Suppose that both of the donors in the above example prefer the same charity above all others, but they have different views about the next best. In that case, each donor hopes that their favorite charity will be funded by the other donor, so that they can then fund their second choice. They might even give to their second favorite charity immediately, assuming that the other donor will therefore give to the shared favorite. The other donor might do the same with *their* second-favorite charity, with the result that the shared favorite charity does not get funded at all.\n\nEffective altruists have proposed a variety of potential ways to improve donor coordination and increase, thereby, the effectiveness of individual donations. For instance, Ben Todd proposes that people give to any charity that they think is among the best that the community should fund, whilst Denis Drescher sketches a general approach to the problem and notes important challenges.^[\\[1\\]](#fni881660e98n)^^[\\[2\\]](#fndrkj02sek2u)^\n\nPhilanthropic coordination is one form of [altruistic coordination](https://forum.effectivealtruism.org/tag/altruistic-coordination); other forms focus on decisions other than donation decisions, such as career decisions.\n\nFurther reading\n---------------\n\nKarnofsky, Holden (2014) [Donor coordination and the “giver’s dilemma”](https://blog.givewell.org/2014/12/02/donor-coordination-and-the-givers-dilemma/), *The GiveWell Blog*, December 2 (updated 18 November 2015).  \n*GiveWell’s analysis of the problem.*\n\nTodd, Benjamin (2018) [Doing good together - how to coordinate effectively, and avoid single-player thinking](https://80000hours.org/articles/coordination/), *80,000 Hours*, September 21.\n\nRelated entries\n---------------\n\n[altruistic coordination](https://forum.effectivealtruism.org/tag/altruistic-coordination) | [diminishing returns](https://forum.effectivealtruism.org/tag/diminishing-returns) | [donation choice](https://forum.effectivealtruism.org/tag/donation-choice) | [effective altruism funding](https://forum.effectivealtruism.org/tag/effective-altruism-funding) | [effective giving](https://forum.effectivealtruism.org/tag/effective-giving-1) | [game theory](https://forum.effectivealtruism.org/tag/game-theory) | [moral cooperation](https://forum.effectivealtruism.org/tag/moral-cooperation) | [moral trade](https://forum.effectivealtruism.org/tag/moral-trade) | [philanthropic diversification](https://forum.effectivealtruism.org/tag/philanthropic-diversification) | [room for more funding](https://forum.effectivealtruism.org/tag/room-for-more-funding)\n\n1.  ^**[^](#fnrefi881660e98n)**^\n    \n    Todd, Benjamin (2016) [The value of coordination](https://80000hours.org/2016/02/the-value-of-coordination/), *80,000 Hours*, February 8.\n    \n2.  ^**[^](#fnrefdrkj02sek2u)**^\n    \n    Drescher, Denis (2016) [Concept for donor coordination](https://impartial-priorities.org/concept-for-donor-coordination.html), *Im­par­tial Pri­or­i­ties*, January 23."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "RNRbmXTqXb2Lc9pih",
    "name": "History of philanthropy",
    "core": false,
    "slug": "history-of-philanthropy",
    "oldSlugs": null,
    "postCount": 17,
    "description": {
      "markdown": "Understanding how people have used money to do good in the past may help donors to do more good in the future. For this reason, the Open Philanthropy Project has commissioned several reports on the **history of philanthropy**.^[\\[1\\]](#fnisvw17pbx3)^ Their research suggests that it may be more effective to be ambitious, to found new organizations, and to build movements advocating for policy change.^[\\[2\\]](#fnhc4ywjxfgav)^\n\nFurther reading\n---------------\n\nKarnofsky, Holden (2012) [Philanthropy’s success stories](https://www.openphilanthropy.org/blog/philanthropys-success-stories), *Open Philanthropy*, March 1.  \n*A summary of OpenPhil's main takeaways from the casebook for* The Foundation: A Great American Secret, *by Joel Fleishman, Scott Kohler and Steven Schindler.*\n\nOpen Philanthropy (2021) [History of philanthropy](http://www.openphilanthropy.org/research/history-of-philanthropy), *Open Philanthropy*.\n\nExternal links\n--------------\n\n[HistPhil](https://histphil.org/). An online publication on the history of philanthropy.\n\nRelated entries\n---------------\n\n[history](https://forum.effectivealtruism.org/tag/history) | [history of effective altruism](https://forum.effectivealtruism.org/tag/history-of-effective-altruism) | [history of existential risk](https://forum.effectivealtruism.org/tag/history-of-existential-risk)\n\n1.  ^**[^](#fnrefisvw17pbx3)**^\n    \n    Open Philanthropy (2021) [History of philanthropy](http://www.openphilanthropy.org/research/history-of-philanthropy), *Open Philanthropy*.\n    \n2.  ^**[^](#fnrefhc4ywjxfgav)**^\n    \n    Karnofsky, Holden (2012) [Philanthropy’s success stories](https://www.openphilanthropy.org/blog/philanthropys-success-stories), *Open Philanthropy*, March 1."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "meXhGpyv97ZPsbn3K",
    "name": "Public giving",
    "core": false,
    "slug": "public-giving",
    "oldSlugs": null,
    "postCount": 17,
    "description": {
      "markdown": "**Public giving** is the practice of making donations publicly. Public giving has the potential to encourage others to give more to charity themselves, and to affect which charities they give to. These considerations have encouraged many members of the effective altruism community to try to be more public about their donations.^[\\[1\\]](#fnspe1riipw6)^\n\nFurther reading\n---------------\n\nWildeford, Peter (2014) [To inspire people to give, be public about your giving](https://forum.effectivealtruism.org/tag/public-giving), *Effective Altruism Forum*, September 16.  \n*An article in favor of being public about donations, to encourage others to donate.*\n\n1.  ^**[^](#fnrefspe1riipw6)**^\n    \n    Kaufman, Jeff (2021) [Donations](https://www.jefftk.com/donations), *Jeff Kaufman’s Blog*."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "hxRMaKvwGqPb43TWB",
    "name": "Research",
    "core": false,
    "slug": "research",
    "oldSlugs": null,
    "postCount": 31,
    "description": {
      "markdown": "For some of the world’s problems, like many forms of cancer, there are currently no solutions. In such cases, research can be a particularly impactful activity. Even in cases where there are ways of dealing with a problem (like HIV), further research may provide more effective tools (like a vaccine).\n\nResearch is often a promising path to impact partly because the knowledge it creates is a non-rival good. Non-rival goods can be provided to a marginal individual at no additional cost. By contrast, when a rival good is consumed, it can’t be consumed by anyone else. Everyone can use Newtonian physics, but the apple Newton ate is lost to the rest of humanity.\n\nUnfortunately, this also means that research tends to be undersupplied by self-interested agents, since the benefits will be widely diffused and hard to monetize. Imagine a piece of research which would be mildly useful to everyone in the world, but is relatively costly to produce. While it would be better for society overall if this line of research was carried out, no individual agent has an incentive to pursue it. Since research tends to be undersupplied by the market despite being potentially very socially valuable, it can be a highly promising path to impact for altruists.\n\nRecognizing this, governments also often fund research. However, governments tend to support the kind of research that is most helpful to their countries and constituencies, rather than focusing on the most promising research areas. As a consequence, they often neglect research into problems that affect individuals with little or no political influence, such as the [global poor](https://forum.effectivealtruism.org/tag/global-poverty), [future people](https://forum.effectivealtruism.org/tag/longtermism), and [nonhuman animals](https://forum.effectivealtruism.org/tag/animal-welfare-1).\n\nFurther reading\n---------------\n\nCotton-Barratt, Owen (2014) [Cost-effectiveness of research: overview](http://www.fhi.ox.ac.uk/cost-effectiveness-of-research-overview/), *Future of Humanity Institute*, December 4.  \n*A series of posts discussing how to estimate the cost-effectiveness of research and related activities.*\n\nGiving What We Can (2015) [Why focus on neglected tropical disease research?](https://www.givingwhatwecan.org/reports/medical-research-1#2-why-focus-on-neglected-tropical-disease-research), in 'Medical research', *Giving What We Can*, September 21 (updated 25 April 2018).  \n*A discussion of why research is plausibly effective.*\n\nOpen Philanthropy (2021) [Scientific research](https://www.openphilanthropy.org/focus/scientific-research), *Open Philanthropy*.  \n*The Open Philanthropy Project’s assessment of scientific research as a focus area.*"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "4cYxRNGdZNWbdxPb7",
    "name": "Dietary change",
    "core": false,
    "slug": "dietary-change",
    "oldSlugs": null,
    "postCount": 60,
    "description": {
      "markdown": "**Dietary change** is seen as a way to reduce [animal suffering](https://forum.effectivealtruism.org/tag/animal-welfare-1), especially in factory farms, by reducing consumption of animal products or avoiding such products altogether.^[\\[1\\]](#fnfgly2nt9p7)^\n\nBesides its direct impact on food production, going vegetarian or vegan can also encourage others to make similar choices, and ultimately help change social norms.^[\\[2\\]](#fn5v73p42jlzg)^ For this reason, many members of the effective altruism community advocate a vegan or vegetarian diet, while some focus on reducing consumption of specific foods associated with most animal deaths or suffering.^[\\[3\\]](#fnbossil6ixy)^^[\\[4\\]](#fnpdjs8iglplj)^\n\nHowever, other members of the community have argued that reducing or eliminating animal products has monetary, attentional, and productivity costs, and that, after accounting for these costs, dietary change may not be a cost-effective intervention for those concerned about animal welfare.^[\\[5\\]](#fnj5nlfvtzvop)^\n\nFurther reading\n---------------\n\nMathur, Maya B. *et al.* (2021) [Interventions to reduce meat consumption by appealing to animal welfare: Meta-analysis and evidence-based recommendations](https://doi.org/10.1016/j.appet.2021.105277), *Appetite*, vol. 164.\n\nTomasik, Brian (2006) [Does vegetarianism make a difference?](https://reducing-suffering.org/does-vegetarianism-make-a-difference/), *Essays on Reducing Suffering*.  \n*A discussion of how vegetarianism can have an impact on suffering.*\n\nTomasik, Brian (2007) [How much direct suffering is caused by various animal foods?](http://reducing-suffering.org/how-much-direct-suffering-is-caused-by-various-animal-foods/), *Essays on Reducing Suffering*.  \n*An analysis of the impact of the production of different foods.*\n\nVinding, Magnus (2020) [Underappreciated consequentialist reasons to avoid consuming animal products](https://magnusvinding.com/2020/10/03/underappreciated-consequentialist-reasons/), *Magnus Vinding’s Blog*, October 3.\n\nWildeford, Peter (2013) [Why eat less meat?](https://www.alignmentforum.org/posts/LbbyQhLkcwAwWmBoj/why-eat-less-meat), *AI Alignment Forum*, July 23.  \n*A clear articulation of the case for eating fewer animal products.*\n\nRelated entries\n---------------\n\n[animal product alternatives](https://forum.effectivealtruism.org/tag/animal-product-alternatives) | [animal welfare](https://forum.effectivealtruism.org/tag/animal-welfare-1) | [cultivated meat](https://forum.effectivealtruism.org/topics/cultivated-meat) | [farmed animal welfare](https://forum.effectivealtruism.org/tag/farmed-animal-welfare) | [marginal charity](https://forum.effectivealtruism.org/tag/marginal-charity)\n\n1.  ^**[^](#fnreffgly2nt9p7)**^\n    \n    Wildeford, Peter (2013) [Why eat less meat?](https://www.alignmentforum.org/posts/LbbyQhLkcwAwWmBoj/why-eat-less-meat), *AI Alignment Forum*, July 23.\n    \n2.  ^**[^](#fnref5v73p42jlzg)**^\n    \n    Tomasik, Brian (2006) [Does vegetarianism make a difference?](https://reducing-suffering.org/does-vegetarianism-make-a-difference/), *Essays on Reducing Suffering*.\n    \n3.  ^**[^](#fnrefbossil6ixy)**^\n    \n    Galef, Julia (2011) [Want to kill fewer animals? Give up eggs](https://web.archive.org/web/20150511174032/https://blogs.scientificamerican.com/guest-blog/want-to-kill-fewer-animals-give-up-eggs-not-meat), *Scientific American Guest Blog*, August 11.\n    \n4.  ^**[^](#fnrefpdjs8iglplj)**^\n    \n    Tomasik, Brian (2007) [How much direct suffering is caused by various animal foods?](http://reducing-suffering.org/how-much-direct-suffering-is-caused-by-various-animal-foods/), *Essays on Reducing Suffering*.\n    \n5.  ^**[^](#fnrefj5nlfvtzvop)**^\n    \n    Lewis, Gregory (2015) [Don’t sweat diet?](https://forum.effectivealtruism.org/posts/Nxmshrz3EeJb7Ng3w/don-t-sweat-diet), *Effective Altruism Forum*, October 22."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "SByiorMd9eYR9rQjf",
    "name": "Ethics of personal consumption",
    "core": false,
    "slug": "ethics-of-personal-consumption",
    "oldSlugs": null,
    "postCount": 22,
    "description": {
      "markdown": "The **ethics of personal consumption** concerns the ethical issues arising from our consumption choices.\n\nThe consumption of certain products is likely to have direct harmful consequences. For example, eating meat will, in expectation, increase the number of animals in factory farms. In addition, consuming some of these products may support harmful social norms. For example, eating meat can support the view that the interests of other species should be discounted or ignored.\n\nMoreover, the issues are not purely [consequentialist](https://forum.effectivealtruism.org/tag/consequentialism), and there are [deontological constraints](https://forum.effectivealtruism.org/tag/deontology) on consumption choices (for instance, it might be unacceptable to buy from sweatshops even if refusing to do so did not create tangibly better outcomes).\n\nThere are, however, reasons to think that changing one’s own consumption decisions is not a cost-effective way to spend one’s time or money. One such factor is opportunity cost: spending more resources on ethically sourced products leaves fewer resources to spend on even more important projects. And in many cases, one person buying less of something can indirectly lead to others buying more, [offsetting](https://forum.effectivealtruism.org/tag/moral-offsetting) some or all the good done.\n\nBudgeting for yourself and others\n---------------------------------\n\nA dollar donated to a cost-effective charity can do over a hundred times more good as a dollar spent on personal consumption.^[\\[1\\]](#fnft9y3z7fu86)^ This might seem to lead to the conclusion that all money not spent on the essentials of life should be donated to charity. However, allowing oneself some discretionary spending may well be necessary to happiness, [productivity](https://forum.effectivealtruism.org/topics/productivity) and commitment to giving.  Most members of the community budget reasonable portions of their income for themselves, to stay motivated, prevent burnout, and increase productivity.^[\\[2\\]](#fnzioq5yunc)^\n\nSome members of the effective altruism community suggest setting a “charity budget”: a clearly defined amount of money to be given to charity each year.^[\\[3\\]](#fnnof4ieltox)^ This allows people to make a decision once per year, rather than every time they purchase something for themselves, which can help to reduce emotional distress. Others have argued that self-investment (for instance in respectable clothing) can increase one’s efficacy in many ways, and that therefore it may be worth prioritizing a certain level of self-investment over direct donations.^[\\[4\\]](#fndtx8485l72v)^\n\nEnvironmental consumption\n-------------------------\n\nClimate change could potentially have a major impact on human health and wellbeing. Other environmental issues like air pollution and environmental degradation are already having serious consequences.\n\nEnvironmentalists often favor interventions that involve changes in personal consumption: travelling less by plane, offsetting carbon emissions, buying more second-hand items, and replacing goods less often, as well as forms of [dietary change](https://forum.effectivealtruism.org/tag/dietary-change). Members of the effective altruist community have generally emphasized other approaches, such as political advocacy and donating to effective environmental charities.\n\nFurther reading\n---------------\n\nKuhn, Ben (2013) [Conversation with Alice Yu on effective environmentalism](https://www.benkuhn.net/alice/), *Ben Kuhn’s Blog*, December.  \n*A general discussion of effective environmentalism, including criticism of local and organic food.*\n\nMacAskill, William (2015) [*Doing Good Better: How Effective Altruism Can Help You Make a Difference*](https://en.wikipedia.org/wiki/Special:BookSources/9781592409662), New York: Random House.  \n*Chapter 1 introduces the idea of the ‘100x Multiplier’, according to which people in developed societies can do at least 100 times more to benefit others as they can to benefit themselves.*\n\nZabel, Claire (2016) [Ethical offsetting is antithetical to EA](https://forum.effectivealtruism.org/posts/Yix7BzSQLJ9TYaodG/ethical-offsetting-is-antithetical-to-ea), *Effective Altruism Forum*, January 5.  \n*A general argument against offsetting.*\n\nRelated entries\n---------------\n\n[dietary change](https://forum.effectivealtruism.org/tag/dietary-change) | [fair trade](https://forum.effectivealtruism.org/tag/fair-trade) | [marginal charity](https://forum.effectivealtruism.org/tag/marginal-charity) | [moral offsetting](https://forum.effectivealtruism.org/tag/moral-offsetting)\n\n1.  ^**[^](#fnrefft9y3z7fu86)**^\n    \n    MacAskill, William (2015) [*Doing Good Better: How Effective Altruism Can Help You Make a Difference*](https://en.wikipedia.org/wiki/Special:BookSources/9781592409662), New York: Random House.\n    \n2.  ^**[^](#fnrefzioq5yunc)**^\n    \n    Kaufman, Jeff (2013) [Keeping choices donation neutral](https://www.jefftk.com/p/keeping-choices-donation-neutral), *Jeff Kaufman’s Blog*, May 11.\n    \n3.  ^**[^](#fnrefnof4ieltox)**^\n    \n    Wise, Julia (2015) [Burnout and self-care](https://forum.effectivealtruism.org/posts/ZGW8Tmc6mDWZTnqyo/burnout-and-self-care), *Effective Altruism Forum*, October 23.\n    \n4.  ^**[^](#fnrefdtx8485l72v)**^\n    \n    Hurford, Peter (2014) [You have a set amount of “weirdness points”. Spend them wisely](https://forum.effectivealtruism.org/posts/MH9suFZbxXCYsr5Z5/you-have-a-set-amount-of-weirdness-points-spend-them-wisely), *Effective Altruism Forum*, November 27."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "TZxL3u4v53Tp5pe7w",
    "name": "Research careers",
    "core": false,
    "slug": "research-careers",
    "oldSlugs": null,
    "postCount": 14,
    "description": {
      "markdown": "[Research](https://forum.effectivealtruism.org/tag/research) into important areas is a potentially very high impact career.\n\nSome of the people who have had the highest impact have been researchers. By developing code breaking machines that allowed the Allies to be far more effective against Nazi U-boats in World War II, mathematician Alan Turing may have saved millions of lives.^[\\[1\\]](#fnir2soclldq)^ Similarly, Norman Borlaug’s development of short-stem disease-resistant wheat is estimated to have saved *tens* of millions of lives.^[\\[2\\]](#fnvui8je85b9h)^\n\nResearch has a significant influence on economic growth. Since a very small proportion of the population is involved in research, the value contributed by the average researcher is significant. Furthermore, the best researchers have a much higher impact than average (median) researchers; so a person who is good fit for research might be able to have a very large impact.\n\nFurther reading\n---------------\n\nTodd, Benjamin (2014) [Which jobs help people the most?](https://80000hours.org/career-guide/high-impact-jobs/), *80,000 Hours*, October.  \n*An overview of research as a promising career approach.*\n\n1.  ^**[^](#fnrefir2soclldq)**^\n    \n    Copeland, Jack (2012) [Alan Turing: The codebreaker who saved “millions of lives”](https://www.bbc.com/news/technology-18419691), *BBC News*, June 19.\n    \n2.  ^**[^](#fnrefvui8je85b9h)**^\n    \n    MacAskill, Will (2015) [*Doing Good Better: How Effective Altruism Can Help You Make a Difference*](https://en.wikipedia.org/wiki/Special:BookSources/9781592409662), New York: Gotham Books.  \n    *Chapter 9 discusses research as a potentially very high impact career.*"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "Asn9cXBLumGiCJQ5g",
    "name": "Epistemology",
    "core": false,
    "slug": "epistemology",
    "oldSlugs": null,
    "postCount": 41,
    "description": {
      "markdown": "**Epistemology** is the study of how people should form [credences](https://forum.effectivealtruism.org/tag/credence) about the nature of the world. \n\nBeliefs and credences are purely evaluative attitudes: they are simply about the way that we think the world is. A person might believe that it will rain, for example, even though they hope that it will not.\n\nBeliefs are all-or-nothing attitudes: we either believe that it will rain or we don't believe that it will rain. Credences, on the other hand, reflect how likely we think it is that something is true, expressed as a real number between 0 and 1. For example, we might think that there is a 80% chance that it will rain, and therefore have a credence of 0.8 that it will rain.\n\nIt is widely held that beliefs are rational if they are supported by our evidence. And credences are rational if they follow the probability axioms (e.g. a credence should never be greater than 1 in any event) and are revised in accordance with Bayes’ rule.\n\nImproving the accuracy of beliefs\n---------------------------------\n\nOne way to improve a person’s capacity to do good is to increase the accuracy of their beliefs. Since people’s actions are determined by their desires and their beliefs, a person aiming to do good will generally do more good the more accurate their beliefs are.\n\nExamples of belief-improving work include reading books, crafting arguments in moral philosophy, writing articles about important problems, and making scientific discoveries.\n\nTwo distinctions are relevant in this context. First, a person can build capacity by improving either their factual or their normative beliefs. Second, a person can build capacity by improving either particular beliefs or general processes of belief-formation.\n\nFurther reading\n---------------\n\nSteup, Matthias & Ram Neta (2005) [Epistemology](https://plato.stanford.edu/archives/fall2020/entries/epistemology/), *The Stanford Encyclopedia of Philosophy*, December 14 (updated 11 April 2020).\n\nRelated entries\n---------------\n\n[Bayesian epistemology](https://forum.effectivealtruism.org/tag/bayesian-epistemology) | [decision theory](https://forum.effectivealtruism.org/tag/decision-theory) | [epistemic deference](https://forum.effectivealtruism.org/tag/epistemic-deference) | [rationality](https://forum.effectivealtruism.org/tag/rationality)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "DatwbfA8aYEYFMeif",
    "name": "Expected value",
    "core": false,
    "slug": "expected-value",
    "oldSlugs": null,
    "postCount": 28,
    "description": {
      "markdown": "The **expected value** of an act is the sum of the value of each of its possible outcomes multiplied by their probability of occurring.\n\nIllustration\n------------\n\nConsider a person choosing between two medical interventions. The first intervention is a drug that succeeds in 60% of cases, and that gives an extra year of healthy life when it succeeds, and has no impact if it fails.\n\nIn the case of this drug, there are only two outcomes: success and failure. So the expected value is:\n\n*(1 year of life × 60%) + (0 years of life ×40%) = 0.6 expected years of life*\n\nConsider another drug that succeeds with a 40% probability, and gives two years of healthy life when it succeeds, but causes harm equal to half a year of healthy life lost when it fails.\n\nThen the expected benefit of this intervention is:\n\n*(2 years of life × 40%)−(0.5 years of life × 60%) = 0.5 expected years of life*\n\nOver many cases, the first drug will likely provide more years of healthy life than the second. So if they cost the same, funding the first drug would add more healthy years of life on average.\n\nFurther reading\n---------------\n\nConley, Sean (2016) [Deworming might have huge impact, but might have close to zero impact](https://blog.givewell.org/2016/07/26/deworming-might-huge-impact-might-close-zero-impact/), *The GiveWell Blog*, July 26.  \n*An example of research using expected value thinking.*\n\nKarnofsky, Holden (2011) [Why we can’t take expected value estimates literally (even when they’re unbiased)](https://blog.givewell.org/2011/08/18/why-we-cant-take-expected-value-estimates-literally-even-when-theyre-unbiased/), *The GiveWell Blog*, August 18 (updated 25 July 2016).  \n*A caution about taking applied expected value estimates literally.*\n\nTodd, Benjamin (2021) [Expected value: how can we make a difference when we’re uncertain what’s true?](https://80000hours.org/articles/expected-value/), *80,000 Hours*, September.  \n*An introduction to the concept of expected value.*\n\nTomasik, Brian (2006) [Does vegetarianism make a difference?](https://reducing-suffering.org/does-vegetarianism-make-a-difference/), *Essays on Reducing Suffering* (updated 25 January 2014).  \n*Another example of expected value reasoning.*\n\nWikipedia (2010) [Von Neumann–Morgenstern utility theorem](https://en.wikipedia.org/wiki/Von_Neumann%E2%80%93Morgenstern_utility_theorem), *Wikipedia*, April 10 (updated 13 February 2021‎).  \n*For proofs that rational agents should select projects with the highest expected value (note that this does not imply economic risk neutrality).*\n\nRelated entries\n---------------\n\n[alternatives to expected value theory](https://forum.effectivealtruism.org/tag/alternatives-to-expected-value-theory) | [altruistic wager](https://forum.effectivealtruism.org/tag/altruistic-wager) | [cluelessness](https://forum.effectivealtruism.org/tag/cluelessness) | [credence](https://forum.effectivealtruism.org/tag/credence) | [decision theory](https://forum.effectivealtruism.org/tag/decision-theory) | [moral uncertainty](https://forum.effectivealtruism.org/tag/moral-uncertainty) | [risk aversion](https://forum.effectivealtruism.org/tag/risk-aversion) | [value of information](https://forum.effectivealtruism.org/tag/value-of-information)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "wnkQJprHFdneqsLtW",
    "name": "Forethought Foundation",
    "core": false,
    "slug": "forethought-foundation",
    "oldSlugs": null,
    "postCount": 5,
    "description": {
      "markdown": "The **Forethought Foundation for Global Priorities Research** (**FF**) is a [global priorities research](https://forum.effectivealtruism.org/tag/global-priorities-research) center.\n\nFF was founded in 2018 by [William MacAskill](https://forum.effectivealtruism.org/tag/william-macaskill). It is part of the [Centre for Effective Altruism](https://forum.effectivealtruism.org/tag/centre-for-effective-altruism-1), and works closely with the [Global Priorities Institute](https://forum.effectivealtruism.org/tag/global-priorities-institute).\n\nAs of March 2022, FF has three primary research areas: [longtermism](https://forum.effectivealtruism.org/tag/longtermism),^[\\[1\\]](#fn16nl2ty4r1z)^ mitigation of [global catastrophic risk](https://forum.effectivealtruism.org/tag/global-catastrophic-risk),^[\\[2\\]](#fnyqlva2qiogk)^ and affecting the [long-run future](https://forum.effectivealtruism.org/tag/long-term-future).^[\\[3\\]](#fnz6x8h6pfklb)^\n\nExternal links\n--------------\n\n[Forethought Foundation for Global Priorities Research](https://www.forethought.org/). Official website.\n\n[Apply for a job](https://www.forethought.org/opportunities).\n\nRelated entries\n---------------\n\n[existential risk](https://forum.effectivealtruism.org/tag/existential-risk) | [global catastrophic risk](https://forum.effectivealtruism.org/tag/global-catastrophic-risk) | [global priorities research](https://forum.effectivealtruism.org/tag/global-priorities-research) |  [long-term future](https://forum.effectivealtruism.org/tag/long-term-future) | [longtermism](https://forum.effectivealtruism.org/tag/longtermism)\n\n1.  ^**[^](#fnref16nl2ty4r1z)**^\n    \n    Forethought Foundation (2021) [Longtermism](https://www.forethought.org/longtermism), *Forethought Foundation*.\n    \n2.  ^**[^](#fnrefyqlva2qiogk)**^\n    \n    Forethought Foundation (2021) [Mitigating catastrophic risk](https://www.forethought.org/mitigating-extinction-and-catastrophe), *Forethought Foundation*.\n    \n3.  ^**[^](#fnrefz6x8h6pfklb)**^\n    \n    Forethought Foundation (2021) [Affecting the very long run](https://www.forethought.org/affecting-the-very-long-run), *Forethought Foundation*."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "QdH9f8TC6G8oGYdgt",
    "name": "Animal welfare",
    "core": false,
    "slug": "animal-welfare-1",
    "oldSlugs": null,
    "postCount": 76,
    "description": {
      "markdown": "**Animal welfare** is a core focus area within [effective altruism](https://forum.effectivealtruism.org/topics/effective-altruism). It focuses on ways to improve the [wellbeing](https://forum.effectivealtruism.org/topics/wellbeing) of nonhuman animals, primarily [in factory farms](https://forum.effectivealtruism.org/topics/farmed-animal-welfare) and [in the wild](https://forum.effectivealtruism.org/topics/wild-animal-welfare).\n\nAnimals currently make up the vast majority of sentient beings. As many as 2.8 trillion animals might be killed for food each year, and there are many more wild animals.^[\\[1\\]](#fnh53dfy2msjh)^  Although many experts agree that animals display some signs of sentience, it remains an open question [which animals are sentient](https://forum.effectivealtruism.org/tag/animal-sentience), and how we should compare welfare between species.\n\nDespite its [importance](https://forum.effectivealtruism.org/tag/importance), this focus area is highly [neglected](https://forum.effectivealtruism.org/tag/neglectedness): animal charities receive only 3% of charitable donations, and 99% of that money is focused on pets, who make up less than 0.1% of all domesticated animals.^[\\[1\\]](#fnh53dfy2msjh)^\n\nThe area also appears to be somewhat [tractable](https://forum.effectivealtruism.org/tag/tractability). [Open Philanthropy](https://forum.effectivealtruism.org/tag/open-philanthropy) estimates that past [corporate cage-free campaigns](https://forum.effectivealtruism.org/tag/corporate-cage-free-campaigns) have spared about 120 years of suffering in battery cages per dollar donated.^[\\[2\\]](#fn9p99275w97f)^\n\nSome people focus more narrowly on [farmed animal welfare](https://forum.effectivealtruism.org/topics/farmed-animal-welfare) or on [wild animal welfare](https://forum.effectivealtruism.org/topics/wild-animal-welfare).\n\nSome have argued that, if one takes the view that future beings matter morally, interventions that seek to reduce short-term animal suffering may not be as cost-effective as interventions that seek to influence [the far future](https://forum.effectivealtruism.org/tag/longtermism). In response, others have argued that animal welfare will also have strong beneficial effects on the far future.^[\\[1\\]](#fnh53dfy2msjh)^ An objection to that argument is that it would be remarkable if an intervention selected for its short-term effects just happened to have better long-run consequences than all other interventions.^[\\[3\\]](#fnvsgdg1eotb)^ In addition, critics have noted that there are theoretical reasons for expecting human interventions to have better long-run effects than animal interventions.^[\\[4\\]](#fn0vj5j5qvz53)^ The considerations bearing on this issue are numerous, complex, and hard to assess, and the debate is currently unresolved.^[\\[5\\]](#fn6xgz1agm288)^\n\nFurther reading\n---------------\n\nMacAskill, W. & Meissner, D. (2020) [Cause prioritization: Farm animal welfare](https://www.utilitarianism.net/acting-on-utilitarianism#farm-animal-welfare), in 'Acting on utilitarianism', *Utilitarianism*, October.\n\nReese, Jacy (2016) [Why animals matter for effective altruism](https://forum.effectivealtruism.org/posts/ch5fq73AFn2Q72AMQ/why-animals-matter-for-effective-altruism), *Effective Altruism Forum*, August 22.\n\nWhittlestone, Jess (2017) [Animal welfare](https://www.effectivealtruism.org/articles/cause-profile-animal-welfare), *Effective Altruism*, November 16.\n\nRelated entries\n---------------\n\n[animal cognition](https://forum.effectivealtruism.org/tag/animal-cognition) | [animal sentience](https://forum.effectivealtruism.org/tag/animal-sentience) | [effective animal advocacy](https://forum.effectivealtruism.org/tag/effective-animal-advocacy) | [farmed animal welfare](https://forum.effectivealtruism.org/tag/farmed-animal-welfare) | [pain and suffering](https://forum.effectivealtruism.org/topics/pain-and-suffering) | [speciesism](https://forum.effectivealtruism.org/topics/speciesism) | [wild animal welfare](https://forum.effectivealtruism.org/tag/wild-animal-welfare)\n\n1.  ^**[^](#fnrefh53dfy2msjh)**^\n    \n    Reese, Jacy (2016) [Why animals matter for effective altruism](https://forum.effectivealtruism.org/posts/ch5fq73AFn2Q72AMQ/why-animals-matter-for-effective-altruism), *Effective Altruism Forum*, August 22.\n    \n2.  ^**[^](#fnref9p99275w97f)**^\n    \n    Bollard, Lewis (2016) [Initial grants to support corporate cage-free reforms](https://www.openphilanthropy.org/blog/initial-grants-support-corporate-cage-free-reforms), *Open Philanthropy*, March 31.\n    \n3.  ^**[^](#fnrefvsgdg1eotb)**^\n    \n    Lewis, Gregory (2016) [Beware surprising and suspicious convergence](https://forum.effectivealtruism.org/posts/omoZDu8ScNbot6kXS/beware-surprising-and-suspicious-convergence), *Effective Altruism Forum*, January 24.\n    \n4.  ^**[^](#fnref0vj5j5qvz53)**^\n    \n    Cotton-Barratt, Owen (2014) [Human and animal interventions: the long- term view](https://forum.effectivealtruism.org/posts/ndvcrHfvay7sKjJGn/human-and-animal-interventions-the-long-term-view), *Effective Altruism Forum*, June 2.\n    \n5.  ^**[^](#fnref6xgz1agm288)**^\n    \n    Tomasik, Brian (2011) [Risks of astronomical future suffering](https://longtermrisk.org/risks-of-astronomical-future-suffering/), *Center on Long-Term Risk*, December."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "ad4KXgJTAB3qrnLRa",
    "name": "Alternatives to expected value theory ",
    "core": false,
    "slug": "alternatives-to-expected-value-theory",
    "oldSlugs": null,
    "postCount": 2,
    "description": {
      "markdown": "Although [expected value](https://forum.effectivealtruism.org/tag/expected-value) theory is the dominant paradigm in decision theory, it has problems, such as those raised by bounded and unbounded values, and by causal and evidential approaches. This leaves open the possibility of discovering an alternative decision theory.\n\nOne possibility would be to introduce some form of [risk aversion](https://forum.effectivealtruism.org/tag/risk-aversion) into the decision theory, such as that proposed by Lara Buchak.^[\\[1\\]](#fninur4qzf36)^ Other alternative theories include rank-dependent expected utility theory, prospect theory, and regret theory, although some of these are more commonly used as descriptive theories than normative models.\n\nFurther reading\n---------------\n\nBriggs, Rachael (2015) [Review of Lara Buchak, *Risk and Rationality*](http://ndpr.nd.edu/news/56397-risk-and-rationality/), *Notre Dame Philosophical Reviews*, March 14.\n\nBuchak, Lara (2013) [*Risk and Rationality*](https://en.wikipedia.org/wiki/Special:BookSources/9780199672165), Oxford: Oxford University Press.  \n*Defends risk aversion as rational.*\n\nTuthill, Jonathan & Darren Frechette (2002) [Non-expected utility theories: Weighted expected, rank dependent, and, cumulative prospect theory utility](http://www.farmdoc.illinois.edu/nccc134/conf_2002/pdf/confp20-02.pdf), *Proceedings of the NCR-134 Conference on Applied Commodity Price Analysis, Forecasting, and Market Risk Management*, St. Louis, MO.\n\nRelated entries\n---------------\n\n[altruistic wager](https://forum.effectivealtruism.org/tag/altruistic-wager) | [cluelessness](https://forum.effectivealtruism.org/tag/cluelessness) | [decision theoretic uncertainty](https://forum.effectivealtruism.org/tag/decision-theoretic-uncertainty) | [decision theory](https://forum.effectivealtruism.org/tag/decision-theory) | [expected value](https://forum.effectivealtruism.org/tag/expected-value) | [risk aversion](https://forum.effectivealtruism.org/tag/risk-aversion) | [value of information](https://forum.effectivealtruism.org/tag/value-of-information)\n\n1.  ^**[^](#fnrefinur4qzf36)**^\n    \n    Buchak, Lara (2013) [*Risk and Rationality*](https://en.wikipedia.org/wiki/Special:BookSources/9780199672165), Oxford: Oxford University Press."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "opMqQFxjFRp7GHWyT",
    "name": "Timing of existential risk mitigation",
    "core": false,
    "slug": "timing-of-existential-risk-mitigation",
    "oldSlugs": [
      "allocating-risk-mitigation-work-over-time"
    ],
    "postCount": null,
    "description": {
      "markdown": "Some risks, like climate change, are likely to have only moderate effects in the near term but potentially large ones in [the long term](https://forum.effectivealtruism.org/tag/longtermism). Others, like [asteroid risk](https://forum.effectivealtruism.org/tag/natural-existential-risk), are roughly evenly distributed over time. Even within risk areas, the mechanism that creates the risk can be different at different time periods. For example, if climate change does create large impacts in the next decade, it is likely to be because of unforseen feedback effects, rather than because of accumulation of warming from industrial emissions.\n\nIn all of these cases, the work that one should do to mitigate risk will differ depending on which time-frames one is considering. A balanced portfolio of risk-mitigation strategies therefore needs to allocate resources against these different timelines.\n\nTwo separate considerations favor a focus on near-term risk. First, we are short-sighted about the nature of future risks and what can be done about them, so our efforts are better targeted when acting on near-term risks. Second, we are the only ones who can act now, whereas future risks can be addressed by others later.\n\nOn the other hand, risks that emerge in the future might be larger. Moreover, working now on building capacity to address risks in the future might help scale up the total amount of effort going to risk reduction.\n\nThe optimal strategy for a risk community is likely to be a mixture of work targeting different time-horizons. There are some basic quantitative models suggesting how to address this balance.^[\\[1\\]](#fnks0qybpnxcs)^\n\nFurther reading\n---------------\n\nCotton-Barratt, Owen (2015) [Allocating risk mitigation across time](https://www.fhi.ox.ac.uk/reports/2015-2.pdf), technical report #2015-2, Future of Humanity Institute, University of Oxford.\n\nOrd, Toby (2014) [The timing of labour aimed at reducing existential risk](https://www.fhi.ox.ac.uk/the-timing-of-labour-aimed-at-reducing-existential-risk/), *Future of Humanity Institute*, July 3.\n\n1.  ^**[^](#fnrefks0qybpnxcs)**^\n    \n    Cotton-Barratt, Owen (2015) [Allocating risk mitigation across time](https://www.fhi.ox.ac.uk/reports/2015-2.pdf), technical report #2015-2, Future of Humanity Institute, University of Oxford."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "X2jgq6Zf58PveQMkc",
    "name": "Aid and paternalism",
    "core": false,
    "slug": "aid-and-paternalism",
    "oldSlugs": null,
    "postCount": 8,
    "description": {
      "markdown": "A popular objection to aid programs is that they involve an unjustifiable form of paternalism. According to this objection, aid programs presuppose that international development agencies and donors from high income countries can understand and solve the problems faced by individuals in developing countries better than those individuals or their governments.\n\nOne approach to address this concern is to focus on areas in which outsiders have a good track record of effectively contributing targeted expertise, such as health and nutrition. This would then empower citizens from developing countries to improve areas in which outsiders lack such expertise. Another approach is to favor unconditional [cash transfers](https://forum.effectivealtruism.org/tag/cash-transfers). This would enable recipients to decide how best to use the resources.\n\nThere have also been attempts by EA organizations to understand and include the moral values of their potential beneficiaries into their decision making processes .^[\\[1\\]](#fns59docp0bks)^\n\nFurther reading\n---------------\n\nHalstead, John (2017) [Where should anti-paternalists donate?](https://johnhalstead.org/index.php/2017/05/04/anti-paternalists-donate/), *John Halstead’s Blog*, May 4.\n\nGiveWell (2019) [IDinsight — Beneficiary preferences survey (2019)](https://www.givewell.org/research/incubation-grants/IDinsight-beneficiary-preferences-march-2019), *GiveWell*, May (updated December 2019).\n\nKarnofsky, Holden (2012) [How not to be a “white in shining armor”](http://blog.givewell.org/2012/04/12/how-not-to-be-a-white-in-shining-armor/), *GiveWell*, April 12.\n\n1.  ^**[^](#fnrefs59docp0bks)**^\n    \n    Redfern, Alice (2019) [Moral weights in the developing world — IDinsight’s Beneficiary Preferences Project](https://forum.effectivealtruism.org/posts/mKGbeX5tQu4zshY4j/alice-redfern-moral-weights-in-the-developing-world), *EAGlobal*, December 19."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "APHcr3kgke7jpCsk8",
    "name": "Russia",
    "core": false,
    "slug": "russia",
    "oldSlugs": null,
    "postCount": 54,
    "description": {
      "markdown": "The **Russia** tag is for posts that are about Russia, that address how Russia is relevant to various issues EAs care about, or that are relevant to how one could have an impact by engaging with Russia.\n\nRelated entries\n---------------\n\n[global outreach](https://forum.effectivealtruism.org/tag/global-outreach) | [international relations](https://forum.effectivealtruism.org/tag/international-relations) | [Ukraine](https://forum.effectivealtruism.org/tag/ukraine)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "u9DvFQmdxmNAySesq",
    "name": "Game theory",
    "core": false,
    "slug": "game-theory",
    "oldSlugs": null,
    "postCount": 11,
    "description": {
      "markdown": "**Game theory** is the study of the structure and the rational strategies for performing in games or gamelike human interactions.\n\nA key concept of game theory is the Nash Equilibrim. If all players have chosen a strategy and no player can benefit from changing their strategy unless other players also change their strategy, we are in a Nash equilibrium.\n\nAs an illustration, consider the problem known as \"the prisoner's dilemma.\" Two suspects face imprisonment, but can get a reduced sentence by betraying each other. However, if neither of them betrays each other, they get a shorter sentence than if both do.\n\nConditional on both prisoners' expected utility functions being such that they each gain utility from getting a shorter prison sentence, but not from the other prisoner getting a shorter prison sentence, both defecting is a Nash equilibrium, even though that is worse for both than both cooperating.\n\nOne real-world analogy to the prisoner's dilemma is climate change mitigation, where it may not be rational for countries to reduce their carbon emissions if they cannot ensure that other countries will as well. Such cases illustrate the usefulness of group decision-making procedures.\n\nFurther reading\n---------------\n\nAlexander, Scott (2014) [Meditations on Moloch](https://slatestarcodex.com/2014/07/30/meditations-on-moloch/), *Slate Star Codex*, July 30.\n\nRoss, Don (1997) [Game theory](https://plato.stanford.edu/entries/game-theory/), *Stanford Encyclopedia of Philosophy*, January 25 (updated 16 December 2019).\n\nWikipedia (2001) [Game theory](https://en.wikipedia.org/wiki/Game_theory), *Wikipedia*, October 31 (updated 28 April 2021).\n\nWikipedia (2001) [Prisoner’s dilemma](https://en.wikipedia.org/wiki/Prisoner%27s_dilemma), *Wikipedia*, August 21 (updated 1 April 2021).\n\nWikipedia (2001) [Tragedy of the commons](https://en.wikipedia.org/wiki/Tragedy_of_the_commons), *Wikipedia*, September 30 (updated 2 May 2021).\n\nWikipedia (2002) [Nash equilibrium](https://en.wikipedia.org/wiki/Nash_equilibrium), *Wikipedia*, March 21 (updated 20 April 2021).\n\nRelated entries\n---------------\n\n[decision theory](https://forum.effectivealtruism.org/tag/decision-theory) | [economics](https://forum.effectivealtruism.org/tag/economics) | [philanthropic coordination](https://forum.effectivealtruism.org/tag/philanthropic-coordination)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "g9QiC7sZRsczqQ7cY",
    "name": "Peace and conflict studies",
    "core": false,
    "slug": "peace-and-conflict-studies",
    "oldSlugs": null,
    "postCount": 6,
    "description": {
      "markdown": "**Peace and conflict studies** is the interdisciplinary study of the causes of peace and conflict.\n\nRelated entries\n---------------\n\n[armed conflict](https://forum.effectivealtruism.org/tag/armed-conflict) | [global governance](https://forum.effectivealtruism.org/tag/global-governance)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "QLgHcETSqb4a7Kf7Q",
    "name": "Qualia Research Institute",
    "core": false,
    "slug": "qualia-research-institute",
    "oldSlugs": [
      "qri"
    ],
    "postCount": 20,
    "description": {
      "markdown": "The **Qualia Research Institute** (**QRI**) is a nonprofit research organization based in San Francisco that focuses on [consciousness research](https://forum.effectivealtruism.org/tag/consciousness-research). QRI's goals are to develop a precise mathematical language for describing phenomenal experience; to understand the nature of emotional valence; and to develop technologies based on this research that improve people’s lives. It was founded in December 2018 by Andrés Gómez Emilsson, Mike Johnson and Romeo Stevens.^[\\[1\\]](#fnvx6v2ocfclk)^\n\nSince 2021 QRI's board of advisors includes [Scott Alexander](https://forum.effectivealtruism.org/tag/scott-alexander) and [David Pearce](https://forum.effectivealtruism.org/tag/david-pearce-1).^[\\[1\\]](#fnvx6v2ocfclk)^\n\nFurther reading\n---------------\n\nShinozuka, Kenneth (2019) [Why we need to study consciousness](https://blogs.scientificamerican.com/observations/why-we-need-to-study-consciousness/), *Scientific American*, September.\n\nZuckerman, Andrew (2021) [Qualia Research Institute: history & 2021 strategy](https://qri.org/blog/History-2021-Strategy), *Qualia Research Institute*, January 25.\n\nExternal links\n--------------\n\n[Qualia Research Institute](https://qri.org/). Official website.\n\n[Apply for a job](https://qri.org/careers).\n\n1.  ^**[^](#fnrefvx6v2ocfclk)**^\n    \n    Zuckerman, Andrew (2021) [Qualia Research Institute: history & 2021 strategy](https://www.qualiaresearchinstitute.org/blog/history-2021-strategy), *Qualia Research Institute*, January 25."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "8zYowZJ2dFZchuC39",
    "name": "Wikis",
    "core": false,
    "slug": "wikis",
    "oldSlugs": null,
    "postCount": 28,
    "description": {
      "markdown": "The **Wikis** tag is for posts about the EA Forum Wiki, Wikipedia, or other wiki-style projects. This includes posts about the value of editing Wikipedia or creating new Wikipedia pages, and posts about the value of or attempts at building EA-focused wikis.\n\nExternal links\n--------------\n\n[WikiProject Effective Altruism](https://en.wikipedia.org/wiki/Wikipedia:WikiProject_Effective_Altruism). An initiative to improve Wikipedia's coverage of topics relevant to effective altruism.\n\n[LessWrong Wiki](https://www.lesswrong.com/tags/all).\n\n[Forecasting Wiki](https://forecasting.wiki/wiki/Main_Page).\n\nRelated entries\n---------------\n\n[community infrastructure](https://forum.effectivealtruism.org/tag/community-infrastructure) | [Effective Altruism Forum](https://forum.effectivealtruism.org/tag/effective-altruism-forum-1)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "nLAPBnCKPHRejXAg9",
    "name": "Donation matching",
    "core": false,
    "slug": "donation-matching",
    "oldSlugs": [
      "donation-matching"
    ],
    "postCount": 44,
    "description": {
      "markdown": "**Donation matching** (also known as **match funding**, **matched giving** and **matching funds**) is an initiative in which a large funder (often an employer) matches individual contributions according to a predetermined match ratio (usually 1:1).\n\n[EA Giving Tuesday](https://forum.effectivealtruism.org/tag/ea-giving-tuesday) is one prominent example.\n\nFurther reading\n---------------\n\nKuhn, Ben (2015) [Does donation matching work?](https://www.benkuhn.net/matching/), *Ben Kuhn’s Blog*, January.\n\nRelated entries\n---------------\n\n[EA Giving Tuesday](https://forum.effectivealtruism.org/tag/ea-giving-tuesday) | [effective giving](https://forum.effectivealtruism.org/tag/effective-giving)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "4aNS4s7SZaXxuobhF",
    "name": "EA Giving Tuesday",
    "core": false,
    "slug": "ea-giving-tuesday",
    "oldSlugs": [
      "giving-tuesday"
    ],
    "postCount": 18,
    "description": {
      "markdown": "**EA Giving Tuesday** is an initiative to direct [matching funds](https://forum.effectivealtruism.org/tag/donation-matching) from Facebook's Giving Tuesday program to EA-aligned organizations.\n\nBackground\n----------\n\nGiving Tuesday is a holiday celebrated on the first Tuesday after American Thanksgiving (either the last week of November or first week of December). Its organizers describe it as^[\\[1\\]](#fnz8hvumdayt8)^\n\n> a global generosity movement unleashing the power of people and organizations to transform their communities and the world. GivingTuesday was created in 2012 as a simple idea: a day that encourages people to do good. Since then, it has grown into a year-round global movement that inspires hundreds of millions of people to give, collaborate, and celebrate generosity.\n\nFor the past several years, Facebook has matched some donations made through their site on Giving Tuesday.^[\\[2\\]](#fnoqfwavryail)^ Since 2017, a group of effective altruists have organized to direct as much of this as possible to effective charities.^[\\[3\\]](#fn79sbizv8x2a)^\n\nExternal links\n--------------\n\n[EA Giving Tuesday](https://www.eagivingtuesday.org/). Official website.\n\n1.  ^**[^](#fnrefz8hvumdayt8)**^\n    \n    GivingTuesday (2021) [About](https://hq.givingtuesday.org/about/), *GivingTuesday*.\n    \n2.  ^**[^](#fnrefoqfwavryail)**^\n    \n    Facebook (2021) [What is Facebook doing for GivingTuesday?](https://www.facebook.com/help/332488213787105), *Facebook’s Help Center*.\n    \n3.  ^**[^](#fnref79sbizv8x2a)**^\n    \n    Norowitz, Avi (2018) [EA #GivingTuesday fundraiser matching retrospective](https://forum.effectivealtruism.org/posts/vuQobY5yQvWdtPiab/ea-givingtuesday-fundraiser-matching-retrospective), *Effective Altruism Forum*, January 13."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "atHNNgekWbYdWASjk",
    "name": "Founders Pledge",
    "core": false,
    "slug": "founders-pledge",
    "oldSlugs": null,
    "postCount": 30,
    "description": {
      "markdown": "**Founders Pledge** is a nonprofit that encourages entrepreneurs to [pledge to donate](https://forum.effectivealtruism.org/tag/donation-pledge) a portion of their profits to [effective charities](https://forum.effectivealtruism.org/tag/effective-giving), and conducts [cause prioritization](https://forum.effectivealtruism.org/tag/cause-prioritization) and [charity evaluation](https://forum.effectivealtruism.org/tag/charity-evaluation) research to assist their members in deciding which organizations to support.\n\nFounders Pledge Funds\n---------------------\n\nIn 2020, Founders Pledge launched two Funds—expert-managed, cause-specific pools of money for regranting—: the **Global Health and Development Fund** and the **Climate Change Fund**.^[\\[1\\]](#fnnv7hp7436na)^^[\\[2\\]](#fntdizktlhw4)^\n\nA third Fund was added in 2021: the **Patient Philanthropy Fund**. The purpose of this Fund is to invest the donations it receives by default, potentially for decades, centuries or millennia, until the managers conclude that an unusually high-impact opportunity exists to safeguard and improve humanity's long-term future. The Fund was launched with $1 million in pre-seed funding, with the goal of growing it to $100 million within the following 10 years.^[\\[3\\]](#fn2q6e1njjsun)^^[\\[4\\]](#fnmn7wr2kbgnr)^^[\\[5\\]](#fnt6kg9lgifa)^\n\nEvaluation\n----------\n\nAs of May 2022, Founders Pledge has received over $8.7 million in funding from [Open Philanthropy](https://forum.effectivealtruism.org/tag/open-philanthropy),^[\\[6\\]](#fnxc3h21sj009)^ and nearly $720,000 from [Effective Altruism Funds](https://forum.effectivealtruism.org/topics/effective-altruism-funds).^[\\[7\\]](#fnvxp9q75mbcs)^^[\\[8\\]](#fnbac7ekt92r7)^^[\\[9\\]](#fn73yq7shui9s)^^[\\[10\\]](#fn7nup7l30cdk)^\n\nFurther reading\n---------------\n\nBell, Douglas (2019) [The radical founders raising billions for Charity—Is this a new social movement?](https://www.forbes.com/sites/douglasbell/2019/08/29/the-radical-philanthropy-group-raising-billions-from-start-up-entrepreneurs--is-this-a-new-social-movement/?sh=4bc479c9276c), *Forbes*, August 29.\n\nExternal links\n--------------\n\n[Founders Pledge](https://founderspledge.com/). Official website.\n\n[Apply for a job](https://founders-pledge.jobs.personio.de/).\n\n[Take the Founders Pledge](https://founderspledge.com/start-your-journey).\n\nRelated entries\n---------------\n\n[cause prioritization](https://forum.effectivealtruism.org/tag/cause-prioritization) | [charity evaluation](https://forum.effectivealtruism.org/tag/charity-evaluation) | [donation pledge](https://forum.effectivealtruism.org/tag/donation-pledge) | [Effective Altruism Funds](https://forum.effectivealtruism.org/tag/effective-altruism-funds)\n\n1.  ^**[^](#fnrefnv7hp7436na)**^\n    \n    Carter, Sam & Stephen Clare (2020) [Why we’re launching a Global Health and Development Fund](https://founderspledge.com/stories/why-were-launching-a-global-health-and-development-fund), *Founders Pledge*, October 9.\n    \n2.  ^**[^](#fnreftdizktlhw4)**^\n    \n    Ackva, Johannes & Anu Khan (2020) [Erik Bergman challenges you to tackle climate change with €1 million match pot](https://founderspledge.com/stories/erik-bergman-challenges-you-to-tackle-climate-change-with-eur1-million-match), *Founders Pledge*, October 23.\n    \n3.  ^**[^](#fnref2q6e1njjsun)**^\n    \n    Hoeijmakers, Sjir (2020) [Long-term investment fund at Founders Pledge](https://forum.effectivealtruism.org/posts/8vfadjWWMDaZsqghq/long-term-investment-fund-at-founders-pledge), *Effective Altruism Forum*, January 10.\n    \n4.  ^**[^](#fnrefmn7wr2kbgnr)**^\n    \n    Hoeijmakers, Sjir (2021) [Announcing the Patient Philanthropy Fund](https://forum.effectivealtruism.org/posts/HYRpeSQx46u278vYK/announcing-the-patient-philanthropy-fund), *Effective Altruism Forum*, October 27.\n    \n5.  ^**[^](#fnreft6kg9lgifa)**^\n    \n    Samuel, Sigal (2021) [Would you donate to a charity that won’t pay out for centuries?](https://www.vox.com/future-perfect/2021/11/3/22760718/patient-philanthropy-fund-charity-longtermism), *Vox*, November 3.\n    \n6.  ^**[^](#fnrefxc3h21sj009)**^\n    \n    Open Philanthropy (2022) [Grants database: Founders Pledge](https://www.openphilanthropy.org/grants/?q=&organization-name=founders-pledge), *Open Philanthropy*.\n    \n7.  ^**[^](#fnrefvxp9q75mbcs)**^\n    \n    Effective Altruism Infrastructure Fund (2018) [July 2018: EA Meta Fund grants](https://funds.effectivealtruism.org/funds/payouts/july-2018-ea-meta-fund-grants), *Effective Altruism Funds*, July.\n    \n8.  ^**[^](#fnrefbac7ekt92r7)**^\n    \n    Effective Altruism Infrastructure Fund (2018) [November 2018: EA Meta Fund grants](https://funds.effectivealtruism.org/funds/payouts/november-2018-ea-meta-fund-grants), *Effective Altruism Funds*, November. \n    \n9.  ^**[^](#fnref73yq7shui9s)**^\n    \n    Effective Altruism Infrastructure Fund (2019) [March 2019: EA Meta Fund grants](https://funds.effectivealtruism.org/funds/payouts/march-2019-ea-meta-fund-grants), *Effective Altruism Funds*, March. \n    \n10.  ^**[^](#fnref7nup7l30cdk)**^\n    \n    Effective Altruism Infrastructure Fund (2020) [July 2020: EA Meta Fund grants](https://funds.effectivealtruism.org/funds/payouts/july-2020-ea-meta-fund-grants), *Effective Altruism Funds*, July."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "GCsanG7v3e8kohMbu",
    "name": "Albert Schweitzer Foundation",
    "core": false,
    "slug": "albert-schweitzer-foundation",
    "oldSlugs": null,
    "postCount": 6,
    "description": {
      "markdown": "The **Albert Schweitzer Foundation** (**ASF**) is a German nonprofit that works to improve [farmed animal welfare](https://forum.effectivealtruism.org/tag/farmed-animal-welfare) standards through litigation, corporate outreach, and [corporate campaigns](https://forum.effectivealtruism.org/tag/corporate-cage-free-campaigns).\n\nHistory\n-------\n\nASF was founded in 2000,^[\\[1\\]](#fnw56p6v0loj8)^ with Albert Schweitzer's fundamental maxim of \"reverence for life\" as their guiding philosophy, and a focus on \"the single greatest source of pain and death for animals: the use of animals and animal products as a food source.\"^[\\[2\\]](#fn1z8hc9qh6f3)^ \n\nEvaluation\n----------\n\nASF is recommended by [Founders Pledge](https://forum.effectivealtruism.org/tag/founders-pledge) as \"one of the most cost-effective animal advocacy organisations in the world.\"^[\\[3\\]](#fn0oga3tf328r)^ As of July 2022, ASF has received over $6 million in funding from [Open Philanthropy](https://forum.effectivealtruism.org/tag/open-philanthropy).^[\\[4\\]](#fnpl1l9xev80b)^\n\nFurther reading\n---------------\n\nClare, Stephen (2020) [The Albert Schweitzer Foundation](https://founderspledge.com/stories/the-albert-schweitzer-foundation-high-impact-funding-opportunity), *Founders Pledge*, November 3.\n\nExternal links\n--------------\n\n[Albert Schweitzer Foundation](https://albertschweitzerfoundation.org/). Official website.\n\n[Donate to ASF](https://donations.albertschweitzerfoundation.org/).\n\nRelated entries\n---------------\n\n[farmed animal welfare](https://forum.effectivealtruism.org/tag/farmed-animal-welfare)\n\n1.  ^**[^](#fnrefw56p6v0loj8)**^\n    \n    Albert Schweitzer Foundation (2018) [Statute](https://albertschweitzerfoundation.org/about-us/statute), *Albert Schweitzer Foundation*.\n    \n2.  ^**[^](#fnref1z8hc9qh6f3)**^\n    \n    Albert Schweitzer Foundation (2021) [Mission statement](https://albertschweitzerfoundation.org/about-us/mission), *Albert Schweitzer Foundation*.\n    \n3.  ^**[^](#fnref0oga3tf328r)**^\n    \n    Clare, Stephen (2020) [The Albert Schweitzer Foundation](https://founderspledge.com/stories/the-albert-schweitzer-foundation-high-impact-funding-opportunity), *Founders Pledge*, November 3.\n    \n4.  ^**[^](#fnrefpl1l9xev80b)**^\n    \n    Open Philanthropy (2022) [Grants database: Albert Schweitzer Foundation](https://www.openphilanthropy.org/grants/?q=&organization-name=albert-schweitzer-foundation), *Open Philanthropy*."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "E5twsh42Rygb7uwbG",
    "name": "Project for Awesome",
    "core": false,
    "slug": "project-for-awesome",
    "oldSlugs": null,
    "postCount": 7,
    "description": {
      "markdown": "**Project for Awesome** (**P4A**) is an annual philanthropic initiative founded by John and Hank Green, in which people vote on YouTube videos about charities and the charities with the most votes receive money.\n\n## External links\n\n[Project for Awesome](https://projectforawesome.com/). Official website."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "p7y4F9REjGwRC37G9",
    "name": "Giving Pledge",
    "core": false,
    "slug": "giving-pledge",
    "oldSlugs": [
      "the-giving-pledge"
    ],
    "postCount": 2,
    "description": {
      "markdown": "The **Giving Pledge** is a campaign to encourage the world's wealthiest individuals and families to [pledge](https://forum.effectivealtruism.org/tag/donation-pledge) the majority of their wealth \"to address society’s most pressing problems.\"^[\\[1\\]](#fncjn6qfdb59)^ It was founded in 2010 by Bill Gates, Melinda Gates, and Warren Buffett. As of 2022, there are a total of 233 pledgers from 28 countries.\n\nExternal links\n--------------\n\n[The Giving Pledge](https://givingpledge.org/). Official website.\n\n1.  ^**[^](#fnrefcjn6qfdb59)**^\n    \n    Giving Pledge (2020) [The Giving Pledge welcomes 13 new signatories](https://givingpledge.org/PressRelease.aspx?date=12.21.2020), *Giving Pledge*, December 21."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "fp7eCfpSNEA4C5cLb",
    "name": "Hedonium",
    "core": false,
    "slug": "hedonium",
    "oldSlugs": null,
    "postCount": 1,
    "description": {
      "markdown": "**Hedonium** is the arrangement of the universe's resources optimized for the production of pleasurable experience.^[\\[1\\]](#fnek4v25cjjmv)^\n\nThe term was introduced by [Carl Shulman](https://forum.effectivealtruism.org/tag/carl-shulman) in a 2012 essay.^[\\[2\\]](#fn4b6sxq66aqt)^ Shulman also defined **dolorium** as the arrangement of the universe's resources optimized for the production of pain. Sometimes the term **utilitronium** is used to express this same concept, though that expression may instead be understood more broadly as referring to whichever state is regarded as the ultimate bearer of value, such as preference satisfaction on preference utilitarianism.\n\nFurther reading\n---------------\n\nArmstrong, Stuart (2015) [Hedonium’s semantic problem](https://www.lesswrong.com/posts/295KiqZKAb55YLBzF/hedonium-s-semantic-problem), *LessWrong*, April 9.\n\nShulman, Carl (2012) [Are pain and pleasure equally energy-efficient?](http://reflectivedisequilibrium.blogspot.com/2012/03/are-pain-and-pleasure-equally-energy.html), *Reflective Disequilibrium*, March 24.\n\nRelated entries\n---------------\n\n[axiology](https://forum.effectivealtruism.org/tag/axiology) | [flourishing futures](https://forum.effectivealtruism.org/tag/flourishing-futures) | [hedonism](https://forum.effectivealtruism.org/tag/hedonism) | [hellish existential catastrophe](https://forum.effectivealtruism.org/tag/hellish-existential-catastrophe) | [long-term future](https://forum.effectivealtruism.org/tag/long-term-future)\n\n1.  ^**[^](#fnrefek4v25cjjmv)**^\n    \n    Bostrom, Nick (2014) [*Superintelligence: Paths, Dangers, Strategies*](https://en.wikipedia.org/wiki/Special:BookSources/9780199678112), Oxford: Oxford University Press, p. 140.\n    \n2.  ^**[^](#fnref4b6sxq66aqt)**^\n    \n    Shulman, Carl (2012) [Are pain and pleasure equally energy-efficient?](http://reflectivedisequilibrium.blogspot.com/2012/03/are-pain-and-pleasure-equally-energy.html), *Reflective Disequilibrium*, March 24."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "nnPozftCou9bcqSHp",
    "name": "One for the World",
    "core": false,
    "slug": "one-for-the-world-1",
    "oldSlugs": null,
    "postCount": 10,
    "description": {
      "markdown": "**One for the World** is an organization that aims to educate students and young professionals about [effective giving](https://forum.effectivealtruism.org/tag/effective-giving-1) and encourages them to [pledge to donate](https://forum.effectivealtruism.org/tag/donation-pledge) at least 1% of their income to effective charities.\n\nFunding\n-------\n\nAs of July 2022, One for the World has received $390,000 in grants from [Effective Altruism Funds](https://forum.effectivealtruism.org/topics/effective-altruism-funds),^[\\[1\\]](#fnqagucpbsqq)^^[\\[2\\]](#fnwmt86kev8m)^^[\\[3\\]](#fnp56lld3d2q)^ and over $150,000 from [Open Philanthropy](https://forum.effectivealtruism.org/topics/open-philanthropy).^[\\[4\\]](#fnl9hdocs06yr)^\n\nExternal links\n--------------\n\n[One for the World](https://www.1fortheworld.org/). Official website.\n\n[Apply for a job](https://www.1fortheworld.org/jobs-at-oftw).\n\n[Donate to One for the World](https://www.1fortheworld.org/take-the-pledge).\n\n[Take the One for the World pledge](https://www.1fortheworld.org/take-the-pledge).\n\nRelated entries\n---------------\n\n[donation pledge](https://forum.effectivealtruism.org/tag/donation-pledge)\n\n1.  ^**[^](#fnrefqagucpbsqq)**^\n    \n    Effective Altruism Infrastructure Fund (2019) [March 2019: EA Meta Fund grants](https://funds.effectivealtruism.org/funds/payouts/march-2019-ea-meta-fund-grants), *Effective Altruism Funds*, March. \n    \n2.  ^**[^](#fnrefwmt86kev8m)**^\n    \n    Effective Altruism Infrastructure Fund (2020) [November 2020: EA Infrastructure Fund grants](https://funds.effectivealtruism.org/funds/payouts/november-2020-ea-infrastructure-fund-grants), *Effective Altruism Funds*, November. \n    \n3.  ^**[^](#fnrefp56lld3d2q)**^\n    \n    Global Health and Development Fund (2019) [October 2019: One for the World](https://funds.effectivealtruism.org/funds/payouts/october-2019-one-for-the-world), *Effective Altruism Funds*, October.\n    \n4.  ^**[^](#fnrefl9hdocs06yr)**^\n    \n    Open Philanthropy (2022) [Grants database: One for the World](https://www.openphilanthropy.org/grants/?q=&organization-name=one-for-the-world), *Open Philanthropy*."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "2SATxXs3BiyjqsQrb",
    "name": "Model uncertainty",
    "core": false,
    "slug": "model-uncertainty",
    "oldSlugs": null,
    "postCount": 14,
    "description": {
      "markdown": "**Model uncertainty** is uncertainty surrounding a model itself, including the model's internal uncertainty estimates.\n\nUncertainty about models\n------------------------\n\nA useful [model](https://forum.effectivealtruism.org/tag/models) is one that is simple enough to be analyzed easily, while nevertheless being similar enough to reality that this analysis can be used as a basis for predictions about the actual world. Unfortunately, it can be difficult to judge whether a given model is in fact similar enough. Furthermore, even if some predictions based on a model come true, this does not necessarily mean that next prediction based on the model will also come true.\n\nA classic illustration of the importance of using appropriate models, as well as the difficulty of noticing when a model is inappropriate, is the 2007 financial crisis. In the years leading up to the crisis, many financial actors made investment decisions on the basis of models that assumed economic stability. Once this simplifying assumption ceased to hold, it became clear that their models had not sufficiently matched reality, and that the outcome of their decisions would be disastrous.\n\nOne strategy for dealing with uncertainty about the appropriateness of models is to construct and weight the predictions of multiple diverse models, rather than relying on a single one. However, in cases of radical uncertainty, not even this method may be enough. It may be that we think that there is a chance that none of the models that we have been able to generate is appropriate, and that we need to factor in what could happen if that were the case. Obviously, it is very hard to say something about such an uncertain case, but it may be possible to say some things. For instance, in their paper \"Probing the Improbable,\" Toby Ord, Rafaela Hillerbrand, and Anders Sandberg argue that in cases where our models about certain low probability, high-risk events - such as [existential risks](https://forum.effectivealtruism.org/tag/existential-risk) \\- are wrong, the chance of disaster may be substantially higher than if the models are right.\n\nUncertainty within models\n-------------------------\n\nWhen using a model to make estimates, we will often be uncertain about what values the model's numerical parameters should have.\n\nFor example, if we decide to use 80,000 Hours' [three-factor framework for selecting cause areas](https://forum.effectivealtruism.org/tag/itn-framework-1), we may be unsure of what value to assign to a given cause area's tractability, or if we are attempting to to estimate the value of a donation to a bednet charity, we may be unsure how many cases of [malaria](https://forum.effectivealtruism.org/tag/malaria) are prevented per bednet distributed.\n\nIt is important to make such uncertainty clear, both so that our views can be be more easily challenged and improved by others and so that we can derive more nuanced conclusions from the models we use.\n\nBy plugging in probability distributions or confidence windows, rather than individual estimates, for the values of the parameters in a given model, we can calculate an output for the model that also reflects uncertainty. However, it is important to be careful when performing such calculations, since small mathematical or conceptual errors can easily lead to incorrect or misleading results. A good tool for avoiding these sorts of errors is  Guesstimate.^[\\[1\\]](#fnw5a06yvp8z)^\n\nIt has also been argued, e.g. by Holden Karnofsky, that in cases with high uncertainty, estimates that assign an intervention a very high [expected value](https://forum.effectivealtruism.org/tag/expected-value) are likely to reflect some unseen bias in the calculation, and should therefore be treated with skepticism.\n\nFurther reading\n---------------\n\nFrigg, Roman & Stephan Hartmann (2006) [Models in science](https://plato.stanford.edu/entries/models-science/), *Stanford Encyclopedia of Philosophy*, February 27 (updated 4 February 2020).\n\nKarnofsky, Holden (2011) [Why we can’t take expected value estimates literally (even when they’re unbiased)](https://blog.givewell.org/2011/08/18/why-we-cant-take-expected-value-estimates-literally-even-when-theyre-unbiased/), *The GiveWell Blog*, August 18 (updated 25 July 2016).  \n*Approach to evaluating uncertain interventions.*\n\nKarnofsky, Holden (2014) [Sequence thinking vs. cluster thinking](https://blog.givewell.org/2014/06/10/sequence-thinking-vs-cluster-thinking/), *The GiveWell Blog*, June 10 (updated 25 July 2016).\n\nOrd, Toby, Rafaela Hillerbrand & Anders Sandberg (2010) [Probing the improbable: methodological challenges for risks with low probabilities and high stakes](http://doi.org/10.1080/13669870903126267), *Journal of risk research*, vol. 13, pp. 191–205.\n\nExternal links\n--------------\n\n[Guesstimate](https://www.getguesstimate.com/). A tool for carrying out calculations under uncertainty.\n\nRelated entries\n---------------\n\n[credence](https://forum.effectivealtruism.org/tag/credence) | [forecasting](https://forum.effectivealtruism.org/tag/forecasting) | [sequence vs. cluster thinking](https://forum.effectivealtruism.org/topics/sequence-vs-cluster-thinking) | [value of information](https://forum.effectivealtruism.org/tag/value-of-information)\n\n1.  ^**[^](#fnrefw5a06yvp8z)**^\n    \n    Gooen, Ozzie (2015) [Guesstimate: An app for making decisions with confidence (intervals)](https://forum.effectivealtruism.org/posts/Bt4nkCGHKBkDk97mn/guesstimate-an-app-for-making-decisions-with-confidence), *Effective Altruism Forum*, December 30."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "FMGBhGz7srLpeL7JF",
    "name": "Job satisfaction",
    "core": false,
    "slug": "job-satisfaction",
    "oldSlugs": null,
    "postCount": 2,
    "description": {
      "markdown": "**Job satisfaction** is a measure of workers' contentedness with their job. Job satisfaction is correlated with career success, and is often intrinsically desired independently of its impact on career outcomes.\n\n[80,000 Hours](https://forum.effectivealtruism.org/tag/80-000-hours) conducted an analysis of the literature on job satisfaction and identified a number of key determinants.^[\\[1\\]](#fnomo99wbp0v)^ These include work that the worker is good at; work that the worker perceives to helps others; and work with [supportive conditions](https://forum.effectivealtruism.org/tag/supportive-conditions), including work that allows for flow-like states, that meet the worker's basic needs, that fits the worker's personal life, and that involves interactions with helpful colleagues.\n\nFurther reading\n---------------\n\nTodd, Benjamin (2014) [We reviewed over 60 studies about what makes for a dream job. Here’s what we found](https://80000hours.org/career-guide/job-satisfaction/), *80,000 Hours*, August (updated April 2017).\n\nRelated entries\n---------------\n\n[career choice](https://forum.effectivealtruism.org/topics/career-choice) | [career framework](https://forum.effectivealtruism.org/topics/career-framework)\n\n1.  ^**[^](#fnrefomo99wbp0v)**^\n    \n    Todd, Benjamin (2014) [We reviewed over 60 studies about what makes for a dream job. Here’s what we found](https://80000hours.org/career-guide/job-satisfaction/), *80,000 Hours*, August (updated April 2017)."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "hXXy9vwxcMnw6WCK4",
    "name": "Supportive conditions",
    "core": false,
    "slug": "supportive-conditions",
    "oldSlugs": null,
    "postCount": 3,
    "description": {
      "markdown": "The **supportive conditions** of a career are the ingredients of job satisfaction that aren't captured by [role impact](https://forum.effectivealtruism.org/tag/role-impact) and [career capital](https://forum.effectivealtruism.org/tag/career-capital)."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "tFS8r3BE3X6ZuzPdk",
    "name": "Demandingness of morality",
    "core": false,
    "slug": "demandingness-of-morality",
    "oldSlugs": null,
    "postCount": 13,
    "description": {
      "markdown": "The **demandingness of morality** is the cost to the agent of complying with moral demands. According to some authors, there is a limit to how much morality can demand from us, and any moral theory or principle that exceeds this limit can for that reason be rejected.\n\nFurther reading\n---------------\n\nBarry, Christian & Holly Lawford-Smith (2019) [On satisfying duties to assist](http://doi.org/10.1093/oso/9780198841364.003.0010), in Hilary Greaves & Theron Pummer (eds.) *Effective Altruism: Philosophical Issues*, Oxford: Oxford University Press, pp. 150–165.\n\nCarlsmith, Joseph (2021) [Care and demandingness](https://forum.effectivealtruism.org/posts/P52eSwfmwaN2uwrcM/care-and-demandingness), *Effective Altruism Forum*, March 8.\n\nKaufman, Jeff (2013) [Altruism isn’t about sacrifice](https://www.jefftk.com/p/altruism-isnt-about-sacrifice), *Jeff Kaufman’s Blog*, September 4.\n\nLazari-Radek, Katarzyna de & Peter Singer (2013) [How much more demanding is utilitarianism than common sense morality?](https://www.cairn.info/revue-internationale-de-philosophie-2013-4-page-427.htm), *Revue Internationale de Philosophie*, vol. 67, pp. 427–438.\n\nLong, Robert (2018) [Demanding gambles](https://experiencemachines.wordpress.com/2018/06/10/demanding-gambles/), *Experience Machines*, June 10.  \n*Notes that the requirement to work on high-risk, high-reward causes places an additional and different type of demand on the agent from what discussions about demandingness usually focus on.*\n\nMacAskill, William, Andreas Mogensen & Toby Ord (2018) [Giving isn’t demanding](http://doi.org/10.1093/oso/9780190648879.003.0007), in Paul Woodruff (ed.) *The Ethics of Giving: Philosophers’ Perspectives on Philanthropy*, New York: Oxford University Press, pp. 178–203.\n\nMacAskill, William & Darius Meissner (2020) [The demandingness objection](https://www.utilitarianism.net/objections-to-utilitarianism/demandingness), *Utilitarianism*.\n\nMogensen, Andreas (2021) [Moral demands and the far future](https://doi.org/10.1111/phpr.12729), *Philosophy and Phenomenological Research*, vol. 103, pp. 567–585.\n\nOrd, Toby (2012) [Global poverty and the demands of morality](https://doi.org/10.1017/CBO9781107279629.013), in John Perry (ed.) *God, the Good, and Utilitarianism*, Cambridge: Cambridge University Press, pp. 177–191.\n\nSachs, Ben (2019) [Demanding the demanding](http://doi.org/10.1093/oso/9780198841364.003.0009), in Hilary Greaves & Theron Pummer (eds.) *Effective Altruism: Philosophical Issues*, Oxford: Oxford University Press, pp. 137–149.\n\nSinger, Peter (2015) [*The Most Good You Can Do: How Effective Altruism Is Changing Ideas about Living Ethically*](https://en.wikipedia.org/wiki/Special:BookSources/9780300180275), New Haven: Yale University Press, ch. 9.\n\nSobel, David (2007) [The impotence of the demandingness objection](https://philpapers.org/rec/SOBTIO), *Philosophers’ Imprint*, vol. 7, pp. 1–17.\n\nTimmerman, Travis (2019) [Effective altruism’s underspecification problem](http://doi.org/10.1093/oso/9780198841364.003.0011), in Hilary Greaves & Theron Pummer (eds.) *Effective Altruism: Philosophical Issues*, Oxford: Oxford University Press, pp. 166–183.\n\nWise, Julia (2013) [Cheerfully](http://www.givinggladly.com/2013/06/cheerfully.html), *Giving Gladly*, June 8.\n\nRelated entries\n---------------\n\n[altruistic motivation](https://forum.effectivealtruism.org/tag/altruistic-motivation) | [excited vs. obligatory altruism](https://forum.effectivealtruism.org/tag/excited-vs-obligatory-altruism) | [effective altruism lifestyle](/tag/effective-altruism-lifestyle) | [supererogation](https://forum.effectivealtruism.org/topics/supererogation) | [utilitarianism](/tag/utilitarianism)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "uQ9kedpmuzy8LcNZb",
    "name": "Conservation",
    "core": false,
    "slug": "conservation",
    "oldSlugs": null,
    "postCount": 22,
    "description": {
      "markdown": "**Conservation** refers to the long-term preservation of goods that would be difficult to replace or substitute. Most typically this will concern endangered species and [biodiversity](https://forum.effectivealtruism.org/topics/biodiversity-loss), but the category may also encompass the preservation of important information, cultures, well-preserved brains, or artifacts.\n\nRelated entries\n---------------\n\n[biodiversity loss](https://forum.effectivealtruism.org/topics/biodiversity-loss) | [climate change](https://forum.effectivealtruism.org/topics/climate-change) | [environmental science](https://forum.effectivealtruism.org/topics/environmental-science)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "StsT7YhvBzr3PEw2N",
    "name": "Effective Thesis",
    "core": false,
    "slug": "effective-thesis",
    "oldSlugs": null,
    "postCount": 16,
    "description": {
      "markdown": "**Effective Thesis** is a project to help university students find high-impact thesis topics.\n\nAs of July 2022, Effective Thesis has advised or is currently advising over 40 different students writing theses in areas such as [global priorities research](https://forum.effectivealtruism.org/tag/global-priorities-research), [animal welfare](https://forum.effectivealtruism.org/tag/animal-welfare-1), [biosecurity](https://forum.effectivealtruism.org/tag/biosecurity), and [AI safety](https://forum.effectivealtruism.org/tag/ai-safety).\n\nFunding\n-------\n\nEffective Thesis has received nearly $250,000 in funding from [Effective Altruism Funds](https://forum.effectivealtruism.org/topics/effective-altruism-funds),^[\\[1\\]](#fn4rtdsbwuomq)^^[\\[2\\]](#fnrd4uqmdcqs)^ and $15,000 from the [Survival and Flourishing](https://forum.effectivealtruism.org/topics/survival-and-flourishing) Fund.^[\\[3\\]](#fnxjtara442a)^\n\nExternal links\n--------------\n\n[Effective Thesis](https://effectivethesis.org/). Official website.\n\nRelated entries\n---------------\n\n[80,000 Hours](https://forum.effectivealtruism.org/tag/80-000-hours) | [academia](https://forum.effectivealtruism.org/tag/academia-1) | [career choice](https://forum.effectivealtruism.org/tag/career-choice)\n\n1.  ^**[^](#fnref4rtdsbwuomq)**^\n    \n    Effective Altruism Infrastructure Fund (2019) [July 2019: EA Meta Fund grants](https://funds.effectivealtruism.org/funds/payouts/july-2019-ea-meta-fund-grants), *Effective Altruism Funds*, July.\n    \n2.  ^**[^](#fnrefrd4uqmdcqs)**^\n    \n    Effective Altruism Infrastructure Fund (2021) [May-August 2021: EA Infrastructure Fund grants](https://funds.effectivealtruism.org/funds/payouts/may-august-2021-ea-infrastructure-fund-grants), *Effective Altruism Funds*, August.\n    \n3.  ^**[^](#fnrefxjtara442a)**^\n    \n    Survival and Flourishing Fund (2019) [SFF-2020-H2 S-process recommendations announcement](https://survivalandflourishing.fund/sff-2020-h2-recommendations), *Survival and Flourishing Fund*."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "zptM9sMiALArsyYhJ",
    "name": "Yew-Kwang Ng",
    "core": false,
    "slug": "yew-kwang-ng",
    "oldSlugs": null,
    "postCount": null,
    "description": {
      "markdown": "**Yew-Kwang Ng** (born 7 August 1942) is a Malaysian economist. He is currently Special Chair Professor at Fudan University and Emeritus Professor at Monash University, and sits on the Board of Advisors of the [Global Priorities Institute](https://forum.effectivealtruism.org/tag/global-priorities-institute).\n\nNg's contributions to [welfare biology](https://forum.effectivealtruism.org/tag/welfare-biology),^[\\[1\\]](#fnevea0tciaq8)^^[\\[2\\]](#fn583xzu1j5nu)^^[\\[3\\]](#fnc8ifk430yg5)^[ welfare economics](https://forum.effectivealtruism.org/tag/welfare-economics),^[\\[4\\]](#fnjme45jbfdxe)^^[\\[5\\]](#fncx8s853w33g)^ [population ethics](https://forum.effectivealtruism.org/tag/population-ethics),^[\\[6\\]](#fnyaxz4u8u4mk)^ the [ethics of existential risk](https://forum.effectivealtruism.org/tag/ethics-of-existential-risk),^[\\[7\\]](#fnewgfg7a2614)^^[\\[8\\]](#fn5ww2t1md61h)^^[\\[9\\]](#fnsl3xylqbe2)^ the measurement of [wellbeing](https://forum.effectivealtruism.org/tag/wellbeing),^[\\[10\\]](#fnc0z0bq6d28)^ and the rehabilitation of [utilitarianism](https://forum.effectivealtruism.org/tag/utilitarianism)^[\\[11\\]](#fnhqcdoqvz1fg)^^[\\[12\\]](#fnv5ws1pvtk6g)^^[\\[13\\]](#fnwg3zsatv3zf)^ make him a figure of central importance in the [history of effective altruism](https://forum.effectivealtruism.org/tag/history-of-effective-altruism).\n\nNg's welfare economics textbook is dedicated to \"the welfare of all sentients.\"^[\\[14\\]](#fnf0t0p31bt6u)^ His latest book is *Happiness—Concept, Measurement and Promotion*.^[\\[15\\]](#fn1impalm2l5fi)^\n\nFurther reading\n---------------\n\nCarpendale, Max (2015) [Welfare biology as an extension of biology: interview with Yew-Kwang Ng](https://doi.org/10.7358/rela-2015-002-carp), *Relations*, vol. 3, pp. 197–202.\n\nNg, Yew-Kwang (2019) [Global extinction and animal welfare: two priorities for effective altruism](https://doi.org/10.1111/1758-5899.12647), *Global Policy*, vol. 10, pp. 258–266.\n\nWiblin, Robert & Keiran Harris (2018) [Prof Yew-Kwang Ng on ethics and how to create a much happier world](https://80000hours.org/podcast/episodes/yew-kwang-ng-anticipating-effective-altruism/), *80,000 Hours*, July 26.\n\nRelated entries\n---------------\n\n[human extinction](https://forum.effectivealtruism.org/tag/human-extinction) | [population ethics](https://forum.effectivealtruism.org/tag/population-ethics) | [utilitarianism](https://forum.effectivealtruism.org/tag/utilitarianism) | [welfare biology](https://forum.effectivealtruism.org/tag/welfare-biology) |[ welfare economics](https://forum.effectivealtruism.org/tag/welfare-economics)  | [welfarism](https://forum.effectivealtruism.org/tag/welfarism) | [wellbeing](https://forum.effectivealtruism.org/tag/wellbeing)\n\n1.  ^**[^](#fnrefevea0tciaq8)**^\n    \n    Ng, Yew-Kwang (1995) [Towards welfare biology: evolutionary economics of animal consciousness and suffering](https://doi.org/10.1007/BF00852469), *Biology and Philosophy*, vol. 10, pp. 255–285.\n    \n2.  ^**[^](#fnref583xzu1j5nu)**^\n    \n    Ng, Yew-Kwang (2016) [How welfare biology and commonsense may help to reduce animal suffering](https://doi.org/10.51291/2377-7478.1012), *Animal Sentience*, vol. 7, pp. 1–10.\n    \n3.  ^**[^](#fnrefc8ifk430yg5)**^\n    \n    Groff, Zach & Yew-Kwang Ng (2019) [Does suffering dominate enjoyment in the animal kingdom? An update to welfare biology](https://doi.org/10.1007/s10539-019-9692-0), *Biology and Philosophy*, vol. 34, pp. 1–16.\n    \n4.  ^**[^](#fnrefjme45jbfdxe)**^\n    \n    Ng, Yew-Kwang (1979) [*Welfare Economics: Introduction and Development of Basic Concepts*](https://doi.org/10.1007/978-1-349-16223-9), London: Macmillan.\n    \n5.  ^**[^](#fnrefcx8s853w33g)**^\n    \n    Ng, Yew-Kwang (2004) [*Welfare Economics: Towards a More Complete Analysis*](https://doi.org/10.1057/9781403944061), London: Palgrave Macmillan.\n    \n6.  ^**[^](#fnrefyaxz4u8u4mk)**^\n    \n    Ng, Yew-Kwang (1989) [What should we do about future generations? Impossibility of Parfit’s Theory X](https://doi.org/10.1017/S0266267100002406), *Economics and Philosophy*, vol. 5, pp. 235–253.\n    \n7.  ^**[^](#fnrefewgfg7a2614)**^\n    \n    Ng, Yew-Kwang (1993) [Should we be very cautious or extremely cautious on measures that may involve our destruction](https://doi.org/10.1007/BF00182449), *Social Choice and Welfare*, vol. 10, pp. 223–247.\n    \n8.  ^**[^](#fnref5ww2t1md61h)**^\n    \n    Ng, Yew-Kwang (2011) [Consumption tradeoff vs. catastrophes avoidance: implications of some recent results in happiness studies on the economics of climate change](https://doi.org/10.1007/s10584-010-9880-z), *Climatic Change*, vol. 105, pp. 109–127.\n    \n9.  ^**[^](#fnrefsl3xylqbe2)**^\n    \n    Ng, Yew-Kwang (2016) [The importance of global extinction in climate change policy](https://doi.org/10.1111/1758-5899.12318), *Global Policy*, vol. 7, pp. 315–322.\n    \n10.  ^**[^](#fnrefc0z0bq6d28)**^\n    \n    Ng, Yew-Kwang (1997) [A case for happiness, cardinalism, and interpersonal comparability](https://doi.org/10.1111/j.1468-0297.1997.tb00087.x), *The Economic Journal*, vol. 107, pp. 1848–1858.\n    \n11.  ^**[^](#fnrefhqcdoqvz1fg)**^\n    \n    Ng, Yew-Kwang & Peter Singer (1981) [An argument for utilitarianism](https://doi.org/10.1080/00455091.1981.10716302), *Canadian Journal of Philosophy*, vol. 11, pp. 229–239.\n    \n12.  ^**[^](#fnrefv5ws1pvtk6g)**^\n    \n    Ng, Yew-Kwang (1990) [Welfarism and utilitarianism: a rehabilitation](https://doi.org/10.1017/S0953820800000650), *Utilitas*, vol. 2, pp. 171–193.\n    \n13.  ^**[^](#fnrefwg3zsatv3zf)**^\n    \n    Ng, Yew-Kwang (2000) [From separability to unweighted sum: a case for utilitarianism](https://doi.org/10.1023/A:1026432128221), *Theory and Decision*, vol. 49, pp. 229–312.\n    \n14.  ^**[^](#fnreff0t0p31bt6u)**^\n    \n    Ng, [*Welfare Economics: Introduction and Development of Basic Concepts*](http://doi.org/10.1007/978-1-349-16223-9), p. vi.\n    \n15.  ^**[^](#fnref1impalm2l5fi)**^\n    \n    Ng, Yew-Kwang (2022) [*Happiness: Concept, Measurement and Promotion*](https://doi.org/10.1007/978-981-33-4972-8), Singapore: Springer."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "MAMDeCftie2S4CgYX",
    "name": "Wild Animal Initiative",
    "core": false,
    "slug": "wild-animal-initiative",
    "oldSlugs": null,
    "postCount": 26,
    "description": {
      "markdown": "**Wild Animal Initiative** (**WAI**) is a nonprofit that works to understand and improve [wild animal welfare](https://forum.effectivealtruism.org/tag/wild-animal-welfare).\n\nHistory\n-------\n\nWAI was founded in 2019 as a merger of two organizations, **Wild-Animal Suffering Research** and **Utility Farm**. Wild-Animal Suffering Research began in 2016 as a project of the [Effective Altruism Foundation](https://forum.effectivealtruism.org/tag/effective-altruism-foundation) and was part of [Sentience Politics](https://forum.effectivealtruism.org/tag/sentience-politics) for a year or so until becoming an independent organization.^[\\[1\\]](#fno8cgmvjf8w)^ Utility Farm launched in 2017.^[\\[2\\]](#fnxdctv48v4mr)^\n\nEvaluation\n----------\n\n[Animal Charity Evaluators](https://forum.effectivealtruism.org/tag/animal-charity-evaluators) rates WAI a \"top charity\"—one of the four organizations awarded their highest rating and the only group focused on wild animal welfare ever to be so rated. ACE considers WAI \"an excellent giving opportunity because of their strong, cost-effective programs and their thorough strategy.\"^[\\[3\\]](#fnmw4l40x23a)^\n\nWild-Animal Suffering Research, Utility Farm, and WAI have received over $1 million in funding from [Effective Altruism Funds](https://forum.effectivealtruism.org/topics/effective-altruism-funds).^[\\[4\\]](#fnwl6v6mn47g8)^^[\\[5\\]](#fn17cldvmknvi)^ In June 2021, WAI was awarded a $3.5 million grant from [Open Philanthropy](https://forum.effectivealtruism.org/tag/open-philanthropy) to support projects relevant to the field of [welfare biology](https://forum.effectivealtruism.org/tag/welfare-biology).^[\\[6\\]](#fn8blji3oojtb)^\n\nFurther reading\n---------------\n\nMatthews, Dylan (2021) [The wild frontier of animal welfare](https://www.vox.com/the-highlight/22325435/animal-welfare-wild-animals-movement), *Vox*, April 21.\n\nExternal links\n--------------\n\n[Wild Animal Initiative](https://www.wildanimalinitiative.org/). Official website.\n\n[Wild Animal Initiative](https://forum.effectivealtruism.org/users/wild_animal_initiative). [Effective Altruism Forum](https://forum.effectivealtruism.org/tag/effective-altruism-forum-1) account.\n\n[Apply for a job](https://www.wildanimalinitiative.org/careers).\n\nRelated entries\n---------------\n\n[Animal Ethics](https://forum.effectivealtruism.org/tag/animal-ethics) | [wild animal welfare](https://forum.effectivealtruism.org/tag/wild-animal-welfare)\n\n1.  ^**[^](#fnrefo8cgmvjf8w)**^\n    \n    Eskander, Persis (2017) [Introducing the Wild-Animal Suffering Research Project](https://was-research.org/blog/introducing-wild-animal-suffering-research-project/), *Wild-Animal Suffering Research*, July 20.\n    \n2.  ^**[^](#fnrefxdctv48v4mr)**^\n    \n    Rowe, Abraham (2017) [An ethic of intervention](https://web.archive.org/web/20180721065832/https://www.utility.farm/words/2017/4/22/an-ethic-of-intervention), *Utility Farm*, April 26.\n    \n3.  ^**[^](#fnrefmw4l40x23a)**^\n    \n    Animal Charity Evaluators (2020) [Wild Animal Initiative review](https://animalcharityevaluators.org/charity-review/wild-animal-initiative/), *Animal Charity Evaluators*, November.\n    \n4.  ^**[^](#fnrefwl6v6mn47g8)**^\n    \n    Bollard, Lewis (2018) [Animal Welfare Fund update](https://docs.google.com/document/d/1nTS2AEOLkuQEmEQ5ePa7opt9eoN_utvWvXPbFGJL90s/edit), *Effective Altruism Funds*, June.\n    \n5.  ^**[^](#fnref17cldvmknvi)**^\n    \n    Animal Welfare Fund (2017) [November 2017: Animal Welfare Fund grants](https://funds.effectivealtruism.org/funds/payouts/november-2017-animal-welfare-fund-grants), *Effective Altruism Funds*, November. \n    \n    Animal Welfare Fund (2018) [March 2018: Animal Welfare Fund grants](https://funds.effectivealtruism.org/funds/payouts/march-2018-animal-welfare-fund-grants), *Effective Altruism Funds*, March. \n    \n    Animal Welfare Fund (2018) [June 2018: Animal Welfare Fund grants](https://funds.effectivealtruism.org/funds/payouts/june-2018-animal-welfare-fund-grants), *Effective Altruism Funds*, June. \n    \n    Animal Welfare Fund (2018) [December 2018: Animal Welfare Fund grants](https://funds.effectivealtruism.org/funds/payouts/december-2018-animal-welfare-fund-grants), *Effective Altruism Funds*, December. \n    \n    Animal Welfare Fund (2019) [March 2019: Animal Welfare Fund grants](https://funds.effectivealtruism.org/funds/payouts/march-2019-animal-welfare-fund-grants), *Effective Altruism Funds*, March.\n    \n    Animal Welfare Fund (2019) [July 2019: Animal Welfare Fund grants](https://funds.effectivealtruism.org/funds/payouts/july-2019-animal-welfare-fund-grants), *Effective Altruism Funds*, July. \n    \n    Animal Welfare Fund (2019) [November 2019: Animal Welfare Fund grants](https://funds.effectivealtruism.org/funds/payouts/november-2019-animal-welfare-fund-grants), *Effective Altruism Funds*, November. \n    \n    Animal Welfare Fund (2020) [March 2020: Animal Welfare Fund grants](https://funds.effectivealtruism.org/funds/payouts/march-2020-animal-welfare-fund-grants), *Effective Altruism Funds*, March. \n    \n    Animal Welfare Fund (2020) [July 2020: Animal Welfare Fund grants](https://funds.effectivealtruism.org/funds/payouts/july-2020-animal-welfare-fund-grants), *Effective Altruism Funds*, July. \n    \n    Animal Welfare Fund (2020) [November 2020: Animal Welfare Fund grants](https://funds.effectivealtruism.org/funds/payouts/november-2020-animal-welfare-fund-grants), *Effective Altruism Funds*, November. \n    \n    Animal Welfare Fund (2021) [May 2021: Animal Welfare Fund grants](https://funds.effectivealtruism.org/funds/payouts/may-2021-animal-welfare-fund-grants), *Effective Altruism Funds*, May. \n    \n6.  ^**[^](#fnref8blji3oojtb)**^\n    \n    Bollard, Lewis (2021) [Wild Animal Initiative — Animal Welfare Research](https://www.openphilanthropy.org/focus/us-policy/farm-animal-welfare/wild-animal-initiative-animal-welfare-research), *Open Philanthropy*, June 15."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "ANKvc4nKjQTF7rJdW",
    "name": "Whole brain emulation",
    "core": false,
    "slug": "whole-brain-emulation",
    "oldSlugs": null,
    "postCount": 12,
    "description": {
      "markdown": "**Whole brain emulation** (sometimes called **mind uploading** or simply **uploading**) is the fine-grain modeling of the computational structure of the human brain.\n\nEvaluation\n----------\n\n[80,000 Hours](https://forum.effectivealtruism.org/tag/80-000-hours) rates whole brain emulation a \"potential highest priority area\": an issue that, if more thoroughly examined, could rank as a top global challenge.^[\\[1\\]](#fn9jpavdlbdrh)^\n\nFurther reading\n---------------\n\nChalmers, David J. (2014) [Uploading: A philosophical analysis](https://doi.org/10.1002/9781118736302.ch6), in Russell Blackford & Damien Broderick (eds.) *Intelligence Unbound: The Future of Uploaded and Machine Minds*, Chichester: John Wiley & Sons, pp. 102–118.\n\nEth, Daniel, Juan-Carlos Foust & Brandon Whale (2013) [The prospects of whole brain emulation within the next half-century](http://doi.org/10.2478/jagi-2013-0008), *Journal of Artificial General Intelligence*, vol. 4, pp. 130–152.\n\nHanson, Robin (2016) [*The Age of Em: Work, Love, and Life When Robots Rule the Earth*](https://en.wikipedia.org/wiki/Special:BookSources/9780198754626), Oxford: Oxford University Press.\n\nSandberg, Anders (2013) [Feasibility of whole brain emulation](http://doi.org/10.1007/978-3-642-31674-6_19), in Vincent C. Müller (ed.) *Philosophy and Theory of Artificial Intelligence*, Berlin: Springer, pp. 251–264.\n\nSandberg, Anders (2014) [Ethics of brain emulations](http://doi.org/10.1080/0952813X.2014.895113), *Journal of Experimental and Theoretical Artificial Intelligence*, vol. 26, pp. 439–457.\n\nSandberg, Anders & Nick Bostrom (2008) [Whole Brain Emulation: A Roadmap](https://www.fhi.ox.ac.uk/brain-emulation-roadmap-report.pdf).\n\nShulman, Carl (2012) [Could we use untrustworthy human brain emulations to make trustworthy ones?](https://www.youtube.com/watch?v=ljHFmznqkYM&list=PLkC3Ey3ATnTCn2_v8dgRmg5MBJtja_YK-&index=20), *The Fifth Conference on Artificial General Intelligence*, December 11.\n\nRelated entries\n---------------\n\n[artificial sentience](https://forum.effectivealtruism.org/tag/artificial-sentience) | [consciousness research](https://forum.effectivealtruism.org/tag/consciousness-research) | [digital person](https://forum.effectivealtruism.org/tag/digital-person) | [long-term future](https://forum.effectivealtruism.org/tag/long-term-future) | [moral patienthood](https://forum.effectivealtruism.org/tag/moral-patienthood) | [non-humans and the long-term future](https://forum.effectivealtruism.org/tag/non-humans-and-the-long-term-future)\n\n1.  ^**[^](#fnref9jpavdlbdrh)**^\n    \n    80,000 Hours (2022) [Our current list of pressing world problems](https://80000hours.org/problem-profiles/), *80,000 Hours*."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "tLi2a6FwjZGLHsXEW",
    "name": "Welfare economics",
    "core": false,
    "slug": "welfare-economics",
    "oldSlugs": null,
    "postCount": 10,
    "description": {
      "markdown": "**Welfare economics** is the branch of economics that seeks to evaluate policies in terms of their effects on [wellbeing](https://forum.effectivealtruism.org/tag/wellbeing)."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "4ntnNW8gpYRs2tKNM",
    "name": "Welfare biology",
    "core": false,
    "slug": "welfare-biology",
    "oldSlugs": null,
    "postCount": 30,
    "description": {
      "markdown": "**Welfare biology** is the study of living things considered as [moral patients](https://forum.effectivealtruism.org/tag/moral-patienthood). The discipline's main purpose is to determine the circumstances affecting [wild animal welfare](https://forum.effectivealtruism.org/tag/wild-animal-welfare). **Applied welfare biology**, in turn, is the application of welfare biology to identify interventions that affect the welfare of wild animals.^[\\[1\\]](#fn6vkgd8bk4m9)^ The field was established by economist [Yew-Kwang Ng](https://forum.effectivealtruism.org/tag/yew-kwang-ng) in a seminal 1995 paper.^[\\[2\\]](#fn8898a3mx0u5)^\n\nFurther reading\n---------------\n\nAnimal Ethics (2018) [Welfare biology](https://www.animal-ethics.org/introduction-to-welfare-biology/), *Animal Ethics*, September 30 (updated 10 December 2021).\n\nCarpendale, Max (2015) [Welfare biology as an extension of biology: Interview with Yew-Kwang Ng](http://doi.org/10.7358/rela-2015-002-carp), *Relations*, vol. 3, pp. 197–202.\n\nFaria, Catia & Oscar Horta (2019) [Welfare biology](http://doi.org/10.4324/9781315105840-41), in Bob Fischer (ed.) *The Routledge Handbook of Animal Ethics*, New York: Routledge, pp. 455–466.\n\nGroff, Zach & Yew-Kwang Ng (2019) [Does suffering dominate enjoyment in the animal kingdom? An update to welfare biology](http://doi.org/10.1007/s10539-019-9692-0), *Biology and Philosophy*, vol. 34, pp. 1–16.\n\nNg, Yew-Kwang (2016) [How welfare biology and commonsense may help to reduce animal suffering](https://animalstudiesrepository.org/animsent/vol1/iss7/1/), *Animal Sentience*, vol. 7, pp. 1–10.\n\nPearce, David (2016) [Compassionate biology: How CRISPR-based “gene drives” could cheaply, rapidly and sustainably reduce suffering throughout the living world](https://www.hedweb.com/gene-drives/index.html), *BLTC Research* (updated 2021).\n\n1.  ^**[^](#fnref6vkgd8bk4m9)**^\n    \n    Faria, Catia & Oscar Horta (2019) [Welfare biology](http://doi.org/10.4324/9781315105840), in Bob Fischer (ed.) *The Routledge Handbook of Animal Ethics*, New York: Routledge, pp. 455–466.\n    \n2.  ^**[^](#fnref8898a3mx0u5)**^\n    \n    Ng, Yew-Kwang (1995) [Towards welfare biology: Evolutionary economics of animal consciousness and suffering](http://doi.org/10.1007/BF00852469), *Biology and Philosophy*, vol. 10, pp. 255–285."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "xSZDegEMcbexn84K5",
    "name": "Warning shot",
    "core": false,
    "slug": "warning-shot",
    "oldSlugs": [
      "a-warning-shot"
    ],
    "postCount": 10,
    "description": {
      "markdown": "A **warning shot** is a [global catastrophe](https://forum.effectivealtruism.org/tag/global-catastrophic-risk) that indirectly reduces [existential risk](https://forum.effectivealtruism.org/topics/existential-risk) by increasing concern about future catastrophes. \n\nTerminology\n-----------\n\nThe expression **warning sign** is sometimes used to describe any event that increases concern about a particular category of existential risk, regardless of whether the event itself constitutes a global catastrophe. For example, plausible candidates for an AI warning sign include not only a catastrophic failure by an AI system but also public outreach campaigns or the publication of an exceptionally persuasive book on AI safety.^[\\[1\\]](#fn3vg8vj5l8a8)^ \n\nA related notion is that of a **fire alarm**, a warning sign that creates *common knowledge* that some technology—typically [avanced artificial intelligence](https://forum.effectivealtruism.org/tag/artificial-intelligence)—actually poses an existential risk.^[\\[2\\]](#fnk1kxvv8081)^\n\nNote, however, that both \"warning shot\" and \"fire alarm\" are sometimes used as synonyms for \"warning sign\".^[\\[3\\]](#fnipfdul9zh8q)^^[\\[4\\]](#fnwq9pdwhdojb)^\n\nFurther reading\n---------------\n\nBeckstead, Nick (2015) [The long-term significance of reducing global catastrophic risks](https://www.openphilanthropy.org/blog/long-term-significance-reducing-global-catastrophic-risks), *Open Philanthropy*, August 13.\n\nCarlsmith, Joseph (2021) [Is power-seeking AI an existential risk?](https://docs.google.com/document/d/1smaI1lagHHcrhoi6ohdq3TYIZv0eNWWZMPEy8C8byYg/edit?usp=embed_facebook), *Open Philanthropy*, April, section 6.2.\n\nRelated entries\n---------------\n\n[artificial intelligence](https://forum.effectivealtruism.org/topics/artificial-intelligence) | [existential risk](https://forum.effectivealtruism.org/topics/existential-risk) | [global catastrophic risk](https://forum.effectivealtruism.org/tag/global-catastrophic-risk)\n\n1.  ^**[^](#fnref3vg8vj5l8a8)**^\n    \n    Hobson, Donald (2020) [Comment on ‘What are the most plausible “AI Safety warning shot” scenarios?’](https://www.alignmentforum.org/posts/hLKKH9CM6NDiJBabC/what-are-the-most-plausible-ai-safety-warning-shot-scenarios), *AI Alignment Forum*, March 26.\n    \n2.  ^**[^](#fnrefk1kxvv8081)**^\n    \n    Grace, Katja (2021) [Beyond fire alarms: freeing the groupstruck](https://aiimpacts.org/beyond-fire-alarms-freeing-the-groupstruck/), *AI Impacts*, September 26.\n    \n3.  ^**[^](#fnrefipfdul9zh8q)**^\n    \n    Kokotajlo, Daniel (2020) [What are the most plausible ‘AI Safety warning shot’ scenarios?](https://www.alignmentforum.org/posts/hLKKH9CM6NDiJBabC/what-are-the-most-plausible-ai-safety-warning-shot-scenarios), *AI Alignment Forum*, March 26.\n    \n4.  ^**[^](#fnrefwq9pdwhdojb)**^\n    \n    McCluskey, Peter (2021) [AI fire alarm scenarios](http://www.bayesianinvestor.com/blog/index.php/2021/12/23/ai-fire-alarm-scenarios/), *Bayesian Investor*, December 23."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "kcjHcrJJoz5rQX4am",
    "name": "Vulnerable world hypothesis",
    "core": false,
    "slug": "vulnerable-world-hypothesis",
    "oldSlugs": null,
    "postCount": 16,
    "description": {
      "markdown": "The **vulnerable world hypothesis** (**VWH**) is the view that there exists some level of technology at which civilization almost certainly gets destroyed unless extraordinary preventive measures are undertaken. VWH was introduced by [Nick Bostrom](https://forum.effectivealtruism.org/tag/nick-bostrom) in 2019.^[\\[1\\]](#fn5oro9ooa0ld)^\n\nHistorical precedents\n---------------------\n\nVersions of VWH have been suggested prior to Bostrom's statement of it, though not defined precisely or analyzed rigorously. An early expression is arguably found in a 1945 address by [Bertrand Russell](https://forum.effectivealtruism.org/tag/bertrand-russell) to the House of Lords concerning the detonation of atomic bombs in Hiroshima and Nagasaki and its implications for the future of humanity.^[\\[2\\]](#fnp5ilpu46ag)^ (Russell frames his concerns specifically about [nuclear warfare](https://forum.effectivealtruism.org/tag/nuclear-warfare-1), but as [Toby Ord](https://forum.effectivealtruism.org/tag/toby-ord) has argued,^[\\[3\\]](#fn5tcetgh7xts)^ this is how early discussions about existential risk were presented, because at the time nuclear power was the only known technology with the potential to cause an [existential catastrophe](https://forum.effectivealtruism.org/tag/existential-catastrophe-1).)\n\n> All that must take place if our scientific civilization goes on, if it does not bring itself to destruction; all that is bound to happen. We do not want to look at this thing simply from the point of view of the next few years; we want to look at it from the point of view of the future of mankind. The question is a simple one: Is it possible for a scientific society to continue to exist, or must such a society inevitably bring itself to destruction? It is a simple question but a very vital one. I do not think it is possible to exaggerate the gravity of the possibilities of evil that lie in the utilization of atomic energy. As I go about the streets and see St. Paul's, the British Museum, the Houses of Parliament and the other monuments of our civilization, in my mind's eye I see a nightmare vision of those buildings as heaps of rubble with corpses all round them. That is a thing we have got to face, not only in our own country and cities, but throughout the civilized world as a real probability unless the world will agree to find a way of abolishing war. It is not enough to make war rare; great and serious war has got to be abolished, because otherwise these things will happen.\n\nFurther reading\n---------------\n\nBostrom, Nick (2019) [The vulnerable world hypothesis](http://doi.org/10.1111/1758-5899.12718), *Global Policy*, vol. 10, pp. 455–476.\n\nBostrom, Nick & Matthew van der Merwe (2021) [How vulnerable is the world?](https://aeon.co/essays/none-of-our-technologies-has-managed-to-destroy-humanity-yet), *Aeon*, February 12.\n\nChristiano, Paul (2016) [Handling destructive technology](https://ai-alignment.com/handling-destructive-technology-85800a12d99), *AI Alignment*, November 14.\n\nHanson, Robin (2018) [Vulnerable world hypothesis](http://www.overcomingbias.com/2018/11/vulnerable-world-hypothesis.html), *Overcoming Bias*, November 16.\n\nHuemer, Michael (2020) [The case for tyranny](https://fakenous.net/?p=1720&fbclid=IwAR3LFWKFUi-pZZY8x2iHf5DJ6vgbpNE8bJe_ZjonwXP9a7dzOHDl_QqUPV0), *Fake Nous*, July 11.\n\nKarpathy, Andrej (2016) [Review of *The Making of the Atomic Bomb*](https://www.goodreads.com/review/show/1385479805), *Goodreads*, December 13.\n\nManheim, David (2020) [The fragile world hypothesis: complexity, fragility, and systemic existential risk](http://doi.org/10.1016/j.futures.2020.102570), *Futures*, vol. 122, pp. 1–8.\n\nPiper, Kelsey (2018) [How technological progress is making it likelier than ever that humans will destroy ourselves](https://www.vox.com/future-perfect/2018/11/19/18097663/nick-bostrom-vulnerable-world-global-catastrophic-risks), *Vox*, November 19.\n\nRozendal, Siebe (2020) [The problem of collective ruin](http://www.sieberozendal.com/blog/the-problem-of-collective-ruin/), *Siebe Rozendal’s Blog*, August 22.\n\nSagan, Carl (1994) [*Pale Blue Dot: A Vision of the Human Future in Space*](https://en.wikipedia.org/wiki/Special:BookSources/0345376595), New York: Random House.\n\nRelated entries\n---------------\n\n[anthropogenic existential risk](https://forum.effectivealtruism.org/tag/anthropogenic-existential-risk) | [differential progress](https://forum.effectivealtruism.org/tag/differential-progress) | [existential security](https://forum.effectivealtruism.org/tag/existential-security) | [global governance](https://forum.effectivealtruism.org/tag/global-governance) | [international organization](https://forum.effectivealtruism.org/tag/international-organization) | [terrorism](https://forum.effectivealtruism.org/tag/terrorism) | [time of perils](https://forum.effectivealtruism.org/topics/time-of-perils)\n\n1.  ^**[^](#fnref5oro9ooa0ld)**^\n    \n    Bostrom, Nick (2019) [The vulnerable world hypothesis](http://doi.org/10.1111/1758-5899.12718), *Global Policy*, vol. 10, pp. 455–476.\n    \n2.  ^**[^](#fnrefp5ilpu46ag)**^\n    \n    Russell, Bertrand (1945) [The international situation](https://hansard.parliament.uk/Lords/1945-11-28/debates/2e5bb9d2-def1-4a01-b4cd-2404e33b7a69/LordsChamber#contribution-8e35cb23-4c57-49dd-93d1-74fcba1e91ad), *The Parliamentary Debates (Hansard)*, vol. 138, pp. 87–93, p. 89.\n    \n3.  ^**[^](#fnref5tcetgh7xts)**^\n    \n    Ord, Toby (2020) [*The Precipice: Existential Risk and the Future of Humanity*](https://en.wikipedia.org/wiki/Special:BookSources/1526600218), London: Bloomsbury Publishing, ch. 2."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "b2DHmi9bStX9wSpCf",
    "name": "Vasili Arkhipov",
    "core": false,
    "slug": "vasili-arkhipov",
    "oldSlugs": null,
    "postCount": null,
    "description": {
      "markdown": "**Vasili Alexandrovich Arkhipov** (30 January 1926 – 19 August 1998) was a Soviet military officer. As second-in-command of a nuclear-armed submarine during the [Cuban Missile Crisis](https://forum.effectivealtruism.org/tag/cuban-missile-crisis), Arkhipov blocked the captain's decision to launch a nuclear torpedo against the US Navy, likely averting a large-scale [nuclear war](https://forum.effectivealtruism.org/tag/nuclear-warfare-1). Reflecting on this incident forty years later, Thomas Blanton, director of the National Security Archive, remarked that \"a guy called Vasili Arkhipov saved the world.\"^[\\[1\\]](#fnfa5qr4qmooi)^\n\nIn 2017, Arkhipov was posthumously awarded the inaugural [Future of Life Award](https://forum.effectivealtruism.org/tag/future-of-life-institute#Future_of_Life_Award). In presenting the award to Arkhipov's surviving family, Future of Life Institute president Max Tegmark remarked that \"Vasili Arkhipov is arguably the most important person in modern history, thanks to whom October 27 2017 isn’t the 55th anniversary of World War III.\"^[\\[2\\]](#fngo6md4zcizb)^\n\nOne of the rooms in the [Future of Humanity Institute](https://forum.effectivealtruism.org/tag/future-of-humanity-institute) is named after Arkhipov.\n\nFurther reading\n---------------\n\nRoberts, Priscilla Mary (ed.) (2012) [*Cuban Missile Crisis: The Essential Reference Guide*](https://en.wikipedia.org/wiki/Special:BookSources/9781610690669), Santa Barbara, California: ABC-CLIO, pp. 13-14.\n\nSavranskaya, Svetlana (2005) [New sources on the role of Soviet submarines in the Cuban missile crisis](http://doi.org/10.1080/01402390500088312), *Journal of Strategic Studies*, vol. 28, pp. 233–259.\n\nRelated entries\n---------------\n\n[Cuban Missile Crisis](https://forum.effectivealtruism.org/tag/cuban-missile-crisis) |  [Stanislav Petrov](https://forum.effectivealtruism.org/tag/stanislav-petrov)\n\n1.  ^**[^](#fnreffa5qr4qmooi)**^\n    \n    Lloyd, Marion (2002) 'Soviets close to using A-bomb in 1962 crisis, forum is told', *Boston Globe*, October 13, p. A20.\n    \n2.  ^**[^](#fnrefgo6md4zcizb)**^\n    \n    Future of Life Institute (2017) [Future Of Life Award 2017](https://futureoflife.org/2017/10/27/future-of-life-award-2017/), *Future of Life Institute*, October 27."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "Yy8JXmjGZrujD2RCP",
    "name": "Value of information",
    "core": false,
    "slug": "value-of-information",
    "oldSlugs": null,
    "postCount": 17,
    "description": {
      "markdown": "The **value of information** is the extent to which additional information improves an agent's decision. This depends on how likely that information is to change the agent's decision and how much of an improvement that change would be. Value of information will usually be higher the less [resilient](https://forum.effectivealtruism.org/tag/credal-resilience) the agent's relevant credences currently are, because that will tend to mean new evidence is more likely to change the agent's credences.\n\nFurther reading\n---------------\n\nAskell, Amanda (2017) [The moral value of information](https://www.effectivealtruism.org/articles/the-moral-value-of-information-amanda-askell/), *Effective Altruism*, June 4.\n\nMuehlhauser, Luke (2013) [Review of Douglas Hubbard, *How to Measure Anything*](https://www.lesswrong.com/posts/ybYBCK9D7MZCcdArB/how-to-measure-anything), *LessWrong*, August 7.\n\nRelated entries\n---------------\n\n[alternatives to expected value theory](https://forum.effectivealtruism.org/tag/alternatives-to-expected-value-theory) | [credence](https://forum.effectivealtruism.org/tag/credence) | [explore-exploit tradeoff](https://forum.effectivealtruism.org/topics/explore-exploit-tradeoff) | [forecasting](https://forum.effectivealtruism.org/tag/forecasting) | [model uncertainty](https://forum.effectivealtruism.org/tag/model-uncertainty)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "iS9JYedpFjEr3DAKe",
    "name": "Value lock-in",
    "core": false,
    "slug": "value-lock-in",
    "oldSlugs": null,
    "postCount": 17,
    "description": {
      "markdown": "**Value lock-in** is a state in which the values determining the [long-term future](https://forum.effectivealtruism.org/tag/long-term-future) of Earth-originating life can no longer be altered.\n\nFurther reading\n---------------\n\nMacAskill, William (2022) [*What We Owe the Future*](https://en.wikipedia.org/wiki/Special:BookSources/978-1-5416-1862-6), New York: Basic Books, ch. 4.\n\nRiedel, Jess (2021) [Value lock-in notes](https://jessriedel.com/index_files/Value%20Lock-in%20Notes%202021%20(Public%20version).pdf), *Jess Riedel’s Website*, July 25.\n\nRelated entries\n---------------\n\n[dystopia](https://forum.effectivealtruism.org/tag/dystopia) | [hinge of history](https://forum.effectivealtruism.org/tag/hinge-of-history) | [longtermism](https://forum.effectivealtruism.org/tag/longtermism) | [moral advocacy](https://forum.effectivealtruism.org/tag/moral-advocacy)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "3qKmzJFPfzMya8gQY",
    "name": "Utilitarian Society",
    "core": false,
    "slug": "utilitarian-society",
    "oldSlugs": null,
    "postCount": 1,
    "description": {
      "markdown": "The **Utilitarian Society** was an informal discussion group established by [John Stuart Mill](https://forum.effectivealtruism.org/tag/john-stuart-mill) and others in 1823."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "38M9bHenApRdx397k",
    "name": "Unprecedented risks",
    "core": false,
    "slug": "unprecedented-risks",
    "oldSlugs": null,
    "postCount": null,
    "description": {
      "markdown": "**Unprecedented risks** are risks which have never occurred. Such risks pose a number of unique challenges. Importantly, [existential risks](https://forum.effectivealtruism.org/tag/existential-risk) are necessarily unprecedented.\n\nFurther reading\n---------------\n\nOrd, Toby (2020) [*The Precipice: Existential Risk and the Future of Humanity*](https://en.wikipedia.org/wiki/Special:BookSources/1-5266-0021-8), London: Bloomsbury Publishing, ch. 7."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "K4ReLNPtip4uD6crC",
    "name": "Unknown existential risk",
    "core": false,
    "slug": "unknown-existential-risk",
    "oldSlugs": [
      "unknown-existential-risks"
    ],
    "postCount": 1,
    "description": {
      "markdown": "An **unknown existential risk** is an [existential risk](https://forum.effectivealtruism.org/tag/existential-risk) which has not yet been identified.\n\nEven at the point when Leo Szilard discovered the concept of a nuclear chain reaction, other nuclear scientists were dismissive of the possibility of ever generating energy from atomic forces. The first atomic bombs were built just over a decade later.\n\nIn the same way, ideas which we currently laugh off might one day become a reality. Such scenarios are extremely hard to predict and plan around. Most of the ideas we currently laugh off will probably continue to be laughable. Yet it would be overconfident to assume that we have already identified all significant existential risks, or that we are at present in a position to do so.^[\\[1\\]](#fntukn2ts7t8f)^\n\n1.  ^**[^](#fnreftukn2ts7t8f)**^\n    \n    Bostrom, Nick (2002) [Existential risks: Analyzing human extinction scenarios and related hazards](https://www.jetpress.org/volume9/risks.html), *Journal of Evolution and Technology*, vol. 9, section 4.7."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "8PCyDb4pfLNoiKcyr",
    "name": "Universal basic income",
    "core": false,
    "slug": "universal-basic-income",
    "oldSlugs": null,
    "postCount": 15,
    "description": {
      "markdown": "**Universal basic income** (**UBI**) is a government program of periodic cash payments unconditionally delivered to every adult and sufficient to pay for a person's basic needs.\n\nFurther reading\n---------------\n\nLowrey, Annie (2018) [*Give People Money: The Simple Idea to Solve Inequality and Revolutionise Our Lives*](https://en.wikipedia.org/wiki/Special:BookSources/0753545772), London: WH Allen.\n\nGiveDirectly (2019) [UBI (Universal Basic Income): An overview](https://www.givedirectly.org/basic-income/), *GiveDirectly*.\n\nRelated entries\n---------------\n\n[automation](https://forum.effectivealtruism.org/tag/automation) | [global poverty](https://forum.effectivealtruism.org/tag/global-poverty) | [policy](https://forum.effectivealtruism.org/tag/policy)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "vAGCYp8HjqFfXSKHg",
    "name": "Unilateralist's curse",
    "core": false,
    "slug": "unilateralist-s-curse",
    "oldSlugs": null,
    "postCount": 3,
    "description": {
      "markdown": "The **unilateralist's curse** is the phenomenon whereby, when each of many altruistic agents have the power to bring about some state of affairs whose net value is negative but unknown to these agents, the probability that the state will be realized grows with the number of agents who decide to act based on their own private judgment.\n\nSalient examples include decisions to leak information about [weapons](https://forum.effectivealtruism.org/tag/weapons-of-mass-destruction) technologies, potential decisions by individual nations to use geoengineering to mitigate [climate change](https://forum.effectivealtruism.org/tag/climate-change), and the unilateral decision to introduce rabbits to Australia.  \n  \nTo avoid the unilateralist’s curse, members of a group might implement a group decision-making procedure, deliberate with others before taking action, or create a norm of [deferring](https://forum.effectivealtruism.org/tag/epistemic-deference) to the beliefs or actions of the other members of the group.\n\nFurther reading\n---------------\n\nBostrom, Nick, Thomas Douglas & Anders Sandberg (2016) [The unilateralist's curse and the case for a principle of conformity](http://doi.org/10.1080/02691728.2015.1108373), *Social Epistemology*, vol. 30, pp. 350-371.\n\nLewis, Gregory (2018) [Horsepox synthesis: A case of the unilateralist's curse?](https://thebulletin.org/2018/02/horsepox-synthesis-a-case-of-the-unilateralists-curse/), *Bulletin of the Atomic Scientists*, February 19.  \n*Usefully connects the curse to other factors*\n\nSchubert, Stefan & Ben Garfinkel (2017) [Hard-to-reverse decisions destroy option value](https://www.centreforeffectivealtruism.org/blog/hard-to-reverse-decisions-destroy-option-value/), *Centre for Effective Altruism*, March 17.\n\nZhang, Linchuan (2020) [Framing issues with the unilateralist's curse](https://forum.effectivealtruism.org/posts/myp9Y9qJnpEEWhJF9/linch-s-shortform#no9szQcHeYS94GYgx), *Effective Altruism Forum*, January 17.\n\nRelated entries\n---------------\n\n[accidental harm](https://forum.effectivealtruism.org/tag/accidental-harm) | [information hazard](https://forum.effectivealtruism.org/tag/information-hazard) | [optimizer's curse](https://forum.effectivealtruism.org/topics/optimizer-s-curse)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "eWtqraqATbHfGANKb",
    "name": "Trinity",
    "core": false,
    "slug": "trinity",
    "oldSlugs": null,
    "postCount": null,
    "description": {
      "markdown": "**Trinity** was the code name of the first detonation of a nuclear device. The detonation occurred on 16 July 1945 at 11:29:21 UTC. According to [Toby Ord](https://forum.effectivealtruism.org/tag/toby-ord), Trinity marks the beginning of \"The Precipice\", a period of heightened [existential risk](https://forum.effectivealtruism.org/tag/existential-risk) which humanity must navigate safely to realize its long-term potential.^[\\[1\\]](#fn3ws6d422cmg)^\n\nFurther reading\n---------------\n\nElse, Jon (1981) [The day after Trinity](https://www.imdb.com/title/tt0080594/?ref_=fn_al_tt_1), *KTEH*.\n\nOrd, Toby (2020) [*The Precipice: Existential Risk and the Future of Humanity*](https://en.wikipedia.org/wiki/Special:BookSources/1526600218), London: Bloomsbury Publishing.\n\nRelated entries\n---------------\n\n[Bulletin of the Atomic Scientists](/tag/bulletin-of-the-atomic-scientists) | [existential risk](https://forum.effectivealtruism.org/tag/existential-risk) | [existential security](https://forum.effectivealtruism.org/tag/existential-security) | [Manhattan Project](https://forum.effectivealtruism.org/tag/manhattan-project) | [nuclear warfare](https://forum.effectivealtruism.org/tag/nuclear-warfare-1) | [Russell–Einstein Manifesto](https://forum.effectivealtruism.org/tag/russell-einstein-manifesto)\n\n1.  ^**[^](#fnref3ws6d422cmg)**^\n    \n    Ord, Toby (2020) [*The Precipice: Existential Risk and the Future of Humanity*](https://en.wikipedia.org/wiki/Special:BookSources/1526600218), London: Bloomsbury Publishing,  ch. 4. Other authors have also proposed the Trinity test as marking the beginning of a new epoch.  Thus, Jan Zalasiewicz and colleagues \"suggest that the Anthropocene (formal or informal) be defined to begin historically at the moment of detonation of the Trinity A-bomb at Alamogordo, New Mexico, at 05:29:21 Mountain War Time (± 2 s) July 16, 1945 (= 11:29:21 Co-ordinated Universal Time = Greenwich Mean Time).\" (Zalasiewicz, Jan *et al.* (2015) [When did the Anthropocene begin? A mid-twentieth century boundary level is stratigraphically optimal](https://doi.org/10.1016/j.quaint.2014.11.045), *Quaternary International*, vol. 383, pp. 196–203.)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "M6ovvHbKFF29sp2g9",
    "name": "Total existential risk",
    "core": false,
    "slug": "total-existential-risk",
    "oldSlugs": null,
    "postCount": 1,
    "description": {
      "markdown": "**Total existential risk** is the cumulative risk of an [existential catastrophe](https://forum.effectivealtruism.org/tag/existential-risk).\n\nThe concept of *total existential risk* allows for comparisons of different specific risks in terms of their contribution to the overall risk of catastrophe. This comparison can be made because the particular existential risks are assumed to differ only in their probability and not also in their severity. The assumption is typically warranted since world histories involving existential catastrophes tend to differ in value in minor ways, relative to how each differs from world histories where human potential is fully realized. Permanent [civilizational collapse](https://forum.effectivealtruism.org/tag/civilizational-collapse), for instance, may be somewhat better or somewhat worse than [human extinction](https://forum.effectivealtruism.org/tag/human-extinction); but both are incalculably worse than a world in which humanity has attained its full potential.^[\\[1\\]](#fn8dmjw0addi)^\n\nThe assumption may fail to hold in special cases, however. First, a [hellish existential catastrophe](https://forum.effectivealtruism.org/tag/hellish-existential-catastrophe) does not only destroy potential value; it also creates disvalue on an astronomical scale. If the catastrophe is as bad as it could possibly be, it would be significantly worse than a non-hellish existential catastrophe.\n\nSecond, as Ord notes, some risks may more likely occur in worlds with high potential. A technology that contributes to a risk of this sort would be penalized if assessed by the metric of total existential risk. A straightforward example is [artificial intelligence](https://forum.effectivealtruism.org/tag/artificial-intelligence), which increases existential risk from [AI alignment](https://forum.effectivealtruism.org/tag/ai-alignment) but can also bring humanity closer to realizing its potential.^[\\[1\\]](#fn8dmjw0addi)^\n\nFurther reading\n---------------\n\nOrd, Toby (2020) [*The Precipice: Existential Risk and the Future of Humanity*](https://en.wikipedia.org/wiki/Special:BookSources/1526600218), London: Bloomsbury Publishing, ch. 6.\n\nRelated entries\n---------------\n\n[existential risk](https://forum.effectivealtruism.org/tag/existential-risk) | [hellish existential catastrophe](https://forum.effectivealtruism.org/tag/hellish-existential-catastrophe)\n\n1.  ^**[^](#fnref8dmjw0addi)**^\n    \n    Ord, Toby (2020) [*The Precipice: Existential Risk and the Future of Humanity*](https://en.wikipedia.org/wiki/Special:BookSources/1526600218), London: Bloomsbury Publishing."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "4FJfjegoRpr4k7ksD",
    "name": "Tool AI",
    "core": false,
    "slug": "tool-ai",
    "oldSlugs": null,
    "postCount": null,
    "description": {
      "markdown": "**Tool AI** is a type of [artificial intelligence](https://forum.effectivealtruism.org/tag/artificial-intelligence) that resembles a tool rather than an agent. Tool AIs are claimed to pose fewer risks than agent AIs, and are for this reason proposed as a possible solution (or dissolution) of the [AI alignment problem](https://forum.effectivealtruism.org/tag/ai-alignment).\n\nFurther reading\n---------------\n\nBostrom, Nick (2014) [*Superintelligence: Paths, Dangers, Strategies*](https://en.wikipedia.org/wiki/Special:BookSources/9780199678112), Oxford: Oxford University Press, pp. 151–155.\n\nRelated entries\n---------------\n\n[oracle AI](https://forum.effectivealtruism.org/tag/oracle-ai) | [sovereign AI](https://forum.effectivealtruism.org/tag/sovereign-ai) | [comprehensive AI services](https://forum.effectivealtruism.org/tag/comprehensive-ai-services)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "YN9XA7pTkX2F2PdWR",
    "name": "Time of perils",
    "core": false,
    "slug": "time-of-perils",
    "oldSlugs": [
      "time-of-perils",
      "time-of-perils-hypothesis"
    ],
    "postCount": 4,
    "description": {
      "markdown": "The **time of perils** is a hypothetical period in human history during which [existential risk](https://forum.effectivealtruism.org/topics/existential-risk) is unusually high. The **time of perils hypothesis** is the view that we are currently living in a time of perils.\n\nDuration of the time of perils\n------------------------------\n\nCarl Shulman has argued that the time of perils will likely last only a few centuries because of various technological developments that are jointly expected to eventually bring down existential risk per century to very low levels. As Shulman writes,^[\\[1\\]](#fn7z9emt9nimt)^\n\n> It's quite likely the extinction/existential catastrophe rate approaches zero within a few centuries if civilization survives, because:\n> \n> 1.  Riches and technology make us comprehensively immune to  natural disasters.\n> 2.  Cheap ubiquitous detection, barriers, and sterilization make civilization [immune to biothreats](https://reflectivedisequilibrium.blogspot.com/2020/05/what-would-civilization-immune-to.html)\n> 3.  Advanced tech makes neutral parties immune to the effects of nuclear winter.\n> 4.  Local cheap production makes for small supply chains that can regrow from disruption as industry becomes more like information goods.\n> 5.  Space colonization creates robustness against local disruption.\n> 6.  Aligned AI blocks threats from misaligned AI (and many other things).\n> 7.  Advanced technology enables stable policies (e.g. the same AI police systems enforce treaties banning WMD war for billions of years), and the world is likely to wind up in some stable situation (bouncing around until it does).\n\nFurther reading\n---------------\n\nMacAskill, William (2022) [Are we living at the hinge of history?](https://doi.org/10.1093/oso/9780192894250.003.0013), in Jeff McMahan *et al.* (eds.) *Ethics and Existence: The Legacy of Derek Parfit*, Oxford: Oxford University Press, pp. 331–357.\n\nRelated entries\n---------------\n\n[hinge of history](https://forum.effectivealtruism.org/topics/hinge-of-history) | [vulnerable world hypothesis](https://forum.effectivealtruism.org/topics/vulnerable-world-hypothesis)\n\n1.  ^**[^](#fnref7z9emt9nimt)**^\n    \n    Shulman, Carl (2022) [Comment on “The discount rate is not zero”](https://forum.effectivealtruism.org/posts/zLZMsthcqfmv5J6Ev/?commentId=Nr35E6sTfn9cPxrwQ), *Effective Altruism Forum*, September 2."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "xs2CK2yPzFKXjAj8N",
    "name": "The Life You Can Save",
    "core": false,
    "slug": "the-life-you-can-save",
    "oldSlugs": null,
    "postCount": 18,
    "description": {
      "markdown": "**The Life You Can Save** (**TLYCS**) is a nonprofit organization that encourages individuals to donate 1% of their income to [effective charities](https://forum.effectivealtruism.org/tag/effective-giving). TLYCS was founded by [Peter Singer](https://forum.effectivealtruism.org/tag/peter-singer), following the release of his book, [*The Life You Can Save: Acting Now to End World Poverty*](https://forum.effectivealtruism.org/tag/the-life-you-can-save-book).\n\nImpact\n------\n\nIn 2021, TLYCS moved nearly $22.7 million to their recommended charities, either through the website's donation platform or as a direct result of their outreach activities.^[\\[1\\]](#fnz05jkgkcv0b)^\n\nRecommendations\n---------------\n\nAs of  2022, TLYCS recommends the following charities:\n\n*   [Against Malaria Foundation](https://forum.effectivealtruism.org/tag/against-malaria-foundation)\n*   Carbon180\n*   Clean Air Task Force\n*   [Development Media International](https://forum.effectivealtruism.org/tag/development-media-international)\n*   Equalize Health\n*   Evergreen Collaborative\n*   [Evidence Action](https://forum.effectivealtruism.org/tag/evidence-action)\n*   Fistula Foundation\n*   Fred Hollows Foundation\n*   [GiveDirectly](https://forum.effectivealtruism.org/tag/givedirectly)\n*   [GAIN](https://forum.effectivealtruism.org/tag/global-alliance-for-improved-nutrition)’s Salt Iodization Program\n*   [Helen Keller International](https://forum.effectivealtruism.org/tag/helen-keller-international)\n*   [Innovations for Poverty Action](https://forum.effectivealtruism.org/tag/innovations-for-poverty-action)\n*   [Iodine Global Network](https://forum.effectivealtruism.org/tag/iodine-global-network)\n*   [Living Goods](https://forum.effectivealtruism.org/tag/living-goods)\n*   [Malaria Consortium](https://forum.effectivealtruism.org/tag/malaria-consortium)\n*   [New Incentives](https://forum.effectivealtruism.org/tag/new-incentives)\n*   One Acre Fund\n*   Oxfam\n*   Population Services International\n*   [Sanku - Project Healthy Children](https://forum.effectivealtruism.org/tag/sanku)\n*   [SCI Foundation](https://forum.effectivealtruism.org/tag/sci-foundation)\n*   Seva Foundation\n*   Village Enterprise\n*   [Zusha!](https://forum.effectivealtruism.org/topics/zusha)\n\nFurther reading\n---------------\n\nCharlie Bresler (2015) 'The Life You Can Save', in Ryan Carey (ed.), [*The Effective Altruism Handbook*](https://en.wikipedia.org/wiki/Special:BookSources/9781534935778), 1st ed., Oxford: The Centre for Effective Altruism, pp. 117-119.\n\nExternal links\n--------------\n\n[The Life You Can Save](https://www.thelifeyoucansave.org/). Official website.\n\n[Apply for a job](https://www.thelifeyoucansave.org/work-and-volunteer-with-us/).\n\n[Donate to The Life You Can Save](https://www.thelifeyoucansave.org/match-campaign-2022/).\n\nRelated entries\n---------------\n\n[charity evaluation](https://forum.effectivealtruism.org/tag/charity-evaluation) | [donation pledge](https://forum.effectivealtruism.org/tag/donation-pledge) | [effective giving](https://forum.effectivealtruism.org/tag/effective-giving-1) | [intervention evaluation](https://forum.effectivealtruism.org/tag/intervention-evaluation) | [Peter Singer](https://forum.effectivealtruism.org/tag/peter-singer) | [*The Life You Can Save*](https://forum.effectivealtruism.org/tag/the-life-you-can-save-book)\n\n1.  ^**[^](#fnrefz05jkgkcv0b)**^\n    \n    The Life You Can Save (2022) [2021 annual report](https://www.thelifeyoucansave.org.au/2021-annual-report/), *The Life You Can Save*."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "FP5CdLWGzGKDYcqmv",
    "name": "The Humane League",
    "core": false,
    "slug": "the-humane-league",
    "oldSlugs": null,
    "postCount": 15,
    "description": {
      "markdown": "**The Humane League** (**THL**) is a nonprofit organization that works to improve animal welfare standards through [corporate outreach](https://forum.effectivealtruism.org/tag/corporate-cage-free-campaigns), media outreach, and grassroots campaigns.\n\nHistory\n-------\n\nTHL was founded in 2005 as a small grassroots organization to protest foie gras at local restaurants in Philadelphia. It has since refocused on ending all forms of abuse in animals raised for food. It currently operates in four countries in three different continents and is part of Open Wing Alliance, a global coalition of organizations working to eliminate battery cages around the world.^[\\[1\\]](#fnxwjolmk5ol)^\n\nEvaluation\n----------\n\n[Animal Charity Evaluators](https://forum.effectivealtruism.org/tag/animal-charity-evaluators) rates THL a \"top charity\"—one of the four organizations awarded their highest rating. ACE considers THL \"an excellent giving opportunity because of their strong, cost-effective programs, their robust track record of strengthening the movement, and their healthy organizational culture.\"^[\\[2\\]](#fnekh65uj5kc)^ THL is also recommended by [Founders Pledge](https://forum.effectivealtruism.org/tag/founders-pledge) as \"one of the most cost-effective animal advocacy organisations in the world\".^[\\[3\\]](#fnkt3bs0itqm)^\n\nAs of July 2022, THL has received nearly $46 million in grants from [Open Philanthropy](https://forum.effectivealtruism.org/tag/open-philanthropy).^[\\[4\\]](#fnc27e5qc0nar)^\n\nFurther reading\n---------------\n\nClare, Stephen (2020) [The Humane League](https://founderspledge.com/stories/the-humane-league-high-impact-funding-opportunity), *Founders Pledge*, November 3.\n\nSánchez, Sebastián (2018) [Timeline of The Humane League](https://timelines.issarice.com/wiki/Timeline_of_The_Humane_League), *Timelines Wiki*.\n\nExternal links\n--------------\n\n[The Humane League](https://thehumaneleague.org/). Official website.\n\n[Apply for a job](https://thehumaneleague.org/work-with-us).\n\n[Donate to The Humane League](https://donate.thehumaneleague.org/donate).\n\n1.  ^**[^](#fnrefxwjolmk5ol)**^\n    \n    The Humane League (2021) [Our mission](https://thehumaneleague.org/our-mission), *The Humane League*.\n    \n2.  ^**[^](#fnrefekh65uj5kc)**^\n    \n    Animal Charity Evaluators (2020) [The Humane League review](https://animalcharityevaluators.org/charity-review/the-humane-league/), *Animal Charity Evaluators*, November.\n    \n3.  ^**[^](#fnrefkt3bs0itqm)**^\n    \n    Clare, Stephen (2020) [The Humane League](https://founderspledge.com/stories/the-humane-league-high-impact-funding-opportunity), *Founders Pledge*, November 3.\n    \n4.  ^**[^](#fnrefc27e5qc0nar)**^\n    \n    Open Philanthropy (2022) [Grants database: The Humane League](https://www.openphilanthropy.org/grants/?q=&organization-name=the-humane-league), *Open Philanthropy*."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "4YbBrYqW8XtSg6wcz",
    "name": "Temporal discounting",
    "core": false,
    "slug": "temporal-discounting",
    "oldSlugs": null,
    "postCount": 14,
    "description": {
      "markdown": "**Temporal discounting** (also called **time discounting**) is the discounting of the value of a good the further into the future it is expected to be realized.\n\nPeople often think that we should value goods in the future less than goods now. There are a variety of different reasons why we might discount the future: for instance, we might simply care less about the future than we do the present (this is known as *pure time preference*). Alternately, we might care just as much about my future, but think that there is some probability that it will not be possible to reap the benefits at that time (for instance, I might care less about my personal income in 40 years, simply because there's a reasonable chance that I will be dead by then, and not able to enjoy the income). A variety of other reasons might apply, depending on the good under discussion.\n\nA discount function shows how the value of a good decreases if it occurs at different times. A particularly common form of discount function is exponential. In this case, the discount rate (in annualized form) is the percentage decrease in the value of a good, one year into the future, compared to now. So if you value a sweet in a year 20% less than having a sweet now, you are using a discount rate of 20%.\n\nIt is generally thought that different discount functions should be used for different goods. This is because the reasons to discount apply to different extents for different goods. For instance, I might apply a lower discount rate for helping others than I do for myself, because it's more likely that (some) other people will be around in 60 years time than it is that I will still be alive in 60 years time.\n\nMembers of the effective altruism community have often argued against pure time discounting, and so for lower discounting of future welfare. This has contributed to some people focusing on issues relating to the [long-run future](https://forum.effectivealtruism.org/tag/longtermism).\n\nFurther reading\n---------------\n\nCotton-Barratt, Owen (2020) [Discounting for uncertainty in health](http://doi.org/10.1093/med/9780190082543.003.0014), in Nir Eyal *et al.* (eds.) *Measuring the Global Burden of Disease: Philosophical Dimensions*, Oxford: Oxford University Press, pp. 243–256.\n\nGreaves, Hilary (2017) [Discounting for public policy: a survey](http://doi.org/10.1017/S0266267117000062), *Economics and philosophy*, vol. 33, pp. 391–439.\n\nOrd, Toby & Robert Wiblin (2013) [Should we discount future health benefits when considering cost-effectiveness?](http://web.archive.org/web/20130501092024/http://www.givingwhatwecan.org/sites/givingwhatwecan.org/files/attachments/discounting-health2.pdf), *Giving What We Can*, May 1.  \n*Debate from researchers in the community.*"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "CMrXsZWmkKGfWBd7d",
    "name": "Technological completion conjecture",
    "core": false,
    "slug": "technological-completion-conjecture",
    "oldSlugs": null,
    "postCount": 3,
    "description": {
      "markdown": "The **technological completion conjecture** is the hypothesis that all possible technological capabilities allowed by the laws of physics will eventually be obtained, provided that scientific and technological development efforts do not cease. It was proposed by [Nick Bostrom](https://forum.effectivealtruism.org/tag/nick-bostrom) in a number of publications.^[\\[1\\]](#fnd3jhnhsksji)^^[\\[2\\]](#fn3zx1r3rsf7m)^^[\\[3\\]](#fnes7o6tf5xam)^\n\nFurther reading\n---------------\n\nBostrom, Nick (2009) [The future of humanity](https://doi.org/10.1057/9780230227279_10), in Jan Kyrre Berg Olsen, Evan Selinger & Søren Riis (eds.) *New Waves in Philosophy of Technology*, London: Palgrave Macmillan, pp. 189–194.\n\nRelated entries\n---------------\n\n[differential progress](https://forum.effectivealtruism.org/tag/differential-progress) | [macrostrategy](https://forum.effectivealtruism.org/tag/macrostrategy)\n\n1.  ^**[^](#fnrefd3jhnhsksji)**^\n    \n    Bostrom, Nick (2009) [The future of humanity](http://doi.org/10.1057/9780230227279_10), in Jan Kyrre Berg Olsen, Evan Selinger & Søren Riis (eds.) *New Waves in Philosophy of Technology*, London: Palgrave Macmillan, pp. 186–215.\n    \n2.  ^**[^](#fnref3zx1r3rsf7m)**^\n    \n    Bostrom, Nick (2014) [*Superintelligence: Paths, Dangers, Strategies*](https://en.wikipedia.org/wiki/Special:BookSources/978-0-19-967811-2), Oxford: Oxford University Press.\n    \n3.  ^**[^](#fnrefes7o6tf5xam)**^\n    \n    Bostrom, Nick (2014) [Crucial considerations and wise philanthropy](http://www.stafforini.com/blog/bostrom/), *Good Done Right*, July 9."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "imRLA8wwErncHXW4b",
    "name": "Target Malaria",
    "core": false,
    "slug": "target-malaria",
    "oldSlugs": null,
    "postCount": 2,
    "description": {
      "markdown": "**Target Malaria** is a nonprofit research consortium working to develop [gene drive](https://forum.effectivealtruism.org/tag/gene-drives) technologies to control the mosquitoes that transmit [malaria](https://forum.effectivealtruism.org/tag/malaria) in sub-Saharan Africa. It is led by Austin Burt, a professor of evolutionary genetics at Imperial College London.\n\nActivities\n----------\n\nIn July 2019, after obtaining approval from the National Biosafety Agency and the ethics committee of the Institut de Recherche en Sciences de la Santé of Burkina Faso, Target Malaria released a strain of genetically modified (but non gene drive) sterile male mosquito in Bana, a town in the Balé province of that West African country.^[\\[3\\]](#fn6m4h8dvr89x)^ Target Malaria has also research teams in Cape Verde, Ghana, Mali, and Uganda.\n\nEvaluation\n----------\n\nTarget Malaria is primarily funded by the [Bill & Melinda Gates Foundation](https://forum.effectivealtruism.org/tag/bill-and-melinda-gates-foundation) and [Open Philanthropy](https://forum.effectivealtruism.org/tag/open-philanthropy), which in May 2017 recommended a $17.5 million grant over four years.^[\\[2\\]](#fnbkgvppzdhgd)^^[\\[4\\]](#fn4xxg8tqoduk)^ Open Philanthropy estimates that their grant to Target Malaria is competitive with donations to the [Against Malaria Foundation](https://forum.effectivealtruism.org/tag/against-malaria-foundation).^[\\[5\\]](#fndzgoi5e8h1r)^\n\nFurther reading\n---------------\n\nDunphy, Siobhán (2020) [Interview with Professor Austin Burt: role of gene drive technology in the context of the EU’s Biodiversity Strategy 2030](https://www.europeanscientist.com/en/public-health/interview-with-professor-austin-burt-role-of-gene-drive-technology/), *European Scientist*, October 28.\n\nNorth, Ace R., Austin Burt & H. Charles J. Godfray (2019) [Modelling the potential of genetic control of malaria mosquitoes at national scale](http://doi.org/10.1186/s12915-019-0645-5), *BMC Biology*, vol. 17, pp. 1–12.\n\nExternal links\n--------------\n\n[Target Malaria](https://targetmalaria.org/). Official website.\n\nRelated entries\n---------------\n\n[gene drives](https://forum.effectivealtruism.org/tag/gene-drives) | [malaria](https://forum.effectivealtruism.org/tag/malaria)\n\n1.  ^**[^](#fnrefryshlf2elzo)**^\n    \n    Scudellari, Megan (2019) [Self-destructing mosquitoes and sterilized rodents: the promise of gene drives](http://doi.org/10.1038/d41586-019-02087-5), *Nature*, vol. 571, pp. 160–162.\n    \n2.  ^**[^](#fnrefbkgvppzdhgd)**^\n    \n    Open Philanthropy (2017) [Target Malaria — gene drives for malaria control](https://www.openphilanthropy.org/focus/scientific-research/miscellaneous/target-malaria-general-support), *Open Philanthropy*, May.\n    \n3.  ^**[^](#fnref6m4h8dvr89x)**^\n    \n    Diabate, Abdoulaye (2019) [Target Malaria proceeded with a small-scale release of genetically modified sterile male mosquitoes in Bana, a village in Burkina Faso](https://targetmalaria.org/target-malaria-proceeded-with-a-small-scale-release-of-genetically-modified-sterile-male-mosquitoes-in-bana-a-village-in-burkina-faso/), *Target Malaria's Blog*, July 1.\n    \n4.  ^**[^](#fnref4xxg8tqoduk)**^\n    \n    Burt, Austin (2021) [2021: progress during challenging times](https://targetmalaria.org/2021-progress-during-challenging-times/), *Target Malaria's Blog*, January 18.\n    \n5.  ^**[^](#fnrefdzgoi5e8h1r)**^\n    \n    Open Philanthropy (2017) [Rough Target Malaria cost-effectiveness calculation](https://docs.google.com/spreadsheets/d/1E8qu474nUUvPjK21oBqGkdQ9FuXanigOgSGfumhL3_c/edit#gid=1061406209), *Open Philanthropy*."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "7fCxZm5PghjWPqknE",
    "name": "Systemic change",
    "core": false,
    "slug": "systemic-change",
    "oldSlugs": null,
    "postCount": 47,
    "description": {
      "markdown": "**Systemic change** is change relating to practices or institutions established as normative or customary throughout a political, social, or economic system.\n\nSome [critics of effective altruism](https://forum.effectivealtruism.org/tag/criticism-of-effective-altruism) allege that its proponents have failed to engage with systemic change.^[\\[1\\]](#fnqyujpe49oz)^ Whether they are right or not depends on what is meant by the term. Members of the community are working on political issues within areas such as [macroeconomic policy](https://forum.effectivealtruism.org/tag/macroeconomic-stabilization), [immigration reform](https://forum.effectivealtruism.org/tag/immigration-reform) and [land use reform](https://forum.effectivealtruism.org/tag/land-use-reform).^[\\[2\\]](#fndip5razdtl)^ However, work on certain types of \"radical\" change, such as attempts to overthrow capitalism or abolish private property, are much less common.^[\\[3\\]](#fn4e1lz55z5yw)^\n\nFurther reading\n---------------\n\nEffective Altruism (2016) [Does effective altruism neglect systemic change?](https://www.effectivealtruism.org/faqs-criticism-objections/#does-effective-altruism-neglect-systemic-change), in 'Frequently asked questions and common objections', *Effective Altruism*.  \n*More information on this topic, including a list of articles arguing that effective altruists neglect “systemic change”.*\n\nOpen Philanthropy (2021) [Policy](http://www.openphilanthropy.org/research/cause-reports#Policy), in 'Cause reports', *Open Philanthropy*.  \n*The Open Philanthropy Project works on many projects seeking to bring about systemic change.*\n\nShulman, Carl (2019) [Some personal thoughts on EA and systemic change](https://forum.effectivealtruism.org/posts/QYH9yJ4WfHRs3ftJD/some-personal-thoughts-on-ea-and-systemic-change), *Effective Altruism Forum*, September 26.\n\nWiblin, Robert (2015) [Effective altruists love systemic change](https://80000hours.org/2015/07/effective-altruists-love-systemic-change/), *80,000 Hours*, July 8.  \n*Examples of members of the effective altruism community working on systemic change.*\n\nRelated entries\n---------------\n\n[electoral reform](https://forum.effectivealtruism.org/tag/electoral-reform) | [immigration reform](https://forum.effectivealtruism.org/tag/immigration-reform) | [improving institutional decision-making](https://forum.effectivealtruism.org/topics/improving-institutional-decision-making) | [land use reform](https://forum.effectivealtruism.org/tag/land-use-reform) | [longtermist institutional reform](https://forum.effectivealtruism.org/topics/longtermist-institutional-reform) | [macroeconomic policy](https://forum.effectivealtruism.org/tag/macroeconomic-policy) | [policy](https://forum.effectivealtruism.org/tag/policy)\n\n1.  ^**[^](#fnrefqyujpe49oz)**^\n    \n    Srinivasan, Amia (2015) [Stop the robot apocalypse](https://www.lrb.co.uk/the-paper/v37/n18/amia-srinivasan/stop-the-robot-apocalypse), *London Review of Books*, vol. 37, pp. 1–10.\n    \n2.  ^**[^](#fnrefdip5razdtl)**^\n    \n    Wiblin, Robert (2015) [Effective altruists love systemic change](https://80000hours.org/2015/07/effective-altruists-love-systemic-change/), *80,000 Hours*, July 8.\n    \n3.  ^**[^](#fnref4e1lz55z5yw)**^\n    \n    Chappell, Richard Yetter (2016) [Effective altruism, radical politics and radical philanthropy](http://www.philosophyetc.net/2016/04/effective-altruism-radical-politics-and.html), *Philosophy, et Cetera*, April 20."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "srLqySqxyZRhz8DJX",
    "name": "Supervolcano",
    "core": false,
    "slug": "supervolcano",
    "oldSlugs": null,
    "postCount": 9,
    "description": {
      "markdown": "A **supervolcano** is an unusually large volcano with the potential to produce an eruption with major effects on the global climate system.\n\nRecommendations\n---------------\n\nIn [*The Precipice: Existential Risk and the Future of Humanity*](https://forum.effectivealtruism.org/tag/the-precipice), [Toby Ord](https://forum.effectivealtruism.org/tag/toby-ord) offers several [policy](https://forum.effectivealtruism.org/tag/policy) and [research](https://forum.effectivealtruism.org/tag/research) recommendations for handling risks from supervolcanic eruptions:^[\\[1\\]](#fnxjqcoyeuy58)^\n\n*   Find all the places where supervolcanic eruptions have occurred in the past.\n*   Improve the very rough estimates on how frequent these eruptions are, especially for the largest eruptions.\n*   Improve our modeling of volcanic winter scenarios to see what sizes of eruption could pose a plausible threat to humanity.\n*   Liaise with leading figures in the asteroid community to learn lessons from them in their modeling and management.\n\n1.  ^**[^](#fnrefxjqcoyeuy58)**^\n    \n    Ord, Toby (2020) [*The Precipice: Existential Risk and the Future of Humanity*](https://en.wikipedia.org/wiki/Special:BookSources/1-5266-0021-8), London: Bloomsbury Publishing, pp. 277–278."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "QFB9sPSD3b9c7sb74",
    "name": "The Life You Can Save (book)",
    "core": false,
    "slug": "the-life-you-can-save-book",
    "oldSlugs": [
      "the-life-you-can-save-book"
    ],
    "postCount": null,
    "description": {
      "markdown": "***The Life You Can Save: Acting Now to End World Poverty*** is a book by [Peter Singer](https://forum.effectivealtruism.org/tag/peter-singer). It was published in March 2009.\n\nThe book played a major role in the formation of the effective altruism community.^[\\[1\\]](#fneyfbrp3sdzh)^ It also inspired [Cari Tuna](https://forum.effectivealtruism.org/tag/cari-tuna) and [Dustin Moskovitz](https://forum.effectivealtruism.org/tag/dustin-moskovitz) to establish the philanthropic foundation [Good Ventures](https://forum.effectivealtruism.org/tag/good-ventures), which has granted over $1.3 billion to organizations working on many of the world's most pressing problems.^[\\[2\\]](#fnn9e9vxkgo2)^^[\\[3\\]](#fnce4ujpvg0s)^^[\\[4\\]](#fniv2ex3sj5z)^\n\nIn December 2019, a revised 10th Anniversary Edition of the book was released, in both ebook and audiobook formats. The audiobook is narrated by Stephen Fry, Paul Simon, Kristen Bell, and other prominent actors, artists and philanthropists. Both the ebook and the audiobook can be downloaded for free.\n\nFurther reading\n---------------\n\nSinger, Peter (2009) [*The Life You Can Save: Acting Now to End World Poverty*](https://en.wikipedia.org/wiki/Special:BookSources/9780812981568), New York: Random House.\n\nExternal links\n--------------\n\n[The Life You Can Save](https://www.thelifeyoucansave.org/the-book/). Official website.\n\nRelated entries\n---------------\n\n[*Doing Good Better*](https://forum.effectivealtruism.org/tag/doing-good-better) | [Peter Singer](https://forum.effectivealtruism.org/tag/peter-singer) | [The Life You Can Save](https://forum.effectivealtruism.org/tag/the-life-you-can-save)\n\n1.  ^**[^](#fnrefeyfbrp3sdzh)**^\n    \n    Piper, Kelsey (2020) [The world’s problems overwhelmed me. This book empowered me](https://www.vox.com/future-perfect/21561606/peter-singer-life-you-can-save-effective-altruism), *Vox*, December 11.\n    \n2.  ^**[^](#fnrefn9e9vxkgo2)**^\n    \n    Tuna, Cari (2011) [Guest post from Cari Tuna](https://blog.givewell.org/2011/12/23/guest-post-from-cari-tuna/), *GiveWell*, December 23.\n    \n3.  ^**[^](#fnrefce4ujpvg0s)**^\n    \n    Preston, Caroline (2012) [Another Facebook co-founder gets philanthropic](https://www.philanthropy.com/article/another-facebook-co-founder-gets-philanthropic/), *The Chronicle of Philanthropy*, January 10.\n    \n4.  ^**[^](#fnrefiv2ex3sj5z)**^\n    \n    Gunther, Marc (2018) [Giving in the light of reason](https://ssir.org/articles/entry/giving_in_the_light_of_reason), *Stanford Social Innovation Review*."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "TAYEAQNNGTJ7pYZhN",
    "name": "Superintelligence (book)",
    "core": false,
    "slug": "superintelligence-book",
    "oldSlugs": [
      "superintelligence-paths-dangers-strategies"
    ],
    "postCount": 10,
    "description": {
      "markdown": "***Superintelligence: Paths, Dangers, Strategies*** is a book by [Nick Bostrom](https://forum.effectivealtruism.org/tag/nick-bostrom). It was published on 3 July 2014.\n\nFurther reading\n---------------\n\nBostrom, Nick (2014) [*Superintelligence: Paths, Dangers, Strategies*](https://en.wikipedia.org/wiki/Special:BookSources/9780199678112), Oxford: Oxford University Press.\n\nRelated entries\n---------------\n\n[Nick Bostrom](https://forum.effectivealtruism.org/tag/nick-bostrom) | [*Human Compatible*](https://forum.effectivealtruism.org/tag/human-compatible) | [superintelligence](https://forum.effectivealtruism.org/tag/superintelligence) | [*The Precipice*](https://forum.effectivealtruism.org/tag/the-precipice)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "CGjF5aeqYM45MQyPt",
    "name": "Suffering-focused ethics",
    "core": false,
    "slug": "suffering-focused-ethics",
    "oldSlugs": null,
    "postCount": 38,
    "description": {
      "markdown": "**Suffering-focused ethics** is a family of views in [normative ethics](https://forum.effectivealtruism.org/tag/normative-ethics) that assign primary moral importance to the alleviation of suffering. [Negative utilitarianism](https://forum.effectivealtruism.org/tag/negative-utilitarianism) is one example of a suffering-focused ethical view, though other views in this family also assign intrinsic value to things besides suffering, including positive hedonic states as well as non-hedonic states.\n\nFurther reading\n---------------\n\nAird, Michael (2020) [Collection of evidence about views on longtermism, time discounting, population ethics, significance of suffering vs happiness, etc. among non-EAs](https://forum.effectivealtruism.org/posts/EMKf4Gyee7BsY2RP8/michaela-s-shortform?commentId=GgW24uSGwTvwP7Hwr), *Effective Altruism Forum*, May 10.  \n*Many additional resources with some relevance to this topic.*\n\nGloor, Lukas (2016) [The case for suffering-focused ethics](https://longtermrisk.org/the-case-for-suffering-focused-ethics/), *Center on Long-Term Risk*, August 26.\n\nVinding, Magnus (2020) [*Suffering-Focused Ethics: Defense and Implications*](https://magnusvinding.com/2020/05/31/suffering-focused-ethics-defense-and-implications/)*,* Copenhagen: Ratio Ethica.\n\nRelated entries\n---------------\n\n[axiology](https://forum.effectivealtruism.org/tag/axiology) | [ethics of existential risk](https://forum.effectivealtruism.org/tag/ethics-of-existential-risk) | [s-risk](https://forum.effectivealtruism.org/tag/s-risk)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "yyBLt99t9gF3tLsTn",
    "name": "Leadership",
    "core": false,
    "slug": "leadership",
    "oldSlugs": null,
    "postCount": 2,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "W6vTruNLjHmF84WgJ",
    "name": "Steven Pinker",
    "core": false,
    "slug": "steven-pinker",
    "oldSlugs": null,
    "postCount": 3,
    "description": {
      "markdown": "**Steven Arthur Pinker** (born 18 September 1954) is a Canadian-American cognitive psychologist, linguist, and author.\n\nInvolvement with effective altruism\n-----------------------------------\n\nPinker has described effective altruism as \"one of the great new ideas of the 21st century\",^[\\[1\\]](#fnc4boa7orgq)^ and has written that \"movements that aim to spread scientific sophistication such as \\[…\\] effective altruism have a vast potential to enhance human welfare.\"^[\\[2\\]](#fnysltgo3su8)^ He has also collaborated with Harvard Effective Altruism and has expressed support for [Giving What We Can](https://forum.effectivealtruism.org/tag/giving-what-we-can).^[\\[3\\]](#fnhh50h2ab4e7)^\n\nFurther reading\n---------------\n\nTimalsina, Tarun (2021) [Effective altruism: An interview with Steven Pinker](https://harvardpolitics.com/effective-altruism-an-interview-with-steven-pinker/), *Harvard Political Review*.\n\nExternal links\n--------------\n\n[Steven Pinker](https://stevenpinker.com/). Official website.\n\n1.  ^**[^](#fnrefc4boa7orgq)**^\n    \n    MacAskill, William (2015) [*Doing Good Better: How Effective Altruism Can Help You Make a Difference*](https://en.wikipedia.org/wiki/Special:BookSources/978-1-59240-966-2), New York: Random House.\n    \n2.  ^**[^](#fnrefysltgo3su8)**^\n    \n    Pinker, Steven (2018) [*Enlightenment Now: The Case for Reason, Science, Humanism, and Progress*](https://en.wikipedia.org/wiki/Special:BookSources/978-0-525-42757-5), New York: Viking, ch. 22.\n    \n3.  ^**[^](#fnrefhh50h2ab4e7)**^\n    \n    Pinker, Steven (2016) [The simplest way to make a huge difference to the world: Join Giving What We Can](https://twitter.com/sapinker/status/781666786441826305), *Twitter*, September 30."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "hfaGG2i8gZu9PBLBp",
    "name": "Status quo bias",
    "core": false,
    "slug": "status-quo-bias",
    "oldSlugs": null,
    "postCount": 3,
    "description": {
      "markdown": "**Satus quo bias** is a [cognitive bias](https://forum.effectivealtruism.org/tag/cognitive-bias) that consists of giving undue preference to the existing state of affairs.\n\nFurther reading\n---------------\n\nKahneman, Daniel & Amos Tversky (eds.) (2000) [*Choices, Values, and Frames*](https://en.wikipedia.org/wiki/Special:BookSources/0-521-62749-4), Cambridge: Cambridge University Press.\n\nSamuelson, William & Richard Zeckhauser (1988) [Status quo bias in decision making](https://doi.org/10.1007/BF00055564), *Journal of Risk and Uncertainty*, vol. 1, pp. 7–59.\n\nRelated entries\n---------------\n\n[cognitive bias](https://forum.effectivealtruism.org/tag/cognitive-bias) | [reversal test](https://forum.effectivealtruism.org/tag/reversal-test)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "yPshgJDmMhXmjESiA",
    "name": "State vs. step risk",
    "core": false,
    "slug": "state-vs-step-risk",
    "oldSlugs": [
      "state-risk-and-step-risk"
    ],
    "postCount": null,
    "description": {
      "markdown": "A **state risk** is a risk associated with being in a particular state, whereas a **step risk** (also called a **transition risk**) is a risk arising from transitioning to a new state. [Nick Bostrom](https://forum.effectivealtruism.org/tag/nick-bostrom) appears to have originated the distinction in his book [*Superintelligence*](https://forum.effectivealtruism.org/tag/superintelligence-book), although some mentions of it predate the book's publication.^[\\[1\\]](#fnc3n6aotkfn)^\n\nThe cumulative state risk associated with being in some state grows as a function of the time spent in that state. [Natural existential risks](https://forum.effectivealtruism.org/tag/natural-existential-risk) are typically state risks. For example, absent deflection efforts, the risk that an [asteroids](https://forum.effectivealtruism.org/tag/asteroids) of a certain size collides with Earth by 2030 is higher than the risk that it does so by 2029. The longer humanity exposes itself to a state risk, the higher its probability of succumbing to the associated catastrophe. For this reason, the are *pro tanto* reasons for reducing state risks as soon as possible.\n\nThings are different with step risks. Here the threat arises only when the transition to the new state begins, and the overall risk incurred during this transition is not generally a function of its total duration. Thus, with step risks there is no presumption in favor or against either *postponing* or *prolonging* the transition; what is appropriate will vary depending on characteristics specific to each risk. Some [anthropogenic existential risks](https://forum.effectivealtruism.org/tag/anthropogenic-existential-risk) are plausibly viewed as step risks, with [AI risk](https://forum.effectivealtruism.org/tag/ai-risk) being perhaps the clearest example.\n\nSince state risks are correlated with natural existential risks, and step risks with anthropogenic existential risks, the latter's much greater share of [total existential risk](https://forum.effectivealtruism.org/tag/total-existential-risk) suggests that most of this risk is posed by transitioning to new states, rather than by remaining in a given state.^[\\[2\\]](#fncykji9ilky)^ This finding has important implications for the strategic management of existential risk.\n\nFurther reading\n---------------\n\nBostrom, Nick (2014) [*Superintelligence: Paths, Dangers, Strategies*](https://en.wikipedia.org/wiki/Special:BookSources/9780199678112), Oxford: Oxford University Press, ch. 14.\n\nOrd, Toby (2020) [*The Precipice: Existential Risk and the Future of Humanity*](https://en.wikipedia.org/wiki/Special:BookSources/1526600218), London: Bloomsbury Publishing, ch. 7.\n\nSandberg, Anders (2017) [Survivorship curves and existential risk](http://aleph.se/andart2/statistics/survivorship-curves-and-existential-risk/), *Andart II*, February 8.\n\nRelated entries\n---------------\n\n[anthropogenic existential risk](https://forum.effectivealtruism.org/tag/anthropogenic-existential-risk) | [existential risk](https://forum.effectivealtruism.org/tag/existential-risk) | [natural existential risk](https://forum.effectivealtruism.org/tag/natural-existential-risks) | [total existential risk](https://forum.effectivealtruism.org/tag/total-existential-risk)\n\n1.  ^**[^](#fnrefc3n6aotkfn)**^\n    \n    For example Beckstead, Nick (2013) [How to compare broad and targeted attempts to shape the far future](http://intelligence.org/wp-content/uploads/2013/07/Beckstead-Evaluating-Options-Using-Far-Future-Standards.pdf), July 13.\n    \n2.  ^**[^](#fnrefcykji9ilky)**^\n    \n    Ord, Toby (2020) [*The Precipice: Existential Risk and the Future of Humanity*](https://en.wikipedia.org/wiki/Special:BookSources/1526600218), London: Bloomsbury Publishing."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "2T3FbAK3Zxk5kJktF",
    "name": "Stanislav Petrov",
    "core": false,
    "slug": "stanislav-petrov",
    "oldSlugs": null,
    "postCount": 6,
    "description": {
      "markdown": "**Stanislav Yevgrafovich Petrov** (7 September 1939 – 19 May 2017) was a Soviet military officer.\n\nOn 26 September 1983, Petrov defied Soviet military protocol and classified reports by an early-warning system of an incoming missile strike from the United States as a false alarm. Because of this decision, which likely avoided a large-scale [nuclear war](https://forum.effectivealtruism.org/tag/nuclear-warfare-1) between the Soviet Union and the United States, Petrov is often referred to as \"the man who saved the world.\"^[\\[1\\]](#fnyy9c6larmz)^ His decision to report the incident as a false alarm has been described as \"the most important decision of all time.\"^[\\[2\\]](#fnv4fhvn4feqs)^\n\nRecognition\n-----------\n\nIn 2018, Petrov was posthumously awarded the [Future of Life Award](https://forum.effectivealtruism.org/tag/future-of-life-institute#Future_of_Life_Award).^[\\[3\\]](#fnh8pzictv1jn)^\n\nOne of the rooms in the [Future of Humanity Institute](https://forum.effectivealtruism.org/tag/future-of-humanity-institute) is named after Petrov.\n\nSeptember 26 is recognized as [Petrov Day](https://forum.effectivealtruism.org/tag/petrov-day) by many members of the [rationality](https://forum.effectivealtruism.org/tag/rationality-community) and [effective altruism](https://forum.effectivealtruism.org/tag/effective-altruism) communities.\n\nFurther reading\n---------------\n\nChan, Sewell (2017) [Stanislav Petrov, Soviet officer who helped avert nuclear war, is dead at 77](https://www.nytimes.com/2017/09/18/world/europe/stanislav-petrov-nuclear-war-dead.html), *The New York Times*, September 18.\n\nMatthews, Dylan (2018) [36 years ago today, one man saved us from world-ending nuclear war](https://www.vox.com/2018/9/26/17905796/nuclear-war-1983-stanislav-petrov-soviet-union), *Vox*, September 26.\n\nExternal links\n--------------\n\n[Bright Star Sound](https://www.brightstarsound.com/). A website dedicated to Stanislav Petrov.\n\nRelated entries\n---------------\n\n[nuclear warfare](https://forum.effectivealtruism.org/tag/nuclear-warfare-1) | [Petrov Day](https://forum.effectivealtruism.org/tag/petrov-day) | [Vasili Arkhipov](https://forum.effectivealtruism.org/tag/vasili-arkhipov)\n\n\\\\\\[\\\\\\]\n\n1.  ^**[^](#fnrefyy9c6larmz)**^\n    \n    Aksenov, Pavel (2013) [Stanislav Petrov: The man who may have saved the world](https://www.bbc.com/news/world-europe-24280831), *BBC News*, September 23.\n    \n2.  ^**[^](#fnrefv4fhvn4feqs)**^\n    \n    Morris, Ian (2014) [*War! What Is It Good for? Conflict and the Progress of Civilization from Primates to Robots*](https://en.wikipedia.org/wiki/Special:BookSources/978-0-374-28600-2), New York: Farrar, Straus and Giroux, p. 4.\n    \n3.  ^**[^](#fnrefh8pzictv1jn)**^\n    \n    Future of Life Institute (2018) [Future Of Life Award 2018](https://futureoflife.org/2018/09/26/future-of-life-award-2018/), *Future of Life Institute*, September 26."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "rrHWYw3NMJ9tmpSPi",
    "name": "Stanford Existential Risks Initiative",
    "core": false,
    "slug": "stanford-existential-risks-initiative",
    "oldSlugs": [
      "stanford-existential-risk-initiative",
      "stanford-existential-risks-initiative"
    ],
    "postCount": 23,
    "description": {
      "markdown": "**Stanford Existential Risks Initiative** (**SERI**) is a collaboration between faculty and students of Stanford University dedicated to mitigating [global catastrophic risks](https://forum.effectivealtruism.org/tag/global-catastrophic-risk).^[\\[1\\]](#fnbq88yj5f5ub)^ It was launched on 15 May 2020.^[\\[2\\]](#fnqc1jrnl0it)^\n\nSERI runs an undergraduate summer research program, various speaker events and discussions, and a class by Professors Stephen Luby and Paul Edwards on \"preventing human extinction\".^[\\[3\\]](#fn6z56wl5toeu)^ \n\nFunding\n-------\n\nAs of June 2022, SERI has received over $1.5 million in funding from [Open Philanthropy](https://forum.effectivealtruism.org/topics/open-philanthropy),^[\\[4\\]](#fn8dvkb3rbg0w)^ and $125,000 from the [Survival and Flourishing](https://forum.effectivealtruism.org/topics/survival-and-flourishing) Fund,^[\\[5\\]](#fn3xm29jmbq9l)^ and $60,000 from [Effective Altruism Funds](https://forum.effectivealtruism.org/topics/effective-altruism-funds).^[\\[6\\]](#fnyod65gee00p)^\n\nExternal links\n--------------\n\n[Stanford Existential Risks Initiative](https://cisac.fsi.stanford.edu/content/stanford-existential-risks-initiative). Official website.\n\n[Apply for a job](https://cisac.fsi.stanford.edu/content/employment). \n\nRelated entries\n---------------\n\n[Cambridge Existential Risks Initiative](https://forum.effectivealtruism.org/tag/cambridge-existential-risk-initiative) | [fellowships & internships](https://forum.effectivealtruism.org/tag/fellowships-and-internships) | [field building](https://forum.effectivealtruism.org/tag/field-building) | [research training programs](https://forum.effectivealtruism.org/tag/research-training-programs)\n\n1.  ^**[^](#fnrefbq88yj5f5ub)**^\n    \n    Stanford Existential Risks Initiative (2021) [Our mission](https://cisac.fsi.stanford.edu/content/stanford-existential-risks-initiative), *Stanford Existential Risks Initiative*.\n    \n2.  ^**[^](#fnrefqc1jrnl0it)**^\n    \n    Veit, Cooper (2020) [Stanford Existential Risk Initiative tackles global threats](https://www.stanforddaily.com/2020/05/20/stanford-existential-risk-initiative-tackles-global-threats/), *The Stanford Daily*, May 20.\n    \n3.  ^**[^](#fnref6z56wl5toeu)**^\n    \n    Luby, Stephen & Paul Edwards (2020) [Preventing human extinction](https://explorecourses.stanford.edu/search?view=catalog&filter-coursestatus-Active=on&page=0&catalog=&q=THINK65), Course syllabus, Stanford University.\n    \n4.  ^**[^](#fnref8dvkb3rbg0w)**^\n    \n    Open Philanthropy (2022) [Grants database: Stanford Existential Risks Initiative](https://www.openphilanthropy.org/grants/?q=&organization-name=stanford-existential-risk-initiative&organization-name=stanford-existential-risks-initiative), *Open Philanthropy*.\n    \n5.  ^**[^](#fnref3xm29jmbq9l)**^\n    \n    Survival and Flourishing Fund (2019) [SFF-2020-H2 S-process recommendations announcement](https://survivalandflourishing.fund/sff-2020-h2-recommendations), *Survival and Flourishing Fund*.\n    \n6.  ^**[^](#fnrefyod65gee00p)**^\n    \n    Long-Term Future Fund (2021) [May 2021: Long-Term Future Fund grants](https://funds.effectivealtruism.org/funds/payouts/may-2021-long-term-future-fund-grants), *Effective Altruism Funds*, May."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "TAntDdocSCFKkoBQ2",
    "name": "Speed superintelligence",
    "core": false,
    "slug": "speed-superintelligence",
    "oldSlugs": null,
    "postCount": null,
    "description": {
      "markdown": "A **speed superintelligence** is a system that can do what a typical human mind can do, but much faster.\n\nFurther reading\n---------------\n\nBostrom, Nick (2014) [*Superintelligence: Paths, Dangers, Strategies*](https://en.wikipedia.org/wiki/Special:BookSources/9780199678112), Oxford: Oxford University Press, pp. 53–54.\n\nRelated entries\n---------------\n\n[collective superintelligence](https://forum.effectivealtruism.org/tag/collective-superintelligence) | [quality superintelligence](https://forum.effectivealtruism.org/tag/quality-superintelligence) | [superintelligence](https://forum.effectivealtruism.org/tag/superintelligence)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "9Duj2vKNfSnv76ubX",
    "name": "Speciesism",
    "core": false,
    "slug": "speciesism",
    "oldSlugs": null,
    "postCount": 10,
    "description": {
      "markdown": "**Speciesism** is discrimination in favor of one species, typically the human species, over others, typically in the context of exploitation or mistreatment of nonhuman animals.\n\nFurther reading\n---------------\n\nAnimal Ethics (2017) [Speciesism](https://web.archive.org/web/20171016221402/https://www.animal-ethics.org/ethics-animals-section/speciesism/), *Animal Ethics*, September.\n\nMacAskill, Will & Darius Meissner (2020) [Anti-speciesism: Expanding the moral circle across species](https://www.utilitarianism.net/utilitarianism-and-practical-ethics#speciesism), in 'Utilitarianism and practical ethics', *An Introduction to Utilitarianism* (updated 2022).\n\nSinger, Peter (2002) [All animals are equal...](https://en.wikipedia.org/wiki/Special:BookSources?isbn=0-06-001157-2), in *Animal Liberation*, New York: HarperCollins, pp. 1-23.\n\nRelated entries\n---------------\n\n[animal welfare](https://forum.effectivealtruism.org/topics/animal-welfare-1) | [cognitive bias](https://forum.effectivealtruism.org/tag/cognitive-bias) | [moral circle expansion](https://forum.effectivealtruism.org/tag/moral-circle-expansion-1) | [moral patienthood](https://forum.effectivealtruism.org/tag/moral-patient) | [welfare biology](https://forum.effectivealtruism.org/topics/welfare-biology)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "x8mstmr3Picyx42ed",
    "name": "Space governance",
    "core": false,
    "slug": "space-governance",
    "oldSlugs": null,
    "postCount": 11,
    "description": {
      "markdown": "**Space governance** (also called **space law**) is the set of rules, principles and [standards](https://forum.effectivealtruism.org/topics/standards-and-regulation) governing activities related to outer space.\n\nEvaluation\n----------\n\n[80,000 Hours](https://forum.effectivealtruism.org/tag/80-000-hours) rates space governance a \"potential highest priority area\": an issue that, if more thoroughly examined, could rank as a top global challenge.^[\\[1\\]](#fnhe0yw9e4j5p)^\n\nFurther reading\n---------------\n\nBaumann, Tobias (2020) [Space governance is important, tractable and neglected](https://forum.effectivealtruism.org/posts/QkRq6aRA84vv4xsu9/space-governance-is-important-tractable-and-neglected), *Effective Altruism Forum*, January 7.\n\nMoorhouse, Fin (2022) [Space governance](https://80000hours.org/problem-profiles/space-governance/), *80,000 Hours*, February 14.\n\nRajagopalan, Rajeswari Pillai (2018) [Space governance](http://doi.org/10.1093/acrefore/9780190647926.013.107), *Oxford Research Encyclopedia of Planetary Science*, August 28.\n\nRelated entries\n---------------\n\n[space colonization](https://forum.effectivealtruism.org/tag/space-colonization) | [global governance](https://forum.effectivealtruism.org/tag/global-governance) | [law](https://forum.effectivealtruism.org/tag/law) | [standards and regulation](https://forum.effectivealtruism.org/topics/standards-and-regulation)\n\n1.  ^**[^](#fnrefhe0yw9e4j5p)**^\n    \n    80,000 Hours (2022) [Our current list of pressing world problems](https://80000hours.org/problem-profiles/), *80,000 Hours*."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "4u8AJBtxz9zPpLnRr",
    "name": "Space colonization",
    "core": false,
    "slug": "space-colonization",
    "oldSlugs": null,
    "postCount": 34,
    "description": {
      "markdown": "**Space colonization** is the establishment of self-sufficient human settlements outside Earth.  **Interstellar colonization** is the establishment of settlements outside of the solar system.\n\nThe feasibility of interstellar colonization has substantial implications for [the long-term future](https://forum.effectivealtruism.org/tag/longtermism).\n\nIf it is feasible, then this would raise the [upper bound](https://forum.effectivealtruism.org/tag/universe-s-resources) on the number of people who could eventually live by many orders of magnitude. On the other hand, given the [Fermi paradox](https://forum.effectivealtruism.org/tag/fermi-paradox), its feasibility could be taken as evidence that humans are likely to go prematurely [extinct](https://forum.effectivealtruism.org/tag/human-extinction).\n\nThere are a number of difficulties that would need to be surmounted before humans, or any other intelligent species, could begin to colonize other star systems. For instance, there would be very large energy requirements, the ability to manage extremely long (possibly intergenerational) flight lengths, and many distinct [engineering](https://forum.effectivealtruism.org/topics/engineering) challenges, such as the need to safeguard against high-velocity collisions with space dust.\n\nAlthough there is not yet a substantial body of literature addressing the question, the small number of scientists who have examined interstellar colonization appear to be on the whole optimistic about its long-term feasibility.\n\nFurther reading\n---------------\n\nBeckstead, Nick (2014) [Will we eventually be able to colonize other stars? Notes from a preliminary review](http://globalprioritiesproject.org/2014/06/will-we-eventually-be-able-to-colonize-other-stars-notes-from-a-preliminary-review/), *Global Priorities Project*, June 22.  \n*A summary of the small academic literature on interstellar colonization.*\n\nKovic, Marko (2021) [Risks of space colonization](https://doi.org/10.1016/j.futures.2020.102638), *Futures*, vol. 126.\n\nMathieu, Edouard & Max Roser (2022) [Space exploration and satellites](https://ourworldindata.org/space-exploration-satellites), *Our World in Data*, June 14.\n\nShulman, Carl (2020) [The High Frontier, space based solar power, and space manufacturing](http://reflectivedisequilibrium.blogspot.com/2020/05/book-review-high-frontier-space-based.html), *Reflective disequilibrium*, May 23.\n\nRelated entries\n---------------\n\n[Fermi paradox](https://forum.effectivealtruism.org/tag/fermi-paradox) | [flourishing futures](https://forum.effectivealtruism.org/tag/flourishing-futures) | [Great Filter](https://forum.effectivealtruism.org/tag/great-filter) | [long-term future](https://forum.effectivealtruism.org/tag/long-term-future) | [nonhumans and the long-term future](https://forum.effectivealtruism.org/tag/non-humans-and-the-long-term-future) | [space governance](https://forum.effectivealtruism.org/tag/space-governance) | [universe's resources](https://forum.effectivealtruism.org/tag/universe-s-resources)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "dWNTJhiFHyNug5arF",
    "name": "Sovereign AI",
    "core": false,
    "slug": "sovereign-ai",
    "oldSlugs": null,
    "postCount": 2,
    "description": {
      "markdown": "A **sovereign AI** is an [artificial intelligence](https://forum.effectivealtruism.org/tag/artificial-intelligence) that acts autonomously in the world, in pursuit of potentially long-range objectives.\n\nFurther reading\n---------------\n\nBostrom, Nick (2014) [*Superintelligence: Paths, Dangers, Strategies*](https://en.wikipedia.org/wiki/Special:BookSources/9780199678112), Oxford: Oxford University Press, pp. 148-150.\n\nRelated entries\n---------------\n\n[oracle AI](https://forum.effectivealtruism.org/tag/oracle-ai) | [tool AI](https://forum.effectivealtruism.org/tag/tool-ai)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "CW63CzA4LwnCiFnge",
    "name": "Society for the Diffusion of Useful Knowledge",
    "core": false,
    "slug": "society-for-the-diffusion-of-useful-knowledge",
    "oldSlugs": null,
    "postCount": 1,
    "description": {
      "markdown": "The **Society for the Diffusion of Useful Knowledge** (**SDUK**) was an educational society established in 1826 that published a series of inexpensive books on scientific and technical subjects.\n\nThe goal of the SDUK, which has been characterized as \"a Benthamite-oriented group\",^[\\[1\\]](#fnhvtd4sq9kjo)^ was to \"improv\\[e\\] workers' efficiency and ad\\[d\\] to the nation's pool of inventors and engineers.\"^[\\[2\\]](#fncjix1c7ajvk)^ In a letter to [John Stuart Mill](https://forum.effectivealtruism.org/tag/john-stuart-mill), Thomas Carlyle—famous for his criticisms of [democracy](https://forum.effectivealtruism.org/tag/democracy), [economics](https://forum.effectivealtruism.org/tag/economics), and [utilitarianism](https://forum.effectivealtruism.org/tag/utilitarianism)—derided the Society's \"triumphant quackle-quackling intent only on sine and cosine.\"^[\\[3\\]](#fnv1inmjp0xsf)^\n\nFurther reading\n---------------\n\nAshton, Rosemary (2008) [Society for the Diffusion of Useful Knowledge (act. 1826–1846)](https://doi.org/10.1093/ref:odnb/59807), *Oxford Dictionary of National Biography*, May 24.\n\nGrobel, Monica Christina (1933) *The Society for the Diffusion of Useful Knowledge, 1826-1846*, MA thesis, University of London.\n\nHamzo, George & James E. Crimmins (2013) [Society for the Diffusion of Useful Knowledge](https://en.wikipedia.org/wiki/Special:BookSources/9781350021662), in James E. Crimmins (ed.) *The Bloomsbury Encyclopedia of Utilitarianism*, London: Bloomsbury Academic, pp. 527–528.\n\nRelated entries\n---------------\n\n[communities adjacent to effective altruism](https://forum.effectivealtruism.org/tag/communities-adjacent-to-effective-altruism)  \n \n\n1.  ^**[^](#fnrefhvtd4sq9kjo)**^\n    \n    Fuller, Margaret (1991) [*‘These Sad but Glorious Days’: Dispatches from Europe, 1846-1850*](https://en.wikipedia.org/wiki/Special:BookSources/0-300-05038-0), edited by Larry J. Reynolds & Susan Belasco Smith, New Haven: Yale University Press, p. 41, fn. 5\n    \n2.  ^**[^](#fnrefcjix1c7ajvk)**^\n    \n    Altick, Richard D. (1973) [*Victorian People and Ideas: A Companion for the Modern Reader of Victorian Literature*](https://en.wikipedia.org/wiki/Special:BookSources/9780393093766), New York: W. W. Norton & Company, p. 257.\n    \n3.  ^**[^](#fnrefv1inmjp0xsf)**^\n    \n    Carlyle, Thomas (1835) [Letter to John Stuart Mill: 30 October 1835](https://doi.org/10.1215/lt-18351030-TC-JSM-01), *The Carlyle Letters Online*."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "DcHjxcAjS347jZEwX",
    "name": "Viktor Zhdanov",
    "core": false,
    "slug": "viktor-zhdanov",
    "oldSlugs": null,
    "postCount": 1,
    "description": {
      "markdown": "**Viktor Mikhailovich Zhdanov** (13 February 1914 – 14 July 1987) was a [Ukrainian](https://forum.effectivealtruism.org/tag/ukraine) virologist who played a key role in the [Smallpox Eradication Programme](https://forum.effectivealtruism.org/tag/smallpox-eradication-programme). Some people in the [effective altruism](https://forum.effectivealtruism.org/tag/effective-altruism) community, including Gordon Irlam and [William MacAskill](https://forum.effectivealtruism.org/tag/william-macaskill), have argued that Zhdanov has done more good for humanity than any other individual in history.^[\\[1\\]](#fnwml6n5c9bhm)^^[\\[2\\]](#fnlcpj5nxbmf)^\n\nIn 2020, Zhdanov and William Foege were awarded the [Future of Life Award](https://forum.effectivealtruism.org/tag/future-of-life-institute#Future_of_Life_Award).^[\\[3\\]](#fnq9j49gzdsl8)^\n\nFurther reading\n---------------\n\nBukrinskaya, Alice (1991) [In memory of Victor Zhdanov](https://doi.org/10.1007/BF01316759), *Archives of Virology*, vol. 121, pp. 237–240.\n\nDattani, Saloni (2020) [The story of Viktor Zhdanov](https://www.worksinprogress.co/the-story-of-viktor-zhdanov/), *Works in Progress*, August 28.\n\nIrlam, Gordon (2012) [In praise of Viktor Zhdanov](https://80000hours.org/2012/02/in-praise-of-viktor-zhdanov/), *80,000 Hours*, February 23.\n\nRelated entries\n---------------\n\n[Smallpox Eradication Programme](https://forum.effectivealtruism.org/tag/smallpox-eradication-programme)\n\n1.  ^**[^](#fnrefwml6n5c9bhm)**^\n    \n    Irlam, Gordon (2012) [In praise of Viktor Zhdanov](https://80000hours.org/2012/02/in-praise-of-viktor-zhdanov/), *80,000 Hours*, February 23.\n    \n2.  ^**[^](#fnreflcpj5nxbmf)**^\n    \n    Macaskill, William (2015) [The best person who ever lived is an unknown Ukrainian man](https://boingboing.net/2015/07/30/the-best-person-who-ever-lived.html), *Boing Boing*, July 30.\n    \n3.  ^**[^](#fnrefq9j49gzdsl8)**^\n    \n    Future of Life Institute (2020) [Future of Life Award 2020](https://futureoflife.org/2020/11/16/future-of-life-award-2020/), *Future of Life Institute*, November 16."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "jfHserdBxrGxW2ScR",
    "name": "Smallpox Eradication Programme",
    "core": false,
    "slug": "smallpox-eradication-programme",
    "oldSlugs": null,
    "postCount": 1,
    "description": {
      "markdown": "The **Smallpox Eradication Programme** was a program by the [World Health Organization](https://forum.effectivealtruism.org/topics/world-health-organization) to eradicate smallpox worldwide. The program was created at the instigation of [Viktor Zhdanov](https://forum.effectivealtruism.org/tag/viktor-zhdanov), a Soviet virologist, and ran between 1966 and 1980, when the eradication of smallpox was officially certified.\n\nRelated entries\n---------------\n\n[Viktor Zhdanov](https://forum.effectivealtruism.org/tag/viktor-zhdanov)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "sSXSfvfRsGykDZS4z",
    "name": "Singleton",
    "core": false,
    "slug": "singleton",
    "oldSlugs": null,
    "postCount": 3,
    "description": {
      "markdown": "A **singleton** is a world order in which there is a single decision-making agency at the highest level. The **singleton hypothesis** is the hypothesis that Earth-originating intelligent life will eventually assume the form a singleton.\n\nThe concept of a singleton expresses an abstract idea. A singleton could take the form of [world government](https://forum.effectivealtruism.org/topics/global-governance), such as global [democracy](https://forum.effectivealtruism.org/topics/democracy) or global [totalitarianism](https://forum.effectivealtruism.org/topics/totalitarianism), but this need not be so. What is required for humanity to constitute a singleton is for it to behave roughly like a coherent agent.^[\\[1\\]](#fn57v5nrv70yb)^^[\\[2\\]](#fnpukudumsrha)^\n\nFurther reading\n---------------\n\nBostrom, Nick (2004) [The future of human evolution](https://en.wikipedia.org/wiki/Special:BookSources/9780974347226), in Charles Tandy (ed.) *Death and Anti-Death: Two Hundred Years after Kant, Fifty Years after Turing*, vol. 2, Palo Alto, California: Ria University Press, pp. 339–371.\n\nBostrom, Nick (2006) [What is a singleton?](https://addletonacademicpublishers.com/105-linguistic-and-philosophical-investigations/volume-5-2-2006/234-what-is-a-singleton), *Linguistic and Philosophical Investigations*, vol. 5, pp. 48–54.\n\nBostrom, Nick (2014) [*Superintelligence: Paths, Dangers, Strategies*](https://en.wikipedia.org/wiki/Special:BookSources/978-0-19-967811-2), Oxford: Oxford University Press, pp. 104–107.\n\nRelated entries\n---------------\n\n[global governance](https://forum.effectivealtruism.org/tag/global-governance) | [macrostrategy](https://forum.effectivealtruism.org/tag/macrostrategy)\n\n1.  ^**[^](#fnref57v5nrv70yb)**^\n    \n    Bostrom, Nick (2014) [*Superintelligence: Paths, Dangers, Strategies*](https://en.wikipedia.org/wiki/Special:BookSources/978-0-19-967811-2), Oxford: Oxford University Press, p. 83. \n    \n2.  ^**[^](#fnrefpukudumsrha)**^\n    \n    Ord Toby (2020) [*The Precipice: Existential Risk and the Future of Humanity*](https://en.wikipedia.org/wiki/Special:BookSources/1-5266-0021-8), London: Bloomsbury Publishing, p. 397."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "xzv2Ei9pawZmAPcWu",
    "name": "Simulation argument",
    "core": false,
    "slug": "simulation-argument",
    "oldSlugs": null,
    "postCount": 8,
    "description": {
      "markdown": "The **simulation argument** is an argument for the conclusion that, if humanity reaches a stage where it can run sufficiently realistic simulations of its history and decides to run them, we are almost certainly living in one such simulation.\n\nSome philosophers and scientists have argued that it may be possible for sufficiently advanced computer simulations of people to have [subjective experiences](https://forum.effectivealtruism.org/tag/sentience-1), just as flesh-and-blood people do. In particular, it might be the case that the experiences of simulated people are so realistic that they are subjectively indistinguishable from those of flesh-and-blood human beings. If so, it wouldn't be possible to tell, from the inside, whether one is a real or a simulated being.\n\nThis argument has led some philosophers and scientists to ask whether we could in fact be merely simulated. If so, [our future](https://forum.effectivealtruism.org/tag/longtermism) could ultimately be cut short, if the simulation is ever halted.\n\nAt least two assumptions are necessary for the hypothesis that we are in a simulation to be possible. First, it must be assumed that simulations would indeed be capable of having subjective experiences. Second, it must be assumed that computers that are powerful enough to run such simulations are technically feasible.\n\nProceeding from these assumptions, [Nick Bostrom](https://forum.effectivealtruism.org/topics/nick-bostrom) has argued that one of the following three hypotheses must be true:^[\\[1\\]](#fn5kxgcfupaav)^\n\n1.  The human species is very likely to go extinct before reaching a “posthuman” stage\n2.  Any posthuman civilization is extremely unlikely to run a significant number of simulations of their evolutionary history (or variations thereof)\n3.  We are almost certainly living in a computer simulation\n\nThe core of Bostrom’s argument is that, if simulations are run, then in the long run simulated people will vastly outnumber flesh-and-blood people. He notes, however, that this notion is not necessarily airtight. If the universe is [infinite](https://forum.effectivealtruism.org/tag/infinite-ethics), then statements about the fraction of people who are simulated may be meaningless.\n\nExternal links\n--------------\n\n[The Simulation Argument](https://www.simulation-argument.com/). Website dedicated to the argument.\n\n1.  ^**[^](#fnref5kxgcfupaav)**^\n    \n    Bostrom, Nick (2003) [Are we living in a computer simulation?](http://doi.org/10.1111/1467-9213.00309) *The philosophical quarterly*, vol. 53, pp. 243–255."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "o6TStp5ZW5aJBHQZC",
    "name": "Sentience",
    "core": false,
    "slug": "sentience-1",
    "oldSlugs": null,
    "postCount": 6,
    "description": {
      "markdown": "**Sentience** is the capacity to feel, or have conscious experience. (Sometimes the term is used more narrowly to refer to the capacity to feel pleasure or pain.) It is generally accepted that possessing this capacity is a necessary condition for counting as a [moral patient](https://forum.effectivealtruism.org/tag/moral-patienthood).\n\nViews about the distribution of sentience\n-----------------------------------------\n\nPhilosophers and scientists discuss three broad hypotheses on what entities are sentient.\n\nFirst, it is possible that only humans are sentient. This is currently an uncommon view, although it has a long history. For instance, the 17th-century philosopher René Descartes put forward influential arguments to the effect that animals lack internal experience, and until several decades ago animal experimenters and veterinarians were taught to disregard apparent pain responses.\n\nSecond, it is possible that only sufficiently advanced nonhuman animals are sentient (see [animal sentience](https://forum.effectivealtruism.org/topics/animal-sentience)). This is the most common view: that other creatures such as chimpanzees, dogs, and pigs also have internal experiences, but that there is some cut-off point beyond which species such as clams, jellyfish, and sea-sponges lie. A conservative cut-off of this sort might include only primates, and a liberal cut-off might go so far as to include insects.\n\nThird, it is possible that beings other than human and nonhuman animals are sentient (see [artificial sentience](https://forum.effectivealtruism.org/topics/artificial-sentience)). Some philosophers argue that sufficiently advanced [artificial intelligence](https://forum.effectivealtruism.org/tag/artificial-intelligence) would be capable of experiencing these feelings, or that sufficiently detailed computer simulations of people would have the same experiences that flesh-and-blood people do.^[\\[1\\]](#fnzyg7zrfi14)^ Also, it is not necessarily inconceivable that plants, relatively simple machines, or even fundamental physical processes, can experience pleasure or pain, although there are very few proponents of these views.\n\nFurther reading\n---------------\n\nWiblin, Robert, Arden Koehler & Keiran Harris (2019) [David Chalmers on the nature and ethics of consciousness](https://80000hours.org/podcast/episodes/david-chalmers-nature-ethics-consciousness/), *80,000 Hours*, December 16.\n\nRelated entries\n---------------\n\n[animal sentience](https://forum.effectivealtruism.org/topics/animal-sentience) | [artificial sentience](https://forum.effectivealtruism.org/topics/artificial-sentience) | [moral patienthood](https://forum.effectivealtruism.org/topics/moral-patienthood) | [pain and suffering](https://forum.effectivealtruism.org/tag/pain-and-suffering)\n\n1.  ^**[^](#fnrefzyg7zrfi14)**^\n    \n    Tomasik, Brian (2009) [Do bugs feel pain?](http://reducing-suffering.org/do-bugs-feel-pain), *Essays on Reducing Suffering*, April 7."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "JxpDgT6g94K7zEpcp",
    "name": "SCI Foundation",
    "core": false,
    "slug": "sci-foundation",
    "oldSlugs": null,
    "postCount": 4,
    "description": {
      "markdown": "The **SCI Foundation** (**SCI**), formerly known as the **Schistosomiasis Control Initiative**, is a charity that works with governments in sub-Saharan Africa to create or scale up [deworming](https://forum.effectivealtruism.org/tag/deworming) programs.\n\nHistory\n-------\n\nSCI was founded in 2002 as an initiative within Imperial College London with a £20 million grant from the [Bill & Melinda Gates Foundation](https://forum.effectivealtruism.org/tag/bill-and-melinda-gates-foundation).^[\\[1\\]](#fndukpp758ou9)^ In 2019, SCI became an independent organization, adopting its current name.^[\\[1\\]](#fndukpp758ou9)^\n\nEvaluation\n----------\n\nSCI was a [GiveWell](https://forum.effectivealtruism.org/tag/givewell) top-rated charity between 2011 and 2022.^[\\[2\\]](#fnezrezp4z8z4)^ ^[\\[3\\]](#fn90b4j9hx2k7)^ GiveWell estimates that SCI can deworm a child at a cost of around $1.^[\\[4\\]](#fnfxkn4ykprpf)^^[\\[5\\]](#fn5zvyazkix9x)^\n\nAs of July 2022, SCI Foundation has received over $27.1 million in funding from [Open Philanthropy](https://forum.effectivealtruism.org/tag/open-philanthropy),^[\\[6\\]](#fn3fhg0bkkbcq)^ and $1.5 million from [Effective Altruism Funds](https://forum.effectivealtruism.org/topics/effective-altruism-funds).^[\\[7\\]](#fnmjx5zgh6yea)^\n\n[The Life You Can Save](https://forum.effectivealtruism.org/topics/the-life-you-can-save) features SCI in their list of \"best charities\".^[\\[8\\]](#fn08b3od06lgkw)^\n\nFurther reading\n---------------\n\nGiveWell (2021) [All content on SCI Foundation](https://www.givewell.org/charities/sci-foundation/all-content), *GiveWell*, November.\n\nSánchez, Sebastián *et al.* (2019) [Timeline of Schistosomiasis Control Initiative](https://timelines.issarice.com/wiki/Timeline_of_Schistosomiasis_Control_Initiative), *Timelines Wiki*, March 9 (updated 16 April 2021‎).\n\nExternal links\n--------------\n\n[SCI Foundation](https://schistosomiasiscontrolinitiative.org/). Official website.\n\n[Apply for a job](https://schistosomiasiscontrolinitiative.org/jobs).\n\n[Donate to the SCI Foundation](https://schistosomiasiscontrolinitiative.org/make-a-donation).\n\n1.  ^**[^](#fnrefdukpp758ou9)**^\n    \n    SCI Foundation (2020) [Our history](https://schistosomiasiscontrolinitiative.org/about/our-history), *SCI Foundation*.\n    \n2.  ^**[^](#fnrefezrezp4z8z4)**^\n    \n    GiveWell (2021) [2021 GiveWell cost-effectiveness analysis — version 3](https://docs.google.com/spreadsheets/d/1B1fODKVbnGP4fejsZCVNvBm5zvI1jC7DhkaJpFk6zfo/edit#gid=1377543212), *GiveWell*, July 6.\n    \n3.  ^**[^](#fnref90b4j9hx2k7)**^\n    \n    For the year 2019, GiveWell maintained their recommendation, but noted that while \"SCI offers donors an outstanding opportunity to accomplish good with their donations \\[...\\], at this time, it does not have capacity to use new donations effectively.\"^[\\[9\\]](#fnlsebe3b6tkh)^ This notice was removed in 2020.\n    \n4.  ^**[^](#fnreffxkn4ykprpf)**^\n    \n    GiveWell (2021) [SCI Foundation](https://www.givewell.org/charities/sci-foundation), *GiveWell*, November.\n    \n5.  ^**[^](#fnref5zvyazkix9x)**^\n    \n    GiveWell (2021) [Our top charities](https://www.givewell.org/charities/top-charities), *GiveWell*, November.\n    \n6.  ^**[^](#fnref3fhg0bkkbcq)**^\n    \n    Open Philanthropy (2022) [Grants database: SCI Foundation - Schistosomiasis Control Initiative](https://www.openphilanthropy.org/grants/?q=&organization-name=schistosomiasis-control-initiative&organization-name=sci-foundation), *Open Philanthropy*.\n    \n7.  ^**[^](#fnrefmjx5zgh6yea)**^\n    \n    Global Health and Development Fund (2018) [April 2018: Schistosomiasis Control Initiative](https://funds.effectivealtruism.org/funds/payouts/april-2018-schistosomiasis-control-initiative), *Effective Altruism Funds*, April.\n    \n8.  ^**[^](#fnref08b3od06lgkw)**^\n    \n    The Life You Can Save (2021) [Best charities](https://www.thelifeyoucansave.org/best-charities/), *The Life You Can Save*.\n    \n9.  ^**[^](#fnreflsebe3b6tkh)**^\n    \n    GiveWell (2019) [Our top charities](https://web.archive.org/web/20191230235836/https://www.givewell.org/charities/top-charities), *GiveWell*, November."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "QnZTmv325TTgZuHnn",
    "name": "Scared Straight",
    "core": false,
    "slug": "scared-straight",
    "oldSlugs": null,
    "postCount": null,
    "description": {
      "markdown": "**Scared Straight** was an awareness program involving frequent visits to adult prison facilities by at-risk youth and juvenile delinquents. Despite its popularity and reliance on an intuitively plausible mechanism of action, the program was shown in numerous studies to be not just ineffective but actively [harmful](https://forum.effectivealtruism.org/tag/accidental-harm), making young people more likely to commit crimes in later life.\n\nAccording to one meta-analysis, each dollar spent on the Scared Straight program had a net social cost of over $200.^[\\[1\\]](#fnv0shi7ijk6)^^[\\[2\\]](#fntlyfdmmgb5j)^\n\nRelated entries\n---------------\n\n[PlayPump](https://forum.effectivealtruism.org/tag/playpump)\n\n1.  ^**[^](#fnrefv0shi7ijk6)**^\n    \n    Aos, Steve *et al.* (2004) [Benefits and costs of prevention programs for youth](http://www.wsipp.wa.gov/ReportFile/881), Olympia, Washington: Washington State Institute for Public Policy, p. 7.\n    \n2.  ^**[^](#fnreftlyfdmmgb5j)**^\n    \n    Aos, Steve *et al.* (2004) [Benefits and costs of prevention programs for youth: technical appendix](http://www.wsipp.wa.gov/ReportFile/882), Olympia, Washington: Washington State Institute for Public Policy, p. 112."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "ns9DfYvtox2pxEFxD",
    "name": "Scott Alexander",
    "core": false,
    "slug": "scott-alexander",
    "oldSlugs": null,
    "postCount": 18,
    "description": {
      "markdown": "**Scott Alexander** (born 1984) is a pseudonymous American psychiatrist and blogger. Alexander is the author of the blogs Slate Star Codex (SSC) and its successor Astral Codex Ten (ACX), and a contributor—originally under the user name **Yvain**—to the community blog [LessWrong](https://forum.effectivealtruism.org/tag/lesswrong).\n\nBackground\n----------\n\nAlexander studied philosophy as an undergraduate, and in 2012 graduated from University College Cork School of Medicine, Ireland. He did his residency at St. Mary Mercy Hospital in Michigan, United States. He specializes in treatment-resistant depression and his areas of interest within psychiatry include chronotherapy, behavioral genetics, and the ontology of psychiatric disorders. One of Alexander's blog posts^[\\[1\\]](#fnbp7vt5jghu5)^ was subsequently revised and published in *Pharmacology Research & Perspectives.*^[\\[2\\]](#fnsdctli4eu7l)^\n\nBefore establishing his own practice, Lorien Psychiatry, Alexander worked at LifeStance Health (formerly Pacific Coast Psychiatric Associates) and served as Senior Health Researcher of MetaMed, a medical consulting company.^[\\[3\\]](#fnu430tt23cce)^^[\\[4\\]](#fngrktaufo6i)^^[\\[5\\]](#fnpsghbo11shi)^ Since 2021, Alexander has been on the advisor board of the [Qualia Research Institute](https://forum.effectivealtruism.org/tag/qualia-research-institute).\n\nBlogging career\n---------------\n\nSSC launched in early 2013. Except for a missing letter \"n\", the name is a perfect anagram of Alexander's name. (The blog's header displayed an image of the missing letter \"to restore cosmic balance\"^[\\[6\\]](#fn9qk7vdxuo1t)^)  SSC was shut down in mid-2020, after a *New York Times* journalist ignored Alexander's request not to reveal his true identity.^[\\[7\\]](#fn9zp125fel6o)^^[\\[8\\]](#fnxpmreehr2dj)^\n\nACX launched in early 2021 - its name is a perfect anagram of \"Scott Alexander\". It is hosted on Substack, an online publishing platform. Although readers can opt for a paid subscription, Alexander notes that \"\\[a\\]ll important ACX content is and always will be free.\"^[\\[9\\]](#fn53y92wfwieq)^\n\nIn November 2021, Alexander announced *ACX Grants*, an initiative to award a total of $250,000 in small grants to promising projects, with a minimum of paperwork.^[\\[10\\]](#fn7rtjovtpln5)^ The budget grew to $1.3 million after several outside funders contributed to the initiative. Alexander announced the results in late December. Out of 656 submissions, 38 projects received funded; the median grant was $40,000.^[\\[11\\]](#fndd0p6x77ie7)^\n\nInfluence and reception\n-----------------------\n\nAlexander's writings, particularly his SSC posts and some of his earlier LessWrong posts, have been highly influential within the [rationality](https://forum.effectivealtruism.org/tag/rationality-community) and effective altruism communities. He has received praise from many prominent figures, including [Bryan Caplan](https://forum.effectivealtruism.org/tag/bryan-caplan),^[\\[12\\]](#fnn1wsl523t0h)^[ Tyler Cowen](https://forum.effectivealtruism.org/tag/tyler-cowen),^[\\[13\\]](#fnjlqbu47huy)^[ Steven Pinker](https://forum.effectivealtruism.org/tag/steven-pinker),^[\\[14\\]](#fn7o50nx66pmv)^ and others.^[\\[15\\]](#fn1wb8xj121h7)^^[\\[16\\]](#fn8c76tp4hzs9)^^[\\[17\\]](#fnwxup5971ug)^^[\\[18\\]](#fnxu2xjp1w3bs)^\n\nFurther reading\n---------------\n\nBensinger, Rob (2015) [The Library of Scott Alexandria](https://www.lesswrong.com/posts/vwqLfDfsHmiavFAGP/the-library-of-scott-alexandria), *LessWrong*, September 13.  \n*A selection of Alexander's writings from SSC, LessWrong, and LiveJournal.*\n\nCrawford, Jason (2021) [Who is Scott Alexander and what is he about?](https://jasoncrawford.org/guide-to-scott-alexander-and-slate-star-codex), *Jason Crawford’s Blog*, February 13.  \n*A selection of Alexander's writings from SSC.*\n\nNgo, Richard (2020) [EA reading list: Scott Alexander](https://forum.effectivealtruism.org/s/NKTk9s4tZPiA4aySj/p/n2CFotW9eQmSJoGTy), *Effective Altruism Forum*, August 3.\n\nExternal links\n--------------\n\n[Astral Codex Ten](https://astralcodexten.substack.com/). Alexander's current blog.\n\n[Slate Star Codex](https://slatestarcodex.com/). Alexander's previous blog.\n\n[LiveJournal blog](https://archive.fo/fCFQx). A complete archive of Alexander's early blog.\n\n[Scott Alexander](https://forum.effectivealtruism.org/users/scott-alexander). [Effective Altruism Forum](https://forum.effectivealtruism.org/tag/effective-altruism-forum-1) account.\n\nRelated entries\n---------------\n\n[rationality community](https://forum.effectivealtruism.org/tag/rationality-community)\n\n1.  ^**[^](#fnrefbp7vt5jghu5)**^\n    \n    Alexander, Scott (2015) [Prescriptions, paradoxes, and perversities](https://slatestarcodex.com/2015/04/30/prescriptions-paradoxes-and-perversities/), *Slate Star Codex*, April 30.\n    \n2.  ^**[^](#fnrefsdctli4eu7l)**^\n    \n    Siskind, Scott et al. (2017) [Higher patient satisfaction with antidepressants correlates with earlier drug release dates across online user-generated medical databases](http://doi.org/10.1002/prp2.355), *Pharmacology Research and Perspectives*, vol. 5, pp. 1–9.\n    \n3.  ^**[^](#fnrefu430tt23cce)**^\n    \n    MetaMed (2013) [About: Our scientists, doctors & researchers](http://web.archive.org/web/20130403064221/http://metamed.com/our-scientists-doctors-researchers), *MetaMed*.\n    \n4.  ^**[^](#fnrefgrktaufo6i)**^\n    \n    Siskind, Scott (2018) [Scott Siskind, MD](http://web.archive.org/web/20180504090152/pcpasf.com/scott-siskind-md), *Pacific Coast Psychiatric Associates*.\n    \n5.  ^**[^](#fnrefpsghbo11shi)**^\n    \n    WebMD Care (2021) [Scott Alexander Siskind](https://doctor.webmd.com/doctor/scott-siskind-3481d1e8-fd8c-44b9-9d6e-44c32a16350f-overview), *WebMD Care*.\n    \n6.  ^**[^](#fnref9qk7vdxuo1t)**^\n    \n    Alexander, Scott (2013) [You’re probably wondering why I’ve called you here today](https://slatestarcodex.com/2013/02/12/youre-probably-wondering-why-ive-called-you-here-today/), *Slate Star Codex*, February 12.\n    \n7.  ^**[^](#fnref9zp125fel6o)**^\n    \n    Hoonhout, Tobias (2020) [What an NYT reporter’s doxing threat says about the paper’s ‘standards’](https://www.nationalreview.com/news/what-a-nyt-reporters-doxxing-threat-says-about-the-papers-standards/), *National Review*, June 23.\n    \n8.  ^**[^](#fnrefxpmreehr2dj)**^\n    \n    Alexander, Scott (2021) [Still alive](https://astralcodexten.substack.com/p/still-alive), *Astral Codex Ten*, January 21.\n    \n9.  ^**[^](#fnref53y92wfwieq)**^\n    \n    Alexander, Scott (2021) [What is Astral Codex Ten?](https://astralcodexten.substack.com/about), *Astral Codex Ten*.\n    \n10.  ^**[^](#fnref7rtjovtpln5)**^\n    \n    Alexander, Scott (2021) [Apply for an ACX Grant](https://astralcodexten.substack.com/p/apply-for-an-acx-grant), *Astral Codex Ten*, November 11.\n    \n11.  ^**[^](#fnrefdd0p6x77ie7)**^\n    \n    Alexander, Scott (2021) [ACX Grants results](https://astralcodexten.substack.com/p/acx-grants-results), *Astral Codex Ten*, December 28.\n    \n12.  ^**[^](#fnrefn1wsl523t0h)**^\n    \n    Caplan, Bryan (2014) [Read Scott Alexander](https://www.econlib.org/archives/2014/10/read_scott_alex.html), *EconLog*, October 28.\n    \n13.  ^**[^](#fnrefjlqbu47huy)**^\n    \n    Cowen, Tyler (2018) [Holding up a mirror to intellectuals of the left](https://www.bloomberg.com/opinion/articles/2018-04-24/holding-up-a-mirror-to-the-intellectuals-of-the-left), *Bloomberg*, April 24.\n    \n14.  ^**[^](#fnref7o50nx66pmv)**^\n    \n    Pinker, Steven (2021) [A typical essay by Scott Alexander is deeper, better reasoned, better referenced, more original, and wittier than 99% of the opinion pieces in MSM](https://twitter.com/sapinker/status/1360787817459253251), *Twitter*, February 14.\n    \n15.  ^**[^](#fnref1wb8xj121h7)**^\n    \n    Friedman, David D. (2020) [Slate Star Codex and The New York Times](http://daviddfriedman.blogspot.com/2020/06/slate-star-codex-and-new-york-times.html), *Ideas*, June 26.\n    \n16.  ^**[^](#fnref8c76tp4hzs9)**^\n    \n    Haider, Sarah (2020) [Scott Alexander’s Slate Star Codex is the best blog on the internet](https://twitter.com/SarahTheHaider/status/1276160510333988868), *Twitter*, June 25.\n    \n17.  ^**[^](#fnrefwxup5971ug)**^\n    \n    Graham, Paul (2017) [There’s no one writing now that I admire more than Scott Alexander](https://twitter.com/paulg/status/909070952465600512?lang=en), *Twitter*, September 16.\n    \n18.  ^**[^](#fnrefxu2xjp1w3bs)**^\n    \n    Aaronson, Scott (2020) [Pseudonymity as a trivial concession to genius](https://www.scottaaronson.com/blog/?p=4870), *Shtetl-Optimized*, June 23."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "2SvpeuuGWp2PSz5CX",
    "name": "Russell–Einstein Manifesto",
    "core": false,
    "slug": "russell-einstein-manifesto",
    "oldSlugs": null,
    "postCount": null,
    "description": {
      "markdown": "The **Russell–Einstein Manifesto** was a document issued in London on 9 July 1955 to alert the world about the threat of [human extinction](https://forum.effectivealtruism.org/tag/human-extinction) posed by [nuclear weapons](https://forum.effectivealtruism.org/tag/nuclear-weapons).\n\nDrafted by [Bertrand Russell](https://forum.effectivealtruism.org/tag/bertrand-russell), the document featured among its signatories some of the most eminent scientists of the time, including Max Born, Linus Pauling, and Albert Einstein, who signed it just days before his death. The manifesto was adopted as the founding document of the [Pugwash Conferences on Science and World Affairs](https://forum.effectivealtruism.org/tag/pugwash-conferences-on-science-and-world-affairs).\n\nRelated entries\n---------------\n\n[armed conflict](https://forum.effectivealtruism.org/tag/armed-conflict) | [Bulletin of the Atomic Scientists](/tag/bulletin-of-the-atomic-scientists) | [existential risk](https://forum.effectivealtruism.org/tag/existential-risk) | [human extinction](https://forum.effectivealtruism.org/tag/human-extinction) | [Manhattan Project](https://forum.effectivealtruism.org/tag/manhattan-project) | [nuclear disarmament movement](https://forum.effectivealtruism.org/tag/nuclear-disarmament-movement) |  [Pugwash Conferences on Science and World Affairs](https://forum.effectivealtruism.org/tag/pugwash-conferences-on-science-and-world-affairs) |[Trinity](https://forum.effectivealtruism.org/tag/trinity)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "i3LLLFiWDXcr6e8s7",
    "name": "Room for more funding",
    "core": false,
    "slug": "room-for-more-funding",
    "oldSlugs": [
      "an-organization-s-room-for-more-funding"
    ],
    "postCount": 2,
    "description": {
      "markdown": "**Room for more funding** is a measure of an organization's capacity to absorb additional donations.\n\nOne way to look at organizations or focus areas is to extrapolate from average past effectiveness. A weakness of this approach is that it does not account for [diminishing returns](https://forum.effectivealtruism.org/tag/diminishing-returns). If the amount of funding is increased, and if extra funding has diminishing returns, estimates based on past average effectiveness will overstate future effectiveness.\n\nGiveWell uses the concept of “room for more funding” to address this problem.^[\\[1\\]](#fnhbmamplngve)^ If an organization shows steeply diminishing returns, it is said to have “no room for more funding”; conversely, if it appears that the organization is not yet seeing diminishing returns, the organization has “room for more funding.” It is a matter of definition how steeply returns must be diminishing before an organization lacks room for more funding.\n\nA related concept is that of a “funding gap,” which is the difference between the amount of money an organization currently has and the amount of money it would need to raise in order for it to no longer have room for more funding. Max Dalton gives a fuller analysis of what we might mean by a funding gap.^[\\[2\\]](#fnhu8nf92by05)^\n\nHowever, it has been argued that \"funding gap\" models are generally less accurate or clear than models that show impact as a function of funding.^[\\[3\\]](#fn6xbjagpon7a)^\n\nFurther reading\n---------------\n\nGiveWell (2016) [Room for more funding](https://www.givewell.org/how-we-work/criteria/room-for-more-funding), *GiveWell*.\n\nRelated entries\n---------------\n\n[diminishing returns](https://forum.effectivealtruism.org/tag/diminishing-returns) | [donation choice](https://forum.effectivealtruism.org/tag/donation-choice) | [effective altruism funding](https://forum.effectivealtruism.org/tag/effective-altruism-funding) | [effective giving](https://forum.effectivealtruism.org/tag/effective-giving-1) | [neglectedness](https://forum.effectivealtruism.org/tag/neglectedness) | [philanthropic coordination](https://forum.effectivealtruism.org/tag/philanthropic-coordination) | [philanthropic diversification](https://forum.effectivealtruism.org/tag/philanthropic-diversification) | [thinking at the margin](https://forum.effectivealtruism.org/tag/thinking-at-the-margin)\n\n1.  ^**[^](#fnrefhbmamplngve)**^\n    \n    Karnofsky, Holden (2009) [Some simple ways to check “room for more funding”](https://blog.givewell.org/2009/12/17/some-simple-ways-to-check-room-for-more-funding/), *The GiveWell Blog*, December 17 (updated 17 August 2011).\n    \n2.  ^**[^](#fnrefhu8nf92by05)**^\n    \n    Dalton, Max (2017) [Defining returns functions and funding gaps](https://www.centreforeffectivealtruism.org/blog/defining-returns-functions-and-funding-gaps/), *Centre for Effective Altruism*, May 23.\n    \n3.  ^**[^](#fnref6xbjagpon7a)**^\n    \n    Dalton, Max & Owen Cotton-Barratt (2017) [Selecting the appropriate model for diminishing returns](https://www.centreforeffectivealtruism.org/blog/selecting-the-appropriate-model-for-diminishing-returns/), *Centre for Effective Altruism*, May 23."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "CmJvkWiguPFGmeX3G",
    "name": "Role impact",
    "core": false,
    "slug": "role-impact",
    "oldSlugs": null,
    "postCount": 3,
    "description": {
      "markdown": "The **role impact** of a career is the extent to which working in that role allows a person to have a positive impact.\n\nIn [80,000 Hours](https://forum.effectivealtruism.org/tag/80-000-hours)'s [career framework](https://forum.effectivealtruism.org/tag/career-framework), a person’s role impact is a function of two factors: the [importance](https://forum.effectivealtruism.org/tag/importance), [neglectedness](https://forum.effectivealtruism.org/tag/neglectedness), and [tractability](https://forum.effectivealtruism.org/tag/tractability) of the problem associated to the chosen career, and the size of the person’s direct and indirect contributions to that problem.\n\nFurther reading\n---------------\n\nTodd, Benjamin (2015) [What should you look for in a job? Introducing our framework](https://80000hours.org/articles/framework/), *80,000 Hours*, July."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "EXSauvNaQzjH7kvdG",
    "name": "Reversal test",
    "core": false,
    "slug": "reversal-test",
    "oldSlugs": null,
    "postCount": 4,
    "description": {
      "markdown": "The **reversal test** is a debiasing heuristic for reducing [status quo bias](https://forum.effectivealtruism.org/tag/status-quo-bias). The test was introduced by [Nick Bostrom](https://forum.effectivealtruism.org/tag/nick-bostrom) and [Toby Ord](https://forum.effectivealtruism.org/tag/toby-ord) in a 2006 article.^[\\[1\\]](#fn3y3j6wkjp39)^ Although the authors proposed it as a tool for reducing status quo bias specifically in the field of applied ethics, the test is applicable much more generally, to the evaluation of any decision involving a potential deviation from the status quo along some continuous dimension.\n\nThe reversal test\n-----------------\n\nBostrom and Ord define the reversal test as follows:^[\\[2\\]](#fnecey4743k5)^\n\n> When a proposal to change a certain parameter is thought to have bad overall consequences, consider a change to the same parameter in the opposite direction. If this is also thought to have bad overall consequences, then the onus is on those who reach these conclusions to explain why our position cannot be improved through changes to this parameter. If they are unable to do so, then we have reason to suspect that they suffer from status quo bias.\n\nThe justification for the reversal test is that, since for most parameters only a small fraction of all possible values constitute local optima, the status quo can in most cases be improved upon by deviating from it in one of the two possible directions. Thus, if a deviation in one of these directions is regarded as undesirable, a deviation in the opposite should, absent credible reasons to the contrary, be regarded as desirable.\n\nThe authors illustrate the test with an application to [cognitive enhancement](https://forum.effectivealtruism.org/tag/cognitive-enhancement):^[\\[3\\]](#fnjpu2xxmg93h)^\n\n> The great majority of those who judge increases to intelligence to be worse than the status quo would likely also judge decreases to be worse than the status quo. But this puts them in the rather odd position of maintaining that the net value for society provided by our current level of intelligence is at a local optimum, with small changes in either direction producing something worse. We can then ask for an explanation of why this should be thought to be so. If no sufficient reason is provided, our suspicion that the original judgment was influenced by status quo bias is corroborated.\n\nAdam Kolber offers another application of the test to decisions concerning memory enhancement.^[\\[4\\]](#fnnidl4uhcneq)^ The test has also been used in discussions about [anti-aging research](https://forum.effectivealtruism.org/tag/anti-aging-research): \"if there is merit in the suggestion that too long a life, with its end out of sight and mind, might diminish its worth, one might wonder whether we have already gone too far in increasing longevity.\"^[\\[5\\]](#fn8y969qqn66o)^\n\nThe double reversal test\n------------------------\n\nIn addition to the reversal test, Bostrom and Ord proposed a **double reversal test**:^[\\[6\\]](#fngu53qi46lx4)^\n\n> Suppose it is thought that increasing a certain parameter and decreasing it would both have bad overall consequences. Consider a scenario in which a natural factor threatens to move the parameter in one direction and ask whether it would be good to counterbalance this change by an intervention to preserve the status quo. If so, consider a later time when the naturally occurring factor is about to vanish and ask whether it would be a good idea to intervene to reverse the first intervention. If not, then there is a strong prima facie case for thinking that it would be good to make the first intervention even in the absence of the natural countervailing factor.\n\nAs an example, consider an irreversible medical procedure that prevents a certain poisonous chemical in the water supply from causing mild cognitive impairment. This procedure is widely regarded as desirable because of its protective effects on brain function. Eventually, a technology is developed that completely removes the chemical from the water supply. At this point, the medical procedure, which operates by offsetting cognitive impairment via a commensurate degree of cognitive enhancement, now comes to produce a net increase in cognitive functioning. However, while opponents of human enhancement would object to an intervention that directly conferred this boost in mental ability, they do not believe that in this scenario the chemical should be reintroduced in the water supply, or that cognitive functioning should otherwise impaired. These seemingly inconsistent attitudes suggest that the opposition to human enhancement is not rationally justified, and derives from status quo bias.\n\nAn alternative reversal test\n----------------------------\n\nThe reversal and double reversal tests may be distinguished from a related heuristic that also involves a certain kind of reversal and is also intended to combat status quo bias.^[\\[7\\]](#fnro9qu4q4fqq)^ Whenever a change to the status quo is being considered, this heuristic suggests that the change be reframed as the status quo, and the status quo as the change. For example, a person considering whether to move to a new city could imagine that they are already living in this new city, and consider instead if they would move to the city where they do in fact live.^[\\[8\\]](#fncwozu5q7nfr)^ Or an investor considering whether to sell a financial asset could instead consider whether they would buy this asset if they were not already invested in it.\n\nThis alternative reversal test has been applied to Robert Nozick's \"experience machine\" thought experiment, considered by many to raise a decisive objection to mental state theories of [wellbeing](https://forum.effectivealtruism.org/tag/wellbeing) generally and [hedonism](https://forum.effectivealtruism.org/tag/hedonism) specifically.^[\\[9\\]](#fnxjx2zzoxrwf)^ Adam Kolber and Joshua Greene have each independently suggested considering, besides Nozick's original question of whether we would connect to the experience machine, the question of whether we would disconnect from it were we already connected.^[\\[10\\]](#fnqnxbzcrgcfs)^^[\\[11\\]](#fn3t16a4x2r7k)^ The authors speculate that most people would answer both questions negatively, suggesting that our intuitive responses are being influenced by status quo bias. Subsequent experimental results by Felipe De Brigard and Dan Weijers have on the whole vindicated those speculations.^[\\[12\\]](#fnu5w47irwnei)^^[\\[13\\]](#fnvcsq3oxegh)^\n\nFurther reading\n---------------\n\nBostrom, Nick & Toby Ord (2006) [The reversal test eliminating status quo bias in applied ethics](http://doi.org/10.1086/505233), *Ethics*, vol. 116, pp. 656–679.\n\nRelated entries\n---------------\n\n[status quo bias](https://forum.effectivealtruism.org/tag/status-quo-bias)\n\n1.  ^**[^](#fnref3y3j6wkjp39)**^\n    \n    Bostrom, Nick & Toby Ord (2006) [The reversal test: eliminating status quo bias in applied ethics](http://doi.org/10.1086/505233), *Ethics*, vol. 116, pp. 656–679.\n    \n2.  ^**[^](#fnrefecey4743k5)**^\n    \n    Bostrom & Ord, [The reversal test](http://doi.org/10.1086/505233), pp. 664-665.\n    \n3.  ^**[^](#fnrefjpu2xxmg93h)**^\n    \n    Bostrom & Ord, [The reversal test](http://doi.org/10.1086/505233), p. 664.\n    \n4.  ^**[^](#fnrefnidl4uhcneq)**^\n    \n    Kolber, Adam J. (2006) [Therapeutic forgetting: the legal and ethical implications of memory dampening](https://scholarship.law.vanderbilt.edu/vlr/vol59/iss5/2), *Vanderbilt Law Review*, vol. 59, pp. 1559–1626, pp. 1610-1611.\n    \n5.  ^**[^](#fnref8y969qqn66o)**^\n    \n    The President’s Council on Bioethics (2003) [*Beyond Therapy: Biotechnology and the Pursuit of Happiness*](https://biotech.law.lsu.edu/research/pbc/reports/beyondtherapy/beyond_therapy_final_report_pcbe.pdf), Washington, D.C., p. 196.\n    \n6.  ^**[^](#fnrefgu53qi46lx4)**^\n    \n    Bostrom & Ord, [The reversal test](http://doi.org/10.1086/505233), p. 673.\n    \n7.  ^**[^](#fnrefro9qu4q4fqq)**^\n    \n    Weijers, Dan (2011) [Intuitive biases in judgments about thought experiments: the experience machine revisited](https://philpapers.org/rec/WEIIBI), *Philosophical Writings*, vol. 50, pp. 1–18, p. 10.\n    \n8.  ^**[^](#fnrefcwozu5q7nfr)**^\n    \n    Salamon, Anna (2012) [Checklist of rationality habits](https://www.lesswrong.com/posts/ttGbpJQ8shBi8hDhh/checklist-of-rationality-habits), *LessWrong*, November 7, 3.2.\n    \n9.  ^**[^](#fnrefxjx2zzoxrwf)**^\n    \n    Dan Weijers lists 30 separate publications expressing this opinion.^[\\[12\\]](#fnu5w47irwnei)^\n    \n10.  ^**[^](#fnrefqnxbzcrgcfs)**^\n    \n    Kolber, Adam J. (1994) [Mental statism and the experience machine](https://ssrn.com/abstract=1322059), *Bard Journal of Social Sciences*, vol. 3, pp. 10–17.\n    \n11.  ^**[^](#fnref3t16a4x2r7k)**^\n    \n    Greene, Joshua D. (2001) 'A psychological perspective on Nozick’s experience machine and Parfit’s repugnant conclusion', Society for Philosophy and Psychology Annual Meeting, Cincinnati, Ohio.\n    \n12.  ^**[^](#fnrefu5w47irwnei)**^\n    \n    Weijers, Dan (2014) [Nozick’s experience machine is dead, long live the experience machine!](http://doi.org/10.1080/09515089.2012.757889), *Philosophical Psychology*, vol. 27, pp. 513–535.\n    \n13.  ^**[^](#fnrefvcsq3oxegh)**^\n    \n    de Brigard, Felipe (2010) [If you like it, does it matter if it’s real?](http://doi.org/10.1080/09515080903532290), *Philosophical Psychology*, vol. 23, pp. 43–57."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "oyrF4wcA4H3t6gp8B",
    "name": "Replaceability",
    "core": false,
    "slug": "replaceability",
    "oldSlugs": null,
    "postCount": 12,
    "description": {
      "markdown": "In the context of [career choice](https://forum.effectivealtruism.org/tag/career-choice), a person's **replaceability** is the degree to which their [role impact](https://forum.effectivealtruism.org/tag/role-impact) would vary should that position be occupied by the next most likely candidate.\n\nBenjamin Todd illustrates this idea with an example:^[\\[1\\]](#fnim86lg4rjg)^\n\n> Suppose you become a surgeon and perform 100 life saving operations. Naively it seems like your impact is to save 100 people’s lives. If you hadn’t taken the job, however, someone else likely would have taken it instead. So your true (counterfactual) impact is less than the good you do directly.\n\nThis type of consideration is relevant not only to careers, but to donations (“Would someone else have fulfilled charity X’s funding gap if I hadn’t?”) and fundraising (“If I persuade someone to give money, would they have given it anyway?”).\n\nHowever, it is often unclear to what an extent replaceability applies for a given action, and 80,000 Hours notes that for many career decisions replaceability has only limited relevance.\n\nFurther reading\n---------------\n\nChristiano, Paul (2013) [Replaceability](https://rationalaltruist.com/2013/01/22/replaceability/), *Rational Altruist*, January 22.  \n*An alternative view.*\n\nKuhn, Ben (2013) [Replaceability in altruism](https://forum.effectivealtruism.org/posts/urayBifZX4cj74okH/replaceability-in-altruism), *Effective Altruism Forum*, August 29.  \n*A discussion regarding charitable donations.*\n\nO’Keeffe-O’Dononvan, Rossa (2014) [What does economics tell us about replaceability?](https://80000hours.org/2014/07/what-does-economics-tell-us-about-replaceability/), *80,000 Hours*, July 17.  \n*An analysis with regards to careers.*\n\nRieber, Lila (2015) [The bittersweetness of replaceability](https://forum.effectivealtruism.org/posts/CsRyvwwop66dgo5gu/the-bittersweetness-of-replaceability), *Effective Altruism Forum*, July 12.  \n*A more personal approach to the arguments, and a discussion of the various ways in which they can be applied.*\n\nRelated entries\n---------------\n\n[altruistic coordination](https://forum.effectivealtruism.org/tag/altruistic-coordination) | [counterfactual reasoning](https://forum.effectivealtruism.org/tag/counterfactual-reasoning) | [impact assessment](https://forum.effectivealtruism.org/tag/impact-assessment) | [markets for altruism](https://forum.effectivealtruism.org/tag/markets-for-altruism)\n\n1.  ^**[^](#fnrefim86lg4rjg)**^\n    \n    Todd, Benjamin (2015) [‘Replaceability’ isn’t as important as you might think (or we’ve suggested)](https://80000hours.org/2015/07/replaceability-isnt-as-important-as-you-might-think-or-weve-suggested/), *80,000 Hours*, July 27."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "jRK6K3XRLhcqCWfzn",
    "name": "Rationality community",
    "core": false,
    "slug": "rationality-community",
    "oldSlugs": null,
    "postCount": 4,
    "description": {
      "markdown": "The **rationality community** (also known as the **rationalist community**, **rationality movement** and **rationalist movement**) is a social and intellectual community interested in [Bayesian epistemology](https://forum.effectivealtruism.org/tag/bayesian-epistemology), [cognitive bias](https://forum.effectivealtruism.org/tag/cognitive-bias), [existential risk](https://forum.effectivealtruism.org/tag/existential-risk) and other ideas related to [epistemic and instrumental rationality](https://forum.effectivealtruism.org/tag/instrumental-vs-epistemic-rationality), centered around the website [LessWrong](https://forum.effectivealtruism.org/tag/lesswrong) and most prominently represented in the writings of [Eliezer Yudkowsky](https://forum.effectivealtruism.org/tag/eliezer-yudkowsky) and [Scott Alexander](https://forum.effectivealtruism.org/tag/scott-alexander).\n\nRelated entries\n---------------\n\n[AI alignment](https://forum.effectivealtruism.org/tag/ai-alignment) | [Scott Alexander](https://forum.effectivealtruism.org/tag/scott-alexander) | [Center for Applied Rationality](/tag/center-for-applied-rationality) | [cognitive bias](https://forum.effectivealtruism.org/tag/cognitive-bias) | [communities adjacent to effective altruism](https://forum.effectivealtruism.org/tag/communities-adjacent-to-effective-altruism) | [existential risk](https://forum.effectivealtruism.org/tag/existential-risk) | [Julia Galef](https://forum.effectivealtruism.org/tag/julia-galef) | [LessWrong](https://forum.effectivealtruism.org/tag/lesswrong) | [Machine Intelligence Research Institute](/tag/machine-intelligence-research-institute) | [rationality](https://forum.effectivealtruism.org/tag/rationality) | [Eliezer Yudkowsky](https://forum.effectivealtruism.org/tag/eliezer-yudkowsky)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "rEz2Hoe4Pn74DpK8S",
    "name": "Quality superintelligence",
    "core": false,
    "slug": "quality-superintelligence",
    "oldSlugs": null,
    "postCount": null,
    "description": {
      "markdown": "A **quality superintelligence** is a system at least as fast as the typical human mind but qualitatively much more intelligent.\n\nFurther reading\n---------------\n\nBostrom, Nick (2014) [*Superintelligence: Paths, Dangers, Strategies*](https://en.wikipedia.org/wiki/Special:BookSources/9780199678112), Oxford: Oxford University Press, pp. 56–57.\n\nRelated entries\n---------------\n\n[collective superintelligence](https://forum.effectivealtruism.org/tag/collective-superintelligence) | [speed superintelligence](https://forum.effectivealtruism.org/tag/speed-superintelligence) | [superintelligence](https://forum.effectivealtruism.org/tag/superintelligence)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "GQkyZbfZvxHYYhurT",
    "name": "Progress studies",
    "core": false,
    "slug": "progress-studies",
    "oldSlugs": null,
    "postCount": 35,
    "description": {
      "markdown": "**Progress studies** is an [intellectual movement](https://forum.effectivealtruism.org/tag/social-and-intellectual-movements) focused on figuring out why progress happens and how to make it happen faster.^[\\[1\\]](#fn4ufihpjtphb)^\n\nThe field examines the economic, technological, scientific, cultural, and organizational changes that have improved standards of living over [human history](https://forum.effectivealtruism.org/tag/history), seeking to identify the individuals, cultures, and institutions responsible for this progress, and to apply this knowledge to the design of interventions aimed at further improving the human condition.\n\nFurther reading\n---------------\n\nCollison, Patrick & Tyler Cowen (2019) [We need a new science of progress](https://www.theatlantic.com/science/archive/2019/07/we-need-new-science-progress/594946/), *The Atlantic*, July 30.\n\nCrawford, Jason (2019) [Progress studies as a moral imperative](https://rootsofprogress.org/progress-studies-a-moral-imperative), *The Roots of Progress*, September 7.\n\nMay, Daniel (2019) [Progress studies reading list](https://web.archive.org/web/20211008161931/https://danielmay.io/progress), *Daniel May’s Blog*, August.\n\nPiper, Kelsey (2022) [To make progress, we need to study it](https://www.vox.com/future-perfect/2022/2/11/22923756/to-make-progress-we-need-to-study-it), *Vox*, February 11.\n\nWhitaker, Nick (2021) [The crux: collaboration](https://www.highmodernism.com/blog/the-crux-collaboration), *High Modernism*, December 14.\n\nExternal links\n--------------\n\n[The Roots of Progress](https://rootsofprogress.org/). A nonprofit organization about progress studies.\n\n[Progress Forum](https://progressforum.org).  An online discussion forum for the progress community in the style of the [Effective Altruism Forum](https://forum.effectivealtruism.org/topics/effective-altruism-forum-1).\n\nRelated entries\n---------------\n\n[communities adjacent to effective altruism](https://forum.effectivealtruism.org/tag/communities-adjacent-to-effective-altruism) | [cultural evolution](https://forum.effectivealtruism.org/tag/cultural-evolution) | [differential progress](https://forum.effectivealtruism.org/tag/differential-progress) | [economic growth](https://forum.effectivealtruism.org/tag/economic-growth) | [history](https://forum.effectivealtruism.org/tag/history) | [scientific progress](https://forum.effectivealtruism.org/tag/scientific-progress) | [The Roots of Progress](https://forum.effectivealtruism.org/tag/the-roots-of-progress) | [rationality community](https://forum.effectivealtruism.org/topics/rationality-community)\n\n1.  ^**[^](#fnref4ufihpjtphb)**^\n    \n    Piper, Kelsey (2022) [To make progress, we need to study it](https://www.vox.com/future-perfect/2022/2/11/22923756/to-make-progress-we-need-to-study-it), *Vox*, February 11."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "FJXfP8HxJ4gNsTw5K",
    "name": "Prioritarianism",
    "core": false,
    "slug": "prioritarianism",
    "oldSlugs": null,
    "postCount": 3,
    "description": {
      "markdown": "**Prioritarianism** is the view in [normative ethics](https://forum.effectivealtruism.org/tag/normative-ethics) that increasing people's [wellbeing](https://forum.effectivealtruism.org/tag/wellbeing) is [intrinsically](https://forum.effectivealtruism.org/tag/intrinsic-vs-instrumental-value) more valuable the worse off these people are.\n\nFurther reading\n---------------\n\nBroome, John (2015) [Equality versus priority: A useful distinction](http://doi.org/10.1017/S0266267115000097), *Economics and Philosophy*, vol. 31, pp. 219–228.\n\nOrd, Toby (2015) [A new counterexample to prioritarianism](http://doi.org/10.1017/S0953820815000059), *Utilitas*, vol. 27, pp. 298–302.\n\nParfit, Derek (1991) [*Equality or Priority*](http://hdl.handle.net/1808/12405), Lawrence: University of Kansas.\n\nRelated entries\n---------------\n\n[equal consideration of interest](https://forum.effectivealtruism.org/topics/equal-consideration-of-interests) | [moral philosophy](https://forum.effectivealtruism.org/tag/moral-philosophy) | [normative ethics](https://forum.effectivealtruism.org/tag/normative-ethics)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "XaS2BjmEqkSQGWSrx",
    "name": "Principle of epistemic deference",
    "core": false,
    "slug": "principle-of-epistemic-deference",
    "oldSlugs": null,
    "postCount": 1,
    "description": {
      "markdown": "The **principle of epistemic deference** is the heuristic principle according to which, since the beliefs of a [superintelligence](https://forum.effectivealtruism.org/tag/superintelligence) are more likely to be true than those of human beings, humanity should defer to it whenever feasible.\n\nFurther reading\n---------------\n\nBostrom, Nick (2014) [*Superintelligence: Paths, Dangers, Strategies*](https://en.wikipedia.org/wiki/Special:BookSources/9780199678112), Oxford: Oxford University Press, p. 211.\n\nRelated entries\n---------------\n\n[epistemic deference](https://forum.effectivealtruism.org/tag/epistemic-deference) | [superintelligence](https://forum.effectivealtruism.org/tag/superintelligence)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "RhWCRXjTagDpLk2L5",
    "name": "Prediction markets",
    "core": false,
    "slug": "prediction-markets",
    "oldSlugs": null,
    "postCount": 25,
    "description": {
      "markdown": "**Prediction markets** (also known as **information markets** and **idea futures**) are markets for trading contracts about future events.\n\nFurther reading\n---------------\n\nHanson, Robin (1995) [Could gambling save science? Encouraging an honest consensus](https://doi.org/10.1080/02691729508578768), *Social Epistemology*, vol. 9, pp. 3–33.\n\nLagerros, Jacob & Nuño Sempere (2021) [Database of prediction markets](https://docs.google.com/spreadsheets/d/1XB1GHfizNtVYTOAD_uOyBLEyl_EV7hVtDYDXLQwgT7k/edit?usp=embed_facebook), *Google Sheets*.\n\nSempere, Nuño, Misha Yagudin & Eli Lifland (2021) [Prediction markets in the corporate setting](https://forum.effectivealtruism.org/posts/dQhjwHA7LhfE8YpYF/prediction-markets-in-the-corporate-setting), *Effective Altruism Forum*, December 31.\n\nRelated entries\n---------------\n\n[improving institutional decision-making](https://forum.effectivealtruism.org/topics/improving-institutional-decision-making) | [model uncertainty](https://forum.effectivealtruism.org/tag/model-uncertainty) | [forecasting](https://forum.effectivealtruism.org/tag/forecasting) | [value of information](https://forum.effectivealtruism.org/tag/value-of-information) | [Robin Hanson](https://forum.effectivealtruism.org/tag/robin-hanson)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "A7AdvWziptT6uRTpf",
    "name": "Poor Economics",
    "core": false,
    "slug": "poor-economics",
    "oldSlugs": [
      "poor-economics"
    ],
    "postCount": null,
    "description": {
      "markdown": "***Poor Economics: A Radical Rethinking of the Way to Fight Global Poverty*** is a book by [Abhijit Banerjee](https://forum.effectivealtruism.org/tag/abhijit-banerjee) and [Esther Duflo](https://forum.effectivealtruism.org/tag/esther-duflo). It was published on 26 April 2011.\n\nFurther reading\n---------------\n\nBanerjee, Abhijit & Esther Duflo (2011) [*Poor Economics: A Radical Rethinking of the Way to Fight Global Poverty*](https://en.wikipedia.org/wiki/Special:BookSources/9781586487980), New York: PublicAffairs.\n\nExternal links\n--------------\n\n[Poor Economics](https://web.archive.org/web/20170609001954/http://www.pooreconomics.com/). Official website.\n\nRelated entries\n---------------\n\n[Abdul Latif Jameel Poverty Action Lab](https://forum.effectivealtruism.org/tag/abdul-latif-jameel-poverty-action-lab) |  [Abhijit Banerjee](https://forum.effectivealtruism.org/tag/abhijit-banerjee) | [Esther Duflo](https://forum.effectivealtruism.org/tag/esther-duflo) | [global health and development](https://forum.effectivealtruism.org/tag/global-health-and-development) | [global poverty](https://forum.effectivealtruism.org/tag/global-poverty) | [randomized controlled trials](https://forum.effectivealtruism.org/tag/randomized-controlled-trials)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "93YpWe795zPv5hWsp",
    "name": "Polarity",
    "core": false,
    "slug": "polarity",
    "oldSlugs": null,
    "postCount": null,
    "description": {
      "markdown": "In international relations, **polarity** is any of the various ways in which power is distributed within the international system. International relations scholars distinguish among **unipolar**, **bipolar** and **multipolar** scenarios depending on whether there is at the global level one, two, or more than two centers of power, respectively, though the term \"multipolar\" is also sometimes used to refer to scenarios that are not unipolar, i.e. where there is more than one center of power.^[\\[1\\]](#fn0ua6xl82nco)^\n\nRelated entries\n---------------\n\n[global governance](https://forum.effectivealtruism.org/tag/global-governance) | [great power conflict](https://forum.effectivealtruism.org/tag/great-power-conflict) | [international relations](https://forum.effectivealtruism.org/tag/international-relations) | [singleton](https://forum.effectivealtruism.org/tag/singleton)\n\n1.  ^**[^](#fnref0ua6xl82nco)**^\n    \n    Bostrom, Nick (2014) [*Superintelligence: Paths, Dangers, Strategies*](https://en.wikipedia.org/wiki/Special:BookSources/978-0-19-967811-2), Oxford: Oxford University Press, ch. 11."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "z7CGKF3NPCWgdwFYv",
    "name": "Ploughshares Fund",
    "core": false,
    "slug": "ploughshares-fund",
    "oldSlugs": null,
    "postCount": null,
    "description": {
      "markdown": "The **Ploughshares Fund** is a grantmaking foundation that funds initiatives to reduce risks posed by [nuclear weapons](https://forum.effectivealtruism.org/tag/nuclear-weapons).\n\nExternal links\n--------------\n\n[Ploughshares Fund](https://ploughshares.org/). Official website.\n\n[Apply for a job](https://ploughshares.org/about-us/jobs)."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "PbosA6FPoLQ8BNdCW",
    "name": "Philosophic Radicals",
    "core": false,
    "slug": "philosophic-radicals",
    "oldSlugs": null,
    "postCount": null,
    "description": {
      "markdown": "The **Philosophic Radicals** (sometimes called the **Philosophical Radicals**) were a group of followers of [Jeremy Bentham](https://forum.effectivealtruism.org/tag/jeremy-bentham), led by [John Stuart Mill](https://forum.effectivealtruism.org/tag/john-stuart-mill), politically active in England during the first half of the nineteenth century.\n\nFurther reading\n---------------\n\nHamburger, Joseph (1965) [*Intellectuals in Politics: John Stuart Mill and the Philosophic Radicals*](https://en.wikipedia.org/wiki/Special:BookSources/978-0-300-00532-5), New Haven: Yale University Press.\n\nPringle-Pattison, A. Seth (1907) *The Philosophical Radicals and Other Essays*, Edinburgh: William Blackwood and sons.\n\nRosen, Frederick (2011) [From Jeremy Bentham’s radical philosophy to J. S. Mill’s philosophic radicalism](https://doi.org/10.1017/CHOL9780521430562.010), in Gareth Stedman Jones & Gregory Claeys (eds.) *The Cambridge History of Nineteenth-Century Political Thought*, Cambridge: Cambridge University Press, pp. 257–294.\n\nRosen, Frederick (2013) [Philosophic radicalism](https://www.bloomsburycollections.com/book/the-bloomsbury-encyclopedia-of-utilitarianism), in James E. Crimmins (ed.) *The Bloomsbury Encyclopedia of Utilitarianism*, London: Bloomsbury Academic, pp. 410–412.\n\nThomas, William (1979) [*The Philosophic Radicals: Nine Studies in Theory and Practice, 1817-1841*](https://en.wikipedia.org/wiki/Special:BookSources/9780198224907), Oxford: Clarendon Press.\n\nRelated entries\n---------------\n\n[communities adjacent to effective altruism](https://forum.effectivealtruism.org/tag/communities-adjacent-to-effective-altruism) | [Jeremy Bentham](https://forum.effectivealtruism.org/tag/jeremy-bentham) | [John Stuart Mill](https://forum.effectivealtruism.org/tag/john-stuart-mill) | [utilitarianism](https://forum.effectivealtruism.org/tag/utilitarianism)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "LaxkvWtJcYWWsRFoD",
    "name": "Philip Tetlock",
    "core": false,
    "slug": "philip-tetlock",
    "oldSlugs": null,
    "postCount": 8,
    "description": {
      "markdown": "**Philip Eyrikson Tetlock** (born 2 March 1954) is a Canadian-American political scientist, currently a professor at the University of Pennsylvania.\n\nPhilip Tetlock is famous for his work on the concept and practice of good judgement, including on how to make accurate probabilistic predictions, which he discusses in his book *Superforecasting: The Art and Science of Prediction.*\n\nFurther reading\n---------------\n\nGalef, Julia (2021) [Julia Galef Interviews Philip Tetlock for ‘The Ezra Klein Show’](https://www.nytimes.com/2021/12/03/podcasts/transcript-ezra-klein-podcast-philip-tetlock.html), *The New York Times*, December 3.\n\nWiblin, Robert (2017) [Prof Tetlock on predicting catastrophes, why keep your politics secret, and when experts know more than you](https://80000hours.org/podcast/episodes/prof-tetlock-predicting-the-future/), *80,000 Hours*, November 20.\n\nWiblin, Robert & Keiran Harris (2019) [Accurately predicting the future is central to absolutely everything. Professor Tetlock has spent 40 years studying how to do it better](https://80000hours.org/podcast/episodes/philip-tetlock-forecasting-research/), *80,000 Hours*, June 28.\n\nTetlock, Philip E. (2006) [*Expert Political Judgment: How Good Is It? How Can We Know?*](https://en.wikipedia.org/wiki/Special:BookSources/978-0-691-12871-9), Princeton: Princeton Univ. Press.\n\nTetlock, Philip E. & Dan Gardner (2015) [*Superforecasting: The Art and Science of Prediction*](https://en.wikipedia.org/wiki/Special:BookSources/978-0-8041-3669-3), New York: Crown Publishers.\n\nExternal links\n--------------\n\n[Philip E. Tetlock](https://www.sas.upenn.edu/tetlock/). Official website.\n\nRelated entries\n---------------\n\n[forecasting](https://forum.effectivealtruism.org/topics/forecasting)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "ZLNgqQ4qdA6ztGXnb",
    "name": "Peter Singer",
    "core": false,
    "slug": "peter-singer",
    "oldSlugs": null,
    "postCount": 22,
    "description": {
      "markdown": "**Peter Albert David Singer** (born 6 July 1946) is an Australian philosopher, currently a professor of philosophy at Princeton University. Singer is generally regarded as one the most influential living philosophers, and has been a key figure behind both the [animal welfare](https://forum.effectivealtruism.org/tag/effective-animal-advocacy) and the [effective altruism](https://forum.effectivealtruism.org/tag/effective-altruism) movements.\n\nSinger is the recipient of the 2021 Berggruen Prize, sometimes described as \"the Nobel Prize for philosophy\". The committee praised Singer for reinvigorating [utilitarianism](https://forum.effectivealtruism.org/tag/utilitarianism) both within academia and as a force in the world.^[\\[1\\]](#fnrwl1zxaaqtn)^ Singer announced that he would donate half of the $1 million prize to [The Life You Can Save](https://forum.effectivealtruism.org/tag/the-life-you-can-save), with the remainder split in a roughly 2:1 ratio between organizations recommended by [Animal Charity Evaluators](https://forum.effectivealtruism.org/tag/animal-charity-evaluators) chosen by Singer and organizations recommended by The Life You Can Save chosen by the public on a vote to be held in late 2021.^[\\[2\\]](#fnjrmg11u18l)^\n\nFurther reading\n---------------\n\nGalef, Julia (2013) [Being a utilitarian in the real world (Peter Singer)](http://rationallyspeakingpodcast.org/97-being-a-utilitarian-in-the-real-world-peter-singer/), *Rationally Speaking*, November 24.\n\nMacAskill, William & Darius Meissner (2020) [Peter Singer](https://www.utilitarianism.net/utilitarian-thinker/peter-singer), *Utilitarianism*.\n\nPiper, Kelsey (2020) [What philosopher Peter Singer has learned in 45 years of advocating for animals](https://www.vox.com/future-perfect/2020/10/27/21529060/animal-rights-philosopher-peter-singer-why-vegan-book), *Vox*, October 27.\n\nRighetti, Luca & Moorhouse, Fin (2020) [Peter Singer on speciesism, lockdown ethics, and controversial ideas](https://hearthisidea.com/episodes/peter), *Hear This Idea*, December 6.\n\nSchultz, Bart (2013) [Peter Singer](https://www.bloomsburycollections.com/book/the-bloomsbury-encyclopedia-of-utilitarianism), in James E. Crimmins (ed.) *The Bloomsbury Encyclopedia of Utilitarianism*, London: Bloomsbury Academic, pp. 509–514.\n\nWiblin, Robert, Arden Koehler & Keiran Harris (2019) [Peter Singer on being provocative, EA, how his moral views have changed, & rescuing children drowning in ponds](https://80000hours.org/podcast/episodes/peter-singer-advocacy-and-the-life-you-can-save/), *80,000 Hours*, December 5.\n\nExternal links\n--------------\n\n[Peter Singer](https://petersinger.info/). Official website.\n\nRelated entries\n---------------\n\n[demandingness of morality](https://forum.effectivealtruism.org/tag/demandingness-of-morality) | [farmed animal welfare](https://forum.effectivealtruism.org/tag/farmed-animal-welfare) | [global poverty](https://forum.effectivealtruism.org/tag/global-poverty) | [The Life You Can Save](https://forum.effectivealtruism.org/tag/the-life-you-can-save) | [*The Life You Can Save: Acting Now to End World Poverty*](https://forum.effectivealtruism.org/tag/the-life-you-can-save-book)\n\n1.  ^**[^](#fnrefrwl1zxaaqtn)**^\n    \n    Schuessler, Jennifer (2021) [Peter Singer Wins $1 Million Berggruen Prize](https://www.nytimes.com/2021/09/07/arts/peter-singer-berggruen-prize.html), *The New York Times*, September 7.\n    \n2.  ^**[^](#fnrefjrmg11u18l)**^\n    \n    Singer, Peter (2021) [How to give away a million dollars](https://www.project-syndicate.org/commentary/how-to-give-away-a-million-dollars-by-peter-singer-2021-09), *Project Syndicate*, September 7."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "665bdaA2m2kJj9CgN",
    "name": "Personal identity",
    "core": false,
    "slug": "personal-identity",
    "oldSlugs": null,
    "postCount": 7,
    "description": {
      "markdown": "**Personal identity** is the set of necessary and sufficient conditions that make a person the same over time.\n\nFurther reading\n---------------\n\nBroome, John (1991) [Utilitarian metaphysics?](https://en.wikipedia.org/wiki/Special:BookSources/0-521-39274-8), in Jon Elster & John E. Roemer (eds.) *Interpersonal Comparisons of Well-Being*, Cambridge: Cambridge University Press, pp. 70–95.\n\nParfit, Derek (1984) [*Reasons and Persons*](https://en.wikipedia.org/wiki/Special:BookSources/0-19-824908-X), Oxford: Clarendon Press, part 3.\n\nShoemaker, David W. (2005) [Personal identity and ethics](https://plato.stanford.edu/archives/win2019/entries/identity-ethics/), *Stanford Encyclopedia of Philosophy*, December 20 (updated 11 October 2019).\n\nRelated entries\n---------------\n\n[moral philosophy](https://forum.effectivealtruism.org/tag/moral-philosophy) | [philosophy](https://forum.effectivealtruism.org/tag/philosophy) | [philosophy of mind](https://forum.effectivealtruism.org/tag/philosophy-of-mind)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "jeWy74PhrGMvSBLq6",
    "name": "Personal fit",
    "core": false,
    "slug": "personal-fit",
    "oldSlugs": null,
    "postCount": 16,
    "description": {
      "markdown": "**Personal fit** is a measure of a person's chances of excelling in a particular job.\n\nFormally, personal fit it may be defined as the ratio of the person's [productivity](https://forum.effectivealtruism.org/topics/productivity) in the job to the average productivity of other candidates likely to take that job. Personal fit is one of the factors in [80,000 Hours](https://forum.effectivealtruism.org/tag/80-000-hours)' original [career framework](https://forum.effectivealtruism.org/tag/career-framework), as well as a key element in their current list of strategic considerations.^[\\[1\\]](#fnicamz6rovqe)^\n\nPersonal fit measures a person's absolute advantage in a job. But a person's comparative becomes relevant when [coordination](https://forum.effectivealtruism.org/tag/altruistic-coordination) is feasible. In such cases, a career with lower personal fit may have a greater community impact.^[\\[2\\]](#fn388skbuku1l)^\n\nFurther reading\n---------------\n\nTodd, Benjamin (2021) [Personal fit: why being good at your job is even more important than people think](https://80000hours.org/articles/personal-fit/), *80,000 Hours*, September.\n\n1.  ^**[^](#fnreficamz6rovqe)**^\n    \n    Todd, Benjamin (2019) [A guide to using your career to help solve the world’s most pressing problems](https://80000hours.org/key-ideas/#career-strategy), *80,000 Hours*, March.\n    \n2.  ^**[^](#fnref388skbuku1l)**^\n    \n    Todd, Benjamin (2018) [Doing good together - how to coordinate effectively, and avoid single-player thinking](https://80000hours.org/articles/coordination/), *80,000 Hours*, September 21, section 3."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "wcGJwBkFNBuLbvTWP",
    "name": "Person-affecting views",
    "core": false,
    "slug": "person-affecting-views",
    "oldSlugs": null,
    "postCount": 15,
    "description": {
      "markdown": "In [population ethics](https://forum.effectivealtruism.org/tag/population-ethics), **person-affecting views** are views that attempt to capture the [intuition](https://forum.effectivealtruism.org/topics/intuition-of-neutrality) that an outcome can be bad only if it is bad for people. (By 'people' it is meant a [moral patient](https://forum.effectivealtruism.org/tag/moral-patienthood) rather than a human being.) [Derek Parfit](https://forum.effectivealtruism.org/tag/derek-parfit) distinguishes between **narrow person-affecting views**, which hold that an outcome can be bad only if it is bad for the people who exist in this outcome, and **wide person-affecting views**, which allow that an outcome can be bad if some different attainable outcome would have benefited people in it more.\n\nFurther reading\n---------------\n\nParfit, Derek (1984) [*Reasons and Persons*](https://en.wikipedia.org/wiki/Special:BookSources/019824908X), Oxford: Clarendon Press, pp. 393–401.\n\nThomas, Teruji (2019) [The asymmetry, uncertainty, and the long term](https://globalprioritiesinstitute.org/wp-content/uploads/Teruji_Thomas_asymmetry_uncertainty.pdf), Global Priorities Institute, section 2.3.\n\nRelated entries\n---------------\n\n[ethics of existential risk](/tag/ethics-of-existential-risk) | [intuition of neutrality](https://forum.effectivealtruism.org/tag/intuition-of-neutrality) | [population ethics](https://forum.effectivealtruism.org/tag/population-ethics) | [total view](https://forum.effectivealtruism.org/topics/total-view)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "ffmj8y8jF3dKA9ed4",
    "name": "Oxford Prioritisation Project",
    "core": false,
    "slug": "oxford-prioritisation-project",
    "oldSlugs": null,
    "postCount": null,
    "description": {
      "markdown": "The **Oxford Prioritisation Project** was a [cause prioritization](https://forum.effectivealtruism.org/tag/cause-prioritization) research initiative by a team of volunteer students at the University of Oxford. The project ran between January and May 2017.\n\nFurther reading\n---------------\n\nLagerros, Jacob & Tom Sittler (2017) [Oxford Prioritisation Project review](https://fragile-credences.github.io/oxprioreview/), *Fragile Credences*, October 12.\n\nExternal links\n--------------\n\n[Oxford Prioritisation Project](http://web.archive.org/web/20200220220021/https://oxpr.io/). Official website retrieved from the Wayback Machine."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "MsZJrnxnoG7QRnhYe",
    "name": "Orthogonality thesis",
    "core": false,
    "slug": "orthogonality-thesis",
    "oldSlugs": null,
    "postCount": 3,
    "description": {
      "markdown": "The **orthogonality thesis** is the view that intelligence and motivation are not mutually interdependent. According to the orthogonality thesis, an intelligent agent could in principle combine any level of cognitive ability with any list of ultimate goals.\n\nFurther reading\n---------------\n\nBostrom, Nick (2012) [The superintelligent will: motivation and instrumental rationality in advanced artificial agents](http://doi.org/10.1007/s11023-012-9281-3), *Minds and Machines*, vol. 22, pp. 71–85.\n\nYudkowsky, Eliezer (2013) [Five theses, two lemmas, and a couple of strategic implications](https://intelligence.org/2013/05/05/five-theses-two-lemmas-and-a-couple-of-strategic-implications/), *Machine Intelligence Research Institute's Blog*, May 5.\n\nRelated entries\n---------------\n\n[instrumental convergence thesis](https://forum.effectivealtruism.org/tag/instrumental-convergence-thesis)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "ttscKWtYrApLz57H8",
    "name": "Oracle AI",
    "core": false,
    "slug": "oracle-ai",
    "oldSlugs": null,
    "postCount": null,
    "description": {
      "markdown": "An **oracle AI** is an AI system that only answers questions. It is a proposed solution to the [AI alignment](https://forum.effectivealtruism.org/tag/ai-alignment) problem, apparently first suggested by [Nick Bostrom](https://forum.effectivealtruism.org/tag/nick-bostrom).\n\nFurther reading\n---------------\n\nBostrom, Nick (2014) [*Superintelligence: Paths, Dangers, Strategies*](https://en.wikipedia.org/wiki/Special:BookSources/9780199678112), Oxford: Oxford University Press, pp. 145–148.\n\nMuehlhauser, Luke (2012) [A taxonomy of oracle AIs](https://www.lesswrong.com/posts/XddMs9kSGtm6L8522/a-taxonomy-of-oracle-ais), *LessWrong*, March 8.\n\nRelated entries\n---------------\n\n[sovereign AI](https://forum.effectivealtruism.org/tag/sovereign-ai) | [tool AI](https://forum.effectivealtruism.org/tag/tool-ai)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "uu4eksgjZX5Ru8XHm",
    "name": "Nuclear winter",
    "core": false,
    "slug": "nuclear-winter",
    "oldSlugs": null,
    "postCount": 31,
    "description": {
      "markdown": "**Nuclear winter** is a severe and prolonged global climatic cooling effect triggered by stratospheric smoke from firestorms following a large-scale [nuclear war](https://forum.effectivealtruism.org/tag/nuclear-warfare-1).\n\nRelated entries\n---------------\n\n[civilizational collapse](https://forum.effectivealtruism.org/tag/civilizational-collapse) | [existential risk](https://forum.effectivealtruism.org/tag/existential-risk) | [existential risk factor](https://forum.effectivealtruism.org/tag/existential-risk-factor) | [nuclear security](https://forum.effectivealtruism.org/tag/nuclear-security) | [nuclear warfare](https://forum.effectivealtruism.org/tag/nuclear-warfare-1)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "XpJJ9fdrcYH8fsHsf",
    "name": "Nuclear Threat Initiative",
    "core": false,
    "slug": "nuclear-threat-initiative",
    "oldSlugs": null,
    "postCount": 4,
    "description": {
      "markdown": "The **Nuclear Threat Initiative** is a nonprofit organization that works to prevent [global catastrophic risks](https://forum.effectivealtruism.org/tag/global-catastrophic-risk) from nuclear, radiological, chemical and biological [weapons of mass destruction](https://forum.effectivealtruism.org/tag/weapons-of-mass-destruction).\n\nEvaluation\n----------\n\nAs of July 2022, NTI's [biosecurity](https://forum.effectivealtruism.org/tag/biosecurity) program has received over $20.4 million in grants from [Open Philanthropy](https://forum.effectivealtruism.org/tag/open-philanthropy).^[\\[1\\]](#fnexhen4ca9ei)^ NTI is one of the four organizations recommended by [Founders Pledge](https://forum.effectivealtruism.org/tag/founders-pledge) in their cause report on safeguarding the [long-term future](https://forum.effectivealtruism.org/tag/long-term-future).^[\\[2\\]](#fnyz4w4p5nov)^\n\nFurther reading\n---------------\n\nRohlfing, Joan (2021) [Beyond the precipice: a new nuclear paradigm for surviving the Anthropocene](https://www.youtube.com/watch?v=zRuYzX77PK0), *Effective Altruism Global*, October 30.\n\nWiblin, Robert & Keiran Harris (2022) [Joan Rohlfing on how to avoid catastrophic nuclear blunders](https://80000hours.org/podcast/episodes/joan-rohlfing-avoiding-catastrophic-nuclear-blunders/), *80,000 Hours*, March 29.\n\nExternal links\n--------------\n\n[Nuclear Threat Initiative](https://www.nti.org/). Official website.\n\n[Apply for a job](https://www.nti.org/about/work-with-us/). \n\n[Donate to NTI](https://www.nti.org/donate/). \n\nRelated entries\n---------------\n\n[biosecurity](https://forum.effectivealtruism.org/tag/biosecurity) | [global catastrophic biological risk](https://forum.effectivealtruism.org/tag/global-catastrophic-biological-risk) | [global catastrophic risk](https://forum.effectivealtruism.org/tag/global-catastrophic-risk) | [Johns Hopkins Center for Health Security](https://forum.effectivealtruism.org/tag/johns-hopkins-center-for-health-security) | [nuclear warfare](https://forum.effectivealtruism.org/tag/nuclear-warfare-1) | [weapons of mass destruction](https://forum.effectivealtruism.org/tag/weapons-of-mass-destruction)\n\n1.  ^**[^](#fnrefexhen4ca9ei)**^\n    \n    Open Philanthropy (2021) [Grants database: Nuclear Threat Initiative](https://www.openphilanthropy.org/grants/?q=&organization-name=nuclear-threat-initiative), *Open Philanthropy*.\n    \n2.  ^**[^](#fnrefyz4w4p5nov)**^\n    \n    Halstead, John (2019) [Safeguarding the future cause area report](https://assets.ctfassets.net/x5sq5djrgbwu/5C1hNPO8RK2E3RzH9dj88M/1fd2c52ab1e534af95c25c5ebea92b49/Cause_Report_-_Safeguarding_the_Future.pdf), *Founders Pledge*, January (updated December 2020)."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "BWHXesTAEwQNmfFbN",
    "name": "Nuclear disarmament movement",
    "core": false,
    "slug": "nuclear-disarmament-movement",
    "oldSlugs": null,
    "postCount": 1,
    "description": {
      "markdown": "The **nuclear disarmament movement** is a [social movement](https://forum.effectivealtruism.org/tag/social-and-intellectual-movements) that campaigns for reducing or eliminating nuclear weapons.\n\nThe nuclear disarmament movement may be regarded as the first social movement ever to be concerned with [existential risk](https://forum.effectivealtruism.org/tag/existential-risk). Because at the time the only known major risk to humanity's long-term potential was posed by nuclear weapons, however, the concern was not framed in terms of existential risk generally, but specifically in terms of risks of [nuclear war](https://forum.effectivealtruism.org/tag/nuclear-warfare-1). As [Toby Ord](https://forum.effectivealtruism.org/users/toby_ord) writes, \"existential risk was a highly influential idea of the twentieth century. But because there was one dominant risk, it all happened under the banner of nuclear war.\"^[\\[1\\]](#fnm08ajnt2d4i)^\n\nFurther reading\n---------------\n\nWittner, Lawrence S. (2009) [*Confronting the Bomb: A Short History of the World Nuclear Disarmament Movement*](https://doi.org/10.1111/j.1468-0130.2011.00738.x), Stanford: Stanford University Press.\n\nRelated entries\n---------------\n\n[Bulletin of the Atomic Scientists](https://forum.effectivealtruism.org/tag/bulletin-of-the-atomic-scientists) | [nuclear security](https://forum.effectivealtruism.org/tag/nuclear-security) | [nuclear warfare](https://forum.effectivealtruism.org/tag/nuclear-warfare-1) | [Russell–Einstein Manifesto](https://forum.effectivealtruism.org/tag/russell-einstein-manifesto) | [social and intellectual movements](https://forum.effectivealtruism.org/tag/social-and-intellectual-movements) | [Trinity](https://forum.effectivealtruism.org/tag/trinity)\n\n1.  ^**[^](#fnrefm08ajnt2d4i)**^\n    \n    Ord, Toby (2020) [*The Precipice: Existential Risk and the Future of Humanity*](https://en.wikipedia.org/wiki/Special:BookSources/1526600218), London: Bloomsbury Publishing, p. 63."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "ZqbQmGg42ZeNabTqD",
    "name": "Applied ethics",
    "core": false,
    "slug": "applied-ethics",
    "oldSlugs": null,
    "postCount": 6,
    "description": {
      "markdown": "**Applied ethics** is the study of how agents ought to act in particular contexts. Together with [metaethics](https://forum.effectivealtruism.org/tag/metaethics) and [normative ethics](https://forum.effectivealtruism.org/tag/normative-ethics), it is one of the three main branches of [moral philosophy](https://forum.effectivealtruism.org/topics/moral-philosophy).\n\nFurther reading\n---------------\n\nMacAskill, William & Darius Meissner (2020) [Acting on utilitarianism](https://www.utilitarianism.net/acting-on-utilitarianism), *Utilitarianism*.\n\nSinger, Peter (2011) [*Practical Ethics*](https://en.wikipedia.org/wiki/Special:BookSources/9780521707688), 3rd ed., Cambridge: Cambridge University Press.\n\nRelated entries\n---------------\n\n[bioethics](https://forum.effectivealtruism.org/tag/bioethics) | [metaethics](https://forum.effectivealtruism.org/tag/metaethics) | [normative ethics](https://forum.effectivealtruism.org/tag/normative-ethics)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "Qs5ALksTSkKabHgx3",
    "name": "Normative ethics",
    "core": false,
    "slug": "normative-ethics",
    "oldSlugs": null,
    "postCount": 19,
    "description": {
      "markdown": "**Normative ethics** is the study of general moral theories and principles. Together with [metaethics](https://forum.effectivealtruism.org/tag/metaethics) and [applied ethics](https://forum.effectivealtruism.org/tag/applied-ethics), it is one of the three main branches of [moral philosophy](https://forum.effectivealtruism.org/topics/moral-philosophy).\n\nFurther reading\n---------------\n\nKagan, Shelly (1998) [*Normative Ethics*](https://en.wikipedia.org/wiki/Special:BookSources/0-8133-0845-3), Boulder, Colorado: Westview Press.\n\nRelated entries\n---------------\n\n[applied ethics](https://forum.effectivealtruism.org/tag/applied-ethics) | [axiology](https://forum.effectivealtruism.org/tag/axiology) | [consequentialism](https://forum.effectivealtruism.org/tag/consequentialism) | [metaethics](https://forum.effectivealtruism.org/tag/metaethics) | [prioritarianism](https://forum.effectivealtruism.org/tag/prioritarianism)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "HL4AwFtK7wYjeQpLN",
    "name": "Cellular Agriculture Society",
    "core": false,
    "slug": "cellular-agriculture-society",
    "oldSlugs": null,
    "postCount": null,
    "description": {
      "markdown": "The **Cellular Agriculture Society** is a nonprofit organization working to advance understanding of [cellular agriculture](https://forum.effectivealtruism.org/tag/cultured-meat).\n\nFunding\n-------\n\nAs of July 2022, Cellular Agriculture Society has received $50,000 in funding from [Effective Altruism Funds](https://forum.effectivealtruism.org/topics/effective-altruism-funds).^[\\[1\\]](#fnl8vrrthjggf)^\n\nFurther reading\n---------------\n\nAnimal Charity Evaluators (2018) [Cellular Agriculture Society review](https://animalcharityevaluators.org/charity-review/cellular-agriculture-society/), *Animal Charity Evaluators*, November.\n\nExternal links\n--------------\n\n[Cellular Agriculture Society](https://www.cellag.org/). Official website.\n\nRelated entries\n---------------\n\n[cultivated meat](https://forum.effectivealtruism.org/topics/cultivated-meat)\n\n1.  ^**[^](#fnrefl8vrrthjggf)**^\n    \n    Animal Welfare Fund (2018) [June 2018: Animal Welfare Fund grants](https://funds.effectivealtruism.org/funds/payouts/june-2018-animal-welfare-fund-grants), *Effective Altruism Funds*, June."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "YyQRdJiQMrvw43hgJ",
    "name": "Faunalytics",
    "core": false,
    "slug": "faunalytics",
    "oldSlugs": null,
    "postCount": 10,
    "description": {
      "markdown": "**Faunalytics** is a nonprofit organization that provides animal advocates with access to relevant research.\n\nEvaluation\n----------\n\n[Animal Charity Evaluators](https://forum.effectivealtruism.org/tag/animal-charity-evaluators) rates Faunalytics a \"top charity\"—one of the three organizations awarded their highest rating. ACE considers Faunalytics \"an excellent giving opportunity because of their strong programs aimed at strengthening the animal advocacy movement.\"^[\\[1\\]](#fnx2g7ugpingh)^\n\nAs of July 2022, Faunalytics has received over $130,000 in funding from [Effective Altruism Funds](https://forum.effectivealtruism.org/topics/effective-altruism-funds).^[\\[2\\]](#fnfqulwdvshu)^^[\\[3\\]](#fnhaotn85caw)^^[\\[4\\]](#fn3uiazyig35y)^^[\\[5\\]](#fngucsfiwknfu)^ \n\nFurther reading\n---------------\n\nAnimal Charity Evaluators (2021) [Faunalytics review](https://animalcharityevaluators.org/charity-review/faunalytics/), *Animal Charity Evaluators*, November.\n\nExternal links\n--------------\n\n[Faunalytics](https://faunalytics.org/). Official website.\n\n[Donate to Faunalytics](https://faunalytics.org/donate/).\n\n1.  ^**[^](#fnrefx2g7ugpingh)**^\n    \n    Animal Charity Evaluators (2021) [Faunalytics review](https://animalcharityevaluators.org/charity-review/faunalytics/), *Animal Charity Evaluators*, November.\n    \n2.  ^**[^](#fnreffqulwdvshu)**^\n    \n    Animal Welfare Fund (2018) [December 2018: Animal Welfare Fund grants](https://funds.effectivealtruism.org/funds/payouts/december-2018-animal-welfare-fund-grants), *Effective Altruism Funds*, December.\n    \n3.  ^**[^](#fnrefhaotn85caw)**^\n    \n    Animal Welfare Fund (2019) [March 2019: Animal Welfare Fund grants](https://funds.effectivealtruism.org/funds/payouts/march-2019-animal-welfare-fund-grants), *Effective Altruism Funds*, March.\n    \n4.  ^**[^](#fnref3uiazyig35y)**^\n    \n    Animal Welfare Fund (2019) [November 2019: Animal Welfare Fund grants](https://funds.effectivealtruism.org/funds/payouts/november-2019-animal-welfare-fund-grants), *Effective Altruism Funds*, November.\n    \n5.  ^**[^](#fnrefgucsfiwknfu)**^\n    \n    Animal Welfare Fund (2021) [November 2021: Animal Welfare Fund grants](https://funds.effectivealtruism.org/funds/payouts/november-2021-animal-welfare-fund-grants), *Effective Altruism Funds*, November.\n    \n6.  ^**[^](#fnrefa7m9o0e2hj)**^\n    \n    Giving What We Can (2022) [What are the best charities to donate to in 2022?](https://www.givingwhatwecan.org/best-charities-to-donate-to-2022/), *Giving What We Can*."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "XXta2uTyJjXRpS3a2",
    "name": "Fish Welfare Initiative",
    "core": false,
    "slug": "fish-welfare-initiative",
    "oldSlugs": null,
    "postCount": 8,
    "description": {
      "markdown": "The **Fish Welfare Initiative** is a nonprofit organization that conducts research on ways to improve [fish welfare](https://forum.effectivealtruism.org/tag/fish-welfare)—especially the welfare of farmed fish—and implements interventions based on this research.\n\nFunding\n-------\n\nAs of July 2022, Fish Welfare Initiative has received $345,000 in grants from [Effective Altruism Funds](https://forum.effectivealtruism.org/topics/effective-altruism-funds).^[\\[1\\]](#fn077hzww6hd7t)^^[\\[2\\]](#fnnemdo8orlzq)^^[\\[3\\]](#fnf25puu04jls)^^[\\[4\\]](#fn3782mqeldiw)^^[\\[5\\]](#fnw6cm0k29ach)^\n\nFurther reading\n---------------\n\nTorrella, Kenny (2021) [The next frontier for animal welfare: Fish](https://www.vox.com/future-perfect/22301931/fish-animal-welfare-plant-based), *Vox*, March 2.\n\nKing-Nobles, Haven & Tom Billington (2022) [Our 2021 year in review](https://www.fishwelfareinitiative.org/post/2021review), *Fish Welfare Initiative*, January 23.\n\nExternal links\n--------------\n\n[Fish Welfare Initiative](https://www.fishwelfareinitiative.org/). Official website.\n\n[Apply for a job](https://www.fishwelfareinitiative.org/jobs).\n\nRelated entries\n---------------\n\n[animal welfare](https://forum.effectivealtruism.org/tag/animal-welfare-1) | [Aquatic Life Institute](https://forum.effectivealtruism.org/tag/aquatic-life-institute) | [farmed animal welfare](https://forum.effectivealtruism.org/tag/farmed-animal-welfare) | [fish welfare](https://forum.effectivealtruism.org/tag/fish-welfare)\n\n1.  ^**[^](#fnref077hzww6hd7t)**^\n    \n    Animal Welfare Fund (2020) [July 2020: Animal Welfare Fund grants](https://funds.effectivealtruism.org/funds/payouts/july-2020-animal-welfare-fund-grants), *Effective Altruism Funds*, July. \n    \n2.  ^**[^](#fnrefnemdo8orlzq)**^\n    \n    Animal Welfare Fund (2020) [November 2020: Animal Welfare Fund grants](https://funds.effectivealtruism.org/funds/payouts/november-2020-animal-welfare-fund-grants), *Effective Altruism Funds*, November. \n    \n3.  ^**[^](#fnreff25puu04jls)**^\n    \n    Animal Welfare Fund (2021) [May 2021: Animal Welfare Fund grants](https://funds.effectivealtruism.org/funds/payouts/may-2021-animal-welfare-fund-grants), *Effective Altruism Funds*, May. \n    \n4.  ^**[^](#fnref3782mqeldiw)**^\n    \n    Animal Welfare Fund (2021) [July 2021: Animal Welfare Fund grants](https://funds.effectivealtruism.org/funds/payouts/july-2021-animal-welfare-fund-grants), *Effective Altruism Funds*, July. \n    \n5.  ^**[^](#fnrefw6cm0k29ach)**^\n    \n    Animal Welfare Fund (2021) [November 2021: Animal Welfare Fund grants](https://funds.effectivealtruism.org/funds/payouts/november-2021-animal-welfare-fund-grants), *Effective Altruism Funds*, November."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "dxXZij9HKjdHRjPka",
    "name": "Animal Ethics",
    "core": false,
    "slug": "animal-ethics",
    "oldSlugs": null,
    "postCount": 29,
    "description": {
      "markdown": "**Animal Ethics** is a nonprofit organization that works to spread anti-[speciesist](https://forum.effectivealtruism.org/tag/speciesism) messages in academia and to a general audience. It was founded in 2012.^[\\[1\\]](#fnquzknwo0pkj)^\n\nEvaluation\n----------\n\nAnimal Ethics was rated a Standout Charity by [Animal Charity Evaluators](https://forum.effectivealtruism.org/tag/animal-charity-evaluators) between December 2015 and November 2017.^[\\[2\\]](#fn9egcxgb9p7e)^ Animal Ethics is also one of the two organizations with a focus on animal welfare recommended by [Brian Tomasik](https://forum.effectivealtruism.org/tag/brian-tomasik).^[\\[3\\]](#fnwtngrj7rddi)^\n\nAs of June 2022, Animal Ethics has received $70,000 in funding from [Effective Altruism Funds](https://forum.effectivealtruism.org/topics/effective-altruism-funds).^[\\[4\\]](#fnu86z3ltco7h)^\n\nFurther reading\n---------------\n\nAnimal Charity Evaluators (2017) [Animal ethics: Comprehensive review](https://web.archive.org/web/20210423000617/https://animalcharityevaluators.org/charity-review/animal-ethics/2017-nov/#comprehensive-review), *Animal Charity Evaluators*, November.\n\nExternal links\n--------------\n\n[Animal Ethics](https://www.animal-ethics.org/). Official website.\n\n[Animal Ethics](https://forum.effectivealtruism.org/users/animal_ethics). [Effective Altruism Forum](https://forum.effectivealtruism.org/tag/effective-altruism-forum-1) account.\n\nRelated entries\n---------------\n\n[speciesism](https://forum.effectivealtruism.org/tag/speciesism)\n\n1.  ^**[^](#fnrefquzknwo0pkj)**^\n    \n    Smith, Allison (2015) [Conversation with Leah Mckelvie and Oscar Horta, Animal Ethics](https://animalcharityevaluators.org/charity-reviews/charity-conversations/leah-mckelvie-and-oscar-horta/), *Animal Charity Evaluators*, August 19.\n    \n2.  ^**[^](#fnref9egcxgb9p7e)**^\n    \n    Animal Charity Evaluators (2017) [Animal ethics: Comprehensive review](https://web.archive.org/web/20210423000617/https://animalcharityevaluators.org/charity-review/animal-ethics/2017-nov/#comprehensive-review), *Animal Charity Evaluators*, November.\n    \n3.  ^**[^](#fnrefwtngrj7rddi)**^\n    \n    Tomasik, Brian (2015) [Why I support the Humane Slaughter Association](https://reducing-suffering.org/why-i-support-the-humane-slaughter-association/), *Essays on Reducing Suffering*, February 24.\n    \n4.  ^**[^](#fnrefu86z3ltco7h)**^\n    \n    Animal Welfare Fund (2018) [June 2018: Animal Welfare Fund grants](https://funds.effectivealtruism.org/funds/payouts/june-2018-animal-welfare-fund-grants), *Effective Altruism Funds*, June."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "Ato38zrg3c8hfMstw",
    "name": "Anima International",
    "core": false,
    "slug": "anima-international",
    "oldSlugs": null,
    "postCount": 6,
    "description": {
      "markdown": "**Anima International** is an umbrella of European organizations focused on improving welfare standards for [farm animals](https://forum.effectivealtruism.org/tag/farmed-animal-welfare) through corporate and media outreach, undercover investigations, and legislative work.\n\nHistory\n-------\n\nAnima International evolved as a collaboration between the Scandinavian vegan advocacy group **Anima** and the Polish investigations group **Open Cages** \\[*Otwarte Klatki*\\]. After working together for a number of years, the organizations formally merged in 2018.^[\\[1\\]](#fno3fhfsygq2p)^ As of 2022, Anima International operates in seven different European countries.^[\\[2\\]](#fnjecurodp7pb)^\n\nEvaluation\n----------\n\n[Animal Charity Evaluators](https://forum.effectivealtruism.org/tag/animal-charity-evaluators) considers Anima International \"an excellent giving opportunity due to their impactful programs and their strong track record of building the capacity of the movement in relatively neglected areas in Eastern Europe.\"^[\\[3\\]](#fna3a1o0ui4u)^ As of July 2022, Anima, Anima International, and Open Cages have received almost $9.4 million in funding from [Open Philanthropy](https://forum.effectivealtruism.org/tag/open-philanthropy),^[\\[4\\]](#fnxt50vasyw39)^ and nearly $190,000 from [Effective Altruism Funds](https://forum.effectivealtruism.org/topics/effective-altruism-funds).^[\\[5\\]](#fnp9q40fyxcif)^^[\\[6\\]](#fncd0z0dzxnc9)^^[\\[7\\]](#fnw63lm98fkvb)^^[\\[8\\]](#fnh0w0gyz1sd)^\n\nFurther reading\n---------------\n\nAnimal Charity Evaluators (2020) [Anima International review](https://animalcharityevaluators.org/charity-review/anima/), *Animal Charity Evaluators*, November.\n\nExternal links\n--------------\n\n[Anima International](https://animainternational.org/). Official website.\n\n[Apply for a job](https://animainternational.org/get-involved/jobs).\n\n[Donate to Anima International](https://animainternational.org/donate).\n\nRelated entries\n---------------\n\n[farmed animal welfare](https://forum.effectivealtruism.org/tag/farmed-animal-welfare)\n\n1.  ^**[^](#fnrefo3fhfsygq2p)**^\n    \n    Anima International (2021) [History of Anima International](https://animainternational.org/about/history/), *Anima International*.\n    \n2.  ^**[^](#fnrefjecurodp7pb)**^\n    \n    Anima International (2022) [Organisations of Anima International](https://animainternational.org/about/organisations/), *Anima International*.\n    \n3.  ^**[^](#fnrefa3a1o0ui4u)**^\n    \n    Animal Charity Evaluators (2020) [Anima International review](https://animalcharityevaluators.org/charity-review/anima/), *Animal Charity Evaluators*, November.\n    \n4.  ^**[^](#fnrefxt50vasyw39)**^\n    \n    Open Philanthropy (2022) [Grants database: Anima, Anima International, and Otwarte Klatki](https://www.openphilanthropy.org/grants/?q=&organization-name=anima&organization-name=anima-international&organization-name=otwarte-klatki), *Open Philanthropy*.\n    \n5.  ^**[^](#fnrefp9q40fyxcif)**^\n    \n    Animal Welfare Fund (2017) [April 2017: Animal Welfare Fund grants](https://funds.effectivealtruism.org/funds/payouts/april-2017-animal-welfare-fund-grants), *Effective Altruism Funds*, April.\n    \n6.  ^**[^](#fnrefcd0z0dzxnc9)**^\n    \n    Animal Welfare Fund (2019) [March 2019: Animal Welfare Fund grants](https://funds.effectivealtruism.org/funds/payouts/march-2019-animal-welfare-fund-grants), *Effective Altruism Funds*, March.\n    \n7.  ^**[^](#fnrefw63lm98fkvb)**^\n    \n    Animal Welfare Fund (2020) [November 2020: Animal Welfare Fund grants](https://funds.effectivealtruism.org/funds/payouts/november-2020-animal-welfare-fund-grants), *Effective Altruism Funds*, November.\n    \n8.  ^**[^](#fnrefh0w0gyz1sd)**^\n    \n    Animal Welfare Fund (2021) [July 2021: Animal Welfare Fund grants](https://funds.effectivealtruism.org/funds/payouts/july-2021-animal-welfare-fund-grants), *Effective Altruism Funds*, July."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "juMz8547qhTKmKNsd",
    "name": "Aquatic Life Institute",
    "core": false,
    "slug": "aquatic-life-institute",
    "oldSlugs": null,
    "postCount": 1,
    "description": {
      "markdown": "The **Aquatic Life Institute** is a nonprofit organization that funds research related to the welfare of aquatic animals and carries out interventions informed by this research.\n\nFunding\n-------\n\nAs of June 2022, the Aquatic Life Institute has received $80,000 in funding from [Effective Altruism Funds](https://forum.effectivealtruism.org/topics/effective-altruism-funds).^[\\[1\\]](#fnvhen8kq8pwd)^\n\nFurther reading\n---------------\n\nTorrella, Kenny (2021) [The next frontier for animal welfare: Fish](https://www.vox.com/future-perfect/22301931/fish-animal-welfare-plant-based), *Vox*, March 2.\n\nExternal links\n--------------\n\n[Aquatic Life Institute](https://ali.fish/). Official website.\n\n[Apply for a job](https://ali.fish/we-are-hiring).\n\nRelated entries\n---------------\n\n[animal welfare](https://forum.effectivealtruism.org/tag/animal-welfare-1) | [fish welfare](https://forum.effectivealtruism.org/tag/fish-welfare) | [Fish Welfare Initiative](https://forum.effectivealtruism.org/tag/fish-welfare-initiative) | [wild animal welfare](https://forum.effectivealtruism.org/tag/wild-animal-welfare)\n\n1.  ^**[^](#fnrefvhen8kq8pwd)**^\n    \n    Animal Welfare Fund (2021) [November 2021: Animal Welfare Fund grants](https://funds.effectivealtruism.org/funds/payouts/november-2021-animal-welfare-fund-grants), *Effective Altruism Funds*, November."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "DR6M63tJ9tAiAuc4i",
    "name": "Norman Borlaug",
    "core": false,
    "slug": "norman-borlaug",
    "oldSlugs": null,
    "postCount": null,
    "description": {
      "markdown": "**Norman Ernest Borlaug** (25 March 1914 - 12 September 2009) was an American agricultural scientist who won the Nobel Prize for his work breeding high-yield varieties of wheat. The introduction of these new crops into developing countries, with the associated increase in agricultural productivity and reduction in deaths from starvation, arguably make Borlaug responsible for saving an extraordinary number of human lives.^[\\[1\\]](#fn6ucmdrlncnb)^\n\nFurther reading\n---------------\n\nBailey, Ronald (2009) [Norman Borlaug: the man who saved more human lives than any other has died](https://reason.com/2009/09/13/norman-borlaug-the-man-who-sav/), *Reason.Com*, September 13.\n\nEasterbrook, Gregg (1997) [Forgotten benefactor of humanity](https://www.theatlantic.com/magazine/archive/1997/01/forgotten-benefactor-of-humanity/306101/), *The Atlantic*, January.\n\nRelated entries\n---------------\n\n[global poverty](https://forum.effectivealtruism.org/tag/global-poverty)\n\n1.  ^**[^](#fnref6ucmdrlncnb)**^\n    \n    [Toby Ord](https://forum.effectivealtruism.org/tag/toby-ord) estimates that Borlaug is responsible for saving \"tens to hundreds of millions\" of lives. See Ord, Toby (2020) [*The Precipice: Existential Risk and the Future of Humanity*](https://en.wikipedia.org/wiki/Special:BookSources/1-5266-0021-8), London: Bloomsbury Publishing, p. 345."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "4Cpbg8Jxjx3xKijFe",
    "name": "Negative utilitarianism",
    "core": false,
    "slug": "negative-utilitarianism",
    "oldSlugs": null,
    "postCount": 13,
    "description": {
      "markdown": "**Negative utilitarianism** (**NU**) is a version of [utilitarianism](https://forum.effectivealtruism.org/tag/utilitarianism) whose standard account holds that an act is morally right if and only if it leads to less suffering than any of its alternatives. NU was originally developed as an alternative to [classical utilitarianism](https://forum.effectivealtruism.org/tag/classical-utilitarianism), which regards suffering and happiness as equally important, and is a leading example of a [suffering-focused view](https://forum.effectivealtruism.org/tag/suffering-focused-ethics), a broader family of ethical positions that assign primary—though not necessarily exclusive or overriding—moral importance to the alleviation of suffering.\n\nTypes of negative utilitarianism\n--------------------------------\n\nAs noted, the standard form of NU requires agents to minimize suffering. However, several variants to this canonical version have been proposed. These variants result from revising standard NU along one or more dimensions.\n\nThe first and most commonly discussed dimension of variation concerns the relative moral weight accorded to suffering and happiness. Standard NU may be regarded as a \"strong\" form of NU, holding that no amount of happiness can ever count for more than any amount of suffering. By contrast, \"weak\" versions of NU hold instead that a given quantity of suffering counts for more than a corresponding quantity of happiness, but accept that large enough quantities of happiness can in principle outweigh any quantity of suffering.^[\\[1\\]](#fndrfkstonsb)^^[\\[2\\]](#fnucvaywvdoj)^^[\\[3\\]](#fn6uejj4nbzyg)^^[\\[4\\]](#fnlxvot47rbmn)^ Strong NU views may be further subdivided into *lexical NU* and *absolute NU*, which either affirm or deny, respectively, that happiness counts for something.^[\\[3\\]](#fn6uejj4nbzyg)^ On strong lexical NU, of two outcomes equally unpleasant, one counts for more than the other if it is the more pleasant of the two; whereas on absolute strong NU both outcomes count equally. Between strong lexical NU and weak NU, there is room for an intermediate or hybrid form of NU, sometimes called *lexical threshold NU* ,^[\\[3\\]](#fn6uejj4nbzyg)^^[\\[5\\]](#fnwcb3nt87a1d)^ according to which there is some amount of suffering that no amount of happiness can outweigh, but otherwise suffering can be outweighed by a large enough amount of happiness.\n\nA second dimension of variation concerns whether or not NU is formulated in [hedonistic](https://forum.effectivealtruism.org/tag/hedonism) terms. Standard NU is hedonistic in that it makes a claim about the relative moral weight of suffering and happiness. But versions of NU have also been formulated in terms of preferences, rather than hedonic states. These *preferentist* NU views hold that the frustration of a preference counts for more than its satisfaction. (How much more will depend on the type of NU–strong absolute, strong lexical, lexical threshold, or weak–that preferentism is combined with.) More generally, NU may be presented as a broader theory about negative and positive [wellbeing](https://forum.effectivealtruism.org/tag/wellbeing): on this variant, what is bad for a person counts for more than what is good for a person–regardless of whether these goods and bads are hedonic states, preferences, something else, or a combination thereof.\n\nA third dimension of variation relates to the location of the boundary demarcating the states which are morally contrasted. Standard NU holds that the location of this boundary coincides with hedonic neutrality. But some hedonistic negative utilitarians have instead defended a view on which the boundary is *below* neutrality. On this view, sometimes called \"critical-level (hedonistic) NU\", the contrast is not between suffering and happiness, but rather between intense enough suffering and other hedonic states. This view also admits a formulation in terms of preferences, or wellbeing more generally.\n\nFinally, different versions of NU may be obtained depending on whether NU is regarded as a *criterion of rightness* or as a *decision procedure*. Standard NU is generally understood to provide a criterion of rightness, that is, as a specification of the conditions under which acts are right or wrong. But NU may instead be interpreted as a decision procedure, that is, as a practical guide for choosing how to act. The claim here is that agents deliberating about what to do should strive to minimize suffering. Someone who is not a standard NU may still defend NU as a decision procedure if they think that following this procedure is more likely to result in acts that better conform to the requirements of morality, whatever those are. This view is analogous to some forms of [prioritarianism](https://forum.effectivealtruism.org/tag/prioritarianism) or egalitarianism, where outcomes that benefit the worst off, or that promote a more equal distribution of resources, are favored not because intrinsic value is placed on priority or equality, but instead because following these principles generally produces better outcomes.\n\nFurther reading\n---------------\n\nGustafsson, Johan (2022) [Against negative utilitarianism](https://johanegustafsson.net/papers/against-negative-utilitarianism.pdf), *Johan Gustafsson’s Website*, June 6.\n\nKnutsson, Simon (2018) [Thoughts on Ord’s “Why I’m not a negative utilitarian”](https://www.simonknutsson.com/thoughts-on-ords-why-im-not-a-negative-utilitarian), *Simon Knutsson's Blog*, July.\n\nOrd, Toby (2013) [Why I’m not a negative utilitarian](http://www.amirrorclear.net/academic/ideas/negative-utilitarianism/), *Toby Ord’s Blog*, February 28.\n\nTomasik, Brian (2013) [Three types of negative utilitarianism](https://reducing-suffering.org/three-types-of-negative-utilitarianism/), *Essays on Reducing Suffering*, March 23.\n\nVinding, Magnus (2022) [Point-by-point critique of “Why I’m not a negative utilitarian”](https://centerforreducingsuffering.org/point-by-point-critique-of-why-im-not-a-negative-utilitarian/), *Center for Reducing Suffering*, May 30.\n\nVinding, Magnus (2022) [Reply to Gustafsson’s “Against negative utilitarianism”](https://centerforreducingsuffering.org/reply-to-gustafsson/), *Center for Reducing Suffering*, June 7.\n\nRelated entries\n---------------\n\n[axiology](https://forum.effectivealtruism.org/tag/axiology) | [normative ethics](https://forum.effectivealtruism.org/tag/normative-ethics) | [s-risk](https://forum.effectivealtruism.org/tag/s-risk) | [suffering-focused ethics](https://forum.effectivealtruism.org/tag/suffering-focused-ethics) | [utilitarianism](https://forum.effectivealtruism.org/tag/utilitarianism)\n\n1.  ^**[^](#fnrefdrfkstonsb)**^\n    \n    Griffin, James (1979) [Is unhappiness morally more important than happiness?](http://doi.org/10.2307/2219182), *The Philosophical Quarterly*, vol. 29, pp. 47–55.\n    \n2.  ^**[^](#fnrefucvaywvdoj)**^\n    \n    Arrhenius, Gustaf & Krister Bykvist (1995) *Future Generations and Interpersonal Compensations: Moral Aspects of Energy Use*, Uppsala: Uppsala University.\n    \n3.  ^**[^](#fnref6uejj4nbzyg)**^\n    \n    Ord, Toby (2013) [Why I’m not a negative utilitarian](http://www.amirrorclear.net/academic/ideas/negative-utilitarianism/), *Toby Ord’s Blog*, February 28.\n    \n4.  ^**[^](#fnreflxvot47rbmn)**^\n    \n    Knutsson, Simon (2019) [The world destruction argument](http://doi.org/10.1080/0020174X.2019.1658631), *Inquiry*, pp. 1–20.\n    \n5.  ^**[^](#fnrefwcb3nt87a1d)**^\n    \n    Tomasik, Brian (2013) [Three types of negative utilitarianism](https://reducing-suffering.org/three-types-of-negative-utilitarianism/), *Essays on Reducing Suffering*, March 23."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "X6ktJyvZu4BSCviZF",
    "name": "Natural existential risk",
    "core": false,
    "slug": "natural-existential-risk",
    "oldSlugs": [
      "natural-existential-risks"
    ],
    "postCount": 2,
    "description": {
      "markdown": "A **natural existential risk** is an [existential risk](https://forum.effectivealtruism.org/tag/existential-risk) arising from natural processes rather than from intentional or accidental human activity.\n\nUnderlying natural phenomena have been causing extinction on earth since life began. Asteroid collisions can result in mass extinction by causing particulate matter to block out sunlight and disrupt plant life for several years. Super-volcanoes can cause similar harm to agriculture. More exotic cosmological phenomena, like gamma-ray bursts, are conjectured to pose a natural existential risk.\n\nWe have a fairly good sense of the frequency of such risks, because of the historical record. As a consequence, we can estimate that they are relatively unlikely to cause human extinction in any given century. For this reason, it is generally believed that [anthropogenic existential risks](https://forum.effectivealtruism.org/tag/anthropogenic-existential-risks) pose a more serious threat than do natural risks.\n\nFurther reading\n---------------\n\nCotton-Barratt, Owen *et al.* (2016) [Global catastrophic risks 2016](http://globalprioritiesproject.org/2016/04/global-catastrophic-risks-2016/), Annual Report, Global Priorities Project.  \n*A report focusing on natural catastrophic risks, as well as other types of global catastrophic risk.*\n\nSnyder-Beattie, Andrew, Toby Ord & Michael B. Bonsall (2019) [An upper bound for the background rate of human extinction](http://doi.org/10.1038/s41598-019-47540-7), Scientific Reports, vol. 9, pp. 1–9."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "DoQjDfsYwM4PdW8J8",
    "name": "Médecins Sans Frontières",
    "core": false,
    "slug": "medecins-sans-frontieres",
    "oldSlugs": null,
    "postCount": 1,
    "description": {
      "markdown": "**Médecins Sans Frontières** is an international medical organization focused on providing care to individuals at risk from violent conflict or endemic disease.\n\nFurther reading\n---------------\n\nGiveWell (2012) [Doctors Without Borders (Médecins Sans Frontières, MSF)](https://www.givewell.org/international/charities/doctors-without-borders), *GiveWell*, October.\n\nSánchez, Sebastián (2019) [Timeline of Médecins Sans Frontières](https://timelines.issarice.com/wiki/Timeline_of_M%C3%A9decins_Sans_Fronti%C3%A8res), *Timelines Wiki*, April 1 (updated 20 March 2021‎).\n\nExternal links\n--------------\n\n[Médecins Sans Frontières](https://www.msf.org/). Official website.\n\n[Apply for a job](https://www.msf.org/work-msf)."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "Y7DckZtXjsddZykpa",
    "name": "Movement collapse",
    "core": false,
    "slug": "movement-collapse",
    "oldSlugs": null,
    "postCount": 9,
    "description": {
      "markdown": "**Movement collapse** is the process through which [social or intellectual movements](https://forum.effectivealtruism.org/tag/social-and-intellectual-movements), including the [effective altruism](https://forum.effectivealtruism.org/topics/effective-altruism) movement, can become extinct or otherwise lose their potential. **Movement resilience** is a movement's ability to resist or recover from collapse. \n\nFurther reading\n---------------\n\nAird, Michael (2020) [Collection of EA analyses of how social social movements rise, fall, can be influential, etc.](https://forum.effectivealtruism.org/posts/EMKf4Gyee7BsY2RP8/michaela-s-shortform?commentId=ta7dfYdgqFza4bnKp), *Effective Altruism Forum*, March 26.  \n*Many additional resources on this topic or related topics.*\n\nBaron, Rebecca (2019) [Movement collapse scenarios](https://forum.effectivealtruism.org/posts/KeBgeY8XvYb3pbFRA/movement-collapse-scenarios), *Effective Altruism Forum*, August 27.\n\nSempere, Nuño (2019) [Why do social movements fail: two concrete examples](https://forum.effectivealtruism.org/posts/7Pxx7kSQejX2MM2tE/why-do-social-movements-fail-two-concrete-examples), *Effective Altruism Forum*, October 4.\n\nRelated entries\n---------------\n\n[building effective altruism](https://forum.effectivealtruism.org/tag/building-effective-altruism-1) | [civilizational collapse](https://forum.effectivealtruism.org/topics/civilizational-collapse) | [social and intellectual movements](https://forum.effectivealtruism.org/tag/social-and-intellectual-movements) | [value drift](https://forum.effectivealtruism.org/tag/value-drift) | [value of movement growth](https://forum.effectivealtruism.org/tag/value-of-movement-growth)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "k6yzxfcz7DWzkWMrP",
    "name": "Motivation selection method",
    "core": false,
    "slug": "motivation-selection-method",
    "oldSlugs": [
      "motivation-selection-methods"
    ],
    "postCount": null,
    "description": {
      "markdown": "A **motivation selection method** is method that attempts to prevent undesirable outcomes from advanced [artificial intelligence](https://forum.effectivealtruism.org/tag/artificial-intelligence) by influencing what the AI wants to do. Motivation selection methods encompass direct specification, domesticity, [indirect normativity](https://forum.effectivealtruism.org/tag/indirect-normativity), and augmentation.^[\\[1\\]](#fn2mk6eej72fc)^ Motivation selection methods may be contrasted with [capability control methods](https://forum.effectivealtruism.org/tag/capability-control-method), which attempt instead to restrict what the AI can do.\n\nFurther reading\n---------------\n\nBostrom, Nick (2014) [*Superintelligence: Paths, Dangers, Strategies*](https://en.wikipedia.org/wiki/Special:BookSources/9780199678112), Oxford: Oxford University Press, pp. 138-143.\n\nRelated entries\n---------------\n\n[AI alignment](https://forum.effectivealtruism.org/tag/ai-alignment) | [capability control method](https://forum.effectivealtruism.org/tag/capability-control-method) | [indirect normativity](https://forum.effectivealtruism.org/tag/indirect-normativity)\n\n1.  ^**[^](#fnref2mk6eej72fc)**^\n    \n    Bostrom, Nick (2014) [*Superintelligence: Paths, Dangers, Strategies*](https://en.wikipedia.org/wiki/Special:BookSources/9780199678112), Oxford: Oxford University Press, pp. 138-143."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "ueaaDcHsCBcA4z6g7",
    "name": "Moral trade",
    "core": false,
    "slug": "moral-trade",
    "oldSlugs": null,
    "postCount": 9,
    "description": {
      "markdown": "**Moral trade** is the process where individuals or groups with different moral views agree to take actions or exchange resources in order to bring about outcomes which are better from the perspective of everyone involved.\n\nTake the following example: Greg's moral view assigns more weight to the current plight of nonhuman animals than it does to extreme poverty, and Paul's moral view assigns more weight to poverty than to non-human animals. Greg doesn't currently make donations to anti-poverty charities, but believes it would be better if he did, and Paul isn't vegetarian, but believes it would be better if he was. Greg has a strong moral preference for Paul to be a vegetarian, and Paul has a strong moral preference for Greg to donate more to anti-poverty charities. They propose a trade: Paul will become a vegetarian, if Greg agrees to give up some basic luxuries and give 1% of his income to anti-poverty charity. This outcome is better from the view of both Paul and Greg's moral theories.\n\nMoral trade allows new opportunities for coordinated action that are morally Pareto optimal^[\\[1\\]](#fnznmhb2inpyq)^ —that is, situations where the coordinated action is better by the view of all parties, than the alternative.\n\nFurther reading\n---------------\n\nDonnelly, Ruairí (2017) [Moral trade](https://forum.effectivealtruism.org/posts/5x8Qhy4bB8Arj5SBS/ruairi-donnelly-moral-trade), *Effective Altruism Global*, August 11.\n\nKaufman, Jeff (2015) [Ethics trading](https://www.jefftk.com/p/ethics-trading), *Jeff Kaufman’s Blog*, January 24.\n\nOrd, Toby (2015) [Moral trade](http://doi.org/10.1086/682187), *Ethics*, vol. 126, pp. 118–138.\n\nRelated entries\n---------------\n\n[altruistic coordination](https://forum.effectivealtruism.org/tag/altruistic-coordination) | [building effective altruism](https://forum.effectivealtruism.org/tag/building-effective-altruism-1) | [epistemic deference](https://forum.effectivealtruism.org/tag/epistemic-deference) | [moral advocacy](https://forum.effectivealtruism.org/tag/moral-advocacy) | [moral cooperation](https://forum.effectivealtruism.org/tag/moral-cooperation) | [moral uncertainty](https://forum.effectivealtruism.org/tag/moral-uncertainty) | [philanthropic coordination](https://forum.effectivealtruism.org/tag/philanthropic-coordination)\n\n1.  ^**[^](#fnrefznmhb2inpyq)**^\n    \n    Wikipedia (2002) [Pareto efficiency](https://en.wikipedia.org/wiki/Pareto_efficiency), *Wikipedia*, March 22 (updated 8 May 2021‎)."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "fgMSXHf7N5HWADKWy",
    "name": "Moral offsetting",
    "core": false,
    "slug": "moral-offsetting",
    "oldSlugs": null,
    "postCount": 15,
    "description": {
      "markdown": "**Moral offsetting** (sometimes called **ethical offsetting**) is the practice of undoing wrongs caused in one area by donating to charities working in those areas or engaging in other relevant acts of altruism.\n\nFurther reading\n---------------\n\nAlexander, Scott (2015) [Ethics offsets](https://slatestarcodex.com/2015/01/04/ethics-offsets/), *Slate Star Codex*, January 4.\n\nFoerster, Thomas (2019) [Moral offsetting](http://doi.org/10.1093/pq/pqy068), *The Philosophical Quarterly*, vol. 69, pp. 617–635.\n\nFoerster, Thomas (2019) [Moral offsetting bibliography](https://vaccha.com/moral-offsetting/), *Vacchablogga*, March 3.\n\nLevy, Neil (2015) [Moral offsetting](http://doi.org/10.5840/enviroethics19824322), *Practical Ethics*, January 7.\n\nZabel, Claire (2016) [Ethical offsetting is antithetical to EA](https://forum.effectivealtruism.org/posts/Yix7BzSQLJ9TYaodG/ethical-offsetting-is-antithetical-to-ea), *Effective Altruism Forum*, January 5."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "x7DX53AJSsPnrkiad",
    "name": "Moral cooperation",
    "core": false,
    "slug": "moral-cooperation",
    "oldSlugs": null,
    "postCount": 15,
    "description": {
      "markdown": "**Moral cooperation** is the process where groups with different moral views act together to attain mutual benefits.\n\nFurther reading\n---------------\n\nOesterheld, Caspar (2017) [Multiverse-wide cooperation via correlated decision making](https://longtermrisk.org/multiverse-wide-cooperation-via-correlated-decision-making/), *Center on Long-Term Risk*, August 10.\n\nTomasik, Brian (2015) [Reasons to be nice to other value systems](https://longtermrisk.org/reasons-to-be-nice-to-other-value-systems/), *Center on Long-Term Risk*, August 29 (updated 17 October 2017).\n\nRelated entries\n---------------\n\n[acausal trade](https://forum.effectivealtruism.org/tag/acausal-trade) | [altruistic coordination](https://forum.effectivealtruism.org/tag/altruistic-coordination) | [moral trade](https://forum.effectivealtruism.org/tag/moral-trade) | [philanthropic coordination](https://forum.effectivealtruism.org/tag/philanthropic-coordination)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "e3GZCACefBjXSxgJ9",
    "name": "Metaculus",
    "core": false,
    "slug": "metaculus",
    "oldSlugs": null,
    "postCount": 40,
    "description": {
      "markdown": "**Metaculus** is a reputation-based site for soliciting and aggregating [predictions](https://forum.effectivealtruism.org/tag/forecasting). It was founded in November 2015 by astrophysicist Anthony Aguirre, cosmologist Greg Laughlin and data scientist Max Wainwright.^[\\[1\\]](#fn2zjz0gnpwnl)^^[\\[2\\]](#fnkofamnqbtlc)^\n\nFunding\n-------\n\nAs of August 2022, Metaculus has received $5.6 million in grants from [Open Philanthropy](https://forum.effectivealtruism.org/topics/open-philanthropy),^[\\[3\\]](#fnu7o72mvqc2a)^ and over $300,000 from [Effective Altruism Funds](https://forum.effectivealtruism.org/topics/effective-altruism-funds).^[\\[4\\]](#fnywiphoqmgy)^\n\nFurther reading\n---------------\n\nAguirre, Anthony (2021) [A primer on the Metaculus scoring rule](https://metaculus.medium.com/a-primer-on-the-metaculus-scoring-rule-eb9a974cd204), *Medium*, February 27.\n\ndan (2020) [A preliminary look at Metaculus and expert forecasts](https://www.metaculus.com/news/2020/06/02/LRT/), *Metaculus*, June 2.\n\nHobbhahn, Marius (2022) [How good were our AI timeline predictions so far?](https://www.metaculus.com/notebooks/10624/how-good-were-our-ai-timeline-predictions-so-far/), *Metaculus*, April 10.\n\nExternal links\n--------------\n\n[Metaculus](https://www.metaculus.com/ ). Official website.\n\n[Apply for a job](https://apply.workable.com/metaculus/).\n\nRelated entries\n---------------\n\n[AI forecasting](https://forum.effectivealtruism.org/tag/ai-forecasting) | [forecasting](https://forum.effectivealtruism.org/tag/forecasting) | [prediction markets](https://forum.effectivealtruism.org/tag/prediction-markets)\n\n1.  ^**[^](#fnref2zjz0gnpwnl)**^\n    \n    Mann, Adam (2016) [The power of prediction markets](http://doi.org/10.1038/538308a), *Nature*, October 18.\n    \n2.  ^**[^](#fnrefkofamnqbtlc)**^\n    \n    Shelton, Jim (2016) [Metaculus: A prediction website with an eye on science and technology](https://news.yale.edu/2016/11/02/metaculus-prediction-website-eye-science-and-technology), *YaleNews*, November 2.\n    \n3.  ^**[^](#fnrefu7o72mvqc2a)**^\n    \n    Open Philanthropy (2022) [Grants database: Metaculus](https://www.openphilanthropy.org/grants/?q=&organization-name=metaculus), *Open Philanthropy.*\n    \n4.  ^**[^](#fnrefywiphoqmgy)**^\n    \n    Effective Altruism Infrastructure Fund (2021) [May-August 2021: EA Infrastructure Fund grants](https://funds.effectivealtruism.org/funds/payouts/may-august-2021-ea-infrastructure-fund-grants), *Effective Altruism Funds*, August."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "9sPxLLjCXHGQdtwpJ",
    "name": "Mercy for Animals",
    "core": false,
    "slug": "mercy-for-animals",
    "oldSlugs": null,
    "postCount": 6,
    "description": {
      "markdown": "**Mercy for Animals** (MFA) is an animal protection organization that conducts undercover investigations of factory farms and engages in outreach activities to promote [veganism](https://forum.effectivealtruism.org/tag/dietary-change).\n\nEvaluation\n----------\n\nMFA was rated a Top Charity by [Animal Charity Evaluators](https://forum.effectivealtruism.org/tag/animal-charity-evaluators) from May 2014 to November 2017. ACE currently rates MFA a Standout Charity.^[\\[1\\]](#fnhvmju0ys84)^\n\nAs of July 2022, MFA has received over $13.8 million in funding from [Open Philanthropy](https://forum.effectivealtruism.org/tag/open-philanthropy).^[\\[2\\]](#fnyb3hrvxhp3r)^\n\nFurther reading\n---------------\n\nRunkle, Nathan & Gene Stone (2017) [*Mercy for Animals: One Man’s Quest to Inspire Compassion, and Improve the Lives of Farm Animals*](https://en.wikipedia.org/wiki/Special:BookSources/978-0-399-57405-4), New York: Avery.\n\nSánchez, Sebastián (2018) [Timeline of Mercy For Animals](https://timelines.issarice.com/wiki/Timeline_of_Mercy_For_Animals), *Timelines Wiki*, July 2 (updated 18 March 2021‎).\n\nExternal links\n--------------\n\n[Mercy for Animals](https://mercyforanimals.org/). Official website.\n\n[Apply for a job](https://mercyforanimals.org/jobs/).\n\n[Donate to Mercy for Animals](https://act.mercyforanimals.org/page/she-needs-you/donation-form-13).\n\nRelated links\n-------------\n\n[dietary change](https://forum.effectivealtruism.org/tag/dietary-change) | [farmed animal welfare](https://forum.effectivealtruism.org/tag/farmed-animal-welfare)\n\n1.  ^**[^](#fnrefhvmju0ys84)**^\n    \n    Animal Charity Evaluators (2021) [Mercy For Animals review](https://animalcharityevaluators.org/charity-review/mercy-for-animals/), *Animal Charity Evaluators*, November.\n    \n2.  ^**[^](#fnrefyb3hrvxhp3r)**^\n    \n     Open Philanthropy (2022) [Grants database: Mercy For Animals](https://www.openphilanthropy.org/grants/?q=&organization-name=mercy-for-animals), *Open Philanthropy*."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "kddYsCBvCAm5Bcm2H",
    "name": "Donation choice",
    "core": false,
    "slug": "donation-choice",
    "oldSlugs": [
      "donation-choice"
    ],
    "postCount": 108,
    "description": {
      "markdown": "The **donation choice** tag is used for posts about donation choices such as whether, how much, where and when to donate.\n\nRelated entries\n---------------\n\n[career choice](https://forum.effectivealtruism.org/tag/career-choice) | [diminishing returns](https://forum.effectivealtruism.org/tag/diminishing-returns) | [donation writeup](https://forum.effectivealtruism.org/tag/donation-writeup) | [earning to give](https://forum.effectivealtruism.org/tag/earning-to-give) | [personal finance](https://forum.effectivealtruism.org/tag/personal-finance) | [philanthropic coordination](https://forum.effectivealtruism.org/tag/philanthropic-coordination) | [philanthropic diversification](https://forum.effectivealtruism.org/tag/philanthropic-diversification) | [room for more funding](https://forum.effectivealtruism.org/tag/room-for-more-funding) | [timing of philanthropy](https://forum.effectivealtruism.org/tag/timing-of-philanthropy)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "FspXpkminZipsFoTr",
    "name": "Volunteering",
    "core": false,
    "slug": "volunteering",
    "oldSlugs": null,
    "postCount": 40,
    "description": {
      "markdown": "Use this tag for posts discussing **volunteering** and tips for how to volunteer or manage volunteers.\n\nFor posts that seek volunteers for a project, use the [**take action**](https://forum.effectivealtruism.org/tag/take-action) tag."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "HX23NQt69iNnq66C6",
    "name": "Mass distribution of long-lasting insecticide-treated nets",
    "core": false,
    "slug": "mass-distribution-of-long-lasting-insecticide-treated-nets",
    "oldSlugs": null,
    "postCount": 1,
    "description": {
      "markdown": "**Mass distribution of long-lasting insecticide-treated nets** (LLINs) is an intervention against [malaria](https://forum.effectivealtruism.org/tag/malaria) consisting of the free distribution of LLINs to achieve universal ownership within a population. There is strong evidence that LLINs prevent malaria.\n\nFurther reading\n---------------\n\nGiveWell (2013) [Mass distribution of long-lasting insecticide-treated nets (LLINs)](https://www.givewell.org/international/technical/programs/insecticide-treated-nets), *GiveWell*, November (updated March 2018)."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "LG6x3h3pL6PXysNKY",
    "name": "Marginal charity",
    "core": false,
    "slug": "marginal-charity",
    "oldSlugs": [
      "marginal-charity"
    ],
    "postCount": null,
    "description": {
      "markdown": "**Marginal charity** is the idea that individuals can have the most social gain by unit of private loss by shifting their choices marginally in a prosocial direction. A person's choices are by default close to the private optimum, which sometimes diverges significantly from the social optimum. Thus, slight deviations away from the latter and toward the former should achieve outsized social gains. The expression \"marginal charity\" was introduced by [Robin Hanson](https://forum.effectivealtruism.org/tag/robin-hanson),^[\\[1\\]](#fnew0k4spqkwu)^ though as the author notes, the idea is a relatively straightforward implication of optimization theory.\n\nPossible examples of marginal charity include [divesting](https://forum.effectivealtruism.org/tag/divestment) from the most harmful companies or industries, [reducing consumption of animal products](https://forum.effectivealtruism.org/tag/dietary-change), and being generally nicer to others.^[\\[2\\]](#fnq70mbgmhlg)^^[\\[3\\]](#fndx39h26mwzc)^\n\nKevin Simler and Robin Hanson speculate that marginal charity, despite its efficiency, is not very popular because acts of marginal charity tend to be indistinguishable from ordinary self-interested behavior. As a consequence, such acts are ill-suited to play the role of moral signaling, which requires behavior to be visibly costly to the agent.^[\\[4\\]](#fnikem918nc4)^^[\\[5\\]](#fnpxvdzn077z)^ For example, a moderately altruistic developer who estimates that a profit-maximizing building should be 12 stories high may decide to build one with 13 stories instead. From the outside, however, this decision does not look any more altruistic than the purely self-interested alternative.^[\\[2\\]](#fnq70mbgmhlg)^\n\nMarginal charity need not involve changes in a person's own behavior—it can apply to cases where others are paid to be more marginally prosocial. For example, [longtermists](https://forum.effectivealtruism.org/tag/longtermism) can pay self-interested or short-termist demographers to slightly expand the time horizon of their projections.^[\\[3\\]](#fndx39h26mwzc)^\n\nFurther reading\n---------------\n\nHanson, Robin (2012) [Marginal charity](https://www.overcomingbias.com/2012/11/marginal-charity.html), *Overcoming Bias*, November 24.\n\nRelated entries\n---------------\n\n[dietary change](https://forum.effectivealtruism.org/tag/dietary-change) | [divestment](https://forum.effectivealtruism.org/tag/divestment) | [ethics of personal consumption](https://forum.effectivealtruism.org/tag/ethics-of-personal-consumption)\n\n1.  ^**[^](#fnrefew0k4spqkwu)**^\n    \n    Hanson, Robin (2012) [Marginal charity](https://www.overcomingbias.com/2012/11/marginal-charity.html), *Overcoming Bias*, November 24.\n    \n2.  ^**[^](#fnrefq70mbgmhlg)**^\n    \n    Wiblin, Robert & Keiran Harris (2018) [Why we have to lie to ourselves about why we do what we do, according to Prof Robin Hanson](https://80000hours.org/podcast/episodes/robin-hanson-on-lying-to-ourselves/), *80,000 Hours*, March 28.\n    \n3.  ^**[^](#fnrefdx39h26mwzc)**^\n    \n    Trammell, Philip (2019) [Marginal charity for other people](https://philiptrammell.com/blog/44), *Philip Trammell’s Blog*, September 9.\n    \n4.  ^**[^](#fnrefikem918nc4)**^\n    \n    Hanson, Robin (2014) [Neglecting win-win help](https://www.overcomingbias.com/2014/08/neglecting-win-win-help.html), *Overcoming Bias*, August 15.\n    \n5.  ^**[^](#fnrefpxvdzn077z)**^\n    \n    Simler, Kevin & Robin Hanson (2017) [*The Elephant in the Brain: Hidden Motives in Everyday Life*](https://en.wikipedia.org/wiki/The_Elephant_in_the_Brain), Oxford: Oxford University Press, pp.  222-223."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5GNwgf92y57qBq3TA",
    "name": "Manhattan Project",
    "core": false,
    "slug": "manhattan-project",
    "oldSlugs": null,
    "postCount": 2,
    "description": {
      "markdown": "The **Manhattan Project** was the codename for a research and development undertaking to develop an atom bomb during World War II. It culminated on 16 July 1945 with [Trinity](https://forum.effectivealtruism.org/tag/trinity), the first detonation of a nuclear weapon.\n\nFurther reading\n---------------\n\nElse, Jon (1981) [The Day after Trinity](https://www.imdb.com/title/tt0080594/?ref_=fn_al_tt_1), KTEH.  \n*A* [*documentary*](https://forum.effectivealtruism.org/tag/documentaries) *featuring interviews with many of the scientists involved in the Manhattan Project.*\n\nRhodes, Richard (1986) [*The Making of the Atomic Bomb*](https://en.wikipedia.org/wiki/Special:BookSources/0-671-44133-7), New York: Simon & Schuster.\n\nRelated entries\n---------------\n\n[Bulletin of the Atomic Scientists](https://forum.effectivealtruism.org/tag/bulletin-of-the-atomic-scientists) | [nuclear warfare](https://forum.effectivealtruism.org/tag/nuclear-warfare-1) | [Russell–Einstein Manifesto](https://forum.effectivealtruism.org/tag/russell-einstein-manifesto) | [Trinity](https://forum.effectivealtruism.org/tag/trinity)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "wisMsCsny2cafg97C",
    "name": "Malaria Consortium",
    "core": false,
    "slug": "malaria-consortium",
    "oldSlugs": null,
    "postCount": 5,
    "description": {
      "markdown": "**Malaria Consortium** is a British charity that works on preventing, controlling, and treating [malaria](https://forum.effectivealtruism.org/tag/malaria) and other communicable diseases in Africa and Asia.\n\nIts seasonal malaria chemoprevention (SMC) program—one among many programs carried out by this organization—distributes preventive anti-malarial drugs to children under five to reduce incidence during the high transmission season.\n\nEvaluation\n----------\n\nMalaria Consortium's SMC program has been a [GiveWell](https://forum.effectivealtruism.org/tag/givewell) top-rated charity since 2016,^[\\[1\\]](#fny7e25nces5n)^ and its cost-effectiveness is estimated to be in the same range as GiveWell's other priority programs.^[\\[2\\]](#fn7dhqrkn1nsd)^ GiveWell estimates that SMC can protect a child from malaria at a cost of about $7, and that a donation to this program has an average [cost-effectiveness](https://forum.effectivealtruism.org/tag/cost-effectiveness-analysis) of $5,000 per life saved.^[\\[3\\]](#fnb2c1napnmna)^^[\\[4\\]](#fnp192e4ph93l)^^[\\[5\\]](#fnubvkqjo0il)^ (The cost of protecting a child from malaria is much lower than the cost of saving a life because a small fraction of people who receive protection would otherwise have died of malaria, and because of other factors.)^[\\[4\\]](#fnp192e4ph93l)^\n\nMalaria Consortium is also featured in [The Life You Can Save](https://forum.effectivealtruism.org/tag/the-life-you-can-save)'s list of \"best charities\".^[\\[6\\]](#fndz44bhlhlgg)^\n\nAs of August 2022, Malaria Consortium has received over $188 million in funding from [Open Philanthropy](https://forum.effectivealtruism.org/tag/open-philanthropy),^[\\[7\\]](#fnn8d81uhcjlm)^ and over $3.1 million from [Effective Altruism Funds](https://forum.effectivealtruism.org/topics/effective-altruism-funds).^[\\[8\\]](#fnj7miegu5f2)^^[\\[9\\]](#fnd1gu52wobim)^\n\nFurther reading\n---------------\n\nSanchez, Sebastian *et al.* (2021) [Timeline of Malaria Consortium](https://timelines.issarice.com/wiki/Timeline_of_Malaria_Consortium), *Timelines Wiki*.\n\nExternal links\n--------------\n\n[Malaria Consortium](https://www.malariaconsortium.org/). Official website.\n\n[Apply for a job](https://malariaconsortium.current-vacancies.com/Careers/Malaria%20Consortium%20vacancy%20search%20page-2061).\n\n[Donate to Malaria Consortium](https://www.malariaconsortium.org/support/donate.htm).\n\n1.  ^**[^](#fnrefy7e25nces5n)**^\n    \n    Sanchez, Sebastian *et al.* (2021) [Timeline of Malaria Consortium](https://timelines.issarice.com/wiki/Timeline_of_Malaria_Consortium), *Timelines Wiki*.\n    \n2.  ^**[^](#fnref7dhqrkn1nsd)**^\n    \n    GiveWell (2021) [Malaria Consortium – Seasonal Malaria Chemoprevention](https://www.givewell.org/charities/malaria-consortium), *GiveWell*, November.\n    \n3.  ^**[^](#fnrefb2c1napnmna)**^\n    \n    GiveWell (2022) [Our top charities](https://www.givewell.org/charities/top-charities), *GiveWell*, July.\n    \n4.  ^**[^](#fnrefp192e4ph93l)**^\n    \n    GiveWell (2022) [How we produce impact estimates](https://www.givewell.org/impact-estimates), *GiveWell*, July. \n    \n5.  ^**[^](#fnrefubvkqjo0il)**^\n    \n    GiveWell (2022) [GiveWell directed grants to top charities with impact information (2020 onward)](https://docs.google.com/spreadsheets/d/1z065ab9PPMu9i5KiQ4yLyQJPFQCfEzHSgtHulPiZeBo/edit#gid=1407352843), *GiveWell*, July.\n    \n6.  ^**[^](#fnrefdz44bhlhlgg)**^\n    \n    The Life You Can Save (2021) [Best charities](https://www.thelifeyoucansave.org/best-charities/), *The Life You Can Save*.\n    \n7.  ^**[^](#fnrefn8d81uhcjlm)**^\n    \n    Open Philanthropy (2022) [Grants database: Malaria Consortium](https://www.openphilanthropy.org/grants/?q=&organization-name=malaria-consortium), *Open Philanthropy*.\n    \n8.  ^**[^](#fnrefj7miegu5f2)**^\n    \n    Global Health and Development Fund (2019) [March 2019: Malaria Consortium](https://funds.effectivealtruism.org/funds/payouts/march-2019-malaria-consortium), *Effective Altruism Funds*, March. \n    \n9.  ^**[^](#fnrefd1gu52wobim)**^\n    \n    Global Health and Development Fund (2021) [February 2021: Global Health and Development Fund grants](https://funds.effectivealtruism.org/funds/payouts/february-2021-global-health-and-development-fund-grants), *Effective Altruism Funds*, February."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "cgjMFtsn3AKLDWCWq",
    "name": "Malaria",
    "core": false,
    "slug": "malaria",
    "oldSlugs": null,
    "postCount": 30,
    "description": {
      "markdown": "**Malaria** is a life-threatening infectious disease affecting humans and other animals. It is caused by parasites spread through the bites of infected female mosquitoes. In 2020, there were approximately 241 million cases and 627,000 deaths worldwide from malaria.^[\\[1\\]](#fnd9xa6f7522m)^\n\nMultiple randomized controlled trials show that [mass distribution of long-lasting insecticide-treated nets](https://forum.effectivealtruism.org/tag/mass-distribution-of-long-lasting-insecticide-treated-nets) reduces malaria fevers and prevents deaths from the disease.^[\\[2\\]](#fno6bkgaoui5c)^ Despite the existence of effective prevention methods, tens of millions of people are still unprotected from malaria, and the [funding gap](https://forum.effectivealtruism.org/tag/room-for-more-funding) for net distributions is in the hundreds of millions of dollars.\n\nThe [Against Malaria Foundation](https://forum.effectivealtruism.org/tag/against-malaria-foundation) provides funding for distribution of bed nets to high-risk populations. As of July 2022, [GiveWell](https://forum.effectivealtruism.org/tag/givewell) estimates that it costs $5.19 to purchase and distribute an AMF-funded net,^[\\[3\\]](#fnbzborcpzo5v)^ and that a marginal donation of $5,500 to AMF is expected to avert the death of a child under five.^[\\[4\\]](#fnqavfjunhr8)^^[\\[5\\]](#fn2jefvu48ps5)^\n\nWhile distribution of nets is the most common method to combat malaria at present, there are other promising approaches. The [Malaria Consortium](https://forum.effectivealtruism.org/tag/malaria-consortium) works on preventing, controlling, and treating malaria and other communicable diseases in Africa and Asia. One intervention they carry out is seasonal malaria chemoprevention (SMC) programs, which seek to distribute preventive anti-malarial drugs to children under the age of five.^[\\[6\\]](#fnko1kfm2nvo)^^[\\[7\\]](#fnz9fo2xg1fa)^\n\nIn principle, malaria could be controlled, and ultimately eradicated, by means of an effective vaccine. RTS,S/AS01 (trade name Mosquirix) is a recombinant protein-based malaria vaccine approved for use by European regulators in July 2015. Among children aged 5–17 months who received 4 doses of RTS,S/AS01, efficacy was 36% over 4 years of follow-up.^[\\[8\\]](#fn8ltde1cfoju)^ In October 2021, the World Health Organization endorsed RTS,S/AS01 for \"widespread use\" among children, making it the first vaccine candidate to receive that recommendation.^[\\[9\\]](#fndncma5wtq6)^ The WHO decision was partly based on results from an ongoing pilot program started in 2019 intended to vaccinate 360,000 children per year in Malawi, Ghana and Kenya over a five-year period.^[\\[10\\]](#fnr7n8hphczrq)^ A month before the WHO announcement, a randomized trial had found that a treatment combining antimalarial medication with a four-dose schedule of the RTS,S vaccine, culminating with a booster shot just before the most vulnerable season, was much more effective than either of those methods alone.^[\\[11\\]](#fn2rcj9cmxqfm)^ In 2020, a study using a standard model of malaria transmission estimated that 4.3 million cases and 22,000 deaths in children under five could be averted annually assuming 30 million people are vaccinated, provided areas with the highest malaria burden are prioritized.^[\\[12\\]](#fnzt5wqismf79)^\n\nYet another approach to combat malaria is to eliminate the mosquito species responsible for spreading the disease, or to modify it genetically to render it incapable of carrying malaria. [Gene drives](https://forum.effectivealtruism.org/tag/gene-drives)—genetic modifications designed to spread through a population at higher-than-normal rates of inheritance—could achieve both of these goals. In 2016, [Open Philanthropy](https://forum.effectivealtruism.org/tag/open-philanthropy) made a grant to enable the formation of a working group to further investigate genetic modification as a form of malaria control,^[\\[13\\]](#fnd9rb631o5nt)^ and the following year it made a $17.5 million grant to [Target Malaria](https://forum.effectivealtruism.org/tag/target-malaria), a nonprofit research consortium working to develop gene drive technologies to eradicate malaria in sub-Saharan Africa.^[\\[14\\]](#fna8cwx52z9kc)^\n\nFurther reading\n---------------\n\nHillebrandt, Hauke (2015) [Bednets have prevented 450 million cases of malaria](https://www.givingwhatwecan.org/post/2015/12/bednets-have-prevented-450-million-cases-of-malaria/), *Giving What We Can*, December 18.\n\nRoser, Max (2022) [Malaria: One of the leading causes of child deaths, but progress is possible and you can contribute to it](https://ourworldindata.org/malaria-introduction), *Our World in Data*, March 22.\n\nSánchez, Sebastián (2019) [Timeline of malaria vaccine](https://timelines.issarice.com/wiki/Timeline_of_malaria_vaccine), *Timelines Wiki*.\n\nSánchez, Sebastián (2020) [Timeline of malaria](https://timelines.issarice.com/wiki/Timeline_of_malaria), *Timelines Wiki*.\n\nSnowden, James (2016) [The economic benefits of malaria eradication](https://www.givingwhatwecan.org/post/2016/01/the-economic-benefits-of-malaria-eradication/), *Giving What We Can*, January 18.\n\nWiblin, Robert & Keiran Harris (2022) [James Tibenderana on the state of the art in malaria control and elimination](https://80000hours.org/podcast/episodes/james-tibenderana-malaria-control-and-elimination/), *80,000 Hours*, May 9.\n\nRelated entries\n---------------\n\n[global health and development](https://forum.effectivealtruism.org/tag/global-health-and-development) | [mass distribution of long-lasting insecticide-treated nets](https://forum.effectivealtruism.org/tag/mass-distribution-of-long-lasting-insecticide-treated-nets)\n\n1.  ^**[^](#fnrefd9xa6f7522m)**^\n    \n    World Health Organization (2021) [*World Malaria Report 2021*](https://www.who.int/teams/global-malaria-programme/reports/world-malaria-report-2021), Geneva: World Health Organization, p. 23.\n    \n2.  ^**[^](#fnrefo6bkgaoui5c)**^\n    \n    GiveWell (2018) [Mass distribution of long-lasting insecticide-treated nets (LLINs)](https://www.givewell.org/international/technical/programs/insecticide-treated-nets), *GiveWell*, March.\n    \n3.  ^**[^](#fnrefbzborcpzo5v)**^\n    \n    GiveWell (2021) [Against malaria foundation](https://www.givewell.org/charities/amf), *GiveWell*, November.\n    \n4.  ^**[^](#fnrefqavfjunhr8)**^\n    \n    GiveWell (2022) [How we produce impact estimates](https://www.givewell.org/impact-estimates), *GiveWell*, July. \n    \n5.  ^**[^](#fnref2jefvu48ps5)**^\n    \n    For caveats, see GiveWell (2017) [Cost-effectiveness](https://www.givewell.org/how-we-work/our-criteria/cost-effectiveness), *GiveWell*, November.\n    \n6.  ^**[^](#fnrefko1kfm2nvo)**^\n    \n    GiveWell (2018) [Seasonal malaria chemoprevention](https://www.givewell.org/international/technical/programs/seasonal-malaria-chemoprevention), *GiveWell*, October.\n    \n7.  ^**[^](#fnrefz9fo2xg1fa)**^\n    \n    GiveWell (2020) [Malaria Consortium – Seasonal malaria chemoprevention](https://www.givewell.org/charities/malaria-consortium), *GiveWell*, November.\n    \n8.  ^**[^](#fnref8ltde1cfoju)**^\n    \n    Laurens, Matthew B. (2019) [RTS,S/AS01 vaccine (Mosquirix™): an overview](https://doi.org/10.1080/21645515.2019.1669415), *Human Vaccines & Immunotherapeutics*, vol. 16, pp. 480–489.\n    \n9.  ^**[^](#fnrefdncma5wtq6)**^\n    \n    Piper, Kelsey (2021) [Why the WHO approval of the first malaria vaccine is a big deal](https://www.vox.com/future-perfect/2021/10/6/22712898/who-malaria-vaccine-approval), *Vox*, October 6.\n    \n10.  ^**[^](#fnrefr7n8hphczrq)**^\n    \n    World Health Organization (2021) [WHO recommends groundbreaking malaria vaccine for children at risk](https://www.who.int/news/item/06-10-2021-who-recommends-groundbreaking-malaria-vaccine-for-children-at-risk), October 6.\n    \n11.  ^**[^](#fnref2rcj9cmxqfm)**^\n    \n    Chandramohan, Daniel *et al.* (2021) [Seasonal malaria vaccination with or without seasonal malaria chemoprevention](https://doi.org/10.1056/NEJMoa2026330), *New England Journal of Medicine*, vol. 385, pp. 1005–1017.\n    \n12.  ^**[^](#fnrefzt5wqismf79)**^\n    \n    Hogan, Alexandra B., Peter Winskill & Azra C. Ghani (2020) [Estimated impact of RTS,S/AS01 malaria vaccine allocation strategies in sub-Saharan Africa: A modelling study](https://doi.org/10.1371/journal.pmed.1003377), *PLOS Medicine*, vol. 17, e1003377.\n    \n13.  ^**[^](#fnrefd9rb631o5nt)**^\n    \n    Open Philanthropy (2016) [Foundation for the National Institutes of Health — working group on malaria gene drive testing path](https://www.openphilanthropy.org/focus/scientific-research/miscellaneous/foundation-national-institutes-health-working-group), *Open Philanthropy*, July.\n    \n14.  ^**[^](#fnrefa8cwx52z9kc)**^\n    \n    Open Philanthropy (2017) [Target Malaria — gene drives for malaria control](https://www.openphilanthropy.org/focus/scientific-research/miscellaneous/target-malaria-general-support), *Open Philanthropy*, May."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "FELCq67CuyBM2aEjc",
    "name": "Eliezer Yudkowsky",
    "core": false,
    "slug": "eliezer-yudkowsky",
    "oldSlugs": null,
    "postCount": 12,
    "description": {
      "markdown": "**Eliezer Shlomo Yudkowsky** (born 11 September 1979) is an American [artificial intelligence](https://forum.effectivealtruism.org/tag/artificial-intelligence) researcher and the co-founder of the [Machine Intelligence Research Institute](https://forum.effectivealtruism.org/tag/machine-intelligence-research-institute).\n\nFurther reading\n---------------\n\nChristiano, Paul (2022) [Where I agree and disagree with Eliezer](https://www.alignmentforum.org/posts/CoZhXrhpQxpy9xw9y/where-i-agree-and-disagree-with-eliezer), *AI Alignment Forum*, June 19.\n\nLessWrong (2012) [Eliezer Yudkowsky](https://www.lesswrong.com/tag/eliezer-yudkowsky), *LessWrong Wiki*, October 29.\n\nRice, Issa (2018) [List of discussions between Eliezer Yudkowsky and Paul Christiano](https://causeprioritization.org/List_of_discussions_between_Eliezer_Yudkowsky_and_Paul_Christiano), *Cause Prioritization Wiki*, March 21.\n\nHarris, Sam (2018) [AI: racing toward the brink: A conversation with Eliezer Yudkowsky](https://www.samharris.org/podcasts/making-sense-episodes/116-ai-racing-toward-brink), *Making Sense*, February 6.\n\nExternal links\n--------------\n\n[Eliezer Yudkowsky](https://forum.effectivealtruism.org/users/eliezeryudkowsky). [Effective Altruism Forum](https://forum.effectivealtruism.org/tag/effective-altruism-forum-1) account.\n\nRelated entries\n---------------\n\n[AI alignment](https://forum.effectivealtruism.org/tag/ai-alignment) | [Center for Applied Rationality](https://forum.effectivealtruism.org/tag/center-for-applied-rationality) | [LessWrong](https://forum.effectivealtruism.org/tag/lesswrong) | [Machine Intelligence Research Institute](https://forum.effectivealtruism.org/tag/machine-intelligence-research-institute) | [rationality community](https://forum.effectivealtruism.org/tag/rationality-community)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "xNQKPkAeSbDD85pbJ",
    "name": "Machine Intelligence Research Institute",
    "core": false,
    "slug": "machine-intelligence-research-institute",
    "oldSlugs": null,
    "postCount": 28,
    "description": {
      "markdown": "The **Machine Intelligence Research Institute** (**MIRI**) is a non-profit research institute. Its mission is \"to develop formal tools for the clean design and analysis of general-purpose AI systems, with the intent of making such systems safer and more reliable when they are developed.\"^[\\[1\\]](#fnk8qd5edrw4)^\n\nHistory\n-------\n\nMIRI was founded in 2000 as the **Singularity Institute for Artificial Intelligence** by Brian Atkins, Sabine Atkins and [Eliezer Yudkowsky](https://forum.effectivealtruism.org/tag/eliezer-yudkowsky).^[\\[2\\]](#fno6zyxjrcfeq)^ It adopted its current name in 2013.^[\\[3\\]](#fncoahb85fxlm)^\n\nFunding\n-------\n\nAs of July 2022, MIRI has received over $14.7 million in funding from [Open Philanthropy](https://forum.effectivealtruism.org/topics/open-philanthropy),^[\\[4\\]](#fn8w6v9df3nov)^ nearly $900,000 from the [Survival and Flourishing](https://forum.effectivealtruism.org/topics/survival-and-flourishing) Fund,^[\\[5\\]](#fnraxts7feeu)^^[\\[6\\]](#fnuep3xxzvezk)^ and over $670,000 from [Effective Altruism Funds](https://forum.effectivealtruism.org/topics/effective-altruism-funds).^[\\[7\\]](#fnui4lqgapu7p)^^[\\[8\\]](#fn3l7g23x5ugg)^^[\\[9\\]](#fn57iu5k62m1e)^^[\\[10\\]](#fn2bu39a9ez43)^ \n\nFurther reading\n---------------\n\nKarnofsky, Holden (2012) [Thoughts on the Singularity Institute (SI)](https://www.lesswrong.com/posts/6SGqkCgHuNr7d4yJm/thoughts-on-the-singularity-institute-si), *LessWrong*, May 11.\n\nLessWrong (2020) [Machine Intelligence Research Institute (MIRI)](https://www.lesswrong.com/tag/machine-intelligence-research-institute-miri), *LessWrong Wiki*, July 9.\n\nLuke Muehlhauser (2015) 'Machine Intelligence Research Institute', in Ryan Carey (ed.), [*The Effective Altruism Handbook*](http://www.stafforini.com/docs/Carey%20-%20The%20effective%20altruism%20handbook.pdf), 1st ed., Oxford: The Centre for Effective Altruism, pp. 127-129\n\nRice, Issa *et al.* (2017) [Timeline of Machine Intelligence Research Institute Full timeline](https://timelines.issarice.com/wiki/Timeline_of_Machine_Intelligence_Research_Institute), *Timelines Wiki*, June 30 (updated 19 June 2022).\n\nExternal links\n--------------\n\n[Machine Intelligence Research Institute](http://intelligence.org/). Official website.\n\n[Apply for a job](https://intelligence.org/get-involved/#careers).\n\n[Donate to MIRI](https://intelligence.org/donate/).\n\nRelated entries\n---------------\n\n[AI alignment](https://forum.effectivealtruism.org/tag/ai-alignment) | [Center for Applied Rationality](https://forum.effectivealtruism.org/tag/center-for-applied-rationality) | [Eliezer Yudkowsky](https://forum.effectivealtruism.org/tag/eliezer-yudkowsky) |  [rationality community](https://forum.effectivealtruism.org/tag/rationality-community)\n\n1.  ^**[^](#fnrefk8qd5edrw4)**^\n    \n    Machine Intelligence Research Institute (2021) [About MIRI](https://intelligence.org/about/), *Machine Intelligence Research Institute*.\n    \n2.  ^**[^](#fnrefo6zyxjrcfeq)**^\n    \n    Singularity Institute for Artificial Intelligence (2000) [About SIAI](https://web.archive.org/web/20060704101132/http://www.singinst.org:80/about.html), *Singularity Institute for Artificial Intelligence*.\n    \n3.  ^**[^](#fnrefcoahb85fxlm)**^\n    \n    Muehlhauser, Luke (2013) [We are now the “Machine Intelligence Research Institute” (MIRI)](https://intelligence.org/2013/01/30/we-are-now-the-machine-intelligence-research-institute-miri/), *Machine Intelligence Research Institute*, January 30.\n    \n4.  ^**[^](#fnref8w6v9df3nov)**^\n    \n    Open Philanthropy (2022) [Grants database: Machine Intelligence Research Institute](https://www.openphilanthropy.org/grants/?q=&organization-name=machine-intelligence-research-institute), *Open Philanthropy.*\n    \n5.  ^**[^](#fnrefraxts7feeu)**^\n    \n    Survival and Flourishing Fund (2019a) [SFF-2020-H1 S-process recommendations announcement](https://survivalandflourishing.fund/sff-2020-h1-recommendations), *Survival and Flourishing Fund*. \n    \n6.  ^**[^](#fnrefuep3xxzvezk)**^\n    \n    Survival and Flourishing Fund (2019b) [SFF-2020-H2 S-process recommendations announcement](https://survivalandflourishing.fund/sff-2020-h2-recommendations), *Survival and Flourishing Fund*.\n    \n7.  ^**[^](#fnrefui4lqgapu7p)**^\n    \n    Long-Term Future Fund (2018) [July 2018: Long-Term Future Fund grants](https://funds.effectivealtruism.org/funds/payouts/july-2018-long-term-future-fund-grants), *Effective Altruism Funds*, July. \n    \n8.  ^**[^](#fnref3l7g23x5ugg)**^\n    \n    Long-Term Future Fund (2018) [November 2018: Long-Term Future Fund grants](https://funds.effectivealtruism.org/funds/payouts/november-2018-long-term-future-fund-grants), *Effective Altruism Funds*, November. \n    \n9.  ^**[^](#fnref57iu5k62m1e)**^\n    \n    Long-Term Future Fund (2019) [April 2019: Long-Term Future Fund grants and recommendations](https://funds.effectivealtruism.org/funds/payouts/april-2019-long-term-future-fund-grants-and-recommendations), *Effective Altruism Funds*, April. \n    \n10.  ^**[^](#fnref2bu39a9ez43)**^\n    \n    Long-Term Future Fund (2020) [April 2020: Long-Term Future Fund grants and recommendations](https://funds.effectivealtruism.org/funds/payouts/april-2020-long-term-future-fund-grants-and-recommendations), *Effective Altruism Funds*, April."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "9oh7q75MbpMDEPuWQ",
    "name": "Logic of the larder",
    "core": false,
    "slug": "logic-of-the-larder",
    "oldSlugs": null,
    "postCount": 1,
    "description": {
      "markdown": "The **logic of the larder** is an objection to the argument that people concerned with [farmed animal welfare](https://forum.effectivealtruism.org/tag/farmed-animal-welfare) should abstain from consuming products derived from animals raised in factory farms. The objection is that reducing demand for meat, eggs or milk causes fewer animals to come into existence, and thus reduces net animal welfare.\n\nA number of responses have been given to this objection. First, many of the animals raised in factory farms have net negative welfare. Causing fewer of these animals to exist, therefore, reduces overall animal suffering. Second, rearing domesticated animals reduces the number of wild animals that exist by an even greater number, so the assumption that veganism causes fewer animals to exist may be false. Finally, even if increasing demand for animal products was positive on the whole, there are other, more cost-effective ways to help animals, and to improve the world.^[\\[1\\]](#fnrmxxbdw6y5s)^\n\nHowever, [Carl Schulman](https://forum.effectivealtruism.org/tag/carl-shulman) has pointed out the tension involved in promoting dietary change as a way to reduce farm animal suffering and expressing concern for [wild animal suffering](https://forum.effectivealtruism.org/tag/wild-animal-welfare).^[\\[2\\]](#fnhzouivuezx)^ If wild animals tend not to lead net-positive lives, and a vegan diet increases the number of animals in the world, a vegan diet will increase, rather than decrease, the number of beings living net-negative lives. Partly for these reasons, some animal advocates in the effective altruism community favor methods of reducing animal suffering that do not involve dietary change, such as humane slaughter methods.^[\\[3\\]](#fnvo2yox61mmn)^\n\nFurther reading\n---------------\n\nHanson, Robin (2002) [Why meat is moral, and veggies are immoral](http://mason.gmu.edu/~rhanson/meat.html), *Robin Hanson’s Website*, July 10.\n\nMatheny, Jason Gaverick & Kai M. A. Chan (2005) [Human diets and animal welfare: the illogic of the larder](http://doi.org/10.1007/s10806-005-1805-x), *Journal of Agricultural and Environmental Ethics*, vol. 18, pp. 579–594.\n\nSalt, Henry Stephens (1914) [Logic of the Larder](http://www.animal-rights-library.com/texts-c/salt02.htm), Excerpt from *The Humanities of Diet*, Manchester: The Vegetarian Society.\n\n1.  ^**[^](#fnrefrmxxbdw6y5s)**^\n    \n    Matheny, Jason Gaverick & Kai M. A. Chan (2005) [Human diets and animal welfare: the illogic of the larder](http://doi.org/10.1007/s10806-005-1805-x), *Journal of Agricultural and Environmental Ethics*, vol. 18, pp. 579–594.\n    \n2.  ^**[^](#fnrefhzouivuezx)**^\n    \n    Shulman, Carl (2013) [Vegan advocacy and pessimism about wild animal welfare](http://reflectivedisequilibrium.blogspot.com/2013/07/vegan-advocacy-and-pessimism-about-wild.html), *Reflective Disequilibrium*, July 30.\n    \n3.  ^**[^](#fnrefvo2yox61mmn)**^\n    \n    Tomasik, Brian (2008) [How does vegetarianism impact wild-animal suffering?](https://reducing-suffering.org/vegetarianism-and-wild-animals/), *Essays on Reducing Suffering* (updated 17 June 2019)."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "L6NqHZkLc4xZ7YtDr",
    "name": "Effective giving",
    "core": false,
    "slug": "effective-giving-1",
    "oldSlugs": [
      "effective-giving"
    ],
    "postCount": 67,
    "description": {
      "markdown": "**Effective giving** is the branch of [effective altruism](https://forum.effectivealtruism.org/tag/effective-altruism) focused on charitable donations.\n\nThe main purpose of effective giving is to support a charity that can achieve an especially large amount of good with that support. However, effective giving on a sufficient scale can also influence the charitable sector, giving charities an incentive to improve their own effectiveness.\n\nIssues related to effective giving\n----------------------------------\n\nThere are a variety of issues related to effective giving.\n\nSome of these issues relate to how to ensure that the donation has the largest direct impact possible. For instance, donors often tend to give to multiple organizations, but there are reasons to think that it may be more effective not to [diversify donations](https://forum.effectivealtruism.org/tag/philanthropic-diversification) in this way. Relatedly, donors have choices about [when to give their money](https://forum.effectivealtruism.org/tag/timing-of-philanthropy). For instance, they could decide to save money, and give it away later, rather than giving now. There continues to be debate about which of these options is more effective in general. Third, one person’s donations may impact the effectiveness of another person’s donations, raising the tricky question of [how they should coordinate to collectively maximize their impact](https://forum.effectivealtruism.org/tag/philanthropic-coordination).\n\nOther issues focus more on the indirect impact of donations, in particular the impact on others who are considering whether and where to donate. One way to increase your positive impact on others’ donations is to [give publicly](https://forum.effectivealtruism.org/tag/public-giving).\n\nFinally, one way of improving the effectiveness of donations is to try to identify the features shared by past cost-effective donations. For this reason, studying the [history of philanthropy](https://forum.effectivealtruism.org/tag/history-of-philanthropy) may be a source of lessons that could improve how donors fund projects.\n\nFurther reading\n---------------\n\nBirkwood, Susannah (2016) [Effective altruism: Will donors change their ways?](http://www.thirdsector.co.uk/effective-altruism-will-donors-change-ways/fundraising/article/1384629), *Third Sector*, February 25.  \n*A look at effective altruism and the effectiveness of charities*\n\nRelated entries\n---------------\n\n[constraints on effective altruism](https://forum.effectivealtruism.org/tag/constraints-on-effective-altruism) | [diminishing returns](https://forum.effectivealtruism.org/tag/diminishing-returns) | [philanthropic coordination](https://forum.effectivealtruism.org/tag/philanthropic-coordination) | [philanthropic diversification](https://forum.effectivealtruism.org/tag/philanthropic-diversification) | [room for more funding](https://forum.effectivealtruism.org/tag/room-for-more-funding)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "749QfmuqCx642wFti",
    "name": "Liv Boeree",
    "core": false,
    "slug": "liv-boeree",
    "oldSlugs": null,
    "postCount": null,
    "description": {
      "markdown": "**Olivia \"Liv\" Boeree** (born 18 July 1984) is a British poker player and the co-founder of [Raising for Effective Giving](https://forum.effectivealtruism.org/tag/raising-for-effective-giving).  As measured by lifetime money winnings, Boeree ranks among the top ten female poker players of all time.^[\\[1\\]](#fn64crjw3zcfa)^\n\nFurther reading\n---------------\n\nBoeree, Liv (2018) [How an 18th-century priest gave us the tools to make better decisions](https://www.vox.com/future-perfect/2018/11/30/18096751/bayes-theorem-rule-rationality-reason), *Vox*, November 30.\n\nZhang, Linchuan (2016) [The poker pro who wants to save the world](https://www.huffpost.com/entry/the-poker-pro-who-wants-to-save-the-world_b_58323653e4b0d28e55215178), *Huffington Post*, November 21.\n\nExternal links\n--------------\n\n[Liv Boeree](https://livboeree.com/). Official website.\n\nRelated entries\n---------------\n\n[effective giving](https://forum.effectivealtruism.org/tag/effective-giving-1) | [Longview Philanthropy](https://forum.effectivealtruism.org/tag/longview-philanthropy) | [Raising for Effective Giving](https://forum.effectivealtruism.org/tag/raising-for-effective-giving)\n\n1.  ^**[^](#fnref64crjw3zcfa)**^\n    \n    The Hendon Mob (2022) [Women’s all time money list](https://pokerdb.thehendonmob.com/ranking/137/), *The Hendon Mob*."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "v6QikrTuQwq6G4cxL",
    "name": "LessWrong",
    "core": false,
    "slug": "lesswrong",
    "oldSlugs": null,
    "postCount": 18,
    "description": {
      "markdown": "**LessWrong** (sometimes spelled **Less Wrong**) is a community blog and forum dedicated to improving [epistemic and instrumental rationality](https://forum.effectivealtruism.org/tag/instrumental-vs-epistemic-rationality).\n\nHistory\n-------\n\nIn November 2006, the group blog Overcoming Bias was launched, with Robin Hanson and [Eliezer Yudkowsky](https://forum.effectivealtruism.org/tag/eliezer-yudkowsky) as its primary authors.^[\\[1\\]](#fn2ercwowgfxn)^ In early March 2009, Yudkowsky founded LessWrong, repurposing his contributions to Overcoming Bias as the seed content for this \"community blog devoted to refining the art of human rationality.\"^[\\[2\\]](#fnd4g06m6jn04)^^[\\[3\\]](#fn1sizgthf2ce)^ This material was organized as a number of \"sequences\", or thematic collections of posts to be read in a specific order, later published in book form.^[\\[4\\]](#fnsh7d3s45y5)^ Shortly thereafter, [Scott Alexander](https://forum.effectivealtruism.org/tag/scott-alexander) joined as a regular contributor.\n\nAround 2013, Yudkowsky switched his primary focus to writing fan fiction, and Alexander launched his own blog, Slate Star Codex, to which most of his contributions were posted. As a consequence of this and other developments, posting quality and frequency on LessWrong began to decline.^[\\[5\\]](#fn2o4uajdt5uk)^ By 2015, activity on the site was a fraction of what it had been in its heyday.^[\\[6\\]](#fnn36x9ixpmr)^\n\nLessWrong was relaunched as **LessWrong 2.0** in late 2017 on a new codebase and a full-time, dedicated team.^[\\[7\\]](#fnabl9gb4gep7)^^[\\[8\\]](#fn38kaghpi1th)^ The launch coincided with the release of Yudkowsky's book *Inadequate Equilibria*, posted as a series of chapters and also released as a book.^[\\[9\\]](#fnra8su8uzj7)^ Since the relaunch, activity has recovered and has remained at steady levels.^[\\[6\\]](#fnn36x9ixpmr)^^[\\[10\\]](#fnvtwy7gers19)^\n\nIn 2021, the LessWrong team became [Lightcone Infrastructure](https://forum.effectivealtruism.org/tag/lightcone-infrastructure), broadening its scope to encompass other projects related to the [rationality community](https://forum.effectivealtruism.org/tag/rationality-community) and the [future of humanity](https://forum.effectivealtruism.org/tag/long-term-future).^[\\[11\\]](#fnbjsgenvdlvv)^\n\nFunding\n-------\n\nAs of July 2022, LessWrong and Lightcone Infrastructure have received over $2.3 million in funding from the [Survival and Flourishing](https://forum.effectivealtruism.org/topics/survival-and-flourishing) Fund,^[\\[12\\]](#fn8l11lhpduwm)^^[\\[13\\]](#fnfc7jk23mfto)^^[\\[14\\]](#fnipmn7b61w3a)^ $2 million from the [Future Fund](https://forum.effectivealtruism.org/topics/future-fund),^[\\[15\\]](#fn8ehsp1ty9qn)^ and $760,000 from [Open Philanthropy](https://forum.effectivealtruism.org/topics/open-philanthropy).^[\\[16\\]](#fnhb83zwh7c47)^\n\nFurther reading\n---------------\n\nMonteiro, Chris *et al.* (2017) [History of Less Wrong](https://wiki.lesswrong.com/wiki/History_of_Less_Wrong), *LessWrong Wiki*, October 20.\n\nExternal links\n--------------\n\n[LessWrong](https://www.lesswrong.com/). Official website.\n\n[Donate to LessWrong](https://www.lesswrong.com/donate).\n\nRelated entries\n---------------\n\n[AI Alignment Forum](https://forum.effectivealtruism.org/tag/ai-alignment-forum) | [Effective Altruism Forum](https://forum.effectivealtruism.org/tag/effective-altruism-forum-1) | [Lightcone Infrastructure](https://forum.effectivealtruism.org/tag/lightcone-infrastructure) | [rationality community](https://forum.effectivealtruism.org/tag/rationality-community)\n\n1.  ^**[^](#fnref2ercwowgfxn)**^\n    \n    Hanson, Robin (2009) [About](http://www.overcomingbias.com/about), *Overcoming Bias*.\n    \n2.  ^**[^](#fnrefd4g06m6jn04)**^\n    \n    Alexander, Scott (2014) [Five years and one week of Less Wrong](https://slatestarcodex.com/2014/03/13/five-years-and-one-week-of-less-wrong/), *Slate Star Codex*, March 13.\n    \n3.  ^**[^](#fnref1sizgthf2ce)**^\n    \n    Bloom, Ruben (2019) [A brief history of LessWrong](https://www.lesswrong.com/posts/S69ogAGXcc9EQjpcZ/a-brief-history-of-lesswrong), *LessWrong*, May 31.\n    \n4.  ^**[^](#fnrefsh7d3s45y5)**^\n    \n    Yudkowsky, Eliezer (2015) [*Rationality: From AI to Zombies*](https://en.wikipedia.org/wiki/Special:BookSources/9781939311146), Berkeley: Machine Intelligence Research Institute.\n    \n5.  ^**[^](#fnref2o4uajdt5uk)**^\n    \n    Alexander, Scott (2017) [Comment on 'A history of the rationality community?'](https://www.reddit.com/r/slatestarcodex/comments/6tt3gy/a_history_of_the_rationality_community/dloghua/), *Reddit*, August 15.\n    \n6.  ^**[^](#fnrefn36x9ixpmr)**^\n    \n    Bloom, Ruben (2019) [Data analysis of LW: activity levels + age distribution of user accounts](https://www.lesswrong.com/posts/9dA6GfuDca3Zh3RMa/data-analysis-of-lw-activity-levels-age-distribution-of-user), *LessWrong*, May 14.\n    \n7.  ^**[^](#fnrefabl9gb4gep7)**^\n    \n    Vaniver (2017) [LW 2.0 open beta live](https://www.lesswrong.com/posts/SR8cqwbMLmKqR7p5s), *LessWrong*, September 20.\n    \n8.  ^**[^](#fnref38kaghpi1th)**^\n    \n    Bloom, Ruben *et al.* (2019) [Welcome to LessWrong!](https://www.lesswrong.com/about), *LessWrong*, June 14.\n    \n9.  ^**[^](#fnrefra8su8uzj7)**^\n    \n    Yudkowsky, Eliezer (2017) [*Inadequate Equilibria: Where and How Civilizations Get Stuck*](https://en.wikipedia.org/wiki/Special:BookSources/9781939311191), Berkeley: Machine Intelligence Research Institute.\n    \n10.  ^**[^](#fnrefvtwy7gers19)**^\n    \n    \"the conclusion that the LW community recovered from its previous decline holds\" (Sempere, Nuño (2021) [Shallow evaluations of longtermist organizations](https://forum.effectivealtruism.org/posts/xmmqDdGqNZq5RELer/shallow-evaluations-of-longtermist-organizations), *Effective Altruism Forum*, June 24)\n    \n11.  ^**[^](#fnrefbjsgenvdlvv)**^\n    \n    Habryka, Oliver (2021) [The LessWrong Team is now Lightcone Infrastructure, come work with us!](https://www.lesswrong.com/posts/eR7Su77N2nK3e5YRZ/the-lesswrong-team-is-now-lightcone-infrastructure-come-work-3), *LessWrong*, September 30.\n    \n12.  ^**[^](#fnref8l11lhpduwm)**^\n    \n    Survival and Flourishing Fund (2019) [SFF-2020-H1 S-process recommendations announcement](https://survivalandflourishing.fund/sff-2020-h1-recommendations), *Survival and Flourishing Fund*. \n    \n13.  ^**[^](#fnreffc7jk23mfto)**^\n    \n    Survival and Flourishing Fund (2020) [SFF-2021-H1 S-process recommendations announcement](https://survivalandflourishing.fund/sff-2021-h1-recommendations), *Survival and Flourishing Fund*. \n    \n14.  ^**[^](#fnrefipmn7b61w3a)**^\n    \n    Survival and Flourishing Fund (2020) [SFF-2021-H2 S-process recommendations announcement](https://survivalandflourishing.fund/sff-2021-h2-recommendations), *Survival and Flourishing Fund*.\n    \n15.  ^**[^](#fnref8ehsp1ty9qn)**^\n    \n    Future Fund (2022) [Our grants and investments: LessWrong](https://ftxfuturefund.org/all-grants/?_organization_name=lightcone-infrastructure), *Future Fund*.\n    \n16.  ^**[^](#fnrefhb83zwh7c47)**^\n    \n    Open Philanthropy (2022) [Grants database: LessWrong](https://www.openphilanthropy.org/grants/?q=&organization-name=lesswrong), *Open Philanthropy*."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "22mbmH7siaKxEyfoN",
    "name": "Dylan Matthews",
    "core": false,
    "slug": "dylan-matthews",
    "oldSlugs": null,
    "postCount": 6,
    "description": {
      "markdown": "**Dylan Matthews** is an American journalist, currently a Senior Correspondent and Lead Writer for [Future Perfect](https://forum.effectivealtruism.org/tag/future-perfect).\n\nFurther reading\n---------------\n\nGalef, Julia (2019) [Global poverty has fallen, but what should we conclude from that? (Dylan Matthews)](http://rationallyspeakingpodcast.org/234-global-poverty-has-fallen-but-what-should-we-conclude-from-that-dylan-matthews/), *Rationally Speaking*, May 27.\n\nMacAskill, William (2015) [What do journalists say about journalism as a high-impact career?](https://80000hours.org/2015/09/what-do-journalists-say-about-journalism-as-a-high-impact-career-interviews-with-dylan-matthews-derek-thompson-and-shaun-raviv/), *80,000 Hours*, September 3.\n\nExternal links\n--------------\n\n[Dylan Matthews](https://www.vox.com/authors/dylan). *Vox* profile.\n\n[Dylan Matthews](https://forum.effectivealtruism.org/users/dylan-matthews-1). [Effective Altruism Forum](https://forum.effectivealtruism.org/tag/effective-altruism-forum-1) account.\n\nRelated entries\n---------------\n\n[global poverty](https://forum.effectivealtruism.org/tag/global-poverty) | [kidney donation](https://forum.effectivealtruism.org/tag/kidney-donation) | [journalism](https://forum.effectivealtruism.org/tag/journalism)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "msQq8yAHTmwxX9BaP",
    "name": "Future Perfect",
    "core": false,
    "slug": "future-perfect",
    "oldSlugs": null,
    "postCount": 14,
    "description": {
      "markdown": "**Future Perfect** is a section of *Vox*'s website focused on news stories relevant from an [effective altruism](https://forum.effectivealtruism.org/tag/effective-altruism) perspective.\n\nFuture Perfect launched on 15 October 2018.^[\\[1\\]](#fnwxf8yaepfsr)^As of August 2022, the team consists of nine people:  Bryan Walsh, Izzie Ramirez, Kenny Torrella, Sigal Samuel, [Dylan Matthews](https://forum.effectivealtruism.org/tag/dylan-matthews), [Kelsey Piper](https://forum.effectivealtruism.org/tag/kelsey-piper), Muizz Akhtar, Miranda Dixon-Luinenburg, and Siobhan McDonough.^[\\[2\\]](#fn75673c8zv8n)^\n\nFuture Perfect is partly funded by private donations. Major donors since 2018 include the Rockefeller Foundation (2018-2019), James McClave (2020), [Animal Charity Evaluators](https://forum.effectivealtruism.org/tag/animal-charity-evaluators) (2020-2021), and the BEMC Foundation (2021-2023).^[\\[3\\]](#fnk6jqyfoovbo)^\n\nIn April 2022, Future Perfect launched Pandemic-Proof, a series focused on \"upgrades we can make today to fight the pandemics of tomorrow\".^[\\[4\\]](#fn59ot6u9l0ue)^\n\nFurther reading\n---------------\n\nMatthews, Dylan (2021) [Future Perfect: past, present, and future](https://kill-the-newsletter.com/alternates/nghud8b907iv3fhs.html), *Future Perfect Newsletter*, October 29.\n\nPiper, Kelsey (2019) [Future Perfect: a year of coverage](https://www.youtube.com/watch?v=7tiAghChX5Q&list=WL&index=60&t=0s), *Effective Altruism Global*, October 19.\n\nWiblin, Robert & Keiran Harris (2019) [Can journalists still write about important things?](https://80000hours.org/podcast/episodes/kelsey-piper-important-advocacy-in-journalism/), *80,000 Hours*, February 27.\n\nExternal links\n--------------\n\n[Future Perfect](https://www.vox.com/future-perfect). Official website\n\nRelated entries\n---------------\n\n[Dylan Matthews](https://forum.effectivealtruism.org/tag/dylan-matthews) | [journalism](https://forum.effectivealtruism.org/tag/journalism) | [Kelsey Piper](https://forum.effectivealtruism.org/tag/kelsey-piper) | [news relevant to effective altruism](https://forum.effectivealtruism.org/topics/news-relevant-to-effective-altruism)\n\n1.  ^**[^](#fnrefwxf8yaepfsr)**^\n    \n    Matthews, Dylan (2018) [Future Perfect, explained](https://www.vox.com/future-perfect/2018/10/15/17924288/future-perfect-explained), *Vox*, October 15.\n    \n2.  ^**[^](#fnref75673c8zv8n)**^\n    \n    Vox (2021) [Masthead](https://www.vox.com/pages/masthead), *Vox*.\n    \n3.  ^**[^](#fnrefk6jqyfoovbo)**^\n    \n    Vox (2021) [Support Future Perfect](https://www.vox.com/2020/1/7/21020439/support-future-perfect), *Vox*, October 29.\n    \n4.  ^**[^](#fnref59ot6u9l0ue)**^\n    \n    Vox Staff (2022) [Pandemic-Proof](https://www.vox.com/23001426/pandemic-proof), *Vox*, April 4."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "dMMW2NCk38S8giAeW",
    "name": "Kelsey Piper",
    "core": false,
    "slug": "kelsey-piper",
    "oldSlugs": null,
    "postCount": 6,
    "description": {
      "markdown": "**Kelsey Piper** is an American journalist, currently a Staff Writer for [Future Perfect](https://forum.effectivealtruism.org/tag/future-perfect).\n\nBackground\n----------\n\nPiper studied symbolic systems at Stanford University. During her studies, she joined [Giving What We Can](https://forum.effectivealtruism.org/tag/giving-what-we-can) and founded Stanford Effective Altruism.^[\\[1\\]](#fnknfqe5hbs9g)^ After graduation, Piper worked as lead of the writing team at Triplebyte, a recruiting and technical screening platform for tech companies.\n\nJournalism career\n-----------------\n\nPiper joined *Vox* in September 2018.^[\\[2\\]](#fnyizq76h6f6k)^ As of August 2022, she has published over 300 articles on an extensive range of topics related to [effective altruism](https://forum.effectivealtruism.org/tag/effective-altruism), including [animal product alternatives](https://forum.effectivealtruism.org/tag/animal-product-alternatives),^[\\[3\\]](#fn10kj59tzmxc)^^[\\[4\\]](#fne41m73ysyqt)^ [cash transfers](https://forum.effectivealtruism.org/tag/cash-transfers),^[\\[5\\]](#fnvu7nfp1io2s)^ [climate change](https://forum.effectivealtruism.org/tag/climate-change),^[\\[6\\]](#fn8uyf3dtxelr)^ [COVID-19 pandemic](https://forum.effectivealtruism.org/tag/covid-19-pandemic),^[\\[7\\]](#fnmfulf1zu838)^^[\\[8\\]](#fnu9g3qiz5v4l)^ [cultured meat](https://forum.effectivealtruism.org/tag/cultured-meat),^[\\[9\\]](#fni3xsd0mx4f)^ [deworming](https://forum.effectivealtruism.org/tag/deworming),^[\\[10\\]](#fnvu97xa3n0wh)^^[\\[11\\]](#fncey4fzjki0c)^ the [Doomsday Clock](https://forum.effectivealtruism.org/tag/bulletin-of-the-atomic-scientists#Doomsday_Clock),^[\\[12\\]](#fnwgvs1fbvjo)^^[\\[13\\]](#fn80pa2f2fffs)^ [electoral reform](https://forum.effectivealtruism.org/tag/electoral-reform),^[\\[14\\]](#fns338jbd5bcn)^^[\\[15\\]](#fnspjacjld6ip)^ [farmed animal welfare](https://forum.effectivealtruism.org/tag/farmed-animal-welfare),^[\\[16\\]](#fnyouxyra5d6)^ [forecasting](https://forum.effectivealtruism.org/tag/forecasting),^[\\[17\\]](#fne7sqjo6c06p)^^[\\[18\\]](#fnj97pccn845q)^[ foreign aid](https://forum.effectivealtruism.org/tag/foreign-aid),^[\\[19\\]](#fn0v9qmjjn1wfn)^^[\\[20\\]](#fn499609hqw6)^ [global catastrophic biological risk](https://forum.effectivealtruism.org/tag/global-catastrophic-biological-risk),^[\\[21\\]](#fn8lpbrp00ehw)^^[\\[22\\]](#fnkguhuckuwl)^ [global catastrophic risk](https://forum.effectivealtruism.org/tag/global-catastrophic-risk),^[\\[23\\]](#fn9tgx1s9pz5b)^^[\\[24\\]](#fnz4lhh6w6rw)^ the [hinge of history hypothesis](https://forum.effectivealtruism.org/tag/hinge-of-history-hypothesis),^[\\[25\\]](#fnlc04z4n6n8k)^ [human extinction](https://forum.effectivealtruism.org/tag/human-extinction),^[\\[26\\]](#fn49n68t8fwf2)^ [information hazards](https://forum.effectivealtruism.org/tag/information-hazard),^[\\[27\\]](#fno1chmbldb2k)^ [malaria](https://forum.effectivealtruism.org/tag/malaria),^[\\[28\\]](#fnlea1pkllkjk)^^[\\[29\\]](#fn3lb4349xryc)^[ ](https://forum.effectivealtruism.org/tag/nuclear-warfare-1)[nuclear winter](https://forum.effectivealtruism.org/tag/nuclear-winter),^[\\[30\\]](#fnfh24raggus5)^^[\\[31\\]](#fns3s1xmw4927)^ [pandemic preparedness](https://forum.effectivealtruism.org/tag/pandemic-preparedness), ^[\\[32\\]](#fn0r6cz4hhcm7)^[prediction markets](https://forum.effectivealtruism.org/tag/prediction-markets),^[\\[33\\]](#fnn0mp3wy3qla)^ [space colonization](https://forum.effectivealtruism.org/tag/space-colonization),^[\\[34\\]](#fn7xnwdb7pky2)^ the [timing of philanthropy](https://forum.effectivealtruism.org/tag/timing-of-philanthropy),^[\\[35\\]](#fnbe7r528lyur)^^[\\[36\\]](#fnmncvmpxbzon)^ [universal basic income](https://forum.effectivealtruism.org/tag/universal-basic-income),^[\\[37\\]](#fnete9k00nz9g)^^[\\[38\\]](#fn7ugtl66sjjd)^ and the [vulnerable world hypothesis](https://forum.effectivealtruism.org/tag/vulnerable-world-hypothesis),^[\\[39\\]](#fnllmsvq9km3i)^ among many others.^[\\[40\\]](#fnytfsz4pb8b)^\n\nPiper's article outlining the case for taking [artificial intelligence](https://forum.effectivealtruism.org/tag/artificial-intelligence) as an [existential risk](https://forum.effectivealtruism.org/tag/existential-risk) seriously has been praised by many as a rigorous yet accessible introduction to the topic.^[\\[41\\]](#fnt2zh7zt5kx7)^\n\nIn early February 2020, when less than a dozen [COVID-19](https://forum.effectivealtruism.org/tag/covid-19-pandemic) cases had been confirmed in the United States and many media outlets and health authorities were dismissive of the risks posed by SARS-CoV-2,^[\\[42\\]](#fnm1gt30axfg)^^[\\[43\\]](#fnvpsp57ryld)^^[\\[44\\]](#fnjtfixf0nx1f)^ Piper wrote about the possibility that it might become a [global pandemic](https://forum.effectivealtruism.org/tag/pandemic-preparedness) and emphasized the importance of an early response.^[\\[7\\]](#fnmfulf1zu838)^^[\\[45\\]](#fn5gd1hk9id5a)^\n\nFurther reading\n---------------\n\nGalef, Julia (2019) [Big picture journalism: Covering the topics that matter in the long run (Kelsey Piper)](https://play.acast.com/s/rationallyspeaking/0B113F44-C46B-4913-9964-5949B50EA156), *Rationally Speaking*, April 2.\n\nGalef, Julia (2021) [How to reason about COVID, and other hard things (Kelsey Piper)](http://rationallyspeakingpodcast.org/258-how-to-reason-about-covid-kelsey-piper/), *Rationally Speaking*, September 13.\n\nLindmark, Rhys (2019) [Kelsey Piper, Vox: Effective altruist news, memetic immunity, questions social justice can answer](https://podcasts.apple.com/us/podcast/13-kelsey-piper-vox-effective-altruist-news-memetic/id1254196635?i=1000443971258), *Grey Mirror*, July 8.\n\nWiblin, Robert & Keiran Harris (2019) [Can journalists still write about important things?](https://80000hours.org/podcast/episodes/kelsey-piper-important-advocacy-in-journalism/), *80,000 Hours*, February 27.\n\nExternal links\n--------------\n\n[Kelsey Piper](https://www.vox.com/authors/kelsey-piper). *Vox* profile.\n\n[Kelsey Piper](https://forum.effectivealtruism.org/users/kelsey-piper-1). [Effective Altruism Forum](https://forum.effectivealtruism.org/tag/effective-altruism-forum-1) account.\n\n[The Unit of Caring](https://theunitofcaring.tumblr.com/). Piper's blog.\n\nRelated entries\n---------------\n\n[Dylan Matthews](https://forum.effectivealtruism.org/tag/dylan-matthews) | [Future Perfect](https://forum.effectivealtruism.org/tag/future-perfect) | [journalism](https://forum.effectivealtruism.org/tag/journalism)\n\n1.  ^**[^](#fnrefknfqe5hbs9g)**^\n    \n    Zabel, Claire (2015) [A different take on giving back](https://www.stanforddaily.com/2015/02/05/a-different-take-on-giving-back/), *The Stanford Daily*, February 5.\n    \n2.  ^**[^](#fnrefyizq76h6f6k)**^\n    \n    Piper, Kelsey (2018) [This summer, Vox announced they’re going to launch a new department dedicated to writing about effective altruism](https://theunitofcaring.tumblr.com/post/178128805796/this-summer-vox-announced-theyre-going-to-launch), *The Unit of Caring*, September 15.\n    \n3.  ^**[^](#fnref10kj59tzmxc)**^\n    \n    Piper, Kelsey (2019) [The rise of meatless meat, explained](https://www.vox.com/2019/5/28/18626859/meatless-meat-explained-vegan-impossible-burger), *Vox*, May 28.\n    \n4.  ^**[^](#fnrefe41m73ysyqt)**^\n    \n    Piper, Kelsey (2020) [The next challenge for plant-based meat: Winning the price war against animal meat](https://www.vox.com/future-perfect/21366607/beyond-impossible-plant-based-meat-factory-farming), *Vox*, August 18.\n    \n5.  ^**[^](#fnrefvu7nfp1io2s)**^\n    \n    Piper, Kelsey (2020) [This charity is giving cash directly to Americans suffering during the coronavirus crisis](https://www.vox.com/future-perfect/2020/3/20/21186007/coronavirus-pandemic-donate-help-cash-benefits), *Vox*, March 20.\n    \n6.  ^**[^](#fnref8uyf3dtxelr)**^\n    \n    Piper, Kelsey (2019) [Is climate change an “existential threat” — or just a catastrophic one?](https://www.vox.com/future-perfect/2019/6/13/18660548/climate-change-human-civilization-existential-risk), June 28.\n    \n7.  ^**[^](#fnrefmfulf1zu838)**^\n    \n    Piper, Kelsey (2020) [Don’t scold people for worrying about the coronavirus](https://www.vox.com/future-perfect/2020/2/6/21121303/coronavirus-wuhan-panic-pandemic-outbreak), *Vox*, February 6.\n    \n8.  ^**[^](#fnrefu9g3qiz5v4l)**^\n    \n    Piper, Kelsey (2021) [How bad research clouded our understanding of Covid-19](https://www.vox.com/future-perfect/22776428/ivermectin-science-publication-research-fraud), *Vox*, December 17.\n    \n9.  ^**[^](#fnrefi3xsd0mx4f)**^\n    \n    Piper, Kelsey (2020) [Singapore is the first country in the world to approve lab-grown chicken products](https://www.vox.com/future-perfect/2020/12/2/22125518/lab-grown-chicken-meat-singapore-bioreactor-approve), *Vox*, December 2.\n    \n10.  ^**[^](#fnrefvu97xa3n0wh)**^\n    \n    Piper, Kelsey (2020) [A new study finds that giving kids deworming treatment still benefits them 20 years later](https://www.vox.com/future-perfect/2020/8/6/21354847/kremer-miguel-worms-deworming), *Vox*, August 6.\n    \n11.  ^**[^](#fnrefcey4fzjki0c)**^\n    \n    Piper, Kelsey (2022) [The return of the “worm wars”](https://www.vox.com/future-perfect/2022/7/19/23268786/deworming-givewell-effective-altruism-michael-hobbes), *Vox*, July 19.\n    \n12.  ^**[^](#fnrefwgvs1fbvjo)**^\n    \n    Piper, Kelsey (2019) [Doomsday clock creators: “We’re playing Russian roulette with humanity”](https://www.vox.com/future-perfect/2019/1/24/18195527/doomsday-clock-2019-two-minutes-from-midnight), *Vox*, January 24.\n    \n13.  ^**[^](#fnref80pa2f2fffs)**^\n    \n    Piper, Kelsey (2020) [The Doomsday Clock is now at “100 seconds to midnight.” Here’s what that means.](https://www.vox.com/future-perfect/2020/1/23/21079028/climate-change-extinction-nuclear-war-ai-existential-risk-doomsday-clock), *Vox*, January 23.\n    \n14.  ^**[^](#fnrefs338jbd5bcn)**^\n    \n    Piper, Kelsey (2018) [This city just approved a new election system never tried before in America](https://www.vox.com/future-perfect/2018/11/15/18092206/midterm-elections-vote-fargo-approval-voting-ranked-choice), *Vox*, November 15.\n    \n15.  ^**[^](#fnrefspjacjld6ip)**^\n    \n    Piper, Kelsey (2020) [California’s ballot initiative system isn’t working. How do we fix it?](https://www.vox.com/future-perfect/2020/11/6/21549654/california-ballot-initiative-proposition-direct-democracy), *Vox*, November 6.\n    \n16.  ^**[^](#fnrefyouxyra5d6)**^\n    \n    Piper, Kelsey (2020) [Farms have bred chickens so large that they’re in constant pain](https://www.vox.com/future-perfect/21437054/chickens-factory-farming-animal-cruelty-welfare), *Vox*, September 23.\n    \n17.  ^**[^](#fnrefe7sqjo6c06p)**^\n    \n    Piper, Kelsey (2019) [How to get better at predicting the future](https://www.vox.com/future-perfect/2019/4/5/18290870/forecasting-tetlock-prediction-markets-betting), *Vox*, April 5.\n    \n18.  ^**[^](#fnrefj97pccn845q)**^\n    \n    Matthews, Dylan, Kelsey Piper & Sigal Samuel (2022) [22 things we think will happen in 2022](https://www.vox.com/future-perfect/22824620/predicting-midterms-covid-roe-wade-oscars-2022), *Vox*, January 1.\n    \n19.  ^**[^](#fnref0v9qmjjn1wfn)**^\n    \n    Piper, Kelsey (2019) [Why foreign aid is getting better at saving lives](https://www.vox.com/2019/5/7/18528724/billionaires-charity-aid-kumar), *Vox*, May 7.\n    \n20.  ^**[^](#fnref499609hqw6)**^\n    \n    Piper, Kelsey (2020) [Some aid gets misdirected. Did the World Bank try to suppress a paper saying so?](https://www.vox.com/future-perfect/2020/2/21/21144624/development-aid-world-bank-paper), *Vox*, February 21.\n    \n21.  ^**[^](#fnref8lpbrp00ehw)**^\n    \n    Piper, Kelsey (2020) [The next deadly pathogen could come from a rogue scientist. Here’s how we can prevent that.](https://www.vox.com/future-perfect/2020/2/11/21076585/dna-synthesis-assembly-viruses-biosecurity), *Vox*, February 11.\n    \n22.  ^**[^](#fnrefkguhuckuwl)**^\n    \n    Piper, Kelsey (2020) [Why some labs work on making viruses deadlier — and why they should stop](https://www.vox.com/2020/5/1/21243148/why-some-labs-work-on-making-viruses-deadlier-and-why-they-should-stop), *Vox*, May 1.\n    \n23.  ^**[^](#fnref9tgx1s9pz5b)**^\n    \n    Piper, Kelsey (2019) [Why governments are bad at facing catastrophic risks — and how they could get better](https://www.vox.com/future-perfect/2019/8/23/20828408/global-catastrophic-risks-national-policy), *Vox*, August 23.\n    \n24.  ^**[^](#fnrefz4lhh6w6rw)**^\n    \n    Piper, Kelsey (2022) [The most interesting thing *Don’t Look Up* has to say about the apocalypse](https://www.vox.com/future-perfect/2022/1/8/22872066/dont-look-up-mckay-dicaprio-existential-risk-apocalypse), *Vox*, January 8.\n    \n25.  ^**[^](#fnreflc04z4n6n8k)**^\n    \n    Piper, Kelsey (2019) [Is this the most important century in human history?](https://www.vox.com/2019/9/26/20880334/is-this-the-most-important-century-in-human-history), *Vox*, September 26.\n    \n26.  ^**[^](#fnref49n68t8fwf2)**^\n    \n    Piper, Kelsey (2019) [Human extinction would be a uniquely awful tragedy. Why don’t we act like it?](https://www.vox.com/future-perfect/2019/11/7/20903337/human-extinction-pessimism-hopefulness-future), *Vox*, November 7.\n    \n27.  ^**[^](#fnrefo1chmbldb2k)**^\n    \n    Piper, Kelsey (2022) [When scientific information is dangerous](https://www.vox.com/future-perfect/2022/3/30/23001712/ai-research-virus-scientific-information-dangerous), *Vox*, March 30.\n    \n28.  ^**[^](#fnreflea1pkllkjk)**^\n    \n    Piper, Kelsey (2021) [The new malaria vaccine is a total game changer](https://www.vox.com/future-perfect/22399386/malaria-vaccine-r21mm-public-health-global-child-mortality), *Vox*, April 28.\n    \n29.  ^**[^](#fnref3lb4349xryc)**^\n    \n    Piper, Kelsey (2021) [Why the WHO approval of the first malaria vaccine is a big deal](https://www.vox.com/future-perfect/2021/10/6/22712898/who-malaria-vaccine-approval), *Vox*, October 6.\n    \n30.  ^**[^](#fnreffh24raggus5)**^\n    \n    Piper, Kelsey (2019) [The man who wants to save humanity from nuclear winter](https://www.vox.com/future-perfect/2019/7/25/20707644/nuclear-winter-famine-apocalypse-allfed), *Vox*, July 25.\n    \n31.  ^**[^](#fnrefs3s1xmw4927)**^\n    \n    Piper, Kelsey (2019) [Study: a nuclear war between India and Pakistan could lead to a mini-nuclear winter](https://www.vox.com/future-perfect/2019/10/9/20903418/study-nuclear-war-india-pakistan-could-lead-to-mini-nuclear-winter), *Vox*, October 9.\n    \n32.  ^**[^](#fnref0r6cz4hhcm7)**^\n    \n    Piper, Kelsey (2022) [What the media needs to get right in the next pandemic](https://www.vox.com/future-perfect/23017140/media-journalists-pandemic-covid-lab-leak-coronavirus), *Vox*, April 10.\n    \n33.  ^**[^](#fnrefn0mp3wy3qla)**^\n    \n    Piper, Kelsey (2020) [Why prediction markets are bad at predicting who’ll be president](https://www.vox.com/future-perfect/2020/2/14/21137882/prediction-markets-bloomberg-sanders-president), *Vox*, February 14.\n    \n34.  ^**[^](#fnref7xnwdb7pky2)**^\n    \n    Piper, Kelsey (2018) [Jeff Bezos and Elon Musk want to colonize space to save humanity](https://www.vox.com/future-perfect/2018/10/22/17991736/jeff-bezos-elon-musk-colonizing-mars-moon-space-blue-origin-spacex), *Vox*, October 22.\n    \n35.  ^**[^](#fnrefbe7r528lyur)**^\n    \n    Piper, Kelsey (2020) [Don’t wait: The case for giving sooner rather than later](https://www.vox.com/future-perfect/21742988/donations-deciding-when-give), *Vox*, November 30.\n    \n36.  ^**[^](#fnrefmncvmpxbzon)**^\n    \n    Piper, Kelsey (2021) [Should charities spend your money now — or save it to help people later?](https://www.vox.com/future-perfect/22822406/give-well-give-directly-philanthropy), *Vox*, December 10.\n    \n37.  ^**[^](#fnrefete9k00nz9g)**^\n    \n    Piper, Kelsey (2019) [The important questions about universal basic income haven’t been answered yet](https://www.vox.com/future-perfect/2019/2/13/18220838/universal-basic-income-ubi-nber-study), *Vox*, February 13.\n    \n38.  ^**[^](#fnref7ugtl66sjjd)**^\n    \n    Piper, Kelsey (2020) [How a basic income experiment helped these Kenyans weather the Covid-19 crisis](https://www.vox.com/future-perfect/2020/9/2/21409142/basic-income-kenya-weather-covid-19-crisis), *Vox*, September 2.\n    \n39.  ^**[^](#fnrefllmsvq9km3i)**^\n    \n    Piper, Kelsey (2018) [How technological progress is making it likelier than ever that humans will destroy ourselves](https://www.vox.com/future-perfect/2018/11/19/18097663/nick-bostrom-vulnerable-world-global-catastrophic-risks), *Vox*, November 19.\n    \n40.  ^**[^](#fnrefytfsz4pb8b)**^\n    \n    Vox (2022) [Kelsey Piper profile and activity](https://www.vox.com/authors/kelsey-piper), *Vox*.\n    \n41.  ^**[^](#fnreft2zh7zt5kx7)**^\n    \n    Piper, Kelsey (2018) [The case for taking AI seriously as a threat to humanity](https://www.vox.com/future-perfect/2018/12/21/18126576/ai-artificial-intelligence-machine-learning-safety-alignment), *Vox*, December 21.\n    \n42.  ^**[^](#fnrefm1gt30axfg)**^\n    \n    Vox (2020) [Is this going to be a deadly pandemic? No](https://archive.is/LH4Ff), *Twitter*, January 31.\n    \n43.  ^**[^](#fnrefvpsp57ryld)**^\n    \n    Parmet, Wendy & Michael Sinha (2020) [Why we should be wary of an aggressive government response to coronavirus](https://www.washingtonpost.com/outlook/2020/02/03/why-we-should-be-wary-an-aggressive-government-response-coronavirus/), *Washington Post*, February 3.\n    \n44.  ^**[^](#fnrefjtfixf0nx1f)**^\n    \n    [Scott Alexander](https://forum.effectivealtruism.org/topics/scott-alexander) lists many additional examples in Alexander, Scott (2020) [A failure, but not of prediction](https://slatestarcodex.com/2020/04/14/a-failure-but-not-of-prediction/), *Slate Star Codex*, April 15.\n    \n45.  ^**[^](#fnref5gd1hk9id5a)**^\n    \n    In a piece published two years later, Piper says that she is \"proud of \\[the article\\] overall\", but regrets having dismissed the lab-leak hypothesis as a \"conspiracy theory\".^[\\[32\\]](#fn0r6cz4hhcm7)^"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "o7e4gLn9WpEhAruze",
    "name": "Julia Galef",
    "core": false,
    "slug": "julia-galef",
    "oldSlugs": null,
    "postCount": 12,
    "description": {
      "markdown": "**Julia Galef** (born 4 July 1983) is an American author and podcaster. She is the co-founder and former president of the [Center for Applied Rationality](https://forum.effectivealtruism.org/tag/center-for-applied-rationality), the host of the *Rationally Speaking* podcast, and the author of *The Scout Mindset.*\n\nBackground\n----------\n\nGalef studied statistics at Columbia University. After completing her studies, she worked as a freelance journalist for various publications.^[\\[1\\]](#fn62bqcn8s8bm)^ In 2012, she co-founded the Center for Applied Rationality,^[\\[2\\]](#fnrwe6osumtep)^ and served as that organization's president until 2016.^[\\[3\\]](#fnqalta20yp1n)^\n\n*Rationally Speaking*\n---------------------\n\nIn 2010, Galef and her co-host Massimo Pigliucci launched *Rationally Speaking*, a podcast about rationality, skepticism, and other topics. In 2015, Pigliucci left, and Galef became the podcast's sole host and producer.^[\\[4\\]](#fnolew4wy4zr)^\n\nPast guests of the show include [Bryan Caplan](https://forum.effectivealtruism.org/tag/bryan-caplan),^[\\[5\\]](#fnq23r80tqzk)^^[\\[6\\]](#fn60wdsjp2kt3)^^[\\[7\\]](#fnhigr1gr3y7i)^ [Tyler Cowen](https://forum.effectivealtruism.org/tag/tyler-cowen),^[\\[8\\]](#fnbzhwyd9ae4k)^ [Robin Hanson](https://forum.effectivealtruism.org/tag/robin-hanson),^[\\[9\\]](#fnbon628tat8f)^ [Holden Karnofsky](https://forum.effectivealtruism.org/tag/holden-karnofsky),^[\\[10\\]](#fnbyg62des6ks)^ [William MacAskill](https://forum.effectivealtruism.org/tag/william-macaskill),^[\\[11\\]](#fn0stxeyy541b)^ [Dylan Matthews](https://forum.effectivealtruism.org/tag/dylan-matthews),^[\\[12\\]](#fn5g3gsjvjby9)^^[\\[13\\]](#fn3ytxeuc3e74)^ [Toby Ord](https://forum.effectivealtruism.org/tag/toby-ord),^[\\[14\\]](#fnagy8esbyx4f)^ [Kelsey Piper](https://forum.effectivealtruism.org/tag/kelsey-piper),^[\\[15\\]](#fnn0qqkxmyol9)^^[\\[16\\]](#fnc3lygnxvr5t)^ [Anders Sandberg](https://forum.effectivealtruism.org/tag/anders-sandberg),^[\\[17\\]](#fn1ak29t5t4jhj)^ [Peter Singer](https://forum.effectivealtruism.org/tag/peter-singer),^[\\[18\\]](#fnwjww3cvqyh)^ and [Philip Tetlock](https://forum.effectivealtruism.org/tag/philip-tetlock).^[\\[19\\]](#fnlsgyzzkwjo)^\n\n*The Scout Mindset*\n-------------------\n\nIn 2021, Galef published her first book, *The Scout Mindset: Why Some People See Things Clearly and Others Don’t* .\n\nThe book's title refers to a trait, common among people with good judgment, which Galef defines as \"the motivation to see things as they are, not as you wish they were.\"^[\\[20\\]](#fn7w1pwwcjbih)^ Galef contrasts this *scout mindset* to a *soldier mindset*. Unlike scouts, who seek out evidence that improves the accuracy of their beliefs and treat being wrong as an opportunity to revise their model of the world, soldiers seek out evidence that confirms their existing beliefs and treat being wrong as a humiliating defeat.^[\\[21\\]](#fns6nbwtrspkn)^\n\n*The Scout Mindset* has received praise from [Philip Tetlock](https://forum.effectivealtruism.org/tag/philip-tetlock),^[\\[22\\]](#fn6n7uusiv9sj)^ [Scott Alexander](https://forum.effectivealtruism.org/tag/scott-alexander),^[\\[23\\]](#fn4u9wj5ae69i)^ and [Dylan Matthews](https://forum.effectivealtruism.org/tag/dylan-matthews),^[\\[24\\]](#fnj1rm19me8i)^ among others.\n\nThe organization [80,000 Hours](https://forum.effectivealtruism.org/tag/80-000-hours) has cited Galef as an example of a highly successful career in advocacy for [improved institutional decision-making](https://forum.effectivealtruism.org/topics/improving-institutional-decision-making).^[\\[25\\]](#fnv16xncn9vv)^\n\nFurther reading\n---------------\n\nBensinger, Rob (2021) [Outline of Galef’s ‘Scout Mindset’](https://forum.effectivealtruism.org/posts/HDAXztEbjJsyHLKP7/outline-of-galef-s-scout-mindset), *Effective Altruism Forum*, August 9.\n\nMatthews, Dylan (2021) [The book that changed how I think about thinking: A conversation with writer Julia Galef on how to think less like a soldier and more like a scout](https://www.vox.com/future-perfect/22410374/julia-galef-book-scout-mindset-interview-think), *Vox*, May 3.\n\nWiblin, Robert (2017) [Julia Galef on making humanity more rational, what EA does wrong, and why Twitter isn’t all bad](https://80000hours.org/podcast/episodes/is-it-time-for-a-new-scientific-revolution-julia-galef-on-how-to-make-humans-smarter/), *80,000 Hours*, September 13.\n\nExternal links\n--------------\n\n[Julia Galef](https://juliagalef.com/). Official website.\n\n[Rationally Speaking](http://rationallyspeakingpodcast.org/). Official website.\n\nRelated entries\n---------------\n\n[Center for Applied Rationality](https://forum.effectivealtruism.org/tag/center-for-applied-rationality) | [rationality community](https://forum.effectivealtruism.org/tag/rationality-community)\n\n1.  ^**[^](#fnref62bqcn8s8bm)**^\n    \n    Galef, Julia (2015) [My story](https://juliagalef.com/about-me/), *Julia Galef’s Website*.\n    \n2.  ^**[^](#fnrefrwe6osumtep)**^\n    \n    \"CFAR is a 501(c)(3) non-profit operating out of Berkeley, California, originally founded in 2012 by Anna Salamon, Julia Galef, Valentine Smith, and Andrew Critch.\" (Center for Applied Rationality (2022) [Mission](http://www.rationality.org/about/mission/), *Center for Applied Rationality*)\n    \n3.  ^**[^](#fnrefqalta20yp1n)**^\n    \n    Galef, Julia (2017) [I’m overdue for a career update!](https://www.facebook.com/julia.galef/posts/10103010525130302), *Facebook*, February 28.\n    \n4.  ^**[^](#fnrefolew4wy4zr)**^\n    \n    Galef, Julia (2021) [About the podcast](http://rationallyspeakingpodcast.org/about-the-podcast/), *Rationally Speaking*.\n    \n5.  ^**[^](#fnrefq23r80tqzk)**^\n    \n    Galef, Julia (2015) [Does parenting matter? (Bryan Caplan)](http://rationallyspeakingpodcast.org/144-does-parenting-matter-bryan-caplan/), *Rationally Speaking*, October 4.\n    \n6.  ^**[^](#fnref60wdsjp2kt3)**^\n    \n    Galef, Julia (2018) [The case against education (Bryan Caplan)](http://rationallyspeakingpodcast.org/202-the-case-against-education-bryan-caplan/), *Rationally Speaking*, February 18.\n    \n7.  ^**[^](#fnrefhigr1gr3y7i)**^\n    \n    Galef, Julia (2019) [The case for open borders (Bryan Caplan)](http://rationallyspeakingpodcast.org/242-why-consciousness-is-an-illusion-keith-frankish/), *Rationally Speaking*, November 11.\n    \n8.  ^**[^](#fnrefbzhwyd9ae4k)**^\n    \n    Galef, Julia (2019) [Defending big business against its critics (Tyler Cowen)](http://rationallyspeakingpodcast.org/232-defending-big-business-against-its-critics-tyler-cowen/), *Rationally Speaking*, April 29.\n    \n9.  ^**[^](#fnrefbon628tat8f)**^\n    \n    Galef, Julia (2015) [Most human behavior is signaling (Robin Hanson)](http://rationallyspeakingpodcast.org/135-most-human-behavior-is-signaling-robin-hanson/), *Rationally Speaking*, May 31.\n    \n10.  ^**[^](#fnrefbyg62des6ks)**^\n    \n    Galef, Julia & Massimo Pigliucci (2011) [Evidence-based philanthropy (Holden Karnofsky)](http://rationallyspeakingpodcast.org/38-evidence-based-philanthropy-holden-karnofsky/), *Rationally Speaking*, July 3.\n    \n11.  ^**[^](#fnref0stxeyy541b)**^\n    \n    Galef, Julia (2017) [Moral uncertainty (Will MacAskill)](http://rationallyspeakingpodcast.org/181-moral-uncertainty-will-macaskill/), *Rationally Speaking*, April 2.\n    \n12.  ^**[^](#fnref5g3gsjvjby9)**^\n    \n    Galef, Julia (2017) [The science and ethics of kidney donation (Dylan Matthews)](http://rationallyspeakingpodcast.org/177-the-science-and-ethics-of-kidney-donation-dylan-matthews/), *Rationally Speaking*, February 5.\n    \n13.  ^**[^](#fnref3ytxeuc3e74)**^\n    \n    Galef, Julia (2019) [Global poverty has fallen, but what should we conclude from that? (Dylan Matthews)](http://rationallyspeakingpodcast.org/234-global-poverty-has-fallen-but-what-should-we-conclude-from-that-dylan-matthews/), *Rationally Speaking*, May 27.\n    \n14.  ^**[^](#fnrefagy8esbyx4f)**^\n    \n    Galef, Julia (2021) [Humanity on the precipice (Toby Ord)](http://rationallyspeakingpodcast.org/262-humanity-on-the-precipice-toby-ord/), *Rationally Speaking*, December 10.\n    \n15.  ^**[^](#fnrefn0qqkxmyol9)**^\n    \n    Galef, Julia (2019) [Big picture journalism: Covering the topics that matter in the long run (Kelsey Piper)](https://play.acast.com/s/rationallyspeaking/0B113F44-C46B-4913-9964-5949B50EA156), *Rationally Speaking*, April 2.\n    \n16.  ^**[^](#fnrefc3lygnxvr5t)**^\n    \n    Galef, Julia (2021) [How to reason about COVID, and other hard things (Kelsey Piper)](http://rationallyspeakingpodcast.org/258-how-to-reason-about-covid-kelsey-piper/), *Rationally Speaking*, September 13.\n    \n17.  ^**[^](#fnref1ak29t5t4jhj)**^\n    \n    Galef, Julia (2018) [The long-term future of humanity (Anders Sandberg)](http://rationallyspeakingpodcast.org/215-the-long-term-future-of-humanity-anders-sandberg/), *Rationally Speaking*, August 19.\n    \n18.  ^**[^](#fnrefwjww3cvqyh)**^\n    \n    Galef, Julia (2013) [Being a utilitarian in the real world (Peter Singer)](http://rationallyspeakingpodcast.org/97-being-a-utilitarian-in-the-real-world-peter-singer/), *Rationally Speaking*, November 24.\n    \n19.  ^**[^](#fnreflsgyzzkwjo)**^\n    \n    Galef, Julia (2015) [Superforecasting: The art and science of prediction (Philip Tetlock)](http://rationallyspeakingpodcast.org/145-superforecasting-the-art-and-science-of-prediction-philip-tetlock/), *Rationally Speaking*, October 20.\n    \n20.  ^**[^](#fnref7w1pwwcjbih)**^\n    \n    Galef, Julia (2021) [*The Scout Mindset: Why Some People See Things Clearly and Others Don’t*](https://en.wikipedia.org/wiki/Special:BookSources/9780735217553), New York: Portfolio, p. ix.\n    \n21.  ^**[^](#fnrefs6nbwtrspkn)**^\n    \n    Galef, [*The Scout Mindset,*](https://en.wikipedia.org/wiki/Special:BookSources/9780735217553) p. 14.\n    \n22.  ^**[^](#fnref6n7uusiv9sj)**^\n    \n    Tetlock, Philip E. (2021) [The scouts in Julia Galef's superb new book, Scout Mindset, are the heroes—just as the superforecasters were in Superforecasting.](https://twitter.com/PTetlock/status/1382311316380090369), *Twitter*, April 14.\n    \n23.  ^**[^](#fnref4u9wj5ae69i)**^\n    \n    Alexander, Scott (2021) [Book Review: The Scout Mindset](https://astralcodexten.substack.com/p/book-review-the-scout-mindset), *Astral Codex Ten*, September 29.\n    \n24.  ^**[^](#fnrefj1rm19me8i)**^\n    \n    Matthews, Dylan (2021) [The book that changed how I think about thinking: A conversation with writer Julia Galef on how to think less like a soldier and more like a scout](https://www.vox.com/future-perfect/22410374/julia-galef-book-scout-mindset-interview-think), *Vox*, May 3.\n    \n25.  ^**[^](#fnrefv16xncn9vv)**^\n    \n    Whittlestone, Jess (2017) [Improving institutional decision-making](https://80000hours.org/problem-profiles/improving-institutional-decision-making/), *80,000 Hours*, September."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "Y9yK5y6imSek2PGBK",
    "name": "John Stuart Mill",
    "core": false,
    "slug": "john-stuart-mill",
    "oldSlugs": null,
    "postCount": 2,
    "description": {
      "markdown": "**John Stuart Mill** (20 May 1806 - 7 May 1873) was an English philosopher and economist. A student of [Jeremy Bentham](https://forum.effectivealtruism.org/tag/jeremy-bentham), Mill promoted the ideas of [utilitarianism](https://forum.effectivealtruism.org/tag/utilitarianism) and liberalism and has been called “the most influential English language philosopher of the nineteenth century”. His most influential works include his books *Utilitarianism* (1863) and *On Liberty* (1859).\n\nFurther reading\n---------------\n\nHeydt, Colin (2006) [John Stuart Mill (1806—1873)](https://iep.utm.edu/milljs/), *Internet Encyclopedia of Philosophy*, October 24.\n\nMacAskill, William & Darius Meissner (2020) [John Stuart Mill](https://www.utilitarianism.net/utilitarian-thinker/john-stuart-mill), *Utilitarianism*.\n\nMacleod, Christopher (2016) [John Stuart Mill](https://plato.stanford.edu/entries/mill/), *Stanford Encyclopedia of Philosophy*, August 25 (updated 14 May 2020).\n\nMill, John Stuart (1963–91) [*Collected Works of John Stuart Mill*](https://oll.libertyfund.org/title/robson-collected-works-of-john-stuart-mill-in-33-vols), Toronto: University of Toronto Press.\n\nRelated entries\n---------------\n\n[Jeremy Bentham](https://forum.effectivealtruism.org/tag/jeremy-bentham) | [utilitarianism](https://forum.effectivealtruism.org/tag/utilitarianism)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "ynDqsZWwbsgpyo8MR",
    "name": "Jeremy Bentham",
    "core": false,
    "slug": "jeremy-bentham",
    "oldSlugs": null,
    "postCount": 5,
    "description": {
      "markdown": "**Jeremy Bentham** (15 February 1748 - 6 June 1832) was an English philosopher and social reformer regarded as the founder of modern [utilitarianism](https://forum.effectivealtruism.org/tag/utilitarianism).\n\nFurther reading\n---------------\n\nBentham, Jeremy (1838–43) [*The Works of Jeremy Bentham, Published under the Superintendence of his Executor, John Bowring*](https://oll.libertyfund.org/page/bentham-toc), Edinburgh: William Tait.\n\nCrimmins, James E. (2015) [Jeremy Bentham](https://plato.stanford.edu/entries/bentham/), *Stanford Encyclopedia of Philosophy*, March 17 (updated 4 May 2021).\n\nMacAskill, William & Darius Meissner (2020) [Jeremy Bentham](https://www.utilitarianism.net/utilitarian-thinker/jeremy-bentham), *Utilitarianism*.\n\nSweet, William (2001) [Jeremy Bentham (1748—1832)](https://iep.utm.edu/bentham/), *Internet Encyclopedia of Philosophy*, April 11.\n\nThe Bentham Project (1999) [About Jeremy Bentham](https://www.ucl.ac.uk/bentham-project/about-jeremy-bentham), *The Bentham Project*.\n\nRelated entries\n---------------\n\n[classical utilitarianism](https://forum.effectivealtruism.org/topics/classical-utilitarianism) | [hedonism](https://forum.effectivealtruism.org/tag/hedonism) | [John Stuart Mill](https://forum.effectivealtruism.org/tag/john-stuart-mill) | [utilitarianism](https://forum.effectivealtruism.org/tag/utilitarianism)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "vXvpw3X9Cgne9extC",
    "name": "Jeff McMahan",
    "core": false,
    "slug": "jeff-mcmahan",
    "oldSlugs": null,
    "postCount": 2,
    "description": {
      "markdown": "**Jefferson Allen McMahan** (born 30 August 1954) is an American philosopher. He is currently Sekyra and White’s Professor of Moral Philosophy at the University of Oxford.\n\nMcMahan has published extensively on topics of interest to [effective altruism](https://forum.effectivealtruism.org/tag/effective-altruism), such as [population ethics](https://forum.effectivealtruism.org/tag/population-ethics);^[\\[1\\]](#fnj2rm6yr1zt)^^[\\[2\\]](#fn1ws8v7ernn)^ [existential risk](https://forum.effectivealtruism.org/tag/existential-risk) from [nuclear war](https://forum.effectivealtruism.org/tag/nuclear-warfare-1);^[\\[3\\]](#fn8cq3ds3kbii)^^[\\[4\\]](#fnqp2tphimr7f)^ animal ethics^[\\[5\\]](#fnfxljrwf970a)^—including the [ethics of eating animals](https://forum.effectivealtruism.org/tag/farmed-animal-welfare),^[\\[6\\]](#fnvdt6l3g50al)^ the [moral status of animals](https://forum.effectivealtruism.org/tag/moral-patienthood),^[\\[7\\]](#fnd42mxs1atrp)^ and [wild animal welfare](https://forum.effectivealtruism.org/tag/wild-animal-welfare)^[\\[8\\]](#fnk5npm3pgqog)^^[\\[9\\]](#fnujos1dzad2c)^^[\\[10\\]](#fny03s3vkjp)^^[\\[11\\]](#fns7mhsjii1gm)^—; and the [demandingness of morality](https://forum.effectivealtruism.org/tag/demandingness-of-morality).^[\\[12\\]](#fn3knomd40tap)^ He has also published a response to various [criticisms of effective altruism](https://forum.effectivealtruism.org/tag/criticism-of-effective-altruism).^[\\[13\\]](#fnmp9o8v18kpi)^\n\nFurther reading\n---------------\n\nPetersen, Thomas & Jesper Ryberg (2007) [Jeff McMahan](https://en.wikipedia.org/wiki/Special:BookSources/9788792130006), in Thomas Petersen & Jesper Ryberg (eds.) *Normative Ethics: 5 Questions*, New York: Automatic Press, pp. 67–75.\n\nUniversity of Oxford (2020) [Professor Jeff McMahan](https://www.ox.ac.uk/research/research-in-conversation/how-live-happy-life/professor-jeff-mcmahan), *University of Oxford*.\n\nExternal links\n--------------\n\n[Jeff McMahan](https://www.philosophy.ox.ac.uk/people/jeff-mcmahan). Official website.\n\nRelated entries\n---------------\n\n[Derek Parfit](https://forum.effectivealtruism.org/tag/derek-parfit) | [population ethics](https://forum.effectivealtruism.org/tag/population-ethics) | [wild animal welfare](https://forum.effectivealtruism.org/tag/wild-animal-welfare)\n\n1.  ^**[^](#fnrefj2rm6yr1zt)**^\n    \n    McMahan, Jeff (1981) [Problems of population theory](http://doi.org/10.1086/292301), *Ethics*, vol. 92, pp. 96–127.\n    \n2.  ^**[^](#fnref1ws8v7ernn)**^\n    \n    McMahan, Jeff (2013) [Causing people to exist and saving people’s lives](http://doi.org/10.1007/s10892-012-9139-1), *Journal of Ethics*, vol. 17, pp. 5–35.\n    \n3.  ^**[^](#fnref8cq3ds3kbii)**^\n    \n    McMahan, Jeff (1986) [Nuclear deterrence and future generations](https://en.wikipedia.org/wiki/Special:BookSources/9780847672585), in Avner Cohen & Steven Lee (eds.) *Nuclear Weapons and the Future of Humanity: The Fundamental Questions*, Totowa, New Jersey: Rowman & Allanheld, pp. 319–339.\n    \n4.  ^**[^](#fnrefqp2tphimr7f)**^\n    \n    McMahan, Jeff (1981) [*British Nuclear Weapons: For and Against*](https://en.wikipedia.org/wiki/Special:BookSources/9780862450496), London: Junction Books.\n    \n5.  ^**[^](#fnreffxljrwf970a)**^\n    \n    McMahan, Jeff (2003) [Animals](https://en.wikipedia.org/wiki/Special:BookSources/9781557865946) in R. G. Frey & Christopher Wellman (eds.) *A Companion to Applied Ethics*, Oxford: Blackwell, pp. 525–536.\n    \n6.  ^**[^](#fnrefvdt6l3g50al)**^\n    \n    McMahan, Jeff (2008) [Eating animals the nice way](http://doi.org/10.1162/daed.2008.137.1.66), *Dædalus*, vol. 137, pp. 66–76.\n    \n7.  ^**[^](#fnrefd42mxs1atrp)**^\n    \n    McMahan, Jeff (2005) [Our fellow creatures](http://doi.org/10.1007/s10892-005-3512-2), *Journal of Ethics*, vol. 9, pp. 353–380.\n    \n8.  ^**[^](#fnrefk5npm3pgqog)**^\n    \n    McMahan, Jeff (2010) [The meat eaters](http://opinionator.blogs.nytimes.com/2010/09/19/the-meat-eaters/), *The New York Times*, September 19.\n    \n9.  ^**[^](#fnrefujos1dzad2c)**^\n    \n    McMahan, Jeff (2010) [Predators: a response](http://opinionator.blogs.nytimes.com/2010/09/28/predators-a-response/), *The New York Times*, September 28.\n    \n10.  ^**[^](#fnrefy03s3vkjp)**^\n    \n    McMahan, Jeff (2015) [The moral problem of predation](taylorfrancis.com/chapters/edit/10.4324/9780203154410-18/moral-problem-predation-jeff-mcmahan?context=ubx&refId=7a056ef6-a988-443e-9dd5-811b27a7d199), in Andrew Chignell, Terence Cuneo & Matthew Halteman (eds.) *Philosophy Comes to Dinner: Arguments about the Ethics of Eating*, New York: Routledge, pp. 268–294.\n    \n11.  ^**[^](#fnrefs7mhsjii1gm)**^\n    \n    Cf. Faria, Catia (2015) [Making a difference on behalf of animals living in the wild: interview with Jeff McMahan](https://www.ledonline.it/index.php/Relations/article/view/819), *Relations*, vol. 3, pp. 81–84.\n    \n12.  ^**[^](#fnref3knomd40tap)**^\n    \n    McMahan, Jeff (2018) [Doing good and doing the best](http://doi.org/10.1093/oso/9780190648879.003.0004), in Paul Woodruff (ed.) *The Ethics of Giving: Philosophers’ Perspectives on Philanthropy*, New York: Oxford University Press, pp. 78–102.\n    \n13.  ^**[^](#fnrefmp9o8v18kpi)**^\n    \n    McMahan, Jeff (2016) [Philosophical critiques of effective altruism](http://doi.org/10.5840/tpm20167379), *The Philosophers’ Magazine*, vol. 73, pp. 92–99."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "cKrruZ8hRd6Dn9MWg",
    "name": "Iterated embryo selection",
    "core": false,
    "slug": "iterated-embryo-selection",
    "oldSlugs": null,
    "postCount": 5,
    "description": {
      "markdown": "**Iterated embryo selection (IES)** is a reproductive technology that involves repeated cycles of *in vitro* sequencing and selection of embryos. Because it compresses multiple generations of selection into a fraction of one human maturation period, IES could theoretically increase heritable traits by several standard deviations in a relatively short period of time. In its current form, the technology was first described by [Carl Shulman](https://forum.effectivealtruism.org/tag/carl-shulman) in 2009,^[\\[1\\]](#fnbfdzsaws1p)^ and the idea was further developed in a 2014 paper by Shulman and [Nick Bostrom](https://forum.effectivealtruism.org/tag/nick-bostrom).^[\\[2\\]](#fn382b05g7ox6)^ If applied to cognitive ability, IES could potentially constitute a path to [superintelligence](https://forum.effectivealtruism.org/tag/superintelligence).^[\\[3\\]](#fn6fw3plzt73o)^\n\nIES consists of the following four steps:^[\\[4\\]](#fnthkwdqub0hi)^\n\n1.  Genotyping and selecting embryos higher in desired genetic characteristics.\n2.  Extracting stem cells from those embryos and converting them to gametes, maturing within six months or less.\n3.  Crossing the new gametes to produce embryos.\n4.  Repeating until large genetic changes have accumulated.\n\nTechnically, the main impediment to IES appears to be the difficulty of deriving gametes from embryonic pluripotent stem cells (PSCs). The median [Metaculus](https://forum.effectivealtruism.org/tag/metaculus) respondent estimates that the first live birth from stem cell-derived gametes will occur by 2036.^[\\[5\\]](#fnw9ktzt4dh3j)^ Socially, the main impediment is apparent opposition from prospective parents and from the general public. Public approval for preimplantation genetic diagnosis for intelligence has been found to range from 13%^[\\[6\\]](#fnm07glihs5wf)^ to 19%^[\\[7\\]](#fna3xrmcg1ty)^ to 28%^[\\[8\\]](#fnuvk46v1ma4h)^. However, Shulman and Bostrom note that these relatively low approval ratings are comparable to attitudes towards in vitro fertilization before the birth of the first IFV baby in 1978. After Louise Brown was born, approval went up dramatically, so a similar change may be expected following a successful demonstration of IES.\n\nFurther reading\n---------------\n\nBranwen, Gwern (2016) [Embryo selection for intelligence](https://www.gwern.net/Embryo-selection), *Gwern.net*, January 22 (updated 18 January 2020).\n\nShulman, Carl & Nick Bostrom (2014) [Embryo selection for cognitive enhancement: curiosity or game-changer?](http://doi.org/10.1111/1758-5899.12123), *Global Policy*, vol. 5, pp. 85–92.\n\n1.  ^**[^](#fnrefbfdzsaws1p)**^\n    \n    Shulman, Carl (2009) [What is multi-generational in vitro embryo selection?](http://theuncertainfuture.com/faq.html#7), *The Uncertain Future*.\n    \n2.  ^**[^](#fnref382b05g7ox6)**^\n    \n    Shulman, Carl & Nick Bostrom (2014) [Embryo selection for cognitive enhancement: curiosity or game-changer?](http://doi.org/10.1111/1758-5899.12123), *Global Policy*, vol. 5, pp. 85–92.\n    \n3.  ^**[^](#fnref6fw3plzt73o)**^\n    \n    Bostrom, Nick (2014) [*Superintelligence: Paths, Dangers, Strategies*](https://en.wikipedia.org/wiki/Special:BookSources/9780199678112), Oxford: Oxford University Press.\n    \n4.  ^**[^](#fnrefthkwdqub0hi)**^\n    \n    Shulman & Bostrom, [Embryo selection for cognitive enhancement](http://doi.org/10.1111/1758-5899.12123), p. 87.\n    \n5.  ^**[^](#fnrefw9ktzt4dh3j)**^\n    \n    Stafforini, Pablo (2019) [When will the first human baby from stem cell-derived gametes be born?](https://www.metaculus.com/questions/3034/when-will-the-first-human-baby-from-stem-cell-derived-gametes-be-born/), *Metaculus*, August 19.\n    \n6.  ^**[^](#fnrefm07glihs5wf)**^\n    \n    Hathaway, Feighanne, Esther Burns & Harry Ostrer (2009) [Consumers’ desire towards current and prospective reproductive genetic testing](http://doi.org/10.1007/s10897-008-9199-3), *Journal of Genetic Counseling*, vol. 18, pp. 137–146, p. 140.\n    \n7.  ^**[^](#fnrefa3xrmcg1ty)**^\n    \n    Winkelman, William D. *et al.* (2015) [Public perspectives on the use of preimplantation genetic diagnosis](http://doi.org/10.1007/s10815-015-0456-8), *Journal of Assisted Reproduction and Genetics*, vol. 32, pp. 665–675, p. 668.\n    \n8.  ^**[^](#fnrefuvk46v1ma4h)**^\n    \n    Kalfoglou, A. *et al.* (2004) [Reproductive genetic testing: What America thinks](http://web.archive.org/web/20060620032617/http://www.dnapolicy.org/images/reportpdfs/ReproGenTestAmericaThinks.pdf), Genetics and Public Policy Center, p. 11."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "ceb2G8vpHLSqr379z",
    "name": "Intuition of neutrality",
    "core": false,
    "slug": "intuition-of-neutrality",
    "oldSlugs": null,
    "postCount": 2,
    "description": {
      "markdown": "The **intuition of neutrality** is the view in [population ethics](https://forum.effectivealtruism.org/tag/population-ethics) that, roughly, adding a person to the population is in itself ethically neutral. More precisely, the view states that adding a person whose lifetime wellbeing falls within a certain positive range makes the world neither intrinsically better nor intrinsically worse.^[\\[1\\]](#fn4lv3ng57fv6)^\n\nFurther reading\n---------------\n\nBroome, John (2000) [Cost‐benefit analysis and population](http://doi.org/10.1086/468101), *The Journal of Legal Studies*, vol. 29, pp. 953–970.\n\nNarveson, Jan (1973) [Moral problems of population](http://doi.org/10.5840/monist197357134), *Monist*, vol. 57, pp. 62–86.\n\nRabinowicz, Włodek (2009) [Broome and the intuition of neutrality](http://doi.org/10.1111/j.1533-6077.2009.00174.x), *Philosophical Issues*, vol. 19, pp. 389–411.\n\nRelated entries\n---------------\n\n[person-affecting views](https://forum.effectivealtruism.org/tag/person-affecting-views) | [population ethics](https://forum.effectivealtruism.org/tag/population-ethics)\n\n1.  ^**[^](#fnref4lv3ng57fv6)**^\n    \n    Broome, John (2004) [*Weighing Lives*](http://doi.org/10.1093/019924376X.001.0001), Oxford: Oxford University Press."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "CGFhjcjkc9qWLwES2",
    "name": "Introspective hedonism",
    "core": false,
    "slug": "introspective-hedonism",
    "oldSlugs": null,
    "postCount": 5,
    "description": {
      "markdown": "**Introspective hedonism** is the view that the [intrinsic value](https://forum.effectivealtruism.org/tag/intrinsic-vs-instrumental-value) of hedonic experience can be known directly by introspection.\n\nFurther reading\n---------------\n\nDocker, Gus (2021) [The feeling of value - Sharon Hewitt Rawlette](https://www.utilitarianpodcast.com/the-feeling-of-value-sharon-hewitt-rawlette/), *Utilitarian Podcast*, July 25.\n\nRachels, Stuart (1998) [*Hedonic Value*](https://catalog.syr.edu/vwebv/holdingsInfo?bibId=2169584), doctoral thesis, Syracuse University.\n\nRawlette, Sharon Hewitt (2016) [*The Feeling of Value: Moral Realism Grounded in Phenomenal Consciousness*](https://en.wikipedia.org/wiki/Special:BookSources/9781534768017), King George, Virginia: Dudley & White.\n\nStern, Bastian (2016) [*Pleasure, Suffering and the Experience of Value*](https://ora.ox.ac.uk/objects/uuid:603a9b3e-3f94-41bd-9715-1062d96384fd), doctoral thesis, University of Oxford.\n\nRelated entries\n---------------\n\n[axiology](https://forum.effectivealtruism.org/tag/axiology) | [hedonism](https://forum.effectivealtruism.org/tag/hedonism) | [metaethics](https://forum.effectivealtruism.org/tag/metaethics) | [utilitarianism](https://forum.effectivealtruism.org/tag/utilitarianism)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "Tae8RJ8yTo6rkqJYf",
    "name": "Intrinsic vs. instrumental value",
    "core": false,
    "slug": "intrinsic-vs-instrumental-value",
    "oldSlugs": [
      "intrinsic-vs-instrumental-value",
      "intrinsic-value-and-instrumental-value"
    ],
    "postCount": 5,
    "description": {
      "markdown": "**Intrinsic value** (sometimes called **terminal value**) is the value something has for its own sake. **Instrumental value**, by contrast, is the value something has by virtue of its effects on other things.\n\nFurther reading\n---------------\n\nZimmerman, Michael J. (2002) [Intrinsic vs. extrinsic value](https://plato.stanford.edu/archives/spr2015/entries/value-intrinsic-extrinsic/), *Stanford Encyclopedia of Philosophy*, October 22 (updated 9 January 2019).\n\nRelated entries\n---------------\n\n[axiology](https://forum.effectivealtruism.org/tag/axiology)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "6CyJuFdYiwhmxRaFA",
    "name": "Welfarism",
    "core": false,
    "slug": "welfarism",
    "oldSlugs": null,
    "postCount": 3,
    "description": {
      "markdown": "**Welfarism** is the [axiological](https://forum.effectivealtruism.org/tag/axiology) view that only [wellbeing](https://forum.effectivealtruism.org/tag/wellbeing) has [intrinsic value](https://forum.effectivealtruism.org/tag/intrinsic-vs-instrumental-value).\n\nFurther reading\n---------------\n\nMacAskill, William & Darius Meissner (2020) 'Welfarism', in [Elements and types of utilitarianism](https://www.utilitarianism.net/types-of-utilitarianism), *Utilitarianism*.\n\nCrisp, Roger (2001) 'Welfarism', in [Well-being](https://plato.stanford.edu/entries/well-being/), *Stanford Encyclopedia of Philosophy*, November 6 (updated 6 September 2017).\n\nRelated entries\n---------------\n\n[axiology](https://forum.effectivealtruism.org/tag/axiology) | [consequentialism](https://forum.effectivealtruism.org/tag/consequentialism)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "L9WRoWG6KBCFzNH98",
    "name": "Wellbeing",
    "core": false,
    "slug": "wellbeing",
    "oldSlugs": null,
    "postCount": 22,
    "description": {
      "markdown": "**Wellbeing** (also called **welfare**) is what is good for a person, or what makes their life go well or is in a person's interest. It is generally agreed that all plausible moral views regard wellbeing as having [intrinsic value](https://forum.effectivealtruism.org/tag/intrinsic-vs-instrumental-value) , with some views—[welfarist](https://forum.effectivealtruism.org/tag/welfarism) views—holding that nothing but wellbeing is intrinsically valuable."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "kopfHcQwn7rqeWTaA",
    "name": "Interpersonal comparisons of wellbeing",
    "core": false,
    "slug": "interpersonal-comparisons-of-wellbeing",
    "oldSlugs": null,
    "postCount": 3,
    "description": {
      "markdown": "**Interpersonal comparisons of wellbeing** (also called **interpersonal comparisons of utility**) are comparisons involving the [welfare](https://forum.effectivealtruism.org/tag/wellbeing) levels of different people. Such comparisons are often presupposed in evaluations of social outcomes and in explanations of human behavior.\n\nFurther reading\n---------------\n\nD’Aspremont, Claude (2008) [Interpersonal utility comparisons (new developments)](https://doi.org/10.1057/9780230226203.1551), in Steven N. Durlauf & Lawrence E. Blume (eds.) *The New Palgrave Dictionary of Economics*, 2nd ed., London: Palgrave Macmillan, pp. 3250–3254.\n\nElster, Jon & John E. Roemer (eds.) (1991) [*Interpersonal Comparisons of Well-Being*](https://doi.org/10.1017/CBO9781139172387), Cambridge: Cambridge University Press.\n\nGreaves, Hilary & Harvey Lederman (2018) [Extended preferences and interpersonal comparisons of well-being](https://doi.org/10.1111/phpr.12334), *Philosophy and Phenomenological Research*, vol. 96, pp. 636–667.\n\nHarsanyi, John C. (1980) [Cardinal welfare, individualistic ethics, and interpersonal comparisons of utility](https://doi.org/10.1007/978-94-010-9327-9_2), in *Essays on Ethics, Social Behavior, and Scientific Explanation*, Dordrecht: Springer Netherlands, pp. 6–23.\n\nNg, Yew-Kwang (2013) [Interpersonal utility](https://doi.org/10.5040/9781350021679.0018), in James E. Crimmins (ed.) *The Bloomsbury Encyclopedia of Utilitarianism*, London: Bloomsbury Academic, pp. 279–280.\n\nRelated entries\n---------------\n\n[utilitarianism](https://forum.effectivealtruism.org/tag/utilitarianism) | [welfare economics](https://forum.effectivealtruism.org/tag/welfare-economics) | [wellbeing](https://forum.effectivealtruism.org/tag/wellbeing)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "3Z7FXJYrWrG5thPuf",
    "name": "Intelligence explosion",
    "core": false,
    "slug": "intelligence-explosion",
    "oldSlugs": null,
    "postCount": 4,
    "description": {
      "markdown": "An **intelligence explosion** (sometimes called a **technological singularity**, or **singularity** for short) is a hypothesized event in which a sufficiently advanced [artificial intelligence](https://forum.effectivealtruism.org/tag/artificial-intelligence) rapidly attains [superhuman intellectual ability](https://forum.effectivealtruism.org/tag/superintelligence) by a process of recursive self-improvement.\n\nFurther reading\n---------------\n\nBostrom, Nick (2014) [*Superintelligence: Paths, Dangers, Strategies*](https://en.wikipedia.org/wiki/Special:BookSources/978-0-19-967811-2), Oxford: Oxford University Press.\n\nChalmers, David J. (2010) [The singularity: A philosophical analysis](https://www.ingentaconnect.com/content/imp/jcs/2010/00000017/f0020009/art00001), *Journal of Consciousness Studies*, vol. 17, pp. 7–65.\n\nPearce, David (2012) [The biointelligence explosion: how recursively self-improving organic robots will modify their own source code and bootstrap our way to full-spectrum superintelligence](http://doi.org/10.1007/978-3-642-32560-1_11), in Amnon H. Eden *et al.* (eds.) *Singularity Hypotheses: A Scientific and Philosophical Assessment*, Berlin: Springer, pp. 199–238.\n\nSandberg, Anders (2013) [An overview of models of technological singularity](https://doi.org/10.1002/9781118555927.ch36), in Max More & Natasha Vita-More (eds.) *The Transhumanist Reader: Classical and Contemporary Essays on the Science, Technology, and Philosophy of the Human Future*, Malden: Wiley, pp. 376–394.\n\nVinding, Magnus (2017) [A contra AI FOOM reading list](https://magnusvinding.com/2017/12/16/a-contra-ai-foom-reading-list/), *Magnus Vinding’s Blog*, December (updated June 2022).\n\nRelated entries\n---------------\n\n[AI skepticism](https://forum.effectivealtruism.org/topics/ai-skepticism) | [AI takeoff](https://forum.effectivealtruism.org/tag/ai-takeoff) | [artificial intelligence](https://forum.effectivealtruism.org/tag/artificial-intelligence) | [flourishing futures](https://forum.effectivealtruism.org/topics/flourishing-futures) | [superintelligence](https://forum.effectivealtruism.org/tag/superintelligence) | [transformative artificial intelligence](https://forum.effectivealtruism.org/tag/transformative-artificial-intelligence)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "eqFGMqFaAY95KekSM",
    "name": "Inside vs. outside view",
    "core": false,
    "slug": "inside-vs-outside-view",
    "oldSlugs": [
      "inside-view-vs-outside-view"
    ],
    "postCount": 10,
    "description": {
      "markdown": "The **inside view** and the **outside view** are two alternative approaches to [forecasting](https://forum.effectivealtruism.org/tag/forecasting). Whereas the inside view attempts to make predictions based on an understanding of the details of a problem, the outside view—also called **reference class forecasting**—instead looks at similar past situations and predicts based on those outcomes. For example, in trying to predict the time it will take a team to design an academic curriculum, a forecaster can either look at the characteristics of the curriculum to be designed and of the curriculum designers (inside view) or consider the time it has taken past teams to design similar curricula (outside view).^[\\[1\\]](#fnip1b2vhw34)^\n\nTerminology\n-----------\n\nThe terms \"inside view\" and \"outside view\" are sometimes used to refer to other contrasts. First, the terms are used to contrast the views reached exclusively via individual reasoning and the views that also take into account the fact that other reasoners have reached different views. Second, the terms are used to contrast the perspective of someone looking at the problem \"from the inside\", or first-person perspective, and the perspective of someone looking at the problem \"from the outside\", or third-person perspective. For example,  [Scott Alexander](https://forum.effectivealtruism.org/tag/scott-alexander) asks whether \"it’s better to model \\[depressed patients'\\] behavior as based on mysterious brain chemicals rather than on rational choice\", and answers that \"\\[i\\]t would be really weird if depression were the one area where we could always count on the inside view not to lead us astray.\"^[\\[2\\]](#fn8n1xaeweq0q)^\n\nNoting that the terms are used in these and other related senses, some have proposed that they should be avoided altogether, and replaced by less ambiguous alternatives.^[\\[3\\]](#fnza2l2x8ewqk)^\n\nFurther reading\n---------------\n\nLessWrong (2020) [Inside/outside view](https://www.lesswrong.com/revisions/tag/inside-outside-view), *LessWrong Wiki*.\n\nRelated entries\n---------------\n\n[cognitive bias](https://forum.effectivealtruism.org/tag/cognitive-bias) | [epistemic deference](https://forum.effectivealtruism.org/tag/epistemic-deference) | [discussion norms](https://forum.effectivealtruism.org/tag/discussion-norms) | [forecasting](https://forum.effectivealtruism.org/tag/forecasting) | [independent impressions](https://forum.effectivealtruism.org/tag/independent-impressions)\n\n1.  ^**[^](#fnrefip1b2vhw34)**^\n    \n    Kahneman, Daniel & Dan Lovallo (1993) [Timid choices and bold forecasts: A cognitive perspective on risk taking](http://doi.org/10.1287/mnsc.39.1.17), *Management Science*, vol. 39, pp. 17–31.\n    \n2.  ^**[^](#fnref8n1xaeweq0q)**^\n    \n    Alexander, Scott (2015) [Chemical imbalance](https://slatestarcodex.com/2015/04/05/chemical-imbalance/), *Slate Star Codex*, April 5.\n    \n3.  ^**[^](#fnrefza2l2x8ewqk)**^\n    \n    Kokotajlo, Daniel (2021) [Taboo “Outside View”](https://forum.effectivealtruism.org/posts/wYpARcC4WqMsDEmYR/taboo-outside-view), *Effective Altruism Forum*, June 17."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "J4Ffy5XkRaJtGMeD8",
    "name": "Infinite ethics",
    "core": false,
    "slug": "infinite-ethics",
    "oldSlugs": null,
    "postCount": 11,
    "description": {
      "markdown": "**Infinite ethics** is the branch of [moral philosophy](https://forum.effectivealtruism.org/tag/moral-philosophy) that studies the ethical implications of living in an infinite universe.\n\nThe possibility that the universe is infinite, and that it might therefore contain an infinite number of morally relevant beings, is a challenge to moral theories that instruct us to increase the good in the world. Briefly, the trouble is that in an infinite universe the amount of good may be infinite no matter what we do, and it is difficult to compare infinities.\n\nCosmologists commonly regard the universe as infinite in their models. However, this is partly for mathematical simplicity, and there is also an alternative hypothesis: It is possible that the universe “curls in on itself,” in the same way that the two-dimensional surface of the Earth does, such that traveling far enough in any one direction would ultimately bring you back to where you started.\n\nThe present situation, roughly, is that an infinite universe would be consistent with all of the data that cosmologists currently have available to them, but it is very difficult to discern the difference between a space that is infinite and a space that is simply enormously big. There is no widely accepted probability that one or the other hypothesis is true.\n\nIt is also important to note that a universe could be infinite without also containing infinite value. For instance, an infinite universe which contained only a finite number of life-harboring regions would not pose a problem to aggregative moral theories. However, cosmological evidence points strongly toward the universe also being “uniform,” which means that all sufficiently large-scale regions should have approximately the same properties, including the ability to harbor life.\n\nFurther reading\n---------------\n\nArntzenius, Frank (2014) [Utilitarianism, decision theory and eternity](http://doi.org/10.1111/phpe.12036), *Philosophical Perspectives*, vol. 28, pp. 31–58.\n\nAskell, Amanda (2018) [*Pareto Principles in Infinite Ethics*](https://askell.io/publication/pareto-principles-in-infinite-ethics), PhD thesis, New York University.\n\nBostrom, Nick (2016) [Infinite ethics](http://philpapers.org/rec/ETHIE), *Analysis and Metaphysics*, vol. 10, pp. 9–59.  \n*An in-depth discussion of the potential moral implications of an infinite universe.*\n\nCenter on Long-Term Risk (2020) [Infinity in ethics](https://longtermrisk.org/infinity-in-ethics/), *Center on Long-Term Risk*.\n\nTomasik, Brian (2014) [Should we believe in infinity?](https://reducing-suffering.org/believe-infinity/), *Essays on Reducing Suffering*, December 15 (updated 26 October 2018).\n\nTreutlein, Johannes (2018) [A wager against Solomonoff induction](https://casparoesterheld.com/2018/03/31/a-wager-against-solomonoff-induction/), *The Universe From an Intentional Stance*, March 31.\n\nWest, Ben (2015) [Problems and solutions in infinite ethics](https://forum.effectivealtruism.org/posts/9D6zKRPfaALiBhnnN/problems-and-solutions-in-infinite-ethics), *Effective Altruism Forum*, January 1.\n\nWiblin, Robert & Keiran Harris (2018) [Tackling the ethics of infinity, being clueless about the effects of our actions, and having moral empathy for intellectual adversaries, with philosopher Dr Amanda Askell](https://80000hours.org/podcast/episodes/amanda-askell-moral-empathy/), *80,000 Hours*, September 11.\n\nRelated entries\n---------------\n\n[axiology](https://forum.effectivealtruism.org/tag/axiology) | [moral philosophy](https://forum.effectivealtruism.org/tag/moral-philosophy)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "zZbc2Mvp7BRGxF2yP",
    "name": "Indirect normativity",
    "core": false,
    "slug": "indirect-normativity",
    "oldSlugs": null,
    "postCount": null,
    "description": {
      "markdown": "**Indirect normativity** is an approach to the [AI alignment](https://forum.effectivealtruism.org/tag/ai-alignment) problem that attempts to specify AI values indirectly, such as by reference to what a rational agent would value under idealized conditions, rather than via direct specification.\n\nFurther reading\n---------------\n\nBostrom, Nick (2014) [*Superintelligence: Paths, Dangers, Strategies*](https://en.wikipedia.org/wiki/Special:BookSources/9780199678112), Oxford: Oxford University Press, ch. 13.\n\nChristiano, Paul (2012) [A formalization of indirect normativity](https://ordinaryideas.wordpress.com/2012/04/21/indirect-normativity-write-up/), *Ordinary Ideas*, April 21.\n\nYudkowsky, Eliezer (2013) [Five theses, two lemmas, and a couple of strategic implications](https://intelligence.org/2013/05/05/five-theses-two-lemmas-and-a-couple-of-strategic-implications/), *Machine Intelligence Research Institute's Blog*, May 5.\n\nRelated links\n-------------\n\n[AI alignment](https://forum.effectivealtruism.org/tag/ai-alignment) | [motivation selection method](https://forum.effectivealtruism.org/tag/motivation-selection-method)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "W3Wnemq6zoiXwSNiC",
    "name": "Thinking at the margin",
    "core": false,
    "slug": "thinking-at-the-margin",
    "oldSlugs": null,
    "postCount": 6,
    "description": {
      "markdown": "To **think at the margin** is to assess the impact of a decision by considering the effects of spending an additional, or *marginal*, unit of resources. Since resources typically have [diminishing marginal impact](https://forum.effectivealtruism.org/tag/diminishing-returns) (i.e., each additional unit is less impactful than the previous unit), thinking at the margin is critical for properly [assessing the impact](https://forum.effectivealtruism.org/tag/impact-assessment) of a decision.\n\nRelated entries\n---------------\n\n[diminishing returns](https://forum.effectivealtruism.org/tag/diminishing-returns) | [impact assessment](https://forum.effectivealtruism.org/tag/impact-assessment) | [room for more funding](https://forum.effectivealtruism.org/tag/room-for-more-funding)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "gouqafb3g5bb6fuM7",
    "name": "Neglectedness",
    "core": false,
    "slug": "neglectedness",
    "oldSlugs": null,
    "postCount": 20,
    "description": {
      "markdown": "The **neglectedness** of a problem (also called its **uncrowdedness**) is the amount of resources currently allocated to solving it. Neglectedness is one of the three factors in the [ITN framework](https://forum.effectivealtruism.org/tag/itn-framework-1).\n\nFurther reading\n---------------\n\nChristiano, Paul (2014) [Neglectedness and impact](https://80000hours.org/2014/01/neglectedness-and-impact/), *80,000 Hours*, January 14.\n\nOesterheld, Caspar (2020) [Complications in evaluating neglectedness](https://casparoesterheld.com/2017/06/25/complications-in-evaluating-neglectedness/), *The Universe From an Intentional Stance*, June 25.  \n*A blog post noting a number of issues that make the estimation of neglectedness more complicated than it first appears.*\n\nŠimčikas, Saulius (2020) [A cause can be too neglected](https://forum.effectivealtruism.org/posts/NktbYpwa48u23c5XL/a-cause-can-be-too-neglected), *Effective Altruism Forum*, April 3.  \n*Although working on a cause tends to produce more value in expectation the more neglected this cause is, the relationship may break down as the cause approaches maximum neglectedness, because of setup costs associated with initiating work on a very novel cause.*\n\nRelated entries\n---------------\n\n[cause prioritization](https://forum.effectivealtruism.org/tag/cause-prioritization) | [diminishing returns](https://forum.effectivealtruism.org/tag/diminishing-returns) | [impact assessment](https://forum.effectivealtruism.org/tag/impact-assessment) | [importance](https://forum.effectivealtruism.org/tag/importance)  | [ITN framework](https://forum.effectivealtruism.org/tag/itn-framework-1) | [room for more funding](https://forum.effectivealtruism.org/tag/room-for-more-funding) | [tractability](https://forum.effectivealtruism.org/tag/tractability)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "LntHSATTnW3cft2rr",
    "name": "Tractability",
    "core": false,
    "slug": "tractability",
    "oldSlugs": null,
    "postCount": 3,
    "description": {
      "markdown": "The **tractability** of a problem (also called its **solvability**) is the degree to which it is solvable by a given increase in the resources allocated to it. Together with [importance](https://forum.effectivealtruism.org/tag/importance) and [neglectedness](https://forum.effectivealtruism.org/tag/neglectedness), importance is one of the three factors in the [ITN framework](https://forum.effectivealtruism.org/topics/itn-framework-1).\n\nFurther reading\n---------------\n\nKwa, Thomas (2022) [Most problems don’t differ dramatically in tractability (under certain assumptions)](https://forum.effectivealtruism.org/posts/4rGpNNoHxxNyEHde3/most-problems-don-t-differ-dramatically-in-tractability), *Effective Altruism Forum*, May 3.\n\nRelated entries\n---------------\n\n[importance](https://forum.effectivealtruism.org/tag/importance) | [ITN framework](https://forum.effectivealtruism.org/topics/itn-framework-1) | [neglectedness](https://forum.effectivealtruism.org/tag/neglectedness)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "f2Y8FabnfC7hNsoms",
    "name": "Importance",
    "core": false,
    "slug": "importance",
    "oldSlugs": null,
    "postCount": 3,
    "description": {
      "markdown": "The **importance** of a problem (also called its **scale**) is the value of solving it.^[\\[1\\]](#fnkdjn36zyic9)^ Together with [tractability](https://forum.effectivealtruism.org/tag/tractability) and [neglectedness](https://forum.effectivealtruism.org/tag/neglectedness), importance is one of the three factors in the [ITN framework](https://forum.effectivealtruism.org/tag/itn-framework-1).\n\nFurther reading\n---------------\n\nTodd, Ben (2013) [A framework for strategically selecting a cause](https://80000hours.org/2013/12/a-framework-for-strategically-selecting-a-cause/), *80,000 Hours*, December 19.\n\nRelated entries\n---------------\n\n[ITN framework](https://forum.effectivealtruism.org/tag/itn-framework) | [neglectedness](https://forum.effectivealtruism.org/tag/neglectedness) | [tractability](https://forum.effectivealtruism.org/tag/tractability)\n\n1.  ^**[^](#fnrefkdjn36zyic9)**^\n    \n    Ord, Toby (2020) [*The Precipice: Existential Risk and the Future of Humanity*](https://en.wikipedia.org/wiki/Special:BookSources/1-5266-0021-8), London: Bloomsbury Publishing, p. 181."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "RMJJEuE2iXZM6de6c",
    "name": "Indirect long-term effects",
    "core": false,
    "slug": "indirect-long-term-effects",
    "oldSlugs": null,
    "postCount": 17,
    "description": {
      "markdown": "**Indirect long-term effects** are effects on the long-run future from interventions targeted at the short-term. Other terms that have been used for this concept or somewhat similar concepts include **flow-through effects**,^[\\[1\\]](#fnjlz1k1i5b4g)^^[\\[2\\]](#fntpcdo95rcrc)^^[\\[3\\]](#fny7dqvfoiaji)^^[\\[4\\]](#fngoaoypyzzpj)^ **ripple effects**,^[\\[5\\]](#fnw9jjlhopomn)^^[\\[6\\]](#fnd6t5kgfqg2h)^ **knock-on effects**^[\\[7\\]](#fnq16wg8b5u9)^^[\\[8\\]](#fn60ar42nd92y)^^[\\[9\\]](#fncymb7aw7r2l)^ and **cascading effects**.\n\nWhen evaluating the outcome of an action, a distinction can be made between the action’s direct and indirect effects. Although the boundary between these two categories is often imprecise, direct effects are those effects that are relatively *obvious* and *intended*. Indirect effects, in turn, are effects that are either non-obvious (i.e., it is difficult to determine whether or to what extent they follow from the relevant actions), unintended, or both.\n\nFor instance, reduced [malaria](https://forum.effectivealtruism.org/tag/malaria) incidence is a relatively direct effect of bed-net distribution, whereas more indirect effects may include improved education and increased [GDP growth](https://forum.effectivealtruism.org/tag/economic-growth) (which in turn may have even further long-run effects).\n\nMany kinds of indirect effects have received attention within the effective altruism community, including [negative side-effects of disseminating true information](https://forum.effectivealtruism.org/tag/information-hazard), [negative-side effects of rule-breaking](https://forum.effectivealtruism.org/tag/naive-vs-sophisticated-consequentialism), and [replaceability effects](https://forum.effectivealtruism.org/tag/replaceability).\n\nFurther reading\n---------------\n\nWiblin, Robert (2016) [Making sense of long-term indirect effects](https://www.effectivealtruism.org/articles/making-sense-of-long-term-indirect-effects-rob-wiblin/), *Effective Altruism*, August 7.\n\nRelated entries\n---------------\n\n[accidental harm](https://forum.effectivealtruism.org/tag/accidental-harm) | [broad vs. narrow interventions](https://forum.effectivealtruism.org/tag/broad-vs-narrow-interventions) | [cluelessness](https://forum.effectivealtruism.org/tag/cluelessness) | [existential risk factor](https://forum.effectivealtruism.org/tag/existential-risk-factor) | [forecasting](https://forum.effectivealtruism.org/tag/forecasting) | [naive vs. sophisticated consequentialism](https://forum.effectivealtruism.org/tag/naive-vs-sophisticated-consequentialism) | [spillover effects](https://forum.effectivealtruism.org/tag/spillover-effects)\n\n1.  ^**[^](#fnrefjlz1k1i5b4g)**^\n    \n    Karnofsky, Holden (2013) [Flow-through effects](https://blog.givewell.org/2013/05/15/flow-through-effects/), *The GiveWell Blog*, May 15.\n    \n2.  ^**[^](#fnreftpcdo95rcrc)**^\n    \n    Karnofsky, Holden *et al.* (2013) [Flow through effects conversation](https://www.jefftk.com/p/flow-through-effects-conversation), *Jeff Kaufman’s Blog*, August 19.\n    \n3.  ^**[^](#fnrefy7dqvfoiaji)**^\n    \n    Shulman, Carl (2013) [What proxies to use for flow-through effects?](http://reflectivedisequilibrium.blogspot.com/2013/12/what-proxies-to-use-for-flow-through.html), *Reflective Disequilibrium*, December 11.\n    \n4.  ^**[^](#fnrefgoaoypyzzpj)**^\n    \n    Wiblin, Robert (2016) [Making sense of long-term indirect effects](https://www.effectivealtruism.org/articles/making-sense-of-long-term-indirect-effects-rob-wiblin/), *Effective Altruism*, August 7.\n    \n5.  ^**[^](#fnrefw9jjlhopomn)**^\n    \n    Beckstead, Nick (2013) [*On the overwhelming importance of shaping the far future*](https://doi.org/10.7282/T35M649T), Doctoral thesis, Rutgers University.\n    \n6.  ^**[^](#fnrefd6t5kgfqg2h)**^\n    \n    Whittlestone, Jess (2017) [The long-term future](https://www.effectivealtruism.org/articles/cause-profile-long-run-future/), *Effective Altruism*, November 16.\n    \n7.  ^**[^](#fnrefq16wg8b5u9)**^\n    \n    Gaensbauer, Evan (2016) [Effective altruism, environmentalism, and climate change: an introduction](https://forum.effectivealtruism.org/posts/dmrLcaYGk6yhJa2mZ/effective-altruism-environmentalism-and-climate-change-an), *Effective Altruism Forum*, March 10.\n    \n8.  ^**[^](#fnref60ar42nd92y)**^\n    \n    Greaves, Hilary (2016) [Cluelessness](https://doi.org/10.1093/arisoc/aow018), *Proceedings of the Aristotelian Society*, vol. 116, pp. 311–339.\n    \n9.  ^**[^](#fnrefcymb7aw7r2l)**^\n    \n    Snowden, James (2017) [The economic benefits of malaria eradication](https://www.givingwhatwecan.org/post/2016/01/the-economic-benefits-of-malaria-eradication/), *The Giving What We Can blog*, January 18."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "FW557aEwQb9Lws3zs",
    "name": "Effective Altruism Group Organisers' Survey",
    "core": false,
    "slug": "effective-altruism-group-organisers-survey",
    "oldSlugs": null,
    "postCount": 6,
    "description": {
      "markdown": "Related entries\n---------------\n\n[Data (EA community)](https://forum.effectivealtruism.org/tag/data-ea-community-1) | [Effective Altruism Survey](https://forum.effectivealtruism.org/tag/effective-altruism-survey) | [surveys](https://forum.effectivealtruism.org/tag/surveys)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "ji3pmNYeGpF9A3YuT",
    "name": ".impact",
    "core": false,
    "slug": "impact",
    "oldSlugs": null,
    "postCount": 6,
    "description": {
      "markdown": "**.impact** was a venue for communicating and collaborating on effective altruist projects. In 2017, .impact and [Students for High-Impact Charity](https://forum.effectivealtruism.org/tag/students-for-high-impact-charity) teamed up to form [Rethink Charity](https://forum.effectivealtruism.org/tag/rethink-charity).^[\\[1\\]](#fnep64ht7wbvu)^\n\nExternal links\n--------------\n\n[.impact](http://web.archive.org/web/20170530035614/http://dotimpact.im/). Official website, archived from the original.\n\nRelated entries\n---------------\n\n[Rethink Charity](https://forum.effectivealtruism.org/tag/rethink-charity) | [Students for High-Impact Charity](https://forum.effectivealtruism.org/tag/students-for-high-impact-charity)\n\n1.  ^**[^](#fnrefep64ht7wbvu)**^\n    \n    Barnett, Tee (2017) [.impact is now Rethink Charity](https://forum.effectivealtruism.org/posts/a3sScjxdgNTCseBMJ/impact-is-now-rethink-charity), *Effective Altruism Forum*, May 30."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "uKE7T4tguvEiL5Wbf",
    "name": "LEAN",
    "core": false,
    "slug": "lean",
    "oldSlugs": [
      "lean",
      "local-effective-altruism-network"
    ],
    "postCount": 15,
    "description": {
      "markdown": "The **Local Effective Altruism Network** (**LEAN**) was a meta effective altruism project of [.impact](https://forum.effectivealtruism.org/tag/impact) (now [Rethink Charity](https://forum.effectivealtruism.org/tag/rethink-charity)) operational from 2015 to 2020. Its original mission was \"to help local Effective Altruism (EA) groups come into existence and thrive\"^[\\[1\\]](#fns5uglfwbjv)^ and began as a volunteer-led, grassroots, and open community project.\n\nOver its years of operation, LEAN:\n\n*   provided grassroots support and helped seed \"over 100 local groups across the world by setting up websites and Facebook groups, offering ongoing direct support to organizers, as well as covering the costs of essential technology.\"^[\\[1\\]](#fns5uglfwbjv)^\n*   created and launched the first version of the [Effective Altruism Hub](https://forum.effectivealtruism.org/tag/effective-altruism-hub) from 2013 to 2019.\n*   ran the first [Effective Altruism Survey](https://forum.effectivealtruism.org/tag/effective-altruism-survey) in 2014 and then re-ran it in 2015 and 2017, before making it a joint collaboration with [Rethink Priorities](https://forum.effectivealtruism.org/tag/rethink-priorities), and the first-ever systematic survey of local groups in 2017.^[\\[2\\]](#fn6jbvhlgy3th)^^[\\[3\\]](#fnnj19wo5cqb)^\n*   worked on creating the first consolidated website for EA Groups Resources (resources.eahub.org), which it managed until 2020 when it was transferred to the [Centre for Effective Altruism](https://forum.effectivealtruism.org/topics/centre-for-effective-altruism-1).\n\nBefore going offline, LEAN was hosted at `localeffectivealtruism.network`. The website does not appear to have been archived.\n\nRelated entries\n---------------\n\n[effective altruism groups](https://forum.effectivealtruism.org/tag/effective-altruism-groups) | [.impact](https://forum.effectivealtruism.org/tag/impact)\n\n1.  ^**[^](#fnrefs5uglfwbjv)**^\n    \n    Trzesimiech, Michal (2019) [Local Effective Altruism Network’s new focus for 2019](https://forum.effectivealtruism.org/posts/Cvi7hnTYMk5qutkDg/local-effective-altruism-network-s-new-focus-for-2019), *Effective Altruism Forum*, March 30.\n    \n2.  ^**[^](#fnref6jbvhlgy3th)**^\n    \n    Herzig, Richenda (2018) [2017 LEAN impact assessment: Evaluation & strategic conclusions](https://forum.effectivealtruism.org/posts/qwZj8f7eq4HxmBzus/2017-lean-impact-assessment-evaluation-and-strategic), *Effective Altruism Forum*, February 28.\n    \n3.  ^**[^](#fnrefnj19wo5cqb)**^\n    \n    Herzig, Richenda (2018) [2017 LEAN impact assessment: Quantitative findings](https://rtcharity.org/2017-lean-impact-assessment-quantitative-findings/), *Rethink Charity*, January 5."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "ZCYNJEZi9kxNmbt4n",
    "name": "Effective Altruism Hub",
    "core": false,
    "slug": "effective-altruism-hub",
    "oldSlugs": [
      "ea-hub"
    ],
    "postCount": 11,
    "description": {
      "markdown": "The **Effective Altruism Hub** (often spelled **EA Hub**) is an independent volunteer project which hosts a global community and groups directory for the [effective altruism](https://forum.effectivealtruism.org/tag/effective-altruism) community.\n\nHistory\n-------\n\nThe first version of the EA Hub was launched by [LEAN](https://forum.effectivealtruism.org/tag/lean) in 2015. A new team assumed leadership in 2020, with fiscal sponsorship by [Rethink Charity](https://forum.effectivealtruism.org/tag/rethink-charity). In early 2022, the EA Hub announced that it was suspending feature developments, with plans to retire the website once a suitable replacement platform is found.^[\\[1\\]](#fndvi1egr5e8h)^\n\nExternal links\n--------------\n\n[Effective Altruism Hub](https://eahub.org/). Official website.\n\n1.  ^**[^](#fnrefdvi1egr5e8h)**^\n    \n    Agarwalla, Vaidehi (2022) [The EA Hub is suspending new feature development (with plans to retire)](https://forum.effectivealtruism.org/posts/FdWv7t8Pyxyxe6xnb/the-ea-hub-is-suspending-new-feature-development-with-plans), *Effective Altruism Forum*, February 27."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "cadWfdeB8ZXfZY6F9",
    "name": "Centre for Effective Altruism",
    "core": false,
    "slug": "centre-for-effective-altruism-1",
    "oldSlugs": [
      "centre-for-effective-altruism-cea"
    ],
    "postCount": 113,
    "description": {
      "markdown": "The **Centre for Effective Altruism** (**CEA**) is a nonprofit organisation that focuses on growing and supporting the [effective altruism](https://forum.effectivealtruism.org/tag/effective-altruism) community. CEA runs the [Effective Altruism Forum](https://forum.effectivealtruism.org/tag/effective-altruism-forum-1) and [Effective Altruism Global](https://forum.effectivealtruism.org/tag/effective-altruism-global), and provides support for [effective altruism groups](https://forum.effectivealtruism.org/tag/effective-altruism-groups) and community building grants to group organisers.\n\nFunding \n--------\n\nAs of August 2022, CEA has received over $34.7 million in funding from [Open Philanthropy](https://forum.effectivealtruism.org/topics/open-philanthropy),^[\\[1\\]](#fnxavmp2nnzb)^ nearly $14 million from the [Future Fund](https://forum.effectivealtruism.org/topics/future-fund),^[\\[2\\]](#fnhm65ocnc4sg)^ and over $450,000 from [Effective Altruism Funds](https://forum.effectivealtruism.org/topics/effective-altruism-funds).^[\\[3\\]](#fnh80aucozj0g)^^[\\[4\\]](#fna1buqw7y3wf)^^[\\[5\\]](#fnlmd6qaqn8t)^ ^[\\[6\\]](#fn8was726xvh2)^^[\\[7\\]](#fns9b7p1hokm8)^\n\nFurther reading\n---------------\n\nDalton, Max (2021) [CEA’s strategy](https://www.centreforeffectivealtruism.org/strategy), *Centre for Effective Altruism*, March.\n\nExternal links\n--------------\n\n[Centre for Effective Altruism](https://www.centreforeffectivealtruism.org/). Official website.\n\n[Centre for Effective Altruism](https://forum.effectivealtruism.org/users/centre-for-effective-altruism) \\- [Effective Altruism Forum](https://forum.effectivealtruism.org/tag/effective-altruism-forum-1) account.\n\n[Apply for a job](https://www.centreforeffectivealtruism.org/careers).\n\n[Donate to CEA](https://www.centreforeffectivealtruism.org/donate/).\n\nRelated entries\n---------------\n\n[effective altruism](https://forum.effectivealtruism.org/tag/effective-altruism)\n\n1.  ^**[^](#fnrefxavmp2nnzb)**^\n    \n    Open Philanthropy (2022) [Grants database: Centre for Effective Altruism](https://www.openphilanthropy.org/grants/?q=&organization-name=center-for-effective-altruism&organization-name=centre-for-effective-altruism), *Open Philanthropy*.\n    \n2.  ^**[^](#fnrefhm65ocnc4sg)**^\n    \n    Future Fund (2022) [Our grants and investments: Centre for Effective Altruism](https://ftxfuturefund.org/our-grants/?_organization_name=centre-for-effective-altruism), *Future Fund*.\n    \n3.  ^**[^](#fnrefh80aucozj0g)**^\n    \n    Effective Altruism Infrastructure Fund (2018) [July 2018: EA Meta Fund grants](https://funds.effectivealtruism.org/funds/payouts/july-2018-ea-meta-fund-grants), *Effective Altruism Funds*, July.\n    \n4.  ^**[^](#fnrefa1buqw7y3wf)**^\n    \n    Effective Altruism Infrastructure Fund (2018) [November 2018: EA Meta Fund grants](https://funds.effectivealtruism.org/funds/payouts/november-2018-ea-meta-fund-grants), *Effective Altruism Funds*, November.\n    \n5.  ^**[^](#fnreflmd6qaqn8t)**^\n    \n    Long-Term Future Fund (2018) [July 2018: Long-Term Future Fund grants](https://funds.effectivealtruism.org/funds/payouts/july-2018-long-term-future-fund-grants), *Effective Altruism Funds*, July.\n    \n6.  ^**[^](#fnref8was726xvh2)**^\n    \n    Effective Altruism Infrastructure Fund (2019) [July 2019: EA Meta Fund grants](https://funds.effectivealtruism.org/funds/payouts/july-2019-ea-meta-fund-grants), *Effective Altruism Funds*, July.\n    \n7.  ^**[^](#fnrefs9b7p1hokm8)**^\n    \n    Effective Altruism Infrastructure Fund (2019) [November 2019: EA Meta Fund grants](https://funds.effectivealtruism.org/funds/payouts/november-2019-ea-meta-fund-grants), *Effective Altruism Funds*, November."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "DzTDgwNJoCx8PBByA",
    "name": "Family planning",
    "core": false,
    "slug": "family-planning",
    "oldSlugs": null,
    "postCount": 23,
    "description": {
      "markdown": "**Family planning** is the use of contraception to limit or space out the number of children born to a couple, and the provision of contraceptive methods for that use. \n\nFamily planning is widely recognized as a highly cost-effective health intervention, especially due to its effects on reducing unintended pregnancy and reducing sexually transmitted disease. It has been estimated that $1 spent on family planning can avert from $2 (in Ethiopia) to $9 (in Bolivia) in health costs, with an average of $8 for all women using all modern contraceptive methods.^[\\[1\\]](#fnjp82p7a4nj)^\n\nEffect on maternal mortality\n----------------------------\n\nContraceptive use reduces the risk of death per birth by preventing high-risk pregnancy.^[\\[1\\]](#fnjp82p7a4nj)^^[\\[2\\]](#fnbif6qyc7kwu)^ These include but are not limited to pregnancy in the very young, pregnancy in older women, births that are spaced very close together, and high parity births (births that occur in a woman who has already given birth many times).\n\nEffect on child mortality\n-------------------------\n\nIn developing countries, children born two years or earlier after an older sibling were at a 60% increased risk of death in infancy, while those born between two and three years had a 10% increase, compared with those born after intervals of four to five years.^[\\[3\\]](#fn6pjhd9fnpeq)^ Contraceptive use to increase the birth interval therefore reduces child mortality.^[\\[1\\]](#fnjp82p7a4nj)^\n\n1.  ^**[^](#fnrefjp82p7a4nj)**^\n    \n    Tsui, A. O., R. McDonald-Mosley & A. E. Burke (2010) [Family planning and the burden of unintended pregnancies](https://doi.org/10.1093/epirev/mxq012), *Epidemiologic Reviews*, vol. 32, pp. 152–174.\n    \n2.  ^**[^](#fnrefbif6qyc7kwu)**^\n    \n    Stover, John & John Ross (2010) [How increased contraceptive use has reduced maternal mortality](http://doi.org/10.1007/s10995-009-0505-y), *Maternal and Child Health Journal*, vol. 14, pp. 687–695.\n    \n3.  ^**[^](#fnref6pjhd9fnpeq)**^\n    \n    Rutstein, S. O. (2005) [Effects of preceding birth intervals on neonatal, infant and under-five years mortality and nutritional status in developing countries: evidence from the demographic and health surveys](http://doi.org/10.1016/j.ijgo.2004.11.012), *International Journal of Gynecology & Obstetrics*, vol. 89, pp. S7–S24."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "QvvdayWWSvmeBCtqZ",
    "name": "Community experiences",
    "core": false,
    "slug": "community-experiences",
    "oldSlugs": [
      "community-experiences"
    ],
    "postCount": 109,
    "description": {
      "markdown": "The **community experiences** tag covers posts about members of the EA community recounting their own or others' experiences with the EA movement - such as origin stories, lessons learnt or experiences participating in an event, job, internship or community.\n\n## Related entries\n\n[effective altruism lifestyle](https://forum.effectivealtruism.org/tag/effective-altruism-lifestyle)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "DRwt8MuBiogPjfYKP",
    "name": "Philippines",
    "core": false,
    "slug": "philippines",
    "oldSlugs": null,
    "postCount": 16,
    "description": {
      "markdown": "The **Philippines** tag is mainly for posts on one or more of the following topics relevant to the Philippines:\n\n1.  The EA community in the Philippines, known as EA Philippines\n2.  Causes/issues in the Philippines that could be important to work on or donate to for the global EA community or the EA Philippines community\n3.  Career or donation advice tailored for Filipinos or people from the Philippines\n4.  How the Philippines is relevant to one or more priority issues/causes of the EA community\n\n## External links\n\n[Effective Altruism Philippines](https://www.facebook.com/EffectiveAltruismPhilippines/). Facebook page. "
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "ihpwNfh2ZxR4ZHaAK",
    "name": "Prize",
    "core": false,
    "slug": "prize",
    "oldSlugs": null,
    "postCount": 55,
    "description": {
      "markdown": "A **prize** is something of value given to reward success in a contest or to recognize outstanding achievement. Prizes have been proposed as an alternative to existing methods for incentivizing innovation, such as patents and grants.^[\\[1\\]](#fnv0x7khvnkmh)^\n\nFurther reading\n---------------\n\nBrunt, Liam, Josh Lerner & Tom Nicholas (2012) [Inducement prizes and innovation](https://doi.org/10.1111/joie.12002), *Journal of Industrial Economics*, vol. 60, pp. 657–696.\n\nGraff Zivin, Joshua & Elizabeth Lyons (2020) [The effects of prize structures on innovative performance](http://doi.org/10.3386/w26737), working paper 26737, National Bureau of Economic Research.\n\nStiglitz, Joseph (2006) [Innovation: A better way than patents](https://www.newscientist.com/article/dn10090-innovation-a-better-way-than-patents/), *New Scientist*, September 17.\n\nRelated entries\n---------------\n\n[certificate of impact](https://forum.effectivealtruism.org/tag/certificate-of-impact) | [markets for altruism](https://forum.effectivealtruism.org/tag/markets-for-altruism) | [bounty (open)](https://forum.effectivealtruism.org/topics/bounty-open)\n\n1.  ^**[^](#fnrefv0x7khvnkmh)**^\n    \n    Clancy, Matthew S. & Gian Carlo Moschini (2013) [Incentives for innovation: Patents, prizes, and research contracts](https://doi.org/10.1093/aepp/ppt012), *Applied Economic Perspectives and Policy*, vol. 35, pp. 206–241."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "gE5tjjgKjiFL59pyc",
    "name": "Future of Life Institute",
    "core": false,
    "slug": "future-of-life-institute",
    "oldSlugs": null,
    "postCount": 17,
    "description": {
      "markdown": "The **Future of Life Institute** (**FLI**) is a non-profit that works to reduce [existential risk](https://forum.effectivealtruism.org/tag/existential-risk) from [powerful technologies](https://forum.effectivealtruism.org/tag/anthropogenic-existential-risk), particularly [artificial intelligence](https://forum.effectivealtruism.org/tag/ai-risk), and to promote [positive visions of the future](https://forum.effectivealtruism.org/tag/flourishing-futures). FLI's work consists of grantmaking, educational outreach, and advocating for better policymaking in the United Nations, the United States government, and European Union institutions.\n\nHistory\n-------\n\nFLI was founded in 2014 by [Jaan Tallinn](https://forum.effectivealtruism.org/tag/jaan-tallinn), Max Tegmark, Viktoriya Krakovna, Anthony Aguirre and Meia Chita-Tegmark.\n\nFunding\n-------\n\nAs of July 2022, FLI has received over $1.9 million in funding from [Open Philanthropy](https://forum.effectivealtruism.org/topics/open-philanthropy),^[\\[1\\]](#fnssougodl1eq)^ and $500,000 from the [Survival and Flourishing](https://forum.effectivealtruism.org/topics/survival-and-flourishing) Fund.^[\\[2\\]](#fn637f5e6rogi)^^[\\[3\\]](#fnj6cw789zvik)^\n\nFuture of Life Award\n--------------------\n\nEvery year, FLI awards a prize, named the Future of Life Award, to one or more individuals judged to have had an extraordinary positive social impact but whose contributions are not sufficiently widely recognized. As of 2022, sixteen people have been so honored: [Vasili Arkhipov](https://forum.effectivealtruism.org/tag/vasili-arkhipov) (2017), [Stanislav Petrov](https://forum.effectivealtruism.org/tag/stanislav-petrov) (2018), Matthew Meselson (2019), [Viktor Zhdanov](https://forum.effectivealtruism.org/tag/viktor-zhdanov) and William Folge (2020), and Joe Farman, Susan Solomon and Stephen Andersen (2021), and Jeannie Peterson, Paul Crutzen, John Birks, Richard Turco, Brian Toon, [Carl Sagan](https://forum.effectivealtruism.org/topics/carl-sagan), Georgiy Stenchikov and Alan Robock (2022).^[\\[4\\]](#fn9e7gliv6iws)^\n\nExternal links\n--------------\n\n[Future of Life Institute](https://futureoflife.org/). Official website.\n\n[Apply for a job](https://futureoflife.org/job-postings/).\n\n[Donate to the Future of Life Institute](https://futureoflife.org/donate-fli/).\n\n1.  ^**[^](#fnrefssougodl1eq)**^\n    \n    Open Philanthropy (2022) [Grants database: Future of Life Institute](https://www.openphilanthropy.org/grants/?q=&organization-name=future-of-life-institute), *Open Philanthropy*.\n    \n2.  ^**[^](#fnref637f5e6rogi)**^\n    \n    Survival and Flourishing Fund (2019a) [SFF-2020-H1 S-process recommendations announcement](https://survivalandflourishing.fund/sff-2020-h1-recommendations), *Survival and Flourishing Fund*. \n    \n3.  ^**[^](#fnrefj6cw789zvik)**^\n    \n    Survival and Flourishing Fund (2019b) [SFF-2020-H2 S-process recommendations announcement](https://survivalandflourishing.fund/sff-2020-h2-recommendations), *Survival and Flourishing Fund*.\n    \n4.  ^**[^](#fnref9e7gliv6iws)**^\n    \n    Future of Life Institute (2022) [Future of Life Award: celebrating the unsung heroes of our time](https://futureoflife.org/future-of-life-award/), *Future of Life Institute*."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "CoSj6ztCccBxbxXoz",
    "name": "Instrumental convergence thesis",
    "core": false,
    "slug": "instrumental-convergence-thesis",
    "oldSlugs": [
      "instrumental-convergence"
    ],
    "postCount": 4,
    "description": {
      "markdown": "The **instrumental convergence thesis** is the alleged overlap in instrumental goals expected to be exhibited by a broad class of advanced [AI systems](https://forum.effectivealtruism.org/tag/artificial-intelligence), due to their usefulness for the attainment of a wide range of possible final goals.\n\nCommonly expected instrumental goals towards which all intelligent agents would converge are self-preservation, goal preservation, and resource acquisition.\n\nFurther reading\n---------------\n\nBostrom, Nick (2012) [The superintelligent will: motivation and instrumental rationality in advanced artificial agents](http://doi.org/10.1007/s11023-012-9281-3), *Minds and Machines*, vol. 22, pp. 71–85.\n\nYudkowsky, Eliezer (2013) [Five theses, two lemmas, and a couple of strategic implications](https://intelligence.org/2013/05/05/five-theses-two-lemmas-and-a-couple-of-strategic-implications/), *Machine Intelligence Research Institute's Blog*, May 5.\n\nRelated entries\n---------------\n\n[basic AI drive](https://forum.effectivealtruism.org/tag/basic-ai-drive) | [orthogonality thesis](https://forum.effectivealtruism.org/tag/orthogonality-thesis)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "qGmmwWhnWhxR9i9HE",
    "name": "Innovations for Poverty Action",
    "core": false,
    "slug": "innovations-for-poverty-action",
    "oldSlugs": null,
    "postCount": 2,
    "description": {
      "markdown": "**Innovations for Poverty Action** (**IPA**) is a non-profit organization that carries out research on poverty alleviation and development programs, and advocates for the use of this research in decision-making.\n\nThe [Abdul Latif Jameel Poverty Action Lab](https://forum.effectivealtruism.org/tag/abdul-latif-jameel-poverty-action-lab) (J-PAL) is a close partner of IPA.\n\nHistory\n-------\n\nIPA was founded in 2011 by Dean Karlan, an American development economist.\n\nEvaluation\n----------\n\nIn 2011, [GiveWell](https://forum.effectivealtruism.org/tag/givewell) rated IPA a [standout charity](https://forum.effectivealtruism.org/topics/givewell).^[\\[1\\]](#fnl2pqdcpoyqm)^ As of July 2022, IPA has received over $3 million in funding from [Effective Altruism Funds](https://forum.effectivealtruism.org/topics/effective-altruism-funds),^[\\[2\\]](#fnhyveam4pdpl)^^[\\[3\\]](#fn4dwygma5xid)^^[\\[4\\]](#fn4q3wf1c50kb)^^[\\[5\\]](#fnhsh9ywlm0ma)^ and $350,000 from [Open Philanthropy](https://forum.effectivealtruism.org/tag/open-philanthropy),^[\\[6\\]](#fni3bhnym7pe)^ and is also featured in [The Life You Can Save](https://forum.effectivealtruism.org/tag/the-life-you-can-save)'s list of \"best charities\".^[\\[7\\]](#fnf5mcbxp93a9)^\n\nFurther reading\n---------------\n\nSebastian Sanchez *et al.* (2021) [Timeline of Innovations for Poverty Action](https://timelines.issarice.com/wiki/Timeline_of_Innovations_for_Poverty_Action), *Timelines Wiki*.\n\nExternal links\n--------------\n\n[Innovations for Poverty Action](https://www.poverty-action.org/). Official website.\n\n[Apply for a job](https://www.poverty-action.org/work-with-ipa/current-opportunities).\n\n[Donate to IPA](https://donate.poverty-action.org/give/192232/#!/donation/checkout).\n\nRelated entries\n---------------\n\n[Abdul Latif Jameel Poverty Action Lab](https://forum.effectivealtruism.org/tag/abdul-latif-jameel-poverty-action-lab)\n\n1.  ^**[^](#fnrefl2pqdcpoyqm)**^\n    \n    GiveWell (2011) [Innovations for Poverty Action (IPA)](https://www.givewell.org/international/charities/ipa), *GiveWell*, November 28.\n    \n2.  ^**[^](#fnrefhyveam4pdpl)**^\n    \n    Global Health and Development Fund (2020) [July 2020: Innovations for Poverty Action](https://funds.effectivealtruism.org/funds/payouts/july-2020-innovations-for-poverty-action), *Effective Altruism Funds*, July. \n    \n3.  ^**[^](#fnref4dwygma5xid)**^\n    \n    Global Health and Development Fund (2020) [November 2020: Innovations for Poverty Action](https://funds.effectivealtruism.org/funds/payouts/november-2020-innovations-for-poverty-action), *Effective Altruism Funds*, November. \n    \n4.  ^**[^](#fnref4q3wf1c50kb)**^\n    \n    Global Health and Development Fund (2020) [December 2020: Innovations for Poverty Action](https://funds.effectivealtruism.org/funds/payouts/december-2020-innovations-for-poverty-action), *Effective Altruism Funds*, December. \n    \n5.  ^**[^](#fnrefhsh9ywlm0ma)**^\n    \n    Global Health and Development Fund (2021) [May 2021: Innovations for Poverty Action](https://funds.effectivealtruism.org/funds/payouts/may-2021-innovations-for-poverty-action), *Effective Altruism Funds*, May.\n    \n6.  ^**[^](#fnrefi3bhnym7pe)**^\n    \n    Open Philanthropy (2022) [Grants database: Innovations for Poverty Action](https://www.openphilanthropy.org/grants/?q=&organization-name=innovations-for-poverty-action), *Open Philanthropy*.\n    \n7.  ^**[^](#fnreff5mcbxp93a9)**^\n    \n    The Life You Can Save (2021) [Best charities](https://www.thelifeyoucansave.org/best-charities/), *The Life You Can Save*."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "7Y5hYpbAGFrDrPfZM",
    "name": "Moral patienthood",
    "core": false,
    "slug": "moral-patienthood",
    "oldSlugs": [
      "moral-patient"
    ],
    "postCount": 23,
    "description": {
      "markdown": "**Moral patienthood** is the condition of deserving moral consideration. A **moral patient** is an entity that possesses moral patienthood.\n\nWhile it is normally agreed that typical humans are moral patients, there is debate about the patienthood of many other types of beings, including human embryos, [non-human animals](https://forum.effectivealtruism.org/topics/animal-welfare-1), [future people](https://forum.effectivealtruism.org/topics/population-ethics), and [digital sentients](https://forum.effectivealtruism.org/topics/artificial-sentience).\n\nMoral patienthood should not be confused with moral agency.^[\\[1\\]](#fnrnoqqfllm4h)^ For example, we might think that a baby lacks moral agency - it lacks the ability to judge right from wrong, and to act on the basis of reasons - but that it is still a moral patient, in the sense that those with moral agency should care about their well-being.\n\nIf we assume a [welfarist](https://forum.effectivealtruism.org/tag/welfarism) theory of the good, the question of patienthood can be divided into two sub-questions: *Which entities can have well-being?* and *Whose well-being is morally relevant?*  Each question can in turn be broken down into the question of which characteristics or capacities are relevant and the question of which beings have those capacities.\n\nFirst, which entities can have well-being? A majority of scientists now agree that many non-human animals, including mammals, birds, and fish, are [conscious](https://forum.effectivealtruism.org/tag/consciousness) and capable of feeling [pain](https://forum.effectivealtruism.org/tag/pain-and-suffering),^[\\[2\\]](#fnt60bk5cujqs)^ but this claim is more contentious in philosophy.^[\\[3\\]](#fne1y75hdul44)^ This question is vital for assessing the value of interventions aimed at improving [farm](https://forum.effectivealtruism.org/tag/farmed-animal-welfare) and/or [wild animal welfare](https://forum.effectivealtruism.org/tag/wild-animal-initiative). A smaller but growing field of study considers whether artificial intelligences might be conscious in morally relevant ways.^[\\[4\\]](#fn2rxiodkjlc)^\n\nSecond, whose well-being do we care about? Some have argued that future beings have less value, even though they will be just as conscious as today’s beings are now. This reduction could be assessed in the form of a [discount rate](https://forum.effectivealtruism.org/tag/temporal-discounting) on future value, so that experiences occurring one year from now are worth, say, 3% less than they do at present. Alternatively, it could be assessed by valuing individuals who do not yet exist less than current beings, for reasons related to the non-identity problem^[\\[5\\]](#fnrgwe2i34tle)^ (see also [population ethics](https://forum.effectivealtruism.org/tag/population-ethics)). It is contentious whether these approaches are correct. Moreover, in light of the astronomical number of individuals who could potentially exist in the future, assigning some value to future people implies that virtually all value—at least for welfarist theories—will reside in the far future^[\\[6\\]](#fny11tsvcv4d)^ (see also [longtermism](https://forum.effectivealtruism.org/tag/longtermism)).\n\nFurther reading\n---------------\n\nAnimal Ethics (2017) [*The relevance of sentience*](https://www.animal-ethics.org/sentience-section/relevance-of-sentience/), *Animal Ethics*, September.\n\nBostrom, Nick & Eliezer Yudkowsky (2014) [The ethics of artificial intelligence](https://doi.org/10.1017/CBO9781139046855.020), in Keith Frankish & William M. Ramsey (eds.) *The Cambridge Handbook of Artificial Intelligence*, Cambridge: Cambridge University Press, pp. 316–334.\n\nKagan, Shelly (2019) [*How to Count Animals, More or Less*](https://en.wikipedia.org/wiki/Special:BookSources/9780191868177), Oxford: Oxford University Press.\n\nMacAskill, W. & Meissner, D. (2020) [The expanding moral circle](https://www.utilitarianism.net/utilitarianism-and-practical-ethics#the-expanding-moral-circle), in [*Introduction to Utilitarianism*](https://www.utilitarianism.net/).\n\nMuehlhauser, Luke (2017) [2017 report on consciousness and moral patienthood](https://www.openphilanthropy.org/2017-report-consciousness-and-moral-patienthood), *Open Philanthropy*, June.\n\nTomasik, Brian (2014) [Do artificial reinforcement-learning agents matter morally?](https://foundational-research.org/files/do-artificial-reinforcement-learning-agents-matter-morally.pdf), arXiv:1410.8233.\n\nRelated entries\n---------------\n\n[axiology](https://forum.effectivealtruism.org/tag/axiology) | [consciousness research](https://forum.effectivealtruism.org/tag/consciousness-research) | [moral circle expansion](https://forum.effectivealtruism.org/tag/moral-circle-expansion-1) | [moral weight](https://forum.effectivealtruism.org/tag/moral-weight) | [speciesism](https://forum.effectivealtruism.org/tag/speciesism) | [valence](https://forum.effectivealtruism.org/tag/valence)\n\n1.  ^**[^](#fnrefrnoqqfllm4h)**^\n    \n    Wikipedia (2004) [Distinction between moral agency and moral patienthood](https://en.wikipedia.org/wiki/Moral_agency#Distinction_between_moral_agency_and_moral_patienthood), in 'Moral agency', *Wikipedia*, September 25 (updated 14 November 2020‎).\n    \n2.  ^**[^](#fnreft60bk5cujqs)**^\n    \n    Low, Philip *et al.* (2012) [The Cambridge declaration on consciousness](http://fcmconference.org/img/CambridgeDeclarationOnConsciousness.pdf), *Francis Crick Memorial Conference*, July 7.\n    \n3.  ^**[^](#fnrefe1y75hdul44)**^\n    \n    Allen, Colin & Michael Trestman (2016) [Animal consciousness](http://plato.stanford.edu/entries/consciousness-animal/#summary), in Edward Zalta (ed.), *Stanford Encyclopedia of Philosophy*.\n    \n4.  ^**[^](#fnref2rxiodkjlc)**^\n    \n    Wikipedia (2003) [Artificial consciousness](https://en.wikipedia.org/wiki/Artificial_consciousness), *Wikipedia*, March 13 (updated 24 April 2021‎).\n    \n5.  ^**[^](#fnrefrgwe2i34tle)**^\n    \n    Roberts, M. A. (2019) [The nonidentity problem](http://plato.stanford.edu/entries/nonidentity-problem/), in Edward Zalta (ed.), *Stanford Encyclopedia of Philosophy*.\n    \n6.  ^**[^](#fnrefy11tsvcv4d)**^\n    \n    Bostrom, Nick (2009) [Astronomical waste: the opportunity cost of delayed technological development](http://intelligence.org/files/AstronomicalWaste.pdf), *Utilitas* 15(3), pp. 308-314."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "KEiyjtcdx2dJXdWLj",
    "name": "Perverse instantiation",
    "core": false,
    "slug": "perverse-instantiation",
    "oldSlugs": null,
    "postCount": null,
    "description": {
      "markdown": "**Perverse instantiation** is a type of [malignant AI failure mode](https://forum.effectivealtruism.org/tag/malignant-ai-failure-mode) involving the satisfaction of an AI's goals in ways contrary to the intentions of those who programmed it.\n\nFurther reading\n---------------\n\nBostrom, Nick (2014) [*Superintelligence: Paths, Dangers, Strategies*](https://en.wikipedia.org/wiki/Special:BookSources/9780199678112), Oxford: Oxford University Press, pp. 120–122.\n\nRelated entries\n---------------\n\n[existential risk](https://forum.effectivealtruism.org/tag/existential-risk) | [infrastructure profusion](https://forum.effectivealtruism.org/tag/infrastructure-profusion) | [malignant AI failure mode](https://forum.effectivealtruism.org/tag/malignant-ai-failure-mode) | [mind crime](https://forum.effectivealtruism.org/tag/mind-crime) | [superintelligence](https://forum.effectivealtruism.org/tag/superintelligence)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "ZpH5Pzhpyu6bhtpaP",
    "name": "Mind crime",
    "core": false,
    "slug": "mind-crime",
    "oldSlugs": null,
    "postCount": null,
    "description": {
      "markdown": "**Mind crime** (sometimes spelled **mindcrime**) is a type of [malignant AI failure mode](https://forum.effectivealtruism.org/tag/malignant-ai-failure-modes) involving the mistreatment of [morally relevant](https://forum.effectivealtruism.org/tag/moral-patienthood) computational processes.\n\nFurther reading\n---------------\n\nBostrom, Nick (2014) [*Superintelligence: Paths, Dangers, Strategies*](https://en.wikipedia.org/wiki/Special:BookSources/9780199678112), Oxford: Oxford University Press, pp. 125-126.\n\nRelated entries\n---------------\n\n[artificial sentience](https://forum.effectivealtruism.org/topics/artificial-sentience) | [existential risk](https://forum.effectivealtruism.org/tag/existential-risk) | [infrastructure profusion](https://forum.effectivealtruism.org/tag/infrastructure-profusion) | [malignant AI failure mode](https://forum.effectivealtruism.org/tag/malignant-ai-failure-mode) | [perverse instantiation](https://forum.effectivealtruism.org/tag/perverse-instantiation) | [superintelligence](https://forum.effectivealtruism.org/tag/superintelligence)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "JEHutiZopomHSkaSC",
    "name": "Malignant AI failure mode",
    "core": false,
    "slug": "malignant-ai-failure-mode",
    "oldSlugs": [
      "ai-failure-modes",
      "malignant-ai-failure-modes",
      "malignant-ai-failure-modes"
    ],
    "postCount": 2,
    "description": {
      "markdown": "An **AI failure mode** is a way in which a project to develop machine [superintelligence](https://forum.effectivealtruism.org/tag/superintelligence) may fail. ***Malignant*** **AI failure modes** are AI failure modes that result in an [existential catastrophe](https://forum.effectivealtruism.org/tag/existential-catastrophe-1).\n\n[Nick Bostrom](https://forum.effectivealtruism.org/tag/nick-bostrom) categorizes malignant AI failure modes into three basic types: [perverse instantiation](https://forum.effectivealtruism.org/tag/perverse-instantiation), which involves the satisfaction of an AI's goals in ways contrary to the intentions of those who programmed it; [infrastructure profusion](https://forum.effectivealtruism.org/tag/infrastructure-profusion), which involves the transformation of large parts of the accessible universe into infrastructure in the service of some goal that impedes the realization of humanity's long-term potential; and [mind crime](https://forum.effectivealtruism.org/tag/mind-crime), which involves the mistreatment of [morally relevant](https://forum.effectivealtruism.org/tag/moral-patienthood) computational processes.\n\nFurther reading\n---------------\n\nBostrom, Nick (2014) [*Superintelligence: Paths, Dangers, Strategies*](https://en.wikipedia.org/wiki/Special:BookSources/9780199678112), Oxford: Oxford University Press, pp. 119-126.\n\nRelated entries\n---------------\n\n[existential risk](https://forum.effectivealtruism.org/tag/existential-risk) | [infrastructure profusion](https://forum.effectivealtruism.org/tag/infrastructure-profusion) | [mind crime](https://forum.effectivealtruism.org/tag/mind-crime) | [perverse instantiation](https://forum.effectivealtruism.org/tag/perverse-instantiation) | [superintelligence](https://forum.effectivealtruism.org/tag/superintelligence)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "JJaqR2YiohdTLc2dQ",
    "name": "Infrastructure profusion",
    "core": false,
    "slug": "infrastructure-profusion",
    "oldSlugs": null,
    "postCount": null,
    "description": {
      "markdown": "**Infrastructure profusion** is a type of [malignant AI failure mode](https://forum.effectivealtruism.org/tag/malignant-ai-failure-modes) involving the transformation of large parts of the accessible universe into infrastructure in the service of some goal that impedes the realization of humanity's long-term potential.\n\nFurther reading\n---------------\n\nBostrom, Nick (2014) [*Superintelligence: Paths, Dangers, Strategies*](https://en.wikipedia.org/wiki/Special:BookSources/9780199678112), Oxford: Oxford University Press, pp. 122–125.\n\nRelated entries\n---------------\n\n[existential risk](https://forum.effectivealtruism.org/tag/existential-risk) | [malignant AI failure mode](https://forum.effectivealtruism.org/tag/malignant-ai-failure-mode) | [mind crime](https://forum.effectivealtruism.org/tag/mind-crime) | [perverse instantiation](https://forum.effectivealtruism.org/tag/perverse-instantiation) | [superintelligence](https://forum.effectivealtruism.org/tag/superintelligence)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "F8vvnTmyNMfnoKRJ2",
    "name": "Holden Karnofsky",
    "core": false,
    "slug": "holden-karnofsky",
    "oldSlugs": null,
    "postCount": 29,
    "description": {
      "markdown": "**Holden Karnofsky** (born 1981) is an American philanthropist. He is co-CEO of [Open Philanthropy](https://forum.effectivealtruism.org/tag/open-philanthropy) and a co-founder and board member of [GiveWell](https://forum.effectivealtruism.org/tag/givewell).\n\nFurther reading\n---------------\n\nGalef, Julia & Massimo Pigliucci (2011) [Evidence-based philanthropy (Holden Karnofsky)](http://rationallyspeakingpodcast.org/38-evidence-based-philanthropy-holden-karnofsky/), *Rationally Speaking*, July 3.\n\nKarnofsky, Holden (2021) [The ‘most important century’ blog post series](https://www.cold-takes.com/most-important-century/), *Cold Takes*, September 24.\n\nKlein, Ezra (2021) [Transcript: Ezra Klein interviews Holden Karnofsky](https://www.nytimes.com/2021/10/05/podcasts/transcript-ezra-klein-interviews-holden-karnofsky.html), *The New York Times*, October 5.\n\nTodd, Benjamin (2014) [Interview: Holden Karnofsky on cause selection](https://80000hours.org/2014/10/interview-holden-karnofsky-on-cause-selection/), *80,000 Hours*, October 3.\n\nWiblin, Robert & Keiran Harris (2018) [The world’s most intellectual foundation is hiring. Holden Karnofsky, founder of GiveWell, on how philanthropy can have maximum impact by taking big risks](https://80000hours.org/podcast/episodes/holden-karnofsky-open-philanthropy/), *80,000 Hours*, February 27.\n\nWiblin, Robert & Keiran Harris (2021) [Holden Karnofsky on the most important century](https://80000hours.org/podcast/episodes/holden-karnofsky-most-important-century/), *80,000 Hours*, August 19.\n\nExternal links\n--------------\n\n[Cold Takes](https://www.cold-takes.com/). Karnofsky's blog.\n\n[Holden Karnofsky](https://forum.effectivealtruism.org/users/holdenkarnofsky). [Effective Altruism Forum](https://forum.effectivealtruism.org/tag/effective-altruism-forum-1) account.\n\nRelated entries\n---------------\n\n[cause prioritization](https://forum.effectivealtruism.org/tag/cause-prioritization) | [GiveWell](https://forum.effectivealtruism.org/tag/givewell) | [intervention evaluation](https://forum.effectivealtruism.org/tag/intervention-evaluation) | [longtermism](https://forum.effectivealtruism.org/tag/longtermism) | [Open Philanthropy](https://forum.effectivealtruism.org/tag/open-philanthropy)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "WBNJeShE3fD9DaDh7",
    "name": "Hilary Greaves",
    "core": false,
    "slug": "hilary-greaves",
    "oldSlugs": null,
    "postCount": 7,
    "description": {
      "markdown": "**Hilary Greaves** (born 29 September 1978) is a British philosopher. She is currently a professor at the University of Oxford and the director of the [Global Priorities Institute](https://forum.effectivealtruism.org/tag/global-priorities-institute).\n\nGreaves earned a BA in philosophy and physics from Oxford and a PhD in philosophy from Rutgers. Her doctoral thesis, completed under the supervision of Frank Arntzenius, was \"Spacetime symmetries and the CPT theorem\".^[\\[1\\]](#fn2u3b4m45miq)^ Prior to her current professorship, she held appointments at Merton and Somerville Colleges.\n\nGreaves’ current work is on issues related to [global priorities research](https://forum.effectivealtruism.org/tag/global-priorities-research). Her research interests include [moral philosophy](https://forum.effectivealtruism.org/tag/moral-philosophy) (including foundational issues in [consequentialism](https://forum.effectivealtruism.org/tag/consequentialism), interpersonal aggregation, [population ethics](https://forum.effectivealtruism.org/tag/population-ethics), and [moral uncertainty](https://forum.effectivealtruism.org/tag/moral-uncertainty)), formal epistemology, and the philosophy of physics.\n\nFurther reading\n---------------\n\nGreaves, Hilary (2020) [Evidence, cluelessness, and the long term](https://forum.effectivealtruism.org/posts/LdZcit8zX89rofZf3/evidence-cluelessness-and-the-long-term-hilary-greaves), *Effective Altruism Forum*, November 1.\n\nGreaves, Hilary & John Cusbert (2021) [Comparing existence and non-existence](https://www.iffs.se/media/23233/climate_ethics_vol3_webb.pdf#page=164), in Joe Roussos & Paul Bowman (eds.) *Studies on Climate Ethics and Future Generations Vol. 3*, Stockholm: Institute for Futures Studies, pp. 163–196.\n\nWiblin, Robert & Keiran Harris (2018) [Philosophy Prof Hilary Greaves on moral cluelessness, population ethics, probability within a multiverse, & harnessing the brainpower of academia to tackle the most important research questions](https://80000hours.org/podcast/episodes/hilary-greaves-global-priorities-institute/), *80,000 Hours*, October 23.\n\nExternal links\n--------------\n\n[Hilary Greaves](https://users.ox.ac.uk/~mert2255/). Personal homepage.\n\n1.  ^**[^](#fnref2u3b4m45miq)**^\n    \n    Greaves, Hilary (2008) [*Spacetime Symmetries and the CPT Theorem*](https://doi.org/10.7282/T3CF9QFX), PhD thesis, Rutgers University."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "FZFX5NNLb7kBmzZwa",
    "name": "High Impact Athletes",
    "core": false,
    "slug": "high-impact-athletes",
    "oldSlugs": null,
    "postCount": 5,
    "description": {
      "markdown": "**High Impact Athletes** (**HIA**) is a nonprofit that spreads the ideas of [effective altruism](https://forum.effectivealtruism.org/tag/effective-altruism) among professional athletes and encourages them to [pledge to donate](https://forum.effectivealtruism.org/tag/donation-pledge) a percentage of their income to [effective charities](https://forum.effectivealtruism.org/tag/effective-giving-1).\n\nHistory\n-------\n\nHIA was founded in 2020 by Marcus Daniell, an Olympic bronze medallist tennis player from New Zealand. Daniell is also HIA's executive director.\n\nFunding\n-------\n\nAs of July 2022, HIA has received $350,000 in funding from the [Future Fund](https://forum.effectivealtruism.org/topics/future-fund),^[\\[1\\]](#fnumum6uvnl6r)^ and $110,000 from [Effective Altruism Funds](https://forum.effectivealtruism.org/topics/effective-altruism-funds).^[\\[2\\]](#fnzk9nam35u5)^\n\nFurther reading\n---------------\n\nDaniell, Marcus (2020) [Introducing High Impact Athletes](https://forum.effectivealtruism.org/posts/Fm3HWDZKtwdkvBzGL/introducing-high-impact-athletes), *Effective Altruism Forum*, November 30.\n\nRighetti, Luca & Fin Moorhouse (2021) [Marcus Daniell on High Impact Athletes, communicating EA, and the purpose of sport](https://hearthisidea.com/episodes/marcus), *Hear This Idea*, February 8.\n\nExternal links\n--------------\n\n[High-Impact Athletes](https://highimpactathletes.org/). Official website.\n\nRelated entries\n---------------\n\n[donation pledge](https://forum.effectivealtruism.org/tag/donation-pledge) | [High Impact Medicine](https://forum.effectivealtruism.org/topics/high-impact-medicine) | [Raising for Effective Giving](https://forum.effectivealtruism.org/tag/raising-for-effective-giving)\n\n1.  ^**[^](#fnrefumum6uvnl6r)**^\n    \n    Future Fund (2022) [Our grants and investments: High Impact Athletes](https://ftxfuturefund.org/all-grants/?_organization_name=high-impact-athletes), *Future Fund*.\n    \n2.  ^**[^](#fnrefzk9nam35u5)**^\n    \n    Effective Altruism Infrastructure Fund (2021) [May 2021: EA Infrastructure Fund grants](https://funds.effectivealtruism.org/funds/payouts/may-2021-ea-infrastructure-fund-grants), *Effective Altruism Funds*, May."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "QvoqEEmb3FAAa5Cki",
    "name": "Hellish existential catastrophe",
    "core": false,
    "slug": "hellish-existential-catastrophe",
    "oldSlugs": null,
    "postCount": 1,
    "description": {
      "markdown": "A **hellish existential catastrophe** (sometimes called a **hyper existential catastrophe**^[\\[1\\]](#fngfccwew1ugh)^^[\\[2\\]](#fngbwrpz88aa)^) is a catastrophe that not only [destroys humanity's long-term potential](https://forum.effectivealtruism.org/tag/existential-catastrophe-1) but also [creates negative value on an astronomical scale](https://forum.effectivealtruism.org/tag/s-risk).\n\nFurther reading\n---------------\n\nAlthaus, David & Lukas Gloor (2019) [Reducing risks of astronomical suffering: a neglected priority](https://longtermrisk.org/reducing-risks-of-astronomical-suffering-a-neglected-priority/), *Center on Long-Term Risk*, August.\n\nOrd, Toby (2020) [*The Precipice: Existential Risk and the Future of Humanity*](https://en.wikipedia.org/wiki/Special:BookSources/1526600218), London: Bloomsbury Publishing, ch. 6, fn. 14.\n\nRelated entries\n---------------\n\n[existential catastrophe](https://forum.effectivealtruism.org/tag/existential-catastrophe-1) | [hedonium](https://forum.effectivealtruism.org/tag/hedonium) | [s-risk](https://forum.effectivealtruism.org/tag/s-risk) | [total existential risk](https://forum.effectivealtruism.org/tag/total-existential-risk)\n\n1.  ^**[^](#fnrefgfccwew1ugh)**^\n    \n    Bostrom, Nick (2014) [Crucial considerations and wise philanthropy](http://www.stafforini.com/blog/bostrom/), *Good Done Right*, July 9.\n    \n2.  ^**[^](#fnrefgbwrpz88aa)**^\n    \n    Arbital (2017) [Separation from hyperexistential risk](https://arbital.com/p/hyperexistential_separation/), *Arbital*."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "bmmquQhyA4zho9heJ",
    "name": "Hedonism",
    "core": false,
    "slug": "hedonism",
    "oldSlugs": null,
    "postCount": 10,
    "description": {
      "markdown": "**Hedonism** is the moral theory according to which all and only pleasant and unpleasant states have intrinsic value.\n\nFurther reading\n---------------\n\nChappell, Richard Y., Darius Meissner & William MacAskill (2022) [Hedonism](https://www.utilitarianism.net/theories-of-wellbeing#hedonism), in 'Theories of well-being', *An Introduction to Utilitarianism*.\n\nMoore, Andrew (2004) [Hedonism](https://plato.stanford.edu/entries/hedonism/), *Stanford Encyclopedia of Philosophy*, April 20 (updated 13 December 2019).\n\nRelated entries\n---------------\n\n[axiology](https://forum.effectivealtruism.org/tag/axiology) | [introspective hedonism](https://forum.effectivealtruism.org/tag/introspective-hedonism)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "DBmJNQT3g8raiuAEt",
    "name": "Good Ventures",
    "core": false,
    "slug": "good-ventures",
    "oldSlugs": null,
    "postCount": 3,
    "description": {
      "markdown": "**Good Ventures** is a private foundation based in San Francisco. It was co-founded by [Cari Tuna](https://forum.effectivealtruism.org/tag/cari-tuna) and [Dustin Moskovitz](https://forum.effectivealtruism.org/tag/dustin-moskovitz).\n\nAs of August 2022, Good Ventures has granted over $1.7 billion to organizations working in [global health and development](https://forum.effectivealtruism.org/tag/global-health-and-development), [farmed animal welfare](https://forum.effectivealtruism.org/tag/farmed-animal-welfare), [global catastrophic risks](https://forum.effectivealtruism.org/tag/global-catastrophic-risk), and other cause areas.^[\\[1\\]](#fnq07j0akgskn)^\n\nFurther reading\n---------------\n\nMatthews, Dylan (2015) [You have $8 billion. You want to do as much good as possible. What do you do?](https://www.vox.com/2015/4/24/8457895/givewell-open-philanthropy-charity), *Vox*, April 24.\n\nExternal links\n--------------\n\n[Good Ventures](https://www.goodventures.org/). Official website.\n\n[Apply for a job](https://www.goodventures.org/about-us/jobs).\n\nRelated links\n-------------\n\n[GiveWell](https://forum.effectivealtruism.org/tag/givewell) | [Open Philanthropy](https://forum.effectivealtruism.org/tag/open-philanthropy)\n\n1.  ^**[^](#fnrefq07j0akgskn)**^\n    \n    Good Ventures (2022) [Grants database](https://www.goodventures.org/our-portfolio/grants-database), *Good Ventures*."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "pvNeDbr3TbWvjt8rn",
    "name": "Cari Tuna",
    "core": false,
    "slug": "cari-tuna",
    "oldSlugs": null,
    "postCount": 3,
    "description": {
      "markdown": "**Cari Tuna** (born 1986) is an American journalist and philanthropist. She is the co-founder and president of [Good Ventures](https://forum.effectivealtruism.org/tag/good-ventures), the president of [Open Philanthropy](https://forum.effectivealtruism.org/tag/open-philanthropy), and a board member of [GiveWell](https://forum.effectivealtruism.org/tag/givewell).\n\nBackground\n----------\n\nTuna grew up in Evansville, Indiana. Both of her parents were medical doctors.  In high school, she was valedictorian, served as student council president, and founded her school’s Amnesty International chapter.^[\\[1\\]](#fnvss92lqw82l)^ She studied political science at Yale University, where she received a BA degree in 2008.^[\\[2\\]](#fn4e0cu0ulqbl)^\n\nJournalism career\n-----------------\n\nWhile pursuing her undergraduate studies, Tuna wrote extensively for *Yale Daily News*. She subsequently contributed to the *Evansville Courier & Press*, interned at the *Minneapolis Star Tribune* and, between 2008 and 2011, was a reporter for *The Wall Street Journal*, where she covered the California economy, the real-estate market, and the higher-education system.^[\\[1\\]](#fnvss92lqw82l)^^[\\[2\\]](#fn4e0cu0ulqbl)^\n\nPhilanthropic career\n--------------------\n\nTuna started dating [Dustin Moskovitz](https://forum.effectivealtruism.org/tag/dustin-moskovitz), the co-founder of Facebook and Asana, around 2009. In December 2010, Tuna and Moskovitz signed the [Giving Pledge](https://forum.effectivealtruism.org/tag/giving-pledge), becoming the youngest couple ever to do so.^[\\[1\\]](#fnvss92lqw82l)^ To fulfill that commitment, they established the foundation Good Ventures, of which Tuna became its president. At around that time, while preparing to transition from journalism to philanthropy, Tuna read [Peter Singer](https://forum.effectivealtruism.org/tag/peter-singer)'s [*The Life You Can Save*](https://forum.effectivealtruism.org/tag/the-life-you-can-save-book), which introduced her \"to the idea of not just trying to do some good with your giving, but doing as much good as you can.\"^[\\[3\\]](#fni65idlyy8hf)^^[\\[4\\]](#fn33wwfcr4lzh)^^[\\[5\\]](#fnftss3zol5se)^ It was also from that book that Tuna first learned about GiveWell. Shortly thereafter, she and Moskovitz met [Holden Karnofsky](https://forum.effectivealtruism.org/tag/holden-karnofsky), who was then GiveWell's co-executive director. Tuna was impressed by GiveWell's commitment to both [transparency](https://forum.effectivealtruism.org/tag/transparency) and [cause neutrality](https://forum.effectivealtruism.org/tag/cause-neutrality), and a collaboration between Good Ventures and GiveWell ensued. In April 2011, Tuna joined GiveWell's board of directors;^[\\[4\\]](#fn33wwfcr4lzh)^ in December that year, Good Ventures gave substantial grants to GiveWell's top-rated organizations;^[\\[3\\]](#fni65idlyy8hf)^ and in June 2012, Karnofsky announced that GiveWell and Good Ventures planned to \"act as a single team\",^[\\[6\\]](#fnq7ym509wwya)^ which resulted in the creation of GiveWell Labs and, in August 2014, of the Open Philanthropy Project.^[\\[7\\]](#fn735tnmhsgra)^\n\nUnder Tuna's leadership, Good Ventures has, as of August 2022, granted over $1.7 billion to organizations working in [global health and development](https://forum.effectivealtruism.org/tag/global-health-and-development), [farmed animal welfare](https://forum.effectivealtruism.org/tag/farmed-animal-welfare), [global catastrophic risks](https://forum.effectivealtruism.org/tag/global-catastrophic-risk), and other cause areas.^[\\[8\\]](#fncupx7zedz5g)^\n\nFurther reading\n---------------\n\nMacAskill, William (2016) [Fireside chat with Cari Tuna](https://www.eaglobal.org/talks/doing-philanthropy-better-q-and-a-with-cari-tuna/), *Effective Altruism Global*, August 7.\n\nMatthews, Dylan (2018) [You have $8 billion. You want to do as much good as possible. What do you do?](https://www.vox.com/2015/4/24/8457895/givewell-open-philanthropy-charity), *Vox*, October 16.\n\nOlanoff, Drew (2013) [Dustin Moskovitz and Cari Tuna launch site for their philanthropic foundation, Good Ventures](https://techcrunch.com/2013/03/12/dustin-moskovitz-and-cari-tuna-launch-site-for-their-philanthropic-foundation-good-ventures/), *TechCrunch*, March 12.\n\nSchultz, Abby (2019) [Open Philanthropy Project’s Cari Tuna on funding global health](https://www.barrons.com/articles/open-philanthropy-projects-cari-tuna-on-funding-global-health-51569261600), *Penta*, September 23.\n\nRelated entries\n---------------\n\n[Dustin Moskovitz](https://forum.effectivealtruism.org/tag/dustin-moskovitz) | [GiveWell](https://forum.effectivealtruism.org/tag/givewell) | [Good Ventures](https://forum.effectivealtruism.org/tag/good-ventures) | [Open Philanthropy](https://forum.effectivealtruism.org/tag/open-philanthropy)\n\n1.  ^**[^](#fnrefvss92lqw82l)**^\n    \n    Cha, Ariana Eunjung (2014) [Cari Tuna and Dustin Moskovitz: young Silicon Valley billionaires pioneer new approach to philanthropy](https://www.washingtonpost.com/business/billionaire-couple-give-plenty-to-charity-but-they-do-quite-a-bit-of-homework/2014/12/26/19fae34c-86d6-11e4-b9b7-b8632ae73d25_print.html), *The Washington Post*, December 26.\n    \n2.  ^**[^](#fnref4e0cu0ulqbl)**^\n    \n    Callahan, David (2013) [Meet Cari Tuna, the woman giving away Dustin Moskovitz’s Facebook fortune](https://web.archive.org/web/20141012003248/https://www.insidephilanthropy.com/tech-philanthropy/2013/9/12/meet-cari-tuna-the-woman-giving-away-dustin-moskovitzs-faceb.html), *Inside Philanthropy*, September 12.\n    \n3.  ^**[^](#fnrefi65idlyy8hf)**^\n    \n    Tuna, Cari (2011) [Guest post from Cari Tuna](https://blog.givewell.org/2011/12/23/guest-post-from-cari-tuna/), *The GiveWell Blog*, December 23.\n    \n4.  ^**[^](#fnref33wwfcr4lzh)**^\n    \n    Preston, Caroline (2012) [Another Facebook co-founder gets philanthropic](https://www.philanthropy.com/article/another-facebook-co-founder-gets-philanthropic/), *The Chronicle of Philanthropy*, January 10.\n    \n5.  ^**[^](#fnrefftss3zol5se)**^\n    \n    Gunther, Marc (2018) [Giving in the light of reason](https://ssir.org/articles/entry/giving_in_the_light_of_reason), *Stanford Social Innovation Review*.\n    \n6.  ^**[^](#fnrefq7ym509wwya)**^\n    \n    Karnofsky, Holden (2012) [GiveWell and Good Ventures](https://blog.givewell.org/2012/06/28/givewell-and-good-ventures/), *The GiveWell Blog*, June 28.\n    \n7.  ^**[^](#fnref735tnmhsgra)**^\n    \n    Karnofsky, Holden (2014) [Open Philanthropy Project (formerly GiveWell Labs)](https://blog.givewell.org/2014/08/20/open-philanthropy-project-formerly-givewell-labs/), *The GiveWell Blog*, August 20.\n    \n8.  ^**[^](#fnrefcupx7zedz5g)**^\n    \n    Good Ventures (2021) [Grants database](https://www.goodventures.org/our-portfolio/grants-database), *Good Ventures*."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "strvznFk2PyiZd4PM",
    "name": "Good Food Institute",
    "core": false,
    "slug": "good-food-institute",
    "oldSlugs": null,
    "postCount": 15,
    "description": {
      "markdown": "The **Good Food Institute** (**GFI**) is a nonprofit based in Washington, D.C., that promotes both plant-based and [cell-based](https://forum.effectivealtruism.org/tag/cultured-meat) [animal product alternatives](https://forum.effectivealtruism.org/tag/animal-product-alternatives).\n\nHistory\n-------\n\nGFI was founded in 2016 as a sister organization of [Mercy for Animals](https://forum.effectivealtruism.org/tag/mercy-for-animals).^[\\[1\\]](#fnseiqvmznog)^ Bruce Friedrich has been the executive director since GFI's foundation.\n\nEvaluation\n----------\n\nGFI is recommended by [Founders Pledge](https://forum.effectivealtruism.org/tag/founders-pledge) as \"one of the most cost-effective animal advocacy organizations in the world.\"^[\\[2\\]](#fnuk8ipeazxi)^ GFI is also one of the two [animal welfare](https://forum.effectivealtruism.org/tag/animal-welfare-1) charities recommended by [Raising for Effective Giving](https://forum.effectivealtruism.org/tag/raising-for-effective-giving).^[\\[3\\]](#fn4b8yjyajq38)^ \n\nAs of July 2022, GFI has received $16.5 million in grants from [Open Philanthropy](https://forum.effectivealtruism.org/tag/open-philanthropy).^[\\[4\\]](#fnw3motwqzixh)^\n\nFurther reading\n---------------\n\nGood Food Institute (2022) [Year in review 2021](https://gfi.org/resource/year-in-review-2021/), *Good Food Institute*.\n\nHarris, Keiran & Robert Wiblin (2018) [Bruce Friedrich makes the case that inventing outstanding meat replacements is the most effective way to help animals](https://80000hours.org/podcast/episodes/bruce-friedrich-good-food-institute/), *80,000 Hours*, February 19.\n\nRighetti, Luca & Moorhouse, Fin (2021) [Bruce Friedrich on protein alternatives and the Good Food Institute](https://hearthisidea.com/episodes/bruce), *Hear This Idea*, January 4.\n\nExternal links\n--------------\n\n[Good Food Institute](https://www.gfi.org/). Official website.\n\n[Apply for a job](https://gfi.org/careers/).\n\n[Donate to the Good Food Institute](https://gfi.org/donate/).\n\n1.  ^**[^](#fnrefseiqvmznog)**^\n    \n    Bowie, Richard (2016) [MFA launches new sister organization](https://vegnews.com/2016/3/mfa-launches-new-sister-organization), *VegNews.Com*, March 4.\n    \n2.  ^**[^](#fnrefuk8ipeazxi)**^\n    \n    Clare, Stephen (2020) [The Good Food Institute](https://founderspledge.com/stories/the-good-food-institute-high-impact-funding-opportunity), *Founders Pledge*, November 3.\n    \n3.  ^**[^](#fnref4b8yjyajq38)**^\n    \n    Raising for Effective Giving (2021) [Animal welfare](https://reg-charity.org/recommended-charities/animal-welfare/), *Raising for Effective Giving*.\n    \n4.  ^**[^](#fnrefw3motwqzixh)**^\n    \n    Open Philanthropy (2022) [Grants database: The Good Food Institute](https://www.openphilanthropy.org/grants/?q=&organization-name=the-good-food-institute), *Open Philanthropy*."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "qpEppEmw8XbitnYj2",
    "name": "Global Challenges Foundation",
    "core": false,
    "slug": "global-challenges-foundation",
    "oldSlugs": null,
    "postCount": 1,
    "description": {
      "markdown": "The **Global Challenges Foundation** is a Swedish nonprofit that promotes [global governance](https://forum.effectivealtruism.org/tag/global-governance) reforms for reducing [global catastrophic risk](https://forum.effectivealtruism.org/tag/global-catastrophic-risk). It was founded in 2012 by businessman and author László Szombatfalvy.\n\nExternal links\n--------------\n\n[Global Challenges Foundation](https://globalchallenges.org/). Official website."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "44jJ7iqTWJYdEWpTo",
    "name": "Global Catastrophic Risk Institute",
    "core": false,
    "slug": "global-catastrophic-risk-institute",
    "oldSlugs": null,
    "postCount": 14,
    "description": {
      "markdown": "The **Global Catastrophic Risk Institute (GCRI)** is a think tank that analyzes risks to the survival of human civilization.\n\nFunding\n-------\n\nAs of June 2022, GCRI has received nearly $450,000 in funding from the [Survival and Flourishing](https://forum.effectivealtruism.org/topics/survival-and-flourishing) Fund.^[\\[1\\]](#fnmz5girxthoq)^^[\\[2\\]](#fncs6os19fhvb)^^[\\[3\\]](#fn03b3hboohsyh)^^[\\[4\\]](#fnexk1hx7naqw)^^[\\[5\\]](#fncofc5isovvc)^\n\nExternal links\n--------------\n\n[Global Catastrophic Risk Institute](https://gcrinstitute.org/). Official website.\n\n[Apply for a job](https://gcrinstitute.org/get-involved/#jobs).\n\n1.  ^**[^](#fnrefmz5girxthoq)**^\n    \n    Survival and Flourishing Fund (2018) [SFF-2019-Q4 S-process recommendations announcement](https://survivalandflourishing.fund/sff-2019-q4-recommendations), *Survival and Flourishing Fund*. \n    \n2.  ^**[^](#fnrefcs6os19fhvb)**^\n    \n    Survival and Flourishing Fund (2019) [SFF-2020-H1 S-process recommendations announcement](https://survivalandflourishing.fund/sff-2020-h1-recommendations), *Survival and Flourishing Fund*. \n    \n3.  ^**[^](#fnref03b3hboohsyh)**^\n    \n    Survival and Flourishing Fund (2019) [SFF-2020-H2 S-process recommendations announcement](https://survivalandflourishing.fund/sff-2020-h2-recommendations), *Survival and Flourishing Fund*. \n    \n4.  ^**[^](#fnrefexk1hx7naqw)**^\n    \n    Survival and Flourishing Fund (2020) [SFF-2021-H1 S-process recommendations announcement](https://survivalandflourishing.fund/sff-2021-h1-recommendations), *Survival and Flourishing Fund*. \n    \n5.  ^**[^](#fnrefcofc5isovvc)**^\n    \n    Survival and Flourishing Fund (2021) [SFF-2022-H1 S-Process recommendations announcement](https://survivalandflourishing.fund/sff-2022-h1-recommendations), *Survival and Flourishing Fund*."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "snoR2mXkTa84R5cCG",
    "name": "Global catastrophic biological risk",
    "core": false,
    "slug": "global-catastrophic-biological-risk",
    "oldSlugs": null,
    "postCount": 29,
    "description": {
      "markdown": "A **global catastrophic biological risk** (**GCBR**) is a [global catastrophic risk](https://forum.effectivealtruism.org/tag/global-catastrophic-risk) that is biological in nature.^[\\[1\\]](#fn6363p557wiv)^^[\\[2\\]](#fnkdpzkjlks7m)^\n\nExistential risks from biotechnology\n------------------------------------\n\n[Biotechnology](https://forum.effectivealtruism.org/tag/biotechnology) offers potentially exciting ways to make people much better off, to find new ways to tackle disease, and to address climate change. However, because many biological systems are at least in principle self-replicating, significant modifications of some kinds could also pose serious risks.\n\nA life-sciences area which currently receives attention from risk experts is gain-of-function (GOF) research, in which particularly concerning pathogens like H5N1 influenza are modified to increase their virulence or transmissibility. Researchers are divided as to whether the benefits of such experiments are worth the risks that come from either accidental release of pathogens or the misuse of discoveries by malicious actors.^[\\[3\\]](#fnc9uhv3315wt)^^[\\[4\\]](#fnx0gguef8uh)^ In the near term, such pathogens probably pose a non-existential global catastrophic risk, but there is some chance that related life-sciences work might one day pose an [existential risk](https://forum.effectivealtruism.org/tag/existential-risk).\n\nEvaluation\n----------\n\n[80,000 Hours](https://forum.effectivealtruism.org/tag/80-000-hours) rates reducing global catastrophic biological risks a \"highest priority area\": a problem at the top of their ranking of global issues assessed by [importance, tractability and neglectedness](https://forum.effectivealtruism.org/tag/itn-framework-1).^[\\[5\\]](#fnhsk6k08h9n6)^\n\nRecommendations\n---------------\n\nIn [*The Precipice: Existential Risk and the Future of Humanity*](https://forum.effectivealtruism.org/tag/the-precipice), [Toby Ord](https://forum.effectivealtruism.org/tag/toby-ord) offers several [policy](https://forum.effectivealtruism.org/tag/policy) and [research](https://forum.effectivealtruism.org/tag/research) recommendations for handling risks from engineered pandemics:^[\\[6\\]](#fn2uciqervllc)^\n\n*   Bring the Biological Weapons Convention into line with the Chemical Weapons Convention: taking its budget from $1.4 million up to $80 million, increasing its staff commensurately, and granting the power to investigate suspected breaches.\n*   Strengthen the WHO’s ability to respond to emerging pandemics through rapid disease surveillance, diagnosis and control. This involves increasing its funding and powers, as well as R&D on the requisite technologies.\n*   Ensure that all DNA synthesis is screened for dangerous pathogens. If full coverage can’t be achieved through self-regulation by synthesis companies, then some form of international regulation will be needed.\n*   Increase transparency around accidents in BSL-3 and BSL-4 laboratories.\n*   Develop standards for dealing with information hazards, and incorporate these into existing review processes.\n*   Run scenario-planning exercises for severe engineered pandemics.\n\nFurther reading\n---------------\n\nEsvelt, Kevin (2020) [Mitigating catastrophic biorisks](https://forum.effectivealtruism.org/posts/9iPdD5veF78kQmhiv/kevin-esvelt-mitigating-catastrophic-biorisks), *Effective Altruism Forum*, September 3.\n\nInglesby, Thomas V. & Amesh A. Adalja (eds.) (2019) [*Global Catastrophic Biological Risks*](http://doi.org/10.1007/978-3-030-36311-6), Cham: Springer International Publishing.\n\nLempel, Howie & Keiran Harris (2020) [Dr Greg Lewis on Covid-19 and reducing global catastrophic biological risks](https://80000hours.org/podcast/episodes/greg-lewis-covid-19-global-catastrophic-biological-risks/), *80,000 Hours*, April 17.\n\nLewis, Gregory (2020) [Reducing global catastrophic biological risks](https://80000hours.org/problem-profiles/global-catastrophic-biological-risks/), *80,000 Hours*, March 9.\n\nKilbourne, Edwin Dennis (2008) [Plagues and pandemics: past, present, and future](https://en.wikipedia.org/wiki/Special:BookSources/9780198570509), in Nick Bostrom & Milan M. Ćirković (eds.) *Global Catastrophic Risks*, Oxford: Oxford University Press, pp. 287–307.\n\nMillett, Piers & Andrew Snyder-Beattie (2017) [Existential risk and cost-effective biosecurity](http://doi.org/10.1089/hs.2017.0028), *Health Security*, vol. 15, pp. 373–383.\n\nNouri, Ali & Christopher F. Chyba (2008) [Biotechnology and biosecurity](https://en.wikipedia.org/wiki/Special:BookSources/9780198570509), in Nick Bostrom & Milan M. Ćirković (eds.) *Global Catastrophic Risks*, Oxford: Oxford University Press, pp. 450–480.\n\nShulman, Carl (2020) [What do historical statistics teach us about the accidental release of pandemic bioweapons?](http://reflectivedisequilibrium.blogspot.com/2020/10/what-do-historical-statistics-teach-us.html), *Reflective Disequilibrium*, October 15.\n\nShulman, Carl (2020) [Envisioning a world immune to global catastrophic biological risks](http://reflectivedisequilibrium.blogspot.com/2020/05/what-would-civilization-immune-to.html), *Reflective Disequilibrium*, October 15.\n\nWiblin, Robert & Keiran Harris (2018) [The careers and policies that can prevent global catastrophic biological risks, according to world-leading health security expert Dr Inglesby](https://80000hours.org/podcast/episodes/tom-inglesby-health-security/), *80,000 Hours*, April 18.\n\nYassif, Jaime (2017) [Reducing global catastrophic biological risks](http://doi.org/10.1089/hs.2017.0049), *Health Security*, vol. 15, pp. 329–330.\n\nRelated entries\n---------------\n\n[biosecurity](https://forum.effectivealtruism.org/tag/biosecurity) | [biosurveillance](https://forum.effectivealtruism.org/tag/biosurveillance) | [existential risk](https://forum.effectivealtruism.org/tag/existential-risk) | [global catastrophic risk](https://forum.effectivealtruism.org/tag/global-catastrophic-risk) | [Johns Hopkins Center for Health Security](/tag/johns-hopkins-center-for-health-security) | [Nucleic Acid Observatory](https://forum.effectivealtruism.org/topics/nucleic-acid-observatory) | [Nuclear Threat Initiative](https://forum.effectivealtruism.org/tag/nuclear-threat-initiative)\n\n1.  ^**[^](#fnref6363p557wiv)**^\n    \n    Lewis, Gregory (2020) [Reducing global catastrophic biological risks](https://80000hours.org/problem-profiles/global-catastrophic-biological-risks/), *80,000 Hours*, March 9.\n    \n2.  ^**[^](#fnrefkdpzkjlks7m)**^\n    \n    Schoch-Spana, Monica *et al.* (2017) [Global catastrophic biological risks: toward a working definition](http://doi.org/10.1089/hs.2017.0038), *Health Security*, vol. 15, pp. 323–328.\n    \n3.  ^**[^](#fnrefc9uhv3315wt)**^\n    \n    Duprex, W. Paul *et al.* (2015) [Gain-of-function experiments: Time for a real debate](http://doi.org/10.1038/nrmicro3405), *Nature Reviews Microbiology*, vol. 13, pp. 58–64.\n    \n4.  ^**[^](#fnrefx0gguef8uh)**^\n    \n    Selgelid, Michael J. (2016) [Gain-of-Function research: Ethical analysis](http://doi.org/10.1007/s11948-016-9810-1), *Science and Engineering Ethics*, vol. 22, pp. 923–964.\n    \n5.  ^**[^](#fnrefhsk6k08h9n6)**^\n    \n    80,000 Hours (2021) [Our current list of the most important world problems](https://80000hours.org/problem-profiles/), *80,000 Hours*.\n    \n6.  ^**[^](#fnref2uciqervllc)**^\n    \n    Ord, Toby (2020) [*The Precipice: Existential Risk and the Future of Humanity*](https://en.wikipedia.org/wiki/Special:BookSources/1-5266-0021-8), London: Bloomsbury Publishing, p. 279–280."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "RRcLA4Kn8pasimQZr",
    "name": "GiveWell",
    "core": false,
    "slug": "givewell",
    "oldSlugs": null,
    "postCount": 114,
    "description": {
      "markdown": "**GiveWell** is a nonprofit [charity evaluator](https://forum.effectivealtruism.org/tag/charity-evaluation) based in San Francisco. They conduct and publish research into the most [cost-effective](https://forum.effectivealtruism.org/tag/cost-effectiveness) [giving opportunities](https://forum.effectivealtruism.org/tag/effective-giving-1) in [global health and development](https://forum.effectivealtruism.org/tag/global-health-and-development).^[\\[1\\]](#fn72r7tzztq54)^\n\nHistory\n-------\n\nGiveWell was started by [Holden Karnofsky](https://forum.effectivealtruism.org/tag/holden-karnofsky) and Elie Hassenfeld in 2007.^[\\[2\\]](#fnxezkhd1z87h)^ GiveWell also helped incubate [Open Philanthropy](https://forum.effectivealtruism.org/tag/open-philanthropy), which was spun off as a separate entity in 2017.^[\\[2\\]](#fnxezkhd1z87h)^\n\nTop charities\n-------------\n\nGiveWell publishes a list of \"top charities\" based on [cost-effectiveness](https://forum.effectivealtruism.org/tag/cost-effectiveness) and quality of evidence.^[\\[3\\]](#fnfmltfwvoez)^ The list is updated annually. As of August 2022, the top GiveWell charities are:\n\n1.  [Malaria Consortium](https://forum.effectivealtruism.org/tag/malaria-consortium)'s seasonal [malaria](https://forum.effectivealtruism.org/tag/malaria) chemoprevention program;\n2.  [Against Malaria Foundation](https://forum.effectivealtruism.org/tag/against-malaria-foundation), which provides [long-lasting insecticide-treated nets](https://forum.effectivealtruism.org/tag/mass-distribution-of-long-lasting-insecticide-treated-nets)\n3.  [Helen Keller International](https://forum.effectivealtruism.org/tag/helen-keller-international)'s vitamin A [supplementation program](https://forum.effectivealtruism.org/tag/micronutrient-fortification); and\n4.  [New Incentives](https://forum.effectivealtruism.org/tag/new-incentives), which provides conditional [cash transfers](https://forum.effectivealtruism.org/tag/cash-transfers) for routine childhood vaccinations.\n\nStandout charities\n------------------\n\nGiveWell used to also recognize a number of \"standout charities\" which, despite not meeting all of the criteria to be a top charity, were rated above nearly every other organization considered for evaluation.\n\nThe \"standout charity\" designation was discontinued in October 2021. GiveWell found that it caused confusion among some donors and was inconsistent with the goal of directing funds to the most cost-effective organizations.^[\\[4\\]](#fndge4ijy3rx)^\n\nImpact\n------\n\nGiveWell estimates that it has directed over $1.1 billion to its recommended charities since 2012.^[\\[5\\]](#fn8yt4qd1oa95)^ According to their latest report, the money directed to them in 2021 was nearly $330 million, a 50% increase from the roughly $220 million directed over the previous year.^[\\[6\\]](#fnxy0hhz5u5q)^^[\\[7\\]](#fn11khuq08v7)^\n\nIn November 2021, Open Philanthropy announced a substantial increase in the funds it plans to allocate to GiveWell's recommended charities: $300 million for 2021, with tentative plans to give an additional $500 million per year over the following two years. The decision was based on the perceived growth in GiveWell's ability to identify cost-effective opportunities, the significant growth in the assets Open Philanthropy expects itself and other related organizations to eventually distribute, and an increase in how much Open Philanthropy values saving lives relative to boosting income.^[\\[8\\]](#fnvipeazezdzb)^\n\nFurther reading\n---------------\n\nGiveWell (2015) 'GiveWell', in Ryan Carey (ed.) [*The Effective Altruism Handbook*](http://www.stafforini.com/docs/Carey%20-%20The%20effective%20altruism%20handbook.pdf), 1st ed., Oxford: The Centre for Effective Altruism, pp. 109-113\n\nGiveWell (2021) [GiveWell’s year-end community event 2021](https://vimeo.com/657122643), December 9.  \n*A conversation with journalist Matthew Yglesias and GiveWell CEO Elie Hassenfeld on GiveWell’s origins, evolving role, and latest research.*\n\nGreenberg, Spencer (2022) [Why it’s so hard to have confidence that charities are doing good (with Elie Hassenfeld)](https://clearerthinkingpodcast.com/episode/096), *Clearer Thinking*, March 17.  \n*An interview with GiveWell's CEO.*\n\nPerry, Suzanne (2007) [A quest for the best](https://www.philanthropy.com/article/A-Quest-for-the-Best/165685), *The Chronicle of Philanthropy*, vol. 20.\n\nStrom, Stephanie (2007) [2 young hedge-fund veterans stir up the world of philanthropy](https://www.nytimes.com/2007/12/20/us/20charity.html), *The New York Times*, December 20.\n\nExternal links\n--------------\n\n[GiveWell](https://www.givewell.org/). Official website.\n\n[GiveWell](https://forum.effectivealtruism.org/users/givewell). [Effective Altruism Forum](https://forum.effectivealtruism.org/tag/effective-altruism-forum-1) account.\n\n[Apply for a job](https://www.givewell.org/about/jobs).\n\n[Donate to GiveWell](https://secure.givewell.org/).\n\nRelated links\n-------------\n\n[charity evaluation](https://forum.effectivealtruism.org/tag/charity-evaluation) | [Good Ventures](https://forum.effectivealtruism.org/tag/good-ventures) | [Open Philanthropy](https://forum.effectivealtruism.org/tag/open-philanthropy)\n\n1.  ^**[^](#fnref72r7tzztq54)**^\n    \n    GiveWell (2017) [GiveWell: A medium-depth overview](https://www.givewell.org/about/givewell-overview), *GiveWell*, June.\n    \n2.  ^**[^](#fnrefxezkhd1z87h)**^\n    \n    GiveWell (2018) [Our story](https://www.givewell.org/about/story), *GiveWell*, February.\n    \n3.  ^**[^](#fnreffmltfwvoez)**^\n    \n    GiveWell (2021) [Our top charities](https://www.givewell.org/charities/top-charities), *GiveWell*, November.\n    \n4.  ^**[^](#fnrefdge4ijy3rx)**^\n    \n    Hassenfeld, Elie (2021) [We’re discontinuing the standout charity designation](https://blog.givewell.org/2021/10/05/discontinuing-standout-charity-designation/), *The GiveWell Blog*, October 5.\n    \n5.  ^**[^](#fnref8yt4qd1oa95)**^\n    \n    GiveWell (2022) [GiveWell funds directed summary](https://docs.google.com/spreadsheets/d/1ztnYGpuS3vB7TRGUL1BqoeKY0bNNAVR2TGEr_zJCJh4/edit#gid=0), in 'Public GiveWell metrics for the 2021 metrics report', *GiveWell*, August. \n    \n6.  ^**[^](#fnrefxy0hhz5u5q)**^\n    \n    GiveWell (2022) [GiveWell’s impact](https://www.givewell.org/about/impact), *GiveWell*, August.\n    \n7.  ^**[^](#fnref11khuq08v7)**^\n    \n    Dey, Robin (2022) [GiveWell’s 2021 metrics report](https://blog.givewell.org/2022/08/05/givewells-2021-metrics-report/), *The GiveWell Blog*, August 5.\n    \n8.  ^**[^](#fnrefvipeazezdzb)**^\n    \n    Berger, Alexander (2021) [2021 allocation to GiveWell top charities: why we’re giving more going forward](https://www.openphilanthropy.org/blog/2021-allocation-givewell-top-charities-why-we-re-giving-more-going-forward), *Open Philanthropy*, November 22."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "YALdt6qMPbqmamjJY",
    "name": "Doing Good Better",
    "core": false,
    "slug": "doing-good-better",
    "oldSlugs": [
      "doing-good-better",
      "doing-good-better",
      "doing-good-better-book"
    ],
    "postCount": 6,
    "description": {
      "markdown": "***Doing Good Better: How Effective Altruism Can Help You Make a Difference*** is a book by [William MacAskill](https://forum.effectivealtruism.org/tag/william-macaskill). It was published on July 28, 2015.\n\nFurther reading\n---------------\n\nMacAskill, William (2015) [*Doing Good Better: How Effective Altruism Can Help You Make a Difference*](https://en.wikipedia.org/wiki/Special:BookSources/9781592409662), New York: Random House.\n\nRelated entries\n---------------\n\n[*The Life You Can Save*](https://forum.effectivealtruism.org/tag/the-life-you-can-save-book) | [*The Precipice*](https://forum.effectivealtruism.org/tag/the-precipice)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "gACMrqLQexrQvbxsw",
    "name": "William MacAskill",
    "core": false,
    "slug": "william-macaskill",
    "oldSlugs": null,
    "postCount": 35,
    "description": {
      "markdown": "**William David MacAskill** (born 24 March 1987 as **William David Crouch**) is a Scottish philosopher. He is an Associate Professor of Philosophy at the University of Oxford, a Senior Research Fellow at the [Global Priorities Institute](https://forum.effectivealtruism.org/tag/global-priorities-institute) and the Director and Founder of the [Forethought Foundation for Global Priorities Research](https://forum.effectivealtruism.org/tag/forethought-foundation). MacAskill is also the author of [*Doing Good Better*](https://forum.effectivealtruism.org/tag/doing-good-better) and [*What We Owe the Future*](https://forum.effectivealtruism.org/tag/what-we-owe-the-future), and the co-founder of [Giving What We Can](https://forum.effectivealtruism.org/tag/giving-what-we-can), [80,000 Hours](https://forum.effectivealtruism.org/tag/80-000-hours) and the [Centre for Effective Altruism](https://forum.effectivealtruism.org/tag/centre-for-effective-altruism-1).\n\nFurther reading\n---------------\n\nGalef, Julia (2017) [Moral uncertainty (Will MacAskill)](http://rationallyspeakingpodcast.org/181-moral-uncertainty-will-macaskill/), *Rationally Speaking*, April 2.\n\nMacAskill, William & Darius Meissner (2020) [*Introduction to Utilitarianism: An Online Textbook*](https://www.utilitarianism.net/), Oxford.\n\nMacAskill, William, Krister Bykvist & Toby Ord (2020) [*Moral Uncertainty*](https://www.moraluncertainty.com/), Oxford: Oxford University Press.\n\nPerry, Lucas (2018) [AI alignment podcast: moral uncertainty and the path to AI alignment with William MacAskill](https://futureoflife.org/2018/09/17/moral-uncertainty-and-the-path-to-ai-alignment-with-william-macaskill/), *AI Alignment podcast*, September 17.  \n*Interview with William MacAskill about moral uncertainty and other topics.*\n\nWiblin, Robert & Keiran Harris (2018) [Our descendants will probably see us as moral monsters. what should we do about that?](https://80000hours.org/podcast/episodes/will-macaskill-moral-philosophy/), *80,000 Hours*, January 19.  \n*Interview with William MacAskill about the long reflection and other topics.*\n\nExternal links\n--------------\n\n[William MacAskill](https://www.williammacaskill.com/). Official website.\n\n[William MacAskill](https://forum.effectivealtruism.org/users/william_macaskill). [Effective Altruism Forum](https://forum.effectivealtruism.org/tag/effective-altruism-forum-1) account."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "EJ5wPvSxpLcXvYPZZ",
    "name": "Foresight Institute",
    "core": false,
    "slug": "foresight-institute",
    "oldSlugs": null,
    "postCount": 1,
    "description": {
      "markdown": "The **Foresight Institute** is an American nonprofit that promotes the beneficial development of [transformative technology](https://forum.effectivealtruism.org/tag/transformative-development). It was founded in 1986 by [Eric Drexler](https://forum.effectivealtruism.org/tag/eric-drexler) and Christine Peterson.^[\\[1\\]](#fntojvnocprwc)^^[\\[2\\]](#fnelmavdks5sq)^\n\nFurther reading\n---------------\n\nForrest, David & Christine Peterson (2010) [The Foresight Institute](http://web.archive.org/web/20150316114303/foresight.org/about/2010-04-07_Foresight_presentation_ANSI_rev09.pdf), *ANSI TAG Meeting to ISO TC/229 on Nanotechnologies*, April 7.\n\nHayes, Sean A. (2010) [Foresight Institute](https://doi.org/10.4135/9781412972093.n96), in David Guston (ed.) *Encyclopedia of Nanoscience and Society*, Thousand Oaks, California: SAGE Publications, pp. 253–254.\n\nExternal links\n--------------\n\n[Foresight Institute](https://foresight.org/). Official website.\n\nRelated entries\n---------------\n\n[aging research](https://forum.effectivealtruism.org/tag/aging-research) | [atomically precise manufacturing](https://forum.effectivealtruism.org/tag/atomically-precise-manufacturing) | [brain-computer interfaces](https://forum.effectivealtruism.org/tag/brain-computer-interfaces) | [Eric Drexler](https://forum.effectivealtruism.org/tag/eric-drexler) | [space colonization](https://forum.effectivealtruism.org/tag/space-colonization) | [transhumanism](https://forum.effectivealtruism.org/tag/transhumanism)\n\n1.  ^**[^](#fnreftojvnocprwc)**^\n    \n    Forrest, David & Christine Peterson (2010) [The Foresight Institute](http://web.archive.org/web/20150316114303/foresight.org/about/2010-04-07_Foresight_presentation_ANSI_rev09.pdf), *ANSI TAG Meeting to ISO TC/229 on Nanotechnologies*, April 7.\n    \n2.  ^**[^](#fnrefelmavdks5sq)**^\n    \n    Hayes, Sean A. (2010) [Foresight Institute](https://doi.org/10.4135/9781412972093.n96), in David Guston (ed.) *Encyclopedia of Nanoscience and Society*, Thousand Oaks, California: SAGE Publications, pp. 253–254."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "YJ3CNWY5Lv4aK5CW4",
    "name": "Foreign aid",
    "core": false,
    "slug": "foreign-aid",
    "oldSlugs": null,
    "postCount": 19,
    "description": {
      "markdown": "**Foreign aid** is the transfer of capital, goods, or services from a country or [international organization](https://forum.effectivealtruism.org/tag/international-organization) to the government or population of another country.\n\nForeign aid advocacy as an intervention\n---------------------------------------\n\nIn absolute terms, the United States is the largest donor among members of the Development Assistance Committee (DAC), spending around $35 billion annually on foreign aid. However, the US gives a smaller proportion of its GDP than most other DAC countries. If the US gave as large a fraction of its GDP as the United Kingdom does, its foreign aid spending would be close to $110 billion. Advocating for increased U.S. foreign aid spending is thus sometimes considered a potentially high-impact intervention. Besides pushing for increased spending, advocates can push for more [cost-effective](https://forum.effectivealtruism.org/tag/cost-effectiveness) spending.^[\\[1\\]](#fn4w281u1o4pl)^\n\nFurther reading\n---------------\n\nKenny, Charles (2021) [Allocating global aid to maximize utility](https://www.cgdev.org/blog/allocating-global-aid-maximize-utility), *Center for Global Development*, January 11.\n\nOpen Philanthropy (2015) [Advocacy for improved or increased U.S. foreign aid](https://www.openphilanthropy.org/research/cause-reports/advocacy-improved-or-increased-us-foreign-aid), *Open Philanthropy*, April 20.\n\nRelated entries\n---------------\n\n[foreign aid skepticism](https://forum.effectivealtruism.org/tag/foreign-aid-skepticism) | [global poverty](https://forum.effectivealtruism.org/tag/global-poverty)\n\n1.  ^**[^](#fnref4w281u1o4pl)**^\n    \n    Open Philanthropy (2015) [Advocacy for improved or increased U.S. foreign aid](https://www.openphilanthropy.org/research/cause-reports/advocacy-improved-or-increased-us-foreign-aid), *Open Philanthropy*, April 20."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "oWSovHP8qdB8oYu8s",
    "name": "Fermi paradox",
    "core": false,
    "slug": "fermi-paradox",
    "oldSlugs": null,
    "postCount": 16,
    "description": {
      "markdown": "The **Fermi paradox** is the alleged inconsistency between the lack of evidence of [extraterrestrial intelligence](https://forum.effectivealtruism.org/tag/extraterrestrial-intelligence) and the various high estimates for its probability. The solution to the paradox has implications for [the long-term future](https://forum.effectivealtruism.org/tag/long-term-future), in particular the feasibility of [interstellar colonization](https://forum.effectivealtruism.org/tag/space-colonization) and the likelihood of [extinction](https://forum.effectivealtruism.org/tag/human-extinction).\n\nThe expected number of interstellar civilizations observable from Earth is the product of the number of planets in the galaxy and the following three probabilities:\n\n*   The probability that any given planet produces intelligent life\n*   The probability that any given intelligent species develops an interstellar civilization\n*   The probability that any given interstellar civilization would be detectable from Earth at this time\n\nSince the number of planets in the galaxy is in the hundreds of billions, even very small values of the three probabilities would imply that we should see many interstellar civilizations. However, we observe none. This means that at least one of the probabilities must be *extremely* low.\n\nThe second probability is most concerning, since it suggests something about the future of humanity. If it is the extremely low probability, then this means either that interstellar colonization is infeasible or that humanity is extremely likely to go prematurely extinct before engaging in it.\n\nGenerally, the more highly we rate the first and third probabilities, the lower the second probability would have to be, which should decrease our [credence](https://forum.effectivealtruism.org/tag/credence) in the possibility of humanity ever leaving the solar system.\n\nFurther reading\n---------------\n\nBoeree, Liv (2018) [Why haven’t we found aliens yet?](https://www.vox.com/science-and-health/2018/7/3/17522810/aliens-fermi-paradox-drake-equation), *Vox*, July 3.\n\nBostrom, Nick (2008) [Where are they?](https://www.technologyreview.com/2008/04/22/220999/where-are-they/), *MIT technology review*, April 22, pp. 72–77.  \n*An exploration of the Fermi Paradox as it relates to the risk of human extinction*\n\nĆirković, Milan M. (2018) [*The Great Silence: Science and Philosophy of Fermi’s Paradox*](https://en.wikipedia.org/wiki/Special:BookSources/9780199646302), Oxford: Oxford University Press.\n\nMiller, James (2020) [The Fermi paradox and x-risk](https://www.youtube.com/watch?v=jisNYZpmnmU), *AstralCodexTen Online Meetup*, August 17.\n\nSandberg, Anders, Eric Drexler & Toby Ord (2018) [Dissolving the Fermi Paradox](http://arxiv.org/abs/1806.02404), arXiv:1806.02404.\n\nWiblin, Robert & Keiran Harris (2018) [Where are the aliens? Three new resolutions to the Fermi Paradox. And how we could easily colonise the whole universe](https://80000hours.org/podcast/episodes/anders-sandberg-fermi-paradox/), *80,000 Hours*, May 8.  \n*An interview with Anders Sandberg.*\n\nRelated entries\n---------------\n\n[extraterrestrial intelligence](https://forum.effectivealtruism.org/tag/extraterrestrial-intelligence) | [grabby aliens](https://forum.effectivealtruism.org/topics/grabby-aliens) | [Great Filter](https://forum.effectivealtruism.org/tag/great-filter) | [non-humans and the long-term future](https://forum.effectivealtruism.org/tag/non-humans-and-the-long-term-future) | [space colonization](https://forum.effectivealtruism.org/tag/space-colonization) | [space governance](https://forum.effectivealtruism.org/tag/space-governance)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "8ciLeMvZBj6A6hThv",
    "name": "Utilitarianism",
    "core": false,
    "slug": "utilitarianism",
    "oldSlugs": null,
    "postCount": 68,
    "description": {
      "markdown": "**Utilitarianism** is the family of moral theories according to which the rightness of actions is determined solely by the sum total of [wellbeing](https://forum.effectivealtruism.org/tag/wellbeing) they produce.\n\nElements of utilitarianism\n--------------------------\n\nAll ethical theories belonging to the utilitarian family share four defining elements:\n\n1.  **Consequentialism** is the view that the moral rightness of actions (or rules, policies, etc.) depends on, and only on, the value of their *consequences*.\n2.  **Welfarism** is the view that only the *welfare* (also called *wellbeing*) of individuals determines how good a particular state of the world is.\n3.  **Impartiality** is the view that the identity of individuals is irrelevant to the value of an outcome.\n4.  **Additive Aggregationism** is the view that the value of the world is given by the sum of the values of its parts, where these parts are some kind of local phenomena such as experiences, lives, or societies.\n\nClassical utilitarianism\n------------------------\n\nThe original and most influential version of utilitarianism is *classical utilitarianism*. Classical utilitarianism accepts [hedonism](https://forum.effectivealtruism.org/tag/hedonism) as a theory of welfare, the view that wellbeing consists of positive and negative conscious experiences. Moreover, classical utilitarianism accepts the [total view](https://forum.effectivealtruism.org/topics/total-view) of population ethics, according to which one outcome is better than another if and only if it contains a greater sum total of wellbeing, where wellbeing can be increased either by making existing people better off or by creating new people with good lives.\n\nActing on utilitarianism\n------------------------\n\nUtilitarianism has important implications for how we should think about leading an ethical life. Because utilitarianism weighs the wellbeing of everyone equally, it implies that we should make helping others a very significant part of our lives. In helping others, we should try to use our resources to do the most good, impartially considered, that we can. Since not all ways of helping others are equally effective, utilitarianism implies that we should carefully choose which problems to work on and by what means.\n\nTo do the most good they can, in practice, many utilitarians donate a significant portion of their income to address the world’s most pressing problems, devote their careers to doing good, and aspire to high degrees of cooperativeness, personal integrity and honesty.\n\nInfluential utilitarians\n------------------------\n\n*   Predecessor: Mozi\n*   [Jeremy Bentham](https://forum.effectivealtruism.org/tag/jeremy-bentham)\n*   [John Stuart Mill](https://forum.effectivealtruism.org/tag/john-stuart-mill)\n*   Harriet Taylor Mill\n*   Henry Sidgwick\n*   [Peter Singer](https://forum.effectivealtruism.org/tag/peter-singer)\n\nFurther reading\n---------------\n\nGreenberg, Spencer (2021) [Episode 042: Utilitarianism and its flavors with Nick Beckstead](https://clearerthinkingpodcast.com/?ep=042), *Clearer Thinking*, May 15.\n\nLazari-Radek, Katarzyna de & Peter Singer (2017) [*Utilitarianism: A Very Short Introduction*](http://doi.org/10.1093/actrade/9780198728795.001.0001), Oxford: Oxford University Press.\n\nNathanson, Stephen (2014) [Act and rule utilitarianism](https://iep.utm.edu/util-a-r/), *Internet Encyclopedia of Philosophy*, September.\n\nSebo, Jeff (2021) [Utilitarianism and nonhuman animals](https://www.utilitarianism.net/guest-essays/utilitarianism-and-nonhuman-animals), *Utilitarianism*.\n\nSinnott-Armstrong, Walter (2003) [Consequentialism](https://plato.stanford.edu/entries/consequentialism/), *Stanford Encyclopedia of Philosophy*, May 20 (updated 3 June 2019).\n\nExternal links\n--------------\n\n[Utilitarianism.net](https://www.utilitarianism.net/). An online textbook written by [William MacAskill](https://forum.effectivealtruism.org/tag/william-macaskill), Darius Meißner and Richard Yetter Chappell.\n\n[Utilitarianism.com](https://www.utilitarianism.com/). A repository of resources by [David Pearce](https://forum.effectivealtruism.org/topics/david-pearce-1).\n\n[Utilitarian Podcast](https://www.utilitarianpodcast.com/). A podcast on by Gus Docker.\n\nRelated entries\n---------------\n\n[axiology](https://forum.effectivealtruism.org/tag/axiology) | [consequentialism](https://forum.effectivealtruism.org/tag/consequentialism) | [demandingness of morality](https://forum.effectivealtruism.org/tag/demandingness-of-morality) | [equal consideration of interests](https://forum.effectivealtruism.org/topics/equal-consideration-of-interests) | [hedonism](https://forum.effectivealtruism.org/tag/hedonism) | [normative ethics](https://forum.effectivealtruism.org/tag/normative-ethics) | [total view](https://forum.effectivealtruism.org/topics/total-view) | [welfarism](https://forum.effectivealtruism.org/tag/welfarism)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "FBTCnrdQp9aDa6AJ3",
    "name": "Timing of philanthropy",
    "core": false,
    "slug": "timing-of-philanthropy",
    "oldSlugs": [
      "timing-of-philanthropy",
      "timing-of-philanthropy"
    ],
    "postCount": 55,
    "description": {
      "markdown": "The **timing of philanthropy** is the selection of the optimal schedule for philanthropic donations.\n\n## Related entries\n [investing](https://forum.effectivealtruism.org/tag/investing) | [patient altruism](https://forum.effectivealtruism.org/tag/patient-altruism)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "NFg5boJftCyZHQu2W",
    "name": "The Precipice",
    "core": false,
    "slug": "the-precipice",
    "oldSlugs": [
      "the-precipice"
    ],
    "postCount": 30,
    "description": {
      "markdown": "***The Precipice: Existential Risk and the Future of Humanity*** is a book by [Toby Ord](https://forum.effectivealtruism.org/tag/toby-ord). It was published on 5 March 2020.\n\nThe book is an extended argument for the conclusion that reducing [existential risk](https://forum.effectivealtruism.org/tag/existential-risk) is the fundamental challenge of our time. The book's title refers to a period of heightened risk which humanity must navigate safely to realize its long-term potential. Ord dates the beginning of this period to the [Trinity test](https://forum.effectivealtruism.org/tag/trinity), when the first atomic bomb was detonated.\n\nConnection to longtermism\n-------------------------\n\n*The Precipice* characterizes [longtermism](https://forum.effectivealtruism.org/tag/longtermism) as an ethic \"especially concerned with the impacts of our actions upon the longterm future\", and on which \"our most important role may be how we shape—or fail to shape—that story.\"^[\\[1\\]](#fn93gwmtvjutg)^ This characterization may suggest that the book's central thesis restates the longtermist thesis. However, while the two are related, they are different. First, according to Ord the case for reducing existential risk does not presuppose longtermism, and can be made even if that view is rejected. As he writes, \"\\[o\\]ne doesn’t have to approach existential risk from \\[a longtermist\\] direction\" since \"there is already a strong moral case just from the immediate effects.\"^[\\[1\\]](#fn93gwmtvjutg)^ Second, while reducing existential risk is an obvious path to influencing the long-term future, there may be other ways of exerting such a lasting influence.\n\nFurther reading\n---------------\n\nAird, Michael (2020) [List of things I’ve written or may write that are relevant to *The Precipice*](https://forum.effectivealtruism.org/posts/EMKf4Gyee7BsY2RP8/michaela-s-shortform), *Effective Altruism Forum*, April 6.\n\nAlexander, Scott (2020) [Book Review: The Precipice](https://slatestarcodex.com/2020/04/01/book-review-the-precipice/), *Slate Star Codex*, April 2.\n\nOrd, Toby (2020) [*The Precipice: Existential Risk and the Future of Humanity*](https://en.wikipedia.org/wiki/Special:BookSources/1526600218), London: Bloomsbury Publishing.\n\nWiblin, Robert, Arden Koehler & Keiran Harris (2020) [Toby Ord on *The Precipice* and humanity’s potential futures](https://80000hours.org/podcast/episodes/toby-ord-the-precipice-existential-risk-future-humanity/), *80,000 Hours*, March 7.\n\nExternal links\n--------------\n\n[*The Precipice*](https://theprecipice.com/). Official website.\n\nRelated entries\n---------------\n\n[existential risk](https://forum.effectivealtruism.org/tag/existential-risk) | [existential security](https://forum.effectivealtruism.org/tag/existential-security) | [long-term future](https://forum.effectivealtruism.org/tag/long-term-future) | [Toby Ord](https://forum.effectivealtruism.org/tag/toby-ord) | [Trinity](https://forum.effectivealtruism.org/tag/trinity) |  [*What We Owe the Future*](https://forum.effectivealtruism.org/tag/what-we-owe-the-future)\n\n1.  ^**[^](#fnref93gwmtvjutg)**^\n    \n    Ord, Toby (2020) [*The Precipice: Existential Risk and the Future of Humanity*](https://en.wikipedia.org/wiki/Special:BookSources/1526600218), London: Bloomsbury Publishing, p. 46."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "KSACmPWom8xmSB2bL",
    "name": "Toby Ord",
    "core": false,
    "slug": "toby-ord",
    "oldSlugs": null,
    "postCount": 14,
    "description": {
      "markdown": "**Toby David Godfrey Ord** (born 18 July 1979) is an Australian philosopher. He is a Senior Research Fellow at the [Future of Humanity Institute](https://forum.effectivealtruism.org/tag/future-of-humanity-institute). He is also the founder of [Giving What We Can](https://forum.effectivealtruism.org/tag/giving-what-we-can), the co-founder of the [Centre for Effective Altruism](https://forum.effectivealtruism.org/tag/centre-for-effective-altruism-1), and the author of [*The Precipice: Existential Risk and the Future of Humanity*](https://forum.effectivealtruism.org/tag/the-precipice).\n\nFurther reading\n---------------\n\nGalef, Julia (2021) [Humanity on the precipice (Toby Ord)](http://rationallyspeakingpodcast.org/262-humanity-on-the-precipice-toby-ord/), *Rationally Speaking*, December 10.\n\nMatthews, Dylan (2020) [This man has donated at least 10% of his salary to charity for 11 years running](https://www.vox.com/future-perfect/21728925/charity-10-percent-tithe-giving-what-we-can-toby-ord), *Vox*, November 30.\n\nWiblin, Robert, Arden Koehler & Keiran Harris (2020) [Toby Ord on The Precipice and humanity’s potential futures](https://80000hours.org/podcast/episodes/toby-ord-the-precipice-existential-risk-future-humanity/), *80,000 Hours*, March 7.\n\nExternal links\n--------------\n\n[Toby Ord](http://tobyord.com/). Official website.\n\n[Toby Ord](https://forum.effectivealtruism.org/users/toby_ord). [Effective Altruism Forum](https://forum.effectivealtruism.org/tag/effective-altruism-forum-1) account.\n\nRelated entries\n---------------\n\n[Nick Bostrom](https://forum.effectivealtruism.org/tag/nick-bostrom) | [existential risk](https://forum.effectivealtruism.org/tag/existential-risk) | [Giving What We Can](https://forum.effectivealtruism.org/tag/giving-what-we-can) | [global poverty](https://forum.effectivealtruism.org/tag/global-poverty) | [Derek Parfit](https://forum.effectivealtruism.org/tag/derek-parfit) | [*The Precipice*](https://forum.effectivealtruism.org/tag/the-precipice)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "p7oNW6ZZ8MiSwCPxa",
    "name": "New Harvest",
    "core": false,
    "slug": "new-harvest",
    "oldSlugs": null,
    "postCount": 5,
    "description": {
      "markdown": "**New Harvest** is a research institute focusing on [cultured meat](https://forum.effectivealtruism.org/tag/cultured-meat) and other [animal product alternatives](https://forum.effectivealtruism.org/tag/animal-product-alternatives). It was founded in 2004 by Jason Matheny.\n\nEvaluation\n----------\n\nNew Harvest was rated a Standout Charity by [Animal Charity Evaluators](https://forum.effectivealtruism.org/tag/animal-charity-evaluators) from December 2015 to November 2017, and has again been a Standout Charity since November 2021.^[\\[1\\]](#fnqwf4l070k1)^\n\nFurther reading\n---------------\n\nAnimal Charity Evaluators (2021) [New Harvest review](https://animalcharityevaluators.org/charity-review/new-harvest/), *Animal Charity Evaluators*, November.\n\nExternal links\n--------------\n\n[New Harvest](https://new-harvest.org/). Official website.\n\n[Donate to New Harvest](https://new-harvest.org/donate).\n\nRelated entries\n---------------\n\n[animal product alternatives](https://forum.effectivealtruism.org/tag/animal-product-alternatives) | [cultured meat](https://forum.effectivealtruism.org/tag/cultured-meat)\n\n1.  ^**[^](#fnrefqwf4l070k1)**^\n    \n    Animal Charity Evaluators (2021) [New Harvest review](https://animalcharityevaluators.org/charity-review/new-harvest/), *Animal Charity Evaluators*, November."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "t2vpvfTBvbY3YQsZv",
    "name": "Felicifia",
    "core": false,
    "slug": "felicifia",
    "oldSlugs": null,
    "postCount": 1,
    "description": {
      "markdown": "**Felicifia** was a [utilitarianism](https://forum.effectivealtruism.org/tag/utilitarianism) discussion forum active between late 2006 and the mid-2010s.\n\nThe forum hosted discussions, by contributors such as [Toby Ord](https://forum.effectivealtruism.org/tag/toby-ord), [Carl Shulman](https://forum.effectivealtruism.org/tag/carl-shulman), and [Brian Tomasik](https://forum.effectivealtruism.org/tag/brian-tomasik), about many ideas that would later become a core part of [effective altruism](https://forum.effectivealtruism.org/tag/effective-altruism), including [earning to give](https://forum.effectivealtruism.org/tag/earning-to-give), [wild animal welfare](https://forum.effectivealtruism.org/tag/wild-animal-welfare), the [timing of philanthropy](https://forum.effectivealtruism.org/tag/timing-of-philanthropy), and others. [Sam Bankman-Fried](https://forum.effectivealtruism.org/tag/sam-bankman-fried) was a regular visitor in the early 2010s, and credits that forum for introducing him to the effective altruism community.^[\\[1\\]](#fn7zd7aqxs03w)^\n\nHistory\n-------\n\nTwo versions of the Felicifia forum existed. The original version (\"**Old Felicifia**\") was created in late 2006 to early 2007 by Seth Baum, as a group blog/forum which superseded an earlier solo blog by Baum with the same name.^[\\[2\\]](#fn70bw542i74j)^ In 2008, Ryan Carey and Sasha Cooper created a new version (\"**New Felificia**\"),^[\\[3\\]](#fn03rz2caowqw7)^ and Old Felicifia was abandoned shortly thereafter.^[\\[4\\]](#fn0liwld7ap1s)^\n\nFurther reading\n---------------\n\nFrancini, Louis (2020) [Complete archive of the Felicifia forum](https://forum.effectivealtruism.org/posts/Qx9WMJfc7uCB6jsCJ/complete-archive-of-the-felicifia-forum), *Effective Altruism Forum*, May 30.\n\nExternal links\n--------------\n\n[Felicifia](https://felicifia.github.io/forum/index.html). Complete archive of New Felicifia.\n\n[Old Felicifia](https://oldfelicifia.org/). Incomplete archive of Old Felicifia.\n\nRelated entries\n---------------\n\n[history of effective altruism](https://forum.effectivealtruism.org/tag/history-of-effective-altruism) | [utilitarianism](https://forum.effectivealtruism.org/tag/utilitarianism)\n\n1.  ^**[^](#fnref7zd7aqxs03w)**^\n    \n    Wiblin, Robert & Keiran Harris (2022) [Sam Bankman-Fried on taking a high-risk approach to crypto and doing good](https://80000hours.org/podcast/episodes/sam-bankman-fried-high-risk-approach-to-crypto-and-doing-good/), *80,000 Hours*, April 14.\n    \n2.  ^**[^](#fnref70bw542i74j)**^\n    \n    Baum, Seth (2006) [New users start here](https://web.archive.org/web/20070821002318/http://felicifia.blogspot.com:80/2006/09/new-users-start-here.html), *(Old) Felicifia*, September 24.\n    \n3.  ^**[^](#fnref03rz2caowqw7)**^\n    \n    Anthis, Jacy Reese (2022) [Some early history of effective altruism](https://jacyanthis.com/some-early-history-of-effective-altruism), *Jacy Reese Anthis’s Website*, May 15 (updated 7 June 2022).\n    \n4.  ^**[^](#fnref0liwld7ap1s)**^\n    \n    Tomasik, Brian (2018) [History of Old Felicifia](https://oldfelicifia.org/forum-history/), *Old Felicifia*, December 7."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "jAQmzprDCQonpKTsh",
    "name": "Fabianism",
    "core": false,
    "slug": "fabianism",
    "oldSlugs": null,
    "postCount": 1,
    "description": {
      "markdown": "**Fabianism** was a British form of socialism most active in the late 19th and early 20th century that pursued gradualist, nonviolent social reform. The Fabians have been described as \"direct heirs of the Enlightenment in its English Utilitarian branch.\"^[\\[1\\]](#fnsnnyjoagqfm)^\n\nFurther reading\n---------------\n\nAlexander, Scott (2018) [Book review: *History of the Fabian Society*](https://slatestarcodex.com/2018/04/30/book-review-history-of-the-fabian-society/), *Slate Star Codex*, April 30.\n\nMack, Mary Peter (1955) [The Fabians and utilitarianism](http://doi.org/10.2307/2707528), *Journal of the History of Ideas*, vol. 16, pp. 76.\n\nMorgan, Kevin (2017) [Fabian socialism](https://www.bloomsburycollections.com/book/the-bloomsbury-encyclopedia-of-utilitarianism), in James E. Crimmins (ed.) *The Bloomsbury Encyclopedia of Utilitarianism*, London: Bloomsbury Academic, pp. 158–161.\n\nRelated entries\n---------------\n\n[communities adjacent to effective altruism](https://forum.effectivealtruism.org/tag/communities-adjacent-to-effective-altruism) | [social and intellectual movements](https://forum.effectivealtruism.org/tag/social-and-intellectual-movements) | [utilitarianism](https://forum.effectivealtruism.org/tag/utilitarianism)\n\n1.  ^**[^](#fnrefsnnyjoagqfm)**^\n    \n    McBriar, Alan M. (1962) [*Fabian Socialism and English Politics: 1884-1918*](https://en.wikipedia.org/wiki/Special:BookSources/9780521093514), Cambridge: Cambridge University Press, p. 149."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "stDouuTZuFRFXp2ar",
    "name": "Extraterrestrial intelligence",
    "core": false,
    "slug": "extraterrestrial-intelligence",
    "oldSlugs": null,
    "postCount": 18,
    "description": {
      "markdown": "**Extraterrestrial intelligence** is hypothetical intelligent life that did not originate on Earth.\n\nFurther reading\n---------------\n\nDrake, Frank & Carl Sagan (1997) [The search for extraterrestrial intelligence](https://www.scientificamerican.com/article/the-search-for-extraterre/), *Scientific American*, January 6.\n\nShostak, Seth (2021) [Extraterrestrial intelligence](https://www.britannica.com/science/extraterrestrial-intelligence), *Encyclopedia Britannica*, December 1.\n\nRelated entries\n---------------\n\n[Fermi paradox](https://forum.effectivealtruism.org/tag/fermi-paradox) | [grabby aliens](https://forum.effectivealtruism.org/topics/grabby-aliens) | [Great Filter](https://forum.effectivealtruism.org/tag/great-filter) | [nonhumans and the long-term future](https://forum.effectivealtruism.org/tag/non-humans-and-the-long-term-future) | [space colonization](https://forum.effectivealtruism.org/tag/space-colonization) | [space governance](https://forum.effectivealtruism.org/tag/space-governance)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "c6KzNDjNe4D3sebJ6",
    "name": "Existential security",
    "core": false,
    "slug": "existential-security",
    "oldSlugs": null,
    "postCount": 3,
    "description": {
      "markdown": "**Existential security** is a stable state of negligible [existential risk](https://forum.effectivealtruism.org/tag/existential-risk).\n\nReaching existential security is the first stage in a grand strategy of human development proposed by [Toby Ord](https://forum.effectivealtruism.org/tag/toby-ord) in his book [*The Precipice*](https://forum.effectivealtruism.org/tag/the-precipice), to be followed initially by the [long reflection](https://forum.effectivealtruism.org/tag/long-reflection) and ultimately by the full realization of human potential.^[\\[1\\]](#fnfg28xcsxdpr)^ This first stage may itself be decomposed into two substages: first, reducing the most pressing existential risks and second, virtually eliminating all future risks. Reaching existential security thus involves both avoiding immediate failure and making eventual failure impossible. Ord refers to these substages as *preserving* and *protecting* humanity's potential, respectively.^[\\[2\\]](#fn6iw4w35di6e)^ The substages may also be characterized in terms of the temporal scope of the risks involved: while preserving humanity's potential aims at reducing short-term existential risks, protecting humanity's potential aims at reducing long-term existential risks.\n\nFurther reading\n---------------\n\nAird, Michael (2020) [What is existential security?](https://forum.effectivealtruism.org/posts/gugGJGdakND6HtbDx/what-is-existential-security), *Effective Altruism Forum*, September 1.\n\nOrd, Toby (2020) [*The Precipice: Existential Risk and the Future of Humanity*](https://en.wikipedia.org/wiki/Special:BookSources/1526600218), London: Bloomsbury Publishing, ch. 7.\n\nRelated entries\n---------------\n\n[existential risk](https://forum.effectivealtruism.org/tag/existential-risk) | [long reflection](https://forum.effectivealtruism.org/tag/long-reflection) | [longtermism](https://forum.effectivealtruism.org/tag/longtermism) | [vulnerable world hypothesis](https://forum.effectivealtruism.org/tag/vulnerable-world-hypothesis) | [Trinity](https://forum.effectivealtruism.org/tag/trinity)\n\n1.  ^**[^](#fnreffg28xcsxdpr)**^\n    \n    Ord, Toby (2020) [*The Precipice: Existential Risk and the Future of Humanity*](https://en.wikipedia.org/wiki/Special:BookSources/1526600218), London: Bloomsbury Publishing, p. 189.\n    \n2.  ^**[^](#fnref6iw4w35di6e)**^\n    \n    Ord, [*The Precipice*](https://en.wikipedia.org/wiki/Special:BookSources/1526600218), pp. 189–191."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "wvxSKgKkxqptdenFm",
    "name": "Existential risk factor",
    "core": false,
    "slug": "existential-risk-factor",
    "oldSlugs": null,
    "postCount": 22,
    "description": {
      "markdown": "An **existential risk factor** is a factor that increases the probability of an [existential catastrophe](https://forum.effectivealtruism.org/tag/existential-catastrophe-1). Conversely, an **existential security factor** is a factor that *decreases* the probability of an existential catastrophe.^[\\[1\\]](#fngmkob0nvxb)^ Analogous concepts have been used to analyze risks of [human extinction](https://forum.effectivealtruism.org/tag/human-extinction)^[\\[2\\]](#fnmrwsn1edcs8)^ and [s-risks](https://forum.effectivealtruism.org/tag/s-risk).^[\\[3\\]](#fnc9w4v3lzc5v)^\n\nFurther reading\n---------------\n\nBaumann, Tobias (2019) [Risk factors for s-risks](http://centerforreducingsuffering.org/risk-factors-for-s-risks/), *Center for Reducing Suffering*, February 13.\n\nCotton-Barratt, Owen, Max Daniel & Anders Sandberg (2020) [Defence in depth against human extinction: prevention, response, resilience, and why they all matter](http://doi.org/10.1111/1758-5899.12786), *Global Policy*, vol. 11, pp. 271–282.\n\nOrd, Toby (2020) [*The Precipice: Existential Risk and the Future of Humanity*](https://en.wikipedia.org/wiki/Special:BookSources/1526600218), London: Bloomsbury Publishing.\n\nRelated entries\n---------------\n\n[broad vs. narrow interventions](https://forum.effectivealtruism.org/tag/broad-vs-narrow-interventions) | [civilizational collapse](/tag/civilizational-collapse) | [compound existential risk](https://forum.effectivealtruism.org/tag/compound-existential-risk) | [emergency response](https://forum.effectivealtruism.org/tag/emergency-response) | [existential catastrophe](https://forum.effectivealtruism.org/tag/existential-catastrophe-1) | [existential risk](https://forum.effectivealtruism.org/tag/existential-risk) | [indirect long-term effects](https://forum.effectivealtruism.org/tag/indirect-long-term-effects)\n\n1.  ^**[^](#fnrefgmkob0nvxb)**^\n    \n    Ord, Toby (2020) [*The Precipice: Existential Risk and the Future of Humanity*](https://en.wikipedia.org/wiki/Special:BookSources/1526600218), London: Bloomsbury Publishing.\n    \n2.  ^**[^](#fnrefmrwsn1edcs8)**^\n    \n    Cotton-Barratt, Owen, Max Daniel & Anders Sandberg (2020) [Defence in depth against human extinction: prevention, response, resilience, and why they all matter](http://doi.org/10.1111/1758-5899.12786), *Global Policy*, vol. 11, pp. 271–282.\n    \n3.  ^**[^](#fnrefc9w4v3lzc5v)**^\n    \n    Baumann, Tobias (2019) [Risk factors for s-risks](http://centerforreducingsuffering.org/risk-factors-for-s-risks/), *Center for Reducing Suffering*, February 13."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "c2tb3HXz9X6DHZAKh",
    "name": "Effective altruism",
    "core": false,
    "slug": "effective-altruism",
    "oldSlugs": null,
    "postCount": 2,
    "description": {
      "markdown": "**Effective altruism** is the project of using evidence and reason to find out how to do the most good and to act on these findings.\n\nFurther reading\n---------------\n\n80,000 Hours (2021) [Effective altruism: Ten global problems](https://80000hours.org/podcast/effective-altruism-ten-global-problems/), *80,000 Hours*.\n\nBerkey, Brian (2021) [The philosophical core of effective altruism](http://doi.org/10.1111/josp.12347), *Journal of Social Philosophy*, vol. 52, pp. 92–113.\n\nGreaves, Hilary & Theron Pummer (2019) [Introduction](http://doi.org/10.1093/oso/9780198841364.003.0016), in Hilary Greaves & Theron Pummer (eds.) *Effective Altruism: Philosophical Issues*, Oxford: Oxford University Press, pp. 1–9.\n\nHarris, Keiran & Robert Wiblin (2021) [Effective altruism in a nutshell](https://80000hours.org/2021/10/effective-altruism-in-a-nutshell/), *80,000 Hours*, October 18.\n\nMacAskill, William & Theron Pummer (2020) [Effective altruism](http://doi.org/10.1002/9781444367072.wbiee883), in Hugh LaFollette (ed.) *International Encyclopedia of Ethics*, Chichester, West Sussex: Wiley.\n\nSinger, Peter (2019) [Foreword](https://oxford.universitypressscholarship.com/view/10.1093/oso/9780198841364.001.0001/oso-9780198841364), in Hilary Greaves & Theron Pummer (eds.) *Effective Altruism: Philosophical Issues*, Oxford: Oxford University Press, pp. v–vi.\n\nRelated entries\n---------------\n\n[criticism of effective altruism](https://forum.effectivealtruism.org/tag/criticism-of-effective-altruism) | [definition of effective altruism](https://forum.effectivealtruism.org/tag/definition-of-effective-altruism) | [history of effective altruism](https://forum.effectivealtruism.org/tag/history-of-effective-altruism) | [philosophy of effective altruism](https://forum.effectivealtruism.org/topics/philosophy-of-effective-altruism)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "Y58JXAG3TYRFphwyf",
    "name": "Excited vs. obligatory altruism",
    "core": false,
    "slug": "excited-vs-obligatory-altruism",
    "oldSlugs": [
      "excited-altruism-and-obligatory-altruism"
    ],
    "postCount": 12,
    "description": {
      "markdown": "**Excited altruism** and **obligatory altruism** are two contrasting views about the reasons for engaging in [effective altruism](https://forum.effectivealtruism.org/tag/effective-altruism). According to obligatory altruism, we should act altruistically because this is what morality requires of us; failing to do so would be morally wrong. By contrast, excited altruism stresses that being able to make a significant difference in the life of others is an exciting opportunity. On this approach, emphasis is given to the fact that altruistic actions can have a large positive impact on others, as well as improving the altruist’s own life.\n\n[Holden Karnofsky](https://forum.effectivealtruism.org/tag/holden-karnofsky) offers a clear articulation of the excited altruist perspective:^[\\[1\\]](#fno1ihx1ltl2)^^[\\[2\\]](#fn975qsczsy6e)^\n\n> Critics of effective altruism worry that we’re trying to choose causes based on calculations about how to help the world as much as possible, rather than based on what causes excite us… I think such people fundamentally misunderstand effective altruism. I think they imagine that we have passions for particular causes, and are trying to submerge our passions in the service of rationality. That isn’t the case. Rather, effective altruism is what we are passionate about. We’re excited by the idea of making the most of our resources and helping others as much as possible.\n\n[Peter Singer](https://forum.effectivealtruism.org/tag/peter-singer),^[\\[3\\]](#fnk9zfh4prfvn)^ perhaps the most prominent exponent of obligatory altruism, wonders what would Karnofsky do\n\n> if he were to wake up one morning and find he has a passion for working in a soup kitchen and little passion for his work at GiveWell. Would he go and work at the soup kitchen, even though he would do much more good if he continued to work at GiveWell? In response Holden said he found it difficult to engage with a hypothetical question that involved such a fundamental transformation, but added, “I would have a tough decision and would have a real chance of opting for the soup kitchen.” Why, though, is this even a tough decision? Why does Holden not say simply, “Yes, of course, then I would work at the soup kitchen”? Because, I suggest, reason is playing a role in his decision making, as it should.\n\nExcited and obligatory altruism, however, need not be mutually exclusive: one may regard the prospect of improving the lives of others as both a moral obligation and an exciting opportunity. Nor are these two perspectives jointly exhaustive. Nate Soares rejects obligatory altruism because \"guilt and shame are poor motivators, and that self-imposed obligations are often harmful\", but he also rejects excited altruism because \"Lives hang in the balance. The entire future hangs in the balance. To call this an 'exciting opportunity' rings false.\"^[\\[4\\]](#fna2sfw1bxxyc)^\n\nEven if one thinks that there is ultimately a requirement to help others effectively, the motivation to act on this requirement may derive from a feeling of excitement more than from a sense of duty. Conversely, some effective altruists may find the sense of duty deeply motivating despite judging that there is ultimately no reason—besides personal inclination—to engage in acts of altruism; [Brian Tomasik](https://forum.effectivealtruism.org/tag/brian-tomasik) is one effective altruist who exemplifies this approach.\n\nData about the relative prevalence of excited and obligatory altruism in the effective altruism community is scarce. An informal Facebook poll (*n* = 68) from 2015 found that 57% of effective altruists viewed EA primarily as an obligation, whereas 43% viewed it primarily as an opportunity.^[\\[5\\]](#fn84iwhh3z8hy)^\n\nFurther reading\n---------------\n\nKarnofsky, Holden (2013) [Excited altruism](https://blog.givewell.org/2013/08/20/excited-altruism/), *The GiveWell Blog*, August 20 (updated 25 July 2016).\n\nSinger, Peter (2015) [*The Most Good You Can Do: How Effective Altruism Is Changing Ideas About Living Ethically*](https://en.wikipedia.org/wiki/Special:BookSources/9780300180275), New Haven: Yale University Press, ch. 8.\n\nRelated entries\n---------------\n\n[altruistic motivation](https://forum.effectivealtruism.org/tag/altruistic-motivation) | [demandingness of morality](https://forum.effectivealtruism.org/tag/demandingness-of-morality)\n\n1.  ^**[^](#fnrefo1ihx1ltl2)**^\n    \n    Karnofsky, Holden (2013) [Excited altruism](https://blog.givewell.org/2013/08/20/excited-altruism/), *The GiveWell Blog*, August 20 (updated 25 July 2016).\n    \n2.  ^**[^](#fnref975qsczsy6e)**^\n    \n    See also Sotala, Kaj (2014) [Effective altruism as the most exciting cause in the world](https://forum.effectivealtruism.org/posts/LwmEr3B9dpBrFq3du/effective-altruism-as-the-most-exciting-cause-in-the-world), *Effective Altruism Forum*, September 26.\n    \n3.  ^**[^](#fnrefk9zfh4prfvn)**^\n    \n    Singer, Peter (2015) [*The Most Good You Can Do: How Effective Altruism Is Changing Ideas About Living Ethically*](https://en.wikipedia.org/wiki/Special:BookSources/9780300180275), New Haven: Yale University Press, p. 93.\n    \n4.  ^**[^](#fnrefa2sfw1bxxyc)**^\n    \n    Soares, Nate (2015) [Altruistic motivations](http://mindingourway.com/altruistic-motivations/), *Minding Our Way*, August 2.\n    \n5.  ^**[^](#fnref84iwhh3z8hy)**^\n    \n    Gordon-Brown, Alexander (2015) [Effective altruism: duty or opportunity?](https://www.facebook.com/groups/effective.altruists/permalink/815061285216897/), *Effective Altruism Facebook Group*."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "eFm4ibEkakypL2SxF",
    "name": "Evolution heuristic",
    "core": false,
    "slug": "evolution-heuristic",
    "oldSlugs": null,
    "postCount": 1,
    "description": {
      "markdown": "The **evolution heuristic** (sometimes referred to as the **wisdom of nature**) is a method, proposed by [Nick Bostrom](https://forum.effectivealtruism.org/tag/nick-bostrom) and [Anders Sandberg](https://forum.effectivealtruism.org/tag/anders-sandberg),^[\\[1\\]](#fnysg6ett9yae)^ for evaluating possible forms of human [enhancement](https://forum.effectivealtruism.org/tag/cognitive-enhancement).\n\nHuman beings are evolved organisms, whose traits have been optimized by a long process of random mutation and natural selection. Attempts to enhance such systems undertaken without due consideration of how they originated are likely to backfire. This insight may be expressed as an **evolutionary optimality challenge**:^[\\[2\\]](#fnj813lk1ui7m)^\n\n> If the proposed intervention would result in an enhancement, why have we not already evolved to be that way?\n\nThe evolution heuristic holds that a failure to provide an answer to that question creates a presumption against the proposed intervention. But the heuristic also identifies three types of considerations that could answer the question, and hence defeat the presumption:^[\\[3\\]](#fn7xc9sp6je6r)^\n\n1.  *Changed tradeoffs*. The environment may have changed in significant ways relative to that which prevailed during most of our evolutionary history.\n2.  *Value discordance*. Evolution selects for inclusive genetic fitness, whereas interventions to improve the human condition are assessed according to some moral standard, such as the welfare of the individuals affected.\n3.  *Evolutionary restrictions*. Current technology enables various modifications to the human organism that natural selection could not attain with the resources at its disposal.\n\nFurther reading\n---------------\n\nBostrom, Nick & Anders Sandberg (2009) [The wisdom of nature: an evolutionary heuristic for human enhancement](https://doi.org/10.1007/978-94-024-0979-6_12), in Julian Savulescu & Nick Bostrom (eds.) *Human Enhancement*, Oxford: Oxford University Press, pp. 375–416.\n\nHeuer, Kelly (2013) [The “wisdom of nature” argument in contemporary bioethics](https://www.academia.edu/319003/The_Wisdom_of_Nature_Argument_In_Contemporary_Bioethics), unpublished.\n\n1.  ^**[^](#fnrefysg6ett9yae)**^\n    \n    Bostrom, Nick & Anders Sandberg (2009) [The wisdom of nature: an evolutionary heuristic for human enhancement](https://doi.org/10.1007/978-94-024-0979-6_12), in Julian Savulescu & Nick Bostrom (eds.) *Human Enhancement*, Oxford: Oxford University Press, pp. 375–416.\n    \n2.  ^**[^](#fnrefj813lk1ui7m)**^\n    \n    Bostrom & Sandberg, [The wisdom of nature](https://doi.org/10.1007/978-94-024-0979-6_12), p. 378.\n    \n3.  ^**[^](#fnref7xc9sp6je6r)**^\n    \n    Bostrom & Sandberg, [The wisdom of nature](https://doi.org/10.1007/978-94-024-0979-6_12), pp. 378-380."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "PsmpTYtNo4kh3Cfim",
    "name": "Michael Kremer",
    "core": false,
    "slug": "michael-kremer",
    "oldSlugs": null,
    "postCount": null,
    "description": {
      "markdown": "**Michael Robert Kremer** (born 12 November 1964) is an American economist, currently a professor of economics at the University of Chicago. Together with [Esther Duflo](https://forum.effectivealtruism.org/tag/esther-duflo) and [Abhijit Banerjee](https://forum.effectivealtruism.org/tag/abhijit-banerjee), he was awarded the 2019 Nobel Prize in Economic Sciences for his use of [randomized controlled trials](https://forum.effectivealtruism.org/tag/randomized-controlled-trials) in social science to alleviate [global poverty](https://forum.effectivealtruism.org/tag/global-poverty).^[\\[1\\]](#fnfxfqsomcdsa)^\n\nKremer is a member of [Giving What We Can](https://forum.effectivealtruism.org/tag/giving-what-we-can).\n\nBackground\n----------\n\nKremer studied at Harvard University, where he majored in social studies and, in 1992, received a doctorate in economics. He was a professor at the Massachusetts Institute of Technology from 1993 to 1999, and at Harvard from 1999 to 2020. In September 2020, he joined the University of Chicago as University Professor in the department of economics and the school of public policy.\n\nFurther reading\n---------------\n\nDuignan, Brian (2019) [Michael Kremer](https://www.britannica.com/biography/Michael-Kremer), *Encyclopedia Britannica*, October 25 (updated 8 November 2021).\n\nExternal links\n--------------\n\n[Michael Kremer](https://economics.uchicago.edu/directory/michael-kremer). University of Chicago homepage.\n\n1.  ^**[^](#fnreffxfqsomcdsa)**^\n    \n    Royal Swedish Academy of Sciences (2019) [The Prize in Economic Sciences 2019](https://www.nobelprize.org/uploads/2019/10/press-economicsciences2019-2.pdf), *The Nobel Prize*, October 14."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "ZsW45yAqDqQQgcNnk",
    "name": "Esther Duflo",
    "core": false,
    "slug": "esther-duflo",
    "oldSlugs": null,
    "postCount": 1,
    "description": {
      "markdown": "**Esther Duflo** (born 25 October 1972) is a French economist, currently a professor of economics at the Massachusetts Institute of Technology (MIT). Together with [Abhijit Banerjee](https://forum.effectivealtruism.org/tag/abhijit-banerjee) and [Michael Kremer](https://forum.effectivealtruism.org/tag/michael-kremer), she was awarded the 2019 Nobel Prize in Economic Sciences for her use of [randomized controlled trials](https://forum.effectivealtruism.org/tag/randomized-controlled-trials) to alleviate [global poverty](https://forum.effectivealtruism.org/tag/global-poverty).^[\\[1\\]](#fnl2r9pv5u4up)^\n\nBackground\n----------\n\nDuflo double-majored in economics and history at the École Normale Supérieure, and subsequently received a master's degree in economics from DELTA (now the Paris School of Economics) and a doctoral degree in economics from MIT. After completing her studies, she joined MIT's faculty and was, in 2005, appointed Abdul Latif Jameel Professor of Poverty Alleviation and Development Economics. Together with Abhijit Banerjee and Sendhil Mullainathan, she founded the [Abdul Latif Jameel Poverty Action Lab](https://forum.effectivealtruism.org/tag/abdul-latif-jameel-poverty-action-lab) (J-PAL) in 2003, and remains one of the two Directors of that global research center.\n\nFurther reading\n---------------\n\nDuflo, Esther (2019) [Biographical](https://www.nobelprize.org/prizes/economic-sciences/2019/duflo/biographical/), *The Nobel Prize*.\n\nDuignan, Brian (2021) [Esther Duflo](https://www.britannica.com/biography/Esther-Duflo), *Encyclopedia Britannica*, October 21.\n\nExternal links\n--------------\n\n[Esther Duflo](https://economics.mit.edu/faculty/eduflo). MIT homepage.\n\n1.  ^**[^](#fnrefl2r9pv5u4up)**^\n    \n    Royal Swedish Academy of Sciences (2019) [The Prize in Economic Sciences 2019](https://www.nobelprize.org/uploads/2019/10/press-economicsciences2019-2.pdf), *The Nobel Prize*, October 14."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "NFZNj5yfuWxz4m9z3",
    "name": "Randomized controlled trials",
    "core": false,
    "slug": "randomized-controlled-trials",
    "oldSlugs": [
      "randomized-controlled-trials-in-social-science"
    ],
    "postCount": 19,
    "description": {
      "markdown": "**Randomized controlled trials** (**RCTs**) are an experimental form of [impact evaluation](https://forum.effectivealtruism.org/topics/impact-assessment) in which the population receiving the program or policy intervention is chosen at random from the eligible population, and a control group is also chosen at random from the same eligible population.^[\\[1\\]](#fnrazqet4fpsa)^\n\nFurther reading\n---------------\n\nBouguen, Adrien *et al.* (2019) [Using randomized controlled trials to estimate long-run impacts in development economics](http://doi.org/10.1146/annurev-economics-080218-030333), *Annual Review of Economics*, vol. 11, pp. 523–561.\n\nKarnofsky, Holden (2012) [How we evaluate a study](https://blog.givewell.org/2012/08/23/how-we-evaluate-a-study/), *The GiveWell Blog*, August 23 (updated 2 September 2016).\n\nOgden, Timothy (2020) [RCTs in development economics, their critics and their evolution](http://doi.org/10.1093/oso/9780198865360.003.0006), in Florent Bédécarrats, Isabelle Guérin & François Roubaud (eds.) *Randomized Control Trials in the Field of Development*, Oxford: Oxford University Press, pp. 126–151.\n\n1.  ^**[^](#fnrefrazqet4fpsa)**^\n    \n    White, Howard, Shagun Sabarwal & Thomas de Hoop (2014) 'Randomized Controlled Trials (RCTs)', in *Methodological Briefs: Impact Evaluation 7*, Florence: UNICEF Office of Research"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5WaR4BZAYNWdDtdui",
    "name": "Global poverty",
    "core": false,
    "slug": "global-poverty",
    "oldSlugs": null,
    "postCount": 26,
    "description": {
      "markdown": "**Global poverty** is the worldwide lack of material possessions or income to satisfy basic human needs.\n\nPoverty is a multidimensional concept, incorporating a number of different factors which impact the lives of the poor. The World Bank uses an international poverty line of $1.90 a day (adjusted for local purchasing power), and estimates that, in 2013, 10.7% of the world’s population lived below this line.^[\\[1\\]](#fn6gieyy3gh1j)^ Lack of economic resources has direct consequences on many other aspects of life, including [food security](https://forum.effectivealtruism.org/tag/food-security) and access to healthcare and water services.\n\n[Cash transfers](https://forum.effectivealtruism.org/tag/cash-transfers) and [deworming](https://forum.effectivealtruism.org/tag/deworming) programs are generally regarded as highly cost-effective interventions for reducing extreme poverty.\n\nFurther reading\n---------------\n\nKaufman, Jeff (2015) [Why global poverty?](https://www.jefftk.com/p/why-global-poverty), *Jeff Kaufman’s Blog*, August 11.\n\nRavallion, Martin (2016) [*The Economics of Poverty: History, Measurement, and Policy*](https://en.wikipedia.org/wiki/Special:BookSources/9780190212773), Oxford: Oxford University Press.\n\nRoser, Max & Esteban Ortiz-Ospina (2013) [Global extreme poverty](https://ourworldindata.org/extreme-poverty), *Our World in Data*, May 25 (updated 2019).\n\nRelated entries\n---------------\n\n[economic growth](https://forum.effectivealtruism.org/tag/economic-growth) | [food security](https://forum.effectivealtruism.org/tag/food-security) | [global health and development](https://forum.effectivealtruism.org/tag/global-health-and-development) | [poverty trap](https://forum.effectivealtruism.org/topics/poverty-trap)\n\n1.  ^**[^](#fnref6gieyy3gh1j)**^\n    \n    World Bank (2016) [*Poverty and Shared Prosperity 2016: Taking on Inequality*](http://doi.org/10.1596/978-1-4648-0958-3), Washington: World Bank."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "8729b26vvCSQE7H5N",
    "name": "Effective Altruism Survey",
    "core": false,
    "slug": "effective-altruism-survey",
    "oldSlugs": null,
    "postCount": 53,
    "description": {
      "markdown": "The **Effective Altruism Survey** is an annual [survey](https://forum.effectivealtruism.org/tag/surveys) of the effective altruism community.\n\nThe first survey took place in 2014, as a project of [Rethink Charity](https://forum.effectivealtruism.org/tag/rethink-charity). Currently, the survey is conducted by [Rethink Priorities](https://forum.effectivealtruism.org/tag/rethink-priorities), which also publishes detailed analyses of the survey data.\n\nExternal links\n--------------\n\n[Effective Altruism Survey](https://rethinkpriorities.org/ea-survey). Official website.\n\nRelated entries\n---------------\n\n[Data (EA community)](https://forum.effectivealtruism.org/tag/data-ea-community-1) | [Effective Altruism Group Organisers' Survey](https://forum.effectivealtruism.org/tag/effective-altruism-group-organisers-survey) | [surveys](https://forum.effectivealtruism.org/tag/surveys)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "JwQtyW6FD7YLW23Gp",
    "name": "Rethink Charity",
    "core": false,
    "slug": "rethink-charity",
    "oldSlugs": null,
    "postCount": 72,
    "description": {
      "markdown": "**Rethink Charity** is a nonprofit that promotes donations to high-impact interventions.\n\nCurrent projects include [RC Forward](https://forum.effectivealtruism.org/tag/rc-forward), [EA Giving Tuesday](https://forum.effectivealtruism.org/tag/ea-giving-tuesday), and Rethink Charity fiscal sponsorship services.^[\\[1\\]](#fngp98fq84gqh)^ Past projects include [LEAN](https://forum.effectivealtruism.org/tag/lean), [Rethink Priorities](https://forum.effectivealtruism.org/tag/rethink-priorities), [Students for High-Impact Charity](https://forum.effectivealtruism.org/tag/students-for-high-impact-charity), the [Effective Altruism Hub](https://forum.effectivealtruism.org/tag/ea-hub), and the [Effective Altruism Survey](https://forum.effectivealtruism.org/tag/effective-altruism-survey). \n\nBefore 2017, Rethink Charity was known as [.impact](https://forum.effectivealtruism.org/tag/impact). \n\nExternal links\n--------------\n\n[Rethink Charity](https://www.rethink.charity). Official website.\n\n[Donate to Rethink Charity](https://rethink.charity/donate).\n\n1.  ^**[^](#fnrefgp98fq84gqh)**^\n    \n    Rethink Charity (2022) [Fiscal sponsorship with Rethink Charity](https://rethink.charity/fiscal-sponsorship), *Rethink Charity*."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "3J3dtqjHD2bmLGGAv",
    "name": "Effective Altruism Global",
    "core": false,
    "slug": "effective-altruism-global",
    "oldSlugs": null,
    "postCount": 75,
    "description": {
      "markdown": "*Do not use this tag for EA Global talks, unless the talks themselves are about EA Global.*\n\n**Effective Altruism Global** (**EAG**; also spelled **EA Global**) is a recurring effective altruism conference series.\n\nMost EA Global talks are recorded, and many have full transcripts.\n\nPast EA Global speakers include [Nick Beckstead](https://forum.effectivealtruism.org/tag/nick-beckstead), [Liv Boeree](https://forum.effectivealtruism.org/tag/liv-boeree), [Nick Bostrom](https://forum.effectivealtruism.org/tag/nick-bostrom), George Church, Olivia Fox Cabane, [Paul Christiano](https://forum.effectivealtruism.org/tag/paul-christiano), [Eric Drexler](https://forum.effectivealtruism.org/tag/eric-drexler), Kevin Esvelt, Bruce Friedrich, [Julia Galef](https://forum.effectivealtruism.org/tag/julia-galef), Rachel Glennerster, Joseph Gordon-Levitt, [Hilary Greaves](https://forum.effectivealtruism.org/tag/hilary-greaves), A. J. Jacobs, [Holden Karnofsky](https://forum.effectivealtruism.org/tag/holden-karnofsky), Ezra Klein, [Michael Kremer](https://forum.effectivealtruism.org/tag/michael-kremer), [Robin Hanson](https://forum.effectivealtruism.org/tag/robin-hanson), [William MacAskill](https://forum.effectivealtruism.org/tag/william-macaskill), Jason Gaverick Matheny, [Elon Musk](https://forum.effectivealtruism.org/tag/elon-musk), [Toby Ord](https://forum.effectivealtruism.org/tag/toby-ord), [Stuart Russell](https://forum.effectivealtruism.org/tag/stuart-russell), [Peter Singer](https://forum.effectivealtruism.org/tag/peter-singer), Cass Sunstein, [Jaan Tallinn](https://forum.effectivealtruism.org/tag/jaan-tallinn), [Philip Tetlock](https://forum.effectivealtruism.org/tag/philip-tetlock), [Kelsey Piper](https://forum.effectivealtruism.org/tag/kelsey-piper), [Cari Tuna](https://forum.effectivealtruism.org/tag/cari-tuna) and [Eliezer Yudkowsky](https://forum.effectivealtruism.org/tag/eliezer-yudkowsky), among others.\n\nExternal links\n--------------\n\n[EA Global](https://www.eaglobal.org/).  Official website.\n\n[EA Global](https://forum.effectivealtruism.org/users/ea-global). [Effective Altruism Forum](https://forum.effectivealtruism.org/tag/effective-altruism-forum-1) account with links to videos and transcripts.\n\nRelated entries\n---------------\n\n[conferences](https://forum.effectivealtruism.org/tag/conferences) | [EAGx](https://forum.effectivealtruism.org/tag/eagx) | [Coordination Forum](https://forum.effectivealtruism.org/tag/leaders-forum)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "6HWv7R5rbyZHkcMyE",
    "name": "Effective Altruism Foundation",
    "core": false,
    "slug": "effective-altruism-foundation",
    "oldSlugs": null,
    "postCount": 8,
    "description": {
      "markdown": "The **Effective Altruism Foundation** (**EAF**) is a nonprofit based in London. Its projects have included the [Center on Long-Term Risk](https://forum.effectivealtruism.org/tag/center-on-long-term-risk) and [Raising for Effective Giving](https://forum.effectivealtruism.org/tag/raising-for-effective-giving).\n\nFunding\n-------\n\nAs of July 2022, EAF has received nearly $2.3 million in grants from [Open Philanthropy](https://forum.effectivealtruism.org/topics/open-philanthropy),^[\\[1\\]](#fnh2x5a72kag)^ and $30,000 from [Effective Altruism Funds](https://forum.effectivealtruism.org/topics/effective-altruism-funds).^[\\[2\\]](#fn7h8l8erhw2q)^\n\nExternal links\n--------------\n\n[Effective Altruism Foundation](https://ea-foundation.org/). Official website.\n\n[Donate to the Effective Altruism Foundation](https://ea-foundation.org/donate-ea/).\n\nRelated entries\n---------------\n\n[Center on Long-Term Risk](https://forum.effectivealtruism.org/tag/center-on-long-term-risk) | [Raising for Effective Giving](https://forum.effectivealtruism.org/tag/raising-for-effective-giving)\n\n1.  ^**[^](#fnrefh2x5a72kag)**^\n    \n    Open Philanthropy (2022) [Grants database: Effective Altruism Foundation](https://www.openphilanthropy.org/grants/?q=&organization-name=effective-altruism-foundation), *Open Philanthropy*.\n    \n2.  ^**[^](#fnref7h8l8erhw2q)**^\n    \n    Animal Welfare Fund (2017) [April 2017: Animal Welfare Fund grants](https://funds.effectivealtruism.org/funds/payouts/april-2017-animal-welfare-fund-grants), *Effective Altruism Funds*, April."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "mcxeYnXAZNBDTjJuL",
    "name": "Raising for Effective Giving",
    "core": false,
    "slug": "raising-for-effective-giving",
    "oldSlugs": null,
    "postCount": 5,
    "description": {
      "markdown": "**Raising for Effective Giving** (**REG**) is a nonprofit that spreads [effective altruism](https://forum.effectivealtruism.org/tag/effective-altruism) ideas among poker professionals and encourages them to [pledge to donate](https://forum.effectivealtruism.org/tag/donation-pledge) a percentage of their income to effective charities.\n\nHistory\n-------\n\nREG was founded in 2014 by professional poker players [Liv Boeree](https://forum.effectivealtruism.org/tag/liv-boeree), Igor Kurganov, Phil Gruissem and Stefan Huber.^[\\[1\\]](#fn9n55i1l4zn)^\n\nExternal links\n--------------\n\n[Raising for Effective Giving](https://reg-charity.org/). Official website.\n\nRelated entries\n---------------\n\n[donation pledge](https://forum.effectivealtruism.org/tag/donation-pledge) | [Effective Altruism Foundation](https://forum.effectivealtruism.org/tag/effective-altruism-foundation/) | [High Impact Athletes](https://forum.effectivealtruism.org/tag/high-impact-athletes)\n\n1.  ^**[^](#fnref9n55i1l4zn)**^\n    \n    Raising for Effective Giving (2022) [Team](https://reg-charity.org/about/team/), *Raising for Effective Giving*."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "4JAa3nwAmjEoe8Rgx",
    "name": "Totalitarianism",
    "core": false,
    "slug": "totalitarianism",
    "oldSlugs": null,
    "postCount": 23,
    "description": {
      "markdown": "**Totalitarianism** is an all-embracing system of government that exercises virtually complete control over every aspect of individual life. ***Robust*** **totalitarianism** may be defined as a type of totalitarianism particularly effective at enforcing its ideological vision and preventing internal and external threats to its authority.\n\nCharacteristics\n---------------\n\nBenito Mussolini famously characterized totalitarianism as \"all within the state, nothing outside the state, none against the state.\"^[\\[1\\]](#fnk8ygoel1d6q)^ Contemporary scholars have listed several distinctive features of totalitarian regimes. These features include a radical official ideology, usually exclusionary and future-oriented; a single party, typically led by one man; a monopoly of the means of both persuasion and coercion; a centrally planned economy, in which most professional activities are part of the state; and extreme [polarization](https://forum.effectivealtruism.org/tag/political-polarization) and widespread use of [terror](https://forum.effectivealtruism.org/tag/terrorism) in all spheres of life.^[\\[2\\]](#fn3v923odr1t5)^^[\\[3\\]](#fna5qji60xdhw)^^[\\[4\\]](#fnoemid6eomnp)^ Totalitarian regimes are estimated to have been responsible for the deaths of over 125 million people in the 20th century, mostly in the Soviet Union, Nazi Germany, and communist China.^[\\[5\\]](#fnwq5yh8r3h3g)^ To this tragic loss of life needs to be added the major loss of [quality of life](https://forum.effectivealtruism.org/tag/wellbeing) experienced by those living under such regimes.\n\nRobust totalitarianism as a catastrophic and existential risk\n-------------------------------------------------------------\n\nBecause of its scale, the threat of robust totalitarianism constitutes a [global catastrophic risk](https://forum.effectivealtruism.org/tag/global-catastrophic-risk). If the totalitarian regime has the potential to be both global and stable, it could also constitute an [existential risk](https://forum.effectivealtruism.org/tag/existential-risk)—specifically a risk of an unrecoverable [dystopia](https://forum.effectivealtruism.org/tag/dystopia).\n\nAdvances in [artificial intelligence](https://forum.effectivealtruism.org/tag/artificial-intelligence) in areas such as lie detection, social persuasion and deception, autonomous weapons, and ubiquitous surveillance could entrench existing totalitarian regimes. These developments may also [cause democracies to slide into totalitarianism](https://forum.effectivealtruism.org/tag/safeguarding-liberal-democracy).^[\\[6\\]](#fnhwktxygoxx5)^ On the other hand, AI could conceivably destabilize totalitarian systems or protect against their emergence.^[\\[7\\]](#fnwnabi8fm50q)^ To this date, no detailed analysis exists of the potential impact of artificial intelligence on the risk of robust totalitarianism. The literature on robust totalitarianism in general is itself very small.^[\\[8\\]](#fnkf66sg490ji)^\n\nEvaluation\n----------\n\n[80,000 Hours](https://forum.effectivealtruism.org/tag/80-000-hours) rates risks of robust totalitarianism a \"potential highest priority area\": an issue that, if more thoroughly examined, could rank as a top global challenge.^[\\[10\\]](#fnfjgm1g1sz5a)^\n\nFurther reading\n---------------\n\nAird, Michael (2020) [Collection of sources related to dystopias and \"robust totalitarianism\"](https://forum.effectivealtruism.org/posts/EMKf4Gyee7BsY2RP8/michaela-s-shortform?commentId=8GJtZ6DrEn5MDf6dZ), *Effective Altruism Forum*, March 30.  \n*Many additional resources on this topic.*\n\nCaplan, Bryan (2008) [The totalitarian threat](https://en.wikipedia.org/wiki/Special:BookSources/9780199606504), in Nick Bostrom & Milan M. Ćirković (eds.) *Global Catastrophic Risks*, Oxford: Oxford University Press, pp. 504–519.\n\nRelated entries\n---------------\n\n[dystopia](https://forum.effectivealtruism.org/tag/dystopia) | [global governance](https://forum.effectivealtruism.org/tag/global-governance)\n\n1.  ^**[^](#fnrefk8ygoel1d6q)**^\n    \n    Mussolini, Benito (1932) 'La dottrina del fascismo', in *Enciclopedia italiana di scienze, lettere ed arti*, Roma: Istituto della Enciclopedia Italiana.\n    \n2.  ^**[^](#fnref3v923odr1t5)**^\n    \n    Friedrich, Carl J. & Zbigniew K. Brzezinski (1965) *Totalitarian Dictatorship and Autocracy*, 2nd ed., Cambridge: Harvard University Press, p. 22.\n    \n3.  ^**[^](#fnrefa5qji60xdhw)**^\n    \n    Aron, Raymond (1965) *Démocratie et totalitarisme*, Paris: Gallimard, ch. 15.\n    \n4.  ^**[^](#fnrefoemid6eomnp)**^\n    \n    Holmes, Leslie (2001) [Totalitarianism](http://doi.org/10.1016/B0-08-043076-7/01240-7), in Neil J. Smelser & Paul B. Baltes (eds.) *International Encyclopedia of the Social & Behavioral Sciences*, Amsterdam: Elsevier, pp. 15788–15791.\n    \n5.  ^**[^](#fnrefwq5yh8r3h3g)**^\n    \n    Bernholz, Peter (2000) [Totalitarianism](http://doi.org/10.1007/978-0-306-47828-4_201), in Charles K. Rowley & Friedrich Schneider (eds.) *The Encyclopedia of Public Choice*, Boston: Springer, pp. 565–569, p. 568.\n    \n6.  ^**[^](#fnrefhwktxygoxx5)**^\n    \n    Dafoe, Allan (2018) [AI governance: A research agenda](https://www.fhi.ox.ac.uk/wp-content/uploads/GovAI-Agenda.pdf), Future of Humanity Institute, University of Oxford, section 4.1.\n    \n7.  ^**[^](#fnrefwnabi8fm50q)**^\n    \n    Adamczewski, Tom (2019) [A shift in arguments for AI risk](https://fragile-credences.github.io/prioritising-ai/), *Fragile Credences*, May 25, section 'Robust totalitarianism'.\n    \n8.  ^**[^](#fnrefkf66sg490ji)**^\n    \n    Caplan, Bryan (2008) [The totalitarian threat](https://en.wikipedia.org/wiki/Special:BookSources/9780199606504), in Nick Bostrom & Milan M. Ćirković (eds.) *Global Catastrophic Risks*, Oxford: Oxford University Press, pp. 504–519.\n    \n9.  ^**[^](#fnref1s2cbixdbiz)**^\n    \n    Koehler, Arden (2020) [Problem areas beyond 80,000 Hours’ current priorities](https://forum.effectivealtruism.org/posts/xoxbDsKGvHpkGfw9R/problem-areas-beyond-80-000-hours-current-priorities), *Effective Altruism Forum*, June 22, section 'Risks of stable totalitarianism'.\n    \n10.  ^**[^](#fnreffjgm1g1sz5a)**^\n    \n    80,000 Hours (2022) [Our current list of pressing world problems](https://80000hours.org/problem-profiles/), *80,000 Hours*."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "PBRXdf9oY8DfLERhd",
    "name": "Dustin Moskovitz",
    "core": false,
    "slug": "dustin-moskovitz",
    "oldSlugs": null,
    "postCount": 2,
    "description": {
      "markdown": "**Dustin Aaron Moskovitz** (born 22 May 1984) is an American entrepreneur and philanthropist. He is the  co-founder of Facebook and Asana, and the co-founder and current secretary and treasurer of [Good Ventures](https://forum.effectivealtruism.org/tag/good-ventures).\n\nFurther reading\n---------------\n\nCha, Ariana Eunjung (2014) [Cari Tuna and Dustin Moskovitz: young Silicon Valley billionaires pioneer new approach to philanthropy](https://www.washingtonpost.com/business/billionaire-couple-give-plenty-to-charity-but-they-do-quite-a-bit-of-homework/2014/12/26/19fae34c-86d6-11e4-b9b7-b8632ae73d25_print.html), *The Washington Post*, December 26.\n\nKruppa, Miles (2020) [Dustin Moskovitz, the philanthropist conquering Silicon Valley](https://www.ft.com/content/15ffce3b-cee3-44ad-b961-e22459b7b7b2), *Financial Times*, October 2.\n\nRelated entries\n---------------\n\n[Cari Tuna](https://forum.effectivealtruism.org/tag/cari-tuna) | [Good Ventures](https://forum.effectivealtruism.org/tag/good-ventures) | [Open Philanthropy](https://forum.effectivealtruism.org/tag/open-philanthropy)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "YtosWqdr5wZDrW9N6",
    "name": "Doomsday argument",
    "core": false,
    "slug": "doomsday-argument",
    "oldSlugs": null,
    "postCount": 6,
    "description": {
      "markdown": "The **doomsday argument** is the argument that the human species will soon go extinct, because otherwise the present generation of humans will be among the first to ever live, which is antecedently very improbable.\n\nFurther reading\n---------------\n\nBostrom, Nick (2008) [The doomsday argument](http://doi.org/10.1017/s1477175600002943), *Think*, vol. 6, pp. 23–28.\n\nBostrom, Nick (2016) [What is the doomsday argument?](https://www.closertotruth.com/episodes/what-the-doomsday-argument), *Closer to Truth*, October 3.\n\nGott, J. Richard (1993) [Implications of the Copernican principle for our future prospects](http://doi.org/10.1038/363315a0), *Nature*, vol. 363, pp. 315–319.\n\nLeslie, John (1996) [*The End of the World: The Science and Ethics of Human Extinction*](https://en.wikipedia.org/wiki/Special:BookSources/0203007727), London: Routledge.\n\nPoundstone, William (2019) [*The Doomsday Calculation: How an Equation That Predicts the Future Is Transforming Everything We Know about Life and the Universe*](https://en.wikipedia.org/wiki/Special:BookSources/9780316440707), New York: Little, Brown Spark.\n\nRichmond, Alasdair (2006) [The doomsday argument](http://doi.org/10.1111/j.1468-0149.2006.00392.x), *Philosophical Books*, vol. 47, pp. 129–142."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "8KFE4gCjrguHcDfuv",
    "name": "Nick Bostrom",
    "core": false,
    "slug": "nick-bostrom",
    "oldSlugs": null,
    "postCount": 21,
    "description": {
      "markdown": "**Nick Bostrom** (born **Niklas Boström** on 10 March 1973) is a Swedish philosopher. He is a professor at the University of Oxford, the director of the [Future of Humanity Institute](https://forum.effectivealtruism.org/tag/future-of-humanity-institute), and the author of [*Superintelligence: Paths, Dangers, Strategies*](https://forum.effectivealtruism.org/tag/superintelligence-book).\n\nFurther reading\n---------------\n\nKhatchadourian, Raffi (2015) [The doomsday invention: will artificial intelligence bring us utopia or destruction?](https://www.newyorker.com/magazine/2015/11/23/doomsday-invention-artificial-intelligence-nick-bostrom), *The New Yorker*, November 16.\n\nExternal links\n--------------\n\n[Nick Bostrom](https://nickbostrom.com/). Official website.\n\n[Radio Bostrom](https://radiobostrom.com/).  Audio narrations of Bostrom's academic papers.\n\nRelated entries\n---------------\n\n[astronomical waste](https://forum.effectivealtruism.org/tag/astronomical-waste) | [crucial consideration](https://forum.effectivealtruism.org/tag/crucial-consideration) | [existential risk](https://forum.effectivealtruism.org/tag/existential-risk) | [information hazard](https://forum.effectivealtruism.org/tag/information-hazard) | [oracle AI](https://forum.effectivealtruism.org/tag/oracle-ai) | [simulation argument](https://forum.effectivealtruism.org/tag/simulation-argument) | [singleton](https://forum.effectivealtruism.org/tag/singleton) | [technological completion conjecture](https://forum.effectivealtruism.org/tag/technological-completion-conjecture) | [unilateralist's curse](https://forum.effectivealtruism.org/tag/unilateralist-s-curse) | [vulnerable world hypothesis](https://forum.effectivealtruism.org/tag/vulnerable-world-hypothesis)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "HuAmqLKiZqj2fCB3g",
    "name": "Deworm the World Initiative",
    "core": false,
    "slug": "deworm-the-world-initiative",
    "oldSlugs": null,
    "postCount": 2,
    "description": {
      "markdown": "**Deworm the World Initiative** is a program run by [Evidence Action](https://forum.effectivealtruism.org/tag/evidence-action) that advocates for, supports, and evaluates government-run school-based [deworming](https://forum.effectivealtruism.org/tag/deworming) programs.\n\nEvaluation\n----------\n\nAs of May 2022, [GiveWell](https://forum.effectivealtruism.org/tag/givewell) estimates that Deworm the World Initiative can deworm a child at a cost of around $1.^[\\[1\\]](#fn67iqbwg9ypa)^^[\\[2\\]](#fnv20qb2umgfm)^^[\\[3\\]](#fneo8dimj0rvh)^ Deworm the World Initiative was a [GiveWell](https://forum.effectivealtruism.org/tag/givewell) top-rated charity between 2013 and 2022, and is featured in [The Life You Can Save](https://forum.effectivealtruism.org/tag/the-life-you-can-save)'s list of \"best charities\".^[\\[4\\]](#fnhx9bngzkal)^\n\nFurther reading\n---------------\n\nGiveWell (2021) [Evidence Action’s Deworm the World Initiative](https://www.givewell.org/charities/deworm-world-initiative), *GiveWell*, November.\n\nExternal links\n--------------\n\n[Deworm the World Initiative](https://www.evidenceaction.org/dewormtheworld/). Official website.\n\nRelated entries\n---------------\n\n[deworming](https://forum.effectivealtruism.org/tag/deworming) | [Evidence Action](https://forum.effectivealtruism.org/tag/evidence-action)\n\n1.  ^**[^](#fnref67iqbwg9ypa)**^\n    \n    GiveWell (2021) [Evidence Action’s Deworm the World Initiative](https://www.givewell.org/charities/deworm-world-initiative), *GiveWell*, November.\n    \n2.  ^**[^](#fnrefv20qb2umgfm)**^\n    \n    GiveWell (2021) [Our top charities](https://www.givewell.org/charities/top-charities), *GiveWell*, November.\n    \n3.  ^**[^](#fnrefeo8dimj0rvh)**^\n    \n    GiveWell (2021) [2021 GiveWell cost-effectiveness analysis — version 3](https://docs.google.com/spreadsheets/d/1B1fODKVbnGP4fejsZCVNvBm5zvI1jC7DhkaJpFk6zfo/edit#gid=1377543212), *GiveWell*, July 6.\n    \n4.  ^**[^](#fnrefhx9bngzkal)**^\n    \n    The Life You Can Save (2021) [Best charities](https://www.thelifeyoucansave.org/best-charities/), *The Life You Can Save*."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "fy8fpaetjFu9Akg4Z",
    "name": "Evidence Action",
    "core": false,
    "slug": "evidence-action",
    "oldSlugs": null,
    "postCount": 4,
    "description": {
      "markdown": "**Evidence Action** is a nonprofit that implements several evidence-based and cost-effective programs to combat [global poverty](https://forum.effectivealtruism.org/tag/global-poverty).\n\nEvaluation\n----------\n\nEvidence Action's [Deworm the World Initiative](https://forum.effectivealtruism.org/tag/deworm-the-world-initiative) program was a [GiveWell](https://forum.effectivealtruism.org/tag/givewell) top charity between 2013 and 2022. Their [Dispensers for Safe Water](https://forum.effectivealtruism.org/tag/dispensers-for-safe-water) program was a GiveWell [standout charity](https://forum.effectivealtruism.org/tag/givewell#Standout_charities) from 2017 to 2021, when that designation was discontinued. \n\nIn the past, GiveWell also recommended Evidence Action's No Lean Season program, which provided no-interest loans to poor rural households between planting and the major rice harvest in rural northern Bangladesh.^[\\[1\\]](#fnlpasiuxxbnb)^ This recommendation was later discontinued following a disappointing 2017 study and the termination of Evidence Action's partnership with the organization implementing the program.^[\\[2\\]](#fn2c8ec652gfl)^\n\nAs of July 2022, [Open Philanthropy](https://forum.effectivealtruism.org/tag/open-philanthropy) has granted over $96 million to Evidence Action.^[\\[3\\]](#fnqowuh176jm)^\n\n[The Life You Can Save](https://forum.effectivealtruism.org/tag/the-life-you-can-save) features Evidence Action in their list of \"best charities\".^[\\[4\\]](#fnb44izq1l2t4)^\n\nFurther reading\n---------------\n\nGiveWell (2021) [What is the relationship between Evidence Action and Deworm the World Initiative?](https://www.givewell.org/charities/deworm-world-initiative/evidence-action-and-deworm-the-world), *GiveWell*.\n\nRussell, Lauren (2021) [The evidence to policy pipeline: How open policy analysis can transform deworming policy](https://www.evidenceaction.org/the-evidence-to-policy-pipeline-how-open-policy-analysis-can-transform-deworming-policy/), *Evidence Action*, April 1.\n\nExternal links\n--------------\n\n[Evidence Action](https://www.evidenceaction.org/). Official website.\n\n[Apply for a job](https://www.evidenceaction.org/work-with-us/).\n\n[Donate to Evidence Action](https://www.evidenceaction.org/donate/).\n\nRelated entries\n---------------\n\n [Deworm the World Initiative](https://forum.effectivealtruism.org/tag/deworm-the-world-initiative) | [global poverty](https://forum.effectivealtruism.org/tag/global-poverty)\n\n1.  ^**[^](#fnreflpasiuxxbnb)**^\n    \n    GiveWell (2018) [Evidence Action’s No Lean Season](https://www.givewell.org/charities/no-lean-season), *GiveWell*, November (updated June 2019).\n    \n2.  ^**[^](#fnref2c8ec652gfl)**^\n    \n    Hollander, Catherine (2019) [Evidence Action is shutting down No Lean Season](https://blog.givewell.org/2019/06/06/evidence-action-is-shutting-down-no-lean-season/), *The GiveWell Blog*, June 6 (updated 14 January 2020).\n    \n3.  ^**[^](#fnrefqowuh176jm)**^\n    \n    Open Philanthropy (2021) [Grants database: Evidence Action - Evidence Action Beta](https://www.openphilanthropy.org/grants/?q=&organization-name=evidence-action&organization-name=evidence-action-beta), *Open Philanthropy*.\n    \n4.  ^**[^](#fnrefb44izq1l2t4)**^\n    \n    The Life You Can Save (2021) [Best charities](https://www.thelifeyoucansave.org/best-charities/), *The Life You Can Save*."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5sDXCYLtZYdrjCN2H",
    "name": "Deworming",
    "core": false,
    "slug": "deworming",
    "oldSlugs": null,
    "postCount": 18,
    "description": {
      "markdown": "**Deworming** is the administration of anthelmintic drugs to treat humans afflicted with parasitic worms.\n\nParasitic worms (also known as helminths) can cause a variety of health conditions and symptoms of varying severity. The most common parasitic worm infection, ascariasis, is estimated to affect 800 million to 1.1 billion people.^[\\[1\\]](#fngjb34atrlll)^ Experts disagree about whether the health effects of parasitic worms are relatively minor or quite severe.^[\\[2\\]](#fnlnqlox194m)^^[\\[3\\]](#fnh3em4g9y34e)^\n\nParasitic worm infections can be treated through mass drug administration.^[\\[4\\]](#fn99mhzptiex4)^ This method of deworming has low costs and a high level of success in reducing worm loads (though reinfection can be rapid). There is also some evidence that reducing worm loads among children results in substantial increases in future earnings,^[\\[5\\]](#fnz3ccdsjgn7f)^ though some have challenged these findings.^[\\[6\\]](#fn7fdtbbpo4f)^^[\\[7\\]](#fnh5ayeix1dp6)^\n\nDue to the low cost and the probability that it will increase future earnings, [GiveWell](https://forum.effectivealtruism.org/tag/givewell) regards mass drug administration as a priority program^[\\[8\\]](#fnpfoun288fu8)^ and rates donating to deworming charities as having high [expected value](https://forum.effectivealtruism.org/tag/expected-value).^[\\[9\\]](#fn7w2ayt2kwoy)^ Two of GiveWell's top recommended charities, [Deworm the World Initiative](https://forum.effectivealtruism.org/tag/deworm-the-world-initiative) and [SCI Foundation](https://forum.effectivealtruism.org/tag/sci-foundation), focus on deworming through mass drug administration.\n\nFurther reading\n---------------\n\nGiveWell (2013) [Combination deworming (mass drug administration targeting both schistosomiasis and soil-transmitted helminths)](https://www.givewell.org/international/technical/programs/deworming), *GiveWell*, December (updated January 2018).\n\nRelated entries\n---------------\n\n[global health and development](https://forum.effectivealtruism.org/tag/global-health-and-development)\n\n1.  ^**[^](#fnrefgjb34atrlll)**^\n    \n    Centers for Disease Control and Prevention (2013) [Parasites - Soil-transmitted helminths](https://www.cdc.gov/parasites/sth/index.html), *Centers for Disease Control and Prevention* (updated 27 October 2020).\n    \n2.  ^**[^](#fnreflnqlox194m)**^\n    \n    Taylor-Robinson, David C. *et al.* (2019) [Public health deworming programmes for soil-transmitted helminths in children living in endemic areas](https://doi.org/10.1002/14651858.CD000371.pub7), *Cochrane Database of Systematic Reviews*, issue 9, art. no. CD000371.\n    \n3.  ^**[^](#fnrefh3em4g9y34e)**^\n    \n    Croke, Kevin *et al.* (2016) [Does mass deworming affect child nutrition? Meta-analysis, cost-effectiveness, and statistical power](http://doi.org/10.3386/w22382), Working Paper No. 22382, National Bureau of Economic Research.\n    \n4.  ^**[^](#fnref99mhzptiex4)**^\n    \n    GiveWell (2013) [Combination deworming (mass drug administration targeting both schistosomiasis and soil-transmitted helminths)](https://www.givewell.org/international/technical/programs/deworming), *GiveWell*, December (updated January 2018).\n    \n5.  ^**[^](#fnrefz3ccdsjgn7f)**^\n    \n    Hicks, Joan Hamory, Michael Kremer & Edward Miguel (2015) [The case for mass treatment of intestinal helminths in endemic areas](http://doi.org/10.1371/journal.pntd.0004214), *PLoS Neglected Tropical Diseases*.\n    \n6.  ^**[^](#fnref7fdtbbpo4f)**^\n    \n    Humphreys, Macartan (2015) [What has been learned from the deworming replications: A nonpartisan view](http://www.columbia.edu/~mh2245/w/worms.html), *Columbia University*, August 18.\n    \n7.  ^**[^](#fnrefh5ayeix1dp6)**^\n    \n    Jullien, Sophie, David Sinclair & Paul Garner (2016) [The impact of mass deworming programmes on schooling and economic development: An appraisal of long-term studies](http://doi.org/10.1093/ije/dyw283), *International Journal of Epidemiology*, vol. 45, pp. 2140–2153.\n    \n8.  ^**[^](#fnrefpfoun288fu8)**^\n    \n    GiveWell (2009) [Research on programs](https://www.givewell.org/research/research-on-programs#Prioritized_list_of_programs), *GiveWell* (updated April 2021).\n    \n9.  ^**[^](#fnref7w2ayt2kwoy)**^\n    \n    Conley, Sean (2016) [Deworming might have huge impact, but might have close to zero impact](https://blog.givewell.org/2016/07/26/deworming-might-huge-impact-might-close-zero-impact/), *The GiveWell Blog*, July 26."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "zreDs5WmBJp9wcuK6",
    "name": "Development Media International",
    "core": false,
    "slug": "development-media-international",
    "oldSlugs": null,
    "postCount": 2,
    "description": {
      "markdown": "**Development Media International** (**DMI**) is a British organization that produces radio and television programming in developing countries aimed at encouraging the adoption of improved health practices.\n\nEvaluation\n----------\n\n[GiveWell](https://forum.effectivealtruism.org/tag/givewell) rated DMI a [standout charity](https://forum.effectivealtruism.org/tag/givewell#Standout_charities) from 2014 to 2021, when that designation was discontinued.^[\\[1\\]](#fn5fb1gtzg0ru)^ DMI is also featured in [The Life You Can Save](https://forum.effectivealtruism.org/tag/the-life-you-can-save)'s list of \"best charities\".^[\\[2\\]](#fn8053933yn68)^\n\nAs of July 2022, DMI has received $1.1 million in funding from [Open Philanthropy](https://forum.effectivealtruism.org/tag/open-philanthropy).^[\\[3\\]](#fn3wqms75krad)^\n\nFurther reading\n---------------\n\nGiveWell (2021) [Development Media International](https://www.givewell.org/charities/DMI-July-2021-Version), *GiveWell*, July.\n\nHead, Roy (2019) [How to put your own charity to the test](https://forum.effectivealtruism.org/posts/ctL62pQDcSM3p4GWj/roy-head-how-to-put-your-own-charity-to-the-test), *Effective Altruism Global*, October 18.\n\nExternal links\n--------------\n\n[Development Media International](https://www.developmentmedia.net/). Official website.\n\n[Apply for a job](https://developmentmedia.bamboohr.com/jobs/).\n\n[Donate to Development Media International](https://www.developmentmedia.net/donate/). \n\n1.  ^**[^](#fnref5fb1gtzg0ru)**^\n    \n    GiveWell (2021) [Development Media International](https://www.givewell.org/charities/DMI-July-2021-Version), *GiveWell*, July.\n    \n2.  ^**[^](#fnref8053933yn68)**^\n    \n    The Life You Can Save (2021) [Best charities](https://www.thelifeyoucansave.org/best-charities/), *The Life You Can Save*.\n    \n3.  ^**[^](#fnref3wqms75krad)**^\n    \n    Open Philanthropy (2022) [Grants database: Development Media International](https://www.openphilanthropy.org/grants/?q=&organization-name=development-media-international), *Open Philanthropy*."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "rmDb3Mk5iKcNF5LL6",
    "name": "Derek Parfit",
    "core": false,
    "slug": "derek-parfit",
    "oldSlugs": null,
    "postCount": 6,
    "description": {
      "markdown": "**Derek Antony Parfit** (11 December 1942 – 1 January 2017) was a British philosopher. He is the author of *Reasons and Persons* and *On What Matters*.\n\nIn an obituary, [Jeff McMahan](https://forum.effectivealtruism.org/tag/jeff-mcmahan) referred to Parfit as \"one of the most important philosophers of the past half-century and, in the view of many, the single best moral philosopher in more than a century.\"^[\\[1\\]](#fnjbz1fg55snn)^ *Reasons and Persons* is \"widely regarded as the most important work in utilitarian moral philosophy in the twentieth century,\"^[\\[2\\]](#fnq7p76xlje0h)^ and has been described as \"perhaps the most argument-filled book ever to have been written\"^[\\[3\\]](#fnmwe9a762cub)^ and as \"contain\\[ing\\] the highest ratio of important insights per page of any philosophical work.\"^[\\[4\\]](#fnsykq2ucgvab)^\n\nHuman extinction\n----------------\n\nIn the concluding section of the concluding chapter of *Reasons and Persons*, Parfit draws a comparison between three possible outcomes:\n\n1.  Peace.\n2.  A nuclear war that kills 99% of the world's existing population.\n3.  A nuclear war that kills 100%.\n\nEveryone agrees that (3) is worse than (2), and that (2) is worse than (1). But Parfit argues that, although most people believe the difference between (1) and (2) to be greater than the difference between (2) and (3), the reverse is in fact the case. Although many more additional people will die in (2) relative to (1), than in (3) relative to (2), only (3) involves the death of everyone alive, and hence of the species as a whole. (2) kills almost everyone in the present generation, but (3) prevents all future generations from ever being born. Since the vast majority of potential people do not yet exist, according to Parfit (3) represents a far greater loss than (2). Appreciation of this critical insight has motivated many effective altruists to focus on reducing the risk of [human extinction](https://forum.effectivealtruism.org/tag/human-extinction), and of [existential risk](https://forum.effectivealtruism.org/tag/existential-risk) more generally.\n\nEffective altruism\n------------------\n\nTowards the end of his life, Parfit became increasingly interested in [effective altruism](https://forum.effectivealtruism.org/tag/effective-altruism) and supportive of it. In one of his last talks—an event at the Oxford Union organized by [Giving What We Can](https://forum.effectivealtruism.org/tag/giving-what-we-can)—he remarked:^[\\[5\\]](#fnjdcnhj7h32r)^\n\n> I am immensely heartened by the way in which, here at Oxford and some other places—Harvard, Rutgers, elsewhere—, various groups have been started whose aim is to relieve the great suffering and early deaths of some of the world’s 2 billion poorest people. I really admire this, and applaud you for doing it. It is the greatest moral problem, I think, that we rich people are likely to face in our lives, and most likely to act in ways that are seriously wrong.\n\nIn the final pages of the third volume of *On What Matters*, Parfit expressed the intention of writing a final volume applying his views on [metaethics](https://forum.effectivealtruism.org/tag/metaethics) and [normative ethics](https://forum.effectivealtruism.org/tag/normative-ethics) to practical issues, including [global poverty](https://forum.effectivealtruism.org/tag/global-poverty) and [existential risk](https://forum.effectivealtruism.org/tag/existential-risk). Although Parfit died before the book could be written, he arguably inspired others to carry out that project in some form. As Simon Beard, a Senior Research Associate at the [Centre for the Study of Existential Risk](https://forum.effectivealtruism.org/tag/centre-for-the-study-of-existential-risk), writes:^[\\[6\\]](#fnzdy68d75z6k)^\n\n> Derek had hoped to write a 5th book, *On What Matters Volume 4*, in which he would present the theories that he had been working on concerning the ethics of future generations and in which he could finally achieve the goal he had set out to achieve back in 1968, of applying the principles of moral philosophy to real-world problems. He would never complete it. However, by inspiring others to work in Effective Altruism, and giving them sound arguments to support their convictions that people can, and should, do a lot more good than they currently do, this does not seem to matter so much. His work was, ultimately, worthwhile.\n\nParfit's disciples include [Jeff McMahan](https://forum.effectivealtruism.org/tag/jeff-mcmahan) and [Toby Ord](https://forum.effectivealtruism.org/tag/toby-ord).\n\nFurther reading\n---------------\n\nChappell, Richard Yetter (2021) [*Parfit’s Ethics*](http://doi.org/10.1017/9781108582377), Cambridge: Cambridge University Press.\n\nDancy, Jonathan (2020) [Derek Parfit](https://www.thebritishacademy.ac.uk/publishing/memoirs/19/parfit-derek-1942-2017/), *Biographical Memoirs of Fellows of the British Academy*, vol. 19, pp. 37–57.\n\nMacFarquhar, Larissa (2011) [How to be good](https://www.newyorker.com/magazine/2011/09/05/how-to-be-good), *The New Yorker*, September 5.\n\nMulgan, Tim (2013) [Derek Parfit](https://www.bloomsburycollections.com/book/the-bloomsbury-encyclopedia-of-utilitarianism), in James E. Crimmins (ed.) *The Bloomsbury Encyclopedia of Utilitarianism*, London: Bloomsbury Academic, pp. 403–406.\n\nTemkin, Larry & Jimmy Goodrich (2019) [Derek Parfit](https://onlinelibrary.wiley.com/doi/abs/10.1002/9781444367072.wbiee910), in Hugh LaFollette (ed.) *International Encyclopedia of Ethics*, Hoboken, New Jersey: Wiley-Blackwell.\n\nExternal links\n--------------\n\n[Derek Parfit: a bibliography](http://www.stafforini.com/blog/derek-parfit-a-bibliography/). A near-complete list of writings.\n\n[Celebration of Derek Parfit](https://vimeopro.com/user16707643/celebration-of-derek-parfit/page/1).\n\nRelated entries\n---------------\n\n[consequentialism](https://forum.effectivealtruism.org/tag/consequentialism) | [Jeff McMahan](https://forum.effectivealtruism.org/tag/jeff-mcmahan) | [normative ethics](https://forum.effectivealtruism.org/tag/normative-ethics) | [population ethics](https://forum.effectivealtruism.org/tag/population-ethics) | [Toby Ord](https://forum.effectivealtruism.org/tag/toby-ord)\n\n1.  ^**[^](#fnrefjbz1fg55snn)**^\n    \n    McMahan, Jeff (2017) [Derek Parfit (1942-2017)](https://philosophynow.org/issues/119/Derek_Parfit_1942-2017), *Philosophy Now*, April.\n    \n2.  ^**[^](#fnrefq7p76xlje0h)**^\n    \n    Mulgan, Tim (2013) [Derek Parfit](https://www.bloomsburycollections.com/book/the-bloomsbury-encyclopedia-of-utilitarianism), in James E. Crimmins (ed.) *The Bloomsbury Encyclopedia of Utilitarianism*, London: Bloomsbury Academic, p. 404.\n    \n3.  ^**[^](#fnrefmwe9a762cub)**^\n    \n    Baier, Annette C. (1984) [Review of Derek Parfit, *Reasons and Persons*](http://doi.org/10.1111/j.1468-0149.1984.tb01925.x), *Philosophical Books*, vol. 25, p. 220.\n    \n4.  ^**[^](#fnrefsykq2ucgvab)**^\n    \n    Chappell, Richard Yetter (2020) [Parfit’s Ethics (manuscript)](https://www.philosophyetc.net/2020/08/parfits-ethics-manuscript.html), *Philosophy, et cetera*, August 4.\n    \n5.  ^**[^](#fnrefjdcnhj7h32r)**^\n    \n    Parfit, Derek (2015) [Full address](https://www.youtube.com/watch?v=xTUrwO9-B_I), *Oxford Union*, October 10, 1:02.\n    \n6.  ^**[^](#fnrefzdy68d75z6k)**^\n    \n    Beard, Simon (2020) [Parfit bio](https://sjbeard.weebly.com/parfit-bio.html), *Simon Beard's Website*."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "xpH2fAfu6Wn6EXm6i",
    "name": "Demis Hassabis",
    "core": false,
    "slug": "demis-hassabis",
    "oldSlugs": null,
    "postCount": null,
    "description": {
      "markdown": "**Demis Hassabis** (born 27 July 1976) is a British AI researcher and neuroscientist. He is the CEO and co-founder of [DeepMind](https://forum.effectivealtruism.org/tag/deepmind).\n\nFurther reading\n---------------\n\nBurton-Hill, Clemency (2016) [The superhero of artificial intelligence: can this genius keep it in check?](https://www.theguardian.com/technology/2016/feb/16/demis-hassabis-artificial-intelligence-deepmind-alphago), *The Observer*, February 16.\n\nFord, Martin (2018) [*Architects of Intelligence: The Truth about AI from the People Building It*](https://en.wikipedia.org/wiki/Special:BookSources/978-1-78913-151-2), Birmingham: Packt, ch. 8.\n\nHassabis, Demis (2018) [Creativity and AI: the Rothschild Foundation Lecture](https://www.youtube.com/watch?v=d-bvsJWmqlc), *Royal Academy of Arts*, September 7.\n\nRelated entries\n---------------\n\n[DeepMind](https://forum.effectivealtruism.org/tag/deepmind)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "fJZQgnZiuoiDuKmq5",
    "name": "DeepMind",
    "core": false,
    "slug": "deepmind",
    "oldSlugs": null,
    "postCount": 6,
    "description": {
      "markdown": "**DeepMind** is a British AI company. Its mission is to \"solve intelligence, and then use that to solve everything else\".^[\\[1\\]](#fnp9xhui2r9a)^\n\nFurther reading\n---------------\n\nSánchez, Sebastián (2018) [Timeline of DeepMind](https://timelines.issarice.com/wiki/Timeline_of_DeepMind), *Timelines Wiki*, February 8.\n\nWiblin, Robert & Keiran Harris (2019) [DeepMind’s plan to make AI systems robust & reliable, why it’s a core issue in AI design, and how to succeed at AI research](https://80000hours.org/podcast/episodes/pushmeet-kohli-deepmind-safety-research/), *80,000 Hours*, June 3.\n\nRelated entries\n---------------\n\n[artificial intelligence](https://forum.effectivealtruism.org/tag/artificial-intelligence) | [Demis Hassabis](https://forum.effectivealtruism.org/tag/demis-hassabis)\n\nExternal links\n--------------\n\n[DeepMind](https://deepmind.com/). Official website.\n\n[Apply for a job](https://www.deepmind.com/careers).\n\n1.  ^**[^](#fnrefp9xhui2r9a)**^\n    \n    Burton-Hill, Clemency (2016) [The superhero of artificial intelligence: can this genius keep it in check?](https://www.theguardian.com/technology/2016/feb/16/demis-hassabis-artificial-intelligence-deepmind-alphago), *The Observer*, February 16."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "kLwaCPEK2PpQs46vn",
    "name": "Decisive strategic advantage",
    "core": false,
    "slug": "decisive-strategic-advantage",
    "oldSlugs": null,
    "postCount": null,
    "description": {
      "markdown": "A **decisive strategic advantage** a position of strategic superiority sufficient to allow an agent to achieve complete world domination.\n\nFurther reading\n---------------\n\nBostrom, Nick (2014) [*Superintelligence: Paths, Dangers, Strategies*](https://en.wikipedia.org/wiki/Special:BookSources/9780199678112), Oxford: Oxford University Press, ch. 5\n\nChristiano, Paul (2019) [What failure looks like](https://www.lesswrong.com/posts/HBxe6wdjxK239zajf/what-failure-looks-like), *LessWrong*, March 17.\n\nKokotajlo, Daniel (2019) [Soft takeoff can still lead to decisive strategic advantage](https://www.lesswrong.com/posts/PKy8NuNPknenkDY74/soft-takeoff-can-still-lead-to-decisive-strategic-advantage), *LessWrong*, August 23.\n\nTegmark, Max (2017) [*Life 3.0: Being Human in the Age of Artificial Intelligence*](https://en.wikipedia.org/wiki/Special:BookSources/9781101970317), New York: Knopf."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "eHxvmKaeQCJ6EpAXZ",
    "name": "Decision-theoretic uncertainty",
    "core": false,
    "slug": "decision-theoretic-uncertainty",
    "oldSlugs": null,
    "postCount": 5,
    "description": {
      "markdown": "**Decision-theoretic uncertainty** is uncertainty surrounding the correct [decision theory](https://forum.effectivealtruism.org/tag/decision-theory). It is a type of [normative uncertainty](https://forum.effectivealtruism.org/tag/normative-uncertainty-1).\n\nFurther reading\n---------------\n\nMacAskill, William (2016) [Smokers, psychos, and decision-theoretic uncertainty](http://doi.org/10.5840/jphil2016113929), *Journal of Philosophy*, vol. 113, pp. 425–445.\n\nMacaskill, William *et al.* (2021) [The evidentialist’s wager](http://doi.org/10.5840/jphil2021118622), *Journal of Philosophy*, vol. 118, pp. 320–342.\n\nNozick, Robert (1993) [*The Nature of Rationality*](https://en.wikipedia.org/wiki/Special:BookSources/9780691020969), Princeton, New Jersey: Princeton University Press.\n\nSepielli, Andrew (2014) [What to do when you don’t know what to do when you don’t know what to do…](http://doi.org/10.1111/nous.12010), *Noûs*, vol. 48, pp. 521–544.\n\nTitelbaum, Michael G. (2015) [Rationality’s fixed point (or: in defense of right reason)](http://doi.org/10.1093/acprof:oso/9780198722762.003.0009), *Oxford Studies in Epistemology*, vol. 5, pp. 253–294.\n\nRelated entries\n---------------\n\n[alternatives to expected value theory](https://forum.effectivealtruism.org/tag/alternatives-to-expected-value-theory) | [altruistic wager](https://forum.effectivealtruism.org/tag/altruistic-wager) | [decision theory](https://forum.effectivealtruism.org/tag/decision-theory) | [fanaticism](https://forum.effectivealtruism.org/tag/fanaticism) | [moral uncertainty](https://forum.effectivealtruism.org/tag/moral-uncertainty) | [normative uncertainty](https://forum.effectivealtruism.org/tag/normative-uncertainty-1)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "AfybZGoQ4qcPzqe5y",
    "name": "Decision theory",
    "core": false,
    "slug": "decision-theory",
    "oldSlugs": null,
    "postCount": 39,
    "description": {
      "markdown": "**Decision theory** is the study of rational decision-making under conditions of empirical uncertainty.\n\nDecision theory is normative: it provides us with a means of assessing decisions, and guidance on how to make the best decisions. The goal is not necessarily to show how real-life agents actually make decisions, which is explored in behavioral economics and the psychology of decision making.\n\nSuppose that a person has more than one action available to them, that they are [uncertain](https://forum.effectivealtruism.org/tag/credence) about the state of the world, and that they know that different outcomes will result from their action depending on what the true state of the world is. Even once they have assigned values to each of these outcomes, they will need a function to process those value-assigned outcomes to the actions available to them: one that can rank the actions by how good they are. For example, if they think that they should maximize [expected value](https://forum.effectivealtruism.org/tag/expected-value), then the function will rank all of the actions available to them by the amount of value that each of them is expected to produce.\n\nFurther reading\n---------------\n\nOesterheld, Caspar (2017) [A comprehensive list of decision theories](https://casparoesterheld.com/a-comprehensive-list-of-decision-theories/), *The Universe from an Intentional Stance*, June 22.\n\nPeterson, Martin (2017) [*An Introduction to Decision Theory*](https://doi.org/10.1017/9781316585061), 2nd ed., Cambridge: Cambridge University Press.\n\nRelated entries\n---------------\n\n[alternatives to expected value theory](https://forum.effectivealtruism.org/tag/alternatives-to-expected-value-theory) | [altruistic wager](https://forum.effectivealtruism.org/tag/altruistic-wager) | [credence](https://forum.effectivealtruism.org/tag/credence) | [decision theoretic uncertainty](https://forum.effectivealtruism.org/tag/decision-theoretic-uncertainty) | [economics](https://forum.effectivealtruism.org/tag/economics) | [game theory](https://forum.effectivealtruism.org/tag/game-theory) | [risk aversion](https://forum.effectivealtruism.org/tag/risk-aversion) | [sequence vs. cluster thinking](https://forum.effectivealtruism.org/topics/sequence-vs-cluster-thinking)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "u3mq5mQnMy3Y8uGkG",
    "name": "Debunking argument",
    "core": false,
    "slug": "debunking-argument",
    "oldSlugs": null,
    "postCount": 1,
    "description": {
      "markdown": "A **debunking argument** is an argument that appeals to the origin of a belief to undermine its justification.\n\nFurther reading\n---------------\n\nKahane, Guy (2011) [Evolutionary debunking arguments](https://doi.org/10.1111/j.1468-0068.2010.00770.x), *Noûs*, vol. 45, pp. 103–125.\n\nKorman, Daniel Z. (2019) [Debunking arguments](https://doi.org/10.1111/phc3.12638), *Philosophy Compass*, vol. 14, pp. 1–17.\n\nRelated entries\n---------------\n\n[cognitive bias](https://forum.effectivealtruism.org/tag/cognitive-bias)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "mQTWrBKM9WgA5KJig",
    "name": "David Chalmers",
    "core": false,
    "slug": "david-chalmers",
    "oldSlugs": null,
    "postCount": 1,
    "description": {
      "markdown": "**David John Chalmers** (born 20 April 1966) is an Australian philosopher.\n\nFurther reading\n---------------\n\nAzhar, Azeem (2022) [The meaning of life in the metaverse (with David Chalmers)](https://www.listennotes.com/podcasts/azeem-azhars/the-meaning-of-life-in-the-MpjwEbt5TzU/), *Azeem Azhar’s Exponential View*, March 23.\n\nPerry, Lucas (2022) [David Chalmers on *Reality+: Virtual Worlds and the Problems of Philosophy*](https://futureoflife.org/2022/01/26/david-chalmers-on-reality-virtual-worlds-and-the-problems-of-philosophy/), *Future of Life Institute*, January 26.\n\nSosis, Clifford (2016) [David Chalmers](http://www.whatisitliketobeaphilosopher.com/#/david-chalmers/), *What is it like to be a philosopher?*, September 28.  \n*Extended interview, with an emphasis on Chalmers's early life.*\n\nWiblin, Robert, Arden Koehler & Keiran Harris (2019) [David Chalmers on the nature and ethics of consciousness](https://80000hours.org/podcast/episodes/david-chalmers-nature-ethics-consciousness/), *80,000 Hours*, December 16.  \n*Wide-ranging, four-hour-long interview.*\n\nExternal links\n--------------\n\n[David Chalmers](https://consc.net/). Official website.\n\nRelated entries\n---------------\n\n[consciousness research](https://forum.effectivealtruism.org/tag/consciousness-research) | [intelligence explosion](https://forum.effectivealtruism.org/tag/intelligence-explosion) | [philosophy of mind](https://forum.effectivealtruism.org/tag/philosophy-of-mind) | [sentience](https://forum.effectivealtruism.org/tag/sentience-1)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "nkzSuZwYvbkX6AuYj",
    "name": "Cultivated meat",
    "core": false,
    "slug": "cultivated-meat",
    "oldSlugs": [
      "cultured-meat",
      "cultivated-meat",
      "cultured-meat"
    ],
    "postCount": 45,
    "description": {
      "markdown": "**Cultivated meat** (also known as **clean meat** and **cultured meat**) is meat produced from *in vitro* cultivation of animal cells.\n\nFurther reading\n---------------\n\nCargill, Natalie & Keiran Harris (2018) [How exactly clean meat is created & the advances needed to get it into every supermarket, according to food scientist Marie Gibbons](https://80000hours.org/podcast/episodes/marie-gibbons-clean-meat/), *80,000 Hours*, April 10.\n\nHuang, Amy (2020) [Closing gaps in alternative protein science](https://www.youtube.com/watch?v=YJGMeL4mKvk), *EA Student Summit 2020*, October 24.\n\nHumbird, David (2021) [Scale-up economics for cultured meat](https://doi.org/10.1002/bit.27848), *Biotechnology and Bioengineering*, vol. 118, pp. 3239–3250.\n\nSpecht, Liz et al. (2021) [Statement addressing TEA analyses](https://gfi.org/cultivated/tea-statement/), *Good Food Institute*.\n\nExternal links\n--------------\n\n[Cultivated Meat Research Tools Database](https://gfi.org/resource/es-draft-cultivated-meat-research-tools-database/).\n\nRelated entries\n---------------\n\n[animal product alternatives](https://forum.effectivealtruism.org/tag/animal-product-alternatives) | [biotechnology](https://forum.effectivealtruism.org/topics/biotechnology) | [dietary change](https://forum.effectivealtruism.org/tag/dietary-change) | [farmed animal welfare](https://forum.effectivealtruism.org/tag/farmed-animal-welfare) | [meat-eater problem](https://forum.effectivealtruism.org/topics/meat-eater-problem)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "6wkQxvwDZCWKqgvrQ",
    "name": "Cultural lag",
    "core": false,
    "slug": "cultural-lag",
    "oldSlugs": null,
    "postCount": 1,
    "description": {
      "markdown": "**Cultural lag** is the thesis that culture tends to lag behind technology, and that this lag is at the root of many of the world's most pressing problems. The thesis was first formulated explicitly by the American sociologist William Fielding Ogburn (1886-1959).^[\\[1\\]](#fnewq6zklyxl)^\n\nFurther reading\n---------------\n\nRomero Moñivas, Jesús (2007) [Cultural lag](https://doi.org/10.1002/9781405165518.wbeosc198), in George Ritzer (ed.) *The Blackwell Encyclopedia of Sociology*, Oxford: John Wiley & Sons.\n\nRelated entries\n---------------\n\n[cultural evolution](https://forum.effectivealtruism.org/tag/cultural-evolution) | [cultural persistence](https://forum.effectivealtruism.org/tag/cultural-persistence) | [differential progress](/tag/differential-progress)\n\n1.  ^**[^](#fnrefewq6zklyxl)**^\n    \n    Ogburn, William Fielding (1923) *Social Change with Respect to Culture and Original Nature*, New York: B. W. Huebsch."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "9hrj7vzeXFWjWgAQi",
    "name": "Cuban Missile Crisis",
    "core": false,
    "slug": "cuban-missile-crisis",
    "oldSlugs": null,
    "postCount": 4,
    "description": {
      "markdown": "The **Cuban Missile Crisis** was a confrontation between the United States and the Soviet Union during October 1962 over the presence of Soviet nuclear-capable missiles in Cuba.  Arthur Schlesinger, the historian and former Kennedy aide, called it \"the most dangerous confrontation in human history.\"^[\\[1\\]](#fnyt2h3ttk6sc)^\n\nFurther reading\n---------------\n\nBlanton, Thomas, William Burr & Svetlana Savranskaya (2012) [The underwater Cuban Missile Crisis: Soviet submarines and the risk of nuclear war](https://nsarchive2.gwu.edu/NSAEBB/NSAEBB399/), *The National Security Archive*, October 24.\n\nBlight, James G. & Janet M. Lang (2012) [*The Armageddon Letters: Kennedy, Khrushchev, Castro in the Cuban Missile Crisis*](https://en.wikipedia.org/wiki/Special:BookSources/9781442216815), Lanham: Rowman & Littlefield.\n\nChang, Laurence & Peter Kornbluh (eds.) (1992) [*Cuban Missile Crisis, 1962: A Documents Reader*](https://en.wikipedia.org/wiki/Special:BookSources/9781565840447), New York: The New Press.\n\nEllsberg, Daniel (2017) [*The Doomsday Machine: Confessions of a Nuclear War Planner*](https://en.wikipedia.org/wiki/Special:BookSources/9781608196708), New York: Bloomsbury, ch. 13.\n\nKavka, Gregory (1986) [Morality and nuclear politics: lessons from the Missile Crisis](https://en.wikipedia.org/wiki/Special:BookSources/9780847672585), in Avner Cohen & Steven Lee (eds.) *Nuclear Weapons and the Future of Humanity: The Fundamental Questions*, Totowa, New Jersey: Rowman & Allanheld, pp. 233–254.\n\nMcNamara, Robert S. (1992) [One minute to doomsday](https://www.nytimes.com/1992/10/14/opinion/one-minute-to-doomsday.html), *The New York Times*, October 14.\n\nSherwin, Martin J. (2020) [*Gambling with Armageddon: Nuclear Roulette from Hiroshima to the Cuban Missile Crisis, 1945-1962*](https://en.wikipedia.org/wiki/Special:BookSources/9780307266880), New York: Alfred A. Knopf.\n\nSmith, E. Timothy (2010) [Cuban Missile Crisis](https://en.wikipedia.org/wiki/Special:BookSources/9780195334685), in Nigel Young (ed.) *The Oxford International Encyclopedia of Peace*, Oxford: Oxford University Press, pp. 518–521.\n\nSavranskaya, Svetlana (2005) [New sources on the role of Soviet submarines in the Cuban missile crisis](http://doi.org/10.1080/01402390500088312), *Journal of Strategic Studies*, vol. 28, pp. 233–259.\n\nRelated entries\n---------------\n\n[Vasili Arkhipov](https://forum.effectivealtruism.org/tag/vasili-arkhipov) | [nuclear warfare](https://forum.effectivealtruism.org/tag/nuclear-warfare-1) |  [Stanislav Petrov](https://forum.effectivealtruism.org/tag/stanislav-petrov)\n\n1.  ^**[^](#fnrefyt2h3ttk6sc)**^\n    \n    Lloyd, Marion (2002) 'Soviets close to using A-bomb in 1962 crisis, forum is told', *Boston Globe*, October 13, p. A20."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "esfWkmTPD6umRaWoB",
    "name": "Crucial consideration",
    "core": false,
    "slug": "crucial-consideration",
    "oldSlugs": null,
    "postCount": 15,
    "description": {
      "markdown": "A **crucial consideration** is a consideration that warrants a major reassessment of a cause or intervention.\n\nThe concept was introduced by [Nick Bostrom](https://forum.effectivealtruism.org/tag/nick-bostrom) in a 2007 article^[\\[1\\]](#fnxdk9z17b98)^ and applied in subsequent publications.^[\\[2\\]](#fnb5gsslbvkdm)^^[\\[3\\]](#fn39lvhbnzsrd)^\n\nRelated concepts\n----------------\n\nBesides introducing the concept of a crucial consideration, Bostrom introduced two other concepts closely related to it. First, the concept a *crucial consideration component*, or a consideration that is not itself a crucial consideration, but which has the potential to become one when conjoined with additional considerations still unknown.^[\\[2\\]](#fnb5gsslbvkdm)^ As Bostrom writes, a crucial consideration component is \"the kind of thing of which we would say: 'This looks really intriguing, this could be important; I’m not really sure what to make of it at the moment.' On its own, maybe it doesn’t tell us anything, but maybe there’s another piece that, when combined, will somehow yield an important result.\"^[\\[3\\]](#fn39lvhbnzsrd)^\n\nSecond, the concept of a *deliberation ladder*, or a sequence of crucial considerations resulting in successive reassessments of the same cause or intervention.^[\\[2\\]](#fnb5gsslbvkdm)^ Consider, for illustration, an altruist who initially becomes a vegan out of concern for the treatment of animals in factory farms. Later, this person is exposed to the [logic of the larder](https://forum.effectivealtruism.org/tag/logic-of-the-larder) and concludes that consuming animal products is permissible because it increases the total number of animals. Finally, the altruist comes to believe that [farm animal welfare](https://forum.effectivealtruism.org/tag/farmed-animal-welfare) is net negative and reverts to a vegan diet, reasoning that, since demand for animal products increases animals in expectation, it also increases net suffering. Many additional \"deliberation ladders\" can be imagined, related to the impact of meat consumption on the number of animals who feed on other animals, on climate change and its effects on [wild animals](https://forum.effectivealtruism.org/tag/wild-animal-welfare), on public perception of the [moral status](https://forum.effectivealtruism.org/tag/moral-patienthood) of nonhuman animals, and on other considerations.\n\nImplications\n------------\n\nThe potential existence of yet undiscovered crucial considerations raises a very serious challenge for any attempt to do good effectively on a large scale. As Bostrom writes, \"Our noblest and most carefully considered attempts to effect change in the world might well be pushing things further *away* from where they ought to be. Perhaps around the corner lurks some crucial consideration that we have ignored, such that if we thought of it and were able to accord it its due weight in our reasoning, it would convince us that our guiding beliefs and our struggles to date had been orthogonal or worse to the direction that would then come to appear to us as the right one.\"^[\\[4\\]](#fn7ry650sg1w8)^ Such a challenge is particularly serious for [longtermists](https://forum.effectivealtruism.org/tag/longtermism): the additional difficulty associated with trying to influence the [far future](https://forum.effectivealtruism.org/tag/long-term-future), and the greater [neglect](https://forum.effectivealtruism.org/tag/neglectedness) of this area until very recently, strongly suggest that relevant crucial considerations remain undiscovered.\n\nFurther reading\n---------------\n\nBostrom, Nick (2014) [Crucial considerations and wise philanthropy](https://www.effectivealtruism.org/articles/crucial-considerations-and-wise-philanthropy-nick-bostrom/), *Effective Altruism*, July 9.\n\nBostrom, Nick (2016) [Macrostrategy](https://www.youtube.com/watch?v=f9HvMLSD0jo), *Bank of England*, April 11.  \n*Starting at 22:00, discusses crucial considerations and related concepts.*\n\nRelated entries\n---------------\n\n[crux](https://forum.effectivealtruism.org/topics/crux)\n\n1.  ^**[^](#fnrefxdk9z17b98)**^\n    \n    Bostrom, Nick (2007) [Technological revolutions: ethics and policy in the dark](http://doi.org/10.1002/9780470165874.ch10), in Nigel M. de S. Cameron & M. Ellen Mitchell (eds.) *Nanoscale: Issues and Perspectives for the Nano Century*, Hoboken, New Jersey: John Wiley & Sons, pp. 129–152, p. 149.\n    \n2.  ^**[^](#fnrefb5gsslbvkdm)**^\n    \n    Bostrom, Nick (2014) [*Superintelligence: Paths, Dangers, Strategies*](https://en.wikipedia.org/wiki/Special:BookSources/9780199678112), Oxford: Oxford University Press.\n    \n3.  ^**[^](#fnref39lvhbnzsrd)**^\n    \n    Bostrom, Nick (2014) [Crucial considerations and wise philanthropy](https://www.effectivealtruism.org/articles/crucial-considerations-and-wise-philanthropy-nick-bostrom/), *Effective Altruism*, July 9.\n    \n4.  ^**[^](#fnref7ry650sg1w8)**^\n    \n    Bostrom, Nick (2007) [Technological revolutions](http://doi.org/10.1002/9780470165874.ch10), pp. 149-150."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "Fm9Jpm3Z9Y3aezDv7",
    "name": "Cost-benefit analysis",
    "core": false,
    "slug": "cost-benefit-analysis",
    "oldSlugs": null,
    "postCount": 13,
    "description": {
      "markdown": "**Cost-benefit analysis** (**CBA**) is a collection of methods for evaluating the social costs and benefits of alternative social arrangements.\n\nCost-benefit analysis estimates the ratio between the cost and the benefit of the intervention. Interventions with a lower ratio are said to provide a higher return. Cost-benefit analysis can be used in finance or business to refer only to private returns to an intervention, but in the context of effective altruism, it is more relevant to consider social costs and social benefits.\n\nIn cost-benefit analysis, both cost and benefit are measured in monetary terms (in contrast to [cost-effectiveness analysis](https://forum.effectivealtruism.org/tag/cost-effectiveness-analysis), where benefits are expressed in terms of some non-monetary outcome). Costs are generally already monetary. However, the outcomes of a program may be primarily non-monetary (for instance, the program might mostly have impact by improving health). In such cases, a monetary value will be assigned to the non-monetary unit (for instance, a value will be assigned to a year of additional healthy life). Then all non-monetary benefits will be converted into a monetary form, for comparison with costs. The valuation of non-monetary benefits is often a difficult and contentious aspect of cost-benefit analysis.\n\nCost-benefit analysis can be carried out on an average level (looking at the total cost and total impact of an intervention), or at the marginal level (examining the impact of an additional dollar).\n\nFurther reading\n---------------\n\nBronsteen, John, Christopher Buccafusco & Jonathan S. Masur (2012) [Well-being analysis vs. cost-benefit analysis](https://heinonline.org/HOL/Page?handle=hein.journals/duklr62&id=1637&div=&collection=), *Duke Law Journal*, vol. 62, pp. 1603–1689.\n\nHansson, Sven Ove (2019) [Cost-benefit analysis: Philosophical issues](https://doi.org/10.1057/978-1-349-95121-5_2918-1), in *The New Palgrave Dictionary of Economics*, London: Palgrave Macmillan, pp. 2388–2391.\n\nSen, Amartya (2000) [The discipline of cost-benefit analysis](https://doi.org/10.1086/468100), *The Journal of Legal Studies*, vol. 29, pp. 931–952.\n\nWeimer, David L. (2019) [Cost-benefit analysis](https://doi.org/10.1057/978-1-349-95121-5_2918-1), in *The New Palgrave Dictionary of Economics*,  London: Palgrave Macmillan, pp. 2383–2388.\n\nRelated entries\n---------------\n\n[cost-effectiveness](https://forum.effectivealtruism.org/tag/cost-effectiveness) | [cost-effectiveness analysis](https://forum.effectivealtruism.org/tag/cost-effectiveness-analysis) | [economics](https://forum.effectivealtruism.org/tag/economics) | [welfare economics](https://forum.effectivealtruism.org/tag/welfare-economics)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "xRJck3ZPfsv7vxSmi",
    "name": "Cosmopolitanism",
    "core": false,
    "slug": "cosmopolitanism",
    "oldSlugs": null,
    "postCount": 2,
    "description": {
      "markdown": "**Cosmopolitanism** is the view that gives the interests of people of other nationalities a weight equal to those of one’s compatriots.\n\nBecause people in [poorer countries](https://forum.effectivealtruism.org/tag/global-poverty) are more likely than people in richer countries to face severe problems that could be solved at [little cost](https://forum.effectivealtruism.org/tag/cost-effectiveness), it is often a good general heuristic to focus one’s attention on impoverished parts of the world, rather than looking for giving opportunities in one’s local area (assuming one is from a richer country).\n\nFurther reading\n---------------\n\nKleingeld, Pauline & Eric Brown (2002) [Cosmopolitanism](https://plato.stanford.edu/entries/cosmopolitanism/), *Stanford Encyclopedia of Philosophy*, February 23 (updated 17 October 2019).\n\nMacAskill, William & Darius Meissner (2020) [Cosmopolitanism: Expanding the moral circle across geography](https://www.utilitarianism.net/utilitarianism-and-practical-ethics#cosmopolitanism), in 'Utilitarianism and practical ethics', *Utilitarianism*."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "cxnjB5eXPaKJGv4Rv",
    "name": "Corporate animal welfare campaigns",
    "core": false,
    "slug": "corporate-animal-welfare-campaigns",
    "oldSlugs": [
      "corporate-cage-free-campaigns"
    ],
    "postCount": 28,
    "description": {
      "markdown": "**Corporate animal welfare campaigns** attempt to convince retailers, restaurants, manufacturers, foodservice, catering, and hospitality companies to commit to only use higher welfare animal products. Companies that don’t agree to make such a commitment when asked are targeted by social media campaigns, protests, and negative advertising that exposes the cruelty behind the animal products that the company uses or produces. Perhaps to avoid the negative publicity, most companies agree to make commitments without the need of a public campaign.^[\\[1\\]](#fnu0zy0u1aujo)^\n\nSo far, the most common ask has been to stop using eggs that come from chickens kept in cages. Cages prevent chickens from engaging in their natural behaviors. In the U.S. and some parts of Europe, cage-free campaigns have already secured commitments from all the companies that use the most eggs, or even achieved a legislative ban on cages. In these countries, animal advocacy organizations started campaigning to improve the welfare of chickens raised for meat by slowing down their growth, giving them more space, etc.\n\nAccording to research by the [Welfare Footprint Project](https://forum.effectivealtruism.org/topics/welfare-footprint-project), both of these asks substantially decrease hours in pain experienced by farmed chickens,^[\\[2\\]](#fnmmuw1ptvn0r)^^[\\[3\\]](#fn9hg9k8pbiku)^ decreasing chicken suffering by an estimated 30%–60%.^[\\[4\\]](#fngceq8q5r677)^^[\\[5\\]](#fnwqbb5sgu07g)^\n\nAccording to estimates by Šimčikas,^[\\[6\\]](#fnvcgxbd4d6sm)^ corporate campaigns between 2015 and the end of 2018 will improve the welfare of 9 to 120 years of chicken life per dollar spent. Capriati estimated that in 2015-2018, corporate campaigns by [The Humane League](https://forum.effectivealtruism.org/topics/the-humane-league)  \"achieved an outcome roughly as good as 10 hen-years shift from battery cages to aviaries per dollar received.\"^[\\[7\\]](#fnhlnfu84bgle)^ As a result of such high apparent cost-effectiveness, such campaigns have received nearly 60% donations from major animal welfare donors from the effective altruism community, who disclose their donations between 2019 and September 2021.^[\\[8\\]](#fnmk88rhscski)^ \n\nMost of the organizations that pursue corporate campaigns are part of the Open Wing Alliance.^[\\[9\\]](#fnfhyrhrxw04b)^ \n\nFurther reading\n---------------\n\nBollard, Lewis (2016) [Initial grants to support corporate cage-free reforms](https://www.openphilanthropy.org/blog/initial-grants-support-corporate-cage-free-reforms), *Open Philanthropy*, March 31.\n\nŠimčikas, Saulius (2019) [Corporate campaigns affect 9 to 120 years of chicken life per dollar spent](https://forum.effectivealtruism.org/posts/L5EZjjXKdNgcm253H/corporate-campaigns-affect-9-to-120-years-of-chicken-life), *Effective Altruism Forum*, July 8.\n\nRelated entries\n---------------\n\n [effective animal advocacy](https://forum.effectivealtruism.org/tag/effective-animal-advocacy) | [farmed animal welfare](https://forum.effectivealtruism.org/tag/farmed-animal-welfare)\n\n1.  ^**[^](#fnrefu0zy0u1aujo)**^\n    \n    See the “Campaign or Dialogue?” column in [this spreadsheet](https://docs.google.com/spreadsheets/d/1i7YrHgFstsW-ymIjqMKlxbzRDmHsuE0ruUvaHESF4uI/edit?usp=sharing) from [Chicken Watch](https://chickenwatch.org/). The vast majority of commitments were won by dialogue.\n    \n2.  ^**[^](#fnrefmmuw1ptvn0r)**^\n    \n    Schuck-Paim, Cynthia & Wladimir J. Alonso (2021) [Quantifying the welfare impact of the transition to indoor cage-free housing systems](https://welfarefootprint.org/research-projects/laying-hens/), *Welfare Footprint Project*.\n    \n3.  ^**[^](#fnref9hg9k8pbiku)**^\n    \n    Schuck-Paim, Cynthia & Wladimir J. Alonso (2022) [Impact of the better chicken commitment and the adoption of slower-growing breeds on the welfare of broiler chickens](https://welfarefootprint.org/broilers/), *Welfare Footprint Project*.\n    \n4.  ^**[^](#fnrefgceq8q5r677)**^\n    \n    Šimčikas, Saulius (2022) [Comment on “EAA is relatively overinvesting in corporate welfare reforms”](https://forum.effectivealtruism.org/posts/kHdKWmTcS3FfcYAZj/eaa-is-relatively-overinvesting-in-corporate-welfare-reforms), *Effective Altruism Forum*, January 6.\n    \n5.  ^**[^](#fnrefwqbb5sgu07g)**^\n    \n    Indirect effects of welfare reforms have been overviewed in Anthis, Jacy Reese (2017) [Momentum vs. complacency from welfare reforms](https://www.sentienceinstitute.org/foundational-questions-summaries#momentum-vs.-complacency-from-welfare-reforms), in 'A summary of evidence for foundational questions in effective animal advocacy', *Sentience Institute*, June 2 (updated 13 August 2020), and Šimčikas, Saulius (2019) [Indirect effects](https://forum.effectivealtruism.org/posts/L5EZjjXKdNgcm253H/corporate-campaigns-affect-9-to-120-years-of-chicken-life#Indirect_effects), in 'Corporate campaigns affect 9 to 120 years of chicken life per dollar spent', *Effective Altruism Forum*, July 8.\n    \n6.  ^**[^](#fnrefvcgxbd4d6sm)**^\n    \n    Šimčikas, Saulius (2019) [Corporate campaigns affect 9 to 120 years of chicken life per dollar spent](https://forum.effectivealtruism.org/posts/L5EZjjXKdNgcm253H/corporate-campaigns-affect-9-to-120-years-of-chicken-life), *Effective Altruism Forum*, July 8.\n    \n7.  ^**[^](#fnrefhlnfu84bgle)**^\n    \n    Capriati, Marinella (2018) [Cause area report: Corporate campaigns for animal welfare](https://founderspledge.com/research/fp-animal-welfare), Founders Pledge.\n    \n8.  ^**[^](#fnrefmk88rhscski)**^\n    \n    Ozden, James (2021) [Analysis of EA funding within animal welfare from 2019-2021](https://forum.effectivealtruism.org/posts/6H9QGZkdMzDEdKNCX/analysis-of-ea-funding-within-animal-welfare-from-2019-2021-1), *Effective Altruism Forum*, September 27.\n    \n9.  ^**[^](#fnreffhyrhrxw04b)**^\n    \n    For a list of such organizations, see Open Wing Alliance (2022) [Organizations](https://openwingalliance.org/organizations), *Open Wing Alliance*."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "6o3vu25Xtd4ncTWjh",
    "name": "Copenhagen Consensus Center",
    "core": false,
    "slug": "copenhagen-consensus-center",
    "oldSlugs": null,
    "postCount": 3,
    "description": {
      "markdown": "The **Copenhagen Consensus Center** (**CCC**) is a US think-tank. It conducts research on cost-effective solutions to pressing global problems, and advises philanthropists and policymakers on the basis of this research.\n\nHistory\n-------\n\nCCC was founded in 2006 with funding from the Danish government. It grew out of a conference held two years earlier dedicated to answering the question: \"Where should the world invest an additional $50 billion over the next four years to do the most good?\" An expert panel of nine economists compiled a preliminary list of \"major challenges facing humanity\". The list was then narrowed down to include the ten problems judged to be the most pressing. For each problem, an essay was commissioned to an external economist with expertise in the relevant field. This essay described the scope of the problem, considered possible solutions to it, and provided an extensive overview of the existing cost-benefit analyses in the literature. Finally, the expert panel reviewed these essays and ranked the problems. Both the essays by the specialist economists and the justification of the rankings by the expert panel were subsequently published in a book.^[\\[1\\]](#fn7g1wyx3og6c)^ Further Copenhagen Consensus conferences, following the same process and methodology, were held in 2008 and 2012.^[\\[2\\]](#fnpc1z98rz88)^^[\\[3\\]](#fnsxf8rn1pfo)^\n\nCCC and effective altruism\n--------------------------\n\nIt is instructive to consider the ways in which CCC and effective altruism compare in their approach to cause evaluation. CCC's motto is: \"In a world with limited budgets and attention spans, we need to find effective ways to do the most good for the most people.\"^[\\[4\\]](#fn819423amfh4)^ In a similar vein, Lomborg describes the \"core idea\" behind CCC as follows: \"with scarce resources to tackle the problems of the world, prioritization is necessary.\"^[\\[5\\]](#fn1v9e9par75w)^ Back when [GiveWell](https://forum.effectivealtruism.org/tag/givewell) started investigating standout programs, it found CCC to be \"the only case we have seen of an independent panel of experts attempting to identify the most promising philanthropic investments.\"^[\\[6\\]](#fnabe470j01c5)^ One of the commissioned papers for the 2012 conference was co-written by [Toby Ord](https://forum.effectivealtruism.org/tag/toby-ord).^[\\[7\\]](#fn05f0xk20pjn6)^ Some effective altruists have in the past collaborated with CCC.^[\\[8\\]](#fnxv827vylls)^\n\nOn the other hand, there are also a number of important differences between CCC and effective altruism. CCC appears to be short-termist, restricting the space of interventions it is willing to consider to those affecting the present generation. It is also human-centric, entirely neglecting interventions that could benefit non-human animals. These differences may be traceable to CCC's exclusive reliance on [cost-benefit analysis](https://forum.effectivealtruism.org/tag/cost-benefit-analysis) (CBA). As Lomborg acknowledges, \"The ranking in the Copenhagen Consensus is based on a CBA, measuring the costs and benefits to a global community at the relevant prices.\"^[\\[9\\]](#fncvqiimupzz)^ Short-termism and human-centeredness are built into that framework, insofar as the prices are determined exclusively by the preferences of existing humans: non-human animals and unborn people are weighted only to the degree that the present human generation cares about them. By contrast, effective altruism tends to do [cause prioritization](https://forum.effectivealtruism.org/tag/cause-prioritization) by relying on a broader evidence base.\n\nFurther reading\n---------------\n\nGertler, Aaron (2019) [The global priorities of the Copenhagen Consensus](https://forum.effectivealtruism.org/posts/YReJJ8MZdASANojrT/the-global-priorities-of-the-copenhagen-consensus), *Effective Altruism Forum*, January 7.\n\nGiveWell (2010) [Criteria for evaluating programs - 2009-2011](https://www.givewell.org/international/technical/criteria/program-evaluation#CopenhagenConsensus), *GiveWell*, November.\n\nHurford, Peter & Andreas Mogensen (2013) [Smart Development Goals: a promising opportunity to influence aid spending via post-MDGs? An evaluation of “Copenhagen Consensus Centre”](https://downloads.ctfassets.net/dhpcfh1bs3p6/6vdohoHQswAUs2jAB3W9Wx/bde1cbf08829eaee279d914f402060bd/CCC.pdf), Oxford: Giving What We Can.\n\nLomborg, Bjørn (ed.) (2007) [*Solutions for the World’s Biggest Problems: Costs and Benefits*](http://doi.org/10.1017/CBO9780511493560), Cambridge: Cambridge University Press.\n\nLomborg, Bjørn (ed.) (2014) [*How to Spend $75 Billion to Make the World a Better Place*](https://en.wikipedia.org/wiki/Special:BookSources/9781940003030), Lowell, Massachusetts: Copenhagen Consensus Center.\n\nTuna, Cari (2013) [A conversation with the Copenhagen Consensus Center](https://www.goodventures.org/research-and-ideas/conversations/copenhagen-consensus-center-december-16-2013), *Good Ventures*, December 16.\n\nWiblin, Robert & Kristian Rönn (2013) [The Copenhagen Consensus: making a bet on catching a big fish](https://www.givingwhatwecan.org/post/2013/06/the-copenhagen-consensus-making-a-bet-on-catching-a-big-fish/), *The Giving What We Can Blog*, June 14.\n\nExternal links\n--------------\n\n[Copenhagen Consensus Center](https://www.copenhagenconsensus.com). Official website.\n\n[Apply for a job](https://www.copenhagenconsensus.com/careers).\n\n1.  ^**[^](#fnref7g1wyx3og6c)**^\n    \n    Lomborg, Bjørn (ed.) (2004) [*Global Crises, Global Solutions*](https://en.wikipedia.org/wiki/Special:BookSources/9780521606141), Cambridge: Cambride University Press.\n    \n2.  ^**[^](#fnrefpc1z98rz88)**^\n    \n    Lomborg, Bjørn (ed.) (2009) [*Global Crises, Global Solutions*](https://en.wikipedia.org/wiki/Special:BookSources/9780521571218), 2nd ed., Cambridge: Cambridge University Press.\n    \n3.  ^**[^](#fnrefsxf8rn1pfo)**^\n    \n    Lomborg, Bjørn (ed.) (2013) [*Global Problems, Smart Solutions: Costs and Benefits*](http://doi.org/10.1017/CBO9781139600484), Cambridge: Cambridge University Press.\n    \n4.  ^**[^](#fnref819423amfh4)**^\n    \n    Copenhagen Consensus Center (2020) [Our story](https://www.copenhagenconsensus.com/our-story), *Copenhagen Consensus Center*.\n    \n5.  ^**[^](#fnref1v9e9par75w)**^\n    \n    Lomborg, Bjørn (ed.) (2004) [*Global Crises, Global Solutions*](https://en.wikipedia.org/wiki/Special:BookSources/9780521606141), p. 1.\n    \n6.  ^**[^](#fnrefabe470j01c5)**^\n    \n    GiveWell (2020) [Intervention reports](https://www.givewell.org/research/intervention-reports), *GiveWell*, May.\n    \n7.  ^**[^](#fnref05f0xk20pjn6)**^\n    \n    Jamison, Dean T. *et al.* (2013) [Infectious disease, injury, and reproductive health](http://doi.org/10.1017/CBO9781139600484.009), in Bjørn Lomborg (ed.) *Global Problems, Smart Solutions: Costs and Benefits*, Cambridge: Cambridge University Press, pp. 390–438.\n    \n8.  ^**[^](#fnrefxv827vylls)**^\n    \n    Kleňha, Jan (2018) [Policy prioritization in a developed country](https://forum.effectivealtruism.org/posts/rMhZo3anefAPJ9ZKK/policy-prioritization-in-a-developed-country), *Effective Altruism Forum*, March 8.\n    \n9.  ^**[^](#fnrefcvqiimupzz)**^\n    \n    Lomborg, Bjørn (ed.) (2004) [*Global Crises, Global Solutions*](https://en.wikipedia.org/wiki/Special:BookSources/9780521606141), p. 3."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "iycC2QnTTg6XiQo6G",
    "name": "Consequentialism",
    "core": false,
    "slug": "consequentialism",
    "oldSlugs": null,
    "postCount": 16,
    "description": {
      "markdown": "**Consequentialism** is the normative theory according to which the rightness of an act is determined solely by the goodness of its consequences.\n\nCombining consequentialism with welfarism—the view that [well-being](https://forum.effectivealtruism.org/tag/wellbeing) is the only source of value—yields [utilitarianism](https://forum.effectivealtruism.org/tag/utilitarianism), the theory that the morally right act is the one that maximizes well-being. When utilitarianism is further combined with [hedonism](https://forum.effectivealtruism.org/tag/hedonism) as an account of well-being, the result is hedonistic, or classical, utilitarianism—an influential theory held by [Jeremy Bentham](https://forum.effectivealtruism.org/tag/jeremy-bentham), [John Stuart Mill](https://forum.effectivealtruism.org/tag/john-stuart-mill), and Henry Sidgwick, according to which the morally right act is that which maximizes the surplus of happiness over suffering. If instead utilitarianism is combined with a desire-fulfilment account of well-being, the result is preference utilitarianism, which holds that the morally right act is that which maximizes preference satisfaction.\n\nConsequentialism may instead be combined with a non-welfarist axiology. One such theory is pluralistic consequentialism, on which the right act is that which maximizes the overall degree to which various different values—including both well-being and [non well-being sources of value](https://forum.effectivealtruism.org/tag/non-wellbeing-sources-of-value)—are realized.\n\nAnother important difference between consequentialist views is whether the nature of the beneficiary influences how we weight the good. Classical utilitarians, for example, would argue that one unit of pleasure is equally good no matter who experiences it, while prioritarians argue that it would be better if that unit of pleasure was experienced by someone who is relatively worse off.\n\nFurther reading\n---------------\n\nMacAskill, William & Darius Meissner (2020) 'Consequentialism', in [Elements and types of utilitarianism](https://www.utilitarianism.net/types-of-utilitarianism), *Utilitarianism*.\n\nSebo, Jeff & Tyler John (2020) [Consequentialism and nonhuman animals](https://doi.org/10.1093/oxfordhb/9780190905323.013.32), in Douglas W. Portmore (ed.) *The Oxford Handbook of Consequentialism*, New York: Oxford University Press, pp. 563–591.\n\nSinnott-Armstrong, Walter (2003) [Consequentialism](https://plato.stanford.edu/entries/consequentialism/), *Stanford Encyclopedia of Philosophy*, May 20 (updated 3 June 2019).\n\nRelated entries\n---------------\n\n[Derek Parfit](https://forum.effectivealtruism.org/tag/derek-parfit) | [normative ethics](https://forum.effectivealtruism.org/tag/normative-ethics) | [utilitarianism](https://forum.effectivealtruism.org/tag/utilitarianism) | [welfarism](https://forum.effectivealtruism.org/tag/welfarism)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "xQN359f9pDuNJRdjg",
    "name": "Consciousness research",
    "core": false,
    "slug": "consciousness-research",
    "oldSlugs": [
      "consciousness"
    ],
    "postCount": 32,
    "description": {
      "markdown": "**Consciousness research** is research about the subjective quality of experience.\n\nFurther reading\n---------------\n\nAnimal Ethics (2017) [Animal sentience](https://www.animal-ethics.org/sentience-section/animal-sentience/), *Animal Ethics*.\n\nExternal links\n--------------\n\n[Rethink Priorities' publications](https://www.rethinkpriorities.org/publications). Many articles on this topic under the headings 'Capacity for welfare & moral status' and 'Invertebrate welfare'.\n\nRelated entries\n---------------\n\n[axiology](https://forum.effectivealtruism.org/tag/axiology) | [philosophy of mind](https://forum.effectivealtruism.org/tag/philosophy-of-mind) | [Qualia Research Institute](https://forum.effectivealtruism.org/tag/qualia-research-institute) | [valence](https://forum.effectivealtruism.org/tag/valence)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "ZSi2mbSrXJTTr7StZ",
    "name": "Computronium",
    "core": false,
    "slug": "computronium",
    "oldSlugs": null,
    "postCount": null,
    "description": {
      "markdown": "**Computronium** is the arrangement of physical resources optimized for its use as a computing substrate.\n\nFurther reading\n---------------\n\nBostrom, Nick (2014) [*Superintelligence: Paths, Dangers, Strategies*](https://en.wikipedia.org/wiki/Special:BookSources/9780199678112), Oxford: Oxford University Press.\n\nBremermann, Hans J. (1967) [Quantum noise and information](https://projecteuclid.org/ebooks/berkeley-symposium-on-mathematical-statistics-and-probability/Proceedings%20of%20the%20Fifth%20Berkeley%20Symposium%20on%20Mathematical%20Statistics%20and%20Probability,%20Volume%204:%20Biology%20and%20Problems%20of%20Health/chapter/Quantum%20noise%20and%20information/bsmsp/1200513783?tab=ChapterArticleLink), *Proceedings of the Fifth Berkeley Symposium on Mathematical Statistics and Probability*, vol. 4, pp. 15–20.  \n*An attempt to estimate the upper limits of computation.*\n\nLloyd, Seth (2000) [Ultimate physical limits to computation](http://doi.org/10.1038/35023282), *Nature*, vol. 406, pp. 1047–1054.\n\nSandberg, Anders (2018) [Answer to 'Computronium and time dilation and Bremermann’s limit'](https://physics.stackexchange.com/questions/407658/computronium-and-time-dilation-and-bremermanns-limit), *Stack Exchange*, May 23.\n\nRelated entries\n---------------\n\n[hedonium](https://forum.effectivealtruism.org/tag/hedonium) | [universe's resources](https://forum.effectivealtruism.org/tag/universe-s-resources)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "jfLmGNJZLZjcrAxbR",
    "name": "Computation hazard",
    "core": false,
    "slug": "computation-hazard",
    "oldSlugs": null,
    "postCount": null,
    "description": {
      "markdown": "A **computation hazard** (also known as a **metacomputational hazard**^[\\[1\\]](#fnwz4k60pviyl)^^[\\[2\\]](#fn0kmu0i3kxrfl)^) is a risk arising from great computational power.^[\\[3\\]](#fnuhjk5pnx0vn)^\n\nFurther reading\n---------------\n\nLessWrong (2012) [Computation hazard](https://www.lesswrong.com/tag/computation-hazard), *LessWrong Wiki*, October 3.\n\nYudkowsky, Eliezer (2008) [Nonperson predicates](https://www.lesswrong.com/posts/wqDRRx9RqwKLzWt7R/nonperson-predicates), *LessWrong*, December 27.\n\n1.  ^**[^](#fnrefwz4k60pviyl)**^\n    \n    Muehlhauser, Luke (2012) [A taxonomy of Oracle AIs](https://www.lesswrong.com/posts/XddMs9kSGtm6L8522/a-taxonomy-of-oracle-ais), *LessWrong*, March 9.\n    \n2.  ^**[^](#fnref0kmu0i3kxrfl)**^\n    \n    Muehlhauser, Luke (2012) [AI risk & opportunity: Questions we want answered](https://www.lesswrong.com/posts/3w7XHLf8AYRvtoN8b/ai-risk-and-opportunity-questions-we-want-answered), *LessWrong*, April 1.\n    \n3.  ^**[^](#fnrefuhjk5pnx0vn)**^\n    \n    Altair, Alex (2012) [Computation hazards](https://www.lesswrong.com/posts/JLHipmPf55x6cuvsN/computation-hazards), *LessWrong*, June 13."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "hhFr2bWLftFhMQHu9",
    "name": "Superintelligence",
    "core": false,
    "slug": "superintelligence",
    "oldSlugs": null,
    "postCount": 3,
    "description": {
      "markdown": "A **superintelligence** is a cognitive system whose intellectual performance across all relevant domains vastly exceeds that of any human. Forms of superintelligence include [quality superintelligence](https://forum.effectivealtruism.org/topics/quality-superintelligence), [speed superintelligence](https://forum.effectivealtruism.org/topics/speed-superintelligence) and [collective superintelligence](https://forum.effectivealtruism.org/topics/collective-superintelligence).\n\nIf a superintelligence comes to exist, it could conceivably be either a machine (created through substantial progress in [artificial intelligence](https://forum.effectivealtruism.org/tag/artificial-intelligence)) or a biological entity (created through genetic engineering or other human modification).\n\nSince intelligence is the distinctive trait that has enabled humans to develop a civilization and become the dominant species on Earth, the development of much smarter agents than us would arguably be the most significant event in human history.\n\nWhile it is difficult to predict, or even to conceive, what a future in which such agents exist would look like, several philosophers and computer scientists have recently argued that the arrival of superintelligence, particularly machine superintelligence, could pose an [existential risk](https://forum.effectivealtruism.org/tag/ai-risks). On the other hand, if these risks are avoided, a superintelligence could be greatly beneficial, and might enable many of the world’s problems to be solved.\n\nFurther reading\n---------------\n\nBostrom, Nick (2014) [*Superintelligence: Paths, Dangers, Strategies*](https://en.wikipedia.org/wiki/Special:BookSources/9780199678112), Oxford: Oxford University Press.\n\nRelated entries\n---------------\n\n[artificial intelligence](https://forum.effectivealtruism.org/tag/artificial-intelligence) | [collective superintelligence](https://forum.effectivealtruism.org/tag/collective-superintelligence) | [quality superintelligence](https://forum.effectivealtruism.org/tag/quality-superintelligence) | [speed superintelligence](https://forum.effectivealtruism.org/tag/speed-superintelligence)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "GDdzkNztJ2QaBrgeR",
    "name": "Comprehensive AI Services",
    "core": false,
    "slug": "comprehensive-ai-services",
    "oldSlugs": null,
    "postCount": 2,
    "description": {
      "markdown": "**Comprehensive AI services** (**CAIS**) is an approach to understanding [superintelligence](https://forum.effectivealtruism.org/tag/superintelligence) proposed by [Eric Drexler](https://forum.effectivealtruism.org/tag/eric-drexler). In Drexler's words, \"CAIS provides a model of flexible, general intelligence in which agents are a class of service-providing products, rather than a natural or necessary engine of progress in themselves.\"^[\\[1\\]](#fn53a475906n)^ On this approach, advanced AI systems are conceived as \"services\", or systems with an expanding set of capabilities, one of which is the optional capability of agency.\n\nFurther reading\n---------------\n\nAlexander, Scott (2019) [Review of Eric Drexler, *Reframing Superintelligence: Comprehensive AI Services as General Intelligence*](https://slatestarcodex.com/2019/08/27/book-review-reframing-superintelligence/), *Slate Star Codex*, August 27.\n\nBaumann, Tobias (2020) [Summary of Eric Drexler’s work on reframing AI safety](https://s-risks.org/summary-of-eric-drexlers-work-on-reframing-ai-safety/), *Reducing Risks of Future Suffering*, May 21.  \n*A clear bullet-point summary of Drexler 2019.*\n\nDrexler, K. Eric (2019) [Reframing superintelligence: Comprehensive AI services as general intelligence](https://www.fhi.ox.ac.uk/wp-content/uploads/Reframing_Superintelligence_FHI-TR-2019-1.1-1.pdf), technical report #2019-1, Future of Humanity Institute, University of Oxford.\n\nHanson, Robin (2019) [How lumpy AI services?](https://www.overcomingbias.com/2019/02/how-lumpy-ai-services.html), *Overcoming Bias*, February 14.\n\nMcCluskey, Peter (2019) [Drexler on AI risk](http://www.bayesianinvestor.com/blog/index.php/2019/01/30/drexler-on-ai-risk/), *Bayesian Investor Blog*, January 30.\n\nNgo, Richard (2019) [Comments on CAIS](https://www.lesswrong.com/posts/HvNAmkXPTSoA4dvzv/comments-on-cais), *LessWrong*, January 12.\n\nShah, Rohin (2019) [Reframing superintelligence: Comprehensive AI services as general intelligence](https://www.lesswrong.com/posts/x3fNwSe5aWZb5yXEG/reframing-superintelligence-comprehensive-ai-services-as), *LessWrong*, January 8.  \n*A very clear summary of the CAIS approach.*\n\nRelated entries\n---------------\n\n[AI alignment](https://forum.effectivealtruism.org/tag/ai-alignment) | [Eric Drexler](https://forum.effectivealtruism.org/tag/eric-drexler) | [superintelligence](https://forum.effectivealtruism.org/tag/superintelligence) | [tool AI](https://forum.effectivealtruism.org/tag/tool-ai)\n\n1.  ^**[^](#fnref53a475906n)**^\n    \n    Drexler, K. Eric (2019) [Reframing superintelligence: Comprehensive AI services as general intelligence](https://www.fhi.ox.ac.uk/reframing/), technical report #2019-1, Future of Humanity Institute, University of Oxford."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "NpwSZKiN68nrZTnTb",
    "name": "Eric Drexler",
    "core": false,
    "slug": "eric-drexler",
    "oldSlugs": [
      "k-eric-drexler"
    ],
    "postCount": 3,
    "description": {
      "markdown": "**Kim Eric Drexler** (born 25 April 1955) is an American author and engineer, currently Senior Research Fellow at the [Future of Humanity Institute](https://forum.effectivealtruism.org/tag/future-of-humanity-institute).\n\nDrexler is most famous for his pioneering work on [atomically precise manufacturing](https://forum.effectivealtruism.org/tag/atomically-precise-manufacturing), especially as presented in his books *Engines of Creation*,^[\\[1\\]](#fn7xwyc6us0qy)^ a popular introduction to the core ideas, and *Nanosystems*,^[\\[2\\]](#fntfzktinjvyl)^ a comprehensive graduate-level survey of the field (itself based on his MIT doctoral thesis^[\\[3\\]](#fnls58ka5o0xl)^^[\\[4\\]](#fnophkxnz01is)^). More recently, Drexler's research has focused on [AI risk](https://forum.effectivealtruism.org/tag/ai-risks), where he has developed an approach he calls [Comprehensive AI Services](https://forum.effectivealtruism.org/tag/comprehensive-ai-services).^[\\[5\\]](#fnq6zpcqk66x)^ Drexler has also made original contributions to the [Fermi paradox](https://forum.effectivealtruism.org/tag/fermi-paradox),^[\\[6\\]](#fnezrstzwdd9)^ [space colonization](https://forum.effectivealtruism.org/tag/space-colonization),^[\\[7\\]](#fned54p4rc2m)^ distributed computing,^[\\[8\\]](#fn8dc0oy8jq63)^ hypertext publishing,^[\\[9\\]](#fng550odilbe)^ and other fields.^[\\[10\\]](#fn1nsug7t9rs6)^^[\\[11\\]](#fnracaew6jr29)^\n\nFurther reading\n---------------\n\nAmato, Ivan (1991) [The apostle of nanotechnology](http://doi.org/10.1126/science.254.5036.1310), *Science*, vol. 254, pp. 1310–1311.  \n*A short profile of Drexler, focusing on his early work in nanotechnology.*\n\nBassett, Deborah R. (2010) [Drexler, K. Eric](http://sk.sagepub.com/reference/nanoscience/n95.xml), in David Guston (ed.) *Encyclopedia of Nanoscience and Society*, Thousand Oaks, CA: Sage Publications, pp. 168–170.  \n*A brief entry on Drexler, with emphasis on his role in the development of nanotechnology.*\n\nBulletin of the Atomic Scientists (2007) [The stealth threat: An interview with K. Eric Drexler](http://doi.org/10.2968/063001018), *Bulletin of the Atomic Scientists*, vol. 63, pp. 55–58.\n\nDrexler, K. Eric (2013) [*Radical Abundance: How a Revolution in Nanotechnology Will Change Civilization*](https://en.wikipedia.org/wiki/Special:BookSources/9781610391146), New York: PublicAffairs.\n\nEdwards, Steven A. (2006) [*The Nanotech Pioneers: Where Are They Taking Us?*](http://doi.org/10.1002/9783527612086), Weinheim: Wiley-VCH.  \n*A short biography of Drexler is found on pp. 18-21.*\n\nRegis, Ed (1990) [*Great Mambo Chicken and the Transhuman Condition: Science Slightly over the Edge*](https://en.wikipedia.org/wiki/Special:BookSources/0201092581), Reading, MA: Addison-Wesley Publishing Company.\n\nRegis, Ed (2004) [The incredible shrinking man](https://www.wired.com/2004/10/drexler/), *Wired*, October 1.  \n*A profile of Drexler.*\n\n1.  ^**[^](#fnref7xwyc6us0qy)**^\n    \n    Drexler, K. Eric (1986) [*Engines of Creation: The Coming Era of Nanotechnology*](https://en.wikipedia.org/wiki/Special:BookSources/0-385-19973-2), New York: Anchor Books.\n    \n2.  ^**[^](#fnreftfzktinjvyl)**^\n    \n    Drexler, K. Eric (1992) [*Nanosystems: Molecular Machinery, Manufacturing, and Computation*](https://en.wikipedia.org/wiki/Special:BookSources/047157547X), New York: John Wiley & Sons.\n    \n3.  ^**[^](#fnrefls58ka5o0xl)**^\n    \n    Drexler, K. Eric (1991) [*Molecular Machinery and Manufacturing with Applications to Computation*](https://dspace.mit.edu/handle/1721.1/27999), PhD thesis, Massachusetts Institute of Technology.\n    \n4.  ^**[^](#fnrefophkxnz01is)**^\n    \n    See also Drexler, K. Eric (1981) [Molecular engineering: An approach to the development of general capabilities for molecular manipulation](http://doi.org/10.1073/pnas.78.9.5275), *Proceedings of the National Academy of Sciences*, vol. 78, pp. 5275–5278.\n    \n5.  ^**[^](#fnrefq6zpcqk66x)**^\n    \n    Drexler, K. Eric (2019) [Reframing superintelligence: Comprehensive AI services as general intelligence](https://www.fhi.ox.ac.uk/wp-content/uploads/Reframing_Superintelligence_FHI-TR-2019-1.1-1.pdf), Future of Humanity Institute, University of Oxford.\n    \n6.  ^**[^](#fnrefezrstzwdd9)**^\n    \n    Sandberg, Anders, K. Eric Drexler & Toby Ord (2018) [Dissolving the Fermi paradox](http://arxiv.org/abs/1806.02404), *arXiv*, June 6.\n    \n7.  ^**[^](#fnrefed54p4rc2m)**^\n    \n    Drexler, K. Eric & N. J. May (1979) [High performance solar sails and related reflecting devices](http://doi.org/10.2514/6.1979-1418), *4th Conference on Space Manufacturing Facilities Princeton University*, American Institute of Aeronautics and Astronautics.\n    \n8.  ^**[^](#fnref8dc0oy8jq63)**^\n    \n    Miller, Mark S. & K. Eric Drexler (1988) 'Markets and computation: Agoric open systems', in B. A. Huberman (ed.) [*The Ecology of Computation*](https://en.wikipedia.org/wiki/Special:BookSources/0444703756), Amsterdam: North-Holland, pp. 133–176.\n    \n9.  ^**[^](#fnrefg550odilbe)**^\n    \n    Drexler (1986) [*Engines of Creation*](https://en.wikipedia.org/wiki/Special:BookSources/0-385-19973-2)*,* ch. 14.\n    \n10.  ^**[^](#fnref1nsug7t9rs6)**^\n    \n    Drexler, K. Eric (2009) [How to understand everything (and why)](http://web.archive.org/web/20190428194036/metamodern.com/2009/05/17/how-to-understand-everything-and-why/), *Metamodern*, May 17.\n    \n11.  ^**[^](#fnrefracaew6jr29)**^\n    \n    Drexler, K. Eric (2009) [How to learn about everything](http://web.archive.org/web/20160306022816/http://metamodern.com/2009/05/27/how-to-learn-about-everything), *Metamodern*, May 27."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "9uJXgzGZnXxBC4pDa",
    "name": "Compound existential risk",
    "core": false,
    "slug": "compound-existential-risk",
    "oldSlugs": null,
    "postCount": 2,
    "description": {
      "markdown": "A **compound existential risk** (also known as a **combination existential risk**^[\\[1\\]](#fnuwelfopicnl)^ and as an **intermediate existential risk**^[\\[2\\]](#fn3e80v29nbqb)^) is an [existential risk](https://forum.effectivealtruism.org/tag/existential-risk) arising from two or more distinct events.\n\nOwen Cotton-Barratt, Max Daniel and [Anders Sandberg](https://forum.effectivealtruism.org/tag/anders-sandberg) illustrate the concept of a (non-existential) compound risk with the following historical incident:^[\\[3\\]](#fnl4tyw93g0v)^\n\n> the deadliest accident in aviation history occurred when two planes collided on an airport runway; this was only possible because a previous terrorist attack on another airport had caused congestion due to rerouted planes, which disabled the prevention measure of using separate routes for taxiing and takeoff.\n\nThe events whose conjunction constitute the compound existential risk may, but need not, themselves be existential risks. For example, [climate change](https://forum.effectivealtruism.org/tag/climate-change) may pose a small existential risk in and of itself but, in addition, pose a compound risk by increasing international tensions and in turn triggering an [AI race](https://forum.effectivealtruism.org/tag/ai-race).^[\\[1\\]](#fnuwelfopicnl)^\n\nFurther reading\n---------------\n\nBrennan, Ozy (2019) [Combination existential risks](https://thingofthings.wordpress.com/2019/01/14/combination-existential-risks/), *Thing of Things*, January 14.  \n*Introduces the concept of a combination existential risk and presents a preliminary taxonomy.*\n\nRelated entries\n---------------\n\n[conjunctive vs. disjunctive risk models](https://forum.effectivealtruism.org/topics/conjunctive-vs-disjunctive-risk-models) | [existential risk](https://forum.effectivealtruism.org/tag/existential-risk) | [existential risk factor](https://forum.effectivealtruism.org/tag/existential-risk-factor)\n\n1.  ^**[^](#fnrefuwelfopicnl)**^\n    \n    Brennan, Ozy (2019) [Combination existential risks](https://thingofthings.wordpress.com/2019/01/14/combination-existential-risks/), *Thing of Things*, January 14.\n    \n2.  ^**[^](#fnref3e80v29nbqb)**^\n    \n    Ćirković, Milan M., Anders Sandberg & Nick Bostrom (2010) [Anthropic shadow: observation selection effects and human extinction risks](http://doi.org/10.1111/j.1539-6924.2010.01460.x), *Risk Analysis*, vol. 30, pp. 1495–1506.\n    \n3.  ^**[^](#fnrefl4tyw93g0v)**^\n    \n    Cotton-Barratt, Owen, Max Daniel & Anders Sandberg (2020) [Defence in depth against human extinction: prevention, response, resilience, and why they all matter](http://doi.org/10.1111/1758-5899.12786), *Global Policy*, vol. 11, pp. 271-282, p. 279."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "GEPQBNvGSnP9sqMY5",
    "name": "Collective superintelligence",
    "core": false,
    "slug": "collective-superintelligence",
    "oldSlugs": [
      "collective-superintelligence",
      "collective-superintelligence"
    ],
    "postCount": 1,
    "description": {
      "markdown": "A **collective superintelligence** is a system composed of a large number of smaller intellects such that the system’s overall performance across many very general domains vastly outstrips that of any current cognitive system.^[\\[1\\]](#fnk429ba1w7s)^\n\nFurther reading\n---------------\n\nBostrom, Nick (2014) [*Superintelligence: Paths, Dangers, Strategies*](https://en.wikipedia.org/wiki/Special:BookSources/9780199678112), Oxford: Oxford University Press.\n\nNgo, Richard (2020a) [AGIs as collectives](https://www.alignmentforum.org/posts/HekjhtWesBWTQW5eF/agis-as-populations), *AI Alignment Forum*, May 22.\n\nNgo, Richard (2020b) [AI safety from first principles: superintelligence](https://www.alignmentforum.org/s/mzgtmmTKKn5MuCzFJ/p/eG3WhHS8CLNxuH6rT), *AI Alignment Forum*, September 28.\n\nRelated entries\n---------------\n\n[quality superintelligence](https://forum.effectivealtruism.org/tag/quality-superintelligence) | [speed superintelligence](https://forum.effectivealtruism.org/tag/speed-superintelligence) | [superintelligence](https://forum.effectivealtruism.org/tag/superintelligence)\n\n1.  ^**[^](#fnrefk429ba1w7s)**^\n    \n    Bostrom, Nick (2014) [*Superintelligence: Paths, Dangers, Strategies*](https://en.wikipedia.org/wiki/Special:BookSources/9780199678112), Oxford: Oxford University Press, pp. 54–56."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5MpGXvigxcCzdkt6P",
    "name": "Cognitive enhancement",
    "core": false,
    "slug": "cognitive-enhancement",
    "oldSlugs": [
      "cognitive-enhancement",
      "cognitive-enhancement"
    ],
    "postCount": 25,
    "description": {
      "markdown": "**Cognitive enhancement** is the amplification or extension of core mental capacities through improvement or augmentation of internal or external information processing systems.\n\nFurther reading\n---------------\n\nBostrom, Nick & Anders Sandberg (2009) [Cognitive enhancement: methods, ethics, regulatory challenges](http://doi.org/10.1007/s11948-009-9142-5), *Science and Engineering Ethics*, vol. 15, pp. 311–341.\n\nBostrom, Nick (2014) [*Superintelligence: Paths, Dangers, Strategies*](https://en.wikipedia.org/wiki/Special:BookSources/978-0-19-967811-2), Oxford: Oxford University Press.\n\nCinel, Caterina, Davide Valeriani & Riccardo Poli (2019) [Neurotechnologies for human cognitive augmentation: Current state of the art and future prospects](http://doi.org/10.3389/fnhum.2019.00013), *Frontiers in Human Neuroscience*, vol. 13.\n\nColzato, Lorenza S. (ed.) (2017) [*Theory-Driven Approaches to Cognitive Enhancement*](http://doi.org/10.1007/978-3-319-57505-6), Cham: Springer International Publishing.\n\nDresler, Martin *et al.* (2019) [Hacking the brain: Dimensions of cognitive enhancement](http://doi.org/10.1021/acschemneuro.8b00571), *ACS Chemical Neuroscience*, vol. 10, pp. 1137–1148.\n\nRelated entries\n---------------\n\n[cognitive decline](https://forum.effectivealtruism.org/topics/cognitive-decline) | [evolution heuristic](https://forum.effectivealtruism.org/tag/evolution-heuristic) | [transhumanism](https://forum.effectivealtruism.org/tag/transhumanism)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "Direa63BcP6pjAcFh",
    "name": "Cluelessness",
    "core": false,
    "slug": "cluelessness",
    "oldSlugs": null,
    "postCount": 44,
    "description": {
      "markdown": "**Cluelessness** is radical uncertainty about the long-term effects of our actions.\n\nSimple versus complex cluelessness\n----------------------------------\n\nAll actions we take have huge effects on the future. One way of seeing this is by considering identity-altering actions. Imagine that Amy passes her friend on the street and they stop to chat. Amy and her friend will now be on a different trajectory than they would have been otherwise. They will interact with different people, at a different time, in a different place, or in a different way than if they hadn’t paused. This will eventually change the circumstances of a conception event such that a different person will now be born because they paused to speak on the street. Now, when the person who is conceived takes actions, Amy will be causally responsible for those actions and their effects. She is also causally responsible for all the effects flowing from those effects.\n\nThis is an example of ***simple*** **cluelessness**, which isn't generally considered problematic. In the above example, Amy has no reason to believe that the many consequences that would follow from pausing would be better than the many consequences that follow from not pausing. Amy has *evidential symmetry* between the two following claims:\n\n*   Pausing to chat would have catastrophic effects for humanity\n*   Not pausing to chat would have catastrophic effects for humanity\n\nAnd similarly, Amy has evidential symmetry between the two following claims:\n\n*   Pausing to chat would have miraculous effects for humanity\n*   Not pausing to chat would have miraculous effects for humanity\n\n(The example assumes that there is nothing particularly special about this chat — eg. Amy and her friend are not chatting about starting a nuclear war or influencing AI policy.)\n\nBy *evidential symmetry* between two actions it is meant that, though massive value or disvalue could come from a given action, these effects could equally easily, and in precisely analogous ways, result from the relevant alternative actions. In the previous scenario, it was assumed that each of the possible people that will be born are as likely as each other to be the next Norman Borlaug. And each of the possible people are as likely as each other to be the next Joseph Stalin.\n\nSo this situation is not problematic; the possible effects, though they are huge, cancel out precisely in an expected value estimate.\n\nCluelessness is problematic, however, in situations where there is no evidential symmetry. For a pair of actions (act one and act two), **complex cluelessness** obtains when:\n\n*   There are reasons to think that the effects of act one would systematically ^[\\[1\\]](#fn6yb492fcwj8)^ tend to be substantially better than those of act two;\n*   There are reasons to think that the effects of act two would systematically tend to be substantially better than those of act one;\n*   It is unclear how to weigh up these reasons against one another.\n\nFor example, there are some reasons to think that the long-term effects of a marginally higher economic growth rate would be good—for example, via driving more patient and pro-social attitudes. This would mean that taking action to increase economic growth could have much better effects than not taking the action. We have some reasons to think that the long-term effects of a marginally higher economic growth rate would be bad —for example, via increased carbon emissions leading to climate change. This would mean that not taking the action that increases economic growth could be a much better idea. It is not immediately obvious that one of these is better than the other, but we also cannot say they have equal expected value. That would need either evidential symmetry, or a very detailed expected value estimate.\n\nSome authors claim that complex cluelessness implies that we should be very skeptical of interventions whose claim to cost-effectiveness is through their direct, proximate effects. As Benjamin Todd and others have argued, the long-term effects of these actions probably dominate.^[\\[2\\]](#fn8j6rdm3ki7)^ But we do not know what the long-term effects of many interventions are or just how good or bad they will be.\n\nActions we take today have indirect long-term effects, and they seem to dominate over the direct near-term effects. In the absence of evidential symmetry, these long-term effects cannot be ignored. So it seems that those concerned about future generations have to justify interventions via their long-term effects, rather than their proximate ones.\n\nFurther reading\n---------------\n\nGreaves, Hilary (2020) [Evidence, cluelessness, and the long term](https://forum.effectivealtruism.org/posts/LdZcit8zX89rofZf3/evidence-cluelessness-and-the-long-term-hilary-greaves), *Effective Altruism Forum*, November 1.\n\nMogensen, Andreas (2020) [Maximal cluelessness](https://doi.org/10.1093/pq/pqaa021), *The Philosophical Quarterly*, vol. 71, pp. 141–162.\n\nSchubert, Stefan (2022) [Against cluelessness: pockets of predictability](https://stefanfschubert.com/blog/2022/5/18/against-cluelessness-pockets-of-predictability), *Stefan Schubert’s Blog*, May 18.\n\nTarsney, Christian (2022) [The epistemic challenge to longtermism](https://globalprioritiesinstitute.org/wp-content/uploads/Tarsney-Epistemic-Challenge-to-Longtermism.pdf), GPI Working Paper No. 3-2022, Global Priorities Institute.\n\nRelated entries\n---------------\n\n[accidental harm](https://forum.effectivealtruism.org/tag/accidental-harm) | [alternatives to expected value theory](https://forum.effectivealtruism.org/tag/alternatives-to-expected-value-theory) | [crucial consideration](https://forum.effectivealtruism.org/topics/crucial-consideration) | [](https://forum.effectivealtruism.org/topics/crucial-consideration) [expected value](https://forum.effectivealtruism.org/tag/expected-value) | [forecasting](https://forum.effectivealtruism.org/tag/forecasting) | [indirect long-term effects](https://forum.effectivealtruism.org/tag/indirect-long-term-effects) | [long-range forecasting](https://forum.effectivealtruism.org/tag/long-range-forecasting) | [model uncertainty](https://forum.effectivealtruism.org/tag/model-uncertainty) | [value of information](https://forum.effectivealtruism.org/tag/value-of-information)\n\n1.  ^**[^](#fnref6yb492fcwj8)**^\n    \n    An explanation of what is meant by ‘systematically’ can be found in section 5 of Greaves, Hilary (2016) [Cluelessness](http://doi.org/10.1093/arisoc/aow018), *Proceedings of the Aristotelian Society*, vol. 116, pp. 311–339.\n    \n2.  ^**[^](#fnref8j6rdm3ki7)**^\n    \n    Todd, Benjamin (2017) [Longtermism: the moral significance of future generations](https://80000hours.org/articles/future-generations/), *80,000 Hours*, October."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "sYwHfprY3qxc5ZG3r",
    "name": "Charity Entrepreneurship",
    "core": false,
    "slug": "charity-entrepreneurship",
    "oldSlugs": null,
    "postCount": 57,
    "description": {
      "markdown": "**Charity Entrepreneurship** is a non-profit organization that conducts research and offers training programs aimed at creating high-impact charities. It is an example of a [charity incubator](https://forum.effectivealtruism.org/tag/charity-incubation). To date, the cause areas their research and incubation activities have mostly focused on are [global health and development](https://forum.effectivealtruism.org/tag/global-health-and-development), [animal welfare](https://forum.effectivealtruism.org/tag/animal-welfare-1), and \"EA meta\",^[\\[1\\]](#fnrrhcpgov9j)^ though they've also incubated charities focused on human subjective [wellbeing](https://forum.effectivealtruism.org/tag/wellbeing) and [climate change](https://forum.effectivealtruism.org/tag/climate-change).\n\nFunding\n-------\n\nAs of July 2022, Charity Entrepreneurship has received nearly $2.4 million in funding from [Open Philanthropy](https://forum.effectivealtruism.org/topics/open-philanthropy),^[\\[2\\]](#fnoi4wiuj1c4r)^^[\\[3\\]](#fn2q28j6s1qt)^ over $1 million from [Effective Altruism Funds](https://forum.effectivealtruism.org/topics/effective-altruism-funds),^[\\[4\\]](#fn0w1qrvjpyzz)^^[\\[5\\]](#fn9alrdfxy89t)^^[\\[6\\]](#fnl5sr6xaz62r)^^[\\[7\\]](#fnt37c872uo8p)^^[\\[8\\]](#fneuakr86g1er)^^[\\[9\\]](#fn60gme3gtxi8)^ and $470,000 from the [Future Fund](https://forum.effectivealtruism.org/topics/future-fund).^[\\[10\\]](#fnmb554l7g84)^ \n\nFurther reading\n---------------\n\nCharity Entrepreneurship (2021) [About us](https://www.charityentrepreneurship.com/about-us.html), *Charity Entrepreneurship*.\n\nSavoie, Joey (2019) [Charity entrepreneurship](https://www.effectivealtruism.org/articles/ea-global-2018-charity-entrepreneurship/), *Effective Altruism*, February 22.\n\nSavoie, Joey, Patrick Stadler & Antonia Shann (2021) [*How to Launch a High-Impact Nonprofit*](https://en.wikipedia.org/wiki/Special:BookSources/979-8784119001), 2nd ed.\n\nExternal links\n--------------\n\n[Charity Entrepreneurship](https://www.charityentrepreneurship.com/). Official website.\n\n[Apply for a job](https://www.charityentrepreneurship.com/jobs).\n\n[Donate to Charity Entrepreneurship](https://www.charityentrepreneurship.com/donate/).\n\nRelated entries\n---------------\n\n[charity incubation](https://forum.effectivealtruism.org/tag/charity-incubation) | [Charity Science Foundation](https://forum.effectivealtruism.org/tag/charity-science-foundation) | [entrepreneurship](https://forum.effectivealtruism.org/tag/entrepreneurship)\n\n1.  ^**[^](#fnrefrrhcpgov9j)**^\n    \n    Savoie, Joey & Vaidehi Agarwalla (2021) [Why EA meta, and the top 3 charity ideas in the space](https://forum.effectivealtruism.org/posts/TcefyQHfYpAKvD6jq/why-ea-meta-and-the-top-3-charity-ideas-in-the-space), *Effective Altruism Forum*, January 6.\n    \n2.  ^**[^](#fnrefoi4wiuj1c4r)**^\n    \n    Open Philanthropy (2022) [Grants database: Charity Entrepreneurship](https://www.openphilanthropy.org/grants/?q=&organization-name=charity-entrepreneurship), *Open Philanthropy*.\n    \n3.  ^**[^](#fnref2q28j6s1qt)**^\n    \n    Open Philanthropy (2022) [Grants database: Charity Science](https://www.openphilanthropy.org/grants/?q=&organization-name=charity-science), *Open Philanthropy*.\n    \n4.  ^**[^](#fnref0w1qrvjpyzz)**^\n    \n    Animal Welfare Fund (2018) [December 2018: Animal Welfare Fund grants](https://funds.effectivealtruism.org/funds/payouts/december-2018-animal-welfare-fund-grants), *Effective Altruism Funds*, December.\n    \n5.  ^**[^](#fnref9alrdfxy89t)**^\n    \n    Effective Altruism Infrastructure Fund (2018) [November 2018: EA Meta Fund grants](https://funds.effectivealtruism.org/funds/payouts/november-2018-ea-meta-fund-grants), *Effective Altruism Funds*, November.\n    \n6.  ^**[^](#fnrefl5sr6xaz62r)**^\n    \n    Effective Altruism Infrastructure Fund (2019) [November 2019: EA Meta Fund grants](https://funds.effectivealtruism.org/funds/payouts/november-2019-ea-meta-fund-grants), *Effective Altruism Funds*, November.\n    \n7.  ^**[^](#fnreft37c872uo8p)**^\n    \n    Effective Altruism Infrastructure Fund (2020) [March 2020: EA Meta Fund Grants](https://funds.effectivealtruism.org/funds/payouts/march-2020-ea-meta-fund-grants), *Effective Altruism Funds*, March.\n    \n8.  ^**[^](#fnrefeuakr86g1er)**^\n    \n    Effective Altruism Infrastructure Fund (2020) [November 2020: EA Infrastructure Fund grants](https://funds.effectivealtruism.org/funds/payouts/november-2020-ea-infrastructure-fund-grants), *Effective Altruism Funds*, November.\n    \n9.  ^**[^](#fnref60gme3gtxi8)**^\n    \n    Effective Altruism Infrastructure Fund (2022) [September-December 2021: EA Infrastructure Fund](https://funds.effectivealtruism.org/funds/payouts/september-december-2021-ea-infrastructure-fund), *Effective Altruism Funds*, July.\n    \n10.  ^**[^](#fnrefmb554l7g84)**^\n    \n    Future Fund (2022) [Our grants and investments: Charity Entrepreneurship](https://ftxfuturefund.org/our-grants/?_organization_name=charity-entrepreneurship), *Future Fund*."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "zSRskeMdpG22sBjzN",
    "name": "Certificate of impact",
    "core": false,
    "slug": "certificate-of-impact",
    "oldSlugs": [
      "certificates-of-impact",
      "certificates-of-impact",
      "certificates-of-impact"
    ],
    "postCount": 33,
    "description": {
      "markdown": "A **certificate of impact** (also known as an **impact certificate**) is a kind of altruistic instrument at the center of a funding model proposed by [Paul Christiano](https://forum.effectivealtruism.org/tag/paul-christiano). Certificates of impact attempt to harness the benefits of the price system in altruistic contexts, where prices are usually unavailable.\n\nOn this model, altruistic work receives some or all of its funding after completion rather than beforehand. Once an individual or organization completes work with a positive social impact, they can apply for a certificate of impact. They can then sell this certificate to another organization or individual. Following the sale, the new certificate holder can claim credit for the impact of the project, and the organization that carried out the project must acknowledge that they have sold its impact.\n\nThis scheme has been proposed and tested by some members of the effective altruism community. They argue that the scheme is better than funding incomplete projects, because it allows for payments by results rather than by effort, which better incentivizes those running the project to do a good job. They also argue that awarding certificates of impact might be simpler and clearer than pre-funding projects.\n\nRelated models\n--------------\n\nRecently, a number of related funding models have been proposed.\n\nIn early May 2021, the organization Noora Health—which implements educational programs for mothers of newborns in South Asia—launched a non-fungible token (NFT) which may in some respects be regarded as a certificate of impact. The auction opened with a list price of $2.5 million, and computer scientist and tech entrepreneur Paul Graham—a long-time supporter of the organization—placed the winning bid of ETH 1337, at the time worth $5.23 million.^[\\[1\\]](#fnpi5f0vtnmcs)^^[\\[2\\]](#fnu0im1k0qj3c)^ A difference between this use of NFTs and certificates of impact as conceived by Christiano is that those bidding in the NFT auction are paying for the prospect of future impact, whereas an impact purchase is a transaction involving the transfer of past impact.\n\nIn late July, a group of authors in collaboration with Vitalik Buterin proposed a model called *retroactive public goods funding*.^[\\[3\\]](#fn0v2jt9yqaph)^ The model consists of a decentralized autonomous organization (DAO), called \"the Results Oracle\", that funds projects considered to have high social value. The funding is done retrospectively rather than prospectively, by assessing the value of the project after it has been completed. Once the Results Oracle evaluates a project, it can send the reward to the person or group responsible for the project or, alternatively, it can use the funds to establish a price floor for a token associated with the project. As the authors note, rewarding via a project token in effect creates a [prediction market](https://forum.effectivealtruism.org/tag/prediction-markets) for the amount of funding the Results Oracle will decide to allocate to the project, and allows the same project to be funded multiple times, or by other sources besides the Results Oracle.\n\nFurther reading\n---------------\n\ncasebash (2020) [Making impact purchases viable](https://forum.effectivealtruism.org/posts/AMQg4hCRGFXaHzvsd/making-impact-purchases-viable), *Effective Altruism Forum*, April 17.\n\nChristiano, Paul (2014) [Certificates of impact](https://rationalaltruist.com/2014/11/15/certificates-of-impact/), *Rational Altruist*, November 15.\n\nChristiano, Paul & Katja Grace (2015a) [Why certificates?](https://impactpurchase.org/why-certificates/), *The Impact Purchase*.  \n*An explanation of the benefits of certificates of impact.*\n\nChristiano, Paul & Katja Grace (2015b) [Certificates of impact](https://impactpurchase.org/certificates-of-impact/), *The Impact Purchase*.\n\nHoffman, Ben (2016) [Minimum viable impact purchases](http://benjaminrosshoffman.com/minimum-viable-impact-purchases/), *Compass Rose*, August 29.\n\nKuhn, Ben (2015) [I just sold half of a blog post](https://www.benkuhn.net/impact-purchase/), *Ben Kuhn’s Blog*, April.  \n*A good intuitive description of the idea of impact purchase.*\n\nLinsefors, Linda (2020) [The case for impact purchase | part 1](https://forum.effectivealtruism.org/posts/7iptwuSyzDzxsEY5z/the-case-for-impact-purchase-or-part-1), *Effective Altruism Forum*, April 14.\n\nRose, Eli (2020) [Do impact certificates help if you’re not sure your work is effective?](https://forum.effectivealtruism.org/posts/k8JnovPNiQHQQyvGu/do-impact-certificates-help-if-you-re-not-sure-your-work-is), *Effective Altruism Forum*, February 12.\n\nRelated entries\n---------------\n\n[effective altruism funding](https://forum.effectivealtruism.org/tag/effective-altruism-funding) | [markets for altruism](https://forum.effectivealtruism.org/tag/markets-for-altruism) | [prize](https://forum.effectivealtruism.org/tag/prize)\n\n1.  ^**[^](#fnrefpi5f0vtnmcs)**^\n    \n    Noora Health (2021) [Save thousands of lives](https://opensea.io/assets/0x495f947276749ce646f68ac8c248420045cb7b5e/96773753706640817147890456629920587151705670001482122310561805592519359070209), *OpenSea*.\n    \n2.  ^**[^](#fnrefu0im1k0qj3c)**^\n    \n    Graham, Paul (2021) [An NFT that saves lives](http://paulgraham.com/nft.html), *Paul Graham’s Website*, May.\n    \n3.  ^**[^](#fnref0v2jt9yqaph)**^\n    \n    Wang, Jinglan *et al.* (2021) [Retroactive public goods funding](https://medium.com/ethereum-optimism/retroactive-public-goods-funding-33c9b7d00f0c), *Ethereum Optimism Blog*, July 20."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "uf2cTQ4KhffmYsM3G",
    "name": "Paul Christiano",
    "core": false,
    "slug": "paul-christiano",
    "oldSlugs": null,
    "postCount": 12,
    "description": {
      "markdown": "**Paul Christiano** is an American [AI safety](https://forum.effectivealtruism.org/tag/ai-safety) researcher. Christiano runs the [Alignment Research Center](https://forum.effectivealtruism.org/tag/alignment-research-center), and is a research associate at the [Future of Humanity Institute](https://forum.effectivealtruism.org/tag/future-of-humanity-institute), a board member at [Ought](https://forum.effectivealtruism.org/tag/ought) and a technical advisor for [Open Philanthropy](https://forum.effectivealtruism.org/tag/open-philanthropy). Previously, he ran the language model alignment team at [OpenAI](https://forum.effectivealtruism.org/tag/openai).\n\nFurther reading\n---------------\n\nChristiano, Paul (2021) [AMA: Paul Christiano, alignment researcher](https://www.alignmentforum.org/posts/7qhtuQLCCvmwCPfXK/ama-paul-christiano-alignment-researcher), *AI Alignment Forum*, April 28.\n\nNgo, Richard (2020) [EA reading list: Paul Christiano](https://forum.effectivealtruism.org/posts/wfvAgFgdJEf9w7ZFb/ea-reading-list-paul-christiano), *Effective Altruism Forum*, August 4.\n\nNguyen, Chi (2020) [My understanding of Paul Christiano’s iterated amplification AI safety research agenda](https://forum.effectivealtruism.org/posts/2ZeHrfJr9uHHJ2e8J/my-understanding-of-paul-christiano-s-iterated-amplification), *Effective Altruism Forum*, August 15.\n\nRice, Issa (2018a) [List of discussions between Paul Christiano and Wei Dai](https://causeprioritization.org/List_of_discussions_between_Paul_Christiano_and_Wei_Dai), *Cause Prioritization Wiki*.\n\nRice, Issa (2018b) [List of discussions between Eliezer Yudkowsky and Paul Christiano](https://causeprioritization.org/List_of_discussions_between_Eliezer_Yudkowsky_and_Paul_Christiano), *Cause Prioritization Wiki*.\n\nWiblin, Robert & Keiran Harris (2018) [Dr Paul Christiano on how OpenAI is developing real solutions to the \"AI alignment problem\", and his vision of how humanity will progressively hand over decision-making to AI systems](https://80000hours.org/podcast/episodes/paul-christiano-ai-alignment-solutions/), *80,000 Hours*, October 2.\n\nExternal links\n--------------\n\n[Paul Christiano](https://paulfchristiano.com/). Official website.\n\n[Paul Christiano](https://forum.effectivealtruism.org/users/paul_christiano). [Effective Altruism Forum](https://forum.effectivealtruism.org/tag/effective-altruism-forum-1) account."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "bX3po4ANFEBvLM4vt",
    "name": "Ought",
    "core": false,
    "slug": "ought",
    "oldSlugs": null,
    "postCount": 15,
    "description": {
      "markdown": "**Ought** is a research lab that develops mechanisms for delegating open-ended thinking to advanced machine learning systems.\n\nFunding\n-------\n\nAs of July 2022, Ought has received $5 million in funding from the [Future Fund](https://forum.effectivealtruism.org/topics/future-fund),^[\\[1\\]](#fnl1lzvmsb9e)^ over $3.1 million from [Open Philanthropy](https://forum.effectivealtruism.org/topics/open-philanthropy),^[\\[2\\]](#fnem9a5l3zfsh)^ nearly $800,000 in funding from the [Survival and Flourishing](https://forum.effectivealtruism.org/topics/survival-and-flourishing) Fund,^[\\[3\\]](#fnvecsuf3mh58)^^[\\[4\\]](#fnhmtg486slys)^ and $60,000 from [Effective Altruism Funds](https://forum.effectivealtruism.org/topics/effective-altruism-funds).^[\\[5\\]](#fntv8ffs8y0h)^^[\\[6\\]](#fne016fv5yctq)^\n\nFurther reading\n---------------\n\nStuhlmüller, Andreas & Jungwon Byun (2022) [Ought’s theory of change](https://forum.effectivealtruism.org/posts/raFAKyw7ofSo9mRQ3/ought-s-theory-of-change), *Effective Altruism Forum*, April 11.\n\nExternal links\n--------------\n\n[Ought](https://ought.org/). Official website.\n\n[Apply for a job](https://ought.org/careers).\n\n[Donate to Ought](https://ought.org/donate).\n\n1.  ^**[^](#fnrefl1lzvmsb9e)**^\n    \n    Future Fund (2022) [Our grants and investments: Ought](https://ftxfuturefund.org/all-grants/?_organization_name=ought), *Future Fund*.\n    \n2.  ^**[^](#fnrefem9a5l3zfsh)**^\n    \n    Open Philanthropy (2022) [Grants database: Ought](https://www.openphilanthropy.org/grants/?q=&organization-name=ought), *Open Philanthropy*.\n    \n3.  ^**[^](#fnrefvecsuf3mh58)**^\n    \n    Survival and Flourishing Fund (2018) [SFF-2019-Q4 S-process recommendations announcement](https://survivalandflourishing.fund/sff-2019-q4-recommendations), *Survival and Flourishing Fund*. \n    \n4.  ^**[^](#fnrefhmtg486slys)**^\n    \n    Survival and Flourishing Fund (2020) [SFF-2021-H2 S-process recommendations announcement](https://survivalandflourishing.fund/sff-2021-h2-recommendations), *Survival and Flourishing Fund*.\n    \n5.  ^**[^](#fnreftv8ffs8y0h)**^\n    \n    Long-Term Future Fund (2018) [November 2018: Long-Term Future Fund grants](https://funds.effectivealtruism.org/funds/payouts/november-2018-long-term-future-fund-grants), *Effective Altruism Funds*, November. \n    \n6.  ^**[^](#fnrefe016fv5yctq)**^\n    \n    Long-Term Future Fund (2019) [August 2019: Long-Term Future Fund grants and recommendations](https://funds.effectivealtruism.org/funds/payouts/august-2019-long-term-future-fund-grants-and-recommendations), *Effective Altruism Funds*, August."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "v4gzbpLGCKaRABXPr",
    "name": "OpenAI",
    "core": false,
    "slug": "openai",
    "oldSlugs": null,
    "postCount": 15,
    "description": {
      "markdown": "**OpenAI** is an [AI safety](https://forum.effectivealtruism.org/tag/ai-safety) research and deployment company based in San Francisco.\n\nFunding\n-------\n\nIn March 2017 OpenAI received a grant of $30 million from [Open Philanthropy](https://forum.effectivealtruism.org/topics/open-philanthropy).^[\\[1\\]](#fnsduxt6tuuy)^\n\nExternal links\n--------------\n\n[OpenAI](https://openai.com/). Official website.\n\n[Apply for a job](https://openai.com/careers/).\n\nRelated entries\n---------------\n\n[AI safety](https://forum.effectivealtruism.org/tag/ai-safety) | [Anthropic](https://forum.effectivealtruism.org/tag/anthropic)\n\n1.  ^**[^](#fnrefsduxt6tuuy)**^\n    \n    Open Philanthropy (2022) [Grants database: Open AI](https://www.openphilanthropy.org/grants/?q=&organization-name=openai), *Open Philanthropy*."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "oNiQsBHA3i837sySD",
    "name": "AI safety",
    "core": false,
    "slug": "ai-safety",
    "oldSlugs": null,
    "postCount": 126,
    "description": {
      "markdown": "**AI safety** is the study of ways to reduce risks posed by [artificial intelligence](https://forum.effectivealtruism.org/tag/artificial-intelligence).\n\nAI safety as a career\n---------------------\n\n[80,000 Hours](https://forum.effectivealtruism.org/tag/80-000-hours)' medium-depth investigation rates technical AI safety research a \"priority path\"—among the most promising career opportunities the organization has identified so far.^[\\[1\\]](#fnoy0q6nfb1j)^^[\\[2\\]](#fnle8osyeymtm)^\n\nFurther reading\n---------------\n\nGates, Vael (2022) [Resources I send to AI researchers about AI safety](https://forum.effectivealtruism.org/posts/8sAzgNcssH3mdb8ya/resources-i-send-to-ai-researchers-about-ai-safety), *Effective Altruism Forum*, June 13.\n\nKrakovna, Victoria (2017) [Introductory resources on AI safety research](https://vkrakovna.wordpress.com/2016/02/28/introductory-resources-on-ai-safety-research/), *Victoria Krakovna's Blog*, October 19.  \n*A list of readings on AI safety.*\n\nNgo, Richard (2019) [Disentangling arguments for the importance of AI safety](https://forum.effectivealtruism.org/posts/LprnaEj3uhkmYtmat/disentangling-arguments-for-the-importance-of-ai-safety), *Effective Altruism Forum*, January 21.\n\nRelated entries\n---------------\n\n[AI alignment](https://forum.effectivealtruism.org/tag/ai-alignment) | [AI interpretability](https://forum.effectivealtruism.org/topics/ai-interpretability) | [AI risk](https://forum.effectivealtruism.org/tag/ai-risk) | [cooperative AI](https://forum.effectivealtruism.org/topics/cooperative-ai-1) \n\n1.  ^**[^](#fnrefoy0q6nfb1j)**^\n    \n    Todd, Benjamin (2018) [The highest impact career paths our research has identified so far](https://80000hours.org/articles/high-impact-careers/), *80,000 Hours*, August 12.\n    \n2.  ^**[^](#fnrefle8osyeymtm)**^\n    \n    Todd, Benjamin (2021) [AI safety technical research](https://80000hours.org/career-reviews/ai-safety-researcher/), *80,000 Hours*, October."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "34whaWCHQD5ydpxgu",
    "name": "Centre for the Study of Existential Risk",
    "core": false,
    "slug": "centre-for-the-study-of-existential-risk",
    "oldSlugs": null,
    "postCount": 24,
    "description": {
      "markdown": "The **Centre for the Study of Existential Risk** (**CSER**) is a research center at the University of Cambridge dedicated to the study and mitigation of [existential risks](https://forum.effectivealtruism.org/tag/existential-risk).\n\nHistory\n-------\n\nCSER was founded in 2012 by Huw Price, [Jaan Tallinn](https://forum.effectivealtruism.org/tag/jaan-tallinn), and Lord Martin Rees.^[\\[1\\]](#fnv2ky8ntba1)^\n\nFunding\n-------\n\nAs of June 2022, CSER has received over $200,000 in funding from the [Survival and Flourishing](https://forum.effectivealtruism.org/topics/survival-and-flourishing) Fund.^[\\[2\\]](#fnyzvnc9qxwj)^^[\\[3\\]](#fnj97l9i0x8r)^\n\nFurther reading\n---------------\n\nMaas, Matthijs (2022) [A primer & some reflections on recent CSER work](https://forum.effectivealtruism.org/posts/WJZAc6fTYNbb5DeAW/a-primer-and-some-reflections-on-recent-cser-work-eab-talk), *Effective Altruism Forum*, April 12.\n\nExternal links\n--------------\n\n[Centre for the Study of Existential Risk](https://www.cser.ac.uk/). Official website.\n\n[Apply for a job](https://www.cser.ac.uk/about-us/careers/).\n\nRelated entries\n---------------\n\n[existential risk](https://forum.effectivealtruism.org/tag/existential-risk) | [Future of Humanity Institute](https://forum.effectivealtruism.org/tag/future-of-humanity-institute) | [Leverhulme Center for the Future of Intelligence](https://forum.effectivealtruism.org/tag/leverhulme-center-for-the-future-of-intelligence)\n\n1.  ^**[^](#fnrefv2ky8ntba1)**^\n    \n    University of Cambridge (2012) [Humanity’s last invention and our uncertain future](https://www.cam.ac.uk/research/news/humanitys-last-invention-and-our-uncertain-future), *University of Cambridge*, November 25.\n    \n2.  ^**[^](#fnrefyzvnc9qxwj)**^\n    \n    Survival and Flourishing Fund (2018) [SFF-2019-Q4 S-process recommendations announcement](https://survivalandflourishing.fund/sff-2019-q4-recommendations), *Survival and Flourishing Fund*. \n    \n3.  ^**[^](#fnrefj97l9i0x8r)**^\n    \n    Survival and Flourishing Fund (2020) [SFF-2021-H1 S-process recommendations announcement](https://survivalandflourishing.fund/sff-2021-h1-recommendations), *Survival and Flourishing Fund*."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "4FDyuFrgeFEtvnhqh",
    "name": "Centre for the Governance of AI",
    "core": false,
    "slug": "centre-for-the-governance-of-ai",
    "oldSlugs": null,
    "postCount": 15,
    "description": {
      "markdown": "The **Centre for the Governance of AI** (**GovAI**) is an [AI governance](https://forum.effectivealtruism.org/topics/ai-governance) research center.\n\nHistory\n-------\n\nGovAI was founded in 2018, as part of the [Future of Humanity Institute](https://forum.effectivealtruism.org/tag/future-of-humanity-institute). In June 2021, it became an independent organization.^[\\[1\\]](#fnq391t0l6ufm)^\n\nFunding\n-------\n\nAs of July 2022, GovAI has received nearly $3 million in funding from [Open Philanthropy](https://forum.effectivealtruism.org/tag/open-philanthropy)^[\\[2\\]](#fnt14nlg7nqw8)^, over $600,000 from the [Survival and Flourishing](https://forum.effectivealtruism.org/topics/survival-and-flourishing) Fund, ^[\\[3\\]](#fnp8o46htwmss)^^[\\[4\\]](#fnnzrd0bloxt)^ and over $170,000 from [Effective Altruism Funds](https://forum.effectivealtruism.org/topics/effective-altruism-funds).^[\\[5\\]](#fn1816esk6hik)^\n\nFurther reading\n---------------\n\nDafoe, Allan (2018) [AI governance: a research agenda](https://www.fhi.ox.ac.uk/wp-content/uploads/GovAI-Agenda.pdf), Centre for the Governance of AI, Future of Humanity Institute, University of Oxford.\n\nMonrad, Joshua & Mojmír Stehlík (2019) [An Interview with Ben Garfinkel, Governance of AI Program Researcher](https://thepolitic.org/an-interview-with-ben-garfinkel-governance-of-ai-program-researcher/), *The Politic*, June 20.\n\nPerry, Lucas (2019) [AI Alignment Podcast: On the governance of AI with Jade Leung](https://futureoflife.org/2019/07/22/on-the-governance-of-ai-with-jade-leung/), *Future of Life Institute*, July 22.\n\nWiblin, Robert & Keiran Harris (2018) [Prof Allan Dafoe on trying to prepare the world for the possibility that ai will destabilise global politics](https://80000hours.org/podcast/episodes/allan-dafoe-politics-of-ai/), *80,000 Hours*, May 18.\n\nExternal links\n--------------\n\n[Centre for the Governance of AI](https://governance.ai/). Official website.\n\n[Apply for a job](https://www.governance.ai/opportunities/open-positions).\n\nRelated entries\n---------------\n\n[AI governance](https://forum.effectivealtruism.org/topics/ai-governance) | [AI safety](https://forum.effectivealtruism.org/tag/ai-safety) | [artificial intelligence](https://forum.effectivealtruism.org/tag/artificial-intelligence) | [Windfall Clause](https://forum.effectivealtruism.org/topics/windfall-clause)\n\n1.  ^**[^](#fnrefq391t0l6ufm)**^\n    \n    Centre for the Governance of AI (2021) [Centre for the Governance of AI](https://governance.ai), *Centre for the Governance of AI*, June.\n    \n2.  ^**[^](#fnreft14nlg7nqw8)**^\n    \n    Open Philanthropy (2022) [Grants database: Centre for the Governance of AI](https://www.openphilanthropy.org/grants/?q=&organization-name=centre-for-the-governance-of-ai), *Open Philanthropy.*\n    \n3.  ^**[^](#fnrefp8o46htwmss)**^\n    \n    Survival and Flourishing Fund (2020a) [SFF-2021-H1 S-process recommendations announcement](https://survivalandflourishing.fund/sff-2021-h1-recommendations), *Survival and Flourishing Fund*. \n    \n4.  ^**[^](#fnrefnzrd0bloxt)**^\n    \n    Survival and Flourishing Fund (2020b) [SFF-2021-H2 S-process recommendations announcement](https://survivalandflourishing.fund/sff-2021-h2-recommendations), *Survival and Flourishing Fund*.\n    \n5.  ^**[^](#fnref1816esk6hik)**^\n    \n    Long-Term Future Fund (2021) [July 2021: Long-Term Future Fund grants](https://funds.effectivealtruism.org/funds/payouts/july-2021-long-term-future-fund-grants), *Effective Altruism Funds*, July."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "HcrmBi99Mk5jwiYC2",
    "name": "Centre for Enabling EA Learning & Research",
    "core": false,
    "slug": "centre-for-enabling-ea-learning-and-research",
    "oldSlugs": null,
    "postCount": 19,
    "description": {
      "markdown": "The **Centre for Enabling EA Learning & Research** (also known as the **EA Hotel**) is a nonprofit based in Blackpool that provides serviced accommodation and board to effective altruists.\n\nFunding\n-------\n\nAs of June 2022, the Centre for Enabling EA Learning & Research has received over $80,000 in funding from the [Survival and Flourishing](https://forum.effectivealtruism.org/topics/survival-and-flourishing) Fund.^[\\[1\\]](#fno1k6lrzsrna)^ \n\nFurther reading\n---------------\n\nAlexander, Scott (2018) [Practically-a-book review: EA Hotel](https://slatestarcodex.com/2018/08/20/practically-a-book-review-ea-hotel/), *Slate Star Codex*, August 21.\n\nThe Economist (2018) [Daytrippers and utilitarians](https://www.economist.com/britain/2018/09/06/effective-altruists-aim-to-change-the-world-from-a-blackpool-hotel), *The Economist*, September 8.\n\nExternal links\n--------------\n\n[Centre for Enabling EA Learning & Research](https://ceealar.org/). Official website.\n\n[Centre for Enabling EA Learning & Research](https://forum.effectivealtruism.org/users/ceaalar). [Effective Altruism Forum](https://forum.effectivealtruism.org/tag/effective-altruism-forum-1) account.\n\n1.  ^**[^](#fnrefo1k6lrzsrna)**^\n    \n    Survival and Flourishing Fund (2020) [SFF-2021-H1 S-process recommendations announcement](https://survivalandflourishing.fund/sff-2021-h1-recommendations), *Survival and Flourishing Fund*."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "JHuxBxXqhEixEYuvs",
    "name": "Center for Security and Emerging Technology",
    "core": false,
    "slug": "center-for-security-and-emerging-technology",
    "oldSlugs": null,
    "postCount": 3,
    "description": {
      "markdown": "The **Center for Security and Emerging Technology** (**CSET**) is a [think tank](https://forum.effectivealtruism.org/tag/think-tanks) dedicated to policy analysis at the intersection of national and international security and emerging technologies. CSET's founder and director is Jason Gaverick Matheny.^[\\[1\\]](#fn40s03czjvz6)^ CSET is the largest [AI policy](https://forum.effectivealtruism.org/tag/ai-governance) research center in the United States.^[\\[2\\]](#fn2nq2fsdo3st)^\n\nHistory\n-------\n\nCSET was established in January 2019 with a $55 million grant from [Open Philanthropy](https://forum.effectivealtruism.org/tag/open-philanthropy).^[\\[3\\]](#fno8uhbt0sms)^ Its mission is to study the security impacts of emerging technologies, support the academic work in security and technology studies, and deliver nonpartisan analysis to the policy community. For its first two years, CSET plans to focus on the intersection of security and artificial intelligence, particularly on national competitiveness, talent and knowledge flows and relationships with other technologies.\n\nAs of July 2022, CSET has grown to more than 50 full-time staff. Thanks to an increase by more than $50 million of their original Open Philanthropy grant,^[\\[4\\]](#fn9wbpdwd526p)^ their total available funding for the following five years is in excess of $100 million.^[\\[5\\]](#fniao12pck9u)^\n\nEvaluation\n----------\n\nCSET is one of the four organizations recommended by [Founders Pledge](https://forum.effectivealtruism.org/tag/founders-pledge) in their cause report on safeguarding the [long-term future](https://forum.effectivealtruism.org/tag/long-term-future).^[\\[6\\]](#fn89r8f21snda)^\n\nFurther reading\n---------------\n\nCenter for Security and Emerging Technology (2020) [About us](https://cset.georgetown.edu/about-us/), *Center for Security and Emerging Technology*.\n\nGeorgetown University (2019a) [Largest U.S. center on artificial intelligence, policy comes to Georgetown](https://www.georgetown.edu/news/largest-u-s-center-on-artificial-intelligence-policy-comes-to-georgetown/), *Georgetown University*, February 28.\n\nGeorgetown University (2019b) [Q&A with Jason Matheny, founding director of the new Center for Security and Emerging Technology](https://www.georgetown.edu/news/qa-with-jason-matheny-founding-director-of-the-center-for-security-and-emerging-technology/), *Georgetown University*, February 28.\n\nWiblin, Robert & Keiran Harris (2019) [The new 30-person research group in DC investigating how emerging technologies could affect national security](https://80000hours.org/podcast/episodes/helen-toner-on-security-and-emerging-technology/), *80,000 Hours*, July 17.  \n*An interview with Helen Toner, CSET's Director of Strategy.*\n\nExternal links\n--------------\n\n[Center for Security and Emerging Technology](https://cset.georgetown.edu/). Official website.\n\n[Apply for a job](https://cset.georgetown.edu/careers/).\n\nRelated entries\n---------------\n\n[AI governance](https://forum.effectivealtruism.org/topics/ai-governance) | [think tanks](https://forum.effectivealtruism.org/tag/think-tanks) \n\n1.  ^**[^](#fnref40s03czjvz6)**^\n    \n    Anderson, Nick (2019) [Georgetown launches think tank on security and emerging technology](https://www.washingtonpost.com/local/education/georgetown-launches-think-tank-on-security-and-emerging-technology/2019/02/27/d6dabc62-391f-11e9-a2cd-307b06d0257b_story.html), *The Washington Post*, February 28.\n    \n2.  ^**[^](#fnref2nq2fsdo3st)**^\n    \n    Institute for Technology Law & Policy (2019) [Georgetown launches new $55 million center on security & emerging technology](http://web.archive.org/web/20190630060206/http://georgetowntech.org/news-fullposts/2019/2/27/february-27-2019-georgetown-launches-new-55-million-center-on-security-amp-emerging-technologies), *Institute for Technology Law & Policy*, February 28.\n    \n3.  ^**[^](#fnrefo8uhbt0sms)**^\n    \n    Muehlhauser, Luke (2019) [Georgetown University — Center for Security and Emerging Technology](https://www.openphilanthropy.org/giving/grants/georgetown-university-center-security-and-emerging-technology), *Open Philanthropy*.\n    \n4.  ^**[^](#fnref9wbpdwd526p)**^\n    \n    Open Philanthropy (2022) [Grants database: Center for Security and Emerging Technology](https://www.openphilanthropy.org/grants/?q=&organization-name=center-for-security-and-emerging-technology), *Open Philanthropy.*\n    \n5.  ^**[^](#fnrefiao12pck9u)**^\n    \n    Center for Security and Emerging Technology (2021) [New grant agreement boosts CSET Funding to more than $100 million over five years](https://cset.georgetown.edu/article/new-grant-agreement-boosts-cset-funding-to-more-than-100-million-over-five-years/), *Center for Security and Emerging Technology*, August 25.\n    \n6.  ^**[^](#fnref89r8f21snda)**^\n    \n    Halstead, John (2019) [Safeguarding the future cause area report](https://assets.ctfassets.net/x5sq5djrgbwu/5C1hNPO8RK2E3RzH9dj88M/1fd2c52ab1e534af95c25c5ebea92b49/Cause_Report_-_Safeguarding_the_Future.pdf), *Founders Pledge*, January (updated December 2020)."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "65WH2M6HfT9x4BD2v",
    "name": "Center for Reducing Suffering",
    "core": false,
    "slug": "center-for-reducing-suffering",
    "oldSlugs": null,
    "postCount": 15,
    "description": {
      "markdown": "The **Center for Reducing Suffering** (**CRS**) is a research center focusing on [s-risks](https://forum.effectivealtruism.org/tag/s-risk). It was founded in 2020 by Tobias Baumann and Magnus Vinding.\n\nFurther reading\n---------------\n\nCenter for Reducing Suffering (2021) [About us](https://centerforreducingsuffering.org/about-us/), *Center for Reducing Suffering*.\n\nExternal links\n--------------\n\n[Center for Reducing Suffering](https://centerforreducingsuffering.org/). Official website.\n\n[Apply for a job](https://centerforreducingsuffering.org/get-involved/).\n\nRelated entries\n---------------\n\n[artificial sentience](https://forum.effectivealtruism.org/tag/artificial-sentience) | [hellish existential catastrophe](https://forum.effectivealtruism.org/tag/hellish-existential-catastrophe) | [invertebrate welfare](https://forum.effectivealtruism.org/tag/invertebrate-welfare) | [s-risks](https://forum.effectivealtruism.org/tag/s-risks) | [wild animal welfare](https://forum.effectivealtruism.org/tag/wild-animal-welfare)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "TDtyo2vBPmvzXW7xG",
    "name": "Center for Human-Compatible Artificial Intelligence",
    "core": false,
    "slug": "center-for-human-compatible-artificial-intelligence",
    "oldSlugs": null,
    "postCount": 14,
    "description": {
      "markdown": "The **Center for Human-Compatible Artificial Intelligence** (**CHAI**) is an [AI alignment](https://forum.effectivealtruism.org/tag/ai-alignment) research center at the University of California, Berkeley. Its mission is \"to develop the conceptual and technical wherewithal to reorient the general thrust of AI research towards provably beneficial systems.\"^[\\[1\\]](#fn02ix4ksssmfy)^\n\nFunding\n-------\n\nAs of June 2022, CHAI has received over $17.1 million in funding from [Open Philanthropy](https://forum.effectivealtruism.org/topics/open-philanthropy),^[\\[2\\]](#fn4bk6tugdxxp)^ nearly $780,000 from the [Survival and Flourishing](https://forum.effectivealtruism.org/topics/survival-and-flourishing) Fund,^[\\[3\\]](#fn03nadqf00d3l)^ and over $120,000 from [Effective Altruism Funds](https://forum.effectivealtruism.org/topics/effective-altruism-funds).^[\\[4\\]](#fnloj0nzz3o5i)^^[\\[5\\]](#fnu9zbul40t1)^\n\nEvaluation\n----------\n\nCHAI is one of the four organizations recommended by [Founders Pledge](https://forum.effectivealtruism.org/tag/founders-pledge) in their cause report on safeguarding the [long-term future](https://forum.effectivealtruism.org/tag/long-term-future).^[\\[6\\]](#fnxfaus11b66c)^\n\nFurther reading\n---------------\n\nOpen Philanthropy (2016) [UC Berkeley — Center for Human-Compatible AI (2016)](https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/uc-berkeley-center-human-compatible-ai), *Open Philanthropy*, August.\n\nRice, Issa (2018) [Timeline of Center for Human-Compatible AI](https://timelines.issarice.com/wiki/Timeline_of_Center_for_Human-Compatible_AI), *Timelines Wiki*, February 8.\n\nExternal links\n--------------\n\n[Center for Human-Compatible Artificial Intelligence](https://humancompatible.ai/). Official website.\n\n[Apply for a job](https://humancompatible.ai/jobs).\n\n[Donate to CHAI](https://humancompatible.ai/donate/).\n\nRelated entries\n---------------\n\n[AI alignment](https://forum.effectivealtruism.org/tag/ai-alignment) | [Berkeley Existential Risk Initiative](https://forum.effectivealtruism.org/tag/berkeley-existential-risk-initiative) | [*Human Compatible*](https://forum.effectivealtruism.org/tag/human-compatible) | [Stuart Russell](https://forum.effectivealtruism.org/tag/stuart-russell)\n\n1.  ^**[^](#fnref02ix4ksssmfy)**^\n    \n    Center for Human-Compatible Artificial Intelligence (2021) [About](https://humancompatible.ai/about), *Center for Human-Compatible Artificial Intelligence*.\n    \n2.  ^**[^](#fnref4bk6tugdxxp)**^\n    \n    Open Philanthropy (2022) [Grants database: Center for Human-Compatible AI](https://www.openphilanthropy.org/grants/?q=%22Center+for+Human-Compatible%22&organization-name=uc-berkeley), *Open Philanthropy*.\n    \n3.  ^**[^](#fnref03nadqf00d3l)**^\n    \n    Survival and Flourishing Fund (2019) [SFF-2020-H2 S-process recommendations announcement](https://survivalandflourishing.fund/sff-2020-h2-recommendations), *Survival and Flourishing Fund*.\n    \n4.  ^**[^](#fnrefloj0nzz3o5i)**^\n    \n    Long-Term Future Fund (2020) [September 2020: Long-Term Future Fund grants](https://funds.effectivealtruism.org/funds/payouts/september-2020-long-term-future-fund-grants), *Effective Altruism Funds*, September.\n    \n5.  ^**[^](#fnrefu9zbul40t1)**^\n    \n    Long-Term Future Fund (2021) [May 2021: Long-Term Future Fund grants](https://funds.effectivealtruism.org/funds/payouts/may-2021-long-term-future-fund-grants), *Effective Altruism Funds*, May.\n    \n6.  ^**[^](#fnrefxfaus11b66c)**^\n    \n    Halstead, John (2019) [Safeguarding the future cause area report](https://assets.ctfassets.net/x5sq5djrgbwu/5C1hNPO8RK2E3RzH9dj88M/1fd2c52ab1e534af95c25c5ebea92b49/Cause_Report_-_Safeguarding_the_Future.pdf), *Founders Pledge*, January (updated December 2020)."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "G6bzbQHkukfYxf8RW",
    "name": "Polaris Ventures",
    "core": false,
    "slug": "polaris-ventures",
    "oldSlugs": [
      "center-for-emerging-risk-research"
    ],
    "postCount": null,
    "description": {
      "markdown": "**Polaris Ventures** is a Swiss nonprofit that works on improving the quality of life of future generations.\n\nPolaris Ventures was founded in 2019 under the name **Center for Emerging Risk Research** (**CERR**). It adopted its current name in July 2022.\n\nPolaris Ventures makes grants in fields such as [artificial intelligence](https://forum.effectivealtruism.org/tag/artificial-intelligence), [decision theory](https://forum.effectivealtruism.org/tag/decision-theory), [game theory](https://forum.effectivealtruism.org/tag/game-theory), [cause prioritization](https://forum.effectivealtruism.org/tag/cause-prioritization), [population ethics](https://forum.effectivealtruism.org/tag/population-ethics) and [peace and conflict studies](https://forum.effectivealtruism.org/tag/peace-and-conflict-studies).  As of July 2022, its assets are around $200 million. \n\nIn early May 2021, Polaris Ventures committed $15 million to support the [Cooperative AI Foundation](https://forum.effectivealtruism.org/tag/cooperative-ai-foundation). In September 2021, it committed $3 million to Carnegie Mellon University to help establish the Foundations of Cooperative AI Lab led by Vincent Conitzer.\n\nFurther reading\n---------------\n\nbusiness-monitor.ch (2021) [Center for Emerging Risk Research](https://business-monitor.ch/en/companies/1001049-center-for-emerging-risk-research), *business-monitor.ch*.\n\nCenter for Emerging Risk Research (2021) [Transparency](https://emergingrisk.ch/transparency/), *Center for Emerging Risk Research*.\n\nTorges, Stefan (2020) [Center on Long-Term Risk: 2021 plans & 2020 review](https://forum.effectivealtruism.org/posts/93o6JwmdPPPuTXbYv/center-on-long-term-risk-2021-plans-and-2020-review), *Effective Altruism Forum*, December 8.\n\nExternal links\n--------------\n\n[Center for Emerging Risk Research](https://emergingrisk.ch/). Official website.\n\nRelated links\n-------------\n\n[AI alignment](https://forum.effectivealtruism.org/tag/ai-alignment) | [cause prioritization](https://forum.effectivealtruism.org/tag/cause-prioritization) | [Center on Long-Term Risk](https://forum.effectivealtruism.org/tag/center-on-long-term-risk) | [Cooperative AI Foundation](https://forum.effectivealtruism.org/tag/cooperative-ai-foundation) | [decision theory](https://forum.effectivealtruism.org/tag/decision-theory) | [game theory](https://forum.effectivealtruism.org/tag/game-theory) | [longtermism](https://forum.effectivealtruism.org/tag/longtermism) | [peace and conflict studies](https://forum.effectivealtruism.org/tag/peace-and-conflict-studies) | [population ethics](https://forum.effectivealtruism.org/tag/population-ethics)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "kyPpS9ye3P5yT2Y6Q",
    "name": "Center for Applied Rationality",
    "core": false,
    "slug": "center-for-applied-rationality",
    "oldSlugs": null,
    "postCount": 10,
    "description": {
      "markdown": "The **Center for Applied Rationality** is a nonprofit organization based in Berkeley, California, that teaches techniques to improve [epistemic and instrumental rationality](https://forum.effectivealtruism.org/tag/instrumental-vs-epistemic-rationality).\n\nHistory\n-------\n\nCFAR was founded in 2012 by Anna Salamon, [Julia Galef](https://forum.effectivealtruism.org/tag/julia-galef), Valentine Smith and Andrew Critch.^[\\[1\\]](#fntp734g98u88)^ Galef served as president until 2016, when Salamon took her place; as of May 2022, she continues to serve in that capacity. Salamon, Jesse Liptrap and Michael Blume currently sit on the board of directors.\n\nFunding\n-------\n\nAs of June 2022, CFAR has received over 3.5 million in funding from [Open Philanthropy](https://forum.effectivealtruism.org/topics/open-philanthropy),^[\\[2\\]](#fnnf6c3i0pbjq)^  over $1.6 million from the [Survival and Flourishing](https://forum.effectivealtruism.org/topics/survival-and-flourishing) Fund,^[\\[3\\]](#fn410tpv4k4ly)^^[\\[4\\]](#fnckx83puqeat)^^[\\[5\\]](#fn6ljkszm72hq)^ and over $300,000 from [Effective Altruism Funds](https://forum.effectivealtruism.org/topics/effective-altruism-funds).^[\\[6\\]](#fn1jinvazxrshh)^^[\\[7\\]](#fnw8rsivmcymq)^\n\nFurther reading\n---------------\n\nRice, Issa (2017) [Timeline of Center for Applied Rationality](https://timelines.issarice.com/wiki/Timeline_of_Center_for_Applied_Rationality), *Timelines Wiki*, July 11.\n\nExternal links\n--------------\n\n[Center for Applied Rationality](https://www.rationality.org/). Official website.\n\n[Donate to CFAR](https://www.rationality.org/donate/).\n\nRelated entries\n---------------\n\n[Eliezer Yudkowsky](https://forum.effectivealtruism.org/tag/eliezer-yudkowsky) | [Julia Galef](https://forum.effectivealtruism.org/tag/julia-galef) | [Machine Intelligence Research Institute](https://forum.effectivealtruism.org/tag/machine-intelligence-research-institute) | [rationality community](https://forum.effectivealtruism.org/tag/rationality-community)\n\n1.  ^**[^](#fnreftp734g98u88)**^\n    \n    \"CFAR is a 501(c)(3) non-profit operating out of Berkeley, California, originally founded in 2012 by Anna Salamon, Julia Galef, Valentine Smith, and Andrew Critch.\" (Center for Applied Rationality (2022) [Mission](http://www.rationality.org/about/mission/), *Center for Applied Rationality*)\n    \n2.  ^**[^](#fnrefnf6c3i0pbjq)**^\n    \n    Open Philanthropy (2022) [Grants database: Center for Applied Rationality](https://www.openphilanthropy.org/grants/?q=&organization-name=center-for-applied-rationality), *Open Philanthropy*.\n    \n3.  ^**[^](#fnref410tpv4k4ly)**^\n    \n    Survival and Flourishing Fund (2018) [SFF-2019-Q4 S-process recommendations announcement](https://survivalandflourishing.fund/sff-2019-q4-recommendations), *Survival and Flourishing Fund*. \n    \n4.  ^**[^](#fnrefckx83puqeat)**^\n    \n    Survival and Flourishing Fund (2019) [SFF-2020-H2 S-process recommendations announcement](https://survivalandflourishing.fund/sff-2020-h2-recommendations), *Survival and Flourishing Fund*. \n    \n5.  ^**[^](#fnref6ljkszm72hq)**^\n    \n    Survival and Flourishing Fund (2020) [SFF-2021-H1 S-process recommendations announcement](https://survivalandflourishing.fund/sff-2021-h1-recommendations), *Survival and Flourishing Fund*.\n    \n6.  ^**[^](#fnref1jinvazxrshh)**^\n    \n    Long-Term Future Fund (2018) [July 2018: Long-Term Future Fund grants](https://funds.effectivealtruism.org/funds/payouts/july-2018-long-term-future-fund-grants), *Effective Altruism Funds*, July.\n    \n7.  ^**[^](#fnrefw8rsivmcymq)**^\n    \n    Long-Term Future Fund (2019) [April 2019: Long-Term Future Fund grants and recommendations](https://funds.effectivealtruism.org/funds/payouts/april-2019-long-term-future-fund-grants-and-recommendations), *Effective Altruism Funds*, April."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "69w9jjBQ7QMG8vRA4",
    "name": "Cause X",
    "core": false,
    "slug": "cause-x",
    "oldSlugs": null,
    "postCount": 21,
    "description": {
      "markdown": "**Cause X** is a cause area currently neglected by the effective altruism community, typically due to some form of moral blindness or fundamental oversight, yet more important than all the causes currently prioritized by it.\n\nThe idea was introduced by [William MacAskill](https://forum.effectivealtruism.org/tag/william-macaskill),^[\\[1\\]](#fneytiqj2b18a)^ possibly by analogy with [Derek Parfit](https://forum.effectivealtruism.org/tag/derek-parfit)'s \"Theory X\", a currently unknown hypothetical theory which would solve a number of important open problems in [population ethics](https://forum.effectivealtruism.org/tag/population-ethics).^[\\[2\\]](#fnsjhush0l07r)^ (\"Cause X\" is also sometimes used loosely to refer to any promising and neglected cause,^[\\[3\\]](#fnyfk965xsx6p)^^[\\[4\\]](#fn41pf6gr37so)^ but this is not how the expression is generally understood.)^[\\[5\\]](#fnrjm7iwwz74)^\n\nDoes Cause X exist?\n-------------------\n\nThe existence of Cause X should not be taken for granted; whether there is a Cause X is an open question. In contrast to Parfit's Theory X, however, there are no impossibility theorems that could disprove the existence of Cause X.^[\\[6\\]](#fnzxgrnseokn)^ Furthermore, if there is a Cause X, there is no determinate way of ascertaining that it has been found; this is another disanalogy with Theory X, whose conditions are defined with sufficient precision that it is possible to establish when a theory satisfies them.\n\nThere are, however, good reasons for expecting Cause X to exist. All previous generations overlooked highly important causes—usually by neglecting large groups of morally relevant beings, such as women, racial minorities, and nonhuman animals—,^[\\[7\\]](#fno7e899lj9t9)^ so it would be a remarkable coincidence if our generation was the first to avoid this moral shortcoming.^[\\[1\\]](#fneytiqj2b18a)^^[\\[8\\]](#fniob062ibn8n)^^[\\[9\\]](#fn80hvcbjhjnb)^ In addition, the appearance that the present generation is different may itself be [debunked](https://forum.effectivealtruism.org/tag/debunking-argument) as a manifestation of the \"end of history illusion\",^[\\[10\\]](#fn8vhob9tf085)^ \"new era thinking\",^[\\[11\\]](#fnip5d6b32kb)^ and related [cognitive biases](https://forum.effectivealtruism.org/tag/cognitive-biases). Furthermore, the space of possible cause areas is vast, and humans have only recently begun to explore it systematically, so it seems antecedently very likely that a cause more important than the current top causes remains to be found.^[\\[8\\]](#fniob062ibn8n)^ Yet another argument for the existence of Cause X is that the effective altruism community has, to a certain extent, changed its views about which causes are most important over the years. An induction from this history suggests further changes in what causes the community will consider most impactful.\n\nHeuristics for finding Cause X\n------------------------------\n\nA number of heuristics for finding Cause X have been proposed.^[\\[3\\]](#fnyfk965xsx6p)^^[\\[12\\]](#fn6it36fyk5gb)^ Kerry Vaughan suggests three such heuristics:\n\nHeuristic 1: *Moral circle expansion*. One apparently robust historical trend since at least the last few centuries is the gradual [expansion of the circle of moral concern](https://forum.effectivealtruism.org/tag/moral-circle-expansion-1).^[\\[13\\]](#fn3x5cflvt4pw)^^[\\[14\\]](#fnrd98jg0yf3)^^[\\[15\\]](#fn505osw5siy6)^ Furthermore, this expansion appears to account for much of the moral progress that occurred during this period. Thus, a plausible heuristic is to push this expansion even further. This heuristic suggests [wild animal welfare](https://forum.effectivealtruism.org/tag/wild-animal-welfare), [invertebrate welfare](https://forum.effectivealtruism.org/tag/invertebrate-welfare), [artificial sentience](https://forum.effectivealtruism.org/tag/artificial-sentience), as well as research on [moral patienthood](https://forum.effectivealtruism.org/tag/moral-patienthood), as Cause X candidates.\n\nHeuristic 2: *Transformative technology*. Technological progress has the potential to radically transform the world in moral relevant respects. This assessment is plausible both on the basis of historical analysis—many of the most significant changes have occurred due to some form of human innovation—and upon consideration of various anticipated technologies not yet developed, such as [whole brain emulation](https://forum.effectivealtruism.org/tag/whole-brain-emulation),  [artificial general intelligence](https://forum.effectivealtruism.org/tag/human-level-artificial-intelligence), and [atomically precise manufacturing](https://forum.effectivealtruism.org/tag/atomically-precise-manufacturing). The heuristic suggests work on making these technologies safer, such as [AI safety](https://forum.effectivealtruism.org/tag/ai-safety), as well as [differential technological development](https://forum.effectivealtruism.org/tag/differential-progress), as Cause X candidates.\n\nHeuristic 3: *Crucial considerations*. A [crucial consideration](https://forum.effectivealtruism.org/tag/crucial-consideration) is one that warrants a major reassessment of a cause's impact. Actively looking for such considerations is thus of clear relevance for finding Cause X. Here the heuristic would favor making lists of crucial considerations, as well as the search for additional \"[deliberation ladders](https://forum.effectivealtruism.org/tag/crucial-consideration#Related_concepts)\" in existing arguments for specific causes.\n\nThe meta-heuristic of holding events where attempts are made to find potential Cause X candidates has also been suggested as an effective discovery method.^[\\[16\\]](#fnryao7u9mo9)^ Such events could take the form of informal meetups, conference workshops, or academic conferences.\n\nFurther reading\n---------------\n\nMacAskill, William (2016) [Moral progress and Cause X](https://www.effectivealtruism.org/articles/moral-progress-and-cause-x/) , *Effective Altruism*, October 7.\n\nRelated entries\n---------------\n\n[cause candidates](https://forum.effectivealtruism.org/tag/cause-candidates) | [cause prioritization](https://forum.effectivealtruism.org/tag/cause-prioritization) | [global priorities research](https://forum.effectivealtruism.org/tag/global-priorities-research) | [less-discussed causes](https://forum.effectivealtruism.org/tag/less-discussed-causes)\n\n1.  ^**[^](#fnrefeytiqj2b18a)**^\n    \n    MacAskill, William (2016) [Moral progress and Cause X](https://www.effectivealtruism.org/articles/moral-progress-and-cause-x/) , *Effective Altruism*, October 7.\n    \n2.  ^**[^](#fnrefsjhush0l07r)**^\n    \n    Parfit, Derek (1984) [*Reasons and Persons*](https://en.wikipedia.org/wiki/Special:BookSources/019824908X), Oxford: Clarendon Press.\n    \n3.  ^**[^](#fnrefyfk965xsx6p)**^\n    \n    Savoie, Joey (2019) [Cause X guide](https://forum.effectivealtruism.org/posts/kFmFLcdSFKo2GFJkc/cause-x-guide), *Effective Altruism Forum*, September 1.\n    \n4.  ^**[^](#fnref41pf6gr37so)**^\n    \n    Gómez Emilsson, Andrés (2019) [Cause X – what will the new shiny effective altruist cause be?](https://qualiacomputing.com/2019/02/07/cause-x-what-will-the-new-shiny-effective-altruist-cause-be/), *Qualia Computing*, February 7.\n    \n5.  ^**[^](#fnrefrjm7iwwz74)**^\n    \n    Rice, Issa (2019) [Comment on “Cause X guide”](https://forum.effectivealtruism.org/posts/kFmFLcdSFKo2GFJkc/cause-x-guide#dqCy9FEuLDP25hw9a), *Effective Altruism Forum*, September 1.\n    \n6.  ^**[^](#fnrefzxgrnseokn)**^\n    \n    Ng, Yew-Kwang (1989) [What should we do about future generations? Impossibility of Parfit’s Theory X](https://doi.org/10.1017/S0266267100002406), *Economics and Philosophy*, vol. 5, pp. 235–253.\n    \n7.  ^**[^](#fnrefo7e899lj9t9)**^\n    \n    Karnofsky, Holden (2017) [Radical empathy](https://www.openphilanthropy.org/blog/radical-empathy), *Open Philanthropy*, February 16.\n    \n8.  ^**[^](#fnrefiob062ibn8n)**^\n    \n    Williams, Evan G. (2015) [The possibility of an ongoing moral catastrophe](https://doi.org/10.1007/s10677-015-9567-7), *Ethical Theory and Moral Practice*, vol. 18, pp. 971–982.\n    \n9.  ^**[^](#fnref80hvcbjhjnb)**^\n    \n    Cf. Lewis, Gregory (2016) [Beware surprising and suspicious convergence](https://forum.effectivealtruism.org/posts/omoZDu8ScNbot6kXS/beware-surprising-and-suspicious-convergence), *Effective Altruism Forum*, January 24.\n    \n10.  ^**[^](#fnref8vhob9tf085)**^\n    \n    Quoidbach, Jordi, Daniel T. Gilbert & Timothy D. Wilson (2013) [The end of history illusion](https://doi.org/10.1126/science.1229294), *Science*, vol. 339, pp. 96–98.\n    \n11.  ^**[^](#fnrefip5d6b32kb)**^\n    \n    Schiller, Robert J. (2015) [*Irrational Exuberance*](https://en.wikipedia.org/wiki/Special:BookSources/978-0-691-16626-1), 3rd ed., Princeton, New Jersey: Princeton University Press.\n    \n12.  ^**[^](#fnref6it36fyk5gb)**^\n    \n    Vaughan, Kerry (2016) [Three heuristics for finding cause X](https://www.effectivealtruism.org/articles/three-heuristics-for-finding-cause-x/), *Effective Altruism*, November 4.\n    \n13.  ^**[^](#fnref3x5cflvt4pw)**^\n    \n    Singer, Peter (1981) [*The Expanding Circle: Ethics and Sociobiology*](https://en.wikipedia.org/wiki/Special:BookSources/9780198246466), Oxford: Clarendon Press.\n    \n14.  ^**[^](#fnrefrd98jg0yf3)**^\n    \n    Pinker, Steven (2011) [*The Better Angels of Our Nature: Why Violence Has Declined*](https://en.wikipedia.org/wiki/Special:BookSources/9780670022953), New York: Viking.\n    \n15.  ^**[^](#fnref505osw5siy6)**^\n    \n    But cf. Branwen, Gwern (2019) [The narrowing circle](https://www.gwern.net/The-Narrowing-Circle), *Gwern Branwen's Website*, April 27.\n    \n16.  ^**[^](#fnrefryao7u9mo9)**^\n    \n    Gómez Emilsson, Andrés (2020) [Improve your indoor air quality by 99% by optimizing the use of HEPA filters](https://qualiacomputing.com/2020/09/16/improve-your-indoor-air-quality-by-99-by-optimizing-the-use-of-hepa-filters/), *Qualia Computing*, September 16."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "wPjsmKwAoW8XhBEzE",
    "name": "Carl Shulman",
    "core": false,
    "slug": "carl-shulman",
    "oldSlugs": null,
    "postCount": 12,
    "description": {
      "markdown": "**Carl Matthew Shulman** (born ca. 1982) is a Canadian-American researcher. He is currently a Research Associate at the [Future of Humanity Institute](https://forum.effectivealtruism.org/tag/future-of-humanity-institute) and an Advisor to [Open Philanthropy](https://forum.effectivealtruism.org/tag/open-philanthropy). Previously, he was a Research Fellow at the [Machine Intelligence Research Institute](https://forum.effectivealtruism.org/tag/machine-intelligence-research-institute) and Director of Careers Research at [80,000 Hours](https://forum.effectivealtruism.org/tag/80-000-hours). He also worked at Clarium Capital, an investment management and hedge fund company, and at Reed Smith LLP, a global law firm. Shulman attended New York University School of Law and holds a BA in philosophy from Harvard University.\n\nShulman has made substantive contributions to several areas, including [career choice](https://forum.effectivealtruism.org/tag/career-choice), [open borders](https://forum.effectivealtruism.org/tag/immigration-reform), [wild animal welfare](https://forum.effectivealtruism.org/tag/wild-animal-welfare), [existential risk](https://forum.effectivealtruism.org/tag/existential-risk), [decision theory](https://forum.effectivealtruism.org/tag/decision-theory), [whole brain emulation](https://forum.effectivealtruism.org/tag/whole-brain-emulation), [earning to give](https://forum.effectivealtruism.org/tag/earning-to-give), among many others. Concepts and ideas original to him include [iterated embryo selection](https://forum.effectivealtruism.org/tag/iterated-embryo-selection), [hedonium](https://forum.effectivealtruism.org/tag/hedonium), [donor lotteries](https://forum.effectivealtruism.org/tag/donor-lotteries), and others.\n\nShulman administers a $5 million discretionary fund held by the [Centre for Effective Altruism](https://forum.effectivealtruism.org/tag/centre-for-effective-altruism-1), funded by an [Open Philanthropy](https://forum.effectivealtruism.org/tag/open-philanthropy) grant.^[\\[1\\]](#fnyltpattrxsk)^\n\nFurther reading\n---------------\n\nMuehlhauser, Luke (2011) [Just the facts, ma’am!](https://www.lesswrong.com/posts/7YLuXtKqWiybJAmeo/just-the-facts-ma-am), *LessWrong*, December 10.\n\nRice, Issa (2020) [Timeline of Carl Shulman publications](https://timelines.issarice.com/wiki/Timeline_of_Carl_Shulman_publications), *Timelines Wiki*.\n\nWiblin, Robert & Keiran Harris (2021) [Carl Shulman on the common-sense case for existential risk work and its practical implications](https://80000hours.org/podcast/episodes/carl-shulman-common-sense-case-existential-risks/), *80,000 Hours*, October 5.\n\nZabel, Claire (2016) [Research advice from Carl Shulman](https://docs.google.com/document/d/1_yuuheVqp1quDfkuRcpoW_HO7jPaI7QnRjF1zl_VovU/edit), May 28.\n\nExternal links\n--------------\n\n[Reflective disequilibrium](http://reflectivedisequilibrium.blogspot.com/). Shulman's blog.\n\n[Carl Shulman](https://forum.effectivealtruism.org/users/carlshulman). [Effective Altruism Forum](https://forum.effectivealtruism.org/tag/effective-altruism-forum-1) account.\n\n1.  ^**[^](#fnrefyltpattrxsk)**^\n    \n    Open Philanthropy (2018) [Centre for Effective Altruism — new discretionary fund](https://www.openphilanthropy.org/giving/grants/centre-for-effective-altruism-new-discretionary-fund), *Open Philanthropy*, March."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "toct8RYvTvgontEts",
    "name": "Open Philanthropy",
    "core": false,
    "slug": "open-philanthropy",
    "oldSlugs": null,
    "postCount": 154,
    "description": {
      "markdown": "**Open Philanthropy** (previously the **Open Philanthropy Project**) is a research and grantmaking foundation based in San Francisco.\n\nHistory\n-------\n\nOpen Philanthropy launched in late 2011, as a partnership between [Good Ventures](https://forum.effectivealtruism.org/tag/good-ventures) and [GiveWell](https://forum.effectivealtruism.org/tag/givewell).^[\\[1\\]](#fntgc02qi0fmg)^ The partnership operated under the name **GiveWell Labs** before adopting, in August 2014, the name **Open Philanthropy Project**.^[\\[2\\]](#fn4vww1w7m4ep)^ It continued to be part of GiveWell until 2017, when it became an independent organization.^[\\[3\\]](#fnh926zy6ps5f)^ The name was changed to **Open Philanthropy** around December 2019.\n\nLeadership\n----------\n\n[Cari Tuna](https://forum.effectivealtruism.org/tag/cari-tuna) is Open Philanthropy's president, and [Holden Karnofsky](https://forum.effectivealtruism.org/tag/holden-karnofsky) and [Alexander Berger](https://forum.effectivealtruism.org/tag/alexander-berger) are its two co-CEOs. Karnofsky oversees grantmaking in [biosecurity](https://forum.effectivealtruism.org/tag/biosecurity), [AI safety](https://forum.effectivealtruism.org/tag/ai-safety) and [longtermism](https://forum.effectivealtruism.org/tag/longtermism), while Berger oversees grantmaking in [global health and development](https://forum.effectivealtruism.org/tag/global-health-and-development), [farmed animal welfare](https://forum.effectivealtruism.org/tag/farmed-animal-welfare), [scientific research](https://forum.effectivealtruism.org/tag/meta-science) and other areas within [global health and wellbeing](https://forum.effectivealtruism.org/tag/global-health-and-wellbeing).^[\\[4\\]](#fnxd1uaf9jzyi)^\n\nFunding opportunities\n---------------------\n\nApplications are currently open for young individuals interested in obtaining financial support for pursuing careers that help improve the [long-term future](https://forum.effectivealtruism.org/tag/long-term-future).^[\\[5\\]](#fn9x4lp40gzct)^\n\nFurther reading\n---------------\n\nMatthews, Dylan (2015) [You have $8 billion. You want to do as much good as possible. What do you do?](https://www.vox.com/2015/4/24/8457895/givewell-open-philanthropy-charity), *Vox*, April 24 (updated 16 October 2018).\n\nWiblin, Robert (2017) [You want to do as much good as possible and have billions of dollars. What do you do?](https://80000hours.org/podcast/episodes/nick-beckstead-giving-billions/), *80,000 Hours*, October 11.  \n*An interview with Nick Beckstead, a program officer at Open Philanthropy.*\n\nWiblin, Robert & Keiran Harris (2018) [The world’s most intellectual foundation is hiring. Holden Karnofsky, founder of GiveWell, on how philanthropy can have maximum impact by taking big risks](https://80000hours.org/podcast/episodes/holden-karnofsky-open-philanthropy/), *80,000 Hours*, February 27.  \n*An interview with Holden Karnofsky, Open Philanthropy's CEO.*\n\nExternal links\n--------------\n\n[Open Philanthropy](https://www.openphilanthropy.org/). Official website.\n\n[Apply for funding](https://www.openphilanthropy.org/giving/guide-for-grant-seekers).\n\n[Apply for a job](https://www.openphilanthropy.org/careers/).\n\nRelated entries\n---------------\n\n[Cari Tuna](https://forum.effectivealtruism.org/tag/cari-tuna) | [GiveWell](https://forum.effectivealtruism.org/tag/givewell) | [Good Ventures](https://forum.effectivealtruism.org/tag/good-ventures) | [Holden Karnofsky](https://forum.effectivealtruism.org/tag/holden-karnofsky)\n\n1.  ^**[^](#fnreftgc02qi0fmg)**^\n    \n    Karnofsky, Holden (2011) [Announcing GiveWell Labs](https://blog.givewell.org/2011/09/08/announcing-givewell-labs/), *The GiveWell Blog*, September 8 (updated 2 September 2014).\n    \n2.  ^**[^](#fnref4vww1w7m4ep)**^\n    \n    Karnofsky, Holden (2014) [Open Philanthropy Project (formerly GiveWell Labs)](https://www.openphilanthropy.org/blog/open-philanthropy-project-formerly-givewell-labs), *Open Philanthropy*, August 20.\n    \n3.  ^**[^](#fnrefh926zy6ps5f)**^\n    \n    Karnofsky, Holden (2017) [The Open Philanthropy Project is now an independent organization](https://www.openphilanthropy.org/blog/open-philanthropy-project-now-independent-organization), *Open Philanthropy*, June 12.\n    \n4.  ^**[^](#fnrefxd1uaf9jzyi)**^\n    \n    Karnofsky, Holden (2021) [Open Philanthropy’s new co-CEO](https://www.openphilanthropy.org/research/open-philanthropys-new-co-ceo/), *Open Philanthropy*, June 15.\n    \n5.  ^**[^](#fnref9x4lp40gzct)**^\n    \n    Open Philanthropy (2020) [Early-career funding for individuals interested in improving the long-term future](https://www.openphilanthropy.org/focus/other-areas/early-career-funding-individuals-interested-improving-long-term-future), *Open Philanthropy*, August 31."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "vsK5JgE4dm6cX7rtq",
    "name": "Career framework",
    "core": false,
    "slug": "career-framework",
    "oldSlugs": null,
    "postCount": 4,
    "description": {
      "markdown": "\nThe **career framework** is a model developed by [80,000 Hours](https://forum.effectivealtruism.org/tag/80-000-hours) to evaluate careers in terms of their social impact. The standard version of the model comprises four primary factors: [career capital](https://forum.effectivealtruism.org/tag/career-capital), [role impact](https://forum.effectivealtruism.org/tag/role-impact), [supportive conditions](https://forum.effectivealtruism.org/tag/supportive-conditions), and [personal fit](https://forum.effectivealtruism.org/tag/personal-fit). The first three factors are additive, which interact multiplicatively with the fourth factor.\n\nMore recently, the model has been extended to capture the value of [coordination](https://forum.effectivealtruism.org/tag/altruistic-coordination). This extension incorporates two additional factors: *relative fit*, or the person's comparative advantage relative to members of the [effective altruism](https://forum.effectivealtruism.org/tag/effective-altruism) community, and *community capital*, or the extent to which the person's actions increase the community's future impact and ability to coordinate.\n\n## Further reading\n\nTodd, Benjamin (2019) [A guide to using your career to help solve the world’s most pressing problems](https://80000hours.org/key-ideas/#career-strategy), *80,000 Hours*, March.\n\nTodd, Benjamin (2015) [What should you look for in a job? Introducing our framework](https://80000hours.org/articles/framework/), *80,000 Hours*, July.\n\n## Related entries\n\n[career choice](https://forum.effectivealtruism.org/tag/career-choice) \n"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "nksRuSzEhEjgLNXWM",
    "name": "Career capital",
    "core": false,
    "slug": "career-capital",
    "oldSlugs": null,
    "postCount": 25,
    "description": {
      "markdown": "**Career capital** is the stock of resources that increase the future impact of a person's career. It consists of skills, connections, credentials, and runway.^[\\[1\\]](#fnff5421zl5s)^\n\nAccording to [80,000 Hours](https://forum.effectivealtruism.org/tag/80-000-hours), building career capital is one of the most effective ways for individuals to have an impact over the course of their careers. This is especially the case for individuals at the beginning of their careers, since the value of this capital is roughly proportional to the remaining duration of a person's career. In addition, 80,000 Hours used to recommend building *flexible* career capital—skills, connections, and credentials useful across a wide variety of jobs—to individuals uncertain about their careers,^[\\[2\\]](#fn1ckuzmcoubl)^ though more recently they have de-emphasized this recommendation, stressing instead the importance of career capital that is most useful in the highest-impact jobs.^[\\[3\\]](#fn7g5a22p7d5l)^\n\nFurther reading\n---------------\n\nTodd, Benjamin (2021) [Career capital: how best to invest in yourself](https://80000hours.org/articles/career-capital/), *80,000 Hours*, September.\n\nRelated entries\n---------------\n\n[career choice](https://forum.effectivealtruism.org/tag/career-choice) | [career framework](https://forum.effectivealtruism.org/tag/career-framework)\n\n1.  ^**[^](#fnrefff5421zl5s)**^\n    \n    Todd, Benjamin (2015) [What should you look for in a job? Introducing our framework](https://80000hours.org/articles/framework/), *80,000 Hours*, July.\n    \n2.  ^**[^](#fnref1ckuzmcoubl)**^\n    \n    Todd, Benjamin (2014) [Which jobs put you in the best long-term position?](https://80000hours.org/career-guide/career-capital/), *80,000 Hours*, October.\n    \n3.  ^**[^](#fnref7g5a22p7d5l)**^\n    \n    Todd, Benjamin (2019) [Before committing to management consulting, consider directly entering priority paths, policy, startups, and other options](https://80000hours.org/articles/alternatives-to-consulting/), *80,000 Hours*, November."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "Qej4FwPwDReNKo7hM",
    "name": "Bulletin of the Atomic Scientists",
    "core": false,
    "slug": "bulletin-of-the-atomic-scientists",
    "oldSlugs": null,
    "postCount": 1,
    "description": {
      "markdown": "The ***Bulletin of the Atomic Scientists*** is a journal and nonprofit organization focusing on risks of [human extinction](https://forum.effectivealtruism.org/tag/human-extinction), especially from [nuclear warfare](https://forum.effectivealtruism.org/tag/nuclear-warfare-1), [climate change](https://forum.effectivealtruism.org/tag/climate-change), and [transformative technology](https://forum.effectivealtruism.org/tag/transformative-development). It was founded in 1945 by former scientists and engineers of the [Manhattan Project](https://forum.effectivealtruism.org/tag/manhattan-project), and was the first academic journal ever devoted to promoting [nuclear disarmament](https://forum.effectivealtruism.org/tag/nuclear-disarmament-movement) and preventing human extinction.\n\nHistory\n-------\n\nTwo events were key precursors to the formation of the *Bulletin of the Atomic Scientists*. First, the writing of the Franck Report in June 1945, which argued against the use of nuclear weapons against civilian populations in Japan and instead recommended that the bomb be demonstrated in an uninhabited area. The Report was authored primarily by Eugene Rabinowitch, who would become one of the *Bulletin*'s co-founders. Second, the creation of Atomic Scientists of Chicago (ASC), an organization open to any past or present scientific employee of the Manhattan Project with the mission \"to address the moral and social responsibilities of scientists regarding the use of nuclear energy and to promote public awareness of its possible consequences.\"^[\\[1\\]](#fn67k940n43rl)^ ASC became the *Bulletin*'s founding organization.\n\nDoomsday Clock\n--------------\n\nSince 1947, the *Bulletin* has maintained a \"Doomsday Clock\", intended to provide a vivid depiction of how close humanity is to \"destroying the world\". (The *Bulletin* does not appear to define the meaning of that expression precisely.) Every year, the clock is set to a certain number of minutes and seconds to \"midnight\", with times closer to midnight representing higher risks of catastrophe. The Doomsday Clock measures risk on a merely ordinal scale: it does not purport to claim that, for example, 23:58 is associated with half the risk of 23:59, or that the difference between 23:57 and 23:56 is of the same magnitude as the difference between 23:55 and 23:54, as would be the case if the risk was measured on ratio or interval scales, respectively.\n\nThe Doomsday Clock was last updated on 20 January 2022. It was set to 100 seconds to midnight.^[\\[2\\]](#fnut0svbto0o8)^\n\nNuclear Notebook\n----------------\n\nSince 1987, the *Bulletin* also publishes the Nuclear Notebook, an accounting of world nuclear arsenals compiled by the Federation of American Scientists (FAS). The Nuclear Notebook is currently prepared by Hans Kristensen and Matt Korda, researchers from the Nuclear Information Project at FAS.\n\nFurther reading\n---------------\n\nElder, Robert K. & J. C. Gabel (2022) [*The Doomsday Clock at 75*](https://en.wikipedia.org/wiki/Special:BookSources/978-1-955125-15-4), Los Angeles: Hat & Beard Press.\n\nSmith, Alice Kimball (1965) [*A Peril and a Hope: The Scientists’ Movement in America: 1945-47*](https://en.wikipedia.org/wiki/Special:BookSources/9780262690263), Chicago: University of Chicago Press.\n\nWalsh, Bryan (2022) [What the Doomsday Clock is really counting down to](https://www.vox.com/22893594/doomsday-clock-nuclear-war-climate-change-risk), *Vox*, January 21.\n\nWilson, Henrietta (2010) [Bulletin of the Atomic Scientists](https://en.wikipedia.org/wiki/Special:BookSources/9780195338409), in Nigel J. Young (ed.) *The Oxford International Encyclopedia of Peace*, Oxford: Oxford University Press, pp. 217–220.  \n*An excellent concise history of the Bulletin of the Atomic Scientists.*\n\nExternal links\n--------------\n\n[Bulletin of the Atomic Scientists](https://thebulletin.org/). Official website.\n\n[Doomsday Clock](https://thebulletin.org/doomsday-clock/).\n\n[Nuclear Notebook](https://thebulletin.org/nuclear-risk/nuclear-weapons/nuclear-notebook/).\n\nRelated entries\n---------------\n\n[human extinction](https://forum.effectivealtruism.org/tag/human-extinction) | [Manhattan Project](https://forum.effectivealtruism.org/tag/manhattan-project) | [nuclear disarmament movement](https://forum.effectivealtruism.org/tag/nuclear-disarmament-movement) | [nuclear warfare](https://forum.effectivealtruism.org/tag/nuclear-warfare-1) |  [Russell-Einstein Manifesto](https://forum.effectivealtruism.org/tag/russell-einstein-manifesto) | [Trinity](https://forum.effectivealtruism.org/tag/trinity)\n\n1.  ^**[^](#fnref67k940n43rl)**^\n    \n    University of Chicago Library (2007) [Guide to the Atomic Scientists of Chicago Records 1943-1955](https://www.lib.uchicago.edu/e/scrc/findingaids/view.php?eadid=ICU.SPCL.ASCHICAGO).\n    \n2.  ^**[^](#fnrefut0svbto0o8)**^\n    \n    Mecklin, John (2022) [At doom’s doorstep: It is 100 seconds to midnight](https://thebulletin.org/doomsday-clock/current-time/), *Bulletin of the Atomic Scientists*, January 20."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "goRBaqHo55dZHRCGr",
    "name": "Transformative development",
    "core": false,
    "slug": "transformative-development",
    "oldSlugs": [
      "transformative-technology"
    ],
    "postCount": 3,
    "description": {
      "markdown": "A **transformative development** is a development at least as significant as the agricultural or industrial revolutions.^[\\[1\\]](#fn7gy1i28n578)^\n\nFurther reading\n---------------\n\nKarnofsky, Holden (2016) [Some background on our views regarding advanced artificial intelligence](https://www.openphilanthropy.org/blog/some-background-our-views-regarding-advanced-artificial-intelligence), *Open Philanthropy*, May 6, section 1.\n\nMuehlhauser, Luke (2017) [How big a deal was the Industrial Revolution?](http://lukemuehlhauser.com/industrial-revolution/), *Luke Muehlhauser’s Website*.\n\nRelated entries\n---------------\n\n[economic growth](https://forum.effectivealtruism.org/tag/economic-growth) | [transformative artificial intelligence](https://forum.effectivealtruism.org/tag/transformative-artificial-intelligence)\n\n1.  ^**[^](#fnref7gy1i28n578)**^\n    \n    Karnofsky, Holden (2016) [Some background on our views regarding advanced artificial intelligence](https://www.openphilanthropy.org/blog/some-background-our-views-regarding-advanced-artificial-intelligence), *Open Philanthropy*, May 6, section 1."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "qWG2DZyhsfFjnshgf",
    "name": "Bryan Caplan",
    "core": false,
    "slug": "bryan-caplan",
    "oldSlugs": null,
    "postCount": 5,
    "description": {
      "markdown": "**Bryan Douglas Caplan** (born 8 April 1971) is an American economist and author. He is a professor of economics at George Mason University and the author of*The Myth of the Rational Voter: Why Democracies Choose Bad Policies*,^[\\[1\\]](#fnfwvr60ndj66)^ *Selfish Reasons to Have More Kids: Why Being a Great Parent Is Less Work and More Fun Than You Think*,^[\\[2\\]](#fn2kwnno0qcq3)^*The Case Against Education: Why the Education System Is a Waste of Time and Money,*^[\\[3\\]](#fnq5q7zgrmxf)^ and *Open Borders: The Science and Ethics of Immigration*.^[\\[4\\]](#fndmhmqeujgvb)^\n\nFurther reading\n---------------\n\nCaplan, Bryan (2010) [An intellectual autobiography](https://en.wikipedia.org/wiki/Special:BookSources/978-1-61016-002-5), in Walter Block (ed.) *I Chose Liberty*, Auburn: Ludwig von Mises Institute, pp. 73–82.\n\nGalef, Julia (2019) [The case for open borders (Bryan Caplan)](http://rationallyspeakingpodcast.org/242-why-consciousness-is-an-illusion-keith-frankish/), *Rationally Speaking*, November 11.\n\nRighetti, Luca & Fin Moorhouse (2021) [Bryan Caplan on causes of poverty and the case for open borders](https://hearthisidea.com/episodes/bryan), *Hear This Idea*, August 23.\n\nWiblin, Robert & Keiran Harris (2018) [Economist Bryan Caplan thinks education is mostly pointless showing off. We test the strength of his case](https://80000hours.org/podcast/episodes/bryan-caplan-case-for-and-against-education/), *80,000 Hours*, May 22.\n\nWiblin, Robert & Keiran Harris (2022) [Bryan Caplan on why lazy parenting is actually OK](https://80000hours.org/podcast/episodes/bryan-caplan-parenting-workers-betting/), *80,000 Hours*, April 5.  \n*A wide-ranging interview with Bryan Caplan on parenting, education, open borders, philosophy, betting, and more.*\n\nRelated entries\n---------------\n\n[ideological Turing test](https://forum.effectivealtruism.org/tag/ideological-turing-test) | [immigration reform](https://forum.effectivealtruism.org/tag/immigration-reform) | [land use reform](https://forum.effectivealtruism.org/tag/land-use-reform) | [parenting](https://forum.effectivealtruism.org/tag/parenting) | [Michael Huemer](https://forum.effectivealtruism.org/topics/michael-huemer) | [Robin Hanson](https://forum.effectivealtruism.org/tag/robin-hanson) | [Tyler Cowen](https://forum.effectivealtruism.org/tag/tyler-cowen)\n\n1.  ^**[^](#fnreffwvr60ndj66)**^\n    \n    Caplan, Bryan (2007) [*The Myth of the Rational Voter: Why Democracies Choose Bad Policies*](https://en.wikipedia.org/wiki/Special:BookSources/978-0-691-13873-2), Princeton: Princeton University Press.\n    \n2.  ^**[^](#fnref2kwnno0qcq3)**^\n    \n    Caplan, Bryan (2011) [*Selfish Reasons to Have More Kids: Why Being a Great Parent Is Less Work and More Fun than You Think*](https://en.wikipedia.org/wiki/Special:BookSources/978-0-465-01867-3), New York: Basic Books.\n    \n3.  ^**[^](#fnrefq5q7zgrmxf)**^\n    \n    Caplan, Bryan (2018) [*The Case against Education: Why the Education System Is a Waste of Time and Money*](https://en.wikipedia.org/wiki/Special:BookSources/978-0-691-19645-9), Princeton: Princeton University Press.\n    \n4.  ^**[^](#fnrefdmhmqeujgvb)**^\n    \n    Caplan, Bryan & Zach Weinersmith (2019) [*Open Borders: The Science and Ethics of Immigration*](https://en.wikipedia.org/wiki/Special:BookSources/978-1-250-31696-7), New York: First Second."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "qSY46CSmJxwWxjDon",
    "name": "Broad vs. narrow interventions",
    "core": false,
    "slug": "broad-vs-narrow-interventions",
    "oldSlugs": null,
    "postCount": 6,
    "description": {
      "markdown": "The philosopher [Nick Beckstead](https://forum.effectivealtruism.org/tag/nick-beckstead) has distinguished between two different ways of influencing the long-term future: **broad interventions**, which \"focus on unforeseeable benefits from ripple effects\", and **narrow** (or **targeted**) **interventions**, which \"aim for more specific effects on the far future, or aim at a relatively narrow class of possible ripple effects.\"^[\\[1\\]](#fn8llkay3btxe)^\n\nClarifying the distinction\n--------------------------\n\nThe chain of causation connecting an intervention with its intended effect can be analysed along two separate dimensions. One dimension concerns the number of *causal steps* in the chain. Another dimension concerns the number of *causal paths* in the chain. In one sense of the term, broad interventions involve both many steps and many paths, while narrow interventions involve both few steps and few paths. For example, the broad intervention of promoting peace can reduce [existential risk](https://forum.effectivealtruism.org/tag/existential-risk) in countless different ways, each of which involves a long sequence of events culminating in the risk reduction. By contrast, the narrow intervention of [distributing bed nets](https://forum.effectivealtruism.org/tag/mass-distribution-of-long-lasting-insecticide-treated-nets) saves lives in just one way (by protecting people from mosquito bites) and in just a few steps (distribution > installation > protection).\n\nHowever, interventions with many causal steps may have few causal paths, and interventions with many causal paths may have few causal steps. It is therefore convenient to have separate terms for each of these dimensions of variation. Some effective altruists reserve the terms \"narrow\" and \"broad\" for interventions with few or many causal paths, and use the terms \"direct\" and \"indirect\" for interventions with few or many causal steps.^[\\[2\\]](#fnviy6vh108e)^\n\nAssessing broad and narrow interventions\n----------------------------------------\n\nA number of arguments in favor of either broad or narrow interventions have been offered.^[\\[3\\]](#fnz970yuy0av)^ A commonly given consideration in favor of broad interventions concerns their apparently superior historical track record. This point has been made independently by a number of authors at around the same time.^[\\[4\\]](#fngptc7ipqmxb)^ Beckstead himself writes:^[\\[5\\]](#fnjn1xcjw95rr)^\n\n> Suppose that in 1500 CE, someone wrote a forward-looking novel that featured a technology from the present day, such as a telephone. And suppose another person read this novel and then set for himself the goal that, in the future, people utilized rapid long-distance communication as effectively as possible. He would know that if making telephones was actually a good idea, future people would be in a much better position to find a way to create telephones and use them effectively. He would know very little about telephones or hw they might be discovered, so it would not make sense offer him to do something very targeted, such as drafting potential telephone designs. It would make more sense, I believe, for him to help in very broad ways (such as becoming a teacher or fighting political and religious threats to the advance of science), thereby empowering future generations to discover and effectively utilize rapid long-distance communication.\n\nSimilarly, [Brian Tomasik](https://forum.effectivealtruism.org/tag/brian-tomasik) writes:^[\\[6\\]](#fnre8nwe9bflb)^\n\n> imagine an effective altruist in the year 1800 trying to optimize his positive impact. He would not know most of modern economics, political science, game theory, physics, cosmology, biology, cognitive science, psychology, business, philosophy, probability theory, computation theory, or manifold other subjects that would have been crucial for him to consider. If he tried to place his bets on the most significant object-level issue that would be relevant centuries later, he'd almost certainly get it wrong. I doubt we would fare substantially better today at trying to guess a specific, concrete area of focus more than a few decades out. \\[...\\] What this 1800s effective altruist might have guessed correctly would have been the importance of world peace, philosophical reflection, positive-sum social institutions, and wisdom. Promoting those in 1800 may have been close to the best thing this person could have done, and this suggests that these may remain among the best options for us today.\n\nAnd Gwern Branwen writes:^[\\[7\\]](#fn95ckmnkc7hc)^\n\n> Imagine someone in England in 1500 who reasons the same way about x-risk: humanity might be destroyed, so preventing that is the most important task possible. He then spends the rest of his life researching the Devil and the Apocalypse. Such research is, unfortunately, of no value whatsoever unless it produces arguments for atheism demonstrating that that entire line of enquiry is useless and should not be pursued further. But as the Industrial and Scientific Revolutions were just beginning, with exponential increases in global wealth and science and technology and population, ultimately leading to vaccine technology, rockets and space programs, and enough wealth to fund all manner of investments in x-risk reduction, he could instead had made a perhaps small but real contribution by contributing to economic growth by work & investment or making scientific discoveries.\n\nIn response to these claims, [Toby Ord](https://forum.effectivealtruism.org/tag/toby-ord) argues that comparisons with previous centuries may be misleading, because the bulk of the existential risk to which humanity is currently exposed is [anthropogenic](https://forum.effectivealtruism.org/tag/anthropogenic-existential-risks) in nature, and originates in technologies developed only since around the mid-20th century. Narrow interventions aimed specifically at mitigating the risks posed by such technologies should thus be expected to accomplish much more than similar efforts in previous centuries. Ord also points out that broad interventions receive tens of thousands of times more funding than do narrow interventions, so even people with reasonable differences about the relative merits of broad and targeted interventions should favor the latter, given their much higher [neglectedness](https://forum.effectivealtruism.org/tag/neglectedness).^[\\[8\\]](#fn01oq5yvmcjoj)^\n\nFurther reading\n---------------\n\nBeckstead, Nick (2013) [*On the Overwhelming Importance of Shaping the Far Future*](http://doi.org/10.7282/T35M649T), Doctoral thesis, Rutgers University.\n\nKoehler, Arden, Benjamin Todd, Robert Wiblin & Keiran Harris (2020) [Benjamin Todd on varieties of longtermism and things 80,000 Hours might be getting wrong](https://80000hours.org/podcast/episodes/ben-todd-on-varieties-of-longtermism), *The 80,000 Hours Podcast*, September.\n\nOrd, Toby (2020) [*The Precipice: Existential Risk and the Future of Humanity*](https://en.wikipedia.org/wiki/Special:BookSources/1526600218), London: Bloomsbury Publishing.\n\nWiblin, Robert (2015) [What is a “broad intervention” and what is a “narrow intervention”? Are we confusing ourselves?](https://forum.effectivealtruism.org/posts/yisrgRsi4v3uyhujw/what-is-a-broad-intervention-and-what-is-a-narrow), *Effective Altruism Forum*, December 19.\n\nRelated entries\n---------------\n\n[civilizational collapse](/tag/civilizational-collapse) | [existential risk factor](https://forum.effectivealtruism.org/tag/existential-risk-factor) | [indirect long-term effects](https://forum.effectivealtruism.org/tag/indirect-long-term-effects)\n\n1.  ^**[^](#fnref8llkay3btxe)**^\n    \n    Beckstead, Nick (2013) [*On the Overwhelming Importance of Shaping the Far Future*](http://doi.org/10.7282/T35M649T), Doctoral thesis, Rutgers University.\n    \n2.  ^**[^](#fnrefviy6vh108e)**^\n    \n    Cotton-Barratt, Owen (2015) [Comment on “What is a ‘broad intervention’ and what is a ‘narrow intervention’? Are we confusing ourselves?”](https://forum.effectivealtruism.org/posts/yisrgRsi4v3uyhujw/what-is-a-broad-intervention-and-what-is-a-narrow?commentId=AurDJ4sxceAXBFEXZ), *Effective Altruism Forum*, December 19.\n    \n3.  ^**[^](#fnrefz970yuy0av)**^\n    \n    For example, Nick Beckstead (2013) [How to compare broad and targeted attempts to shape the far future](http://intelligence.org/wp-content/uploads/2013/07/Beckstead-Evaluating-Options-Using-Far-Future-Standards.pdf), July 13.\n    \n4.  ^**[^](#fnrefgptc7ipqmxb)**^\n    \n    The philosopher J. J. C. Smart made this point decades earlier: \"Could Jeremy Bentham or Karl Marx (to take two very different political theorists) have foreseen the atom bomb? Could they have foreseen automation? Can we foresee the technology of the next century?\" (Smart, J. J. C. (1973) [An outline of a system of utilitarian ethics](http://doi.org/10.1017/CBO9780511840852.001), in J. J. C. Smart & Bernard Williams (eds.) *Utilitarianism: For and Against*, Cambridge: Cambridge University Press, pp. 1–74, p. 64)\n    \n5.  ^**[^](#fnrefjn1xcjw95rr)**^\n    \n    Beckstead, Nick (2013) [*On the Overwhelming Importance of Shaping the Far Future*](http://doi.org/10.7282/T35M649T), p. 145.\n    \n6.  ^**[^](#fnrefre8nwe9bflb)**^\n    \n    Tomasik, Brian (2013) [Charity cost-effectiveness in an uncertain world](https://longtermrisk.org/charity-cost-effectiveness-in-an-uncertain-world/), *Center on Long-Term Risk*, October 28.\n    \n7.  ^**[^](#fnref95ckmnkc7hc)**^\n    \n    Branwen, Gwern (2014) [Optimal existential risk reduction investment](https://www.gwern.net/Statistical-notes#optimal-existential-risk-reduction-investment), *Gwern.net*, July 17.\n    \n8.  ^**[^](#fnref01oq5yvmcjoj)**^\n    \n    Ord, Toby (2020) [*The Precipice: Existential Risk and the Future of Humanity*](https://en.wikipedia.org/wiki/Special:BookSources/1526600218), London: Bloomsbury Publishing, ch. 6."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "kAKc6CALvEw7fYkFo",
    "name": "Brian Tomasik",
    "core": false,
    "slug": "brian-tomasik",
    "oldSlugs": null,
    "postCount": 5,
    "description": {
      "markdown": "**Brian Tomasik** (born 15 March 1987) is an American researcher.\n\nTomasik's website, **Essays on Reducing Suffering** (originally called **Utilitarian Essays**), features over 130 articles on a wide range of topics, including [animal suffering](https://forum.effectivealtruism.org/tag/animal-welfare-1), [future suffering](https://forum.effectivealtruism.org/tag/hellish-existential-catastrophe), [consciousness](https://forum.effectivealtruism.org/tag/consciousness), and [ethics](https://forum.effectivealtruism.org/tag/moral-philosophy). His essay 'Why activists should consider making lots of money'^[\\[1\\]](#fnn6n89cl2vi)^ is the first explicit articulation of the argument for [earning to give](https://forum.effectivealtruism.org/tag/earning-to-give).^[\\[2\\]](#fnuafh2e794h)^ Perhaps more than any other individual, Tomasik is responsible for establishing the field of [wild animal welfare](https://forum.effectivealtruism.org/tag/wild-animal-welfare).\n\nBackground\n----------\n\nTomasik studied computer science, mathematics and statistics at Swarthmore College. Between 2009 and 2013, he worked at Microsoft as a software development engineer. He was an active contributor to [Felicifia](https://forum.effectivealtruism.org/tag/felicifia) for most of that forum's history, and served on the board of [Animal Charity Evaluators](https://forum.effectivealtruism.org/tag/animal-charity-evaluators) between 2012 and 2015. He co-founded the Foundational Research Institute in 2013 (renamed to the [Center on Long-Term Risk](https://forum.effectivealtruism.org/tag/center-on-long-term-risk) in 2020), and remains one of that institute's advisors. He is also an advisor at the [Center for Reducing Suffering](https://forum.effectivealtruism.org/tag/center-for-reducing-suffering).\n\nFurther reading\n---------------\n\nElmore, Holly & Aleš Flídr (2017) [The Turing Test #5: Brian Tomasik](https://harvardeapodcast.com/2017/09/17/episode-5-brian-tomasik/), *The Turing Test*, September 17.\n\nPerry, Lucas (2018) [The metaethics of joy, suffering, and artificial intelligence with Brian Tomasik and David Pearce](https://futureoflife.org/2018/08/16/ai-alignment-podcast-metaethics-of-joy-suffering-with-brian-tomasik-and-david-pearce/), *AI Alignment Podcast*, August 16.\n\nTomasik, Brian (2020) [Résumé](https://briantomasik.com/resume/), *Brian Tomasik’s Website*.\n\nWiblin, Robert (2012) [Interview with Brian Tomasik](https://80000hours.org/2012/11/interview-with-brian-tomasik/), *80,000 Hours*, November 10.\n\nExternal links\n--------------\n\n[Brian Tomasik](https://briantomasik.com/). Personal website.\n\n[Brian Tomasik](https://forum.effectivealtruism.org/users/brian_tomasik). [Effective Altruism Forum](https://forum.effectivealtruism.org/tag/effective-altruism-forum-1) account.\n\n[Essays on Reducing Suffering](https://reducing-suffering.org/). Tomasik's essays.\n\n1.  ^**[^](#fnrefn6n89cl2vi)**^\n    \n    Tomasik, Brian (2006) [Why activists should consider making lots of money](https://reducing-suffering.org/why-activists-should-consider-making-lots-of-money/), *Essays on Reducing Suffering*.\n    \n2.  ^**[^](#fnrefuafh2e794h)**^\n    \n    Kaufman, Jeff (2012) [History of “earning to give”](https://www.jefftk.com/p/history-of-earning-to-give), *Jeff Kaufman’s Blog*, September 18."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "EJb2KZYkRs8rtNG7f",
    "name": "Center on Long-Term Risk",
    "core": false,
    "slug": "center-on-long-term-risk",
    "oldSlugs": null,
    "postCount": 45,
    "description": {
      "markdown": "The **Center on Long-Term Risk** (**CLR**) is a research institute that aims to mitigate [s-risks](https://forum.effectivealtruism.org/tag/s-risk) from advanced AI. Its research agenda focuses on encouraging cooperative behavior in and avoiding conflict between [transformative AI systems](https://forum.effectivealtruism.org/topics/transformative-artificial-intelligence).^[\\[1\\]](#fnhzxpzna8km9)^\n\nHistory\n-------\n\nCLR was founded in July 2013 as the **Foundational Research Institute**;^[\\[2\\]](#fnadclyyn92gt)^ it adopted its current name in March 2020.^[\\[3\\]](#fncvpov2dkx7n)^ CLR is part of the [Effective Altruism Foundation](https://forum.effectivealtruism.org/tag/effective-altruism-foundation).\n\nFunding\n-------\n\nAs of June 2022, CLR has received over $1.2 million in funding from the [Survival and Flourishing](https://forum.effectivealtruism.org/topics/survival-and-flourishing) Fund.^[\\[4\\]](#fn3qx0d0u2h3n)^ \n\nFurther reading\n---------------\n\nRice, Issa (2018) [Timeline of Foundational Research Institute](https://timelines.issarice.com/wiki/Timeline_of_Foundational_Research_Institute), *Timelines Wiki*.\n\nTorges, Stefan (2022) [CLR’s annual report 2021](https://forum.effectivealtruism.org/posts/BJk3TrEzsdSiuJTKa/clr-s-annual-report-2021), *Effective Altruism Forum*, February 26.\n\nExternal links\n--------------\n\n[Center on Long-Term Risk](https://longtermrisk.org/). Official website.\n\n[Apply for a job](https://longtermrisk.org/work-with-us/).\n\nRelated entries\n---------------\n\n[AI risk](https://forum.effectivealtruism.org/topics/ai-risk) | [Effective Altruism Foundation](https://forum.effectivealtruism.org/tag/effective-altruism-foundation/) | [s-risk](https://forum.effectivealtruism.org/tag/s-risk)\n\n1.  ^**[^](#fnrefhzxpzna8km9)**^\n    \n    Clifton, Jesse (2020) [Cooperation, Conflict, and Transformative Artificial Intelligence: A Research Agenda](https://longtermrisk.org/research-agenda), *Center on Long-Term Risk*.\n    \n2.  ^**[^](#fnrefadclyyn92gt)**^\n    \n    Center on Long-Term Risk (2020) [Transparency](https://longtermrisk.org/transparency), *Center on Long-Term Risk*, November.\n    \n3.  ^**[^](#fnrefcvpov2dkx7n)**^\n    \n    Vollmer, Jonas (2020) [EAF/FRI are now the Center on Long-Term Risk (CLR)](https://ea-foundation.org/blog/eaf-fri-are-now-clr/), *Effective Altruism Foundation*, March 6.\n    \n4.  ^**[^](#fnref3qx0d0u2h3n)**^\n    \n    Survival and Flourishing Fund (2020) [SFF-2021-H1 S-process recommendations announcement](https://survivalandflourishing.fund/sff-2021-h1-recommendations), *Survival and Flourishing Fund*."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "3yv56vTrjpGn79Pko",
    "name": "S-risk",
    "core": false,
    "slug": "s-risk",
    "oldSlugs": [
      "s-risks"
    ],
    "postCount": 42,
    "description": {
      "markdown": "An **s-risk**, or **suffering risk**, is a risk involving the creation of [suffering](https://forum.effectivealtruism.org/tag/pain-and-suffering) on an astronomical scale.\n\nEvaluation\n----------\n\n[80,000 Hours](https://forum.effectivealtruism.org/tag/80-000-hours) rates s-risks a \"potential highest priority area\": an issue that, if more thoroughly examined, could rank as a top global challenge.^[\\[1\\]](#fnzdokdqtfa0e)^\n\nFurther reading\n---------------\n\nAlthaus, David & Lukas Gloor (2019) [Reducing risks of astronomical suffering: a neglected priority](https://longtermrisk.org/reducing-risks-of-astronomical-suffering-a-neglected-priority/), *Center on Long-Term Risk*, August.\n\nBaumann, Tobias (2017) [S-risks: an introduction](http://centerforreducingsuffering.org/intro/), *Center for Reducing Suffering*, August 15.\n\nTomasik, Brian (2019) [Risks of astronomical future suffering](https://longtermrisk.org/risks-of-astronomical-future-suffering/), *Center on Long-Term Risk*, July 2.\n\nRelated entries\n---------------\n\n[Center for Reducing Suffering](https://forum.effectivealtruism.org/tag/center-for-reducing-suffering) | [Center on Long-Term Risk](https://forum.effectivealtruism.org/tag/center-on-long-term-risk) | [ethics of existential risk](https://forum.effectivealtruism.org/tag/ethics-of-existential-risk) | [hellish existential catastrophe](https://forum.effectivealtruism.org/tag/hellish-existential-catastrophe) | [pain and suffering](https://forum.effectivealtruism.org/tag/pain-and-suffering) | [suffering-focused ethics](https://forum.effectivealtruism.org/tag/suffering-focused-ethics)\n\n1.  ^**[^](#fnrefzdokdqtfa0e)**^\n    \n    80,000 Hours (2022) [Our current list of pressing world problems](https://80000hours.org/problem-profiles/), *80,000 Hours*."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "3guG7mGfaDEhxL3gg",
    "name": "Bill & Melinda Gates Foundation",
    "core": false,
    "slug": "bill-and-melinda-gates-foundation",
    "oldSlugs": null,
    "postCount": 3,
    "description": {
      "markdown": "The **Bill & Melinda Gates Foundation** (**BMGF**) is an American private foundation with a primary focus on [global health](https://forum.effectivealtruism.org/tag/global-health-and-development) and [global poverty](https://forum.effectivealtruism.org/tag/global-poverty).\n\nFurther reading\n---------------\n\nLangley, Kathryn (2016) [Gates Foundation](https://www.britannica.com/topic/Gates-Foundation), *Encyclopedia Britannica*, August 1 (updated 23 January 2019).\n\nRice, Issa & Sebastián Sánchez (2017) [Timeline of Bill & Melinda Gates Foundation](https://timelines.issarice.com/wiki/Timeline_of_Bill_%26_Melinda_Gates_Foundation), *Timelines Wiki*, May 9.\n\nExternal links\n--------------\n\n[Bill & Melinda Gates Foundation](https://www.gatesfoundation.org/). Official website.\n\n[Apply for a job](https://www.gatesfoundation.org/about/careers).\n\nRelated entries\n---------------\n\n [global health and development](https://forum.effectivealtruism.org/tag/global-health-and-development) | [global poverty](https://forum.effectivealtruism.org/tag/global-poverty)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "nD8GdYX2Bbt5gdZpd",
    "name": "Bertrand Russell",
    "core": false,
    "slug": "bertrand-russell",
    "oldSlugs": null,
    "postCount": 1,
    "description": {
      "markdown": "**Bertrand Arthur William Russell** (18 May 1872 – 2 February 1970) was a British philosopher and social activist.\n\nFurther reading\n---------------\n\nIrvine, Andrew David (1995) [Bertrand Russell](https://plato.stanford.edu/archives/win2020/entries/russell/), *The Stanford Encyclopedia of Philosophy*, December 7 (updated 27 May 2020).\n\nLenz, John R. (1996) [Pugwash and Russell’s legacy](https://users.drew.edu/~jlenz/pugwash.html), *The Bertrand Russell Society Quarterly*, vol. 89, pp. 18–24.\n\nSeckel, Al (1984) [Russell and the Cuban Missile Crisis](https://doi.org/10.15173/russell.v4i2.1632), *Russell: the Journal of Bertrand Russell Studies*, vol. 4, pp. 253–261.\n\nRelated entries\n---------------\n\n[John Stuart Mill](https://forum.effectivealtruism.org/tag/john-stuart-mill) | [nuclear disarmament movement](https://forum.effectivealtruism.org/tag/nuclear-disarmament-movement) |  [Pugwash Conferences on Science and World Affairs](https://forum.effectivealtruism.org/tag/pugwash-conferences-on-science-and-world-affairs) | [Russell–Einstein Manifesto](https://forum.effectivealtruism.org/tag/russell-einstein-manifesto) |[Trinity](https://forum.effectivealtruism.org/tag/trinity)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "A2ZzqESan6kFret2k",
    "name": "Berkeley Existential Risk Initiative",
    "core": false,
    "slug": "berkeley-existential-risk-initiative",
    "oldSlugs": null,
    "postCount": 16,
    "description": {
      "markdown": "The **Berkeley Existential Risk Initiative** (**BERI**) is a US-based public charity that collaborates with university research groups working to reduce existential risk, by providing them with free services and support.\n\nHistory\n-------\n\nBERI was founded in 2017 by Andrew Critch^[\\[1\\]](#fn4wms43uhyf6)^^[\\[2\\]](#fnzpt6vnc14b)^ and has been run by Sawyer Bernath since 2020.^[\\[3\\]](#fn0wja4x7p409b)^\n\nActivities\n----------\n\nA full list of the organizations BERI helps is available [here](https://existence.org/collaborations/). Detailed information about BERI's activities can be found on [BERI's transparency page](https://existence.org/transparency/).\n\nFunding\n-------\n\nAs of July 2022, BERI has received >$7 million in funding (both for general support and collaborations with other organizations) from Jaan Tallinn, >$3.3 million from [Open Philanthropy](https://forum.effectivealtruism.org/topics/open-philanthropy),^[\\[4\\]](#fnxxmimmzylc)^ >1.6 million from the [Survival and Flourishing Fund](https://forum.effectivealtruism.org/topics/survival-and-flourishing),^[\\[5\\]](#fn4y7bjeijz49)^^[\\[6\\]](#fni5k53pwbbni)^^[\\[7\\]](#fnk601canmxs)^^[\\[8\\]](#fnlq6nn6w6lud)^^[\\[9\\]](#fn1t5b85xvrxc)^ $100,000 from the [Future Fund](https://forum.effectivealtruism.org/topics/future-fund),^[\\[10\\]](#fnoirnc4gts5b)^ and ~$5 million from other funders. Details can be found on [BERI's transparency page](https://existence.org/transparency/#donors).\n\nFurther reading\n---------------\n\nRice, Issa *et al* (2018) [Timeline of Berkeley Existential Risk Initiative](https://timelines.issarice.com/wiki/Timeline_of_Berkeley_Existential_Risk_Initiative), *Timelines Wiki*.\n\nExternal links\n--------------\n\n[Berkeley Existential Risk Initiative](https://existence.org/). Official website.\n\n[Apply for a job](https://existence.org/jobs/).\n\n[Donate to BERI](https://existence.org/donating/).\n\nRelated entries\n---------------\n\n[Center for Human-Compatible Artificial Intelligence](https://forum.effectivealtruism.org/tag/center-for-human-compatible-artificial-intelligence) | [Centre for the Study of Existential Risk](https://forum.effectivealtruism.org/tag/centre-for-the-study-of-existential-risk) | [existential risk](https://forum.effectivealtruism.org/tag/existential-risk) | [Future of Humanity Institute](https://forum.effectivealtruism.org/tag/future-of-humanity-institute)\n\n1.  ^**[^](#fnref4wms43uhyf6)**^\n    \n    Vaughan, Kerry (2017) [Update on Effective Altruism Funds](https://forum.effectivealtruism.org/posts/MsaS8JKrR8nnxyPkK/update-on-effective-altruism-funds), *Effective Altruism Forum*, April 20.\n    \n2.  ^**[^](#fnrefzpt6vnc14b)**^\n    \n    Berkeley Existential Risk Initiative (2018) [Semi-annual report](https://existence.org/docs/08-2017-semi-annual-report.pdf), *Berkeley Existential Risk Initiative*, August.\n    \n3.  ^**[^](#fnref0wja4x7p409b)**^\n    \n    Berkeley Existential Risk Initiative (2020) [Board and Staff Changes](https://existence.org/board-and-staff-changes), *Andrew Critch*, February\n    \n4.  ^**[^](#fnrefxxmimmzylc)**^\n    \n    Open Philanthropy (2022) [Grants database: Berkeley Existential Risk Initiative](https://www.openphilanthropy.org/grants/?q=&organization-name=berkeley-existential-risk-initiative), *Open Philanthropy*.\n    \n5.  ^**[^](#fnref4y7bjeijz49)**^\n    \n    Survival and Flourishing Fund (2019) [SFF-2020-H2 S-process recommendations announcement](https://survivalandflourishing.fund/sff-2020-h2-recommendations), *Survival and Flourishing Fund*.\n    \n6.  ^**[^](#fnrefi5k53pwbbni)**^\n    \n    Survival and Flourishing Fund (2019a) [SFF-2020-H1 S-process recommendations announcement](https://survivalandflourishing.fund/sff-2020-h1-recommendations), *Survival and Flourishing Fund*.\n    \n7.  ^**[^](#fnrefk601canmxs)**^\n    \n    Survival and Flourishing Fund (2019b) [SFF-2020-H2 S-process recommendations announcement](https://survivalandflourishing.fund/sff-2020-h2-recommendations), *Survival and Flourishing Fund*.\n    \n8.  ^**[^](#fnreflq6nn6w6lud)**^\n    \n    Survival and Flourishing Fund (2020a) [SFF-2021-H1 S-process recommendations announcement](https://survivalandflourishing.fund/sff-2021-h1-recommendations), *Survival and Flourishing Fund*.\n    \n9.  ^**[^](#fnref1t5b85xvrxc)**^\n    \n    Survival and Flourishing Fund (2020b) [SFF-2021-H2 S-process recommendations announcement](https://survivalandflourishing.fund/sff-2021-h2-recommendations), *Survival and Flourishing Fund*.\n    \n10.  ^**[^](#fnrefoirnc4gts5b)**^\n    \n    Future Fund (2022) [Our grants and investments: Berkeley Existential Risk Initiative](https://ftxfuturefund.org/our-grants/?_organization_name=berkeley-existential-risk-initiative), *Future Fund*."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "EffJiWCBbFn5Z3iMb",
    "name": "Basic AI drive",
    "core": false,
    "slug": "basic-ai-drive",
    "oldSlugs": null,
    "postCount": 1,
    "description": {
      "markdown": "A **basic AI drive** is a tendency that virtually any [AI system](https://forum.effectivealtruism.org/tag/artificial-intelligence) is expected to possess by default unless explicitly counteracted by human designers.^[\\[1\\]](#fnnsrrozxq8gb)^\n\nFurther reading\n---------------\n\nOmohundro, Stephen M. (2007) [The nature of self-improving artificial intelligence](https://selfawaresystems.com/2007/10/05/paper-on-the-nature-of-self-improving-artificial-intelligence/), Self-aware systems.\n\nOmohundro, Stephen M. (2008) [The basic AI drives](https://en.wikipedia.org/wiki/Special:BookSources/978-1-58603-833-5), in Pei Wang, Ben Goertzel & Stan Franklin (eds.) *Artificial General Intelligence, 2008: Proceedings of the First AGI Conference*, Amsterdam: IOS Press, pp. 483–492.\n\nShulman, Carl (2010) [Omohundro’s \"basic AI drives \" and catastrophic risks](https://intelligence.org/files/BasicAIDrives.pdf), Machine Intelligence Research Institute.\n\nRelated entries\n---------------\n\n[instrumental convergence thesis](https://forum.effectivealtruism.org/tag/instrumental-convergence)\n\n1.  ^**[^](#fnrefnsrrozxq8gb)**^\n    \n    Omohundro, Stephen M. (2008) [The basic AI drives](https://en.wikipedia.org/wiki/Special:BookSources/978-1-58603-833-5), in Pei Wang, Ben Goertzel & Stan Franklin (eds.) *Artificial General Intelligence, 2008: Proceedings of the First AGI Conference*, Amsterdam: IOS Press, pp. 483–492."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "gn5XrdqN4bwix9CT8",
    "name": "Astronomical waste",
    "core": false,
    "slug": "astronomical-waste",
    "oldSlugs": null,
    "postCount": 9,
    "description": {
      "markdown": "**Astronomical waste** is the loss of potential value resulting from delaying the efficient exploitation of the [universe's resources](https://forum.effectivealtruism.org/tag/universe-s-resources). The term and the concept expressed by it were introduced by [Nick Bostrom](https://forum.effectivealtruism.org/tag/nick-bostrom) in a seminal paper.^[\\[1\\]](#fn50capbten0t)^\n\nThe accessible universe is vast, and virtually all of it remains unexploited. The Virgo Supercluster contains \\\\(10^{13}\\\\) stars, and the energy of each star could power \\\\(10^{42}\\\\) computations per second. The human brain can perform about \\\\(10^{17}\\\\) computations per second. Assuming that the morally relevant properties of the brain—such as phenomenal consciousness—supervene on its functional organization, it follows that the universe could support, every second, an amount of value equivalent to that realized in \\\\(10^{13} \\\\times 10^{42} \\\\div 10^{17} = 10^{28}\\\\) human lives. The moral costs of failing to actualize this potential thus appear to be enormous.\n\nIn relative terms, however, the costs may be quite modest. The cosmos has existed for about 10 billion years, so one should not antecedently expect cosmological processes to cause value to decay by more than 1 part in 10 billion or so per year. And the observational evidence appears to be consistent with this prior assessment. The finitude, expansion, and burndown of the universe seem all to be occurring at a slow enough rate as to be in line with the estimate based on the duration of the universe so far.^[\\[2\\]](#fn38v46w5bueu)^\n\nIf the opportunity costs of delaying the exploitation of the universe's resources are so low in relative terms, however large they may be in absolute terms, it follows that such costs are unimportant relative to the costs arising from exposure to [existential risk](https://forum.effectivealtruism.org/tag/existential-risk), which are much higher in comparison. Over the next decade, maybe a billionth of total attainable value will be lost as a result of failing to arrange the universe optimally. Over that same decade, perhaps a thousandth of this value will be lost in expectation from exposure to a 0.1 percent risk of an existential catastrophe. The costs from existential risk exposure thus appear to exceed the opportunity costs from delayed expansion by several orders of magnitude.\n\nThus, although upon first noticing the astronomical costs of delayed technological development an altruist may be tempted to conclude that such development should be hastened, that conclusion does not survive careful reflection. Because the bulk of existential risk is posed by [anthropogenic existential risks](https://forum.effectivealtruism.org/tag/anthropogenic-existential-risks) from new technologies, accelerating the development of new technology will itself have major effects on existential risk. Such effects will dwarf any gains from reduction of astronomical waste, and should therefore be the primary consideration for decision making.\n\nA note about terminology\n------------------------\n\nAstronomical waste is often cited as a consideration in favor of [longtermism](https://forum.effectivealtruism.org/tag/longtermism). When authors talk about \"astronomical waste\" in these contexts, however, what they typically mean by that phrase are not the costs of delayed expansion, but the costs of failed (or flawed) expansion. Thus, [Carl Shulman](https://forum.effectivealtruism.org/tag/carl-shulman) mentions \"the expected Astronomical Waste if humanity were rendered extinct by a sudden asteroid impact.\"^[\\[3\\]](#fn8cmzntaw7z9)^ Similarly, linking to Bostrom's paper, Gwern Branwen writes that \"human extinction represents the loss of literally astronomical amounts of utility.\"^[\\[4\\]](#fnnwz5spo9tj)^ And Siebe Rozendal writes that \"Extinction would be an ‘astronomical waste’.\"^[\\[5\\]](#fnvn45cr797ss)^^[\\[6\\]](#fntew3mbba4k)^ The expression *astronomical stakes*^[\\[7\\]](#fnd9jw30oybef)^^[\\[8\\]](#fnlb2an4aje6m)^ may be used to express this idea, while reserving *astronomical waste* to refer to the  opportunity costs of delayed technological development.\n\nFurther reading\n---------------\n\nBostrom, Nick (2003) [Astronomical waste: the opportunity cost of delayed technological development](https://doi.org/10.1017/S0953820800004076), *Utilitas*, vol. 15, pp. 308–314.\n\nChristiano, Paul (2013) [Astronomical waste](https://rationalaltruist.com/2013/04/30/astronomical-waste/), *Rational Altruist*, April 30.\n\nRelated entries\n---------------\n\n[differential progress](https://forum.effectivealtruism.org/tag/differential-progress) | [ethics of existential risk](https://forum.effectivealtruism.org/tag/ethics-of-existential-risk) | [space colonization](https://forum.effectivealtruism.org/tag/space-colonization) | [speeding up development](https://forum.effectivealtruism.org/tag/speeding-up-development)\n\n1.  ^**[^](#fnref50capbten0t)**^\n    \n    Bostrom, Nick (2003) [Astronomical waste: the opportunity cost of delayed technological development](https://doi.org/10.1017/S0953820800004076), *Utilitas*, vol. 15, pp. 308–314.\n    \n2.  ^**[^](#fnref38v46w5bueu)**^\n    \n    Christiano, Paul (2013) [Astronomical waste](https://rationalaltruist.com/2013/04/30/astronomical-waste/), *Rational Altruist*, April 30.\n    \n3.  ^**[^](#fnref8cmzntaw7z9)**^\n    \n    Shulman, Carl (2012) [Are pain and pleasure equally energy-efficient?](http://reflectivedisequilibrium.blogspot.com/2012/03/are-pain-and-pleasure-equally-energy.html), *Reflective Disequilibrium*, March 24.\n    \n4.  ^**[^](#fnrefnwz5spo9tj)**^\n    \n    Branwen, Gwern (2020) [Optimal existential risk reduction investment](https://www.gwern.net/Statistical-notes#optimal-existential-risk-reduction-investment), *Gwern.net*, May 28.\n    \n5.  ^**[^](#fnrefvn45cr797ss)**^\n    \n    Rozendal, Siebe (2019) [Eight high-level uncertainties about global catastrophic and existential risk](https://forum.effectivealtruism.org/posts/QjKRBcobCzeeerMbP/eight-high-level-uncertainties-about-global-catastrophic-and), *Effective Altruism Forum*, November 28.\n    \n6.  ^**[^](#fnreftew3mbba4k)**^\n    \n    See also Wei Dai (2014) [Is the potential astronomical waste in our universe too small to care about?](https://www.lesswrong.com/posts/BNbxueXEcm6dCkDuk/is-the-potential-astronomical-waste-in-our-universe-too), *LessWrong*, October 21, Gregory Lewis (2018) [The person-affecting value of existential risk reduction](https://forum.effectivealtruism.org/posts/dfiKak8ZPa46N7Np6/the-person-affecting-value-of-existential-risk-reduction), *Effective Altruism Forum*, April 13, and David Kristoffersson (2020) [The ‘far future’ is not just the far future](https://forum.effectivealtruism.org/posts/X5aJKx3f6z5sX2Ji4/the-far-future-is-not-just-the-far-futu), *Effective Altruism Forum*, January 16.\n    \n7.  ^**[^](#fnrefd9jw30oybef)**^\n    \n    Bostrom, Nick (2015) [Astronomical stakes](https://www.youtube.com/watch?v=fmQkLKLmfQU), *Effective Altruism Global*, November 25.\n    \n8.  ^**[^](#fnreflb2an4aje6m)**^\n    \n    Wiblin, Robert (2016) [Making sense of long-term indirect effects](https://www.effectivealtruism.org/articles/making-sense-of-long-term-indirect-effects-rob-wiblin/), *Effective Altruism*, August 7."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "zHqdAsiiXiyaS3zw9",
    "name": "Asteroids",
    "core": false,
    "slug": "asteroids",
    "oldSlugs": null,
    "postCount": 7,
    "description": {
      "markdown": "**Asteroids** are small, rocky objects that orbit the Sun. Depending on their size, asteroids with the potential to collide with planet Earth could pose a [global catastrophic risk](https://forum.effectivealtruism.org/tag/global-catastrophic-risk) or even an [existential risk](https://forum.effectivealtruism.org/tag/existential-risk).\n\nRecommendations\n---------------\n\nIn [*The Precipice: Existential Risk and the Future of Humanity*](https://forum.effectivealtruism.org/tag/the-precipice), [Toby Ord](https://forum.effectivealtruism.org/tag/toby-ord) offers several [policy](https://forum.effectivealtruism.org/tag/policy) and [research](https://forum.effectivealtruism.org/tag/research) recommendations for handling risks from asteroids and comets:^[\\[1\\]](#fnxjqcoyeuy58)^\n\n*   Research the deflection of 1 km+ asteroids and comets, perhaps restricted to methods that couldn’t be weaponized such as those that don’t lead to accurate changes in trajectory.\n*   Bring short-period comets into the same risk framework as near-Earth asteroids.\n*   Improve our understanding of the risks from long-period comets.\n*   Improve our modeling of impact winter scenarios, especially for 1–10 km asteroids. Work with experts in climate modeling and nuclear winter modeling to see what modern models say.\n\nFurther reading\n---------------\n\nNewberry, Toby (2021) [How cost-effective are efforts to detect near-Earth-objects?](https://globalprioritiesinstitute.org/wp-content/uploads/Toby-Newberry_How-cost-effective-are-efforts-to-detect-near-Earth-Objects.pdf), GPI technical report no. T1-2021, Global Priorities Institute, University of Oxford.\n\n1.  ^**[^](#fnrefxjqcoyeuy58)**^\n    \n    Ord, Toby (2020) [*The Precipice: Existential Risk and the Future of Humanity*](https://en.wikipedia.org/wiki/Special:BookSources/1-5266-0021-8), London: Bloomsbury Publishing, p. 277."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "pmALcrKQBDgFMyqqt",
    "name": "Global catastrophic risk",
    "core": false,
    "slug": "global-catastrophic-risk",
    "oldSlugs": null,
    "postCount": 72,
    "description": {
      "markdown": "A **global catastrophic risk** (**GCR**) is an event that poses a risk of major harm on a global scale.^[\\[1\\]](#fntw2j4u1d6pd)^\n\nGCRs risks include, but are not restricted to, [existential risks](https://forum.effectivealtruism.org/tag/existential-risk). Examples of non-existential GCRs include risks of hundreds of millions of people dying due to a [natural pandemic](https://forum.effectivealtruism.org/tag/pandemic-preparedness) or due to [anthropogenic climate change](https://forum.effectivealtruism.org/tag/climate-change).\n\nSuch catastrophic risks have obviously bad direct effects: they may involve many people dying, or our technological capabilities being greatly reduced. There may also be bad indirect effects, for instance by destabilizing political systems in a way that increases the likelihood of war or totalitarian government.\n\nSome GCRs which are not themselves existential risks could still increase existential risk via their indirect effects. Such GCRs may be regarded as [existential risk factors](https://forum.effectivealtruism.org/tag/existential-risk-factor), or as components of a [compound existential risk](https://forum.effectivealtruism.org/tag/compound-existential-risk). Arguably, climate change might increase political tensions, hastening [nuclear](https://forum.effectivealtruism.org/tag/nuclear-warfare-1) or [biological](https://forum.effectivealtruism.org/tag/global-catastrophic-biological-risk) warfare. Alternatively, civilization could eventually rebound to something like its previous state. The Black Death—the deadliest catastrophe in human history—killed something like 10% of the world’s population without obviously affecting humanity’s long-term potential.^[\\[2\\]](#fnenohcf5qapo)^\n\nEven if global catastrophic risks do not pose an existential risk, they might still be high priority causes justified purely by their nearer-term consequences.\n\nFurther reading\n---------------\n\nAird, Michael (2020) [Collection of some definitions of global catastrophic risks (GCRs)](https://forum.effectivealtruism.org/posts/EMKf4Gyee7BsY2RP8/michaela-s-shortform?commentId=vgqcDG9nMdMx8c3aj), *Effective Altruism Forum*, February 28.  \n*Many additional resources on this topic.*\n\nAvin, Shahar *et al.* (2018) [Classifying global catastrophic risks](http://doi.org/10.1016/j.futures.2018.02.001), *Futures*, vol. 102, pp. 20–26.\n\nBostrom, Nick & Milan Ćirković (eds.) (2008) [*Global Catastrophic Risks*](https://en.wikipedia.org/wiki/Special:BookSources/9780198570509), Oxford: Oxford University Press.\n\nCotton-Barratt, Owen *et al.* (2016) [Global catastrophic risks 2016](http://globalprioritiesproject.org/2016/04/global-catastrophic-risks-2016/), *Global Priorities Project*.  \n*A report examining various types of global catastrophic risk.*\n\nKoehler, Arden & Keiran Harris (2020) [Owen Cotton-Barratt on epistemic systems & layers of defence against potential global catastrophes](https://80000hours.org/podcast/episodes/owen-cotton-barratt-epistemic-systems/), *80,000 Hours*, December 16.\n\nOpen Philanthropy (2016) [Global catastrophic risks](https://www.openphilanthropy.org/focus/global-catastrophic-risks), *Open Philanthropy*, March 2.\n\nRelated entries\n---------------\n\n[civilizational collapse](/tag/civilizational-collapse) | [existential risk](/tag/existential-risk) | [existential risk factor](/tag/existential-risk-factor) | [global catastrophic biological risk](/tag/global-catastrophic-biological-risk) | [Nuclear Threat Initiative](https://forum.effectivealtruism.org/tag/nuclear-threat-initiative) | [warning shot](/tag/warning-shot)\n\n1.  ^**[^](#fnreftw2j4u1d6pd)**^\n    \n    Bostrom, Nick & Milan Ćirković (eds.) (2008) [*Global Catastrophic Risks*](https://en.wikipedia.org/wiki/Special:BookSources/9780198570509), Oxford: Oxford University Press.\n    \n2.  ^**[^](#fnrefenohcf5qapo)**^\n    \n    Muehlhauser, Luke (2017) [How big a deal was the Industrial Revolution?](http://lukemuehlhauser.com/industrial-revolution/), *Luke Muehlhauser’s Website*."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "hRJEG4TCvLRGKE4Fu",
    "name": "Artificial sentience",
    "core": false,
    "slug": "artificial-sentience",
    "oldSlugs": null,
    "postCount": 44,
    "description": {
      "markdown": "**Artificial sentience** (sometimes called **digital sentience** or **machine sentience**) is the capacity for [subjective experience](https://forum.effectivealtruism.org/tag/sentience-1) instantiated in artificial minds, such as arising from [artificial general intelligence](https://forum.effectivealtruism.org/tag/human-level-artificial-intelligence) or [whole brain emulation](https://forum.effectivealtruism.org/tag/whole-brain-emulation).\n\nEvaluation\n----------\n\n[80,000 Hours](https://forum.effectivealtruism.org/topics/80-000-hours) rates artificial sentience a \"potential highest priority area\": an issue that, if more thoroughly examined, could rank as a top global challenge.^[\\[1\\]](#fnzp6dgyhlew)^\n\nFurther reading\n---------------\n\nHarris, Jamie & Jacy Reese Anthis (2021) [The moral consideration of artificial entities: a literature review](https://doi.org/10.1007/s11948-021-00331-8), *Science and Engineering Ethics*, vol. 27.\n\nLong, Robert (2022) [Key questions about artificial sentience: an opinionated guide](https://experiencemachines.substack.com/p/key-questions-about-artificial-sentience), *Experience Machines*, April 25.\n\nShulman, Carl & Nick Bostrom (2021) [Sharing the world with digital minds](https://doi.org/10.1093/oso/9780192894076.001.0001), in Steven Clarke, Hazem Zohny & Julian Savulescu (eds.) *Rethinking Moral Status*, Oxford: Oxford University Press, pp. 306–326.\n\nRelated entries\n---------------\n\n[Center for Reducing Suffering](https://forum.effectivealtruism.org/tag/center-for-reducing-suffering) | [ethics of artificial intelligence](https://forum.effectivealtruism.org/tag/ethics-of-artificial-intelligence) | [long-term future](https://forum.effectivealtruism.org/tag/long-term-future) | [mind crime](https://forum.effectivealtruism.org/tag/mind-crime) | [moral circle expansion](https://forum.effectivealtruism.org/tag/moral-circle-expansion-1) | [moral patienthood](https://forum.effectivealtruism.org/tag/moral-patienthood) | [non-humans and the long-term future](https://forum.effectivealtruism.org/tag/non-humans-and-the-long-term-future) | [whole brain emulation](https://forum.effectivealtruism.org/tag/whole-brain-emulation)\n\n1.  ^**[^](#fnrefzp6dgyhlew)**^\n    \n    80,000 Hours (2022) [Our current list of pressing world problems](https://80000hours.org/problem-profiles/), *80,000 Hours*."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "Sgx48Pf8PzmTxSEEG",
    "name": "Aging research",
    "core": false,
    "slug": "aging-research",
    "oldSlugs": [
      "anti-aging-research"
    ],
    "postCount": 31,
    "description": {
      "markdown": "**Aging research** is research aimed at slowing down the aging process or at repairing the damage caused by it. At present, comparatively few resources are spent on aging research, relative to the benefits that breakthroughs in this area could bring about. For example, in 2019 the National Institutes of Health spent less than two percent of its budget on aging.^[\\[1\\]](#fnb1axnd3pnj)^\n\nThere is some disagreement concerning the best way to make progress on aging. Some researchers claim that progress depends crucially on improving our knowledge of the metabolic pathways involved in the aging process—this is the approach favored by most research institutions. An alternative approach seeks to find ways to periodically repair the cellular and molecular damage caused by aging, without necessarily understanding the aging process itself. The SENS Research Foundation, an organization explicitly set up with the aim of ultimately ending aging, has pioneered this alternative approach.^[\\[2\\]](#fnrp38d0gka5)^\n\nFurther reading\n---------------\n\nBarnett, Matthew (2020) [Effects of anti-aging research on the long-term future](https://forum.effectivealtruism.org/posts/LxmJJobC6DEneYSWB/effects-of-anti-aging-research-on-the-long-term-future), *Effective Altruism Forum*, February 27.\n\nBeckstead, Nick (2017) [Mechanisms of Aging](https://www.openphilanthropy.org/research/cause-reports/scientific-research/mechanisms-aging), *Open Philanthropy*, September.\n\nRelated entries\n---------------\n\n[transhumanism](https://forum.effectivealtruism.org/tag/transhumanism)\n\n1.  ^**[^](#fnrefb1axnd3pnj)**^\n    \n    National Institutes of Health (2020) [Estimates of funding for various research, condition, and disease categories (RCDC)](https://report.nih.gov/categorical_spending.aspx), February 24.\n    \n2.  ^**[^](#fnrefrp38d0gka5)**^\n    \n    De Grey, Aubrey & Michael Rae (2007) [*Ending Aging: The Rejuvenation Breakthroughs That Could Reverse Human Aging in Our Lifetime*](https://en.wikipedia.org/wiki/Special:BookSources/978-0-312-36706-0), New York: St. Martin’s Press."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "uEnvZnLG7Cbd8jeco",
    "name": "Anthropogenic existential risk",
    "core": false,
    "slug": "anthropogenic-existential-risk",
    "oldSlugs": [
      "anthropogenic-existential-risks"
    ],
    "postCount": 6,
    "description": {
      "markdown": "An **anthropogenic existential risk** is an [existential risk](https://forum.effectivealtruism.org/tag/existential-risk) arising from intentional or accidental human activity rather than underlying [natural processes](https://forum.effectivealtruism.org/tag/natural-existential-risk).\n\nNew technologies have played a huge part in the massive growth in human flourishing over the past centuries. However, they also pose some serious risks. [Nuclear weapons](https://forum.effectivealtruism.org/tag/nuclear-warfare-1), for example, may have created the potential for wars that result in human extinction. Other technologies may pose similar risks in the future, such as synthetic biology (see [global catastrophic biological risk](https://forum.effectivealtruism.org/tag/global-catastrophic-biological-risk)) and [artificial intelligence](https://forum.effectivealtruism.org/tag/ai-risks), as well as risks from [fundamental physics research](https://forum.effectivealtruism.org/tag/existential-risks-from-fundamental-physics-research) and [unknown risks](https://forum.effectivealtruism.org/tag/unknown-existential-risk).\n\nThat our species has so far survived both natural and anthropogenic risks puts an upper bound on how high these risks can be. But humanity has been exposed to natural risks throughout the entirety of its history, whereas anthropogenic risks have emerged only in the last century. This difference between these two types of risks implies that their respective upper bounds are also very different. Specifically, this consideration is generally believed to warrant the conclusion that anthropogenic risks are significantly higher than natural risks.^[\\[1\\]](#fnn6e74do5ms)^^[\\[2\\]](#fnyqbq9ej5ebr)^^[\\[3\\]](#fnl2jqcz9r8e)^ According to [Toby Ord](https://forum.effectivealtruism.org/tag/toby-ord), \"we face about a thousand times more anthropogenic risk over the next century than natural risk.\"^[\\[4\\]](#fnqokji1pqjxb)^ \n\nFurther reading\n---------------\n\nBeckstead, Nick *et al.* (2014) [Unprecedented technological risks](http://globalprioritiesproject.org/wp-content/uploads/2015/04/Unprecedented-Technological-Risks.pdf), Global Priorities Project.\n\nOrd, Toby (2020) [*The Precipice: Existential Risk and the Future of Humanity*](https://en.wikipedia.org/wiki/Special:BookSources/1526600218), London: Bloomsbury Publishing.\n\nRelated entries\n---------------\n\n[differential progress](https://forum.effectivealtruism.org/tag/differential-progress) | [natural existential risk](https://forum.effectivealtruism.org/topics/natural-existential-risk) | [vulnerable world hypothesis](/tag/vulnerable-world-hypothesis) | [weapons of mass destruction](https://forum.effectivealtruism.org/tag/weapons-of-mass-destruction)\n\n1.  ^**[^](#fnrefn6e74do5ms)**^\n    \n    Bostrom, Nick (2004) [The future of human evolution](https://en.wikipedia.org/wiki/Special:BookSources/9780974347226), in Charles Tandy (ed.) *Death and Anti-Death: Two Hundred Years after Kant, Fifty Years after Turing*, vol. 2, Palo Alto, California: Ria University Press, pp. 339–371.\n    \n2.  ^**[^](#fnrefyqbq9ej5ebr)**^\n    \n    Snyder-Beattie, Andrew, Toby Ord & Michael B. Bonsall (2019) [An upper bound for the background rate of human extinction](http://doi.org/10.1038/s41598-019-47540-7), *Scientific Reports*, vol. 9, pp. 1–9.\n    \n3.  ^**[^](#fnrefl2jqcz9r8e)**^\n    \n    Aschenbrenner, Leopold (2020) [Securing posterity](https://worksinprogress.co/issue/securing-posterity/), *Works in Progress*, October 19.\n    \n4.  ^**[^](#fnrefqokji1pqjxb)**^\n    \n    Ord, Toby (2020) [*The Precipice: Existential Risk and the Future of Humanity*](https://en.wikipedia.org/wiki/Special:BookSources/1526600218), London: Bloomsbury Publishing, p. 87."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "azy9tRrCxr9LkhgvL",
    "name": "Anthropics",
    "core": false,
    "slug": "anthropics",
    "oldSlugs": [
      "anthropics-1"
    ],
    "postCount": 22,
    "description": {
      "markdown": "**Anthropics** is the study of observation selection effects.\n\nFurther reading\n---------------\n\nLessWrong (2012) [Observation selection effect](https://www.lesswrong.com/revisions/tag/observation-selection-effect), *LessWrong Wiki*, June 26."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "w7ZuwJcyMYS62tLFp",
    "name": "Anthropic shadow",
    "core": false,
    "slug": "anthropic-shadow",
    "oldSlugs": null,
    "postCount": 2,
    "description": {
      "markdown": "**Anthropic shadow** is the phenomenon involved when attempts to estimate the magnitude of a [catastrophic](https://forum.effectivealtruism.org/tag/global-catastrophic-risk) or [existential](https://forum.effectivealtruism.org/topics/existential-risk) risk are biased by the fact that they are implicitly conditioning on the existence of human observers.\n\nAn event severe enough to destroy all present observers and prevent the emergence of any future observers will necessarily leave no observable traces of its past existence. Such an [anthropic effect](https://forum.effectivealtruism.org/tag/anthropics) will bias any attempt to [estimate the risk of human extinction](https://forum.effectivealtruism.org/tag/estimation-of-existential-risk) based on observed frequencies, causing an underestimation of actual risk.\n\nMilan Ćirković, [Anders Sandberg](https://forum.effectivealtruism.org/tag/anders-sandberg) and [Nick Bostrom](https://forum.effectivealtruism.org/tag/nick-bostrom) have developed a model to quantify and correct for this effect.^[\\[1\\]](#fn4tmt8aywhff)^\n\nFurther reading\n---------------\n\nĆirković, Milan M., Anders Sandberg & Nick Bostrom (2010) [Anthropic shadow: observation selection effects and human extinction risks](http://doi.org/10.1111/j.1539-6924.2010.01460.x), *Risk Analysis*, vol. 30, pp. 1495–1506.\n\nRelated entries\n---------------\n\n[anthropics](https://forum.effectivealtruism.org/tag/anthropics) | [estimation of existential risk](https://forum.effectivealtruism.org/tag/estimation-of-existential-risk) | [existential risk](https://forum.effectivealtruism.org/tag/existential-risk) | [global catastrophic risk](https://forum.effectivealtruism.org/tag/global-catastrophic-risk)\n\n1.  ^**[^](#fnref4tmt8aywhff)**^\n    \n    Ćirković, Milan M., Anders Sandberg & Nick Bostrom (2010) [Anthropic shadow: observation selection effects and human extinction risks](http://doi.org/10.1111/j.1539-6924.2010.01460.x), *Risk Analysis*, vol. 30, pp. 1495–1506."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "MY2hgLkhKPj2bb3tj",
    "name": "Human extinction",
    "core": false,
    "slug": "human-extinction",
    "oldSlugs": null,
    "postCount": 9,
    "description": {
      "markdown": "**Human extinction** is the destruction of all members of the species *Homo sapiens*.\n\nIn [Toby Ord](https://forum.effectivealtruism.org/tag/toby-ord)'s typology, human extinction constitutes one of the three main types of [existential catastrophe](https://forum.effectivealtruism.org/tag/existential-catastrophe-1).^[\\[1\\]](#fndux02oja92h)^\n\nFurther reading\n---------------\n\nBostrom, Nick (2003) [Human extinction](https://en.wikipedia.org/wiki/Special:BookSources/9780028656779), in Paul Demeny & Geoffrey McNicoll (eds.) *Encyclopedia of Population*, New York: Macmillan Reference, pp. 340–342.\n\nBostrom, Nick (2013) [Existential risk prevention as global priority](http://doi.org/10.1111/1758-5899.12002), *Global Policy*, vol. 4, pp. 15–31.\n\nBrauner, Jan & Friederike Grosse-Holz (2018) [The expected value of extinction risk reduction is positive](https://forum.effectivealtruism.org/posts/NfkEqssr7qDazTquW/the-expected-value-of-extinction-risk-reduction-is-positive), *Effective Altruism Forum*, December 9, section 1.3.\n\nCotton-Barratt, Owen, Max Daniel & Anders Sandberg (2020) [Defence in depth against human extinction: prevention, response, resilience, and why they all matter](http://doi.org/10.1111/1758-5899.12786), *Global Policy*, vol. 11, pp. 271–282.  \n*Identifies three defence layers against extinction, corresponding to successive stages in the progression of risk.*\n\nMatheny, Jason Gaverick (2007) [Reducing the risk of human extinction](http://doi.org/10.1111/j.1539-6924.2007.00960.x), *Risk Analysis*, vol. 27, pp. 1335–1344.  \n*A paper exploring the cost-effectiveness of extinction risk reduction.*\n\nOrd, Toby (2020) [*The Precipice: Existential Risk and the Future of Humanity*](https://en.wikipedia.org/wiki/Special:BookSources/1-5266-0021-8), London: Bloomsbury Publishing.\n\nRees, Martin J. (2003) [*Our Final Hour: A Scientist’s Warning: How Terror, Error, and Environmental Disaster Threaten Humankind’s Future in This Century—on Earth and Beyond*](https://en.wikipedia.org/wiki/Special:BookSources/0465068634), New York: Basic Books.\n\nRelated entries\n---------------\n\n[existential catastrophe](https://forum.effectivealtruism.org/tag/existential-catastrophe-1) | [existential risk](https://forum.effectivealtruism.org/tag/existential-risk) | [Great Filter](https://forum.effectivealtruism.org/tag/great-filter) | [long-term future](https://forum.effectivealtruism.org/tag/long-term-future)\n\n1.  ^**[^](#fnrefdux02oja92h)**^\n    \n    Ord, Toby (2020) [*The Precipice: Existential Risk and the Future of Humanity*](https://en.wikipedia.org/wiki/Special:BookSources/1526600218), London: Bloomsbury Publishing, fig. 5.2."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "98f2bDqbwjtdg7cdP",
    "name": "Capability control method",
    "core": false,
    "slug": "capability-control-method",
    "oldSlugs": [
      "ca",
      "capability-control-methods"
    ],
    "postCount": null,
    "description": {
      "markdown": "A **capability control method** is a method that attempt to prevent undesirable outcomes from [artificial intelligence](https://forum.effectivealtruism.org/tag/artificial-intelligence) by restricting what the AI can do. Capability control methods encompass [AI boxing](https://forum.effectivealtruism.org/tag/ai-boxing), incentive methods (including [anthropic capture](https://forum.effectivealtruism.org/tag/anthropic-capture)), stunting, and tripwires.^[\\[1\\]](#fnl1swhof7rb)^ Capability control methods may be contrasted with [motivation selection methods](https://forum.effectivealtruism.org/tag/motivation-selection-method), which attempt instead to restrict what the AI wants to do.\n\nFurther reading\n---------------\n\nBostrom, Nick (2014) [*Superintelligence: Paths, Dangers, Strategies*](https://en.wikipedia.org/wiki/Special:BookSources/9780199678112), Oxford: Oxford University Press, pp. 129-138.\n\nRelated entries\n---------------\n\n[AI alignment](https://forum.effectivealtruism.org/tag/ai-alignment) | [AI boxing](https://forum.effectivealtruism.org/tag/ai-boxing) | [anthropic capture](https://forum.effectivealtruism.org/tag/anthropic-capture) | [motivation selection method](https://forum.effectivealtruism.org/tag/motivation-selection-method)\n\n1.  ^**[^](#fnrefl1swhof7rb)**^\n    \n    Bostrom, Nick (2014) [*Superintelligence: Paths, Dangers, Strategies*](https://en.wikipedia.org/wiki/Special:BookSources/9780199678112), Oxford: Oxford University Press, pp. 129-138."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "AtDxSNDDXPRa5kxjB",
    "name": "Artificial intelligence",
    "core": false,
    "slug": "artificial-intelligence",
    "oldSlugs": null,
    "postCount": 84,
    "description": {
      "markdown": "**Artificial intelligence** (**AI**) is the set of intellectual capacities characteristic of human beings exhibited by machines, as well as the area of research aimed at creating machines with these capacities.\n\nTerminology\n-----------\n\nThe literature on [AI risk](https://forum.effectivealtruism.org/tag/ai-risk) features several commonly used expressions that refer to various types or forms of artificial intelligence. These notions are not always used consistently.\n\nAs noted, **artificial intelligence** is the set of intellectual capacities characteristic of human beings exhibited by machines. But some authors use the term imprecisely, to refer to human-level AI or even to strong AI (a term which itself is very imprecise).^[\\[1\\]](#fnvgnno9cupf)^\n\n**Human-level artificial intelligence** (**HLAI**) is AI that is at least as intelligent as the average or typical human. In one sense, human-level AI requires that the AI exhibits human-level ability in each of the capacities that constitute human intelligence. In another, weaker, sense, the requirement is that these capacities, assessed in the aggregate, are at least equivalent to the aggregate of human capacities. An AI that is weaker than humans on some dimensions, but stronger than humans on others, may count as human-level in this weaker sense. (However, it is unclear how these different capacities should be traded off against one another or what would ground these tradeoffs.^[\\[2\\]](#fneb8nbfx3nme)^) \n\n**Artificial general intelligence** (**AGI**) is AI that does not only exhibit high ability in a wide range of specific domains, but can also generalize across these domains and display other skills that are wide rather than narrow in scope.^[\\[3\\]](#fnstgaqmqo2yj)^ \"Artificial general intelligence\" is sometimes also used as a synonym for \"human-level artificial intelligence\".^[\\[4\\]](#fn0yp7xgr5n8a)^^[\\[5\\]](#fnqy5akpwrn1)^\n\n**High-level machine intelligence** (**HLMI**) is AI that can carry out most human professions at least as well as a typical human. Vincent Müller and [Nick Bostrom](https://forum.effectivealtruism.org/topics/nick-bostrom) coined the expression to overcome the perceived deficiencies of existing terminology.^[\\[6\\]](#fnlfgsj8m28gc)^^[\\[7\\]](#fn0u9x2kfc5q3)^\n\nFinally, \"**strong artificial intelligence\"** (**strong AI**)  is a multiply ambiguous expression that can mean either \"artificial general intelligence\", \"human-level artificial intelligence\" or \"[superintelligence](https://forum.effectivealtruism.org/tag/superintelligence)\", among other things.^[\\[8\\]](#fn1avgveu8s39)^\n\nDiscussion\n----------\n\nToday, AI systems are better than even the smartest people at some intellectual tasks, such as chess, but much worse at others, such as writing academic papers. If AI systems eventually become as good or better than humans at many of these remaining tasks, then their impact will likely be [transformative](https://forum.effectivealtruism.org/tag/transformative-artificial-intelligence). Furthermore, in the extreme case that AI systems eventually become [more capable than humans at all intellectual tasks](https://forum.effectivealtruism.org/tag/superintelligence), this would arguably be the most significant development in human history.\n\nPossible impacts of progress in AI include accelerated scientific progress, large-scale unemployment, novel forms of warfare, and [risks](https://forum.effectivealtruism.org/tag/ai-risk) from unintended behavior in AI systems.\n\nFurther reading\n---------------\n\nChristiano, Paul (2014) [Three impacts of machine intelligence](https://rationalaltruist.com/2014/08/23/three-impacts-of-machine-intelligence), *Rational Altruist*, August 23.\n\nRelated entries\n---------------\n\n[AI governance](https://forum.effectivealtruism.org/topics/ai-governance) | [AI alignment](https://forum.effectivealtruism.org/tag/ai-alignment) | [AI safety](https://forum.effectivealtruism.org/tag/ai-safety) | [human-level artificial intelligence](https://forum.effectivealtruism.org/tag/human-level-artificial-intelligence)\n\n1.  ^**[^](#fnrefvgnno9cupf)**^\n    \n    An example of the latter is Muehlhauser, Luke (2013) [When will AI be created?](https://intelligence.org/2013/05/15/when-will-ai-be-created/), *Machine Intelligence Research Institute*, May 16.\n    \n2.  ^**[^](#fnrefeb8nbfx3nme)**^\n    \n    AI Impacts (2014) [Human-level AI](https://aiimpacts.org/human-level-ai/), *AI Impacts*, January 23.\n    \n3.  ^**[^](#fnrefstgaqmqo2yj)**^\n    \n    See Pennachin, Cassio & Ben Goertzel (2007) [Contemporary approaches to artificial general intelligence](https://doi.org/10.1007/978-3-540-68677-4_1), in Ben Goertzel & Cassio Pennachin (eds.) *Artificial General Intelligence*, Berlin: Springer, pp. 1–30. The expression was popularized, but not coined, by Cassio Pennachin and Ben Goertzel. See Goertzel, Ben (2011) [Who coined the term “AGI”?](https://goertzel.org/who-coined-the-term-agi/), *Ben Goertzel’s Blog*, August 28.\n    \n4.  ^**[^](#fnref0yp7xgr5n8a)**^\n    \n    Muehlhauser, Luke (2013) [What is intelligence?](https://intelligence.org/2013/06/19/what-is-intelligence-2/), *Machine Intelligence Research Institute*, June 19.\n    \n5.  ^**[^](#fnrefqy5akpwrn1)**^\n    \n    Baum, Seth D., Ben Goertzel & Ted G. Goertzel (2011) [How long until human-level AI? Results from an expert assessment](https://doi.org/10.1016/j.techfore.2010.09.006), *Technological Forecasting and Social Change*, vol. 78, pp. 185–195.\n    \n6.  ^**[^](#fnreflfgsj8m28gc)**^\n    \n    Müller, Vincent C. & Nick Bostrom (2016) [Future progress in artificial intelligence: a survey of expert opinion](https://doi.org/10.1007/978-3-319-26485-1_33), in Vincent C. Müller (ed.) *Fundamental Issues of Artificial Intelligence*, Cham: Springer International Publishing, pp. 555–572. \n    \n7.  ^**[^](#fnref0u9x2kfc5q3)**^\n    \n    In their influential survey of machine learning researchers, Katja Grace and  her collaborators define \"high-level machine intelligence\" as follows: \"we have ‘high-level machine intelligence’ when unaided machines can accomplish every task better and more cheaply than human workers.\" (Grace, Katja *et al.* (2017) [When will AI exceed human performance? Evidence from AI experts](http://arxiv.org/abs/1705.08807), *ArXiv*, 1705.08807.) The expression is operationalized further in Muehlhauser, Luke (2015) [What do we know about AI timelines?](https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/ai-timelines), *Open Philanthropy*, October (updated July 2016), section 1.\n    \n8.  ^**[^](#fnref1avgveu8s39)**^\n    \n    Wikipedia (2021) [Strong AI](https://en.wikipedia.org/w/index.php?title=Strong_AI&oldid=1050563218), *Wikipedia*, October 18."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "dNNa2uqMHNNn7tN56",
    "name": "Anthropic capture",
    "core": false,
    "slug": "anthropic-capture",
    "oldSlugs": null,
    "postCount": null,
    "description": {
      "markdown": "**Anthropic capture** is a [capability control method](https://forum.effectivealtruism.org/tag/capability-control-methods) in which an advanced [artificial intelligence](https://forum.effectivealtruism.org/topics/artificial-intelligence) thinks it might be in a simulation and as such attempts to behave in ways that will be rewarded by its simulators.\n\nFurther reading\n---------------\n\nBostrom, Nick (2014) [*Superintelligence: paths, dangers, strategies*](https://en.wikipedia.org/wiki/Special:BookSources/9780199678112), Oxford: Oxford University Press, pp. 134–135."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "FcTiue6jZYz5xZKgR",
    "name": "Animal Charity Evaluators",
    "core": false,
    "slug": "animal-charity-evaluators",
    "oldSlugs": null,
    "postCount": 29,
    "description": {
      "markdown": "**Animal Charity Evaluators** (**ACE**) is a [charity evaluator](https://forum.effectivealtruism.org/tag/charity-evaluation) focused on organizations working to improve [animal welfare](https://forum.effectivealtruism.org/tag/animal-welfare-1).\n\nHistory\n-------\n\nACE was founded in 2012, under the name of **Effective Animal Activism**, as a branch of [80,000 Hours](https://forum.effectivealtruism.org/tag/80-000-hours). It adopted its current name and became an independent organization in 2013.^[\\[1\\]](#fnure68bixj)^ \n\nRecommendations\n---------------\n\nEvery year, ACE publishes a list of \"top charities\", singled out as \"top giving opportunities and as ideal examples of effective advocacy.\" In addition, they highlight a number of \"standout charities\", which \"conduct strong work on behalf of animals\" but which for various reasons do not attain the status of a top charity. As of June 2022, ACE's top charities are [Faunalytics](https://forum.effectivealtruism.org/tag/faunalytics), [The Humane League](https://forum.effectivealtruism.org/tag/the-humane-league), and [Wild Animal Initiative](https://forum.effectivealtruism.org/tag/wild-animal-initiative).\n\nFunding\n-------\n\nAs of June 2022, ACE has received over $1.8 million in funding from [Open Philanthropy](https://forum.effectivealtruism.org/topics/open-philanthropy),^[\\[2\\]](#fn6849t8gtxt8)^ and over $550,000 from [Effective Altruism Funds](https://forum.effectivealtruism.org/topics/effective-altruism-funds).^[\\[3\\]](#fn0k69cruhvb2k)^^[\\[4\\]](#fnapls6visbsr)^^[\\[5\\]](#fnpvuowcxw9s)^\n\nFurther reading\n---------------\n\nBockman, John (2015) [Animal Charity Evaluators](https://en.wikipedia.org/wiki/Special:BookSources/9781534935778), in Ryan Carey (ed.) *The Effective Altruism Handbook*, 1st ed., Oxford: Centre for Effective Altruism, pp. 130–134.\n\nRighetti, Luca & Fin Moorhouse (2020) [Leah Edgerton and Manja Gärtner on Animal Charity Evaluation](https://hearthisidea.com/episodes/ace), *Hear This Idea*, December 15.\n\nSánchez, Sebastián *et al.* (2021) [Timeline of Animal Charity Evaluators](https://timelines.issarice.com/wiki/Timeline_of_Animal_Charity_Evaluators), *Timelines Wiki*.\n\nExternal links\n--------------\n\n[Animal Charity Evaluators](https://animalcharityevaluators.org/). Official website.\n\n[Animal Charity Evaluators](https://forum.effectivealtruism.org/users/animalcharityevaluators). [Effective Altruism Forum](https://forum.effectivealtruism.org/tag/effective-altruism-forum-1) account.\n\n[Apply for a job](https://animalcharityevaluators.org/about/contributors/join-our-team/).\n\n[Donate to Animal Charity Evaluators](https://animalcharityevaluators.org/donate/).\n\nRelated entries\n---------------\n\n[animal welfare](https://forum.effectivealtruism.org/tag/animal-welfare-1) | [charity evaluation](https://forum.effectivealtruism.org/tag/charity-evaluation)\n\n1.  ^**[^](#fnrefure68bixj)**^\n    \n    Perez, Dee (2015) [2015 internal evaluation](https://animalcharityevaluators.org/transparency/internal-evaluations/2015-internal-evaluation/), *Animal Charity Evaluators*, July.\n    \n2.  ^**[^](#fnref6849t8gtxt8)**^\n    \n    Open Philanthropy (2022) [Grants database: Animal Charity Evaluators](https://www.openphilanthropy.org/grants/?organization-name=animal-charity-evaluators), *Open Philanthropy*.\n    \n3.  ^**[^](#fnref0k69cruhvb2k)**^\n    \n    Animal Welfare Fund (2017) [April 2017: Animal Welfare Fund grants](https://funds.effectivealtruism.org/funds/payouts/april-2017-animal-welfare-fund-grants), *Effective Altruism Funds*, April.\n    \n4.  ^**[^](#fnrefapls6visbsr)**^\n    \n    Animal Welfare Fund (2018) [June 2018: Animal Welfare Fund grants](https://funds.effectivealtruism.org/funds/payouts/june-2018-animal-welfare-fund-grants), *Effective Altruism Funds*, June.\n    \n5.  ^**[^](#fnrefpvuowcxw9s)**^\n    \n    Animal Welfare Fund (2020) [March 2020: Animal Welfare Fund grants](https://funds.effectivealtruism.org/funds/payouts/march-2020-animal-welfare-fund-grants), *Effective Altruism Funds*, March."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "HojCxf5dtNNxvJ6t9",
    "name": "Anders Sandberg",
    "core": false,
    "slug": "anders-sandberg",
    "oldSlugs": null,
    "postCount": 4,
    "description": {
      "markdown": "**Anders Sandberg** (born 11 July 1972) is a Swedish computational neuroscientist, currently Senior Research Fellow at the [Future of Humanity Institute](https://forum.effectivealtruism.org/tag/future-of-humanity-institute).\n\nIn 2000, Sandberg co-founded Eudoxa, a science and technology think tank based in Stockholm.^[\\[1\\]](#fnqlftffp4ee)^^[\\[2\\]](#fndf9650eggbw)^\n\nSandberg has been described as \"a multifaceted genius—a philosopher, neuroscientist, futurist, mathematician, and computer graphics artist.\"^[\\[3\\]](#fn1t69ci9off9)^ His contributions span a wide range of topics, including the [Fermi paradox](https://forum.effectivealtruism.org/tag/fermi-paradox), [model uncertainty](https://forum.effectivealtruism.org/tag/model-uncertainty), [cognitive enhancement](https://forum.effectivealtruism.org/tag/cognitive-enhancement), [whole brain emulation](https://forum.effectivealtruism.org/tag/whole-brain-emulation), [intelligence explosion](https://forum.effectivealtruism.org/tag/intelligence-explosion), [oracle AI](https://forum.effectivealtruism.org/tag/oracle-ai), [anthropics](https://forum.effectivealtruism.org/tag/anthropics), and the [future of humanity](https://forum.effectivealtruism.org/tag/long-term-future), among others.\n\nFurther reading\n---------------\n\nGalef, Julia (2018) [The long-term future of humanity (Anders Sandberg)](http://rationallyspeakingpodcast.org/215-the-long-term-future-of-humanity-anders-sandberg/), *Rationally Speaking*, August 19.\n\nNachbauer, Titus (2011) [Transhuman: Do you want to live forever?](https://www.youtube.com/watch?v=3PAj2yorJig), *Nederlandse Filmacademie*.\n\nRighetti, Luca & Fin Moorehouse (2021) [Anders Sandberg on the Fermi paradox, transhumanism, and so much more](https://hearthisidea.com/episodes/anders), *Hear This Idea*, August 2.\n\nWiblin, Robert & Keiran Harris (2018) [Why don’t we see alien civilizations? This Oxford academic suspects they’re sleeping. Here’s the reason](https://80000hours.org/podcast/episodes/anders-sandberg-fermi-paradox/), *80,000 Hours*, May 8.\n\nWiblin, Robert & Keiran Harris (2018) [Oxford university’s Dr Anders Sandberg on if dictators could live forever, the annual risk of nuclear war, solar flares, and more](https://80000hours.org/podcast/episodes/anders-sandberg-extending-life/), *80,000 Hours*, May 29.\n\nExternal links\n--------------\n\n[Anders Sandberg's pages](http://www.aleph.se/). Official website.\n\n1.  ^**[^](#fnrefqlftffp4ee)**^\n    \n    Allern, Sigurd & Ester Pollack (2016) [Swedish advocacy think tanks as news sources and agenda-setters](http://doi.org/10.7146/politik.v19i1.27395), *Politik*, vol. 19, p. 67.\n    \n2.  ^**[^](#fnrefdf9650eggbw)**^\n    \n    Mcgann, James G. (2015) [2014 Global Go To Think Tank Index Report](https://repository.upenn.edu/think_tanks/8/?utm_source=repository.upenn.edu%2Fthink_tanks%2F8&utm_medium=PDF&utm_campaign=PDFCoverPages), no. 8, TTCSP Global Go To Think Tank Index Reports, p. 106.\n    \n3.  ^**[^](#fnref1t69ci9off9)**^\n    \n    Buchanan, Allen (2011) [*Better Than Human: the Promise and Perils of Biomedical Enhancement*](https://en.wikipedia.org/wiki/Special:BookSources/9780190664046), Oxford: Oxford University Press, p. 94."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "22mfqPbYSypkm8GZa",
    "name": "Invertebrate welfare",
    "core": false,
    "slug": "invertebrate-welfare",
    "oldSlugs": [
      "invertebrate-welfare"
    ],
    "postCount": 49,
    "description": {
      "markdown": "**Invertebrate welfare** is the study of the distribution of [sentience](https://forum.effectivealtruism.org/tag/sentience-1) across invertebrates, and of interventions aimed at improving the [welfare](https://forum.effectivealtruism.org/tag/wellbeing) of these animals.\n\nFurther reading\n---------------\n\nSchukraft, Jason (2019) [Invertebrate welfare cause profile](https://forum.effectivealtruism.org/posts/EDCwbDEhwRGZjqY6S/invertebrate-welfare-cause-profile), *Effective Altruism Forum*, July 9.\n\nExternal links\n--------------\n\n[Invertebrate Welfare](https://www.invertebratewelfare.org/). A compilation of research on invertebrate welfare.\n\nRelated entries\n---------------\n\n[farmed animal welfare](https://forum.effectivealtruism.org/tag/farmed-animal-welfare) | [wild animal welfare](https://forum.effectivealtruism.org/tag/wild-animal-welfare)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "qnpsn9LbiD4hkbPRF",
    "name": "Quantified Uncertainty Research Institute",
    "core": false,
    "slug": "quantified-uncertainty-research-institute",
    "oldSlugs": [
      "quri"
    ],
    "postCount": 44,
    "description": {
      "markdown": "The **Quantified Uncertainty Research Institute** (**QURI**) is a nonprofit set up to study [forecasting](https://forum.effectivealtruism.org/tag/forecasting) and epistemics. QURI was founded in 2019.\n\nFunding\n-------\n\nAs of June 2022, QURI has received over $650,000 in funding from the [Survival and Flourishing](https://forum.effectivealtruism.org/topics/survival-and-flourishing) Fund,^[\\[1\\]](#fnc0d2u9mfrd)^^[\\[2\\]](#fnjt827nwjnlq)^ and $200,000 from the [Future Fund](https://forum.effectivealtruism.org/topics/future-fund).^[\\[3\\]](#fn3k73fwbo6ot)^\n\nExternal links\n--------------\n\n[Quantified Uncertainty Research Institute](https://quantifieduncertainty.org/). Official website.\n\n1.  ^**[^](#fnrefc0d2u9mfrd)**^\n    \n    Survival and Flourishing Fund (2019) [SFF-2020-H1 S-process recommendations announcement](https://survivalandflourishing.fund/sff-2020-h1-recommendations), *Survival and Flourishing Fund*. \n    \n2.  ^**[^](#fnrefjt827nwjnlq)**^\n    \n    Survival and Flourishing Fund (2021) [SFF-2022-H1 S-Process recommendations announcement](https://survivalandflourishing.fund/sff-2022-h1-recommendations), *Survival and Flourishing Fund*.\n    \n3.  ^**[^](#fnref3k73fwbo6ot)**^\n    \n    Future Fund (2022) [Our grants and investments: QURI](https://ftxfuturefund.org/all-grants/?_organization_name=quri), *Future Fund*."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "yAYgvwtNu6S8i7op9",
    "name": "Altruistic wager",
    "core": false,
    "slug": "altruistic-wager",
    "oldSlugs": null,
    "postCount": 15,
    "description": {
      "markdown": "An **altruistic wager** is a type of argument that seeks to establish that an agent should act as though a conclusion is true because, given what we know, that seems to be the right decision from an ethical perspective, even if the conclusion does not actually seem likely to be true. For example, one might argue that one should act as though animals are [sentient](https://forum.effectivealtruism.org/tag/sentience-1) and are [moral patients](https://forum.effectivealtruism.org/tag/moral-patienthood) when deciding whether to be a vegetarian, as long as that has any nontrivial chance of being true, because if it is true the ethical harms of eating meat might very much outweigh the ethical or self-interested benefits.\n\nAltruistic wagers often assume an agent should behave according to [expected value theory](https://forum.effectivealtruism.org/tag/expected-value), but some such wagers may also hold under some [alternatives to expected value theory](https://forum.effectivealtruism.org/tag/alternatives-to-expected-value-theory). Some wagers may even hold more strongly under particular alternatives to expected value theory, such as a theory that incorporates [risk-aversion](https://forum.effectivealtruism.org/tag/risk-aversion).\n\nFurther reading\n---------------\n\nBaumann, Tobias (2020) [An overview of wagers for reducing future suffering](https://s-risks.org/an-overview-of-wagers-for-reducing-future-suffering/), *Reducing Risks of Future Suffering*, January 31.\n\nRelated entries\n---------------\n\n[alternatives to expected value theory](https://forum.effectivealtruism.org/tag/alternatives-to-expected-value-theory) | [decision theory](https://forum.effectivealtruism.org/tag/decision-theory) | [decision-theoretic uncertainty](https://forum.effectivealtruism.org/tag/decision-theoretic-uncertainty) | [expected value theory](https://forum.effectivealtruism.org/tag/expected-value) | [fanaticism](https://forum.effectivealtruism.org/tag/fanaticism) | [moral uncertainty](https://forum.effectivealtruism.org/tag/moral-uncertainty) | [risk aversion](https://forum.effectivealtruism.org/tag/risk-aversion)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "hCizAwRZJPbALzGNb",
    "name": "Altruistic motivation",
    "core": false,
    "slug": "altruistic-motivation",
    "oldSlugs": null,
    "postCount": 38,
    "description": {
      "markdown": "**Altruistic motivation** is the study of what moves individuals to act altruistically.\n\nFurther reading\n---------------\n\nNgo, Richard (2020) [EA reading list: EA motivations and psychology](https://forum.effectivealtruism.org/s/NKTk9s4tZPiA4aySj/p/sSygsT2htNsfQ9azn), *Effective Altruism Forum*, August 3.\n\nRelated entries\n---------------\n\n[excited vs. obligatory altruism](https://forum.effectivealtruism.org/tag/excited-vs-obligatory-altruism)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "ekWgkSGGakM4kx3bp",
    "name": "Altruistic coordination",
    "core": false,
    "slug": "altruistic-coordination",
    "oldSlugs": null,
    "postCount": 26,
    "description": {
      "markdown": "**Altruistic coordination** is the study of how individual altruists should act when the impact of each person's decisions depends on the actions of other altruists. Such coordination could take many forms, including [philanthropic coordination](https://forum.effectivealtruism.org/tag/philanthropic-coordination); [moral trade](https://forum.effectivealtruism.org/tag/moral-trade); attempting to consider comparative advantage and [replaceability](https://forum.effectivealtruism.org/tag/replaceability) rather than just [personal fit](https://forum.effectivealtruism.org/tag/personal-fit); building \"community capital\"; and taking a \"portfolio approach\".^[\\[1\\]](#fnqf7m4wt8ppq)^\n\nThis coordination could be achieved via actual discussion between the relevant altruists. Alternatively, an altruist could simply try to roughly predict  how their actions will affect the actions of other altruists, or follow rules of thumb intended to aid in altruistic coordination.\n\nAltruists often fail to attempt altruistic coordination, instead taking actions based on what *would* be the best action *if* other people aiming to do good weren’t responding to what they do. Todd refers to this as \"single-player thinking\".^[\\[1\\]](#fnqf7m4wt8ppq)^ Additionally, even when a person does attempt altruistic coordination, it can be difficult to work precisely how to do so, and the communication and trust involved in good coordination can be costly to maintain. As more people try to work together to do good, it appears that achieving low-cost coordination will be a difficult but important problem to solve (see also [value of movement growth](https://forum.effectivealtruism.org/tag/value-of-movement-growth)).\n\nFurther reading\n---------------\n\nTodd, Benjamin (2018) [Doing good together - how to coordinate effectively, and avoid single-player thinking](https://80000hours.org/articles/coordination/), *80,000 Hours*, September 21.\n\nRelated entries\n---------------\n\n[building effective altruism](https://forum.effectivealtruism.org/tag/building-effective-altruism-1) | [community infrastructure](https://forum.effectivealtruism.org/tag/community-infrastructure) | [epistemic deference](https://forum.effectivealtruism.org/tag/epistemic-deference) | [moral cooperation](https://forum.effectivealtruism.org/tag/moral-cooperation) | [moral trade](https://forum.effectivealtruism.org/tag/moral-trade) | [philanthropic coordination](https://forum.effectivealtruism.org/tag/philanthropic-coordination)\n\n1.  ^**[^](#fnrefqf7m4wt8ppq)**^\n    \n    Todd, Benjamin (2018) [Doing good together - how to coordinate effectively, and avoid single-player thinking](https://80000hours.org/articles/coordination/), *80,000 Hours*, September 21."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "m7YYPfrBQx5t4t5qb",
    "name": "Foreign aid skepticism",
    "core": false,
    "slug": "foreign-aid-skepticism",
    "oldSlugs": null,
    "postCount": 11,
    "description": {
      "markdown": "**Foreign aid skepticism** is skepticism concerning the impact of [foreign aid](https://forum.effectivealtruism.org/tag/foreign-aid) programs.\n\nForeign aid skeptics such as Dambisa Moyo, William Easterly and Angus Deaton criticize foreign aid programs on various grounds. Three of the most commonly raised objections are that aid programs have been (1) extremely costly, (2) largely ineffective, and (3) often net harmful.^[\\[1\\]](#fnpsy6qes2m8i)^^[\\[2\\]](#fn47edla919c8)^^[\\[3\\]](#fnuym3q4nl89)^ The rest of this entry summarizes some responses to these criticisms.\n\nResponses\n---------\n\n*Costly programs can still be cost-effective*. Aid skeptics often object to the high costs of aid programs. For example, Moyo writes: \"So there we have it: sixty years, over US$1 trillion dollars of African aid, and not much good to show for it.\"^[\\[4\\]](#fnyigme1i79p)^ Similarly, Easterly writes: \"the other tragedy of the world’s poor… is the tragedy in which the West spent $2.3 trillion on foreign aid over the last five decades and still had not managed to get twelve-cent medicines to children to prevent half of all malaria deaths. The West spent $2.3 trillion and still had not managed to get four-dollar bed nets to poor families. The West spent $2.3 trillion and still had not managed to get three dollars to each new mother to prevent five million child deaths.\"^[\\[5\\]](#fnzbwn7uydtu)^\n\nTo assess the cost-effectiveness of aid, however, one needs to consider not just the costs of aid programs, but also their benefits, represented by the number of people affected (over 400 million) and the period during which they were affected (sixty years). When these adjustments are made, it turns out that total aid spending in Africa has amounted to only $40 per person per year.^[\\[6\\]](#fnu6eiy0thlom)^^[\\[7\\]](#fnxddxq10kse)^ A separate estimate concludes that the flow of external development assistance accounts for about 1% of developing world income.^[\\[8\\]](#fncdyoi3ae0w7)^\n\n*Some types of aid are much more effective than others*. Critics of aid have largely focused on economic development, rather than global health. But these two forms of aid differ greatly. The [Smallpox Eradication Programme](https://forum.effectivealtruism.org/tag/smallpox-eradication-programme) , funded in part by international aid, has saved over 60 million lives since 1980.^[\\[6\\]](#fnu6eiy0thlom)^ UNICEF's Campaign for Child Survival is estimated to have saved around 12 million lives by the end of the 1980s.^[\\[9\\]](#fnic3uhvvr6zk)^ The  U.S. President's Emergency Plan for AIDS Relief (PEPFAR) has plausibly saved tens of millions of life-years.^[\\[10\\]](#fntc3uo46ns4r)^ Other examples of successful aid programs include the Global Alliance for Vaccines and Immunization, the Polio Eradication Initiative, and the Onchocerciasis Control Program.^[\\[11\\]](#fnqpq7saybakt)^\n\nEven aid skeptics generally acknowledge these successes. Thus, Deaton writes:^[\\[3\\]](#fnuym3q4nl89)^\n\n> \\[e\\]xternal aid has saved millions of lives in poor countries. UNICEF and other agencies brought antibiotics and vaccinations to millions of children, reducing infant and child mortality. The control and elimination of disease-bearing pests have made safe once-dangerous regions of the world. An international effort eliminated smallpox, and a current effort is close to doing the same for polio. Aid agencies have made oral rehydration therapy available to millions of children and are providing insecticide-treated bed nets to protect against malaria, a disease that still kills a million African children every year. Between 1974 and 2002, a joint effort by the World Bank, the World Health Organization, UNDP, and the UN Food and Agriculture Organization all but eliminated river blindness as a public health.\n\nSimilarly, Easterly writes:^[\\[12\\]](#fnww0ugbf245)^\n\n> There are well known and striking donor success stories, like the elimination of smallpox, the near-eradication of river blindness and Guinea worm, the spread of oral rehydration therapy for treating infant diarrheal diseases, DDT campaigns against malarial mosquitoes (although later halted for environmental reasons), and the success of WHO vaccination programmes against measles and other childhood diseases.\n\n*The forms of aid skeptics focus on are not as ineffective as these skeptics contend*. Even if attention is confined to economic development (which, as noted, is a comparatively ineffective form of aid), the existing evidence fails to support the pessimistic assessment of the aid skeptics. As a leading expert on poverty notes, \"an objective review of the evidence does not suggest that aid typically fails. Indeed, in contrast to the claims in \\[Angus Deaton's\\] *The Great Escape*, the best recent evidence suggests that aid has helped promote economic growth on average over the longer term.\"^[\\[13\\]](#fnoq56xtjea9g)^^[\\[14\\]](#fntkkeo4254kg)^\n\n*Arguments about the effectiveness of foreign aid have little relevance for individual donors*. Aid skeptics typically focus on bilateral or multilateral aid, rather than on the simple, targeted programs that [GiveWell](https://forum.effectivealtruism.org/tag/givewell) and other charity evaluators consider most effective.^[\\[15\\]](#fnysy5uxbcxq)^ Moyo herself stresses that her book \"is not concerned with emergency and charity-based aid\",^[\\[16\\]](#fn3btrltq1vof)^ and objects to those who \"conflate my arguments about structural aid with those about emergency or NGO aid.\"^[\\[17\\]](#fn2gp9arxef7b)^ Similar remarks apply to Easterly, as Amartya Sen notes in a review of his book: the arguments of the aid skeptics should not \"be read as a general skepticism toward the idea that one person can consciously and deliberately do good to another. This is not Easterly's position at all.\"^[\\[18\\]](#fngiur4605yf)^\n\nThus, even if the criticisms are valid and generalizable to all forms of aid, they have very limited bearing on what altruistic individuals should do. As [William MacAskill](https://forum.effectivealtruism.org/tag/william-macaskill) notes: \"even if it turned out that every single development program that we know of does more harm than good, that fact would *not* mean that we can buy a larger house, safe in the knowledge that we have no pressing moral obligations of beneficence upon us. There are thousands of pressing problems that call out for our attention and that we could make significant inroads on with our resources.\"^[\\[19\\]](#fn90e01ihrw9i)^ MacAskill goes on to note a number of ways in which individuals can use their resources to help others effectively:\n\n*   Spare 20 years’ worth of unnecessary incarceration, while not reducing public safety, by donating to organisations working in criminal justice reform.^[\\[20\\]](#fnrv0h9wsu3)^\n*   Spare 1.2 million hens from the cruelty of battery cages by donating to corporate cage-free campaigns.^[\\[21\\]](#fniacm6xz6ch)^\n*   Reduce the chance of a civilisation-ending global pandemic by funding policy research and advocacy on biosecurity issues.^[\\[22\\]](#fn8tz9v9qdq7s)^\n*   Contribute to a more equitable international order by funding policy analysis and campaigning.^[\\[23\\]](#fnnf5bnhffsef)^\n\nFurther reading\n---------------\n\nMacaskill, William (2019) [Aid scepticism and effective altruism](http://www.jpe.ox.ac.uk/papers/aid-scepticism-and-effective-altruism/), *Journal of Practical Ethics*, vol. 7, pp. 49–60, p. 56.\n\nRelated entries\n---------------\n\n[foreign aid](https://forum.effectivealtruism.org/tag/foreign-aid) | [global poverty](https://forum.effectivealtruism.org/tag/global-poverty)\n\n1.  ^**[^](#fnrefpsy6qes2m8i)**^\n    \n    Easterly, William (2006) [*The White Man’s Burden: Why the West’s Efforts to Aid the Rest Have Done So Much Ill and So Little Good*](https://en.wikipedia.org/wiki/Special:BookSources/9780199210824), Oxford: Oxford University Press.\n    \n2.  ^**[^](#fnref47edla919c8)**^\n    \n    Moyo, Dambisa (2009) [*Dead Aid: Why Aid Is Not Working and How There Is a Better Way for Africa*](https://en.wikipedia.org/wiki/Special:BookSources/9780374139568), New York: Farrar, Straus and Giroux.\n    \n3.  ^**[^](#fnrefuym3q4nl89)**^\n    \n    Deaton, Angus (2013) [*The Great Escape: Health, Wealth, and the Origins of Inequality*](https://en.wikipedia.org/wiki/Special:BookSources/9780691153544), Princeton, New Jersey: Princeton University Press.\n    \n4.  ^**[^](#fnrefyigme1i79p)**^\n    \n    Moyo, [*Dead Aid*](https://en.wikipedia.org/wiki/Special:BookSources/9780374139568), p. 52.\n    \n5.  ^**[^](#fnrefzbwn7uydtu)**^\n    \n    Easterly, [*The White Man’s Burden*](https://en.wikipedia.org/wiki/Special:BookSources/9780199210824), p. 15.\n    \n6.  ^**[^](#fnrefu6eiy0thlom)**^\n    \n    MacAskill, William (2015) [*Doing Good Better: Effective Altruism and How You Can Make a Difference*](https://en.wikipedia.org/wiki/Special:BookSources/9781592409662), New York: Random House, ch. 3.\n    \n7.  ^**[^](#fnrefxddxq10kse)**^\n    \n    Cf. Singer, Peter (2009) [*The Life You Can Save: Acting Now to End World Poverty*](https://en.wikipedia.org/wiki/Special:BookSources/9780812981568), New York: Random House, pp. 105-106.\n    \n8.  ^**[^](#fnrefcdyoi3ae0w7)**^\n    \n    Temple, Jonathan (2010) [Aid and conditionality](http://doi.org/10.1016/B978-0-444-52944-2.00005-7), in Dani Rodrik & Mark Rosenzweig (eds.) *Handbook of Development Economics*, vol. 5, Amsterdam: Elsevier, pp. 4415–4523, p. 4431.\n    \n9.  ^**[^](#fnrefic3uhvvr6zk)**^\n    \n    UNICEF (1996) [*The State of the World’s Children 1996*](https://en.wikipedia.org/wiki/Special:BookSources/0192627473), Oxford: Oxford University Press, p. 62.\n    \n10.  ^**[^](#fnreftc3uo46ns4r)**^\n    \n    Heaton, Laura M. *et al.* (2015) [Estimating the impact of the US President’s Emergency Plan for AIDS Relief on HIV treatment and prevention programmes in Africa](https://doi.org/10.1136/sextrans-2014-051991), *Sexually Transmitted Infections*, vol. 91, pp. 615–620. For discussion, see Stafforini, Pablo (2022) [How many lives has the U.S. President’s Emergency Plan for AIDS Relief (PEPFAR) saved?](https://forum.effectivealtruism.org/posts/f2xEp9RAyA2kSZ2qm/how-many-lives-has-the-u-s-president-s-emergency-plan-for), *Effective Altruism Forum*, January 18.\n    \n11.  ^**[^](#fnrefqpq7saybakt)**^\n    \n    Sachs, Jeffrey (2005) [*The End of Poverty: Economic Possibilities for Our Time*](https://en.wikipedia.org/wiki/Special:BookSources/1594200459), New York: Penguin Press, ch. 13.\n    \n12.  ^**[^](#fnrefww0ugbf245)**^\n    \n    Easterly, William (2009) [Some cite good news on aid](http://www.nyudri.org/aidwatcharchive/2009/02/some-cite-good-news-on-aid), *Aid Watch*, February 18.\n    \n13.  ^**[^](#fnrefoq56xtjea9g)**^\n    \n    Ravallion, Martin (2014) [On the role of aid in *The Great Escape*](http://doi.org/10.1111/roiw.12136), *Review of Income and Wealth*, vol. 60, pp. 967–984, p. 982.\n    \n14.  ^**[^](#fnreftkkeo4254kg)**^\n    \n    Cf. Ravallion, Martin (2016) [*The Economics of Poverty: History, Measurement, and Policy*](https://en.wikipedia.org/wiki/Special:BookSources/9780190212773), Oxford: Oxford University Press, section 9.9, p. 529.\n    \n15.  ^**[^](#fnrefysy5uxbcxq)**^\n    \n    Karnofsky, Holden (2015) [The lack of controversy over well-targeted aid](https://blog.givewell.org/2015/11/06/the-lack-of-controversy-over-well-targeted-aid/), *The GiveWell Blog*, November 6.\n    \n16.  ^**[^](#fnref3btrltq1vof)**^\n    \n    Moyo, [*Dead Aid*](https://en.wikipedia.org/wiki/Special:BookSources/9780374139568), p. 7.\n    \n17.  ^**[^](#fnref2gp9arxef7b)**^\n    \n    Moyo, Dambisa (2013) [Dr. Dambisa Moyo responds to Bill Gates’ personal attacks](https://web.archive.org/web/20150712234659/http://www.dambisamoyo.com/?post=dr-dambisa-moyo-responds-to-bill-gates-personal-attacks), *Dambisa Moyo’s Website*, May 30.\n    \n18.  ^**[^](#fnrefgiur4605yf)**^\n    \n    Sen, Amartya (2006) [The man without a plan: can foreign aid work?](http://doi.org/10.2307/20031920), *Foreign Affairs*, vol. 85, pp. 171–177, p. 173.\n    \n19.  ^**[^](#fnref90e01ihrw9i)**^\n    \n    Macaskill, William (2019) [Aid scepticism and effective altruism](http://www.jpe.ox.ac.uk/papers/aid-scepticism-and-effective-altruism/), *Journal of Practical Ethics*, vol. 7, pp. 49–60, p. 56.\n    \n20.  ^**[^](#fnrefrv0h9wsu3)**^\n    \n    Open Philanthropy (2019) [Criminal justice reform strategy](https://www.openphilanthropy.org/focus/us-policy/criminal-justice-reform/criminal-justice-reform-strategy), *Open Philanthropy*, December, section 3.\n    \n21.  ^**[^](#fnrefiacm6xz6ch)**^\n    \n    Open Philanthropy (2016) [The humane league — corporate cage-free campaigns](https://www.openphilanthropy.org/focus/us-policy/farm-animal-welfare/humane-league-corporate-cage-free-campaigns), *Open Philanthropy*, February, section 1.1.2.\n    \n22.  ^**[^](#fnref8tz9v9qdq7s)**^\n    \n    Open Philanthropy (2014) [Biosecurity](https://www.openphilanthropy.org/research/cause-reports/biosecurity), *Open Philanthropy*, January.\n    \n23.  ^**[^](#fnrefnf5bnhffsef)**^\n    \n    Open Philanthropy (2016) [Center for global development — general support 2016](https://www.openphilanthropy.org/focus/global-health-and-development/miscellaneous/center-global-development-general-support-2016), *Open Philanthropy*, February."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "JW7Tde6Nc4eAALtdm",
    "name": "Cryonics",
    "core": false,
    "slug": "cryonics",
    "oldSlugs": [
      "biostasis"
    ],
    "postCount": 8,
    "description": {
      "markdown": "**Cyronics** is the practice of using subfreezing temperatures to preserve the bodies of those who have died of an incurable disease with the goal of restoring good health following sufficient advances in medical technology.\n\nFurther reading\n---------------\n\nGreenberg, Spencer (2021) [‎Clearer Thinking with Spencer Greenberg: Freezing to (not) death: cryonics and the quest for immortality (with Max Marty)](https://clearerthinkingpodcast.com/episode/083), *Clearer Thinking*, December 9.\n\nHanson, Robin (2010) [Cryonics as charity](https://www.overcomingbias.com/2010/07/cryonics-as-charity.html), *Overcoming Bias*, July 12.\n\nUrban, Tim (2016) [Why cryonics makes sense](https://waitbutwhy.com/2016/03/cryonics.html), *Wait But Why*, March 24.\n\nRelated entries\n---------------\n\n[aging research](https://forum.effectivealtruism.org/tag/aging-research) | [transhumanism](https://forum.effectivealtruism.org/tag/transhumanism)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "BosJqW6ggqbhtRosc",
    "name": "AI winter",
    "core": false,
    "slug": "ai-winter",
    "oldSlugs": [
      "ai-winter"
    ],
    "postCount": null,
    "description": {
      "markdown": "An **AI winter** is a period of reduced funding and interest in [artificial intelligence](https://forum.effectivealtruism.org/tag/artificial-intelligence) research and development. Previous periods popularly described as major AI winters were the 1974-1980 and the 1987-1993 periods.\n\nBetween 1971-75 slow advances in speech recognition, the UK government's decision to end support for most AI research in universities, and DARPA's cuts to academic AI research funding led to the first AI winter, lasting until 1980.\n\nFurther reading\n---------------\n\nBostrom, Nick (2014) [*Superintelligence: Paths, Dangers, Strategies*](https://en.wikipedia.org/wiki/Special:BookSources/978-0-19-967811-2), Oxford: Oxford University Press, ch. 1.\n\nConstantin, Sarah (2015) [AI timelines and strategies](https://aiimpacts.org/ai-timelines-and-strategies/), *AI Impacts*, August 20.\n\nRelated entries\n---------------\n\n[artificial intelligence](https://forum.effectivealtruism.org/tag/artificial-intelligence) | [AI forecasting](https://forum.effectivealtruism.org/tag/ai-forecasting) | [AI skepticism](https://forum.effectivealtruism.org/tag/ai-skepticism) | [economics of artificial intelligence](https://forum.effectivealtruism.org/tag/economics-of-artificial-intelligence)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "yHnvv76eTfC4WT9F3",
    "name": "AI takeoff",
    "core": false,
    "slug": "ai-takeoff",
    "oldSlugs": [
      "ai-takeoff"
    ],
    "postCount": 13,
    "description": {
      "markdown": "**AI takeoff** is a hypothesized period of transition during which an advanced [artificial intelligence](https://forum.effectivealtruism.org/tag/artificial-intelligence) acquires [superhuman intellectual capacity](https://forum.effectivealtruism.org/tag/superintelligence).\n\nFurther reading\n---------------\n\nBarnett, Matthew (2020) [Distinguishing definitions of takeoff](https://www.alignmentforum.org/posts/YgNYA6pj2hPSDQiTE/distinguishing-definitions-of-takeoff), *AI Alignment Forum*, February 13.  \n*A compendium of explicit definitions of 'AI takeoff'.*\n\nChristiano, Paul (2018) [Takeoff speeds](https://sideways-view.com/2018/02/24/takeoff-speeds/), *The Sideways View*, February 24.\n\nRelated entries\n---------------\n\n[artificial intelligence](https://forum.effectivealtruism.org/tag/artificial-intelligence) | [decisive strategic advantage](https://forum.effectivealtruism.org/tag/decisive-strategic-advantage) | [superintelligence](https://forum.effectivealtruism.org/tag/superintelligence)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "BF9ioxvBD7wdH6QzE",
    "name": "AI skepticism",
    "core": false,
    "slug": "ai-skepticism",
    "oldSlugs": [
      "ai-skepticism",
      "ai-skepticism",
      "ai-skepticism"
    ],
    "postCount": 27,
    "description": {
      "markdown": "**AI skepticism** is skepticism about arguments that advanced [artificial intelligence](https://forum.effectivealtruism.org/tag/artificial-intelligence) poses a [catastrophic](https://forum.effectivealtruism.org/tag/global-catastrophic-risk) or [existential](https://forum.effectivealtruism.org/tag/existential-risk) risk.\n\nFurther reading\n---------------\n\nBergal, Asya & Robert Long (2019) [Conversation with Robin Hanson](https://aiimpacts.org/conversation-with-robin-hanson/), *AI Impacts*, November 13.\n\nGarfinkel, Ben (2019) [How sure are we about this AI stuff?](https://forum.effectivealtruism.org/posts/9sBAW3qKppnoG3QPq/ben-garfinkel-how-sure-are-we-about-this-ai-stuff), *Effective Altruism Forum*, February 9.\n\nVinding, Magnus (2017/2022) [A Contra AI FOOM Reading List](https://magnusvinding.com/2017/12/16/a-contra-ai-foom-reading-list/).\n\nRelated entries\n---------------\n\n[AI alignment](https://forum.effectivealtruism.org/tag/ai-alignment) | [AI safety](https://forum.effectivealtruism.org/tag/ai-safety) | [criticism of effective altruism](https://forum.effectivealtruism.org/tag/criticism-of-effective-altruism)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "2bKZFw8Q5MCg9K95R",
    "name": "AI risk",
    "core": true,
    "slug": "ai-risk",
    "oldSlugs": [
      "ai-risks"
    ],
    "postCount": 322,
    "description": {
      "markdown": "An **AI risk** is a [catastrophic](https://forum.effectivealtruism.org/topics/global-catastrophic-risk) or [existential](https://forum.effectivealtruism.org/topics/existential-risk) risk arising from the creation of advanced [artificial intelligence](https://forum.effectivealtruism.org/topics/artificial-intelligence) (AI).\n\nDevelopments in AI have the potential to enable people around the world to flourish in hitherto unimagined ways. Such developments might also give humanity tools to address other sources of risk.\n\nDespite this, AI also poses its own risks. AI systems sometimes behave in ways that surprise people. At the moment, such systems are usually narrow in their capabilities - for example, they are excellent at Go, or at minimizing power consumption in a server facility, but they can’t do other tasks. If people designed a machine intelligence that was a sufficiently good general reasoner, or even [better at general reasoning than people are](https://forum.effectivealtruism.org/topics/superintelligence), it might become difficult for human agents to interfere with its functioning. If it then behaved in a way which did not reflect human values, it might pose a real risk to humanity. Such a machine intelligence might use its intellectual superiority to develop a decisive strategic advantage. If its goals were incompatible with human flourishing, it could then pose an [existential risk](https://forum.effectivealtruism.org/topics/existential-risk).\n\nNote that AI could pose an existential risk without being sentient, gaining consciousness, or having any ill will towards humanity. \n\nFurther reading\n---------------\n\nBostrom, Nick (2014) [*Superintelligence: Paths, Dangers, Strategies*](https://en.wikipedia.org/wiki/Special:BookSources/9780199678112), Oxford: Oxford University Press.  \n*Offers a detailed analysis of risks posed by AI.*\n\nChristiano, Paul (2019) [What failure looks like](https://www.alignmentforum.org/posts/HBxe6wdjxK239zajf/what-failure-looks-like), *LessWrong*, March 17.\n\nDewey, Daniel (2015) [Three areas of research on the superintelligence control problem](http://globalprioritiesproject.org/2015/10/three-areas-of-research-on-the-superintelligence-control-problem/), *Global Priorities Project*, October 20.  \n*Provides an overview and suggested reading in AI risk.*\n\nKarnofsky, Holden (2016) [Potential risks from advanced artificial intelligence: the philanthropic opportunity](http://www.openphilanthropy.org/blog/potential-risks-advanced-artificial-intelligence-philanthropic-opportunity), *Open Philanthropy*, May 6.  \n*Explains why the Open Philanthropy Project regards risks from AI as an area worth exploring.*\n\nDai, Wei & Daniel Kokotajlo (2019) [The main sources of AI risk?](https://www.alignmentforum.org/posts/WXvt8bxYnwBYpy9oT/the-main-sources-of-ai-risk), *AI Alignment Forum*, March 21.  \n*An attempt to list all the significant sources of AI risk.*\n\nRelated entries\n---------------\n\n[AI alignment](https://forum.effectivealtruism.org/topics/ai-alignment) | [AI governance](https://forum.effectivealtruism.org/topics/ai-governance) | [AI forecasting](https://forum.effectivealtruism.org/topics/ai-forecasting) | [AI safety](https://forum.effectivealtruism.org/tag/ai-safety) | [instrumental convergence thesis](https://forum.effectivealtruism.org/topics/instrumental-convergence-thesis) | [orthogonality thesis](https://forum.effectivealtruism.org/topics/instrumental-convergence-thesis)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "avZp9JaYhuAkn3MBu",
    "name": "AI Impacts",
    "core": false,
    "slug": "ai-impacts",
    "oldSlugs": null,
    "postCount": 30,
    "description": {
      "markdown": "**AI Impacts** is a nonprofit organization that researches answers to decision-relevant questions about the future of [artificial intelligence](https://forum.effectivealtruism.org/tag/artificial-intelligence). It was founded in 2014 by [Paul Christiano](https://forum.effectivealtruism.org/tag/paul-christiano) and Katja Grace.^[\\[1\\]](#fnj7tu0r1dr5r)^^[\\[2\\]](#fnm87ttm2zkx)^\n\nFunding \n--------\n\nAs of July 2022, AI Impacts has received over $450,000 in funding from the [Survival and Flourishing](https://forum.effectivealtruism.org/tag/survival-and-flourishing) Fund,^[\\[3\\]](#fnxzyd8918vs)^^[\\[4\\]](#fnr6hp12uxy1h)^^[\\[5\\]](#fny87npnoddnj)^ $250,000 from the [Future Fund](https://forum.effectivealtruism.org/topics/future-fund),^[\\[6\\]](#fn6qtoi3x9q7d)^ over $180,000 from [Open Philanthropy](https://forum.effectivealtruism.org/tag/open-philanthropy)^[\\[7\\]](#fn1bv5jpvlys1)^, and $75,000 from [Effective Altruism Funds](https://forum.effectivealtruism.org/topics/effective-altruism-funds).^[\\[8\\]](#fng87m1t61v5n)^\n\nFurther reading\n---------------\n\nGrace, Katja (2022) [Why work at AI Impacts?](https://aiimpacts.org/why-work-at-ai-impacts/), *AI Impacts*, March 6.\n\nExternal links\n--------------\n\n[AI Impacts](https://aiimpacts.org/). Official website.\n\n[AI Impacts](https://forum.effectivealtruism.org/users/ai-impacts). [Effective Altruism Forum](https://forum.effectivealtruism.org/tag/effective-altruism-forum-1) account.\n\n[Apply for a job](https://aiimpacts.org/jobs/).\n\n[Donate to AI Impacts](https://aiimpacts.org/donate/).\n\nRelated entries\n---------------\n\n[AI forecasting](https://forum.effectivealtruism.org/tag/ai-forecasting) | [AI risk](https://forum.effectivealtruism.org/topics/ai-risk)\n\n1.  ^**[^](#fnrefj7tu0r1dr5r)**^\n    \n    Muehlhauser, Luke (2014) [MIRI’s September newsletter](https://intelligence.org/2014/09/01/september-newsletter-2/), *Machine Intelligence Research Institute*, July 1.\n    \n2.  ^**[^](#fnrefm87ttm2zkx)**^\n    \n    Muehlhauser, Luke (2015) [An improved “AI Impacts” website](https://intelligence.org/2015/01/11/improved-ai-impacts-website/), *Machine Intelligence Research Institute*, January 11.\n    \n3.  ^**[^](#fnrefxzyd8918vs)**^\n    \n    Survival and Flourishing Fund (2018) [SFF-2019-Q4 S-process recommendations announcement](https://survivalandflourishing.fund/sff-2019-q4-recommendations), *Survival and Flourishing Fund*.\n    \n4.  ^**[^](#fnrefr6hp12uxy1h)**^\n    \n    Survival and Flourishing Fund (2019) [SFF-2020-H1 S-process recommendations announcement](https://survivalandflourishing.fund/sff-2020-h1-recommendations), *Survival and Flourishing Fund*.\n    \n5.  ^**[^](#fnrefy87npnoddnj)**^\n    \n    Survival and Flourishing Fund (2020) [SFF-2021-H1 S-process recommendations announcement](https://survivalandflourishing.fund/sff-2021-h1-recommendations), *Survival and Flourishing Fund*.\n    \n6.  ^**[^](#fnref6qtoi3x9q7d)**^\n    \n    Future Fund (2022) [Our grants and investments: AI Impacts](https://ftxfuturefund.org/all-grants/?_organization_name=ai-impacts), *Future Fund*.\n    \n7.  ^**[^](#fnref1bv5jpvlys1)**^\n    \n    Open Philanthropy (2022) [Grants database: AI Impacts](https://www.openphilanthropy.org/grants/?q=&organization-name=ai-impacts), *Open Philanthropy*.\n    \n8.  ^**[^](#fnrefg87m1t61v5n)**^\n    \n    Long-Term Future Fund (2020) [September 2020: Long-Term Future Fund grants](https://funds.effectivealtruism.org/funds/payouts/september-2020-long-term-future-fund-grants), *Effective Altruism Funds*, September."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "kZv56672Gyj7J4Gra",
    "name": "Ethics of artificial intelligence",
    "core": false,
    "slug": "ethics-of-artificial-intelligence",
    "oldSlugs": [
      "ai-ethics",
      "ai-ethics"
    ],
    "postCount": 12,
    "description": {
      "markdown": "The **ethics of artificial intelligence** is the study of the ethical issues arising from the creation of [artificial intelligence](https://forum.effectivealtruism.org/tag/artificial-intelligence).\n\nFurther reading\n---------------\n\nBostrom, Nick & Eliezer Yudkowsky (2014) [The ethics of artificial intelligence](https://doi.org/10.1017/CBO9781139046855.020), in Keith Frankish & William M. Ramsey (eds.) *The Cambridge Handbook of Artificial Intelligence*, Cambridge: Cambridge University Press, pp. 316–334.\n\nDubber, Markus D., Frank Pasquale & Sunit Das (eds.) (2020) [*The Oxford Handbook of Ethics of AI*](https://doi.org/10.1093/oxfordhb/9780190067397.001.0001), Oxford: Oxford University Press.\n\nMüller, Vincent C. (2020) [Ethics of artificial intelligence and robotics](https://plato.stanford.edu/archives/win2020/entries/ethics-ai/), *Stanford Encyclopedia of Philosophy*, April 30.\n\nRelated entries\n---------------\n\n[AI governance](https://forum.effectivealtruism.org/topics/ai-governance) | [AI safety](https://forum.effectivealtruism.org/tag/ai-safety) | [artificial intelligence](https://forum.effectivealtruism.org/tag/artificial-intelligence) | [artificial sentience](https://forum.effectivealtruism.org/tag/artificial-sentience)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "Ehfakmznya9QsBin7",
    "name": "AI boxing",
    "core": false,
    "slug": "ai-boxing",
    "oldSlugs": [
      "ai-boxing"
    ],
    "postCount": 1,
    "description": {
      "markdown": "**AI boxing** is a [capability control method](https://forum.effectivealtruism.org/tag/capability-control-methods) that consists of constructing the environment of a powerful AI system so as to minimize its interaction with the outside world.\n\nFurther reading\n---------------\n\nBostrom, Nick (2014) [*Superintelligence: Paths, Dangers, Strategies*](https://en.wikipedia.org/wiki/Special:BookSources/978-0-19-967811-2), Oxford: Oxford University Press, pp. 129–131.\n\nRelated entries\n---------------\n\n[AI alignment](https://forum.effectivealtruism.org/tag/ai-alignment) | [anthropic capture](https://forum.effectivealtruism.org/tag/anthropic-capture) | [capability control method](https://forum.effectivealtruism.org/tag/capability-control-method) | [motivation selection method](https://forum.effectivealtruism.org/tag/motivation-selection-method)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "Q9CvvDgHQ3hs4TtKm",
    "name": "AI Alignment Forum",
    "core": false,
    "slug": "ai-alignment-forum",
    "oldSlugs": null,
    "postCount": 3,
    "description": {
      "markdown": "The **AI Alignment Forum** is a forum for discussing technical research on [AI alignment](https://forum.effectivealtruism.org/tag/ai-alignment) that superseded the **Agent Foundations Forum**, established around 2015.^[\\[1\\]](#fnbap097k8ubj)^\n\nA beta version of the site, at the time named the **Alignment Forum**, was announced on 10 July 2018.^[\\[2\\]](#fnn6mx1am65ug)^ The site under its current name was officially launched on 29 October 2018.^[\\[3\\]](#fn8bdrwpqllap)^ The authors describe its purpose as follows:\n\n> Our first priority is obviously to avert catastrophic outcomes from unaligned Artificial Intelligence. We think the best way to achieve this at the margin is to build an online-hub for AI Alignment research, which both allows the existing top researchers in the field to talk about cutting-edge ideas and approaches, as well as the onboarding of new researchers and contributors.\n> \n> We think that to solve the AI Alignment problem, the field of AI Alignment research needs to be able to effectively coordinate a large number of researchers from a large number of organisations, with significantly different approaches. Two decades ago we might have invested heavily in the development of a conference or a journal, but with the onset of the internet, an online forum with its ability to do much faster and more comprehensive forms of peer-review seemed to us like a more promising way to help the field form a good set of standards and methodologies.\n\nThe AI Alignment Forum is built by [Lightcone Infrastructure](https://forum.effectivealtruism.org/tag/lightcone-infrastructure).\n\nFurther reading\n---------------\n\nHabryka, Oliver *et al.* (2018) [Introducing the AI Alignment Forum (FAQ)](https://www.alignmentforum.org/posts/FoiiRDC3EhjHx7ayY/introducing-the-ai-alignment-forum-faq), *AI Alignment Forum*, October 29.\n\nExternal links\n--------------\n\n[AI Alignment Forum](https://www.alignmentforum.org/). Official website.\n\nRelated entries\n---------------\n\n[Alignment Newsletter](https://forum.effectivealtruism.org/tag/alignment-newsletter) | [LessWrong](https://forum.effectivealtruism.org/tag/lesswrong) | [Lightcone Infrastructure](https://forum.effectivealtruism.org/tag/lightcone-infrastructure)\n\n1.  ^**[^](#fnrefbap097k8ubj)**^\n    \n    LaVictoire, Patrick (2015) [Welcome, new contributors](https://www.alignmentforum.org/posts/5bd75cc58225bf0670374efa/welcome-new-contributors), *Agent Foundations Forum*, March 23.\n    \n2.  ^**[^](#fnrefn6mx1am65ug)**^\n    \n    Arnold, Raymond (2018) [Announcing AlignmentForum.org beta](https://www.alignmentforum.org/posts/JiMAMNAb55Qq24nES/announcing-alignmentforum-org-beta), *AI Alignment Forum*, July 10.\n    \n3.  ^**[^](#fnref8bdrwpqllap)**^\n    \n    Habryka, Oliver *et al.* (2018) [Introducing the AI Alignment Forum (FAQ)](https://www.alignmentforum.org/posts/FoiiRDC3EhjHx7ayY/introducing-the-ai-alignment-forum-faq), *AI Alignment Forum*, October 29."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "YRwer2Ncu7dkzHMY6",
    "name": "Against Malaria Foundation",
    "core": false,
    "slug": "against-malaria-foundation",
    "oldSlugs": [
      "against-malaria-foundation-amf"
    ],
    "postCount": 23,
    "description": {
      "markdown": "The **Against Malaria Foundation** (**AMF**) is a British charity that provides funding for [mass distribution of long-lasting insecticide-treated nets](https://forum.effectivealtruism.org/tag/mass-distribution-of-long-lasting-insecticide-treated-nets) (LLINs) to populations at high risk of [malaria](https://forum.effectivealtruism.org/tag/malaria).\n\nHistory\n-------\n\nIn June 2003, Rob Mather, a strategy consultant based in London, decided to organize a swim with two of his friends to raise funds for a girl who had suffered severe burns in a house fire. Over the following months, Mather's initiative grew in size and scope, eventually leading to the creation of **World Swim Against Malaria** (**WSAM**), which organized a quarter of a million swims in around 160 countries. In August 2004, Mather set up AMF to handle the money raised through WSAM and to raise funds in ways other than swimming.^[\\[1\\]](#fn60lobxtgsub)^^[\\[2\\]](#fnx7gmt8nhswg)^\n\nAMF continues to operate mostly as it did when it first launched. Although registered as a charity in 12 countries, AMF has no offices, but is run from the back room of Mather's house.^[\\[3\\]](#fnfqlpqxquyf9)^\n\nActivities\n----------\n\nIn addition to funding distribution of nets, AMF has more recently supported research on piperonyl butoxide (PBO) nets, a new type of LLIN.^[\\[4\\]](#fnlaao49dmw19)^ Mosquito populations appear to have become more resistant to standard insecticides. According to GiveWell's rough estimates, this resistance has reduced the effectiveness of insecticide-treated nets by one-third in the areas where AMF operates.^[\\[5\\]](#fnmajgbxqz0f)^ Against resistant mosquitoes, PBO nets appear to be more effective than standard insecticide-treated nets. An AMF-funded trial—the largest such trial to assess the effectiveness of PBO nets to date—found that PBO nets reduced malaria 26%, 27% and 16% more than did conventional LLINs after 6, 12 and 18 months, respectively.^[\\[6\\]](#fnoftm90522ba)^\n\nEvaluation\n----------\n\nAs of July 2022, [GiveWell](https://forum.effectivealtruism.org/tag/givewell) estimates that AMF can deliver a LLIN at a cost of about $5, and that a donation to AMF has an average [cost-effectiveness](https://forum.effectivealtruism.org/tag/cost-effectiveness-analysis) of $5,500 per life saved.^[\\[7\\]](#fnv9lxhfgn64)^^[\\[8\\]](#fnssbmdabxil)^^[\\[9\\]](#fn09mgl17tmdrm)^  (The cost of delivering a net is much lower than the cost of saving a life because a small fraction of people who receive a net would otherwise have died of malaria, and because of other factors.)^[\\[8\\]](#fnssbmdabxil)^\n\nAMF has been a [GiveWell](https://forum.effectivealtruism.org/tag/givewell) top-rated charity every year since 2010, with the exception of 2013, when it was excluded from their ranking due to lack of [room for more funding](https://forum.effectivealtruism.org/tag/room-for-more-funding) .^[\\[10\\]](#fngblt5r8eydc)^ AMF is also included in [The Life You Can Save](https://forum.effectivealtruism.org/tag/the-life-you-can-save)'s list of \"best charities\".^[\\[11\\]](#fneqm0g2cmsfi)^\n\nAs of June 2022, AMF has received over $58 million in funding from [Open Philanthropy](https://forum.effectivealtruism.org/tag/open-philanthropy),^[\\[12\\]](#fn7njyyywpsug)^ and over $6.5 million from [Effective Altruism Funds](https://forum.effectivealtruism.org/topics/effective-altruism-funds).^[\\[13\\]](#fn7klr5h00q1)^^[\\[14\\]](#fnfynsdjy92o)^^[\\[15\\]](#fncpsjdb9pfbj)^ \n\nAMF is a highly [transparent](https://forum.effectivealtruism.org/tag/transparency) organization. The AMF website lists every donation received^[\\[16\\]](#fngerw26t1tm5)^ and every LLIN distributed,^[\\[17\\]](#fnbz4bvqwpj7u)^ as well as other pertinent information.^[\\[18\\]](#fndc8mnnby9)^\n\nFurther reading\n---------------\n\nFreeman, Luke (2022) [The journey to founding one of the world’s most effective charities | Rob Mather, AMF](https://www.youtube.com/watch?v=Ex7hgpXfw0U), *Giving What We Can*, August 24.\n\nGiveWell (2021) [Against Malaria Foundation](https://www.givewell.org/charities/amf), *GiveWell*, November.\n\nSánchez, Sebastián *et al* (2021) [Timeline of Against Malaria Foundation](https://timelines.issarice.com/wiki/Timeline_of_Against_Malaria_Foundation), *Timelines Wiki*.\n\nExternal links\n--------------\n\n[Against Malaria Foundation](https://www.againstmalaria.com/). Official website.\n\n[Donate to Against Malaria Foundation](https://www.againstmalaria.com/Donation.aspx).\n\n1.  ^**[^](#fnref60lobxtgsub)**^\n    \n    Against Malaria Foundation (2010) [About us](https://web.archive.org/web/20100219084813/http://www.againstmalaria.com:80/AboutUs.aspx), *Against Malaria Foundation*.\n    \n2.  ^**[^](#fnrefx7gmt8nhswg)**^\n    \n    Against Malaria Foundation (2021) [History](http://www.againstmalaria.com/History.aspx), *Against Malaria Foundation*.\n    \n3.  ^**[^](#fnreffqlpqxquyf9)**^\n    \n    Mather, Rob (2020) [AMA: Rob Mather, founder and CEO of the Against Malaria Foundation](https://forum.effectivealtruism.org/posts/T6Y6BzmvPMbGE7Hh9/ama-rob-mather-founder-and-ceo-of-the-against-malaria), *Effective Altruism Forum*, January 22.\n    \n4.  ^**[^](#fnreflaao49dmw19)**^\n    \n    Hollander, Catherine (2020) [GiveWell donors supported more than direct delivery: AMF and new net research](https://blog.givewell.org/2020/04/23/givewell-donors-supported-more-than-direct-delivery-amf-and-new-net-research/), *GiveWell*, April 23.\n    \n5.  ^**[^](#fnrefmajgbxqz0f)**^\n    \n    GiveWell (2020) [Insecticide resistance and malaria control](https://www.givewell.org/international/technical/programs/insecticide-treated-nets/insecticide-resistance-malaria-control), *GiveWell*, November.\n    \n6.  ^**[^](#fnrefoftm90522ba)**^\n    \n    Sherratt, Peter (2019) [PBO net trial results released (to 18 months)](https://www.againstmalaria.com/Newsitem.aspx?NewsItem=PBO-Net-Trial-Results-Released-to-18-months/m), *Against Malaria Foundation*, November 21.\n    \n7.  ^**[^](#fnrefv9lxhfgn64)**^\n    \n    GiveWell (2022) [Our top charities](https://www.givewell.org/charities/top-charities), *GiveWell*, July.\n    \n8.  ^**[^](#fnrefssbmdabxil)**^\n    \n    GiveWell (2022) [How we produce impact estimates](https://www.givewell.org/impact-estimates), *GiveWell*, July. \n    \n9.  ^**[^](#fnref09mgl17tmdrm)**^\n    \n    GiveWell (2022) [GiveWell directed grants to top charities with impact information (2020 onward)](https://docs.google.com/spreadsheets/d/1z065ab9PPMu9i5KiQ4yLyQJPFQCfEzHSgtHulPiZeBo/edit#gid=1407352843), *GiveWell*, July.\n    \n10.  ^**[^](#fnrefgblt5r8eydc)**^\n    \n    Karnofsky, Holden (2013) [Change in Against Malaria Foundation recommendation status (room-for-more-funding-related)](https://blog.givewell.org/2013/11/26/change-in-against-malaria-foundation-recommendation-status-room-for-more-funding-related/), *The GiveWell Blog*, November 26.\n    \n11.  ^**[^](#fnrefeqm0g2cmsfi)**^\n    \n    The Life You Can Save (2021) [Against Malaria Foundation](https://www.thelifeyoucansave.org/best-charities/against-malaria-foundation/), *The Life You Can Save*.\n    \n12.  ^**[^](#fnref7njyyywpsug)**^\n    \n    Open Philanthropy (2022) [Grants database: Against Malaria Foundation](https://www.openphilanthropy.org/grants/?q=&organization-name=against-malaria-foundation), *Open Philanthropy*.\n    \n13.  ^**[^](#fnref7klr5h00q1)**^\n    \n    Global Health and Development Fund (2017) [April 2017: Against Malaria Foundation](https://funds.effectivealtruism.org/funds/payouts/april-2017-against-malaria-foundation), *Effective Altruism Funds*, April.\n    \n14.  ^**[^](#fnreffynsdjy92o)**^\n    \n    Global Health and Development Fund (2021) [February 2021: Global Health and Development Fund grants](https://funds.effectivealtruism.org/funds/payouts/february-2021-global-health-and-development-fund-grants), *Effective Altruism Funds*, February.\n    \n15.  ^**[^](#fnrefcpsjdb9pfbj)**^\n    \n    Global Health and Development Fund (2022) [January 2022: Against Malaria Foundation](https://funds.effectivealtruism.org/funds/payouts/january-2022-against-malaria-foundation), *Effective Altruism Funds*, January.\n    \n16.  ^**[^](#fnrefgerw26t1tm5)**^\n    \n    Against Malaria Foundation (2021) [Sponsors and donors](https://www.againstmalaria.com/Donations.aspx), *Against Malaria Foundation*.\n    \n17.  ^**[^](#fnrefbz4bvqwpj7u)**^\n    \n    Against Malaria Foundation (2021) [Net distributions — world](https://www.againstmalaria.com/Distributions.aspx), *Against Malaria Foundation*.\n    \n18.  ^**[^](#fnrefdc8mnnby9)**^\n    \n    Against Malaria Foundation (2021) [Transparency](https://www.againstmalaria.com/Transparency.aspx), *Against Malaria Foundation*."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "bSGimoamvLEJsBC7G",
    "name": "Accidental harm",
    "core": false,
    "slug": "accidental-harm",
    "oldSlugs": [
      "accidental-harm"
    ],
    "postCount": 14,
    "description": {
      "markdown": "**Accidental harm** is harm resulting from unforeseen effects of attempts to do good.\n\nFurther reading\n---------------\n\nAird, Michael (2020) [Information hazards and downside risks](https://www.lesswrong.com/s/r3dKPwpkkMnJPbjZE), *LessWrong*, February 18.\n\nDalton, Max & Jonas Volmer (2018) [How to avoid accidentally having a negative impact with your project](https://www.youtube.com/watch?v=RU168E9fLIM), *Effective Altruism Global*, October 27.\n\nHilton, Samuel (2019) [Managing risk in the EA policy space](https://forum.effectivealtruism.org/posts/Q7qzxhwEWeKC3uzK3/managing-risk-in-the-ea-policy-space), *Effective Altruism Forum*, December 9.\n\nKokotajlo, Daniel & Alexandra Oprea (2020) [Counterproductive altruism: The other heavy tail](http://doi.org/10.1111/phpe.12133), *Philosophical Perspectives*, vol. 34, pp. 134–163.\n\nWiblin, Robert & Howie Lempel (2018) [Ways people trying to do good accidentally make things worse, and how to avoid them](https://80000hours.org/articles/accidental-harm/), *80,000 Hours*, October 16.\n\nWikipedia (2002) [Unintended consequences](https://en.wikipedia.org/wiki/Unintended_consequences), *Wikipedia*, October 1 (updated 20 April 2021).  \n*See in particular, \"Unexpected drawbacks\" and \"Perverse results\", not \"Unintended benefits\".*\n\nRelated entries\n---------------\n\n[differential progress](https://forum.effectivealtruism.org/tag/differential-progress) | [information hazard](https://forum.effectivealtruism.org/tag/information-hazard) | [spillover effects](https://forum.effectivealtruism.org/tag/spillover-effects) | [unilateralist's curse](https://forum.effectivealtruism.org/tag/unilateralist-s-curse)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "HeXpqenacwbsi9TCo",
    "name": "Acausal trade",
    "core": false,
    "slug": "acausal-trade",
    "oldSlugs": [
      "acausal-trade"
    ],
    "postCount": 2,
    "description": {
      "markdown": "**Acausal trade** is a speculative form of [cooperation](https://forum.effectivealtruism.org/tag/moral-cooperation) between agents who cannot causally affect or communicate with each other, but can predict each others' decisions sufficiently accurately to act in ways that benefit one another.\n\nFurther reading\n---------------\n\nFox, Joshua *et al.* (2012) [Acausal trade](https://www.lesswrong.com/tag/acausal-trade), *LessWrong Wiki*, March 12 (updated 21 September 2021).\n\nRelated entries\n---------------\n\n[altruistic coordination](https://forum.effectivealtruism.org/tag/altruistic-coordination) | [moral cooperation](https://forum.effectivealtruism.org/tag/moral-cooperation) | [moral trade](https://forum.effectivealtruism.org/tag/moral-trade)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "dHcHjoPQB2HXouD6w",
    "name": "Abhijit Banerjee",
    "core": false,
    "slug": "abhijit-banerjee",
    "oldSlugs": null,
    "postCount": 1,
    "description": {
      "markdown": "**Abhijit Vinayak Banerjee** (born 21 February 1961) is an Indian-born American economist. Together with [Esther Duflo](https://forum.effectivealtruism.org/tag/esther-duflo) and [Michael Kremer](https://forum.effectivealtruism.org/tag/michael-kremer), he was awarded the 2019 Nobel Prize in Economic Sciences for his use of [randomized controlled trials](https://forum.effectivealtruism.org/tag/randomized-controlled-trials) to alleviate [global poverty](https://forum.effectivealtruism.org/tag/global-poverty).^[\\[1\\]](#fn2phsb2tkac)^\n\nBackground\n----------\n\nBanerjee studied at the University of Calcutta and Jawaharlal Nehru University in Delhi, and later at Harvard University, where in 1988 he obtained a doctoral degree in economics. After teaching at Harvard and Princeton, he moved to the Massachusetts Institute of Technology, becoming a full professor in 1996. Together with Esther Duflo and Sendhil Mullainathan, he founded the [Abdul Latif Jameel Poverty Action Lab](https://forum.effectivealtruism.org/tag/abdul-latif-jameel-poverty-action-lab) (J-PAL) in 2003, and remains one of the two Directors of that global research center.\n\nFurther reading\n---------------\n\nBanerjee, Abhijit (2019) [An accidental economist: a brief history](https://www.nobelprize.org/prizes/economic-sciences/2019/banerjee/facts/), *The Nobel Prize*.\n\nExternal links\n--------------\n\n[Abhijit Banerjee](https://economics.mit.edu/faculty/banerjee). MIT homepage.\n\nRelated entries\n---------------\n\n[Esther Duflo](https://forum.effectivealtruism.org/tag/esther-duflo) | [global poverty](https://forum.effectivealtruism.org/tag/global-poverty) | [Michael Kremer](https://forum.effectivealtruism.org/tag/michael-kremer) | [*Poor Economics*](https://forum.effectivealtruism.org/tag/poor-economics) | [randomized controlled trials](https://forum.effectivealtruism.org/tag/randomized-controlled-trials)\n\n1.  ^**[^](#fnref2phsb2tkac)**^\n    \n    Royal Swedish Academy of Sciences (2019) [The Prize in Economic Sciences 2019](https://www.nobelprize.org/uploads/2019/10/press-economicsciences2019-2.pdf), *The Nobel Prize*, October 14."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "2x445EbpkWeSHqv4Y",
    "name": "80,000 Hours",
    "core": false,
    "slug": "80-000-hours",
    "oldSlugs": null,
    "postCount": 186,
    "description": {
      "markdown": "**80,000 Hours** is a nonprofit organization that helps individuals select [high-impact careers](https://forum.effectivealtruism.org/tag/career-choice). It conducts research on pressing social problems and provides one-on-one advice on the basis of this research.\n\nHistory\n-------\n\n80,000 Hours was founded in July 2011 by Ben Todd and [William MacAskill](https://forum.effectivealtruism.org/tag/william-macaskill). The organization was called **High Impact Careers** for a year or so before adopting its current name.^[\\[1\\]](#fn5u551jydpx9)^^[\\[2\\]](#fn5i5g5nqlq0g)^ In its early years, 80,000 Hours placed great emphasis on [earning to give](https://forum.effectivealtruism.org/tag/earning-to-give); since around 2015, the organization has de-emphasized this career path in favor of direct work, especially work in research, advocacy, and policy.^[\\[3\\]](#fn8az4rstzww2)^\n\nApproach\n--------\n\nAt the core of 80,000 Hours' rankings and recommendations are two main \"frameworks\": a [problem framework](https://forum.effectivealtruism.org/tag/problem-framework), which ranks problems in terms of [importance](https://forum.effectivealtruism.org/tag/importance), [tractability](https://forum.effectivealtruism.org/tag/tractability) and [neglectedness](https://forum.effectivealtruism.org/tag/neglectedness), and a [career framework](https://forum.effectivealtruism.org/tag/career-framework), which recommends careers based primarily on [career capital](https://forum.effectivealtruism.org/tag/career-capital), [role impact](https://forum.effectivealtruism.org/tag/role-impact), and [supportive conditions](https://forum.effectivealtruism.org/tag/supportive-conditions), with [personal fit](https://forum.effectivealtruism.org/tag/personal-fit) as a common fourth factor in both frameworks.\n\nFunding\n-------\n\nAs of July 2022, 80,000 Hours has received over $9.8 million in funding from [Open Philanthropy](https://forum.effectivealtruism.org/tag/open-philanthropy),^[\\[4\\]](#fnawqiwiim2ft)^ over 1.1 million from [Effective Altruism Funds](https://forum.effectivealtruism.org/topics/effective-altruism-funds),^[\\[5\\]](#fn7vlfy6cj96v)^^[\\[6\\]](#fn7e0vyumckwe)^^[\\[7\\]](#fnpz4iltcv1ij)^^[\\[8\\]](#fn7x4uwbj3wo)^^[\\[9\\]](#fnue7uy27i1gn)^^[\\[10\\]](#fn4sndugscned)^^[\\[11\\]](#fnoaj93vpllqj)^^[\\[12\\]](#fn2wr3k8ef3hu)^ and nearly $300,000 from the [Survival and Flourishing](https://forum.effectivealtruism.org/topics/survival-and-flourishing) Fund.^[\\[13\\]](#fnmgnltltvy48)^^[\\[14\\]](#fn1eoa6wwdb7e)^\n\nFurther reading\n---------------\n\n80,000 Hours (2014) [About us](https://80000hours.org/about/), *80,000 Hours*, August (updated July 2021).\n\nTodd, Benjamin (2015), '80,000 Hours', in Ryan Carey (ed.), [*The Effective Altruism Handbook*](http://www.stafforini.com/docs/Carey%20-%20The%20effective%20altruism%20handbook.pdf), 1st ed., Oxford: The Centre for Effective Altruism, pp. 120-124\n\nTodd, Benjamin (2016) [*80,000 Hours: Find a Fulfilling Career That Does Good*](https://en.wikipedia.org/wiki/Special:BookSources/9781537324005), Oxford: Centre for Effective Altruism.\n\nExternal links\n--------------\n\n[80,000 Hours](https://80000hours.org/). Official website.\n\n[80,000 Hours](https://forum.effectivealtruism.org/users/80000_hours). [Effective Altruism Forum](https://forum.effectivealtruism.org/tag/effective-altruism-forum-1) account.\n\n[Apply for a job](https://80000hours.org/about/work-with-us/). \n\n[Donate to 80,000 Hours](https://80000hours.org/support-us/donate/).\n\nRelated entries\n---------------\n\n[80,000 Hours Podcast](https://forum.effectivealtruism.org/tag/80-000-hours-podcast) | [career advising](https://forum.effectivealtruism.org/tag/career-advising) | [career choice](https://forum.effectivealtruism.org/tag/career-choice) |  [career framework](https://forum.effectivealtruism.org/tag/career-framework/) |  [problem framework](https://forum.effectivealtruism.org/tag/problem-framework) \n\n1.  ^**[^](#fnref5u551jydpx9)**^\n    \n    MacAskill, William (2014) [The history of the term “effective altruism”](https://forum.effectivealtruism.org/posts/9a7xMXoSiQs3EYPA2/the-history-of-the-term-effective-altruism), *Effective Altruism Forum*, March 10.\n    \n2.  ^**[^](#fnref5i5g5nqlq0g)**^\n    \n    Kaufman, Jeff (2015) [Giving vs doing](https://www.jefftk.com/p/giving-vs-doing), *Jeff Kaufman’s Blog*, November 29.\n    \n3.  ^**[^](#fnref8az4rstzww2)**^\n    \n    MacAskill, William (2015) [80,000 Hours thinks that only a small proportion of people should earn to give long term](https://80000hours.org/2015/07/80000-hours-thinks-that-only-a-small-proportion-of-people-should-earn-to-give-long-term/), *80,000 Hours*, July 6.\n    \n4.  ^**[^](#fnrefawqiwiim2ft)**^\n    \n    Open Philanthropy (2022) [Grants database: 80,000 Hours](https://www.openphilanthropy.org/grants/?q=&organization-name=80000-hours), *Open Philanthropy*.\n    \n5.  ^**[^](#fnref7vlfy6cj96v)**^\n    \n    Effective Altruism Infrastructure Fund (2018) [July 2018: EA Meta Fund grants](https://funds.effectivealtruism.org/funds/payouts/july-2018-ea-meta-fund-grants), *Effective Altruism Funds*, July.\n    \n6.  ^**[^](#fnref7e0vyumckwe)**^\n    \n    Long-Term Future Fund (2018) [July 2018: Long-Term Future Fund grants](https://funds.effectivealtruism.org/funds/payouts/july-2018-long-term-future-fund-grants), *Effective Altruism Funds*, July.\n    \n7.  ^**[^](#fnrefpz4iltcv1ij)**^\n    \n    Effective Altruism Infrastructure Fund (2018) [November 2018: EA Meta Fund grants](https://funds.effectivealtruism.org/funds/payouts/november-2018-ea-meta-fund-grants), *Effective Altruism Funds*, November.\n    \n8.  ^**[^](#fnref7x4uwbj3wo)**^\n    \n    Effective Altruism Infrastructure Fund (2019) [March 2019: EA Meta Fund grants](https://funds.effectivealtruism.org/funds/payouts/march-2019-ea-meta-fund-grants), *Effective Altruism Funds*, March.\n    \n9.  ^**[^](#fnrefue7uy27i1gn)**^\n    \n    Effective Altruism Infrastructure Fund (2019) [July 2019: EA Meta Fund grants](https://funds.effectivealtruism.org/funds/payouts/july-2019-ea-meta-fund-grants), *Effective Altruism Funds*, July.\n    \n10.  ^**[^](#fnref4sndugscned)**^\n    \n    Effective Altruism Infrastructure Fund (2020) [March 2020: EA Meta Fund Grants](https://funds.effectivealtruism.org/funds/payouts/march-2020-ea-meta-fund-grants), *Effective Altruism Funds*, March.\n    \n11.  ^**[^](#fnrefoaj93vpllqj)**^\n    \n    Long-Term Future Fund (2020) [April 2020: Long-Term Future Fund grants and recommendations](https://funds.effectivealtruism.org/funds/payouts/april-2020-long-term-future-fund-grants-and-recommendations), *Effective Altruism Funds*, April.\n    \n12.  ^**[^](#fnref2wr3k8ef3hu)**^\n    \n    Effective Altruism Infrastructure Fund (2020) [July 2020: EA Meta Fund grants](https://funds.effectivealtruism.org/funds/payouts/july-2020-ea-meta-fund-grants), *Effective Altruism Funds*, July.\n    \n13.  ^**[^](#fnrefmgnltltvy48)**^\n    \n    Survival and Flourishing Fund (2018) [SFF-2019-Q4 S-process recommendations announcement](https://survivalandflourishing.fund/sff-2019-q4-recommendations), *Survival and Flourishing Fund*.\n    \n14.  ^**[^](#fnref1eoa6wwdb7e)**^\n    \n    Survival and Flourishing Fund (2019) [SFF-2020-H1 S-process recommendations announcement](https://survivalandflourishing.fund/sff-2020-h1-recommendations), *Survival and Flourishing Fund*."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "ZKCqG3NXwHXXNxvXu",
    "name": "Abdul Latif Jameel Poverty Action Lab",
    "core": false,
    "slug": "abdul-latif-jameel-poverty-action-lab",
    "oldSlugs": [
      "abdul-latif-jameel-poverty-action-lab-j-pal-1"
    ],
    "postCount": 6,
    "description": {
      "markdown": "The **Abdul Latif Jameel Poverty Action Lab** (**J-PAL**) is a global research center working to reduce poverty by ensuring that policy is informed by scientific evidence.\n\nHistory\n-------\n\nJ-PAL was founded in 2003 by [Abhijit Banerjee](https://forum.effectivealtruism.org/tag/abhijit-banerjee), [Esther Duflo](https://forum.effectivealtruism.org/tag/esther-duflo) and Sendhil Mullainathan.^[\\[1\\]](#fnqqyhvq94a9m)^ Rachel Glennerster was its Executive Director between 2004 and 2017. In 2019, Banerjee and Duflo, together with [Michael Kremer](https://forum.effectivealtruism.org/tag/michael-kremer), were awarded the Nobel Memorial Prize in Economics for their use of [randomized controlled trials](https://forum.effectivealtruism.org/tag/randomized-controlled-trials) to alleviate [global poverty](https://forum.effectivealtruism.org/tag/global-poverty).^[\\[2\\]](#fn3gxsjpy8jj3)^\n\n[Innovations for Poverty Action](https://forum.effectivealtruism.org/tag/innovations-for-poverty-action) (IPA) is a close partner of J-PAL.\n\nFunding\n-------\n\nIn January 2019, [GiveWell](https://forum.effectivealtruism.org/tag/givewell) recommended a $1 million grant—via the [Global Health and Development Fund](https://forum.effectivealtruism.org/tag/global-health-and-development-fund)—to [Innovation in Government Initiative](https://forum.effectivealtruism.org/tag/innovation-in-government-initiative), a grantmaking entity that sits within J-PAL.^[\\[3\\]](#fnjaro587gk7)^^[\\[4\\]](#fn3aa8cq10x7m)^\n\nAs of July 2022, J-PAL has received $200,000 in grants from [Open Philanthropy](https://forum.effectivealtruism.org/tag/open-philanthropy).^[\\[5\\]](#fn7g0qsxlrl5y)^\n\nFurther reading\n---------------\n\nBanerjee, Abhijit & Esther Duflo (2011) [*Poor Economics: A Radical Rethinking of the Way to Fight Global Poverty*](https://en.wikipedia.org/wiki/Special:BookSources/9781586487980), New York: PublicAffairs.\n\nBanerjee, Abhijit & Esther Duflo (2019) [*Good Economics for Hard Times: Better Answers to Our Biggest Problems*](https://en.wikipedia.org/wiki/Special:BookSources/9781610399500), New York: PublicAffairs.\n\nBloomberg Businessweek (2010) [The pragmatic rebels](https://www.bloomberg.com/news/articles/2010-07-02/the-pragmatic-rebels), *Bloomberg Businessweek*, July 2.\n\nDhaliwal, Iqbal, Claire Walsh & Elie Hassenfeld (2016) [A conversation with the Abdul Latif Jameel Poverty Action Lab](https://files.givewell.org/files/conversations/Iqbal_Dhaliwal_and_Claire_Walsh_2-12-16_(public).pdf), *GiveWell*, February 12.\n\nGlennerster, Rachel *et al.* (2014) [A conversation with the Abdul Latif Jameel Poverty Action Lab](https://files.givewell.org/files/conversations/Iqbal_Dhaliwal_and_Claire_Walsh_2-12-16_%28public%29.pdf), *GiveWell*, December 22.\n\nSanchez, Sebastian *et al.* (2020) [Timeline of Abdul Latif Jameel Poverty Action Lab](https://timelines.issarice.com/wiki/Timeline_of_Abdul_Latif_Jameel_Poverty_Action_Lab), *Timelines Wiki*, April 10.\n\nExternal links\n--------------\n\n[Abdul Latif Jameel Poverty Action Lab](https://www.povertyactionlab.org/). Official website.\n\n[Apply for a job](https://www.povertyactionlab.org/careers).\n\n[Donate to J-PAL](https://www.povertyactionlab.org/page/support-us).\n\nRelated links\n-------------\n\n[Abhijit Banerjee](https://forum.effectivealtruism.org/tag/abhijit-banerjee)  | [Esther Duflo](https://forum.effectivealtruism.org/tag/esther-duflo) | [Innovations for Poverty Action](https://forum.effectivealtruism.org/tag/innovations-for-poverty-action) | [Innovation in Government Initiative](https://forum.effectivealtruism.org/tag/innovation-in-government-initiative) | [randomized controlled trials](https://forum.effectivealtruism.org/tag/randomized-controlled-trials)\n\n1.  ^**[^](#fnrefqqyhvq94a9m)**^\n    \n    J-PAL (2020) [About us](https://www.povertyactionlab.org/about-j-pal), *J-PAL*.\n    \n2.  ^**[^](#fnref3gxsjpy8jj3)**^\n    \n    Royal Swedish Academy of Sciences (2019) [The Prize in Economic Sciences 2019](https://www.nobelprize.org/uploads/2019/10/press-economicsciences2019-2.pdf), *The Nobel Prize*, October 14.\n    \n3.  ^**[^](#fnrefjaro587gk7)**^\n    \n    GiveWell (2019) [Innovation in government initiative — general support](https://www.givewell.org/research/incubation-grants/innovation-in-government-initiative/december-2018-grant), *GiveWell*, January.\n    \n4.  ^**[^](#fnref3aa8cq10x7m)**^\n    \n    Global Health and Development Fund (2019) [January 2019: J-PAL’s Innovation in Government Initiative](https://funds.effectivealtruism.org/funds/payouts/january-2019-j-pals-innovation-in-government-initiative), *Effective Altruism Funds*, January.\n    \n5.  ^**[^](#fnref7g0qsxlrl5y)**^\n    \n    Open Philanthropy (2021) [Grants database: Abdul Latif Jameel Poverty Action Lab](https://www.openphilanthropy.org/grants/?q=&organization-name=abdul-latif-jameel-poverty-action-lab), *Open Philanthropy*."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "hgPbtuMMGZHqXQhC4",
    "name": "ALLFED",
    "core": false,
    "slug": "allfed",
    "oldSlugs": null,
    "postCount": 27,
    "description": {
      "markdown": "**ALLFED** (the **Alliance to Feed the Earth in Disasters**) is a nonprofit organization dedicated to ensuring that people have enough to eat in the event of a [global catastrophe](https://forum.effectivealtruism.org/tag/global-catastrophic-risk).\n\nFunding\n-------\n\nAs of June 2022, ALLFED has received nearly $1.3 million in funding from the [Survival and Flourishing](https://forum.effectivealtruism.org/topics/survival-and-flourishing) Fund.^[\\[1\\]](#fnvgadg64qolj)^^[\\[2\\]](#fn9hxx1ql1z69)^^[\\[3\\]](#fntvtrs0ffpv)^\n\nFurther reading\n---------------\n\nDenkenberger, David & Joshua M. Pearce (2015) [*Feeding Everyone No Matter What: Managing Food Security after Global Catastrophe*](http://doi.org/10.1016/C2015-0-04027-8), Amsterdam: Academic Press.\n\nGreenberg, Spencer (2021) [What does humanity need to survive after a global catastrophe? (with David Denkenberger)](https://clearerthinkingpodcast.com/episode/081/), *Clearer Thinking*, November 25.\n\nRighetti, Luca & Fin Moorhouse (2022) [Mike Hinge on feeding everyone in a disaster](https://hearthisidea.com/episodes/mike), *Hear This Idea*, April 4.\n\nTieman, Ross (2021) [2021 ALLFED highlights](https://forum.effectivealtruism.org/posts/kqjExMCGYrs7NduGZ/2021-allfed-highlights), *Effective Altruism Forum*, November 17.\n\nWiblin, Robert & Keiran Harris (2018) [We could feed all 8 billion people through a nuclear winter. Dr David Denkenberger is working to make it practical](https://80000hours.org/podcast/episodes/david-denkenberger-allfed-and-feeding-everyone-no-matter-what/), *80,000 Hours*, December 27.\n\nWiblin, Robert & Keiran Harris (2021) [David Denkenberger on using paper mills and seaweed to feed everyone in a catastrophe, ft Sahil Shah](https://80000hours.org/podcast/episodes/david-denkenberger-sahil-shah-using-paper-mills-and-seaweed-in-catastrophes/), *80,000 Hours*, November 29.\n\nExternal links\n--------------\n\n[ALLFED](https://allfed.info/). Official website.\n\n[Apply for a job](https://allfed.info/join-us).\n\nRelated entries\n---------------\n\n[civilizational collapse](https://forum.effectivealtruism.org/tag/civilizational-collapse) | [food security](https://forum.effectivealtruism.org/tag/food-security) | [resilient food](https://forum.effectivealtruism.org/tag/resilient-food)\n\n1.  ^**[^](#fnrefvgadg64qolj)**^\n    \n    Survival and Flourishing Fund (2018) [SFF-2019-Q4 S-process recommendations announcement](https://survivalandflourishing.fund/sff-2019-q4-recommendations), *Survival and Flourishing Fund*.\n    \n2.  ^**[^](#fnref9hxx1ql1z69)**^\n    \n    Survival and Flourishing Fund (2020) [SFF-2021-H1 S-process recommendations announcement](https://survivalandflourishing.fund/sff-2021-h1-recommendations), *Survival and Flourishing Fund*.\n    \n3.  ^**[^](#fnreftvtrs0ffpv)**^\n    \n    Survival and Flourishing Fund (2020) [SFF-2021-H2 S-process recommendations announcement](https://survivalandflourishing.fund/sff-2021-h2-recommendations), *Survival and Flourishing Fund*."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "xsiR75hLgHBgtosDy",
    "name": "Global priorities research",
    "core": false,
    "slug": "global-priorities-research",
    "oldSlugs": [
      "global-priorities-research"
    ],
    "postCount": 108,
    "description": {
      "markdown": "**Global priorities research** (**GPR**) is research into issues that can help decide how to allocate finite resources most cost-effectively.^[\\[1\\]](#fnnyvbflivwv)^ GPR can include finding and [prioritising between different causes](https://forum.effectivealtruism.org/tag/cause-prioritization) as well as [macrostrategy](https://forum.effectivealtruism.org/tag/macrostrategy) or \"foundational\" research that would inform cause prioritization in a less direct way (e.g., research into the [Fermi paradox](https://forum.effectivealtruism.org/tag/fermi-paradox) or the [hinge of history hypothesis](https://forum.effectivealtruism.org/tag/hinge-of-history)).\n\nEvaluation\n----------\n\n[80,000 Hours](https://forum.effectivealtruism.org/tag/80-000-hours) rates GPR a \"highest priority area\": a problem at the top of their ranking of global issues assessed by [importance, tractability and neglectedness](https://forum.effectivealtruism.org/tag/itn-framework-1).^[\\[2\\]](#fnbr35f2n6g3k)^\n\nFurther reading\n---------------\n\nDuda, Roman (2016) [Global priorities research](https://80000hours.org/problem-profiles/global-priorities-research/), *80,000 Hours*, April (updated July 2018).\n\nO’Keeffe-O’Donovan, Rossa (2020) [An introduction to global priorities research](https://forum.effectivealtruism.org/posts/MKWujo6nBvjwYAu82/rossa-o-keeffe-o-donovan-an-introduction-to-global), *Effective Altruism Student Summit 2020*, October 25.  \n*An introduction to global priorities research, including  a discussion of how it differs from cause prioritization research.*\n\nRelated entries\n---------------\n\n[cause candidates](https://forum.effectivealtruism.org/tag/cause-candidates) | [cause prioritization](https://forum.effectivealtruism.org/tag/cause-prioritization) | [Cause X](https://forum.effectivealtruism.org/tag/cause-x) | [Global Priorities Institute](https://forum.effectivealtruism.org/tag/global-priorities-institute) | [less-discussed causes](https://forum.effectivealtruism.org/tag/less-discussed-causes) | [local priorities research](https://forum.effectivealtruism.org/tag/local-priorities-research-1) | [macrostrategy](https://forum.effectivealtruism.org/tag/macrostrategy)\n\n1.  ^**[^](#fnrefnyvbflivwv)**^\n    \n    Global Priorities Institute (2019) [About us](https://globalprioritiesinstitute.org/about-us/), *Global Priorities Institute*.\n    \n2.  ^**[^](#fnrefbr35f2n6g3k)**^\n    \n    80,000 Hours (2021) [Our current list of the most important world problems](https://80000hours.org/problem-profiles/), *80,000 Hours*."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "54Ls7K7N53kYws9ja",
    "name": "Job listing (open)",
    "core": false,
    "slug": "job-listing-open",
    "oldSlugs": [
      "job-listing",
      "job-listing",
      "job-listing-open"
    ],
    "postCount": 150,
    "description": {
      "markdown": "Use **job listing (open)** for posts that discuss a specific job or jobs that one or more organizations are hiring for.\n\nYou can also find job listings on the \"[Who's hiring?](https://forum.effectivealtruism.org/posts/DXcg6N6CGvRA2vrCk/who-s-hiring-may-september-2022)\" post.\n\nAfter a request has been completed, or if it's no longer relevant, downvote this tag and add [job listing (closed)](https://forum.effectivealtruism.org/tag/job-listing-closed). This can be done by anyone, not just the original poster.\n\nThere are also job boards: \n\n*   [A list of job boards](https://tomweinresearch.me/job-boards/)\n*   [Animal Advocacy Careers job board](https://www.animaladvocacycareers.org/job-board)\n*   [The 80,000 Hours job board](https://80000hours.org/job-board/)\n*   [EA internships board](https://ea-internships.pory.app/board)\n\nYou can also find more ways to get involved on the [take action](https://forum.effectivealtruism.org/tag/take-action) page."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "CvG8zdz79XjMTovQK",
    "name": "Cause candidates",
    "core": false,
    "slug": "cause-candidates",
    "oldSlugs": [
      "new-cause-area-cause-x",
      "new-cause-areas-cause-x",
      "new-cause-area-proposals",
      "new-cause-proposals",
      "cause-proposals",
      "new-cause-proposals",
      "cause-proposals",
      "cause-candidates",
      "cause-candidates",
      "cause-candidates"
    ],
    "postCount": 289,
    "description": {
      "markdown": "The **cause candidates** tag is meant for posts that specifically suggest, consider or present a cause area, cause, or intervention. This is independent of the quality of the suggestion, the community consensus about it, or the level of specificity.\n\nFurther reading\n---------------\n\nPicón, Leonardo (2022) [Big list of cause candidates: January 2021–March 2022 update](https://forum.effectivealtruism.org/posts/DBhuERvKRgGpLiK6T/big-list-of-cause-candidates-january-2021-march-2022-update), *Effective Altruism Forum*, April 30.\n\nSempere, Nuño (2020) [Big list of cause candidates](https://forum.effectivealtruism.org/posts/SCqRu6shoa8ySvRAa/big-list-of-cause-candidates), *Effective Altruism Forum*, December 25.\n\nRelated entries\n---------------\n\n[Cause X](https://forum.effectivealtruism.org/tag/cause-x) | [cause prioritization](https://forum.effectivealtruism.org/tag/cause-prioritization) | [global priorities research](https://forum.effectivealtruism.org/tag/global-priorities-research) | [less-discussed causes](https://forum.effectivealtruism.org/tag/less-discussed-causes)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "gxgimN3k2z5BLmgDa",
    "name": "Giving What We Can",
    "core": false,
    "slug": "giving-what-we-can",
    "oldSlugs": null,
    "postCount": 151,
    "description": {
      "markdown": "**Giving What We Can** (**GWWC**) is an organisation dedicated to inspiring and supporting people to give more, and give more effectively. It is a project of the [Centre for Effective Altruism](https://forum.effectivealtruism.org/tag/centre-for-effective-altruism-cea).\n\nGiving What We Can promotes three [donation pledges](https://forum.effectivealtruism.org/tag/donation-pledge), the most prominent of which is a commitment to donate at least 10% of one's income each year in the way one thinks will achieve the most good. As of June 2022, this pledge has been signed by more than 7,500 people.\n\nGiving What We Can also aims to support its members, to help them connect with and support each other, and to [build effective altruism](https://forum.effectivealtruism.org/tag/building-effective-altruism-1), [effective giving](https://forum.effectivealtruism.org/tag/effective-giving-1), and various related cause areas.\n\nHistory\n-------\n\nGiving What We Can was conceived of by [Toby Ord](https://forum.effectivealtruism.org/tag/toby-ord) (who was inspired by the ideas of ethicists such as [Peter Singer](https://forum.effectivealtruism.org/tag/peter-singer)) to commit to donating a large proportion of his income to effective charities.\n\nIn November 2009, Ord and [Will MacAskill](https://forum.effectivealtruism.org/tag/william-macaskill) launched Giving What We Can as an international community of people who were committed to giving more, and giving more effectively. The Centre for Effective Altruism was incorporated in 2011 as registered charity and an umbrella organisation for Giving What We Can and the then newly founded [80,000 Hours](https://forum.effectivealtruism.org/tag/80-000-hours).\n\nGiving What We Can was one of the first in a growing network of like-minded organisations focused on [effective altruism](https://forum.effectivealtruism.org/tag/effective-altruism), the project of using evidence and reason to figure out how to best contribute to helping others, and taking action on that basis.\n\nMembers\n-------\n\nNotable Giving What We Can members include [Sam Bankman-Fried](https://forum.effectivealtruism.org/tag/sam-bankman-fried), [Nick Beckstead](https://forum.effectivealtruism.org/tag/nick-beckstead), [Liv Boeree](https://forum.effectivealtruism.org/tag/liv-boeree), Rachel Glennerster, Jim Greenbaum, [Michael Kremer](https://forum.effectivealtruism.org/tag/michael-kremer), [William MacAskill](https://forum.effectivealtruism.org/tag/william-macaskill), [Dylan Matthews](https://forum.effectivealtruism.org/tag/dylan-matthews), [Derek Parfit](https://forum.effectivealtruism.org/tag/derek-parfit), [Kelsey Piper](https://forum.effectivealtruism.org/tag/kelsey-piper), Janet Radcliffe Richards,  [Peter Singer](https://forum.effectivealtruism.org/tag/peter-singer), and Eva Vivalt.^[\\[1\\]](#fnjhcsnfxxmx)^\n\nFunding\n-------\n\nAs of July 2022, Giving What We Can has received $700,000 in funding from the [Future Fund](https://forum.effectivealtruism.org/topics/future-fund).^[\\[2\\]](#fnaqwsnsj4f8e)^\n\nFurther reading\n---------------\n\nRighetti, Luca & Fin Moorhouse (2020) [Luke Freeman on Giving What We Can and community building](https://hearthisidea.com/episodes/luke), *Hear This Idea*, November 29.\n\nExternal links\n--------------\n\n[Giving What We Can](https://www.givingwhatwecan.org/). Official website.\n\n[Apply for a job](https://www.givingwhatwecan.org/get-involved/careers).\n\n[Take the Giving What We Can pledge](https://www.givingwhatwecan.org/pledge/).\n\nRelated entries\n---------------\n\n[Centre for Effective Altruism](https://forum.effectivealtruism.org/tag/centre-for-effective-altruism-1) | [donation pledge](https://forum.effectivealtruism.org/tag/donation-pledge) | [effective giving](https://forum.effectivealtruism.org/tag/effective-giving-1) | [Giving What We Can Newsletter](https://forum.effectivealtruism.org/topics/giving-what-we-can-newsletter?sortedBy=new) | [Toby Ord](https://forum.effectivealtruism.org/tag/toby-ord)\n\n1.  ^**[^](#fnrefjhcsnfxxmx)**^\n    \n    Giving What We Can (2022) [Our members](https://www.givingwhatwecan.org/about-us/members/), *Giving What We Can*.\n    \n2.  ^**[^](#fnrefaqwsnsj4f8e)**^\n    \n    Future Fund (2022) [Our grants and investments: Giving What We Can](https://ftxfuturefund.org/all-grants/?_organization_name=giving-what-we-can), *Future Fund*."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "Y4EZpMvesbfYoCMjC",
    "name": "Effective Altruism Funds",
    "core": false,
    "slug": "effective-altruism-funds",
    "oldSlugs": [
      "ea-funds-1",
      "effective-altruism-fund"
    ],
    "postCount": 92,
    "description": {
      "markdown": "**Effective Altruism Funds** (often called **EA Funds**) are expert-managed, cause-specific pools of funds for regranting. This structure allows small and large donors to efficiently delegate their charitable decision-making to central fund managers. Since the EA Funds sometimes support small, speculative, or individual projects that would not normally qualify for tax deductions, this can also be tax-efficient, as donations to EA Funds are deductible in some jurisdictions.\n\nEA Funds is a spin-off of the [Centre for Effective Altruism](https://forum.effectivealtruism.org/tag/centre-for-effective-altruism-cea). Currently, four Funds exist:\n\n1.  [Global Health and Development](https://forum.effectivealtruism.org/tag/global-health-and-development-fund)\n2.  [Animal Welfare](https://forum.effectivealtruism.org/tag/animal-welfare-fund)\n3.  [Long-Term Future](https://forum.effectivealtruism.org/tag/long-term-future-fund)\n4.  [Effective Altruism Infrastructure](https://forum.effectivealtruism.org/tag/effective-altruism-infrastructure-fund)\n\nFurther reading\n---------------\n\nShulman, Carl (2016) [Risk-neutral donors should plan to make bets at the margin at least as well as giga-donors in expectation](https://forum.effectivealtruism.org/posts/BhvTMY7K7z97tbHgS/risk-neutral-donors-should-plan-to-make-bets-at-the-margin), *Effective Altruism Forum*, December 30. \n\nExternal Links\n--------------\n\n[Effective Altruism Funds](https://funds.effectivealtruism.org/). Official website.\n\n[Apply for funding](https://funds.effectivealtruism.org/apply-for-funding).\n\nRelated links\n-------------\n\n[Animal Welfare Fund](https://forum.effectivealtruism.org/tag/animal-welfare-fund) | [Effective Altruism Infrastructure Fund](https://forum.effectivealtruism.org/tag/effective-altruism-infrastructure-fund) | [Founders Pledge](https://forum.effectivealtruism.org/tag/founders-pledge) | [Global Health and Development Fund](https://forum.effectivealtruism.org/tag/global-health-and-development-fund) | [Long-Term Future Fund](https://forum.effectivealtruism.org/tag/long-term-future-fund)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "95urDMsyR9s5Ypou5",
    "name": "Near-term AI ethics",
    "core": false,
    "slug": "near-term-ai-ethics",
    "oldSlugs": [
      "near-term-ai-ethics"
    ],
    "postCount": 26,
    "description": {
      "markdown": "**Near-term AI ethics** is the branch of [AI ethics](https://forum.effectivealtruism.org/tag/ethics-of-artificial-intelligence) that studies the moral questions arising from issues in AI that society is already facing or will likely face very soon. Examples include concerns about data privacy, algorithmic bias, self-driving cars, and [autonomous weapons](https://forum.effectivealtruism.org/topics/autonomous-weapon). **Long-term AI ethics**, by contrast, is the branch of [AI ethics](https://forum.effectivealtruism.org/tag/ethics-of-artificial-intelligence) that studies the moral questions arising from issues expected to arise when AI is much more advanced than it is today. Examples include the implications of [artificial general intelligence](https://forum.effectivealtruism.org/topics/artificial-intelligence) or [transformative artificial intelligence](https://forum.effectivealtruism.org/tag/transformative-artificial-intelligence).^[\\[1\\]](#fnrqqpfnudg)^^[\\[2\\]](#fni6hzj96ow3e)^\n\nFurther reading\n---------------\n\nPrunkl, Carina & Jess Whittlestone (2020) [Beyond near- and long-term: towards a clearer account of research priorities in AI ethics and society](https://doi.org/10.1145/3375627.3375803), *Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society*, pp. 138–143.\n\nRelated entries\n---------------\n\n[AI alignment](https://forum.effectivealtruism.org/tag/ai-alignment) | [AI governance](https://forum.effectivealtruism.org/topics/ai-governance) | [ethics of artificial intelligence](https://forum.effectivealtruism.org/tag/ethics-of-artificial-intelligence)\n\n1.  ^**[^](#fnrefrqqpfnudg)**^\n    \n    Prunkl, Carina & Jess Whittlestone (2020) [Beyond near- and long-term: towards a clearer account of research priorities in AI ethics and society](https://doi.org/10.1145/3375627.3375803), *Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society*, pp. 138–143.\n    \n2.  ^**[^](#fnrefi6hzj96ow3e)**^\n    \n    Brundage, Miles (2017) [Guide to working in AI policy and strategy](https://80000hours.org/articles/ai-policy-guide/), *80,000 Hours*, June 7."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "eSWSZv3oF6sydzD9L",
    "name": "ITN framework",
    "core": false,
    "slug": "itn-framework",
    "oldSlugs": [
      "itn",
      "itn-framework-1"
    ],
    "postCount": 35,
    "description": {
      "markdown": "The **importance, tractability and neglectedness framework**, or **ITN framework** for short, is a framework for estimating the value of allocating [marginal](https://forum.effectivealtruism.org/tag/thinking-at-the-margin) resources to solving a problem based on its [importance](https://forum.effectivealtruism.org/tag/importance), [tractability](https://forum.effectivealtruism.org/tag/tractability), and [neglectedness](https://forum.effectivealtruism.org/tag/neglectedness).\n\nHistory\n-------\n\nThe  ITN framework was first developed by [Holden Karnofsky](https://forum.effectivealtruism.org/tag/holden-karnofsky) around 2013 as part of his work for GiveWell Labs (which later became [Open Philanthropy](https://forum.effectivealtruism.org/tag/open-philanthropy)).^[\\[1\\]](#fnuc4xj13qrf)^\n\n[80,000 Hours](https://forum.effectivealtruism.org/tag/80-000-hours) later presented its own, quantitative version of the framework.^[\\[2\\]](#fngi07mnssz6e)^ On this version, developed by Owen Cotton-Barratt in late 2014,^[\\[3\\]](#fnij97eh8n5wr)^ the three factors are formally defined as follows:\n\n*   importance  = good done / % of a problem solved\n*   tractability = % of a problem solved / % increase in resources\n*   neglectedness = % increase in resources / extra person or dollar\n\nWhen these terms are multiplied, some of the units cancel out, resulting in a quantity denominated in good done per extra person or dollar.\n\nOther differences between Karnofsky's model and Cotton-Barratt's are the terminology (\"importance, tractability and uncrowdedness\" is replaced by \"scale, solvability and neglectedness\") and the use of *problems* rather than *causes* as the main unit of analysis.\n\nMore recently, in an article introducing the [SPC framework](https://forum.effectivealtruism.org/topics/spc-framework), [Will MacAskill](https://forum.effectivealtruism.org/topics/william-macaskill), Teruji Thomas and Aron Vallinder replace neglectedness with *leverage*, a factor that describes how the work already being done on a problem affects the cost-effectiveness of additional work. The resulting framework generalizes to problems with constant or increasing returns to additional work, whereas the ITN framework remains appropriate for problems with diminishing, especially logarithmic, returns.^[\\[4\\]](#fnj7nsmnubyva)^^[\\[5\\]](#fnig270ghkvg)^\n\nFurther reading\n---------------\n\n80,000 Hours (2016) [Our current list of especially pressing world problems](https://80000hours.org/problem-profiles/), *80,000 Hours*, June.  \n*A set of applications of the ITN framework.*\n\nDickens, Michael (2016) [Evaluation frameworks (or: when importance / neglectedness / tractability doesn't apply)](http://mdickens.me/2016/06/10/evaluation_frameworks_(or-_when_scale-neglectedness-tractability_doesn%27t_apply)), *Philosophical Multicore*, June 10.  \n*A criticism of the ITN framework.*\n\nMacAskill, William, Teruji Thomas & Aron Vallinder (2022) [The significance, persistence, contingency framework](https://drive.google.com/file/d/1Lapv64IYsvUnaYWDFmZBoDWe_x5_zkr7/view), *What We Owe the Future: Supplementary Materials*.  \n*Section 4 discusses the ITN framework and how it relates to the SPC framework.*\n\nWiblin, Robert (2016) [One approach to comparing global problems in terms of expected impact](https://80000hours.org/articles/problem-framework/), *80,000 Hours*, April (updated October 2019).  \n*80,000 Hours' presentation of the ITN framework.*\n\nRelated entries\n---------------\n\n[career choice](https://forum.effectivealtruism.org/tag/career-choice) | [cause prioritization](https://forum.effectivealtruism.org/tag/cause-prioritization) | [criticism of effective altruism](https://forum.effectivealtruism.org/tag/criticism-of-effective-altruism) | [distribution of cost-effectiveness](https://forum.effectivealtruism.org/tag/distribution-of-cost-effectiveness) | [impact assessment](https://forum.effectivealtruism.org/tag/impact-assessment) | [SPC framework](https://forum.effectivealtruism.org/topics/spc-framework)\n\n1.  ^**[^](#fnrefuc4xj13qrf)**^\n    \n    Karnofsky's thinking evolved gradually. See Karnofsky, Holden (2013) [Flow-through effects](https://blog.givewell.org/2013/05/15/flow-through-effects/), *The GiveWell Blog*, May 15; Karnofsky, Holden (2013) [Refining the goals of GiveWell Labs](https://blog.givewell.org/2013/05/30/refining-the-goals-of-givewell-labs/), *The GiveWell Blog*, May 30; Muehlhauser, Luke (2013) [Holden Karnofsky on transparent research analyses](https://intelligence.org/2013/08/25/holden-karnofsky-interview/), *Machine Intelligence Research Institute*, August 25; Karnofsky, Holden (2014) [Narrowing down U.S. Policy Areas](https://www.openphilanthropy.org/blog/narrowing-down-us-policy-areas), *Open Philanthropy*, May 22.\n    \n2.  ^**[^](#fnrefgi07mnssz6e)**^\n    \n    Wiblin, Robert (2016) [One approach to comparing global problems in terms of expected impact](https://80000hours.org/articles/problem-framework/), *80,000 Hours*, April (updated October 2019).\n    \n3.  ^**[^](#fnrefij97eh8n5wr)**^\n    \n    Cotton-Barratt, Owen (2014) [Estimating cost-effectiveness for problems of unknown difficulty](https://www.fhi.ox.ac.uk/estimating-cost-effectiveness/), *Future of Humanity Institute*, December 4.\n    \n4.  ^**[^](#fnrefj7nsmnubyva)**^\n    \n    MacAskill, William, Teruji Thomas & Aron Vallinder (2022) [The significance, persistence, contingency framework](https://drive.google.com/file/d/1Lapv64IYsvUnaYWDFmZBoDWe_x5_zkr7/view), *What We Owe the Future: Supplementary Materials*.\n    \n5.  ^**[^](#fnrefig270ghkvg)**^\n    \n    MacAskill, William (2022) 'Appendix 3: The SPC framework', in [*What We Owe the Future*](https://en.wikipedia.org/wiki/Special:BookSources/978-1-5416-1862-6), New York: Basic Books."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "irwpAdkLFunRFSZqR",
    "name": "Meat-eater problem",
    "core": false,
    "slug": "meat-eater-problem",
    "oldSlugs": [
      "meat-eater-problem"
    ],
    "postCount": 12,
    "description": {
      "markdown": "The **meat-eater problem** (sometimes called the **poor meat-eater problem**) is the concern that some interventions aimed at helping humans might increase animal product consumption and as a result increase [farmed animal suffering](https://forum.effectivealtruism.org/tag/farmed-animal-welfare), e.g. by increasing real income or human population.\n\nHistory\n-------\n\nThe meat-eater problem appears to have been first explicitly described in a [Felicifia](https://forum.effectivealtruism.org/topics/felicifia) post from 2009.^[\\[1\\]](#fni12x95af82p)^\n\nFurther details\n---------------\n\nSaving human lives, and making humans more prosperous, seem to be obviously good in terms of direct effects. However, humans consume animal products, and these animal products may cause considerable [animal suffering](https://forum.effectivealtruism.org/tag/farmed-animal-welfare). Therefore, improving human lives may lead to negative effects that outweigh the direct positive effects. This “meat-eater problem” suggests that working on [global poverty](https://forum.effectivealtruism.org/tag/global-poverty) may be less effective than is commonly assumed.\n\nAlthough it is very difficult to quantify these effects, one estimate suggests that each additional $1,000 per year for a relatively poor individual may cause between 1 and 190 days of animal suffering, though the estimate should not be taken literally.^[\\[2\\]](#fneskgkd37yfi)^ Some have argued that the problem is less significant by [claiming that animals have net positive lives](https://forum.effectivealtruism.org/tag/logic-of-the-larder), or by arguing that the effect on consumption is relatively small.^[\\[3\\]](#fnm0ozcpbbf1)^\n\nFurther reading\n---------------\n\nHolness-Tofts, Alex (2020) [Poor meat eater problem](https://forum.effectivealtruism.org/posts/mGLYpBXvN3F2KCAP5/poor-meat-eater-problem), *Effective Altruism Forum*, July 10.\n\nShulman, Carl (2015) [Comment on “What is the expected effect of poverty alleviation efforts on existential risk?”](https://forum.effectivealtruism.org/posts/M9PihwHRtmvzLz78g/what-is-the-expected-effect-of-poverty-alleviation-efforts?commentId=a7DD7f8hSLxYhBAc6), *Effective Altruism Forum*, October 2.\n\nRelated entries\n---------------\n\n[cause prioritization](https://forum.effectivealtruism.org/tag/cause-prioritization) | [cultivated meat](https://forum.effectivealtruism.org/topics/cultivated-meat) | [dietary change](https://forum.effectivealtruism.org/topics/dietary-change) | [economic growth](https://forum.effectivealtruism.org/tag/economic-growth) | [farmed animal welfare](https://forum.effectivealtruism.org/tag/farmed-animal-welfare)\n\n1.  ^**[^](#fnrefi12x95af82p)**^\n    \n    Östman, Jesper (2009) [The poor meat eater problem](https://felicifia.github.io/thread/214.html), *Felicifia*, October 26.\n    \n2.  ^**[^](#fnrefeskgkd37yfi)**^\n    \n    Bogosian, Kyle (2015) [Quantifying the impact of economic growth on meat consumption](https://forum.effectivealtruism.org/posts/YH4zm6JDLELnPyLP9/quantifying-the-impact-of-economic-growth-on-meat), *Effective Altruism Forum*, December 22.\n    \n3.  ^**[^](#fnrefm0ozcpbbf1)**^\n    \n    Weathers, Scott (2016) [The meat eater problem: developing an EA response](http://web.archive.org/web/20200220223514/https://forum.effectivealtruism.org/posts/gA57ThbaS3znib242/the-meat-eater-problem-developing-an-ea-response), *Effective Altruism Forum*, February 29."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "FTgGZwcdGXrPjRcpi",
    "name": "Requests (closed)",
    "core": false,
    "slug": "requests-closed",
    "oldSlugs": [
      "requests-closed"
    ],
    "postCount": 101,
    "description": {
      "markdown": "Use **requests (closed)** for posts about requests that have been completed or are otherwise no longer active. This is also to be used for the [**take action**](https://forum.effectivealtruism.org/tag/take-action) and [**funding request**](https://forum.effectivealtruism.org/tag/funding-request) tags.\n\nRelated entries\n---------------\n\n[requests (open)](https://forum.effectivealtruism.org/tag/requests-open)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "dEonhiqoaQRnwnoAt",
    "name": "Requests (open)",
    "core": false,
    "slug": "requests-open",
    "oldSlugs": [
      "requests-open"
    ],
    "postCount": 102,
    "description": {
      "markdown": "Use **requests (open)** for posts that ask for a *specific* and *well-defined* request from the broad EA community.  The request can be very important for your effort to do more good, or it can be something trivial that you are just curious about or if you think that others might find it important - make sure to say how important this is for you in your post. Also, it would help if you could supply time limits, extra information needed, and be responsive to questions in the comments.\n\nFor posts that are a broader call to action, use the [**take action**](https://forum.effectivealtruism.org/topics/take-action) tag.\n\nFor posts that only ask for responses to a question (and not e.g. detailed feedback, funding, etc.), we don't recommend using this tag. Instead, you can create a \"New Question\" (in the same menu as \"New Post\") and it will be listed on the \"[**All Questions**](https://forum.effectivealtruism.org/questions)\" page.\n\nAfter a request has been completed, or if it's no longer relevant, downvote this tag and add [**requests (closed)**](https://forum.effectivealtruism.org/tag/requests-closed). This can be done by anyone, not just the original poster."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "hiTSCyrbHi2Dfctwx",
    "name": "Research training programs",
    "core": false,
    "slug": "research-training-programs",
    "oldSlugs": [
      "research-training-programs"
    ],
    "postCount": 60,
    "description": {
      "markdown": "**Research training programs** are programs intended to help participants test their fit for and build their skills in research or in a particular area of research. These programs may often be called internships or summer research fellowships.\n\nFurther reading\n---------------\n\nAird, Michael (2021) [List of EA-aligned research training programs](https://docs.google.com/spreadsheets/d/1G4eYl0V9FPF4DkcDGUDLXt4_a6AEWZBqElTMYpF7E1I/edit#).\n\nAird, Michael (2021) [Quick notes on what I mean by \"research training programs\"](https://docs.google.com/document/d/10AWy7V5DYpCJEk7OazfT8UkOUkSR41KUT_6mIRmwFeY/edit#).\n\nRelated entries\n---------------\n\n[academia](https://forum.effectivealtruism.org/tag/academia-1) | [career choice](https://forum.effectivealtruism.org/tag/career-choice) | [fellowships & internships](https://forum.effectivealtruism.org/tag/fellowships-and-internships) | [field building](https://forum.effectivealtruism.org/tag/field-building) | [research methods](https://forum.effectivealtruism.org/tag/research-methods) | [scalably using labour](https://forum.effectivealtruism.org/tag/scalably-using-labour) | [working at EA vs non-EA orgs](https://forum.effectivealtruism.org/tag/working-at-ea-vs-non-ea-orgs)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "C4WsLjWE4epWfu493",
    "name": "Discussion norms",
    "core": false,
    "slug": "discussion-norms",
    "oldSlugs": [
      "discussion-norms",
      "discussion-norms",
      "discussion-norms"
    ],
    "postCount": 94,
    "description": {
      "markdown": "The **discussion norms** tag covers posts that suggest or promote norms around how people ought to hold discussions within the [effective altruism community](https://forum.effectivealtruism.org/topics/community).\n\nFurther reading\n---------------\n\nVaintrob, Lizka (2022) [Guide to norms on the Forum](https://forum.effectivealtruism.org/posts/yND9aGJgobm5dEXqF/guide-to-norms-on-the-forum), *Effective Altruism Forum*, April 28.\n\nRelated entries\n---------------\n\n[competitive debating](https://forum.effectivealtruism.org/tag/competitive-debating) | [epistemic deference](https://forum.effectivealtruism.org/tag/epistemic-deference) | [independent impressions](https://forum.effectivealtruism.org/tag/independent-impressions)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "kBxmJcNsf3p6YLPd4",
    "name": "Global Priorities Institute",
    "core": false,
    "slug": "global-priorities-institute",
    "oldSlugs": null,
    "postCount": 75,
    "description": {
      "markdown": "The **Global Priorities Institute** (**GPI**) is an interdisciplinary research centre at the University of Oxford whose aim is to conduct foundational research that informs the decision-making of individuals and institutions seeking to do as much good as possible. It is directed by [Hilary Greaves](https://forum.effectivealtruism.org/tag/hilary-greaves).\n\nGPI was established in January 2018 with a seed grant of £2 million from [Open Philanthropy](https://forum.effectivealtruism.org/tag/open-philanthropy) and a commitment of another £2 million from two private philanthropists.^[\\[1\\]](#fn7f6ofoih4y)^ Currently, GPI's research focuses on two main areas: [longtermism](https://forum.effectivealtruism.org/tag/longtermism) and general issues in [cause prioritization](https://forum.effectivealtruism.org/tag/cause-prioritization).\n\nFurther reading\n---------------\n\nGreaves, Hilary *et al.* (2019) [A research agenda for the Global Priorities Institute](https://globalprioritiesinstitute.org/wp-content/uploads/2017/12/gpi-research-agenda.pdf), *Global Priorities Institute*, February.\n\nMacAskill, William, Rossa O’Keeffe-O’Dononvan & Andreas Mogensen (2018) [Global Priorities Institute: our 2018 goals and research](https://www.youtube.com/watch?v=qzD9TGM0M2M), *Effective Altruism Global*, October 27.\n\nWiblin, Robert & Kieran Harris (2018) [Philosophy Prof Hilary Greaves on moral cluelessness, population ethics, probability within a multiverse, & harnessing the brainpower of academia to tackle the most important research questions](https://80000hours.org/podcast/episodes/hilary-greaves-global-priorities-institute/), *80,000 Hours*, October 23.\n\nExternal links\n--------------\n\n[Global Priorities Institute](https://globalprioritiesinstitute.org/). Official website.\n\n[Global Priorities Institute](https://forum.effectivealtruism.org/users/global-priorities-institute). [Effective Altruism Forum](https://forum.effectivealtruism.org/tag/effective-altruism-forum-1) account.\n\n[Apply for a job](https://globalprioritiesinstitute.org/opportunities/).\n\n[Donate to the Global Priorities Institute](https://globalprioritiesinstitute.org/supporting-gpi/).\n\nRelated entries\n---------------\n\n[global priorities research](https://forum.effectivealtruism.org/tag/global-priorities-research) | [longtermism](https://forum.effectivealtruism.org/tag/longtermism) | [population ethics](https://forum.effectivealtruism.org/tag/population-ethics)\n\n1.  ^**[^](#fnref7f6ofoih4y)**^\n    \n    University of Oxford (2018) [Global Priorities Institute opens at Oxford](https://www.development.ox.ac.uk/news/new-global-priorities-institute-opens), *Development Office*, April 5."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "ZsP4djM7La3kWqMAj",
    "name": "Future of Humanity Institute",
    "core": false,
    "slug": "future-of-humanity-institute",
    "oldSlugs": null,
    "postCount": 25,
    "description": {
      "markdown": "The **Future of Humanity Institute** (**FHI**) is a multidisciplinary research institute at the University of Oxford studying big picture questions for human civilization. It was founded in 2005 by [Nick Bostrom](https://forum.effectivealtruism.org/tag/nick-bostrom), who is also its current Director.\n\nFunding\n-------\n\nAs of July 2022, FHI has received over $20 million in grants from [Open Philanthropy](https://forum.effectivealtruism.org/topics/open-philanthropy),^[\\[1\\]](#fn8w1z26v038t)^ and nearly $250,000 the [Survival and Flourishing](https://forum.effectivealtruism.org/topics/survival-and-flourishing) Fund.^[\\[2\\]](#fnb1x6y65zvu7)^\n\nExternal links\n--------------\n\n[Future of Humanity Institute](https://www.fhi.ox.ac.uk). Official website.\n\n[Donate to the Future of Humanity Institute](https://www.fhi.ox.ac.uk/support-fhi/).\n\nRelated entries\n---------------\n\n[AI governance](https://forum.effectivealtruism.org/topics/ai-governance) | [AI safety](https://forum.effectivealtruism.org/tag/ai-safety) | [atomically precise manufacturing](https://forum.effectivealtruism.org/tag/atomically-precise-manufacturing) | [biosecurity](https://forum.effectivealtruism.org/tag/biosecurity) | [digital person](https://forum.effectivealtruism.org/tag/digital-person) | [macrostrategy](https://forum.effectivealtruism.org/tag/macrostrategy) | [space governance](https://forum.effectivealtruism.org/tag/space-governance)\n\n1.  ^**[^](#fnref8w1z26v038t)**^\n    \n    Open Philanthropy (2022) [Grants database: Future of Humanity Institute](https://www.openphilanthropy.org/grants/?q=&organization-name=Future-of-Humanity-Institute), *Open Philanthropy*.\n    \n2.  ^**[^](#fnrefb1x6y65zvu7)**^\n    \n    Survival and Flourishing Fund (2019) [SFF-2020-H2 S-process recommendations announcement](https://survivalandflourishing.fund/sff-2020-h2-recommendations), *Survival and Flourishing Fund*."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "gCzFfErskfQzD6b95",
    "name": "Rethink Priorities",
    "core": false,
    "slug": "rethink-priorities",
    "oldSlugs": null,
    "postCount": 171,
    "description": {
      "markdown": "**Rethink Priorities** is a think tank dedicated to informing decisions made by high-impact organizations and funders across various cause areas, including  [longtermism](https://forum.effectivealtruism.org/tag/longtermism), [building effective altruism](https://forum.effectivealtruism.org/tag/building-effective-altruism-1)[, animal welfare](https://forum.effectivealtruism.org/tag/animal-welfare-1),  and [global health and development](https://forum.effectivealtruism.org/tag/global-health-and-development).\n\nFunding\n-------\n\nAs of July 2022, Rethink Priorities has received nearly $5.2 million  in funding from [Open Philanthropy](https://forum.effectivealtruism.org/topics/open-philanthropy),^[\\[1\\]](#fnu5pohuwn1ab)^ over $1.3 million from [Effective Altruism Funds](https://forum.effectivealtruism.org/topics/effective-altruism-funds),^[\\[2\\]](#fn82r728ngxy7)^ $700,000 from the [Future Fund](https://forum.effectivealtruism.org/topics/future-fund),^[\\[3\\]](#fnjjr4vy21wil)^ and over $200,000 from the [Survival and Flourishing](https://forum.effectivealtruism.org/topics/survival-and-flourishing) Fund.^[\\[4\\]](#fnvplrf7snt7)^^[\\[5\\]](#fnux0sj69ywk)^\n\nFurther reading\n---------------\n\nWildeford, Peter (2021) [Rethink Priorities 2021 Impact and 2022 Strategy](https://forum.effectivealtruism.org/posts/K7tjvcDurrCj72D7H/rethink-priorities-2021-impact-and-2022-strategy), *Effective Altruism Forum*, November 15.\n\nWildeford, Peter (2021) [We’re Rethink Priorities. Ask Us Anything](https://forum.effectivealtruism.org/posts/D499oMCiFiqHT92TT/we-re-rethink-priorities-ask-us-anything), *Effective Altruism Forum*, November 15.\n\nExternal links\n--------------\n\n[Rethink Priorities](https://www.rethinkpriorities.org/). Official website.\n\n[Apply for a job](https://careers.rethinkpriorities.org/).\n\n[Donate to Rethink Priorities](https://rethinkpriorities.org/donate).\n\n1.  ^**[^](#fnrefu5pohuwn1ab)**^\n    \n    Open Philanthropy (2022) [Grants database: Rethink Priorities](https://www.openphilanthropy.org/grants/?q=&organization-name=rethink-priorities), *Open Philanthropy.*\n    \n2.  ^**[^](#fnref82r728ngxy7)**^\n    \n    Animal Welfare Fund (2018) [June 2018: Animal Welfare Fund grants](https://funds.effectivealtruism.org/funds/payouts/june-2018-animal-welfare-fund-grants), *Effective Altruism Funds*, June.\n    \n    Animal Welfare Fund (2018) [December 2018: Animal Welfare Fund grants](https://funds.effectivealtruism.org/funds/payouts/december-2018-animal-welfare-fund-grants), *Effective Altruism Funds*, December.\n    \n    Animal Welfare Fund (2019) [March 2019: Animal Welfare Fund grants](https://funds.effectivealtruism.org/funds/payouts/march-2019-animal-welfare-fund-grants), *Effective Altruism Funds*, March.\n    \n    Animal Welfare Fund (2019) [July 2019: Animal Welfare Fund grants](https://funds.effectivealtruism.org/funds/payouts/july-2019-animal-welfare-fund-grants), *Effective Altruism Funds*, July.\n    \n    Animal Welfare Fund (2019) [November 2019: Animal Welfare Fund grants](https://funds.effectivealtruism.org/funds/payouts/november-2019-animal-welfare-fund-grants), *Effective Altruism Funds*, November.\n    \n    Animal Welfare Fund (2020) [July 2020: Animal Welfare Fund grants](https://funds.effectivealtruism.org/funds/payouts/july-2020-animal-welfare-fund-grants), *Effective Altruism Funds*, July.\n    \n    Animal Welfare Fund (2021) [May 2021: Animal Welfare Fund grants](https://funds.effectivealtruism.org/funds/payouts/may-2021-animal-welfare-fund-grants), *Effective Altruism Funds*, May.\n    \n    Effective Altruism Infrastructure Fund (2019) [March 2019: EA Meta Fund grants](https://funds.effectivealtruism.org/funds/payouts/march-2019-ea-meta-fund-grants), *Effective Altruism Funds*, March.\n    \n    Effective Altruism Infrastructure Fund (2019) [July 2019: EA Meta Fund grants](https://funds.effectivealtruism.org/funds/payouts/july-2019-ea-meta-fund-grants), *Effective Altruism Funds*, July.\n    \n    Effective Altruism Infrastructure Fund (2020) [March 2020: EA Meta Fund Grants](https://funds.effectivealtruism.org/funds/payouts/march-2020-ea-meta-fund-grants), *Effective Altruism Funds*, March.\n    \n    Effective Altruism Infrastructure Fund (2021) [May 2021: EA Infrastructure Fund grants](https://funds.effectivealtruism.org/funds/payouts/may-2021-ea-infrastructure-fund-grants), *Effective Altruism Funds*, May.\n    \n    Effective Altruism Infrastructure Fund (2021) [May-August 2021: EA Infrastructure Fund grants](https://funds.effectivealtruism.org/funds/payouts/may-august-2021-ea-infrastructure-fund-grants), *Effective Altruism Funds*, August.\n    \n    Long-Term Future Fund (2020) [April 2020: Long-Term Future Fund grants and recommendations](https://funds.effectivealtruism.org/funds/payouts/april-2020-long-term-future-fund-grants-and-recommendations), *Effective Altruism Funds*, April.\n    \n    Long-Term Future Fund (2021) [May 2021: Long-Term Future Fund grants](https://funds.effectivealtruism.org/funds/payouts/may-2021-long-term-future-fund-grants), *Effective Altruism Funds*, May.\n    \n3.  ^**[^](#fnrefjjr4vy21wil)**^\n    \n    Future Fund (2022) [Our grants and investments: Rethink Priorities](https://ftxfuturefund.org/our-grants/?_organization_name=rethink-priorities), *Future Fund*.\n    \n4.  ^**[^](#fnrefvplrf7snt7)**^\n    \n    Survival and Flourishing Fund (2019) [SFF-2020-H2 S-process recommendations announcement](https://survivalandflourishing.fund/sff-2020-h2-recommendations), *Survival and Flourishing Fund*. \n    \n5.  ^**[^](#fnrefux0sj69ywk)**^\n    \n    Survival and Flourishing Fund (2021) [SFF-2022-H1 S-Process recommendations announcement](https://survivalandflourishing.fund/sff-2022-h1-recommendations), *Survival and Flourishing Fund*."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "WtkxKaLPvMDfcw7ss",
    "name": "Community infrastructure",
    "core": false,
    "slug": "community-infrastructure",
    "oldSlugs": [
      "community-tools",
      "community-infrastructure"
    ],
    "postCount": 136,
    "description": {
      "markdown": "**Community infrastructure** refers to potential and current community-wide products, tools, services or mechanisms that enable intra-community coordination.\n\nThis can include:\n\n*   building tools or websites such as [wikis](https://forum.effectivealtruism.org/tag/wikis), forums, tools, and platforms\n*   offering cross-cutting services like shared services to EA organizations\n*   offline events such as [conferences](https://forum.effectivealtruism.org/tag/conferences), community houses, and regional networks\n*   coordination tools such as [donor lotteries](https://forum.effectivealtruism.org/tag/donor-lotteries)\n\nSome community infrastructure may be limited to certain subgroups, such as events and services for leaders but still provide benefits to the wider community.\n\nExamples include: [EA Forum](https://forum.effectivealtruism.org/tag/effective-altruism-forum-1), [EA Funds](https://forum.effectivealtruism.org/tag/effective-altruism-funds), [EA Hub](https://forum.effectivealtruism.org/tag/effective-altruism-hub), [EA Global conferences](https://forum.effectivealtruism.org/tag/effective-altruism-global), Project incubation (e.g. [Charity Entrepreneurship](https://forum.effectivealtruism.org/tag/charity-entrepreneurship)) and the international EA event calendar.\n\nThis is intended to be for infrastructure that serve the specific needs of the EA movement.\n\nFurther reading\n---------------\n\ncafelow (2020) [Connecting with online EA events](https://forum.effectivealtruism.org/posts/KcMnHuXg9Yhwp37kW/connecting-with-online-ea-events), *Effective Altruism Forum*, April 16.\n\nExternal links\n--------------\n\n[EA Hub/Resources](https://resources.eahub.org/). A collection of resources about effective altruism.\n\n[International Effective Altruism Events Calendar](https://calendar.google.com/calendar/u/0/embed?src=ie5uop71imftf4ut2htbv789v8@group.calendar.google.com).\n\nRelated entries\n---------------\n\n[altruistic coordination](https://forum.effectivealtruism.org/tag/altruistic-coordination) | [markets for altruism](https://forum.effectivealtruism.org/tag/markets-for-altruism) | [scalably using labour](https://forum.effectivealtruism.org/tag/scalably-using-labour)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "d4bQXgZhDP43eJMwp",
    "name": "Longtermist institutional reform",
    "core": false,
    "slug": "longtermist-institutional-reform",
    "oldSlugs": [
      "institutions-for-future-generations",
      "institutions-for-future-generations"
    ],
    "postCount": 28,
    "description": {
      "markdown": "**Longtermist institutional reform** is research on how institutions can better represent the interests of future generations in the political process.\n\nFurther reading\n---------------\n\nBaumann, Tobias (2020) [Representing future generations in the political process](https://centerforreducingsuffering.org/representing-future-generations-in-the-political-process/?utm_source=rss&utm_medium=rss&utm_campaign=representing-future-generations-in-the-political-process), *Center for Reducing Suffering*, June 25.\n\nGonzález-Ricoy, Iñigo & Axel Gosseries (eds.) (2016) [Designing institutions for future generations](http://doi.org/10.1093/acprof:oso/9780198746959.003.0001), in *Institutions for Future Generations*, Oxford: Oxford University Press, pp. 3–23.\n\nGoth, Aidan & Matt Lerner (2022) [Longtermist institutional reform](https://www.founderspledge.com/stories/longtermist-institutional-reform), *Founders Pledge*, January 12.\n\nJacobs, Alan M. (2011) [*Governing for the Long Term: Democracy and the Politics of Investment*](https://en.wikipedia.org/wiki/Special:BookSources/9780521171779), Cambridge: Cambridge University Press.\n\nJacobs, Alan M. (2016) [Policy making for the long term in advanced democracies](http://doi.org/10.1146/annurev-polisci-110813-034103), *Annual Review of Political Science*, vol. 19, pp. 433–454.\n\nJohn, Tyler (2019) [Institutions for future generations](https://forum.effectivealtruism.org/posts/op93xvHkJ5KvCrKaj/institutions-for-future-generations), *Effective Altruism Forum*, November 11.\n\nJohn, Tyler (2021) [Empowering future people by empowering the young?](https://philpapers.org/rec/JOHEFP), in *Ageing without Ageism: Conceptual Puzzles and Policy Proposals*, Oxford: Oxford University Press, forthcoming.\n\nJohn, William & William MacAskill (2021) [Longtermist institutional reform](https://en.wikipedia.org/wiki/Special:BookSources/978-0-9957281-8-9), in Natalie Cargill & Tyler John (eds.) *The Long View: Essays on Policy, Philanthropy, and the Long-Term Future*, London: First, pp. 45–60.\n\nJones, Natalie, Mark O’Brien & Thomas Ryan (2018) [Representation of future generations in United Kingdom policy-making](http://doi.org/10.1016/j.futures.2018.01.007), *Futures*, vol. 102, pp. 153–163.\n\nKrznaric, Roman (2019) [Why we need to reinvent democracy for the long-term](https://www.bbc.com/future/article/20190318-can-we-reinvent-democracy-for-the-long-term), *BBC Future*, March 18.\n\nMacAskill, William (2019) [Age-weighted voting](https://forum.effectivealtruism.org/posts/b7BrGrswgANP3eRzd/age-weighted-voting), *Effective Altruism Forum*, July 12.\n\nMcKinnon, Catriona (2017) [Endangering humanity: an international crime?](http://doi.org/10.1080/00455091.2017.1280381), *Canadian Journal of Philosophy*, vol. 47, pp. 395–415.\n\nMoorhouse, Fin & Luca Righetti (2021) [Institutions for the long run: taking future generations seriously in government](https://a764aa28-8f1b-4abd-ad69-eed71af9e23a.filesusr.com/ugd/b589e0_6cc51397ac4b4d78b2f68d8f489b0847.pdf), *Cambridge Journal of Law, Politics, and Art*, vol. 1, pp. 430–437.\n\nNesbit, Martin & Andrea Illés (2015) [Establishing an EU “Guardian for future generations”. Report and recommendations for the World Future Council](http://www.worldfuturecouncil.org/wp-content/uploads/2016/02/IEEP_WFC_2016_Establishing_an_EU_Guardian_for_Future_Generations.pdf), *Institute for European Environmental Policy*, London.\n\nRelated entries\n---------------\n\n[All-Party Parliamentary Group for Future Generations](https://forum.effectivealtruism.org/tag/all-party-parliamentary-group-for-future-generations) | [electoral reform](https://forum.effectivealtruism.org/tag/electoral-reform) | [improving institutional decision-making](https://forum.effectivealtruism.org/topics/improving-institutional-decision-making) | [longtermism](https://forum.effectivealtruism.org/tag/longtermism) | [policy change](https://forum.effectivealtruism.org/tag/policy-change) | [Research Institute for Future Design](https://forum.effectivealtruism.org/topics/research-institute-for-future-design)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "3f2BbrdHC4wajeHXT",
    "name": "China",
    "core": false,
    "slug": "china",
    "oldSlugs": null,
    "postCount": 58,
    "description": {
      "markdown": "The **China** tag is for posts that are about China, that address how China is relevant to various issues EAs care about, or that are relevant to how one could have an impact by engaging with China.\n\nFurther reading\n---------------\n\nWiblin, Robert & Keiran Harris (2020) [China, its AI dream, and what we get wrong about both](https://80000hours.org/podcast/episodes/jeffrey-ding-china-ai-dream/), *80,000 Hours*, February 6.  \n*An extended interview with Jeff Ding about China's AI strategy*.\n\nRelated entries\n---------------\n\n[global outreach](https://forum.effectivealtruism.org/tag/global-outreach) | [international relations](https://forum.effectivealtruism.org/tag/international-relations)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "QEA4yhsS79KZ2Jmdd",
    "name": "Donor lotteries",
    "core": false,
    "slug": "donor-lotteries",
    "oldSlugs": [
      "donor-lotteries",
      "donor-lotteries",
      "donor-lotteries"
    ],
    "postCount": 24,
    "description": {
      "markdown": "A **donor lottery** is an arrangement where multiple prospective donors each contribute to a common pot in exchange for a chance, proportional to the size of the contribution, to win the right to decide how the pot is spent.\n\nHistory\n-------\n\nThe concept of a donor lottery was first explicitly described by [Carl Shulman](https://forum.effectivealtruism.org/tag/carl-shulman) in 2016,^[\\[1\\]](#fncuqxsagmdgm)^^[\\[2\\]](#fn1a62ngmi8ug)^ as a development and refinement of some previous ideas by the author.^[\\[3\\]](#fniliyr78l288)^ The first donor lottery was run in early 2017, by Shulman and [Paul Christiano](https://forum.effectivealtruism.org/tag/paul-christiano).^[\\[4\\]](#fnn2u3yuq1sks)^  Many lotteries have taken place since then, and some of the winners have written detailed reports of the process they followed for allocating the funding pool.^[\\[5\\]](#fn5otqku1xxsa)^^[\\[6\\]](#fnsyn8f4k2vgk)^^[\\[7\\]](#fnpe2zqefru5)^^[\\[8\\]](#fnbva6n8mkqpf)^  Currently, donor lotteries are run by [Effective Altruism Funds](https://forum.effectivealtruism.org/tag/effective-altruism-funds) and held at least once per year. The most recent lottery ran in mid-2021, and the chosen donor won the right to allocate $500,000.^[\\[9\\]](#fn8c311qtjgq4)^ The most recent donor lottery was drawn on 24 January 2022.^[\\[10\\]](#fnux08osf8jb)^\n\nJustification\n-------------\n\nAt its core, a donor lottery is a method for exploiting situations where there are increasing marginal returns to donation: the lottery converts a small budget into a small chance of winning a budget large enough to take advantage of economies of scale.\n\nFor illustration, consider an effective altruist who, instead of donating $1,000, buys a 1% chance of directing $100,000 to their chosen charity. Entering this lottery provides two distinct benefits. First, it cuts down research costs per expected dollar donated by a factor of 100, since researching donation alternatives becomes necessary only in the winning scenario. Second, it allows the altruist to fund projects whose minimum funding size exceeds their charitable budget.\n\nIn principle, someone persuaded that they could do more good in expectation with a small chance of a correspondingly larger donation could realize these gains by high-stakes gambling or high-risk investment. In practice, the associated overhead and transaction costs typically make donor lotteries a more attractive alternative.^[\\[1\\]](#fncuqxsagmdgm)^\n\nFurther reading\n---------------\n\nHoffman, Ben (2016) [Claim explainer: donor lotteries and returns to scale](https://arbital.com/p/75v/), *Arbital*, December 30.\n\nExternal links\n--------------\n\n[Effective Altruism Funds Donor Lottery](https://funds.effectivealtruism.org/donor-lottery).\n\nRelated entries\n---------------\n\n[donation choice](https://forum.effectivealtruism.org/tag/donation-choice) | [donation writeup](https://forum.effectivealtruism.org/tag/donation-writeup) | [effective altruism funding](https://forum.effectivealtruism.org/tag/effective-altruism-funding) | [effective giving](https://forum.effectivealtruism.org/tag/effective-giving-1) | [philanthropic coordination](https://forum.effectivealtruism.org/tag/philanthropic-coordination)\n\n1.  ^**[^](#fnrefcuqxsagmdgm)**^\n    \n    Shulman, Carl (2016) [Creating a donor-advised fund lottery](http://reflectivedisequilibrium.blogspot.com/2016/03/creating-donor-advised-fund-lottery.html), *Reflective Disequilibrium*, March 27.\n    \n2.  ^**[^](#fnref1a62ngmi8ug)**^\n    \n    Shulman, Carl (2016) [Donor lotteries: demonstration and FAQ](https://forum.effectivealtruism.org/posts/WvPEitTCM8ueYPeeH/donor-lotteries-demonstration-and-faq), *Effective Altruism Forum*, December 7.\n    \n3.  ^**[^](#fnrefiliyr78l288)**^\n    \n    Shulman, Carl (2014) [If big donors have much better opportunities than small donors, then small donors can go to Las Vegas, or Wall Street](http://reflectivedisequilibrium.blogspot.com/2014/01/if-big-donors-have-much-better.html), *Reflective Disequilibrium*, January 4.\n    \n4.  ^**[^](#fnrefn2u3yuq1sks)**^\n    \n    Christiano, Paul (2017) [Donor lottery details](https://forum.effectivealtruism.org/posts/jvsooiwXWh3iWnfgp/donor-lottery-details), *Effective Altruism Forum*, January 10.\n    \n5.  ^**[^](#fnref5otqku1xxsa)**^\n    \n    Gleave, Adam (2018) [2017 donor lottery report](https://forum.effectivealtruism.org/posts/SYeJnv9vYzq9oQMbQ/2017-donor-lottery-report), *Effective Altruism Forum*, November 12.\n    \n6.  ^**[^](#fnrefsyn8f4k2vgk)**^\n    \n    Rheingans-Yoo, Ross (2020) [2018-19 Donor Lottery Report, pt. 1](https://forum.effectivealtruism.org/posts/Zk9gz7yAdFiT2Mn9b/2018-19-donor-lottery-report-pt-1), *Effective Altruism Forum*, December 13.\n    \n7.  ^**[^](#fnrefpe2zqefru5)**^\n    \n    Rheingans-Yoo, Ross (2020) [2018-19 Donor Lottery Report, pt. 2](https://forum.effectivealtruism.org/posts/Y4s4MuYQpjfBBHdoR/2018-19-donor-lottery-report-pt-2), *Effective Altruism Forum*, December 14.\n    \n8.  ^**[^](#fnrefbva6n8mkqpf)**^\n    \n    Telleen-Lawton, Timothy (2020) [Donor lottery debrief](https://forum.effectivealtruism.org/posts/QjFJKMSTbTpvsoMvh/donor-lottery-debrief), *Effective Altruism Forum*, August 4.\n    \n9.  ^**[^](#fnref8c311qtjgq4)**^\n    \n    Brinich-Langlois, Patrick (2021) [What would you do if you had half a million dollars?](https://forum.effectivealtruism.org/posts/aui3fCcjXXpgXWjqr/what-would-you-do-if-you-had-half-a-million-dollars-2), *Effective Altruism Forum*, July 14.\n    \n10.  ^**[^](#fnrefux08osf8jb)**^\n    \n    Deere, Sam (2021) [The 2021 EA Funds Donor Lottery is now open](https://forum.effectivealtruism.org/posts/Ci56F69xZDvy9LokL/the-2021-ea-funds-donor-lottery-is-now-open), *Effective Altruism Forum*, November 29."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "X2fKBo8s2gNo93f6n",
    "name": "Funding request (open)",
    "core": false,
    "slug": "funding-request-open",
    "oldSlugs": [
      "funding-request",
      "funding-request"
    ],
    "postCount": 42,
    "description": {
      "markdown": "The **funding request** tag covers posts whose authors are looking for a grant or donation, whether to themselves or to their organization. The tag's purpose is to group posts of this type for would-be donors. \n\nAfter a request has been completed, or if it's no longer relevant, downvote this tag and add **requests (closed)**. This can be done by anyone, not just the original poster."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "aoPgP9xHSGAczRQNt",
    "name": "Markets for altruism",
    "core": false,
    "slug": "markets-for-altruism",
    "oldSlugs": [
      "markets-for-altruism"
    ],
    "postCount": 29,
    "description": {
      "markdown": "**Markets for altruism** are actual or potential market-like mechanisms for allocating resources in altruistic contexts. An example would be [certificates of impact](https://forum.effectivealtruism.org/tag/certificates-of-impact).\n\nFurther reading\n---------------\n\nAird, Michael (2020) [Collection of sources relevant to impact certificates/impact purchases/similar](https://forum.effectivealtruism.org/posts/EMKf4Gyee7BsY2RP8/michaela-s-shortform?commentId=n6DCcCL8LFLGKD9SE), *Effective Altruism Forum*, November 15.  \n*Many additional resources on this topic or related topics.*\n\nRelated entries\n---------------\n\n[certificates of impact](https://forum.effectivealtruism.org/tag/certificates-of-impact) | [community infrastructure](https://forum.effectivealtruism.org/tag/community-infrastructure) | [effective altruism funding](https://forum.effectivealtruism.org/tag/effective-altruism-funding) | [mechanism design](https://forum.effectivealtruism.org/tag/mechanism-design) | [prize](https://forum.effectivealtruism.org/tag/prize) | [scalably using labour](https://forum.effectivealtruism.org/tag/scalably-using-labour) | [socially responsible investing](https://forum.effectivealtruism.org/tag/socially-responsible-investing)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "oeJfrABw8ZjvB2DkB",
    "name": "Motivational",
    "core": false,
    "slug": "motivational",
    "oldSlugs": [
      "motivation",
      "motivational",
      "motivate-your-self",
      "motivational",
      "motivational",
      "motivation-mindset",
      "motivation",
      "motivational",
      "motivational-video",
      "motivational",
      "https-shrinke-me-16dn8izk"
    ],
    "postCount": 107,
    "description": {
      "markdown": "The **Motivational** tag covers posts that focus on inspiring us to do good (either a specific kind of good, or whatever we already want to work on).\n\nWe hope that reading through this collection will lift your spirits and spur you to beneficial action!\n\nIf a post motivates you to do good, or you think it will motivate others, upvote it. There are no wrong answers."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "DGhyx7Bk33Ghi5koD",
    "name": "Autonomous weapon",
    "core": false,
    "slug": "autonomous-weapon",
    "oldSlugs": [
      "autonomous-weapons",
      "autonomous-weapons",
      "autonomous-weapons",
      "autonomous-weapons"
    ],
    "postCount": 8,
    "description": {
      "markdown": "An **autonomous weapon** is a weapon system that, once activated, can select and engage targets without further intervention by a human operator.\n\nFurther reading\n---------------\n\nPiper, Kelsey (2019) [Death by algorithm: the age of killer robots is closer than you think](https://www.vox.com/2019/6/21/18691459/killer-robots-lethal-autonomous-weapons-ai-war), *Vox*, June 21.\n\nExternal links\n--------------\n\n[Lethal AWS](https://autonomousweapons.org/). A website containing several resources with a view to raising awareness about the possible dangers of autonomous weapons.\n\nRelated entries\n---------------\n\n[AI governance](https://forum.effectivealtruism.org/topics/ai-governance) | [AI risk](https://forum.effectivealtruism.org/tag/ai-risk) | [armed conflict](https://forum.effectivealtruism.org/tag/armed-conflict) | [near-term AI ethics](https://forum.effectivealtruism.org/tag/near-term-ai-ethics) | [terrorism](https://forum.effectivealtruism.org/tag/terrorism)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "W47HrrhkZsDZo9yGd",
    "name": "Metascience",
    "core": false,
    "slug": "metascience",
    "oldSlugs": [
      "meta-science",
      "meta-science"
    ],
    "postCount": 69,
    "description": {
      "markdown": "**Metascience** (also known as **meta-research**) is the use of scientific methodology to study science itself.\n\nTopics studied by metascience include\n\n1.  Inefficiencies and misaligned incentives in the academic ecosystem.\n2.  Estimations of the value of scientific advances and subsequent prioritization.\n3.  Efforts to improve academia or science.\n\nFor posts related to improving how one conducts better scientific research, the tag [research methods](https://forum.effectivealtruism.org/tag/research-methods) is more appropriate.\n\nFor posts about the general concept of progress in science, use the tag [scientific progress](https://forum.effectivealtruism.org/tag/scientific-progress).\n\nRelated entries\n---------------\n\n[academia](https://forum.effectivealtruism.org/tag/academia-1/) | [field-building](https://forum.effectivealtruism.org/tag/field-building) | [research methods](https://forum.effectivealtruism.org/tag/research-methods) | [research training programs](https://forum.effectivealtruism.org/tag/research-training-programs) | [scientific progress](https://forum.effectivealtruism.org/tag/scientific-progress)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "uuENf8tJ33Qf3Keq2",
    "name": "Social and intellectual movements",
    "core": false,
    "slug": "social-and-intellectual-movements",
    "oldSlugs": [
      "other-movements",
      "other-movements"
    ],
    "postCount": 40,
    "description": {
      "markdown": "The **social and intellectual movements** tag applies to posts that discuss how other social and intellectual movements relate to effective altruism. For example, they might discuss:\n\n*   Building alliances with them\n*   What we can learn from them\n*   Ways in which we should oppose them\n\nFurther reading\n---------------\n\nAird, Michael (2020) [Collection of EA analyses of how social social movements rise, fall, can be influential, etc.](https://forum.effectivealtruism.org/posts/EMKf4Gyee7BsY2RP8/michaela-s-shortform?commentId=ta7dfYdgqFza4bnKp), *Effective Altruism Forum*, March 26.  \n*Many additional resources on this topic or related topics.*\n\nDella Porta, Donatella & Mario Diani (eds.) (2015) [*The Oxford Handbook of Social Movements*](http://doi.org/10.1093/oxfordhb/9780199678402.001.0001), Oxford: Oxford University Press.\n\nKlandermans, Bert & Conny Roggeband (eds.) (2010) [*Handbook of Social Movements across Disciplines*](http://doi.org/10.1007/978-0-387-70960-4), New York: Springer Science+Business Media.\n\nRelated entries\n---------------\n\n[building effective altruism](https://forum.effectivealtruism.org/tag/building-effective-altruism-1) | [communities adjacent to effective altruism](https://forum.effectivealtruism.org/tag/communities-adjacent-to-effective-altruism) | [criticism of effective altruism](https://forum.effectivealtruism.org/tag/criticism-ea-movement) | [fabianism](https://forum.effectivealtruism.org/tag/fabianism) | [history of effective altruism](https://forum.effectivealtruism.org/tag/history-of-effective-altruism) | [movement collapse](https://forum.effectivealtruism.org/tag/movement-collapse) | [nuclear disarmament movement](https://forum.effectivealtruism.org/tag/nuclear-disarmament-movement) | [religion](https://forum.effectivealtruism.org/tag/religion)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "TtceHin3wTddi4oJ9",
    "name": "Land use reform",
    "core": false,
    "slug": "land-use-reform",
    "oldSlugs": [
      "land-use-reform"
    ],
    "postCount": 8,
    "description": {
      "markdown": "**Land use reform** is the set of interventions aimed at improving regulation of dense housing construction in urban areas.\n\nThe problem\n-----------\n\nLaws at the local level in the United States and many other countries impose strict limits on how much total floor area can be built on a plot of land. Such zoning laws constitute a major obstacle to the construction of dense housing. The resulting increase in housing prices reduces economic efficiency by creating significant deadweight loss; increases inequality by transferring wealth from renters to landowners; and reduces both wages and total economic output by preventing workers from relocating where they can be most productive.\n\nThe effects of zoning laws on housing prices can be estimated by comparing the sale price of housing to the associated costs of land and construction.^[\\[1\\]](#fn75kcqcvfc8j)^[ Open Philanthropy](https://forum.effectivealtruism.org/tag/open-philanthropy) has combined these estimates with rent data and some additional assumptions to conclude that the aggregate \"tax\" on renters in five large metropolitan areas amounts to over $100 billion in deadweight loss per year.^[\\[2\\]](#fno7obmpnctab)^\n\nA study by economists Chang-Tai Hsieh and Enrico Moretti examines the costs resulting from the reduced flow of workers to more productive regions within the United States due to rising housing prices. The authors conclude that land use restrictions depress annual U.S. wages by $1.27 trillion and output by $1.95 trillion.^[\\[3\\]](#fn4a7w0bv5b8t)^^[\\[4\\]](#fneuzbu3wv0nl)^\n\nIf land use restrictions create these problems, why do they persist? In part, the costs of restricting land use in a given location are incurred by workers who would benefit from moving to that location, and who as such do not yet live there. Since restrictions are created at the local level, they are insensitive to the interests of these workers, who do not vote in those jurisdictions. Other costs of restricting land use—such as reduced economic output—are dispersed across society as a whole. Public choice theory explains why governments neglect these costs and instead focus on the concentrated benefits to landowners—even if, in the aggregate, the costs vastly outweigh the benefits.\n\nPossible solutions\n------------------\n\nOpen Philanthropy and [80,000 Hours](https://forum.effectivealtruism.org/tag/80-000-hours) have proposed a number of solutions to the problems caused by land use restrictions, which are quoted below.\n\n### Policy options\n\nPromising options open to policymakers include the following:^[\\[2\\]](#fno7obmpnctab)^\n\n*   \"Local governments in high-wage high-regulation metropolitan areas could simply 'upzone', permitting more and denser development.\"\n*   \"Local governments could change the process by which they decide how to regulate land use. For example, they could adopt a 'zoning budget' targeting an overall level of housing growth, so that restrictions in one area would have to be balanced by expansions elsewhere. This would help align incentives of advocates for individual projects to support greater overall growth.\"\n*   \"Decision-making in land use policy could be re-assigned from local to regional or state authorities, which would likely be less susceptible to neighborhood pressure to oppose new development.\"\n\n### Funding options\n\nPromising options open to funders include the following:^[\\[5\\]](#fn3dxn22esft)^\n\n*   Fund existing local groups, such as YIMBY Action, [California YIMBY](https://forum.effectivealtruism.org/tag/california-yimby) or Open New York, or potential new groups in key housing markets.\n*   \"Fund a campaign to move land use decision-making power from the local to the regional or state level. We are not aware of any existing arrangements of this form in the United States, or of any active efforts to promote them, so this would likely be an exercise in 'active funding.'\"\n*   \"Support the development of a policy consensus (for example, by convening conferences or sponsoring work on this issue in prominent think tanks). This would have the benefits of both encouraging coordination on this issue by policymakers, and improving our understanding of what policy changes are most likely to be beneficial.\"\n\nOpen Philanthropy has explored some of these and other funding options; as of August 2022, it has granted over $12 million to organizations working on land use reform.^[\\[6\\]](#fnhfkkjeas3ld)^\n\n### Career options\n\nPromising career options include the following:^[\\[7\\]](#fn7j6j6wgfs3)^\n\n*   \"Become a researcher on cities and urban policy in academia or a think tank.\"\n*   \"Become a politician at the local or state level and support land use reform.\"\n*   Become a public intellectual and raise awareness and concern for land use reform.\n\n### Direct work options\n\nPromising direct work options include the following:^[\\[7\\]](#fn7j6j6wgfs3)^\n\n*   Vote for or contact local candidates who favor dense housing, and federal candidates who favor moving land use decisions to the city or state level.\n*   Volunteer for a local advocacy group.\n\nFurther reading\n---------------\n\nBowman, Sam, John Myers & Ben Southwood (2021) [The housing theory of everything](https://www.worksinprogress.co/issue/the-housing-theory-of-everything/), *Works in Progress*, September 14.\n\nClare, Stephen (2020) [Cause area report: Housing affordability in England](https://founderspledge.com/stories/housing-affordability-in-england-executive-summary), *Founders Pledge*, April 28.\n\nGlaeser, Edward L. & Joseph Gyourko (2008) [*Rethinking Federal Housing Policy*](https://en.wikipedia.org/wiki/Special:BookSources/9780844742731), Washington, D.C.: The AEI Press.\n\nKaufman, Jeff (2019) [Make more land](https://www.lesswrong.com/posts/vKErZy7TFhjxtyBuG), *LessWrong*, October 16.\n\nOpen philanthropy (2015) [Land use reform](https://www.openphilanthropy.org/research/cause-reports/land-use-reform), *Open Philanthropy*, March.\n\nRelated entries\n---------------\n\n[California YIMBY](https://forum.effectivealtruism.org/tag/california-yimby) | [economic growth](https://forum.effectivealtruism.org/tag/economic-growth) | [global health and wellbeing](https://forum.effectivealtruism.org/tag/global-health-and-wellbeing) | [immigration reform](https://forum.effectivealtruism.org/tag/immigration-reform)\n\n1.  ^**[^](#fnref75kcqcvfc8j)**^\n    \n    Glaeser, Edward L., Joseph Gyourko & Raven Saks (2005) [Why is Manhattan so expensive? Regulation and the rise in housing prices](http://doi.org/10.1086/429979), *Journal of Law and Economics*, vol. 48, pp. 331–369.\n    \n2.  ^**[^](#fnrefo7obmpnctab)**^\n    \n    Open philanthropy (2015) [Land use reform](https://www.openphilanthropy.org/research/cause-reports/land-use-reform), *Open Philanthropy*, March, section 2.1.\n    \n3.  ^**[^](#fnref4a7w0bv5b8t)**^\n    \n    Hsieh, Chang Tai & Enrico Moretti (2019) [Housing constraints and spatial misallocation](http://doi.org/10.1257/MAC.20170388), *American Economic Journal: Macroeconomics*, vol. 11, pp. 1–39.\n    \n4.  ^**[^](#fnrefeuzbu3wv0nl)**^\n    \n    Hsieh, Chang-Tai & Enrico Moretti (2017) [How local housing regulations smother the U.S. economy](https://www.nytimes.com/2017/09/06/opinion/housing-regulations-us-economy.html), *The New York Times*, September 6.\n    \n5.  ^**[^](#fnref3dxn22esft)**^\n    \n    Open philanthropy, [Land use reform](https://www.openphilanthropy.org/research/cause-reports/land-use-reform), section 2.2.\n    \n6.  ^**[^](#fnrefhfkkjeas3ld)**^\n    \n    Open Philanthropy (2022) [Grants database: Land use reform](https://www.openphilanthropy.org/grants/?q=&focus-area=land-use-reform), *Open Philanthropy*.\n    \n7.  ^**[^](#fnref7j6j6wgfs3)**^\n    \n    Wiblin, Robert (2016) [Land use reform](https://80000hours.org/problem-profiles/land-use-reform/), *80,000 Hours*, April 14."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "CDEKHTGHAqLcJPw5A",
    "name": "Impact assessment",
    "core": false,
    "slug": "impact-assessment",
    "oldSlugs": [
      "impact-assessment"
    ],
    "postCount": 112,
    "description": {
      "markdown": "**Impact assessment** is a structured process for evaluating the impact of an intervention. Impact assessments can focus on an intervention's expected impact (*ex-ante* or prospective impact assessment) or on its actual impact (*ex-post* or retrospective impact assessment).^[\\[1\\]](#fneo10yzz4ct)^\n\nImpact assessment is sometimes contrasted with ***impact evaluation***, which is understood to be broader in scope and includes impacts not considered in standard impact assessments.^[\\[1\\]](#fneo10yzz4ct)^\n\nFurther reading\n---------------\n\nFortuny, Loreley (2018) [Impact assessment](https://www.iaia.org/wiki-details.php?ID=4), *International Association for Impact Assessment*.\n\nOECD (2022) [What is impact assessment?](https://www.oecd.org/sti/inno/What-is-impact-assessment-OECDImpact.pdf), *OECD*, March.\n\nRelated entries\n---------------\n\n[cause prioritization](https://forum.effectivealtruism.org/tag/cause-prioritization) | [distribution of cost-effectiveness](https://forum.effectivealtruism.org/tag/distribution-of-cost-effectiveness) | [intervention evaluation](https://forum.effectivealtruism.org/tag/intervention-evaluation) | [less-discussed causes](https://forum.effectivealtruism.org/tag/less-discussed-causes) | [org strategy](https://forum.effectivealtruism.org/tag/org-strategy) | [research methods](https://forum.effectivealtruism.org/tag/research-methods) | [statistical methods](https://forum.effectivealtruism.org/tag/statistical-methods) | [thinking at the margin](https://forum.effectivealtruism.org/tag/thinking-at-the-margin) | [intervention evaluation](https://forum.effectivealtruism.org/topics/intervention-evaluation) | [ITN framework](https://forum.effectivealtruism.org/topics/itn-framework-1) | [cost-effectiveness analysis](https://forum.effectivealtruism.org/topics/cost-effectiveness-analysis)\n\n1.  ^**[^](#fnrefeo10yzz4ct)**^\n    \n    OECD (2022) [What is impact assessment?](https://www.oecd.org/sti/inno/What-is-impact-assessment-OECDImpact.pdf), *OECD*, March."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "9QRdwZ2DDGaHQmJHx",
    "name": "Law",
    "core": false,
    "slug": "law",
    "oldSlugs": null,
    "postCount": 65,
    "description": {
      "markdown": "The **law** tag is for posts discussing EA-related legal research questions, when pursuing a law degree makes sense for EAs, how EAs should use a law degree, and EA-related legal jobs. (That list of topics is adapted from the description of the Effective Altruism & Law Facebook group.)\n\nFurther reading\n---------------\n\nEffective Altruism & Law (2018) [About this group](https://www.facebook.com/groups/265987993996107), *Facebook*, July 29.\n\nRelated entries\n---------------\n\n[AI governance](https://forum.effectivealtruism.org/topics/ai-governance) | [global governance](https://forum.effectivealtruism.org/tag/global-governance) | [improving institutional decision-making](https://forum.effectivealtruism.org/topics/improving-institutional-decision-making) | [international relations](https://forum.effectivealtruism.org/tag/international-relations) | [policy change](https://forum.effectivealtruism.org/tag/policy-change) | [standards and regulation](https://forum.effectivealtruism.org/topics/standards-and-regulation)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "NNdytpR2E4jYKQCNd",
    "name": "Operations",
    "core": false,
    "slug": "operations",
    "oldSlugs": null,
    "postCount": 64,
    "description": {
      "markdown": "The **operations** tag is for posts related to operations roles and tasks, meaning, broadly, those which \"enable everyone else in \\[an\\] organisation to focus on their core tasks and maximise their productivity\".^[\\[1\\]](#fnc4g6qik25ob)^\n\nNot to be confused with [operations research](https://forum.effectivealtruism.org/topics/operations-research).\n\nRelated entries\n---------------\n\n[entrepreneurship](https://forum.effectivealtruism.org/tag/entrepreneurship) | [hiring](https://forum.effectivealtruism.org/tag/hiring) | [org strategy](https://forum.effectivealtruism.org/tag/org-strategy)\n\n1.  ^**[^](#fnrefc4g6qik25ob)**^\n    \n    Todd, Benjamin & Roman Duda (2018) [Why operations management is one of the biggest bottlenecks in effective altruism](https://80000hours.org/articles/operations-management/), *80,000 Hours*, March."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "wbQC9Eg4vNp5B5ohw",
    "name": "Epistemic deference",
    "core": false,
    "slug": "epistemic-deference",
    "oldSlugs": [
      "epistemic-humility",
      "epistemic-humility",
      "deference-and-social-epistemology"
    ],
    "postCount": 45,
    "description": {
      "markdown": "**Epistemic deference** is the process of updating one's beliefs in response to what others appear to believe, even if one ignores the reasons for those beliefs or do not find those reasons persuasive. The question of when, how, and to what extent a rational agent should defer to others has been studied—from somewhat different angles—by philosophers working in social epistemology and by economists working in game theory.\n\nFurther reading\n---------------\n\nAird, Michael (2020) [Collection of discussions of epistemic modesty, “rationalist*EA exceptionalism”, and similar](https://www.lesswrong.com/posts/gcEayv6HtBogfov2n/michaela-s-shortform?commentId=nFrAa4zkCzhRgBMLH), *LessWrong*, March 30.  \n*Many additional resources on this topic or related topics.*\n\nLessWrong (2021) [Modest epistemology](https://www.lesswrong.com/tag/modest-epistemology), *LessWrong Wiki*, April 7.\n\nFrances, Bryan & Jonathan Matheson (2018) [Disagreement](https://plato.stanford.edu/archives/win2019/entries/disagreement/), *Stanford Encyclopedia of Philosophy*, February 23 (updated 13 November 2019).\n\nHuemer, Michael (2019) [On challenging the experts](https://fakenous.net/?p=550), *Fake Nous*, July 6.\n\nRelated entries\n---------------\n\n[altruistic coordination](https://forum.effectivealtruism.org/tag/altruistic-coordination) | [discussion norms](https://forum.effectivealtruism.org/tag/discussion-norms) | [expertise](https://forum.effectivealtruism.org/tag/expertise) | [independent impressions](https://forum.effectivealtruism.org/tag/independent-impressions) | [inside vs. outside view](https://forum.effectivealtruism.org/tag/inside-vs-outside-view) | [moral trade](https://forum.effectivealtruism.org/tag/moral-trade) | [principle of epistemic deference](https://forum.effectivealtruism.org/tag/principle-of-epistemic-deference)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "zQ4hMjcYm5yEKkvQz",
    "name": "Atomically precise manufacturing",
    "core": false,
    "slug": "atomically-precise-manufacturing",
    "oldSlugs": [
      "atomically-precise-manufacturing",
      "atomically-precise-manufacturing",
      "atomically-precise-manufacturing"
    ],
    "postCount": 15,
    "description": {
      "markdown": "**Atomically precise manufacturing** (**APM**) is a proposed technology for assembling a wide variety of macroscopic structures molecule-by-molecule with atomic precision.\n\nEvaluation\n----------\n\n[80,000 Hours](https://forum.effectivealtruism.org/tag/80-000-hours) rates atomically precise manufacturing a \"potential highest priority area\": an issue that, if more thoroughly examined, could rank as a top global challenge.^[\\[1\\]](#fn7hbbkqwoybd)^\n\nFurther reading\n---------------\n\nBeckstead, Nick (2014) [A conversation with Chris Phoenix on August 20, 2014](https://www.openphilanthropy.org/sites/default/files/Chris%20Phoenix%2008-20-14%20%28public%29.pdf), *Open Philanthropy*, August 20.\n\nBeckstead, Nick (2015) [Risks from atomically precise manufacturing](https://www.openphilanthropy.org/research/cause-reports/atomically-precise-manufacturing), *Open Philanthropy*, June.\n\nDrexler, K. Eric (2013) [The physical basis of high-throughput atomically precise manufacturing](http://web.archive.org/web/20160229105510/http://metamodern.com/the-physical-basis-of-APM/), *Metamodern*.\n\nHilton, Benjamin (2022) [Risks from atomically precise manufacturing](https://80000hours.org/problem-profiles/atomically-precise-manufacturing/), *80,000 Hours*, July 29.\n\nMuehlhauser, Luke (2017) [Some case studies in early field growth](https://www.openphilanthropy.org/research/history-of-philanthropy/some-case-studies-early-field-growth), *Open Philanthropy*, August.\n\nRegis, Ed (1990) [*Great Mambo Chicken and the Transhuman Condition: Science Slightly Over the Edge*](https://en.wikipedia.org/wiki/Special:BookSources/9780201092585), Reading, Massachusetts: Addison-Wesley.\n\nRosales, Janna (2010) [Drexler-Smalley debates](http://doi.org/10.4135/9781412972093.n96), in David Guston (ed.) *Encyclopedia of Nanoscience and Society*, Thousand Oaks, California: SAGE Publications, pp. 170–171.\n\n1.  ^**[^](#fnref7hbbkqwoybd)**^\n    \n    80,000 Hours (2022) [Our current list of pressing world problems](https://80000hours.org/problem-profiles/), *80,000 Hours*."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "qHXSQkqM7ynEc5qx9",
    "name": "Global governance",
    "core": false,
    "slug": "global-governance",
    "oldSlugs": [
      "global-governance"
    ],
    "postCount": 54,
    "description": {
      "markdown": "**Global governance** is a system of administrative supervision and decision-making above the level of individual nations intended to cope with problems that emerge at a global level. A key function of global governance is to provide global [public goods](https://forum.effectivealtruism.org/topics/public-goods), such as peace, security, functioning markets, mechanisms for conflict resolution and unified standards for trade and industry.^[\\[1\\]](#fnvxg6b9v9y39)^\n\nEvaluation\n----------\n\n[80,000 Hours](https://forum.effectivealtruism.org/tag/80-000-hours) rates global governance a \"potential highest priority area\": an issue that, if more thoroughly examined, could rank as a top global challenge.^[\\[2\\]](#fnnecqdm7gdj)^\n\nFurther reading\n---------------\n\nLu, Catherine (2006) [World government](https://plato.stanford.edu/archives/spr2021/entries/world-government/), *The Stanford Encyclopedia of Philosophy*, December 4 (updated 5 January 2021).\n\nOrd, Toby (2020) [*The Precipice: Existential Risk and the Future of Humanity*](https://en.wikipedia.org/wiki/Special:BookSources/1-5266-0021-8), London: Bloomsbury Publishing, pp. 199–205.\n\nRhodes, Catherine (2018) [Risks and risk management in systems of international governance](https://www.risksciences.ucla.edu/videos/2018/3/12/risks-and-risk-management-in-systems-of-international-governance), in John Garrick (ed.) *International Colloquium on Catastrophic & Existential Risk*, University of California, Los Angeles, pp. 125–143.\n\nRelated entries\n---------------\n\n[AI governance](https://forum.effectivealtruism.org/topics/ai-governance) | [ballot initiative](https://forum.effectivealtruism.org/tag/ballot-initiative) | [dystopia](https://forum.effectivealtruism.org/tag/dystopia) | [improving institutional decision-making](https://forum.effectivealtruism.org/topics/improving-institutional-decision-making) | [international organization](https://forum.effectivealtruism.org/tag/international-organization) | [international relations](https://forum.effectivealtruism.org/tag/international-relations) | [policy change](https://forum.effectivealtruism.org/tag/policy-change) | [public goods](https://forum.effectivealtruism.org/tag/public-goods) | [space governance](https://forum.effectivealtruism.org/tag/space-governance) | [standards and regulation](https://forum.effectivealtruism.org/topics/standards-and-regulation) | [totalitarianism](https://forum.effectivealtruism.org/tag/totalitarianism) | [United Nations](https://forum.effectivealtruism.org/tag/united-nations-1) | [vulnerable world hypothesis](/tag/vulnerable-world-hypothesis)\n\n1.  ^**[^](#fnrefvxg6b9v9y39)**^\n    \n    Global Challenges Foundation (2020) [What is global governance?](https://globalchallenges.org/global-governance/), *Global Challenges Foundation*.\n    \n2.  ^**[^](#fnrefnecqdm7gdj)**^\n    \n    80,000 Hours (2022) [Our current list of pressing world problems](https://80000hours.org/problem-profiles/), *80,000 Hours*."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "uDAGFwZLscHKfoubc",
    "name": "Improving institutional decision-making",
    "core": false,
    "slug": "improving-institutional-decision-making",
    "oldSlugs": [
      "improving-institutional-decision-making",
      "institutional-decision-making",
      "institutional-decision-making"
    ],
    "postCount": 168,
    "description": {
      "markdown": "**Improving institutional decision-making** is a cause that focuses on increasing the technical quality and [effective altruism](https://forum.effectivealtruism.org/topics/effective-altruism) alignment of the most important decisions made by the world’s most important decision-making bodies.^[\\[1\\]](#fnpg98poczmg)^\n\nImproving institutions\n----------------------\n\nInstitutions such as governments, companies, and charities control significant resources. One potentially effective way to do good, therefore, is to help institutions use these resources in more productive ways.\n\nMembers of the effective altruism community have employed this method extensively. For instance, they have tried to [increase the attention policy-makers give](https://forum.effectivealtruism.org/tag/policy-change) to [existential risk](https://forum.effectivealtruism.org/tag/existential-risk). Similarly, an important goal of effective altruist charity recommendations is to increase the effectiveness of nonprofit organizations. [Within the for-profit sector](https://forum.effectivealtruism.org/tag/influencing-for-profits), altruists have sought to shape the incentives of businesses to make them more aligned with social value, and have also tried to create social value themselves by engaging in social entrepreneurship.\n\nInstitutions can be improved in two different ways: from the *outside* and from the *inside*. Effective altruism organizations try to improve institutions from the outside by giving them advice or, in the case of charities, by evaluating them, whereas individual members of the effective altruism community may work within institutions to help them achieve their ends more effectively.\n\nOne approach to improving decisions is to set up institutional structures that are conducive to good decision-making.^[\\[2\\]](#fnusvfm9rnves)^^[\\[3\\]](#fn2d3u96edm2a)^ This way, institutions like national governments might encourage people to make better decisions (e.g. saving for retirement) or make better decisions themselves (e.g. improving health policy).\n\nEvaluation\n----------\n\n[80,000 Hours](https://forum.effectivealtruism.org/tag/80-000-hours) rates improving institutional decision-making a \"second-highest priority area\": an unusually pressing global problem ranked slightly below their four highest priority areas.^[\\[4\\]](#fnhjq832ykn24)^\n\nFurther reading\n---------------\n\nWhittlestone, Jess (2017) [Improving institutional decision-making](https://80000hours.org/problem-profiles/improving-institutional-decision-making/), *80,000 Hours*, September.\n\nRelated entries\n---------------\n\n[All-Party Parliamentary Group for Future Generations](https://forum.effectivealtruism.org/tag/all-party-parliamentary-group-for-future-generations) | [ballot initiative](https://forum.effectivealtruism.org/tag/ballot-initiative) | [electoral reform](https://forum.effectivealtruism.org/tag/electoral-reform) | [forecasting](https://forum.effectivealtruism.org/tag/forecasting) | [international relations](https://forum.effectivealtruism.org/tag/international-relations) | [longtermist institutional reform](https://forum.effectivealtruism.org/topics/longtermist-institutional-reform) | [policy change](https://forum.effectivealtruism.org/tag/policy-change) | [political polarization](https://forum.effectivealtruism.org/tag/political-polarization)\n\n1.  ^**[^](#fnrefpg98poczmg)**^\n    \n    Clayton, Vicky, Dilhan Perera & ibatra171 (2021) [Refining improving institutional decision-making as a cause area: results from a scoping survey](https://forum.effectivealtruism.org/posts/FqCSZT3pBvoATkR82/refining-improving-institutional-decision-making-as-a-cause), *Effective Altruism Forum*, June 26.\n    \n2.  ^**[^](#fnrefusvfm9rnves)**^\n    \n    Thaler, Richard H. & Cass R. Sunstein (2008) [*Nudge: Improving Decision about Health, Wealth, and Happiness*](https://en.wikipedia.org/wiki/Special:BookSources/978-0-300-12223-7), New Haven: Yale University Press.\n    \n3.  ^**[^](#fnref2d3u96edm2a)**^\n    \n    Whittlestone, Jess (2017) [Improving institutional decision-making](https://80000hours.org/problem-profiles/improving-institutional-decision-making/), *80,000 Hours*, September.\n    \n4.  ^**[^](#fnrefhjq832ykn24)**^\n    \n    80,000 Hours (2021) [Our current list of the most important world problems](https://80000hours.org/problem-profiles/), *80,000 Hours*."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "DwJY53daQyQq8eWo7",
    "name": "Armed conflict",
    "core": false,
    "slug": "armed-conflict",
    "oldSlugs": [
      "armed-conflict",
      "armed-conflict",
      "armed-conflict"
    ],
    "postCount": 57,
    "description": {
      "markdown": "The **armed conflict** tag is for posts about interstate or intrastate violence between armed groups, the drivers of such conflicts, and interventions to reduce the frequency or severity of such conflicts.\n\nFurther reading\n---------------\n\nGarfinkel, Ben & Allan Dafoe (2019) [How does the offense-defense balance scale?](https://doi.org/10.1080/01402390.2019.1631810), *Journal of Strategic Studies*, vol. 42, pp. 736–763.\n\nWiblin, Robert & Keiran Harris (2022) [Chris Blattman on the five reasons wars happen](https://80000hours.org/podcast/episodes/chris-blattman-five-reasons-wars-happen/), *80,000 Hours*, April 28.\n\nRelated entries\n---------------\n\n[autonomous weapon](https://forum.effectivealtruism.org/tag/autonomous-weapon) | [existential risk factor](https://forum.effectivealtruism.org/tag/existential-risk-factor) | [global governance](https://forum.effectivealtruism.org/tag/global-governance) | [great power conflict](https://forum.effectivealtruism.org/tag/great-power-conflict) |  [international relations](https://forum.effectivealtruism.org/tag/international-relations) | [nuclear warfare](https://forum.effectivealtruism.org/tag/nuclear-warfare-1) | [peace and conflict studies](https://forum.effectivealtruism.org/tag/peace-and-conflict-studies) | [polarity](https://forum.effectivealtruism.org/tag/polarity) | [Russell–Einstein Manifesto](https://forum.effectivealtruism.org/tag/russell-einstein-manifesto) | [weapons of mass destruction](https://forum.effectivealtruism.org/tag/weapons-of-mass-destruction)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "riD3AWMYveHmSkhN8",
    "name": "European Union ",
    "core": false,
    "slug": "european-union",
    "oldSlugs": [
      "european-union",
      "european-union-posts-discuss-ideas-and-interventions-for"
    ],
    "postCount": 39,
    "description": {
      "markdown": "The **European Union** is an [international organization](https://forum.effectivealtruism.org/tag/international-organisations) in Europe. An understanding of how the European Union works and its effects on the world can be relevant to various problems of interest in the effective altruism community, and some members of the community have proposed interventions focused on influencing the European Union. For example, the Brussels effect may be an important consideration for [AI governance](https://forum.effectivealtruism.org/tag/ai-governance) efforts, and influencing certain EU regulations may be a cost-effective [farmed animal welfare](https://forum.effectivealtruism.org/tag/farmed-animal-welfare) intervention.^[\\[1\\]](#fn0p8rtzgpoky8)^\n\nFurther reading\n---------------\n\nAird, Michael (2022) [Collection of work on whether/how much people should focus on the EU if they’re interested in AI governance for longtermist/x-risk reasons](https://forum.effectivealtruism.org/posts/EMKf4Gyee7BsY2RP8/michaela-s-shortform?commentId=De4WPBLeszzGWrqjp), *Effective Altruism Forum*, February 11.\n\nSimonin, Denis & Andrea Gavinelli (2019) [The European Union legislation on animal welfare: state of play, enforcement and future activities](https://en.wikipedia.org/wiki/Special:BookSources/978-2-9512167-4-7), in Sophie Hild & Louis Schweitzer (eds.) *Animal Welfare: From Science to Law*, Paris: Fondation Droit Animal, pp. 59–70.\n\nStix, Charlotte (2022) [The ghost of AI governance past, present and future: AI governance in the European Union](https://www.ssrn.com/abstract=3882493), in Justin Bullock & Valerie Hudson (eds.) *The Oxford Handbook on AI Governance*, Oxford: Oxford University Press, forthcoming.\n\nRelated entries\n---------------\n\n[California effect](https://forum.effectivealtruism.org/topics/california-effect) | [global governance](https://forum.effectivealtruism.org/tag/global-governance) | [international organization](https://forum.effectivealtruism.org/tag/international-organization) | [international relations](https://forum.effectivealtruism.org/tag/international-relations) | [law](https://forum.effectivealtruism.org/tag/law) | [United Nations](https://forum.effectivealtruism.org/tag/united-nations-1)\n\n1.  ^**[^](#fnref0p8rtzgpoky8)**^\n    \n    Wikipedia (2018) [Brussels effect](https://en.wikipedia.org/wiki/Brussels_effect), *Wikipedia*, May 17 (updated 25 April 2021‎)."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "8sbHXePJ4L5o8G5aM",
    "name": "Moral uncertainty",
    "core": false,
    "slug": "moral-uncertainty",
    "oldSlugs": [
      "moral-uncertainty"
    ],
    "postCount": 71,
    "description": {
      "markdown": "**Moral uncertainty** is uncertainty about how to act given lack of certainty in any one moral theory, as well as the study of how we ought to act given this uncertainty.\n\nWe are sometimes uncertain about empirical facts, such as whether it will rain tomorrow. But we can also be uncertain about *moral* facts, such as whether it's wrong to steal, or how we should value the well-being of animals. Uncertainty about whether it's wrong to steal is uncertainty about moral or normative facts, while uncertainty about how we should value the well-being of animals is uncertainty about axiological or value facts.\n\nCan moral uncertainty be rational, and what should we do in response to it? We might think that it can never be rational to be uncertain about normative or axiological facts, because such facts are, like mathematical facts, knowable *a priori*. Nevertheless it seems that agents like ourselves are uncertain about non-trivial mathematical facts, and that we are also uncertain about normative and axiological facts. Given this, it seems necessary to develop some account of how we ought to act under moral uncertainty.\n\nSeveral such accounts have been formulated in recent years:\n\n*   **Normative externalism**: We can be required to act in accordance with norms, even if we are not in a position to know what they are.^[\\[1\\]](#fnz1t1w4pw1l)^\n*   **My favorite theory**: We should act in accordance with the moral theory that we are most confident in.^[\\[2\\]](#fnoimahk7thkh)^\n*   **Maximize expected choiceworthiness**: Moral theories rank actions, and we should act so as to maximize the overall expected choiceworthiness of our actions relative to the theories we are uncertain over.^[\\[3\\]](#fn82nltw9cxv3)^ This is an extension of [expected value theory](https://forum.effectivealtruism.org/tag/expected-value) to situations of moral uncertainty.\n*   **The** ***parliamentary model***: Moral theories should be given weight in accordance with our credences in them, and trades can be made between them to determine the optimal set of actions under uncertainty.^[\\[4\\]](#fno3rr9ulfig)^\n\nTwo important problems facing the view that moral uncertainty can affect how we ought to act are the *regress problem* \\- the problem that we will be uncertain not only about typical moral questions, but also about which approach to moral uncertainty is correct, and so on ad infinitum^[\\[5\\]](#fn6303n5ui73y)^ \\- and the *problem of* *intertheoretic comparisons* \\- the problem that there seems to be no principled ways to make comparisons between different moral theories.^[\\[6\\]](#fnu6710q9msxb)^\n\nFurther reading\n---------------\n\nMacAskill, William, Krister Bykvist, & Toby Ord (2020) [*Moral Uncertainty*](https://doi.org/10.1093/oso/9780198722274.001.0001), Oxford: Oxford University Press.\n\nRelated entries\n---------------\n\n[altruistic wager](https://forum.effectivealtruism.org/tag/altruistic-wager) | [decision-theoretic uncertainty](https://forum.effectivealtruism.org/tag/decision-theoretic-uncertainty) | [ethics of existential risk](https://forum.effectivealtruism.org/tag/ethics-of-existential-risk) | [expected value](https://forum.effectivealtruism.org/tag/expected-value) | [fanaticism](https://forum.effectivealtruism.org/tag/fanaticism) | [moral trade](https://forum.effectivealtruism.org/tag/moral-trade) | [normative uncertainty](https://forum.effectivealtruism.org/tag/normative-uncertainty-1)\n\n1.  ^**[^](#fnrefz1t1w4pw1l)**^\n    \n    Weatherson, Brian (2014) [Running risks morally](http://link.springer.com/article/10.1007/s11098-013-0227-2), *Philosophical Studies*,  167: 141–63.\n    \n2.  ^**[^](#fnrefoimahk7thkh)**^\n    \n    Gustafsson, Johan E. & Olle Torpman (2014) [In defence of my favourite theory](http://johanegustafsson.net/papers/in-defence-of-my-favourite-theory.pdf), *Pacific Philosophical Quarterly*, 95(2): 159-174.\n    \n3.  ^**[^](#fnref82nltw9cxv3)**^\n    \n    MacAskill, William (2014) [Normative uncertainty](http://commonsenseatheism.com/wp-content/uploads/2014/03/MacAskill-Normative-Uncertainty.pdf), Doctoral dissertation, University of Oxford.\n    \n4.  ^**[^](#fnrefo3rr9ulfig)**^\n    \n    Bostrom, Nick (2009) [Moral uncertainty - towards a solution?](http://www.overcomingbias.com/2009/01/moral-uncertainty-towards-a-solution.html), *Overcoming Bias*, January 1.\n    \n5.  ^**[^](#fnref6303n5ui73y)**^\n    \n    Sepielli, Andrew (2013) [What to do when you don't know what to do when you don't know what to do…](http://onlinelibrary.wiley.com/doi/10.1111/nous.12010/abstract), *Nous*, 47(1) :521-544.\n    \n6.  ^**[^](#fnrefu6710q9msxb)**^\n    \n    Nissan-Rozen, Ittay (2015) [Against moral hedging](https://www.cambridge.org/core/journals/economics-and-philosophy/article/against-moral-hedging/E818376A231FECAEBFA01A3411A567C0), *Economics and Philosophy*, 31(3): 349-369."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "pnsShJnFiGNhqztkY",
    "name": "Metaethics",
    "core": false,
    "slug": "metaethics",
    "oldSlugs": [
      "meta-ethics",
      "meta-ethics",
      "metaethics",
      "meta-ethics",
      "metaethics",
      "meta-ethics"
    ],
    "postCount": 55,
    "description": {
      "markdown": "**Metaethics** is the study of the nature of ethical properties and statements. It includes questions about the sort of evidence which can be used to support moral ideas, whether morality is culturally relative, and what the nature of moral facts could be.\n\nTwo major positions in metaethics are moral realism and moral antirealism. Moral realism has a number of forms, including:\n\nMoral naturalism: The claim that moral truths are reducible to natural facts in the world.\n\nMoral non-naturalism: The claim that moral trust are unusual types of truths (perhaps equivalent in nature to truths about mathematics).\n\nMoral anti-realism also has a number of forms, including:\n\nMoral error theory: The view that people’s moral practice and language commit them to mind-independent moral truths, but that there are no such truths.\n\nMoral non-cognitivism: The view that moral practice does not commit people to mind-independent moral truth. Moral Non-Cognitivists argue that moral practice is properly understood as an expression of people’s values.\n\nFurther reading\n---------------\n\nLutz, Matthew & James Lenman (2018) [Moral naturalism](https://plato.stanford.edu/entries/naturalism-moral/), *Stanford Encyclopedia of Philosophy*, May 30 (updated 21 December 2021).\n\nJoyce, Richard (2007) [Moral anti-realism](https://plato.stanford.edu/cgi-bin/encyclopedia/archinfo.cgi?entry=moral-anti-realism), *Stanford Encyclopedia of Philosophy*, July 30 (updated 25 October 2016).\n\nRidge, Michael (2003) [Moral non-naturalism](https://plato.stanford.edu/archives/fall2019/entries/moral-non-naturalism/), *Stanford Encyclopedia of Philosophy*, February 1 (updated 21 August 2019).\n\nSayre-McCord, Geoff (2005) [Moral realism](https://plato.stanford.edu/archives/win2020/entries/moral-realism/), *Stanford Encyclopedia of Philosophy*, October 3 (updated 31 October 2020).\n\nSayre-McCord, Geoff (2007) [Metaethics](https://plato.stanford.edu/entries/metaethics/), *Stanford Encyclopedia of Philosophy*, January 23 (updated 28 April 2014).\n\nvan Roojen, Mark (2004) [Moral cognitivism vs. non-cognitivism](https://plato.stanford.edu/entries/moral-cognitivism/), *Stanford Encyclopedia of Philosophy*, January 23 (updated 28 June 2018).\n\nRelated entries\n---------------\n\n[applied ethics](https://forum.effectivealtruism.org/tag/applied-ethics) | [moral philosophy](https://forum.effectivealtruism.org/tag/moral-philosophy) | [normative ethics](https://forum.effectivealtruism.org/tag/normative-ethics)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "mMC3LEMAJWx4FbHW3",
    "name": "Patient altruism",
    "core": false,
    "slug": "patient-altruism",
    "oldSlugs": [
      "patient-altruism-1",
      "patient-longtermism",
      "patient-altruism"
    ],
    "postCount": 69,
    "description": {
      "markdown": "**Patient altruism** (sometimes called **patient longtermism**) is the view that individuals can have a greater positive impact by investing current altruistic resources and spending them later than by spending them now. These resources include both financial and non-financial resources. The narrower view that altruists should invest and later donate *financial* resources, instead of donating them now, is sometimes called **patient philanthropy**, although these expressions are sometimes used interchangeably.\n\nFurther reading\n---------------\n\nBaumann, Tobias (2020) [Thoughts on patient philanthropy](https://s-risks.org/thoughts-on-patient-philanthropy/), *Reducing Risks of Future Suffering*, September 8.\n\nTodd, Benjamin (2020) [The emerging school of patient longtermism](https://80000hours.org/2020/08/the-emerging-school-of-patient-longtermism/), *80,000 Hours*, August 7.\n\nTrammell, Philip (2021) [Dynamic public good provision under time preference heterogeneity: Theory and applications to philanthropy](https://philiptrammell.com/static/PatienceAndPhilanthropy.pdf), unpublished.\n\nWiblin, Robert, Howie Lempel & Keiran Harris (2020) [How becoming a “patient philanthropist” could allow you to do far more good](https://80000hours.org/podcast/episodes/phil-trammell-patient-philanthropy/), *80,000 Hours*, March 17.  \n*Interview with Phil Trammell.*\n\nRelated entries\n---------------\n\n[hinge of history](https://forum.effectivealtruism.org/tag/hinge-of-history) | [investing](https://forum.effectivealtruism.org/tag/investing) | [longtermism](https://forum.effectivealtruism.org/tag/longtermism) | [timing of philanthropy](https://forum.effectivealtruism.org/tag/timing-of-philanthropy)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "ABhAuXcG6rsrSdyq5",
    "name": "Moral weight",
    "core": false,
    "slug": "moral-weight",
    "oldSlugs": [
      "moral-weight"
    ],
    "postCount": 43,
    "description": {
      "markdown": "The **moral weight** tag covers posts about differences in moral status, capacity for welfare, or other factors that might determine differences in the characteristic moral value of the lives, interests, or experiences of different types of potential [moral patients](https://forum.effectivealtruism.org/tag/moral-patienthood) (such as individuals of different species).\n\nFurther reading\n---------------\n\nMuehlhauser, Luke (2017) [2017 report on consciousness and moral patienthood](https://www.openphilanthropy.org/research/2017-report-on-consciousness-and-moral-patienthood/#MoralWeight), *Open Philanthropy*, June.  \n*The idea of “moral weights” is addressed briefly in a few places.*\n\nMuehlhauser, Luke (2018) [Preliminary thoughts on moral weight](https://www.lesswrong.com/posts/2jTQTxYNwo6zb3Kyp/preliminary-thoughts-on-moral-weight), *LessWrong*, August 14.\n\nSchukraft, Jason (2020) [Moral weight series](https://forum.effectivealtruism.org/s/Yw5JeJHyhYQtYw8Z6), *Effective Altruism Forum*, October 19.\n\nRelated entries\n---------------\n\n[axiology](https://forum.effectivealtruism.org/tag/axiology) | [moral circle expansion](https://forum.effectivealtruism.org/tag/moral-circle-expansion-1) | [sentience](https://forum.effectivealtruism.org/tag/sentience-1)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "ZCihBFp5P64JCvQY6",
    "name": "Community",
    "core": true,
    "slug": "community",
    "oldSlugs": [
      "community",
      "my-back-pain-coach-reviews",
      "community",
      "unicode-2",
      "community",
      "community-uktvfeeds"
    ],
    "postCount": 1989,
    "description": {
      "markdown": "The **Community** tag covers posts about the EA community, as well as applying EA in one's personal life. The tag also applies to posts about the Forum itself, since this is a community space.\n\nIf you never touched that setting, or if you view the Forum while logged out, the weighting of community posts will be “-25” by default. (The algorithm will show posts with the Community tag less prominently, as though they all had 25 less karma.)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "pdKwPrcTEyJ6BgD99",
    "name": "Dystopia",
    "core": false,
    "slug": "dystopia",
    "oldSlugs": [
      "dystopias-and-totalitarianism",
      "dystopia",
      "global-dystopia"
    ],
    "postCount": 30,
    "description": {
      "markdown": "A **dystopia** is a civilization devoid of most value.\n\nIn [Toby Ord](https://forum.effectivealtruism.org/tag/toby-ord)'s typology, unrecoverable dystopias constitute one of the three main types of [existential catastrophe](https://forum.effectivealtruism.org/tag/existential-catastrophe-1).^[\\[1\\]](#fnztnov6l84es)^\n\nOrd further subdivides unrecoverable dystopias into three types. A dystopia may be desired by either *none*, *some*, or *most* actors living under it. Ord calls these *undesired dystopias*, *enforced dystopias,* and *desired dystopias*, respectively.^[\\[1\\]](#fnztnov6l84es)^\n\nEnforced dystopias\n------------------\n\nEnforced dystopias are the most familiar type of dystopia. In fiction, they are most prominently represented by George Orwell's *Nineteen-Eighty Four*: \"If you want a picture of the future, imagine a boot stamping on a human face— for ever.\"^[\\[2\\]](#fnw90l2ckr1m)^ Outside of fiction, [North Korea](https://forum.effectivealtruism.org/tag/north-korea) arguably offers an example of a *local* stable dystopia.^[\\[3\\]](#fnv9nc6flfpk)^^[\\[4\\]](#fn3v7afdwemy3)^ Fictional and real enforced dystopias often assume the form of robust [totalitarianism](https://forum.effectivealtruism.org/tag/totalitarianism), though this need not be so.\n\nUndesired dystopias\n-------------------\n\nSuperficially, undesired dystopias may appear unlikely. If no one desires a world, why should we expect it to exist? The answer relates to the mismatch that can sometimes occur between individual and collective preferences: what is rational for each person may be irrational for all people. It may be best for each individual to consume resources without restraint, regardless of what the other individuals do; but if everyone acts in this manner, the result may be resource depletion, which is worse for everyone than an alternative in which everyone moderates their consumption. [Scott Alexander](https://forum.effectivealtruism.org/tag/scott-alexander) offers a toy example of a possible undesired dystopia.^[\\[5\\]](#fnqadpisr38a8)^ Imagine a society governed by two simple rules: first, every person must spend eight hours a day giving themselves strong electric shocks; second, if anyone fails to follow either rule, everyone must unite to kill this person. The result is a world in which everyone ends up giving themselves electric shocks, since they know they will be killed otherwise. As Alexander summarizes, \"Every single citizen hates the system, but for lack of a good coordination mechanism it endures.\"^[\\[5\\]](#fnqadpisr38a8)^\n\nDesired dystopias\n-----------------\n\nJust as one may wonder why undesired dystopias would exist, one may wonder why desired dystopias would be dystopian. Here a relevant example has been provided by [Nick Bostrom](https://forum.effectivealtruism.org/tag/nick-bostrom).^[\\[6\\]](#fnn6rvm9yb4k)^^[\\[7\\]](#fndgcnpzocvie)^ Mass outsourcing to either digital uploads or AI agents could eventually result in a world entirely devoid of [phenomenal consciousness](https://forum.effectivealtruism.org/tag/consciousness). This could happen if it turned out that conscious states could not be instantiated *in silico*. It could also happen if, in this radically new environment, consciousness was selected against due to strong evolutionary pressures. It may, for instance, be more computationally efficient to represent an agent's utility function explicitly rather than to rely on a hedonic reward architecture. On a wide range of theories, wellbeing requires consciousness (although it may not reduce to consciousness), so such a world would be devoid of [moral patients](https://forum.effectivealtruism.org/tag/moral-patienthood), no matter how thriving it may appear to outside observers or how much the world's inhabitants may insist that they *are* conscious or that their lives are worth living. Bostrom describes an imagined \"technologically highly advanced society, containing many sorts of complex structures, some of which are much smarter and more intricate than anything that exists today, in which there would nevertheless be a complete absence of any type of being whose welfare has moral significance. In a sense, this would be an uninhabited society. All the kinds of being that we care even remotely about would have vanished.\"^[\\[6\\]](#fnn6rvm9yb4k)^ Aspects of this possible dystopian future may actually be observed at present in the lives of some non-human animals bred for human consumption.^[\\[8\\]](#fn0dc5wz7cvzpf)^\n\nDystopias and moral value\n-------------------------\n\nSince the concept of a dystopia is defined in terms of the value absent from the world so characterized, whether something is or is not a dystopia may vary depending on the [moral theory](https://forum.effectivealtruism.org/tag/normative-ethics) under consideration. On classical [utilitarianism](https://forum.effectivealtruism.org/tag/utilitarianism), for example, there is an enormous difference in value between [worlds optimized for positive experience](https://forum.effectivealtruism.org/tag/hedonium) and a seemingly desirable world where everyone enjoys the quality of life of the most privileged citizens of today’s most prosperous nations. The permanent entrenchment of the latter type of world may thus, on that theory, count as a dystopia, in the sense that most attainable value would have failed to be realized. Conversely, although Bostrom’s \"unconscious outsourcers\" dystopian scenario would be catastrophic on many plausible moral theories, it may not be so from the perspective of [suffering-focused ethics](https://forum.effectivealtruism.org/tag/suffering-focused-ethics).\n\nFurther reading\n---------------\n\nAird, Michael (2020) [Collection of sources related to dystopias and \"robust totalitarianism\"](https://forum.effectivealtruism.org/posts/EMKf4Gyee7BsY2RP8/michaela-s-shortform?commentId=8GJtZ6DrEn5MDf6dZ), *Effective Altruism Forum*, March 30.  \n*Many additional resources on this topic.*\n\nOrd, Toby (2020) [*The Precipice: Existential Risk and the Future of Humanity*](https://en.wikipedia.org/wiki/Special:BookSources/1526600218), London: Bloomsbury Publishing, pp. 153–158.\n\nRelated entries\n---------------\n\n[civilizational collapse](/tag/civilizational-collapse) | [existential catastrophe](https://forum.effectivealtruism.org/tag/existential-catastrophe-1) | [flourishing futures](https://forum.effectivealtruism.org/tag/flourishing-futures) | [global governance](https://forum.effectivealtruism.org/tag/global-governance) |  [human extinction](https://forum.effectivealtruism.org/tag/human-extinction) | [totalitarianism](https://forum.effectivealtruism.org/tag/totalitarianism) | [value lock-in](https://forum.effectivealtruism.org/tag/value-lock-in)\n\n1.  ^**[^](#fnrefztnov6l84es)**^\n    \n    Ord, Toby (2020) [*The Precipice: Existential Risk and the Future of Humanity*](https://en.wikipedia.org/wiki/Special:BookSources/1526600218), London: Bloomsbury Publishing, fig. 5.2.\n    \n2.  ^**[^](#fnrefw90l2ckr1m)**^\n    \n    Orwell, George (1949) *Nineteen Eighty-Four: A Novel*, London: Secker & Warburg, ch. 3.\n    \n3.  ^**[^](#fnrefv9nc6flfpk)**^\n    \n    Drescher, Denis (2017) [Cause area: Human rights in North Korea](https://forum.effectivealtruism.org/posts/werW78GfeAgBRbvF3/cause-area-human-rights-in-north-korea), *Effective Altruism Forum*, November 20.\n    \n4.  ^**[^](#fnref3v7afdwemy3)**^\n    \n    Drescher, Denis (2020) [Self-study directions 2020](https://impartial-priorities.org/self-study-directions-2020.html), *Impartial Priorities*, June 27.\n    \n5.  ^**[^](#fnrefqadpisr38a8)**^\n    \n    Alexander, Scott (2014) [Meditations on Moloch](https://slatestarcodex.com/2014/07/30/meditations-on-moloch/), *Slate Star Codex*, July 30.\n    \n6.  ^**[^](#fnrefn6rvm9yb4k)**^\n    \n    Bostrom, Nick (2004) [The future of human evolution](https://en.wikipedia.org/wiki/Special:BookSources/9780974347226) in Charles Tandy (ed.) *Death and Anti-Death: Two Hundred Years after Kant, Fifty Years after Turing*, vol. 2, Palo Alto, California: Ria University Press, pp. 339–371.\n    \n7.  ^**[^](#fnrefdgcnpzocvie)**^\n    \n    Bostrom, Nick (2014) [*Superintelligence: Paths, Dangers, Strategies*](https://en.wikipedia.org/wiki/Special:BookSources/9780199678112), Oxford: Oxford University Press, pp. 172-173.\n    \n8.  ^**[^](#fnref0dc5wz7cvzpf)**^\n    \n    Liu, Yuxi (2019) [Evolution “failure mode”: chickens](https://www.lesswrong.com/posts/JgahmfLPPqgTRpFtT/evolution-failure-mode-chickens), *LessWrong*, April 26."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "yc6Tim37shrJbubQk",
    "name": "Differential progress",
    "core": false,
    "slug": "differential-progress",
    "oldSlugs": [
      "differential-progress",
      "differential-progress",
      "differential-progress"
    ],
    "postCount": 42,
    "description": {
      "markdown": "**Differential progress** is the hastening of risk-reducing progress and the delaying of risk-increasing progress.\n\nDifferential progress can be regarded as a generalization of the proposal, first made by [Nick Bostrom](https://forum.effectivealtruism.org/tag/nick-bostrom), to accelerate risk-reducing technological progress and retard risk-increasing technological progress.^[\\[1\\]](#fnvsg8gsgzvcn)^ This proposal, which Bostrom calls **differential technological development**, was originally offered as an alternative to the view that technological progress as a whole should be slowed down in response to concerns about [existential risk](https://forum.effectivealtruism.org/tag/existential-risk). As Bostrom argued, since technology has the potential to both increase and decrease risk, the appropriate response is to handle technologies with different effects on risk differently, rather than having a general policy of technological retardation. In more recent publications, Bostrom understands \"technology\" in a very broad sense, to include \"not only gadgets but also methods, techniques and institution design principles\"^[\\[2\\]](#fnidqq2ho285n)^ and \"scientific ideas, institutional designs, organizational techniques, ideologies, concepts, and memes\".^[\\[3\\]](#fnj1kpoww6c7)^ When the notion of differential technological development is combined with this expansive understanding of technology, it becomes roughly equivalent to differential progress, rather than a special case of it.\n\nThe term \"**differential intellectual progress\"** is sometimes used as a synonym for \"differential progress\", though it may also be used more restrictively to exclude non-intellectual forms of progress.^[\\[4\\]](#fnqagcqeun7ue)^^[\\[5\\]](#fnfyz5u4inxp7)^\n\nFurther reading\n---------------\n\nAird, Michael (2020) [Collection of all prior work I found that explicitly uses the terms differential progress / intellectual progress / technological development](https://forum.effectivealtruism.org/posts/EMKf4Gyee7BsY2RP8/michaela-s-shortform?commentId=xrj9XYjvsGLz6R2i6), *Effective Altruism Forum*, February 24.  \n*Many additional resources on this topic.*\n\nBeckstead, Nick (2015) [Differential technological development: some early thinking](https://blog.givewell.org/2015/09/30/differential-technological-development-some-early-thinking/), *The GiveWell Blog*, September 30 (updated 26 July 2016).  \n*An application of the concept of differential progress to artificial intelligence research.*\n\nChristiano, Paul (2014) [On progress and prosperity](https://forum.effectivealtruism.org/posts/L9tpuR6ZZ3CGHackY/on-progress-and-prosperity), *Effective Altruism Forum*, October 15.  \n*Argues that differential progress is extremely important from an altruistic perspective.*\n\nRelated entries\n---------------\n\n[accidental harm](https://forum.effectivealtruism.org/tag/accidental-harm) | [anthropogenic existential risk](https://forum.effectivealtruism.org/tag/anthropogenic-existential-risk) | [cultural lag](https://forum.effectivealtruism.org/tag/cultural-lag) | [economic growth](https://forum.effectivealtruism.org/tag/economic-growth) | [scientific progress](https://forum.effectivealtruism.org/tag/scientific-progress) | [speeding up development](https://forum.effectivealtruism.org/tag/speeding-up-development) | [vulnerable world hypothesis](https://forum.effectivealtruism.org/tag/vulnerable-world-hypothesis)\n\n1.  ^**[^](#fnrefvsg8gsgzvcn)**^\n    \n    Bostrom, Nick (2002) [Existential risks: Analyzing human extinction scenarios and related hazards](https://www.jetpress.org/volume9/risks.html), *Journal of evolution and technology*, vol. 9., section 9.4.\n    \n2.  ^**[^](#fnrefidqq2ho285n)**^\n    \n    Bostrom, Nick (2009) [The future of humanity](http://doi.org/10.1002/9781444310795.ch98), in Jan Kyrre Berg Olsen, Stig Andur Pedersen & Vincent F. Hendricks (eds.) *A Companion to the Philosophy of Technology*, Oxford: Wiley-Blackwell, pp. 551–557, p. 551.\n    \n3.  ^**[^](#fnrefj1kpoww6c7)**^\n    \n    Bostrom, Nick (2019) [The vulnerable world hypothesis](http://doi.org/10.1111/1758-5899.12718), *Global Policy*, vol. 10, pp. 455–476, p. 458.\n    \n4.  ^**[^](#fnrefqagcqeun7ue)**^\n    \n    Muehlhauser, Luke & Anna Salamon (2012) [Intelligence explosion: evidence and import](http://doi.org/10.1007/978-3-642-32560-1), in Amnon Eden *et al.* (eds.) *Singularity Hypotheses: A Scientific and Philosophical Assessment*, Berlin: Springer, pp. 15–40.\n    \n5.  ^**[^](#fnreffyz5u4inxp7)**^\n    \n    Tomasik, Brian (2015) [Differential intellectual progress as a positive-sum project](https://longtermrisk.org/differential-intellectual-progress-as-a-positive-sum-project/), *Center on Long-Term Risk*, December 21."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "SM397hQ6A9wiDrD3e",
    "name": "Population ethics",
    "core": false,
    "slug": "population-ethics",
    "oldSlugs": [
      "population-ethics"
    ],
    "postCount": 89,
    "description": {
      "markdown": "**Population ethics** is the branch of [normative ethics](/tag/normative-ethics) that attempts to justify and articulate principles for making choices between alternatives involving different people. These differences can consist of changes either in the number of people affected or only in the identities of these people.\n\nFurther reading\n---------------\n\nAird, Michael (2020) [Collection of evidence about views on longtermism, time discounting, population ethics, significance of suffering vs happiness, etc. among non-EAs](https://forum.effectivealtruism.org/posts/EMKf4Gyee7BsY2RP8/michaela-s-shortform?commentId=GgW24uSGwTvwP7Hwr), *Effective Altruism Forum*, May 10.  \n*Many additional resources with some relevance to this topic*.\n\nArrhenius, Gustaf, Jesper Ryberg & Torbjörn Tännsjö (2006) [The repugnant conclusion](https://plato.stanford.edu/entries/repugnant-conclusion/), *Stanford Encyclopedia of Philosophy*, February 16 (updated 16 January 2017).\n\nChappell, Richard Y., Darius Meissner & William MacAskill (2022) [Population ethics](https://www.utilitarianism.net/population-ethics), *An Introduction to Utilitarianism*.\n\nGreaves, Hilary (2017) [Population axiology](http://doi.org/10.1111/phc3.12442), *Philosophy Compass*, vol. 12, pp. 1–15.\n\nKarnofsky, Holden (2022) [Debating myself on whether “extra lives lived” are as good as “deaths prevented”](https://www.cold-takes.com/debating-myself-on-whether-extra-lives-lived-are-as-good-as-deaths-prevented/), *Cold Takes*, March 29.  \n*An engaging yet rigorous introduction to some of the key issues in population ethics in the form of an imaginary dialogue between a proponent and an opponent of total utilitarianism.*\n\nRelated entries\n---------------\n\n[axiology](https://forum.effectivealtruism.org/tag/axiology) | [ethics of existential risk](https://forum.effectivealtruism.org/tag/ethics-of-existential-risk) | [Jeff McMahan](https://forum.effectivealtruism.org/tag/jeff-mcmahan) | [Derek Parfit](https://forum.effectivealtruism.org/tag/derek-parfit) | [person-affecting views](https://forum.effectivealtruism.org/topics/person-affecting-views) | [repugnant conclusion](https://forum.effectivealtruism.org/topics/repugnant-conclusion) | [total view](https://forum.effectivealtruism.org/topics/total-view)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "RysvCQBYTie5u62rH",
    "name": "Civilizational collapse",
    "core": false,
    "slug": "civilizational-collapse",
    "oldSlugs": [
      "civilizational-collapse-and-recovery",
      "civilizational-collapse-and-recovery",
      "civilizational-collapse-and-recovery",
      "civilizational-collapse-and-recovery"
    ],
    "postCount": 72,
    "description": {
      "markdown": "A **civilizational collapse** (sometimes called a **social collapse** or a **societal collapse**) is a  drastic decrease in human population size, or in political, economic or social complexity, across essentially the entire world, for an extended period of time. **Civilizational resilience** is humanity's capacity to resist, or recover from, civilizational collapse.\n\nIn [Toby Ord](https://forum.effectivealtruism.org/tag/toby-ord)'s typology, unrecoverable civilizational collapse constitutes one of the three main types of [existential catastrophe](https://forum.effectivealtruism.org/tag/existential-catastrophe-1).^[\\[1\\]](#fnekxxb4sy0yl)^\n\nEvaluation\n----------\n\n[80,000 Hours](https://forum.effectivealtruism.org/tag/80-000-hours) rates civilizational resilience a \"potential highest priority area\": an issue that, if more thoroughly examined, could rank as a top global challenge.^[\\[2\\]](#fnt5imvgok2yd)^\n\nFurther reading\n---------------\n\nAird, Michael (2020a) [Collection of sources that seem very relevant to the topic of civilizational collapse and/or recovery](https://forum.effectivealtruism.org/posts/EMKf4Gyee7BsY2RP8/michaela-s-shortform?commentId=92ejaz5s5ehAMNH4N), *Effective Altruism Forum*, February 24.  \n*Many additional resources on this topic.*\n\nAird, Michael (2020b) [Civilization re-emerging after a catastrophic collapse](https://forum.effectivealtruism.org/posts/Z8A5qGFbjdehRrkf3/civilization-re-emerging-after-a-catastrophic-collapse), *Effective Altruism Forum*, June 27.\n\nDenkenberger, David & Jeffrey Ladish (2019) [Civilizational collapse: scenarios, prevention, responses](https://www.youtube.com/watch?v=gbYWHBoQ9gM&), *Foresight Institute*, June 24.\n\nHanson, Robin (2008) [Catastrophe, social collapse, and human extinction](https://en.wikipedia.org/wiki/Special:BookSources/9780198570509), in Nick Bostrom & Milan M. Ćirković (eds.) *Global Catastrophic Risks*, Oxford: Oxford University Press, pp. 363–377.\n\nJebari, Karim (2019) [Civilization re-emerging after a catastrophic collapse](https://www.youtube.com/watch?v=Zhx5ieX-HPY), *EAGxNordics*, April 7.\n\nLadish, Jeffrey (2020) [Update on civilizational collapse research](https://forum.effectivealtruism.org/posts/wGqSqWLTwDrytxeER/update-on-civilizational-collapse-research), *Effective Altruism Forum*, February 10.\n\nManheim, David (2020) [A (very) short history of the collapse of civilizations, and why it matters](https://forum.effectivealtruism.org/posts/2gXeP5nS23ShjXqEh/a-very-short-history-of-the-collapse-of-civilizations-and), *Effective Altruism Forum*, August 30.\n\nRodriguez, Luisa (2020) [What is the likelihood that civilizational collapse would directly lead to human extinction (within decades)?](https://forum.effectivealtruism.org/posts/GsjmufaebreiaivF7/what-is-the-likelihood-that-civilizational-collapse-would), *Effective Altruism Forum*, December 24, 2020.\n\nTainter, Joseph A. (1989) [*The Collapse of Complex Societies*](https://en.wikipedia.org/wiki/Special:BookSources/9780521340922), Cambridge: Cambridge University Press.\n\nWiblin, Robert & Arden Koehler (2020) [Mark Lynas on climate change, societal collapse & nuclear energy](https://80000hours.org/podcast/episodes/mark-lynas-climate-change-nuclear-energy/), *80,000 Hours*, August 20.\n\nWiblin, Robert & Keiran Harris (2021) [Luisa Rodriguez on why global catastrophes seem unlikely to kill us all](https://80000hours.org/podcast/episodes/luisa-rodriguez-why-global-catastrophes-seem-unlikely-to-kill-us-all/), *80,000 Hours*, November 19.\n\nRelated entries\n---------------\n\n[broad vs. narrow interventions](https://forum.effectivealtruism.org/tag/broad-vs-narrow-interventions) | [dystopia](https://forum.effectivealtruism.org/tag/dystopia) | [existential catastrophe](https://forum.effectivealtruism.org/tag/existential-catastrophe-1) | [existential risk](https://forum.effectivealtruism.org/tag/existential-risk) | [existential risk factor](https://forum.effectivealtruism.org/tag/existential-risk-factor) | [movement collapse](https://forum.effectivealtruism.org/topics/movement-collapse)\n\n1.  ^**[^](#fnrefekxxb4sy0yl)**^\n    \n    Ord, Toby (2020) [*The Precipice: Existential Risk and the Future of Humanity*](https://en.wikipedia.org/wiki/Special:BookSources/1526600218), London: Bloomsbury Publishing, fig. 5.2.\n    \n2.  ^**[^](#fnreft5imvgok2yd)**^\n    \n    80,000 Hours (2022) [Our current list of pressing world problems](https://80000hours.org/problem-profiles/), *80,000 Hours*."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "hC88x9hNZ7cJYLu8q",
    "name": "History",
    "core": false,
    "slug": "history",
    "oldSlugs": null,
    "postCount": 95,
    "description": {
      "markdown": "The **history** tag is for posts that are strongly focused on historical events or trends (rather than just mentioning these things briefly), or that discuss or make heavy use of historical research methods.\n\nFurther reading\n---------------\n\nAird, Michael (2021) [Collection of EA-associated historical case study research](https://forum.effectivealtruism.org/posts/EMKf4Gyee7BsY2RP8/michaela-s-shortform?commentId=X3EvzxGWeWNRs7Lmv), *Effective Altruism Forum*, June 4.\n\nRelated entries\n---------------\n\n[history of effective altruism](https://forum.effectivealtruism.org/tag/history-of-effective-altruism) | [history of existential risk](https://forum.effectivealtruism.org/tag/history-of-existential-risk) | [history of philanthropy](https://forum.effectivealtruism.org/tag/history-of-philanthropy)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "MDSk2m6ZtzqdJaS36",
    "name": "Political polarization",
    "core": false,
    "slug": "political-polarization",
    "oldSlugs": [
      "political-polarisation",
      "political-polarization"
    ],
    "postCount": 42,
    "description": {
      "markdown": "Further reading\n---------------\n\nAird, Michael (2020) [Collection of EA analyses of political polarisation](https://forum.effectivealtruism.org/posts/EMKf4Gyee7BsY2RP8/michaela-s-shortform?commentId=phSnJ3i4WaZsiy5R2), *Effective Altruism Forum*, June 26.  \n*Many additional resources on this topic.*\n\nRelated entries\n---------------\n\n[electoral politics](https://forum.effectivealtruism.org/topics/electoral-politics) | [United States](https://forum.effectivealtruism.org/topics/united-states)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "bieLqkSR2i3R29xbL",
    "name": "Moral advocacy",
    "core": false,
    "slug": "moral-advocacy",
    "oldSlugs": [
      "moral-advocacy-values-spreading",
      "moral-advocacy-values-spreading"
    ],
    "postCount": 29,
    "description": {
      "markdown": "**Moral advocacy** is the set of activities aimed at promoting moral values considered to be beneficial.\n\nFurther reading\n---------------\n\nBaumann, Tobias (2017) [Arguments for and against moral advocacy](https://prioritizationresearch.com/arguments-for-and-against-moral-advocacy/), *Cause Prioritization Research*, July 5.\n\nChristiano, Paul (2013) [Against moral advocacy](https://rationalaltruist.com/2013/06/13/against-moral-advocacy/), *Rational altruist*, June 13.\n\nTomasik, Brian (2013) [Values spreading is often more important than extinction risk](https://reducing-suffering.org/values-spreading-often-important-extinction-risk/), *Essays on reducing suffering*, April 6.\n\nRelated entries\n---------------\n\n[moral circle expansion](https://forum.effectivealtruism.org/tag/moral-circle-expansion-1) | [moral trade](https://forum.effectivealtruism.org/tag/moral-trade) | [speeding up development](https://forum.effectivealtruism.org/tag/speeding-up-development) | [value drift](https://forum.effectivealtruism.org/tag/value-drift) | [value lock-in](https://forum.effectivealtruism.org/tag/value-lock-in)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "ECCK2u8HCjnCjPzyf",
    "name": "Moral circle expansion",
    "core": false,
    "slug": "moral-circle-expansion-1",
    "oldSlugs": [
      "moral-circles",
      "moral-circle-expansion-1"
    ],
    "postCount": 75,
    "description": {
      "markdown": "**Moral circle expansion** is the attempt to expand the perceived boundaries of the category of [moral patients](https://forum.effectivealtruism.org/tag/moral-patienthood). It has been proposed as a priority cause area^[\\[1\\]](#fnl04gtgd0x2a)^ and as a heuristic for discovering [cause X](https://forum.effectivealtruism.org/tag/cause-x).^[\\[2\\]](#fnjqqn2weh7ib)^\n\nEvaluation\n----------\n\n[80,000 Hours](https://forum.effectivealtruism.org/tag/80-000-hours) rates promoting positive values a \"potential highest priority area\": an issue that, if more thoroughly examined, could rank as a top global challenge.^[\\[3\\]](#fnvsryrrk18kb)^\n\nFurther reading\n---------------\n\nAird, Michael (2020) [Collection of sources relevant to moral circles, moral boundaries, or their expansion](https://forum.effectivealtruism.org/posts/EMKf4Gyee7BsY2RP8/michaela-s-shortform?commentId=s2MMawmjXZsPPhPF2), *Effective Altruism Forum*, May 7.  \n*Many additional resources on this topic or related topics.*\n\nAnthis, Jacy Reese (2018) [Why I prioritize moral circle expansion over artificial intelligence alignment](https://forum.effectivealtruism.org/posts/BY8gXSpGijypbGitT/why-i-prioritize-moral-circle-expansion-over-artificial), *Effective Altruism Forum*, February 20.\n\nKarnofsky, Holden (2017) [Radical empathy](https://www.openphilanthropy.org/blog/radical-empathy), *Open Philanthropy*, February 16.\n\nLaham, Simon M. (2009) [Expanding the moral circle: inclusion and exclusion mindsets and the circle of moral regard](http://doi.org/10.1016/j.jesp.2008.08.012), *Journal of Experimental Social Psychology*, vol. 45, pp. 250–253.\n\nMacAskill, W. & Meissner, D. (2020) [The expanding moral circle](https://www.utilitarianism.net/utilitarianism-and-practical-ethics#the-expanding-moral-circle), in [*Introduction to Utilitarianism*](https://www.utilitarianism.net/).\n\nSinger, Peter (1981) [*The Expanding Circle: Ethics and Sociobiology*](https://en.wikipedia.org/wiki/The_Expanding_Circle), New York: Farrar, Straus and Giroux.\n\nSlinsky, Grue (2019) [The moral circle is not a circle](https://forum.effectivealtruism.org/posts/EXi2Nb9RQWjztgJSm/the-moral-circle-is-not-a-circle), *Effective Altruism Forum*, August 14.\n\nRelated entries\n---------------\n\n[moral advocacy](https://forum.effectivealtruism.org/tag/moral-advocacy) | [moral patienthood](https://forum.effectivealtruism.org/tag/moral-patienthood) | [moral weight](https://forum.effectivealtruism.org/tag/moral-weight) | [speciesism](https://forum.effectivealtruism.org/tag/speciesism) | [value lock-in](https://forum.effectivealtruism.org/tag/value-lock-in)\n\n1.  ^**[^](#fnrefl04gtgd0x2a)**^\n    \n    Anthis, Jacy Reese & Eze Paez (2021) [Moral circle expansion: A promising strategy to impact the far future](http://doi.org/10.1016/j.futures.2021.102756), *Futures*, vol. 130, 104743.\n    \n2.  ^**[^](#fnrefjqqn2weh7ib)**^\n    \n    Vaughan, Kerry (2016) [Three heuristics for finding cause X](https://www.effectivealtruism.org/articles/three-heuristics-for-finding-cause-x/), *Effective Altruism*, November 4.\n    \n3.  ^**[^](#fnrefvsryrrk18kb)**^\n    \n    80,000 Hours (2022) [Our current list of pressing world problems](https://80000hours.org/problem-profiles/), *80,000 Hours*."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "LY5WnHuF8QjvhksGM",
    "name": "Long reflection",
    "core": false,
    "slug": "long-reflection",
    "oldSlugs": [
      "long-reflection"
    ],
    "postCount": 8,
    "description": {
      "markdown": "The **long reflection** is a hypothesized period of time during which humanity works out how best to realize its long-term potential.\n\nSome effective altruists, including [Toby Ord](https://forum.effectivealtruism.org/tag/toby-ord) and [William MacAskill](https://forum.effectivealtruism.org/tag/william-macaskill), have argued that, if humanity succeeds in eliminating [existential risk](https://forum.effectivealtruism.org/tag/existential-risk) or reducing it to acceptable levels, it should not immediately embark on an ambitious and potentially irreversible project of arranging the [universe's resources](https://forum.effectivealtruism.org/tag/universe-s-resources) in accordance to its values, but ought instead to spend considerable time— \"centuries (or more)\";^[\\[1\\]](#fnipthilteu8)^ \"perhaps tens of thousands of years\";^[\\[2\\]](#fnw82ait03vnf)^ \"thousands or millions of years\";^[\\[3\\]](#fnr3gb671cl2)^ \"\\[p\\]erhaps... a million years\"^[\\[4\\]](#fn7t0vu6w86yd)^—figuring out what is in fact of value. The long reflection may thus be seen as an intermediate stage in a rational long-term human developmental trajectory, following an initial stage of [existential security](https://forum.effectivealtruism.org/tag/existential-security) when existential risk is drastically reduced and followed by a final stage when humanity's potential is fully realized.^[\\[1\\]](#fnipthilteu8)^\n\nCriticism\n---------\n\nThe idea of a long reflection has been criticized on the grounds that virtually eliminating all existential risk will almost certainly require taking a variety of large-scale, irreversible decisions—related to [space colonization](https://forum.effectivealtruism.org/tag/space-colonization), [global governance](https://forum.effectivealtruism.org/tag/global-governance), [cognitive enhancement](https://forum.effectivealtruism.org/tag/cognitive-enhancement), and so on—which are precisely the decisions meant to be discussed during the long reflection.^[\\[5\\]](#fnd395d9czg1k)^^[\\[6\\]](#fn8t05x5yy03)^ Since there are pervasive and inescapable tradeoffs between reducing existential risk and retaining moral option value, it may be argued that it does not make sense to frame humanity's long-term strategic picture as one consisting of two distinct stages, with one taking precedence over the other.\n\nFurther reading\n---------------\n\nAird, Michael (2020) [Collection of sources that are highly relevant to the idea of the Long Reflection](https://forum.effectivealtruism.org/posts/H2zno3ggRJaph9P6c/quotes-about-the-long-reflection?commentId=z2ybSC353mPHpCjbn), *Effective Altruism Forum*, June 20.  \n*Many additional resources on this topic.*\n\nWiblin, Robert & Keiran Harris (2018) [Our descendants will probably see us as moral monsters. what should we do about that?](https://80000hours.org/podcast/episodes/will-macaskill-moral-philosophy/), *80,000 Hours*, January 19.  \n*Interview with William MacAskill about the long reflection and other topics.*\n\nRelated entries\n---------------\n\n[dystopia](https://forum.effectivealtruism.org/tag/dystopia) | [existential risk](https://forum.effectivealtruism.org/tag/existential-risk) | [existential security](https://forum.effectivealtruism.org/tag/existential-security) | [long-term future](https://forum.effectivealtruism.org/tag/long-term-future) | [longtermism](https://forum.effectivealtruism.org/tag/longtermism) | [longtermist institutional reform](https://forum.effectivealtruism.org/topics/longtermist-institutional-reform) | [moral uncertainty](https://forum.effectivealtruism.org/tag/moral-uncertainty) | [normative ethics](https://forum.effectivealtruism.org/tag/normative-ethics) | [value lock-in](https://forum.effectivealtruism.org/tag/value-lock-in)\n\n1.  ^**[^](#fnrefipthilteu8)**^\n    \n    Ord, Toby (2020) [*The Precipice: Existential Risk and the Future of Humanity*](https://en.wikipedia.org/wiki/Special:BookSources/1526600218), London: Bloomsbury Publishing.\n    \n2.  ^**[^](#fnrefw82ait03vnf)**^\n    \n    Greaves, Hilary *et al.* (2019) [A research agenda for the Global Priorities Institute](https://globalprioritiesinstitute.org/wp-content/uploads/2017/12/gpi-research-agenda.pdf), Oxford.\n    \n3.  ^**[^](#fnrefr3gb671cl2)**^\n    \n    Dai, Wei (2019) [The argument from philosophical difficulty](https://www.lesswrong.com/posts/w6d7XBCegc96kz4n3/the-argument-from-philosophical-difficulty), *LessWrong*, February 9.\n    \n4.  ^**[^](#fnref7t0vu6w86yd)**^\n    \n    William MacAskill, in Perry, Lucas (2018) [AI alignment podcast: moral uncertainty and the path to AI alignment with William MacAskill](https://futureoflife.org/2018/09/17/moral-uncertainty-and-the-path-to-ai-alignment-with-william-macaskill/), *AI Alignment podcast*, September 17.\n    \n5.  ^**[^](#fnrefd395d9czg1k)**^\n    \n    Stocker, Felix (2020) [Reflecting on the long reflection](https://www.felixstocker.com/blog/reflecting-on-the-long-reflection), *Felix Stocker’s Blog*, August 14.\n    \n6.  ^**[^](#fnref8t05x5yy03)**^\n    \n    Hanson, Robin (2021) [‘Long reflection’ is crazy bad idea](https://www.overcomingbias.com/2021/10/long-reflection-is-crazy-bad-idea.html), *Overcoming Bias*, October 20."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "75HnrKT9FNWKxxtPY",
    "name": "Collections and resources",
    "core": false,
    "slug": "collections-and-resources",
    "oldSlugs": [
      "collections-and-resources"
    ],
    "postCount": 142,
    "description": {
      "markdown": "The **collections and resources** tag covers posts containing lists of EA-related resources. These could be external links or curated Forum content.\n\nExternal links\n--------------\n\n[EA Hub / Resources](https://resources.eahub.org/). Community-related collections and resources."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "eTLv8KzwBGcDip9Wi",
    "name": "Open thread",
    "core": false,
    "slug": "open-thread",
    "oldSlugs": [
      "open-thread",
      "open-thread",
      "jual-obat-penggugur-kandungan-081215505068-cara-menggugurkan"
    ],
    "postCount": 75,
    "description": {
      "markdown": "Use the **open thread** tag for open/welcome and progress threads. That's all!"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "quqf8tyFLagSDErLo",
    "name": "Pain and suffering",
    "core": false,
    "slug": "pain-and-suffering",
    "oldSlugs": [
      "pain-and-suffering"
    ],
    "postCount": 91,
    "description": {
      "markdown": "The **pain and suffering** tag is for posts which discuss ways to measure these phenomena, or issues that are notable for causing especially intense pain and suffering (even if they aren't especially large in scale). \n\nGiven that EA tends to focus on the number of people affected by a problem, we may not notice when a problem affects relatively few people but is extremely bad for those it affects, leading to high overall impact. Posts with this tag may push against this tendency.\n\n**Content note:** Posts with this tag may contain disconcerting or frightening content.\n\n## Related entries\n\n[mental health](https://forum.effectivealtruism.org/tag/mental-health) | [sentience](https://forum.effectivealtruism.org/tag/sentience-1)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "TteDwtS2DckL4kLpT",
    "name": "Farmed animal welfare",
    "core": true,
    "slug": "farmed-animal-welfare",
    "oldSlugs": [
      "farmed-animal-welfare"
    ],
    "postCount": 423,
    "description": {
      "markdown": "Over 60 billion land-dwelling animals are killed each year. The overwhelming majority of these animals are raised in factory farms, where conditions can involve \"intense confinement, inhibition of natural behaviors, untreated health issues, and numerous other causes of suffering\".^[\\[1\\]](#fnh0vz5bt9kl4)^ It’s difficult to measure and quantify the welfare of farmed animals, but some have suggested that the lives of many farmed animals are net-negative - that is, their negative experiences outweigh their positive experiences.^[\\[2\\]](#fnpkr1bv7smx)^^[\\[3\\]](#fn1mseo3stqry)^\n\nThere are a number of promising ways to improve farmed animal welfare.^[\\[4\\]](#fn4xppk7sl62x)^ One approach is reduce the number of farm animals, by persuading the public to reduce consumption of animal products or by facilitating the development of [animal product alternatives](https://forum.effectivealtruism.org/tag/animal-product-alternatives).^[\\[5\\]](#fn78cen6iwjho)^ Another approach is to reduce the suffering farm animals experience, by improving the conditions under which these animals live. [Open Philanthropy](https://forum.effectivealtruism.org/tag/open-philanthropy) estimates that [corporate cage-free campaigns](https://forum.effectivealtruism.org/tag/corporate-cage-free-campaigns) have spared about 250 hens a year of confinement in a cage per each dollar spent. They also claim that the cost-effectiveness of these campaigns is higher than any of the other approaches they have investigated.^[\\[6\\]](#fnts3v8zlaraj)^\n\nFurther reading\n---------------\n\nMacAskill, William & Darius Meissner (2020) [Cause prioritization: Farm animal welfare](https://www.utilitarianism.net/acting-on-utilitarianism#farm-animal-welfare), in 'Acting on utilitarianism', *Utilitarianism*.\n\nWhittlestone, Jess (2017) [Animal welfare](https://www.effectivealtruism.org/articles/cause-profile-animal-welfare/), *Effective Altruism*, November 16.\n\nRelated entries\n---------------\n\n[animal product alternatives](https://forum.effectivealtruism.org/tag/animal-product-alternatives) | [cultured meat](https://forum.effectivealtruism.org/tag/cultured-meat) | [dietary change](https://forum.effectivealtruism.org/tag/dietary-change) | [global health and wellbeing](https://forum.effectivealtruism.org/topics/global-health-and-wellbeing)\n\n1.  ^**[^](#fnrefh0vz5bt9kl4)**^\n    \n    Reese, Jacy (2016) [Why animals matter for effective altruism](https://forum.effectivealtruism.org/posts/ch5fq73AFn2Q72AMQ/why-animals-matter-for-effective-altruism), *Effective Altruism Forum*, August 22.\n    \n2.  ^**[^](#fnrefpkr1bv7smx)**^\n    \n    Tomasik, Brian (2007) [How much direct suffering is caused by various animal foods?](http://reducing-suffering.org/how-much-direct-suffering-is-caused-by-various-animal-foods/), *Essays on Reducing Suffering*.\n    \n3.  ^**[^](#fnref1mseo3stqry)**^\n    \n    Norwood, F. Bailey & Jayson L. Lusk (2011) [*Compassion, by the Pound: The Economics of Farm Animal Welfare*](http://doi.org/10.1093/acprof:osobl/9780199551163.001.0001), New York: Oxford University Press.\n    \n4.  ^**[^](#fnref4xppk7sl62x)**^\n    \n    Open Philanthropy (2013) [Treatment of animals in industrial agriculture](https://www.openphilanthropy.org/research/cause-reports/treatment-animals-industrial-agriculture), *Open Philanthropy*, September.\n    \n5.  ^**[^](#fnref78cen6iwjho)**^\n    \n    Open Philanthropy (2015) [Animal product alternatives](https://www.openphilanthropy.org/research/cause-reports/animal-product-alternatives), *Open Philanthropy*, December.\n    \n6.  ^**[^](#fnrefts3v8zlaraj)**^\n    \n    Bollard, Lewis (2016) [Initial grants to support corporate cage-free reforms](https://www.openphilanthropy.org/blog/initial-grants-support-corporate-cage-free-reforms), *Open Philanthropy*, March 31."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "MWwcDEyw2ThZDReGF",
    "name": "Wild animal welfare",
    "core": true,
    "slug": "wild-animal-welfare",
    "oldSlugs": [
      "wild-animal-welfare"
    ],
    "postCount": 197,
    "description": {
      "markdown": "**Wild animal welfare** is the welfare of non-human animals under natural conditions.\n\nWild animals outnumber animals in [factory farms](https://forum.effectivealtruism.org/topics/farmed-animal-welfare) by several orders of magnitude.[^\\[1\\]^](https://forum.effectivealtruism.org/topics/wild-animal-welfare#fn1q6akonc3jp) In spite of this, the welfare of animals in the wild has received very limited attention. A comprehensive online bibliography in this area lists about fifty publications, most of which are published online or in relatively unknown journals.[^\\[2\\]^](https://forum.effectivealtruism.org/topics/wild-animal-welfare#fncihea1eezql) Wild animal welfare is thus a highly [important](https://forum.effectivealtruism.org/topics/importance) and [neglected](https://forum.effectivealtruism.org/topics/neglectedness) focus area.[^\\[3\\]^](https://forum.effectivealtruism.org/topics/wild-animal-welfare#fnq7q5043tg1f)\n\nAs of yet, there have been no serious attempts to improve wild animal welfare, perhaps because this appears to have low [tractability.](https://forum.effectivealtruism.org/topics/tractability) Some in the effective altruism community, however, have argued that tractability is higher than it might first appear.[^\\[4\\]^](https://forum.effectivealtruism.org/topics/wild-animal-welfare#fnkxmyct37co) Proposed interventions include [advocacy](https://forum.effectivealtruism.org/topics/moral-advocacy) for wild animals and [research](https://forum.effectivealtruism.org/topics/research) into potential solutions.[^\\[5\\]^](https://forum.effectivealtruism.org/topics/wild-animal-welfare#fnovlglai0qu)\n\nFurther reading\n---------------\n\nAnimal Charity Evaluators (2022) [Why wild animals?](https://animalcharityevaluators.org/donation-advice/why-wild-animals/), *Animal Charity Evaluators*, February.  \n*An outline of the main considerations for prioritizing wild animal welfare.*\n\nFaria, Catia (2022) [*Animal Ethics in the Wild: Wild Animal Suffering and Intervention in Nature*](https://en.wikipedia.org/wiki/Special:BookSources/978-1-00-910063-2), Cambridge: Cambridge University Press.\n\nFrancini, Louis *et al.* (2020) [Timeline of wild-animal suffering](https://timelines.issarice.com/wiki/Timeline_of_wild-animal_suffering), *Timelines Wiki*, June 16.\n\nMatthews, Dylan (2021) [The wild frontier of animal welfare](https://www.vox.com/the-highlight/22325435/animal-welfare-wild-animals-movement), *Vox*, April 21.  \n*An engaging overview of recent developments in the field of wild animal welfare, with a focus on Wild Animal Initiative.*\n\nMcMahan, Jeff (2010) [The meat eaters](http://opinionator.blogs.nytimes.com/2010/09/19/the-meat-eaters/), *The New York Times*, September 19.\n\nNg, Yew-Kwang (1995) [Towards welfare biology: evolutionary economics of animal consciousness and suffering](http://www.stafforini.com/txt/Ng%20-%20Towards%20welfare%20biology.pdf), *Biology and Philosophy*, vol. 10, pp. 255–285.\n\nSpurgeon, Jamie (2022) [Helping wild animals](https://www.givingwhatwecan.org/cause-areas/animal-welfare/wild-animals/), *Giving What We Can*.\n\nStafforini, Pablo (2016) [Wild animal welfare: a bibliography](http://www.stafforini.com/blog/wild-animal-suffering-a-bibliography/), *Pablo Stafforini's Blog*, June 6 (updated 28 April 2021).  \n*A somewhat outdated bibliography of publications on wild animal welfare.*\n\nExternal links\n--------------\n\n[Wild animal suffering video course](https://www.animal-ethics.org/wild-animal-suffering-video-course/). An online course about wild animal welfare, by [Animal Ethics](https://forum.effectivealtruism.org/tag/animal-ethics).\n\nRelated entries\n---------------\n\n[animal welfare](https://forum.effectivealtruism.org/topics/animal-welfare-1) | [crustacean welfare](https://forum.effectivealtruism.org/topics/crustacean-welfare) | [farmed animal welfare](https://forum.effectivealtruism.org/topics/farmed-animal-welfare) | [fish welfare](https://forum.effectivealtruism.org/topics/fish-welfare) | [invertebrate welfare](https://forum.effectivealtruism.org/tag/invertebrate-welfare) | [moral circle expansion](https://forum.effectivealtruism.org/topics/moral-circle-expansion-1) | [welfare biology](https://forum.effectivealtruism.org/tag/welfare-biology)\n\n1.  ^**[^](#fnref1q6akonc3jp)**^\n    \n    Tomasik, Brian (2019) [How many wild animals are there?](http://reducing-suffering.org/how-many-wild-animals-are-there/), *Brian Tomasik's Blog*, August 7.\n    \n2.  ^**[^](#fnrefcihea1eezql)**^\n    \n    Stafforini, Pablo (2016) [Wild animal welfare: a bibliography](http://www.stafforini.com/blog/wild-animal-suffering-a-bibliography/), *Pablo Stafforini's Blog*, June 6.\n    \n3.  ^**[^](#fnrefq7q5043tg1f)**^\n    \n    Tomasik, Brian (2015) [The importance of wild-animal suffering](https://foundational-research.org/the-importance-of-wild-animal-suffering/), *Relations*, vol. 3, pp. 133-152.\n    \n4.  ^**[^](#fnrefkxmyct37co)**^\n    \n    Dickens, Michael (2016) [The myth that reducing wild animal suffering is intractable](http://mdickens.me/2016/04/22/the_myth_that_reducing_wild_animal_suffering_is_intractable/), *Philosophical Multicore*, April 22.\n    \n5.  ^**[^](#fnrefovlglai0qu)**^\n    \n    Knutsson, Simon (2016) [Reducing suffering among invertebrates such as insects](https://ea-foundation.org/files/reducing-suffering-invertebrates.pdf), policy paper by Sentience Politics."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "EHLmbEmJ2Qd5WfwTb",
    "name": "Building effective altruism",
    "core": true,
    "slug": "building-effective-altruism-1",
    "oldSlugs": [
      "movement-strategy",
      "movement-building",
      "movement-building"
    ],
    "postCount": 534,
    "description": {
      "markdown": "**Building effective altruism** refers to the family of interventions aimed at growing, shaping or otherwise improving [effective altruism](https://forum.effectivealtruism.org/tag/effective-altruism) as an intellectual community.\n\nExamples of movement building include starting student groups, writing articles, and organizing social gatherings for people interested in effective altruism.\n\nThere is a relevant distinction between work that increases outside awareness of the movement and work that increases its favorability. Awareness and favorability are both limiting factors for movement growth, since a person would need to both know what the movement is and have a positive impression of it to want to become involved.\n\nIdeally, movement-building work would increase both of these factors, but there is sometimes a trade-off between the two. For instance, one way that social movements often draw attention is by generating controversy, which tends to decrease favorability.\n\nIn addition, excessive movement growth may also decrease the effectiveness of the people within the movement.\n\nEvaluation\n----------\n\n[80,000 Hours](https://forum.effectivealtruism.org/tag/80-000-hours) rates building effective altruism a \"highest priority area\": a problem at the top of their ranking of global issues assessed by [importance, tractability and neglectedness](https://forum.effectivealtruism.org/tag/itn-framework-1).^[\\[1\\]](#fnsrtlc2g8s8h)^\n\nFurther reading\n---------------\n\nDuda, Roman (2018) [Building effective altruism](https://80000hours.org/problem-profiles/promoting-effective-altruism/), *80,000 Hours*, March (updated July 2020).\n\nWhittlestone, Jess (2017) [Building an effective altruism community](https://www.effectivealtruism.org/articles/cause-profile-building-an-effective-altruism-community/), *Effective Altruism*, November 16.\n\nRelated entries\n---------------\n\n[altruistic coordination](https://forum.effectivealtruism.org/tag/altruistic-coordination) | [competitive debating](https://forum.effectivealtruism.org/tag/competitive-debating) | [coworking spaces](https://forum.effectivealtruism.org/tag/coworking-spaces) | [effective altruism education](https://forum.effectivealtruism.org/tag/effective-altruism-education) | [global outreach](https://forum.effectivealtruism.org/tag/global-outreach) | [moral trade](https://forum.effectivealtruism.org/tag/moral-trade) | [movement collapse](https://forum.effectivealtruism.org/tag/movement-collapse) | [network building](https://forum.effectivealtruism.org/tag/network-building) | [retreats](https://forum.effectivealtruism.org/tag/retreats) | [social and intellectual movements](https://forum.effectivealtruism.org/tag/social-and-intellectual-movements) | [value of movement growth](https://forum.effectivealtruism.org/tag/value-of-movement-growth)\n\n1.  ^**[^](#fnrefsrtlc2g8s8h)**^\n    \n    80,000 Hours (2021) [Our current list of the most important world problems](https://80000hours.org/problem-profiles/), *80,000 Hours*."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "9tdEph2aXYqwzmmRm",
    "name": "Effective Altruism Forum",
    "core": false,
    "slug": "effective-altruism-forum-1",
    "oldSlugs": [
      "ea-forum-meta",
      "ea-forum-meta",
      "ea-forum-meta",
      "effective-altruism-forum-1",
      "independent-russain-escort-in-bangalore-understand-the-needs"
    ],
    "postCount": 175,
    "description": {
      "markdown": "The **Effective Altruism Forum** (often spelled **EA Forum**) is a community blog and forum focused on discussion of issues related to [effective altruism](https://forum.effectivealtruism.org/tag/effective-altruism). It is supported by the [Centre for Effective Altruism](https://forum.effectivealtruism.org/tag/centre-for-effective-altruism-1).\n\nHistory\n-------\n\nThe EA Forum was launched in 2014, with seed content from a group blog that ran between 2012 and 2014, and published posts by early members of the effective altruism community.^[\\[1\\]](#fn4qjb2u1mex8)^^[\\[2\\]](#fnb78pdczee3m)^ It was relaunched in 2018 with a new codebase and a dedicated team.^[\\[3\\]](#fn3bdj2ptwc6q)^\n\nFurther reading\n---------------\n\nDai, Wei (2019) [Forum participation as a research strategy](https://www.lesswrong.com/posts/rBkZvbGDQZhEymReM/forum-participation-as-a-research-strategy), *LessWrong*, July 30.\n\nVaintrob, Lizka (2022) [How to use the Forum](https://forum.effectivealtruism.org/s/s5zDhfyRPvrpeuRf8), *Effective Altruism Forum*, April 29.\n\nExternal links\n--------------\n\n[EA Forum Digest](https://effectivealtruism.us8.list-manage.com/subscribe?u=52b028e7f799cca137ef74763&id=7457c7ff3e). A weekly email with selected EA Forum posts from the previous week.\n\n[EA Forum Archives](https://forum.effectivealtruism.org/users/ea-forum-archives). EA Forum account used to post archival material.\n\nRelated entries\n---------------\n\n[Centre for Effective Altruism](https://forum.effectivealtruism.org/tag/centre-for-effective-altruism-1) | [effective altruism](https://forum.effectivealtruism.org/tag/effective-altruism)\n\n1.  ^**[^](#fnref4qjb2u1mex8)**^\n    \n    Effective Altruism (2012) [About](http://web.archive.org/web/20130706025208/http://www.effective-altruism.com/about), *Effective Altruism*.\n    \n2.  ^**[^](#fnrefb78pdczee3m)**^\n    \n    Hokama, Rhema (2014) [10 reasons to explore the new Effective Altruism Forum](https://www.thelifeyoucansave.org/blog/10-reasons-to-explore-the-new-effective-altruism-forum/), *The Life You Can Save*, October 5.\n    \n3.  ^**[^](#fnref3bdj2ptwc6q)**^\n    \n    Gertler, Aaron (2018) [Welcome to the New Forum!](https://forum.effectivealtruism.org/posts/h26Kx7uGfQfNewi7d/welcome-to-the-new-forum), *Effective Altruism Forum*, November 7."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "8WsPHtGnsPSvxeeST",
    "name": "Personal development",
    "core": false,
    "slug": "personal-development",
    "oldSlugs": [
      "personal-development"
    ],
    "postCount": 171,
    "description": {
      "markdown": "**Personal development** collects posts about learning skills, being more productive, etc.\n\nRelated entries\n---------------\n\n[practical](https://forum.effectivealtruism.org/topics/practical)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "eRwcqwQ6PaQp59Yct",
    "name": "Less-discussed causes",
    "core": false,
    "slug": "less-discussed-causes",
    "oldSlugs": [
      "unusual-causes",
      "less-discussed-causes"
    ],
    "postCount": 145,
    "description": {
      "markdown": "**Less-discussed causes** are causes that receive comparatively less attention within the effective altruism community.\n\nFurther reading\n---------------\n\nSempere, Nuño (2020) [Big list of cause candidates](https://forum.effectivealtruism.org/posts/SCqRu6shoa8ySvRAa/big-list-of-cause-candidates), *Effective Altruism Forum*, December 25.\n\nRelated entries\n---------------\n\n[Cause X](https://forum.effectivealtruism.org/tag/cause-x) | [impact assessment](https://forum.effectivealtruism.org/tag/impact-assessment)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "o2FZJuxRGHKvXM6HS",
    "name": "Statistics",
    "core": false,
    "slug": "statistics",
    "oldSlugs": [
      "statistical-methods",
      "statistical-methods"
    ],
    "postCount": 45,
    "description": {
      "markdown": "**Statistics** is the discipline that concerns the collection, organization, analysis, interpretation, and presentation of data.\n\nThis tag isn't meant for posts that estimate the impact of specific charities or interventions, but instead for posts about \"doing statistics\" in general.\n\nRelated entries\n---------------\n\n[data science](https://forum.effectivealtruism.org/tag/data-science) | [impact assessment](https://forum.effectivealtruism.org/tag/impact-assessment)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "Fb783KCMY4zGbQca3",
    "name": "Community projects",
    "core": false,
    "slug": "community-projects",
    "oldSlugs": [
      "community-projects"
    ],
    "postCount": 91,
    "description": {
      "markdown": "The **community projects** tag covers discussion of EA community work that doesn't fit into a single cause area (e.g. [EA Giving Tuesday](https://forum.effectivealtruism.org/tag/ea-giving-tuesday) or EA Pen Pals)."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "ee66CtAMYurQreWBH",
    "name": "Existential risk",
    "core": true,
    "slug": "existential-risk",
    "oldSlugs": [
      "existential-risk"
    ],
    "postCount": 747,
    "description": {
      "markdown": "An **existential risk** is the risk of an [existential catastrophe](https://forum.effectivealtruism.org/tag/existential-catastrophe-1), i.e. one that threatens the destruction of humanity’s longterm potential.^[\\[1\\]](#fn0f5x8s34vee)^^[\\[2\\]](#fns39fj4bj7yr)^ Existential risks include [natural risks](https://forum.effectivealtruism.org/tag/natural-existential-risk) such as those posed by [asteroids](https://forum.effectivealtruism.org/tag/asteroids) or [supervolcanoes](https://forum.effectivealtruism.org/tag/supervolcano) as well as [anthropogenic risks](https://forum.effectivealtruism.org/tag/anthropogenic-existential-risks) like mishaps resulting from [synthetic biology](https://forum.effectivealtruism.org/tag/global-catastrophic-biological-risk) or [artificial intelligence](https://forum.effectivealtruism.org/tag/ai-risk).\n\nA number of authors have argued that existential risks are especially important because the [long-run future of humanity](https://forum.effectivealtruism.org/tag/longtermism) matters a great deal.^[\\[1\\]](#fn0f5x8s34vee)^^[\\[3\\]](#fnehnudz7v1f)^^[\\[4\\]](#fnx1zw0p7aful)^^[\\[5\\]](#fnxep7ip80wm)^ Many believe that there is [no intrinsic moral difference](https://forum.effectivealtruism.org/tag/temporal-discounting) between the importance of a life today and one in a hundred years. However, there may be many more people in the future than there are now. They argue, therefore, that it is overwhelmingly important to preserve that potential, even if the risks to humanity are small.\n\nOne objection to this argument is that people have a special responsibility to other people currently alive that they do not have to people who have not yet been born.^[\\[6\\]](#fnql2k3envp7)^ Another objection is that, although it would in principle be important to manage, the risks are currently so unlikely and poorly understood that existential risk reduction is less cost-effective than work on other promising areas.\n\nRecommendations\n---------------\n\nIn [*The Precipice: Existential Risk and the Future of Humanity*](https://forum.effectivealtruism.org/tag/the-precipice), [Toby Ord](https://forum.effectivealtruism.org/tag/toby-ord) offers several [policy](https://forum.effectivealtruism.org/tag/policy) and [research](https://forum.effectivealtruism.org/tag/research) recommendations for handling existential risks:^[\\[7\\]](#fn52gvr4dqg9p)^\n\n*   Explore options for new [international institutions](https://forum.effectivealtruism.org/tag/global-governance) aimed at reducing existential risk, both incremental and revolutionary.\n*   Investigate possibilities for making the deliberate or reckless imposition of [human extinction](https://forum.effectivealtruism.org/tag/human-extinction) risk an international crime.\n*   Investigate possibilities for bringing the [representation of future generations](https://forum.effectivealtruism.org/topics/longtermist-institutional-reform) into national and international democratic institutions.\n*   Each major world power should have an appointed senior government position responsible for registering and responding to existential risks that can be realistically foreseen in the next 20 years.\n*   Find the major [existential risk factors](https://forum.effectivealtruism.org/tag/existential-risk-factor) and security factors—both in terms of absolute size and in the cost-effectiveness of marginal changes.\n*   Target efforts at reducing the likelihood of military conflicts between the US, [Russia](https://forum.effectivealtruism.org/tag/russia) and [China](https://forum.effectivealtruism.org/tag/china).\n*   Improve horizon-scanning for unforeseen and emerging risks.\n*   Investigate [food substitutes](https://forum.effectivealtruism.org/tag/resilient-food) in case of extreme and lasting reduction in the world’s ability to supply food.\n*   Develop better theoretical and practical tools for assessing risks with extremely high stakes that are either [unprecedented](https://forum.effectivealtruism.org/tag/unprecedented-risks) or thought to have extremely low probability.\n*   Improve our understanding of the chance civilization will recover after a [global collapse](https://forum.effectivealtruism.org/tag/civilizational-collapse), what might prevent this, and how to improve the odds.\n*   Develop our thinking about [grand strategy](https://forum.effectivealtruism.org/tag/existential-security) for humanity.\n*   Develop our understanding of the [ethics of existential risk](https://forum.effectivealtruism.org/tag/ethics-of-existential-risk) and valuing the [longterm future](https://forum.effectivealtruism.org/tag/long-term-future).\n\nFurther reading\n---------------\n\nBostrom, Nick (2002) [Existential risks: analyzing human extinction scenarios and related hazards](https://www.jetpress.org/volume9/risks.html), *Journal of Evolution and Technology*, vol. 9.  \n*A paper surveying a wide range of non-extinction existential risks.*\n\nBostrom, Nick (2013) [Existential risk prevention as global priority](http://doi.org/10.1111/1758-5899.12002), *Global Policy*, vol. 4, pp. 15–31.\n\nMatheny, Jason Gaverick (2007) [Reducing the risk of human extinction](http://doi.org/10.1111/j.1539-6924.2007.00960.x), *Risk Analysis*, vol. 27, pp. 1335–1344.  \n*A paper exploring the cost-effectiveness of extinction risk reduction.*\n\nOrd, Toby (2020) [*The Precipice: Existential Risk and the Future of Humanity*](https://en.wikipedia.org/wiki/Special:BookSources/1526600218), London: Bloomsbury Publishing.\n\nOrd, Toby (2020) [Existential risks to humanity](https://en.wikipedia.org/wiki/Special:BookSources/9789211264425) in Pedro Conceição (ed.) *The 2020 Human Development Report: The Next Frontier: Human Development and the Anthropocene*, New York: United Nations Development Programme, pp. 106–111.\n\nRelated entries\n---------------\n\n[civilizational collapse](https://forum.effectivealtruism.org/tag/civilizational-collapse) | [criticism of longtermism and existential risk studies](https://forum.effectivealtruism.org/topics/criticism-of-longtermism-and-existential-risk-studies) **|** [dystopia](https://forum.effectivealtruism.org/tag/dystopia) | [estimation of existential risks](https://forum.effectivealtruism.org/tag/estimation-of-existential-risk) | [ethics of existential risk](https://forum.effectivealtruism.org/tag/ethics-of-existential-risk) | [existential catastrophe](https://forum.effectivealtruism.org/tag/existential-catastrophe-1) | [existential risk factor](https://forum.effectivealtruism.org/tag/existential-risk-factor) | [existential security](https://forum.effectivealtruism.org/tag/existential-security) | [global catastrophic risk](https://forum.effectivealtruism.org/tag/global-catastrophic-risk) | [hinge of history](https://forum.effectivealtruism.org/tag/hinge-of-history) | [longtermism](https://forum.effectivealtruism.org/tag/longtermism) | [Toby Ord](https://forum.effectivealtruism.org/tag/toby-ord) | [rationality community](https://forum.effectivealtruism.org/tag/rationality-community) | [Russell–Einstein Manifesto](https://forum.effectivealtruism.org/tag/russell-einstein-manifesto) | [s-risk](/tag/s-risk)\n\n1.  ^**[^](#fnref0f5x8s34vee)**^\n    \n    Bostrom, Nick (2012) [Frequently asked questions](https://www.existential-risk.org/faq.html), *Existential Risk: Threats to Humanity’s Future* (updated 2013).\n    \n2.  ^**[^](#fnrefs39fj4bj7yr)**^\n    \n    Ord, Toby (2020) [*The Precipice: Existential Risk and the Future of Humanity*](https://en.wikipedia.org/wiki/Special:BookSources/1526600218), London: Bloomsbury Publishing.\n    \n3.  ^**[^](#fnrefehnudz7v1f)**^\n    \n    Beckstead, Nick (2013) [*On the Overwhelming Importance of Shaping the Far Future*](http://doi.org/10.7282/T35M649T), PhD thesis, Rutgers University.\n    \n4.  ^**[^](#fnrefx1zw0p7aful)**^\n    \n    Bostrom, Nick (2013) [Existential risk prevention as global priority](http://doi.org/10.1111/1758-5899.12002), *Global Policy*, vol. 4, pp. 15–31.\n    \n5.  ^**[^](#fnrefxep7ip80wm)**^\n    \n    Greaves, Hilary & William Macaskill (2019) [The case for strong longtermism](https://globalprioritiesinstitute.org/hilary-greaves-william-macaskill-the-case-for-strong-longtermism/), GPI working paper No. 7-2019, Working paper Global Priorities Institute, Oxford University.\n    \n6.  ^**[^](#fnrefql2k3envp7)**^\n    \n    Roberts, M. A. (2009) [The nonidentity problem](https://plato.stanford.edu/entries/nonidentity-problem/), *Stanford Encyclopedia of Philosophy*, July 21 (updated 1 December 2020).\n    \n7.  ^**[^](#fnref52gvr4dqg9p)**^\n    \n    Ord, Toby (2020) [*The Precipice: Existential Risk and the Future of Humanity*](https://en.wikipedia.org/wiki/Special:BookSources/1-5266-0021-8), London: Bloomsbury Publishing, pp. 280–281."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "uCPFdnpD6e4CCX8nC",
    "name": "Research methods",
    "core": false,
    "slug": "research-methods",
    "oldSlugs": [
      "research-methods"
    ],
    "postCount": 137,
    "description": {
      "markdown": "Use the **research methods** tag for posts about how to conduct better research.\n\nThe tag can also be used for posts that present their research in such an exemplary way that a reader could learn about effective research methods simply by reading them.\n\nFor posts about improving research methods done in science, use the [meta-science](https://forum.effectivealtruism.org/tag/meta-science) tag.\n\nRelated entries\n---------------\n\n[academia](https://forum.effectivealtruism.org/tag/academia-1/) | [disentanglement research](https://forum.effectivealtruism.org/tag/disentanglement-research) | [impact assessment](https://forum.effectivealtruism.org/tag/impact-assessment) | [metascience](https://forum.effectivealtruism.org/topics/metascience) | [research summary](https://forum.effectivealtruism.org/tag/research-summary) | [research training programs](https://forum.effectivealtruism.org/tag/research-training-programs) | [scientific progress](https://forum.effectivealtruism.org/tag/scientific-progress) | [theory of change](https://forum.effectivealtruism.org/topics/theory-of-change)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "sWcuTyTB5dP3nas2t",
    "name": "Global health and development",
    "core": true,
    "slug": "global-health-and-development",
    "oldSlugs": [
      "global-health-and-development"
    ],
    "postCount": 562,
    "description": {
      "markdown": "Different initiatives to improve public health, reduce poverty, or increase economic growth - especially in the developing world - are all part of an ongoing project to promote global health and development.\n\nIn 2013, roughly 767 million people lived on less than the equivalent of $1.90 per day, adjusted for purchasing power.^[\\[1\\]](#fnphn1q4i5kdb)^ Lack of economic resources has direct consequences on many aspects of people’s lives, including access to education and healthcare. Poverty and poor health also seriously hinder the [wellbeing](https://forum.effectivealtruism.org/tag/wellbeing) of millions of people. This is why [economic poverty](https://forum.effectivealtruism.org/tag/global-poverty) and [the global burden of disease](https://forum.effectivealtruism.org/tag/global-burden-of-disease-study) are important focus areas for [effective altruism](https://forum.effectivealtruism.org/tag/effective-altruism).\n\nAs a result of widening global inequality, the cost of averting death is much lower in low-income countries. For instance, [GiveWell](https://forum.effectivealtruism.org/tag/givewell) estimates the cost per child life saved through a bed-net distribution funded by the [Against Malaria Foundation](https://forum.effectivealtruism.org/tag/against-malaria-foundation) at about $3,500.^[\\[2\\]](#fnl2img5hgr8l)^ By contrast, the British National Health Service considers it cost-effective to spend £25,000-£37,000 for *a year* of healthy life saved.^[\\[3\\]](#fnn4estvu7tcq)^ This means that donations to charities that work on global poverty and global health can be very cost-effective. Global inequality also affects the impact of cash transfers: given the extent of global inequality, a dollar is worth 66 times as much to a person living in extreme poverty as to the average American.^[\\[4\\]](#fn5i69gk4s2k)^\n\nSome worry that employing aid to tackle these issues is problematic. Effective altruism has sought to address several of these concerns, including worries about [aid effectiveness](https://forum.effectivealtruism.org/tag/foreign-aid-skepticism) and [aid and paternalism](https://forum.effectivealtruism.org/tag/aid-and-paternalism).  \n  \nSee [here](https://forum.effectivealtruism.org/topics/organizations-and-projects-in-effective-altruism?sortedBy=#Global_health_and_development) for a list of organizations working on global health and development. \n\nFurther reading\n---------------\n\nNash, David (2022) [Global development & effective altruism - A brief intro](https://forum.effectivealtruism.org/posts/GowB4TZLDEzJEtAMp/global-development-and-effective-altruism-a-brief-intro), *Effective Altruism Forum*, April 1.\n\nOrd, Toby (2019) [The moral imperative toward cost-effectiveness in global health](http://doi.org/10.1093/oso/9780198841364.003.0002), in Hilary Greaves & Theron Pummer (eds.) *Effective Altruism: Philosophical Issues*, Oxford: Oxford University Press, pp. 29–36.\n\nWhittlestone, Jess (2017) [Global health and development](https://www.effectivealtruism.org/articles/cause-profile-global-health-and-development/), *Effective Altruism*, November 16.\n\nRelated entries\n---------------\n\n[aid and paternalism](https://forum.effectivealtruism.org/tag/aid-and-paternalism) | [Against Malaria Foundation](https://forum.effectivealtruism.org/topics/against-malaria-foundation) | [burden of disease](https://forum.effectivealtruism.org/tag/burden-of-disease) [](https://forum.effectivealtruism.org/topics/deworming) | [Deworm the World](https://forum.effectivealtruism.org/topics/deworm-the-world-initiative) | [deworming](https://forum.effectivealtruism.org/topics/deworming) | [economic growth](https://forum.effectivealtruism.org/tag/economic-growth) | [Evidence Action](https://forum.effectivealtruism.org/topics/evidence-action) | [family planning](https://forum.effectivealtruism.org/tag/family-planning) | [foreign aid](https://forum.effectivealtruism.org/tag/foreign-aid) | [GiveDirectly](https://forum.effectivealtruism.org/topics/givedirectly) | [GiveWell](https://forum.effectivealtruism.org/topics/givewell) | [global health and wellbeing](https://forum.effectivealtruism.org/topics/global-health-and-wellbeing) | [global poverty](https://forum.effectivealtruism.org/tag/global-poverty) | [mass distribution of insecticide-treated nets](https://forum.effectivealtruism.org/topics/mass-distribution-of-long-lasting-insecticide-treated-nets) | [universal basic income](https://forum.effectivealtruism.org/tag/universal-basic-income)\n\n1.  ^**[^](#fnrefphn1q4i5kdb)**^\n    \n    World Bank (2016) [*Poverty and Shared Prosperity 2016: Taking on Inequality*](http://doi.org/10.1596/978-1-4648-0958-3), Washington: World Bank.\n    \n2.  ^**[^](#fnrefl2img5hgr8l)**^\n    \n    GiveWell (2016) [Against Malaria Foundation](https://web.archive.org/web/20170501212854/http://www.givewell.org/charities/against-malaria-foundation), *GiveWell*, November.\n    \n3.  ^**[^](#fnrefn4estvu7tcq)**^\n    \n    Rigby, Jennifer (2014) [Why the NHS thinks a healthy year of life is worth £20,000](https://www.channel4.com/news/drugs-life-breast-cancer-nice-20-000-a-year-of-life-nhs), *4 News*, April 23.\n    \n4.  ^**[^](#fnref5i69gk4s2k)**^\n    \n    Weyl, E. Glen (2018) [The openness-equality trade-off in global redistribution](http://doi.org/10.1111/ecoj.12469), *Economic Journal*, vol. 128, pp. F1–F36."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "z8qFsGt5iXyZiLbjN",
    "name": "Take action",
    "core": false,
    "slug": "take-action",
    "oldSlugs": [
      "get-involved",
      "get-involved",
      "take-action"
    ],
    "postCount": 254,
    "description": {
      "markdown": "The [EA Forum](https://forum.effectivealtruism.org/tag/effective-altruism-forum-1) is mostly about discussing ideas. But ultimately, the reason for discussing those ideas is to take actions that improve the world.\n\nHere are some ways to get involved with effective altruism via the EA Forum or elsewhere:\n\n*   **Jobs:** Find [job listings on the Forum](https://forum.effectivealtruism.org/tag/job-listing-open), apply for [high-impact career advising](https://80000hours.org/speak-with-us/), or visit the job boards on [80,000 Hours](https://80000hours.org/job-board/) and [EA Work Club](https://eawork.club/)\n*   **Volunteering**: Find volunteering opportunities using the [volunteering](https://forum.effectivealtruism.org/topics/volunteering) tag, by visiting [EA Work Club](https://eawork.club/), by looking at the  [EA Opportunities Board](https://ea-internships.pory.app/board), or by directly reaching out to [high-impact non-profit organisations](https://givingwhatwecan.org/best-charities).\n*   **Donating:** Support high-impact work by donating to [high impact causes](https://www.givingwhatwecan.org/cause-areas/) and taking an [effective giving pledge](https://www.givingwhatwecan.org/pledge/).\n*   **Internships and fellowships:** Find [internships and fellowships on the Forum](https://forum.effectivealtruism.org/tag/fellowships-and-internships) or visit the [EA Opportunities Board](https://ea-internships.pory.app/board), which lists everything short of full-time positions.\n*   **Startup ideas:** Start a high impact non-profit by applying to [Charity Entrepreneurship](https://www.charityentrepreneurship.com/), finding a startup idea from this [list of ideas for valuable new non-profit startups](https://forum.effectivealtruism.org/posts/faezoENQwSTyw9iop/ea-megaprojects-continued) (of which there might already be [funding available](https://forum.effectivealtruism.org/posts/faezoENQwSTyw9iop/ea-megaprojects-continued?commentId=8N6ysMWesNbyfzsuJ#comments) if you want to try one of these ideas).\n*   **Research ideas:** Find a research topic by looking at the [central directory for research questions](https://forum.effectivealtruism.org/posts/MsNpJBzv5YhdfNHc9/a-central-directory-for-open-research-questions), or use [Effective Thesis](https://effectivethesis.org/) to help find a high-impact thesis topic.\n*   **Funding opportunities:** Seek funding for a high-impact project by posting using the [funding opportunity](https://forum.effectivealtruism.org/tag/funding-opportunity) tag, applying to [the FTX Future Fund](https://ftxfuturefund.org/apply/), apply to [EA Funds](https://funds.effectivealtruism.org/apply-for-funding), or find [other funding opportunities](https://forum.effectivealtruism.org/posts/DqwxrdyQxcMQ8P2rD/list-of-ea-funding-opportunities).\n\n(This page is an experiment: please [give us feedback](https://drive.google.com/forms/d/e/1FAIpQLScgvoD7WrLgmkdTebNs4QzooGB42iDWk9bDPYOd3-QWKOywrg/viewform) on it!)\n\nUse the **take action** tag for posts that offer readers a way to get involved with useful EA work. It is a call to action, an option to volunteer, or anything else.\n\nIdeally, this tag will only be applied to posts that discuss an active opportunity. If you see a post that has this tag, but the time to get involved has already passed, please downvote this tag's karma so that the post will no longer appear on the \"get involved\" tag page and upvote [**requests (closed)**](https://forum.effectivealtruism.org/tag/requests-closed) tag so that it will be clear that no one had mis-tagged.\n\nFor completable well-defined requests, use [**requests (open)**](https://forum.effectivealtruism.org/tag/requests-open) instead. For posts that seek funding use [**funding request**](https://forum.effectivealtruism.org/tag/funding-request). For invitations to future events use the [**Events page**](https://forum.effectivealtruism.org/events).\n\nRelated entries\n---------------\n\n[bounty (open)](https://forum.effectivealtruism.org/tag/bounty-open) | [effective altruism funding](https://forum.effectivealtruism.org/tag/effective-altruism-funding) | [funding opportunities](https://forum.effectivealtruism.org/tag/funding-opportunity) | [job listing (open)](https://forum.effectivealtruism.org/tag/job-listing-open) | [megaprojects](https://forum.effectivealtruism.org/topics/megaprojects) | [requests (open)](https://forum.effectivealtruism.org/tag/requests-open) | [scalably using labour](https://forum.effectivealtruism.org/tag/scalably-using-labour)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "bBS5GQJgFcobNBLki",
    "name": "Organization updates",
    "core": false,
    "slug": "organization-updates",
    "oldSlugs": [
      "org-update",
      "org-update"
    ],
    "postCount": 339,
    "description": {
      "markdown": "Use the **organization updates** tag for posts where EA-aligned organizations share their recent progress and other news.\n\nNot to be confused with [EA Organization Updates](https://forum.effectivealtruism.org/topics/ea-organization-updates-monthly-series), a monthly post series containing updates about organizations within the [effective altruism](https://forum.effectivealtruism.org/topics/effective-altruism) community.\n\nWhat follows now is a list of EA-related organizations, as defined in Gittins^[\\[1\\]](#fnnz0fwx5dh8h)^, using this tag as a Wiki:\n\nInfrastructure\n--------------\n\n[**80,000 Hours**](https://forum.effectivealtruism.org/tag/80-000-hours) — Does research into how people can have greater impact with their careers. Also maintains a high impact jobs board and produces a podcast.\n\n[**Animal Advocacy Careers**](https://forum.effectivealtruism.org/tag/animal-advocacy-careers) — Seeks to address the career and talent bottlenecks in the animal advocacy movement, especially the farmed animal movement, by providing career services and advice. Incubated by Charity Entrepreneurship.\n\n[**Animal Charity Evaluators**](https://forum.effectivealtruism.org/tag/animal-charity-evaluators) — Evaluates and recommends the most effective animal charities.\n\n[**Ayuda Efectiva**](https://forum.effectivealtruism.org/tag/ayuda-efectiva) — Promotes effective giving in Spain. Their Global Health Fund routes donations to a selection of GiveWell's recommended charities, providing tax deductibility for Spanish donors. They plan to launch similar funds for other cause areas in the near future.\n\n[**Centre for Effective Altruism**](https://forum.effectivealtruism.org/tag/centre-for-effective-altruism-1) — Helps to grow and support the EA community.\n\n[**Charity Entrepreneurship**](https://forum.effectivealtruism.org/tag/charity-entrepreneurship) — Does research into the most effective interventions and incubates charities to implement these interventions.\n\n[**Doebem**](https://forum.effectivealtruism.org/tag/doebem) — A Brazilian-based donation platform which recommends effective charities according to EA principles.\n\n[**Donational**](https://forum.effectivealtruism.org/tag/donational) — A donation platform which recommends effective charities to users, and helps them to pledge and allocate a proportion of their income to those charities.\n\n[**Effective Altruism Foundation**](https://forum.effectivealtruism.org/tag/effective-altruism-foundation) — Implements projects aimed at doing the most good in terms of reducing suffering. Once initiated, projects are carried forward by EAF with differing degrees of independence and in some cases become autonomous organisations. Projects have included Raising for Effective Giving (REG) and the Centre on Long-Term Risk (CLR).\n\n[**Effective Giving**](https://forum.effectivealtruism.org/tag/effective-giving-2) — Helps major donors to explore the best giving strategies backed by evidence and research in order to maximize the impact of their donations.\n\n[**Effektiv-Spenden.org**](https://forum.effectivealtruism.org/tag/effektiv-spenden-org) — The name roughly translates to 'effective giving' (verbatim, 'to donate effectively') in German. The organisation is a dedicated donation regranting platform for Germany that was founded in coordination with the Effective Altruism Foundation. Its main goal is to expand the appeal of donations to effective charities beyond the EA community to the general public by offering a curated set of effective charities for a selected subset of broadly appealing cause areas.\n\n[**Founders Pledge**](https://forum.effectivealtruism.org/tag/founders-pledge) — A community of entrepreneurs who have pledged to give a percentage of the proceeds they make when they exit their company to charity. Researchers at Founders Pledge provide advice on the most high impact giving opportunities to the pledgers.\n\n[**Generation Pledge**](https://forum.effectivealtruism.org/tag/generation-pledge) — Works with a community of inheritors in ultra high net wealth families who are committed to doing good with the resources they have available. Offers opportunities to these individuals to learn, connect, collaborate and take action to create large-scale positive impact.\n\n[**GiEffektivt.No**](https://forum.effectivealtruism.org/tag/gieffektivt-no) — A donation portal based in Norway that fundraises for GiveWell’s top charities ('gi effektivt' means 'give effectively'). Promotes the idea of donating effectively and makes donations easier by lowering transaction costs and offering tax refunds to Norwegian donors.\n\n[**GiveWell**](https://forum.effectivealtruism.org/tag/givewell) — Evaluates and recommends the most effective charities in global health.\n\n[**Giving Multiplier.org**](https://forum.effectivealtruism.org/tag/giving-multiplier) — A website that encourages people to donate to effective charities by splitting their donation between their favourite (non-recommended) charity and a recommended effective charity. The website then matches between 3% to 30% of the donation depending on what proportion the user allocated to the effective charity.\n\n[**Giving What We Can**](https://forum.effectivealtruism.org/tag/giving-what-we-can) — A community which encourages people to pledge to donate a percentage of their income to effective charities.\n\n[**High Impact Athletes**](https://forum.effectivealtruism.org/tag/high-impact-athletes) — Channels donations from current athletes, retired athletes, sport institutions, and sports fans to the most effective and evidence-based charities in the sectors of Global Health & Poverty and Environmental Impact.\n\n[**Impactful Government Careers**](https://forum.effectivealtruism.org/topics/impactful-government-careers) — Helps civil servants in the UK to do the most good they can by running workshops and events, doing research and providing one-on-one career coaching.\n\n[**Legal Priorities Project**](https://forum.effectivealtruism.org/tag/legal-priorities-project) — Conducts legal research that tackles the world’s most pressing problems. Currently focusing on artificial intelligence, synthetic biology, climate change and institutional design.\n\n[**Let's Fund**](https://forum.effectivealtruism.org/tag/let-s-fund) — Aims to direct funds especially to small projects focused on research, policy and advocacy. It allows people to donate to high-risk projects that could have an enormous impact if successful.\n\n[**The Nonlinear Fund**](https://forum.effectivealtruism.org/tag/nonlinear-fund) — Researches,  funds, and seeds AI Safety interventions\n\n[**One for the World**](https://forum.effectivealtruism.org/tag/one-for-the-world-1) — Educates students and young professionals about effective giving and encourages them to donate at least 1% of their income to effective charities. Organises ‘chapters’ which are groups run by volunteers to spread the message at their universities/churches/businesses.\n\n[**Open Philanthropy**](https://forum.effectivealtruism.org/tag/open-philanthropy) — Aims to help philanthropy improve lives effectively through research and grantmaking. Makes grants in areas including U.S. policy, farm animal welfare and global catastrophic risks.\n\n[**Raising for Effective Giving**](https://forum.effectivealtruism.org/tag/raising-for-effective-giving) — Promotes the idea of effective giving, provides donation advice to individuals and organizations, and supports fundraising campaigns and events. Recommends charities within the areas of poverty alleviation, animal welfare, and risks from emerging technologies.\n\n[**Rethink Charity**](https://forum.effectivealtruism.org/tag/rethink-charity) — Produces novel research and community building projects to elevate the effective altruism community toward greater positive impact. Past projects include Rethink Priorities (now an independent organisation) and Students for High Impact Charity (SHIC). Current projects are RC Forward and the EA Hub.\n\n[**Rethink Priorities**](https://forum.effectivealtruism.org/tag/rethink-priorities) — A think tank dedicated to figuring out the best ways to make the world a better place, initially founded as a project of Rethink Charity. Their research is focused on improving the welfare of nonhuman animals, but they also work on existential risks such as nuclear war.\n\n[**SoGive**](https://forum.effectivealtruism.org/tag/sogive) — An online tool that lets you see the impact you’ve made in the world through your charitable giving. The tool will have in-built formulae and analytics to let you see the change you've made in the world in terms like \"I funded the distribution of 30 malaria nets in Africa\" instead of \"I gave £100 to charity\".\n\n[**The Life You Can Save**](https://forum.effectivealtruism.org/tag/the-life-you-can-save) — Organisation founded by Peter Singer which recommends effective non-profits and promotes effective giving.\n\n[**WANBAM**](https://forum.effectivealtruism.org/tag/wanbam) — Aims to inspire and support women, trans people of any gender, and non-binary people who are pursuing high-impact career paths by matching them with mentors.\n\nAnimal Advocacy\n---------------\n\n[**Animal Advocacy Africa**](https://forum.effectivealtruism.org/tag/animal-advocacy-africa) —  Works to develop a collaborative and effective animal advocacy movement in Africa.\n\n[**Albert Schweitzer Foundation**](https://forum.effectivealtruism.org/tag/albert-schweitzer-foundation) — An ACE recommended charity which conducts corporate and vegetarian outreach campaigns.\n\n[**Anima International**](https://forum.effectivealtruism.org/tag/anima-international) — An ACE recommended charity which runs corporate campaigns, undercover investigations, online and offline ad campaigns and outreach.\n\n[**Animal Ask**](https://forum.effectivealtruism.org/tag/animal-ask) — Aims to assist animal advocacy organizations in their efforts to reduce farmed animal suffering by providing research to help optimise and prioritise future asks (the specific request for industry, food companies, or government to improve the welfare of animals in their care). Incubated by Charity Entrepreneurship.\n\n[**Animal Ethics**](https://forum.effectivealtruism.org/tag/animal-ethics) — Does outreach and research to help animals. This includes work on sentience, speciesism, and wild animal suffering.\n\n[**Aquatic Life Institute**](https://forum.effectivealtruism.org/tag/aquatic-life-institute) — Advises and funds research to help aquatic animals. Uses these findings to guide welfare interventions.\n\n[**Farmed Animal Funders**](https://forum.effectivealtruism.org/tag/farmed-animal-funders) — A decentralised funding collaborative, aimed at expediting the end of factory farming through collaboration and shared learning. Open to all individuals and foundations giving more than $250,000 per year to end factory farming.\n\n[**Faunalytics**](https://forum.effectivealtruism.org/tag/faunalytics) — Does research and produces resources to help charities and individuals be more effective animal advocates.\n\n[**Fish Welfare Initiative**](https://forum.effectivealtruism.org/tag/fish-welfare-initiative) — Works to improve the welfare of farmed fish through welfare interventions. Incubated by Charity Entrepreneurship.\n\n[**The Good Food Institute**](https://forum.effectivealtruism.org/tag/good-food-institute) — An ACE recommended charity which promotes and develops competitive alternatives to animal-based meat, dairy, and eggs.\n\n[**The Humane League**](https://forum.effectivealtruism.org/tag/the-humane-league) — An ACE recommended charity which runs a variety of programs that advocate for farmed animals including corporate outreach, online veg advertising, and individual outreach.\n\n[**Sentience Institute**](https://forum.effectivealtruism.org/tag/sentience-institute) — A social science think tank building on and promoting the body of evidence for how to encourage humanity’s moral circle expansion most effectively.\n\n[**Sentience Politics**](https://forum.effectivealtruism.org/tag/sentience-politics) — An anti-speciesist political think tank with the goal of reducing the suffering of all sentient beings. Originally established as a project of EAF.\n\n[**Wild Animal Initiative**](https://forum.effectivealtruism.org/tag/wild-animal-initiative) — Does research to understand and improve the lives of wild animals.\n\nGlobal Health and Poverty\n-------------------------\n\n[**Abdul Latif Jameel Poverty Action Lab (J-PAL)**](https://forum.effectivealtruism.org/tag/abdul-latif-jameel-poverty-action-lab) — Aims to reduce poverty by ensuring that policy is informed by scientific evidence. They do this through research, policy outreach, and training.\n\n[**Against Malaria Foundation (AMF)**](https://forum.effectivealtruism.org/tag/against-malaria-foundation) — A GiveWell recommended charity working to prevent malaria.\n\n[**Canopie**](https://forum.effectivealtruism.org/tag/canopie) — Addresses mental health for pre- and postpartum women through guided cognitive behavioral therapy (CBT). Currently developing and testing a program to treat pregnancy-related mental health issues in the United States, with plans to scale to a low- or middle-income country in 2022. Incubated by Charity Entrepreneurship.\n\n[**END Fund (deworming programme)**](https://forum.effectivealtruism.org/tag/end-fund) — Works to eliminate neglected tropical diseases. GiveWell recommends their deworming programme.\n\n[**Evidence Action (Deworm the World Initiative programme)**](https://forum.effectivealtruism.org/tag/evidence-action) — GiveWell recommends their Deworm the World Initiative programme.\n\n[**Family Empowerment Media (FEM)**](https://forum.effectivealtruism.org/tag/family-empowerment-media) — Aims to enable informed family planning and birth spacing decisions through radio-based communication. Will produce and distribute public service announcements and interactive programs to increase awareness and knowledge of modern forms of contraception. Incubated by Charity Entrepreneurship.\n\n[**Fortify Health**](https://forum.effectivealtruism.org/tag/fortify-health) — Aims to tackle anaemia and neural tube defects in India through flour fortification. Incubated by Charity Entrepreneurship.\n\n[**GiveDirectly**](https://forum.effectivealtruism.org/tag/givedirectly) — A GiveWell recommended charity working on cash transfers.\n\n[**Happier Lives Institute (HLI)**](https://forum.effectivealtruism.org/tag/happier-lives-institute) — Researches the best ways to increase global wellbeing. This includes theoretical research on wellbeing measurement as well as practical research into effective interventions. Incubated by Charity Entrepreneurship.\n\n[**Helen Keller International (Vitamin A supplementation programme)**](https://forum.effectivealtruism.org/tag/helen-keller-international) — GiveWell recommends their Vitamin A supplementation programme.\n\n[**IDInsight**](https://forum.effectivealtruism.org/tag/idinsight) — A global advisory, data analytics, and research organisation that helps development leaders maximize their social impact. Tailors a range of data and evidence tools, including randomized evaluations and machine learning, to help decision-makers design effective programs and rigorously test what works to support communities.\n\n[**Innovations for Poverty Action (IPA)**](https://forum.effectivealtruism.org/tag/innovations-for-poverty-action) — A research and policy non-profit that discovers and promotes effective solutions to global poverty problems. Brings together researchers and decision-makers to design, rigorously evaluate, and refine these solutions and their applications, ensuring that the evidence created is used to improve the lives of the world’s poor.\n\n[**Lead Exposure Elimination Project (LEEP)**](https://forum.effectivealtruism.org/tag/lead-exposure-elimination-project) — Advocates for lead paint regulation to reduce lead poisoning. Incubated by Charity Entrepreneurship.\n\n[**Malaria Consortium**](https://forum.effectivealtruism.org/tag/malaria-consortium) — A GiveWell recommended charity working to prevent malaria.\n\n[**New Incentives**](https://forum.effectivealtruism.org/tag/new-incentives) — A GiveWell recommended charity which works in North West Nigeria to increase the uptake of routine immunizations through cash transfers.\n\n[**Policy Entrepreneurship Network**](https://forum.effectivealtruism.org/tag/policy-entrepreneurship-network) — Supports local grassroots initiatives across the world working on important yet neglected issues in public health policy. Incubated by Charity Entrepreneurship.\n\n[**SCI Foundation**](https://forum.effectivealtruism.org/tag/sci-foundation) — A GiveWell recommended charity working on deworming.\n\n[**Sightsavers (deworming programme)**](https://forum.effectivealtruism.org/tag/sightsavers) — GiveWell recommends their deworming programme.\n\n[**Suvita**](https://forum.effectivealtruism.org/tag/suvita) — An organisation incubated by Charity Entrepreneurship. Recently merged with Charity Science Health, which had been working to send SMS reminders about immunisations to caregivers. The combined organisations now run two complementary programmes — SMS reminders and immunisation ambassadors — which they aim to scale up across India to increase vaccination uptake.\n\nLong-term Future\n----------------\n\n[**Alliance to Feed the Earth in Disasters (ALLFED)**](https://forum.effectivealtruism.org/tag/allfed) — Does research into threats to global food security and ways in which we could feed the world if global food production were significantly threatened.\n\n[**All-Party Parliamentary Group for Future Generations**](https://forum.effectivealtruism.org/tag/appg-on-future-generations) — An informal cross-party group of members of Parliament in the UK. Raises awareness of long-term issues, explores ways to internalise longer-term considerations into decision-making processes, and creates space for dialogue on combating short-termism in policymaking.\n\n[**Berkeley Existential Risk Initiative (BERI)**](https://forum.effectivealtruism.org/tag/berkeley-existential-risk-initiative) — Provides support to the ‘x-risk ecosystem’ (organisations such as FHI and CHAI) by providing funding, staff time and administrative support.\n\n[**Center for Human-Compatible AI (CHAI)**](https://forum.effectivealtruism.org/tag/center-for-human-compatible-artificial-intelligence) — Aims to reorient the general thrust of AI research away from the capability to achieve arbitrary objectives and towards the ability to generate provably beneficial behaviour.\n\n[**Centre for Long-Term Resilience**](https://forum.effectivealtruism.org/tag/centre-for-long-term-resilience) — A UK-based non-profit with current focuses on artificial intelligence, biosecurity, and climate change.\n\n[**Centre on Long-term Risk (CLR)**](https://forum.effectivealtruism.org/tag/center-on-long-term-risk) — A project of EAF. Does research, awards grants and scholarships, and hosts workshops, with a focus on advancing the safety and governance of artificial intelligence as well as understanding other long-term risks.\n\n[**Centre for the Study of Existential Risks (CSER)**](https://forum.effectivealtruism.org/tag/centre-for-the-study-of-existential-risk) — An interdisciplinary research centre within the University of Cambridge working on the study and mitigation of existential risks.\n\n[**Forethought Foundation for Global Priorities Research**](https://forum.effectivealtruism.org/tag/forethought-foundation) — Promotes academic work that addresses the question of how to use our resources to improve the world by as much as possible, especially in the domain of the long-term future. Is planning to offer scholarships and fellowships to students in global priorities research and research grants for established scholars. It is a project of CEA.\n\n[**Future of Humanity Institute (FHI)**](https://forum.effectivealtruism.org/tag/future-of-humanity-institute) — a multidisciplinary research institute at the University of Oxford led by Nick Bostrom. Academics at FHI use the tools of mathematics, philosophy, and social sciences to tackle big-picture questions about humanity and its prospects.\n\n[**Future of Life Institute (FLI)**](https://forum.effectivealtruism.org/tag/future-of-life-institute) — A non-profit seeking to improve the long-term future of humanity on a global scale. Focused on making sure that artificial intelligence is beneficial and reducing risks from nuclear weapons and biotechnology.\n\n[**Global Catastrophic Risk Institute (GCRI)**](https://forum.effectivealtruism.org/tag/global-catastrophic-risk-institute) — A think tank that analyses risks to the survival of human civilisation.\n\n[**Global Priorities Institute**](https://forum.effectivealtruism.org/tag/global-priorities-institute) — An interdisciplinary research centre at the University of Oxford which conducts foundational research (particularly in philosophy and economics) to inform the decision-making of individuals and institutions seeking to do as much good as possible.\n\n[**Leverhulme Centre for the Future of Intelligence (CFI)**](https://forum.effectivealtruism.org/tag/leverhulme-center-for-the-future-of-intelligence) — Aims to build a new interdisciplinary community of researchers who work together to ensure that humans make the best of the opportunities of artificial intelligence over the coming decades.\n\n[**Longview Philanthropy**](https://forum.effectivealtruism.org/tag/longview-philanthropy). Connects the philanthropists with outstanding opportunities to protect future generations.\n\n[**Machine Intelligence Research Institute (MIRI)**](https://forum.effectivealtruism.org/tag/machine-intelligence-research-institute) — Does foundational mathematical research to ensure smarter-than-human artificial intelligence has a positive impact.\n\n[**Projekt Framtid**](https://forum.effectivealtruism.org/tag/projekt-framtid) — A project of EA Sweden. The goal of the project is to promote long-term policies that take sufficient account of the interests of future generations when political decisions are made in Sweden.\n\n[**Simon Institute for Longterm Governance**](https://forum.effectivealtruism.org/tag/simon-institute-for-longterm-governance)  — Aims to support policymakers in their cooperation with future generations.\n\nOther\n-----\n\n[**Center for Applied Rationality (CFAR)**](https://forum.effectivealtruism.org/tag/center-for-applied-rationality) — Does research into methods to improve human rationality and delivers workshops to teach these skills to high-promise individuals.\n\n[**The Center for Election Science (CES)**](https://forum.effectivealtruism.org/tag/center-for-election-science) — Empowers voters through voting methods that strengthen democracy. CES accomplishes its mission through research and collaborating to pass ballot initiatives for approval voting. CES maintains that approval voting elects more consensus-style candidates and is more likely to maintain governmental stability over a long time frame in addition to providing near-term benefit.\n\n[**Centre for Enabling EA Learning and Research (CEEALAR)**](https://forum.effectivealtruism.org/tag/centre-for-enabling-ea-learning-and-research) — The ‘EA Hotel’ in Blackpool. Makes grants to individuals and charities in the form of free or subsidised accommodation at the hotel. Whilst at the hotel, guests can work on research, projects or pursue further study without worrying about funding themselves.\n\n[**Giving Green**](https://forum.effectivealtruism.org/tag/giving-green) — A new initiative at IDinsight that aims to direct dollars towards evidence-backed projects that combat the climate crisis. Incubated by Charity Entrepreneurship.\n\n[**Organisation for the Prevention of Intense Suffering (OPIS)**](https://forum.effectivealtruism.org/tag/organisation-for-the-prevention-of-intense-suffering) — Advocates for solutions to causes of intense suffering and advocates for evidence-based global decision-making which prioritises the prevention and alleviation of intense suffering. Currently focused on cause areas including access to pain relief, treatment of cluster headaches and ending factory farming.\n\n[**Our World in Data**](https://forum.effectivealtruism.org/tag/our-world-in-data) — Researches, analyses, and presents data on their website about the world’s most pressing problems. Aims to make knowledge about these problems accessible and understandable.\n\n[**Qualia Research Institute (QRI)**](https://forum.effectivealtruism.org/tag/qualia-research-institute) — A non-profit research group attempting to study consciousness in a consistent, meaningful, and rigorous way. Much of their work is guided by the goal of understanding the nature of what is good, and how we can do the most good.\n\n[**SparkWave**](https://forum.effectivealtruism.org/tag/sparkwave) — A startup foundry that applies social science, strategic thinking, and iterative experimentation to create socially beneficial software companies. Has founded companies including Clearer Thinking (a website for improving decision making and accessing science-based tools), UpLift (an app for treating depression) and Positly (a platform for fast recruitment of study participants).\n\n1.  ^**[^](#fnrefnz0fwx5dh8h)**^\n    \n    Gittins, Jamie (2020) [List of EA-related organisations](https://forum.effectivealtruism.org/posts/f6kg8T2Lp6rDqxWwG/list-of-ea-related-organisations), *Effective Altruism Forum*, October 16."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "BAbo53eWZFH4cFjkR",
    "name": "Donation writeup",
    "core": false,
    "slug": "donation-writeup",
    "oldSlugs": [
      "donation-writeup",
      "donation-writeup",
      "donation-writeup"
    ],
    "postCount": 93,
    "description": {
      "markdown": "In a **donation writeup**, someone explains why they've chosen to donate or grant money to a particular recipient, or why they've recommended that other people donate. (For example, GiveWell's charity analyses would count.)\n\n## Related entries\n[donation choice](https://forum.effectivealtruism.org/tag/donation-choice) | [earning to give](https://forum.effectivealtruism.org/tag/earning-to-give) | [personal finance](https://forum.effectivealtruism.org/tag/personal-finance) | [timing of philanthropy](https://forum.effectivealtruism.org/tag/timing-of-philanthropy)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "sXXqo3rbghiNW7SwN",
    "name": "Animal product alternatives",
    "core": false,
    "slug": "animal-product-alternatives",
    "oldSlugs": [
      "meat-alternatives",
      "alternative-proteins",
      "animal-product-alternatives-1"
    ],
    "postCount": 86,
    "description": {
      "markdown": "**Animal product alternatives** (or **alternative proteins**) are meat, eggs, and dairy products not made from living animals. Cheap, tasty, and healthy alternatives can [decrease animal product consumption](https://forum.effectivealtruism.org/tag/dietary-change). This would decrease [farmed animal suffering](https://forum.effectivealtruism.org/topics/farmed-animal-welfare) and reduce the negative environmental impact of agriculture.^[\\[1\\]](#fnm9ygb5923c)^\n\nThere are two broad categories of alternatives:\n\n*   **Plant-based** products made from plants and/or fungi. Plant-based meats and milks are already stocked in supermarkets, on the menu in some restaurants and fast-food chains. Estimates suggest that global plant-based meat retail value increased from $2.8 billion in 2017 to $5.6 billion in 2021.^[\\[2\\]](#fn76ys9g3z7e6)^\n*   [**Cultivated meat**](https://forum.effectivealtruism.org/topics/cultured-meat)  is real animal meat produced by cultivating animal cells directly rather than raising an entire animal. Currently, such meat is significantly more expensive than animal meat and production quantities are very small. There has been a lot of disagreement about when such products might become cost-competitive.^[\\[3\\]](#fnmaqc3xm08i)^ Forecasts by [Rethink Priorities](https://forum.effectivealtruism.org/topics/rethink-priorities) suggest limited production through 2050. Many cultured meat companies make much more optimistic claims but so far many of their predictions turned out to be wrong.^[\\[4\\]](#fnwast8pz59po)^\n    *   Acellular agriculture, or **fermentation**, is the process of using microorganisms to produce proteins and fats (as opposed to whole animal cells), e.g. for dairy and eggs. The technical challenges in fermentation appear more surmountable than in cultivated meat.^[\\[5\\]](#fn430jhq8o0sv)^\n\nThese categories are fairly distinct in terms of scientific approach and technical skills required. However, most products are likely to involve a blend of these technologies.\n\nFor-profit investment in animal product alternatives has been increasing. According to Good Food Institute reports, in just 2021, plant-based meat, seafood, egg, and dairy companies raised [$1.9 billion](https://perma.cc/2RF8-EZ9V) dollars, cultivated meat companies raised [$1.38 billion](https://perma.cc/8P8X-YZ24) dollars, and alternative protein fermentation companies raised [$1.69 billion](https://perma.cc/PC4A-5G7R) dollars. In all three categories, investments in 2021 were much higher than in years prior.\n\nFurther reading\n---------------\n\nAnnika Lonkila & Minna Kaljonen (2021) [Promises of meat and milk alternatives: An integrative literature review on emergent research themes](https://doi.org/10.1007/s10460-020-10184-9), *Agriculture and Human Values*, vol. 38, pp. 625–639.\n\nCargill, Natalie & Keiran Harris (2018) [How exactly clean meat is created & the advances needed to get it into every supermarket, according to food scientist Marie Gibbons](https://80000hours.org/podcast/episodes/marie-gibbons-clean-meat/), *80,000 Hours*, April 10.\n\nHuang, Amy (2020) [Closing gaps in alternative protein science](https://www.youtube.com/watch?v=YJGMeL4mKvk), *EA Student Summit 2020*, October 24.\n\nRelated entries\n---------------\n\n[cultivated meat](https://forum.effectivealtruism.org/topics/cultivated-meat) | [dietary change](https://forum.effectivealtruism.org/topics/dietary-change) | [farmed animal welfare](https://forum.effectivealtruism.org/tag/farmed-animal-welfare)\n\n1.  ^**[^](#fnrefm9ygb5923c)**^\n    \n    He, Jiang *et al.* (2020) [A review of research on plant‐based meat alternatives: Driving forces, history, manufacturing, and consumer attitudes](http://doi.org/10.1111/1541-4337.12610), *Comprehensive Reviews in Food Science and Food Safety*, vol. 19, pp. 2639–2656.\n    \n2.  ^**[^](#fnref76ys9g3z7e6)**^\n    \n    The Good Food Institute (2022) [2021 State of the industry report: Plant-based meat, eggs, seafood, and dairy](https://perma.cc/9ADW-799D), *The Good Food Institute* (figure 5).\n    \n3.  ^**[^](#fnrefmaqc3xm08i)**^\n    \n    Linch, Zhang; Dullaghan, Neil (2021) [Cultured meat: A comparison of techno-economic analyses](https://forum.effectivealtruism.org/posts/y8jHKDkhPXApHp2gb/cultured-meat-a-comparison-of-techno-economic-analyses) \n    \n4.  ^**[^](#fnrefwast8pz59po)**^\n    \n    Dullaghan, Neil (2021) [Cultured meat predictions were overly optimistic](https://forum.effectivealtruism.org/posts/YYurNqQDAWNiQJv9K/cultured-meat-predictions-were-overly-optimistic)\n    \n5.  ^**[^](#fnref430jhq8o0sv)**^\n    \n    Yip, Wen (2020) [When can I eat meat again?](https://forum.effectivealtruism.org/posts/4uYebcr5G2jqxuXG3/when-can-i-eat-meat-again)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "mAq9qa8rAyB6vM5z7",
    "name": "Diversity and inclusion",
    "core": false,
    "slug": "diversity-and-inclusion",
    "oldSlugs": [
      "diversity-and-inclusion",
      "diversity-and-inclusion",
      "diversity-and-inclusion"
    ],
    "postCount": 111,
    "description": {
      "markdown": "The **diversity and inclusion** tag covers posts that discuss:\n\n\\* Increasing the diversity and inclusiveness of the EA movement or cause areas within EA.\n\n\\* Diversity and inclusion as elements of cause areas (e.g. interventions meant to decrease racial bias)."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "tmQ7kd3bFTnSQveDv",
    "name": "Philosophy of effective altruism",
    "core": false,
    "slug": "philosophy-of-effective-altruism",
    "oldSlugs": [
      "ea-philosophy",
      "ea-philosophy",
      "ea-philosophy"
    ],
    "postCount": 96,
    "description": {
      "markdown": "The **Philosophy of effective altruism** tag covers posts about the philosophical ideas behind EA, especially when they don't discuss particular causes or research methods.\n\nFurther reading\n---------------\n\nMacAskill, William & Darius Meissner (2020) [Effective altruism](https://www.utilitarianism.net/acting-on-utilitarianism#effective-altruism), in 'Acting on utilitarianism', *Utilitarianism*.\n\nRelated entries\n---------------\n\n[philosophy](https://forum.effectivealtruism.org/topics/philosophy)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "xXRMBbnoM2P8BPF4k",
    "name": "Effective altruism lifestyle",
    "core": false,
    "slug": "effective-altruism-lifestyle",
    "oldSlugs": [
      "ea-lifestyle",
      "ea-lifestyle",
      "ea-lifestyle"
    ],
    "postCount": 171,
    "description": {
      "markdown": "Posts with the **effective altruism lifestyle** tag discuss ways to integrate effective altruism with the rest of one's life. Topics include mental health, childcare, personal finance, and more.\n\nRelated entries\n---------------\n\n[community experiences](https://forum.effectivealtruism.org/tag/community-experiences) | [demandingness of morality](https://forum.effectivealtruism.org/tag/demandingness-of-morality) | [personal finance](https://forum.effectivealtruism.org/tag/personal-finance) | [parenting](https://forum.effectivealtruism.org/tag/parenting) | [self-care](https://forum.effectivealtruism.org/tag/self-care)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "nGqPZkKJuiKfAqDcs",
    "name": "Effective altruism funding",
    "core": false,
    "slug": "effective-altruism-funding",
    "oldSlugs": [
      "ea-funding",
      "ea-funding",
      "ea-funding"
    ],
    "postCount": 228,
    "description": {
      "markdown": "Posts with the **effective altruism funding** tag discuss EA funding networks, the availability of funds for different cause areas, grantmaking strategy, etc.  \n  \nNot meant for posts that discuss specific funding decisions (for that, use the \"[Donation Writeup](https://forum.effectivealtruism.org/tag/donation-writeup)\" tag).\n\nExamples of EA funders includes: [EA Funds](https://forum.effectivealtruism.org/tag/effective-altruism-funds), [Open Philanthropy Project](https://forum.effectivealtruism.org/tag/open-philanthropy), [Survival & Flourishing Fund](https://forum.effectivealtruism.org/tag/survival-and-flourishing-fund), [Founders Pledge Funds](https://forum.effectivealtruism.org/tag/founders-pledge)\n\n## Related entries\n\n[certificate of impact](https://forum.effectivealtruism.org/tag/certificate-of-impact) | [diminishing returns](https://forum.effectivealtruism.org/tag/diminishing-returns) | [grantmaking](https://forum.effectivealtruism.org/tag/grantmaking) | [philanthropic coordination](https://forum.effectivealtruism.org/tag/philanthropic-coordination) | [philanthropic diversification](https://forum.effectivealtruism.org/tag/philanthropic-diversification) | [room for more funding](https://forum.effectivealtruism.org/tag/room-for-more-funding)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "FdGbtrAvvnkrfCbEL",
    "name": "Value drift",
    "core": false,
    "slug": "value-drift",
    "oldSlugs": [
      "value-drift"
    ],
    "postCount": 36,
    "description": {
      "markdown": "**Value drift** is change in the values held by an individual or a community.\n\nValue drift can be a good thing; over time, we may become wiser and adopt more sensible positions. On the other hand, we may want to avoid drifting away from positions we currently value highly. (Imagine talking to your future self and learning that you've moved to the opposite end of the political spectrum. Some people might recoil from this idea and wish to stop the change from happening.)\n\nThere has been some discussion in the effective altruism community about how good or bad various types of value drift are, how we could try to reduce value drift in ourselves or other people, and what implications rates of value drift have for various other efforts (e.g., for [movement-building](https://forum.effectivealtruism.org/tag/building-effective-altruism-1) work).\n\nFurther reading\n---------------\n\nAird, Michael (2020) [Collection of work on value drift that isn't on the EA Forum](https://forum.effectivealtruism.org/posts/EMKf4Gyee7BsY2RP8/michaela-s-shortform?commentId=a3dN6Mehzzipyu5sg), *Effective Altruism Forum*, April 10.\n\nRelated entries\n---------------\n\n[moral advocacy](https://forum.effectivealtruism.org/tag/moral-advocacy) | [movement collapse](https://forum.effectivealtruism.org/tag/movement-collapse)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "aELNHEKtcZtMwEkdK",
    "name": "Biosecurity",
    "core": true,
    "slug": "biosecurity",
    "oldSlugs": null,
    "postCount": 164,
    "description": {
      "markdown": "**Biosecurity** covers the procedures, practices and other measures used to manage risks from biological agents (e.g. viruses, bacteria) or their products (e.g. toxins).\n\nTerminology\n-----------\n\nThe term \"biosecurity\" has different meanings in different contexts. As a few examples:\n\n*   In agriculture, biosecurity usually refers to efforts to protect food crops or livestock from pests, invasive species, and infectious disease.\n*   In national security, biosecurity usually refers to preventing theft, diversion, or deliberate malicious use of biological knowledge, materials or technologies.\n*   In many non-English languages, biosecurity is not commonly distinguished from biosafety, or is seen as a sub-category of it.\n*   Within the effective altruism community, biosecurity has been characterized as primarily covering \"natural pandemics\", \"bioterrorism and the intentional deployment of biological weapons\", and \"dual use research and the possibility of accidental deployment of biological agents.\"^[\\[1\\]](#fn2i9zunnzzca)^\n\nFurther reading\n---------------\n\nAlexanian, Tessa (2021) [A biosecurity and biorisk reading+ list](https://forum.effectivealtruism.org/posts/iAowzcZm87wNrTQCb/a-biosecurity-and-biorisk-reading-list), *Effective Altruism Forum*, March 13 (updated 16 March 2021).\n\nZabel, Claire (2017) [Biosecurity as an EA cause area](https://www.effectivealtruism.org/articles/biosecurity-as-an-ea-cause-area-claire-zabel/), *Effective Altruism*, August 13.\n\nRelated entries\n---------------\n\n[biosurveillance](https://forum.effectivealtruism.org/tag/biosurveillance) | [COVID-19 pandemic](https://forum.effectivealtruism.org/tag/covid-19-pandemic) | [existential risk](https://forum.effectivealtruism.org/tag/existential-risk) | [global catastrophic biological risk](/tag/global-catastrophic-biological-risk) | [Johns Hopkins Center for Health Security](/tag/johns-hopkins-center-for-health-security) | [life sciences research](https://forum.effectivealtruism.org/tag/life-sciences-research) | [Nucleic Acid Observatory](https://forum.effectivealtruism.org/topics/nucleic-acid-observatory) | [Nuclear Threat Initiative](https://forum.effectivealtruism.org/tag/nuclear-threat-initiative) | [terrorism](https://forum.effectivealtruism.org/tag/terrorism) | [weapons of mass destruction](https://forum.effectivealtruism.org/tag/weapons-of-mass-destruction)\n\n1.  ^**[^](#fnref2i9zunnzzca)**^\n    \n    Open Philanthropy (2014) [Biosecurity](https://www.openphilanthropy.org/research/cause-reports/biosecurity), *Open Philanthropy*, January."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "psBzwdY8ipfCeExJ7",
    "name": "Cause prioritization",
    "core": true,
    "slug": "cause-prioritization",
    "oldSlugs": [
      "cause-prioritization",
      "cause-prioritization",
      "cause-prioritization",
      "cause-prioritization",
      "966561082467-where-to-buy-abortion-pills-in-saudi-arabia-mtp-1"
    ],
    "postCount": 574,
    "description": {
      "markdown": "**Cause prioritization** refers to efforts to find the most important causes to work on and compare interventions across different cause areas, so that we can do as much good as possible with the resources available to us. This tag is for posts relevant to cause prioritization.\n\nGovernments and philanthropists spend considerable resources attempting to do good in the world. *How* those resources are used, however, can make an even bigger difference than *how many* resources are available. Some work has been done on prioritizing *within* areas, but the even more important question on how to prioritize *between* areas has received remarkably limited attention. The effective altruism community, given its emphasis on [cause neutrality](https://concepts.effectivealtruism.org/concepts/neutrality-in-focus-area-selection/), is especially interested in answering the latter question.\n\nCause-neutral prioritization research seeks to identify new [promising focus areas](https://concepts.effectivealtruism.org/concepts/promising-areas/), and to compare their relative value (by considering, for instance, whether [animal welfare](https://concepts.effectivealtruism.org/concepts/animal-welfare) is a higher priority than [existential risk](https://concepts.effectivealtruism.org/concepts/existential-risks)). Given its very high importance and neglectedness, and moderate tractability (see [ITN framework](https://forum.effectivealtruism.org/topics/itn-framework)), cause prioritization research is itself often regarded as a highly promising research area.^[\\[1\\]](#fnrkj7kykbci)^\n\nFurther reading\n---------------\n\n80,000 Hours (2016) [Global priorities research](https://80000hours.org/problem-profiles/global-priorities-research/), *80,000 Hours*, April (updated July 2018).\n\nGrace, Katja (2014) [Conversation with Paul Christiano](https://80000hours.org/2014/08/conversation-with-paul-christiano-on-cause-prioritization-research/) [on](https://forum.effectivealtruism.org/posts/b6y9zSkRtxvKSdqcc/paul-christiano-on-cause-prioritization) [cause prioritization research](https://web.archive.org/web/20220712175031/https://80000hours.org/2014/08/conversation-with-paul-christiano-on-cause-prioritization-research), *80,000 Hours*, August 20.\n\nRelated entries\n---------------\n\n[Cause X](https://forum.effectivealtruism.org/topics/cause-x) | [cost-effectiveness](https://forum.effectivealtruism.org/topics/cost-effectiveness) | [distribution of cost-effectiveness](https://forum.effectivealtruism.org/topics/distribution-of-cost-effectiveness) | [global priorities research](https://forum.effectivealtruism.org/topics/global-priorities-research) | [impact assessment](https://forum.effectivealtruism.org/topics/impact-assessment) | [intervention evaluation](https://forum.effectivealtruism.org/topics/intervention-evaluation) | [ITN framework](https://forum.effectivealtruism.org/topics/itn-framework-1)\n\n1.  ^**[^](#fnrefrkj7kykbci)**^\n    \n    Stafforini, Pablo (2014) [Paul Christiano on cause prioritization research](https://forum.effectivealtruism.org/posts/b6y9zSkRtxvKSdqcc/paul-christiano-on-cause-prioritization), *Effective Altruism Forum*, March 23."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "sSZ8pCvackwaY9vBx",
    "name": "Criticism of effective altruism",
    "core": false,
    "slug": "criticism-of-effective-altruism",
    "oldSlugs": [
      "criticism-ea-movement",
      "criticisms-of-effective-altruism"
    ],
    "postCount": 258,
    "description": {
      "markdown": "**Criticism of effective altruism** collects critical discussion of [effective altruism](https://forum.effectivealtruism.org/topics/effective-altruism) ideas (as opposed to criticism of effective altruist [organizations](https://forum.effectivealtruism.org/tag/criticism-of-effective-altruist-organizations), [causes](https://forum.effectivealtruism.org/tag/criticism-of-effective-altruist-causes), or [culture](https://forum.effectivealtruism.org/tag/criticism-of-the-effective-altruism-community)).\n\nFurther reading\n---------------\n\nKuhn, Ben (2013) [A critique of effective altruism](https://www.benkuhn.net/ea-critique/), *Ben Kuhn’s Blog*, December 2.\n\nMcMahan, Jeff (2016) [Philosophical critiques of effective altruism](https://doi.org/10.5840/tpm20167379), *The Philosophers’ Magazine*, vol. 73, pp. 92–99.\n\nNielsen, Michael (2022) [Notes on effective altruism](https://michaelnotebook.com/eanotes/), *Michael’s Notebook*, June 2.\n\nRowe, Abraham (2022) [Critiques of EA that I want to read](https://forum.effectivealtruism.org/posts/n3WwTz4dbktYwNQ2j/critiques-of-ea-that-i-want-to-read), *Effective Altruism Forum*, June 19.\n\nWiblin, Robert & Keiran Harris (2019) [Vitalik Buterin on effective altruism, better ways to fund public goods, the blockchain’s problems so far, and how it could yet change the world](https://80000hours.org/podcast/episodes/vitalik-buterin-new-ways-to-fund-public-goods/), *80,000 Hours*, September 3.\n\nZhang, Linchuan (2021) [The motivated reasoning critique of effective altruism](https://forum.effectivealtruism.org/posts/pxALB46SEkwNbfiNS/the-motivated-reasoning-critique-of-effective-altruism), *Effective Altruism Forum*, September 14.\n\nRelated entries\n---------------\n\n[criticism of effective altruist causes](https://forum.effectivealtruism.org/tag/criticism-of-effective-altruist-causes) | [criticism of effective altruist organizations](https://forum.effectivealtruism.org/tag/criticism-of-effective-altruist-organizations) | [criticism of effective altruism culture](https://forum.effectivealtruism.org/topics/criticism-of-effective-altruism-culture) | [criticism of longtermism and existential risk studies](https://forum.effectivealtruism.org/topics/criticism-of-longtermism-and-existential-risk-studies) **|** [effective altruism](https://forum.effectivealtruism.org/tag/effective-altruism) | [red teaming](https://forum.effectivealtruism.org/tag/red-teaming) | [scalably using labour](https://forum.effectivealtruism.org/tag/scalably-using-labour) | [social and intellectual movements](https://forum.effectivealtruism.org/tag/social-and-intellectual-movements) | [working at EA vs non-EA orgs](https://forum.effectivealtruism.org/tag/working-at-ea-vs-non-ea-orgs)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "FdbA8vts5JPKCEou8",
    "name": "Long-term future",
    "core": false,
    "slug": "long-term-future",
    "oldSlugs": [
      "long-term-future"
    ],
    "postCount": 135,
    "description": {
      "markdown": "The **long-term future** focuses on possible ways in which the future of humanity may unfold over long timescales.\n\nBostrom's typology of possible scenarios\n----------------------------------------\n\n[Nick Bostrom](https://forum.effectivealtruism.org/tag/nick-bostrom) has identified four broad possibilities for the future of humanity.^[\\[1\\]](#fnms8bxsox85m)^\n\nFirst, humans may go prematurely [extinct](https://forum.effectivealtruism.org/tag/human-extinction). Since the universe will eventually become inhospitable, extinction is inevitable in the very long run. However, it is also plausible that people will die out far before this deadline.\n\nSecond, human civilization may plateau, reaching a level of technological advancement beyond which no further advancement is feasible.\n\nThird, human civilization may experience recurrent [collapse](https://forum.effectivealtruism.org/tag/civilizational-collapse), undergoing repeated declines or catastrophes that prevent it from moving beyond a certain level of advancement.\n\nFourth, human civilization may advance so significantly as to become nearly unrecognizable. Bostrom conceptualizes this scenario as a “posthuman” era where people have developed significantly different cognitive abilities, population sizes, body types, sensory or emotional experiences, or life expectancies.\n\nFurther reading\n---------------\n\nBaum, Seth D. *et al.* (2019) [Long-term trajectories of human civilization](http://doi.org/10.1108/FS-04-2018-0037), *Foresight*, vol. 21, pp. 53–83.\n\nBostrom, Nick (2009) [The future of humanity](http://doi.org/10.1057/9780230227279_10), in Jan Kyrre Berg Olsen, Evan Selinger & Søren Riis (eds.) *New Waves in Philosophy of Technology*, London: Palgrave Macmillan, pp. 186–215.\n\nHanson, Robin (1998) [Long-term growth as a sequence of exponential modes](http://mason.gmu.edu/~rhanson/longgrow.pdf), working paper, George Mason University (updated December 2000).\n\nRoodman, David (2020) [Modeling the human trajectory](https://www.openphilanthropy.org/blog/modeling-human-trajectory), *Open Philanthropy*, June 15.\n\nRelated entries\n---------------\n\n[longtermism](https://forum.effectivealtruism.org/tag/longtermism) | [non-humans and the long-term future](/tag/non-humans-and-the-long-term-future) | [space colonization](https://forum.effectivealtruism.org/tag/space-colonization)\n\n1.  ^**[^](#fnrefms8bxsox85m)**^\n    \n    Bostrom, Nick (2009) [The future of humanity](http://doi.org/10.1057/9780230227279_10), in Jan Kyrre Berg Olsen, Evan Selinger & Søren Riis (eds.) *New Waves in Philosophy of Technology*, London: Palgrave Macmillan, pp. 186–215."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "t2L2RziMDLEuHBWNF",
    "name": "Longtermism",
    "core": false,
    "slug": "longtermism",
    "oldSlugs": [
      "longtermism-philosophy"
    ],
    "postCount": 430,
    "description": {
      "markdown": "**Longtermism** is the view that positively influencing the [long-term future](https://forum.effectivealtruism.org/tag/long-term-future) is a key moral priority of our time.^[\\[1\\]](#fn2iwddw4crwt)^^[\\[2\\]](#fnz6yx5b2rpim)^\n\nLongtermism may be seen as following from the conjunction of three key claims.^[\\[3\\]](#fn6frik8lwexe)^ First, future people matter morally. Second, the vast majority of people that will ever exist, if Earth-originating intelligence is not prematurely extinguished, exist in the future. Third, people alive today can predictably influence whether these people exist, and how well their lives go.\n\nTypes of longtermism\n--------------------\n\n### Strong vs. weak longtermism\n\nStrong longtermism holds that positively influencing the long-term future is the key moral priority of our time. This form of longtermism was introduced by [Hilary Greaves](https://forum.effectivealtruism.org/topics/hilary-greaves) and [Will MacAskill](https://forum.effectivealtruism.org/topics/william-macaskill),^[\\[4\\]](#fn4lqrm1tu6v4)^ and has precedents in the work of [Nick Bostrom](https://forum.effectivealtruism.org/tag/nick-bostrom),^[\\[5\\]](#fnijdcuqtcsja)^^[\\[6\\]](#fnrxubxlbto4l)^ [Nick Beckstead](https://forum.effectivealtruism.org/topics/nick-beckstead),^[\\[7\\]](#fnohggtozamwg)^^[\\[8\\]](#fn1j6xqn8ih7f)^ and others. Note that the authors do not define or discuss \"weak\" longtermism; the contrast is rather with longtermism as such, which as noted above holds that positively influencing the long-term future is a key priority, but not necessarily the top priority. \n\n### Patient vs. urgent longtermism\n\nThis distinction can be explained in reference to the [hinge of history hypothesis](https://forum.effectivealtruism.org/topics/hinge-of-history), i.e. the hypothesis that we are currently living at a time where humanity has unusually high influence over the long-term future. Urgent longtermists find the hypothesis plausible and, accordingly, hold that it makes sense to spend our altruistic resources relatively quickly. (Altruistic resources include not just financial assets, but other resources that can accumulate and be spent deliberately in the pursuit of altruistic goals, such as credibility, career capital and coordination ability.) By contrast, patient longtermists hold that the opportunities for influence are not concentrated in the near term and, in line with this, they favour investing these resources so that they can be deployed at some point in the future, when the moments of significant influence arrive.\n\n### Broad vs. targeted longtermism\n\nThis distinction was originally introduced by Nick Beckstead in his doctoral dissertation, *On the Overwhelming Importance of Shaping the Far Future*.^[\\[9\\]](#fn73581e7p8q)^ Targeted (or narrow) longtermism attempts to positively influence the long-term future by focusing on specific, identifiable scenarios, such as the risks of [misaligned AI](https://forum.effectivealtruism.org/topics/ai-risk) or an [engineered pandemic](https://forum.effectivealtruism.org/topics/biosecurity). By contrast, broad longtermism tries to have a long-term influence by pursuing general approaches with the potential to be useful in a broader range of contexts, such as [building effective altruism](https://forum.effectivealtruism.org/topics/building-effective-altruism-1) or [promoting global cooperation](https://forum.effectivealtruism.org/topics/moral-cooperation).\n\nBoth patient and urgent longtermism, and broad and targeted longtermism, are positions that exist on a continuum. The terms \"urgent\"/\"patient\" and \"broad\"/\"targeted\" divide these continua into two discrete regions, similarly to how the terms \"tall\" and \"short\" divide the height continuum. These distinctions may thus be seen as uncovering an underlying dimension along which longtermism can vary. Accordingly, the more relevant questions are \"How patient/urgent should longtermism be?\" or \"How broad/targeted should longtermist interventions be?\", rather than \"Should longtermism be patient or urgent?\" or \"Should longtermism be targeted or narrow?\"\n\n### Other distinctions\n\nOne additional distinction sometimes made—which also originates in Greaves and MacAskill—is between *axiological* and *deontic* longtermism. (\"Axiological\" and \"deontic\" are technical terms borrowed from [moral philosophy](https://forum.effectivealtruism.org/topics/moral-philosophy): \"axiological\" means \"related to what is good or valuable\" and \"deontic\" means \"related to what we ought to do or have reason to do\".) Axiological longtermism holds that positively influencing the long-term future is among the most valuable things we can do, whereas deontic longtermism holds that positively influencing the long-term future is among the things we have most reason to do. Sometimes these views are combined with strong longtermism, so axiological strong longtermism becomes the view that influencing the long-term future is the most valuable thing to do and deontic strong longtermism becomes the view that influencing the long-term future is the thing we have most reason to do.\n\nAnother relevant distinction, introduced by the philosophers Johan Gustafsson and Petra Kosonen, is between *normative* and *prudential* longtermism.^[\\[10\\]](#fn9uksbqguhw)^ Normative longtermism is longtermism as it is normally understood, i.e. as a view about what is valuable, or we have reason to do, from an impersonal or moral perspective. However, one can also consider a form of longtermism focused on what is in a person's self-interest. If humans could live for thousands or millions of years, it could be argued that, even from a self-interested perspective, each person should focus primarily on the long-term effects of their actions, because a person's lifetime wellbeing will be largely determined by those effects.\n\nFurther reading\n---------------\n\nBalfour, Dylan (2021) [Longtermism: how much should we care about the far future?](https://1000wordphilosophy.com/2021/09/17/longtermism/), *1000-Word Philosophy: An Introductory Anthology*, September 17.\n\nBeckstead, Nick (2019) [A brief argument for the overwhelming importance of shaping the far future](https://doi.org/10.1093/oso/9780198841364.003.0006), in Hilary Greaves & Theron Pummer (eds.) *Effective Altruism: Philosophical Issues*, Oxford: Oxford University Press, pp. 80–98.\n\nMacAskill, William (2022) [What is longtermism and why does it matter?](https://www.bbc.com/future/article/20220805-what-is-longtermism-and-why-does-it-matter), *BBC News*, August 8.\n\nMacAskill, William, Hilary Greaves & Elliott Thornley (2021) [The moral case for long-term thinking](https://en.wikipedia.org/wiki/Special:BookSources/978-0-9957281-8-9), in Natalie Cargill & Tyler John (eds.) *The Long View: Essays on Policy, Philanthropy, and the Long-Term Future*, London: First, pp. 19–28.\n\nMoorhouse, Fin (2021) [Introduction to longtermism](https://www.effectivealtruism.org/articles/longtermism/), *Effective Altruism*, January 27.\n\nRoser, Max (2022) [The future is vast: longtermism’s perspective on humanity’s past, present, and future](https://ourworldindata.org/longtermism), *Our World in Data*, March 15.\n\nTodd, Benjamin (2017) [Longtermism: the moral significance of future generations](https://80000hours.org/articles/future-generations/), *80,000 Hours*, October.\n\nWikipedia (2021) [Longtermism](https://en.wikipedia.org/w/index.php?title=Longtermism&oldid=1090541975), *Wikipedia*, October 28 (updated 30 May 2022‎).\n\nExternal links\n--------------\n\n[Longtermism](https://longtermism.com/). Online introduction to longtermism.\n\nRelated entries\n---------------\n\n[criticism of longtermism and existential risk studies](https://forum.effectivealtruism.org/topics/criticism-of-longtermism-and-existential-risk-studies) **|** [ethics of existential risk](/tag/ethics-of-existential-risk) | [existential risk](https://forum.effectivealtruism.org/tag/existential-risk) | [existential security](/tag/existential-security) | [global health and wellbeing](https://forum.effectivealtruism.org/tag/global-health-and-wellbeing) | [institutions for future generations](https://forum.effectivealtruism.org/tag/institutions-for-future-generations) | [long-range forecasting](https://forum.effectivealtruism.org/tag/long-range-forecasting) | [macrostrategy](https://forum.effectivealtruism.org/tag/macrostrategy) | [non-humans and the long-term future](/tag/non-humans-and-the-long-term-future) | [patient altruism](https://forum.effectivealtruism.org/tag/patient-altruism-1) | [trajectory change](https://forum.effectivealtruism.org/tag/trajectory-change)\n\n1.  ^**[^](#fnref2iwddw4crwt)**^\n    \n    Ord, Toby (2020) [*The Precipice: Existential Risk and the Future of Humanity*](https://en.wikipedia.org/wiki/Special:BookSources/1526600218), London: Bloomsbury Publishing, p. 46.\n    \n2.  ^**[^](#fnrefz6yx5b2rpim)**^\n    \n    MacAskill, William (2022) [*What We Owe the Future*](https://en.wikipedia.org/wiki/Special:BookSources/978-1-5416-1862-6), New York: Basic Books.\n    \n3.  ^**[^](#fnref6frik8lwexe)**^\n    \n    Moorhouse, Fin (2021) [Introduction to longtermism](https://www.effectivealtruism.org/articles/longtermism/), *Effective Altruism*, January 27. \n    \n4.  ^**[^](#fnref4lqrm1tu6v4)**^\n    \n    Greaves, Hilary & William Macaskill (2021) [The case for strong longtermism](https://globalprioritiesinstitute.org/hilary-greaves-william-macaskill-the-case-for-strong-longtermism-2/), Global Priorities Institute, University of Oxford.\n    \n5.  ^**[^](#fnrefijdcuqtcsja)**^\n    \n    Bostrom, Nick (2003) [Astronomical waste: The opportunity cost of delayed technological development](https://doi.org/10.1017/S0953820800004076), *Utilitas*, vol. 15, pp. 308–314.\n    \n6.  ^**[^](#fnrefrxubxlbto4l)**^\n    \n    Bostrom, Nick (2013) [Existential risk prevention as global priority](https://doi.org/10.1111/1758-5899.12002), *Global Policy*, vol. 4, pp. 15–31.\n    \n7.  ^**[^](#fnrefohggtozamwg)**^\n    \n    Beckstead, Nick (2013) [*On the Overwhelming Importance of Shaping the Far Future*](http://doi.org/10.7282/T35M649T), doctoral thesis, Rutgers University.\n    \n8.  ^**[^](#fnref1j6xqn8ih7f)**^\n    \n    Beckstead, Nick (2019) [A brief argument for the overwhelming importance of shaping the far future](https://doi.org/10.1093/oso/9780198841364.003.0006), in Hilary Greaves & Theron Pummer (eds.) *Effective Altruism: Philosophical Issues*, Oxford: Oxford University Press, pp. 80–98.\n    \n9.  ^**[^](#fnref73581e7p8q)**^\n    \n    Beckstead, Nick (2013) [*On the Overwhelming Importance of Shaping the Far Future*](https://doi.org/10.7282/T35M649T), PhD thesis, Rutgers University.\n    \n10.  ^**[^](#fnref9uksbqguhw)**^\n    \n    Gustafsson, Johan & Petra Kosonen (2022) 'Prudential longtermism', unpublished.\n    \n11.  ^**[^](#fnrefihuafonhwkm)**^\n    \n    MacAskill, William (2019) [“Longtermism”](https://forum.effectivealtruism.org/posts/qZyshHCNkjs3TvSem/longtermism), *Effective Altruism Forum*, July 25.\n    \n12.  ^**[^](#fnref92v82tny4ss)**^\n    \n    Greaves, Hilary & William MacAskill (2021) [The case for strong longtermism](https://globalprioritiesinstitute.org/hilary-greaves-william-macaskill-the-case-for-strong-longtermism/), working paper."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "9GQf4Ec6ckqvnPBSw",
    "name": "AI alignment",
    "core": false,
    "slug": "ai-alignment",
    "oldSlugs": [
      "ai-alignment",
      "ai-alignment",
      "ai-alignment"
    ],
    "postCount": 406,
    "description": {
      "markdown": "**AI alignment** is research on how to align [AI systems](https://forum.effectivealtruism.org/topics/artificial-intelligence) with human or moral goals.\n\nEvaluation\n----------\n\n[80,000 Hours](https://forum.effectivealtruism.org/tag/80-000-hours) rates AI alignment a \"highest priority area\": a problem at the top of their ranking of global issues assessed by [importance, tractability and neglectedness](https://forum.effectivealtruism.org/tag/itn-framework-1).^[\\[1\\]](#fnorjzl17i9vi)^\n\nFurther reading\n---------------\n\nChristiano, Paul (2020) [Current work in AI alignment](https://forum.effectivealtruism.org/posts/63stBTw3WAW6k45dY/paul-christiano-current-work-in-ai-alignment), *Effective Altruism Forum*, April 3.\n\nShah, Rohin (2020) [What’s been happening in AI alignment?](https://forum.effectivealtruism.org/posts/nqTdRNngCGDD54owu/rohin-shah-what-s-been-happening-in-ai-alignment), *Effective Altruism Forum*, July 29.\n\nExternal links\n--------------\n\n[AI Alignment Forum](https://alignmentforum.org/).\n\nRelated entries\n---------------\n\n[AI governance](https://forum.effectivealtruism.org/topics/ai-governance) | [AI forecasting](https://forum.effectivealtruism.org/tag/ai-forecasting) | [alignment tax](https://forum.effectivealtruism.org/tag/alignment-tax) | [Center for Human-Compatible Artificial Intelligence](https://forum.effectivealtruism.org/tag/center-for-human-compatible-artificial-intelligence) | [Machine Intelligence Research Institute](/tag/machine-intelligence-research-institute) | [rationality community](https://forum.effectivealtruism.org/tag/rationality-community)\n\n1.  ^**[^](#fnreforjzl17i9vi)**^\n    \n    80,000 Hours (2021) [Our current list of the most important world problems](https://80000hours.org/problem-profiles/), *80,000 Hours*."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "oEvBquSzz56ts7pEn",
    "name": "Moral philosophy",
    "core": true,
    "slug": "moral-philosophy",
    "oldSlugs": [
      "moral-philosophy"
    ],
    "postCount": 306,
    "description": {
      "markdown": "The **moral philosophy** tag is very wide-ranging, and can be used alongside more specific tags like [philosophy of effective altruism](https://forum.effectivealtruism.org/topics/philosophy-of-effective-altruism), [longtermism](https://forum.effectivealtruism.org/tag/longtermism), and [moral patienthood](https://forum.effectivealtruism.org/tag/moral-patienthood).\n\nFurther reading\n---------------\n\nParfit, Derek (1984) [*Reasons and Persons*](https://en.wikipedia.org/wiki/Special:BookSources/0-19-824908-X), Oxford: Clarendon press.\n\nSidgwick, Henry (1907) *The Methods of Ethics*, 7th ed., London: Macmillan.\n\nRelated entries\n---------------\n\n[applied ethics](https://forum.effectivealtruism.org/tag/applied-ethics) | [infinite ethics](https://forum.effectivealtruism.org/topics/infinite-ethics) | [metaethics](https://forum.effectivealtruism.org/tag/metaethics) | [normative ethics](https://forum.effectivealtruism.org/tag/normative-ethics) | [philosophy](https://forum.effectivealtruism.org/tag/philosophy)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "M4Cc8pitqt8LXMRqW",
    "name": "Subjective wellbeing",
    "core": false,
    "slug": "subjective-wellbeing",
    "oldSlugs": [
      "subjective-well-being",
      "subjective-well-being"
    ],
    "postCount": 71,
    "description": {
      "markdown": "**Subjective wellbeing** (**SWB**) is a self-reported measure of a person's [wellbeing](https://forum.effectivealtruism.org/topics/wellbeing).\n\nFurther reading\n---------------\n\nDiener, Ed, Shigehiro Oishi & Louis Tay (2018) [Advances in subjective well-being research](https://doi.org/10.1038/s41562-018-0307-6), *Nature Human Behaviour*, vol. 2, pp. 253–260.\n\nRelated entries\n---------------\n\n[adjusted life year](https://forum.effectivealtruism.org/topics/adjusted-life-year) | [wellbeing](https://forum.effectivealtruism.org/topics/wellbeing)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "95jaA45tFMB7ENWe2",
    "name": "Rationality",
    "core": false,
    "slug": "rationality",
    "oldSlugs": null,
    "postCount": 171,
    "description": {
      "markdown": "Use the **Rationality** tag for posts about trying to think more clearly in general (not just about a particular cause, intervention, etc.)\n\nThis tag can cover personal rationality (e.g. making better decisions or predictions) as well as group or movement-level rationality (e.g. how the EA movement can do a better job of estimating the cost-effectiveness of an intervention).\n\n## Related entries\n\n[cognitive bias](https://forum.effectivealtruism.org/tag/cognitive-bias)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "Zj5Xg4ymXCxcD4Mzd",
    "name": "Investing",
    "core": false,
    "slug": "investing",
    "oldSlugs": null,
    "postCount": 76,
    "description": {
      "markdown": "The **Investing** tag covers posts about how we can invest our money to produce higher impact, whether we want to increase our expected returns, create stable funding for long-term endeavors, or pursue other goals related to doing more good.\n\nThis differs from the [Socially responsible investing](https://forum.effectivealtruism.org/tag/socially-responsible-investing) tag, which concerns investments which are directly aimed at generating positive impact.\n\n(Please be very careful when reading free investment advice distributed on an open online forum.)\n\n## Related entries\n[patient altruism](https://forum.effectivealtruism.org/tag/patient-altruism) | [socially responsible investing](https://forum.effectivealtruism.org/tag/socially-responsible-investing)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "Fi2msh65RjSDaEDxx",
    "name": "Climate change",
    "core": false,
    "slug": "climate-change",
    "oldSlugs": [
      "climate-change",
      "climate-change",
      "climate-change"
    ],
    "postCount": 204,
    "description": {
      "markdown": "In the sense that matters most for [effective altruism](https://forum.effectivealtruism.org/tag/effective-altruism), **climate change** refers to large-scale shifts in weather patterns that result from emissions of greenhouse gases such as carbon dioxide and methane largely from fossil fuel consumption. Climate change has the potential to result in—and to some extent is already resulting in—increased natural disasters, increased water and [food insecurity](https://forum.effectivealtruism.org/tag/food-security), and widespread species extinction and habitat loss.\n\nEvaluation\n----------\n\n[80,000 Hours](https://forum.effectivealtruism.org/tag/80-000-hours) rates reducing extreme risks from climate change a \"second-highest priority area\": an unusually pressing global problem ranked slightly below their four highest priority areas.^[\\[1\\]](#fnizmp6zpls8q)^\n\nRecommendations\n---------------\n\nIn [*The Precipice: Existential Risk and the Future of Humanity*](https://forum.effectivealtruism.org/tag/the-precipice), [Toby Ord](https://forum.effectivealtruism.org/tag/toby-ord) offers several [policy](https://forum.effectivealtruism.org/tag/policy) and [research](https://forum.effectivealtruism.org/tag/research) recommendations for handling risks from climate change:^[\\[2\\]](#fnchcrqxsxdus)^\n\n*   Fund research and development of innovative approaches to clean energy.\n*   Fund research into safe [geoengineering](https://forum.effectivealtruism.org/topics/climate-engineering) technologies and geoengineering governance.\n*   Perform more research on the possibilities of a runaway greenhouse effect or moist greenhouse effect. Are there any ways these could be more likely than is currently believed? Are there any ways we could decisively rule them out?\n*   Improve our understanding of the permafrost and methane clathrate feedbacks.\n*   Improve our understanding of cloud feedbacks.\n*   Better characterize our uncertainty about the climate sensitivity: what can and can’t we say about the right-hand tail of the distribution.\n*   Improve our understanding of extreme warming (e.g., 5–20°C), including searching for concrete mechanisms through which it could pose a plausible threat of [human extinction](https://forum.effectivealtruism.org/topics/human-extinction) or the [global collapse of civilization](https://forum.effectivealtruism.org/topics/civilizational-collapse).\n\nFurther reading\n---------------\n\nAckva, Johannes & John Halstead (2020) [Climate change executive summary](https://founderspledge.com/stories/climate-change-executive-summary), *Founders Pledge*, October 6.\n\nHalstead, John (2022) [Climate change & longtermism](https://docs.google.com/document/d/1az3MesNlGDETeJ8jGoyK-MTH-lCKbYg4EP4khQ-7naA), *What We Owe the Future: Supplementary Materials*.\n\nHilton, Benjamin (2022) [Climate change: is climate change the greatest threat facing humanity today?](https://80000hours.org/problem-profiles/climate-change/), *80,000 Hours*, May 18.\n\nWiblin, Robert (2021) [Kelly Wanser on whether to deliberately intervene in the climate](https://80000hours.org/podcast/episodes/kelly-wanser-climate-interventions), *80,000 Hours*, March 26.\n\nWiblin, Robert & Arden Koehler (2020) [Mark Lynas on climate change, societal collapse & nuclear energy](https://80000hours.org/podcast/episodes/mark-lynas-climate-change-nuclear-energy/), *80,000 Hours*, August 20.\n\nExternal links\n--------------\n\n[The Intergovernmental Panel on Climate Change.](https://www.ipcc.ch/) Official website of the [United Nations](https://forum.effectivealtruism.org/topics/united-nations-1) body for assessing the science related to climate change. \n\nRelated entries\n---------------\n\n[biodiversity loss](https://forum.effectivealtruism.org/topics/biodiversity-loss) | [climate engineering](https://forum.effectivealtruism.org/tag/climate-engineering) | [environmental science](https://forum.effectivealtruism.org/topics/environmental-science) | [global catastrophic risk](https://forum.effectivealtruism.org/tag/global-catastrophic-risk) | [nuclear energy](https://forum.effectivealtruism.org/topics/nuclear-energy)\n\n1.  ^**[^](#fnrefizmp6zpls8q)**^\n    \n    80,000 Hours (2021) [Our current list of the most important world problems](https://80000hours.org/problem-profiles/), *80,000 Hours*.\n    \n2.  ^**[^](#fnrefchcrqxsxdus)**^\n    \n    Ord, Toby (2020) [*The Precipice: Existential Risk and the Future of Humanity*](https://en.wikipedia.org/wiki/Special:BookSources/1-5266-0021-8), London: Bloomsbury Publishing, p. 279"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "ZQWzqdimDZdAuGCrH",
    "name": "Hinge of history",
    "core": false,
    "slug": "hinge-of-history",
    "oldSlugs": [
      "hinge-of-history",
      "hinge-of-history",
      "hinge-of-history-hypothesis"
    ],
    "postCount": 46,
    "description": {
      "markdown": "The **hinge of history** is a hypothetical time in human history in which humanity has disproportionate influence over the [long-term future](https://forum.effectivealtruism.org/tag/long-term-future). The **hinge of history hypothesis** is the view that we are currently living at the hinge of history.^[\\[1\\]](#fnx1yj3snhlfn)^^[\\[2\\]](#fny99cs7uydvl)^ The hypothesis has been explicitly endorsed by a number of key figures in the [effective altruism](https://forum.effectivealtruism.org/tag/effective-altruism) community, including [Derek Parfit](https://forum.effectivealtruism.org/tag/derek-parfit),^[\\[3\\]](#fnusloiix2h4j)^^[\\[4\\]](#fn5duigofaygf)^^[\\[5\\]](#fnu4xtdlh92)^ [Toby Ord](https://forum.effectivealtruism.org/tag/toby-ord)^[\\[6\\]](#fnzh34a3evvui)^ and [Holden Karnofsky](https://forum.effectivealtruism.org/tag/holden-karnofsky),^[\\[7\\]](#fn0h5zjvb1gjw)^ and is arguably implicit in the writings of other prominent authors, such as [Eliezer Yudkowsky](https://forum.effectivealtruism.org/tag/eliezer-yudkowsky) and [Nick Bostrom](https://forum.effectivealtruism.org/tag/nick-bostrom).\n\nWhether the hinge of history hypothesis is true may determine how the present generation ought to [prioritize between causes](https://forum.effectivealtruism.org/tag/cause-prioritization) or [choose between giving and investing](https://forum.effectivealtruism.org/tag/timing-of-philanthropy), and may have other important implications.\n\nFurther reading\n---------------\n\nFisher, Richard (2020) [Are we living at the “hinge of history”?](https://web.archive.org/web/20200924112824/https://www.bbc.com/future/article/20200923-the-hinge-of-history-long-termism-and-existential-risk), *BBC Future*, September 23.\n\nKelsey Piper (2019) [Is this the most important century in human history?](https://www.vox.com/2019/9/26/20880334/is-this-the-most-important-century-in-human-history), *Vox*, September 26.\n\nMacAskill, William (2022) [Are we living at the hinge of history?](https://doi.org/10.1093/oso/9780192894250.003.0013), in Jeff McMahan *et al.* (eds.) *Ethics and Existence: The Legacy of Derek Parfit*, Oxford: Oxford University Press, pp. 331–357.\n\nTrammell, Philip (2020) [Philanthropy timing and the Hinge of History](https://youtu.be/AddUn9BFFkA), *Effective Altruism Global*, February 11.\n\nWiblin, Robert & Keiran Harris (2020) [Will MacAskill on the moral case against ever leaving the house, whether now is the hinge of history, and the culture of effective altruism](https://80000hours.org/podcast/episodes/will-macaskill-paralysis-and-hinge-of-history/), *80,000 Hours*, January 24.\n\nRelated entries\n---------------\n\n[cause prioritization](https://forum.effectivealtruism.org/tag/cause-prioritization) | [long-term future](https://forum.effectivealtruism.org/tag/long-term-future) | [patient altruism](https://forum.effectivealtruism.org/tag/patient-altruism) | [time of perils](https://forum.effectivealtruism.org/topics/time-of-perils) | [timing of philanthropy](https://forum.effectivealtruism.org/tag/timing-of-philanthropy)\n\n1.  ^**[^](#fnrefx1yj3snhlfn)**^\n    \n    MacAskill, William (2019) [Are we living at the most influential time in history?](https://forum.effectivealtruism.org/posts/XXLf6FmWujkxna3E6/are-we-living-at-the-most-influential-time-in-history-1), *Effective Altruism Forum*, September 3.\n    \n2.  ^**[^](#fnrefy99cs7uydvl)**^\n    \n    MacAskill, William (2022) [Are we living at the hinge of history?](https://doi.org/10.1093/oso/9780192894250.003.0013), in Jeff McMahan *et al.* (eds.) *Ethics and Existence: The Legacy of Derek Parfit*, Oxford: Oxford University Press, pp. 331–357.\n    \n3.  ^**[^](#fnrefusloiix2h4j)**^\n    \n    \"the part of our moral theory \\[...\\] that covers how we affect future generations \\[...\\] is the most important part of our moral theory, since the next few centuries will be the most important in human history.\" (Parfit, Derek (1984) [*Reasons and Persons*](https://en.wikipedia.org/wiki/Special:BookSources/0-19-824908-X), Oxford: Clarendon Press, p. 351)\n    \n4.  ^**[^](#fnref5duigofaygf)**^\n    \n    \"We live during the hinge of history. Given the scientific and technological discoveries of the last two centuries, the world has never changed as fast. We shall soon have even greater powers to transform, not only our surroundings, but ourselves and our successors. If we act wisely in the next few centuries, humanity will survive its most dangerous and decisive period. Our descendants could, if necessary, go elsewhere, spreading through this galaxy.\" (Parfit, Derek (2011) [*On What Matters*](https://en.wikipedia.org/wiki/Special:BookSources/978-0-19-957280-9), vol. 2, Oxford: Oxford University Press, p. 616)\n    \n5.  ^**[^](#fnrefu4xtdlh92)**^\n    \n    \"I think that we are living now in the most critical part of human history. The twentieth century, I think, was the best and worst of all centuries so far. But it now seems fairly likely that there are no intelligent beings anywhere else in the observable universe. Now, if that's true, we may be living in the most critical part of the history of the universe... \\[The reason\\] why this may be the critical period in the history of the universe is if we are the only rational intelligent beings, it's only we who might provide the origin of what would then become a galaxy-wide civilisation, which lasted for billions of years, and in which life was much better than it is for most human beings. Well, if you look at the scale there between human history so far and what could come about, it's enormous. And what's critical is that we could blow it, we could end it.\" (Parfit, Derek (2015) [Full address](https://www.youtube.com/watch?v=xTUrwO9-B_I), *Oxford Union*, October 10, 12:47–15:51)\n    \n6.  ^**[^](#fnrefzh34a3evvui)**^\n    \n    Ord, Toby (2020) [*The Precipice: Existential Risk and the Future of Humanity*](https://en.wikipedia.org/wiki/Special:BookSources/1526600218), London: Bloomsbury Publishing.\n    \n7.  ^**[^](#fnref0h5zjvb1gjw)**^\n    \n    Karnofsky, Holden (2021) [Roadmap for the ‘most important century’ series](https://www.cold-takes.com/roadmap-for-the-most-important-century-series/), *Cold Takes*, June 30."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "CGameg7coDgLbtgdH",
    "name": "Forecasting",
    "core": false,
    "slug": "forecasting",
    "oldSlugs": null,
    "postCount": 261,
    "description": {
      "markdown": "**Forecasting** is the art and science of predicting the future. Forecasting can help us get a better sense of what will happen in the future, so that we can prepare appropriately. It is a significant aspect of EA strategy around pandemic preparedness (see [biosecurity](https://forum.effectivealtruism.org/tag/biosecurity)), [AI alignment](https://forum.effectivealtruism.org/tag/ai-alignment), [animal product alternatives](https://forum.effectivealtruism.org/tag/animal-product-alternatives), and many other topics.\n\nForecasting is a feedback loop. Forecasters make predictions and then when those predictions resolve forecasters get scores. This allows the forecasters to improve and allows everyone else to know who the best forecasters are. In the future, we can weigh the forecasts of the better forecasters more heavily.\n\nFurther reading\n---------------\n\nAird, Michael (2020) [Failures in technology forecasting? A reply to Ord and Yudkowsky](https://www.lesswrong.com/posts/3qypPmmNHEmqegoFF/failures-in-technology-forecasting-a-reply-to-ord-and), *LessWrong*, May 8.\n\nDart Throwing Spider Monkey (2020) [Intro to forecasting 01 - What is it and why should I care?](https://www.youtube.com/watch?v=e6Q7Ez3PkOw), *YouTube*, October 26.\n\nKokotajlo, Daniel (2019) [Evidence on good forecasting practices from the Good Judgment Project: an accompanying blog post](https://aiimpacts.org/evidence-on-good-forecasting-practices-from-the-good-judgment-project-an-accompanying-blog-post/), *AI Impacts*.  \n*An excellent summary of the evidence on good forecasting practices.*\n\nLewis, Gregory (2020) [Challenges in evaluating forecaster performance](https://forum.effectivealtruism.org/posts/JsTpuMecjtaG5KHbb/challenges-in-evaluating-forecaster-performance), *Effective Altruism Forum*, September 8.\n\nMuehlhauser, Luke (2016) [Evaluation of some technology forecasts from “The Year 2000”](https://www.openphilanthropy.org/evaluation-some-technology-forecasts-year-2000), *Open Philanthropy*, September.\n\nMuehlhauser, Luke (2019) [How feasible is long-range forecasting?](https://www.openphilanthropy.org/blog/how-feasible-long-range-forecasting), *Open Philanthropy*, October 10.\n\nMuehlhauser, Luke (2021) [Superforecasting in a nutshell](http://lukemuehlhauser.com/superforecasting-in-a-nutshell/), *Luke Muehlhauser’s Website*, February 22.\n\nTetlock, Philip E. (2006) [*Expert Political Judgment: How Good Is It? How Can We Know?*](https://en.wikipedia.org/wiki/Special:BookSources/978-0-691-12871-9), Princeton: Princeton Univ. Press.\n\nTetlock, Philip E. & Dan Gardner (2015) [*Superforecasting: The Art and Science of Prediction*](https://en.wikipedia.org/wiki/Special:BookSources/978-0-8041-3669-3), New York: Crown Publishers.\n\nVivalt, Evalt (2020) [Announcing the launch of the Social Science Prediction Platform!](https://evavivalt.com/2020/07/announcing-the-launch-of-the-social-science-prediction-platform), *Eva Vivalt’s Blog*, July 7.\n\nWiblin, Robert (2017) [Prof Tetlock on predicting catastrophes, why keep your politics secret, and when experts know more than you](https://80000hours.org/podcast/episodes/prof-tetlock-predicting-the-future/), *80,000 Hours*, November 20.\n\nWiblin, Robert & Keiran Harris (2019) [Accurately predicting the future is central to absolutely everything. Professor Tetlock has spent 40 years studying how to do it better](https://80000hours.org/podcast/episodes/philip-tetlock-forecasting-research/), *80,000 Hours*, June 28.\n\nExternal links\n--------------\n\n[Forecasting Wiki](https://forecasting.wiki/wiki/Main_Page). A [wiki](https://forum.effectivealtruism.org/topics/wikis) about forecasting and all things related to it.\n\nRelated entries\n---------------\n\n[AI forecasting](https://forum.effectivealtruism.org/tag/ai-forecasting) | [credence](https://forum.effectivealtruism.org/tag/credence) | [estimation of existential risk](https://forum.effectivealtruism.org/tag/estimation-of-existential-risk) | [Forecasting Newsletter](https://forum.effectivealtruism.org/topics/forecasting-newsletter) | [improving institutional decision-making](https://forum.effectivealtruism.org/topics/improving-institutional-decision-making) | [inside vs. outside view](https://forum.effectivealtruism.org/tag/inside-vs-outside-view) | [long-range forecasting](https://forum.effectivealtruism.org/tag/long-range-forecasting) | [Metaculus](https://forum.effectivealtruism.org/tag/metaculus/) | [model uncertainty](https://forum.effectivealtruism.org/tag/model-uncertainty) | [prediction markets](https://forum.effectivealtruism.org/tag/prediction-markets) | [tabletop exercises](https://forum.effectivealtruism.org/topics/tabletop-exercises) | [value of information](https://forum.effectivealtruism.org/tag/value-of-information)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "xCgaGHaEYg69KTTkR",
    "name": "Effective altruism in the media",
    "core": false,
    "slug": "effective-altruism-in-the-media",
    "oldSlugs": [
      "ea-and-the-media",
      "ea-and-the-media",
      "ea-and-the-media",
      "ea-and-the-media"
    ],
    "postCount": 77,
    "description": {
      "markdown": "The **effective altruism in the media** tag covers portrayals of effective altruism in the media, as well as meta-discussions about how to best represent the movement when talking to journalists or publishing one's own work for a public audience.\n\nFurther reading\n---------------\n\nDuda, Roman (2015) [Journalism](https://80000hours.org/career-reviews/journalism/), *80,000 Hours*, June.\n\nWiblin, Robert (2015) [Pursuing fame in art and entertainment](https://80000hours.org/career-reviews/pursuing-fame-in-art-and-entertainment/), *80,000 Hours*, July.\n\nRelated entries\n---------------\n\n[documentaries](https://forum.effectivealtruism.org/tag/documentaries) | [effective altruism art and fiction](https://forum.effectivealtruism.org/tag/effective-altruism-art-and-fiction) | [effective altruism messaging](https://forum.effectivealtruism.org/tag/effective-altruism-messaging/) | [Future Perfect](https://forum.effectivealtruism.org/tag/future-perfect) | [journalism](https://forum.effectivealtruism.org/topics/journalism) | [news relevant to effective altruism](https://forum.effectivealtruism.org/topics/news-relevant-to-effective-altruism)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "4CH9vsvzyk4mSKwyZ",
    "name": "Career choice",
    "core": true,
    "slug": "career-choice",
    "oldSlugs": [
      "career-choice",
      "career-choice",
      "career-choice"
    ],
    "postCount": 506,
    "description": {
      "markdown": "**Career choice** studies the ethical and practical considerations bearing on the choice of a high-impact career.\n\nPeople spend more time on their jobs than on any other activity besides sleep—about 80,000 hours over the course of a typical life.^[\\[1\\]](#fnea7wscznj)^ Choosing the right career may thus be one of the most impactful things an altruistically-motivated person can do.\n\nDespite its high [importance](https://forum.effectivealtruism.org/tag/importance), impact-oriented career choice has received limited attention. One of the earliest publications on the subject was a youth essay by Karl Marx, who held that we should choose the career \"which offers us the widest scope to work for mankind\". Such a career, Marx claimed, would be best not only altruistically but also for the person pursuing it, since \"experience acclaims as happiest the man who has made the greatest number of people happy.\"^[\\[2\\]](#fnfusg63p1s2i)^ In contemporary philosophy, there is a small literature on whether it is permissible to pursue a non-altruistic rather than an altruistic career, but almost no discussion of how different careers should be ethically ranked.^[\\[3\\]](#fnm59lgx4gu9)^ \n\nThe organization [80,000 Hours](https://forum.effectivealtruism.org/tag/80-000-hours) specializes in research about and support for impact-oriented career choice, and has developed a [framework](https://forum.effectivealtruism.org/tag/career-framework) for comparing the social impact of different careers.\n\n[Holden Karnofsky](https://forum.effectivealtruism.org/tag/holden-karnofsky) has identified four alternative frameworks for making career choice decisions: *paths* to particular roles working on particular causes; *aptitudes* a candidate can build in a variety of roles and causes and apply to a variety of jobs; *causes* to which a candidate can contribute with their career; and *heuristics*, such as \"Do work you can be great at\" or \"Do work that builds your career capital and gives you more options.\"^[\\[4\\]](#fnjxkoi9def8)^ 80,000 Hours uses the \"paths\" framework, while Karnofsky has focused mostly on the aptitude framework, though he advises candidates to consider multiple frameworks.\n\nFurther reading\n---------------\n\nCholbi, Michael (2020) [The ethics of choosing careers and jobs](https://en.wikipedia.org/wiki/Special:BookSources/9780190063337), in Bob Fischer (ed.) *College Ethics: a Reader on Moral Issues That Affect You*, Oxford: Oxford University Press, pp. 878–889.  \n*Argues against the \"maximalist\" view that we are morally required to choose the careers that do the most good.*\n\nMacAskill, William (2014) [Replaceability, career choice, and making a difference](http://doi.org/10.1007/s10677-013-9433-4), *Ethical Theory and Moral Practice*, vol. 17, pp. 269–283.\n\nTodd, Benjamin (2016) [*80,000 Hours: Find a Fulfilling Career That Does Good*](https://en.wikipedia.org/wiki/Special:BookSources/9781537324005), Oxford: Centre for Effective Altruism.\n\nTodd, Benjamin (2021) [Planning a high-impact career: A summary of everything you need to know in 7 points](https://80000hours.org/career-planning/summary/), *80,000 Hours*, May 6.\n\nRelated entries\n---------------\n\n[career framework](https://forum.effectivealtruism.org/tag/career-framework) | [job profile](https://forum.effectivealtruism.org/tag/job-profile) | [career advising](https://forum.effectivealtruism.org/tag/career-advising) | [personal fit](https://forum.effectivealtruism.org/tag/personal-fit) | [job listing (open) ](https://forum.effectivealtruism.org/tag/job-listing-open) | [constraints on effective altruism](https://forum.effectivealtruism.org/tag/constraints-on-effective-altruism) | [donation choice](https://forum.effectivealtruism.org/tag/donation-choice) | [research training programs](https://forum.effectivealtruism.org/tag/research-training-programs) | [scalably using labour](https://forum.effectivealtruism.org/tag/scalably-using-labour) | [working at EA vs non-EA orgs](https://forum.effectivealtruism.org/tag/working-at-ea-vs-non-ea-orgs)\n\n1.  ^**[^](#fnrefea7wscznj)**^\n    \n    MacAskill, William (2014) [Replaceability, career choice, and making a difference](http://doi.org/10.1007/s10677-013-9433-4), *Ethical Theory and Moral Practice*, vol. 17, pp. 269–283, p. 269.\n    \n2.  ^**[^](#fnreffusg63p1s2i)**^\n    \n    Marx, Karl (1835) [Reflections of a young man on the choice of a profession](https://en.wikipedia.org/wiki/Special:BookSources/0717804070), in *Collected Works*, Moscow: Progress Publishers, 1975.\n    \n3.  ^**[^](#fnrefm59lgx4gu9)**^\n    \n    MacAskill, [Replaceability, career choice, and making a difference](http://doi.org/10.1007/s10677-013-9433-4).\n    \n4.  ^**[^](#fnrefjxkoi9def8)**^\n    \n    Karnofsky, Holden (2021) [My current impressions on career choice for longtermists](https://forum.effectivealtruism.org/posts/bud2ssJLQ33pSemKH/my-current-impressions-on-career-choice-for-longtermists), *Effective Altruism Forum*, June 4."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "FjQFMA2SuYTQ4ZcJ7",
    "name": "COVID-19 pandemic",
    "core": false,
    "slug": "covid-19-pandemic",
    "oldSlugs": [
      "coronavirus",
      "covid-19-pandemic",
      "966561082467-where-to-buy-abortion-pills-in-saudi-arabia-mtp"
    ],
    "postCount": 179,
    "description": {
      "markdown": "The **COVID-19 pandemic** tag can cover posts about any aspect of the [pandemic](https://forum.effectivealtruism.org/tag/pandemic-preparedness) (biological, social, economic, etc.).\n\nCost of the pandemic\n--------------------\n\nThe [Institute for Progress](https://forum.effectivealtruism.org/tag/institute-for-progress) estimates that the COVID-19 pandemic has caused between $10 and $22 trillion dollars in economic damage and monetized health and life loss in the United States alone (which accounts for about 20% of gross world product and about 4% of world population).^[\\[1\\]](#fnl39js2ogk5l)^\n\nFurther reading\n---------------\n\nBruns, Richard & Nikki Teran (2022) [Weighing the cost of the pandemic](https://progress.institute/weighing-the-cost-of-the-pandemic/), *Institute for Progress*, April 21.\n\nChivers, Tom (2021) [How many lives has bioethics cost?](https://unherd.com/2021/12/how-many-lives-has-bioethics-cost/), *UnHerd*, December 23.\n\nExternal links\n--------------\n\n[Coronavirus pandemic (COVID-19)](https://ourworldindata.org/coronavirus). [Our World in Data](https://forum.effectivealtruism.org/tag/our-world-in-data) explorer.\n\nRelated entries\n---------------\n\n[biosecurity](https://forum.effectivealtruism.org/tag/biosecurity) | [pandemic preparedness](https://forum.effectivealtruism.org/tag/pandemic-preparedness)\n\n1.  ^**[^](#fnrefl39js2ogk5l)**^\n    \n    Bruns, Richard & Nikki Teran (2022) [Weighing the cost of the pandemic](https://progress.institute/weighing-the-cost-of-the-pandemic/), *Institute for Progress*, April 21."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "of9xBvR3wpbp6qsZC",
    "name": "Policy",
    "core": true,
    "slug": "policy",
    "oldSlugs": [
      "policy-change",
      "policy-change"
    ],
    "postCount": 367,
    "description": {
      "markdown": "The **policy** topic is very broad, covering any post about improving government policy (in developing and developed countries alike).\n\nImproving policy\n----------------\n\nGovernments are typically committed to the notion that their policies should be effective. This means that members of the effective altruist community can be in a good position to help governments reach their aims. Moreover, the fact that governments are very powerful, and control significant proportions of world GDP, suggests that helping policy-makers can be a high-value strategy. This strategy can be pursued either from the *outside*—by effective altruist organizations which advise policy-makers—or from the *inside*—by policy-makers who try to do the most good possible.\n\nSome of the highest-impact reforms affect people who are less able to advocate for their own interests, such as [prisoners](https://forum.effectivealtruism.org/tag/criminal-justice-reform) or [migrants](https://forum.effectivealtruism.org/tag/immigration-reform). Other policies, like [macroeconomic policy](https://forum.effectivealtruism.org/tag/macroeconomic-policy) and [land use reform](https://forum.effectivealtruism.org/tag/land-use-reform), have effects that are somewhat diffuse and non-obvious, which makes it difficult to assemble groups to lobby for change. The more mainstream focus areas of [global poverty and health](https://forum.effectivealtruism.org/tag/global-poverty), [animal welfare](https://forum.effectivealtruism.org/tag/animal-welfare-1) and [existential risk](https://forum.effectivealtruism.org/tag/existential-risk) could also be addressed using political advocacy.\n\nFurther reading\n---------------\n\nBowerman, Niel (2014) [Good policy ideas that won’t happen (yet)](https://forum.effectivealtruism.org/posts/n5CNeo9jxDsCit9dj/good-policy-ideas-that-won-t-happen-yet), *Effective Altruism Forum*, September 14.  \n*A look at the viability of changing public policy on certain issues.*\n\nClough, Emily (2015) [Effective altruism’s political blind spot](https://bostonreview.net/world/emily-clough-effective-altruism-ngos), *Boston Review*, July 14.  \n*An example of one of the main criticisms of effective altruism: that it paid insufficient attention to political advocacy in the past.*\n\nFarquhar, Sebastian (2016) [Should EAs do policy?](https://www.youtube.com/watch?v=NB_edlOrPOU&list=PLwp9xeoX5p8P_O5rQg-SNMwQOIvOPF5U2&index=10), *Effective Altruism Global*, August 5.  \n*A talk at EA Global 2016 with an overview of why policy work might be effective.*\n\nGlobal Priorities Project (2015) [New UK aid strategy – prioritising research and crisis response](http://globalprioritiesproject.org/2015/12/new-uk-aid-strategy-prioritising-research-and-crisis-response/), *Global Priorities Project*, December 2.  \n*An example of effective altruist policy work.*\n\nKarnofsky, Holden (2013) [The track record of policy-oriented philanthropy](https://www.openphilanthropy.org/blog/track-record-policy-oriented-philanthropy), *Open Philanthropy*, November 6.  \n*Articles on Open Philanthropy about policy and philanthropy.*\n\nOpen Philanthropy (2016) [U.S. policy](https://www.openphilanthropy.org/focus/us-policy), *Open Philanthropy*.  \n*The Philanthropy Project's assesment of policy as a focus area*\n\nRelated entries\n---------------\n\n[APPG on Future Generations](https://forum.effectivealtruism.org/tag/appg-on-future-generations) | [Democracy Defense Fund](https://forum.effectivealtruism.org/tag/democracy-defense-fund) | [improving institutional decision-making](https://forum.effectivealtruism.org/topics/improving-institutional-decision-making) | [longtermist institutional reform](https://forum.effectivealtruism.org/topics/longtermist-institutional-reform) | [standards and regulation](https://forum.effectivealtruism.org/topics/standards-and-regulation) | [United States](https://forum.effectivealtruism.org/topics/united-states)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "tx3q9KW35B3M6MM4h",
    "name": "Organization strategy",
    "core": false,
    "slug": "organization-strategy",
    "oldSlugs": [
      "org-strategy",
      "org-strategy"
    ],
    "postCount": 117,
    "description": {
      "markdown": "The **organization strategy** tag covers posts about running organizations well (whether for-profit or nonprofit).\n\nIf a post discusses running an EA group rather than another type of organization, please use the [effective altruism groups](https://forum.effectivealtruism.org/tag/effective-altruism-groups) tag instead. (At some point, we may also add a separate \"Group strategy\" tag.)\n\nRelated entries\n---------------\n\n[impact assessment](https://forum.effectivealtruism.org/tag/impact-assessment) | [organization updates](https://forum.effectivealtruism.org/topics/organization-updates) | [theory of change](https://forum.effectivealtruism.org/topics/theory-of-change)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "ZiKs8Bbvse4DMcEPr",
    "name": "Economic growth",
    "core": false,
    "slug": "economic-growth",
    "oldSlugs": [
      "economic-growth",
      "economic-growth",
      "economic-growth"
    ],
    "postCount": 98,
    "description": {
      "markdown": "**Economic growth** is the expansion of inflation-adjusted income over time, usually due to growth in population, investment, or innovation. Economic growth has attracted the attention of the effective altruism community both because growth appears to be a highly cost-effective way to reduce [global poverty](https://forum.effectivealtruism.org/tag/global-poverty) and improve human [wellbeing](https://forum.effectivealtruism.org/tag/wellbeing), and because expected rates of economic growth in the coming centuries have, in the words of [Future of Humanity Institute](https://forum.effectivealtruism.org/tag/future-of-humanity-institute) researcher Ben Garfinkel, \"some bearing on almost every long-run challenge facing the world, from climate change to great power competition to risks from AI.\"^[\\[1\\]](#fnhyjhd0hx45)^\n\nFurther reading\n---------------\n\nAlexander, Scott (2019) [1960: the year the singularity was cancelled](https://slatestarcodex.com/2019/04/22/1960-the-year-the-singularity-was-cancelled/), *Slate Star Codex*, April 22.\n\nAschenbrenner, Leopold (2020a) [Existential risk and growth](https://globalprioritiesinstitute.org/wp-content/uploads/Leopold-Aschenbrenner_Existential-risk-and-growth_.pdf), Global Priorities Institute, University of Oxford.\n\nAschenbrenner, Leopold (2020b) [Securing posterity](https://worksinprogress.co/issue/securing-posterity/), *Works in Progress*, October 19.\n\nDavidson, Tom (2021) [Report on whether AI could drive explosive economic growth](https://www.openphilanthropy.org/blog/report-advanced-ai-drive-explosive-economic-growth), *Open Philanthropy*, June 25.\n\nGarfinkel, Ben (2020) [Does economic history point toward a singularity?](https://forum.effectivealtruism.org/posts/CWFn9qAKsRibpCGq8/does-economic-history-point-toward-a-singularity), *Effective Altruism Forum*, September 2.\n\nHanson, Robin (2016) [*The Age of Em: Work, Love, and Life When Robots Rule the Earth*](https://en.wikipedia.org/wiki/Special:BookSources/9780198754626), Oxford: Oxford University Press.\n\nKremer, Michael (1993) [Population growth and technological change: one million B.C. to 1990](http://doi.org/10.2307/2118405), *The Quarterly Journal of Economics*, vol. 108, pp. 681–716.\n\nNordhaus, William D. (2021) [Are we approaching an economic singularity? Information technology and the future of economic growth](http://doi.org/10.1257/mac.20170105), *American Economic Journal: Macroeconomics*, vol. 13, pp. 299–332.\n\nRoser, Max (2021) [What is economic growth? And why is it so important?](https://ourworldindata.org/what-is-economic-growth), *Our World in Data*, May 13.\n\nRoodman, David (2020) [On the probability distribution of long-term changes in the growth rate of the global economy: an outside view](https://www.openphilanthropy.org/sites/default/files/Modeling-the-human-trajectory.pdf), *Open Philanthropy*.\n\nvon Foerster, Heinz, Patricia M. Mora & Lawrence W. Amiot (1960) [Doomsday: Friday, 13 November, A.D. 2026](http://doi.org/10.1126/science.132.3436.1291), *Science*, vol. 132, pp. 1291–1295.\n\nWiblin, Robert & Keiran Harris (2018) [Economics Prof Tyler Cowen says our overwhelming priorities should be maximising economic growth and making civilisation more stable. Is he right?](https://80000hours.org/podcast/episodes/tyler-cowen-stubborn-attachments/), *80,000 Hours*, October 17.\n\nRelated entries\n---------------\n\n[differential progress](https://forum.effectivealtruism.org/tag/differential-progress) | [economics](https://forum.effectivealtruism.org/tag/economics) | [economics of artificial intelligence](https://forum.effectivealtruism.org/tag/economics-of-ai) | [global poverty](https://forum.effectivealtruism.org/tag/global-poverty) | [population decline](https://forum.effectivealtruism.org/topics/population-decline) | [poverty trap](https://forum.effectivealtruism.org/topics/poverty-trap) | [scientific progress](https://forum.effectivealtruism.org/tag/scientific-progress) | [speeding up development](https://forum.effectivealtruism.org/tag/speeding-up-development)\n\n1.  ^**[^](#fnrefhyjhd0hx45)**^\n    \n    Garfinkel, Ben (2020) [Does economic history point toward a singularity?](https://forum.effectivealtruism.org/posts/CWFn9qAKsRibpCGq8/does-economic-history-point-toward-a-singularity), *Effective Altruism Forum*, September 2."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "X3KB4vCDhbRrC4qTP",
    "name": "Forum Prize",
    "core": false,
    "slug": "forum-prize",
    "oldSlugs": null,
    "postCount": 132,
    "description": {
      "markdown": "The **Forum Prize** tag is used for posts that won the EA Forum Prize, as well as posts that announce the winning posts and comments.\n\nFor posts awarding an independent prize, use the [**Prize**](https://forum.effectivealtruism.org/tag/prize) tag."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "u3Xg8MjDe2e6BvKtv",
    "name": "AI governance",
    "core": false,
    "slug": "ai-governance",
    "oldSlugs": [
      "ai-governance",
      "ai-governance",
      "ai-governance",
      "ai-governance",
      "ai-governance",
      "ai-governance",
      "governance-of-artificial-intelligence"
    ],
    "postCount": 245,
    "description": {
      "markdown": "**AI governance** (or the **governance of artificial intelligence**)  is the study of norms, policies, and institutions that can help humanity navigate the transition to a world with advanced [artificial intelligence](https://forum.effectivealtruism.org/tag/artificial-intelligence). This includes a broad range of subjects, from global coordination around regulating AI development to providing incentives for corporations to be more cautious in their AI research.\n\nAI governance as a career\n-------------------------\n\nLong-term AI policy strategy research and implementation is one of [80,000 Hours](https://forum.effectivealtruism.org/tag/80-000-hours)' \"priority paths\"—the most promising career opportunities the organization has identified so far.^[\\[1\\]](#fnflpsyuzpjiq)^^[\\[2\\]](#fnj2p8j3mqwce)^\n\nFurther reading\n---------------\n\nBrundage, Miles (2017) [Guide to working in AI policy and strategy](https://80000hours.org/articles/ai-policy-guide/), *80,000 Hours*, June 7.\n\nCussins, Jessica (2020) [Summaries of AI policy resources](https://futureoflife.org/ai-policy-resources/), *Future of Life Institute*.\n\nDafoe, Allan (2020) [AI governance: opportunity and theory of impact](https://forum.effectivealtruism.org/posts/42reWndoTEhFqu6T8/ai-governance-opportunity-and-theory-of-impact), *Effective Altruism Forum*, September 17.\n\nMuehlhauser, Luke (2021) [A personal take on longtermist AI governance](https://forum.effectivealtruism.org/posts/M2SBwctwC6vBqAmZW/a-personal-take-on-longtermist-ai-governance), *Effective Altruism Forum*, July 21.\n\nRelated entries\n---------------\n\n[AI alignment](https://forum.effectivealtruism.org/tag/ai-alignment) | [AI forecasting](https://forum.effectivealtruism.org/tag/ai-forecasting) | [alignment tax](https://forum.effectivealtruism.org/tag/alignment-tax) | [antitrust law](https://forum.effectivealtruism.org/topics/antitrust-law) | [compute governance](https://forum.effectivealtruism.org/tag/compute-governance) | [economics of artificial intelligence](https://forum.effectivealtruism.org/tag/economics-of-ai) | [ethics of artificial intelligence](https://forum.effectivealtruism.org/tag/ethics-of-artificial-intelligence) | [global governance](https://forum.effectivealtruism.org/tag/global-governance) | [standards and regulation](https://forum.effectivealtruism.org/topics/standards-and-regulation)\n\n1.  ^**[^](#fnrefflpsyuzpjiq)**^\n    \n    Todd, Benjamin (2018) [The highest impact career paths our research has identified so far](https://80000hours.org/articles/high-impact-careers/), *80,000 Hours*, August 12.\n    \n2.  ^**[^](#fnrefj2p8j3mqwce)**^\n    \n    Todd, Benjamin (2021) [Long-term AI policy strategy research and implementation](https://80000hours.org/career-reviews/ai-policy-and-strategy/), *80,000 Hours*, October."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "A7LTqhZYeXqvQqRMg",
    "name": "Effective altruism groups",
    "core": false,
    "slug": "effective-altruism-groups",
    "oldSlugs": [
      "ea-groups",
      "ea-groups",
      "ea-groups"
    ],
    "postCount": 253,
    "description": {
      "markdown": "Posts with the **effective altruism groups** tag discuss general principles of running EA groups, as well as particular groups and their achievements.\n\nExternal links\n--------------\n\n[EA Groups Resource Centre](https://resources.eagroups.org/). A collection of resources for group organisers curated by the [Centre for Effective Altruism](https://forum.effectivealtruism.org/topics/centre-for-effective-altruism-1).\n\nRelated entries\n---------------\n\n[building effective altruism](https://forum.effectivealtruism.org/tag/building-effective-altruism-1) | [retreats](https://forum.effectivealtruism.org/tag/retreats)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "tHSMZ8brCzhLu4HY8",
    "name": "Scientific progress",
    "core": false,
    "slug": "scientific-progress",
    "oldSlugs": [
      "scientific-progress"
    ],
    "postCount": 75,
    "description": {
      "markdown": "The **scientific progress** tag is for posts about:\n\n*   The potential risks and benefits of progress in different areas\n*   Strategies for changing the rate of progress in different areas\n*   General information related to progress (e.g. factors that tend to encourage it)\n*   Specific examples of progress (e.g. a major advance in an important scientific field)\n\nFor posts about more specific aspects of the scientific ecosystem and how that can be improved, use the [meta-science](https://forum.effectivealtruism.org/tag/meta-science) tag.\n\nRelated entries\n---------------\n\n[academia](https://forum.effectivealtruism.org/tag/academia-1/) | [differential progress](https://forum.effectivealtruism.org/tag/differential-progress) | [metascience](https://forum.effectivealtruism.org/topics/metascience) | [economic growth](https://forum.effectivealtruism.org/tag/economic-growth) |  [research methods](https://forum.effectivealtruism.org/tag/research-methods) | [speeding up development](https://forum.effectivealtruism.org/tag/speeding-up-development)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "ui3ZZqd23HzA7wNwP",
    "name": "Criticism of effective altruist causes",
    "core": false,
    "slug": "criticism-of-effective-altruist-causes",
    "oldSlugs": [
      "criticism-ea-cause-areas"
    ],
    "postCount": 104,
    "description": {
      "markdown": "The **criticism of effective altruist causes** tag covers posts that criticize popular claims about the importance of a given cause area, or an idea within that area. They don't have to be critical of the area as a whole — just one or more popular ideas related to the area.\n\nRelated entries\n---------------\n\n[criticism of effective altruism](https://forum.effectivealtruism.org/tag/criticism-of-effective-altruism) | [criticism of effective altruism culture](https://forum.effectivealtruism.org/topics/criticism-of-effective-altruism-culture) | [criticism of effective altruist organizations](https://forum.effectivealtruism.org/tag/criticism-of-effective-altruist-organizations) |[criticism of longtermism and existential risk studies](https://forum.effectivealtruism.org/topics/criticism-of-longtermism-and-existential-risk-studies)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "pg5MHgnpDWnnWDTyY",
    "name": "Earning to give",
    "core": false,
    "slug": "earning-to-give",
    "oldSlugs": [
      "earning-to-give",
      "earning-to-give",
      "earning-to-give"
    ],
    "postCount": 106,
    "description": {
      "markdown": "**Earning to give** is the pursuit of a lucrative career with the purpose of donating a significant portion of the earnings to cost-effective organizations.\n\nThe value of earning to give may not be immediately obvious, since people tend to think of ethical jobs as those with a *direct* positive impact. A useful framing for comparing the social impact of earning to give with that of traditional careers in the nonprofit sector is to consider whether someone pursuing a lucrative career could earn enough to fund at least one nonprofit worker as competent as him- or herself. This will often be the case because there is considerable salary variation across career-types, because money is a fungible resource that can fund many different causes, and because the impact of a charity worker is usually only marginally higher than that of its [counterfactual replacement](https://forum.effectivealtruism.org/tag/replaceability).^[\\[1\\]](#fnz0j2pvc6cco)^\n\nAlthough this approach has sometimes been represented as *the* highest impact type of career, [80,000 Hours](https://forum.effectivealtruism.org/tag/80-000-hours) currently believes that it is the best option for only a small proportion of members of the [effective altruism](https://forum.effectivealtruism.org/tag/effective-altruism) community.^[\\[2\\]](#fnhbpipz20win)^ To a significant extent, this follows from the fact that the most promising causes tend to be talent-constrained rather than funding-constrained.^[\\[3\\]](#fnyukxg2kbdi)^\n\nThe expression **earning to save** has been suggested for a type of earning to give where earnings are [invested for later disbursement](https://forum.effectivealtruism.org/tag/patient-altruism) rather than donated immediately.^[\\[4\\]](#fn7vd9h23ug6i)^^[\\[5\\]](#fn8qzu27bjq0c)^ A \"moderate\" version of investing until the end of a person's career may be contrasted with an \"extreme\" version of investing for as long as possible (potentially involving many generations, centuries or millennia).\n\nFurther reading\n---------------\n\nMacAskill, William (2014) [Replaceability, career choice, and making a difference](https://doi.org/10.1007/s10677-013-9433-4), *Ethical theory and moral practice*, vol. 17, pp. 269–283.\n\nShulman, Carl (2012) [Salary or startup? How do-gooders can gain more from risky careers](https://80000hours.org/2012/01/salary-or-startup-how-do-gooders-can-gain-more-from-risky-careers/), *80,000 Hours*, January 8.\n\nStafforini, Pablo (2014) [Earning to give: an annotated bibliography](https://www.stafforini.com/blog/writings-on-earning-to-give-an-annotated-bibliography/), *Pablo’s Miscellany*, March 22 (updated 16 September 2016).\n\nTomasik, Brian (2006) [Why activists should consider making lots of money](https://reducing-suffering.org/why-activists-should-consider-making-lots-of-money/), *Essays on Reducing Suffering*.\n\nWiblin, Robert (2017) [Alex Gordon-Brown on making millions for charity each year by working in quant finance](https://80000hours.org/podcast/episodes/the-life-of-a-quant-trader-how-to-earn-and-donate-millions-within-a-few-years/), *80,000 Hours*, August 28.\n\nRelated entries\n---------------\n\n[constraints on effective altruism](https://forum.effectivealtruism.org/tag/constraints-on-effective-altruism) | [investing](https://forum.effectivealtruism.org/tag/investing) | [patient altruism](https://forum.effectivealtruism.org/tag/patient-altruism)\n\n1.  ^**[^](#fnrefz0j2pvc6cco)**^\n    \n    MacAskill, William (2014) [Replaceability, career choice, and making a difference](https://doi.org/10.1007/s10677-013-9433-4), *Ethical Theory and Moral Practice*, vol. 17, pp. 269–283.\n    \n2.  ^**[^](#fnrefhbpipz20win)**^\n    \n    MacAskill, William (2015) [80,000 Hours thinks that only a small proportion of people should earn to give long term](https://80000hours.org/2015/07/80000-hours-thinks-that-only-a-small-proportion-of-people-should-earn-to-give-long-term/), *80,000 Hours*, July 6.\n    \n3.  ^**[^](#fnrefyukxg2kbdi)**^\n    \n    Todd, Benjamin (2015) [Why you should focus more on talent gaps, not funding gaps](https://80000hours.org/2015/11/why-you-should-focus-more-on-talent-gaps-not-funding-gaps/), *80,000 Hours*, November 27.\n    \n4.  ^**[^](#fnref7vd9h23ug6i)**^\n    \n    Arnold, Raymond (2018) [Earning to save (give 1%, save 10%)](https://forum.effectivealtruism.org/posts/3ijnLaws7mCEogD6H/earning-to-save-give-1-save-10), *Effective Altruism Forum*, November 26.\n    \n5.  ^**[^](#fnref8qzu27bjq0c)**^\n    \n    Todd, Benjamin (2020) [An argument for keeping open the option of earning to save](https://forum.effectivealtruism.org/posts/J5aYvsiLoAC46DSuY/an-argument-for-keeping-open-the-option-of-earning-to-save), *Effective Altruism Forum*, August 31."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "cZquHC9dWFnfP5Qo9",
    "name": "Workplace advocacy",
    "core": false,
    "slug": "workplace-advocacy",
    "oldSlugs": [
      "workplace-activism",
      "workplace-activism"
    ],
    "postCount": 28,
    "description": {
      "markdown": "**Workplace advocacy** collects posts about advocating [effective altruism](https://forum.effectivealtruism.org/topics/effective-altruism) in the workplace.\n\nThis might include donation matching, fundraising within one's company, working in corporate social responsibility (CSR), or many other topics.\n\nFor now, the tag also includes posts about internal CSR projects related to EA, even if they weren't necessarily motivated by advocacy. These posts may have their own tag at some point.\n\nFurther reading\n---------------\n\nDouble the Donation (2019) [Corporate matching gift programs: Understanding the basics](https://doublethedonation.com/tips/corporate-matching-gift-programs/), *Double the Donation*.\n\nRelated entries\n---------------\n\n[workplace groups](https://forum.effectivealtruism.org/topics/workplace-groups)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "Ywp7QTvy3ezac3cPa",
    "name": "Information hazard",
    "core": false,
    "slug": "information-hazard",
    "oldSlugs": [
      "infohazards"
    ],
    "postCount": 30,
    "description": {
      "markdown": "An **information hazard** (also known as an **infohazard**) is a risk arising from the spread of true information. The concept was introduced by [Nick Bostrom](https://forum.effectivealtruism.org/tag/nick-bostrom) in a 2011 paper.^[\\[1\\]](#fn8mouiuypb1)^\n\nAn important ethical and practical issue is how information hazards should be treated. To what extent should people suppress acquisition and dissemination of information which may cause harm? The answer to this question both depends on one’s moral views—for instance, whether new knowledge is good in itself, or whether it is wrong to restrict personal liberty—and on one’s empirical views of what the outcome of such suppression is likely to be.\n\nFurther reading\n---------------\n\nAird, Michael (2020) [Collection of all prior work I found that seemed substantially relevant to information hazards](https://forum.effectivealtruism.org/posts/EMKf4Gyee7BsY2RP8/michaela-s-shortform?commentId=dTghHNHmc5qf5znMQ), *Effective Altruism Forum*, February 24.  \n*Many additional resources on this topic.*\n\nBostrom, Nick (2011) [Information hazards: a typology of potential harms from knowledge](http://www.nickbostrom.com/information-hazards.pdf), *Review of contemporary philosophy*, vol. 10, pp. 1–35.\n\nPiper, Kelsey (2022) [When scientific information is dangerous](https://www.vox.com/future-perfect/2022/3/30/23001712/ai-research-virus-scientific-information-dangerous), *Vox*, March 30.\n\nSandberg, Anders (2020) [Anders Sandberg on information hazards](https://youtu.be/Wn2vgQGNI_c), *Slate Star Codex meetup*, July 5.\n\nRelated entries\n---------------\n\n[accidental harm](https://forum.effectivealtruism.org/tag/accidental-harm) | [misinformation](https://forum.effectivealtruism.org/tag/misinformation-1) | [unilateralist's curse](https://forum.effectivealtruism.org/tag/unilateralist-s-curse)\n\n1.  ^**[^](#fnref8mouiuypb1)**^\n    \n    Bostrom, Nick (2011) [Information hazards: a typology of potential harms from knowledge](http://www.nickbostrom.com/information-hazards.pdf), *Review of Contemporary Philosophy*, vol. 10, pp. 1–35."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "328weGzQNmTjyDm4g",
    "name": "Entrepreneurship",
    "core": false,
    "slug": "entrepreneurship",
    "oldSlugs": null,
    "postCount": 100,
    "description": {
      "markdown": "The **Entrepreneurship** tag covers posts that discuss entrepreneurial ventures, including nonprofit startups and for-profit startups founded with social impact in mind.\n\nRelated entries\n---------------\n\n[charity incubation](https://forum.effectivealtruism.org/tag/charity-incubation)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "nj9svkXCASvFayRrR",
    "name": "Ask Me Anything",
    "core": false,
    "slug": "ask-me-anything",
    "oldSlugs": null,
    "postCount": 78,
    "description": {
      "markdown": "**Ask Me Anything**, or **AMA**,  is a post format where people offer to answer questions from Forum users. The format originated with the IAmA subreddit.\n\n## External links\n[IAmA subreddit](https://www.reddit.com/r/IAmA/)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "tE5BRNibhQogNJExQ",
    "name": "Criticism of effective altruist organizations",
    "core": false,
    "slug": "criticism-of-effective-altruist-organizations",
    "oldSlugs": [
      "criticism-ea-orgs"
    ],
    "postCount": 90,
    "description": {
      "markdown": "The **criticism of effective altruist organizations** tag can apply to at least two kinds of posts:\n\n*   Posts criticizing the actions/strategy of a particular EA organization (or multiple orgs)\n*   Posts written by EA orgs or their representatives, if users have written critical comments responding to those posts\n\nRelated entries\n---------------\n\n[criticism of effective altruism](https://forum.effectivealtruism.org/tag/criticism-of-effective-altruism) | [criticism of effective altruism culture](https://forum.effectivealtruism.org/topics/criticism-of-effective-altruism-culture) | [criticism of effective altruist causes](https://forum.effectivealtruism.org/tag/criticism-of-effective-altruist-causes)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "zJv36ZGjWDSwXDEiT",
    "name": "Conferences",
    "core": false,
    "slug": "conferences",
    "oldSlugs": [
      "ea-global",
      "ea-conferences",
      "ea-conferences",
      "ea-conferences",
      "effective-altruism-conferences"
    ],
    "postCount": 117,
    "description": {
      "markdown": "Posts with the **conferences** tag include discussions of EA conference organizing (application criteria, speaker lists, etc.) and reviews of/lessons learned from past conferences, including but not limited to [EA Global](https://forum.effectivealtruism.org/tag/effective-altruism-global) and [EAGx](https://forum.effectivealtruism.org/tag/eagx).\n\nRelated entries\n---------------\n\n[Effective Altruism Global](https://forum.effectivealtruism.org/tag/effective-altruism-global) | [EAGx](https://forum.effectivealtruism.org/tag/eagx)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "jjNySc23WPumhShAp",
    "name": "Global  outreach",
    "core": false,
    "slug": "global-outreach",
    "oldSlugs": [
      "global-outreach"
    ],
    "postCount": 57,
    "description": {
      "markdown": "The **global outreach** tag covers posts about developing EA-aligned communities around the world, especially in places where very few exist so far (for example, Latin America or the Middle East).\n\nRelated entries\n---------------\n\n[building effective altruism](https://forum.effectivealtruism.org/tag/building-effective-altruism-1) | [value of movement growth](https://forum.effectivealtruism.org/tag/value-of-movement-growth)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "c8w7qjiHku7nDrZYp",
    "name": "Effective animal advocacy",
    "core": false,
    "slug": "effective-animal-advocacy",
    "oldSlugs": [
      "animal-welfare-community",
      "animal-welfare-community",
      "animal-welfare-community"
    ],
    "postCount": 53,
    "description": {
      "markdown": "**Effective animal advocacy** (**EAA**) is the group within the effective altruism community primarily focused on [animal welfare](https://forum.effectivealtruism.org/tag/animal-welfare-1).\n\nFurther reading\n---------------\n\nSebo, Jeff (2020) [Effective animal advocacy](https://doi.org/10.4324/9781315105840), in Bob Fischer (ed.) *The Routledge Handbook of Animal Ethics*, New York: Routledge, pp. 530–542.\n\nŠimčikas, Saulius (2019) [Effective animal advocacy resources](https://rethinkpriorities.org/publications/effective-animal-advocacy-resources), *Rethink Priorities*, October 24.\n\nRelated entries\n---------------\n\n[animal cognition](https://forum.effectivealtruism.org/tag/animal-cognition) | [animal sentience](https://forum.effectivealtruism.org/tag/animal-sentience) | [animal welfare](https://forum.effectivealtruism.org/tag/animal-welfare-1) | [farmed animal welfare](https://forum.effectivealtruism.org/tag/farmed-animal-welfare) | [wild animal welfare](https://forum.effectivealtruism.org/tag/wild-animal-welfare)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "DvgmkiotBKc9Ndm2B",
    "name": "Data (EA Community)",
    "core": false,
    "slug": "data-ea-community-1",
    "oldSlugs": null,
    "postCount": 102,
    "description": {
      "markdown": "The **Data (EA Community)** tag collects posts that provide... well, data on the EA community. Where do EAs give? Where do they live? What do they believe?"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "EdwSzDq9bTH9yqQX9",
    "name": "Effective Altruism London",
    "core": false,
    "slug": "effective-altruism-london",
    "oldSlugs": [
      "ea-london-updates",
      "ea-london-update",
      "ea-london-update",
      "ea-london-update",
      "ea-london"
    ],
    "postCount": 31,
    "description": {
      "markdown": "**Effective Altruism London** is an [effective altruism group](https://forum.effectivealtruism.org/tag/effective-altruism-groups) based in London, United Kingdom. It was formed in early 2012.\n\nFurther reading\n---------------\n\nEffective Altruism London (2019) [Effective Altruism London strategy](https://docs.google.com/document/d/1OlqrxnpHSEhstuexv4Se9MWDbiYVq-lk41zw4-vLbMw/edit?usp=sharing&usp=embed_facebook), *Effective Altruism London*.\n\nNash, David (2019) [Effective Altruism London landscape](https://forum.effectivealtruism.org/posts/P4yXkPTkBgQCcSxD3/effective-altruism-london-landscape), *Effective Altruism Forum*, May 17 (updated 2021).\n\nExternal links\n--------------\n\n[Effective Altruism London](https://www.ealondon.com/). Official website.\n\nRelated entries\n---------------\n\n[effective altruism groups](https://forum.effectivealtruism.org/tag/effective-altruism-groups) | [Monthly Overload of Effective Altruism](https://forum.effectivealtruism.org/topics/monthly-overload-of-effective-altruism)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "T6taug8YuACSwYhXz",
    "name": "Fundraising",
    "core": false,
    "slug": "fundraising",
    "oldSlugs": null,
    "postCount": 60,
    "description": {
      "markdown": "The **Fundraising** tag covers posts which discuss fundraising strategy, experimental studies of fundraising methods, etc.\n\nSome fundraising organisations include: [Giving What We Can](https://forum.effectivealtruism.org/tag/giving-what-we-can), [One for the World](https://forum.effectivealtruism.org/tag/one-for-the-world-1), [Effective Giving](https://forum.effectivealtruism.org/tag/effective-giving-2), [Founders’ Pledge](https://forum.effectivealtruism.org/tag/founders-pledge), [Raising for Effective Giving](https://forum.effectivealtruism.org/tag/raising-for-effective-giving), [High Impact Athletes](https://forum.effectivealtruism.org/tag/high-impact-athletes), and [more](https://forum.effectivealtruism.org/topics/organization-updates)."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "u4bgRHcNJtna6bkEL",
    "name": "Mental health",
    "core": false,
    "slug": "mental-health",
    "oldSlugs": [
      "mental-health-cause-area",
      "mental-health\\",
      "mental-health"
    ],
    "postCount": 147,
    "description": {
      "markdown": "*This entry discusses mental health as a cause area. For mental health within the effective altruism community, see* [*effective altruism lifestyle*](https://forum.effectivealtruism.org/tag/effective-altruism-lifestyle) *or* [*self-care*](https://forum.effectivealtruism.org/tag/self-care)*.*\n\n**Mental health** issues cause an enormous amount of suffering on a global scale, and could be just as promising a target for charitable intervention as many other effective altruist cause areas.\n\nFurther detail\n--------------\n\nMental health problems are conditions such as depression, anxiety and schizophrenia. It is unclear precisely what percent of the [DALY](https://forum.effectivealtruism.org/tag/adjusted-life-year) burden worldwide they represent. While some research has indicated they make up about 7% of the worldwide DALY burden, other researchers have suggested that the 7% figure may underestimate the true burden of mental health in a variety of ways, and that the actual figure may be nearly double that.^[\\[1\\]](#fnixfynimdorp)^\n\nMental health problems are typically [neglected](https://forum.effectivealtruism.org/tag/neglectedness), particularly in the developing world, and there are very few charities working in the area. [Giving What We Can](https://forum.effectivealtruism.org/tag/giving-what-we-can) conducted an evaluation of Basic Needs, a charity that implements community-based mental health projects, concluding that it was promising but not as cost-effective as [GiveWell](https://forum.effectivealtruism.org/tag/givewell) ’s recommended charities.^[\\[2\\]](#fnwjifrchlyb)^\n\nSome people care about making everyone happier and more satisfied with their life, over and above treating medical disorders. A few members of the effective altruism community have argued that interventions aimed at increasing happiness may be more cost-effective than [global poverty](https://forum.effectivealtruism.org/tag/global-poverty) interventions.^[\\[3\\]](#fnasxj1g7w9br)^\n\nRelated entries\n---------------\n\n[pain and suffering](https://forum.effectivealtruism.org/tag/pain-and-suffering)\n\n1.  ^**[^](#fnrefixfynimdorp)**^\n    \n    Vigo, Daniel, Graham Thornicroft & Rifat Atun (2016) [Estimating the true global burden of mental illness](http://doi.org/10.1016/S2215-0366(15)00505-2), *The Lancet Psychiatry*, vol. 3, pp. 171–178.\n    \n2.  ^**[^](#fnrefwjifrchlyb)**^\n    \n    Snowden, James & Konstantin Sietzy (2016) [Mental health](https://www.givingwhatwecan.org/report/mental-health/), *Giving What We Can*, February 8.\n    \n3.  ^**[^](#fnrefasxj1g7w9br)**^\n    \n    Plant, Michael (2016) [Is effective altruism overlooking human happiness and mental health? I argue it is](https://forum.effectivealtruism.org/posts/A278Run59FenZ4kae/is-effective-altruism-overlooking-human-happiness-and-mental), *Effective Altruism Forum*, June 22."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "WuDHK7wyXKCjzwjpA",
    "name": "Event strategy",
    "core": false,
    "slug": "event-strategy",
    "oldSlugs": [
      "event-strategy"
    ],
    "postCount": 49,
    "description": {
      "markdown": "Use the **event strategy** tag for posts about running EA events.\n\nRelated entries\n---------------\n\n[Conferences](https://forum.effectivealtruism.org/topics/conferences) | [Effective Altruism Global](https://forum.effectivealtruism.org/topics/effective-altruism-global) | [EAGx](https://forum.effectivealtruism.org/topics/eagx) | [Fellowships & internships](https://forum.effectivealtruism.org/topics/fellowships-and-internships)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "ynkTF7xXRoBRBqJrX",
    "name": "Effective altruism art and fiction",
    "core": false,
    "slug": "effective-altruism-art-and-fiction",
    "oldSlugs": [
      "ea-art-and-fiction",
      "ea-art-and-fiction",
      "ea-art-and-fiction"
    ],
    "postCount": 180,
    "description": {
      "markdown": "The **effective altruism art and fiction** tag covers artistic depictions of EA-related topics and discussion of the role of art and fiction within the EA movement.\n\nFurther reading\n---------------\n\nWiblin, Robert (2015) [Pursuing fame in art and entertainment](https://80000hours.org/career-reviews/pursuing-fame-in-art-and-entertainment/), *80,000 Hours*, July.\n\nRelated entries\n---------------\n\n[effective altruism in the media](https://forum.effectivealtruism.org/tag/effective-altruism-in-the-media) | [effective altruism messaging](https://forum.effectivealtruism.org/tag/effective-altruism-messaging/) | [existential risk fiction](https://forum.effectivealtruism.org/tag/existential-risk-fiction)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5rhkcoSo8YHJfp9rH",
    "name": "Nuclear warfare",
    "core": false,
    "slug": "nuclear-warfare-1",
    "oldSlugs": [
      "nuclear-weapons",
      "nuclear-weapons"
    ],
    "postCount": 115,
    "description": {
      "markdown": "**Nuclear warfare** is military conflict involving the deployment of nuclear weapons.\n\nNuclear war is potentially the primary near-term [anthropogenic existential risk](https://forum.effectivealtruism.org/tag/anthropogenic-existential-risks). Nuclear war would probably not cause human extinction through the direct damage of an exchange. Rather, researchers are concerned about the potential for a [nuclear winter](https://forum.effectivealtruism.org/tag/nuclear-winter): firestorms caused by the explosions could release particulate matter into the stratosphere, causing significant global cooling which would last for several years. This cooling would disrupt global agriculture, which would likely kill many more people than the initial exchange.^[\\[1\\]](#fnd6fpj0ycxki)^\n\nHowever, there is some disagreement about how climate systems would react to the particulate matter and how much soot would actually be created (modern cities are potentially less vulnerable to firestorms than those during the cold war). As a result, it is unclear whether nuclear war poses an [existential risk](https://forum.effectivealtruism.org/tag/existential-risk) or a non-existential global catastrophic risk.\n\nFurther reading\n---------------\n\nBaum, Seth D., Robert De Neufville & Anthony M. Barrett (2018) [A model for the probability of nuclear war](http://gcrinstitute.org/wp-content/uploads/2020/07/042_nuclear-probability.pdf), working paper 18-1, Global Catastrophic Risk Institute.\n\nBaum, Seth D. & Anthony M. Barrett (2018) [A model for the impacts of nuclear war](http://gcrinstitute.org/papers/043_nuclear-impacts.pdf), working paper 18-2, Global Catastrophic Risk Institute.\n\nCirincione, Joseph (2008) [The continuing threat of nuclear war](https://doi.org/10.1093/oso/9780198570509.003.0025), in Nick Bostrom & Milan M. Ćirković (eds.) *Global Catastrophic Risks*, Oxford: Oxford University Press.\n\nOpen Philanthropy (2015) [Nuclear weapons policy](https://www.openphilanthropy.org/research/cause-reports/nuclear-weapons-policy), *Open Philanthropy*, September.  \n*A report exploring possible interventions to address risks from nuclear war.*\n\nRoser, Max (2022) [Nuclear weapons: Why reducing the risk of nuclear war should be a key concern of our generation](https://ourworldindata.org/nuclear-weapons-risk), *Our World in Data*, March 3.\n\nScouras, James (2021) [*On Assessing the Risk of Nuclear War*](https://www.jhuapl.edu/Content/documents/OnAssessingRiskNuclearWar.pdf), Laurel: Johns Hopkins University Applied Physics Laboratory.\n\nRelated entries\n---------------\n\n[armed conflict](https://forum.effectivealtruism.org/tag/armed-conflict) | [Bulletin of the Atomic Scientists](/tag/bulletin-of-the-atomic-scientists) | [civilizational collapse](https://forum.effectivealtruism.org/tag/civilizational-collapse) | [Cuban Missile Crisis](https://forum.effectivealtruism.org/tag/cuban-missile-crisis) | [existential risk](https://forum.effectivealtruism.org/tag/existential-risk) | [existential risk factor](https://forum.effectivealtruism.org/tag/existential-risk-factor) | [Manhattan Project](https://forum.effectivealtruism.org/tag/manhattan-project) | [nuclear disarmament movement](https://forum.effectivealtruism.org/tag/nuclear-disarmament-movement) | [nuclear security](https://forum.effectivealtruism.org/tag/nuclear-security) | [Nuclear Threat Initiative](https://forum.effectivealtruism.org/tag/nuclear-threat-initiative) | [nuclear winter](https://forum.effectivealtruism.org/tag/nuclear-winter) | [Russell-Einstein Manifesto](https://forum.effectivealtruism.org/tag/russell-einstein-manifesto) | [Trinity](https://forum.effectivealtruism.org/tag/trinity) | [weapons of mass destruction](https://forum.effectivealtruism.org/tag/weapons-of-mass-destruction)\n\n1.  ^**[^](#fnrefd6fpj0ycxki)**^\n    \n    Robock, Alan (2010) [Nuclear winter](http://doi.org/10.1002/wcc.45), *WIREs Climate Change*, vol. 1, pp. 418–427."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "f4d3KbWLszzsKqxej",
    "name": "Translation",
    "core": false,
    "slug": "translation",
    "oldSlugs": null,
    "postCount": 20,
    "description": {
      "markdown": "Use the **translation** tag for posts about translating EA content into other languages, or for EA content that has already been translated."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "NTEQR6Saq3u3t22vZ",
    "name": "Research summary",
    "core": false,
    "slug": "research-summary",
    "oldSlugs": [
      "research-summary"
    ],
    "postCount": 129,
    "description": {
      "markdown": "The **research summary** tag can be applied to any post which summarizes a research paper/article originally published outside of the EA Forum.  \n  \n[research methods](https://forum.effectivealtruism.org/tag/research-methods) | [academia](https://forum.effectivealtruism.org/tag/academia-1/) | [impact assessment](https://forum.effectivealtruism.org/tag/impact-assessment) | [metascience](https://forum.effectivealtruism.org/topics/metascience) | [research training programs](https://forum.effectivealtruism.org/tag/research-training-programs) | [scientific progress](https://forum.effectivealtruism.org/tag/scientific-progress) | [disentanglement research](https://forum.effectivealtruism.org/tag/disentanglement-research)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "KJv36NfEfPRLXiAkQ",
    "name": "Electoral reform",
    "core": false,
    "slug": "electoral-reform",
    "oldSlugs": [
      "electoral-reform"
    ],
    "postCount": 33,
    "description": {
      "markdown": "**Electoral reform** is change in electoral systems, including voting systems, to better align public policies with social preferences.\n\nEvaluation\n----------\n\n[80,000 Hours](https://forum.effectivealtruism.org/tag/80-000-hours) rates electoral reform a \"potential highest priority area\": an issue that, if more thoroughly examined, could rank as a top global challenge.^[\\[1\\]](#fnsu5rbon25fq)^\n\nFurther reading\n---------------\n\nQuinn, Jameson (2018) [A voting theory primer for rationalists](https://www.lesswrong.com/posts/D6trAzh6DApKPhbv4/a-voting-theory-primer-for-rationalists), *LessWrong*, April 12.  \n*An excellent introduction to voting theory.*\n\nRelated entries\n---------------\n\n[All-Party Parliamentary Group for Future Generations](https://forum.effectivealtruism.org/tag/all-party-parliamentary-group-for-future-generations) | [Center for Election Science](https://forum.effectivealtruism.org/tag/center-for-election-science) | [improving institutional decision-making](https://forum.effectivealtruism.org/topics/improving-institutional-decision-making) | [longtermist institutional reform](https://forum.effectivealtruism.org/topics/longtermist-institutional-reform)\n\n1.  ^**[^](#fnrefsu5rbon25fq)**^\n    \n    80,000 Hours (2022) [Our current list of pressing world problems](https://80000hours.org/problem-profiles/), *80,000 Hours*."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "rrsDstdgzQnZ69Psy",
    "name": "Personal finance",
    "core": false,
    "slug": "personal-finance",
    "oldSlugs": [
      "personal-finance"
    ],
    "postCount": 53,
    "description": {
      "markdown": "The **personal finance** tag covers posts about... personal finance! Sample topics include:\n\n*   Making tax-efficient donations\n*   Improving one's personal budgeting\n*   Building personal runway before making a career change.\n\nFurther reading\n---------------\n\nDai, Wei (2020) [Tips/tricks/notes on optimizing investments](https://www.lesswrong.com/posts/RWna48W4bPr2ChePz/tips-tricks-notes-on-optimizing-investments), *LessWrong*, May 6.\n\nRelated entries\n---------------\n\n[donation choice](https://forum.effectivealtruism.org/tag/donation-choice) | [earning to give](https://forum.effectivealtruism.org/tag/earning-to-give) | [effective altruism lifestyle](https://forum.effectivealtruism.org/tag/effective-altruism-lifestyle) | [timing of philanthropy](https://forum.effectivealtruism.org/tag/timing-of-philanthropy)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "mrRrkfkn3XECic5M2",
    "name": "Books",
    "core": false,
    "slug": "books",
    "oldSlugs": [
      "ea-books",
      "ea-books",
      "ea-books",
      "ea-books",
      "effective-altruism-books",
      "books",
      "jual-obat-aborsi-081215505068-obat-penggugur-kandungan-asli"
    ],
    "postCount": 126,
    "description": {
      "markdown": "The **books** tag covers posts about books.\n\nIf you're looking for summaries and reviews, you might also find LessWrong's \"[Book reviews](https://www.lesswrong.com/tag/book-reviews)\" tag relevant. On the Forum, the \"[collections and resources](https://forum.effectivealtruism.org/tag/collections-and-resources)\" tag might also be related. \n\nSome EA books have their own tag: [*The Precipice*](https://forum.effectivealtruism.org/tag/the-precipice) | [*Doing Good Better* ](https://forum.effectivealtruism.org/tag/doing-good-better-book) | [*Poor Economics*](https://forum.effectivealtruism.org/tag/poor-economics) | [*Superintelligence*](https://forum.effectivealtruism.org/tag/superintelligence-book) | [*The Life You Can Save*](https://forum.effectivealtruism.org/tag/the-life-you-can-save-book) | [*Human Compatible*](https://forum.effectivealtruism.org/tag/human-compatible-book)  |  [*What We Owe the Future*](https://forum.effectivealtruism.org/tag/what-we-owe-the-future)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "RmWoKLTCYrhRCWvj3",
    "name": "Ballot initiative",
    "core": false,
    "slug": "ballot-initiative",
    "oldSlugs": [
      "direct-democracy",
      "direct-democracy",
      "direct-democracy",
      "direct-democracy"
    ],
    "postCount": 12,
    "description": {
      "markdown": "A **ballot initiative** is a process through which citizens propose and vote on new laws directly.\n\nTaxonomy\n--------\n\nBallot initiatives fall under the general category of a *ballot measure*, or a proposed legislation put to the electorate for a vote. The specific feature of a ballot initiative is that the proposed legislation also originates in the electorate: its sponsors collect signatures from the general public, which then gets to vote on it if sufficiently many signatures are collected. A ballot measure that instead originates in the government is called  a *referendum* in the United States,^[\\[1\\]](#fnhjotc206bzu)^ though in other countries the term may be used more broadly to refer to any type of measure.\n\nPrevalence\n----------\n\nBallot initiatives are relatively rare across the world: their use is concentrated in the United States and Switzerland, and only a few other countries allow and employ them with regularity. All 26 Swiss cantons permit ballot initiatives, as do 24 U.S. states and over half of U.S. cities.^[\\[2\\]](#fnufnau151vaa)^\n\nAssessment\n----------\n\nArguably, ballot initiatives are a more cost-effective way to influence legislation than traditional lobbying. This may be especially the case when the proposed legislation is opposed by powerful interest groups, such as agricultural lobbies opposing reforms to improve [the welfare of farmed animals](https://forum.effectivealtruism.org/tag/farmed-animal-welfare).^[\\[3\\]](#fn5g4apub27rt)^ Even failed initiatives can have significant positive impacts, by raising awareness about the relevant issues and by shifting public attitudes. And besides their direct impact, ballot initiatives can influence policy indirectly by motivating legislators and interest groups.^[\\[2\\]](#fnufnau151vaa)^\n\nThe financial cost of an initiative is uncertain in advance, so estimating the *ex ante* cost-effectiveness of an initiative can be difficult. In the United States, initiatives are also vulnerable to various legal challenges, which may further complicate the analysis. These concerns are to a certain degree alleviated for initiatives in areas with existing precedents. For example, there have been dozens of [electoral reform](https://forum.effectivealtruism.org/tag/electoral-reform) initiatives in the U.S., and collected data about these attempts allow for reasonably robust estimates of many of the key uncertainties.^[\\[4\\]](#fnvucf4n0zcbq)^\n\nBallot initiatives promoted by effective altruist organizations\n---------------------------------------------------------------\n\nOrganizations in the effective altruism community have relied on ballot initiatives to promote a variety of causes.^[\\[2\\]](#fnufnau151vaa)^ The [Center for Election Science](https://forum.effectivealtruism.org/tag/center-for-election-science) was involved in successful initiatives to get approval voting adopted in the U.S. cities of Fargo, North Dakota and St. Louis, Missouri.^[\\[5\\]](#fnzfh80j252c7)^^[\\[6\\]](#fngfabld1he6r)^ [Open Philanthropy](https://forum.effectivealtruism.org/tag/open-philanthropy) granted $4 million to the campaign in favor of Proposition 12, which required cage-free housing and improved space requirements for farmed animals raised in California or used in the production of animal products sold in that state.^[\\[7\\]](#fn6zu8vaqgr1)^ [Guarding Against Pandemics](https://forum.effectivealtruism.org/tag/guarding-against-pandemics) has recently funded a ballot initiative in Denver to promote research on [pandemic-preparedness](https://forum.effectivealtruism.org/tag/pandemic-preparedness).^[\\[8\\]](#fnxi2k1wzmx8)^ In Switzerland, [Sentience Politics](https://forum.effectivealtruism.org/tag/sentience-politics) has supported initiatives to grant fundamental rights for primates^[\\[9\\]](#fnjdxhbn1j9i)^ and to ban factory farming;^[\\[10\\]](#fnf2mbmxadl5)^ and the [Effective Altruism Foundation](https://forum.effectivealtruism.org/tag/effective-altruism-foundation) launched a ballot initiative that more than doubled Zurich's [development aid](https://forum.effectivealtruism.org/tag/foreign-aid) budget.^[\\[11\\]](#fnep55px2a79r)^\n\nFurther reading\n---------------\n\nSchukraft, Jason (2020) [Intervention profile: ballot initiatives](https://forum.effectivealtruism.org/posts/2LdswNsEZAgDfJDzo/intervention-profile-ballot-initiatives), *Effective Altruism Forum*, January 13.\n\nRelated entries\n---------------\n\n[democracy](https://forum.effectivealtruism.org/tag/democracy) | [improving institutional decision-making](https://forum.effectivealtruism.org/topics/improving-institutional-decision-making) | [policy](https://forum.effectivealtruism.org/tag/policy)\n\n1.  ^**[^](#fnrefhjotc206bzu)**^\n    \n    Ballotpedia (2021) [Initiative and referendum](https://ballotpedia.org/Initiative_and_referendum), *Ballotpedia*.\n    \n2.  ^**[^](#fnrefufnau151vaa)**^\n    \n    Schukraft, Jason (2020) [Intervention profile: ballot initiatives](https://forum.effectivealtruism.org/posts/2LdswNsEZAgDfJDzo/intervention-profile-ballot-initiatives), *Effective Altruism Forum*, January 13.\n    \n3.  ^**[^](#fnref5g4apub27rt)**^\n    \n    Muehlhauser, Luke (2017) [A conversation with Lewis Bollard](https://www.openphilanthropy.org/sites/default/files/Lewis_Bollard_02-23-17_%28public%29.pdf), *Open Philanthropy*, February 23.\n    \n4.  ^**[^](#fnrefvucf4n0zcbq)**^\n    \n    Stafforini, Pablo (2016) [Electoral reform initiatives: historical data](https://docs.google.com/spreadsheets/d/14Se60Gic7PMpv4-cY-gw2L4HjKMYf9sb7ztRIXEJ4wQ/edit?usp=drive_web&ouid=105467680319054692970&usp=embed_facebook), unpublished.\n    \n5.  ^**[^](#fnrefzfh80j252c7)**^\n    \n    Piper, Kelsey (2018) [This city just approved a new election system never tried before in America](https://www.vox.com/future-perfect/2018/11/15/18092206/midterm-elections-vote-fargo-approval-voting-ranked-choice), *Vox*, November 15.\n    \n6.  ^**[^](#fnrefgfabld1he6r)**^\n    \n    Ballotpedia (2021) [St. Louis, Missouri, Proposition D, Approval Voting Initiative (November 2020)](https://ballotpedia.org/St._Louis,_Missouri,_Proposition_D,_Approval_Voting_Initiative_(November_2020)), *Ballotpedia*.\n    \n7.  ^**[^](#fnref6zu8vaqgr1)**^\n    \n    Bollard, Lewis (2018) [Prevent Cruelty California — ‘Yes on Prop 12’ Campaign](https://www.openphilanthropy.org/focus/us-policy/farm-animal-welfare/prevent-cruelty-california-prop-12), *Open Philanthropy*, October 3.\n    \n8.  ^**[^](#fnrefxi2k1wzmx8)**^\n    \n    Guarding Against Pandemics (2021) [Guarding Against Pandemics](https://forum.effectivealtruism.org/posts/Btm562wDNEuWXj9Gk/guarding-against-pandemics), *Effective Altruism Forum,* September 18.\n    \n9.  ^**[^](#fnrefjdxhbn1j9i)**^\n    \n    Fasel, Raffael *et al.* (2016) [Fundamental rights for primates](https://ea-foundation.org/files/Fundamental-Rights-for-Primates.pdf), *Sentience Politics*.\n    \n10.  ^**[^](#fnreff2mbmxadl5)**^\n    \n    Swissinfo.ch (2019) [Swiss to vote on banning factory farming](https://web.archive.org/web/20200411001524/https://www.swissinfo.ch/eng/politics/animal-rights_swiss-to-vote-on-banning-factory-farming/45233958), *Swissinfo.ch*, September 17.\n    \n11.  ^**[^](#fnrefep55px2a79r)**^\n    \n    Vollmer, Jonas (2020) [EAF’s ballot initiative doubled Zurich’s development aid](https://forum.effectivealtruism.org/posts/dTdSnbBB2g65b2Fb9/eaf-s-ballot-initiative-doubled-zurich-s-development-aid), *Effective Altruism Forum*, January 13."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "vrpunJqGNgp4g5HfJ",
    "name": "History of effective altruism",
    "core": false,
    "slug": "history-of-effective-altruism",
    "oldSlugs": [
      "ea-history",
      "history-of-ea"
    ],
    "postCount": 41,
    "description": {
      "markdown": "The **history of effective altruism** covers the origins and development of [effective altruism](https://forum.effectivealtruism.org/tag/effective-altruism).\n\nFurther reading\n---------------\n\nAngot, Eve-Léana & Elise Cathala (2019) [Chronologie du mouvement de l’altruisme efficace](https://www.altruismeefficacefrance.org/chronologie-du-mouvement-de-laltruisme-efficace/), *Altruisme Efficace France*, October 14.\n\nAnthis, Jacy Reese (2022) [Some early history of effective altruism](https://jacyanthis.com/some-early-history-of-effective-altruism), *Jacy Reese Anthis’s Website*, May 15 (updated 7 June 2022).\n\nCarey, Ryan (2022) [Comments on Jacy Reese Anthis’ “Some early history of effective altruism”](https://forum.effectivealtruism.org/posts/ZbdNFuEP2zWN5w2Yx/ryancarey-s-shortform?commentId=ag9TgbFENCmj34vDk), *Effective Altruism Forum*, July 2.\n\nKaufman, Jeff (2013) [My effective altruism timeline](https://www.jefftk.com/p/my-effective-altruism-timeline), *Jeff Kaufman’s Blog*, August 10.\n\nMacAskill, William (2014) [The history of the term “effective altruism”](https://forum.effectivealtruism.org/posts/9a7xMXoSiQs3EYPA2/the-history-of-the-term-effective-altruism), *Effective Altruism Forum*, March 10.\n\nMatthews, Dylan (2022) [How effective altruism went from a niche movement to a billion-dollar force](https://www.vox.com/future-perfect/2022/8/8/23150496/effective-altruism-sam-bankman-fried-dustin-moskovitz-billionaire-philanthropy-crytocurrency), *Vox*, August 8.\n\nRice, Issa *et al* (2017) [Timeline of effective altruism](https://timelines.issarice.com/wiki/Timeline_of_effective_altruism), *Timelines Wiki* (updated June 2021).\n\nSinick, Jonah (2013) [A personal history of involvement with effective altruism](https://www.lesswrong.com/posts/QSHwKqyY4GAXKi9tX/a-personal-history-of-involvement-with-effective-altruism), *LessWrong*, June 11.\n\nRelated entries\n---------------\n\n[effective altruism](https://forum.effectivealtruism.org/tag/effective-altruism) | [Felicifia](https://forum.effectivealtruism.org/topics/felicifia) | [history](https://forum.effectivealtruism.org/tag/history) | [history of existential risk](https://forum.effectivealtruism.org/tag/history-of-existential-risk) | [social and intellectual movements](https://forum.effectivealtruism.org/tag/social-and-intellectual-movements)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "Ap3maZNsJw2McNNQy",
    "name": "Effective altruism messaging",
    "core": false,
    "slug": "effective-altruism-messaging",
    "oldSlugs": [
      "ea-messaging",
      "ea-messaging",
      "ea-messaging"
    ],
    "postCount": 218,
    "description": {
      "markdown": "The **effective altruism messaging** tag applies to posts that discuss ways to convey EA-related information to various audiences (e.g. via framing or graphic design).\n\nRelated entries\n---------------\n\n[documentaries](https://forum.effectivealtruism.org/tag/documentaries) | [effective altruism art and fiction](https://forum.effectivealtruism.org/tag/effective-altruism-art-and-fiction) | [effective altruism in the media](https://forum.effectivealtruism.org/tag/effective-altruism-in-the-media) | [moral psychology](https://forum.effectivealtruism.org/tag/moral-psychology) | [definition of effective altruism](https://forum.effectivealtruism.org/tag/definition-of-effective-altruism)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "fmbqY2T9dcWt8dkiT",
    "name": "Criminal justice reform",
    "core": false,
    "slug": "criminal-justice-reform",
    "oldSlugs": [
      "criminal-justice-reform",
      "criminal-justice-reform",
      "criminal-justice-reform"
    ],
    "postCount": 18,
    "description": {
      "markdown": "**Criminal justice reform** is the set of interventions aimed at improving the criminal justice system in a variety of ways. Most [effective altruism](https://forum.effectivealtruism.org/topics/effective-altruism) work in this area is focused on the [United States](https://forum.effectivealtruism.org/topics/united-states),^[\\[1\\]](#fn9s6ditdfhio)^ but posts about other areas of the world are also welcome.\n\nFurther detail\n--------------\n\nCrime can be costly to society, and incarceration can also be costly: to the government that funds prisons, and to the individuals whose freedoms are curtailed. Criminal justice reform focuses on improving the criminal justice and prison systems to reduce these imprisonment costs.\n\nCriminal justice reform is particularly promising in the United States. It is also unusually tractable, due to the adverse US fiscal situation and the confluence of liberal and conservative interest in this area. For these reasons, [Open Philanthropy](https://forum.effectivealtruism.org/topics/open-philanthropy) considers criminal justice reform a promising intervention, and has funded several charities working in this space.^[\\[1\\]](#fn9s6ditdfhio)^\n\nFurther reading\n---------------\n\nOpen Philanthropy (2014) [Criminal justice reform](https://www.openphilanthropy.org/research/criminal-justice-reform/), *Open Philanthropy*, May 12.\n\nSempere, Nuño (2022) [A critical review of Open Philanthropy’s bet on criminal justice reform](https://forum.effectivealtruism.org/posts/h2N9qEbvQ6RHABcae/a-critical-review-of-open-philanthropy-s-bet-on-criminal), *Effective Altruism Forum*, June 16.\n\nRelated entries\n---------------\n\n[Just Impact](https://forum.effectivealtruism.org/topics/just-impact) | [policy](https://forum.effectivealtruism.org/topics/policy)\n\n1.  ^**[^](#fnref9s6ditdfhio)**^\n    \n    Open Philanthropy (2016) [Criminal justice reform](http://www.openphilanthropy.org/focus/us-policy/criminal-justice-reform), *Open Philanthropy*."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "P4BNYQHBy3neNY2Pb",
    "name": "Self-care",
    "core": false,
    "slug": "self-care",
    "oldSlugs": [
      "self-care"
    ],
    "postCount": 153,
    "description": {
      "markdown": "Use the **self-care** tag for posts about caring for one's own emotional, mental, or physical health. \n\nEffective altruism can be intense, difficult, and even overwhelming; fortunately, the Forum contains a lot of good posts on how we can stay healthy and maintain balance as we try to do as much good as we can.\n\n## Related entries\n\n[effective altruism lifestyle](https://forum.effectivealtruism.org/tag/effective-altruism-lifestyle)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "iW9mRiqT3xvmaCbHz",
    "name": "International relations",
    "core": false,
    "slug": "international-relations",
    "oldSlugs": [
      "international-relations"
    ],
    "postCount": 74,
    "description": {
      "markdown": "**International relations** is the study of the relations of states with each other and with [international organizations](https://forum.effectivealtruism.org/tag/international-organization) and other units in the international system.\n\nFurther reading\n---------------\n\nJackson, Robert H., Georg Sørensen & Jørgen Møller (2022) [*Introduction to International Relations: Theories and Approaches*](https://en.wikipedia.org/wiki/Special:BookSources/978-0-19-886220-8), 8th ed., Oxford: Oxford University Press.\n\nRelated entries\n---------------\n\n[armed conflict](https://forum.effectivealtruism.org/tag/armed-conflict) | [international organizations](https://forum.effectivealtruism.org/tag/international-organization) | [global governance](https://forum.effectivealtruism.org/tag/global-governance)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "9cbYFsfL4X8DaQjPy",
    "name": "Hiring",
    "core": false,
    "slug": "hiring",
    "oldSlugs": [
      "ea-hiring",
      "ea-hiring",
      "ea-hiring",
      "effective-altruism-hiring"
    ],
    "postCount": 77,
    "description": {
      "markdown": "The **hiring** tag is used for posts on hiring processes in EA (specific hiring rounds and/or general strategy used by EA orgs for hiring).\n\nIt can also cover discussion of how hiring happens at orgs outside of EA, if especially high-impact positions are available within those orgs (e.g. a post on hiring practices within the British government).\n\nRelated entries\n---------------\n\n[constraints on effective altruism](https://forum.effectivealtruism.org/tag/constraints-on-effective-altruism) | [scalably using labour](https://forum.effectivealtruism.org/tag/scalably-using-labour) | [working at EA vs non-EA orgs](https://forum.effectivealtruism.org/tag/working-at-ea-vs-non-ea-orgs)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "FSEcBJTh4HFHyfanu",
    "name": "Effective altruism education",
    "core": false,
    "slug": "effective-altruism-education",
    "oldSlugs": [
      "ea-education",
      "ea-education",
      "ea-education",
      "ea-education"
    ],
    "postCount": 96,
    "description": {
      "markdown": "The **effective altruism education** tag covers posts about teaching EA concepts; not posts *that* teach, but posts *about* teaching.\n\nRelated entries\n---------------\n\n[building effective altruism](https://forum.effectivealtruism.org/tag/building-effective-altruism-1) | [competitive debating](https://forum.effectivealtruism.org/tag/competitive-debating) | [effective altruism outreach in schools](https://forum.effectivealtruism.org/tag/effective-altruism-outreach-in-schools/)| [teaching materials](https://forum.effectivealtruism.org/tag/teaching-materials)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "Smni8DQnAseEvuewF",
    "name": "Cash transfers",
    "core": false,
    "slug": "cash-transfers",
    "oldSlugs": [
      "cash-transfers",
      "cash-transfers",
      "cash-transfers"
    ],
    "postCount": 41,
    "description": {
      "markdown": "**Cash transfers** are direct payments, typically by governments or nonprofits, made to eligible groups of people.\n\nThree main types of cash transfer programs have been studied: *conditional cash transfers* (CCTs), *unconditional cash transfers* (UCTs), and *business grant programs*.\n\n[GiveWell](https://forum.effectivealtruism.org/tag/givewell) considers cash transfers to have the strongest track record of any non-health intervention, and estimates cash transfer programs to be in the same range of cost-effectiveness as their other priority programs.^[\\[1\\]](#fnuaqyr7no1z)^ As they acknowledge, this assessment is highly sensitive to certain empirical and moral assumptions, particularly concerning the investment returns to cash transfers and the relative value of averting child mortality *vis-à-vis* improving adult income.\n\nCash transfers as a benchmark\n-----------------------------\n\nAn important use of cash transfers, pioneered by [GiveDirectly](https://forum.effectivealtruism.org/tag/givedirectly) and championed by [GiveWell](https://forum.effectivealtruism.org/tag/givewell), is as a benchmark for comparing the value of different causes and for deciding whether to fund certain charities. Cash transfers are especially attractive as a benchmark because (1) they are \"massively scalable\"^[\\[2\\]](#fntnnqtyelgd)^ and (2) their cost-effectiveness can be straightforwardly expressed as a multiplier, given plausible assumptions about the relationship between money and [wellbeing](https://forum.effectivealtruism.org/tag/wellbeing). Specifically, if a doubling of income confers roughly the same benefit to people regardless of income level—as life satisfaction surveys appear to suggest—, then the average citizen of a rich nation can expect to accomplish about 100 times as much by making a cash transfer to someone living in extreme poverty than by donating that money locally or keeping it to herself. This is because people in extreme poverty earn about 100 times less than people in rich countries do on average, so a donation that can double the income of one rich person can also double the income of about 100 extremely poor people.^[\\[3\\]](#fn2yh747rj623)^^[\\[4\\]](#fnxs59w07p1rp)^\n\nFurther reading\n---------------\n\nCarter, Samantha (2019) [Are cash transfers the best policy option?](https://youtu.be/zrUAt1foKJE), *Effective Altruism Global*, October 20.\n\nDevereux, Stephen (2002) [Social protection for the poor: lessons from recent international experience](https://opendocs.ids.ac.uk/opendocs/bitstream/handle/20.500.12413/3907/Wp142.pdf), working paper no. 142, Institute of Development Studies.\n\nEvans, David K. & Anna Popova (2017) [Cash transfers and temptation goods](http://doi.org/10.1086/689575), *Economic Development and Cultural Change*, vol. 65, pp. 189–221.\n\nFiszbein, Ariel & Norbert Schady (2009) [*Conditional Cash Transfers: Reducing Present and Future Poverty*](http://doi.org/10.1596/978-0-8213-7352-1), Washington, D.C.: The World Bank.\n\nGiveWell (2012) [Cash transfers](https://www.givewell.org/international/technical/programs/cash-transfers), *GiveWell*, December (updated November 2018).\n\nMolina Millán, Teresa *et al.* (2019) [Long-term impacts of conditional cash transfers: review of the evidence](http://doi.org/10.1093/wbro/lky005), *The World Bank Research Observer*, vol. 34, pp. 119–159.\n\nPega, Frank *et al.* (2017) [Unconditional cash transfers for reducing poverty and vulnerabilities: effect on use of health services and health outcomes in low- and middle-income countries](https://www.cochranelibrary.com/cdsr/doi/10.1002/14651858.CD011135.pub2/related-content/podcast/51175/pt), *Cochrane Database of Systematic Reviews*, issue 11, art. no. CD011135.\n\n1.  ^**[^](#fnrefuaqyr7no1z)**^\n    \n    GiveWell (2012) [Cash transfers](https://www.givewell.org/international/technical/programs/cash-transfers), *GiveWell*, December (updated November 2018).\n    \n2.  ^**[^](#fnreftnnqtyelgd)**^\n    \n    Future Fund (2022) [Massively scalable projects](https://ftxfuturefund.org/massively-scalable-projects/), *Future Fund*, February 25.\n    \n3.  ^**[^](#fnref2yh747rj623)**^\n    \n    Karnofsky, Holden (2015) [Good ventures and giving now vs. later](https://blog.givewell.org/2015/11/25/good-ventures-and-giving-now-vs-later/), *The GiveWell Blog*, November 25.\n    \n4.  ^**[^](#fnrefxs59w07p1rp)**^\n    \n    Berger, Alexander (2019) [Givewell’s top charities are (increasingly) hard to beat](https://www.openphilanthropy.org/blog/givewells-top-charities-are-increasingly-hard-beat), *Open Philanthropy*, July 9."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "oQ49WDgZiP6geZWwv",
    "name": "Credence",
    "core": false,
    "slug": "credence",
    "oldSlugs": null,
    "postCount": null,
    "description": {
      "markdown": "The **credence** a person has in a proposition is the degree of belief that they have in that proposition: how probable they think it is, given their evidence.\n\nDegrees of belief are usually expressed using real numbers ranging from 0 to 1, where 0 means that the person thinks the proposition is certainly false, 1 means that they think the proposition is certainly true, and 0.5 means that they think that it is just as likely to be true as it is to be false (Hájek 2002). For example, if all of the evidence a person has suggests that there's a 73% chance of rain tomorrow, then their credence that it will rain tomorrow should be 0.73.\n\nRational credences follow the probability axioms. For example: it is irrational to think that \"the sky is blue and it is raining\" is more likely than \"it is raining\" (Wikipedia 2001). Credences are deemed more accurate the closer that they are to the truth (see Wikipedia 2006): if the sky is blue, a credence of 0.3 that the sky is blue is more accurate than a credence of 0.2 that the sky is blue, even if neither is particularly good.\n\nBecause credences can be represented by real numbers, they admit of arbitrarily high degrees of precision: the credence a person should have that their candidate will win the election could be 0.6172935479 … . However it seems that sometimes, especially when people are ignorant about the probability that a given event will occur, their credences should be [imprecise](https://forum.effectivealtruism.org/tag/credal-resilience). Imprecise credences can be represented as an interval. So a person’s credence that their candidate will win the election could be the interval \\[0.6, 0.65\\] instead of a precise credence within that interval (Bradley 2014).\n\nBibliography\n------------\n\nBradley, Seamus (2014) [Imprecise probabilities](https://plato.stanford.edu/entries/imprecise-probabilities/), *Stanford Encyclopedia of Philosophy*, December 20 (updated 19 February 2019).\n\nHájek, Alan (2002) [Interpretations of probability](https://plato.stanford.edu/entries/probability-interpret), sect. 3.3, *Stanford Encyclopedia of Philosophy*, October 21 (updated 28 August 2019).  \n*Discusses probabilities as degrees of belief in a proposition.*\n\nWikipedia (2001) [Probability axioms](https://en.wikipedia.org/wiki/Probability_axioms), *Wikipedia*, September 13 (updated 11 April 2021).\n\nWikipedia (2006) [Scoring rule](https://en.wikipedia.org/wiki/Scoring_rule), *Wikipedia*, February 15 (updated 13 February 2021‎).\n\nRelated entries\n---------------\n\n[credal resilience](https://forum.effectivealtruism.org/tag/credal-resilience) | [decision theory](https://forum.effectivealtruism.org/tag/decision-theory) | [expected value](https://forum.effectivealtruism.org/tag/expected-value) | [forecasting](https://forum.effectivealtruism.org/tag/forecasting) | [model uncertainty](https://forum.effectivealtruism.org/tag/model-uncertainty) | [value of information](https://forum.effectivealtruism.org/tag/value-of-information)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "vFrnmkq23j2DsZ7mQ",
    "name": "Direct work",
    "core": false,
    "slug": "direct-work",
    "oldSlugs": null,
    "postCount": null,
    "description": {
      "markdown": "Working directly on important problems is a path to impact which is obvious to most people.\n\nDirect work does not necessarily mean working for a charity: it can instead involve working for a government body or a for-profit company. In some cases, starting [a for-profit company that does useful work](https://forum.effectivealtruism.org/tag/funding-high-impact-for-profits) can be more effective that working in the non-profit sector, since there is often better feedback, and more available funding, to allow the organization to improve and grow more rapidly. Nor does direct work necessarily mean working on the front line: providing support that improves the [productivity](https://forum.effectivealtruism.org/topics/productivity) of others in the organization can often be very impactful.\n\nA lot of direct work is highly crowded: plenty of charity positions have many high-quality applicants. However, direct work on opportunities that are [important, tractable and neglected](https://forum.effectivealtruism.org/tag/itn-framework-1) may be very high impact. This is especially likely to be the case if the organization is primarily limited by an inability to find good employees (i.e. it is “talent constrained”) rather than by an inability to raise enough funds (“funding constrained”). (See [constraints on effective altruism](https://forum.effectivealtruism.org/tag/constraints-on-effective-altruism) for more details.)\n\nBibliography\n------------\n\nTodd, Benjamin (2014) [Which jobs help people the most?](https://80000hours.org/career-guide/high-impact-jobs/), *80,000 Hours*, October.  \n*An overview of direct work as a promising career approach.*\n\nRelated entries\n---------------\n\n[constraints on effective altruism](https://forum.effectivealtruism.org/tag/constraints-on-effective-altruism)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "LyNhNrYfrssoiYa77",
    "name": "EA Wiki: FAQ",
    "core": false,
    "slug": "ea-wiki-faq",
    "oldSlugs": null,
    "postCount": null,
    "description": {
      "markdown": "What is the EA Wiki?\n--------------------\n\nThe **Effective Altruism Wiki** (henceforth, the **Wiki**) attempts to build an online encyclopedia of effective altruism.\n\nWho can contribute to the Wiki?\n-------------------------------\n\nAnyone with an EA Forum account can edit the Wiki. A team of admins reviews new articles.\n\nHow does the Wiki relate to the tagging system?\n-----------------------------------------------\n\nTags are labels that can be attached to posts, and that help us organize the Forum content. Adopting a clever innovation introduced by LessWrong, we allow Wiki articles to also serve as tags, and vice versa. Thus, clicking on a tag both generates a list of all posts sharing that tag and displays the contents of the corresponding Wiki article. Extending the tag system in this way allows us to integrate the Wiki with the Forum better.\n\nWhy is the Wiki valuable?\n-------------------------\n\nEA content currently exists scattered in countless blog posts, forum articles, scholarly papers and even social media threads. We believe that there is considerable value in presenting all this content in summary form and organized systematically in a single location.\n\nThis helps those new to effective altruism familiarize themselves with its core ideas, and allows experienced effective altruists to find additional publications on a topic of interest. It also contributes to making effective altruism concepts more notable and recognizable; makes this content more easily discoverable on search engines; and facilitates its inclusion in other reference works, such as Wikipedia or the *Stanford Encyclopedia of Philosophy*.\n\nHow can I contribute to the EA Wiki?\n------------------------------------\n\nIf you think you may be a good fit for it, you should consider contributing to the EA Wiki. \n\nThe easiest way to become involved is to look at the [list of entries in need of work](https://forum.effectivealtruism.org/tags/dashboard?focus=allPages#Pages_in_need_of_work). These entries are flagged with a variety of 'TODO' flags, which tell you what sort of action is required. You can also flag entries yourself, if you notice that the entry requires a particular type of improvement.\n\nI have no experience with wikis. Can you help me get started?\n-------------------------------------------------------------\n\nDon't worry—it's pretty simple. Here's some basic guidance:\n\n*   Organize the articles into sections. Start with a concise summary of the subject in the *lead section*, followed by as many sections and subsections as the article requires. Optionally, you may also include after those the sections *Further reading*, *External links* and *Related entries*.\n*   When appropriate, provide *references*. Attach a footnote to the clause or sentence you want to reference. Do not worry about the proper formatting of references; we delegate this task to an assistant who is already familiar with our conventions.\n*   Use internal links only (i.e. links to other EA Wiki articles, or \"wikilinks\"), except in footnotes and the *External links* section.\n*   Write in an encyclopedic tone. Avoid slang, colloquialisms, legalese or unnecessary jargon. Avoid stating as facts claims that would be disputed by someone endorsing reasonable epistemic standards. Avoid making value judgments.\n\nYou can see examples of articles that conform to these standards [here](https://forum.effectivealtruism.org/tag/global-dystopia) and [here](https://forum.effectivealtruism.org/tag/80-000-hours), and learn more by consulting our [*Style Guide*](https://forum.effectivealtruism.org/tag/style-guide).\n\nWhere can I find a list of all Wiki articles?\n---------------------------------------------\n\n[Here](https://forum.effectivealtruism.org/tags/all) is a list of entries organized thematically, followed by a list in alphabetical order.\n\nHow can I see the most recent Wiki-related activity?\n----------------------------------------------------\n\nSee the [Recent Tag & Wiki activity](https://forum.effectivealtruism.org/tags/dashboard#Tag_activity_feed) section of the Wiki dashboard. It shows all Wiki edits and comments, sorted in reverse chronological order, and omits any other Forum activity.\n\nI would like to add references, but it is tedious to do this by hand. I'm also confused by your citation format. Is there a way to make this whole thing less annoying?\n-----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\nYou shouldn't waste time formatting references. Instead, either simply provide minimal bibliographic details for the work you want to cite  (e.g. \"Ord, Precipice\") so that we can then give it the appropriate format, or use Zotero—a popular reference management software—in combination with our  Citation Style Language (CSL) file, as explained in what follows.\n\n1.  Download and install the [Zotero desktop app](https://www.zotero.org/download/), the [Zutilo](https://github.com/wshanks/Zutilo) plugin, and (optionally) the [Zotero Connector](https://www.zotero.org/download/connectors) browser extension.\n2.  Open Zotero and go to 'Preferences'.\n3.  Under the 'Cite' tab, click on 'Get additional styles'.\n4.  Search for the 'Effective Altruism Wiki' style and click on it. This will add it to your style manager.\n5.  Under the 'Export' tab, select 'Effective Altruism Wiki' in the 'Item Format' dropdown menu.\n6.  Go to 'Tools > Zutilo Preferences'.\n7.  Scroll down until you see 'QuickCopy items' line, then select a convenient shortcut key and click 'Apply'.\n\nYou are now ready to insert properly formatted references. With the cursor on the work you want to cite, just press the shortcut key you selected. This will copy the citation to the clipboard in the correct format, which you can paste into the EA Forum editor. Please note that the editor needs to be configured to use markdown. If you don't want to use markdown, you can instead convert the markdown to HTML, and then copy and paste the rendered HTML into the editor. To do this, you can use an online [markdown to HTML converter](https://markdowntohtml.com/), or your favorite text editor.\n\nTo cite a work not currently in your Zotero database, you have two options. First, you can import it from the [EA Wiki public Zotero library](https://www.zotero.org/ea-wiki/library) (a database of all works currently cited somewhere on the Wiki). Second, you can add it with [Zotero Connector](https://www.zotero.org/download/connectors). We recommend setting up a keyboard shortcut. In Chrome, go to 'Settings > Extensions > Keyboard Shortcuts', then scroll down to Zotero Connector. In Firefox, go to 'Settings > Extensions & Themes', click on the wheel, then 'Manage Extension Shortcuts' and scroll down to Zotero Connector.\n\nIf you don't want to install Zotero, you can still access our [Zotero library](https://www.zotero.org/ea-wiki/library) and search for the work you want to reference.  If you find it,\n\n1.  Click on it to select it.\n2.  Click on the 'bookshelf' icon on top ('Create Bibliography').\n3.  Select 'other styles available'\n4.  Choose the 'Effective Altruism Wiki' citation style.\n\nAgain, this will show the reference in markdown, which you can easily convert to HTML.\n\nThe Wiki is missing an entry on a topic I would like to see covered. Can I create it?\n-------------------------------------------------------------------------------------\n\nFirst, check that the topic is not already covered by an entry with a different name. Once you have confirmed that the Wiki is missing an article for this topic, you can propose it [here](https://forum.effectivealtruism.org/posts/cB6LYs8s7afLvpTzm/propose-and-vote-on-potential-ea-wiki-articles-tags-2022) and receive feedback. We recommend this option for most new entries, since it gives experienced users the chance to make useful suggestions. Alternatively, you can also create the entry without asking for feedback. This approach may be appropriate if you think the entry is clearly worth adding (e.g. an entry for GiveWell's newest top charity). However, the entry may be removed if the admins decide that it fails to meet our criteria for inclusion.\n\nWhat are the criteria for inclusion?\n------------------------------------\n\nThe Wiki takes a broadly \"[inclusionist](https://en.wikipedia.org/wiki/Deletionism_and_inclusionism_in_Wikipedia)\" approach and welcomes articles on any topic, as long as they are appropriate for an encyclopedia focused on effective altruism. As a rough heuristic, an entry that is a suitable tag for at least three existing posts is probably worthy of inclusion. \n\nWe may try to codify the inclusion criteria more precisely in the future. We may also revise these criteria in response to external feedback or as we gain more experience.\n\nWho makes Wiki-related decisions?\n---------------------------------\n\nAt the moment, anyone can add new articles, though only admins can delete them. Wiki policies were drafted by the admin team and are periodically revised in response to feedback from contributors. Currently, the admins have the final say on all decisions related to the Wiki. However, we hope to explore other resolution mechanisms for Wiki disputes and other governance structures for the Wiki in the future."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "tXTtEctkg7tsK5b5L",
    "name": "Problem framework",
    "core": false,
    "slug": "problem-framework",
    "oldSlugs": null,
    "postCount": null,
    "description": {
      "markdown": "The **problem framework** is a framework for comparing the value of attempting to solve different problems developed by [80,000 Hours](https://forum.effectivealtruism.org/tag/80-000-hours). The problem framework extends the [ITN framework](https://forum.effectivealtruism.org/tag/itn-framework-1) by adding a fourth factor of [personal fit](https://forum.effectivealtruism.org/tag/personal-fit).\n\nFurther reading\n---------------\n\nWiblin, Robert (2016) [One approach to comparing global problems in terms of expected impact](https://80000hours.org/articles/problem-framework/), *80,000 Hours*, April (updated October 2019).\n\nRelated entries\n---------------\n\n[80,000 Hours](https://forum.effectivealtruism.org/tag/80-000-hours) | [cause prioritization](https://forum.effectivealtruism.org/tag/cause-prioritization) | [ITN framework](https://forum.effectivealtruism.org/tag/itn-framework-1) | [personal fit](https://forum.effectivealtruism.org/tag/personal-fit)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "tXTtEctkg7tsK5b5L",
    "name": "Problem framework",
    "core": false,
    "slug": "problem-framework",
    "oldSlugs": null,
    "postCount": null,
    "description": {
      "markdown": "The **problem framework** is a framework for comparing the value of attempting to solve different problems developed by [80,000 Hours](https://forum.effectivealtruism.org/tag/80-000-hours). The problem framework extends the [ITN framework](https://forum.effectivealtruism.org/tag/itn-framework-1) by adding a fourth factor of [personal fit](https://forum.effectivealtruism.org/tag/personal-fit).\n\nFurther reading\n---------------\n\nWiblin, Robert (2016) [One approach to comparing global problems in terms of expected impact](https://80000hours.org/articles/problem-framework/), *80,000 Hours*, April (updated October 2019).\n\nRelated entries\n---------------\n\n[80,000 Hours](https://forum.effectivealtruism.org/tag/80-000-hours) | [cause prioritization](https://forum.effectivealtruism.org/tag/cause-prioritization) | [ITN framework](https://forum.effectivealtruism.org/tag/itn-framework-1) | [personal fit](https://forum.effectivealtruism.org/tag/personal-fit)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "qinBL2qvsjycoLA5o",
    "name": "Influencing for-profits",
    "core": false,
    "slug": "influencing-for-profits",
    "oldSlugs": null,
    "postCount": null,
    "description": {
      "markdown": "For-profit organizations control a very large share of society’s resources. Though they are normally not driven primarily by altruistic concern, many companies do create social value by producing goods and services demanded by consumers. However, altruists can incentivize companies to increase the social value of their output. For instance, they can engage in [impact investing](https://forum.effectivealtruism.org/tag/impact-investing-1) or [divestment](https://forum.effectivealtruism.org/tag/divestment).\n\nTwo other ways of having a social impact within the for-profit sector, besides trying to change the incentive structure of for-profits, are to [fund a company](https://forum.effectivealtruism.org/tag/funding-high-impact-for-profits) that addresses an important social need, and to [work for a company](https://forum.effectivealtruism.org/tag/earning-to-give) and donate some of those earnings."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "XfvNojwvLSHfa7rEr",
    "name": "Organizations and projects in effective altruism",
    "core": false,
    "slug": "organizations-and-projects-in-effective-altruism",
    "oldSlugs": [
      "organizations-and-projects-in-effective-altruism",
      "organizations-and-proj-966561082467-buy-mifepristone-and"
    ],
    "postCount": null,
    "description": {
      "markdown": "<table><tbody><tr><td style=\"background-color:#e8f0f1;border:1pt solid #000000;padding:5pt;vertical-align:top\"><p><i>💡 You can find more topics on </i><a href=\"https://forum.effectivealtruism.org/topics/all\"><i><u>the main topics page</u></i></a><i>.&nbsp;&nbsp;</i></p><p><i>⚠️ Please note that this page is a </i><a href=\"https://en.wiktionary.org/wiki/beta_version#:~:text=Noun,public%2C%20for%20testing%20and%20feedback.\"><i><u>beta</u></i></a><i> version. If you have feedback on the page, please </i><a href=\"mailto:forum@centreforeffectivealtruism.org\"><i><u>contact us</u></i></a><i>.&nbsp;</i></p></td></tr></tbody></table>\n\nThis page collects pages for charities and other organizations that work on popular EA cause areas, or otherwise have some connection to the movement. \n\nYou might also be interested in the [organization updates](https://forum.effectivealtruism.org/topics/organization-updates) page, the [criticisms of EA organizations](https://forum.effectivealtruism.org/topics/criticism-of-effective-altruist-organizations) page, the [organization strategy](https://forum.effectivealtruism.org/topics/organization-strategy) page, or the [take action](https://forum.effectivealtruism.org/topics/take-action) page. \n\nOrganizations\n=============\n\nGlobal health and development\n-----------------------------\n\n*   [Abdul Latif Jameel Poverty Action Lab](https://forum.effectivealtruism.org/topics/abdul-latif-jameel-poverty-action-lab)\n*   [Against Malaria Foundation](https://forum.effectivealtruism.org/topics/against-malaria-foundation)\n*   [Bill & Melinda Gates Foundation](https://forum.effectivealtruism.org/topics/bill-and-melinda-gates-foundation)\n*   [Charter Cities Institute](https://forum.effectivealtruism.org/topics/charter-cities-institute)\n*   [Copenhagen Consensus Center](https://forum.effectivealtruism.org/topics/copenhagen-consensus-center)\n*   [Development Media International](https://forum.effectivealtruism.org/topics/development-media-international)\n*   [Deworm the World Initiative](https://forum.effectivealtruism.org/topics/deworm-the-world-initiative)\n*   [Dispensers for Safe Water](https://forum.effectivealtruism.org/topics/dispensers-for-safe-water)\n*   [The END Fund](https://forum.effectivealtruism.org/topics/the-end-fund)\n*   [Evidence Action](https://forum.effectivealtruism.org/topics/evidence-action)\n*   [Food Fortification Initiative](https://forum.effectivealtruism.org/topics/food-fortification-initiative)\n*   [GiveDirectly](https://forum.effectivealtruism.org/topics/givedirectly)\n*   [GiveWell](https://forum.effectivealtruism.org/topics/givewell)\n*   [Global Alliance for Improved Nutrition](https://forum.effectivealtruism.org/topics/global-alliance-for-improved-nutrition)\n*   [Global Health and Development Fund](https://forum.effectivealtruism.org/topics/global-health-and-development-fund)\n*   [Happier Lives Institute](https://forum.effectivealtruism.org/topics/happier-lives-institute)\n*   [Helen Keller International](https://forum.effectivealtruism.org/topics/helen-keller-international)\n*   [Iodine Global Network](https://forum.effectivealtruism.org/topics/iodine-global-network)\n*   [Innovations for Poverty Action](https://forum.effectivealtruism.org/topics/innovations-for-poverty-action)\n*   [Innovation in Government Initiative](https://forum.effectivealtruism.org/topics/innovation-in-government-initiative)\n*   [Lead Exposure Elimination Project](https://forum.effectivealtruism.org/topics/lead-exposure-elimination-project)\n*   [Living Goods](https://forum.effectivealtruism.org/topics/living-goods)\n*   [Malaria Consortium](https://forum.effectivealtruism.org/topics/malaria-consortium)\n*   [Médecins Sans Frontières](https://forum.effectivealtruism.org/topics/medecins-sans-frontieres)\n*   [New Incentives](https://forum.effectivealtruism.org/topics/new-incentives)\n*   [Policy Entrepreneurship Network](https://forum.effectivealtruism.org/topics/policy-entrepreneurship-network)\n*   [Precision Development](https://forum.effectivealtruism.org/topics/precision-development)\n*   [Sanku - Project Healthy Children](https://forum.effectivealtruism.org/topics/sanku)\n*   [SCI Foundation](https://forum.effectivealtruism.org/topics/sci-foundation)\n*   [Sightsavers](https://forum.effectivealtruism.org/topics/sightsavers)\n*   [Suvita](https://forum.effectivealtruism.org/topics/suvita)\n*   [Target Malaria](https://forum.effectivealtruism.org/topics/target-malaria)\n*   [Waitlist Zero](https://forum.effectivealtruism.org/topics/waitlist-zero)\n*   [Wave](https://forum.effectivealtruism.org/topics/wave)\n*   [World Health Organization](https://forum.effectivealtruism.org/topics/world-health-organization)\n*   [Zusha!](https://forum.effectivealtruism.org/topics/zusha)\n\nAnimal welfare\n--------------\n\n*   [Albert Schweitzer Foundation](https://forum.effectivealtruism.org/topics/albert-schweitzer-foundation)\n*   [Animal Empathy Philippines](https://forum.effectivealtruism.org/topics/animal-empathy-philippines)\n*   [Anima International](https://forum.effectivealtruism.org/topics/anima-international)\n*   [Animal Advocacy Careers](https://forum.effectivealtruism.org/topics/animal-advocacy-careers)\n*   [Animal Ask](https://forum.effectivealtruism.org/topics/animal-ask)\n*   [Animal Charity Evaluators](https://forum.effectivealtruism.org/topics/animal-charity-evaluators)\n*   [Animal Ethics](https://forum.effectivealtruism.org/topics/animal-ethics)\n*   [Animal Welfare Fund](https://forum.effectivealtruism.org/topics/animal-welfare-fund)\n*   [Aquatic Life Institute](https://forum.effectivealtruism.org/topics/aquatic-life-institute)\n*   [Cellular Agriculture Society](https://forum.effectivealtruism.org/topics/cellular-agriculture-society)\n*   [Compassion in World Farming](https://forum.effectivealtruism.org/topics/compassion-in-world-farming)\n*   [Crustacean Compassion](https://forum.effectivealtruism.org/topics/crustacean-compassion)\n*   [Faunalytics](https://forum.effectivealtruism.org/topics/faunalytics)\n*   [Fish Welfare Initiative](https://forum.effectivealtruism.org/topics/fish-welfare-initiative)\n*   [Good Food Institute](https://forum.effectivealtruism.org/topics/good-food-institute)\n*   [Healthier Hens](https://forum.effectivealtruism.org/topics/healthier-hens)\n*   [Humane Slaughter Association](https://forum.effectivealtruism.org/topics/humane-slaughter-association)\n*   [The Humane League](https://forum.effectivealtruism.org/topics/the-humane-league)\n*   [Mercy for Animals](https://forum.effectivealtruism.org/topics/mercy-for-animals)\n*   [New Harvest](https://forum.effectivealtruism.org/topics/new-harvest)\n*   [Sentience Institute](https://forum.effectivealtruism.org/topics/sentience-institute)\n*   [Sentience Politics](https://forum.effectivealtruism.org/topics/sentience-politics)\n*   [Shrimp Welfare Project](https://forum.effectivealtruism.org/topics/shrimp-welfare-project)\n*   [Vegan Outreach](https://forum.effectivealtruism.org/topics/vegan-outreach)\n*   [Welfare Footprint Project](https://forum.effectivealtruism.org/topics/welfare-footprint-project)\n*   [Wild Animal Initiative](https://forum.effectivealtruism.org/topics/wild-animal-initiative)\n\nMitigation of global risk factors and shaping the future\n--------------------------------------------------------\n\n*   [ALLFED](https://forum.effectivealtruism.org/topics/allfed)\n*   [All-Party Parliamentary Group for Future Generations](https://forum.effectivealtruism.org/topics/all-party-parliamentary-group-for-future-generations)\n*   [Berkeley Existential Risk Initiative](https://forum.effectivealtruism.org/topics/berkeley-existential-risk-initiative)\n*   [Bulletin of the Atomic Scientists](https://forum.effectivealtruism.org/topics/bulletin-of-the-atomic-scientists)\n*   [Cambridge Existential Risks Initiative](https://forum.effectivealtruism.org/topics/cambridge-existential-risk-initiative)\n*   [Center for Reducing Suffering](https://forum.effectivealtruism.org/topics/center-for-reducing-suffering)\n*   [Center for Space Governance](https://forum.effectivealtruism.org/topics/center-for-space-governance)\n*   [Center on Long-Term Risk](https://forum.effectivealtruism.org/topics/center-on-long-term-risk)\n*   [Centre for the Study of Existential Risk](https://forum.effectivealtruism.org/topics/centre-for-the-study-of-existential-risk)\n*   [Foresight Institute](https://forum.effectivealtruism.org/topics/foresight-institute)\n*   [Forethought Foundation](https://forum.effectivealtruism.org/topics/forethought-foundation)\n*   [Future Fund](https://forum.effectivealtruism.org/topics/future-fund)\n*   [Future of Humanity Institute](https://forum.effectivealtruism.org/topics/future-of-humanity-institute)\n*   [Future of Life Institute](https://forum.effectivealtruism.org/topics/future-of-life-institute)\n*   [Global Catastrophic Risk Institute](https://forum.effectivealtruism.org/topics/global-catastrophic-risk-institute)\n*   [Global Challenges Foundation](https://forum.effectivealtruism.org/topics/global-challenges-foundation)\n*   [Global Priorities Institute](https://forum.effectivealtruism.org/topics/global-priorities-institute)\n*   [Guarding Against Pandemics](https://forum.effectivealtruism.org/topics/guarding-against-pandemics)\n*   [Invincible Wellbeing](https://forum.effectivealtruism.org/topics/invincible-wellbeing)\n*   [Long-Term Future Fund](https://forum.effectivealtruism.org/topics/long-term-future-fund)\n*   [Longview Philanthropy](https://forum.effectivealtruism.org/topics/longview-philanthropy)\n*   [Nuclear Threat Initiative](https://forum.effectivealtruism.org/topics/nuclear-threat-initiative)\n*   [Pineapple Operations](https://forum.effectivealtruism.org/topics/pineapple-operations)\n*   [Ploughshares Fund](https://forum.effectivealtruism.org/topics/ploughshares-fund)\n*   [Polaris Ventures](https://forum.effectivealtruism.org/topics/polaris-ventures)\n*   [Research Institute for Future Design](https://forum.effectivealtruism.org/topics/research-institute-for-future-design)\n*   [Simon Institute for Longterm Governance](https://forum.effectivealtruism.org/topics/simon-institute-for-longterm-governance)\n*   [Stanford Existential Risks Initiative](https://forum.effectivealtruism.org/topics/stanford-existential-risks-initiative)\n*   [Survival and Flourishing Fund](https://forum.effectivealtruism.org/topics/survival-and-flourishing-fund)\n\nBuilding effective altruism\n---------------------------\n\n*   [.impact](https://forum.effectivealtruism.org/topics/impact)\n*   [80,000 Hours](https://forum.effectivealtruism.org/topics/80-000-hours)\n*   [Ayuda Efectiva](https://forum.effectivealtruism.org/topics/ayuda-efectiva)\n*   [Centre for Effective Altruism](https://forum.effectivealtruism.org/topics/centre-for-effective-altruism-cea)\n*   [Centre for Enabling EA Learning & Research](https://forum.effectivealtruism.org/topics/centre-for-enabling-ea-learning-and-research)\n*   [Charity Entrepreneurship](https://forum.effectivealtruism.org/topics/charity-entrepreneurship)\n*   [Charity Science Foundation](https://forum.effectivealtruism.org/topics/charity-science-foundation)\n*   [Doebem](https://forum.effectivealtruism.org/topics/doebem)\n*   [Donational](https://forum.effectivealtruism.org/topics/donational)\n*   [Effective Altruism and Consulting Network](https://forum.effectivealtruism.org/topics/effective-altruism-and-consulting-network)\n*   [Effective Altruism Anywhere](https://forum.effectivealtruism.org/topics/effective-altruism-anywhere)\n*   [Effective Altruism Foundation](https://forum.effectivealtruism.org/topics/effective-altruism-foundation)\n*   [Effective Altruism Funds](https://forum.effectivealtruism.org/topics/effective-altruism-funds)\n*   [Effective Altruism Hub](https://forum.effectivealtruism.org/topics/effective-altruism-hub)\n*   [Effective Altruism Infrastructure Fund](https://forum.effectivealtruism.org/topics/effective-altruism-infrastructure-fund)\n*   [Effective Thesis](https://forum.effectivealtruism.org/topics/effective-thesis)\n*   [Effektiv-Spenden.org](https://forum.effectivealtruism.org/topics/effektiv-spenden-org)\n*   [Founders Pledge](https://forum.effectivealtruism.org/topics/founders-pledge)\n*   [Generation Pledge](https://forum.effectivealtruism.org/topics/generation-pledge)\n*   [GiEffektivt.no](https://forum.effectivealtruism.org/topics/gieffektivt-no)\n*   [Giving What We Can](https://forum.effectivealtruism.org/topics/giving-what-we-can)\n*   [Good Growth](https://forum.effectivealtruism.org/topics/good-growth)\n*   [Good Ventures](https://forum.effectivealtruism.org/topics/good-ventures)\n*   [High Impact Athletes](https://forum.effectivealtruism.org/topics/high-impact-athletes)\n*   [High Impact Medicine](https://forum.effectivealtruism.org/topics/high-impact-medicine)\n*   [Let's Fund](https://forum.effectivealtruism.org/topics/let-s-fund)\n*   [The Life You Can Save](https://forum.effectivealtruism.org/topics/the-life-you-can-save)\n*   [Local Effective Altruism Network](https://forum.effectivealtruism.org/topics/lean)\n*   [Longtermist Entrepreneurship Fellowship](https://forum.effectivealtruism.org/topics/longtermist-entrepreneurship-fellowship)\n*   [One for the World](https://forum.effectivealtruism.org/topics/one-for-the-world-1)\n*   [Open Philanthropy](https://forum.effectivealtruism.org/topics/open-philanthropy)\n*   [Raising for Effective Giving](https://forum.effectivealtruism.org/topics/raising-for-effective-giving)\n*   [THINK](https://forum.effectivealtruism.org/topics/think)\n\nArtificial intelligence\n-----------------------\n\n*   [AI Impacts](https://forum.effectivealtruism.org/topics/ai-impacts)\n*   [AI Safety Camp](https://forum.effectivealtruism.org/topics/ai-safety-camp)\n*   [AI Safety Support](https://forum.effectivealtruism.org/topics/ai-safety-support)\n*   [Aligned AI](https://forum.effectivealtruism.org/topics/aligned-ai)\n*   [Alignment Research Center](https://forum.effectivealtruism.org/topics/alignment-research-center)\n*   [Anthropic](https://forum.effectivealtruism.org/topics/anthropic)\n*   [Center for Human-Compatible Artificial Intelligence](https://forum.effectivealtruism.org/topics/center-for-human-compatible-artificial-intelligence)\n*   [Center for Security and Emerging Technology](https://forum.effectivealtruism.org/topics/center-for-security-and-emerging-technology)\n*   [Centre for Long-Term Resilience](https://forum.effectivealtruism.org/topics/centre-for-long-term-resilience)\n*   [Centre for the Governance of AI](https://forum.effectivealtruism.org/topics/centre-for-the-governance-of-ai)\n*   [Cooperative AI Foundation ](https://forum.effectivealtruism.org/topics/cooperative-ai-foundation)\n*   [DeepMind](https://forum.effectivealtruism.org/topics/deepmind)\n*   [Epoch](https://forum.effectivealtruism.org/topics/epoch)\n*   [Fund for Alignment Research](https://forum.effectivealtruism.org/topics/fund-for-alignment-research)\n*   [Leverhulme Center for the Future of Intelligence](https://forum.effectivealtruism.org/topics/leverhulme-center-for-the-future-of-intelligence)\n*   [Machine Intelligence Research Institute](https://forum.effectivealtruism.org/topics/machine-intelligence-research-institute)\n*   [Nonlinear Fund](https://forum.effectivealtruism.org/topics/nonlinear-fund)\n*   [OpenAI](https://forum.effectivealtruism.org/topics/openai)\n*   [Ought](https://forum.effectivealtruism.org/topics/ought)\n*   [People for the Ethical Treatment of Reinforcement Learners](https://forum.effectivealtruism.org/topics/people-for-the-ethical-treatment-of-reinforcement-learners)\n\nOther/ multiple areas\n---------------------\n\n*   [1Day Sooner](https://forum.effectivealtruism.org/topics/1day-sooner)\n*   [Alvea](https://forum.effectivealtruism.org/topics/alvea)\n*   [Apollo Academic Surveys](https://forum.effectivealtruism.org/topics/apollo-academic-surveys)\n*   [Association for Long Term Existence and Resilience](https://forum.effectivealtruism.org/topics/association-for-long-term-existence-and-resilience)\n*   [Asterisk](https://forum.effectivealtruism.org/topics/asterisk)\n*   [Atlas Fellowship](https://forum.effectivealtruism.org/topics/atlas-fellowship)\n*   [Cambridge Summer Programme in Applied Reasoning](https://forum.effectivealtruism.org/topics/cambridge-summer-programme-in-applied-reasoning)\n*   [California YIMBY](https://forum.effectivealtruism.org/topics/california-yimby)\n*   [Canopie](https://forum.effectivealtruism.org/topics/canopie)\n*   [Center for Applied Rationality](https://forum.effectivealtruism.org/topics/center-for-applied-rationality)\n*   [Center for Election Science](https://forum.effectivealtruism.org/topics/center-for-election-science)\n*   [Democracy Defense Fund](https://forum.effectivealtruism.org/topics/democracy-defense-fund)\n*   [Effective Altruism Coaching](https://forum.effectivealtruism.org/topics/effective-altruism-coaching)\n*   [Emergent Ventures](https://forum.effectivealtruism.org/topics/emergent-ventures)\n*   [European Summer Program on Rationality](https://forum.effectivealtruism.org/topics/european-summer-program-on-rationality)\n*   [FTX Foundation](https://forum.effectivealtruism.org/topics/ftx-foundation)\n*   [Giving Green](https://forum.effectivealtruism.org/topics/giving-green)\n*   [Giving Multiplier](https://forum.effectivealtruism.org/topics/giving-multiplier)\n*   [High Impact Professionals](https://forum.effectivealtruism.org/topics/high-impact-professionals)\n*   [Impactful Government Careers](https://forum.effectivealtruism.org/topics/impactful-government-careers)\n*   [Johns Hopkins Center for Health Security](https://forum.effectivealtruism.org/topics/johns-hopkins-center-for-health-security)\n*   [Just Impact](https://forum.effectivealtruism.org/topics/just-impact)\n*   [Legal Priorities Project](https://forum.effectivealtruism.org/topics/legal-priorities-project)\n*   [Leverage Research](https://forum.effectivealtruism.org/topics/leverage-research)\n*   [Magnify Mentoring](https://forum.effectivealtruism.org/topics/magnify-mentoring)\n*   [Momentum](https://forum.effectivealtruism.org/topics/momentum)\n*   [Non-trivial](https://forum.effectivealtruism.org/topics/non-trivial)\n*   [Nucleic Acid Observatory](https://forum.effectivealtruism.org/topics/nucleic-acid-observatory)\n*   [Organisation for the Prevention of Intense Suffering](https://forum.effectivealtruism.org/topics/organisation-for-the-prevention-of-intense-suffering)\n*   [Our World in Data](https://forum.effectivealtruism.org/topics/our-world-in-data)\n*   [Probably Good](https://forum.effectivealtruism.org/topics/probably-good)\n*   [Oxford Prioritization Project](https://forum.effectivealtruism.org/topics/oxford-prioritisation-project)\n*   [Qualia Research Institute](https://forum.effectivealtruism.org/topics/qualia-research-institute)\n*   [Quantified Uncertainty Research Institute](https://forum.effectivealtruism.org/topics/quantified-uncertainty-research-institute)\n*   [RC Forward](https://forum.effectivealtruism.org/topics/rc-forward)\n*   [Rethink Charity](https://forum.effectivealtruism.org/topics/rethink-charity)\n*   [Rethink Priorities](https://forum.effectivealtruism.org/topics/rethink-priorities)\n*   [Schmidt Futures](https://forum.effectivealtruism.org/topics/schmidt-futures)\n*   [Society for the Diffusion of Useful Knowledge](https://forum.effectivealtruism.org/topics/society-for-the-diffusion-of-useful-knowledge)\n*   [SoGive](https://forum.effectivealtruism.org/topics/sogive)\n*   [Spark Wave](https://forum.effectivealtruism.org/topics/sparkwave)\n*   [StrongMinds](https://forum.effectivealtruism.org/topics/strongminds)\n*   [Summer Program on Applied Rationality and Cognition](https://forum.effectivealtruism.org/topics/summer-program-on-applied-rationality-and-cognition)\n*   [Swift Centre for Applied Forecasting](https://forum.effectivealtruism.org/topics/swift-centre-for-applied-forecasting)\n*   [United Nations](https://forum.effectivealtruism.org/topics/united-nations-1)\n\nHighly ineffective work\n-----------------------\n\n*   [Scared Straight](https://forum.effectivealtruism.org/tag/scared-straight)\n*   [PlayPump](https://forum.effectivealtruism.org/tag/playpump)\n\nOther projects\n==============\n\nEvents\n------\n\n*   [Conferences](https://forum.effectivealtruism.org/topics/conferences)\n*   [EAGx](https://forum.effectivealtruism.org/topics/eagx)\n*   [Effective Altruism Global](https://forum.effectivealtruism.org/topics/effective-altruism-global)\n*   [Leaders Forum](https://forum.effectivealtruism.org/topics/ea-leaders-forum)\n*   [Retreats](https://forum.effectivealtruism.org/topics/retreats)\n\nCommunities\n-----------\n\n*   [AI Alignment Forum](https://forum.effectivealtruism.org/topics/ai-alignment-forum)\n*   [Coworking spaces](https://forum.effectivealtruism.org/topics/coworking-spaces)\n*   [Effective Altruism Israel](https://forum.effectivealtruism.org/topics/ea-israel)\n*   [Effective Altruism for Christians](https://forum.effectivealtruism.org/topics/effective-altruism-for-christians)\n*   [Effective Altruism for Jews](https://forum.effectivealtruism.org/topics/effective-altruism-for-jews)\n*   [Effective Altruism London](https://forum.effectivealtruism.org/topics/effective-altruism-london)\n*   [Felicifia](https://forum.effectivealtruism.org/topics/felicifia)\n*   [LessWrong](https://forum.effectivealtruism.org/topics/lesswrong)\n*   [Manifold Markets](https://forum.effectivealtruism.org/topics/manifold-markets)\n*   [Metaculus](https://forum.effectivealtruism.org/topics/metaculus)\n*   [Twitter](https://forum.effectivealtruism.org/topics/twitter)\n\nProjects\n--------\n\n*   [Community projects](https://forum.effectivealtruism.org/topics/community-projects)\n*   [EA Giving Tuesday](https://forum.effectivealtruism.org/topics/ea-giving-tuesday)\n*   [EAEcon](https://forum.effectivealtruism.org/topics/eaecon)\n*   [EA Librarian](https://forum.effectivealtruism.org/topics/ea-librarian)\n*   [Effective Altruism Survey](https://forum.effectivealtruism.org/topics/effective-altruism-survey)\n*   [Effective Altruism Group Organisers' Survey](https://forum.effectivealtruism.org/topics/effective-altruism-group-organisers-survey)\n*   [Effective Altruism Policy Analytics](https://forum.effectivealtruism.org/topics/effective-altruism-policy-analytics)\n*   [Effective animal advocacy](https://forum.effectivealtruism.org/topics/effective-animal-advocacy)\n*   [Giving Pledge](https://forum.effectivealtruism.org/topics/giving-pledge)\n*   [Megaprojects](https://forum.effectivealtruism.org/topics/megaprojects)\n*   [Project for Awesome](https://forum.effectivealtruism.org/topics/project-for-awesome)\n*   [Projekt Framtid](https://forum.effectivealtruism.org/topics/projekt-framtid)\n*   [Students for High-Impact Charity](https://forum.effectivealtruism.org/topics/students-for-high-impact-charity)\n*   [Student projects](https://forum.effectivealtruism.org/topics/student-projects)\n\n[Newsletters](https://forum.effectivealtruism.org/topics/newsletters)\n---------------------------------------------------------------------\n\n*   [Effective Altruism Newsletter](https://forum.effectivealtruism.org/topics/effective-altruism-newsletter)\n*   [Alignment Newsletter](https://forum.effectivealtruism.org/topics/alignment-newsletter)\n*   [Forecasting Newsletter](https://forum.effectivealtruism.org/topics/forecasting-newsletter)\n*   [Future Matters](https://forum.effectivealtruism.org/topics/future-matters)\n*   [Giving What We Can Newsletter](https://forum.effectivealtruism.org/topics/giving-what-we-can-newsletter?sortedBy=new)\n*   [Monthly Overload of Effective Altruism](https://forum.effectivealtruism.org/topics/monthly-overload-of-effective-altruism)\n\n[Books](https://forum.effectivealtruism.org/tag/books) and [Effective altruism art and fiction](https://forum.effectivealtruism.org/tag/effective-altruism-art-and-fiction)\n---------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\n*   [*Doing Good Better*](https://forum.effectivealtruism.org/tag/doing-good-better)\n*   [Future Perfect](https://forum.effectivealtruism.org/tag/future-perfect)\n*   [*Human Compatible*](https://forum.effectivealtruism.org/tag/human-compatible)\n*   [*Poor Economics*](https://forum.effectivealtruism.org/tag/poor-economics)\n*   [*Superintelligence*](https://forum.effectivealtruism.org/tag/superintelligence-book)\n*   [*The Life You Can Save*](https://forum.effectivealtruism.org/tag/the-life-you-can-save)\n*   [*The Precipice*](https://forum.effectivealtruism.org/tag/the-precipice)\n*   [*What We Owe the Future*](https://forum.effectivealtruism.org/tag/what-we-owe-the-future)"
    },
    "parentTag": null,
    "subTags": []
  }
]