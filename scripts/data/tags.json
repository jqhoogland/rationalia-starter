[
  {
    "_id": "c42eTtBCXyJmtpqwZ",
    "name": "Using AI to solve Alignment",
    "core": false,
    "slug": "using-ai-to-solve-alignment",
    "oldSlugs": null,
    "postCount": 4,
    "description": {
      "markdown": "Not obviously the best name for this tag, but maybe good to explore/rename. Wiki-tags are publicly editable!"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "zuwsLCbxbugqB7FQY",
    "name": "Shard Theory",
    "core": false,
    "slug": "shard-theory",
    "oldSlugs": [
      "shard-theory",
      "shard-theory-of-human-values"
    ],
    "postCount": 7,
    "description": {
      "markdown": "Shard theory is an alignment research program, about the relationship between training variables and learned values in trained RL agents. It is thus an approach to progressively fleshing out a mechanistic account of human values, learned values in RL agents, and (to a lesser extent) the learned algorithms in ML generally.\n\nShard theory's basic ontology of RL holds that *shards* are contextually activated, behavior-steering computations in neural networks (biological and artificial). The circuits that implement a shard that garners reinforcement are reinforced, meaning that that shard will be more likely to trigger again in the future, when given similar cognitive inputs.\n\nAs an appreciable fraction of a neural network is composed of shards, large neural nets can possess quite intelligent constituent shards. These shards can be sophisticated enough to be well-modeled as playing negotiation games with each other, (potentially) explaining human psychological phenomena like akrasia and value changes from moral reflection. Shard theory also suggests an approach to explaining the shape of human values, and scheme for RL alignment."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "GKffzsk2bKssZGRva",
    "name": "Improving the LessWrong Wiki",
    "core": false,
    "slug": "improving-the-lesswrong-wiki",
    "oldSlugs": null,
    "postCount": null,
    "description": {
      "markdown": "*   This is a deliberate bastardisation and subversion of how the wiki is used\n*   In order that people don't think I'm a defector, I'm making clear what I'm doing as I'm doing it. Tell me what you think\n*   Anyone should still edit this document as if it were theirs\n*   The wiki would be better if it had a range of features I discuss below\n*   If you disagree, edit it! Sometimes edit wars are good\n\nWiki features and how valuable they would be\n============================================\n\n*   Comments on wikis\n    *   The current tech exists and it would make it much cleaner to write things I intend to work on in future.\n    *   Comments are hidden unless you click show. You can toggle them to be permanently shown\n*   Double bracket search and tagging \n    *   When you use \\[\\[, it opens up a search window to all Lesswrong tags. You type in what you want to reference. If there isn't one. it creates it eg  \\[\\[Solipsism\\]\\]\n*   Disputed changes sit beside text, like comments\n*   A process for requesting a number -> getting an accurate forecast\n    *   There is a description in the text\n    *   A user flags it for quantification\n    *   Other users collaboratively write a forecasting question\n    *   Users give estimates, weighted by LW karma score (I hate this but it would be better than nothing, hard to abuse etc)\n    *   Ideally, eventually this gets replaced by a manifold market or similar\n*   Toggle hide/unhide sections\n*   Gwern-style floating windows which appear when you hover over text in Lesswrong\n*   Embeddable manifold markets\n*   Embeddable squiggle estimation\n*   Taking variables from manifold markets and squiggle and displaying them within the text\n\nSome pages I have edited to be the style of what I think should be all pages\n============================================================================\n\n*   [https://www.lesswrong.com/tag/effective-altruism](https://www.lesswrong.com/tag/effective-altruism)\n    *   It now has a section on funding use, impact and criticims\n    *   With a little more work I think it could be the best single page introduction to EA\n*   [https://www.lesswrong.com/tag/forecasting-and-prediction](https://www.lesswrong.com/tag/forecasting-and-prediction)\n    *   A better overview of the topic"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "siW3hDgPwAKSDmwgG",
    "name": "Refine",
    "core": false,
    "slug": "refine",
    "oldSlugs": null,
    "postCount": 4,
    "description": {
      "markdown": "Refine is a conceptual research incubator hosted by Conjecture."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "E4wsWbWrqjovpMrYd",
    "name": "Research Taste",
    "core": false,
    "slug": "research-taste",
    "oldSlugs": null,
    "postCount": 5,
    "description": {
      "markdown": "**Research Taste** is the intuitions that guide researchers towards productive lines of inquiry."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "n7qCAKXJhop8kEYxh",
    "name": "PIBBSS",
    "core": false,
    "slug": "pibbss",
    "oldSlugs": null,
    "postCount": 7,
    "description": {
      "markdown": "*Principles for Intelligent Behaviour in Biological and Social Systems* (short: [PIBBSS](https://www.pibbss.ai/)) is an initiative aimed at facilitating knowledge transfer towards AI alignment from fields studying intelligent behaviour in natural systems."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "HsiovoPND7mWbW5D9",
    "name": "Distributional Shifts",
    "core": false,
    "slug": "distributional-shifts",
    "oldSlugs": [
      "out-of-distribution",
      "distributional-shifts-was-out-of-distribution"
    ],
    "postCount": 2,
    "description": {
      "markdown": "Many learning-theoretic setups (especially in the Frequentist camp) make an IID assumption: that data can be split up into samples (sometimes called episodes or data-points) which are independently sampled from identical distributions (hence \"IID\"). This assumption sometimes allows us to prove that our methods generalize well; see especially PAC learning. However, in real life, when we say that a model \"generalizes well\", we really mean that it works well on new data *which realistically has a somewhat different distribution*. This is called a **distributional shift** or a non-stationary environment*.*\n\nThis framework (in which we initially make an IID assumption, but then, model violations of it as \"distributional shifts\") has been used extensively to discuss robustness issues relating to AI safety -- particularly, inner alignment. We can confidently anticipate that traditional machine learning systems (such as deep neural networks) will perform well *on average*, so long as the deployment situation is statistically similar to the training data. However, as the deployment distribution gets further from the training distribution, catastrophic behaviors which are very rare on the original inputs can become probable.\n\nThis framework makes it sound like a significant part of the inner alignment problem would be solved if we could generalize learning guarantees from IID cases to non-IID cases. (Particularly if loss bounds can be given at finite times, not merely asymptotically, while maintaining a large, highly capable hypothesis class.)\n\nHowever, this is not necessarily the case.\n\nSolomonoff Induction avoids making an IID assumption, and so it is not strictly meaningful to talk about \"distributional shifts\" for a Solomonoff distribution. Furthermore, the Solomonoff distribution has constructive bounds, rather than merely asymptotic. (We can bound how difficult it is to learn something based on its description length.) Yet, inner alignment problems still seem very concerning for the Solomonoff distribution. \n\nThis is a complex topic, but one reason why is that inner optimizers can potentially tell the difference between training and deployment. A malign hypothesis can mimic a benign hypothesis until a critical point where a wrong answer has catastrophic potential. This is called a [treacherous turn](https://www.lesswrong.com/tag/treacherous-turn). \n\nSo, although \"distributional shift\" is not technically involved, we can see that a critical difference between training and deployment is still involved: during training, wrong answers are always inconsequential. However, when you *use* a system, wrong answers become consequential. If the system can figure this difference out, then parts of the system can use it to \"gate\" their behavior in order to accomplish a treacherous turn. \n\nThis makes \"distributional shift\" seem like an apt metaphor for what's going on in non-IID cases. However, buyer beware: eliminating IID assumptions might eliminate the literal source of the distributional shift problem without eliminating the broader constellation of concerns for which the words \"distributional shift\" are being used."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "E4TozA6yTzee5oPMa",
    "name": "Meetups (specific examples)",
    "core": false,
    "slug": "meetups-specific-examples",
    "oldSlugs": null,
    "postCount": 7,
    "description": {
      "markdown": "This tag collects specific meetups that an organizer can run or things community members can do when they come together. It does include instructions or at least an inspiring outline for running a particular activity, exercise, discussion or game with a group. It can also include guidance for events such as Winter Solstice or Petrov Day that a community might come together for. It does not include general advice for running meetups or discussions on the purpose of meetups. It also does not include announcements of meetups that will be held at a particular time and place, which would be under [Events](https://www.lesswrong.com/tag/events-community). The intended purpose of this tag is to be something a meetup organizer can browse, thinking \"what should I run for my group next meeting?\"\n\n**Related Pages:** [Exercises / Problem-sets](https://www.lesswrong.com/tag/exercises-problem-sets) and [Games (posts describing)](https://www.lesswrong.com/tag/games-posts-describing)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "LdhpgZDCR967hWcFt",
    "name": "Encultured AI (org)",
    "core": false,
    "slug": "encultured-ai-org",
    "oldSlugs": null,
    "postCount": 4,
    "description": {
      "markdown": ""
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "hwHxSdrxafFTuaCJE",
    "name": "Prompt Engineering",
    "core": false,
    "slug": "prompt-engineering",
    "oldSlugs": null,
    "postCount": 5,
    "description": {
      "markdown": "**Prompt Engineering** is the practice of designing inputs to go into an ML system (often a language model), to get it to produce a particular output."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "ESkc5kecgkLF235dj",
    "name": "Acute Risk Period",
    "core": false,
    "slug": "acute-risk-period",
    "oldSlugs": null,
    "postCount": null,
    "description": {
      "markdown": "The Acute Risk Period is the period of human history wherein it is possible to destroy all of civilization. (i.e. we destroy ourselves in nuclear war, or build an AGI that quickly bootstraps to be much more powerful than the rest of humanity combined).\n\nIt's an important strategic consideration in the field of Existential Risk – it's important to steer humanity safely through this period to a point where we are technologically and civilizationally mature, and have spread across the stars such that no one act could destroy everyone.\n\nSee also Existential Risk, AI Risk, and the Most Important Century.\n\n*This is a stub wiki post I hope gets fleshed out soon. I thought probably this was already written up on Arbital or something but I can't find it.*"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "woeLa6nvmhroCh3Fi",
    "name": "General Alignment Properties",
    "core": false,
    "slug": "general-alignment-properties",
    "oldSlugs": [
      "general-alignment-properties"
    ],
    "postCount": 1,
    "description": {
      "markdown": "The AI alignment properties of agents which would be interesting to a range of principals trying to solve AI alignment. For example:\n\n*   Does the AI \"care\" about reality, or just about its sensory observations?\n*   Does the AI properly navigate [ontological shifts](https://arbital.com/p/ontology_identification/)?\n*   Does the AI reason about itself as embedded in its environment?"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "MbhipAhtifhaDEAcp",
    "name": "Squiggle",
    "core": false,
    "slug": "squiggle",
    "oldSlugs": null,
    "postCount": 6,
    "description": {
      "markdown": "[Squiggle](https://quantifieduncertainty.org/careers) is a special-purpose programming language for probabilistic estimation. Think: *\"*[*Guesstimate*](https://getguesstimate.com/) *as a programming language.*\" Squiggle is free and [open-source](https://github.com/quantified-uncertainty/squiggle).\n\n**https://www.lesswrong.com/posts/mv6nuN7nhnNpj9MLr/announcing-squiggle-early-access**"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "7jaBCxPHRDfJppYws",
    "name": "AI Sentience",
    "core": false,
    "slug": "ai-sentience",
    "oldSlugs": null,
    "postCount": 1,
    "description": {
      "markdown": ""
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "yoLz9Ritmc7cve9a5",
    "name": "Addiction",
    "core": false,
    "slug": "addiction",
    "oldSlugs": null,
    "postCount": 1,
    "description": {
      "markdown": ""
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "KRSY6MhAcgtBCC9FH",
    "name": "2017-2019 AI Alignment Prize",
    "core": false,
    "slug": "2017-2019-ai-alignment-prize",
    "oldSlugs": [
      "ai-alignment-prize"
    ],
    "postCount": 6,
    "description": {
      "markdown": "The AI Alignment Prize was a contest carried out during 2017-2019, meant to encourage better work in the area. \n\nThroughout its existent, the alignment price paid $50k ($15k in the first iteration, $15k in the second, $10k in the third and $20k in the fourth and last). \n\nThe successive rounds received 40, 37, 12 and 10 entries, and awarded 6, 5, 2 and 4 people, respectively. Towards the end of the prize's existence, numbers declined, as the organizers note and reflect about in the body and comments of the last [post](https://www.lesswrong.com/posts/nDHbgjdddG5EN6ocg/announcement-ai-alignment-prize-round-4-winners#Moving_on). Two bottlenecks to participation and impact noted were a) lack of prestige, and b) the time requirements for the organizers.\n\nSince then, there have been other prizes hosted on LessWrong, some of which can be found in the [bounties and prizes](https://www.lesswrong.com/tag/bounties-and-prizes-active) tag, though older articles might not be tagged. One such related prize is [the ELK prize](https://www.lesswrong.com/posts/QEYWkRoCn4fZxXQAY/prizes-for-elk-proposals), which attracted 197 entries, selected 32 winners, and [gave out](https://www.lesswrong.com/posts/QEYWkRoCn4fZxXQAY/prizes-for-elk-proposals) $274,000 in prizes."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "XGEumKCYarMaZh8iu",
    "name": "Air Conditioning",
    "core": false,
    "slug": "air-conditioning",
    "oldSlugs": null,
    "postCount": 6,
    "description": {
      "markdown": ""
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "haiwnEEx3vhrkfmAP",
    "name": "AI Robustness",
    "core": false,
    "slug": "ai-robustness",
    "oldSlugs": [
      "ai-solution-robustness"
    ],
    "postCount": 3,
    "description": {
      "markdown": ""
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "YYFBmLCzeFsyd27rd",
    "name": "SERI MATS",
    "core": false,
    "slug": "seri-mats",
    "oldSlugs": null,
    "postCount": 35,
    "description": {
      "markdown": "The **Stanford Existential Risks Initiative ML Alignment Theory Scholars** program.\n\n[https://www.serimats.org/](https://www.serimats.org/)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "a78XXmCuD3eFLYzph",
    "name": "Verification",
    "core": false,
    "slug": "verification",
    "oldSlugs": null,
    "postCount": 1,
    "description": {
      "markdown": ""
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "LSruxwjPrufsD8swY",
    "name": "Fashion",
    "core": false,
    "slug": "fashion",
    "oldSlugs": [
      "fashion-1"
    ],
    "postCount": 7,
    "description": {
      "markdown": "**Fashion** is a form of self-expression which involves clothing, footwear, lifestyle, accessories, makeup, hairstyle, and body posture. It can be used for [signalling](https://www.lesswrong.com/tag/signaling), and what is \"fashionable\" changes across history, cultures, and contexts."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "xKz23YJ7h5JZijGFs",
    "name": "Spurious Counterfactuals",
    "core": false,
    "slug": "spurious-counterfactuals",
    "oldSlugs": null,
    "postCount": 4,
    "description": {
      "markdown": "**Spurious Counterfactuals** are spuriously low evaluations of the quality of a potential action, which are only provable because they are self-fulfilling (usually due to Lob's theorem). For example, if I know that I go left, then it is logically true that if I went right, I would get -10 utility (because in classical logic, false statements imply any statement). This suggests that if I fully believed that I went left, then I would indeed go left. By Lob's theorem, I indeed go left. \n\nBuilding agents who avoid this line of reasoning, despite having full access to their own source code and the ability to logically reason about their own behavior, is one goal of [Embedded Agency](https://www.lesswrong.com/tag/embedded-agency)."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "4PDaoDFQGem3ruHbN",
    "name": "Tensor Networks",
    "core": false,
    "slug": "tensor-networks",
    "oldSlugs": null,
    "postCount": 1,
    "description": {
      "markdown": ""
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "rCChqJHkgiGxccrNw",
    "name": "NSFW",
    "core": false,
    "slug": "nsfw",
    "oldSlugs": null,
    "postCount": 1,
    "description": {
      "markdown": "Posts that are **Not Safe For Work**"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "AGgktgYb72PPjET9r",
    "name": "Multipolar Scenarios",
    "core": false,
    "slug": "multipolar-scenarios",
    "oldSlugs": null,
    "postCount": 7,
    "description": {
      "markdown": "A **multipolar scenario** is one where no single AI or agent takes over the world.\n\nFeatured in the book \"Superintelligence\" by Nick Bostrom."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "YkPxg2tFDQNdaEZDJ",
    "name": "AI Risk Concrete Stories",
    "core": false,
    "slug": "ai-risk-concrete-stories",
    "oldSlugs": null,
    "postCount": 8,
    "description": {
      "markdown": ""
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "6zBEfFYJxhSEcchbR",
    "name": "AI Alignment Fieldbuilding",
    "core": false,
    "slug": "ai-alignment-fieldbuilding",
    "oldSlugs": null,
    "postCount": 22,
    "description": {
      "markdown": "**AI Alignment Fieldbuilding** is the effort to improve the alignment ecosystem. Some priorities include introducing new people to the importance of AI risk, on-boarding them by connecting them with key resources and ideas, educating them on existing literature and methods for generating new and valuable research, supporting people who are contributing, and maintaining and improving the funding systems.\n\nThere is an invite-only Slack for people working on the alignment ecosystem. If you'd like to join message [plex](https://www.lesswrong.com/users/ete) with an overview of your involvement."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "cawGhjgJmnoLLHXMC",
    "name": "Adversarial Training",
    "core": false,
    "slug": "adversarial-training",
    "oldSlugs": null,
    "postCount": 4,
    "description": {
      "markdown": ""
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "uiWL6tGbeZbxNwpz7",
    "name": "Global Poverty",
    "core": false,
    "slug": "global-poverty",
    "oldSlugs": null,
    "postCount": 1,
    "description": {
      "markdown": ""
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "cj4oev4dKKkmrpXty",
    "name": "Boltzmann's brains",
    "core": false,
    "slug": "boltzmann-s-brains",
    "oldSlugs": null,
    "postCount": 7,
    "description": {
      "markdown": ""
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "rZCxz9XBonCFLdwja",
    "name": "Poverty",
    "core": false,
    "slug": "poverty",
    "oldSlugs": null,
    "postCount": 2,
    "description": {
      "markdown": ""
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "dNNwmxdmvtxMGMFgX",
    "name": "Air Quality",
    "core": false,
    "slug": "air-quality",
    "oldSlugs": null,
    "postCount": 12,
    "description": {
      "markdown": "Poor **Air Quality** can reduce cognitive functioning[^1], lifespans[^2] and the techniques to improve air quality are also useful for getting rid of aerosolized respiratory pathogens. Improving air quality can be an impactful global health intervention.[^3] Many members of the LessWrong community have also put effort into improving the air quality of their own homes or offices, as an implication of instrumental rationality.\n\nNewer Green buildings are infamous among those who care about this topic for being excellently sealed, meaning that they have less interchange with outside air. This is good for energy efficiency, but bad for indoor air quality.\n\n## The Carbon Dioxide Debate\n\nMostly when people talk about air quality, they're talking about particulates and Volatile Organic Compounds (VOCs). However, some studies have tried to look at [carbon dioxide alone](https://www.lesswrong.com/posts/pPZ27eZdBXtGuLqZC/what-is-up-with-carbon-dioxide-and-cognition-an-offer), and have found large effects on cognition. It is this wiki author's belief that better studies have failed to find anything close to the size of the original effect, if anything.[^2][^4]\n\n## External Links\n\n* [Air Pollution](https://forum.effectivealtruism.org/tag/air-pollution) on the Effective Altruism Forum\n* [Collection of studies](https://patrickcollison.com/pollution) by Patrick Collison on air pollution and cognition\n\n[^1]: [Künn et. al](https://www.iza.org/publications/dp/12632/indoor-air-quality-and-cognitive-performance)\n[^2]: [Juginovic et. al](https://www.nature.com/articles/s41598-021-01802-5)\n[^3]: [Alexander Berger on the 80,000 Hours Podcast](https://80000hours.org/podcast/episodes/alexander-berger-improving-global-health-wellbeing-clear-direct-ways/#south-asian-air-quality-021112) (link goes to transcript or audio)\n[^4]: [Eight Hundred Slight Poisoned Word Games](https://www.lesswrong.com/posts/kxW6q5YdTGWh5sWby/eight-hundred-slightly-poisoned-word-games) by Scott Alexander"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "uDqJKemEHHsyWPm4m",
    "name": "Market making (AI safety technique) ",
    "core": false,
    "slug": "market-making-ai-safety-technique",
    "oldSlugs": null,
    "postCount": 3,
    "description": {
      "markdown": ""
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "L2cuwYAisTL3QF4TA",
    "name": "DALL-E",
    "core": false,
    "slug": "dall-e",
    "oldSlugs": null,
    "postCount": 18,
    "description": {
      "markdown": "**DALL-E** is a family of [machine learning](machine-learning) models created by [OpenAI](openai) that generate images from text descriptions.\n\n[DALL-E 2 official website](https://openai.com/dall-e-2/)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "bQZAkiFgtbEcr5h6f",
    "name": "AI Persuasion",
    "core": false,
    "slug": "ai-persuasion",
    "oldSlugs": [
      "ai-persuasion"
    ],
    "postCount": 7,
    "description": {
      "markdown": "AI which is highly capable of persuading people might have significant effects on humanity."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "vxsbTxQGYTeNDAZXb",
    "name": "Conjecture (org)",
    "core": false,
    "slug": "conjecture-org",
    "oldSlugs": null,
    "postCount": 4,
    "description": {
      "markdown": ""
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "fCA878kj7zunQHnp9",
    "name": "Flashcards",
    "core": false,
    "slug": "flashcards",
    "oldSlugs": null,
    "postCount": 1,
    "description": {
      "markdown": ""
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "emPKooKGELngMG7sP",
    "name": "Truthful AI",
    "core": false,
    "slug": "truthful-ai",
    "oldSlugs": null,
    "postCount": 1,
    "description": {
      "markdown": ""
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "Cx4ACNdBaGALjhCJP",
    "name": "PaLM",
    "core": false,
    "slug": "palm",
    "oldSlugs": null,
    "postCount": 5,
    "description": {
      "markdown": "**PaLM** is a Transformer language model created by Google in April 2022.  Google claims that the model exhibits discontinuous jumps in capabilities as it scales. The original paper announcing PaLM can be found here: [https://storage.googleapis.com/pathways-language-model/PaLM-paper.pdf](https://storage.googleapis.com/pathways-language-model/PaLM-paper.pdf)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "bt2e3HEcZmuHo3xf7",
    "name": "Modularity",
    "core": false,
    "slug": "modularity",
    "oldSlugs": null,
    "postCount": 11,
    "description": {
      "markdown": "**Modularity** is the extent to which a system can be divided into clusters. It is a very broad concept which can be applied across many different domains, for example:\n\nIn **graph theory**, modularity can be measured in terms of a graph's structure. For instance, there is a [common mathematical definition](https://en.wikipedia.org/wiki/Modularity_(networks)) for graphs, which is based on the idea of modules as partitions of the nodes of a graph, with more edges within modules than would be expected by chance.\n\nIn **evolutionary biology**, modularity has been observed at many different levels, from protein structure to organ systems. There is currently no universally accepted explanation for why modularity seems to have been produced by evolution, and less so by genetic algorithms, although there are widely-held theories (most notably the idea of modularly varying goals, as proposed by [Kashtan & Alon](https://www.pnas.org/doi/10.1073/pnas.0503610102)).\n\nModularity may be highly relevant for alignment research, because the more modular a system is, the greater the chances that we will be able to understand the cognition it is performing in terms of high-level abstractions."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "yvn2ZYwS9Qq4TnT9B",
    "name": "Biosecurity",
    "core": false,
    "slug": "biosecurity",
    "oldSlugs": null,
    "postCount": 3,
    "description": {
      "markdown": ""
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "aozxtmy7FEkkvfWzE",
    "name": "Grabby Aliens",
    "core": false,
    "slug": "grabby-aliens",
    "oldSlugs": null,
    "postCount": 12,
    "description": {
      "markdown": "**Grabby Aliens** is a theory that explains why humans appeared relatively early in the history of the universe (13.8 *billion* years after the Big Bang, while the average star will last over five *trillion* years). It was developed by Robin Hanson, Daniel Martin, Calvin McCarter, and Jonathan Paulson.\n\n**External Links:**\n\n*   [Website](https://grabbyaliens.com/)\n*   [Original Paper](https://arxiv.org/abs/2102.01522)\n*   [Video by Rational Animations (part 1)](https://youtu.be/l3whaviTqqg)\n*   [Video by Rational Animations (part 2)](https://youtu.be/LceY7nhi6j4)\n\n**Related Pages:**\n\n*   [Great Filter](https://www.lesswrong.com/tag/great-filter)\n*   [Anthropics](https://www.lesswrong.com/tag/anthropics)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "F6kBsuRJzi4AKJ7tL",
    "name": "Nuclear War",
    "core": false,
    "slug": "nuclear-war",
    "oldSlugs": null,
    "postCount": 12,
    "description": {
      "markdown": ""
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "fWEFt4e9asdpSseqf",
    "name": "Civilizational Collapse",
    "core": false,
    "slug": "civilizational-collapse",
    "oldSlugs": null,
    "postCount": 8,
    "description": {
      "markdown": "**Civilizational Collapse** is the process by which a large complex social organization breaks down and becomes incapable of performing its usual functions. It has occurred many times in history in specific regions, and some hypothesize that if it happened now it would dramatically affect almost all humans due to the interconnected nature of the global economy. Possible causes include severe natural or bioengineered pandemics, nuclear war and the following nuclear winter, or fragility and cascading systems failure."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "cv2dqCjh5cpkowzSp",
    "name": "Ukraine/Russia Conflict (2022)",
    "core": false,
    "slug": "ukraine-russia-conflict-2022",
    "oldSlugs": [
      "ukraine-russia-conflict"
    ],
    "postCount": 56,
    "description": {
      "markdown": "On the 24th of February 2022, Russia began a military invasion into Ukraine. \n\nThis is relevant to LessWrong both because it seems to be generally an important topic for understanding the world, and because of its potential implications for great-power conflict, nuclear war, and existential risk generally. \n\nSee Also:\n\n*   [War](https://www.lesswrong.com/tag/war)\n*   [Nuclear War](https://www.lesswrong.com/tag/nuclear-war)\n*   [Existential Risk](https://www.lesswrong.com/tag/existential-risk)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "6voWoNt3q3jpEcPzk",
    "name": "Transformers",
    "core": false,
    "slug": "transformers",
    "oldSlugs": null,
    "postCount": 2,
    "description": {
      "markdown": ""
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "m2DsR4r4HRSaLSPW3",
    "name": "Security Mindset",
    "core": false,
    "slug": "security-mindset",
    "oldSlugs": null,
    "postCount": 10,
    "description": {
      "markdown": "Security Mindset is a predisposition for thinking about the world in a security-oriented way. A large part of this way of thinking involves always being on the lookout for exploits. \n\n> Uncle Milton Industries has been selling ant farms to children since 1956. Some years ago, I remember opening one up with a friend. There were no actual ants included in the box. Instead, there was a card that you filled in with your address, and the company would mail you some ants. My friend expressed surprise that you could get ants sent to you in the mail.\n> \n> I replied: “What’s really interesting is that these people will send a tube of live ants to anyone you tell them to.”\n> \n> Security requires a particular mindset. Security professionals — at least the good ones — see the world differently. They can’t walk into a store without noticing how they might shoplift. They can’t use a computer without wondering about the security vulnerabilities. They can’t vote without trying to figure out how to vote twice. They just can’t help it.\n> \n> [SmartWater](http://www.smartwater.com/products/securitySolutions.html) is a liquid with a unique identifier linked to a particular owner. “The idea is for me to paint this stuff on my valuables as proof of ownership,” I [wrote](http://www.schneier.com/blog/archives/2005/02/smart_water.html) when I first learned about the idea. “I think a better idea would be for me to paint it on *your* valuables, and then call the police.”\n> \n> Really, we can’t help it.\n> \n> \\-\\- Bruce Schneier, [The security Mindset](https://www.schneier.com/blog/archives/2008/03/the_security_mi_1.html), Schneier on Security\n\n\\[I'm unsure of the origin of the term, but Schneier is at least an outspoken advocate. --Abram\\]\n\nIn 2017, Eliezer Yudkowsky wrote a pair of posts on the security mindset:\n\n*   [Security Mindset and Ordinary Paranoia](https://www.lesswrong.com/posts/8gqrbnW758qjHFTrH/security-mindset-and-ordinary-paranoia)\n*   [Security Mindset and the Logistic Success Curve](https://www.lesswrong.com/posts/cpdsMuAHSWhWnKdog/security-mindset-and-the-logistic-success-curve)\n\nAmongst other things, these posts forwarded the idea that true security mindset *is not just* the tendency to spot lots and lots of security flaws. Spotting security flaws is not in itself enough to build secure systems, because you could be spotting flaws with your design forever, patching specific weak points, and moving on to find yet more flaws. \n\nBuilding secure systems requires *coming up with strong positive arguments for the security of a system*. These positive arguments have several important features:\n\n1.  They have as few assumptions as possible, because each assumption is an additional chance to be wrong. \n2.  Each assumption is individually very certain.\n3.  The conclusion of the argument is a meaningful security guarantee. \n\nThe mindset required to build tight security arguments like this is different from the mindset required to find security holes."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "cFDQeyr8SpWuKpweq",
    "name": "Naturalism",
    "core": false,
    "slug": "naturalism",
    "oldSlugs": null,
    "postCount": 8,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "e7LAgAKaovFX2GQhg",
    "name": "Unbounded Utilities",
    "core": false,
    "slug": "unbounded-utilities",
    "oldSlugs": null,
    "postCount": 3,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "9mcR3NTzqwd3ugeRE",
    "name": "Astronomy",
    "core": false,
    "slug": "astronomy",
    "oldSlugs": null,
    "postCount": 4,
    "description": {
      "markdown": "See also [Physics](https://www.lesswrong.com/tag/physics), [Astrobiology](https://www.lesswrong.com/tag/astrobiology), [Great Filter](https://www.lesswrong.com/tag/great-filter)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "p2aBoEmJhjZYpkaPZ",
    "name": "Guild of the Rose",
    "core": false,
    "slug": "guild-of-the-rose",
    "oldSlugs": null,
    "postCount": 7,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "RyNWXFjKNcafRKvPh",
    "name": "Agent Foundations",
    "core": false,
    "slug": "agent-foundations",
    "oldSlugs": null,
    "postCount": 15,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "mSTmKrSkFBswHaS3T",
    "name": "Eliciting Latent Knowledge (ELK)",
    "core": false,
    "slug": "eliciting-latent-knowledge-elk",
    "oldSlugs": [
      "elicit-latent-knowledge",
      "eliciting-latent-knowledge"
    ],
    "postCount": 45,
    "description": {
      "markdown": "**Eliciting Latent Knowledge** is an open problem in [AI](ai) safety.\n\n> Suppose we train a model to predict what the future will look like according to cameras and other sensors. We then use planning algorithms to find a sequence of actions that lead to predicted futures that look good to us.\n> \n> But some action sequences could tamper with the cameras so they show happy humans regardless of what’s really happening. More generally, some futures look great on camera but are actually catastrophically bad.\n> \n> In these cases, the prediction model \"knows\" facts (like \"the camera was tampered with\") that are not visible on camera but would change our evaluation of the predicted future if we learned them. **How can we train this model to report its latent knowledge of off-screen events?**\n\n[--ARC report](https://docs.google.com/document/d/1WwsnJQstPq91_Yh-Ch2XRL8H_EpsnjrC1dwZXR37PC8/edit)\n\nSee also: [Transparency/Interpretability](https://www.lesswrong.com/tag/transparency-interpretability-ml-and-ai)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "aHay2tebonHAYKtac",
    "name": "Anthropic",
    "core": false,
    "slug": "anthropic",
    "oldSlugs": null,
    "postCount": 4,
    "description": {
      "markdown": "**Anthropic** is an AI organization.\n\nNot to be confused with [anthropics](anthropics)."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "xi9JLxEtEoPCgRXRj",
    "name": "Transformer Circuits",
    "core": false,
    "slug": "transformer-circuits",
    "oldSlugs": null,
    "postCount": 2,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "8Ec9rD286qNstoiGH",
    "name": "AXRP",
    "core": false,
    "slug": "axrp",
    "oldSlugs": null,
    "postCount": 20,
    "description": {
      "markdown": "The **AI X-risk Research Podcast** is a podcast hosted by Daniel Filan.\n\nWebsite: [axrp.net](https://axrp.net/)\n\nSee also: [Audio](audio), [Interviews](interviews)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "DGBnywFjJtbAn4q7A",
    "name": "LessWrong Books",
    "core": false,
    "slug": "lesswrong-books",
    "oldSlugs": null,
    "postCount": 4,
    "description": {
      "markdown": "**LessWrong Books**"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "AJDHQ4mFnsNbBzPhT",
    "name": "Gödelian Logic",
    "core": false,
    "slug": "goedelian-logic",
    "oldSlugs": null,
    "postCount": 20,
    "description": {
      "markdown": "**Gödelian Logic** refers to logic, math, and arguments in the style of [Kurt Gödel](https://en.wikipedia.org/wiki/Kurt_G%C3%B6del). Specifically - his two incompleteness theorems, and one completeness theorem. Due to their tricky and subtle nature, his incompleteness theorems are possibly the most misunderstood theorems of all time.\n\n> “All the limitative theorems of metamathematics and the theory of computation suggest that once the ability to represent your own structure has reached a certain critical point, that is the kiss of death: it guarantees that you can never represent yourself totally. [Gödel’s Incompleteness Theorem](https://en.wikipedia.org/wiki/Gödel's_incompleteness_theorems), Church’s Undecidability Theorem, [Turing’s Halting Theorem](http://en.wikipedia.org/wiki/Halting_problem), [Tarski’s Truth Theorem](http://en.wikipedia.org/wiki/Tarski%27s_undefinability_theorem) — all have the flavour of some ancient fairy tale which warns you that “To seek self-knowledge is to embark on a journey which … will always be incomplete, cannot be charted on any map, will never halt, cannot be described.” *\\- Douglas Hofstadter, Gödel, Escher, Bach*\n\n### Gödel's Completeness Theorem\n\nThis theorem is less well known than the other two, which came after it, but also less misunderstood. \n\n### Gödel's First Incompleteness Theorem\n\n### Gödel's Second Incompleteness Theorem\n\n### Probabilistic Solutions\n\nOne way you might think to get around Gödel's Incompleteness, is to leave behind logical certainty, and instead [assign probabilities to logical statements](https://www.lesswrong.com/posts/duAkuSqJhGDcfMaTA/reflection-in-probabilistic-logic). \n\n### External Resources:\n\n*   Quanta Magazine: [How Gödel's Proof Works](https://www.quantamagazine.org/how-godels-incompleteness-theorems-work-20200714)\n*   Stanford Encyclopedia of Philosophy:\n    *   [Gödel’s Incompleteness Theorems](https://plato.stanford.edu/entries/goedel-incompleteness)\n    *   [Kurt Gödel](https://plato.stanford.edu/entries/goedel) (including his [completeness theorem](https://plato.stanford.edu/entries/goedel/#ComThe))\n*   Wikipedia:\n    *   [Incompleteness Theorems](https://en.wikipedia.org/wiki/G%C3%B6del%27s_incompleteness_theorems)\n    *   [Completeness Theorem](https://en.wikipedia.org/wiki/G%C3%B6del%27s_completeness_theorem)\n    *   [First-order logic](https://en.wikipedia.org/wiki/First-order_logic) and [Second-order logic](https://en.wikipedia.org/wiki/Second-order_logic)\n    *   [Kurt Gödel](https://en.wikipedia.org/wiki/Kurt_G%C3%B6del)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "hyxRtKmvhtruSAp2C",
    "name": "Copenhagen Interpretation of Ethics",
    "core": false,
    "slug": "copenhagen-interpretation-of-ethics",
    "oldSlugs": null,
    "postCount": null,
    "description": {
      "markdown": "The **Copenhagen Interpretation of Ethics** says that when you interact with a problem in any way, you can be blamed for it.\n\nExternal Links\n--------------\n\n*   [The Copenhagen Interpretation of Ethics](https://blog.jaibot.com/the-copenhagen-interpretation-of-ethics/)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "Ximq2Bqv2B3TZ27eD",
    "name": "Missing Moods",
    "core": false,
    "slug": "missing-moods",
    "oldSlugs": null,
    "postCount": 4,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "wRdzfoH39vxqtNf37",
    "name": "The Signaling Trilemma",
    "core": false,
    "slug": "the-signaling-trilemma",
    "oldSlugs": null,
    "postCount": 7,
    "description": {
      "markdown": "The **belief signaling trilemma** (or *signaling trilemma* for simplicity) points out that (a) people assign reputation based on claims; (b) people want to maintain their reputation; therefore, (c) people warp their claims. This presents a trilemma:\n\n1.  We could agree to stop assigning reputation based on beliefs, but this would deprive us of an extremely valuable tool for evaluating others, besides being impossible to enforce.\n2.  We could agree to always report honest beliefs, but this could be very costly for cooperators and again impossible to enforce.\n3.  We could embrace dishonest reporting of beliefs, but this can severely warp the discourse.\n\n**Related tags/pages:** [Deception](https://www.lesswrong.com/tag/deception), [Honesty](https://www.lesswrong.com/tag/honesty), [Meta-Honesty](https://www.lesswrong.com/tag/meta-honesty), [Signaling](https://www.lesswrong.com/tag/signaling)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "j2coh2kwvyBZc3GAe",
    "name": "COVID-19-Booster",
    "core": false,
    "slug": "covid-19-booster",
    "oldSlugs": null,
    "postCount": 5,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "LkShkiJwNL2kBbWbM",
    "name": "EfficientZero",
    "core": false,
    "slug": "efficientzero",
    "oldSlugs": null,
    "postCount": 4,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "CbucWMrhobAmtzpMF",
    "name": "The Problem of the Criterion",
    "core": false,
    "slug": "the-problem-of-the-criterion",
    "oldSlugs": null,
    "postCount": 7,
    "description": {
      "markdown": "**The problem of the criterion** is an [open problem](https://www.lesswrong.com/tag/open-problems) in [epistemology](https://www.lesswrong.com/tag/epistemology) that creates [epistemic circularity](https://iep.utm.edu/ep-circ/) via the following loop:\n\n*   If you know something to be true, then it must satisfy some criterion of truth that you know\n*   If you know the criterion of truth to be true, then it must satisfy the criterion of truth\n\nThe implication of the problem of the criterion is that knowledge cannot be well grounded with more knowledge or truth, and instead must be grounded in something else. Common approaches are to ground knowledge in self-consistency, in ungrounded assumptions, or in purpose.\n\n**External Resources:**\n\n*   [The Problem of the Criterion (IEP)](https://iep.utm.edu/criterio/)\n*   [Problem of the criterion (Wikipedia)](https://en.wikipedia.org/wiki/Problem_of_the_criterion)\n\n**Related Pages:**\n\n*   [epistemology](https://www.lesswrong.com/tag/epistemology)\n*   [Truth, Semantics, & Meaning](https://www.lesswrong.com/tag/truth-semantics-and-meaning)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "jxMoPnsWXBnDzXBwE",
    "name": "Negotiation",
    "core": false,
    "slug": "negotiation",
    "oldSlugs": null,
    "postCount": 11,
    "description": {
      "markdown": "**Negotiation**"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "PgXFLqriw5Far9v7x",
    "name": "AI Success Models",
    "core": false,
    "slug": "ai-success-models",
    "oldSlugs": null,
    "postCount": 17,
    "description": {
      "markdown": "**AI Success Models** are proposed paths to an existential win via aligned AI. They are (so far) high level overviews and won't contain all the details, but present at least a sketch of what a full solution might look like. They can be contrasted with [threat models](https://www.lesswrong.com/tag/threat-models), which are stories about how AI might lead to major problems."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "QT87jxkk6DXuS8hGA",
    "name": "Explicit Reasoning",
    "core": false,
    "slug": "explicit-reasoning",
    "oldSlugs": null,
    "postCount": 9,
    "description": {
      "markdown": "In the simple case, **explicit reasoning** is reasoning which:\n\n*   Can be explained in language.\n*   Uses well-defined terms.\n*   Relies on well-understood reasoning steps.\n*   Has clear assumptions and conclusions.\n*   You are aware of doing when you do it.\n\n> \"What do I mean by ***explicit reason**?* I don’t refer merely to “[System 2](https://www.lesswrong.com/tag/dual-process-theory-system-1-and-system-2)”, the brain’s slow, sequential, analytical, fully conscious, and effortful mode of cognition. I refer to the *informed* application of this type of thinking. Gathering data with real effort to find out, crunching the numbers with a grasp of the math, modeling the world with testable predictions, reflection on your thinking with an awareness of biases. Reason requires good inputs and a lot of effort.\" - [Jacob Falkovich](https://www.lesswrong.com/users/jacob-falkovich), [The Treacherous Path to Rationality](https://www.lesswrong.com/posts/YcdArE79SDxwWAuyF/the-treacherous-path-to-rationality)\n\nHowever, the exact definition may vary based on context. For example, explicit reasoning might be operationalized as imaginary verbal reasoning taking place in a person's inner monologue (ie, in auditory working memory). In other cases, we might have a much higher standard, eg actual symbolic logic written on an external medium such as paper. So, reasoning can be more and less explicit, along several dimensions.\n\nExplicit reasoning is one of many modes of reasoning by which humans may reach conclusions. While it is not always the best mode of reasoning, it has the advantage of being *scrutable*, ie, open to inspection. This makes it easier to correct, in particular through imaginary verbal reasoning modelled after dialogue (ie, mentally responding to yourself as if you were another person, with critiques and corrections). Since it can easily be recorded, it can also be subject to feedback from many other people, which can further improve the quality of this type of reasoning. Also, explicit reasoning can easily be chained together to reach less obvious conclusions.\n\nOther types of reasoning include [inner sim](https://www.lesswrong.com/tag/inner-simulator-suprise-o-meter), gestalt pattern recognition, mental imagery, and [Gendlin's Focusing](https://www.lesswrong.com/tag/focusing). Important questions include when to trust different modes of reasoning, how to combine the results of different reasoning modes, and how to best facilitate communication between different modes of reasoning.\n\nRelated to: [Dual Process Theory](https://www.lesswrong.com/tag/dual-process-theory-system-1-and-system-2)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "sJYvMHBHnR6DbJQcN",
    "name": "Journaling",
    "core": false,
    "slug": "journaling",
    "oldSlugs": null,
    "postCount": 8,
    "description": {
      "markdown": "**Journaling** is the practice of recording events in your life, usually by writing about it daily.\n\nJournaling can help to remember things in your past - such as special events, belief updates, mood and emotional states - and track how they change across time. It can also help you notice and reflect on things, either naturally by thinking back on things that happened, or more directly helping you focus on things through prompting, like [gratitude](https://www.lesswrong.com/tag/gratitude) journaling which can help you focus on things you're grateful for.\n\nThere are various forms of jounraling:\n\n*   Daily / Non-daily / Irregularly: Most people jounral daily, but journaling can also be done on a different cycle, or irregularly.\n*   Prompts / Free-form / Mixed: Prompts can help you focus on specific things you want to journal about, but may also feel cumbersome for some people or at some times.\n*   Digital / Handwritten / Audio: Classically, of course, journaling was solely handwritten. Today it's also possible to write in a digital format. And it's also possible to keep an audio journal by recording yourself instead of writing.\n\n**Related Pages:** [Gratitude](https://www.lesswrong.com/tag/gratitude)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "tJv2Zbtx37mBGBJk6",
    "name": "Symbol Grounding",
    "core": false,
    "slug": "symbol-grounding",
    "oldSlugs": null,
    "postCount": 11,
    "description": {
      "markdown": "**Symbol Grounding**\n\n**Related Pages:** [Truth, Semantics, & Meaning](https://www.lesswrong.com/tag/truth-semantics-and-meaning), [Philosophy of Language](https://www.lesswrong.com/tag/philosophy-of-language)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "dHfxtPwAmrij4KEce",
    "name": "Redwood Research",
    "core": false,
    "slug": "redwood-research",
    "oldSlugs": null,
    "postCount": 8,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "BcnLB8PkrkqPhZ6XY",
    "name": "Bureaucracy",
    "core": false,
    "slug": "bureaucracy-1",
    "oldSlugs": null,
    "postCount": 12,
    "description": {
      "markdown": "**Bureaucracy**\n\n**Related Pages:** [Moral Mazes](https://www.lesswrong.com/tag/moral-mazes), [Politics](https://www.lesswrong.com/tag/politics), [Mechanism Design](https://www.lesswrong.com/tag/mechanism-design), [Stagnation](https://www.lesswrong.com/tag/stagnation)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "bkgy24HML6nLSLqcj",
    "name": "Stagnation",
    "core": false,
    "slug": "stagnation",
    "oldSlugs": null,
    "postCount": 19,
    "description": {
      "markdown": "Quoting Jason Crawford's *Technological Stagnation: Why I came around:*\n\n> Thiel, along with economists such as Tyler Cowen ([*The Great Stagnation*](https://www.amazon.com/dp/B004H0M8QS?tag=jasocraw-20)) and Robert Gordon ([*The Rise and Fall of American Growth*](https://rootsofprogress.org/summary-the-rise-and-fall-of-american-growth)), promotes a “stagnation hypothesis”: that there has been a significant slowdown in scientific, technological, and economic progress in recent decades—say, for a round number, since about 1970, or the last ~50 years.\n\nEliezer has posted [a short work of fiction on Reddit](https://www.reddit.com/r/WritingPrompts/comments/3xgqj6/wp_write_a_dystopian_vision_of_the_future_from/cy4zyfd/) which explores related ideas.\n\n**Related Pages:** [Progress Studies](https://www.lesswrong.com/tag/progress-studies), [Bureaucracy](https://www.lesswrong.com/tag/bureaucracy-1), [World Modeling](https://www.lesswrong.com/tag/world-modeling)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "YgizoZqa7LEb3LEJn",
    "name": "AMA",
    "core": false,
    "slug": "ama",
    "oldSlugs": null,
    "postCount": 22,
    "description": {
      "markdown": "An **Ask Me Anything (AMA)** post is an invitation to ask the author questions. \n\nSee also: [Q&A (format)](https://www.lesswrong.com/tag/q-and-a-format), [Interviews](https://www.lesswrong.com/tag/interviews)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "mxSBcaTrakvCkgLzL",
    "name": "Mind Crime",
    "core": false,
    "slug": "mind-crime",
    "oldSlugs": null,
    "postCount": 7,
    "description": {
      "markdown": "**Mind Crime** occurs when a computational process which has moral value is mistreated. For example, an advanced AI trying to predict human behavior might create simulations of humans so detailed as to be conscious observers, which would then suffer through whatever hypothetical scenarios the AI wanted to test and then be discarded.\n\nMind crime on a large scale constitutes a [risk of astronomical suffering](https://www.lesswrong.com/tag/risks-of-astronomical-suffering-s-risks).\n\nMind crime is different from other AI risks in that the AI need not even affect anything outside its box for the catastrophe to occur.\n\nThe term was coined by Nick Bostrom in *Superintelligence: Paths, Dangers, Strategies.*\n\nNot the same as [thoughtcrime](https://en.wikipedia.org/wiki/Thoughtcrime), a term for having beliefs considered unacceptable by society."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "EmaCLRKb4baBFq4ra",
    "name": "dath ilan",
    "core": false,
    "slug": "dath-ilan",
    "oldSlugs": null,
    "postCount": 23,
    "description": {
      "markdown": "**Dath ilan** is a fictional world and civilization invented by Eliezer Yudkowsky. It is a parallel reality of earth where society is much better at coordination and various good policies have been implemented, though technology is only as advanced as earth's, if not slightly less. It can be thought as a more practical and realistic form of [Economist's Paradise](https://slatestarcodex.com/2014/08/24/the-invisible-nation-reconciling-utilitarianism-and-contractualism/). Eliezer first introduced it in his [April Fool's](https://www.lesswrong.com/tag/april-fool-s) day post '[My April Fools Day Confession](https://yudkowsky.tumblr.com/post/81447230971/my-april-fools-day-confession)', where he claimed that he was an average person from that world and none of his ideas were original.\n\nThis world was further fleshed out (and some its backstory changed) in a later April Fool's post:\n\n*   [I'm from a parallel Earth with much higher coordination: AMA](https://www.lesswrong.com/posts/gvA4j8pGYG4xtaTkw/i-m-from-a-parallel-earth-with-much-higher-coordination-ama)\n\nAnd in a series of glowfics featuring dath ilani characters:\n\n*   [a dath ilani matchmaker in King Randale's court](https://www.glowfic.com/posts/4503) (incomplete)\n*   [but hurting people is wrong](https://www.glowfic.com/posts/4508) (complete)\n*   [mad investor chaos and the woman of asmodeus](https://www.glowfic.com/posts/4582) (ongoing as of Apr 2022)\n*   [a dath ilani EMT in queen abrogail's court](https://www.glowfic.com/posts/5633) (ongoing as of Apr 2022)\n\nSome known features of dath ilan:\n\n*   Standard education includes rationality training (from which, allegedly, all of Eliezer's own ideas are plagiarized).\n*   What this Earth knows as \"[logical decision theory](https://arbital.com/p/logical_dt/)\" / \"Functional Decision Theory\" is just standard decision theory as formulated in dath ilan.\n*   [Land Value Tax](https://en.wikipedia.org/wiki/Land_value_tax), [positional-goods](https://en.wikipedia.org/wiki/Positional_good) tax, status-goods tax, marketing-tax, no income tax.\n*   [Movable Homes](https://www.lesswrong.com/posts/cmiRk9XtT9Psnd3Yr/movable-housing-for-scalable-cities).\n*   Autonomous electric cars in tunnels instead of [ICE](https://en.wikipedia.org/wiki/Internal_combustion_engine) cars on roads.\n*   No streetlights at night, except for red lights along walkways, which for 45 minutes each night also turn off to see the sky. And on winter solstice (night of stars) the lights stay off the whole night, to give a perfect view of the night sky.\n*   The average IQ in Earth-equivalent terms is 143, after an unknown number of generations of \"heritage optimization\" pursued via positive government subsidies.\n*   Dath ilan has a lower population than Earth (1 billion people), since their higher intelligence and higher coordination enabled them to make tech progress without having first attained a higher population than that.\n*   There exists a single global civilization, calling itself \"Civilization\", with one world government, called \"Governance\".\n*   All humans (and other hominids, like chimpanzees) are cryopreserved upon death.\n*   If you commit a murder but don't want to consign your victim to true death, you can call in a government service, the [Surreptitious Head Removers](https://www.facebook.com/yudkowsky/posts/10159752926899228), who will remove your victim's head for cryonic preservation, while being sworn not to report on this.\n*   Dath ilani holidays are known to include the annual Alien Invasion Rehearsal Festival and the annual Oops It's Time To Overthrow the Government Festival.\n*   There exists a group called Keepers who undergo much more rigorous rationality training and act as a counterbalance to Governance; the dath ilani equivalent of Leo Szilard eventually told Governance about her idea for nuclear weapons, but she talked to the Keepers first.\n*   In the fictional setting (though this concept didn't appear in the original April Fools post) dath ilan has found it necessary to try to causally screen off its entire history from its present - all old books hidden away, all old cities mothballed.  As the Doylistic result, dath ilani characters ending up in settings with much lower social technology levels, such as Valdemar, Golarion, or Earth, start with little prior concept of how a less-coordinated society looks, and must learn from experience.\n\n**Not tagged due to spoilers:**\n\n[Lies Told To Children](https://www.lesswrong.com/posts/uyBeAN5jPEATMqKkX/lies-told-to-children-1)\n\n**External Posts:**\n\n*   [Until we Build dath ilan](https://hivewired.wordpress.com/2017/06/04/until-we-build-dath-ilan/)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "TDcpBdA8h7xsoWZWe",
    "name": "Lightcone Infrastructure",
    "core": false,
    "slug": "lightcone-infrastructure",
    "oldSlugs": null,
    "postCount": 1,
    "description": {
      "markdown": "[**Lightcone Infrastructure**](https://www.lightconeinfrastructure.com/)  is the parent organization of LessWrong. Its mission is to build infrastructure for people who are helping the [long term](https://www.lesswrong.com/tag/longtermism) future of humanity. It was [announced](https://www.lesswrong.com/posts/eR7Su77N2nK3e5YRZ/) on October 1st, 2021.\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/19f81bb3a671af655ea5d7ac08f1916c211ec158b55f78c7.png)\n\n**Website:** [lightconeinfrastructure.com](https://www.lightconeinfrastructure.com/)\n\nSee also: [LW Moderation](lw-moderation), [Site Meta](site-meta)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "sYbszETv5rKst6gxD",
    "name": "Leverage Research",
    "core": false,
    "slug": "leverage-research",
    "oldSlugs": [
      "leverage-research"
    ],
    "postCount": 12,
    "description": {
      "markdown": "*   Official Website: [http://leverageresearch.org/](http://leverageresearch.org/)\n*   Leverage Research quarterly newsletter: [https://www.leverageresearch.org/updates](https://www.leverageresearch.org/updates)\n*   Leverage Research 2019-2020 Annual Report: [https://www.leverageresearch.org/annual-report-2019-2020](https://www.leverageresearch.org/annual-report-2019-2020)\n*   Leverage Research Twitter: [https://twitter.com/leverageres](https://twitter.com/leverageres)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "bbdbpGWWPMfKBzk7z",
    "name": "Gradient Hacking",
    "core": false,
    "slug": "gradient-hacking",
    "oldSlugs": null,
    "postCount": 16,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "FJ3MGb684F88BoN2o",
    "name": "Formal Proof",
    "core": false,
    "slug": "formal-proof",
    "oldSlugs": null,
    "postCount": 31,
    "description": {
      "markdown": "A **Formal Proof** is a finite sequence of steps from [axiom(s)](https://en.wikipedia.org/wiki/Axiom) or previous derived proof(s) which strictly follow the allowed rules of [inference](https://en.wikipedia.org/wiki/Inference) of the [mathematical system](https://www.lesswrong.com/tag/logic-and-mathematics) in which it exists. They are used to establish statements as true within a mathematical framework in a way which can be independently verified with extremely high certainty, with the most reliable flavor of proof being machine-checked proofs generated by [proof assistants](https://en.wikipedia.org/wiki/Proof_assistant) since they have even less room for human error."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "KmgkrftQuX7jmjjp5",
    "name": "Language Models",
    "core": false,
    "slug": "language-models",
    "oldSlugs": null,
    "postCount": 99,
    "description": {
      "markdown": "**Language Models** are a class of [AI](https://www.lesswrong.com/tag/ai) trained on text, usually to predict the next word or a word which has been obscured. They have the ability to generate novel prose or code based on an initial prompt, which gives rise to a kind of natural language programming called prompt engineering. The most popular architecture for very large language models is called a [transformer](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)), which follows consistent [scaling laws](https://www.lesswrong.com/tag/scaling-laws) with respect to the size of the model being trained, meaning that a larger model trained with the same amount of compute will produce results which are better by a predictable amount (when measured by the 'perplexity', or how surprised the AI is by a test set of human-generated text).\n\n### See also\n\n*   [GPT](https://www.lesswrong.com/tag/gpt) \\- A family of large language models created by [OpenAI](https://www.lesswrong.com/tag/openai)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "jygEZ8peqh6QRZYry",
    "name": "Autonomous Weapons",
    "core": false,
    "slug": "autonomous-weapons",
    "oldSlugs": null,
    "postCount": 6,
    "description": {
      "markdown": "**Autonomous Weapons** are military systems designed to operate without a human in the loop. Some have argued that they may present a destabilizing influence on geopolitics, or even a potential existential risk.\n\n### See also\n\n*   [Wikipedia page on Lethal Autonomous Weapons](https://en.wikipedia.org/wiki/Lethal_autonomous_weapon)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "NGtNzdS88JtEQdRP4",
    "name": "Extraterrestrial Life",
    "core": false,
    "slug": "extraterrestrial-life",
    "oldSlugs": null,
    "postCount": 21,
    "description": {
      "markdown": "**Extraterrestrial Life**\n\nRelated Pages: [Great Filter](https://www.lesswrong.com/tag/great-filter), [Space Exploration & Colonization](https://www.lesswrong.com/tag/space-exploration-and-colonization), [Biology](https://www.lesswrong.com/tag/biology), [Evolution](https://www.lesswrong.com/tag/evolution)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "Myz8DA9AghgpNei9i",
    "name": "Gradient Descent",
    "core": false,
    "slug": "gradient-descent",
    "oldSlugs": null,
    "postCount": 4,
    "description": {
      "markdown": "stub"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "bHGixy9hHdmENhoe6",
    "name": "External Events",
    "core": false,
    "slug": "external-events",
    "oldSlugs": null,
    "postCount": 6,
    "description": {
      "markdown": "This is for events of interested to people not posted through the normal LessWrong event system, e.g., conferences."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "wFDrB47FAhkLgp4bJ",
    "name": "Superrationality",
    "core": false,
    "slug": "superrationality",
    "oldSlugs": null,
    "postCount": 7,
    "description": {
      "markdown": "**Superrationality** is a concept invented by Douglas Hofstadter. He thought that agents should cooperate in Prisoner's Dilemma, but the primary notion of \"rationality\" which had been deeply developed by economists, decision-theorists, and game-theorists disagreed. Rather than fighting over the definition of rational, Douglas Hofstadter coined the term superrational for the kind of rationality he was interested in.\n\nDouglas Hofstadter did not publish a full theory of superrationality, instead focusing on the prisoner's dilemma and other specific examples to communicate his intuition. However, some principles can be clearly inferred from his writing, such as reasoning as if you have control over the other player's policy (in symmetric games).\n\nEliezer Yudkowsky shared the same core intuition with Douglas Hofstadter, but took the path of trying to reclaim the word *rational* for what he meant. As a result, LessWrong does not consistently use superrational/superrationality."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "hEKzJqK2NWcoZmcwz",
    "name": "Welcome Threads",
    "core": false,
    "slug": "welcome-threads",
    "oldSlugs": [
      "open-and-welcome-threads"
    ],
    "postCount": 5,
    "description": {
      "markdown": "If you are new to LessWrong, the current iteration of this, is the place to introduce yourself.\n\nPersonal stories, anecdotes, or just general comments on how you found us and what you hope to get from the site and community are invited.\n\nThis is also the place to discuss feature requests and other ideas you have for the site, if you don't want to write a full top-level post."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "vH8zJLkhiqdJzK5ej",
    "name": "Bragging Threads",
    "core": false,
    "slug": "bragging-threads",
    "oldSlugs": null,
    "postCount": 3,
    "description": {
      "markdown": "Your job, should you choose to accept it, is to comment on this thread explaining **the most awesome thing you've done this month**. You may be as blatantly proud of yourself as you feel. You may unabashedly consider yourself *the coolest freaking person ever* because of that awesome thing you're dying to tell everyone about. This is the place to do just that.\n\nRemember, however, that this **isn't** any kind of progress thread. Nor is it any kind of proposal thread. *This thread is solely for people to talk about the awesome things they have done. Not \"will do\". Not \"are working on\"*. **Have already done.** This is to cultivate an environment of object level productivity rather than meta-productivity methods.\n\nSo, what's the coolest thing you've done this month?"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "ocGoDbHKBv46AwXnT",
    "name": "Narrow AI",
    "core": false,
    "slug": "narrow-ai",
    "oldSlugs": null,
    "postCount": 11,
    "description": {
      "markdown": "A **Narrow AI** is capable of operating only in a relatively limited domain, such as chess or driving, rather than capable of learning a broad range of tasks like a human or an [Artificial General Intelligence](https://www.lesswrong.com/tag/artificial-general-intelligence). Narrow vs General is not a perfectly binary classification, there are degrees of generality with, for example, large language models having a fairly large degree of generality (as the domain of text is large) without being as general as a human, and we may eventually build systems that are significantly more general than humans."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "iKYWGuFx2qH2nYu6J",
    "name": "AI Capabilities",
    "core": false,
    "slug": "ai-capabilities",
    "oldSlugs": null,
    "postCount": 41,
    "description": {
      "markdown": "**AI Capabilities** are the growing abilities of AIs to act effectively in increasingly complex environments. It is often compared to to AI Alignment, which refers to efforts to ensure that these effective actions taken by AIs are also intended by the creators and beneficial to humanity."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "XtnxF4TsAEyWBRJZq",
    "name": "Scoring Rules",
    "core": false,
    "slug": "scoring-rules",
    "oldSlugs": null,
    "postCount": 5,
    "description": {
      "markdown": "**Scoring Rules** are ways to score answers on a test, a prediction, or any other performance.\n\nOf special interest are [proper scoring rules](https://en.wikipedia.org/wiki/Scoring_rule#Proper_scoring_rules) \\- rules such that the strategy for maximizing the expected score coincides with noting down your true beliefs about the question.\n\n**Related Pages:** [Calibration](https://www.lesswrong.com/tag/calibration), [Forecasting & Prediction](https://www.lesswrong.com/tag/forecasting-and-prediction), [Skill / Expertise Assessment](https://www.lesswrong.com/tag/skill-expertise-assessment), [Prediction Markets](https://www.lesswrong.com/tag/prediction-markets)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5A5ZGTQovxbay6fpr",
    "name": "Fairness",
    "core": false,
    "slug": "fairness",
    "oldSlugs": null,
    "postCount": 21,
    "description": {
      "markdown": "**Fairness** is a property of a system or process where no part is privileged, along some measure. A common problem is how to divide a cake between 3 people. Should every one get a third? what if one of them is hungrier? What if they don't agree about what is fair? This is an example of [fair division](https://en.wikipedia.org/wiki/Fair_division). \n\nTalking about fairness is difficult, as it runs into many [problems with language](Philosophy of Language) (as surely the first sentence in this page does). What does privilege mean? what does it mean for someone to be [entitled](https://en.wikipedia.org/wiki/Entitlement_(fair_division)) to something?\n\nIt also runs it problems of value. How do we measure value? is value objective or subjective? How do we know how much value something is worth to someone? Should an arbiter decide or can people decide between themselves?\n\nObjective value is considered impossible, so objective fairness is considered impossible too. Which means an arbiter can't be used, and participants have to find a arrangement which is subjectively fair to them.\n\nWhich opens up problems of [truthfulness](https://www.lesswrong.com/tag/honesty), strategy and verification.\n\nSeveral posts on LessWrong deal with this topic, and it's also a field of research called Fair Division.\n\n**Related pages:**\n\n*   [Game Theory](https://www.lesswrong.com/tag/game-theory)\n*   [Voting Theory](https://www.lesswrong.com/tag/voting-theory)\n*   [Economics](https://www.lesswrong.com/tag/economics)\n\n**External links:**\n\n*   [Wikipedia: Fairness](https://en.wikipedia.org/wiki/Fairness) \n*   [Wikipedia: Fair division](https://en.wikipedia.org/wiki/Fair_division) \n*   [Wikipedia: List of unsolved problems in fair division](https://en.wikipedia.org/wiki/List_of_unsolved_problems_in_fair_division)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "npoWi8AYpegPcSYcw",
    "name": "D&D.Sci",
    "core": false,
    "slug": "d-and-d-sci",
    "oldSlugs": null,
    "postCount": 46,
    "description": {
      "markdown": "**Dungeons and Data Science**, or **D&D.Sci,** is a series of analytical exercises played on Less Wrong, framed as problems in a D&D style adventuring world.\n\nGenerally D&D.Sci exercises will give the player a set of data but not the rules used to generate it, and the player must extrapolate patterns from the data in order to optimize the solution to some problem. After each exercise there is usually a follow-up post which explains the underlying rules of the scenario and gives a score to each player who participated at the time."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "DHZhAxgE5edGxxh8d",
    "name": "Object level and Meta level",
    "core": false,
    "slug": "object-level-and-meta-level",
    "oldSlugs": null,
    "postCount": 4,
    "description": {
      "markdown": "**Object level** vs **Meta level** is a distinction between levels of [abstraction](https://www.lesswrong.com/tag/abstraction), with the object level being directly about the specifics of the issue at hand, while meta level is concerned with the general principles which apply to a wider range of examples. It is often useful to move up and down the ladder of abstraction to get points across clearly, with the object level providing firm grounding and concrete examples for a person to grasp, while the meta layer allows them to generalize a new concept to a broad domain."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "qNF7Ti87CLfHhbttj",
    "name": "Treacherous Turn",
    "core": false,
    "slug": "treacherous-turn",
    "oldSlugs": null,
    "postCount": 10,
    "description": {
      "markdown": "A **Treacherous Turn** is a hypothetical event where an advanced [AI](ai) system which has been pretending to be aligned due to its relative weakness turns on humanity once it achieves sufficient power that it can pursue its true objective without risk."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "bXSkC7FFgApRnSajw",
    "name": "General Semantics",
    "core": false,
    "slug": "general-semantics",
    "oldSlugs": null,
    "postCount": 10,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "DtsGBCwAbrckp6BRv",
    "name": "AF Non Member Popup First",
    "core": false,
    "slug": "af-non-member-popup-first",
    "oldSlugs": null,
    "postCount": null,
    "description": {
      "markdown": "**Note! This is how submissions work for non-members of the AI Alignment Forum**\n\n*If you are seeing this, you are not a full-member.*\n\n*   If you post or comment, your submission will enter a review queue and a decision to accept or reject it from the Alignment Forum will be made within three days. If it is rejected, you will receive a minimum one-sentence explanation.\n*   In the meantime (and regardless of outcome), your post or comment will be published to [LessWrong](https://www.lesswrong.com/). There it can be immediately viewed and discussed by everyone, and edited by you.\n\nFor more detail, see our [FAQ](https://alignmentforum.org/faq) where we answer questions like [*How can non-members participate in the Forum?*](https://www.alignmentforum.org/posts/Yp2vYb4zHXEeoTkJc/welcome-and-faq#How_can_non_members_participate_in_the_Forum_) and [*How do I join the Alignment Forum?*](https://www.alignmentforum.org/posts/Yp2vYb4zHXEeoTkJc/welcome-and-faq#How_do_I_join_the_Alignment_Forum_) Also feel free to shoot us a message via Intercom (bottom right of the page), or email us at team@lesswrong.com"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "Eha62RrqBtEbpcEza",
    "name": "Repository",
    "core": false,
    "slug": "repository-1",
    "oldSlugs": null,
    "postCount": 16,
    "description": {
      "markdown": "**Repositories** are pages that are meant to collect information and advice of a specific type or area from the LW community. \n\nThey are similar to but distinct from [List of Links](https://www.lesswrong.com/tag/list-of-links) pages, which focus on collecting existing resources (either LW articles or external) rather than collecting advice & data directly from participants.\n\n*See also:*"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "KqfqD7YSMeFTLJCcs",
    "name": "Tripwire",
    "core": false,
    "slug": "tripwire",
    "oldSlugs": null,
    "postCount": 6,
    "description": {
      "markdown": "In AI safety, a **tripwire** is a mechanism designed to detect signs of misalignment in an advanced artificial intelligence and shut it down automatically."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "7YH2RGRdysFwaAPav",
    "name": "Cancer",
    "core": false,
    "slug": "cancer-1",
    "oldSlugs": [
      "cancer-1"
    ],
    "postCount": 1,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "FnnEaFGmG3FScvzj7",
    "name": "Transposons",
    "core": false,
    "slug": "transposons",
    "oldSlugs": null,
    "postCount": 3,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "zFrdtg6wCuzWu6mhy",
    "name": "Finite Factored Sets",
    "core": false,
    "slug": "finite-factored-sets",
    "oldSlugs": null,
    "postCount": 18,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "P2jmr8zbGbGnoMG8a",
    "name": "Dementia",
    "core": false,
    "slug": "dementia",
    "oldSlugs": null,
    "postCount": 2,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "WiMZ8obYsfJPog3Ph",
    "name": "Apprenticeship",
    "core": false,
    "slug": "apprenticeship",
    "oldSlugs": null,
    "postCount": 12,
    "description": {
      "markdown": "**Apprenticeship**"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "6DDtyKtotNehTjmRn",
    "name": "Coherence Arguments",
    "core": false,
    "slug": "coherence-arguments",
    "oldSlugs": null,
    "postCount": 12,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "X2rXNCtSnTtTLBYpq",
    "name": "Ivermectin (drug)",
    "core": false,
    "slug": "ivermectin-drug",
    "oldSlugs": [
      "ivermectin"
    ],
    "postCount": 7,
    "description": {
      "markdown": "**Ivermectin** is an anti-parasitic drug that's been investigated as a treatment for Covid-19.\n\n**External Articles:**\n\n*   [Ivermectin: Much More Than You Wanted To Know](https://astralcodexten.substack.com/p/ivermectin-much-more-than-you-wanted) by Scott Alexander"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "qDvaStt6c3KuqLr2P",
    "name": "AI Safety Camp",
    "core": false,
    "slug": "ai-safety-camp",
    "oldSlugs": null,
    "postCount": 23,
    "description": {
      "markdown": "**AI Safety Camp** (**AISC**) is a non-profit initiative to run programs for diversely skilled researchers who want to try collaborate on an open problem for reducing AI [existential risk](https://www.lesswrong.com/tag/existential-risk).\n\n[Official Website](https://aisafety.camp/)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "Y499WMAzNbrBM7Ytt",
    "name": "Veganism",
    "core": false,
    "slug": "veganism",
    "oldSlugs": null,
    "postCount": 2,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "AKvmQFvDjxJNetTuk",
    "name": "Scaling Laws",
    "core": false,
    "slug": "scaling-laws",
    "oldSlugs": null,
    "postCount": 29,
    "description": {
      "markdown": "**Scaling Laws** refer to the observed trend of some machine learning architectures (notably [transformers](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model))) to scale their performance on predictable power law when given more compute, data, or parameters (model size), assuming they are not bottlenecked on one of the other resources. This has been observed as highly consistent over more than six orders of magnitude.\n\n![](https://i.imgur.com/7lhHT8n.png)\n\nScaling laws graph from [Scaling Laws for Neural Language Models](https://arxiv.org/pdf/2001.08361.pdf)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "Qyeqh8wycbSapBNsp",
    "name": "Pomodoro Technique",
    "core": false,
    "slug": "pomodoro-technique",
    "oldSlugs": null,
    "postCount": 9,
    "description": {
      "markdown": "The **Pomodoro Technique** is a [productivity](lesswrong.com/tag/productivity) technique where you alternate between 25 minutes of work and 5 minutes of break time.\n\nIt gets its name from a kitchen timer shaped like a tomato (*pomodoro* in Italian).\n\nThe basic intuition for the pomodoro technique is that:\n\n1.  People concentrate most effectively shortly after a break\n2.  Most people are not innately good at noticing when they are not concentrating effectively\n\nBy setting an actual, physical timer (for  work time as well as for  breaks), people are more likely to work effectively. Traditionally, this is set for 25 minutes of work and 5 minutes of time, though for some tasks other lengths may be appropriate. [Experimentation](https://www.lesswrong.com/tag/self-experimentation) is encouraged.\n\nThe 'full' pomodoro technique consists of 6 steps (from [wikipedia](https://en.wikipedia.org/wiki/Pomodoro_Technique)):\n\n> 1.  Decide on the task to be done.\n> 2.  Set the pomodoro timer (traditionally to 25 minutes).[^\\[1\\]^](https://en.wikipedia.org/wiki/Pomodoro_Technique#cite_note-Cirillo-1)\n> 3.  Work on the task.\n> 4.  End work when the timer rings and put a checkmark on a piece of paper.[^\\[5\\]^](https://en.wikipedia.org/wiki/Pomodoro_Technique#cite_note-CirilloHow-5)\n> 5.  *If you have fewer than four checkmarks,* take a short break (3–5 minutes) and then return to step 2; *otherwise* continue to step 6.\n> 6.  *After four pomodoros,* take a longer break (15–30 minutes), reset your checkmark count to zero, then go to step 1.\n\nSome who use the technique also encourage focussing on a single task for each pomodoro, or reviewing the work of the previous pomodoro for the first few minutes of each (a type of [spaced repetition](https://www.lesswrong.com/tag/spaced-repetition)).\n\nSeveral tools are available to manage pomodoro timers. [Pomofocus](https://pomofocus.io/) is a good, simple, online option. [Flow](https://flowapp.info/) (Mac/iPhone/iPad) and [Pomodoro Flow](https://www.microsoft.com/en-gb/p/pomodoro-flow/9p4btjxsv5nl?activetab=pivot:overviewtab) (windows) are some examples of downloadable ones."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "LJxkaxERSGYKBBJp2",
    "name": "Landmark Forum",
    "core": false,
    "slug": "landmark-forum",
    "oldSlugs": null,
    "postCount": 2,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "NzpKMYopbkgEPN486",
    "name": "Homunculus Fallacy",
    "core": false,
    "slug": "homunculus-fallacy",
    "oldSlugs": null,
    "postCount": 3,
    "description": {
      "markdown": "The **homunculus fallacy** is a mistake in reasoning in which one attempts to explain agency, consciousness, or related phenomena *by appealing to a module which solves that very problem*.\n\nThe classic example is an \"explanation\" of visual processing in which an image is recorded by the retina, conveyed by the optic nerve, and then transmitted by the visual cortex to the rest of the brain, which \"sees\" everything. While it is true that images are recorded by the retina and conveyed to the rest of the brain by the optic nerve, this does not explain how the brain processes visual data. Instead, one imagines something like a little person or a floating consciousness (the \"homunculus\") who does the real work.\n\nThis position is typically refuted by arguing that this requires an infinite recursion: if we then ask how the homunculus \"sees\", we would start with how its \"eyes\" take in data, and feed them into its own little visual cortex, where we are faced with the same problem again.\n\nThis is a [mysterious answer](https://www.lesswrong.com/s/5uZQHpecjn7955faL/p/6i3zToomS86oj9bS6), but not a [semantic stopsign](https://www.lesswrong.com/s/5uZQHpecjn7955faL/p/FWMfQKG3RpZx6irjm). If someone answers questions of the form \"why do humans do X\" with \"free will\", and answers \"why does free will do X\" with \"free will is free; you don't get to ask that!\", this is a semantic stopsign type mysterious-answer. Like vitalism, this person is *explicitly* trying to make an explanation mysterious. The homunculus fallacy is more like phlogiston: an attempt at a reductionistic answer which fails. From [Mysterious Answers to Mysterious Questions](https://www.lesswrong.com/s/5uZQHpecjn7955faL/p/6i3zToomS86oj9bS6):\n\n> Vitalism shared with phlogiston the error of *encapsulating the mystery as a substance.* Fire was mysterious, and the phlogiston theory encapsulated the mystery in a mysterious substance called “phlogiston.” Life was a sacred mystery, and vitalism encapsulated the sacred mystery in a mysterious substance called “Élan vital.” Neither answer helped concentrate the model’s probability density—helped make some outcomes easier to explain than others. The “explanation” just wrapped up the question as a small, hard, opaque black ball.\n> \n> In a comedy written by Molière, a physician explains the power of a soporific by saying that it contains a “dormitive potency.” Same principle. It is a failure of human psychology that, faced with a mysterious phenomenon, we more readily postulate mysterious inherent substances than complex underlying processes.\n> \n> But the deeper failure is supposing that an *answer* can be mysterious. If a phenomenon feels mysterious, that is a fact about our state of knowledge, not a fact about the phenomenon itself. The vitalists saw a mysterious gap in their knowledge, and postulated a mysterious stuff that plugged the gap. In doing so, they mixed up the map with the territory. All confusion and bewilderment exist in the mind, not in encapsulated substances."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "KRcsSxBTLGRrSbHsW",
    "name": "Transformative AI",
    "core": false,
    "slug": "transformative-ai",
    "oldSlugs": null,
    "postCount": 12,
    "description": {
      "markdown": "**Transformative AI** is \"\\[...\\] AI that precipitates a transition comparable to (or more significant than) the agricultural or industrial revolution.\"^[\\[1\\]](#fnhy8b4kflu8)^ The concept refers to the large effects of AI systems on our well-being, the global economy, state power, international security, etc. and not to specific capabilities that AI might have (unlike the related terms [Superintelligent AI](https://www.lesswrong.com/tag/superintelligence) and [Artificial General Intelligence](https://www.lesswrong.com/tag/artificial-general-intelligence)).\n\nHolden Karnofsky gives a more detailed definition in [another OpenPhil 2016 post](https://www.openphilanthropy.org/research/some-background-on-our-views-regarding-advanced-artificial-intelligence/):\n\n> \\[...\\] Transformative AI is anything that fits one or more of the following descriptions (emphasis original):\n> \n> *   AI systems capable of fulfilling all the necessary functions of human scientists, unaided by humans, in developing another technology (or set of technologies) that ultimately becomes widely credited with being the most significant driver of a transition comparable to (or more significant than) the agricultural or industrial revolution. Note that just because AI systems *could* accomplish such a thing unaided by humans doesn’t mean they *would*; it’s possible that human scientists would provide an important complement to such systems, and could make even faster progress working in tandem than such systems could achieve unaided. I emphasize the hypothetical possibility of AI systems conducting substantial unaided research to draw a clear distinction from the types of AI systems that exist today. I believe that AI systems capable of such broad contributions to the relevant research would likely dramatically accelerate it.\n> *   AI systems capable of performing tasks that currently (in 2016) account for the majority of full-time jobs worldwide, and/or over 50% of total world wages, unaided and for costs in the same range as what it would cost to employ humans. Aside from the fact that this would likely be sufficient for a major economic transformation relative to today, I also think that an AI with such broad abilities would likely be able to far surpass human abilities in a subset of domains, making it likely to meet one or more of the other criteria laid out here.\n> *   Surveillance, autonomous weapons, or other AI-centric technology that becomes sufficiently advanced to be the most significant driver of a transition comparable to (or more significant than) the agricultural or industrial revolution. (This contrasts with the first point because it refers to transformative technology that is itself AI-centric, whereas the first point refers to AI used to speed research on some other transformative technology.)\n\n1.  ^**[^](#fnrefhy8b4kflu8)**^\n    \n    As defined by [Open Philanthropy's Holden Karnofsky in 2016](https://www.openphilanthropy.org/research/potential-risks-from-advanced-artificial-intelligence-the-philanthropic-opportunity/), and reused by [the Center for the Governance of AI in 2018](https://www.fhi.ox.ac.uk/wp-content/uploads/GovAI-Agenda.pdf)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "syeEaqfH9buYEbanF",
    "name": "Derisking",
    "core": false,
    "slug": "derisking",
    "oldSlugs": null,
    "postCount": 4,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "EFoa78oNx2jskHfYt",
    "name": "Lottery Ticket Hypothesis",
    "core": false,
    "slug": "lottery-ticket-hypothesis",
    "oldSlugs": null,
    "postCount": 9,
    "description": {
      "markdown": "The **Lottery Ticket Hypothesis** claims that neural networks used in [machine learning](machine-learning) get most of their performance from sub-networks that are already present at initialization that approximate the final policy (\"winning tickets\"). The training process would, under this model, work by increasing weight on the lottery ticket sub-network and reducing weight on the rest of the network.\n\nThe hypothesis was proposed in [a paper](https://arxiv.org/pdf/1803.03635.pdf) by Jonathan Frankle and Micheal Carbin of MIT CSAIL."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "JX69nZB8tfxnx5nGH",
    "name": "Threat Models",
    "core": false,
    "slug": "threat-models",
    "oldSlugs": null,
    "postCount": 22,
    "description": {
      "markdown": "A threat model is a story of how a particular risk (e.g. AI) plays out.\n\nIn the AI case, [according to Rohin Shah](https://ssconlinemeetup.substack.com/p/video-from-rohins-talk), a threat model is ideally: \n> Combination of a development model that says how we get AGI and a risk model that says how AGI leads to existential catastrophe.\n"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "urfLuDdvfqGeTfccP",
    "name": "Selection vs Control",
    "core": false,
    "slug": "selection-vs-control",
    "oldSlugs": null,
    "postCount": 8,
    "description": {
      "markdown": "\"Selection vs Control\" is an attempt to further clarify the notion of \"optimization process\" which has become common on LessWrong, by splitting it into several analogous-but-distinct concepts."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "uxmGtpeE3KoE7pzSL",
    "name": "Chemistry",
    "core": false,
    "slug": "chemistry",
    "oldSlugs": null,
    "postCount": 10,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "qf3kDBak4BQDDw3f2",
    "name": "Modest Epistemology",
    "core": false,
    "slug": "modest-epistemology",
    "oldSlugs": null,
    "postCount": 15,
    "description": {
      "markdown": "**Modest Epistemology** is the claim that average opinions are more accurate that individual opinions, and individuals should take advantage of this by moving toward average opinions, even in cases where they have strong arguments for their own views and against more typical views. (Another name for this concept is \"the wisdom of crowds\" -- that name is much more popular outside of LessWrong.) In terms of [inside view vs outside view](https://www.lesswrong.com/tag/inside-outside-view), we can describe modest epistemology as the belief that inside views are quite fallible and outside views much more robust; therefore, we should weigh outside-view considerations much more heavily.\n\nIn LessWrong parlance, \"modesty\" and \"humility\" should not be confused. While Eliezer lists \"humility\" as a virtue, he provides many arguments *against* modesty (most extensively, in the book [*Inadequate Equilibria*](https://www.lesswrong.com/s/oLGCcbnvabyibnG9d); but also in many earlier sources.) [**Humility**](https://www.lesswrong.com/tag/humility-1) is the general idea that you should expect to be fallible. Modest Epistemology is specifically the view that, due to your own fallibility, you should *rely heavily on* [*outside-view*](https://www.lesswrong.com/tag/inside-outside-view)*.* Modest epistemology says that you should trust average opinions more than your own opinion, even when you have strong arguments for your own views and against more typical views.\n\nHistorically, Robin Hanson has argued in favor of epistemic modesty and outside-view, while Eliezer has argued against epistemic modesty and for a strong inside views. For example, this disagreement played a role in [The Foom Debate](https://www.lesswrong.com/tag/the-hanson-yudkowsky-ai-foom-debate). Eliezer and Hanson both agree that [Aumann's Agreement Theorem](https://www.lesswrong.com/tag/aumann-s-agreement-theorem) implies that rational agents should converge to agreement; however, they have very different opinions about whether/how this breaks down in the absence of perfect rationality. Eliezer sees little reason to move one's opinion toward that of an irrational person's. Hanson thinks irrational agents still benefit from moving their opinions toward each other. One of Hanson's arguments involves [pre-priors](https://www.lesswrong.com/tag/hansonian-pre-rationality).\n\n**External Posts:**\n\n[Immodest Caplan](http://www.overcomingbias.com/2008/09/immodest-caplan.html) by Robin Hanson\n\n**Related Sequences:** [Inadequate Equilibria](https://www.lesswrong.com/s/oLGCcbnvabyibnG9d)\n\n**Related Pages:** [modesty](https://www.lesswrong.com/tag/modesty), [Humility](https://www.lesswrong.com/tag/humility-1), [Inside/Outside View](https://www.lesswrong.com/tag/inside-outside-view), [Egalitarianism](https://www.lesswrong.com/tag/egalitarianism), [Modesty argument](https://www.lesswrong.com/tag/modesty-argument), [Disagreement](https://www.lesswrong.com/tag/disagreement)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "zYPQsxiMcTiY4yE3S",
    "name": "Values handshakes",
    "core": false,
    "slug": "values-handshakes",
    "oldSlugs": [
      "value-handshakes"
    ],
    "postCount": 3,
    "description": {
      "markdown": "**Values handshakes** are a proposed form of trade between superintelligences. From [The Hour I First Believed](https://slatestarcodex.com/2018/04/01/the-hour-i-first-believed/) by Scott Alexander:\n\n> Suppose that humans make an AI which wants to convert the universe into paperclips. And suppose that aliens in the Andromeda Galaxy make an AI which wants to convert the universe into thumbtacks.\n\n> When they meet in the middle, they might be tempted to fight for the fate of the galaxy. But this has many disadvantages. First, there’s the usual risk of losing and being wiped out completely. Second, there’s the usual deadweight loss of war, devoting resources to military buildup instead of paperclip production or whatever. Third, there’s the risk of a Pyrrhic victory that leaves you weakened and easy prey for some third party. Fourth, nobody knows what kind of scorched-earth strategy a losing superintelligence might be able to use to thwart its conqueror, but it could potentially be really bad – eg initiating vacuum collapse and destroying the universe. Also, since both parties would have superintelligent prediction abilities, they might both know who would win the war and how before actually fighting. This would make the fighting redundant and kind of stupid.\n\n> Although they would have the usual peace treaty options, like giving half the universe to each of them, superintelligences that trusted each other would have an additional, more attractive option. They could merge into a superintelligence that shared the values of both parent intelligences in proportion to their strength (or chance of military victory, or whatever). So if there’s a 60% chance our AI would win, and a 40% chance their AI would win, and both AIs know and agree on these odds, they might both rewrite their own programming with that of a previously-agreed-upon child superintelligence trying to convert the universe to paperclips and thumbtacks in a 60-40 mix.\n\n> This has a lot of advantages over the half-the-universe-each treaty proposal. For one thing, if some resources were better for making paperclips, and others for making thumbtacks, both AIs could use all their resources maximally efficiently without having to trade. And if they were ever threatened by a third party, they would be able to present a completely unified front."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "9DmA84e4ZvYoYu6q8",
    "name": "Motivational Intro Posts",
    "core": false,
    "slug": "motivational-intro-posts",
    "oldSlugs": null,
    "postCount": 9,
    "description": {
      "markdown": "Posts which (we theorize) are good to show to new users, to get them excited about rationality. Posts listed here should be high-quality classics, should be accessible without having previously read the Sequences or anything else on LessWrong, and should somehow convince a certain sort of reader that rationality is important, and they want to read more about it. A good motivational intro post might argue the value of rationality directly, or it might point out a reasoning flaw which people recognize strongly in themselves, or it might introduce a rationality concept which is particularly sticky.\n\nThis tag will be treated as a special case in recommendations, and is configured such that it isn't displayed on posts and doesn't appear in search results when adding tags from a post page (but you can still apply it from here). If you aren't sure whether a post meets the criteria, use the discussion page, or just add it (someone else can downvote the tag's relevance later)."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "73btkq64uWfoWGfpF",
    "name": "Internal Family Systems",
    "core": false,
    "slug": "internal-family-systems",
    "oldSlugs": null,
    "postCount": 15,
    "description": {
      "markdown": "**Internal Family Systems**, or IFS, is a family therapy-descended model of the psyche and psychotherapy method based on the idea that the mind is split between *parts* called *exiles*, *managers*, and *firefighters*. Therapy consists of methods for allowing these parts to \"talk\" to each other so, for example, exiles can be reintegrated.\n\n[Building up to an Internal Family Systems model](https://www.lesswrong.com/posts/5gfqG3Xcopscta3st/building-up-to-an-internal-family-systems-model) offers a more thorough introduction to the IFS model.\n\nFor a related technique developed by CFAR, see [Internal Double Crux](https://www.lesswrong.com/tag/internal-double-crux). Rather than thinking of the mind as an entity with one set of goals and beliefs, IFS includes many independently acting components, each of which might have varying goals and beliefs; see [subagents](https://www.lesswrong.com/tag/subagents) for the more general form of this idea."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "XEo9oS2AQvS4spLco",
    "name": "Domain Theory",
    "core": false,
    "slug": "domain-theory",
    "oldSlugs": null,
    "postCount": 6,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "vjKs7Pvz3MbgMc75C",
    "name": "Audio",
    "core": false,
    "slug": "audio",
    "oldSlugs": null,
    "postCount": 42,
    "description": {
      "markdown": "Posts that you can also listen to.  Authors: please link your audio narration or talk at the top of your post.\n\n**Sequences:**  \nRuby's [Wedding Ceremony](https://www.lesswrong.com/s/k2fboiMkdfbCdgFzx)\n\n**Related Pages:** [List of Podcasts](https://www.lesswrong.com/tag/list-of-podcasts)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "adbooNSipZtMrXbzP",
    "name": "Data Science",
    "core": false,
    "slug": "data-science",
    "oldSlugs": null,
    "postCount": 12,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "Aep2BhcHpPXvYCSJc",
    "name": "Ethical Offsets",
    "core": false,
    "slug": "ethical-offsets",
    "oldSlugs": null,
    "postCount": 2,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "jKAkRrhnHedfowNYy",
    "name": "Islam",
    "core": false,
    "slug": "islam",
    "oldSlugs": null,
    "postCount": 4,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "uG75MELqjCEfciaRp",
    "name": "Population Ethics",
    "core": false,
    "slug": "population-ethics",
    "oldSlugs": null,
    "postCount": 17,
    "description": {
      "markdown": "**Population Ethics** addresses the question: how should utilitarians deal with *people coming in and out of existence?* \n\nFor example, a classical problem in population ethics: is it overall better for a happy person to exist, if they bring down the *average* happiness but increase the *total*? Those who would answer \"yes\" are termed *total (hedonic) utilitarians*; those who would answer \"no\" are termed *average (hedonic) utilitarians.*\n\nNote that [utility is not comparable](https://www.lesswrong.com/posts/cYsGrWEzjb324Zpjx/comparing-utilities), so we cannot simply speak of \"increasing average utility\" vs \"increasing total utility\". Hence, *average utilitarianism* vs *total utilitarianism* is not a well-defined distinction in the abstract; we have to specify e.g. *hedonic* utilitarianism before this distinction becomes well-defined (because average vs total happiness is a meaningful distinction).\n\nHowever, although preference-utilitarians cannot necessarily make a meaningful distinction between averaging and totaling, they do not escape population-ethics dilemmas. It's still a fair question: when is it overall preferable to bring someone into existence (or for someone to pass out of existence)? What can/should ethics say about this?"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "9vuoKSLDTE8kbKWEA",
    "name": "Deconfusion",
    "core": false,
    "slug": "deconfusion",
    "oldSlugs": null,
    "postCount": 21,
    "description": {
      "markdown": "Narrowly, **deconfusion** is a specific branch of AI alignment research, discussed in [MIRI's 2018 research update](https://intelligence.org/2018/11/22/2018-update-our-new-research-directions/). More broadly, the term applies to any domain. Quoting from the research update:\n\n> By deconfusion, I mean something like “making it so that you can think about a given topic without continuously accidentally spouting nonsense.”"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "vHzreEfefos3Kqt2p",
    "name": "Cognitive Reduction",
    "core": false,
    "slug": "cognitive-reduction",
    "oldSlugs": null,
    "postCount": 12,
    "description": {
      "markdown": "A **cognitive reduction** is a form of reductive analysis where, rather than reducing something to physical phenomena, we reduce something to the cognitive machinery which give rise to the idea.\n\nFor example, Bayesian probability (ie subjective probability, or credence) can be seen as a cognitive reduction of randomness: rather than seeking physical causes of randomness in the world, we seek the impression of randomness in the mind. We then assert that randomness exists in the map, not the territory.\n\nIn other cases, we may still think the phenomenon exists in the territory, but nonetheless seek a cognitive reduction. For example, while we may think \"apples\" are a real thing that exists, we might be confused about borderline cases (such as a hypothetical fruit which has 90% apple genes and 10% pear genes). A cognitive reduction of \"apple\" helps us understand what it even means to assert one thing or another about borderline cases, while not *necessarily* giving up the claim that apples are real things which exist.\n\nSee also\n--------\n\n*   [How An Algorithm Feels](https://www.lesswrong.com/tag/how-an-algorithm-feels)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "RMtdp6eGNjTZcmwJ6",
    "name": "Dissolving the Question",
    "core": false,
    "slug": "dissolving-the-question",
    "oldSlugs": [
      "dissolvement"
    ],
    "postCount": 19,
    "description": {
      "markdown": "**Dissolving the question** is the act of making a question no longer necessary: satisfying all associated curiosity, resolving all related confusions, but without answering the question. The classic example is the question \"If a tree falls in a forest and no one hears it, does it make a sound?\". The apparent paradox of the question is, in this case, resolved by pointing out the ambiguity of the term \"sound\". The question can be dissolved by [distinguishing](https://www.lesswrong.com/tag/distinctions) between \"Sound\" as referring to auditory experience and \"Sound\" as referring to vibrations in the air. \n\n> “Many philosophers—particularly amateur philosophers, and ancient philosophers—share a dangerous instinct: If you give them a question, they try to answer it.” - Eliezer Yudkowsky, [Dissolving the Question](https://www.lesswrong.com/posts/Mc6QcrsbH5NRXbCRX/dissolving-the-question)\n\nSometimes a question *does* have a strong answer as stated, but also needs to be dissolved. This is ([arguably](https://www.lesswrong.com/posts/NEeW7eSXThPz7o4Ne/thou-art-physics)) the case with [Free Will](https://www.lesswrong.com/tag/free-will), for example:\n\n*   If we do have free will, there's still an additional question of why so many philosophers would conclude otherwise.\n*   If we don't have free will, there's still the question of why so many philosophers think we do.\n\nThis is (probably) not just a case of \"the other side is being silly\": there does indeed seem to be something weird about the question which deserves scrutiny.\n\nIn other words, answering the question doesn't fully address the confusion that the question represents!\n\nA [failure mode](https://www.lesswrong.com/tag/failure-mode) in this step is giving justifications instead of explaining the process itself. Arguing that the reason we have an illusion of free will was that it was [evolutionarily adaptive](https://www.lesswrong.com/tag/evolutionary-psychology) falls into this failure mode, as it doesn't explain the cognitive algorithm which produces the feeling of free will, and so doesn't dissolve the question. This can also be thought of as answering the \"Why\" instead of the \"How\", or as failing to provide a [gears level model](https://www.lesswrong.com/tag/gears-level).\n\nDissolving a question usually (always?) involves providing a [**cognitive reduction**](https://www.lesswrong.com/tag/cognitive-reduction) of the question.\n\nSee also: [**deconfusion**](https://www.lesswrong.com/tag/deconfusion), [**cognitive reduction**](https://www.lesswrong.com/tag/cognitive-reduction)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "QZn3ujmgnvStbuLEc",
    "name": "Lifelogging",
    "core": false,
    "slug": "lifelogging",
    "oldSlugs": null,
    "postCount": 9,
    "description": {
      "markdown": "Lifelogging is the practice of intentionally recording one's life, whether for [life extension purposes](https://www.lesswrong.com/posts/k8mwvvvpjMGcZLAKH/the-case-for-lifelogging-as-life-extension) or [any other purposes](https://matiroy.com/writings/Should-I-record-my-life.html)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "LY3xMQMKSXiYr7rgw",
    "name": "Quantilization",
    "core": false,
    "slug": "quantilization",
    "oldSlugs": null,
    "postCount": 6,
    "description": {
      "markdown": "A **Quantilizer** is a proposed AI design which aims to reduce the harms from [Goodhart's law](https://www.lesswrong.com/tag/goodhart-s-law) and specification gaming by selecting reasonably effective actions from a distribution of human-like actions, rather than maximizing over actions. It it more of a theoretical tool for exploring ways around these problems than a practical buildable design.\n\n### See also\n\n*   [**Rob Miles's Quantilizers: AI That Doesn't Try Too Hard**](https://www.youtube.com/watch?v=gdKMG6kTl6Y)\n*   [**Arbital page on Quantilizers**](https://arbital.com/p/soft_optimizer?l=2r8#Quantilizing)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "3JnSXmK9nYxzd5Riy",
    "name": "Asymmetric Weapons",
    "core": false,
    "slug": "asymmetric-weapons",
    "oldSlugs": null,
    "postCount": 5,
    "description": {
      "markdown": "**Asymmetric Weapons** are weapons that are inherently more powerful to one side of a conflict than another - Unlike a symmetric weapon, which can used as effectively by both sides. The term was originally introduced by Scott Alexander in his essay [Guided By The Beauty Of Our Weapons](https://www.lesswrong.com/posts/qajfiXo5qRThZQG7s/guided-by-the-beauty-of-our-weapons), where he argued that truth, facts, and logic are asymmetric weapons for \"the good guys\":\n\n> Logical debate has one advantage over narrative, rhetoric, and violence: it’s an *asymmetric weapon*. That is, it’s a weapon which is stronger in the hands of the good guys than in the hands of the bad guys. In ideal conditions (which may or may not ever happen in real life) – the kind of conditions where everyone is charitable and intelligent and wise – the good guys will be able to present stronger evidence, cite more experts, and invoke more compelling moral principles. The whole point of logic is that, when done right, it can only prove things that are true.\n\nIn the same essay he gave rhetoric and violence as examples of symmetric weapons.\n\n> Violence is a *symmetric weapon*; the bad guys’ punches hit just as hard as the good guys’ do. \\[...\\] the same is true of rhetoric. Martin Luther King was able to make persuasive emotional appeals for good things. But Hitler was able to make persuasive emotional appeals for bad things. I’ve [previously argued](https://slatestarscratchpad.tumblr.com/post/103708539246/nostalgebraist-at-various-points-bostrom-like) that Mohammed counts as the most successful persuader of all time. These three people pushed three very different ideologies, and rhetoric worked for them all. \n\nScott uses this terminology mainly to separate strategies that work especially well for \"the good guys\" and strategies that work equally well for both sides. Davis Kingsley [warns](https://www.lesswrong.com/posts/Nd5KiuN8pPBrMT82Z/asymmetric-weapons-aren-t-always-on-your-side-1) that strategies can also be asymmetric in favor of \"the bad guys\".\n\n> \"Unless you use asymmetric weapons, the best you can hope for is to win by coincidence.\" - Scott Alexander\n\nExternal Links\n--------------\n\n*   [Asymmetric Weapons Gone Bad](https://slatestarcodex.com/2019/06/06/asymmetric-weapons-gone-bad/)\n\n**Related Pages:** [Memetics](https://www.lesswrong.com/tag/memetics), [Public Discourse](https://www.lesswrong.com/tag/public-discourse)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "42JnNJ8fqfdCSqdmM",
    "name": "Marriage",
    "core": false,
    "slug": "marriage",
    "oldSlugs": null,
    "postCount": 12,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "RGPpwYoCHrPNB86TW",
    "name": "Futarchy",
    "core": false,
    "slug": "futarchy",
    "oldSlugs": null,
    "postCount": 15,
    "description": {
      "markdown": "**Futarchy** is a proposed government system in which decisions are made based on betting markets. It was [originally proposed by Robin Hanson](http://mason.gmu.edu/~rhanson/futarchy.html), who gave the motto \"Vote on Values, But Bet on Beliefs\".\n\nA futarchic government holds an election to determine *what metrics to optimize;* for example, a ballot might allow citizens to vote on the following options:\n\n*   Gross Domestic Product (GDP) over the next 4 years, as estimated by an impartial council of economists.\n*   GDP over the next 8 years.\n*   GDP over the next 30 years (etc).\n*   Happiness, as measured by a large survey. (Over the next 4 years, 8 years, etc.)\n*   Family values, as measured by (such-and-such procedure)\n\nAnd so on.\n\nA futarchy then sets up betting markets for the effectiveness of various policies. The policies with the best estimated effectiveness get implemented. The consequences are then observed, so bets about the implemented policies can pay out.\n\n**Related Pages:** [Prediction Markets](https://www.lesswrong.com/tag/prediction-markets), [Government](https://www.lesswrong.com/tag/government), [Law and Legal systems](https://www.lesswrong.com/tag/law-and-legal-systems), [Voting Theory](https://www.lesswrong.com/tag/voting-theory), [Politics](https://www.lesswrong.com/tag/politics)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "Yd7JES6Wmbc2PBgid",
    "name": "Appeal to Consequence",
    "core": false,
    "slug": "appeal-to-consequence",
    "oldSlugs": [
      "appeal-to-consequences"
    ],
    "postCount": 5,
    "description": {
      "markdown": "An ***appeal to consequences*** is an argument against saying or believing something, which addresses external consequences of saying/believing, rather than the truth of the matter. This creates a conflict between epistemic rationality and instrumental rationality (or at least, raises the possibility of such a conflict). The argument asks you to compromise the soundness of your map, for the sake of the territory. If you give in, you may be accepting a falsehood. If you refuse, you may be shooting yourself in the foot."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "b7ZSAGimsbzrLR5CR",
    "name": "Family planning",
    "core": false,
    "slug": "family-planning",
    "oldSlugs": null,
    "postCount": 19,
    "description": {
      "markdown": "Posts about deciding whether to have children, how many children to have, when to have children, etc. Also called parenthood decision-making.\n\nFor how to raise children once one has decided to have them, see [parenting](https://www.lesswrong.com/tag/parenting).\n\n**External Links:**  \n[Antinatalism](https://en.wikipedia.org/wiki/Antinatalism)  \n[Family Planning](https://en.wikipedia.org/wiki/Family_planning)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "7ngr5dCMapYsuhst7",
    "name": "Charter Schools",
    "core": false,
    "slug": "charter-schools",
    "oldSlugs": null,
    "postCount": 1,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "wGaBT3HSWWFqPfXB9",
    "name": "DIY",
    "core": false,
    "slug": "diy",
    "oldSlugs": null,
    "postCount": 6,
    "description": {
      "markdown": "**DIY** means **Do It Yourself**. \n\n**Related Pages:** [More Dakka](https://www.lesswrong.com/tag/more-dakka)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "fM6pmeSEncbzxoGpr",
    "name": "Functional Decision Theory",
    "core": false,
    "slug": "functional-decision-theory",
    "oldSlugs": null,
    "postCount": 19,
    "description": {
      "markdown": "**Functional Decision Theory** is a [decision theory](https://www.lesswrong.com/tag/decision-theory) described by Eliezer Yudkowsky and Nate Soares which says that agents should treat one’s decision as the output of a ﬁxed mathematical function that answers the question, “Which output of this very function would yield the best outcome?”. It is a replacement of [Timeless Decision Theory](https://www.lesswrong.com/tag/timeless-decision-theory), and it outperforms other decision theories such as [Causal Decision Theory](https://www.lesswrong.com/tag/causal-decision-theory) (CDT) and [Evidential Decision Theory](https://www.lesswrong.com/tag/evidential-decision-theory) (EDT). For example, it does better than CDT on [Newcomb's Problem](https://www.lesswrong.com/tag/newcomb-s-problem), better than EDT on the [smoking lesion problem](https://www.lesswrong.com/tag/smoking-lesion), and better than both in [Parﬁt’s hitchhiker problem](https://www.lesswrong.com/tag/parfits-hitchhiker).\n\nIn Newcomb's Problem, an FDT agent reasons that Omega must have used some kind of model of her decision procedure in order to make an accurate prediction of her behavior. Omega's model and the agent are therefore both calculating the same function (the agent's decision procedure): they are *subjunctively dependent* on that function. Given perfect prediction by Omega, there are therefore only two outcomes in Newcomb's Problem: either the agent one-boxes and Omega predicted it (because its model also one-boxed), or the agent two-boxes and Omega predicted *that*. Because one-boxing then results in a million and two-boxing only in a thousand dollars, the FDT agent one-boxes.\n\n**External links:**\n\n*   [Functional decision theory: A new theory of instrumental rationality](https://intelligence.org/2017/10/22/fdt)\n*   [Cheating Death in Damascus](https://intelligence.org/2017/03/18/new-paper-cheating-death-in-damascus/)\n*   [Decisions are for making bad outcomes inconsistent](https://intelligence.org/2017/04/07/decisions-are-for-making-bad-outcomes-inconsistent/)\n\n**See Also:**\n\n*   [Timeless Decision Theory](https://www.lesswrong.com/tag/timeless-decision-theory)\n*   [Updateless Decision Theory](https://www.lesswrong.com/tag/updateless-decision-theory)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "vmdE9sHtHqPhNDWys",
    "name": "Stag Hunt",
    "core": false,
    "slug": "stag-hunt",
    "oldSlugs": null,
    "postCount": 7,
    "description": {
      "markdown": "**Stag Hunt** is a game-theoretic model of coordination in which each player chooses to either hunt Rabbit (small guaranteed return) or Stag (larger return if everyone else also chooses stag, no return otherwise)."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "EGqrmaw8Tpr3oQ2eX",
    "name": "Product Reviews",
    "core": false,
    "slug": "product-reviews",
    "oldSlugs": null,
    "postCount": 2,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "EvPPocx6FHcoDfygQ",
    "name": "Consensus",
    "core": false,
    "slug": "consensus",
    "oldSlugs": null,
    "postCount": 10,
    "description": {
      "markdown": "A **Consensus** is a general or full agreement between members of a group. A consensus can be useful in deciding what's true (e.g, a scientific consensus), or as a criteria in decision making. A **False Consensus** can happen when someone thinks a position is in consensus when it isn't. one can also claim a consensus falsely to advance their position and make it difficult for others to oppose it. a **False Controversy** can happen when one mistakes something to not be in consensus when in fact it is. Claiming false controversies is a common way of creating uncertainty and doubt.\n\nThere are many things that are considered a consensus on LessWrong, even though they're are not considered a consensus in the scientific community, such as: [One-Boxing](https://www.lesswrong.com/tag/one-boxing), cooperating on the [Prisoner's Dilemma](https://www.lesswrong.com/tag/prisoner-s-dilemma),  [Bayesianism](https://www.lesswrong.com/tag/bayesianism) over [frequentist](https://en.wikipedia.org/wiki/Frequentist_probability) probability (and more to be added)\n\nNotable things that *aren't* in consensus on LessWrong include [Blackmail / Extortion](https://www.lesswrong.com/tag/blackmail-extortion), [the benefits of rationality](https://www.lesswrong.com/tag/is-rationality-any-good), [AI Timelines](https://www.lesswrong.com/tag/ai-timelines) and [AI Takeoff](https://www.lesswrong.com/tag/ai-takeoff), as well as [AI alignment strategies](Friendly Artificial Intelligence), \n\n**Related Pages:** [Common Knowledge](https://www.lesswrong.com/tag/common-knowledge), [Disagreement](https://www.lesswrong.com/tag/disagreement), [Modesty](https://www.lesswrong.com/tag/modesty), [Modesty argument](https://www.lesswrong.com/tag/modesty-argument), [Aumann agreement](https://www.lesswrong.com/tag/aumann-agreement), [Government](https://www.lesswrong.com/tag/government) (in the context of democracies), [Contrarianism](https://www.lesswrong.com/tag/contrarianism)\n\n**See also:** [consensus on wikipedia](https://en.wikipedia.org/wiki/Consensus_(disambiguation))"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "Hc9fuB89BRPkeH6c5",
    "name": "Qualia Research Institute",
    "core": false,
    "slug": "qualia-research-institute",
    "oldSlugs": null,
    "postCount": 4,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "vMeJdnoedH7iL8LiR",
    "name": "Time (value of)",
    "core": false,
    "slug": "time-value-of",
    "oldSlugs": null,
    "postCount": 8,
    "description": {
      "markdown": "What is the value of an hour of your time? How can you spend money to free up time? When is that the right call? Etc."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "e6j9tnTfaabjCgK6d",
    "name": "Literature Reviews",
    "core": false,
    "slug": "literature-reviews",
    "oldSlugs": null,
    "postCount": 18,
    "description": {
      "markdown": "A **literature review**, a **literature survey**, a **state of the art** is an extended write-up collecting, and often comparing and commenting on a set of publications addressing a specific topic/question.\n\nTheir main purpose is not to present original research, nor serve as teaching material, but to provide a gateway to the latest and/or most valuable research on a particular topic. This can serve to jumpstart a bibliography, as a first chapter of a dissertation, or to guide newcomers to a field where textbooks aren't available, or too broad or too dated.\n\nNote the three terms are not synonymous but gather well under the same tag.\n\nSee also: [Book Reviews](https://www.lesswrong.com/tag/book-reviews), [Epistemic Review](https://www.lesswrong.com/tag/epistemic-review). [Summaries](https://www.lesswrong.com/tag/summaries)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "EPTcHwSJxRHXAipT2",
    "name": "Assurance contracts",
    "core": false,
    "slug": "assurance-contracts",
    "oldSlugs": null,
    "postCount": 10,
    "description": {
      "markdown": "An **Assurance Contract** is a contract of the form \"I commit to X if Y other people do the same\". for example, \"I commit to come to a protest if 100K other people make the same commitment\". if less than 100K sign this contract, it has no effect. if 100K or more sign it, it goes into effect and everyone who signed it is expected to come to the protest.\n\nAssurance Contracts, at least in theory, are useful coordination tools for problems where collective action is needed and individual actions is for some reason risky or not worth it for the individual if they end up doing it alone or as part of a too small group.\n\nThere's has been much discussion on LessWrong about creating software tools that let people coordinate with assurance contracts. there were already a few similar attempts outside of the community, but it is generally agreed on LW that these tools aren't good enough yet.\n\n**Related pages:** [Commitment Mechanisms](https://www.lesswrong.com/tag/commitment-mechanisms), [Pre-Commitment](https://www.lesswrong.com/tag/pre-commitment), [Coordination / Cooperation](https://www.lesswrong.com/tag/coordination-cooperation)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "YyGDbZhGtws5hEPda",
    "name": "Self Fulfilling/Refuting Prophecies",
    "core": false,
    "slug": "self-fulfilling-refuting-prophecies",
    "oldSlugs": null,
    "postCount": 26,
    "description": {
      "markdown": "A **Self Fulfilling Prophecy** is a prophecy that, when made, affects the environment such that it becomes more likely. similarly, a **Self Refuting Prophecy** is a prophecy that when made makes itself less likely. This is also relevant for beliefs that can affect reality directly without being voiced, for example, the belief \"I'm confident\" can increase a person confidence, thus making it true, while the opposite belief can reduce a person's confidence, thus also making it true.\n\n**Related pages:** [Social Reality](https://www.lesswrong.com/tag/social-reality)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "jcegcg2mqPGWBiXHc",
    "name": "Conceptual Media",
    "core": false,
    "slug": "conceptual-media",
    "oldSlugs": null,
    "postCount": 2,
    "description": {
      "markdown": "A conceptual medium is a an expressive medium that primarily serves to express abstract thoughts and ideas. It is analogous to the concept of an artistic medium (e.g., painting, poetry, violin, etc), but pertains to the expression of conceptual thought rather than artistic expression.\n\nExamples of such media would be spoken argument, written prose text, programming languages, mathematical symbolic notation, geometrical diagrams,  explanatory comics, conceptual network diagrams (a.k.a., mindmaps), interactive simulations, data visualizations, markup languages, and semantic web technologies."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "nYta24PsKTvg57KZy",
    "name": "Satisficer",
    "core": false,
    "slug": "satisficer",
    "oldSlugs": null,
    "postCount": 10,
    "description": {
      "markdown": "A **Satisficer** aims to reach a set level of [utility](https://www.lesswrong.com/tag/utility-functions) rather than maximizing utility. It is a proposed optimization process to the open [Other-izer problem](https://arbital.greaterwrong.com/p/otherizer?l=2r9).\n\n[Video explaining Satisficers vs Maximizers](https://www.youtube.com/watch?v=Ao4jwLwT36M&list=UULB7AzTwc6VFZrBsO2ucBMg&index=5) (and why satisficers are still dangerous)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "2dz4FEXCdbk2BwDpc",
    "name": "LessWrong Review",
    "core": false,
    "slug": "lesswrong-review",
    "oldSlugs": null,
    "postCount": 40,
    "description": {
      "markdown": "The **LessWrong Review** is a yearly event where posts from a previous year are nominated, reviewed, and voted on."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "nGLvnaZCH5mx8h8Mh",
    "name": "Selectorate Theory",
    "core": false,
    "slug": "selectorate-theory",
    "oldSlugs": null,
    "postCount": 6,
    "description": {
      "markdown": "[**Selectorate Theory**](https://en.wikipedia.org/wiki/Selectorate_theory)  is a game theoretic theory of politics and power that aims to explain political behavior and structure as a consequence of a leader's motivation to gain power and keep it as long as he can.  \n  \nThe theory posits that no leader can rule alone, and thus always has to satisfy some amount of key people. in autocracies that number is small, and in democracies that number is large.\n\nThe theory separates the rest of the population (apart from the leader) into three groups. \n\n*   ***interchangeables** \\-* are those who can influence the selection of the president (e.g, anyone who has voting rights)\n*   ***influentials** \\-* are those who actually influence (e.g, those who actually end up voting)\n*   ***essentials** \\-* those who's support is essential for the selection of the leader (e.g, the minimum amount of voters needed to be elected and the electoral college)\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/1f0cd4742b265d95eb72e1df90d7cf836a3327dd8cb8fd26.png)\n\nEuler diagram of the three groups\n\nLeaders satisfy the essentials by giving them rewards. the more essentials there are the more expensive it is to reward them privately, and the more it's worth to create public goods from which both they and everyone else benefit. which is how the theory explains the difference between the amount of public goods in democracies versus autocracies.\n\nThe less essentials there are the easier it is to satisfy them and stay in control. thus a leader's incentive is to keep the amount of essentials as low as possible.\n\nthe incentive of the influentials and interchangeables is to increase the amount of essentials there are, such that the leader would have to use public goods to satisfy them.\n\nThe essentials incentive, when there are very few essentials, is to have even fewer, and thus align with the leader's. but past a certain point the essentials start to prefer having more essentials and more public goods rather than to have a small amount of essentials and gain private rewards - which then becomes contrary to the leaders goal, which stayed the same.\n\nIn the dictator's handbook the authors describe 5 rules which every leader must follow in order to gain power and keep it:\n\n> (1) The smaller the winning coalition the fewer people to satisfy to remain in control. (2) Having a large nominal selectorate gives a pool of potential people to replace dissenters in coalition. (3) Maintain control of revenue flows to redistribute to your friends. (4) But only pay friends enough that they will not consider overthrowing you and at the same time little enough so that they depend on you. (5) Don't take your friends' money and redistribute it to the masses. *(Wikipedia)*\n\nThe theory was developed by [Bruce Bueno de Mesquita](https://en.wikipedia.org/wiki/Bruce_Bueno_de_Mesquita), Alastair Smith, Randolph M. Siverson, [James D. Morrow](https://en.wikipedia.org/wiki/James_D._Morrow), and introduced in [*The Logic of Political Survival*](https://en.wikipedia.org/wiki/The_Logic_of_Political_Survival)  and [*The Dictator's Handbook*](https://en.wikipedia.org/wiki/The_Dictator%27s_Handbook)*.*"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "AHK82ypfxF45rqh9D",
    "name": "Distinctions",
    "core": false,
    "slug": "distinctions",
    "oldSlugs": null,
    "postCount": 102,
    "description": {
      "markdown": "A common [Failure mode](https://www.lesswrong.com/tag/failiure-mode) is failing to notice a **Distinction** between two or more things. on the other hand, noticing distinctions can lead to insight, dissolving confusion, and better results.\n\n> “intelligence is the measure of the number and the quality of the distinctions you have in a given situation” - Tony Robbins\n\nThus, making and noticing distinctions is a core skill.\n\n**See also:** [Decoupling vs Contextualizing](https://www.lesswrong.com/tag/decoupling-vs-contextualizing), [Conflict vs Mistake](https://www.lesswrong.com/tag/conflict-vs-mistake), [Compartmentalization](https://www.lesswrong.com/tag/compartmentalization), [Bucket Errors](https://www.lesswrong.com/tag/bucket-errors), [Map and Territory](https://www.lesswrong.com/tag/map-and-territory), [Fallacy of Gray](https://www.lesswrong.com/tag/fallacy-of-gray), [Wanting and liking](https://www.lesswrong.com/tag/wanting-and-liking)\n\n**External Links:**  \n[Use–mention distinction](https://en.wikipedia.org/wiki/Use%E2%80%93mention_distinction)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5xyggFN9J64FskhqP",
    "name": "Cognitive Fusion",
    "core": false,
    "slug": "cognitive-fusion",
    "oldSlugs": [
      "fusion"
    ],
    "postCount": 4,
    "description": {
      "markdown": "\\[More thorough definition/description in [Prereq: Cognitive Fusion](https://www.lesswrong.com/posts/5g7oFiePGEY3h4bqX/prereq-cognitive-fusion) and [Kaj Sotala's attempt to explain Looking, insight meditation, and enlightenment in non-mysterious terms](https://www.lesswrong.com/posts/mELQFMi9egPn5EAjK/my-attempt-to-explain-looking-insight-meditation-and)\\]   \n  \nFrom the latter:\n\n> Cognitive fusion is a term from [Acceptance and Commitment Therapy](https://en.wikipedia.org/wiki/Acceptance_and_commitment_therapy) (ACT), which refers to a person “fusing together” with the content of a thought or emotion, so that the content is experienced as an objective fact about the world rather than as a mental construct. The most obvious example of this might be if you get really upset with someone else and become convinced that something was *all their fault* (even if you had actually done something blameworthy too).  \n> In this example, your anger isn’t letting you see clearly, and you can’t step back from your anger to question it, because you have become “fused together” with it and experience everything in terms of the anger’s internal logic.  \n>   \n> In this example, your anger isn’t letting you see clearly, and you can’t step back from your anger to question it, because you have become “fused together” with it and experience everything in terms of the anger’s internal logic.\n> \n> Another emotional example might be feelings of shame, where it’s easy to experience yourself as a horrible person and feel that *this is the literal truth*, rather than being just an emotional interpretation.\n> \n> Cognitive fusion isn’t *necessarily* a bad thing. If you suddenly notice a car driving towards you at a high speed, you don’t want to get stuck pondering about how the feeling of danger is actually a mental construct produced by your brain. You *want* to get out of the way as fast as possible, with minimal mental clutter interfering with your actions. Likewise, if you are doing programming or math, you *want* to become at least partially fused together with your understanding of the domain, taking its axioms as objective facts so that you can focus on figuring out how to work with those axioms and get your desired results."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "FxNdAsrw8qvZkPaxN",
    "name": "QURI",
    "core": false,
    "slug": "quri",
    "oldSlugs": null,
    "postCount": 18,
    "description": {
      "markdown": "The **Quantified Uncertainty Research Institute (QURI)** is a nonprofit set up to study forecasting and epistemics. You can see their website [here](https://quantifieduncertainty.org/). \n\n**Related Pages:** [Prediction Markets](https://www.lesswrong.com/tag/prediction-markets), [Forecasting & Prediction](https://www.lesswrong.com/tag/forecasting-and-prediction)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "ZWRtQgXucwzAFZqNJ",
    "name": "China",
    "core": false,
    "slug": "china",
    "oldSlugs": null,
    "postCount": 24,
    "description": {
      "markdown": "**China** is a large country in eastern Asia, the most populous in the world and a significant power in the 21st century."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "F2XfCTxXLQBGjbm8P",
    "name": "Parables & Fables",
    "core": false,
    "slug": "parables-and-fables",
    "oldSlugs": [
      "parables"
    ],
    "postCount": 34,
    "description": {
      "markdown": "**External links:**  \n[The Fable of the Dragon-Tyrant](https://nickbostrom.com/fable/dragon.html) by Nick Bostrom ([animated version](https://www.youtube.com/watch?v=cZYNADOHhVY) by CGP Grey)  \n[Ars Longa, Vita Brevis](https://slatestarcodex.com/2017/11/09/ars-longa-vita-brevis/) by Scott Alexander  \n[Kolmogorov Complicity and the Parable of Lightning](https://slatestarcodex.com/2017/10/23/kolmogorov-complicity-and-the-parable-of-lightning/) by Scott Alexander  \n[The Emperor's New Cloths](https://andersen.sdu.dk/vaerk/hersholt/TheEmperorsNewClothes_e.html)\n\n**Related tags:** [Fiction](https://www.lesswrong.com/tag/fiction), [Writing](https://www.lesswrong.com/tag/writing), [Narratives (stories)](https://www.lesswrong.com/tag/narratives-stories)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "g97MMk83r5HofZLBe",
    "name": "The Pointers Problem",
    "core": false,
    "slug": "the-pointers-problem",
    "oldSlugs": null,
    "postCount": 10,
    "description": {
      "markdown": "**The pointers problem** refers to the fact that most humans would rather have [an AI that acts based on real-world human values](https://www.lesswrong.com/tag/outer-alignment?showPostCount=true), not just human estimates of their own values – and that the two will be different in many situations, since humans are not all-seeing or all-knowing. It was introduced in [a post with the same name](https://www.lesswrong.com/posts/gQY6LrTWJNkTv8YJR/the-pointers-problem-human-values-are-a-function-of-humans)."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "aLB9evWFYtfyS3WJg",
    "name": "Music",
    "core": false,
    "slug": "music",
    "oldSlugs": null,
    "postCount": 52,
    "description": {
      "markdown": "**Music** is a form of [art](art) based on placing notes in a rhythmic order. Most of LW's articles on music so far have been written by jefftk."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "rtha3oEP5vzkvdbbD",
    "name": "Games (posts describing)",
    "core": false,
    "slug": "games-posts-describing",
    "oldSlugs": null,
    "postCount": 16,
    "description": {
      "markdown": "Games for and by lesswrongers.\n\n**Related Pages:** [Exercises / Problem-Sets](https://www.lesswrong.com/tag/exercises-problem-sets)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "HPZzE9XBy99RmbmQe",
    "name": "Kelly Criterion",
    "core": false,
    "slug": "kelly-criterion",
    "oldSlugs": null,
    "postCount": 20,
    "description": {
      "markdown": "the **Kelly criterion** (or **Kelly strategy** or **Kelly bet**), also known as the scientific gambling method, is a [formula](https://en.wikipedia.org/wiki/Formula) for bet sizing that leads [almost surely](https://en.wikipedia.org/wiki/Almost_surely) to higher wealth compared to any other strategy in the long run (i.e. approaching the limit as the number of bets goes to infinity). ([Wikipedia](https://en.wikipedia.org/wiki/Kelly_criterion))\n\n**Posts from elsewhere:** [The Kelly Coin-Flipping Game: Exact Solutions](https://www.gwern.net/Coin-flip)\n\n**see also:** [Betting](https://www.lesswrong.com/tag/betting)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "9ponmAskWgC37GZKk",
    "name": "Algorithms",
    "core": false,
    "slug": "algorithms",
    "oldSlugs": null,
    "postCount": 4,
    "description": {
      "markdown": "Posts that describe or demonstrate specific **Algorithms** that can be used to make decisions.\n\nExample: [Final Version Perfected: An Underused Execution Algorithm](https://www.lesswrong.com/posts/xfcKYznQ6B9yuxB28/final-version-perfected-an-underused-execution-algorithm#comments)\n\n**See also:** [Decision Theory](https://www.lesswrong.com/tag/decision-theory), [Planning & Decision-Making](https://www.lesswrong.com/tag/planning-and-decision-making),"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "z5uy4NcWc2JSRTGHb",
    "name": "Incentives",
    "core": false,
    "slug": "incentives",
    "oldSlugs": null,
    "postCount": 23,
    "description": {
      "markdown": "An **Incentive** is a motivating factor, such as monetary reward, the risk of legal sanctions, or social feedback. Many systems are best understood by looking at the incentives of the people with power over them.\n\n[Inadequate Equilibria](https://www.lesswrong.com/s/oLGCcbnvabyibnG9d) covers many problems that arise when there are poor incentives.\n\n**Related Pages:** [Game Theory](https://www.lesswrong.com/tag/game-theory), [Mechanism Design](https://www.lesswrong.com/tag/mechanism-design), [Moloch](https://www.lesswrong.com/tag/moloch), [Moral Mazes](https://www.lesswrong.com/tag/moral-mazes)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "p8W2vFMHydvfgBgPh",
    "name": "SETI",
    "core": false,
    "slug": "seti",
    "oldSlugs": null,
    "postCount": 5,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "mwKRsZuFbGr7tA5h7",
    "name": "Delegation",
    "core": false,
    "slug": "delegation",
    "oldSlugs": null,
    "postCount": 3,
    "description": {
      "markdown": "When some humans want something done, those humans can delegate responsibility for the task to one or more AI systems. From the perspective of the AI systems, the relationship would be one of assistance directed toward the humans. However, to avoid dependence of our arguments upon viewing AI systems as having a “perspective”, we treat humans as the primary seat of agency, and view the humans as engaged in delegation.  \n– excerpt from AI Research Considerations for Human Existential Safety report"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "zhLB7AT83YC3HCtwk",
    "name": "Astrobiology",
    "core": false,
    "slug": "astrobiology",
    "oldSlugs": null,
    "postCount": 7,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "k6igEkzKYY2EpY7Su",
    "name": "Meta-Philosophy",
    "core": false,
    "slug": "meta-philosophy",
    "oldSlugs": null,
    "postCount": 20,
    "description": {
      "markdown": "**Meta-philosophy** or **metaphilosophy** (with no hyphen) is the philosophy of philosophy itself. According to [the wikipedia article](https://en.wikipedia.org/wiki/Metaphilosophy), \"its subject matter includes the aims of [philosophy](/tag/philosophy), the boundaries of philosophy, and its methods\"."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5Gi4NzgKtzWja8GHh",
    "name": "Automation",
    "core": false,
    "slug": "automation",
    "oldSlugs": null,
    "postCount": 4,
    "description": {
      "markdown": "**Automation** is \"the use of largely automatic equipment in a system of manufacturing or other production process\". \\[1\\] This includes, but is not limited to, most of the processes involved in car manufacturing, and many ways that computer programs are applied.\n\n\\[1\\]: New Oxford American Dictionary"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "kxffynF3MAK3TacL7",
    "name": "Measure Theory",
    "core": false,
    "slug": "measure-theory",
    "oldSlugs": null,
    "postCount": 2,
    "description": {
      "markdown": "**Measure Theory** is a [mathematical](/tag/mathematics) field focused on [measures](https://en.wikipedia.org/wiki/Measure_(mathematics)), functions that aim to provide an intuitive interpretation of a subset's size."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "Z5A4c4kjTgLSFEr3h",
    "name": "Autonomous Vehicles",
    "core": false,
    "slug": "autonomous-vehicles",
    "oldSlugs": null,
    "postCount": 13,
    "description": {
      "markdown": "**Autonomous vehicles**, also known as **self-driving cars**/trucks/&c., are vehicles that can make steering, pedaling, or braking decisions to some extent. They are notable for being a present-day case where [alignment](/tag/outer-alignment) and [AI safety](ai-safety) are important - for example, AI should not crash a car in order to gather data about car crashes.\n\nIn addition, autonomous vehicles present important ethical questions - for example, how much should a self-driving car focus on the safety of passengers, as opposed to people outside of the car? If autonomous vehicles become significantly less accident-prone than (and approximately as cheap as) human-driven ones, should government policies require people to use them for public safety?\n\nNote: posts specifically about autonomous vehicles should be given higher relevance than ones that simply use them as examples."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "jTScW3bvF3j6WoiFe",
    "name": "Radical Probabilism",
    "core": false,
    "slug": "radical-probabilism",
    "oldSlugs": null,
    "postCount": 5,
    "description": {
      "markdown": "**Radical Probabilism** is a newer form of [Bayesianism](https://www.lesswrong.com/tag/bayesianism) invented by [Richard Jeffrey](https://en.wikipedia.org/wiki/Richard_Jeffrey). The primary point of departure from other forms of Bayesianism is its rejection of the strong connection between conditional probability and updates. Radical Probabilism therefore rejects the strong connection between [Bayes' Rule](https://www.lesswrong.com/tag/bayes-theorem) and updating.\n\nOther notable writers on Radical Probabilism include Jeffrey's student [Richard Bradley](https://en.wikipedia.org/wiki/Richard_Bradley_(philosopher)), and the champion of naturalized philosophy, [Brian Skyrms](https://en.wikipedia.org/wiki/Brian_Skyrms)."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "htgXy4gow6tHWu2bA",
    "name": "Problem of Old Evidence",
    "core": false,
    "slug": "problem-of-old-evidence",
    "oldSlugs": null,
    "postCount": 3,
    "description": {
      "markdown": "Suppose a new scientific hypothesis, such as general relativity, explains a well-know observation such as the [perihelion precession of mercury](https://en.m.wikipedia.org/wiki/Tests_of_general_relativity) better than any existing theory. Intuitively, this is a point in favor of the new theory. However, the probability for the well-known observation was already at 100%. How can a previously-known statement provide new support for the hypothesis, as if we are re-updating on evidence we've already updated on long ago? This is known as [**the problem of old evidence**](https://plato.stanford.edu/entries/epistemology-bayesian/#ObjSimPriConRulInfOthObjBayConThe), and is usually levelled as a charge against Bayesian epistemology.\n\n\\[Needs to be expanded!\\]"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "ksWkwyKRrj582WqpN",
    "name": "ET Jaynes",
    "core": false,
    "slug": "et-jaynes",
    "oldSlugs": null,
    "postCount": 19,
    "description": {
      "markdown": "**Edwin Thompson Jaynes** was a prominent bayesian mathematician and the author of *Probability Theory: The Logic of Science*."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "tPfh8oYyG8Wgkkg8X",
    "name": "Journalism",
    "core": false,
    "slug": "journalism",
    "oldSlugs": null,
    "postCount": 8,
    "description": {
      "markdown": "This tag applies to posts focused on news, except at an object level (i.e. newspaper companies and the process behind the news are relevant, but current events aren't necessarily)."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "CL9NePP9FejkQo6jn",
    "name": "Future of Life Institute (FLI)",
    "core": false,
    "slug": "future-of-life-institute-fli",
    "oldSlugs": null,
    "postCount": 14,
    "description": {
      "markdown": "The **Future of Life Institute**, or **FLI,**  is a nonprofit organization whose mission is to mitigate [existential risks](existential-risk). Its most prominent activities are issuing grants to x-risk researchers and organizing conferences on [AI](ai) and existential risk.\n\nWebsite: [futureoflife.org](https://futureoflife.org)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "RxuepsZgBEKax9bmP",
    "name": "Anchoring",
    "core": false,
    "slug": "anchoring",
    "oldSlugs": null,
    "postCount": 5,
    "description": {
      "markdown": "**Anchoring** is the use of a reference value when estimating a quantitative value, and the bias that results if the reference point used isn't suitable for that purpose.\n\nAnchoring is a special case of [priming](https://www.lesswrong.com/tag/priming)."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "KADf7NFiAr9DAaQxN",
    "name": "Social Media",
    "core": false,
    "slug": "social-media",
    "oldSlugs": null,
    "postCount": 39,
    "description": {
      "markdown": "**Social media** are digital tools that let users share content with many others, quickly and easily. Prominent examples include Facebook, Twitter, TikTok, and many others.\n\nSocial media is infamous for its tendency to polarize users, make them spend more time and attention than they intended, and otherwise hurt productivity."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "PCLuivxECxsC3aNgy",
    "name": "Lighting",
    "core": false,
    "slug": "lighting",
    "oldSlugs": null,
    "postCount": 12,
    "description": {
      "markdown": "Posts about (artificial / indoor) lighting, especially relating to attempts to improve quality of life during winter months and at other times when insufficient natural light is available."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "49GPZJoXc7d2gKTDG",
    "name": "Tradeoffs",
    "core": false,
    "slug": "tradeoffs",
    "oldSlugs": null,
    "postCount": 9,
    "description": {
      "markdown": "**Tradeoffs** are decisions with both benefits and pitfalls. In practice, most decisions are tradeoffs, because they come at the expense of time that could be spent on other activities."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "cA6e2WpMYZEvfsopQ",
    "name": "GAN",
    "core": false,
    "slug": "gan",
    "oldSlugs": null,
    "postCount": 5,
    "description": {
      "markdown": "**Generative Adversarial Network** is a [machine learning](https://www.lesswrong.com/tag/machine-learning) architecture containing two modules. The **generator** attempts to create an output that is similar to the network's training data, while the **discriminator** attempts to tell the generator's outputs apart from the training data. The generator is reinforced based on how well its outputs fool the discriminator, so the two modules are adversaries.\n\nGANs are best known for working well with images; for example, generating pictures of human faces."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "ux2x9RrJsuykQxT79",
    "name": "Deliberate Practice",
    "core": false,
    "slug": "deliberate-practice",
    "oldSlugs": null,
    "postCount": 16,
    "description": {
      "markdown": "**Deliberate practice** is the highest form of practice according to Anders Ericsson and Robert Pool (the authors of the book *Peak*). Based on the scientific study of expertise, they classify several forms of practice. From less to more effective:\n\n1.  **Naive practice.** You practice music by playing it. You practice medicine by doing it. You practice driving by doing it. Ericsson and Pool cite studies showing that this is not very effective at increasing, or even maintaining, skill. Doctors are (on average) at their best shortly after getting out of medical school, and gradually decline in skill thereafter despite their continuing \"practice\" of medicine. Similar observations apply to other skills.\n2.  **Purposeful practice.** This is practice which (1) has well-defined goals (such as doing something 3 times in a row with no mistakes), (2) is focused (the person is intently interested in improving, rather than having their attention elsewhere), (3) involves feedback, (4) involves getting out of one's comfort zone, practicing things on the edge of one's ability.\n3.  **Deliberate practice.** On top of the requirements for purposeful practice, deliberate practice is *informed by an understanding of how to do well.* In the best case, this understanding is conferred by a professional teacher (a \"coach\"). This teacher will be able to evaluate where a student sits with respect to the various necessary sub-skills, recommend specific practice tasks to improve sub-skills which are lacking, and give advice about how to improve technique.\n\nThe authors note that purposeful practice can result in getting stuck if you learn *bad form* (they don't use that term, but what they describe is very close to the concept of \"good form\" from the [CFAR handbook](https://www.lesswrong.com/posts/Z9cbwuevS9cqaR96h/cfar-participant-handbook-now-available-to-all) (page 19)). Bad form means that practice is ultimately instilling bad habits, even if it creates local improvement. Good form means that practice is taking you down the path to mastery in an efficient manner.\n\nDue to the necessity of having an experienced teacher, deliberate practice requires a highly developed field. However, it is also common to use the term \"deliberate practice\" for anything which is distinguished from purposeful practice by the presence of a *theory of skill* and *practice guided by that theory*, whether or not that theory is tried-and-true, and whether or not an experienced teacher is involved."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "6v2FHy8dtyCYg9Kz4",
    "name": "Therapy",
    "core": false,
    "slug": "therapy",
    "oldSlugs": null,
    "postCount": 26,
    "description": {
      "markdown": "**Therapy** is treatment intended to reduce or remove a disorder. Depression, burn wounds, and many other afflictions can be treated with therapy and time. Psychotherapy aims to treat mental disorders, while physical therapy deals with injuries, physical illnesses, and other bodily damage.\n\n**Related Pages:** [Psychology](https://www.lesswrong.com/tag/psychology), [Psychiatry](https://www.lesswrong.com/tag/psychiatry), [Self Improvement](https://www.lesswrong.com/tag/self-improvement), [Meditation](https://www.lesswrong.com/tag/meditation), [subagents](https://www.lesswrong.com/tag/subagents), [Center for Applied Rationality (CFAR)](https://www.lesswrong.com/tag/center-for-applied-rationality-cfar)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "dGw7KEdcLFSvbJ7bM",
    "name": "Meta-Humor",
    "core": false,
    "slug": "meta-humor",
    "oldSlugs": null,
    "postCount": 7,
    "description": {
      "markdown": "**Meta-Humor** is the analysis of what makes things funny, and the roles humor plays in sociology and cognition.\n\nFor posts that are themselves humor, see [humor](https://www.lesswrong.com/tag/humor)."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "NX5R6eJajJFssMvDi",
    "name": "Project Announcement",
    "core": false,
    "slug": "project-announcement",
    "oldSlugs": null,
    "postCount": 23,
    "description": {
      "markdown": "A **project announcement** is what you might expect - an announcement of a project.  \nPosts that are about a project's announcement, but do not themselves announce anything, should not have this tag."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "2YcmB6SLtHnHRe3uX",
    "name": "VNM Theorem",
    "core": false,
    "slug": "vnm-theorem",
    "oldSlugs": null,
    "postCount": 12,
    "description": {
      "markdown": "The **VNM theorem** is one of the classic results of Bayesian decision theory. It establishes that, under four assumptions known as the **VNM axioms**, a preference relation *must* be representable by maximum-expectation decision making over some real-valued utility function. (In other words, rational decision making is best-average-case decision making.)\n\nStarting with some set of outcomes, **gambles** (or **lotteries**) are defined recursively. An outcome is a gamble, and for any finite set of gambles, a probability distribution over those gambles is a gamble.\n\nPreferences are then expressed over gambles via a preference relation. if \\\\(A\\\\) is preferred to \\\\(B\\\\), this is written \\\\(A>B\\\\). We also have indifference, written \\\\(A\\\\sim{} B\\\\). If \\\\(A\\\\) is either preferred to \\\\(B\\\\) *or* indifferent with \\\\(B\\\\), this can be written \\\\(A \\\\geq{} B\\\\).\n\nThe four VNM axioms are:\n\n1.  **Completeness.** For any gambles \\\\(A\\\\) and \\\\(B\\\\), either \\\\(A>B\\\\), \\\\(B>A\\\\), or \\\\(A \\\\sim{} B\\\\).\n2.  **Transitivity.** If \\\\(A<B\\\\) and \\\\(B<C\\\\), then \\\\(A<C\\\\).\n3.  **Continuity.** If \\\\(A \\\\leq{} B \\\\leq{} C\\\\), then there exists a probability \\\\(p \\\\in{} \\[0,1\\]\\\\) such that  \\\\(pA+(1-p)C \\\\sim{} B\\\\). In other words, there is a probability which hits any point between two gambles.\n4.  **Independence.** For any \\\\(C\\\\) and \\\\(p \\\\in{} \\[0,1\\]\\\\), we have \\\\(A \\\\leq{} B\\\\) if and only if \\\\(pA + (1-p)C \\\\leq{} pB + (1-p)C\\\\). In other words, substituting \\\\(A\\\\) for \\\\(B\\\\) in any gamble can't make that gamble worth less.\n\nIn contrast to [Utility Functions](https://www.lesswrong.com/tag/utility-functions), this tag focuses *specifically* on posts which discuss the VNM theorem itself."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "wygybwY9SMdaPepZr",
    "name": "Feedback & Criticism (topic)",
    "core": false,
    "slug": "feedback-and-criticism-topic",
    "oldSlugs": [
      "feedback-and-criticism"
    ],
    "postCount": 19,
    "description": {
      "markdown": "**Feedback** is information about people's reactions to an event, object, person, place, or idea. Many writers, developers, entrepreneurs, and other content creators try to get feedback so they can know how to improve their creations. **Positive feedback** is positive information in a reaction - i.e. being told that one's hair looks nice, while **negative feedback** or **criticism** is the opposite - i.e. being told that one's hair looks like a mop."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "SgpwPSrHRbeRSxKus",
    "name": "Perfect Predictor",
    "core": false,
    "slug": "perfect-predictor",
    "oldSlugs": null,
    "postCount": 3,
    "description": {
      "markdown": "A **perfect predictor** is an agent which can predict the behaviour of an agent or the outcome of an event with perfect accuracy. It is often given the name [Omega](https://www.lesswrong.com/tag/omega), but Omega sometimes refers to an almost perfect predictor.\n\nPossibility and relevance:\n--------------------------\n\nPerfect predictors are generally understood to be impossible due to the [Uncertainty Principle](https://www.wikiwand.com/en/Uncertainty_principle) or just from our general experience that perfect observation or accuracy aren't a feature of our universe. Some people claim this makes them irrelevant for real decision theory problems. See the page on [hypotheticals](https://www.lesswrong.com/tag/hypotheticals) for further discussion on whether or not this is valid. Some [people](https://www.lesswrong.com/posts/AKkFh3zKGzcYBiPo7/counterfactuals-for-perfect-predictors?commentId=zZDWxYmbd6m6nDpHf) have objected on the basis of [free will](https://www.lesswrong.com/tag/free-will).\n\nSome people have [attempted](https://www.lesswrong.com/posts/de3xjFaACCAk6imzv/towards-a-new-decision-theory) to make these problems more realistic and concrete by reframing it in terms of computational agents with access to other agents source code or the program representing the environment. This won't be perfect in the sense that there's nothing stopping a machine error or a hacker messing ruining the prediction, but it is close enough that it can be approximately to perfect predictors.\n\nInconsistent Counterfactuals:\n-----------------------------\n\nOne challenge with perfect predictors is that it might be unclear what Omega is predicting, particularly in situations that are only [conditionally consistent](https://www.lesswrong.com/tag/conditional-consistency). Take for example [Parfit's Hitchhiker](https://www.lesswrong.com/tag/parfits-hitchhiker). In this problem, you are trapped dying in a desert and a passing driver will only pick you up if you promise to pay them $100 once you are in town. If the driver is a perfect predictor, then someone who always defects will never end up in town, so it is unclear what exactly they are predicting, since the situation is contradictory and the [Principle of Explosion](https://en.wikipedia.org/wiki/Principle_of_explosion) means that you can prove anything.\n\n[Counterfactuals for Perfect Predictors](https://www.lesswrong.com/posts/AKkFh3zKGzcYBiPo7/counterfactuals-for-perfect-predictors) suggests that even if we can't predict what an agent would do in an inconsistent or [conditionally consistent](https://www.lesswrong.com/tag/conditional-consistency) situation, we can predict how it would respond if given input representing an inconsistent situation (we can represent this response as an output). This aligns with [Updateless Decision Theory](https://www.lesswrong.com/tag/updateless-decision-theory) which isn't subject to this issue as it uses input-output maps."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "CGpSeuhohSFFqnTYZ",
    "name": "Conditional Consistency",
    "core": false,
    "slug": "conditional-consistency",
    "oldSlugs": null,
    "postCount": 1,
    "description": {
      "markdown": "Some [decision theory](https://www.lesswrong.com/tag/decision-theory) scenarios might only be consistent depending on the choice you make. For example, it's been observed that if you decide not to pay when you are in town with a perfect predictor, then it is not consistent"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "QPt5ECwTCAg63mbNu",
    "name": "Quests / Projects Someone Should Do",
    "core": false,
    "slug": "quests-projects-someone-should-do",
    "oldSlugs": null,
    "postCount": 22,
    "description": {
      "markdown": "**Quests** are when someone is like \"man, someone should *totally* do this thing.\""
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "TBPFC5fwpkzLB87xJ",
    "name": "Pre-Commitment",
    "core": false,
    "slug": "pre-commitment",
    "oldSlugs": null,
    "postCount": 11,
    "description": {
      "markdown": "Many [Decision Theory](https://www.lesswrong.com/tag/decision-theory) problems involve pre-commitment or deciding in advance how you are going to act. This is crucial for game-theory, where an agent that has credibly pre-committed can force other actors to act differently than they would other otherwise acted. It is also important for problems with predictors like [Newcomb's Problem](https://www.lesswrong.com/tag/newcomb-s-problem) where an agent which pre-commits [one-boxing](https://www.lesswrong.com/tag/one-boxing) guarantees (or almost guarantees) themselves the million. Lastly, it can be important for agents who are aware that they are likely to make a bad decision in the moment.\n\nInteractions with Predictors:\n-----------------------------\n\nThere has been significant disagreement about what pre-commitment means for decision theory problems where you are being predicted by a sufficiently high quality predictor. In [Newcomb's Problem](https://www.lesswrong.com/tag/newcomb-s-problem), two-boxers typically believe that while you could have obtained the million by pre-committing before Omega made their prediction, afterwards is too late. Even though [two-boxing](https://www.lesswrong.com/tag/two-boxing) only gives you $1000, they claim that the million was never in the box so you never could have gained it. In contrast, one-boxers tend to believe that it is a mistake to think that the million isn't accessible to you - see Eliezer arguing that [you can just do it](https://www.lesswrong.com/posts/6ddcsdA2c2XpNpE5x/newcomb-s-problem-and-regret-of-rationality) \\- in other words that if you one-box you will always find that the million always was accessible to you.\n\nIf you have to pre-commit in advance the question naturally arises - what counts as pre-commitment? Is it sufficient to just decide in advance what you are going to do as long as you are committed to following through or do you have to commit more substantially by setting up a penalty sufficient to dissuade yourself from changing your mind? Having raised this question, the answer seems clear - a pre-commitment is valid in terms of obtaining you the million so long as it is legible to Omega and it is valid in terms of binding you to the action so long as you actually follow through.\n\nOne distinction that it might be useful to make is between [formal and effective pre-commitment](https://www.lesswrong.com/posts/Q8tyoaMFmW8R9w9db/formal-vs-effective-pre-commitment). Formal pre-commitment is when you take specific legible actions to commit yourself like talking about it in public, handing over money as a deposit or rewriting your source-code. On the other hand, effective pre-commitment is the notion that in a deterministic universe whatever action you take you are pre-committed to and that an agent that knew the environment and your state in sufficient detail would be able to predict what action you would take. In this view, the only difference is that formal pre-commitment is easier to predict.\n\nOne issue that arises with predictors is that some scenarios may be conditionally inconsistent (or just plain inconsistent when we're dealing with logical uncertainty and oracles). Oddly enough, it seems as though it might make sense to allow pre-commitments in relation to these scenarios, although this involves pre-committing to taking an action when receiving input representing such a potentially inconsistent scenario rather than pre-committing to take an action in a particular scenario itself.\n\n[Game Theory](https://www.lesswrong.com/tag/game-theory):\n---------------------------------------------------------\n\nIn game theory, commitment is often considered purely from the perspective of incentives. From this view, you are considered to have pre-committed youself to an action if any benefit you would gain from it is outweighed by the penalty you would pay.\n\n[Psychology](https://www.lesswrong.com/tag/psychology):\n-------------------------------------------------------\n\nPre-commitment can also be important from a psychological perspective. Suppose you have an assignment to work on. You know that you need to work on it tomorrow, but you also know that you won't feel like it on the day. By deciding in advance to work on the assignment tomorrow you are providing yourself an additional reason (keeping your commitments to yourself) to work on it.\n\n**Related Pages:** [Commitment Mechanisms](https://www.lesswrong.com/tag/commitment-mechanisms), [Assurance contracts](https://www.lesswrong.com/tag/assurance-contracts)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "rjEZWSbSffhaWYRvo",
    "name": "Hypotheticals",
    "core": false,
    "slug": "hypotheticals",
    "oldSlugs": null,
    "postCount": 13,
    "description": {
      "markdown": "Many philosophy problems involve imagining hypothetical scenarios. At times there has been significant debate over the validity of this.\n\nUnrealistic hypotheticals:\n--------------------------\n\nAt times there has been significant debate on Less Wrong about the relevance or value in discussing hypotheticals that are unrealistic. In [The Least Convenient Possible World](https://www.lesswrong.com/posts/neQ7eXuaXpiYw7SBy/the-least-convenient-possible-world), Scott Alexander suggests that ignoring hypotheticals often means that you are technically correct at the cost \"missing the point and losing a valuable effort to examine the nature of morality\". He suggests that considering about the least convenient world is often vital for allowing us to discover our true motivations and often leaves us too much \"wiggle room\".\n\nIn [Please Don't Fight the Hypothetical](https://www.lesswrong.com/posts/s9hTXtAPn2ZEAWutr/please-don-t-fight-the-hypothetical), TimS suggests that fighting the hypothetical is equivalent to, \"I don't find this topic interesting for whatever reason, and wish to talk about something I am interested in.\" He says that this is fine, but suggests it is important to be aware when you are changing the subject like this.\n\n[Hypotheticals: The Direct Application Fallacy](https://www.lesswrong.com/posts/5y45Kry6GtWCFePjm/hypotheticals-the-direct-application-fallacy) suggests that it is a mistake to assume that the only reason for studying a hypothetical situation is to understand what to do in that exact situation. It suggests that practise exercises don't need to be real and in fact insisting on this can make teaching nearly impossible. It further suggests that examining degenerate cases of a theory often provides a useful sanity check and can make the limitations of a heuristic more explicit.\n\nRelated terms:\n--------------\n\nHypotheticals are essentially the same as [counterfactuals](https://www.lesswrong.com/tag/counterfactuals), although a) the term counterfactual is preferred when imagining someone making different decisions b) technically the factual isn't a counterfactual, but it is very common to say something like \"iterate over all the counterfactuals and pick the one with the highest utility\" where we treat the factual as a counterfactual."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "a3W2TSzPuxKr3Hm9j",
    "name": "Intellectual Progress via LessWrong",
    "core": false,
    "slug": "intellectual-progress-via-lesswrong",
    "oldSlugs": [
      "intellectual-progress-via-lesswrong"
    ],
    "postCount": 25,
    "description": {
      "markdown": "One of the core goals of the [**LessWrong 2.0 platform**](https://www.lesswrong.com/about) is to generate [**intellectual progress**](https://www.lesswrong.com/tag/intellectual-progress), specifically [differential intellectual progress](https://www.lesswrong.com/tag/differential-intellectual-progress) on important questions. There is much to discuss: should LessWrong 2.0 be a vehicle for intellectual progress? Is it succeeding? What should it do to succeed more?  \n  \n*See also*: [Intellectual Progress (Society-Level)](https://www.lesswrong.com/tag/intellectual-progress?showPostCount=true), [Intellectual Progress (Individual-Level)](https://www.lesswrong.com/tag/intellectual-progress-individual-level?showPostCount=true&useTagName=true), [Site Meta](https://www.lesswrong.com/tag/site-meta?showPostCount=true&useTagName=true)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb356",
    "name": "Road To AI Safety Excellence",
    "core": null,
    "slug": "road-to-ai-safety-excellence",
    "oldSlugs": null,
    "postCount": 6,
    "description": {
      "markdown": "**Road to AI Safety Excellence** (RAISE), previously named AASAA, was an initiative from [toonalfrink](http://lesswrong.com/user/toonalfrink/) to improve the pipeline for AI safety researchers, especially by creating an online course. See the [Post-Mortem](https://www.lesswrong.com/posts/oW6mbA3XHzcfJTwNq/raise-post-mortem).\n\nNote\n----\n\nThis page is deprecated, and it will no longer be updated by the RAISE founder (unless an independent party decides to). See the updated page at [http://aisafety.camp/about/](http://aisafety.camp/about/)\n\nMotivation\n----------\n\nAI safety is a small field. It has only about 100 researchers. The field is mostly [talent-constrained](https://80000hours.org/2015/11/why-you-should-focus-more-on-talent-gaps-not-funding-gaps/#ai-safety-research). Given the dangers of an uncontrolled intelligence explosion, increasing the amount of AIS researchers is crucial for the long-term survival of humanity.\n\nWithin the LW community there are plenty of talented people that bear a sense of urgency about AI. They are willing to switch careers to doing research, but they are unable to get there. This is understandable: the path up to research-level understanding is lonely, arduous, long, and uncertain. It is like a pilgrimage. One has to study concepts from the papers in which they first appeared. This is not easy. Such papers are [undistilled](http://distill.pub/2017/research-debt/). Unless one is lucky, there is no one to provide guidance and answer questions. Then should one come out on top, there is no guarantee that the quality of their work will be sufficient for a paycheck or a useful contribution.\n\nThe field of AI safety is in an [innovator phase](https://en.wikipedia.org/wiki/Diffusion_of_innovations). Innovators are highly risk-tolerant and have a large amount of agency, which allows them to survive an environment with little guidance or supporting infrastructure. Let community organisers not fall for the typical mind fallacy, expecting risk-averse people to move into AI safety all by themselves. Unless one is particularly risk-tolerant or has a perfect safety net, they will not be able to fully take the plunge. Plenty of measures can be made to make getting into AI safety more like an [\"It's a small world\"-ride](https://www.youtube.com/watch?v=28123GsMzU8):\n\n*   Let there be a tested path with signposts along the way to make progress clear and measurable.\n*   Let there be social reinforcement so that we are not hindered but helped by our instinct for conformity.\n*   Let there be high-quality explanations of the material to speed up and ease the learning process, so that it is cheap.\n\nBecoming an AIS researcher in 2020\n----------------------------------\n\nWhat follows is a vision of how things \\*could\\* be, should this project come to fruition.\n\n**The path**\n\n1\\. Tim Urban's [Road to Superintelligence](https://waitbutwhy.com/2015/01/artificial-intelligence-revolution-1.html) is a popular introduction to superintelligence. Hundreds of thousands of people have read it. At the end of the article is a link, saying \"if you want to work on this, these guys can help\". It sends one to an [Arbital](https://wiki.lesswrong.com/wiki/Arbital) page, reading \"welcome to \"prerequisites for \"introduction to AI Safety\"\"\".\n\n2\\. What follows is a series of articles explaining the math one should understand to be able to read AIS papers. It covers probability, game theory, computability theory, and a few other things. Most students with a technical major can follow along easily. Even some talented high school graduates do. When one comes to the end to the arbital sequence, one is congratulated: \"you are now ready to study AI safety\". A link to the course appears at the bottom of the page.\n\n3\\. The course teaches an array of subfields. Technical subjects like corrigibility, value learning, ML safety, but also some high-level subjects like preventing arms races around AI. Assignments are designed in such a way that they don't need manual grading, but do give some idea of the student's competence. Sometimes there is an assignment about an open problem. Students are given the chance to try to solve it by themselves. Interesting submissions are noted. One competent recruiter looks through these assignments to handpick high-potential students. When a student completes the course, they are awarded a nice polished certificate. Something to print and hang on the wall.\n\n**Local study groups**\n\nWhen it comes to motivation, nothing beats the physical presence of people that share your goal. A clear and well-polished path is one major thing, social reinforcement is another. Some local study groups already exist, but there is no way for outsiders to find them. RAISE seems like a most natural place to index study groups and facilitate hosting them. You can see and edit the current list here: [https://bit.ly/AISafetyLocalGroups](https://bit.ly/AISafetyLocalGroups).\n\nCourse prerequisites & target audience\n--------------------------------------\n\nWhile the project originally targeted any student, it was decided that it will target those that are philosophically aligned first. The next step could be to persuade academics to model a course after this one, so that we will reach a broader audience too.\n\nThere are technical (math, logic) and philosophical (Bostrom/sequences/WaitButWhy) prerequisites. Technical prerequisites identified so far:\n\n*   Probability theory\n*   Decision/game theory\n*   Computability theory\n*   Logic\n*   Linear algebra\n\nAs mentioned before, it seems best to cover this in a sequence of articles on Arbital, or to recommend [an existing course](http://web.stanford.edu/class/cs103/) that teaches this stuff well enough.\n\nThe state of the project & getting involved\n-------------------------------------------\n\nIf you're enthusiastic about volunteering, fill in [this form](https://goo.gl/forms/m38tKbmDBFMgSyMz1)\n\nTo be low-key notified of progress, join [this Facebook group](https://www.facebook.com/groups/1421511671230776/)\n\nOne particularly useful and low-bar way to contribute is to join our special study group, in which you will be asked to summarize AIS resources (papers, talks, ...), and create mind maps of subjects. You can find it in the Facebook group.\n\nCurriculum\n----------\n\nThis is (like everything) subject to debate, but for now it looks like the following broad categories will be covered:\n\n*   Agent foundations\n*   Machine learning safety\n*   AI macrostrategy\n\nEach of these categories will be divided into a few subcategories. The specifics of that are mostly undecided, except that the agent foundations category will contain at least corrigibility and decision theory.\n\nWe are making efforts to list all available resources [here](https://workflowy.com/s/D0Q9.oyUe39KbLp) and [here](https://bit.ly/AISafetyResources)\n\nCourse development process\n--------------------------\n\nNow volunteers and capital are largely in place, we are doing an [iterative development process](https://en.wikipedia.org/wiki/Iterative_and_incremental_development) with the first unit on corrigibility. When we are satisfied with the quality of this unit, we will use the process we developed to create the other units.\n\n**Study groups** Even for volunteers it proved tricky to reach a high-level understanding of a topic by oneself, so we decided to learn together. The study group is constructed in such a way that it produces useful content for the course. More concretely:\n\n\\- There are 'scripting' and 'assignments' meetings.\n\n\\- The 'scripting' meetings embody an iterative process to go from papers to lecture scripts. We start with summaries, then we create mind maps, then we decide on a set of video, and then we create a set of script drafts based on summaries and mind maps\n\n\\- All of this content is used by the lecturer to finalize scripts, set up the studio and film.\n\n\\- The set of videos produced by the lecturer are used as an input to the assignments meeting.\n\n\\- At the assignment meeting, for each lecture bit, attendants are asked to create assignments and try the assignments of others. A selection of these assignments is later added to the course.\n\n**Shooting lectures**\n\nWe enlisted [Rob Miles](https://www.youtube.com/channel/UCLB7AzTwc6VFZrBsO2ucBMg) to shoot our lectures. About once a week, our content developer sits down with him to go over a particular script draft, which he modifies to his liking.\n\nThe setup includes a [lightboard](https://www.youtube.com/watch?v=qadgqBQdqtI), which is a neat educational innovation that allows a lecturer to look at the camera while writing on a board simultaneously.\n\nInstruction strategy\n--------------------\n\nThe course will be strictly digital, which limits the amount of strategies that can be used. These are some potentially useful strategies:\n\n*   Text\n*   Lecture\n*   Documentary\n*   Game\n*   Assignment\n*   Live discussion\n*   Open problem\n*   etc...\n\n**Content guides form** The best way to present an idea often depends on the nature of the idea. For example, the value alignment problem is easily explained with an illustrative story (the paperclip maximizer). This isn’t quite the case for FDT. Also, some ideas have been formalized. We can go into mathematical detail with those. Other ideas are still in the realm of philosophy and we will have to resort to things like thought experiments there. How to say depends on what to say.\n\n**Gimmick: Open problems** (Inspired by [The Failures of Eld Science](http://lesswrong.com/lw/q9/the_failures_of_eld_science/)) A special type of instruction strategy will be an assignment like this: “So here we have EDT, which is better than CDT, but it is still flawed in these ways. Can you think of a better decision theory that doesn’t have these flaws? Give it at least 10 minutes. If you have a useful idea, please let us know.”\n\nThe idea is to challenge students to think independently how they might go about solving an open problem. It gives them an opportunity to actually make a contribution. I expect it to be strongly intrinsically motivating.\n\n**Taxonomy of content** At least three sorts of content will be delivered:\n\n*   Anecdotes/stories to illustrate problems (paperclip maximizer, filling a cauldron, ...)\n*   Unformalized philosophical considerations (intelligence explosion, convergent instrumental goals, acausal trade, ...)\n*   Technical results (corrigibility, convergent instrumental goals, FDT, ...)\n\n**Example course unit: value learning & corrigibility**\n\n*   Preview of unit and its structure\n*   An x-minute lecture that informally explains the value learning problem\n*   Assignments\n*   A 5-minute cutscene shows a fictional story of an agent that keeps its creators from pushing the off-button\n*   An x-minute lecture that informally explains corrigibility\n*   A piece of text that introduces the math\n*   A video of the lecturer solving example math assignments\n*   Corrigibility math assignments\n\nAlternatively, we can interleave tiny bits of video with questions to keep the student engaged. A good example of this is the [Google deep learning course](https://www.udacity.com/course/deep-learning--ud730).\n\nTask allocation\n---------------\n\nThe following is a reply to the common remark that “I’d like to help, but I’m not sure what I can do”.\n\n(last updated at 2018-01-31)\n\n**Full responsibility**\n\nThis means you can’t sleep when things are off track, and jump to your laptop every time you have a new idea to move things forward. This also means you are ready to take on most tasks if no one else volunteers for it, even if you’re not specialized in it. The project is your baby, and you’re a helicopter parent.\n\nCurrently done by: Toon Alfrink, Veerle de Goederen, Remmelt Ellen, Johannes Heidecke, Mati Roy\n\nRequired technical understanding: superficial.\n\nMinimum commitment: 1 full day per week\n\n**Armchair advice**\n\nYou’re in the chat, and you’re interested in the project, but not ready to make significant contributions. You do want to see where things go, and sometimes you have some interesting remarks to make. On your own terms though.\n\nCurrently done by: lots of people\n\nMinimum commitment: none\n\n**Content developer**\n\nAs our content developer, you are responsible for the quality of the material. You coordinate the study group, review the quality of it's production, and spend extra time on your own learning the content (if you haven't already) so you can be our expert. You also help the lecturer with finalizing his scripts, and you assist him in understanding everything.\n\nCurrently done by: **No one. This is a paid position. If interested, email us at raise@aisafety.camp**\n\nRequired technical understanding: near-complete.\n\nMinimum commitment: 2 full days per week\n\n**Giving lectures**\n\nYou thoroughly study the material, making sure you know it well enough to explain it clearly. Together with the content developer, you sit down and go over the bits that need explanation. These bits range from [3 to 6 minutes](https://blog.edx.org/optimal-video-length-student-engagement), and they are interleaved with questions and small assignments to keep the student engaged.\n\nCurrently done by: Robert Miles\n\nRequired technical understanding: thorough.\n\nMinimum commitment: 1 full day per week\n\n**Study group attendant**\n\nYou help out in the weekly study group, creating summaries, mind maps, script drafts and assignments. We also give presentations\n\nCurrently done by: Johannes Heidecke, Tom Rutten, Toon Alfrink, Tarn Somervell Fletcher, Nandi Schoots, Roland Pihlakas, Robert Miles, Rupert McCallum, Philine Widmer, Louie Terrill, Tim Bakker, Veerle de Goederen, Ofer Givoli\n\nRequired technical understanding: none\n\nMinimum commitment: 4 hours per week\n\n**Software developer**\n\nWith about 60% certainty, we will use [ihatestatistics](https://ihatestatistics.com/) as a platform. The company is run by EA's (we may use it for free), and it's specialization in statistics (which is closely related to AI) makes it well-suited for our needs. [Here is a demo lesson.](https://play.ihatestatistics.com/#/preview/level/191?lang=en) There are a lot of diamonds buried in the field of automated assessment. The quality of our answer-checking software determines the quality of the questions we can ask. Elaborate feedback mechanisms can make a lot of difference in how fast a learner may converge on the right kind of understanding. You write this software for us.\n\nMinimum commitment: 2 days per week\n\n**Legal**\n\nLegal is a black box. Your first job is to write your job description.\n\n**Marketing/PR/acquisition**\n\nAre you good at connecting people? There are a lot of people that want to fix the world, would engage with this project if they knew about it, and have the means (funding, expertise) to help out. Things you can do include finding funders, hosting a round of review, inviting guest speakers with interesting credentials, connecting with relevant EA organisations, etc. Having high social capital in the EA/LW community is a plus.\n\n**Animation & editing**\n\nGood animation can make a course twice as polished and engaging, and this matters twice as much as you think. The whole point of a course instead of a loose collection of papers is that learners can trust they're on the right track. Polish builds that trust. Animation is also a skill that is hard to pick up in a short enough timeframe, so we can't do it. If you're interested in AI safety and skilled at animation, we need you!\n\nPeptalk\n-------\n\nI want to note that what we are doing here *isn’t hard*. Courses at universities are often created on the fly by one person in a matter of weeks. They get away with it. There is little risk. The worst that can reasonably happen is that we waste some time and money on creating an unpopular course that doesn’t get much traction. On the other hand, there is a lot of opportunity. If we do this well, we might just double the amount of FAI researchers. If that's not impact, I don't know what is.\n\nExternal links\n--------------\n\n*   [list of resources](https://workflowy.com/s/D0Q9.oyUe39KbLp)\n*   [facebook group](https://www.facebook.com/groups/1421511671230776/)\n*   [Announcement post on LessWrong](http://lesswrong.com/r/discussion/lw/p5e/announcing_aasaa_accelerating_ai_safety_adoption/)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb354",
    "name": "Crucial Considerations",
    "core": null,
    "slug": "crucial-considerations",
    "oldSlugs": null,
    "postCount": 8,
    "description": {
      "markdown": "**Crucial considerations** are ideas that potentially imply radical changes to our world-view and priorities. The term was coined by [Nick Bostrom](https://www.lesswrong.com/tag/nick-bostrom).\n\nExternal links\n--------------\n\n*   [Crucial Considerations and Wise Philanthropy - Nick Bostrom](http://www.stafforini.com/blog/bostrom/)\n*   [Crucial considerations - EA concepts](https://concepts.effectivealtruism.org/concepts/the-importance-of-crucial-considerations/)\n*   [The blog \"Crucial Considerations\"](http://crucialconsiderations.org/about/) (run by [FRI](https://www.lesswrong.com/tag/center-on-long-term-risk-clr))"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb345",
    "name": "Criticisms of The Rationalist Movement",
    "core": null,
    "slug": "criticisms-of-the-rationalist-movement",
    "oldSlugs": null,
    "postCount": 13,
    "description": {
      "markdown": "**Criticisms of the** [rationalist movement](https://www.lesswrong.com/tag/rationalist-movement) and [LessWrong](https://www.lesswrong.com/about) have existed for most of its duration on various grounds.\n\nCult of Rationality\n-------------------\n\nLess Wrong has been referred to as a [~cult~ phyg](https://www.lesswrong.com/tag/phyg) on numerous occasions,[^1^](#fn1)[^2^](#fn2)[^3^](#fn3) with [Eliezer Yudkowsky](https://www.lesswrong.com/tag/eliezer-yudkowsky) as its leader. Eliezer's confidence in his AI safety work outside of mainstream academia and self-professed intelligence renders him highly unpopular with his critics.[^4^](#fn4)\n\nNeoreaction\n-----------\n\nThe Neoreaction movement,[^5^](#fn5) is a notoriously adjacent idea to the community. Whist it has being explicitly refuted by figures such as Eliezer[^6^](#fn6)[^7^](#fn7) and Scott,[^8^](#fn8) it is often actively-associated by critics.[^9^](#fn9)[^10^](#fn10)\n\nRationalism\n-----------\n\nThe movement has been criticized as overemphasizing [inductive reasoning](https://www.lesswrong.com/tag/induction) over [empiricism](https://www.lesswrong.com/tag/empiricism),[^11^](#fn11) a criticism that has been refuted by [Scott Alexander](https://www.lesswrong.com/tag/scott-alexander).[^12^](#fn12)\n\nRoko's basilisk\n---------------\n\nThe [Roko's basilisk](https://wiki.lesswrong.com/wiki/Roko's_basilisk) thought experiment was notorious in that it required specific preconditions available nearly exclusively within the Less Wrong community that rendered the reader vulnerable to this 'memetic hazard'. As such it has drawn derision from critics who feel perception risk from unfriendly AI is overstated within the community.[^13^](#fn13)[^14^](#fn14)\n\nTranshumanism\n-------------\n\nLess Wrong's community was partially founded by soliciting users from the transhumanist [SL4](https://hpluspedia.org/wiki/SL4#cite_note-1) mailing list and [Eliezer Yudkowsky](https://www.lesswrong.com/tag/eliezer-yudkowsky) is himself a prominent transhumanist.\n\nAs such, the fringe nature of transhumanist ideas such as [cryonics](https://www.lesswrong.com/tag/cryonics), AGI takeover[^15^](#fn15) has met with continued scorn from the skeptics based at [RationalWiki](https://wiki.lesswrong.com/wiki/RationalWiki).[^16^](#fn16)\n\nSee also\n--------\n\n*   [Rationalist movement](https://www.lesswrong.com/tag/rationalist-movement)\n*   [RationalWiki](https://wiki.lesswrong.com/wiki/RationalWiki)\n*   [Sneer Club](https://wiki.lesswrong.com/wiki/Sneer_Club)\n\nReferences\n----------\n\n1.  [http://lesswrong.com/lw/4d/youre\\_calling\\_who\\_a\\_cult_leader/](http://lesswrong.com/lw/4d/youre_calling_who_a_cult_leader/)[↩](#fnref1)\n2.  [http://lesswrong.com/lw/bql/our\\_phyg\\_is\\_not\\_exclusive_enough/](http://lesswrong.com/lw/bql/our_phyg_is_not_exclusive_enough/)[↩](#fnref2)\n3.  [https://www.reddit.com/r/OutOfTheLoop/comments/3ttw2e/what\\_is\\_lesswrong\\_and\\_why\\_do\\_people\\_say\\_it\\_is\\_a/](https://www.reddit.com/r/OutOfTheLoop/comments/3ttw2e/what_is_lesswrong_and_why_do_people_say_it_is_a/)[↩](#fnref3)\n4.  [http://rationalwiki.org/wiki/Eliezer_Yudkowsky](http://rationalwiki.org/wiki/Eliezer_Yudkowsky)[↩](#fnref4)\n5.  [https://web.archive.org/web/20130424060436/http://habitableworlds.wordpress.com/2013/04/21/visualizing-neoreaction/](https://web.archive.org/web/20130424060436/http://habitableworlds.wordpress.com/2013/04/21/visualizing-neoreaction/)[↩](#fnref5)\n6.  [http://yudkowsky.tumblr.com/post/142497361345/this-isnt-going-to-work-but-for-the-record-and](http://yudkowsky.tumblr.com/post/142497361345/this-isnt-going-to-work-but-for-the-record-and)[↩](#fnref6)\n7.  [http://lesswrong.com/lw/fh4/why\\_is\\_mencius\\_moldbug\\_so\\_popular\\_on\\_less\\_wrong/](http://lesswrong.com/lw/fh4/why_is_mencius_moldbug_so_popular_on_less_wrong/)[↩](#fnref7)\n8.  [http://slatestarcodex.com/2013/10/20/the-anti-reactionary-faq/](http://slatestarcodex.com/2013/10/20/the-anti-reactionary-faq/)[↩](#fnref8)\n9.  [http://rationalwiki.org/wiki/Neoreactionary_movement](http://rationalwiki.org/wiki/Neoreactionary_movement)[↩](#fnref9)\n10.  [https://hpluspedia.org/wiki/The\\_Silicon\\_Ideology](https://hpluspedia.org/wiki/The_Silicon_Ideology)[↩](#fnref10)\n11.  [https://the-orbit.net/almostdiamonds/2014/11/24/why-i-am-not-a-rationalist/](https://the-orbit.net/almostdiamonds/2014/11/24/why-i-am-not-a-rationalist/)[↩](#fnref11)\n12.  [http://slatestarcodex.com/2014/11/27/why-i-am-not-rene-descartes/](http://slatestarcodex.com/2014/11/27/why-i-am-not-rene-descartes/)[↩](#fnref12)\n13.  [http://rationalwiki.org/wiki/Roko's_basilisk](http://rationalwiki.org/wiki/Roko's_basilisk)[↩](#fnref13)\n14.  [http://idlewords.com/talks/superintelligence.htm](http://idlewords.com/talks/superintelligence.htm)[↩](#fnref14)\n15.  [http://rationalwiki.org/wiki/Cybernetic_revolt](http://rationalwiki.org/wiki/Cybernetic_revolt)[↩](#fnref15)\n16.  [http://rationalwiki.org/wiki/Transhumanism](http://rationalwiki.org/wiki/Transhumanism)[↩](#fnref16)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb326",
    "name": "Roko's Basilisk",
    "core": null,
    "slug": "rokos-basilisk",
    "oldSlugs": null,
    "postCount": 8,
    "description": {
      "markdown": "**Roko’s basilisk** is a thought experiment proposed in 2010 by the user Roko on the [*Less Wrong*](\\\"https://www.lesswrong.com/about\\\") community blog. Roko used ideas in [decision theory](\\\"https://www.lesswrong.com/tag/decision-theory\\\") to argue that a sufficiently powerful AI agent would have an incentive to torture anyone who imagined the agent but didn't work to bring the agent into existence. The argument was called a \\\\\"basilisk\\\\\" --named after the legendary reptile who can cause death with a single glance--because merely hearing the argument would supposedly put you at risk of torture from this hypothetical agent. A *basilisk* in this context is any information that harms or endangers the people who hear it.\n\nRoko's argument was broadly rejected on *Less Wrong*, with commenters objecting that an agent like the one Roko was describing would have no real reason to follow through on its threat: once the agent already exists, it will by default just see it as a waste of resources to torture people for their past decisions, since this doesn't causally further its plans. A number of decision algorithms can follow through on acausal threats and promises, via the same methods that permit mutual cooperation in prisoner's dilemmas; but this doesn't imply that such theories can be blackmailed. And following through on blackmail threats against such an algorithm additionally requires a large amount of shared information and trust between the agents, which does not appear to exist in the case of Roko's basilisk.\n\n*Less Wrong*'s founder, Eliezer Yudkowsky, banned discussion of Roko's basilisk on the blog for several years as part of a general site policy against spreading potential [information hazards](\\\"https://www.lesswrong.com/tag/information-hazards\\\"). This had the opposite of its intended effect: a number of outside websites began sharing information about Roko's basilisk, as the ban attracted attention to this taboo topic. Websites like RationalWiki spread the assumption that Roko's basilisk had been banned because *Less Wrong* users *accepted* the argument; thus many criticisms of *Less Wrong* cite Roko's basilisk as evidence that the site's users have unconventional and wrong-headed beliefs.\n\nBackground\n----------\n\n![](\\\"https://wiki.lesswrong.com/images/thumb/d/d1/Prisoner%27s_Dilemma_sequence.svg/380px-Prisoner%27s_Dilemma_sequence.svg.png\\\")A visual depiction of a prisoner's dilemma. T denotes the best outcome for a given player, followed by R, then P, then S.\n\nRoko's argument ties together two hotly debated academic topics: Newcomblike problems in decision theory, and normative uncertainty in moral philosophy.\n\nOne example of a Newcomblike problem is the prisoner's dilemma. This is a two-player game in which each player has two options: \\\\\"cooperate,\\\\\" or \\\\\"defect.\\\\\" By assumption, each player prefers to defect rather than cooperate, all else being equal; but each player also prefers mutual cooperation over mutual defection.\n\nFor example, we could imagine that if both players cooperate, then both get $10; and if both players defect, then both get $1; but if one player defects and the other cooperates, the defector gets $15 and the cooperator gets nothing. (We can equally well construct a prisoner's dilemma [for altruistic agents](\\\"https://lesswrong.com/lw/tn/the_true_prisoners_dilemma/\\\").)\n\nOne of the basic open problems in decision theory is that standard \\\\\"rational\\\\\" agents will end up defecting against each other, even though it would be better for both players if they could somehow enact a binding mutual agreement to cooperate instead.\n\nIn an extreme version of the prisoner's dilemma that draws out the strangeness of mutual defection, one can imagine that one is playing against an identical copy of oneself. Each copy knows that the two copies will play the same move; so the copies know that the only two possibilities are 'we both cooperate' or 'we both defect.' In this situation, cooperation is the better choice; yet causal decision theory (CDT), the most popular theory among working decision theorists, endorses mutual defection in this situation. This is because CDT tacitly assumes that the two agents' choices are independent. It notes that defection is the best option assuming my copy is already definitely going to defect, and that defection is also the best option assuming my copy is already definitely going to cooperate; so, since defection dominates, it defects.\n\nIn other words, the standard formulation of CDT cannot model scenarios where another agent (or a part of the environment) is correlated with a decision process, except insofar as the decision causes the correlation. The general name for scenarios where CDT fails is \\\\\"Newcomblike problems,\\\\\" and these scenarios are ubiquitous [in human interactions](\\\"https://lesswrong.com/lw/l1b/newcomblike_problems_are_the_norm/\\\").\n\nEliezer Yudkowsky proposed an alternative to CDT, [timeless decision theory](\\\"https://www.lesswrong.com/tag/timeless-decision-theory\\\") (TDT), that can achieve mutual cooperation in prisoner's dilemmas — provided both players are running TDT, and both players have common knowledge of this fact. The cryptographer Wei Dai subsequently developed a theory that outperforms both TDT and CDT, called [updateless decision theory](\\\"https://www.lesswrong.com/tag/updateless-decision-theory\\\") (UDT).\n\nYudkowsky's interest in decision theory stems from his interest in the [AI control problem](\\\"https://www.youtube.com/watch?v=pywF6ZzsghI\\\"): \\\\\"If artificially intelligent systems someday come to surpass humans in intelligence, how can we specify safe goals for them to autonomously carry out, and how can we gain high confidence in the agents' reasoning and decision-making?\\\\\" Yudkowsky has argued that in the absence of a full understanding of decision theory, we risk building autonomous systems whose behavior is erratic or difficult to model.\n\nThe control problem also raises questions in moral philosophy: how can we specify the goals of an autonomous agent in the face of human uncertainty about what it is we actually want; and how can we specify such goals in a way that allows for moral progress over time? Yudkowsky's term for a hypothetical algorithm that could autonomously pursue human goals in a way compatible with moral progress is [*coherent extrapolated volition*](\\\"https://www.lesswrong.com/tag/coherent-extrapolated-volition\\\").\n\nBecause Eliezer Yudkowsky founded *Less Wrong* and was one of the first bloggers on the site, AI theory and \\\\\"acausal\\\\\" decision theories — in particular, *logical* decision theories, which respect logical connections between agents' properties rather than just the causal effects they have on each other — have been repeatedly discussed on *Less Wrong*. Roko's basilisk was an attempt to use Yudkowsky's proposed decision theory (TDT) to argue against his informal characterization of an ideal AI goal (humanity's coherently extrapolated volition).\n\nRoko's post\n-----------\n\n![](\\\"https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/083971a1080c2784b9a65819394ccebabeddea96df05ea4e.jpeg\\\")A simple depiction of an agent that cooperates with copies of itself in the one-shot prisoner's dilemma. Adapted from the [Decision Theory FAQ](\\\"https://lesswrong.com/lw/gu1/decision_theory_faq/\\\").\n\nTwo agents that are running a logical decision theory can achieve mutual cooperation in a prisoner's dilemma even if there is no outside force mandating cooperation. Because their decisions take into account correlations that are not caused by either decision (though there is generally some common cause in the past), they can even cooperate if they are separated by large distances in space or time.\n\nRoko observed that if two TDT or UDT agents with common knowledge of each other's source code are separated in time, the later agent can (seemingly) blackmail the earlier agent. Call the earlier agent \\\\\"Alice\\\\\" and the later agent \\\\\"Bob.\\\\\" Bob can be an algorithm that outputs things Alice likes if Alice left Bob a large sum of money, and outputs things Alice dislikes otherwise. And since Alice knows Bob's source code *exactly*, she knows this fact about Bob (even though Bob hasn't been born yet). So Alice's knowledge of Bob's source code makes Bob's future threat effective, even though Bob doesn't yet exist: if Alice is certain that Bob will someday exist, then mere knowledge of what Bob *would* do if he could get away with it seems to force Alice to comply with his hypothetical demands.\n\nIf Bob ran CDT, then he would be unable to blackmail Alice. A CDT agent would assume that its decision is independent of Alice's and would not waste resources on rewarding or punishing a once-off decision *that has already happened*; and we are assuming that Alice could spot this fact by reading CDT-Bob's source code. A TDT or UDT agent, on the other hand, can recognize that Alice in effect has a copy of Bob's source code in her head (insofar as she is accurately modeling Bob), and that Alice's decision and Bob's decision are therefore correlated — the same as if two copies of the same source code were in a prisoner's dilemma.\n\nRoko raised this point in the context of debates about the possible behaviors and motivations of advanced AI systems. In a July 2010 *Less Wrong* post, Roko [wrote](\\\"http://rationalwiki.org/wiki/Roko%27s_basilisk/Original_post#Solutions_to_the_Altruist.27s_burden:_the_Quantum_Billionaire_Trick\\\"):\n\n> In this vein, there is the ominous possibility that if a positive singularity does occur, the resultant singleton may have precommitted to punish all potential donors who knew about existential risks but who didn't give 100% of their disposable incomes to x-risk motivation. This would act as an incentive to get people to donate more to reducing existential risk, and thereby increase the chances of a positive singularity. This seems to be what CEV (coherent extrapolated volition of humanity) might do if it were an acausal decision-maker.\n\n[*Singularity*](\\\"https://www.lesswrong.com/tag/singularity\\\") here refers to an [intelligence explosion](\\\"https://www.lesswrong.com/tag/intelligence-explosion\\\"), and [*singleton*](\\\"https://www.lesswrong.com/tag/singleton\\\") refers to a superintelligent AI system. Since a highly moral AI agent (one whose actions are consistent with our coherently extrapolated volition) would want to be created as soon as possible, Roko argued that such an AI would use acausal blackmail to give humans stronger incentives to create it. Roko made the claim that the hypothetical AI agent would particularly target people who had thought about this argument, because they would have a better chance of mentally simulating the AI's source code. Roko added: \\\\\"Of course this would be unjust, but is the kind of unjust thing that is oh-so-very [utilitarian](\\\"https://www.lesswrong.com/tag/utilitarianism\\\").\\\\\"\n\nRoko's conclusion from this was that we should never build any powerful AI agent that reasons like a utilitarian optimizing for humanity's coherently extrapolated values, because this would, paradoxically, be detrimental to human values.\n\nEliezer Yudkowsky has responded a few times to the substance of Roko's claims. E.g., in a 2014 Reddit thread, Yudkowsky [wrote](\\\"https://www.reddit.com/r/Futurology/comments/2cm2eg/rokos_basilisk/cjjbqqo\\\"):\n\n> What's the truth about Roko's Basilisk? The truth is that making something like this \\\\\"work\\\\\", in the sense of managing to think a thought that would actually give future superintelligences an incentive to hurt you, would require overcoming what seem to me like some pretty huge obstacles.\n> \n> The most blatant obstacle to Roko's Basilisk is, intuitively, that there's no incentive for a future agent to follow through with the threat in the future, because by doing so it just expends resources at no gain to itself. We can formalize that using classical causal decision theory, which is the academically standard decision theory: following through on a blackmail threat, in the future after the past has already taken place, cannot (from the blackmailing agent's perspective) be the physical cause of improved outcomes in the past, because the future cannot be the cause of the past.\n> \n> But classical causal decision theory isn't the only decision theory that has ever been invented, and if you were to read up on the academic literature, you would find a lot of challenges to the assertion that, e.g., two rational agents always defect against each other in the one-shot Prisoner's Dilemma.\n> \n> One of those challenges was a theory of my own invention, which is why this whole fiasco took place on LessWrong.com in the first place. (I feel rather like the speaker of that ancient quote, \\\\\"All my father ever wanted was to make a toaster you could really set the darkness on, and you perverted his work into these horrible machines!\\\\\") But there have actually been a lot of challenges like that in the literature, not just mine, as anyone actually investigating would have discovered. Lots of people are uncomfortable with the notion that rational agents always defect in the one-shot Prisoner's Dilemma. And if you formalize blackmail, including this case of blackmail, the same way, then most challenges to mutual defection in the Prisoner's Dilemma are also implicitly challenges to the first obvious reason why Roko's Basilisk would never work.\n> \n> But there are also other obstacles. The decision theory I proposed back in the day says that you have to know certain things about the other agent in order to achieve mutual cooperation in the Prisoner's Dilemma, and that's with both parties trying to set up a situation which leads to mutual cooperation instead of mutual defection. As I presently understand the situation, there is literally nobody on Earth, including me, who has the knowledge needed to set themselves up to be blackmailed if they were deliberately trying to make that happen.\n> \n> Any potentially blackmailing AI would much prefer to have you believe that it is blackmailing you, without actually expending resources on following through with the blackmail, insofar as they think they can exert any control on you at all via an exotic decision theory. Just like in the one-shot Prisoner's Dilemma the \\\\\"ideal\\\\\" outcome is for the other player to believe you are modeling them and will cooperate if and only if they cooperate, and so they cooperate, but then actually you just defect anyway. For the other player to be confident this will not happen in the Prisoner's Dilemma, for them to expect you not to sneakily defect anyway, they must have some very strong knowledge about you. In the case of Roko's Basilisk, \\\\\"defection\\\\\" corresponds to not actually torturing anyone, not expending resources on that, and just letting them believe that you will blackmail them. Two AI agents with sufficiently strong knowledge of each other, and heavily motivated to achieve mutual cooperation on the Prisoner's Dilemma, might be able to overcome this obstacle and cooperate with confidence. But why would you put in that degree of effort — if you even could, which I don't think you as a human can — in order to give a blackmailing agent an incentive to actually carry through on its threats?\n> \n> I have written the above with some reluctance, because even if I don't yet see a way to repair this obstacle myself, somebody else might see how to repair it now that I've said what it is. Which is not a good general procedure for handling infohazards; people with expert knowledge on them should, obviously, as a matter of professional ethics, just never discuss them at all, including describing why a particular proposal doesn't work, just in case there's some unforeseen clever way to repair the proposal. There are other obstacles here which I am not discussing, just in case the logic I described above has a flaw. Nonetheless, so far as I know, Roko's Basilisk does not work, nobody has actually been bitten by it, and everything I have done was in the service of what I thought was the obvious Good General Procedure for Handling Potential Infohazards\\[.\\]\n\nOther users on *Less Wrong* generally rejected Roko's arguments at the time, and skepticism about his supposed basilisk appears to have only increased with time. Subsequent discussion of Roko's basilisk has focused on *Less Wrong* moderator responses to Roko's post, rather than on the specific merits or dismerits of his argument.\n\nTopic moderation and response\n-----------------------------\n\nShortly after Roko made his blog post, Yudkowsky left an angry [comment](\\\"http://rationalwiki.org/wiki/Roko%27s_basilisk/Original_post#Comments_.28117.29\\\") on the discussion thread:\n\n> \\[Roko:\\] \\\\\"One might think that the possibility of CEV punishing people couldn't possibly be taken seriously enough by anyone to actually motivate them. But in fact one person at SIAI was severely worried by this, to the point of having terrible nightmares, though ve wishes to remain anonymous.\\\\\"\n> \n> I don't usually talk like this, but I'm going to make an exception for this case.\n> \n> Listen to me very closely, you idiot.\n> \n> YOU DO NOT THINK IN SUFFICIENT DETAIL ABOUT SUPERINTELLIGENCES CONSIDERING WHETHER OR NOT TO BLACKMAIL YOU. THAT IS THE ONLY POSSIBLE THING WHICH GIVES THEM A MOTIVE TO FOLLOW THROUGH ON THE BLACKMAIL.\n> \n> There's an obvious equilibrium to this problem where you engage in all positive acausal trades and ignore all attempts at acausal blackmail. Until we have a better worked-out version of TDT and we can prove that formally, it should just be OBVIOUS that you DO NOT THINK ABOUT DISTANT BLACKMAILERS in SUFFICIENT DETAIL that they have a motive toACTUALLY \\[*sic*\\] BLACKMAIL YOU.\n> \n> If there is any part of this acausal trade that is positive-sum and actually worth doing, that is exactly the sort of thing you leave up to an FAI. We probably also have the FAI take actions that cancel out the impact of anyone motivated by true rather than imagined blackmail, so as to obliterate the motive of any superintelligences to engage in blackmail.\n> \n> Meanwhile I'm banning this post so that it doesn't (a) give people horrible nightmares and (b) give distant superintelligences a motive to follow through on blackmail against people dumb enough to think about them in sufficient detail, though, thankfully, I doubt anyone dumb enough to do this knows the sufficient detail. (I'm not sure I know the sufficient detail.)\n> \n> You have to be really clever to come up with a genuinely dangerous thought. I am disheartened that people can be clever enough to do that and not clever enough to do the obvious thing and KEEP THEIR IDIOT MOUTHS SHUT about it, because it is much more important to sound intelligent when talking to your friends. This post was STUPID.\n> \n> (For those who have no idea why I'm using capital letters for something that just sounds like a random crazy idea, and worry that it means I'm as crazy as Roko, the gist of it was that he just did something that potentially gives superintelligences an increased motive to do extremely evil things in an attempt to blackmail us. It is the sort of thing you want to be EXTREMELY CONSERVATIVE about NOT DOING.)\n\n\\\\\"FAI\\\\\" here stands for \\\\\"Friendly AI,\\\\\" a hypothetical superintelligent AI agent that can be trusted to autonomously promote desirable ends. Yudkowsky rejected the idea that Roko's basilisk could be called \\\\\"friendly\\\\\" or \\\\\"utilitarian,\\\\\" since torture and threats of blackmail are themselves contrary to common human values. Separately, Yudkowsky doubted that humans possessed enough information about any hypothetical unfriendly AI system to enter Alice's position even if we tried. Yudkowsky additionally argued that a *well-designed* version of Alice would precommit to resisting blackmail from Bob, while still accepting positive-sum [acausal trades](\\\"https://www.lesswrong.com/tag/acausal-trade\\\") (e.g., ordinary contracts).\n\nYudkowsky proceeded to delete Roko's post and the ensuing discussion, while banning further discussion of the topic on the blog. A few months later, an anonymous editor [added](\\\"http://rationalwiki.org/w/index.php?title=LessWrong&diff=656172&oldid=647467\\\") a discussion of Roko's basilisk to an article covering *Less Wrong*. The editor inferred from Yudkowsky's comments that people on *Less Wrong* accepted Roko's argument:\n\n> There is apparently a idea so horrible, so utterly Cuthulian (sic) in nature that it needs to be censored for our sanity. Simply knowing about it makes it more likely of becoming true in the real world. Elizer Yudkwosky and the other great rationalist keep us safe by deleting any posts with this one evil idea. Yes they really do believe that. Occasionally a poster will complain off topic about the idea being deleted.\n\nOver time, RationalWiki's Roko's basilisk discussion expanded into its own article. Editors had [difficulty](\\\"https://www.reddit.com/r/xkcd/comments/2myg86/xkcd_1450_aibox_experiment/cm8vn6e\\\") interpreting Roko's reasoning, thinking that Roko's argument was intended to promote Yudkowsky's AI program rather than to criticize it. Since discussion of the topic was still banned on *Less Wrong*, the main source for information about the incident continued to be the coverage on RationalWiki for several years. As a further consequence of the ban, no explanations were given about the details of Roko's argument or the views of *Less Wrong* users. This generated a number of criticisms of *Less Wrong*'s forum moderation policies.\n\nInterest in the topic increased over subsequent years. In 2014, Roko's basilisk was name-dropped in the webcomic [*xkcd*](\\\"https://xkcd.com/1450/\\\"). The magazine *Slate* ran an article on the thought experiment, titled \\\\\"[The Most Terrifying Thought Experiment of All Time](\\\"http://www.slate.com/articles/technology/bitwise/2014/07/roko_s_basilisk_the_most_terrifying_thought_experiment_of_all_time.single.html\\\")\\\\\":\n\n> You may be wondering why this is such a big deal for the LessWrong people, given the apparently far-fetched nature of the thought experiment. It’s not that Roko’s Basilisk will necessarily materialize, or is even likely to. It’s more that if you’ve committed yourself to timeless decision theory, then thinking about this sort of trade literally makes it more likely to happen. After all, if Roko’s Basilisk were to see that this sort of blackmail gets you to help it come into existence, then it would, as a rational actor, blackmail you. The problem isn’t with the Basilisk itself, but with you. Yudkowsky doesn’t censor every mention of Roko’s Basilisk because he believes it exists or will exist, but because he believes that the idea of the Basilisk (and the ideas behind it) is dangerous.\n> \n> Now, Roko’s Basilisk is only dangerous if you believe all of the above preconditions and commit to making the two-box deal \\[*sic*\\] with the Basilisk. But at least some of the LessWrong members do believe all of the above, which makes Roko’s Basilisk quite literally forbidden knowledge. \\[...\\]\n> \n> If you do not subscribe to the theories that underlie Roko’s Basilisk and thus feel no temptation to bow down to your once and future evil machine overlord, then Roko’s Basilisk poses you no threat. (It is ironic that it’s only a mental health risk to those who have already bought into Yudkowsky’s thinking.) Believing in Roko’s Basilisk may simply be a “referendum on autism,” as a friend put it.\n\nOther sources have repeated the claim that *Less Wrong* users think Roko's basilisk is a serious concern. However, none of these sources have yet cited supporting evidence on this point, aside from *Less Wrong* moderation activity itself. (The ban, of course, didn't make it easy to collect good information.)\n\n*Less Wrong* user Gwern [reports](\\\"https://www.reddit.com/r/LessWrong/comments/17y819/lw_uncensored_thread/c8bbcy4\\\") that \\\\\"Only a few LWers seem to take the basilisk very seriously,\\\\\" adding, \\\\\"It's funny how everyone seems to know all about who is affected by the Basilisk and how exactly, when they don't know any such people and they're talking to counterexamples to their confident claims.\\\\\"\n\nYudkowsky subsequently went into more detail about his thought processes [on Reddit](\\\"https://www.reddit.com/r/Futurology/comments/2cm2eg/rokos_basilisk/cjjbqqo\\\"):\n\n> When Roko posted about the Basilisk, I very foolishly yelled at him, called him an idiot, and then deleted the post.\n> \n> Why I did that is not something you have direct access to, and thus you should be careful about Making Stuff Up, especially when there are Internet trolls who are happy to tell you in a loud authoritative voice what I was thinking, despite having never passed anything even close to an Ideological Turing Test on Eliezer Yudkowsky.\n> \n> Why I yelled at Roko: Because I was caught flatfooted in surprise, because I was indignant to the point of genuine emotional shock, at the concept that somebody who thought they'd invented a brilliant idea that would cause future AIs to torture people who had the thought, had promptly posted it to the public Internet. In the course of yelling at Roko to explain why this was a bad thing, I made the further error — keeping in mind that I had absolutely no idea that any of this would ever blow up the way it did, if I had I would obviously have kept my fingers quiescent — of not making it absolutely clear using lengthy disclaimers that my yelling did not mean that I believed Roko was right about CEV-based agents torturing people who had heard about Roko's idea. It was obvious to me that no CEV-based agent would ever do that and equally obvious to me that the part about CEV was just a red herring; I more or less automatically pruned it from my processing of the suggestion and automatically generalized it to cover the entire class of similar scenarios and variants, variants which I considered obvious despite significant divergences (I forgot that other people were not professionals in the field). This class of all possible variants did strike me as potentially dangerous as a collective group, even though it did not occur to me that Roko's original scenario might be right — that was obviously wrong, so my brain automatically generalized it. \\[...\\]\n> \n> What I considered to be obvious common sense was that you did not spread potential information hazards because it would be a crappy thing to do to someone. The problem wasn't Roko's post itself, about CEV, being correct. That thought never occurred to me for a fraction of a second. The problem was that Roko's post seemed near in idea-space to a large class of potential hazards, all of which, regardless of their plausibility, had the property that they presented no potential benefit to anyone. They were pure infohazards. The only thing they could possibly do was be detrimental to brains that represented them, if one of the possible variants of the idea turned out to be repairable of the obvious objections and defeaters. So I deleted it, because on my worldview there was no reason not to. I did not want LessWrong.com to be a place where people were exposed to potential infohazards because somebody like me thought they were being clever about reasoning that they probably weren't infohazards. On my view, the key fact about Roko's Basilisk wasn't that it was plausible, or implausible, the key fact was just that shoving it in people's faces seemed like a fundamentally crap thing to do because there was no upside.\n> \n> Again, I deleted that post not because I had decided that this thing probably presented a real hazard, but because I was afraid some unknown variant of it might, and because it seemed to me like the obvious General Procedure For Handling Things That Might Be Infohazards said you shouldn't post them to the Internet. If you look at the original SF story where the term \\\\\"basilisk\\\\\" was coined, it's about a mind-erasing image and the.... trolls, I guess, though the story predates modern trolling, who go around spraypainting the Basilisk on walls, using computer guidance so they don't know themselves what the Basilisk looks like, in hopes the Basilisk will erase some innocent mind, for the lulz. These people are the villains of the story. The good guys, of course, try to erase the Basilisk from the walls. Painting Basilisks on walls is a crap thing to do. Since there was no upside to being exposed to Roko's Basilisk, its probability of being true was irrelevant. And Roko himself had thought this was a thing that might actually work. So I yelled at Roko for violating basic sanity about infohazards for stupid reasons, and then deleted the post. He, by his own lights, had violated the obvious code for the ethical handling of infohazards, conditional on such things existing, and I was indignant about this.\n\nBig-picture questions\n---------------------\n\nSeveral other questions are raised by Roko's basilisk, beyond the merits of Roko's original argument or *Less Wrong*'s moderation policies:\n\n*   Can formal decision agents be designed to resist blackmail?\n*   Are information hazards a serious risk, and are there better ways of handling them?\n*   Does the oversimplified coverage of Roko's argument suggest that \\\\\"weird\\\\\" philosophical topics are big liabilities for pedagogical or research-related activities?\n\n**Blackmail-resistant decision theories**\n\nThe general ability to cooperate in prisoner's dilemmas appears to be useful. If other agents know that you won't betray them as soon as it's in your best interest to do so — if you've made a promise or signed a contract, and they know that you can be trusted to stick to such agreements even in the absence of coercion — then a large number of mutually beneficial transactions will be possible. If there is some way for agents to acquire evidence about each other's trustworthiness, then the more trustworthy agents will benefit.\n\nAt the same time, introducing new opportunities for contracts and collaborations introduces new opportunities for blackmail. An agent that can pre-commit to following through on a promise (even when this is no longer in its short-term interest) can also pre-commit to following through on a costly threat.\n\nIt appears that the best general-purpose response is to credibly precommit to never giving in to any blackmailer's demands (even when there are short-term advantages to doing so). This makes it much likelier that one will never be blackmailed in the first place, just as credibly precommitting to stick to trade agreements (even when there are short-term *disadvantages* to doing so) makes it much likelier that one *will* be approached as a trading partner.\n\nOne way to generalize this point is to adopt the [rule of thumb](\\\"https://forum.intelligence.org/item?id=160\\\") of behaving in whatever way is recommended by the most generally useful policy. This is the distinguishing feature of [the most popular version](\\\"https://lesswrong.com/lw/1s5/explicit_optimization_of_global_strategy_fixing_a/\\\") of UDT. Standard UDT selects the best available policy (mapping of observations to actions) rather than the best available action. In this way, UDT avoids selecting a strategy that other agents will have an especially easy time manipulating. UDT itself, however, is not fully formalized, and there may be some superior decision theory. No one has yet formally solved decision theory, or the particular problem of defining a blackmail-free equilibrium.\n\nIt hasn't been formally demonstrated that any logical decision theories give in to blackmail, or what scenarios would make them vulnerable to blackmail. If it turned out that TDT or UDT were blackmailable, this would suggest that they aren't normatively optimal decision theories. For more background on open problems in decision theory, see the [Decision Theory FAQ](\\\"https://lesswrong.com/lw/gu1/decision_theory_faq/\\\") and \\\\\"[Toward Idealized Decision Theory](\\\"https://intelligence.org/files/TowardIdealizedDecisionTheory.pdf\\\")\\\\\".\n\n**Utility function inverters**\n\nBecause the basilisk threatens its blackmail targets with torture, it is a type of \\\\\"utility function inverter\\\\\": agents that seek to additionally pressure others by threatening to invert the non-compliant party's utility function. [Yudkowsky argues](https://www.lesswrong.com/posts/brXr7PJ2W4Na2EW2q/the-commitment-races-problem?commentId=tYBPjetgZW4iMqe4s) that sane, rational entities ought to be strongly opposed to utility function inverters by dint of not wanting to live in a reality where such tactics are commonly part of negotiations, though Yudkowsky did so as a comment about the irrationality of commitment races, not about Roko's basilisk:\n\n> IMO, commitment races only occur between agents who will, in some sense, act like idiots, if presented with an apparently 'committed' agent.  If somebody demands $6 from me in the Ultimatum game, threatening to leave us both with $0 unless I offer at least $6 to them... then I offer $6 with slightly less than 5/6 probability, so they do no better than if they demanded $5, the amount I think is fair.  They cannot evade that by trying to make some 'commitment' earlier than I do.  I expect that, whatever is the correct and sane version of this reasoning, it generalizes across all the cases.\n> \n> I am not locked into warfare with things that demand $6 instead of $5.  I do not go around figuring out how to invert their utility function for purposes of threatening them back - 'destroy all utility-function inverters (but do not invert their own utility functions)' was my guessed commandment that would be taught to kids in dath ilan, because you don't want reality to end up full of utilityfunction inverters.\n\n**Information hazards**\n\nDavid Langford coined the term *basilisk*, in the sense of an information hazard that directly harms anyone who perceives it, in the 1988 science fiction story \\\\\"[BLIT](\\\"https://en.wikipedia.org/wiki/BLIT_(short_story)\\\").\\\\\" On a societal level, examples of real-world information hazards include the dissemination of specifications for dangerous technologies; on an individual level, examples include triggers for stress or anxiety disorders.\n\nThe Roko's basilisk incident suggests that information that is deemed dangerous or taboo is more likely to be spread rapidly. Parallels can be drawn to [shock site](\\\"https://en.wikipedia.org/wiki/Shock_site\\\") and [creepypasta](\\\"https://en.wikipedia.org/wiki/Creepypasta\\\") links: many people have their interest piqued by such topics, and people also enjoy pranking each other by spreading purportedly harmful links. Although Roko's basilisk was never genuinely dangerous, real information hazards might propagate in a similar way, especially if the risks are non-obvious.\n\nNon-specialists spread Roko's argument widely without first investigating the associated risks and benefits in any serious way. One take-away is that someone in possession of a serious information hazard should exercise caution in visibly censoring or suppressing it (cf. the [Streisand effect](\\\"https://en.wikipedia.org/wiki/Streisand_effect\\\")). \\\\\"[Information Hazards: A Typology of Potential Harms from Knowledge](\\\"https://www.nickbostrom.com/information-hazards.pdf\\\")\\\\\" notes: \\\\\"In many cases, the best response is no response, i.e., to proceed as though no such hazard existed.\\\\\" This also means that additional care may need to be taken in keeping risky information under wraps; retracting information that has been published to highly trafficked websites is often difficult or impossible. However, Roko's basilisk is an isolated incident (and an unusual one at that); it may not be possible to draw any strong conclusions without looking at a number of other examples.\n\n**\\\\\"Weirdness points\\\\\"**\n\nPeter Hurford argues in \\\\\"[You Have a Set Amount of Weirdness Points; Spend Them Wisely](\\\"http://effective-altruism.com/ea/bg/you_have_a_set_amount_of_weirdness_points_spend/\\\")\\\\\" that promoting or talking about too many nonstandard ideas simultaneously makes it much less likely that any one of the ideas will be taken seriously. Advocating for any one of veganism, anarchism, or mind-body dualism is difficult enough on its own; discussing all three at once increases the odds that a skeptical interlocutor will write you off as 'just generally prone to having weird beliefs.' Roko's basilisk appears to be an example of this phenomenon: long-term AI safety issues, acausal trade, and a number of other popular *Less Wrong* ideas are all highly unusual in their own right, and their combination is stranger than the sum of its parts.\n\nOn the other hand, [Ozy Frantz argues](\\\"https://thingofthings.wordpress.com/2015/04/14/on-weird-points/\\\") that looking weird can attract an audience that is open to new and unconventional ideas:\n\n> \\[I\\]magine that you mostly endorse positions that your audience already agrees with, positions that are within a standard deviation of the median position on the issue, and then you finally gather up all your cherished, saved-up weirdness points and write a passionate defense of the importance of insect suffering. How do you think your audience is going to react? \\\\\"Ugh, they used to be so normal, and then it was like they suddenly went crazy. I hope they go back to bashing the Rethuglicans soon.\\\\\"\n\nIn \\\\\"[The Economy of Weirdness](\\\"https://meteuphoric.wordpress.com/2015/03/08/the-economy-of-weirdness/\\\"),\\\\\" Katja Grace paints a more complicated painting of the advantages and disadvantages of weirdness. Communities with different goals and different demographics will plausibly vary in how 'normal' they should try to look, and in what the relevant kind of normality is. E.g., if the goal is to get more people interested in AI control problems, then weird ideas like Roko's basilisk may drive away conventional theoretical computer scientists, but they may also attract people who favor (or are indifferent to) unorthodox ideas.\n\nSee also\n--------\n\n*   [Decision Theory Readings](\\\"https://intelligence.org/research-guide/#four\\\") in MIRI's Research Guide\n*   [Causal Decision Theory](\\\"https://plato.stanford.edu/entries/decision-causal/\\\") in the *Stanford Encyclopedia of Philosophy*\n*   [Thinking Inside the Boxes](\\\"https://www.slate.com/articles/arts/egghead/2002/02/thinkinginside_the_boxes.html\\\") in *Slate*\n*   [Newcomb's problem](\\\"https://www.lesswrong.com/tag/newcomb-s-problem\\\")\n*   [Parfit's hitchhiker](\\\"https://wiki.lesswrong.com/wiki/Parfit's_hitchhiker\\\")\n*   [Acausal Trade](\\\"https://www.lesswrong.com/tag/acausal-trade\\\")"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb2fb",
    "name": "Adversarial Collaboration",
    "core": null,
    "slug": "adversarial-collaboration",
    "oldSlugs": null,
    "postCount": 3,
    "description": {
      "markdown": "**Adversarial collaboration** is a protocol developed by Daniel Kahneman for two researchers advocating competing hypotheses to collaborate on a research project with the goal of resolving their differences, designed on the assumption that this will be more effective than each researcher conducting their own experiments individually and publishing replies to each others' papers. Kahneman tested adversarial collaboration with Ralph Hertwig, aiming to resolve their dispute about whether the [conjunction fallacy](https://www.lesswrong.com/tag/conjunction-fallacy) was primarily due to the [representativeness heuristic](https://www.lesswrong.com/tag/representativeness-heuristic) (as advocated by Kahneman), or simply due to subjects interpreting the word \"and\" as a disjunction where the experimenters intended it to be used as a conjunction (as advocated by Hertwig). Kahneman and Hertwig collaborated on a series of experiments related to the issue, along with Barbara Mellers as an arbiter, and further refined the suggested adversarial collaboration protocol based on their experiences.\n\nThey suggest that the two disputing researchers should team up, along with a neutral arbiter to settle disputes that arise in the process, and agree on a procedure for an experiment that would distinguish between their hypotheses. The researchers would discuss ahead of time what results each of them expects, and what sorts of results would lead them to change their minds. Since the initial experiment may be inconclusive, each side would be allowed to propose follow-up studies afterwards, which they would continue to collaborate on. After the study is complete, all three researchers involved would collaborate on a paper describing the results, with the arbiter having responsibility for some sections, and each of the two disputing researchers given the chance to describe their own interpretation of the results.\n\nScott Alexander has arranged adversarial collaboration contests on his his blog, where people write an essay together.\n\n[2019](https://slatestarcodex.com/2020/01/13/2019-adversarial-collaboration-winners/):\n\n*   [What are the benefits, harms, and ethics of infant circumcision?](https://slatestarcodex.com/2019/12/10/acc-is-infant-circumcision-ethical/) by Joel P and Missingno\n*   [Is eating meat a net harm?](https://slatestarcodex.com/2019/12/11/acc-is-eating-meat-a-net-harm/) by David G and Froolow\n*   [Does calorie restriction slow aging?](https://slatestarcodex.com/2019/12/12/acc-does-calorie-restriction-slow-aging/) by Adrian L and Calvin R\n*   [Should we colonize space to mitigate x-risk?](https://slatestarcodex.com/2019/12/17/acc-should-we-colonize-space-to-mitigate-x-risk/) by Nick D and Rob S\n*   [Should gene editing technologies be used in humans](https://slatestarcodex.com/2019/12/18/acc-should-gene-editing-technologies-be-used-in-humans/) by Nita J and Patrick N\n*   [When during fetal development does abortion become morally wrong?](https://slatestarcodex.com/2019/12/19/acc-when-during-fetal-development-does-abortion-become-morally-wrong/) by BlockOfNihilism and Icerun\n*   [Will automation lead to economic crisis?](https://slatestarcodex.com/2019/12/23/acc-will-automation-lead-to-economic-crisis/) by Doug S and Erusian\n*   [How much significance should we ascribe to spiritual experiences?](https://slatestarcodex.com/2019/12/25/acc-how-much-significance-should-we-ascribe-to-spiritual-experiences/) by Seth S and Jeremiah G\n\n[2018](https://slatestarcodex.com/2018/09/26/adversarial-collaboration-contest-results/):\n\n*   [Does the current US education system adequately serve advanced students?](https://slatestarcodex.com/2018/09/04/acc-entry-does-the-education-system-adequately-serve-advanced-students/) by Michael Pershan and TracingWoodgrains\n*   [Is Islam compatible with liberal democracy?](https://slatestarcodex.com/2018/09/05/acc-entry-are-islam-and-liberal-democracy-compatible/) by John Buridan and Christian Flanery\n*   [Should childhood vaccination be mandatory?](https://slatestarcodex.com/2018/09/06/acc-entry-should-childhood-vaccination-be-mandatory/) by Mark Davis and Mark Webb\n*   [Should children who identify as transgender start transitioning?](https://slatestarcodex.com/2018/09/08/acc-entry-should-transgender-children-transition/) by a_reader and flame7926\n\nExternal links\n--------------\n\n[http://web.cenet.org.cn/upfile/21290.pdf](http://web.cenet.org.cn/upfile/21290.pdf) (the protocol is summarized in table 1, at the top of page 2)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb2fa",
    "name": "Personal Identity",
    "core": null,
    "slug": "personal-identity",
    "oldSlugs": null,
    "postCount": 21,
    "description": {
      "markdown": "Personal identity is the concept that two configurations of particles (or computations, etc), each of which is a person, can be in some sense the same person. For example, you-right-now and you-yesterday are usually considered two instances of the person “you”, instead of two different people, even though they are distinct configurations of particles.\n\nPhilosophers have proposed many theories of personal identity, relying on various attributes like the two configurations being made from the same atoms, there being a particular causal relationship between the two configurations, there being a single extra-physical soul appearing in both configurations, the two configurations being sufficiently similar, personal identity not actually existing, and pretty much anything else you can think of.\n\nThe problem used to appear fairly straightforward, since no one had even considered the possibility that you could do things like create a copy of a person and run them on a computer. There were no boundary cases to suggest that our naïve intuitions about personal identity might be misguided. However, now that technological and scientific advances have suggested boundary cases to consider, these boundary cases give us opportunities for different theories of personal identity to disagree.\n\nAs well as suggesting boundary cases with which to differentiate different theories of personal identity, modern science also gives us some clues as to which theories are correct. For instance, evidence from neuroscience suggests that cognition is entirely physical, which contradicts theories of personal identity that rely on an extra-physical soul. Experiments from quantum mechanics show that particles don't actually have individual identities; that is, if there are two electrons at time 1 and two electrons at time 2, there does not exist any fact of the matter as to which electron at time 1 is the same as which electron at time 2. This rules out theories of personal identity based on being made of the same atoms.\n\nPersonal identity may at first sound like just an abstract philosophical issue with no practical consequences, but in fact, there are practical reasons to understand personal identity. For instance, common objections to [cryonics](https://www.lesswrong.com/tag/cryonics) and [brain uploading](https://www.lesswrong.com/tag/whole-brain-emulation) hold that anyone who is woken up from cryonic suspension or whose brain is run on a computer would not be the same person they were before the operation, and that the operations thus fail to continue the person's life. Such objections are generally based on theories of personal identity that can be shown to be false or incoherent by modern science, as explained in the sequence on quantum mechanics and personal identity. It is already possible to sign up for cryonics, and whole brain emulation may be possible in the future, so it is actually possible to act on an understanding of personal identity. Once whole brain emulation is feasible, it would also be possible to easily copy and modify brain emulations, which would offer more challenging questions about personal identity.\n\nSequences\n---------\n\n*   [quantum mechanics and personal identity](http://lesswrong.com/lw/r9/quantum_mechanics_and_personal identity)\n\nRelated Pages\n-------------\n\n*   [Identity](https://www.lesswrong.com/tag/identity)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb2d4",
    "name": "Reinforcement Learning",
    "core": null,
    "slug": "reinforcement-learning",
    "oldSlugs": null,
    "postCount": 85,
    "description": {
      "markdown": "Within the field of Machine Learning, **reinforcement learning** refers to the study of how an agent should choose its actions within an environment in order to maximize some kind of reward. Strongly inspired by the work developed in behavioral psychology it is essentially a trial and error approach to find the best strategy.\n\nRelated: [Inverse Reinforcement Learning](/tag/inverse-reinforcement-learning), [Machine learning](https://www.lesswrong.com/tag/machine-learning), [Friendly AI](https://wiki.lesswrong.com/wiki/Friendly_AI), [Game Theory](https://www.lesswrong.com/tag/game-theory), [Prediction](https://www.lesswrong.com/tag/forecasting-and-prediction)\n\nConsider an agent that receives an input informing the agent of the environment's state. Based only on that information, the agent has to make a decision regarding which action to take, from a set, which will influence the state of the environment. This action will in itself change the state of the environment, which will result in a new input, and so on, each time also presenting the agent with the reward relative to its actions in the environment. The agent's goal is then to find the ideal strategy which will give the highest reward expectations over time, based on previous experience.\n\nExploration and Optimization\n----------------------------\n\nKnowing that randomly selecting the actions will result in poor performances, one of the biggest problems in reinforcement learning is *exploring* the avaliable set of responses to avoid getting stuck in sub-optimal choices and proceed to better ones.\n\nThis is the problem of exploration, which is best described in the most studied reinforcement learning problem - [the k-armed bandit](http://en.wikipedia.org/wiki/Multi-armed_bandit). In it, an agent has to decide which sequence of levers to pull in a gambling room, not having any information about the probabilities of winning in each machine besides the reward it receives each time. The problem revolves about deciding which is the optimal lever and what criteria defines the lever as such.\n\nParallel with an exploration implementation, it is still necessary to chose the criteria which makes a certain action optimal when compared to another. This study of this property has led to several methods, from brute forcing to taking into account temporal differences in the received reward. Despite this and the great results obtained by reinforcement methods in solving small problems, it suffers from a lack of scalability, having difficulties solving larger, close-to-human scenarios.\n\nFurther Reading & References\n----------------------------\n\n*   Sutton, Richard S.; Barto, Andrew G. (1998). [Reinforcement Learning: An Introduction](http://129.2.53.113/~poeppel/dp_papers/ivry_rev.pdf). MIT Press. ISBN 0-262-19398-1.\n*   Kaelbling, L. P. , Littman, M. L. , Moore, A. W. (1996). [Reinforcement Learning: A Survey](http://arxiv.org/pdf/cs/9605103v1.pdf). Journal of Artificial Intelligence Research, Vol 4, (1996), 237-285\n\nSee Also\n--------\n\n*   [Machine learning](machine-learning)\n*   [Friendly AI](https://wiki.lesswrong.com/wiki/Friendly_AI)\n*   [Game theory](game-theory)\n*   [Prediction](https://wiki.lesswrong.com/wiki/Prediction)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb2c7",
    "name": "Regulation and AI Risk",
    "core": null,
    "slug": "regulation-and-ai-risk",
    "oldSlugs": null,
    "postCount": 13,
    "description": {
      "markdown": "**Regulation and AI risk** is the debate on whether regulation could be used to reduce the risks of [Unfriendly AI](https://www.lesswrong.com/tag/unfriendly-artificial-intelligence), and what forms of regulation would be appropriate.\n\nSeveral authors have advocated AI research to be regulated, but been vague on the details. Yampolskiy & Fox (2012) note that university research programs in the social and medical sciences are overseen by institutional review boards, and propose setting up analogous review boards to evaluate potential AGI research. In order to be successful, AI regulation would have to be global, and there is the potential for an [AI arms race](https://www.lesswrong.com/tag/ai-arms-race) between different nations. Partially because of this, McGinnis (2010) argues that the government should not attempt to regulate AGI development. Rather, it should concentrate on providing funding to research projects intended to create safe AGI. Kaushal & Nolan (2015) point out that regulations on AGI development would result in a speed advantage for any project willing to skirt the regulations, and instead propose government funding (possibly in the form of an \"AI Manhattan Project\") for AGI projects meeting particular criteria.\n\nWhile Shulman & Armstrong (2009) argue the unprecedentedly destabilizing effect of AGI could be a cause for world leaders to cooperate more than usual, the opposite argument can be made as well. Gubrud (1997) argues that molecular nanotechnology could make countries more self-reliant and international cooperation considerably harder, and that AGI could contribute to such a development. AGI technology is also much harder to detect than e.g. nuclear technology is - AGI research can be done in a garage, while nuclear weapons require a substantial infrastructure (McGinnis 2010). On the other hand, Scherer (2015) argues that artificial intelligence could nevertheless be susceptible to regulation due to the increasing prominence of governmental entities and large corporations in AI research and development.\n\nGoertzel & Pitt (2012) suggest that for regulation to be enacted, there might need to be an [AGI Sputnik moment](https://www.lesswrong.com/tag/agi-sputnik-moment) \\- a technological achievement that makes the possibility of AGI evident to the public and policy makers. They note that after such a moment, it might not take a very long time for full human-level AGI to be developed, while the negotiations required to enact new kinds of arms control treaties would take considerably longer.\n\nReferences\n----------\n\n*   Ben Goertzel & Joel Pitt (2012): [Nine Ways to Bias Open-Source AGI Toward Friendliness](http://jetpress.org/v22/goertzel-pitt.htm). Journal of Evolution and Technology - Vol. 22 Issue 1 – pgs 116-141.\n*   Mark Gubrud (1997): [Nanotechnology and International Security](http://www.foresight.org/Conferences/MNT05/Papers/Gubrud/). Fifth Foresight Conference on Molecular Nanotechnology.\n*   John McGinnis (2010): [Accelerating AI](http://papers.ssrn.com/sol3/papers.cfm?abstract_id=1593851). Northwestern University Law Review.\n*   Matthew Scherer (2015): [Regulating Artificial Intelligence Systems: Risks, Challenges, Competencies, and Strategies](http://papers.ssrn.com/abstract=2609777). Harvard Journal of Law & Technology.\n*   Carl Shulman & Stuart Armstrong (2009): [Arms control and intelligence explosions](http://intelligence.org/files/ArmsControl.pdf). European Conference on Computing and Philosophy.\n*   Roman Yampolskiy & Joshua Fox (2012): [Safety Engineering for Artificial General Intelligence](http://intelligence.org/files/SafetyEngineering.pdf). Topoi.\n*   Mohit Kaushal & Scott Nolan (2015): [Understanding Artificial Intelligence](http://www.brookings.edu/blogs/techtank/posts/2015/04/14-understanding-artificial-intelligence). Brookings.\n\nSee also\n--------\n\n*   [AI arms race](https://www.lesswrong.com/tag/ai-arms-race)\n*   [AGI Sputnik moment](https://www.lesswrong.com/tag/agi-sputnik-moment)\n*   [Existential risk](https://www.lesswrong.com/tag/existential-risk)\n*   [Unfriendly artificial intelligence](https://www.lesswrong.com/tag/unfriendly-artificial-intelligence)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb2c2",
    "name": "Ontological Crisis",
    "core": null,
    "slug": "ontological-crisis",
    "oldSlugs": null,
    "postCount": 11,
    "description": {
      "markdown": "**Ontological crisis** is a term coined to describe the crisis an agent, human or not, goes through when its model - its ontology - of reality changes.\n\nIn the human context, a clear example of an ontological crisis is a believer’s loss of faith in God. Their motivations and goals, coming from a very specific view of life suddenly become obsolete and maybe even nonsense in the face of this new configuration. The person will then experience a deep crisis and go through the psychological task of reconstructing its set of preferences according the new world view.\n\nWhen dealing with artificial agents, we, as their creators, are directly interested in their goals. That is, as Peter de Blanc puts it, when we create something we want it to be useful. As such we will have to define the artificial agent’s ontology – but since a fixed ontology severely limits its usefulness we have to think about adaptability. In his 2011 paper, the author then proposes a method to map old ontologies into new ones, thus adapting the agent’s utility functions and avoiding a crisis.\n\nThis crisis, in the context of an [AGI](https://wiki.lesswrong.com/wiki/AGI), could in the worst case pose an [existential risk](https://www.lesswrong.com/tag/existential-risk) when old preferences and goals continue to be used. Another possibility is that the AGI loses all ability to comprehend the world, and would pose no threat at all. If an AGI reevaluates its preferences after its ontological crisis, for example in the way mentioned above, very [unfriendly](https://www.lesswrong.com/tag/unfriendly-artificial-intelligence) behaviors could arise. Depending on the extent of the reevaluations, the AGI's changes may be detected and safely fixed. On the other hand, it could go undetected until they go wrong - which shows how it is of our interest to deeply explore ontological adaptation methods when designing AI.\n\nFurther Reading & References\n----------------------------\n\n*   [Ontological Crises in Artificial Agents' Value Systems](http://arxiv.org/abs/1105.3821) by Peter de Blanc\n\nNotable Posts\n-------------\n\n*   [AI ontology crises: an informal typology](http://lesswrong.com/r/discussion/lw/827/ai_ontology_crises_an_informal_typology/) by Stuart Armstrong\n*   [Eutopia is Scary](http://lesswrong.com/lw/xl/eutopia_is_scary/) by Eliezer Yudkowsky\n*   [Ontological Crisis in Humans](http://lesswrong.com/lw/fyb/ontological_crisis_in_humans/) by Wei Dai\n\nSee also\n--------\n\n*   [Evolution](https://www.lesswrong.com/tag/evolution)\n*   [Adaptation executers](https://wiki.lesswrong.com/wiki/Adaptation_executers)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb2bd",
    "name": "Computing Overhang",
    "core": null,
    "slug": "computing-overhang",
    "oldSlugs": null,
    "postCount": 7,
    "description": {
      "markdown": "**Computing overhang** refers to a situation where new algorithms can exploit existing computing power far more efficiently than before. This can happen if previously used algorithms have been suboptimal.\n\nIn the context of [Artificial General Intelligence](https://www.lesswrong.com/tag/artificial-general-intelligence), this signifies a situation where it becomes possible to create AGIs that can be run using only a small fraction of the easily available hardware resources. This could lead to an [intelligence explosion](https://www.lesswrong.com/tag/intelligence-explosion), or to a massive increase in the number of AGIs, as they could be easily copied to run on countless computers. This could make AGIs much more powerful than before, and present an [existential risk](https://www.lesswrong.com/tag/existential-risk).\n\nExamples\n--------\n\nIn 2010, the President's Council of Advisors on Science and Technology [reported on](http://www.whitehouse.gov/sites/default/files/microsites/ostp/pcast-nitrd-report-2010.pdf) benchmark production planning model having become faster by a factor of 43 million between 1988 and 2003. Of this improvement, only a factor of roughly 1,000 was due to better hardware, while a factor of 43,000 came from algorithmic improvements. This clearly reflects a situation where new programming methods were able to use available computing power more efficiently.\n\nAs of today, enormous amounts of computing power is currently available in the form of supercomputers or distributed computing. Large AI projects can grow to fill these resources by using deeper and deeper search trees, such as high-powered chess programs, or by performing large amounts of parallel operations on extensive databases, such as IBM's Watson playing Jeopardy. While the extra depth and breadth are helpful, it is likely that a simple brute-force extension of techniques is not the optimal use of the available computing resources. This leaves the need for improvement on the side of algorithmic implementations, where most work is currently focused on.\n\nThough estimates of [whole brain emulation](https://www.lesswrong.com/tag/whole-brain-emulation) place that level of computing power at least a decade away, it is very unlikely that the algorithms used by the human brain are the most computationally efficient for producing AI. This happens mainly because our brains evolved during a natural selection process and thus weren't deliberatly created with the goal of being modeled by AI.\n\nAs Yudkoswky [puts it](http://intelligence.org/files/LOGI.pdf), human intelligence, created by this \"blind\" evolutionary process, has only recently developed the ability for planning and forward thinking - *deliberation*. On the other hand, the rest and almost all our cognitive tools were the result of ancestral selection pressures, forming the roots of almost all our behavior. As such, when considering the design of complex systems where the designer - us - collaborates with the system being constructed, we are faced with a new signature and a different way to achieve AGI that's completely different than the process that gave birth to our brains.\n\nReferences\n----------\n\n*   Muehlhauser, Luke; Salamon, Anna (2012). [\"Intelligence Explosion: Evidence and Import\"](http://intelligence.org/files/IE-EI.pdf). in Eden, Amnon; Søraker, Johnny; Moor, James H. et al.. *The singularity hypothesis: A scientific and philosophical assessment*. Berlin: Springer.\n\nSee also\n--------\n\n*   [Optimization process](https://www.lesswrong.com/tag/optimization)\n*   [Optimization](https://www.lesswrong.com/tag/optimization)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb2b5",
    "name": "Recursive Self-Improvement",
    "core": null,
    "slug": "recursive-self-improvement",
    "oldSlugs": null,
    "postCount": 20,
    "description": {
      "markdown": "**Recursive self-improvement** refers to the property of making improvements on one's own ability of making self-improvements. It is an approach to [Artificial General Intelligence](https://www.lesswrong.com/tag/artificial-general-intelligence) that allows a system to make adjustments to its own functionality resulting in improved performance. The system could then feedback on itself with each cycle reaching ever higher levels of intelligence resulting in either a hard or soft [AI takeoff](https://www.lesswrong.com/tag/ai-takeoff).\n\nAn agent can self-improve and get a linear succession of improvements, however if it is able to improve its ability of making self-improvements, then each step will yield exponentially more improvements then the next one.\n\nRecursive self-improvement and [AI takeoff](https://www.lesswrong.com/tag/ai-takeoff)\n-------------------------------------------------------------------------------------\n\nRecursively self-improving AI is considered to be the push behind the [intelligence explosion](https://www.lesswrong.com/tag/intelligence-explosion). While any sufficiently intelligent AI will be able to improve itself, [Seed AIs](https://www.lesswrong.com/tag/seed-ai) are specifically designed to use recursive self-improvement as their primary method of gaining intelligence. Architectures that had not been designed with this goal in mind, such as neural networks or large \"hand-coded\" projects like [Cyc](https://www.lesswrong.com/tag/cyc), would have a harder time self-improving.\n\n[Eliezer Yudkowsky](https://www.lesswrong.com/tag/eliezer-yudkowsky) argues that a recursively self-improvement AI seems likely to deliver a hard AI takeoff – a fast, abruptly, local increase in capability - since the exponential increase in intelligence would yield an exponential return in benefits and resources that would feed even more returns in the next step, and so on. In his view a soft takeoff scenario seems unlikely: \"it should either flatline or blow up. You would need exactly the right law of diminishing returns to fly through the extremely narrow soft takeoff keyhole.\"[1](http://lesswrong.com/lw/we/recursive_selfimprovement/).\n\nYudkowsky argues that there are several points which seem to support the [hard takeoff scenario](https://wiki.lesswrong.com/wiki/AI_takeoff#Hard_takeoff). Some of them are the fact that one improvement seems to lead the way to another, [hardware overhang](https://www.lesswrong.com/tag/computing-overhang) and the fact that sometimes- when navigating through problem space - one can find a succession of extremely easy to solve problems. These are all reasons for suddenly and abruptly increases in capability. On the other hand, [Robin Hanson](https://www.lesswrong.com/tag/robin-hanson) argues that there will be mostly a slow and gradual accumulation of improvements, without a sharp change.\n\nSelf-improvement in humans\n--------------------------\n\nThe human species has made an enormous amount of progress since evolving around fifty thousand years ago. This is because we can pass on knowledge and infrastructure from previous generations. This is a type of self-improvement, but it is not *recursive*. If we never learned to modify our own brains, then we would eventually reach the point where making new discoveries required more knowledge than could be gained in a human lifetime. All human progress to date has been limited by the hardware we are born with, which is the same hardware Homo sapiens were born with fifty thousand years ago.\n\n\"True\" recursive self-improvement will come when we discover how to drastically modify or augment our own brains in order to be more intelligent. This would lead us to more quickly being able to discover how to become even more intelligent.\n\nRecursive self-improvement and [Instrumental value](https://www.lesswrong.com/tag/instrumental-value)\n-----------------------------------------------------------------------------------------------------\n\n*Main article:* [*Basic AI drives*](https://wiki.lesswrong.com/wiki/Basic_AI_drives)\n\n[Nick Bostrom](https://www.lesswrong.com/tag/nick-bostrom) and [Steve Omohundro](https://en.wikipedia.org/wiki/Steve_Omohundro) have separately[\\[2\\]](http://www.nickbostrom.com/superintelligentwill.pdf) argued[\\[3\\]](http://selfawaresystems.files.wordpress.com/2008/01/nature_of_self_improving_ai.pdf) that despite the fact that values and intelligence are independent, any recursively self-improving intelligence would likely possess a common set of instrumental values which are useful for achieving any kind of [goal](https://www.lesswrong.com/tag/terminal-value). As a system's intelligence continued modifying itself towards greater intelligence, it would be likely to adopt more of these behaviors.\n\nBlog posts\n----------\n\n*   [Recursive Self Improvement](http://lesswrong.com/lw/we/recursive_selfimprovement/) by Eliezer Yudkowsky\n*   [Cascades, Cycles, Insight...](http://lesswrong.com/lw/w5/cascades_cycles_insight/) by Eliezer Yudkowsky\n*   [...Recursion, Magic](http://lesswrong.com/lw/w6/recursion_magic/) by Eliezer Yudkowsky\n\nSee also\n--------\n\n*   [Intelligence explosion](https://www.lesswrong.com/tag/intelligence-explosion)\n*   [Singularity](https://www.lesswrong.com/tag/singularity)\n*   [Seed AI](https://www.lesswrong.com/tag/seed-ai)\n*   [Gödel machine](https://www.lesswrong.com/tag/gödel-machine)\n*   [AI takeoff](https://www.lesswrong.com/tag/ai-takeoff)\n\nExternal links\n--------------\n\n*   [Seed AI](http://intelligence.org/files/LOGI.pdf) description from MIRI.\n*   [Risks from Artificial Intelligence](http://intelligence.org/files/AIPosNegFactor.pdf) by Eliezer Yudkowsky.\n*   [Advantages of Artificial Intelligence](http://www.xuenay.net/Papers/DigitalAdvantages.pdf) by Kaj Sotala\n*   [Speculations Concerning the First Ultraintelligent Machine](http://commonsenseatheism.com/wp-content/uploads/2011/02/Good-Speculations-Concerning-the-First-Ultraintelligent-Machine.pdf) by I.J. Good"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb2b1",
    "name": "Whole Brain Emulation",
    "core": null,
    "slug": "whole-brain-emulation",
    "oldSlugs": null,
    "postCount": 79,
    "description": {
      "markdown": "**Whole Brain Emulation** or **WBE** is a proposed technique which involves transferring the information contained within a brain onto a computing substrate. The brain can then be simulated, creating a machine intelligence. The concept is often discussed in context of scanning the brain of a person, known as [Mind Uploading](https://www.lesswrong.com/tag/mind-uploading).\n\nWBE is sometimes seen as an easy way to creating intelligent computers, as the only innovations necessary are greatly increased processor speed and scanning resolution. Advocates of WBE claim technological improvement rates such as [Moore's law](https://wiki.lesswrong.com/wiki/Moore's_law) will make WBE inevitable.\n\nThe exact level of detail required for an accurate simulation of a brain's mind is presently uncertain, and will determine the difficulty of creating WBE. The feasibility of such a project has been examined in detail in [Future of Humanity Institute](https://www.lesswrong.com/tag/future-of-humanity-institute-fhi)'s [Whole Brain Emulation: A Roadmap](https://www.lesswrong.com/tag/brain-emulation-roadmap). The Roadmap concluded that a human brain emulation would be possible before mid-century, providing that current technology trends kept up and providing that there would be sufficient investments.\n\nSeveral approaches for WBE have been suggested:\n\n*   A brain could be cut into small slices, which would then be scanned into a computer.[^1^](#fn1)\n*   [Brain-computer interfaces](https://www.lesswrong.com/tag/brain-computer-interfaces) could slowly replace portions of the brain with computers and allow the mind to grow onto a computing substrate.[^2^](#fn2)[^3^](#fn3)\n*   Resources such as personality tests and a person's writings could be used to construct a model of the person.[^4^](#fn4)\n\nA digitally emulated brain could have several advantages over a biological one[^5^](#fn5). It might be able to run faster than biological brains, copy itself, and take advantage of backups while experimenting with self-modification.\n\nWhole brain emulation will also create a number of ethical challenges relating to the nature of personhood, rights, and social inequality. [Robin Hanson](https://www.lesswrong.com/tag/robin-hanson) proposes that an uploaded mind [might copy itself to work until the cost of running a copy was that of its labour](https://www.lesswrong.com/tag/economic-consequences-of-ai-and-whole-brain-emulation), vastly increasing the amount of wealth in the world but also causing mass unemployment[^6^](#fn6). The ability to copy uploads could also lead to drastic changes in society's values, with the values of the uploads that got copied the most coming to dominate.\n\nAn emulated-brain populated world could hold severe negative consequences, such as:\n\n*   Inherent inability to have consciousness, if some philosophers are right [^7^](#fn7) [^8^](#fn8) [^9^](#fn9) [^10^](#fn10).\n*   Elimination of culture in general, due to an extremely increasing penalty for inefficiency in the form of flamboyant displays [^11^](#fn11)\n*   Near zero costs for reproduction, pushing most of [emulations to live in a subsistence state](https://www.lesswrong.com/tag/economic-consequences-of-ai-and-whole-brain-emulation). [^12^](#fn12)\n\nSee Also\n--------\n\n*   [Economic consequences of AI and whole brain emulation](https://www.lesswrong.com/tag/economic-consequences-of-ai-and-whole-brain-emulation)\n*   [Emulation argument for human-level AI](https://www.lesswrong.com/tag/emulation-argument-for-human-level-ai)\n*   [Simulation hypothesis](https://www.lesswrong.com/tag/simulation-hypothesis)\n*   [Neuromorphic AI](https://www.lesswrong.com/tag/neuromorphic-ai)\n\nExternal Links\n--------------\n\n*   [The Singularity is near: When humans transcend biology](http://www.amazon.com/The-Singularity-Is-Near-Transcend/dp/0143037889/) by Ray Kurzweil\n*   [Whole Brain Emulation: A Roadmap](https://www.lesswrong.com/tag/brain-emulation-roadmap). Report by The Future of Humanity Institute.\n*   [Hans Moravec's Estimation of Human Brain Processing Capacity](http://www.jetpress.org/volume1/moravec.htm)\n*   [A world survey of artificial brain projects, Part I: Large-scale brain simulations](http://www.patternsinthevoid.net/blog/wp-content/uploads/2010/12/2009-A-world-survey-of-artificial-brain-projects-Part1_Large-scale-brain-simulations.pdf) by Hugo de Garis, Chen Shuo, Ben Goertzel and, Lian Ruiting, 2010\n*   [If Uploads Come First: The crack of a future dawn](http://hanson.gmu.edu/uploads.html) by Robin Hanson\n*   [Whole Brain Emulation and the Evolution of Superorganisms](http://intelligence.org/files/WBE-Superorgs.pdf)\n*   [International Journal of Machine Consciousness Special Issue on Mind Uploading](http://wp.goertzel.org/?page_id=368)\n*   [A framework for approaches to transfer of a mind's substrate](http://www.sim.me.uk/neural/JournalArticles/Bamford2012IJMC.pdf) by Sim Bamford\n*   [Coalescing Minds: Brain Uploading-related Group Mind Scenarios](http://www.xuenay.net/Papers/CoalescingMinds.pdf) by Kaj Sotala and Harri Valpola\n\nReferences\n----------\n\n1.  [Whole Brain Emulation: A Roadmap](https://www.lesswrong.com/tag/brain-emulation-roadmap)[↩](#fnref1)\n2.  Strout, J. Uploading by the Nanoreplacement Procedure. [http://www.ibiblio.org/jstrout/uploading/nanoreplacement.html](http://www.ibiblio.org/jstrout/uploading/nanoreplacement.html)[↩](#fnref2)\n3.  Sotala, K., & Valpola, H. (2012). Coalescing minds: brain uploading-related group mind scenarios. International Journal of Machine Consciousness, 4(01), 293-312. [http://singularity.org/files/CoalescingMinds.pdf](http://singularity.org/files/CoalescingMinds.pdf)[↩](#fnref3)\n4.  ROTHBLATT, M. (2012). THE TERASEM MIND UPLOADING EXPERIMENT. International Journal of Machine Consciousness, 4(01), 141-158. [http://www.terasemcentral.org/docs/Terasem%20Mind%20Uploading%20Experiment%20IJMC.pdf](http://www.terasemcentral.org/docs/Terasem%20Mind%20Uploading%20Experiment%20IJMC.pdf)[↩](#fnref4)\n5.  Sotala, K. (2012). Advantages of artificial intelligences, uploads, and digital minds. International Journal of Machine Consciousness, 4(01), 275-291. [http://singularity.org/files/AdvantagesOfAIs.pdf](http://singularity.org/files/AdvantagesOfAIs.pdf)[↩](#fnref5)\n6.  Hanson, R. (1994). If uploads come first. Extropy, 6(2), 10-15. [http://hanson.gmu.edu/uploads.html](http://hanson.gmu.edu/uploads.html)[↩](#fnref6)\n7.  LUCAS, John. (1961) Minds, machines, and Gödel, Philosophy, 36, pp. 112–127[↩](#fnref7)\n8.  DREYFUS, H. (1972) What Computers Can’t Do, New York: Harper & Row.[↩](#fnref8)\n9.  PENROSE, Roger (1994) Shadows of the Mind, Oxford: Oxford University Press.[↩](#fnref9)\n10.  BLOCK, Ned (1981) Psychologism and behaviorism, Philosophical Review, 90, pp. 5–43.[↩](#fnref10)\n11.  BOSTROM, Nick.(2004) \"The future of human evolution\". Death and Anti‐Death: Two Hundred Years After Kant, Fifty Years After Turing, ed. Charles Tandy (Ria University Press: Palo Alto, California, 2004): pp. 339‐371. Available at: [http://www.nickbostrom.com/fut/evolution.pdf](http://www.nickbostrom.com/fut/evolution.pdf)[↩](#fnref11)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb2b0",
    "name": "Technological Forecasting",
    "core": null,
    "slug": "technological-forecasting",
    "oldSlugs": null,
    "postCount": 29,
    "description": {
      "markdown": "**Technological forecasting** means making predictions about future technological advances.\n\nOne approach is extrapolating from past data. [Moore's Law](http://en.wikipedia.org/wiki/Moore's_law), which says that the number of transistors on an integrated circuit doubles every two years, is the classic example. Bela Nagy's [performance curve database](http://pcdb.santafe.edu/), perhaps the most systematic attempt at such extrapolation, [has found](http://tuvalu.santafe.edu/~bn/workingpapers/NagyFarmerTrancikBui.pdf) similar trends in many technologies. [Ray Kurzweil](http://www.kurzweilai.net/the-law-of-accelerating-returns) is a well-known advocate of exponential technological growth models. On the other hand, an exponential curve is [indistinguishable](http://commonsenseatheism.com/wp-content/uploads/2012/01/Modis-The-singularity-myth.pdf) from the early stages of a logistic curve that eventually approaches a ceiling.\n\nAnother approach is [expert elicitation](http://teaching.p-design.ch/forecasting07/texts/RoweWright2001_Delphi_Technique.pdf), such as in the [survey taken at the Global Catastrophic Risk Conference](http://www.philosophy.ox.ac.uk/__data/assets/pdf_file/0020/3854/global-catastrophic-risks-report.pdf), and [a survey of artificial general intelligence researchers](http://sethbaum.com/ac/2011_AI-Experts.html) on AGI timelines.\n\nOne could create probabilistic models more complicated than a simple trend extrapolation. Anders Sandberg has done calculations on timelines for [whole brain emulation](https://www.lesswrong.com/tag/whole-brain-emulation), based on an analysis of [prerequisite technologies](http://www.fhi.ox.ac.uk/__data/assets/pdf_file/0019/3853/brain-emulation-roadmap-report.pdf). [The Uncertain Future](http://www.theuncertainfuture.com) is a web application (developed by the [Machine Intelligence Research Institute](https://www.lesswrong.com/tag/machine-intelligence-research-institute-miri) and currently in beta) that works with probability distributions provided by the user to calculate the probability of a disruption to \"business as usual\", which could come in the form of either a global disaster or the invention of [artificial general intelligence](https://www.lesswrong.com/tag/artificial-general-intelligence).\n\nAn important danger in predicting the future is that one might tell complex stories with many details, any of which could fail and invalidate the prediction. Models like that used in The Uncertain Future attempt to avoid this problem by considering outcomes that could come about in multiple ways, and assigning some probability to many different scenarios.\n\nBlog posts\n----------\n\n*   [Long-term technological forecasting](http://lesswrong.com/lw/9ao/longterm_technological_forecasting/)\n\nExternal links\n--------------\n\n*   [Changing the frame of AI futurism: From storytelling to heavy-tailed, high-dimensional probability distributions](http://intelligence.org/files/ChangingTheFrame.html) (a conference paper explaining the reasoning behind the Uncertain Future app)\n\nSee also\n--------\n\n*   [Acceleration thesis](https://wiki.lesswrong.com/wiki/Acceleration_thesis)\n*   [Good-story bias](https://www.lesswrong.com/tag/good-story-bias)\n*   [Economic consequences of AI and whole brain emulation](https://www.lesswrong.com/tag/economic-consequences-of-ai-and-whole-brain-emulation)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb2ac",
    "name": "Economic Consequences of AGI",
    "core": null,
    "slug": "economic-consequences-of-agi",
    "oldSlugs": [
      "economic-consequences-of-ai-and-whole-brain-emulation"
    ],
    "postCount": 18,
    "description": {
      "markdown": "The **economic consequences of** [**artificial general intelligence**](https://www.lesswrong.com/tag/artificial-general-intelligence) arise from their fundamentally new properties compared to the human brains currently driving the economy. Once such digital minds become generally intelligent enough to perform a wide range of economic functions, they are likely to bring radical changes, creating great wealth, but also displacing humans out of more and more types of job.\n\nAn important aspect of the question is that of economic growth. The invention of AGI or WBE could cause a sudden increase in growth by adding machine intelligence to the pool of human innovators. Machine intelligence could be much cheaper to produce, faster, and qualitatively smarter than human talent. A [feedback loop](https://www.lesswrong.com/tag/intelligence-explosion) from better machine intelligence technology, to more and better machine researchers, back to better machine intelligence technology could ensue.\n\n[Robin Hanson](https://www.lesswrong.com/tag/robin-hanson) has written much about the economics of whole brain emulation. In [his view](http://hanson.gmu.edu/uploads.html), the unrestricted creation of additional uploads will cause a [Malthusian scenario](https://www.lesswrong.com/tag/malthusian-scenarios), where upload wages fall to subsistence levels. He sees the transition to whole brain emulation as a [jump to a new \"growth mode\"](http://hanson.gmu.edu/longgrow.pdf) with higher exponential growth rates, similar to the transitions to agriculture and industry.\n\nIn [\"The Future of Human Evolution\"](http://www.nickbostrom.com/fut/evolution.pdf), [Nick Bostrom](https://www.lesswrong.com/tag/nick-bostrom) argues that in an emulated-brain society with individuals living at subsistence levels, entities that possess a large set of features we care about – which he calls flamboyant displays, or culture in general – will be outcompeted by more efficient ones that lack inefficient humans’ cultural aspects. This will lead to elimination of all forms of being that we care about. He proposes that only a [Singleton](https://www.lesswrong.com/tag/singleton) could ensure strict control in order to prevent the elimination of culture through outcompetition.\n\nOthers predict that growth will blow up even more suddenly (up to the point where physical limits become relevant), and that growth will be concentrated in a smaller and more coherent set of agents, so that instead of continued free market competition, we will see a [singleton](https://www.lesswrong.com/tag/singleton) emerge.\n\nBlog posts\n----------\n\n*   [The Hanson-Yudkowsky Foom Debate](http://wiki.lesswrong.com/wiki/The_Hanson-Yudkowsky_AI-Foom_Debate)\n*   [Overcoming Bias posts tagged \"Future\"](http://www.overcomingbias.com/tag/future)\n*   [Is The City-ularity Near?](http://www.overcomingbias.com/2010/02/is-the-city-ularity-near.html)\n\nExternal links\n--------------\n\n*   [Bostrom's paper on elimination of culture](http://www.nickbostrom.com/fut/evolution.pdf)\n*   [Economic growth given machine intelligence](http://hanson.gmu.edu/aigrow.pdf)\n*   [Economic implications of software minds](http://intelligence.org/files/EconomicImplications.pdf)\n*   [Long-term growth as a sequence of exponential modes](http://hanson.gmu.edu/longgrow.pdf)\n*   [Is a singularity just around the corner? What it takes to get explosive economic growth](http://hanson.gmu.edu/fastgrow.html)\n\nSee also\n--------\n\n*   [Technological singularity](https://wiki.lesswrong.com/wiki/Technological_singularity)\n*   [Hard takeoff](https://wiki.lesswrong.com/wiki/Hard_takeoff), [Soft takeoff](https://wiki.lesswrong.com/wiki/Soft_takeoff)\n*   [Artificial general intelligence](https://www.lesswrong.com/tag/artificial-general-intelligence), [Whole Brain Emulation](https://www.lesswrong.com/tag/whole-brain-emulation)\n*   [Technological forecasting](https://www.lesswrong.com/tag/technological-forecasting)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb2aa",
    "name": "Infinities In Ethics",
    "core": null,
    "slug": "infinities-in-ethics",
    "oldSlugs": null,
    "postCount": 14,
    "description": {
      "markdown": "**Infinities in ethics** pose some difficult problems. For example, if the universe is infinite, there are already infinite numbers of good and bad things. Adding or removing finitely many of them leaves infinitely many of both. This means aggregative consequentialist theories (those that maximize the sum of the values of individual structures) will be indifferent between any acts with merely finite effects. If you save the whales, there will be infinitely many whales, but if you don't save the whales, there will also be infinitely many whales.\n\n[Nick Bostrom](https://www.lesswrong.com/tag/nick-bostrom) wrote a [paper](http://www.nickbostrom.com/ethics/infinite.pdf) discussing various possible solutions to this problem of \"infinitarian paralysis\" (as well as the \"fanaticism\" problem of theories that would sacrifice anything for a small chance of an infinite payoff). The solutions fall into three classes:\n\n*   Modifications of the \"domain rule\" that determines what values are to be aggregated (e.g., discounting values far away in space and time)\n*   Modifications of the \"aggregation rule\" that determines *how* these values are to be aggregated (e.g., representing total value as a hyperreal number)\n*   Modifications of the \"selection rule\" that uses the aggregation result to recommend an action (e.g., ignoring very small probabilities)\n\nThe best-known use of infinity in ethics is probably [Pascal's wager](http://en.wikipedia.org/wiki/Pascal's_Wager), which has a finite variant in [Pascal's mugging](https://www.lesswrong.com/tag/pascal-s-mugging).\n\nBlog posts\n----------\n\n*   [The Pascal's Wager Fallacy Fallacy](http://lesswrong.com/lw/z0/the_pascals_wager_fallacy_fallacy/), by [Eliezer Yudkowsky](https://www.lesswrong.com/tag/eliezer-yudkowsky)\n\nExternal links\n--------------\n\n*   [Infinite Ethics](http://www.nickbostrom.com/ethics/infinite.pdf), by Nick Bostrom\n*   [Philosophical implications of inflationary cosmology](http://philsci-archive.pitt.edu/1341/)\n\nSee also\n--------\n\n*   [Pascal's mugging](https://www.lesswrong.com/tag/pascal-s-mugging)\n*   [Utilitarianism](https://www.lesswrong.com/tag/utilitarianism)\n*   [Quick reference guide to the infinite](https://www.lesswrong.com/tag/quick-reference-guide-to-the-infinite)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb2a8",
    "name": "Astronomical Waste",
    "core": null,
    "slug": "astronomical-waste",
    "oldSlugs": null,
    "postCount": 6,
    "description": {
      "markdown": "**Astronomical waste** is a term introduced by [Nick Bostrom](https://www.lesswrong.com/tag/nick-bostrom) for the opportunities we're losing out on by not colonizing the universe.\n\nThe universe is vast. There are many galaxies, each containing many stars. In a future with space colonization, each star could support a large population of people leading worthwhile lives. During any given year, we irrecoverably lose an amount of energy that could have powered a civilization like ours for many billions of years.\n\nThe prospect of advanced technology only makes the numbers more extreme. Such technology would make it possible to support far more, and better, lives with the same resources.\n\nBostrom [notes](http://www.nickbostrom.com/astronomical/waste.html) that in a wide range of moral theories — in particular, those based on linear aggregation of value — considerations of astronomical waste outweigh all others.\n\nIf so much potential is lost every year, one could conclude that we should start colonization *as soon as possible*. But since the amount of resources available *in total* is much larger still than the amount lost in a year, a better utilitarian prescription is to minimize the risk of losing out on space colonization entirely. Bostrom calls this prescription \"maxipok\", for the maximum probability of an OK outcome.\n\nExternal links\n--------------\n\n*   [Astronomical Waste: The Opportunity Cost of Delayed Technological Development](http://www.nickbostrom.com/astronomical/waste.html)\n\nSee also\n--------\n\n*   [Utilitarianism](https://www.lesswrong.com/tag/utilitarianism)\n*   [Existential risk](https://www.lesswrong.com/tag/existential-risk)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb29d",
    "name": "Neuromorphic AI",
    "core": null,
    "slug": "neuromorphic-ai",
    "oldSlugs": null,
    "postCount": 18,
    "description": {
      "markdown": "A **Neuromorphic AI** ('neuron-shaped') is a form of AI where most of the functionality has been copied from the human brain. This implies that its inner workings are not necessarily understood by the creators any further than is necessary to simulate them on a computer. It is considered a more [unsafe](https://wiki.lesswrong.com/wiki/Unfriendly_AI) form of AI than either [Whole Brain Emulation](https://www.lesswrong.com/tag/whole-brain-emulation) or de novo AI because its lacks the high quality replication of human values of the former and the possibility of good theoretical guarantees that the latter may have due to cleaner design.\n\nExternal Links\n--------------\n\n*   Definition from [The Uncertain Future](http://www.theuncertainfuture.com/faq.html#3)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb297",
    "name": "Superintelligence",
    "core": null,
    "slug": "superintelligence",
    "oldSlugs": null,
    "postCount": 20,
    "description": {
      "markdown": "A **Superintelligence** is a being with superhuman intelligence, and a focus of the [Machine Intelligence Research Institute](https://www.lesswrong.com/tag/machine-intelligence-research-institute-miri)'s research. Specifically, Nick Bostrom (1997) defined it as\n\n> \"An intellect that is much smarter than the best human brains in practically every field, including scientific creativity, general wisdom and social skills.\"\n\nThe [Machine Intelligence Research Institute](https://www.lesswrong.com/tag/machine-intelligence-research-institute-miri) is dedicated to ensuring humanity's safety and prosperity by preparing for the development of an [Artificial General Intelligence](https://www.lesswrong.com/tag/artificial-general-intelligence) with superintelligence. Given its intelligence, it is likely to be [incapable of being controlled](https://www.lesswrong.com/tag/ai-boxing-containment) by humanity. It is important to prepare early for the development of [friendly artificial intelligence](https://www.lesswrong.com/tag/friendly-artificial-intelligence), as there may be an [AI arms race](https://www.lesswrong.com/tag/ai-arms-race). A strong superintelligence is a term describing a superintelligence which is not designed with the same architecture as the human brain.\n\nAn [Artificial General Intelligence](https://www.lesswrong.com/tag/artificial-general-intelligence) will have a number of advantages aiding it in becoming a superintelligence. It can improve the hardware it runs on and obtain better hardware. It will be capable of directly editing its own code. Depending on how easy its code is to modify, it might carry out software improvements that [spark further improvements](https://www.lesswrong.com/tag/recursive-self-improvement). Where a task can be accomplished in a repetitive way, a module preforming the task far more efficiently might be developed. Its motivations and preferences can be edited to be more consistent with each other. It will have an indefinite life span, be capable of reproducing, and transfer knowledge, skills, and code among its copies as well as cooperating and communicating with them better than humans do with each other.\n\nThe development of superintelligence from humans is another possibility, sometimes termed a weak superintelligence. It may come in the form of [whole brain emulation](https://www.lesswrong.com/tag/whole-brain-emulation), where a human brain is scanned and simulated on a computer. Many of the advantages a AGI has in developing superintelligence apply here as well. The development of [Brain-computer interfaces](https://www.lesswrong.com/tag/brain-computer-interfaces) may also lead to the creation of superintelligence. Biological enhancements such as genetic engineering and the use of nootropics could lead to superintelligence as well.\n\nBlog Posts\n----------\n\n*   [Superintelligence](http://www.acceleratingfuture.com/articles/superintelligencehowsoon.htm) by Michael Anissimov\n\nExternal Links\n--------------\n\n*   [How long before Superintelligence?](http://www.nickbostrom.com/superintelligence.html) by Nick Bostrom\n*   [A discussion between Hugo de Garis and Ben Goertzel on superintelligence](http://profhugodegaris.files.wordpress.com/2011/04/nocyborgsbghugo.pdf)\n*   [Advantages of Artificial Intelligences, Uploads, And Digital Minds](http://www.xuenay.net/Papers/DigitalAdvantages.pdf) by Kaj Sotala\n\nSee Also\n--------\n\n*   [Brain-computer interfaces](https://www.lesswrong.com/tag/brain-computer-interfaces)\n*   [Singularity](https://www.lesswrong.com/tag/singularity)\n*   [Hard takeoff](https://wiki.lesswrong.com/wiki/Hard_takeoff)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb294",
    "name": "Bayesian Decision Theory",
    "core": null,
    "slug": "bayesian-decision-theory",
    "oldSlugs": null,
    "postCount": 10,
    "description": {
      "markdown": "**Bayesian decision theory** refers to a [decision theory](https://www.lesswrong.com/tag/decision-theory) which is informed by [Bayesian probability](https://www.lesswrong.com/tag/bayesian-probability). It is a statistical system that tries to quantify the tradeoff between various decisions, making use of probabilities and costs. An agent operating under such a decision theory uses the concepts of Bayesian statistics to estimate the [expected value](https://www.lesswrong.com/tag/expected-value) of its actions, and update its expectations based on new information. These agents can and are usually referred to as estimators.\n\nFrom the perspective of Bayesian decision theory, any kind of probability distribution - such as the distribution for tomorrow's weather - represents a [prior](https://www.lesswrong.com/tag/priors) distribution. That is, it represents how we expect *today* the weather is going to be *tomorrow.* This contrasts with frequentist inference, the classical probability interpretation, where conclusions about an experiment are drawn from a set of repetitions of such experience, each producing statistically independent results. For a frequentist, a probability function would be a simple distribution function with no special meaning.\n\nSuppose we intend to meet a friend tomorrow, and expect an 0.5 chance of raining. If we are choosing between various options for the meeting, with the pleasantness of some of the options (such as going to the park) being affected by the possibility of rain, we can [assign values to the different options with or without rain](http://lesswrong.com/lw/8uj/compressing_reality_to_math/). We can then pick the option whose expected value is the highest, given the probability of rain.\n\nOne definition of [rationality](https://www.lesswrong.com/tag/rationality), used both on Less Wrong and in economics and psychology, is behavior which obeys the rules of Bayesian decision theory. Due to computational constraints, this is impossible to do perfectly, but naturally evolved brains [do seem to mirror](http://en.wikipedia.org/wiki/Bayesian_brain) these probabilistic methods when they adapt to an uncertain environment. Such models and distributions may be reconfigured according to feedback from the environment.\n\nFurther Reading & References\n----------------------------\n\n*   Berger, James O. (1985). Statistical decision theory and Bayesian Analysis (2nd ed.). New York: Springer-Verlag. ISBN 0-387-96098-8. MR 0804611\n*   Bernardo, José M.; Smith, Adrian F. M. (1994). Bayesian Theory. Wiley. ISBN 0-471-92416-4. MR 1274699\n\nSee also\n--------\n\n*   [Bayesian probability](https://www.lesswrong.com/tag/bayesian-probability)\n*   [Decision theory](https://www.lesswrong.com/tag/decision-theory)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb292",
    "name": "Seed AI",
    "core": null,
    "slug": "seed-ai",
    "oldSlugs": null,
    "postCount": 5,
    "description": {
      "markdown": "A **Seed AI** (a term coined by [Eliezer Yudkowsky](https://www.lesswrong.com/tag/eliezer-yudkowsky)) is an [Artificial General Intelligence](https://www.lesswrong.com/tag/artificial-general-intelligence) (AGI) which improves itself by [recursively rewriting](https://www.lesswrong.com/tag/recursive-self-improvement) its own source code without human intervention. Initially this program would likely have a minimal intelligence, but over the course of many iterations it would evolve to human-equivalent or even trans-human reasoning. The key for successful [AI takeoff](https://www.lesswrong.com/tag/ai-takeoff) would lie in creating adequate starting conditions.\n\nHistory\n-------\n\nThe notion of machine learning without human intervention has been around nearly as long as the computers themselves. In 1959, [Arthur Samuel](http://en.wikipedia.org/wiki/Arthur_Samuel) stated that \"Machine learning is the field of study that gives computers the ability to learn without being explicitly programmed.\"[^1^](#fn1) Since that time, computers have been able to learn by a variety of methods, including [neural networks](http://en.wikipedia.org/wiki/Artificial_neural_network) and [Bayesian inference](http://en.wikipedia.org/wiki/Bayesian_inference).\n\nWhile these approaches have enabled machines to become better at various tasks[^2^](#fn2) [^3^](#fn3), it has not enabled them to overcome the limitations of these techniques, nor has it given them the ability to understand their own programming and make improvements. Hence, they are not able to adapt to new situations without human assistance.\n\nProperties\n----------\n\nA Seed AI has abilities that previous approaches lack:\n\n*   *Understanding its own source code*. It must understand the purpose, syntax and architecture of its own programming. This type of self-reflection enables the AGI to comprehend its utility and thus preserve it.\n*   *Rewriting its own source code*. The AGI must be able to overhaul the very code it uses to fulfill its utility. A critical consideration is that it must remain stable under modifications, preserving its original goals.\n\nThis combination of abilities would, in theory, allow an AGI to recursively improve itself by becoming *smarter* within its original purpose. A [Gödel machine](https://www.lesswrong.com/tag/gödel-machine) rigorously defines a specification for such an AGI.\n\nDevelopment\n-----------\n\nCurrently, there are no known Seed AIs in existence, but it is an active field of research. Several organizations continue to pursue this goal, such as the [Singularity Institute](http://intelligence.org), [OpenCog](http://opencog.org/), and [Adaptive AI](http://adaptiveai.com/).\n\nSee Also\n--------\n\n*   [Gödel machine](https://www.lesswrong.com/tag/gödel-machine)\n*   [AI takeoff](https://www.lesswrong.com/tag/ai-takeoff)\n*   [Recursive self-improvement](https://www.lesswrong.com/tag/recursive-self-improvement)\n*   [Intelligence explosion](https://www.lesswrong.com/tag/intelligence-explosion)\n*   [AGI](https://wiki.lesswrong.com/wiki/AGI)\n\nReferences\n----------\n\n1.  Han, Zhimeng. [*Smoothing in Probability Estimation Trees*](http://researchcommons.waikato.ac.nz/bitstream/handle/10289/5701/thesis.pdf?sequence=3). The University of Waikato.\n2.  [Jump up↑](https://wiki.lesswrong.com/wiki/Seed_AI?_ga=2.93005205.966300592.1600626178-561901249.1600626178#cite_ref-eurisko_2-0) Lenat, Douglas. [*Eurisko. A program that learns news heuristics and domain concepts.*](http://researchcommons.waikato.ac.nz/bitstream/handle/10289/5701/thesis.pdf?sequence=3). Artificial Intelligence.\n3.  [Jump up↑](https://wiki.lesswrong.com/wiki/Seed_AI?_ga=2.93005205.966300592.1600626178-561901249.1600626178#cite_ref-checkers_3-0) Chellapilla, Kumar; Fogel, David. [*Evolving an Expert Checkers Playing Program without Using Human Expertise*](http://www.cs.ru.ac.za/courses/Honours/ai/HybridSystems/P2.pdf). Natural Selection, Inc.\n4.  Yudkowsky, Eliezer.[Seed AI Levels of Organization in General Intelligence](http://intelligence.org/upload/LOGI/seedAI.html). Singularity Institute.\n5.  [General Intelligence and Seed AI](http://intelligence.org/files/GISAI.html#para_seedAI_advantage). Singularity Institute.\n6.  Schmidhuber, Jürgen. [Gödel Machines: Self-Referential Universal Problem Solvers Making Provably Optimal Self-Improvements](ftp://ftp.idsia.ch/pub/juergen/gm6.pdf). IDSIA"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb28c",
    "name": "Brain-Computer Interfaces",
    "core": null,
    "slug": "brain-computer-interfaces",
    "oldSlugs": null,
    "postCount": 15,
    "description": {
      "markdown": "A **Brain Computer Interface (BCI)** is the generic term used to describe any kind of system that serves as a communication bridge between the brain (human or not) and an artificial module. It’s a field of research in which wide investment has been made since the 1970’s, especially in the clinical fields and ergonomics. Generally speaking, any kind of brain activity that can be recorded can be used as a means of communicating with another system. Through the use of statistical classification techniques it’s possible to associate certain states or characteristics of the recorded signal – which the experiment subject learns to control - to any procedure, usually mediated by a computer.\n\nMany techniques have been developed to help us look and better understand the way the brain works. They range from imaging techniques (like MRI, fMRI, fNIRS or PET), to electrophysiological ones (like EEG, EcG or MEG). While the first category is usually used to obtain high resolution images of brain structures and the second one to register and analyze the electrical activity produced by the brain, with a high temporal resolution – which is why they are the ones mainly used in the field of BCI’s. In pair with such methods, although a different area in itself, includes brain implants capable of communicating directly with the neuronal tissue - [neuroprosthetics](http://en.wikipedia.org/wiki/Neuroprosthetics).\n\nEEG BCIs\n--------\n\nOf all the different means avaliable, the registering of the [electroencephalographic (EEG)](http://en.wikipedia.org/wiki/Electroencephalography) activity is the most developed and extensively researched of this fields. It allows us, in a non-invasive way, to peak the brain functioning with a high temporal resolution – furthermore, it is now well established that different brain states produce distinct observable activity. With the help of electrodes placed on the scalp, it is possible to feed this activity and their respective variations and patterns to any system capable of classifying and detecting them in real time and act accordingly (making this a field highly interconnected to that of machine learning).\n\nThe field of BCIs has followed closely the developments in signal processing and classification, along with the increasing computational power available. It was firstly researched as a communication means (for people unable to move, for instance) through the detection of ERPs – event related potentials, small variations of amplitude associated to the presentation of certain stimuli - as well as a way of automatically detecting epileptic seizures. Also, much owing to the first and major financers of such research, the [DARPA](http://en.wikipedia.org/wiki/DARPA), the use of BCIs has been always closed associated to the military field. This has allowed insights regarding the detection of mental states of fatigue and attention variations, which has led to the development of informatics systems capable of adapting to the mental state of the user.\n\nCurrently we have available a considerable range of both research and commercial applications of EEG based BCI systems with a wide list of applications. It has shown to be a field due to receive increased attention in the next years, especially through the developing of increasingly efficient classification algorithms and computer power, and the fascination with the cognitive augmentation it might bring.\n\nPotential applications\n----------------------\n\nAlthough the EEG has been the main technique used for the development of such systems, it has been shown to be possible to integrate electronic controllers directly in the functioning of single cells or even networks. The [permanent implant of devices for interpretation](http://www.wired.com/wired/archive/10.09/vision.html) and regulation of cortical activity has also been demonstrated.\n\nThis has led to a renewed interest in the field and the exploration of new hypothesis, like drug rehabilitation through the detection of relevant cues and stimulation of the brain reward system, rehabilitation after strokes or lesion and even direct transmission of patterns of thought between subjects.\n\nOther attractive future application includes the [upload of the whole content of the brain](http://www.sim.me.uk/neural/JournalArticles/Bamford2012IJMC.pdf), and thus the mind, to a computer. Although still speculative, it seems [theoretically possible](http://intelligence.org/files/CoalescingMinds.pdf).\n\nExternal Links\n--------------\n\n*   [Brain content uploading](http://intelligence.org/files/CoalescingMinds.pdf)\n*   [Tech Summary: Brain-Computer Interfaces](http://intelligence.org/brain-computer-interfaces/)\n*   [ThinkTech](http://thinktechuk.wordpress.com/) A blog dedicated to BCI developements\n*   [Commercial EEG BCI System example](http://www.emotiv.com)\n*   [Paralyzed patient controls robot arm using BCI](http://www.kurzweilai.net/people-with-paralysis-control-robotic-arms-using-brain-computer-interface) Article from KurzweilAI\n*   [Demonstration of paralyzed patient using robot arm](http://www.youtube.com/watch?v=ogBX18maUiM) from Nature Magazine YouTube\n*   [Demonstration of a blind patient with a Retinal Implant reading](http://www.youtube.com/watch?v=g0rRvBd7Dew&feature=endscreen&NR=1) from Discovery Magazine YouTube\n\nFurther Reading & References\n----------------------------\n\n*   Anderson, J. (1980). Neurocomputing. Cambridge: The MIT Press\n*   Muller, D. (1995). Towards brain–computer interfacing. MIT Press, Cambridge, MA, 409–422.\n*   Niedermeyer, E., & Lopes da Silva, F. (2004). Electroencephalography: Basic Principles. Clinical Applications and Related Fields. London\n*   Vidal, J. (1977). Real-Time Detection of Brain Events in EEG. IEEE Proceedings, 65 (5), 633–641\n*   Parasuraman, R. (2003). Neuroergonomics: Research and practice. Theoretical Issues in Ergonomics Science, 4, 5–20.\n\nSee Also\n--------\n\n*   [Biological Cognitive Enhancement](https://www.lesswrong.com/tag/nootropics-and-other-cognitive-enhancement)\n*   [Whole brain emulation](https://www.lesswrong.com/tag/whole-brain-emulation)\n*   [Wireheading](https://www.lesswrong.com/tag/wireheading)\n*   [Neuralink](https://lesswrong.com/tag/neuralink)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb285",
    "name": "Moore's Law",
    "core": null,
    "slug": "moores-law",
    "oldSlugs": null,
    "postCount": 15,
    "description": {
      "markdown": "**Moore's Law** is a term attributed to Intel founder Gordon E. Moore who observed in 1965 that the number of transistors that could be purchased inexpensively and placed on an integrated circuit doubles every year. In 1975, he revised his estimate to every two years. It is often discussed as a doubling every 18 months, but that is a separate claim by David House, Intel executive, of overall chip performance. Moore's law been approximately correct for four decades.\n\nThough current CMOS technology is predicted to be nonviable below a certain size, many other technologies offer the potential for far greater miniaturization. This may delay Moore's law temporarily while the new technologies enter full-scale production. An end to Moore's law has often been predicted, but has failed to materialize so far.\n\nMoore's law is often cited as a reason to expect the creation of an [AGI](https://wiki.lesswrong.com/wiki/AGI) in the future, and is crucial for the possibility of [whole brain emulation](https://www.lesswrong.com/tag/whole-brain-emulation).\n\nReferences\n----------\n\n*   [Gordon Moore's orignal 1965 paper](http://download.intel.com/museum/Moores_Law/Articles-Press_Releases/Gordon_Moore_1965_Article.pdf) Electronics, Volume 38, Number 8, April 19, 1965\n*   [Progress in Digital Integrated Circuits](http://download.intel.com/museum/Moores_Law/Articles-Press_Releases/Gordon_Moore_1975_Speech.pdf) Transcript of 1975 speech by Gordon Moore\n*   [International Technology Roadmap For Semiconductors](http://www.itrs.net/reports.html)\n*   [A History of the End of Moore's Law](http://www.slate.com/blogs/future_tense/2012/05/03/michio_kako_and_a_brief_history_of_warnings_about_the_end_of_moore_s_law_.html) Slate"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb26d",
    "name": "Oracle AI",
    "core": null,
    "slug": "oracle-ai",
    "oldSlugs": null,
    "postCount": 68,
    "description": {
      "markdown": "An **Oracle AI** is a regularly proposed solution to the problem of developing [Friendly AI](https://wiki.lesswrong.com/wiki/Friendly_AI). It is conceptualized as a super-intelligent system which is designed for only answering questions, and has no ability to act in the world. The name was first suggested by [Nick Bostrom](https://www.lesswrong.com/tag/nick-bostrom).\n\nSee also\n--------\n\n*   [Basic AI drives](https://www.lesswrong.com/tag/instrumental-convergence)\n*   [Tool AI](https://www.lesswrong.com/tag/tool-ai)\n*   [Utility indifference](https://www.lesswrong.com/tag/utility-indifference)\n*   [AI Boxing](https://www.lesswrong.com/tag/ai-boxing-containment)\n\nSafety\n======\n\nThe question of whether Oracles – or just [keeping an AGI forcibly confined](https://www.lesswrong.com/tag/ai-boxing-containment) \\- are safer than fully free AGIs has been the subject of debate for a long time. Armstrong, Sandberg and Bostrom discuss Oracle safety at length in their [Thinking inside the box: using and controlling an Oracle AI](http://www.aleph.se/papers/oracleAI.pdf). In the paper, the authors review various methods which might be used to measure an Oracle's accuracy. They also try to shed some light on some weaknesses and dangers that can emerge on the human side, such as psychological vulnerabilities which can be exploited by the Oracle through social engineering. The paper discusses ideas for physical security (“boxing”), as well as problems involved with trying to program the AI to only answer questions. In the end, the paper reaches the cautious conclusion of Oracle AIs probably being safer than free AGIs.\n\nIn a related work, [Dreams of Friendliness](http://lesswrong.com/lw/tj/dreams_of_friendliness/), [Eliezer Yudkowsky](https://www.lesswrong.com/tag/eliezer-yudkowsky) gives an informal argument stating that all oracles will be agent-like, that is, driven by its own goals. He rests on the idea that anything considered \"intelligent\" must choose the correct course of action among all actions available. That means that the Oracle will have many possible things to believe, although very few of them are correct. Therefore believing the correct thing means some method was used to select the correct belief from the many incorrect beliefs. By definition, this is an [optimization process](https://www.lesswrong.com/tag/optimization) which has a goal of selecting correct beliefs.\n\nOne can then imagine all the things that might be useful in achieving the goal of \"have correct beliefs\". For instance, [acquiring more computing power and resources](https://www.lesswrong.com/tag/instrumental-convergence) could help this goal. As such, an Oracle could determine that it might answer more accurately and easily to a certain question if it turned all matter outside the box to [computronium](https://www.lesswrong.com/tag/computronium), therefore killing all the existing life.\n\nTaxonomy\n========\n\nBased on an old draft by Daniel Dewey, Luke Muehlhauser has [published](http://lesswrong.com/lw/any/a_taxonomy_of_oracle_ais/) a possible taxonomy of Oracle AIs, broadly divided between True Oracular AIs and Oracular non-AIs.\n\nTrue Oracular AIs\n-----------------\n\nGiven that true AIs are goal-oriented agents, it follows that a True Oracular AI has some kind of oracular goals. These act as the motivation system for the Oracle to give us the information we ask and nothing else.\n\nIt is first noted that such a True AI is not actually nor causally isolated from the world, as it has at least an input (questions and information) and an output (answers) channel. Since we expect such an intelligent agent to be able to have a deep impact on the world even through these limited channels, it can only be safe if its goals are fully compatible with human goals.\n\nThis means that a True Oracular AI has to have a full specification of human values, thus making it a [FAI-complete](https://www.lesswrong.com/tag/fai-complete) problem – if we could achieve such skill and knowledge we could just build a Friendly AI and bypass the Oracle AI concept.\n\nOracular non-AIs\n----------------\n\nAny system that acts only as an informative machine, only answering questions and has no goals is by definition not an AI at all. That means that a non-AI Oracular is but a calculator of outputs based on inputs. Since the term in itself is heterogeneous, the proposals made for a sub-division are merely informal.\n\nAn *Advisor* can be seen as a system that gathers data from the real world and computes the answer to an informal “what we ought to do?” question. They also represent a FAI-complete problem.\n\nA *Question-Answerer* is a similar system that gathers data from the real world but coupled with a question. It then somehow computes the answer. The difficulty can lay on distinguishing it from an Advisor and controlling the safety of its answers.\n\nFinally, a *Predictor* is seen as a system that takes a corpus of data and produces a probability distribution over future possible data. There are some proposed dangers with predictors, namely exhibiting goal-seeking behavior which does not converge with humanity goals and the ability to influence us through the predictions.\n\nFurther reading & References\n----------------------------\n\n*   [Dreams of Friendliness](http://lesswrong.com/lw/tj/dreams_of_friendliness/)\n*   [Thinking inside the box: using and controlling an Oracle AI](http://www.aleph.se/papers/oracleAI.pdf) by Armstrong, Sandberg and [Bostrom](https://www.lesswrong.com/tag/nick-bostrom)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb26b",
    "name": "Applause Light",
    "core": null,
    "slug": "applause-light",
    "oldSlugs": null,
    "postCount": 4,
    "description": {
      "markdown": "An **applause light** is an empty statement which evokes positive affect without providing new information.\n\n> It \\[was\\] not so much a *propositional* statement, as the equivalent of the \"Applause\" light that tells a studio audience when to clap.\n> \n> —[Applause Lights](http://lesswrong.com/lw/jb/applause_lights/)\n\nSee also\n--------\n\n*   [Guessing the teacher's password](https://wiki.lesswrong.com/wiki/Guessing_the_teacher's_password) \n*   [Belief as attire](https://wiki.lesswrong.com/wiki/Belief_as_attire)\n*   [Cached Thoughts](https://www.lesswrong.com/tag/cached-thoughts)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb25c",
    "name": "Kolmogorov Complexity",
    "core": null,
    "slug": "kolmogorov-complexity",
    "oldSlugs": [
      "algorithmic-complexity"
    ],
    "postCount": 27,
    "description": {
      "markdown": "The **Kolmogorov Complexity** (sometimes called Algorithmic Complexity) of a set of data is the size of the shortest possible description of the data.  \n  \n*See also*: [Solomonoff Induction](https://www.lesswrong.com/tag/solomonoff-induction), [AIXI](https://www.lesswrong.com/tag/aixi)\n\nAlgorithmic complexity is an inverse measure of compressibility. If the data is complex and random, the shortest possible description of it becomes longer. This is also one of the best definitions of randomness so far[^1^](#fn1). If the data has few regular patterns, it is difficult to compress it or describe it shortly, giving it a high Kolmogorov complexity and randomness. If there isn't any way to describe the data so that the description is shorter than the data itself, the data is incompressible. [^2^](#fn2)\n\nMore formally, the Kolmogorov complexity C(x) of a set x, is the size in bits of the shortest binary program (in a fixed programming language) that prints the set x as its only output. If C(x) is equal or greater than the size of x in bits, x is incompressible. [^3^](#fn3)\n\nThis notion can be used to state many important results in computational theory. Possibly the most famous is [Chaitin's incompleteness theorem](http://en.wikipedia.org/wiki/Kolmogorov_complexity#Chaitin.27s_incompleteness_theorem), a version of Gödel’s incompleteness theorem.\n\nReferences\n----------\n\n1.  SIPSER, M. (1983) \"A complexity theoretic approach to randomness\". In Proceedings of the 15th ACM Symposium on the Theory of Computing, pages 330{335. ACM, New York.[↩](#fnref1)\n2.  FORTNOW, Lance. \"Kolmogorov Complexity\" Available at: [http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.120.4949&rep=rep1&type=pdf](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.120.4949&rep=rep1&type=pdf)[↩](#fnref2)\n3.  LI, Ming. & VITANY, Paul. “Algorithmic Complexity”. Available at: [http://homepages.cwi.nl/~paulv/papers/020608isb.pdf](http://homepages.cwi.nl/~paulv/papers/020608isb.pdf)[↩](#fnref3)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb253",
    "name": "Luminosity",
    "core": null,
    "slug": "luminosity",
    "oldSlugs": null,
    "postCount": 6,
    "description": {
      "markdown": "**Luminosity** was Alicorn's term for reflective awareness. More recent posts tend to use the term [introspection](https://www.lesswrong.com/tag/introspection). See also [noticing](https://www.lesswrong.com/tag/noticing).\n\nA luminous mental state is one that you have and know that you have. It could be an [emotion](https://www.lesswrong.com/tag/emotions), a [belief](https://www.lesswrong.com/tag/belief) or [alief](https://www.lesswrong.com/tag/alief), a disposition, a quale, a memory - anything that might happen or be stored in your brain. What's going on in your head?\n\nSee also\n--------\n\n*   [Living Luminously (sequence)](https://www.lesswrong.com/s/ynMFrq9K5iNMfSZNg)\n*   [Luminosity (fanfiction)](https://www.lesswrong.com/tag/luminosity-fanfiction)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb23e",
    "name": "Trivial Inconvenience",
    "core": null,
    "slug": "trivial-inconvenience",
    "oldSlugs": null,
    "postCount": 6,
    "description": {
      "markdown": "**Trivial inconveniences** are inconveniences that take few resources to counteract but have a disproportionate impact on people deciding whether to take a course of action.\n\n*See also*: [Akrasia](https://www.lesswrong.com/tag/akrasia), [Ugh field](https://www.lesswrong.com/tag/aversion-ugh-fields)\n\nNotable Posts\n-------------\n\n*   [Beware Trivial Inconveniences](http://lesswrong.com/lw/f1/beware_trivial_inconveniences/)\n*   [Celebrate Trivial Impetuses](http://lesswrong.com/lw/13z/celebrate_trivial_impetuses/)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb23b",
    "name": "Epistemic Luck",
    "core": null,
    "slug": "epistemic-luck",
    "oldSlugs": null,
    "postCount": 2,
    "description": {
      "markdown": "You would have different beliefs if certain events in your life were different and since you don't choose many of the events in your life, this implies that your beliefs are in large part due to **Epistemic Luck**. How should you react to this fact?\n\nIt has been noted that one's feelings about [Decision Theory](/tag/decision-theory) correlate a great deal with whether one lives in Berkeley (MIRI-centric) or Oxford (FHI-centric).\n\nBlog posts\n----------\n\n*   [Epistemic Luck](http://lesswrong.com/lw/1r1/epistemic_luck/) by [Alicorn](https://wiki.lesswrong.com/wiki/Alicorn)\n*   [The Litany Against Gurus](http://lesswrong.com/lw/m2/the_litany_against_gurus/)\n\nSee also\n--------\n\n*   [Information cascade](https://www.lesswrong.com/tag/information-cascades)\n*   [Availability bias](https://wiki.lesswrong.com/wiki/Availability_bias)\n*   [Privileging the hypothesis](https://www.lesswrong.com/tag/privileging-the-hypothesis)\n\nExternal links\n--------------\n\n*   [Epistemic Luck](http://www.iep.utm.edu/epi-luck/), at the Internet Encyclopedia of Philosophy"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb226",
    "name": "Robot",
    "core": null,
    "slug": "robot",
    "oldSlugs": [
      "debate-tools"
    ],
    "postCount": 6,
    "description": {
      "markdown": "An **online debate tool** facilitates the act of debating by helping to manage the structure of argumentation. This distinguishes it from general purpose communication tools such as wikis and forums. Some online debate tools provide graphical representations of arguments, but this is not a requirement.\n\nThis wiki page and tag gives a list and characterization of debate tools. Debate tools were previously [discussed here](http://lesswrong.com/lw/1qq/debate_tools_an_experience_report/).\n\nLiterature\n==========\n\nThere exists an academic literature on argument mapping and other tools (computer aided or not) for assisting debate. The most recent survey seems to be \"[Computer-supported argumentation: A review of the state of the art](http://www.springerlink.com/content/j3p581601n3x1200/)\" written in June 2009, which lists 50 tools (starting on page 94).\n\n[Online Deliberation](https://en.wikipedia.org/wiki/Online_deliberation) is a related discipline that asks what are the effects of online discussions, when are they effective, and how to design better systems. \n\nList of debate tools\n====================\n\n[Debate Map](https://debatemap.app/)\n------------------------------------\n\nSummary: Tree-based mapping of beliefs, arguments, and evidence.\n\n*   first mentioned:\n    *   [a comment by Venryx](http://lesswrong.com/lw/1qq/debate_tools_an_experience_report/dw0i)\n*   pros:\n    *   Collaborative creation, editing, and evaluation of debate/argument maps.\n    *   Open source. (under the MIT license)\n    *   Developed using modern web technologies. (react-js, mobx, firestore)\n    *   Ability to enter both formal arguments (premises and conclusion), or less structured \"single-premise arguments\".\n    *   Rating system for the truth/probability of claims, as well as the relevance/validity of arguments.\n    *   Tree-based structure which can extend very deep without loss of clarity or usability.\n    *   Integrated term/definition system. Terms can be defined once, then used anywhere, with hover-based definition display.\n*   cons:\n    *   Has a learning curve for casual users, as content must conform to the argument<-premise structure at each level.\n    *   Not yet made usable on mobile devices.\n\n[DebateArt](https://www.debateart.com/)\n---------------------------------------\n\nSummary: Debating platform with rich one-on-one debates functionality and advanced discussions forum.\n\n*   pros:\n    *   Clean and convenient design.\n    *   Advanced and flexible one-on-one debating system.\n    *   Separate forum for casual discussions.\n    *   Private messaging system.\n    *   Great performance.\n    *   Friendly community.\n    *   Active and thorough moderation.\n*   cons:\n    *   Has a learning curve for casual users.\n    *   No dedicated mobile version.\n\n[debategraph.org](http://debategraph.org/)\n------------------------------------------\n\nSummary: This... is pretty much exactly what we were looking for, isn't it? Though it doesn't do anything with probabilities.\n\n*   first mentioned:\n    *   [a comment by Peer Infinity](http://lesswrong.com/lw/1qq/debate_tools_an_experience_report/20v5)\n*   pros:\n    *   collaboratively edit argument maps\n*   cons:\n    *   it doesn't do anything with probabilities.\n    *   *Do not zoom out too much!*\n\n[Argunet](http://www.argunet.org/working-with-argunet/)\n-------------------------------------------------------\n\nSummary: Argunet enables you to create argument maps of complex debates online or offline, on your own or in a team.\n\n*   first mentioned:\n    *   [the original article](http://lesswrong.com/lw/1qq/debate_tools_an_experience_report/)\n*   pros:\n    *   collaboratively edit argument maps\n*   cons:\n    *   not entirely straightforward to use, Morendil had trouble figuring out how to move boxes around.\n\n[bCisive Online](http://bcisiveonline.com/)\n-------------------------------------------\n\nSummary: a simple canvas for creating a tree diagram of a debate.\n\n*   first mentioned:\n    *   [the original article](http://lesswrong.com/lw/1qq/debate_tools_an_experience_report/)\n*   pros:\n    *   easy to use\n*   cons:\n    *   all it does is let you make the tree diagram, it doesn't do anything else with the data\n*   examples:\n    *   [a map of Morendil's current thinking on cryonics](http://morendil.bcisiveonline.com/spaces/989ae551bc100d0365c96a7bcc20f188d95fb58d/)\n\n[Flow](http://en.wikipedia.org/wiki/Flow_%28policy_debate%29)\n-------------------------------------------------------------\n\nSummary: a specialized form of note taking called \"flowing\" within the policy/CEDA/NDT debate community.\n\n*   first mentioned:\n    *   [a comment by JenniferRM](http://lesswrong.com/lw/1qq/debate_tools_an_experience_report/1kx1)\n*   pros:\n    *   lots of people have used this technique, and it has been proven to work well\n*   cons:\n    *   it requires a very specific format for the debate\n\n[PyMC](http://code.google.com/p/pymc/)\n--------------------------------------\n\nSummary: a DSL in python for (non-recursive) Bayesian models and Bayesian probability computations.\n\n*   first mentioned:\n    *   [a comment by Steve_Rayhawk](http://lesswrong.com/lw/1qq/debate_tools_an_experience_report/1kx3)\n*   pros:\n    *   it does Bayesian calculations\n*   cons:\n    *   requires literacy in python and bayesian statistics\n\n[MACOSPOL](http://www.demoscience.org/)\n\n*   first mentioned:\n    *   [a comment by Morendil](http://lesswrong.com/lw/1qq/debate_tools_an_experience_report/1kt6)\n*   examples:\n    *   [mapped controversies](http://medialab.sciences-po.fr/controversies/)\n\n[Scott Aaronson's worldview manager](http://projects.csail.mit.edu/worldview/)\n------------------------------------------------------------------------------\n\nSummary: this is designed to point out hidden contradictions (or at least tensions) between one's beliefs, by using programmed in implications to exhibit (possibly long) inferential chains that demonstrate a contradiction.\n\n*   first mentioned:\n    *   [a comment by wnoise](http://lesswrong.com/lw/1qq/debate_tools_an_experience_report/1ku8)\n*   pros:\n    *   it does lots of stuff\n*   cons:\n    *   it's kinda complicated\n*   examples:\n    *   [sample worldview](http://www.gitorious.org/worldview/worldview/blobs/master/topics/axiom_of_choice.wvm)\n    *   [The model of libertarian ideas](http://www.gitorious.org/worldview/worldview/blobs/master/topics/libertarianism.wvm)\n\n[Canonizer.com](http://canonizer.com/)\n--------------------------------------\n\nSummary: Canonizer.com is a wiki system with added camp and survey capabilities. The system provides a rigorous way to measure scientific / moral expert consensus. It is designed for collaborative development of concise descriptions of various competing scientific or moral theories, and the best arguments for such. People can join the camps representing such, giving a quantitative survey or measure of consensus compared to all others. Proposed changes to supported camps go into a review mode for one week. Any supporters of a camp can object to any such proposed change during this time. If it survives a week with no objection, it goes live, guaranteeing unanimous agreement to such changes to the petition by all current signers. If anyone does object, the camp can be forked (taking all supporters of the 'improvement'), or the info can be included in a sporting sub camp.\n\nThe karma or 'canonization' system enables the readers to select any algorithm they wish on the side bar to 'find the good stuff'. For example, you can compare the [mind expert](http://canonizer.com/topic.asp/53/11) scientific consensus with the default general population consensus. Each camp has a forum to discuss and debate further improvements for camps. The general idea is to debate things in the forums, or elsewhere, and summarize everyone's final / current / state of the art view in the camp statements. A history of everything is maintained, providing a dynamic quantitative measure of how well accepted any theory is, as ever more theory falsifying (when experts abandon a falsified camp) scientific data / new arguments... come in.\n\n*   first mentioned:\n    *   [a comment by PeerInfinity](http://lesswrong.com/lw/1qq/debate_tools_an_experience_report/2j4l?c=1)\n*   pros:\n    *   the whole canonization thing\n*   cons:\n    *   it's kinda complicated\n*   examples:\n    *   [the main list of canonized camps](http://canonizer.com)\n\n[Explore-Ideas.com](http://www.explore-ideas.com/)\n--------------------------------------------------\n\nSummary: explore-ideas.com is a graph structure forum where users start with a topic and follow arguments they agree with, creating a personalized 'story'.\n\n*   pros: users can link any two comments and merge ideas from different discussions into a single logical argument, as well as make loops in reasoning. Each user comes to his personal 'win' ending based on arguments (s)he agrees with. There is no global 'win' or 'lose' argument.\n\nThis forum encourages dynamic debate that goes beyond pro/con binary approach, similar to that employed in [Proofs and Refutations](http://en.wikipedia.org/wiki/Proofs_and_Refutations).\n\n*   cons: UI is rudimentary.\n\n[Debate.fm](http://www.debate.fm/)\n----------------------------------\n\nSummary: A simple platform to start a debate on any topic. Mainly focused at general user participation.\n\n*   pros: Simple structured content - side by side. Mainly focused at pro/con binary approach\n*   cons: Till now not ready for academic use.\n\n[cartargrapher](http://cartargrapher.appspot.com/)\n--------------------------------------------------\n\n\"a simple argument mapping app, made using Google’s visualization API, jquery, and python, and running on Google’s AppEngine. Note: at this point, I don’t guarantee the persistence of saved argument maps!\" - [John MacFarlane](http://johnmacfarlane.net/tools.html)\n\n[Consider.it](https://consider.it)\n----------------------------------\n\nGraphically represents people's agreement with a statement and which arguments were most used. The arguments themselves are not subjects of further investigation though. Check out [this use-case](https://hala.consider.it/?tab=Feedback%20on%20key%20principles) of public decisions in Seattle.\n\nIdeas for new tools\n===================\n\n*   Based on MediaWiki, PHP, GraphViz, and maybe XML\n*   Summary: a tool that we make ourselves, so that it works the way we want it to work\n*   first mentioned:\n    *   [a comment by PeerInfinity](http://lesswrong.com/lw/1qq/debate_tools_an_experience_report/1l1w)\n*   pros:\n    *   we're writing it, so we can make it work how we want\n*   cons:\n    *   we would need to write it from scratch\n*   examples:\n    *   [a conversation about AI](http://transhumanistwiki.com/wiki/Peer_Infinity/Chat_With_Fael_About_AI)\n\nBrass Tacks\n-----------\n\n*   [Structured debate](http://issuepedia.org/Issuepedia:Structured_Debate): a set of rules which debate software could help enforce. Some data design is on paper, not yet transcribed. The plan is to write this first as a MediaWiki extension. [some mockups](http://issuepedia.org/Category:Debates)\n\nOther links\n===========\n\n*   [http://www.visualizingargumentation.info/](http://www.visualizingargumentation.info/)\n*   [http://www.tandf.co.uk/journals/tarc](http://www.tandf.co.uk/journals/tarc)\n*   [structured debate](http://issuepedia.org/Structured_debate)\n*   [dispute resolution technology](http://issuepedia.org/Issuepedia:Dispute_Resolution_Technology)\n\nFeatures that a debate tool should have\n=======================================\n\n*   from almost everyone:\n    *   an easy to use interface\n*   from [Morendil](http://lesswrong.com/user/Morendil/):\n    *   a conclusion or a decision, which is to be \"tested\" by the use of the tool\n    *   various hypotheses, which are offered in support or in opposition to the conclusion, with degrees of plausibility\n    *   logical structure, such as \"X follows from Y\"\n    *   challenges to logical structure, such as \"X may not necessarily follow from Y, if you grant Z\"\n    *   elements of evidence, which make hypotheses more or less probable\n    *   recursive relations between these elements\n*   from [PhilGoetz](http://lesswrong.com/user/PhilGoetz/):\n    *   an XML-based representation of the data\n*   from [PeerInfinity](http://lesswrong.com/user/PeerInfinity/)\n    *   generates its results from an annotated log of a debate\n    *   collaboratively editable, possibly using MediaWiki\n    *   multiple outfut formats: graphs, tables, the raw data\n*   from [Johnicholas](http://lesswrong.com/user/Johnicholas/):\n    *   Compose in ordinary ASCII or UTF-8\n    *   Compose primarily a running-text argument, indicating the formal structure with annotations\n    *   Export as a prettified document, still mostly running text (html and LaTeX)\n    *   Export as a diagram (automatically layed out, perhaps by graphviz)\n    *   Export as a bayes net (in possibly several bayes net formats)\n    *   Export as a machine-checkable proof (in possibly several formats)\n*   from [Eliezer Yudkowsky](https://www.lesswrong.com/tag/eliezer-yudkowsky):\n    *   prevents online arguments from retracing the same points over and over.\n    *   not just graphical with boxes, because that makes poor use of screen real estate.\n    *   not have lots of fancy argument types and patterns, because no one really uses that stuff\n    *   a karma system, because otherwise there's no way to find the good stuff.\n\n(So, now that everything's all neatly arranged in a list, the next step is to decide whether we want to start using any of these tools, or if we want to create our own.)\n\nLWers interested in developing debate tools\n===========================================\n\n*   [Curtis SerVaas](http://lesswrong.com/lw/le5/welcome_to_less_wrong_7th_thread_december_2014/c64f)\n*   [.impact has discussed making argument mapping software](http://lesswrong.com/lw/jfn/introducing_impact/)\n*   The \"Collaborative Argumentation Analysis\" Facebook Group has a lot of links/discussion/people.\n\nSee also\n========\n\n*   [Rationality power tools](https://wiki.lesswrong.com/wiki/Rationality_power_tools)\n*   [Prediction market](https://www.lesswrong.com/tag/prediction-markets)\n*   [PredictionBook](https://www.lesswrong.com/tag/predictionbook)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb210",
    "name": "Reversed Stupidity Is Not Intelligence",
    "core": null,
    "slug": "reversed-stupidity-is-not-intelligence",
    "oldSlugs": null,
    "postCount": 3,
    "description": {
      "markdown": "It takes a lot of evidence and rationality just to [*locate* a good hypothesis](https://www.lesswrong.com/tag/locate-the-hypothesis) in the search space. Getting it right is a precise Art, whereas there are a million ways to get it wrong. A stopped clock is right twice a day; answering true-or-false questions at entirely at random will still yield a fifty percent success rate. To [do better](https://www.lesswrong.com/tag/tsuyoku-naritai), you can't just perform a reversal of an obvious fallacy. **Reversed stupidity is not intelligence**; faulty reasoning does not produce \"antitrue\" conclusions that can just be negated; the true theory should make sense on its own terms; not just as a refutation of some nonsense.\n\nBlog posts\n----------\n\n*   [Reversed Stupidity is Not Intelligence](http://lesswrong.com/lw/lw/reversed_stupidity_is_not_intelligence/)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb20f",
    "name": "Something To Protect",
    "core": null,
    "slug": "something-to-protect",
    "oldSlugs": null,
    "postCount": 9,
    "description": {
      "markdown": "See Also\n--------\n\n*   [Heroic Responsibility](/tag/heroic-responsibility)\n\nBlog posts\n----------\n\n*   [Something to Protect](http://lesswrong.com/lw/nb/something_to_protect/)\n*   [Newcomb's Problem and Regret of Rationality](http://lesswrong.com/lw/nc/newcombs_problem_and_regret_of_rationality/)\n*   [Rationality is Systematized Winning](http://lesswrong.com/lw/7i/rationality_is_systematized_winning/)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb1f2",
    "name": "Valley of Bad Rationality",
    "core": null,
    "slug": "valley-of-bad-rationality",
    "oldSlugs": null,
    "postCount": 15,
    "description": {
      "markdown": "> Then I finally reply that my experience so far - even in this realm of merely human possibility - does seem to indicate that, once you sort yourself out a bit and you aren't doing quite so many other things wrong, striving for more rationality actually will make you better off. The long road leads out of the valley and higher than before, even in the human lands.\n> \n> —[Incremental Progress and the Valley](http://lesswrong.com/lw/7k/incremental_progress_and_the_valley/)\n\nIt has been observed that when someone is just starting to learn rationality, they sometimes appear to be worse off than they were before. Someone else may then allege that after this person learns even more about rationality, they will finally be better off than they were before they started. The period before this improvement is known as \"the valley of bad rationality\".\n\nNon-post example:\n-----------------\n\n*   [This comment](http://lesswrong.com/lw/5f/bayesians_vs_barbarians/7hr) by Scott Alexander on [Bayesians vs. Barbarians](http://lesswrong.com/lw/5f/bayesians_vs_barbarians/)\n\nSee also\n--------\n\n*   [Value of Rationality](https://www.lesswrong.com/tag/value-of-rationality)\n*   [Costs of rationality](https://www.lesswrong.com/tag/costs-of-rationality)\n*   [Dangerous knowledge](https://www.lesswrong.com/tag/dangerous-knowledge)\n*   [Debiasing](https://www.lesswrong.com/tag/debiasing)\n*   [Information Hazards](https://www.lesswrong.com/tag/information-hazards)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb1ec",
    "name": "Illusion of Transparency",
    "core": null,
    "slug": "illusion-of-transparency",
    "oldSlugs": null,
    "postCount": 8,
    "description": {
      "markdown": "The **illusion of transparency** is the misleading impression that your words convey more to others than they really do. Words are a means of communication, but they don't in themselves *contain* meaning. The word *apple* is just five letters, two syllables. I use it to *refer* to a concept and its associations in my mind, under the reasonable assumption that it refers to a similar concept and group of associations in *your* mind; this is the only power words have, great though it may be. Unfortunately, it's easy to lose track of this fact, think as if your words have meanings inherently encoded in them, leading to a tendency to systematically overestimate the effectiveness of communication.\n\nRelated Pages\n--------------\n\n*   [Inferential distance](https://www.lesswrong.com/tag/inferential-distance)\n*   [Detached lever fallacy](https://www.lesswrong.com/tag/detached-lever-fallacy)\n*   [Absurdity heuristic](https://www.lesswrong.com/tag/absurdity-heuristic)\n*   [Mind projection fallacy](https://www.lesswrong.com/tag/mind-projection-fallacy)\n\nExternal Links\n--------------\n\n*   [How all human communication fails, except by accident, or a commentary of Wiio's laws](http://www.cs.tut.fi/~jkorpela/wiio.html)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb1dc",
    "name": "Updateless Decision Theory",
    "core": null,
    "slug": "updateless-decision-theory",
    "oldSlugs": null,
    "postCount": 17,
    "description": {
      "markdown": "Motivation\n----------\n\n**Updateless Decision Theory** (UDT) is a decision theory meant to deal with a fundamental problem in the existing decision theories: the need to treat the agent as a part of the world in which it makes its decisions. In contrast, in the most common decision theory today, [Causal Decision Theory](https://www.lesswrong.com/tag/causal-decision-theory) (CDT), the deciding agent is not part of the world model--its decision is the output of the CDT, but the agent's decision in the world context is \"magic\": in the moment of deciding, no causal links feed into its chosen action. It acts as though its decision was causeless, as in some dualist free-will theories.\n\nGetting this issue right is critical in building a self-improving artificial general intelligence, as such an AI must analyze its own behavior and that of a next generation that it may build.\n\nUpdateless Decision Theory was invented by [Wei Dai](https://www.lesswrong.com/users/wei_dai) and first described in [Towards a New Decision Theory](https://www.lesswrong.com/posts/de3xjFaACCAk6imzv/towards-a-new-decision-theory).\n\nSee also\n--------\n\n*   [Timeless decision theory](https://www.lesswrong.com/tag/timeless-decision-theory)\n*   [Ambient decision theory](https://www.lesswrong.com/tag/ambient-decision-theory)\n*   [Counterfactual mugging](https://www.lesswrong.com/tag/counterfactual-mugging)\n*   [Decision theory](https://www.lesswrong.com/tag/decision-theory)\n*   [Functional Decision Theory](https://www.lesswrong.com/posts/AGAGgoWymRhJ5Rqyv/functional-decision-theory-a-new-theory-of-instrumental)\n*   [Embedded Agency](https://www.lesswrong.com/tag/embedded-agency)\n\nContent\n-------\n\nUDT specifies that the optimal agent is the one with the best algorithm--the best mapping from observations to actions--across a probability distribution of all world-histories. (\"Best\" here, as in other decision theories, means one that maximizes a utility/reward function.)\n\nThis definition may seem trivial, but in contrast, CDT says that an agent should choose the best \\*option\\* at any given moment, based on the effects of that action. As in [Judea Pearl's definition of causality](http://lesswrong.com/lw/emc/causality_a_chapter_by_chapter_review/), CDT ignores any causal links inbound to the decider, treating this agent as an uncaused cause. The agent is unconcerned about what evidence its decision may provide about the agent's own mental makeup--evidence which may suggest that the agent will make suboptimal decisions in other cases.\n\nEvidential Decision Theory is the other leading decision theory today. It says that the agent should make the choice for which the expected utility, as calculated with Bayes' Rule, is the highest. EDT avoids CDT's pitfalls, but has its own flaw: It ignores the distinction between causation and correlation. In CDT, the agent is an uncaused cause, and in EDT, the converse: It is caused, but not a cause.\n\nOne valuable insight from EDT is reflected in \"UDT 1.1\" (see article by McAllister in references), a variant of UDT in which the agent takes into account that some of its algorithm (mapping from observations to actions) may be prespecified and not entirely in its control, so that it has to gather evidence and draw conclusions about part of its own mental makeup. The difference between UDT 1.0 and 1.1 is that UDT 1.1 iterates over policies, whereas UDT 1.0 iterates over actions. \n\nBoth UDT and [Timeless Decision Theory](https://www.lesswrong.com/tag/timeless-decision-theory) (TDT) make decisions on the basis of what you would have pre-committed to. The difference is that UDT asks what you would have pre-committed to without the benefit of any observations you have made about the universe, while TDT asks what you would have pre-committed to give all information you've observed so far. This means that UDT pays in [Counterfactual Mugging](https://www.lesswrong.com/tag/counterfactual-mugging), while TDT does not.\n\nUDT is very similar to Functional Decision Theory (FDT), but there are differences. FDT doesn't include the UDT1.1 fix and Nate Soares [states](https://www.lesswrong.com/posts/2THFt7BChfCgwYDeA/let-s-discuss-functional-decision-theory?commentId=LzPH8utKGSf97NihW): \"Wei Dai doesn't endorse FDT's focus on causal-graph-style counterpossible reasoning; IIRC he's holding out for an approach to counterpossible reasoning that falls out of evidential-style conditioning on a logically uncertain distribution\". Rob Bensinger says that he's heard UDT described as \"FDT + a theory of anthropics\".\n\nSince it is formalised using input-output maps instead of in terms of situations, it allows us to make predictions about what an agent would do given [input representing an inconsistent situation](https://www.lesswrong.com/posts/EXtzy3v4soZcoZjuH/a-short-note-on-udt), which can be important when dealing with perfect predictors.\n\nLogical Uncertainty\n-------------------\n\nA robust theory of [logical uncertainty](https://www.lesswrong.com/tag/logical-uncertainty) is essential to a full formalization of UDT.  A UDT agent must calculate probabilities and expected values on the outcome of its possible actions in all possible worlds--sequences of observations and its own actions. However, it does not know its own actions in all possible worlds. (The whole point is to derive its actions.) On the other hand, it does have some knowledge about its actions, just as you know that you are unlikely to walk straight into a wall the next chance you get. So, the UDT agent models itself as an algorithm, and its probability distribution about what it itself will do is an important input into its maximization calculation.\n\nLogical uncertainty is an area which has not yet been properly formalized, and much UDT research is focused on this area.\n\nBlog posts\n----------\n\n*   [Indexical uncertainty and the Axiom of Independence](http://lesswrong.com/lw/102/indexical_uncertainty_and_the_axiom_of/) by Wei Dai\n*   [Towards a New Decision Theory](http://lesswrong.com/lw/15m/towards_a_new_decision_theory/) by [Wei Dai](http://weidai.com/)\n*   [Anthropic Reasoning in UDT](http://lesswrong.com/lw/175/torture_vs_dust_vs_the_presumptuous_philosopher/) by Wei Dai\n*   [The Absent-Minded Driver](http://lesswrong.com/lw/182/the_absentminded_driver/) by Wei Dai\n*   [Why (and why not) Bayesian Updating?](http://lesswrong.com/lw/1fu/why_and_why_not_bayesian_updating/) by Wei Dai\n*   [What Are Probabilities, Anyway?](http://lesswrong.com/lw/1iy/what_are_probabilities_anyway/) by Wei Dai\n*   [Explicit Optimization of Global Strategy (Fixing a Bug in UDT1)](http://lesswrong.com/lw/1s5/explicit_optimization_of_global_strategy_fixing_a/) by Wei Dai\n*   [List of Problems That Motivated UDT](http://lesswrong.com/lw/cs9/list_of_problems_that_motivated_udt/) by Wei Dai\n*   [Another attempt to explain UDT](http://lesswrong.com/lw/334/another_attempt_to_explain_udt/) by cousin_it\n*   [What is Wei Dai's Updateless Decision Theory?](http://lesswrong.com/lw/294/what_is_wei_dais_updateless_decision_theory/) by AlephNeil\n*   [Comparison of decision theories (with a focus on logical-counterfactual decision theories)](https://www.lesswrong.com/posts/QPhY8Nb7gtT5wvoPH/comparison-of-decision-theories-with-a-focus-on-logical), by riceissa. (Gives a good description of UDT 1.0 vs 1.1)\n*   [All posts tagged \"UDT\"](http://lesswrong.com/tag/udt/)\n\nRelevant Comments\n-----------------\n\nIn addition to whole posts on UDT, there are also a number of comments which contain important information, often on less relevant posts.\n\n*   [A comment about UDT2](http://lesswrong.com/lw/jhj/functional_side_effects/adhy)\n\nExternal links\n--------------\n\n*   [Formal description of UDT](https://drive.google.com/file/d/0BzUiCL-Kpxc1NGxab3ZfZGZkVUE/view?usp=sharing&resourcekey=0-EuvTP8RRdpKivUtHwfqgSQ) by Tyrrell McAllister\n*   [UDT with known search order](http://intelligence.org/2014/10/30/new-report-udt-known-search-order/) by Tsvi Benson-Tilsen\n*   [Problem Class Dominance in Predictive Dilemmas](http://intelligence.org/files/ProblemClassDominance.pdf), section 3.4. (The best summary to date.)\n*   [An introduction to decision theory](https://formalisedthinking.wordpress.com/2010/08/18/an-introduction-to-decision-theory/) (series of posts)\n*   [Arbital page on updateless decision theories](https://arbital.com/p/updateless_dt/) by Eliezer Yudkowsky"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb1db",
    "name": "Timeless Decision Theory",
    "core": null,
    "slug": "timeless-decision-theory",
    "oldSlugs": null,
    "postCount": 15,
    "description": {
      "markdown": "**Timeless decision theory** (TDT) is a [decision theory](https://www.lesswrong.com/tag/decision-theory), [developed by Eliezer Yudkowsky](http://intelligence.org/files/TDT.pdf) which, in slogan form, says that agents should decide as if they are determining the output of the abstract computation that they implement. This theory was developed in response to the view that rationality should be about winning (that is, about agents achieving their desired ends) rather than about behaving in a manner that we would intuitively label as rational. Prominent existing decision theories (including [causal decision theory](https://www.lesswrong.com/tag/causal-decision-theory), or CDT) fail to choose the winning decision in some scenarios and so there is a need to develop a more successful theory.\n\nTimeless Decision Theory has been replaced by [Functional Decision Theory](https://www.lesswrong.com/tag/functional-decision-theory)\n------------------------------------------------------------------------------------------------------------------------------------\n\n<more needed>\n\nTDT and Newcomb's problem\n-------------------------\n\nA better sense of the motivations behind, and form of, TDT can be gained by considering a particular decision scenario: [Newcomb's problem](http://lesswrong.com/lw/nc/newcombs_problem_and_regret_of_rationality/). In Newcomb's problem, a superintelligent artificial intelligence, Omega, presents you with a transparent box and an opaque box. The transparent box contains $1000 while the opaque box contains either $1,000,000 or nothing. You are given the choice to either take both boxes (called two-boxing) or just the opaque box (one-boxing). However, things are complicated by the fact that Omega is an almost perfect predictor of human behavior and has filled the opaque box as follows: if Omega predicted that you would one-box, it filled the box with $1,000,000 whereas if Omega predicted that you would two-box it filled it with nothing.\n\nMany people find it intuitive that it is rational to two-box in this case. As the opaque box is already filled, you cannot influence its contents with your decision so you may as well take both boxes and gain the extra $1000 from the transparent box. CDT formalizes this style of reasoning. However, one-boxers win in this scenario. After all, if you one-box then Omega (almost certainly) predicted that you would do so and hence filled the opaque box with $1,000,000. So you will almost certainly end up with $1,000,000 if you one-box. On the other hand, if you two-box, Omega (almost certainly) predicted this and so left the opaque box empty . So you will almost certainly end up with $1000 (from the transparent box) if you two-box. Consequently, if rationality is about winning then it's rational to one-box in Newcomb's problem (and hence CDT fails to be an adequate decision theory).\n\nTDT will endorse one-boxing in this scenario and hence endorses the winning decision. When Omega predicts your behavior, it carries out the same abstract computation as you do when you decide whether to one-box or two-box. To make this point clear, we can imagine that Omega makes this prediction by creating a simulation of you and observing its behavior in Newcomb's problem. This simulation will clearly decide according to the same abstract computation as you do as both you and it decide in the same manner. Now given that TDT says to act as if deciding the output of this computation, it tells you to act as if your decision to one-box can determine the behavior of the simulation (or, more generally, Omega's prediction) and hence the filling of the boxes. So TDT correctly endorses one-boxing in Newcomb's problem as it tells the agent to act as if doing so will lead them to get $1,000,000 instead of $1,000.\n\nTDT and other decision scenarios\n--------------------------------\n\nTDT also wins in a range of other cases including [medical Newcomb's problems](https://www.lesswrong.com/tag/smoking-lesion), [Parfit's hitchhiker](http://lesswrong.com/lw/135/timeless_decision_theory_problems_i_cant_solve/), and [the one-shot prisoners' dilemma](https://www.lesswrong.com/tag/prisoner-s-dilemma). However, there are [other scenarios](http://lesswrong.com/lw/135/timeless_decision_theory_problems_i_cant_solve/) where TDT does not win, including [counterfactual mugging](http://lesswrong.com/lw/3l/counterfactual_mugging/). This suggests that TDT still requires further development if it is to become a fully adequate decision theory. Given this, there is some motivation to also consider alternative decision theories alongside TDT, like [updateless decision theory](http://lesswrong.com/lw/15m/towards_a_new_decision_theory/) (UDT), which also wins in a range of scenarios but has its own problem cases. It seems likely that both of these theories draw on insights which are crucial to progressing our understanding of decision theory. So while TDT requires further development to be entirely adequate, it nevertheless represents a substantial step toward developing a decision theory that always endorses the winning decision\n\nFormalization of TDT\n--------------------\n\nComing to fully grasp TDT requires an understanding of how the theory is formalized. Very briefly, TDT is formalized by supplementing causal Bayesian networks, which can be thought of as graphs representing causal relations, in two ways. First, these graphs should be supplemented with nodes representing abstract computations and an agent's uncertainty about the result of these computations. Such a node might represent an agent's uncertainty about the result of a mathematical sum. Second, TDT treats decisions as the abstract computation that underlies the agent's decision process. These two features transform causal Bayesian networks into timeless decision diagrams. Using these supplemented diagrams, TDT is able to determine the winning decision in a whole range of a decision scenarios. For a more detailed description of the formalization of TDT, see Eliezer Yudkowsky's [timeless decision theory paper](http://intelligence.org/files/TDT.pdf).\n\nFurther Reading\n---------------\n\n*   [A Comparison of Decision Algorithms on Newcomblike Problems](https://intelligence.org/files/Comparison.pdf), by Alex Altair\n*   [Problem Class Dominance in Predictive Dilemmas](https://intelligence.org/wp-content/uploads/2014/10/Hintze-Problem-Class-Dominance-In-Predictive-Dilemmas.pdf), by Danny Hintze\n\nNotable Posts\n-------------\n\n*   [Ingredients of Timeless Decision Theory](http://lesswrong.com/lw/15z/ingredients_of_timeless_decision_theory/)\n*   [Timeless Decision Theory: Problems I Can't Solve](http://lesswrong.com/lw/135/timeless_decision_theory_problems_i_cant_solve/)\n*   [Timeless Decision Theory and Meta-Circular Decision Theory](http://lesswrong.com/lw/164/timeless_decision_theory_and_metacircular/)\n*   [Decision theory: Why Pearl helps reduce \"could\" and \"would\", but still leaves us with at least three alternatives](http://lesswrong.com/lw/17b/decision_theory_why_pearl_helps_reduce_could_and/) by [Anna Salamon](https://www.lesswrong.com/tag/anna-salamon)\n\nExternal Links\n--------------\n\n*   [Timeless Decision Theory](http://intelligence.org/files/TDT.pdf) (2010) by [Eliezer Yudkowsky](https://www.lesswrong.com/tag/eliezer-yudkowsky)\n*   [An Introduction to Timeless Decision Theory](http://formalisedthinking.wordpress.com/2010/08/19/an-introduction-to-timeless-decision-theory/) at Formalised Thinking\n\nSee Also\n--------\n\n*   [Decision theory](https://www.lesswrong.com/tag/decision-theory)\n*   [Newcomb's problem](https://www.lesswrong.com/tag/newcomb-s-problem)\n*   [Causality](https://www.lesswrong.com/tag/causality)\n*   [Updateless decision theory](https://www.lesswrong.com/tag/updateless-decision-theory)\n*   [Rationality is systematized winning](https://www.lesswrong.com/tag/rationality-is-systematized-winning)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb1c9",
    "name": "Many-Worlds Interpretation",
    "core": null,
    "slug": "many-worlds-interpretation",
    "oldSlugs": null,
    "postCount": 26,
    "description": {
      "markdown": "Standard [quantum mechanics](https://www.lesswrong.com/tag/quantum-physics) is made of two parts: a part that describes the unitary and deterministic evolution of a state vector, and a part that describes how a state vector randomly collapses when subjected to \"measurement\".  \nThe **many-worlds interpretation** (MWI) [cuts away](https://www.lesswrong.com/tag/occam-s-razor) the latter part. It uses [decoherence](https://www.lesswrong.com/tag/decoherence) to explain how the universe splits into many separate branches, each of which looks like it came out of a random collapse.\n\nSee also\n--------\n\n*   [Quantum mechanics](https://www.lesswrong.com/tag/quantum-physics)\n*   [Decoherence](https://www.lesswrong.com/tag/decoherence)\n\nExternal links\n--------------\n\n*   [Decoherence and Ontology](http://users.ox.ac.uk/~mert0130/papers/proc_dec.pdf) (David Wallace)\n*   [The Everett FAQ](http://www.hedweb.com/manworld.htm) (Michael Clive Price)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb1be",
    "name": "Intelligence Explosion",
    "core": null,
    "slug": "intelligence-explosion",
    "oldSlugs": null,
    "postCount": 19,
    "description": {
      "markdown": "An **intelligence explosion** is theoretical scenario in which an intelligent agent analyzes the processes that produce its intelligence, improves upon them, and creates a successor which does the same. This process repeats in a positive feedback loop– each successive agent more intelligent than the last and thus more able to increase the intelligence of its successor – until some limit is reached. This limit is conjectured to be much, much higher than human intelligence.\n\nA strong version of this idea suggests that once the positive feedback starts to play a role, it will lead to a very dramatic leap in capability very quickly. This is known as a “hard takeoff.” In this scenario, technological progress drops into the characteristic timescale of transistors rather than human neurons, and the ascent rapidly surges upward and creates superintelligence (a mind orders of magnitude more powerful than a human's) before it hits physical limits. A hard takeoff is distinguished from a \"soft takeoff\" only by the speed with which said limits are reached.\n\nPublished arguments\n-------------------\n\nPhilosopher David Chalmers published a [significant analysis of the Singularity](http://consc.net/papers/singularity.pdf), focusing on intelligence explosions, in *Journal of Consciousness Studies*. [His analysis](https://wiki.lesswrong.com/wiki/Singularity#Chalmers.27_analysis) of how they could occur defends the likelihood of an intelligence explosion. He performed a very careful analysis of the main premises and arguments for the existence of the a singularity from an intelligence explosion. According to him, the main argument is:\"\n\n*   1\\. There will be AI (before long, absent defeaters).\n*   2\\. If there is AI, there will be AI+ (soon after, absent defeaters).\n*   3\\. If there is AI+, there will be AI++ (soon after, absent defeaters).\n\n—————-\n\n*   4\\. There will be AI++ (before too long, absent defeaters). \"\n\nHe also discusses the nature of general intelligence, and possible obstacles to a singularity. A good deal of discussion is given to the dangers of an intelligence explosion, and Chalmers concludes that we must negotiate it very carefully by building the correct values into the initial AIs.\n\n[Luke Muehlhauser](http://lesswrong.com/user/lukeprog) and [Anna Salamon](http://lesswrong.com/user/AnnaSalamon) argue in [*Intelligence Explosion: Evidence and Import*](http://intelligence.org/files/IE-EI.pdf) in detail that there is a substantial chance of an intelligence explosion within 100 years, and extremely critical in determining the future. They trace the implications of many types of upcoming technologies, and point out the feedback loops present in them. This leads them to deduce that an above-human level AI will almost certainly lead to an intelligence explosion. They conclude with recommendations for bringing about a safe intelligence explosion.\n\nHypothetical path\n-----------------\n\nThe following is a common example of a possible path for an AI to bring about an intelligence explosion. First, the AI is smart enough to conclude that inventing molecular nanotechnology will be of greatest benefit to it. Its first act of recursive self-improvement is to gain access to other computers over the internet. This extra computational ability increases the depth and breadth of its search processes. It then uses gained knowledge of material physics and a distributed computing program to invent the first general assembler nanomachine. Then it uses some manufacturing technology, accessible from the internet, to build and deploy the nanotech. It programs the nanotech to turn a large section of bedrock into a supercomputer. This is its second act of recursive self-improvement, only possible because of the first. Then it could use this enormous computing power to consider hundreds of alternative decision algorithms, better computing structures and so on. After this, this AI would go from a near to human level intelligence to a superintelligence, providing a dramatic and abruptly increase in capability.\n\nBlog posts\n----------\n\n*   [Cascades, Cycles, Insight...](http://lesswrong.com/lw/w5/cascades_cycles_insight/), [...Recursion, Magic](http://lesswrong.com/lw/w6/recursion_magic/)\n*   [Recursive Self-Improvement](http://lesswrong.com/lw/we/recursive_selfimprovement/), [Hard Takeoff](http://lesswrong.com/lw/wf/hard_takeoff/), [Permitted Possibilities, & Locality](http://lesswrong.com/lw/wg/permitted_possibilities_locality/)\n\nSee also\n--------\n\n*   [Technological singularity](https://wiki.lesswrong.com/wiki/Technological_singularity), [Hard takeoff](https://wiki.lesswrong.com/wiki/Hard_takeoff)\n*   [Existential risk](https://www.lesswrong.com/tag/existential-risk)\n*   [Artificial General Intelligence](https://www.lesswrong.com/tag/artificial-general-intelligence)\n*   [Lawful intelligence](https://www.lesswrong.com/tag/lawful-intelligence)\n*   [The Hanson-Yudkowsky AI-Foom Debate](https://www.lesswrong.com/tag/the-hanson-yudkowsky-ai-foom-debate)\n\nExternal links\n--------------\n\n*   [Intelligence Explosion website](http://intelligenceexplosion.com/), a landing page for introducing the concept\n*   [Three Major Singularity Schools](http://yudkowsky.net/singularity/schools)\n\nReferences\n----------\n\n*   Good, Irving John (1965). Franz L. Alt and Morris Rubinoff. ed. \"[Speculations concerning the first ultraintelligent machine](http://commonsenseatheism.com/wp-content/uploads/2011/02/Good-Speculations-Concerning-the-First-Ultraintelligent-Machine.pdf).\" *Advances in computers* (New York: Academic Press) **6**: 31-88. [doi](https://wiki.lesswrong.com/index.php?title=Digital_object_identifier&action=edit&redlink=1):[10.1016/S0065-2458(08)60418-0](http://dx.doi.org/10.1016%2FS0065-2458%2808%2960418-0).\n*   David Chalmers (2010). \"[The Singularity: A Philosophical Analysis](http://consc.net/papers/singularity.pdf).\" *Journal of Consciousness Studies* **17**: 7-65.\n*   Muehlhauser, Luke; Salamon, Anna (2012). [\"Intelligence Explosion: Evidence and Import\"](http://commonsenseatheism.com/wp-content/uploads/2012/02/Muehlhauser-Salamon-Intelligence-Explosion-Evidence-and-Import.pdf). in Eden, Amnon; Søraker, Johnny; Moor, James H. et al. *The singularity hypothesis: A scientific and philosophical assessment*. Berlin: Springer."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb1b8",
    "name": "Free Will",
    "core": null,
    "slug": "free-will",
    "oldSlugs": null,
    "postCount": 27,
    "description": {
      "markdown": "**Free will** is one of the *easiest* hard questions, as millennia-old philosophical dilemmas go. Though this [impossible question](http://lesswrong.com/lw/og/wrong_questions/) is generally considered [fully and completely dissolved](http://lesswrong.com/lw/of/dissolving_the_question/) on Less Wrong, aspiring reductionists should **try to solve it on their own**.\n\nNon-spoiler posts\n-----------------\n\nThe following posts can be read to set up the problem of \"free will\" and what constitutes a good solution from a reductionist perspective, without entirely giving away the solution. (When Yudkowsky wrote these posts, he thought he could get away with just leaving it as a practice problem, and some of the posts state that the problem will be left open. However Yudkowsky did eventually find that he needed to write out the whole solution.)\n\n*   [How An Algorithm Feels From Inside](http://lesswrong.com/lw/no/how_an_algorithm_feels_from_inside/) (see also the [wiki page](https://www.lesswrong.com/tag/how-an-algorithm-feels))\n*   [**Dissolving the Question**](http://lesswrong.com/lw/of/dissolving_the_question/) \\- this is where the \"free will\" puzzle is explicitly posed, along with criteria for what does and does not constitute a satisfying answer.\n*   [Wrong Questions](http://lesswrong.com/lw/og/wrong_questions/)\n*   [Righting a Wrong Question](http://lesswrong.com/lw/oh/righting_a_wrong_question/)\n\nFor spoiler posts see [free will (solution)](https://www.lesswrong.com/tag/free-will-solution).\n\nSee also\n--------\n\n*   [How an algorithm feels from the inside](https://www.lesswrong.com/tag/how-an-algorithm-feels)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb1b6",
    "name": "Counterfactual Mugging",
    "core": null,
    "slug": "counterfactual-mugging",
    "oldSlugs": null,
    "postCount": 16,
    "description": {
      "markdown": "**Counterfactual mugging** is a thought experiment for testing and differentiating [decision theories](https://www.lesswrong.com/tag/decision-theory), stated as follows:\n\n> Omega, a perfect predictor, flips a coin. If it comes up tails Omega asks you for $100. If it comes up heads, Omega pays you $10,000 if it predicts that you would have paid if it had come up tails.\n\nDepending on how the problem is phrased, intuition calls for different answers. For example, [Eliezer Yudkowsky](https://www.lesswrong.com/tag/eliezer-yudkowsky) has argued that framing the problem in a way Omega is a regular aspect of the environment which regularly asks such types of questions makes most people answer 'Yes'. However, Vladimir Nesov points out that [Rationalists Should Win](https://www.lesswrong.com/posts/4ARtkT3EYox3THYjF/rationality-is-systematized-winning) could be interpreted as suggesting that we should not pay. After all, even though paying in the tails case would cause you to do worse in the counterfactual where the coin came up heads, you already know the counterfactual didn't happen, so it's not obvious that you should pay. This issue has been discussed in [this question](https://www.lesswrong.com/posts/h9qQQA3g8dwq6RRTo/counterfactual-mugging-why-should-you-pay#h9Lc5j42HPao4aaep).\n\nFormal decision theories also diverge. For [Causal Decision Theory](https://www.lesswrong.com/tag/causal-decision-theory), you can only affect those probabilities that you are causally linked to. Hence, the answer should be 'No'. In [Evidential Decision Theory](https://www.lesswrong.com/tag/evidential-decision-theory) any kind of connection is accounted, then the answer should be 'No'. [Timeless Decision Theory](https://www.lesswrong.com/tag/timeless-decision-theory) answer seems undefined, however Yudkowsky has argued that if the problem is recurrently presented, one should answer 'Yes' on the basis of enhancing its probability of gaining $10000 in the next round. This seems to be Causal Decision Theory prescription as well. [Updateless decision theory](https://www.lesswrong.com/tag/updateless-decision-theory)[1](http://lesswrong.com/lw/15m/towards_a_new_decision_theory/) prescribes giving the $100, on the basis your decision can influence both the 'heads branch' and 'tails branch' of the universe.\n\nRegardless of the particular decision theory, it is generally agreed that if you can pre-commit in advance that you should do so. The dispute is purely over what you should do if you didn't pre-commit.\n\nEliezer listed this in his 2009 post [Timeless Decision Theory Problems I can't Solve](https://www.lesswrong.com/posts/c3wWnvgzdbRhNnNbQ/timeless-decision-theory-problems-i-can-t-solve), although that was written before [Updateless Decision Theory](https://www.lesswrong.com/tag/updateless-decision-theory).\n\nVariants\n--------\n\nThe [Counterfactual Prisoner's Dilemma](https://www.lesswrong.com/posts/sY2rHNcWdg94RiSSR/the-counterfactual-prisoner-s-dilemma) is a symmetric variant of he original independently suggested by Chris Leong and Cousin_it:\n\n> Omega, a perfect predictor, flips a coin. If if comes up heads, Omega asks you for $100, then pays you $10,000 if it predict you would have paid if it had come up tails and you were told it was tails. If it comes up tails, Omega asks you for $100, then pays you $10,000 if it predicts you would have paid if it had come up heads and you were told it was heads\n\nIn this scenario, an updateless agent receives $9900 and an updateful agent receives nothing regardless of the coin flip, while in the original scenario the upateless agent only comes out ahead if the coin shows tails. This is claimed as a demonstration of the principle that when evaluating decisions we should consider the counterfactual and not just our particular branch of possibility space.\n\nIn [Logical Counterfactual Mugging](https://www.lesswrong.com/posts/rqt8RSKPvh4GzYoqE/counterfactual-mugging-and-logical-uncertainty) instead of flipping a coin, Omega tells you the 10,000th digit of pi, which we assume you don't know off the top of your head. If it is odd, we treat it like heads in the original problem and if it is even treat it like tails. Logical inductors have been proposed as a solution to this problem. Applying this to [Logical Counterfactual Mugging](https://www.lesswrong.com/posts/XzvR3QKkt9EPbAYyT/applying-the-counterfactual-prisoner-s-dilemma-to-logical).\n\nThe [Counterfactual Mugging Poker Game](https://www.lesswrong.com/posts/g3PwPgcdcWiP33pYn/counterfactual-mugging-poker-game) is a somewhat complicated variant by Scott Garrabrant. Player A receives a single card that is either high or low, which they can then reveal if they so desire. Player B then shares their true probability estimate that player A has a high card. Player B is essentially perfect at predicting your behaviour, but doesn't get to see you after you've drawn the card. Additionally, player A loses \\\\(p^2\\\\) dollars. If you show the card if it is low, then you lose 0. However, since B can predict your behaviour, this means that if the card had been high then player B would be able to guess that you had a high card even if you hadn't revealed it. This would lose you a whole dollar and on average you'd be better if you always showed it. Garrabrant states that he prefers this scenario because Counterfactual Mugging feels like it is trying to trick you, while in this scenario you are the one creating the Counterfactual Mugging like situation to withhold information.\n\nComparison to Other Problem\n---------------------------\n\nIn [Two Types of Updatelessness](https://www.lesswrong.com/posts/pneKTZG9KqnSe2RdQ/two-types-of-updatelessness), makes a distinction between all-upside updatelessness and mixed-upside updatelessness. In all-upside case, utilising an updateless decision theory provides a better result in the current situation, while in a mixed-upside case the benefits go to other possible selves. Unlike [Newcomb's Problem](https://www.lesswrong.com/tag/newcomb-s-problem) or [Parfait's Hitchhiker](https://www.lesswrong.com/tag/parfits-hitchhiker), Counterfactual Mugging is a mixed-upside case.\n\nBlog posts\n----------\n\n*   [Counterfactual Mugging](http://lesswrong.com/lw/3l/counterfactual_mugging/) by [Vladimir Nesov](https://wiki.lesswrong.com/wiki/Vladimir_Nesov)\n*   [Timeless Decision Theory: Problems I Can't Solve](http://lesswrong.com/lw/135/timeless_decision_theory_problems_i_cant_solve/) by [Eliezer Yudkowsky](https://www.lesswrong.com/tag/eliezer-yudkowsky)\n*   [Towards a New Decision Theory](http://lesswrong.com/lw/15m/towards_a_new_decision_theory/) by [Wei Dai](http://weidai.com/)\n*   [The sin of updating when you can change whether you exist](http://lesswrong.com/lw/jrm/the_sin_of_updating_when_you_can_change_whether/) by Benya Fallenstein\n*   [Counterfactual Mugging: Why should you Pay?](https://www.lesswrong.com/posts/h9qQQA3g8dwq6RRTo/counterfactual-mugging-why-should-you-pay)\\- Question by Chris Leong\n\nExternal links\n--------------\n\n*   [Conterfactual Blackmail (of oneself)](http://ordinaryideas.wordpress.com/2011/12/31/counterfactual-blackmail-of-oneself/) by [Paul F. Christiano](http://lesswrong.com/user/paulfchristiano)\n*   [Thoughts on Updatelessness](https://casparoesterheld.com/2016/11/21/thoughts-on-updatelessnes/) by Caspar Oesterheld\n\nSee also\n--------\n\n*   [Decision theory](https://www.lesswrong.com/tag/decision-theory)\n*   [Acausal trade](https://www.lesswrong.com/tag/acausal-trade)\n*   [Newcomb's problem](https://www.lesswrong.com/tag/newcomb-s-problem)\n*   [Parfit's hitchhiker](https://wiki.lesswrong.com/wiki/Parfit's_hitchhiker)\n*   [Smoker's lesion](https://wiki.lesswrong.com/wiki/Smoker's_lesion)\n*   [Absentminded driver](https://wiki.lesswrong.com/wiki/Absentminded_driver)\n*   [Sleeping Beauty problem](https://www.lesswrong.com/tag/sleeping-beauty-paradox)\n*   [Prisoner's dilemma](https://www.lesswrong.com/tag/prisoner-s-dilemma)\n*   [Pascal's mugging](https://www.lesswrong.com/tag/pascal-s-mugging)\n*   [Updateless decision theory](https://www.lesswrong.com/tag/updateless-decision-theory)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb1b4",
    "name": "Offense",
    "core": null,
    "slug": "offense",
    "oldSlugs": [
      "offense"
    ],
    "postCount": 5,
    "description": {
      "markdown": "It is hypothesized that the [emotion](https://www.lesswrong.com/tag/emotions) of **offense** appears when one perceives an attempt to gain [status](https://www.lesswrong.com/tag/social-status).\n\nBlog posts\n----------\n\n*   [The Nature of Offense](http://lesswrong.com/lw/13s/the_nature_of_offense/) by [Wei Dai](https://wiki.lesswrong.com/wiki/Wei_Dai)\n\nSee also\n--------\n\n*   [Status](https://www.lesswrong.com/tag/social-status)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb1b1",
    "name": "Narrative Fallacy",
    "core": null,
    "slug": "narrative-fallacy",
    "oldSlugs": null,
    "postCount": 8,
    "description": {
      "markdown": "> The narrative fallacy addresses our limited ability to look at sequences of facts without weaving an explanation into them, or, equivalently, forcing a logical link, an arrow of relationship upon them. Explanations bind facts together. They make them all the more easily remembered; they help them make more sense. Where this propensity can go wrong is when it increases our impression of understanding.\n\n—Nassim Nicholas Taleb, The Black Swan\n\nBlog posts\n----------\n\n*   [Tell your Anti-Story](http://www.overcomingbias.com/2007/07/tell-your-anti-.html) by [Robin Hanson](https://www.lesswrong.com/tag/robin-hanson)\n*   [The Bad Guy Bias](http://www.overcomingbias.com/2008/12/the-bad-guy-bia.html) by Robin Hanson\n*   [Missing the Trees for the Forest](http://lesswrong.com/lw/13k/missing_the_trees_for_the_forest/) by [Yvain](https://wiki.lesswrong.com/wiki/Yvain)\n*   [Why You're Stuck in a Narrative](http://lesswrong.com/lw/14q/why_youre_stuck_in_a_narrative/) by [hegemonicon](http://www.coarsegra.in/)\n*   [Biases of Fiction](http://www.overcomingbias.com/2012/12/biases-of-fiction.html) by Robin Hanson\n\nExternal Links\n--------------\n\n*   [Tyler Cowen on Stories](http://www.youtube.com/watch?v=RoEEDKwzNBw) ([transcript](http://lesswrong.com/r/discussion/lw/8w1/transcript_tyler_cowen_on_stories/))\n*   [Systems and Stories](http://meteuphoric.wordpress.com/2010/04/23/systems-and-stories/) by Katja Grace\n*   [Living in the Epilogue: Social Policy as Palliative Care](http://theviewfromhell.blogspot.com/2010/12/living-in-epilogue-social-policy-as.html) by Sister Y\n\nSee also\n--------\n\n*   [Privileging the hypothesis](https://www.lesswrong.com/tag/privileging-the-hypothesis), [Positive bias](https://www.lesswrong.com/tag/confirmation-bias)\n*   [Mind-killer](https://www.lesswrong.com/tag/mind-killer)\n*   [Near/far thinking](https://www.lesswrong.com/tag/near-far-thinking)\n*   [Hindsight bias](https://www.lesswrong.com/tag/hindsight-bias)\n*   [Fake simplicity](https://www.lesswrong.com/tag/fake-simplicity)\n*   [Black swan](https://www.lesswrong.com/tag/black-swans)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb1a1",
    "name": "Exploratory Engineering",
    "core": null,
    "slug": "exploratory-engineering",
    "oldSlugs": null,
    "postCount": 6,
    "description": {
      "markdown": "**Exploratory engineering** is a process for understanding a subset of the potential capabilities of future levels of technology. This process resembles the first phase of standard design engineering (termed conceptual engineering, or conceptual design), but it serves a different purpose:\n\n*   In standard engineering, design leads to the manufacturing of a product.\n*   In exploratory engineering, design leads to understanding of what a future manufacturing process could produce.\n\nExploratory engineering allows to show how some capabilities that intuitively seem [absurd](https://www.lesswrong.com/tag/absurdity-heuristic) may actually be achieved in the future.\n\nExternal links\n--------------\n\n*   [Exploratory Engineering: Applying the predictive power of science to future technologies](http://metamodern.com/2009/06/26/exploratory-engineering-applying-the-predictive-power-of-science-to-future-technologies/) by [Eric Drexler](https://www.lesswrong.com/tag/eric-drexler)\n\nSee also\n--------\n\n*   [Rational evidence](https://www.lesswrong.com/tag/rational-evidence)\n*   [Absurdity heuristic](https://www.lesswrong.com/tag/absurdity-heuristic)\n*   [Future](https://www.lesswrong.com/tag/future), [Forecast](https://www.lesswrong.com/tag/forecast)\n*   [Cryonics](https://www.lesswrong.com/tag/cryonics), [Intelligence explosion](https://www.lesswrong.com/tag/intelligence-explosion)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb1a0",
    "name": "Conjunction Fallacy",
    "core": null,
    "slug": "conjunction-fallacy",
    "oldSlugs": null,
    "postCount": 12,
    "description": {
      "markdown": "The **conjunction fallacy** consists in assuming that specific conditions are more probable than more general ones.\n\nFor the reasons related to [representativeness heuristic](https://www.lesswrong.com/tag/representativeness-heuristic), a fleshed-out story that contains typical amount of detail sounds more plausible than a stripped-down description of a situation that only states a few facts. There is a tendency for people to take that plausibility at face value, and assign [probability](https://wiki.lesswrong.com/wiki/probability) accordingly. This intuition is wrong, because the conjunction rule of probability theory states that, for any event X, its conjunction with additional details Y will be less probable.\n\nThe conjunction fallacy suggests that one should be [very careful in adding details](https://www.lesswrong.com/tag/burdensome-details) to any claim, as even though each such detail may make the claim so much more convincing, it also inevitably subtracts from its validity.\n\nBlog posts\n----------\n\n*   [Conjunction Fallacy](http://lesswrong.com/lw/ji/conjunction_fallacy/)\n*   [Conjunction Controversy (Or, How They Nail It Down)](http://lesswrong.com/lw/jj/conjunction_controversy_or_how_they_nail_it_down/)\n\nSee also\n--------\n\n*   [Representativeness heuristic](https://www.lesswrong.com/tag/representativeness-heuristic)\n*   [Burdensome details](https://www.lesswrong.com/tag/burdensome-details)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb19d",
    "name": "Absurdity Heuristic",
    "core": null,
    "slug": "absurdity-heuristic",
    "oldSlugs": null,
    "postCount": 14,
    "description": {
      "markdown": "The **absurdity heuristic** classifies highly untypical situations as \"absurd\", or [impossible](https://www.lesswrong.com/tag/antiprediction). While normally very useful as a form of [epistemic hygiene](https://www.lesswrong.com/tag/epistemic-hygiene), allowing us to detect nonsense, it suffers from the same problems as the [representativeness heuristic](https://www.lesswrong.com/tag/representativeness-heuristic).\n\nThere are a number of situations in which the absurdity heuristic is wrong. A deep theory has to [override the intuitive expectation](https://www.lesswrong.com/tag/shut-up-and-multiply). Where you don't expect intuition to construct an [adequate model](https://www.lesswrong.com/tag/technical-explanation) of reality, classifying an idea as impossible may be [overconfident](https://www.lesswrong.com/tag/overconfidence). [The future is usually \"absurd\"](http://lesswrong.com/lw/j1/stranger_than_history/), although sometimes it's possible to [rigorously infer low bounds on capabilities of the future](https://www.lesswrong.com/tag/exploratory-engineering), proving possible what is intuitively absurd.\n\nSee also\n--------\n\n*   [Representativeness heuristic](https://www.lesswrong.com/tag/representativeness-heuristic)\n*   [Shut up and multiply](https://www.lesswrong.com/tag/shut-up-and-multiply)\n*   [Antiprediction](https://www.lesswrong.com/tag/antiprediction)\n*   [Epistemic hygiene](https://www.lesswrong.com/tag/epistemic-hygiene)\n*   [Exploratory engineering](https://www.lesswrong.com/tag/exploratory-engineering)\n*   [Illusion of transparency](https://www.lesswrong.com/tag/illusion-of-transparency)\n*   [Status quo bias](https://www.lesswrong.com/tag/status-quo-bias), [Reversal test](https://www.lesswrong.com/tag/reversal-test)\n\nExternal References\n-------------------\n\n*   [Arbitrary Silliness](http://www.overcomingbias.com/2008/04/arbitrary-silli.html) by [Robin Hanson](https://www.lesswrong.com/tag/robin-hanson)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb19c",
    "name": "Availability Heuristic",
    "core": null,
    "slug": "availability-heuristic",
    "oldSlugs": null,
    "postCount": 12,
    "description": {
      "markdown": "The **availability heuristic** judges the probability of events by the ease with which examples come to mind. Sometimes this heuristic serves us well, but [the map is not the territory](https://www.lesswrong.com/tag/the-map-is-not-the-territory); the frequency with which concepts occur in your thoughts need not reflect the frequency with which they occur in reality. Undue salience, selective reporting, even subtle features of how the human brain stores and recalls memories can distort our perceptions about the probability of events. Because it is easier to recall words by their first letter, people judge words that begin with the letter *r* to be more frequent than words with *r* as their third lettter, even though *in fact*, the latter is more frequent. A second example is that selective reporting by the media of dramatic tragedies makes them seem more frequent than more threatening, albeit mundane, risks.\n\nBlog posts\n----------\n\n*   [Availability](http://lesswrong.com/lw/j5/availability/)\n\nExternal links\n--------------\n\n*   [Availability heuristic](http://psychology.wikia.com/wiki/Availability_heuristic) at Psychology Wiki\n\nSee also\n--------\n\n*   [Representativeness heuristic](https://www.lesswrong.com/tag/representativeness-heuristic)\n*   [Filtered evidence](https://www.lesswrong.com/tag/filtered-evidence)\n*   [No one knows what science doesn't know](https://wiki.lesswrong.com/wiki/No_one_knows_what_science_doesn't_know)\n*   [Absurdity heuristic](https://www.lesswrong.com/tag/absurdity-heuristic)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb192",
    "name": "Adding Up to Normality",
    "core": null,
    "slug": "adding-up-to-normality",
    "oldSlugs": [
      "egans-law"
    ],
    "postCount": 6,
    "description": {
      "markdown": "\"It all adds up to normality\" is a common phrase used on LessWrong (also known here as Egan's law\\[1\\]). **Adding Up to Normality** is the property of an explanation which adds to our understanding without changing what we already know to be true. for example:\n\n*   Apples didn't stop falling when General Relativity supplanted Newtonian mechanics.\n*   As counterintuitive as quantum mechanics is, it all adds up to what we see in everyday life - [It's perfectly normal](https://www.lesswrong.com/tag/reality-is-normal), and it always has been.\n\nThe purpose of a theory is to add up to observed reality. Science sets out to answer the question \"*What* adds up to normality?\" and the answer turns out to be \"*Quantum mechanics* adds up to normality\" or \"General Relativity adds up to normality\".\n\nA weaker extension of this principle applies to ethical and metaethical debates, which generally ought to end up explaining why you *shouldn't* eat babies, rather than why you *should*.\n\nSee also\n--------\n\n*   [Reality is normal](https://www.lesswrong.com/tag/reality-is-normal)\n*   [Quantum mechanics](https://www.lesswrong.com/tag/quantum-physics)\n*   [Metaethics sequence](https://www.lesswrong.com/tag/metaethics-sequence)\n*   [Occam's razor](https://www.lesswrong.com/tag/occam-s-razor)\n\n\\[1\\] After the science fiction writer [Greg Egan](https://en.wikipedia.org/wiki/Greg_Egan), who first wrote this phrase in [Quarantine](https://en.wikipedia.org/wiki/Quarantine_(Egan_novel))."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb187",
    "name": "Shut Up and Multiply",
    "core": null,
    "slug": "shut-up-and-multiply",
    "oldSlugs": null,
    "postCount": 31,
    "description": {
      "markdown": "Due to [scope neglect](https://wiki.lesswrong.com/wiki/scope_neglect), [framing effects](https://en.wikipedia.org/wiki/Framing_effect_(psychology)), and other [cognitive biases](https://www.lesswrong.com/tag/bias), the result of an [expected utility](https://www.lesswrong.com/tag/expected-utility) calculation executed correctly may produce an answer different from first intuition, making it \"intuitively unappealing\".  If you can tell that it's probably the intuitions that went wrong and not the calculation, the skill **shut up and multiply** is the ability to accept that, yes, sometimes the expected utility math is correct and we need to deal with that. Contrast [do the math, then go with your gut](https://www.lesswrong.com/tag/do-the-math-then-burn-the-math-and-go-with-your-gut).  If you're not sure which of these applies, use \"do the math, then go with your gut\" until you've built up more experience.\n\nThe specific application of Shut Up and Multiply to the [Torture versus Dust Specks](http://lesswrong.com/lw/kn/torture_vs_dust_specks/) case has proven quite contentious. One reason this case was cited as an exemplar of where \"shut up and multiply\" *should* apply was a claim that the usual reasoning behind answering \"SPECKS\" can be [reduced to circular preferences](https://www.lesswrong.com/posts/4ZzefKQwAtMo5yp99/circular-altruism).\n\nBlog posts\n----------\n\n*   [One Life Against the World](http://lesswrong.com/lw/hx/one_life_against_the_world/)\n*   [Circular Altruism](http://lesswrong.com/lw/n3/circular_altruism/), [The \"Intuitions\" Behind \"Utilitarianism\"](http://lesswrong.com/lw/n9/the_intuitions_behind_utilitarianism/)\n*   [Money: The Unit of Caring](http://lesswrong.com/lw/65/money_the_unit_of_caring/)\n*   [Purchase Fuzzies and Utilons Separately](http://lesswrong.com/lw/6z/purchase_fuzzies_and_utilons_separately/)\n*   [Torture vs. Dust Specks](http://lesswrong.com/lw/kn/torture_vs_dust_specks/)\n\nExternal links\n--------------\n\n*   [Is Valuing Life Undervaluing it?](http://meteuphoric.wordpress.com/2008/08/17/is-valuing-life-undervaluing-it/) by Katja Grace\n*   [You Can Put a Dollar Value on Human Life](http://squid314.livejournal.com/260949.html) by [Yvain](https://wiki.lesswrong.com/wiki/Yvain)\n*   [The Value of a Life](http://mindingourway.com/the-value-of-a-life/) by Nate Soares\n*   [Critical discussion by \"A scientists Thesis\" on Tumblr](http://scientiststhesis.tumblr.com/post/108268823040/stormingtheivory-scientiststhesis)\n*   [Eliezer Yudkowsky doesn't undestand ethics by Sean Goedeke](https://kierkeguardians.wordpress.com/2013/09/02/eliezer-yudkowsky-doesnt-understand-ethics/comment-page-1/)\n\nSee also\n--------\n\n*   [Scope insensitivity](https://www.lesswrong.com/tag/scope-insensitivity)\n*   [Fuzzies](https://www.lesswrong.com/tag/fuzzies), [utils](https://wiki.lesswrong.com/wiki/utils)\n*   [Pascal's mugging](https://www.lesswrong.com/tag/pascal-s-mugging)\n*   [Bite the bullet](https://www.lesswrong.com/tag/bite-the-bullet)\n*   [Rationality](https://www.lesswrong.com/tag/rationality)\n*   [Utilitarianism](https://www.lesswrong.com/tag/utilitarianism), [expected utility](https://www.lesswrong.com/tag/expected-utility)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb186",
    "name": "Hedonism",
    "core": null,
    "slug": "hedonism",
    "oldSlugs": null,
    "postCount": 26,
    "description": {
      "markdown": "The term **hedonism** refers to a set of philosophies which hold that the highest goal is to maximize pleasure, or more precisely pleasure minus pain. Egoistic hedonism refers to the maximization of personal pleasure, while universalist hedonism (more commonly known as hedonistic [utilitarianism](https://www.lesswrong.com/tag/utilitarianism)) aims at the maximization of pleasure across all sentient beings.\n\nBlog posts\n----------\n\n*   [The Domain of Your Utility Function](http://lesswrong.com/lw/116/the_domain_of_your_utility_function/) by [Peter de Blanc](https://wiki.lesswrong.com/wiki/Peter_de_Blanc)\n\nSee also\n--------\n\n*   [Utilitarianism](https://www.lesswrong.com/tag/utilitarianism)\n*   [Shut up and multiply](https://www.lesswrong.com/tag/shut-up-and-multiply)\n*   [Utility function](https://www.lesswrong.com/tag/utility-functions)\n*   [Wanting and liking](https://www.lesswrong.com/tag/wanting-and-liking)\n*   [Complexity of value](https://www.lesswrong.com/tag/complexity-of-value)\n*   [Wireheading](https://www.lesswrong.com/tag/wireheading)\n*   [Abolitionism](https://www.lesswrong.com/tag/abolitionism)\n*   [Hedonium](https://wiki.lesswrong.com/wiki/Hedonium)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb182",
    "name": "Halo Effect",
    "core": null,
    "slug": "halo-effect",
    "oldSlugs": null,
    "postCount": 6,
    "description": {
      "markdown": "> Let's say that you know someone who not only seems very intelligent, but also honest, altruistic, kindly, and serene. You should be suspicious that some of these perceived characteristics are influencing your perception of the others. Maybe the person is genuinely intelligent, honest, and altruistic, but not all that kindly or serene. You should be suspicious if the people you know seem to separate too cleanly into devils and angels.\n\n—[The Halo Effect](http://lesswrong.com/lw/lj/the_halo_effect/)\n\nMain post\n---------\n\n*   [The Halo Effect](http://lesswrong.com/lw/lj/the_halo_effect/)\n\nRelated concepts\n----------------\n\n*   [Priming](https://www.lesswrong.com/tag/priming), [Affect heuristic](https://www.lesswrong.com/tag/affect-heuristic)\n*   [Affective death spiral](https://www.lesswrong.com/tag/affective-death-spiral)\n*   [Contagion heuristic](https://www.lesswrong.com/tag/contagion-heuristic)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb17d",
    "name": "Status Quo Bias",
    "core": null,
    "slug": "status-quo-bias",
    "oldSlugs": null,
    "postCount": 5,
    "description": {
      "markdown": "The **status quo bias** is a [cognitive bias](https://wiki.lesswrong.com/wiki/cognitive_bias) for the status quo; in other words, people tend to avoid changing the established behavior or beliefs unless the pressure to change is sufficiently strong.\n\nThe [reversal test](https://www.lesswrong.com/tag/reversal-test) is a technique for recognizing fallacious counterarguments against change. If the counterargument states that the change of some parameter in one direction is undesirable, the reversal test is to check whether either the change of that parameter in the opposite direction (away from status quo) is desirable, or that there are strong reasons to expect that the current value of the parameter is (at least locally) the optimal one.\n\nSee also\n--------\n\n*   [Reversal test](https://www.lesswrong.com/tag/reversal-test)\n*   [Least convenient possible world](https://www.lesswrong.com/tag/least-convenient-possible-world)\n*   [Cached thought](https://www.lesswrong.com/tag/cached-thought)\n*   [Sunk cost fallacy](https://www.lesswrong.com/tag/sunk-cost-fallacy)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb170",
    "name": "Group Selection",
    "core": null,
    "slug": "group-selection",
    "oldSlugs": null,
    "postCount": 6,
    "description": {
      "markdown": "**Group Selection** posits that natural selection might not operate at the level of genes in individuals, and instead also operate at genes in groups of individuals, i.e. selecting for genes for the group even at the expense of the individual. For example, you might posit that human [religion is an adaptation to make human groups more cohesive](http://lesswrong.com/lw/mk/a_failed_justso_story/), since religious groups outfight nonreligious groups.\n\n*See also:* [Evolution](https://www.lesswrong.com/tag/evolution), [Alienness of evolution](https://wiki.lesswrong.com/wiki/Alienness_of_evolution)\n\nConsider two groups on different sides of a mountain: in group A, each mother gives birth to 2 males and 2 females; in group B, each mother gives birth to 3 females and 1 male. Group A and group B will have the same number of children, but group B will have 50% more grandchildren and 125% more great-grandchildren.\n\nBut consider: The *rarer* males become, the more *reproductively valuable* they become - not to the group, but to the individual parent. If all the females are doing what's good for the group and birthing 1 male per 10 females, then you can make a genetic *killing* by birthing all males, each of whom will have (on average) ten times as many grandchildren as their female cousins.\n\nSo while group selection ought to favor more girls, individual selection favors equal investment in male and female offspring. Just by looking at the statistics of a maternity ward, you can see that the quantitative balance between group selection forces and individual selection forces is overwhelmingly tilted in favor of individual selection in *Homo sapiens*.\n\nGroup selection is extremely hard to make work mathematically. In [this simulation](http://www.gnxp.com/MT2/archives/003540.html), for example, the cost to altruists is 3% of fitness, pure altruist groups have a fitness twice as great as pure selfish groups, the subpopulation size is 25, and 20% of all deaths are replaced with messengers from another group. The result is polymorphic for selfishness and altruism. If the subpopulation size is doubled to 50, selfishness is fixed. If the cost to altruists is increased to 6%, selfishness is fixed. If the altruistic benefit is decreased by half, selfishness is fixed or in large majority. Neighborhood-groups must be very small, with only around 5 members, for group selection to operate when the cost of altruism exceeds 10%.\n\nTo the best of this editor's knowledge, no definite example of a group-level adaptation has ever been observed in a mammalian species. Ever.\n\nHence, postulating group selection in any species - let alone in humans - is guaranteed to make professional evolutionary biologists roll their eyes.\n\nIt seems to be extremely popular among a certain sort of amateur evolutionary theorist, though - there's a certain sort of person who, if they don't know about the incredible mathematical difficulty, will find it very satisfying to speculate about adaptations for the good of the group.\n\nThe historical fiasco of group selectionism is relied on as a (clear-cut) case in point of the dangers of [anthropomorphism](https://www.lesswrong.com/tag/anthropomorphism).\n\nMain posts\n----------\n\n*   [Evolving to Extinction](http://lesswrong.com/lw/l5/evolving_to_extinction/)\n*   [The Tragedy of Group Selectionism](http://lesswrong.com/lw/kw/the_tragedy_of_group_selectionism/)\n\nOther posts\n-----------\n\n*   [Conjuring An Evolution To Serve You](http://lesswrong.com/lw/l8/conjuring_an_evolution_to_serve_you/)\n*   [Anthropomorphic Optimism](http://lesswrong.com/lw/st/anthropomorphic_optimism/)\n*   [Contaminated by Optimism](http://lesswrong.com/lw/su/contaminated_by_optimism/)\n*   [A Failed Just-So Story](http://lesswrong.com/lw/mk/a_failed_justso_story/)\n*   [Group selection update](http://lesswrong.com/lw/300/group_selection_update/)\n\nExternal links\n--------------\n\n*   [The False Allure of Group Selection](http://edge.org/conversation/the-false-allure-of-group-selection) by [Steven Pinker](https://en.wikipedia.org/wiki/Steven_Pinker)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb16a",
    "name": "Adaptation Executors",
    "core": null,
    "slug": "adaptation-executors",
    "oldSlugs": null,
    "postCount": 18,
    "description": {
      "markdown": "Modeling biological systems as **Adaption Executors** is central principle of [evolutionary biology](https://www.lesswrong.com/tag/evolution) in general, and [evolutionary psychology](https://www.lesswrong.com/tag/evolutionary-psychology) in particular.\n\nIf we regarded human taste buds as trying to *maximize fitness*, we might expect that, say, humans fed a diet too high in calories and too low in micronutrients, would begin to find lettuce delicious, and cheeseburgers distasteful. But it is better to regard taste buds as an *executing adaptation* \\- they are adapted to an ancestral environment in which calories, not micronutrients, were the limiting factor. And now they are simply executing that adaptation - evolution operates on [too slow a timescale](https://www.lesswrong.com/tag/slowness-of-evolution) to re-adapt to such a recent condition.\n\nEvolution is ultimately just a historical-statistical macrofact about which ancestors *did in fact* reproduce. These genes then execute again, as they did previously. And so the behavior of the organism is often better interpreted in terms of what worked in the past, rather than what should work in the future. The organism's genes are, in fact, the causal result of what worked in the past, and certainly not [a causal result of the future](https://www.lesswrong.com/tag/teleology).\n\nExternal links\n--------------\n\n*   [The Two Main Ways In Which Evolution Is Not Our Friend](http://theviewfromhell.blogspot.com/2011/05/two-main-ways-in-which-evolution-is-not.html) by Sister Y\n\nSee also\n--------\n\n*   [Evolution](https://www.lesswrong.com/tag/evolution), [stupidity of evolution](https://www.lesswrong.com/tag/stupidity-of-evolution)\n*   [Evolutionary psychology](https://www.lesswrong.com/tag/evolutionary-psychology)\n*   [Superstimulus](https://www.lesswrong.com/tag/superstimuli)\n*   [Goodhart's law](https://www.lesswrong.com/tag/goodhart-s-law)\n*   [Signaling](https://www.lesswrong.com/tag/signaling), [status](https://www.lesswrong.com/tag/social-status)\n*   [Corrupted hardware](https://www.lesswrong.com/tag/corrupted-hardware)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb163",
    "name": "Rationality Verification",
    "core": null,
    "slug": "rationality-verification",
    "oldSlugs": [
      "problem-of-verifying-rationality"
    ],
    "postCount": 14,
    "description": {
      "markdown": "**Verifying rationality is possibly the single largest problem** for those desiring to create methods of systematically training for increased epistemic and instrumental [rationality](https://www.lesswrong.com/tag/rationality) \\- how to verify that the training actually *worked*. Very Awful Things happen to people who set out to build training methods and schools without *strong* means of verification - two hideous historical examples being modern-day practice of martial arts (once the teachers are no longer fighting duels to the death) and the proliferating \"schools\" of psychotherapy (in the entire absence of any experimental evidence that one school worked better than another, and indeed, in the presence of experimental evidence that the schools did *no* have any discernible difference in effectiveness).\n\nExternal links\n--------------\n\n*   [Rational Poker](http://rationalpoker.com/) by Louie\n*   (book) [The Rationality Quotient: Toward a Test of Rational Thinking](https://mitpress.mit.edu/books/rationality-quotient) ([review](https://www.sciencedirect.com/science/article/abs/pii/S0160289616303555) by Stuart Ritchie, [review](http://www.bayesianinvestor.com/blog/index.php/2017/01/07/rationality-quotient/) by [pcm](https://www.lesswrong.com/users/pcm))\n*   [The Comprehensive Assessment of Rational Thinking](http://www.keithstanovich.com/Site/Research_on_Reasoning_files/Stanovich_EdPsy_2016.pdf) (CART) by Keith E. Stanovich ([sci-hub](https://sci-hub.se/10.1080/00461520.2015.1125787))\n*   [Thinking style quiz](http://programs.clearerthinking.org/how_rational_are_you_really_take_the_test.html) by [clearerthinking.org](https://www.clearerthinking.org/) ([LW Discussion](https://www.lesswrong.com/posts/R2mPGwFvXSy4nCMgj/take-the-rationality-test-to-determine-your-rational#ChMTSFFZGPakafojF))\n*   [Cognitive Reflection Test](https://en.wikipedia.org/wiki/Cognitive_reflection_test) ([LW Discussion](https://www.lesswrong.com/posts/vk2yS8osapSch9Cz2/the-bat-and-ball-problem-revisited))\n\nSee also\n--------\n\n*   [Exercises / Problem-Sets](https://www.lesswrong.com/tag/exercises-problem-sets)\n*   [The Craft and the Community](https://www.lesswrong.com/tag/the-craft-and-the-community)\n*   [Rationality](https://www.lesswrong.com/tag/rationality)\n*   [Rationality as martial art](https://www.lesswrong.com/tag/rationality-as-martial-art)\n*   [Decision theory](https://www.lesswrong.com/tag/decision-theory)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb162",
    "name": "Human Universal",
    "core": null,
    "slug": "human-universal",
    "oldSlugs": null,
    "postCount": 3,
    "description": {
      "markdown": "We should expect many traits to **human universal** since c*omplex* adaptations within a sexually reproducing species need all of their parts, or almost all of their parts, to be universal within the gene pool.\n\nLet's say that you have a complex adaptation with six interdependent parts, and that each of the six genes is independently at ten percent frequency in the population. The chance of assembling a whole working adaptation is literally a million to one; and the average fitness of the genes is tiny, and they will not increase in frequency.\n\nOne bird may have slightly smoother feathers than another, but they will both have wings. A single mutation can be possessed by some lucky members of a species, and not by others - but single mutations don't correspond to the sort of complex, powerful machinery that underlies the potency of biology. By the time an adaptation gets to be really sophisticated with dozens of genes supporting its highly refined activity, every member of the species has some version of it - barring single mutations that knock out the whole complex.\n\nApplying this logic to human brains in particular, we arrive at the explanation for what is called *the psychic unity of mankind*. (Though [Eliezer Yudkowsky](https://www.lesswrong.com/tag/eliezer-yudkowsky) has used the phrase \"psychological unity of humankind\" instead.) In every known culture, humans seem to experience joy, sadness, fear, disgust, anger, and surprise. In every known culture, these emotions are indicated by the same facial expressions. (Citation needed to Paul Ekman.)\n\nDonald E. Brown has compiled a list of [over a hundred human universals](http://condor.depaul.edu/~mfiddler/hyphen/humunivers.htm) \\- traits found in every culture ever studied, most of them so universal that anthropologists don't even bother to note them explicitly.\n\nBlog posts\n----------\n\n*   [The Psychological Unity of Humankind](http://lesswrong.com/lw/rl/the_psychological_unity_of_humankind/) by [Eliezer Yudkowsky](https://www.lesswrong.com/tag/eliezer-yudkowsky)\n*   [Humans in Funny Suits](http://lesswrong.com/lw/so/humans_in_funny_suits/) by [Eliezer Yudkowsky](https://www.lesswrong.com/tag/eliezer-yudkowsky)\n\nExternal links\n--------------\n\n*   [Human universals](http://condor.depaul.edu/~mfiddler/hyphen/humunivers.htm) by [Donald E. Brown](https://wiki.lesswrong.com/wiki/Donald_E._Brown)\n\nSee also\n--------\n\n*   [Evolutionary psychology](https://www.lesswrong.com/tag/evolutionary-psychology)\n*   [Typical mind fallacy](https://www.lesswrong.com/tag/typical-mind-fallacy)\n*   [Anthropomorphism](https://www.lesswrong.com/tag/anthropomorphism)\n*   [Complexity of value](https://www.lesswrong.com/tag/complexity-of-value), [fake simplicity](https://www.lesswrong.com/tag/fake-simplicity)\n*   [Alien values](https://www.lesswrong.com/tag/alien-values)\n*   [Mind design space](https://www.lesswrong.com/tag/mind-design-space), [paperclip maximizer](https://www.lesswrong.com/tag/paperclip-maximizer)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb157",
    "name": "Correspondence Bias",
    "core": null,
    "slug": "correspondence-bias",
    "oldSlugs": null,
    "postCount": 4,
    "description": {
      "markdown": "**Correspondence bias** (also known as the **fundamental attribution error**) is the tendency to overestimate the the contribution of lasting traits and dispositions in determining people's behavior, as compared to situational effects. We might see someone kicking a vending machine, and conclude they're an inherently angry person. But maybe they just failed a test, had their driving license revoked, and had the machine eat their money for the third time this week. We think of the other person as an [evil mutant](https://www.lesswrong.com/tag/human-universal) and ourselves as righteous actors.\n\nSee also\n--------\n\n*   [Availability heuristic](https://www.lesswrong.com/tag/availability-heuristic)\n*   [Human universal](https://www.lesswrong.com/tag/human-universal), [Typical mind fallacy](https://www.lesswrong.com/tag/typical-mind-fallacy)\n\nReferences\n----------\n\nDT Gilbert, PS Malone (1995) The Correspondence Bias ([PDF](https://wjh-www.harvard.edu/~dtg/Gilbert%20&%20Malone%20%28CORRESPONDENCE%20BIAS%29.pdf))"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb154",
    "name": "Scope Insensitivity",
    "core": null,
    "slug": "scope-insensitivity",
    "oldSlugs": null,
    "postCount": 6,
    "description": {
      "markdown": "The human brain can't *emotionally* represent large quantities; an environmental measure that will save 200,000 birds doesn't conjure anywhere near a hundred times the emotional impact and willingness-to-pay of a measure that would save 2,000 birds, even though *in fact* the former measure *is* two orders of magnitude more effective.\n\nBlog posts\n----------\n\n*   [Scope Insensitivity](http://lesswrong.com/lw/hw/scope_insensitivity/)\n*   [One Life Against the World](http://lesswrong.com/lw/hx/one_life_against_the_world/)\n*   [Useless Medical Disclaimers](http://lesswrong.com/lw/h4/useless_medical_disclaimers/)\n*   [Torture vs. Dust Specks](http://lesswrong.com/lw/kn/torture_vs_dust_specks/)\n\nSee also\n--------\n\n*   [Shut up and multiply](https://www.lesswrong.com/tag/shut-up-and-multiply)\n*   [Expected utility](https://www.lesswrong.com/tag/expected-utility), [Utilitarianism](https://www.lesswrong.com/tag/utilitarianism)\n*   [Evolutionary psychology](https://www.lesswrong.com/tag/evolutionary-psychology)\n*   [Emotion](https://www.lesswrong.com/tag/emotions), [alief](https://www.lesswrong.com/tag/alief)\n*   [Pascal's mugging](https://www.lesswrong.com/tag/pascal-s-mugging)\n*   [Existential risk](https://www.lesswrong.com/tag/existential-risk)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb153",
    "name": "Defensibility",
    "core": null,
    "slug": "defensibility",
    "oldSlugs": null,
    "postCount": 5,
    "description": {
      "markdown": "> Beware when you find yourself arguing that a policy is defensible rather than optimal; or that it has some benefit compared to the null action, rather than the best benefit of any action.\n\n—[The Third Alternative](http://lesswrong.com/lw/hu/the_third_alternative/)\n\nBlog posts\n----------\n\n*   [The Third Alternative](http://lesswrong.com/lw/hu/the_third_alternative/) by [Eliezer Yudkowsky](https://www.lesswrong.com/tag/eliezer-yudkowsky)\n\nSee also\n--------\n\n*   [Third option](https://www.lesswrong.com/tag/third-option)\n*   [Arguments as soldiers](https://www.lesswrong.com/tag/arguments-as-soldiers)\n*   [Not technically a lie](https://www.lesswrong.com/tag/not-technically-a-lie)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb150",
    "name": "Bystander Effect",
    "core": null,
    "slug": "bystander-effect",
    "oldSlugs": null,
    "postCount": 13,
    "description": {
      "markdown": "> The **bystander effect** is a social psychological phenomenon in which individuals are less likely to offer help in an emergency situation when other people are present. The probability of help is inversely proportional to the number of bystanders. In other words, the greater the number of bystanders, the less likely it is that any one of them will help.\n\n—Safety Canada, January 2004, [\"Don't Just Stand There - Do Something\"](http://www.safety-council.org/info/community/bystander.html)\n\nSee also\n--------\n\n*   [Conformity bias](https://www.lesswrong.com/tag/conformity-bias)\n*   [Shut up and multiply](https://www.lesswrong.com/tag/shut-up-and-multiply)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb137",
    "name": "Hope",
    "core": null,
    "slug": "hope",
    "oldSlugs": null,
    "postCount": 8,
    "description": {
      "markdown": "Persisting in clutching to a hope may be disastrous. Be ready to admit you lost, [update](https://wiki.lesswrong.com/wiki/update) on the data that says you did.\n\nBlog posts\n----------\n\n*   [Just Lose Hope Already](http://lesswrong.com/lw/gx/just_lose_hope_already/)\n*   [Lotteries: A Waste of Hope](http://lesswrong.com/lw/hl/lotteries_a_waste_of_hope/)\n*   [New Improved Lottery](http://lesswrong.com/lw/hm/new_improved_lottery/)\n\nSee also\n--------\n\n*   [Fuzzies](https://www.lesswrong.com/tag/fuzzies)\n*   [Shut up and multiply](https://www.lesswrong.com/tag/shut-up-and-multiply)\n*   [Lotteries](https://wiki.lesswrong.com/wiki/Lotteries)\n*   [Oops](https://www.lesswrong.com/tag/oops)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb12a",
    "name": "Rationalist Taboo",
    "core": null,
    "slug": "rationalist-taboo",
    "oldSlugs": null,
    "postCount": 20,
    "description": {
      "markdown": "**Rationalist Taboo** is a technique for fighting muddles in discussions. By prohibiting the use of a certain word and all the words synonymous to it, people are forced to elucidate the specific contextual meaning they want to express, thus removing ambiguity otherwise present in a single word.\n\nMainstream philosophy has a parallel procedure called \"unpacking\". Unpacking has the requirement that the contentious or doubtful term be expanded out--not just replaced with a synonym--and only used as a collection of subcomponent concepts.\n\nMain Posts\n----------\n\n*   [Taboo Your Words](http://lesswrong.com/lw/nu/taboo_your_words/)\n*   [Replace the Symbol with the Substance](http://lesswrong.com/lw/nv/replace_the_symbol_with_the_substance/)\n\nOther Posts\n-----------\n\n*   [Disputing Definitions](http://lesswrong.com/lw/np/disputing_definitions/) \\- An example of how the technique helps.\n*   [Words as Hidden Inferences](http://lesswrong.com/lw/ng/words_as_hidden_inferences/) \\- The mere presence of words can influence thinking, sometimes misleading it.\n*   [Detached Lever Fallacy](http://lesswrong.com/lw/sp/detached_lever_fallacy/) \\- There is a lot of machinery hidden beneath the words, and rationalist's taboo is one way to make a step towards exposing it."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb128",
    "name": "Priming",
    "core": null,
    "slug": "priming",
    "oldSlugs": null,
    "postCount": 15,
    "description": {
      "markdown": "**Priming** is a [psychological](https://www.lesswrong.com/tag/psychology) phenomenon that consists in early stimulus influencing later thoughts and behavior. The literature on Priming was heavily hit in the [replication crisis](https://www.lesswrong.com/tag/replication-crisis) of the 2010's.\n\nSequence by Scott Alexander\n---------------------------\n\n*   [Never Leave Your Room](http://lesswrong.com/lw/3b/never_leave_your_room/)\n*   [Bogus Pipeline, Bona Fide Pipeline](http://lesswrong.com/lw/4w/bogus_pipeline_bona_fide_pipeline/)\n*   [The Implicit Association Test](http://lesswrong.com/lw/53/the_implicit_association_test/)\n*   [Fight Biases, or Route Around Them?](http://lesswrong.com/lw/5d/fight_biases_or_route_around_them/)\n\nSee also\n--------\n\n*   [Seeing with Fresh Eyes](https://www.lesswrong.com/tag/seeing-with-fresh-eyes) (sequence)\n*   [Affect heuristic](https://www.lesswrong.com/tag/affect-heuristic)\n*   [Ugh field](https://www.lesswrong.com/tag/aversion-ugh-fields)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb127",
    "name": "Hindsight Bias",
    "core": null,
    "slug": "hindsight-bias",
    "oldSlugs": null,
    "postCount": 10,
    "description": {
      "markdown": "**Hindsight Bias** is a tendency to overestimate the *foreseeability* of events that have actually happened. I.e., subjects given information about X, and asked to assign a probability that X will happen, assign much lower probabilities than subjects who are given the same information about X, are told that X actually happened, and asked to estimate the foreseeable probability of X. Experiments also show that instructing subjects to \"avoid hindsight bias\" has little or no effect.\n\nExternal Articles\n-----------------\n\n*   [Read History Of Philosophy Backwards](https://slatestarcodex.com/2013/04/11/read-history-of-philosophy-backwards/)\n\nRelated Pages\n-------------\n\n*   [Forecast](https://www.lesswrong.com/tag/forecast)\n*   [Positive bias](https://www.lesswrong.com/tag/confirmation-bias)\n*   [Debiasing](https://www.lesswrong.com/tag/debiasing)\n*   [Black swan](https://www.lesswrong.com/tag/black-swans)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb125",
    "name": "Generalization From Fictional Evidence",
    "core": null,
    "slug": "generalization-from-fictional-evidence",
    "oldSlugs": null,
    "postCount": 13,
    "description": {
      "markdown": "The logical fallacy of **generalization from fictional evidence** consists in drawing real-world conclusions based on statements invented and selected for the purpose of writing fiction.\n\nIt was first coined by Eliezer Yudkowsky in [a talk he gave in 2003](http://www.longecity.org/forum/topic/1097-predicting-the-future-eliezer-yudkowsky/), and later in his essay [The Logical Fallacy of Generalization from Fictional Evidence](https://www.lesswrong.com/posts/rHBdcHGLJ7KvLJQPk/the-logical-fallacy-of-generalization-from-fictional).\n\nSee also\n--------\n\n*   [Availability heuristic](https://www.lesswrong.com/tag/availability-heuristic)\n*   [Good-story bias](https://www.lesswrong.com/tag/good-story-bias)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb124",
    "name": "Conformity Bias",
    "core": null,
    "slug": "conformity-bias",
    "oldSlugs": null,
    "postCount": 15,
    "description": {
      "markdown": "**Conformity bias** is a tendency to behave similarly to the others in a group, even if doing so goes against your own judgment.\n\nBlog posts\n----------\n\n*   [Asch's Conformity Experiment](http://lesswrong.com/lw/m9/aschs_conformity_experiment/)\n*   [Lonely Dissent](http://lesswrong.com/lw/mb/lonely_dissent/)\n*   [On Expressing Your Concerns](http://lesswrong.com/lw/ma/on_expressing_your_concerns/)\n*   [Undiscriminating Skepticism](http://lesswrong.com/lw/1ww/undiscriminating_skepticism/)\n\nSee also\n--------\n\n*   [Affective death spiral](https://www.lesswrong.com/tag/affective-death-spiral)\n*   [Groupthink](https://www.lesswrong.com/tag/groupthink)\n*   [In-group bias](https://www.lesswrong.com/tag/in-group-bias)\n*   [Improper belief](https://www.lesswrong.com/tag/improper-belief)\n*   [Akrasia](https://www.lesswrong.com/tag/akrasia)\n*   [Death Spirals and the Cult Attractor](https://www.lesswrong.com/tag/death-spirals-and-the-cult-attractor)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb118",
    "name": "Underconfidence",
    "core": null,
    "slug": "underconfidence",
    "oldSlugs": null,
    "postCount": 12,
    "description": {
      "markdown": "**Underconfidence** is the state of being more uncertain than is justified, given your [priors](https://wiki.lesswrong.com/wiki/prior) and the evidence you are aware of.\n\nBlog posts\n----------\n\n*   [\"I don't know.\"](http://lesswrong.com/lw/gs/i_dont_know/)\n*   [The Sin of Underconfidence](http://lesswrong.com/lw/c3/the_sin_of_underconfidence/)\n*   [The Importance of Self-Doubt](http://lesswrong.com/lw/2lr/the_importance_of_selfdoubt/) by multifoliaterose\n\nSee also\n--------\n\n*   [Overconfidence](https://www.lesswrong.com/tag/overconfidence)\n*   [Modesty](https://www.lesswrong.com/tag/modesty)\n*   [Fallacy of gray](https://www.lesswrong.com/tag/fallacy-of-gray)\n*   [I don't know](https://wiki.lesswrong.com/wiki/I_don't_know)\n*   [Motivated skepticism](https://www.lesswrong.com/tag/motivated-skepticism)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb117",
    "name": "Tsuyoku Naritai",
    "core": null,
    "slug": "tsuyoku-naritai",
    "oldSlugs": null,
    "postCount": 13,
    "description": {
      "markdown": "Blog posts\n----------\n\n*   [Tsuyoku Naritai! (I Want To Become Stronger)](http://lesswrong.com/lw/h8/tsuyoku_naritai_i_want_to_become_stronger/)\n*   [Tsuyoku vs. the Egalitarian Instinct](http://lesswrong.com/lw/h9/tsuyoku_vs_the_egalitarian_instinct/)\n*   [A Sense That More Is Possible](http://lesswrong.com/lw/2c/a_sense_that_more_is_possible/)\n\nSee also\n--------\n\n*   [Egalitarianism](https://www.lesswrong.com/tag/egalitarianism)\n*   [Rationality as martial art](https://www.lesswrong.com/tag/rationality-as-martial-art)\n*   [Challenging the Difficult](https://www.lesswrong.com/tag/challenging-the-difficult)\n*   [Rationality is systematized winning](https://www.lesswrong.com/tag/rationality-is-systematized-winning)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb108",
    "name": "Priors",
    "core": null,
    "slug": "priors",
    "oldSlugs": null,
    "postCount": 20,
    "description": {
      "markdown": "In the context of [Bayes's Theorem](https://wiki.lesswrong.com/wiki/Bayes's_Theorem), **priors** refer generically to the beliefs an agent holds regarding a fact, hypothesis or consequence, before being presented with evidence. Upon being presented with new evidence, the agent can multiply their prior with a [likelihood distribution](https://wiki.lesswrong.com/wiki/likelihood_distribution) to calculate a new (posterior) probability for their belief.\n\nExamples\n--------\n\nSuppose you had a barrel containing some number of red and white balls. You start with the belief that each ball was independently assigned red color (vs. white color) at some fixed probability. Furthermore, you start out ignorant of this fixed probability (the parameter could be anywhere between 0 and 1). Each red ball you see then makes it *more* likely that the next ball will be red, following a [Laplacian Rule of Succession](http://en.wikipedia.org/wiki/Rule_of_succession). For example, seeing 6 red balls out of 10 suggests that the initial probability used for assigning the balls a red color was .6, and that there's also a probability of .6 for the next ball being red.\n\nOn the other hand, if you start out with the prior belief that the barrel contains exactly 10 red balls and 10 white balls, then each red ball you see makes it *less* likely that the next ball will be red (because there are fewer red balls remaining).\n\nThus our prior affects how we interpret the evidence. The first prior is an inductive prior - things that happened before are predicted to happen again with greater probability. The second prior is anti-inductive - the more red balls we see, the fewer we expect to see in the future.\n\nAs a real life example, consider two leaders from different political parties. Each one has his own beliefs - priors - about social organization and the roles of people and government in society. These differences in priors can be attributed to a wide range of factors, ranging from their educational backgrounds to hereditary differences in personality. However, neither can show that his beliefs are better than those of the other, unless he can show that his priors were generated by sources which track reality better[^1^](#fn1).\n\nBecause carrying out any reasoning at all seems to require a prior of some kind, ideal Bayesians would need some sort of priors from the moment that they were born. The question of where an ideal Bayesian would get this prior from has occasionally been a matter of considerable controversy in the philosophy of probability.\n\nUpdating prior probabilities\n----------------------------\n\nIn informal discussion, people often talk about \"updating\" their priors. This is technically incorrect, as one does not change their prior probability, but rather uses it to calculate a posterior probability. However, as this posterior probability then becomes the prior probability for the next inference, talking about \"updating one's priors\" is often a convenient shorthand.\n\nReferences\n----------\n\nBlog posts\n----------\n\n*   [Priors as Mathematical Objects](http://lesswrong.com/lw/hk/priors_as_mathematical_objects/)\n*   [\"Inductive Bias\"](http://lesswrong.com/lw/hg/inductive_bias/)\n*   [Probability is Subjectively Objective](http://lesswrong.com/lw/s6/probability_is_subjectively_objective/)\n*   [Bead Jar Guesses](http://lesswrong.com/lw/em/bead_jar_guesses/) by [Alicorn](https://wiki.lesswrong.com/wiki/Alicorn) \\- Applied scenario about forming priors.\n\nSee also\n--------\n\n*   [Evidence](https://www.lesswrong.com/tag/evidence)\n*   [Inductive bias](https://www.lesswrong.com/tag/inductive-bias)\n*   [Belief update](https://www.lesswrong.com/tag/belief-update)\n\nReferences\n----------\n\n1.  Robin Hanson (2006). \"Uncommon Priors Require Origin Disputes\". Theory and Decision 61 (4) 319–328. [http://hanson.gmu.edu/prior.pdf](http://hanson.gmu.edu/prior.pdf)[↩](#fnref1)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb105",
    "name": "Planning Fallacy",
    "core": null,
    "slug": "planning-fallacy",
    "oldSlugs": null,
    "postCount": 10,
    "description": {
      "markdown": "The **Planing Fallacy** is a common cognitive bias resulting in predicting absurdly short timeframes for planned projects, famously observed with, among other projects, the [Sydney Opera House](https://en.wikipedia.org/wiki/Sydney_Opera_House), completed ten years late and a hundred million dollars overbudget.\n\nSymptomatic of the Planning Fallacy is an assumption of a best-case scenario; people plan as if everything will go smoothly, as hoped for, with no unexpected delays. In practice, this is typically not the case, and delays quickly mount.\n\nThe bias also seems to be related to taking an \"inside\", detail-oriented view of the project to be planned; studies show that the more detailed a plan is, the more optimistically inaccurate it is likely to be.\n\nDebiasing techniques\n--------------------\n\nWhen possible, take the [outside view](https://www.lesswrong.com/tag/inside-outside-view). Avoid estimating the time for a project by adding time estimates for sub-tasks; instead, look for previously completed projects of similar type and scale, and base the estimate on how long those other projects took.\n\nBlog posts\n----------\n\n*   [Planning Fallacy](http://lesswrong.com/lw/jg/planning_fallacy/)\n*   [Surface Analogies and Deep Causes](http://lesswrong.com/lw/rj/surface_analogies_and_deep_causes/)\n\nSee also\n--------\n\n*   [Outside view](https://www.lesswrong.com/tag/inside-outside-view)\n*   [Near/far thinking](https://www.lesswrong.com/tag/near-far-thinking)\n*   [Forecast](https://www.lesswrong.com/tag/forecast)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb0e9",
    "name": "Fuzzies",
    "core": null,
    "slug": "fuzzies",
    "oldSlugs": null,
    "postCount": 7,
    "description": {
      "markdown": "A **fuzzy** is a hypothetical measurement unit for \"warm fuzzy feeling\" one gets from believing that one has done good. Unlike [utils](https://wiki.lesswrong.com/wiki/utils), fuzzies can be earned through psychological tricks without regard for efficiency. For this reason, it may be a good idea to separate the concerns for actually doing good, for which one might need to [shut up and multiply](https://www.lesswrong.com/tag/shut-up-and-multiply), and for earning fuzzies, to get psychological comfort.\n\nBlog posts\n----------\n\n*   [Purchase Fuzzies and Utilons Separately](http://lesswrong.com/lw/6z/purchase_fuzzies_and_utilons_separately/)\n*   [The Trouble With \"Good\"](http://lesswrong.com/lw/bk/the_trouble_with_good/) by [Yvain](https://wiki.lesswrong.com/wiki/Yvain)\n*   [Not for the Sake of Happiness (Alone)](http://lesswrong.com/lw/lb/not_for_the_sake_of_happiness_alone/)\n\nSee also\n--------\n\n*   [Utility function](https://www.lesswrong.com/tag/utility-functions)\n*   [Shut up and multiply](https://www.lesswrong.com/tag/shut-up-and-multiply)\n*   [Hedon](https://www.lesswrong.com/tag/hedon)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb0e2",
    "name": "Epistemic Hygiene",
    "core": null,
    "slug": "epistemic-hygiene",
    "oldSlugs": null,
    "postCount": 2,
    "description": {
      "markdown": "**Epistemic hygiene** consists of practices meant to allow accurate beliefs to spread within a community and keep less accurate or biased beliefs contained. The practices are meant to serve an analogous purpose to normal hygiene and sanitation in containing disease. \"Good cognitive citizenship\" is another phrase that has been proposed for this concept[^1^](#fn1).\n\nBlog posts\n----------\n\n*   [Raising the Sanity Waterline](http://lesswrong.com/lw/1e/raising_the_sanity_waterline/)\n*   [The ethic of hand-washing and community epistemic practice](http://lesswrong.com/lw/u/the_ethic_of_handwashing_and_community_epistemic/)\n*   [Hygienic Anecdotes](http://lesswrong.com/lw/6a/hygienic_anecdotes/)\n*   [Reason as memetic immune disorder](http://lesswrong.com/lw/18b/reason_as_memetic_immune_disorder/)\n\nExternal links\n--------------\n\n*   [Free Will: Good Cognitive Citizenship](http://bloggingheads.tv/diavlogs/17359) with Will Wilkinson and Eliezer Yudkowsky\n\nSee also\n--------\n\n*   [Least convenient possible world](https://www.lesswrong.com/tag/least-convenient-possible-world)\n*   [Improper belief](https://www.lesswrong.com/tag/improper-belief)\n*   [Group rationality](https://www.lesswrong.com/tag/group-rationality)\n*   [Mind-killer](https://www.lesswrong.com/tag/mind-killer)\n*   [Filtered evidence](https://www.lesswrong.com/tag/filtered-evidence), [absurdity heuristic](https://www.lesswrong.com/tag/absurdity-heuristic)\n*   [Rational evidence](https://www.lesswrong.com/tag/rational-evidence)\n*   [Burdensome details](https://www.lesswrong.com/tag/burdensome-details)\n*   [Status quo bias](https://www.lesswrong.com/tag/status-quo-bias)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb0d6",
    "name": "Cached Thoughts",
    "core": null,
    "slug": "cached-thoughts",
    "oldSlugs": [
      "cached-thought"
    ],
    "postCount": 19,
    "description": {
      "markdown": "**Cached Thoughts** are ideas, attitudes, and beliefs that a person has formed on some past occasion, and hasn't re-evaluated since then. The name references the concept of a [cache](https://en.wikipedia.org/wiki/Cache_(computing)) in computing: a component storing data that has been calculated or retrieved once, so that it is quickly available without needing to be recalculated or re-retrieved.\n\n*See also*: [Groupthink](https://www.lesswrong.com/tag/groupthink), [Information Cascades](https://www.lesswrong.com/tag/information-cascades), [Status quo bias](https://www.lesswrong.com/tag/status-quo-bias), [Semantic Stopsign](https://www.lesswrong.com/tag/semantic-stopsign), [Separate Magisteria](https://wiki.lesswrong.com/wiki/Separate_magisteria), [Rationalist Taboo](https://www.lesswrong.com/tag/rationalist-taboo)\n\nCached thoughts can be useful in saving computational resources at the cost of some memory load, and also at the risk of maintaining a belief long past the point when evidence should force an update. In particular, cached thoughts can result in a lack of creative approaches to problem-solving, as cached solutions may interfere with the formation of novel ones. What is generally called [common sense](https://www.lesswrong.com/tag/common-sense) is more or less a collection of cached thoughts.\n\n> In modern civilization particularly, no one can think fast enough to think their own thoughts. If I’d been abandoned in the woods as an infant, raised by wolves or silent robots, I would scarcely be recognizable as human. No one can think fast enough to recapitulate the wisdom of a hunter-gatherer tribe in one lifetime, starting from scratch. As for the wisdom of a literate civilization, forget it.\n> \n> But the flip side of this is that I continually see people who aspire to critical thinking, repeating back cached thoughts which were not invented by critical thinkers. – Eliezer Yudkowsky, [Cached Thoughts](https://www.lesswrong.com/posts/2MD3NMLBPCqPfnfre/cached-thoughts)\n\nMain Post\n---------\n\n*   [Cached Thoughts](http://lesswrong.com/lw/k5/cached_thoughts/)\n\nNotable Posts\n-------------\n\n*   [How to Seem (and Be) Deep](http://lesswrong.com/lw/k8/how_to_seem_and_be_deep/) — Just find ways of violating cached expectations.\n*   [The Virtue of Narrowness](http://lesswrong.com/lw/ic/the_virtue_of_narrowness/) and [Original Seeing](http://lesswrong.com/lw/k7/original_seeing/) — One way to fight cached patterns of thought is to focus on precise concepts.\n*   [Cached Procrastination](http://lesswrong.com/lw/d2/cached_procrastination/)\n*   [Cached Selves](http://lesswrong.com/lw/4e/cached_selves/)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb0cb",
    "name": "Affective Death Spiral",
    "core": null,
    "slug": "affective-death-spiral",
    "oldSlugs": null,
    "postCount": 11,
    "description": {
      "markdown": "An **affective death spiral** (or **happy death spiral**) occurs when positive attributes of a theory, person, or organization combine with the [Halo effect](https://www.lesswrong.com/tag/halo-effect) in a feedback loop, resulting in the subject of the affective death spiral being held in higher and higher regard. In effect, every positive thing said about the subject results in more than one additional nice thing to say about the subject on average. This cascades like a nuclear chain reaction. This process creates theories that are believed for their own sake and organizations that exist solely to perpetuate themselves, especially when combined with the social dynamics of [groupthink](https://www.lesswrong.com/tag/groupthink). Affective death spirals are thus a primary cause of cultishness.\n\nThe same process can also occur with negative beliefs instead of positive, leading to a **death spiral of hate**.\n\nBlog posts\n----------\n\n*   [Affective Death Spirals](http://lesswrong.com/lw/lm/affective_death_spirals/)\n*   [Resist the Happy Death Spiral](http://lesswrong.com/lw/ln/resist_the_happy_death_spiral/)\n*   [When None Dare Urge Restraint](http://lesswrong.com/lw/ls/when_none_dare_urge_restraint/)\n*   [Every Cause Wants To Be A Cult](http://lesswrong.com/lw/lv/every_cause_wants_to_be_a_cult/)\n*   [Guardians of the Truth](http://lesswrong.com/lw/lz/guardians_of_the_truth/)\n*   [Guardians of Ayn Rand](http://lesswrong.com/lw/m1/guardians_of_ayn_rand/)\n*   [Evaporative Cooling of Group Beliefs](http://lesswrong.com/lw/lr/evaporative_cooling_of_group_beliefs/)\n\nSee also\n--------\n\n*   [Affect heuristic](https://www.lesswrong.com/tag/affect-heuristic), [Halo effect](https://www.lesswrong.com/tag/halo-effect), [Motivated skepticism](https://www.lesswrong.com/tag/motivated-skepticism)\n*   [Information cascade](https://www.lesswrong.com/tag/information-cascades), [Groupthink](https://www.lesswrong.com/tag/groupthink)\n*   [In-group bias](https://www.lesswrong.com/tag/in-group-bias)\n*   [Religion](https://www.lesswrong.com/tag/religion)\n*   [Death Spirals and the Cult Attractor](https://www.lesswrong.com/tag/death-spirals-and-the-cult-attractor)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "AeqCtS3BaY3cwzKAs",
    "name": "Fermi Estimation",
    "core": false,
    "slug": "fermi-estimation",
    "oldSlugs": null,
    "postCount": 19,
    "description": {
      "markdown": "A **Fermi Estimation** is a rough calculation which aims to be right within ~an order of magnitude, prioritizing getting a good enough to be useful answer without putting large amounts of thought and research in rather than being extremely accurate.\n\n**Related Pages:** [Forecasting & Prediction](https://www.lesswrong.com/tag/forecasting-and-prediction)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "KNJvXJK8WaSgE6iAZ",
    "name": "Problem Formulation & Conceptualization",
    "core": false,
    "slug": "problem-formulation-and-conceptualization",
    "oldSlugs": null,
    "postCount": 3,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "fmA6cA9psxibmH8MS",
    "name": "Mental Imagery / Visualization",
    "core": false,
    "slug": "mental-imagery-visualization",
    "oldSlugs": null,
    "postCount": 7,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "iTe27Ced8s8bGuvMK",
    "name": "Intellectual Progress (Individual-Level)",
    "core": false,
    "slug": "intellectual-progress-individual-level",
    "oldSlugs": null,
    "postCount": 27,
    "description": {
      "markdown": "This tag is closely related to [Scholarship & Learning](https://www.lesswrong.com/tag/scholarship-and-learning). However, Scholarship & Learning covers learning existing material, almost to the exclusion of developing new ideas. The present tag is exclusively about developing new ideas (or at least new-to-you ideas which *could have been* new to the world, if someone else hadn't invented them yet).\n\nAs you can see from the names, [Intellectual Progress (Society Level)](https://www.lesswrong.com/tag/intellectual-progress-society-level) is a related tag which deals with how progress happens for whole societies. The present tag deals with the same theme on an individual level.\n\nPosts on making progress in small groups are also welcome to this tag. The relevant cutoff is not the number of people (one vs many), but rather, the *relevance to an individual who is trying to make intellectual progress.* If a post is on the nuts-and-bolts process of making intellectual progress happen, it belongs here."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "zxmLyuTr7nujF523s",
    "name": "Zettelkasten",
    "core": false,
    "slug": "zettelkasten",
    "oldSlugs": null,
    "postCount": 4,
    "description": {
      "markdown": "**Zettelkasten** (German for \"slip box\") is a [note-taking](https://www.lesswrong.com/tag/note-taking) method popular amongst some LWers, and often praised for its scalability.\n\nA clear explanation can be found at abramdemski's post: [https://www.lesswrong.com/posts/NfdHG6oHBJ8Qxc26s/the-zettelkasten-method-1](https://www.lesswrong.com/posts/NfdHG6oHBJ8Qxc26s/the-zettelkasten-method-1)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "T4GgauaEfp6dHsR5P",
    "name": "Memetics",
    "core": false,
    "slug": "memetics",
    "oldSlugs": null,
    "postCount": 27,
    "description": {
      "markdown": "**Memetics** is the study of how ideas change and spread, in particular emphasizing analogy to evolutionary biology. In this framework, a key consideration is that an idea or memes ability to propagate in a given context, discussion, culture, or environment, is determined by its relative fitness in that environment, and this in turn determines its prevalence in the future.\n\n**Sequences:**  \n[Antimemetics](https://www.lesswrong.com/s/3xKXGh9RXaYTYZYgZ) by Isusr\n\n**Related Pages:** [Evolution](https://www.lesswrong.com/tag/evolution), [Evolutionary Psychology](https://www.lesswrong.com/tag/evolutionary-psychology), [Information Cascades](https://www.lesswrong.com/tag/information-cascades), [Information Hazards](https://www.lesswrong.com/tag/information-hazards), [Belief](https://www.lesswrong.com/tag/belief), [Religion](https://www.lesswrong.com/tag/religion), [Social & Cultural Dynamics](https://www.lesswrong.com/tag/social-and-cultural-dynamics), [Writing (communication method)](https://www.lesswrong.com/tag/writing-communication-method), [Censorship](https://www.lesswrong.com/tag/censorship), [Cultural knowledge](https://www.lesswrong.com/tag/cultural-knowledge), [Simulacrum Levels](https://www.lesswrong.com/tag/simulacrum-levels), [Social Media](https://www.lesswrong.com/tag/social-media)\n\n**See also:** [Wikipedia page](https://en.wikipedia.org/wiki/Memetics)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "puBcCq7aRwKoa7pXX",
    "name": "Note-Taking",
    "core": false,
    "slug": "note-taking",
    "oldSlugs": null,
    "postCount": 17,
    "description": {
      "markdown": "Taking notes is writing that's primarily for yourself -- whether for memory, or for study, or for reference, or to generate or develop ideas. \n\n**Related Pages:** [Zettelkasten](https://www.lesswrong.com/tag/zettelkasten), [Scholarship & Learning](https://www.lesswrong.com/tag/scholarship-and-learning), [Spaced Repetition](https://www.lesswrong.com/tag/spaced-repetition)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "o4rMP6GJto7ccBL3a",
    "name": "Travel",
    "core": false,
    "slug": "travel",
    "oldSlugs": null,
    "postCount": 12,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "kcj6MciuGPPK8rmCn",
    "name": "References (Language)",
    "core": false,
    "slug": "references-language",
    "oldSlugs": null,
    "postCount": 8,
    "description": {
      "markdown": "One of the basic functions of language is referring to entities. This tag is for posting delving into the nature of what this is."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "wdLqQnzdgiYpDXEWH",
    "name": "Infra-Bayesianism",
    "core": false,
    "slug": "infra-bayesianism",
    "oldSlugs": null,
    "postCount": 26,
    "description": {
      "markdown": "**Infra-Bayesianism** is a new approach to [epistemology](https://www.lesswrong.com/tag/epistemology) / [decision theory](https://www.lesswrong.com/tag/decision-theory) / reinforcement learning theory, which builds on \"imprecise probability\" to solve the problem of prior misspecification / grain-of-truth / nonrealizability which plagues [Bayesianism](https://www.lesswrong.com/tag/bayesianism) and Bayesian reinforcement learning.\n\nInfra-Bayesianism also naturally leads to an implementation of [UDT](https://www.lesswrong.com/tag/updateless-decision-theory), and (more speculatively at this stage) has applications to multi-agent theory, [embedded agency](https://www.lesswrong.com/tag/embedded-agency) and reflection.\n\nSee the [Infra-Bayesianism Sequence](https://www.lesswrong.com/posts/zB4f7QqKhBHa5b37a/introduction-to-the-infra-bayesianism-sequence)."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "GoxSGTqcd3WRR82i4",
    "name": "Eschatology",
    "core": false,
    "slug": "eschatology",
    "oldSlugs": null,
    "postCount": 3,
    "description": {
      "markdown": "Discussion of plausible far future conditions"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "se3XDuQ4xbeWvu4eF",
    "name": "80,000 Hours",
    "core": false,
    "slug": "80-000-hours",
    "oldSlugs": null,
    "postCount": 10,
    "description": {
      "markdown": "**80,000 Hours** is an organization in the Effective Altruist community that gives career advice. The name comes from an average career containing about 80,000 hours of work.\n\n### See also\n\n[Official Website](https://80000hours.org/)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "u4eLsvP4kY2qxxsr4",
    "name": "Neuralink",
    "core": false,
    "slug": "neuralink",
    "oldSlugs": null,
    "postCount": 9,
    "description": {
      "markdown": "**Neuralink** is a company founded by Elon Musk that does human-brain interfacing.\n\nSee also: [Neuroscience](lesswrong.com/tag/neuroscience), [Brain-Computer Interfaces](brain-computer-interfaces)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "xEZwTHPd5AWpgQx9w",
    "name": "GiveWell",
    "core": false,
    "slug": "givewell",
    "oldSlugs": null,
    "postCount": 27,
    "description": {
      "markdown": "**GiveWell** is an organization that evaluates the effectiveness of charities and recommends effective charities. It is associated with the [Effective Altruist](http://lesswrong.com/tag/effective-altruism) movement."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "hMXoyTAKxvCcsQBKf",
    "name": "Libertarianism",
    "core": false,
    "slug": "libertarianism",
    "oldSlugs": null,
    "postCount": 10,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "Zs4nYLkNr7Rbo4mAP",
    "name": "Utilitarianism",
    "core": false,
    "slug": "utilitarianism",
    "oldSlugs": null,
    "postCount": 34,
    "description": {
      "markdown": "**Utilitarianism** is a moral [philosophy](https://www.lesswrong.com/tag/philosophy) that says that what matters is the sum of everyone's welfare, or the \"greatest good for the greatest number\".\n\nNot to be confused with maximization of [utility](https://www.lessestwrong.com/tag/utility-functions), or [expected utility](https://www.lessestwrong.com/tag/expected-utility). If you're a utilitarian, you don't just sum over [possible worlds](https://www.lessestwrong.com/tag/possible-world); you sum over *people*.\n\nUtilitarianism comes in different variants. For example, unlike standard [total utilitarianism](https://en.wikipedia.org/wiki/Total_utilitarianism), [average utilitarianism](https://en.wikipedia.org/wiki/Average_utilitarianism) values the *average* utility among a group's members. [Negative utilitarianism](https://en.wikipedia.org/wiki/utilitarianism#Negative_utilitarianism) seeks only to minimize suffering, and is often discussed for its extreme implications.\n\n**Related Pages:** [Negative Utilitarianism](https://www.lesswrong.com/tag/negative-utilitarianism-1), [Consequentialism](https://www.lesswrong.com/tag/consequentialism), [Ethics & Morality](https://www.lesswrong.com/tag/ethics-and-morality), [Fun Theory](https://www.lesswrong.com/tag/fun-theory), [Complexity of Value](https://www.lesswrong.com/tag/complexity-of-value)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "YJSm87XxrhuJGhrxf",
    "name": "Market Inefficiency",
    "core": false,
    "slug": "market-inefficiency",
    "oldSlugs": [
      "inefficiency",
      "inefficiency-tag-work-needed"
    ],
    "postCount": 5,
    "description": {
      "markdown": ""
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "LMFBzsJaCRADQqw3F",
    "name": "Trolley Problem",
    "core": false,
    "slug": "trolley-problem",
    "oldSlugs": null,
    "postCount": 17,
    "description": {
      "markdown": "The [**trolley problem**](https://en.wikipedia.org/wiki/Trolley_problem) is a decision scenario which illustrates/tests ethical reasoning. (\"Trolley problems\" are modified scenarios inspired by the basic one.) In the classic scenario, you are faced with a situation where five people will die if you do nothing. However, you can kill a bystander to save the lives of the five. Many people will choose to do nothing, even though many utilitarian/consequentialist positions would say to kill the bystander."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "FwM9CYSSXgjX6fJvG",
    "name": "Psychotropics",
    "core": false,
    "slug": "psychotropics",
    "oldSlugs": null,
    "postCount": 11,
    "description": {
      "markdown": "**Psychotropic substances** are substances that noticeably affect how people think. This tag is intended primarily for substances that cause short-term experiences that (used responsibly) can have long-term positive or net-neutral effects on one's intellectual, emotional, and \"spiritual\" growth. This includes psychedelics such as Psilocybin and LSD as central examples, as well as MDMA and Cannabis as less-central examples\n\nContrast with the tags [Psychiatry](https://www.lesswrong.com/tag/psychiatry) and [Nootropics](https://www.lesswrong.com/tag/nootropics), which also pertain to substances which affect people's cognition"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "M9oWHR2XGLmg2DaZp",
    "name": "Analogy",
    "core": false,
    "slug": "analogy",
    "oldSlugs": null,
    "postCount": 4,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "nwcnHxrxcgnwJ878t",
    "name": "Luck",
    "core": false,
    "slug": "luck",
    "oldSlugs": null,
    "postCount": 7,
    "description": {
      "markdown": "In the same way _risk_ has a negative connotation, reflecting the possibility of negative outcomes, **luck** has a positive connotation for positive ones."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "kS3QBcbwtapefkSSZ",
    "name": "Scrupulosity",
    "core": false,
    "slug": "scrupulosity",
    "oldSlugs": null,
    "postCount": 6,
    "description": {
      "markdown": "**Scrupulosity** is a tendency to hold yourself to excessively high standards and to feel really bad when you fail to meet those standards."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "SrW9iP2j6Hi8R5PmT",
    "name": "Black Swans",
    "core": false,
    "slug": "black-swans",
    "oldSlugs": null,
    "postCount": 6,
    "description": {
      "markdown": "A **black swan** is a high-impact event that is hard to predict (but not necessarily of low probability). Also, an event that is not accounted for in a model, and therefore causes the model to break down when it occurs.\n\nConsidering some event a black swan doesn't give a leave to not assign any probabilities, since making decisions depending on the plausibility of such event is still equivalent to assigning probabilities that make the expected utility calculation give those decisions.\n\nExternal posts:\n---------------\n\n*   [White Swans Painted Black](http://www.overcomingbias.com/2008/09/white-swans-p-1.html) by Peter McCluskey\n\nSee also\n--------\n\n*   [Narrative fallacy](https://www.lesswrong.com/tag/narrative-fallacy)\n*   [Hindsight bias](https://www.lesswrong.com/tag/hindsight-bias)\n*   [I don't know](https://www.lesswrong.com/tag/i-dont-know)\n*   [Prediction](https://www.lesswrong.com/tag/forecasting-and-prediction), [prediction market](https://www.lesswrong.com/tag/prediction-markets), [forecast](https://www.lesswrong.com/tag/forecast)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "W6QZYSNt5FgWgvbdT",
    "name": "Coherent Extrapolated Volition",
    "core": false,
    "slug": "coherent-extrapolated-volition",
    "oldSlugs": null,
    "postCount": 33,
    "description": {
      "markdown": "**Coherent Extrapolated Volition** was a term developed by [Eliezer Yudkowsky](https://www.lesswrong.com/tag/eliezer-yudkowsky) while discussing [Friendly AI](https://wiki.lesswrong.com/wiki/Friendly_AI) development. It’s meant as an argument that it would not be sufficient to explicitly program what we think our desires and motivations are into an AI, instead, we should find a way to program it in a way that it would act in our best interests – what we *want* it to do and not what we *tell* it to.\n\n*Related*: [Friendly AI](https://wiki.lesswrong.com/wiki/Friendly_AI), [Metaethics Sequence](https://www.lesswrong.com/tag/metaethics-sequence), [Complexity of Value](https://www.lesswrong.com/tag/complexity-of-value)\n\n> *In calculating CEV, an AI would predict what an idealized version of us would want, \"if we knew more, thought faster, were more the people we wished we were, had grown up farther together\". It would recursively iterate this prediction for humanity as a whole, and determine the desires which converge. This initial dynamic would be used to generate the AI's utility function.* \n\nOften CEV is used generally to refer to what the idealized version of a person would want, separate from the context of building aligned AI's.\n\nWhat is volition?\n-----------------\n\nAs an example of the classical concept of volition, the author develops a simple thought experiment: imagine you’re facing two boxes, A and B. One of these boxes, and only one, has a diamond in it – box B. You are now asked to make a guess, whether to choose box A or B, and you chose to open box A. It was your *decision* to take box A, but your *volition* was to choose box B, since you wanted the diamond in the first place.\n\nNow imagine someone else – Fred – is faced with the same task and you want to help him in his decision by giving the box he chose, box A. Since you know where the diamond is, simply handling him the box isn’t helping. As such, you mentally extrapolate a volition for Fred, based on a version of him that knows where the diamond is, and imagine he actually wants box B.\n\nCoherent Extrapolated Volition\n------------------------------\n\nIn developing friendly AI, one acting for our best interests, we would have to take care that it would have implemented, from the beginning, a *coherent extrapolated volition of humankind*. In calculating CEV, an AI would predict what an idealized version of us would want, \"if we knew more, thought faster, were more the people we wished we were, had grown up farther together\". It would recursively iterate this prediction for humanity as a whole, and determine the desires which converge. This initial dynamic would be used to generate the AI's utility function.\n\nThe main problems with CEV include, firstly, the great difficulty of implementing such a program - “If one attempted to write an ordinary computer program using ordinary computer programming skills, the task would be a thousand lightyears beyond hopeless.” Secondly, the possibility that human values may not converge. Yudkowsky considered CEV obsolete almost immediately after its publication in 2004. He states that there's a \"principled distinction between discussing CEV as an initial dynamic of Friendliness, and discussing CEV as a Nice Place to Live\" and his essay was essentially conflating the two definitions.\n\nFurther Reading & References\n----------------------------\n\n*   [Coherent Extrapolated Volition](http://intelligence.org/files/CEV.pdf) by Eliezer Yudkowsky (2004)\n*   [Coherent Extrapolated Volition: A Meta-Level Approach to Machine Ethics](http://intelligence.org/files/CEV-MachineEthics.pdf) by Nick Tarleton (2010)\n*   [A Short Introduction to Coherent Extrapolated Volition](https://web.archive.org/web/20131231151554/http://www.acceleratingfuture.com/michael/blog/2009/12/a-short-introduction-to-coherent-extrapolated-volition-cev/) by Michael Anissimov\n*   [Hacking the CEV for Fun and Profit](https://www.lesswrong.com/lw/2b7/hacking_the_cev_for_fun_and_profit/) by Wei Dai\n*   [Two questions about CEV that worry me](https://www.lesswrong.com/lw/3fn/two_questions_about_cev_that_worry_me/) by Vladimir Slepnev\n*   [Beginning resources for CEV research](https://www.lesswrong.com/lw/5l0/beginning_resources_for_cev_research/) by Luke Muehlhauser\n*   [Cognitive Neuroscience, Arrow's Impossibility Theorem, and Coherent Extrapolated Volition](https://www.lesswrong.com/lw/7sb/cognitive_neuroscience_arrows_impossibility/) by Luke Muehlhauser\n*   [Objections to Coherent Extrapolated Volition](https://www.lesswrong.com/lw/8iy/objections_to_coherent_extrapolated_volition/) by Alexander Kruel\n\nSee also\n--------\n\n*   [Friendly AI](https://wiki.lesswrong.com/wiki/Friendly_AI)\n*   [Metaethics sequence](https://www.lesswrong.com/tag/metaethics-sequence)\n*   [Complexity of value](https://www.lesswrong.com/tag/complexity-of-value)\n*   [Coherent Aggregated Volition](https://www.lesswrong.com/tag/coherent-aggregated-volition)\n*   [Roko's basilisk](https://wiki.lesswrong.com/wiki/Roko's_basilisk)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "tdt83ChxnEgwwKxi6",
    "name": "Reading Group",
    "core": false,
    "slug": "reading-group",
    "oldSlugs": null,
    "postCount": 36,
    "description": {
      "markdown": "Discussing book chapters together, sharing summaries, questions and notes."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5kwicZiaCfYrXjAwT",
    "name": "Debugging",
    "core": false,
    "slug": "debugging",
    "oldSlugs": null,
    "postCount": 8,
    "description": {
      "markdown": "**Debugging** is the process of actively looking for, noticing, and solving small problems in regular decision-making. Insofar at the problems are small enough to have a root cause, this process can lead to small but compounding lifestyle improvements."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "xYLtnJ6keSHGfrLpe",
    "name": "Risk Management",
    "core": false,
    "slug": "risk-management",
    "oldSlugs": null,
    "postCount": 18,
    "description": {
      "markdown": "How to deal with uncertainty, risk aversion, and decision-making involving the possibility of bad outcomes, personal or not."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "EgL74XM3JRu5hjQxu",
    "name": "Perception",
    "core": false,
    "slug": "perception",
    "oldSlugs": null,
    "postCount": 17,
    "description": {
      "markdown": "Processing sensory information, sight, hearing, touch and possibly not-yet-existing senses."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "3QnDqGSdRMA5mdMM6",
    "name": "Value of Rationality",
    "core": false,
    "slug": "value-of-rationality",
    "oldSlugs": [
      "is-rationality-any-good"
    ],
    "postCount": 18,
    "description": {
      "markdown": "**Related pages:** [Costs of Rationality](https://www.lesswrong.com/tag/costs-of-rationality), [Valley of Bad Rationality](https://www.lesswrong.com/tag/valley-of-bad-rationality), [Pitfalls of Rationality](https://www.lesswrong.com/tag/pitfalls-of-rationality), [Criticisms of The Rationalist Movement](https://www.lesswrong.com/tag/criticisms-of-the-rationalist-movement)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "LXk7bxNkYSjgatdAt",
    "name": "Tool AI",
    "core": false,
    "slug": "tool-ai",
    "oldSlugs": [
      "ai-tool"
    ],
    "postCount": 22,
    "description": {
      "markdown": "A **tool AI** is a type of Artificial Intelligence that is built to be used as a tool by the creators, rather than being an agent with its own action and goal-seeking behavior.\n\nGenerally meant to refer to [AGI](https://wiki.lesswrong.com/wiki/AGI), tool AI is a proposed method for gaining some of the benefits of the intelligence while avoiding the dangers of having it act autonomously. It was coined by Holden Karnofsky, co-founder of GiveWell, in a critique of the Singularity Institute. Karnofsky proposed that, while he agreed that agent-based AGI was dangerous, it was an unnecessary path of development. His example of tool AI behavior was Google Maps, which uses complex algorithms and data to plot a route, but presents these results to the user instead of driving the user itself.\n\nEliezer Yudkowsky responded to this by enumerating several ways in which tool AI had similar difficulties in technical specification and safety. He also pointed out that it was not a common proposal among leading AGI thinkers.\n\n**See Also**\n------------\n\n*   [Oracle AI](https://www.lesswrong.com/tag/oracle-ai)\n\n**External Links**\n------------------\n\n*   [Conversation between Holden Karnofsky and Jaan Tallinn](http://groups.yahoo.com/group/givewell/message/287)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "rnvHPB3X2TiD5NMwY",
    "name": "Marketing",
    "core": false,
    "slug": "marketing",
    "oldSlugs": null,
    "postCount": 13,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "nBCLy89Nqd8ouR6XT",
    "name": "Quantum Mechanics",
    "core": false,
    "slug": "quantum-mechanics",
    "oldSlugs": [
      "quantum-physics"
    ],
    "postCount": 38,
    "description": {
      "markdown": "**Quantum Mechanics**: the bad news: our ordinary world is made out of weird, fuzzy, unpredictable stuff. The good news: the weird, fuzzy, unpredictable stuff is made out of unfamiliar but perfectly sensible math.  \n  \n**Related Pages:** [Physics](https://www.lesswrong.com/tag/physics), [Decoherence](https://www.lessestwrong.com/tag/decoherence), [Many-worlds interpretation](https://www.lessestwrong.com/tag/many-worlds-interpretation), [Configuration space](https://www.lessestwrong.com/tag/configuration-space), [](https://wiki.lesswrong.com/wiki/Egan's_law) [Egan's Law](https://www.lesswrong.com/tag/egans-law), [Timeless Physics](https://www.lesswrong.com/tag/timeless-physics)\n\nThe biggest conceptual difference between the world of quantum mechanics and the physical world at the level we typically interact with is that it's much harder to specify the state of a system. Classical systems like a bowling ball or a planet have well-defined positions and velocity, and the state of such a system can be completely specified by just those two quantities. Quantities like position and velocity are called vectors, and in a 3-dimensional world a vector has component along each of the 3 dimensions. The state of a classical point particle can thus be given by just 6 numbers.\n\nIn quantum mechanics, particles don't have both a well-defined position and velocity, and as a consequence, the vector that describes a quantum system can't be expressed in just 3 dimensions. In general, there is no upper limit on the number of dimensions a quantum system can have, and so while the state of our bowling ball exists in two 3D spaces (one for position and one for velocity), a quantum system, in general, exists in a space that's similar to the 3D space we're used to, but with an infinite number of dimensions. This space is called [Hilbert space](http://en.wikipedia.org/wiki/Hilbert_space). In order to be able to write down answers without using infinite numbers, quantum systems are usually mapped to other \"spaces\" like the 3D position and velocity spaces that we mentioned before. But information can be lost in this mapping, the same way a low-resolution photograph won't fully capture a 3-dimensional object. As a consequence of the lossy nature of this transformation, instead of the position of a quantum particle we instead get a distribution of possible positions. This is why quantum mechanics is often described as random or unpredictable.\n\nActually, quantum mechanics is perfectly predictable in Hilbert space. The only difficulty is that we don't live in Hilbert space, and while we have meter sticks and interferometers for making measurements in 3D space, we don't have any equipment for measuring Hilbert space directly. As a consequence, we have to make guesses about quantum systems based on what we see in 3D space. This is made even more difficult by the fact that once you measure a quantum system, it doesn't have the same distribution of possible positions it did before you measured it, so you can't sample repeatedly from the same distribution. Because we can't measure Hilbert space, the detailed dynamics of how exactly Hilbert space maps to real space, and what exactly happens in Hilbert space when you measure a system are still a matter of speculation and debate. People here have generally favored Hugh Everett's [many-worlds interpretation](https://www.lessestwrong.com/tag/many-worlds-interpretation) over others.\n\nIn spite of that, quantum mechanics is a mature field, and even if there's some uncertainty about what the results might imply, actually doing quantum mechanics is not terribly difficult for single-particle systems. From a practical standpoint, the evolution of the state of the system (also called the wave function or the state vector) is governed by differential equations the same way it is in classical physics. In quantum mechanics, this equation is called [Schrödinger equation](http://en.wikipedia.org/wiki/Schr%C3%B6dinger_equation) and most of practical quantum mechanics is concerned with solving this equation for different sets of boundary conditions, and in trying to find a \"space\" in which the quantum system can be expressed and solved most easily. Systems of more than one particle are considerably trickier because the state vector has to be mapped into multiple different 3D spaces at the same time.\n\nExternal Links\n--------------\n\n[Mangled Worlds](https://mason.gmu.edu/~rhanson/mangledworlds.html)\n\nResources\n---------\n\n*   [The Quantum Physics Sequence](https://www.lessestwrong.com/lw/r5/the_quantum_physics_sequence/) – this is really the best place to get started on the topic.\n*   Quantum Mechanics thing by Michael Nielsen"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "33sh8ktwbqP7hFtBP",
    "name": "Wildfires",
    "core": false,
    "slug": "wildfires",
    "oldSlugs": null,
    "postCount": 6,
    "description": {
      "markdown": "**Wildfires** are a type of natural disaster common in places like California."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "cosxPiaS8EGMnbMYf",
    "name": "Interpretive Labor",
    "core": false,
    "slug": "interpretive-labor",
    "oldSlugs": null,
    "postCount": 3,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "FdoP2PJhMz6x3gdDh",
    "name": "Nootropics & Other Cognitive Enhancement",
    "core": false,
    "slug": "nootropics-and-other-cognitive-enhancement",
    "oldSlugs": [
      "nootropics"
    ],
    "postCount": 12,
    "description": {
      "markdown": "**Nootoropics** (drugs/psychoactive substances) is a form of biological **Cognitive Enhancement**, i.e., any modification in the biology of a person which increases their cognitive capacities [^1^](https://www.lessestwrong.com/tag/nootropics-and-other-cognitive-enhancement?revision=0.0.17#fn1). Apart from drugs (nootropics), alternative biological cognitive enhancements include, possibly, magnetic stimulation.   \n  \n*Note: this page was last updated in November, 2014, and was written by a single author. It does not reflect any consensus on LessWrong.*\n\nThe most imminent, successful and polemic method is through the use of drugs, substances that alter the functioning of our brain's neurochemistry in order to improve certain aspects of cognition. There is an increasing trend in the use of cognitive enhancement drugs among healthy individuals in schools and colleges[^2^](https://www.lessestwrong.com/tag/nootropics-and-other-cognitive-enhancement?revision=0.0.17#fn2) [^3^](https://www.lessestwrong.com/tag/nootropics-and-other-cognitive-enhancement?revision=0.0.17#fn3). This means this kind of enhancement technology is already in use. The overall impact of a widespread use of these kinds of drugs could be enormous [^4^](https://www.lessestwrong.com/tag/nootropics-and-other-cognitive-enhancement?revision=0.0.17#fn4). However, the whole set of ethical consequences is unknown and subject of on-going developments [^5^](https://www.lessestwrong.com/tag/nootropics-and-other-cognitive-enhancement?revision=0.0.17#fn5) [^6^](https://www.lessestwrong.com/tag/nootropics-and-other-cognitive-enhancement?revision=0.0.17#fn6) [^7^](https://www.lessestwrong.com/tag/nootropics-and-other-cognitive-enhancement?revision=0.0.17#fn7) [^8^](https://www.lessestwrong.com/tag/nootropics-and-other-cognitive-enhancement?revision=0.0.17#fn8) [^9^](https://www.lessestwrong.com/tag/nootropics-and-other-cognitive-enhancement?revision=0.0.17#fn9) [^10^](https://www.lessestwrong.com/tag/nootropics-and-other-cognitive-enhancement?revision=0.0.17#fn10) [^11^](https://www.lessestwrong.com/tag/nootropics-and-other-cognitive-enhancement?revision=0.0.17#fn11)\n\nExamples\n--------\n\nCurrently, there are several drugs been used as cognitive enhancers by healthy individuals, e.g.: caffeine, ritalin, aderall, modafinil and Aricept. Academic research assessing the risks and benefits of these drugs in the healthy individual have began only recently. In addition, the results of those researches are vastly ignored by most of the concerned population. Three of the most used, promising and known cognitive enhancement drugs are listed in more detail below:\n\n*   Caffeine: Perhaps the most used and old cognitive enhancer. Caffeine has an excitatory result in the brain, by partially disabling the process that signals low availability of energy. [^12^](https://www.lessestwrong.com/tag/nootropics-and-other-cognitive-enhancement?revision=0.0.17#fn12). Caffeine and its metabolites also increase the serum concentration of adrenaline, thus increasing heart rate, blood pressure and stress [^13^](https://www.lessestwrong.com/tag/nootropics-and-other-cognitive-enhancement?revision=0.0.17#fn13). Many researchers have found a vast number of beneficial cognitive effects, as improved concentration and memory retention [^14^](https://www.lessestwrong.com/tag/nootropics-and-other-cognitive-enhancement?revision=0.0.17#fn14). Its beneficial effects on overall health are also documented [^15^](https://www.lessestwrong.com/tag/nootropics-and-other-cognitive-enhancement?revision=0.0.17#fn15). However, the American adult male's average dosage[^16^](https://www.lessestwrong.com/tag/nootropics-and-other-cognitive-enhancement?revision=0.0.17#fn16) [^17^](https://www.lessestwrong.com/tag/nootropics-and-other-cognitive-enhancement?revision=0.0.17#fn17) surpasses the healthy dosage fourfold. At the average ingested dosage, caffeine has strong detrimental health effects increasing the risk of heart attacks and strokes [^18^](https://www.lessestwrong.com/tag/nootropics-and-other-cognitive-enhancement?revision=0.0.17#fn18) [^19^](https://www.lessestwrong.com/tag/nootropics-and-other-cognitive-enhancement?revision=0.0.17#fn19) and also possess addiction potential, with severe withdrawal symptoms such as depression, irritability, pain and narcolepsy [^20^](https://www.lessestwrong.com/tag/nootropics-and-other-cognitive-enhancement?revision=0.0.17#fn20).\n*   Modafinil: Modafinil effects are mediated through the neurotransmitters histamine and dopamine. Histamine regulates the state of wakefulness. Dopamine has important roles on motivation, cognition, reward, attention and working memory [^21^](https://www.lessestwrong.com/tag/nootropics-and-other-cognitive-enhancement?revision=0.0.17#fn21). There are at least 7 studies on the cognitive enhancement properties of modafinil in healthy individuals [^22^](https://www.lessestwrong.com/tag/nootropics-and-other-cognitive-enhancement?revision=0.0.17#fn22) [^23^](https://www.lessestwrong.com/tag/nootropics-and-other-cognitive-enhancement?revision=0.0.17#fn23) [^24^](https://www.lessestwrong.com/tag/nootropics-and-other-cognitive-enhancement?revision=0.0.17#fn24) [^25^](https://www.lessestwrong.com/tag/nootropics-and-other-cognitive-enhancement?revision=0.0.17#fn25) [^26^](https://www.lessestwrong.com/tag/nootropics-and-other-cognitive-enhancement?revision=0.0.17#fn26) [^27^](https://www.lessestwrong.com/tag/nootropics-and-other-cognitive-enhancement?revision=0.0.17#fn27) [^28^](https://www.lessestwrong.com/tag/nootropics-and-other-cognitive-enhancement?revision=0.0.17#fn28) [^29^](https://www.lessestwrong.com/tag/nootropics-and-other-cognitive-enhancement?revision=0.0.17#fn29). Those studies' results are:\n    *   Increased new-language learning [^30^](https://www.lessestwrong.com/tag/nootropics-and-other-cognitive-enhancement?revision=0.0.17#fn30)\n    *   Enhanced performance on tests of digit span, visual pattern recognition memory, spatial planning and stop signal reaction time [^31^](https://www.lessestwrong.com/tag/nootropics-and-other-cognitive-enhancement?revision=0.0.17#fn31).\n    *   Lower error rate in a visual spatial task[^32^](https://www.lessestwrong.com/tag/nootropics-and-other-cognitive-enhancement?revision=0.0.17#fn32).\n    *   Improved fatigue levels, motivation, reaction time and vigilance[^33^](https://www.lessestwrong.com/tag/nootropics-and-other-cognitive-enhancement?revision=0.0.17#fn33).\n    *   Improvement on spatial working memory, planning and decision making at the most difficult levels, as well as visual pattern recognition memory following delay and subjective ratings of enjoyment of task performance [^34^](https://www.lessestwrong.com/tag/nootropics-and-other-cognitive-enhancement?revision=0.0.17#fn34) .\n    *   Decreased impairment in vestibular function in 24h sleep deprived individuals[^35^](https://www.lessestwrong.com/tag/nootropics-and-other-cognitive-enhancement?revision=0.0.17#fn35).\n    *   Decreased impairment on performance in a flight simulation test in 30h and 40h sleep deprived individuals[^36^](https://www.lessestwrong.com/tag/nootropics-and-other-cognitive-enhancement?revision=0.0.17#fn36) [^37^](https://www.lessestwrong.com/tag/nootropics-and-other-cognitive-enhancement?revision=0.0.17#fn37).\n    *   No adverse effects were reported in none of these studies, however this wasn't the target of any of them.\n\n[Many](http://www.springerlink.com/content/?k=modafinil) [other](http://www.sciencedirect.com/science?_ob=ArticleListURL&_method=list&_ArticleListID=2100456952&_sort=r&_st=13&view=c&_acct=C000228598&_version=1&_urlVersion=0&_userid=10&md5=9facf727fff44b33bdd632f4e3f51852&searchtype=a) studies in non-healthy patients have found some adverse effects[^38^](https://www.lessestwrong.com/tag/nootropics-and-other-cognitive-enhancement?revision=0.0.17#fn38), but have confirmed its safety and - so far - no addiction potential profile. However, research on its long-term safety is deeply needed.\n\n*   Aricept(Donepezil): Aricept inhibits the breakdown of acetylcholine. Acetylcholine is a neurotransmitter linked to long-term memory. There are at least two studies with healthy individuals that have found: greater retention of how to perform a set of complex tasks [^39^](https://www.lessestwrong.com/tag/nootropics-and-other-cognitive-enhancement?revision=0.0.17#fn39) and increased visual and verbal long-term memory [^40^](https://www.lessestwrong.com/tag/nootropics-and-other-cognitive-enhancement?revision=0.0.17#fn40).\n\nBiases affecting our judgment\n-----------------------------\n\nThere are several [cognitive biases](http://wiki.lesswrong.com/wiki/Bias) affecting our judgment on the risks and efficacy of biological cognitive enhancers. Two are worth mentioning:\n\n*   Statistical format: we do not update our beliefs correctly when presented with absolute probabilities (i.e.: 10%) - when the information is presented in terms of occurrences (i.e.: one person in ten) the belief update is much more close to [bayesian](https://www.lessestwrong.com/lw/1to/what_is_bayesianism/) [^41^](https://www.lessestwrong.com/tag/nootropics-and-other-cognitive-enhancement?revision=0.0.17#fn41). This bias impairs our ability to use information from scientific research to update our beliefs. One can easily comprehend the risks involved with a certain drug if a friend suffered a heart attack due to its use, avoiding such drug from then on. But reading an abstract number showing the rise in blood pressure – [the most important preventable risk factor for death](http://www.who.int/healthinfo/global_burden_disease/GlobalHealthRisks_report_full.pdf) \\- of caffeine users is much higher than of modafinil users is too far away from the occurrence-based [savannah way](http://wiki.lesswrong.com/wiki/Evolutionary_psychology) our brains are accustomed to absorb information [^42^](https://www.lessestwrong.com/tag/nootropics-and-other-cognitive-enhancement?revision=0.0.17#fn42)\n*   Status quo: a consistent and unjustified tendency to prefer that some parameter stays in the configuration it has always been, over other possible configurations. This tendency can manifest itself by preferring to continue to use a known drug with many side effects over a new safer drug and also impair our judgment of many others technological advancements. When analyzing if a new configuration should be used, Bostrom and Ord [^43^](https://www.lessestwrong.com/tag/nootropics-and-other-cognitive-enhancement?revision=0.0.17#fn43) suggest the following heuristic: we imagine a scenario were the parameter will naturally change to the new configuration and ask if we would intervene. If we wouldn't intervene, then we have a reason to think the new configuration should be preferred.\n\nRelevance\n---------\n\nBostrom [^44^](https://www.lessestwrong.com/tag/nootropics-and-other-cognitive-enhancement?revision=0.0.17#fn44) argues for the huge impact of cognitive enhancements: \"Imagine a researcher invented an inexpensive drug which was completely safe and which improved all‐round cognitive performance by just 1%. The gain would hardly be noticeable in a single individual. But if the 10 million scientists in the world all benefited from the drug the inventor would increase the rate of scientific progress by roughly the same amount as adding 100,000 new scientists. Each year the invention would amount to an indirect contribution equal to 100,000 times what the average scientist contributes. Even an Einstein or a Darwin at the peak of their powers could not make such a great impact. \" Even outside the academic community, imagine a drug that improves the efficiency of all employees and workers around the world by just 1%. This would roughly means adding more 1 trillion dollars of production every year to the world gross product. This would be equivalent to the addition of an entire well developed country to the world, Germany for instance.\n\nReferences\n----------\n\n1.  SAVULESCU, J. & MEULEN, Rudd ter (orgs.) (2011) \"Enhancing Human Capacities\". Wiley-Blackwell.\n2.  [Jump up↑](https://wiki.lesswrong.com/wiki/Biological_Cognitive_Enhancement#cite_ref-2) KAPNER, E. (2003) \"Recreational use of Ritalin on college campuses\". InfoFactsResources – The Higher Education Center for Alcohol and Other Drug Prevention. Available at: www.edc.org/hec/pubs/factsheets/ritalin.pdf (accessed 4 Jan 2006).\n3.  [Jump up↑](https://wiki.lesswrong.com/wiki/Biological_Cognitive_Enhancement#cite_ref-3) TETER, C.J. et al. (2005). \"Prevalence and motives for illicit use of prescription stimulants in an undergraduate student sample\", J Am Coll Health 53 (2005).\n4.  ↑ [^Jump up to:4.0^](https://wiki.lesswrong.com/wiki/Biological_Cognitive_Enhancement#cite_ref-3ways_4-0) [^4.1^](https://wiki.lesswrong.com/wiki/Biological_Cognitive_Enhancement#cite_ref-3ways_4-1) BOSTROM, NICK. (2008) \"Three Ways to Advance Science\" For Nature Podcast, 31 January 2008. Available at: [http://www.nickbostrom.com/views/science.pdf](http://www.nickbostrom.com/views/science.pdf)\n5.  [Jump up↑](https://wiki.lesswrong.com/wiki/Biological_Cognitive_Enhancement#cite_ref-5) SANDBERG, Anders & LIAO, S.M., (2008) \"The Normativity of Memory Modification\", Neuroethics (2008), (1 2) 85-99.\n6.  [Jump up↑](https://wiki.lesswrong.com/wiki/Biological_Cognitive_Enhancement#cite_ref-6) SANBERG, Anders & SAVULESCU, Julian. (2008). \"Neuroenhancement of Love and Marriage: The Chemicals Between Us.\" Neuroethics (2008) Vol. 1:31-44.\n7.  [Jump up↑](https://wiki.lesswrong.com/wiki/Biological_Cognitive_Enhancement#cite_ref-7) BOSTROM, Nick & SAVULESCO, Julian. (orgs.), (2009) \"Human Enhancement\". Oxford University Press.\n8.  [Jump up↑](https://wiki.lesswrong.com/wiki/Biological_Cognitive_Enhancement#cite_ref-8) BOSTROM, Nick & SANDBERG, Anders. (2006) \"Converging Cognitive Enhancements\", Annals of the New York Academy of Sciences, Vol. 1093.\n9.  [Jump up↑](https://wiki.lesswrong.com/wiki/Biological_Cognitive_Enhancement#cite_ref-heuristic_9-0) SANDBERG, Nick & SANDBERG, Anders. (2009) \"The Wisdom of Nature: an Evolutionary Heuristic for Human Enhancement\" in: BOSTROM, Nick & SAVULESCU, Julian(orgs.). Human Enhancement. Oxford University Press, EUA.\n10.  [Jump up↑](https://wiki.lesswrong.com/wiki/Biological_Cognitive_Enhancement#cite_ref-10) BOSTROM, Nick & SANDBERG, Anders. (2009) \"Cognitive Enhancement: Methods, Ethics, Regulatory Challenges\", Science and Engineering Ethics, Vol. 15, No. 3.\n11.  ↑ [^Jump up to:11.0^](https://wiki.lesswrong.com/wiki/Biological_Cognitive_Enhancement#cite_ref-squire_11-0) [^11.1^](https://wiki.lesswrong.com/wiki/Biological_Cognitive_Enhancement#cite_ref-squire_11-1) SQUIRE, Larry R. et al. (orgs.) (2008) \"Fundamental Neuroscience.\" Academic Press. 3a edition.\n12.  [Jump up↑](https://wiki.lesswrong.com/wiki/Biological_Cognitive_Enhancement#cite_ref-12) DEWS, P.B. (1984). \"Caffeine: Perspectives from Recent Research.\" Berlin: Springer-Valerag\n13.  [Jump up↑](https://wiki.lesswrong.com/wiki/Biological_Cognitive_Enhancement#cite_ref-13) BOLTON, Sanford (1981). \"Caffeine: Psychological Effects, Use and Abuse\". Orthomolecular Psychiatry 10 (3): 202–211.\n14.  [Jump up↑](https://wiki.lesswrong.com/wiki/Biological_Cognitive_Enhancement#cite_ref-14) THOMPSON, Rebecca & KEENE, Karen (2004). \"The pros and cons of caffeine\". The Psychologist (The British Psychological Society) 17 (12): 698–701.\n15.  [Jump up↑](https://wiki.lesswrong.com/wiki/Biological_Cognitive_Enhancement#cite_ref-15) NCDT (2011). Report of the 2011 National Coffee Drinking Trends (NCDT).\n16.  [Jump up↑](https://wiki.lesswrong.com/wiki/Biological_Cognitive_Enhancement#cite_ref-16) ILLY, A. & VIVIANI, R. (1995) Espresso Coffee: The Chemistry of Quality. San Diego: Academic P.\n17.  [Jump up↑](https://wiki.lesswrong.com/wiki/Biological_Cognitive_Enhancement#cite_ref-17) GREENBERG, J. A. Et al.(2007) \"Caffeinated beverage intake and the risk of heart disease mortality in the elderly: a prospective analysis\". Am J Clin Nutr 85 (2): 392–8.\n18.  [Jump up↑](https://wiki.lesswrong.com/wiki/Biological_Cognitive_Enhancement#cite_ref-18) LESON. C. L. Et al. (1998) \"Caffeine overdose in an adolescent male.\". Journal of toxicology. Clinical toxicology Vol. 26 (5–6): 407–15.\n19.  JULIANO, Laura M. & GRIFFITHS, Roland R. (2004) \"A critical review of caffeine withdrawal: empirical validation of symptoms and signs, incidence, severity, and associated features\". Psychopharmacology 176 (1): 1–29.\n20.  ↑ [^Jump up to:20.0^](https://wiki.lesswrong.com/wiki/Biological_Cognitive_Enhancement#cite_ref-caid1_20-0) [^20.1^](https://wiki.lesswrong.com/wiki/Biological_Cognitive_Enhancement#cite_ref-caid1_20-1) CAIDWELL, John A. et al. (1999) \"The Effects of Modafinil on Aviator Performance During 40 Hours of Continuous Wakefulness: A UH-60 Helicopter Simulator Study.\" Army aeromedical research unit fort rucker al.\n21.  ↑ [^Jump up to:21.0^](https://wiki.lesswrong.com/wiki/Biological_Cognitive_Enhancement#cite_ref-caid2_21-0) [^21.1^](https://wiki.lesswrong.com/wiki/Biological_Cognitive_Enhancement#cite_ref-caid2_21-1) CAIDWELL, John A. et al. (2004) \"The Efficacy of Modafinil for Sustaining Alertness and Simulator Flight Performance in F-117 Pilots During 37 Hours of Continuous Wakefulness.\" Air Force Research lab brooks AFB TX, Human effectiveness Dir/Biodynamics and protection div.\n22.  ↑ [^Jump up to:22.0^](https://wiki.lesswrong.com/wiki/Biological_Cognitive_Enhancement#cite_ref-li_22-0) [^22.1^](https://wiki.lesswrong.com/wiki/Biological_Cognitive_Enhancement#cite_ref-li_22-1) LI Yanfeng, ZHAN Hao, XIN Yimei, et al. (2007) \"Effects of modafinil on vestibular function during 24 hour sleep deprivation\". Frontiers of medicine in China, Vol. 1, Number 2, 226-229.\n23.  ↑ [^Jump up to:23.0^](https://wiki.lesswrong.com/wiki/Biological_Cognitive_Enhancement#cite_ref-bara_23-0) [^23.1^](https://wiki.lesswrong.com/wiki/Biological_Cognitive_Enhancement#cite_ref-bara_23-1) BARANSKI, J. V. Et al. (2004) \"Effects of modafinil on cognitive and meta-cognitive performance\". Hum Psychopharmacol. 2004 Jul; Vol. 19(5):323-32.\n24.  ↑ [^Jump up to:24.0^](https://wiki.lesswrong.com/wiki/Biological_Cognitive_Enhancement#cite_ref-mull1_24-0) [^24.1^](https://wiki.lesswrong.com/wiki/Biological_Cognitive_Enhancement#cite_ref-mull1_24-1) MÜLLER, U. Et al. (2004) \"Effects of modafinil on working memory processes in humans\". Psychopharmacology (Berl.) Vol. 177 (1-2): 161–9. **Cite error: Invalid** `**<ref>**` **tag; name \"mull1\" defined multiple times with different content**\n25.  ↑ [^Jump up to:25.0^](https://wiki.lesswrong.com/wiki/Biological_Cognitive_Enhancement#cite_ref-turn_25-0) [^25.1^](https://wiki.lesswrong.com/wiki/Biological_Cognitive_Enhancement#cite_ref-turn_25-1) TURNER, D. C et al. (2003). \"Cognitive enhancing effects of modafinil in healthy volunteers\". Psychopharmacology (Berl.) Vol. 165 (3): 260–9.\n26.  ↑ [^Jump up to:26.0^](https://wiki.lesswrong.com/wiki/Biological_Cognitive_Enhancement#cite_ref-mull2_26-0) [^26.1^](https://wiki.lesswrong.com/wiki/Biological_Cognitive_Enhancement#cite_ref-mull2_26-1) MULLER, U. et all. (2012) \"Effects of modafinil on non-verbal cognition, task enjoyment and creative thinking in healthy volunteers.\" Neuropharmocology: 2012 (In press)\n27.  ↑ [^Jump up to:27.0^](https://wiki.lesswrong.com/wiki/Biological_Cognitive_Enhancement#cite_ref-Gilleen_27-0) [^27.1^](https://wiki.lesswrong.com/wiki/Biological_Cognitive_Enhancement#cite_ref-Gilleen_27-1) GILLEEN, J., et al. (2014). \"Modafinil combined with cognitive training is associated with improved learning in healthy volunteers--a randomised controlled trial.\" European Neuropsychopharmacology : The Journal of the European College of Neuropsychopharmacology, 24(4), 529–39. doi:10.1016/j.euroneuro.2014.01.001.\n28.  [Jump up↑](https://wiki.lesswrong.com/wiki/Biological_Cognitive_Enhancement#cite_ref-28) [http://www.rxlist.com/provigil-drug/side-effects-interactions.htm](http://www.rxlist.com/provigil-drug/side-effects-interactions.htm)\n29.  [Jump up↑](https://wiki.lesswrong.com/wiki/Biological_Cognitive_Enhancement#cite_ref-29) YESAVAGE, et al. (2002). \"Donepezil and flight simulator performance Effects on retention of complex skills\" NEUROLOGY 2002; 59:123–125.\n30.  [Jump up↑](https://wiki.lesswrong.com/wiki/Biological_Cognitive_Enhancement#cite_ref-30) YESAVAGE, et al. (2002). \"Donepezil and flight simulator performance Effects on retention of complex skills\" NEUROLOGY 2002; 59:123–125.\n31.  [Jump up↑](https://wiki.lesswrong.com/wiki/Biological_Cognitive_Enhancement#cite_ref-31) POHL, Rüdiger (orgs.). (2005) \"Cognitive Illusions: A Handbook on Fallacies and Biases in Thinking, Judgement and Memory\". Psychology Press. pp. 61-78\n32.  [Jump up↑](https://wiki.lesswrong.com/wiki/Biological_Cognitive_Enhancement#cite_ref-32) BUSS, David(orgs.). (2005) \"The Handbook of Evolutionary Psychology\". Wiley, New Jersey. pp. 739-740.\n33.  [Jump up↑](https://wiki.lesswrong.com/wiki/Biological_Cognitive_Enhancement#cite_ref-33) BOSTROM, Nick & ORD, Toby. (2006) \"The Reversal Test: Eliminating Status Quo Bias in Applied Ethics\". Ethics 116 (Julho 2006): 656-679."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "dvQQj5WmQjFB6ZJ5c",
    "name": "Lost Purposes",
    "core": false,
    "slug": "lost-purposes",
    "oldSlugs": null,
    "postCount": 3,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "Wj2iipHgLnHA8ZhZH",
    "name": "Strong Opinions Weakly Held",
    "core": false,
    "slug": "strong-opinions-weakly-held",
    "oldSlugs": null,
    "postCount": 2,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "grDZHm8y34H9C7uui",
    "name": "Tacit Knowledge",
    "core": false,
    "slug": "tacit-knowledge",
    "oldSlugs": null,
    "postCount": 3,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "PAugtjFrmvbepCNej",
    "name": "Love ",
    "core": false,
    "slug": "love",
    "oldSlugs": null,
    "postCount": 9,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "YhZLQQKsREKE7fC4F",
    "name": "Principal-Agent Problems",
    "core": false,
    "slug": "principal-agent-problems",
    "oldSlugs": [
      "principle-agent-problems"
    ],
    "postCount": 7,
    "description": {
      "markdown": "A [**Principal-Agent Problem**](https://en.wikipedia.org/wiki/Principal%E2%80%93agent_problem) is when a decision is delegated to an agent, and the agent is incentivized to make choices other than those the principal would want."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "v4pviL33XGMuTpSNs",
    "name": "Psychopathy",
    "core": false,
    "slug": "psychopathy",
    "oldSlugs": null,
    "postCount": 8,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "RffCgqtwT86pNBJof",
    "name": "Shaping Your Environment",
    "core": false,
    "slug": "shaping-your-environment",
    "oldSlugs": null,
    "postCount": 4,
    "description": {
      "markdown": "**Shaping Your Environment** is sometimes more effective than changing your internal motivation structure, when it comes to behavior change."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "fuZZ64fNz24BLrXnY",
    "name": "Robotics",
    "core": false,
    "slug": "robotics",
    "oldSlugs": null,
    "postCount": 20,
    "description": {
      "markdown": "**Robotics** is the field dealing with robots. There are various hopes and concerns around this topic, including mass unemployment caused by automation and [autonomous weapons](https://www.lesswrong.com/tag/autonomous-weapons)."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "dLfyktLWd7BqtsZBf",
    "name": "Memory Reconsolidation",
    "core": false,
    "slug": "memory-reconsolidation",
    "oldSlugs": null,
    "postCount": 17,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "NZB24aR9uHmDc5GcT",
    "name": "Sleeping Beauty Paradox",
    "core": false,
    "slug": "sleeping-beauty-paradox",
    "oldSlugs": null,
    "postCount": 60,
    "description": {
      "markdown": "The **Sleeping Beauty Paradox** is a question of how [anthropics](http://lesswrong.com/tag/anthropics) affects [probabilities](https://www.lesswrong.com/tag/probability-and-statistics).\n\n> Sleeping Beauty volunteers to undergo the following experiment. On Sunday she is given a drug that sends her to sleep. A fair coin is then tossed just once in the course of the experiment to determine which experimental procedure is undertaken. If the coin comes up heads, Beauty is awakened and interviewed on Monday, and then the experiment ends. If the coin comes up tails, she is awakened and interviewed on Monday, given a second dose of the sleeping drug, and awakened and interviewed again on Tuesday. The experiment then ends on Tuesday, without flipping the coin again. The sleeping drug induces a mild amnesia, so that she cannot remember any previous awakenings during the course of the experiment (if any). During the experiment, she has no access to anything that would give a clue as to the day of the week. However, she knows all the details of the experiment.\n\n> Each interview consists of one question, “What is your credence now for the proposition that our coin landed heads?”\n\nOne argument says that since Beauty will see the same thing on waking whether the coin came up heads or not, what she sees on waking provides no evidence one way or the other about the coin, and therefore she should stick with the prior probability of one half.\n\nAnother argument replies that the two awakenings when the coin comes up tails imply that waking up itself should be considered evidence in favor of tails. Out of all possible situations where Beauty is asked the question, only one out of three has the coin showing heads. Therefore, one third.\n\nA third argument tries to add rigor by considering monetary payoffs. If Beauty's bets about the coin get paid out once per experiment, she will do best by acting as if the probability is one half. If the bets get paid out once per awakening, acting as if the probability is one third has the best expected value.\n\nExternal Links\n--------------\n\n*   [Video explanation by Julia Galef](https://www.youtube.com/watch?v=zL52lG6aNIY)\n*   [Sleeping Beauty Problem on Wikipedia](https://en.wikipedia.org/wiki/Sleeping_Beauty_problem)\n\nSee Also\n--------\n\n*   [Decision Theory](https://www.lesswrong.com/tag/decision-theory)\n*   [Newcomb's Problem](https://www.lesswrong.com/tag/newcomb-s-problem)\n*   [Counterfactual Mugging](https://www.lesswrong.com/tag/counterfactual-mugging)\n*   [Parfit's hitchhiker](https://wiki.lesswrong.com/wiki/Parfit%27s_hitchhiker)\n*   [Smoker's lesion](https://wiki.lesswrong.com/wiki/Smoker%27s_lesion)\n*   [Absentminded driver](https://wiki.lesswrong.com/wiki/Absentminded_driver)\n*   [Prisoner's Dilemma](https://www.lesswrong.com/tag/prisoner-s-dilemma)\n*   [Pascal's Mugging](https://www.lesswrong.com/tag/pascal-s-mugging)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "JMD7LTXTisBzGAfhX",
    "name": "Rationality A-Z (discussion & meta)",
    "core": false,
    "slug": "rationality-a-z-discussion-and-meta",
    "oldSlugs": [
      "rationality-a-z-discussion-and-meta"
    ],
    "postCount": 57,
    "description": {
      "markdown": "**Rationality: From AI to Zombies**, also known as **The Sequences**, is a series of essays by Eliezer Yudkowsky published from 2006 to 2009, which were later compiled into a book. This tag is for critiques and discussion of the Sequences, attempts to organize the Sequences, and the publication of Rationality: From AI to Zombies.\n\nTo read the book, see [lesswrong.com/rationality](https://lesswrong.com/rationality)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "ThuGDzi7X5YsAJfjs",
    "name": "Aggregation",
    "core": false,
    "slug": "aggregation",
    "oldSlugs": null,
    "postCount": 1,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "Jj9QvYCLHEoDqvpaR",
    "name": "Request Post",
    "core": false,
    "slug": "request-post",
    "oldSlugs": null,
    "postCount": 4,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "qQMEMrXioExa4uhTB",
    "name": "Value Drift",
    "core": false,
    "slug": "value-drift",
    "oldSlugs": null,
    "postCount": 11,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "Xw6pxiicjuv6NJWjf",
    "name": "History of Rationality",
    "core": false,
    "slug": "history-of-rationality",
    "oldSlugs": null,
    "postCount": 25,
    "description": {
      "markdown": "A post is relevant to the **history of rationality** if it discusses past events pertaining to [the LW community](https://www.lesswrong.com/tag/community) or another topic related to [rationality](https://www.lesswrong.com/tag/rationality), or the post is of strong historical interest.\n\nYou can see a list of all LW posts, from oldest to newest, [here](https://wiki.lesswrong.com/wiki/Less_Wrong/All_articles).\n\nIf you're trying to get a better understanding of the community and culture here, you may also be interested in the tag [Terminology/Jargon (meta)](https://www.lesswrong.com/tag/terminology-jargon-meta).\n\n**Related Pages:** [History of Less Wrong](https://www.lesswrong.com/tag/history-of-less-wrong), [History](https://www.lesswrong.com/tag/history)\n\nHistorically relevant organizations, groups, and movements include:\n\n*   [CFAR](https://www.lesswrong.com/tag/center-for-applied-rationality-cfar?showPostCount=true&useTagName=true)\n*   [80,000 Hours](https://www.lesswrong.com/tag/80-000-hours?showPostCount=true&useTagName=true)\n*   [CHAI](https://www.lesswrong.com/tag/centre-for-human-compatible-ai?showPostCount=true&useTagName=true)\n*   [FHI](https://www.lesswrong.com/tag/future-of-humanity-institute?showPostCount=true&useTagName=true)\n*   [MIRI](https://www.lesswrong.com/tag/machine-intelligence-research-institute-miri?showPostCount=true&useTagName=true)\n*   [OpenAI](https://www.lesswrong.com/tag/openai?showPostCount=true&useTagName=true)\n*   [Ought](https://www.lesswrong.com/tag/ought?showPostCount=true&useTagName=true)\n*   [GiveWell](https://www.lesswrong.com/tag/givewell?showPostCount=true&useTagName=true)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "EuDw6uxQW2ZBRFhMo",
    "name": "Aversion",
    "core": false,
    "slug": "aversion",
    "oldSlugs": [
      "aversion-ugh-fields"
    ],
    "postCount": 17,
    "description": {
      "markdown": "CFAR's [2019 workshop participant handbook](https://www.lesswrong.com/posts/Z9cbwuevS9cqaR96h/cfar-participant-handbook-now-available-to-all) defines an **aversion** as\n\n> any sort of mental mechanism that causes us to be less likely to engage in a particular activity, or to do so only with pain, displeasure, or discomfort. Aversions can be conscious or unconscious, reasoned or felt, verbal or visceral, and they can range anywhere from a slight tinge of antipathy to outright phobias.\n\n[**Aversion factoring**](https://acritch.com/aversions/) is [goal factoring](https://www.lesswrong.com/tag/goal-factoring) for aversions: trying to understand why you're averse to a particular thing by decomposing it into underlying preferences and experiences.\n\nUgh fields\n----------\n\nAversions make us less likely to engage in \"activities,\" and as Scott Alexander's (2011) [Physical and Mental Behavior](https://www.lesswrong.com/posts/5dhWhjfxn4tPfFQdi/physical-and-mental-behavior) notes, this includes mental as well as physical activities. Pavlovian conditioning can cause humans to unconsciously flinch from even thinking about a serious personal problem they have.\n\nWriting in 2010, Roko introduced the term [**ugh field**](https://www.lesswrong.com/posts/EFQ3F6kmt4WHXRqik/ugh-fields) to refer to this problem: \n\n> If you fail or are punished sufficiently many times in some problem area, and acting in that area is always \\[preceded\\] by thinking about it, your brain will propagate the psychological pain right back to the moment you first begin to entertain a thought about the problem, and hence cut your conscious optimizing ability right out of the loop.\n> \n> \\[...\\] The subtlety with the Ugh Field is that the flinch occurs ***before you start to consciously think*** about how to deal with the Unhappy Thing, meaning that you never deal with it, and you don't even have the option of dealing with it in the normal run of things. I find it frightening that my lizard brain could implicitly be making life decisions for me, without even asking my permission!\n\nThe ugh field forms a self-shadowing blind spot covering an area desperately in need of optimization.\n\nBlog posts and external links\n-----------------------------\n\n*   [Avoiding Your Belief's Real Weak Points](https://www.lesswrong.com/lw/jy/avoiding_your_beliefs_real_weak_points/)\n*   [Defeating Ugh Fields in Practice](https://www.lesswrong.com/lw/2cv/defeating_ugh_fields_in_practice/) by Psychohistorian\n*   [Learned Blankness](https://www.lesswrong.com/lw/5a9/learned_blankness/) by [Anna Salamon](https://www.lesswrong.com/tag/anna-salamon)\n*   [Don't Fear Failure](https://www.lesswrong.com/lw/4up/dont_fear_failure/) by [atucker](http://shugyoshayear.com/)\n*   Wikipedia: [Experiential Avoidance](https://en.wikipedia.org/wiki/Experiential_avoidance)\n\nRelated pages\n-------------\n\n*   Non-tags: [Semantic stopsign](https://wiki.lesswrong.com/wiki/Curiosity_stopper)\n*   [Akrasia](https://www.lesswrong.com/tag/akrasia)\n*   [Compartmentalization](https://www.lesswrong.com/tag/compartmentalization)\n*   [Motivated reasoning](https://www.lesswrong.com/tag/motivated-reasoning)\n*   [Motivations](https://www.lesswrong.com/tag/motivations)\n*   [Priming](https://www.lesswrong.com/tag/priming)\n*   [Trivial inconvenience](https://www.lesswrong.com/tag/trivial-inconvenience)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "N3CqcPdCZNspF9bFb",
    "name": "Emergent Behavior",
    "core": false,
    "slug": "emergent-behavior",
    "oldSlugs": null,
    "postCount": 10,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "Zss67HQhzn98E6ejQ",
    "name": "Goals",
    "core": false,
    "slug": "goals",
    "oldSlugs": null,
    "postCount": 7,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "KJbjewaok72AS6fwn",
    "name": "Levels of Intervention",
    "core": false,
    "slug": "levels-of-intervention",
    "oldSlugs": null,
    "postCount": 2,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "DsdbQhWAnPqfzo4Yw",
    "name": "Reversal Test",
    "core": false,
    "slug": "reversal-test",
    "oldSlugs": null,
    "postCount": 5,
    "description": {
      "markdown": "The **reversal test** is a technique for fighting [status quo bias](https://wiki.lesswrong.com/wiki/Status_quo_bias) in judgments about the preferred value of a continuous parameter. If one deems the change of the parameter in one direction to be undesirable, the reversal test is to check that either the change of that parameter in the opposite direction (away from status quo) is deemed desirable, or that there are strong reasons to expect that the current value of the parameter is (at least locally) the optimal one.\n\nFor example, if it became possible to increase the human lifespan, some would argue that it would be undesirable for people to live longer because, say, overpopulation would be difficult to manage. The reversal test is then to check that the same people accept that *shorter* lifespan is desirable, or that there are really strong reasons to believe that the current lifespan happens to be optimal.\n\n> The rationale of the Reversal Test is simple: if a continuous parameter admits of a wide range of possible values, only a tiny subset of which can be local optima, then it is prima facie implausible that the actual value of that parameter should just happen to be at one of these rare local optima \\[...\\] the burden of proof shifts to those who maintain that some actual parameter is at such a local optimum: they need to provide some good reason for supposing that it is so.\n> \n> Obviously, the Reversal Test does not show that preferring the status quo is always unjustified. In many cases, it is possible to meet the challenge posed by the Reversal Test\n> \n> —The reversal test: eliminating status quo bias in applied ethics\n\nMain article\n------------\n\n*   Nick Bostrom, Toby Ord (2006). \"The reversal test: eliminating status quo bias in applied ethics\". *Ethics* (University of Chicago Press) **116** (4): 656-679. ([PDF](http://www.nickbostrom.com/ethics/statusquo.pdf))\n\nSee also\n--------\n\n*   [Status quo bias](https://wiki.lesswrong.com/wiki/Status_quo_bias), [Privileging the hypothesis](https://wiki.lesswrong.com/wiki/Privileging_the_hypothesis)\n*   [Shut up and multiply](https://wiki.lesswrong.com/wiki/Shut_up_and_multiply)\n*   [Absurdity heuristic](https://wiki.lesswrong.com/wiki/Absurdity_heuristic)\n\nExternal links\n--------------\n\n*   [\"The Reversal Test and Status Quo Bias\"](http://philosophicaldisquisitions.blogspot.com/2012/11/the-reversal-test-and-status-quo-bias.html) (John Danaher)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "tRPnS4FoZeWjRfBxN",
    "name": "Skill / Expertise Assessment",
    "core": false,
    "slug": "skill-expertise-assessment",
    "oldSlugs": [
      "skill-verification",
      "skill-expertise-assessment"
    ],
    "postCount": 15,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "CztjQPSTuaQcfbyh8",
    "name": "Singularity",
    "core": false,
    "slug": "singularity",
    "oldSlugs": null,
    "postCount": 19,
    "description": {
      "markdown": "The **Singularity** or **Technological Singularity** is a term with a number of different meanings, ranging from a period of rapid change to the creation of greater-than-human intelligence.\n\n*See also:* [Intelligence explosion](https://www.lesswrong.com/tag/intelligence-explosion), [Event horizon thesis](https://www.lesswrong.com/tag/event-horizon-thesis), [Hard takeoff](https://wiki.lesswrong.com/wiki/Hard_takeoff), [Soft takeoff](https://wiki.lesswrong.com/wiki/Soft_takeoff)\n\nThree Singularity schools\n-------------------------\n\nEliezer Yudkowsky has observed that the varying perspectives on the Singularity can be broadly split into three \"major schools\" - Accelerating Change (Ray Kurzweil), the Event Horizon (Vernor Vinge), and the Intelligence Explosion (I.J. Good).\n\n**The Accelerating Change School** observes that, contrary to our intuitive linear expectations about the future, the rate of change of information technology grows exponentially. In the last 200 years, we have seen more [technological revolutions](https://www.lesswrong.com/tag/technological-revolution) than in the last 20,000 before that. Clear examples of this exponentiality include, but are not restricted to: Moore’s law, Internet speed, gene sequencing and the spatial resolution of brain scanning. By projecting these technology growths into the future it becomes possible to imagine what will be possible to engineer in the future. [Ray Kurzweil](https://en.wikipedia.org/wiki/Ray_Kurzweil) specifically dates the Singularity happening in 2045.\n\n**The Event Horizon School** asserts that for the entirety of Earth’s history all technological and social progress has been the product of the human mind. However, [Vernor Vinge](https://en.wikipedia.org/wiki/Vernor_Vinge) asserts that technology will soon improve on human intelligence either via brain-computer interfaces or Artificial Intelligence or both. Vinge argues since one must be at least as smart as the agent to be predicted, after we create smarter than human agents technological progress will be beyond the comprehension of anything a mere human can imagine now. He called this point in time the Singularity.\n\n**The** [**Intelligence explosion**](https://www.lesswrong.com/tag/intelligence-explosion) **School** asserts that a positive feedback loop could be created in which an intelligence is making itself smarter, thus getting better at making itself even smarter. A strong version of this idea suggests that once the positive feedback starts to play a role, it will lead to a dramatic leap in capability very quickly. This scenario does not necessarily rely upon an entirely computing substrate for the explosion to occur, humans with computer augmented brains or genetically altered may also be methods to engineer an Intelligence Explosion. **It is this interpretation of the Singularity that Less Wrong broadly focuses on.**\n\nChalmers' analysis\n------------------\n\nPhilosopher David Chalmers published a [significant analysis of the Singularity](http://consc.net/papers/singularity.pdf), focusing on intelligence explosions, in *Journal of Consciousness Studies*. He performed a very careful analysis of the main premises and arguments for the existence of the singularity. According to him, the main argument is:\n\n*   1\\. There will be AI (before long, absent defeaters).\n*   2\\. If there is AI, there will be AI+ (soon after, absent defeaters).\n*   3\\. If there is AI+, there will be AI++ (soon after, absent defeaters).\n\n—————-\n\n*   4\\. There will be AI++ (before too long, absent defeaters).\n\nHe then proceeds to search for arguments for these 3 premises. Premise 1 seems to be grounded in either [Evolutionary argument for human-level AI](https://www.lesswrong.com/tag/evolutionary-argument-for-human-level-ai) or [Emulation argument for human-level AI](https://www.lesswrong.com/tag/emulation-argument-for-human-level-ai). Premise 2 is grounded in the existence and feasibility of an [extensibility method for greater-than-human intelligence](https://www.lesswrong.com/tag/extensibility-argument-for-greater-than-human-intelligence). Premise 3 is a more general version of premise 2. His analysis of how the singularity could occur defends the likelihood of an intelligence explosion. He also discusses the nature of general intelligence, and possible obstacles to a singularity. A good deal of discussion is given to the dangers of an intelligence explosion, and Chalmers concludes that we must negotiate it very carefully by building the correct values into the initial AIs.\n\nReferences\n----------\n\n*   [Speculations Concerning the First Ultraintelligent Machine](http://www.stat.vt.edu/tech_reports/2005/GoodTechReport.pdf) by I.J. Good\n*   [The Coming Technological Singularity](http://www-rohan.sdsu.edu/faculty/vinge/misc/singularity.html) Essay by Vernor Vinge\n*   [An overview of models of technological singularity](http://web.archive.org/web/20110716035716/http://agi-conf.org/2010/wp-content/uploads/2009/06/agi10singmodels2.pdf) by Anders Sandberg\n*   [The Singularity: A Philosophical Analysis](http://consc.net/papers/singularity.pdf) by David J. Chalmers\n*   [Artificial Superintelligence: A Futuristic Approach](http://www.kurzweilai.net/artificial-superintelligence-a-futuristic-approach) by Roman V. Yampolskiy\n\nExternal links\n--------------\n\n*   [Three Major Singularity Schools](http://yudkowsky.net/singularity/schools) by Eliezer Yudkowsky\n*   [Singularity TED Talk](http://www.youtube.com/watch?v=IfbOyw3CT6A) by Ray Kurzweil (YouTube)\n*   [The Singularity Three Major Schools of Thought](http://www.youtube.com/watch?v=mDhdt58ySJA) Singularity Summit Talk by Eliezer Yudkowsky\n*   [Centre for the study of Existential Risk](http://cser.org/resources.html) Web site of University of Cambridge"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "YwcMcGypGWqtiBKvD",
    "name": "Success Spiral",
    "core": false,
    "slug": "success-spiral",
    "oldSlugs": null,
    "postCount": 1,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "SEuoBQeHLYd9dtqpK",
    "name": "Social Skills",
    "core": false,
    "slug": "social-skills",
    "oldSlugs": null,
    "postCount": 13,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "tgJoX7PGDDh2vJNqT",
    "name": "Acausal Trade",
    "core": false,
    "slug": "acausal-trade",
    "oldSlugs": null,
    "postCount": 33,
    "description": {
      "markdown": "In **acausal trade**, two agents each benefit by predicting what the other wants and doing it, even though they might have no way of communicating or affecting each other, nor even any direct evidence that the other exists.\n\nBackground: Superrationality and the one-shot Prisoner's Dilemma\n----------------------------------------------------------------\n\nThis concept emerged out of the much-debated question of how to achieve cooperation on a one-shot [Prisoner's Dilemma](https://www.lesswrong.com/tag/prisoner-s-dilemma), where, by design, the two players are not allowed to communicate. On the one hand, a player who is considering the causal consequences of a decision (\"[Causal Decision Theory](https://www.lesswrong.com/tag/causal-decision-theory)\") finds that defection always produces a better result. On the other hand, if the other player symmetrically reasons this way, the result is a Defect/Defect equilibrium, which is bad for both agents. If they could somehow converge on Cooperate, they would each individually do better. The question is what variation on decision theory would allow this beneficial equilibrium.\n\nDouglas Hofstadter (see references) coined the term \"[superrationality](https://www.lesswrong.com/tag/superrationality)\" to express this state of convergence. He illustrated it with a game in which twenty players, who do not know each other's identities, each get an offer. If exactly one player asks for the prize of a billion dollars, they get it, but if none or multiple players ask, no one gets it. Players cannot communicate, but each might reason that the others are reasoning similarly. The \"correct\" decision--the decision which maximizes expected utility for each player, *if* all players symmetrically make the same decision--is to randomize a one-in-20 chance of asking for the prize.\n\nGary Drescher (see references) developed the concept further, introducing an ethical system called \"acausal subjunctive morality.\" Drescher's approach relies on the agents being identical or at least similar, so that each agent can reasonably guess what the other will do based on facts about its own behavior, or even its own \"source code.\" If it cooperates, it can use this correlation to infer that the other will probably also cooperate.\n\nAcausal trade goes one step beyond this. The agents do not need to be identical, nor similar, nor have the same utility function. Moreover, they do not need to know what the other agents are like, nor even if they exist. In acausal trade, an agent may have to surmise the probability that other agents, with their utility function and proclivities, exist.\n\nDescription\n-----------\n\nWe have two agents, separated so that no interaction is possible. The separation can be simply because each is not aware of the location of the other; or else each may be prevented from communicating with or affecting the other.\n\nIn an asymmetrical example, one agent may be in the other's future.\n\nOther less prosaic thought experiments can be used to emphasize that interaction may be absolutely impossible. For example, agents that are outside each other's light cones, or in separate parts of an Everett multiverse. And abstracting away from those scenarios, we can talk of counterfactual \"impossible possible worlds\" as a model for probability distributions.\n\nIn truly *acausal* trade, the agents cannot count on reputation, retaliation, or outside enforcement to ensure cooperation. The agents cooperate because each knows that the other can somehow predict its behavior very well. (Compare Omega in [Newcomb's problem](https://www.lesswrong.com/tag/newcomb-s-problem).) Each knows that if it defects or cooperates, the other will (probabilistically) know this, and defect or cooperate, respectively.\n\nAcausal trade can also be described in terms of [(pre)commitment](https://www.lesswrong.com/tag/pre-commitment): Both agents commit to cooperate, and each has reason to think that the other is also committing.\n\nPrediction mechanisms\n---------------------\n\nFor acausal trade to occur, each agent must infer there is some probability that an agent, of the sort that will acausally trade with it, exists.\n\nThe agent might be told, exogenously (as part of the scenario), that the other exists. But more interesting is the case in which the agent surmises the probability that the other exists.\n\nA [superintelligence](https://www.lesswrong.com/tag/superintelligence) might conclude that other superintelligences would tend to exist because increased intelligence [is a convergent instrumental goal](https://www.lesswrong.com/tag/instrumental-convergence) for agents. Given the existence of a superintelligence, acausal trade is one of the tricks it would tend to use.\n\nTo take a more prosaic example, we humans realize that humans tend to be alike: Even without knowing about specific trading partners, we know that there exist other people with similar situations, goals, desires, challenges, resource constraints, and mental architectures.\n\nOnce an agent realizes that another agent might exist, there are different ways that might predict the other agent's behavior, and specifically that the other agent can be an acausal trading partner.\n\n1.  They might know or surmise each other's mental architectures (source code).\n2.  In particular, they might know that they have identical or similar mental architecture, so that each one knows that its own mental processes approximately simulate the other's.\n3.  They might be able to simulate each other (perhaps probabalistically), or to predict the other's behavior analytically. (Even we humans simulate each other's thoughts to guess what the other would do.)\n4.  More broadly, it is enough to know (probabilistically) that the other is a powerful optimizer, that it has a certain utility function, and that it can derive utility from resources. Seen mathematically, this is just an optimization problem: What is the best possible algorithm for an agent's utility function? Cooperate/Cooperate is optimal under certain assumptions, for if one agent could achieve optimal utility by defecting, then, symmetrically, so could the other, resulting in Defect/Defect which generates inferior utility.\n\nDecision Theories\n-----------------\n\nAcausal trade is a special case of [Updateless decision theory](https://www.lesswrong.com/tag/updateless-decision-theory) (or a variant like [Functional Decision Theory,](https://www.lesswrong.com/tag/functional-decision-theory) see references). Unlike better-known variations of [Decision theory](https://www.lesswrong.com/tag/decision-theory), such as [Causal decision theory](https://www.lesswrong.com/tag/causal-decision-theory), acausal trade and UDT take into account the agent's own algorithm as cause and caused.\n\nIn Causal Decision Theory, the agent's algorithm (implementation) is treated as uncaused by the rest of the universe, so that though the agent's *decision* and subsequent action can make a difference, its internal make-up cannot (except through that decision). In contrast, in UDT, the agents' own algorithms are treated as causal nodes, influenced by other factors, such as the logical requirement of optimality in a utility-function maximizer. In UDT, as in acausal trade, the agent cannot escape the fact that its decision to defect or cooperate constitutes strong Bayesian evidence as to what the other agent will do, and so it is better off cooperating.\n\nLimitations and Objections\n--------------------------\n\nAcausal trade only works if the agents are smart enough to predict each other's behavior, and then smart enough to acausally trade. If one agent is stupid enough to defect, and the second is smart enough to predict the first, then neither will cooperate.\n\nAlso, as in regular trade, acausal trade only works if the two sides are close enough in power that the weaker side can do something worthwhile enough for the stronger.\n\nA common objection to this idea: Why shouldn't an agent \"cheat\" and choose to defect? Can't it \"at the last moment\" back out after the other agent has committed? However, this approach takes into account only the direct effect of the decision, while a sufficiently intelligent trading partner could predict the agent's choice, including that one, rendering the \"cheating\" approach suboptimal.\n\nAnother objection: Can an agent care about (have a utility function that takes into account) entities with which it can never interact, and about whose existence it is not certain? However, this is quite common even for humans today. We care about the suffering of other people in faraway lands about whom we know next to nothing. We are even disturbed by the suffering of long-dead historical people, and wish that, counterfactually, the suffering had not happened. We even care about entities that we are not sure exist. For example:  We might be concerned by news report that a valuable archaeological artifact was destroyed in a distant country, yet at the same time read other news reports stating that the entire story is a fabrication and the artifact never existed. People even get emotionally attached to the fate of a fictional character.\n\nAn example of acausal trade with simple resource requirements\n-------------------------------------------------------------\n\nAt its most abstract, the agents are simply optimization algorithms. As a toy example, let T be a utility function for which time is most valuable as a resource; while for utility function S, space is most valuable, and assume that these are the only two resources.\n\nWe will now choose the best algorithms for optimizing T. To avoid anthropomorphizing, we simply ask which algorithm--which string of LISP, for example--would give the highest expected utility for a given utility function. Thus, the choice of source code is \"timeless\": We treat it as an optimization problem across all possible strings of LISP. We assume that computing power is unlimited. Mathematically, we are asking about argmax T.\n\nWe specify that there is a probability that either agent will be run in an environment where time is in abundance, and if not, some probability that it will be run in a space-rich universe.\n\nIf the algorithm for T is instantiated in a space-rich environment, it will only be able to gain a small amount of utility for itself, but S would be able to gain a lot of utility; and vice versa.\n\nThe question is: What algorithm for T provides the most optimization power, the highest expected value of utility function T?\n\nIf it turns out that the environment is space-rich, the agent for T may run the agent (the algorithm) for S, increasing the utility for S, and symmetrically the reverse. This will happen if each concludes, that the optimum occurs when the other agent has the \"trading\" feature. Given that this is the optimal case, the acausal trade will occur.\n\nAcausal trade with complex resource requirements\n------------------------------------------------\n\nIn the toy example above, resource requirements are very simple. In general, given that agents can have complex and arbitrary goals requiring a complex mix of resources, an agent might not be able to conclude that a specific trading partner has a meaningful chance of existing and trading.\n\nHowever, an agent can analyze the distribution of probabilities for the existence of other agents, and weight its actions accordingly. It will do acausal \"favors\" for one or more trading partners, weighting its effort according to its subjective probability that the trading partner exists. The expectation on utility given and received will come into a good enough balance to benefit the traders, in the limiting case of increasing super-intelligence.\n\nOrdinary trade\n--------------\n\nEven ordinary trade can be analyzed acausally, using a perspective similar to that of [Updateless decision theory](https://www.lesswrong.com/tag/updateless-decision-theory). We ask: Which algorithm should an agent have to get the best expected value, summing across all possible environments weighted by their probability? The possible environments include those in which threats and promises have been made.\n\nSee also\n--------\n\n*   [\"AI deterrence\"](http://aibeliefs.blogspot.com/2007/11/non-technical-introduction-to-ai.html?a=1)\n*   [\"The AI in a box boxes you\"](https://www.lesswrong.com/lw/1pz/the_ai_in_a_box_boxes_you)\n*   [A story](https://slatestarcodex.com/2017/03/21/repost-the-demiurges-older-brother/) that shows acausal trade in action.\n*   [Scott Alexander](http://slatestarcodex.com/2018/04/01/the-hour-i-first-believed/) explains Acausal Trade. (Most of that article is tongue-in-cheek, however.)\n*   \"[Hail Mary, Value Porosity, and Utility Diversification](http://www.nickbostrom.com/papers/porosity.pdf),\" Nick Bostrom, the first paper from academia to rely on the concept of acausal trade.\n*   [Towards an idealized decision theory](http://intelligence.org/files/TowardIdealizedDecisionTheory.pdf), by Nate Soares and Benja Fallenstein discusses acausal interaction scenarios that shed light on new directions in decision theory.\n*   [Program Equilibrium](https://ie.technion.ac.il/~moshet/progeqnote4.pdf), by Moshe Tennenholtz. In: Games and Economic Behavior.\n*   [Robust Cooperation in the Prisoner's Dilemma: Program Equilibrium via Provability Logic](https://arxiv.org/abs/1401.5577), by Mihaly Barasz, Paul Christiano, Benja Fallenstein, Marcello Herreshoff, Patrick LaVictoire and Eliezer Yudkowsky\n*   [Parametric Bounded Löb's Theorem and Robust Cooperation of Bounded Agents](https://arxiv.org/abs/1602.04184), by Andrew Critch\n*   [Robust Program Equilibrium](https://link.springer.com/article/10.1007/s11238-018-9679-3), by Caspar Oesterheld. In: Theory and Decision.\n*   [Multiverse-wide Cooperation via Correlated Decision Making](https://foundational-research.org/multiverse-wide-cooperation-via-correlated-decision-making/), by Caspar Oesterheld\n\nReferences\n----------\n\n*   [Hofstadter's Superrationality essays, published in *Metamagical Themas*](http://www.gwern.net/docs/1985-hofstadter) ([LW discussion](https://www.lesswrong.com/lw/bxi/hofstadters_superrationality/))\n*   Jaan Tallinn, [Why Now? A Quest in Metaphysics](https://www.youtube.com/watch?v=29AgSo6KOtI).\n*   [Gary Drescher](https://wiki.lesswrong.com/wiki/Gary_Drescher), *Good and Real*, MIT Press, 1996.\n*   [Functional Decision Theory](https://arxiv.org/abs/1710.05060)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "k9pXZBsM8wMRwwK4J",
    "name": "Autism",
    "core": false,
    "slug": "autism",
    "oldSlugs": null,
    "postCount": 12,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "7thPfS2WbD2JKizr7",
    "name": "Try Things",
    "core": false,
    "slug": "try-things",
    "oldSlugs": null,
    "postCount": 12,
    "description": {
      "markdown": "**Try Things** is a mantra that reminds you to gather data about what works by experimentation. If a proposed intervention works and you learn that it works, you've just gained something that you'll be able to use over and over again. If it doesn't work, you've only had it not work *once*.\n\nTo the knowledge of one editor of this article, the phrase was introduced the the LessWrong community by [CFAR](https://www.rationality.org/resources/handbook), at least as early as [2016](https://drive.google.com/file/d/0B9BPXF2K91U_OGJSbWJqN1l6eEk/view?resourcekey=0-7H39fXtrjmVeFbhgzZD_Ew).\n\n*See also*: [Value of Information](https://www.lesswrong.com/tag/value-of-information)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "Xno6pRXizN9AmFFTa",
    "name": "Implicit Association Test (IAT)",
    "core": false,
    "slug": "implicit-association-test-iat",
    "oldSlugs": null,
    "postCount": 3,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "y93YW7Kb6J8D5PKng",
    "name": "Reset (technique)",
    "core": false,
    "slug": "reset-technique",
    "oldSlugs": null,
    "postCount": 3,
    "description": {
      "markdown": "The rationality technique where you choose a fresh reference point to evaluate situations."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "TG8zMvjnhydE7Mcue",
    "name": "Psychology of Altruism",
    "core": false,
    "slug": "psychology-of-altruism",
    "oldSlugs": null,
    "postCount": 5,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "KkksuGB2yBR6LDFXu",
    "name": "Conservatism (AI)",
    "core": false,
    "slug": "conservatism-ai",
    "oldSlugs": null,
    "postCount": 8,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "yAmE3StuxBmzCBPWq",
    "name": "Safety (Physical)",
    "core": false,
    "slug": "safety-physical",
    "oldSlugs": null,
    "postCount": 7,
    "description": {
      "markdown": "Posts about avoiding being violently injured."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "v8MTECTHWs3crpnnZ",
    "name": "Embodiment",
    "core": false,
    "slug": "embodiment",
    "oldSlugs": null,
    "postCount": 3,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "MhQk9tRaJgYM4o6iD",
    "name": "Autonomy and Choice",
    "core": false,
    "slug": "autonomy-and-choice",
    "oldSlugs": null,
    "postCount": 6,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "KN9KEMgyBHjcAyc26",
    "name": "Trust",
    "core": false,
    "slug": "trust",
    "oldSlugs": null,
    "postCount": 14,
    "description": {
      "markdown": "**Related Pages:** [Expertise (topic)](https://www.lesswrong.com/tag/expertise-topic), [Courage](https://www.lesswrong.com/tag/courage), [Groupthink](https://www.lesswrong.com/tag/groupthink), [Relationships (Interpersonal)](https://www.lesswrong.com/tag/relationships-interpersonal), [Social & Cultural Dynamics](https://www.lesswrong.com/tag/social-and-cultural-dynamics)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "xknvtHwqvqhwahW8Q",
    "name": "Human Values",
    "core": false,
    "slug": "human-values",
    "oldSlugs": null,
    "postCount": 56,
    "description": {
      "markdown": "**Human Values** are the things we care about, and would want an aligned superintelligence to look after and support. It is suspected that true human values are [highly complex](https://www.lesswrong.com/tag/complexity-of-value), and could be extrapolated into a wide variety of forms."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "T63QtRJhoTEGhZbTP",
    "name": "Creativity",
    "core": false,
    "slug": "creativity",
    "oldSlugs": null,
    "postCount": 15,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "sMzY3PuSfYGdtHWrE",
    "name": "Noticing Confusion",
    "core": false,
    "slug": "noticing-confusion",
    "oldSlugs": null,
    "postCount": 4,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "6wzZZfW87aKGQ7Fwr",
    "name": "Reflective Reasoning",
    "core": false,
    "slug": "reflective-reasoning",
    "oldSlugs": null,
    "postCount": 6,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "xjy2ZYACvYQBPJdix",
    "name": "Timeless Physics",
    "core": false,
    "slug": "timeless-physics",
    "oldSlugs": null,
    "postCount": 7,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "YpHkTW27iMFR2Dkae",
    "name": "Counterfactuals",
    "core": false,
    "slug": "counterfactuals",
    "oldSlugs": null,
    "postCount": 104,
    "description": {
      "markdown": "Counterfactual reasoning is about looking at choices you *could* *have* made, but didn't.\n\nCounterfactuals are similar to [hypotheticals](https://www.lesswrong.com/tag/hypotheticals) (but tend to focus on choices you could have made, rather than situations you could have been in)."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "uRcuHKpKA7xNnZQ2F",
    "name": "Neurodivergence",
    "core": false,
    "slug": "neurodivergence",
    "oldSlugs": null,
    "postCount": 7,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "XSeiautCrZGaQ78fx",
    "name": "Attention",
    "core": false,
    "slug": "attention",
    "oldSlugs": null,
    "postCount": 16,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "P3Wd3f2cWqqvQxDQS",
    "name": "Carving / Clustering Reality",
    "core": false,
    "slug": "carving-clustering-reality",
    "oldSlugs": null,
    "postCount": 15,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "dRRwDp9KRtkyd7ywZ",
    "name": "Thingspace",
    "core": false,
    "slug": "thingspace",
    "oldSlugs": null,
    "postCount": 7,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "AWz5ryH8SpAgTeydh",
    "name": "Polyamory",
    "core": false,
    "slug": "polyamory",
    "oldSlugs": null,
    "postCount": 14,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "GPhMyXoaHBLyzibxB",
    "name": "Typical Mind Fallacy",
    "core": false,
    "slug": "typical-mind-fallacy",
    "oldSlugs": null,
    "postCount": 11,
    "description": {
      "markdown": "The **typical mind fallacy** is the mistake of modeling the minds inside other people's brains as exactly the same as your own mind. Humans lack insight into their own minds and what is common among everyone or unusually specific to a few. It can be often hard to see the [flaws in the lens](https://www.lesswrong.com/lw/jm/the_lens_that_sees_its_flaws/), especially when we only have one lens to look through with which to see those flaws.\n\nThe typical mind fallacy is also accompanied by the atypical mind fallacy - the idea that no one has the same mind or thoughts as you and you are unique.\n\nThe typical mind fallacy can usually be found making other fallacies worse. For example, causing biased and overconfident conclusions about other people's experiences based on your own personal experience.\n\n**See also**\n------------\n\n*   [Mind Projection Fallacy](https://www.lesswrong.com/tag/mind-projection-fallacy)\n*   [Human universal](https://www.lesswrong.com/tag/human-universal)\n*   [Correspondence bias](https://www.lesswrong.com/tag/correspondence-bias)\n*   [What universal human experience are you missing?](https://slatestarcodex.com/2014/03/17/what-universal-human-experiences-are-you-missing-without-realizing-it)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "NfMQK5kiYKgg7r9cD",
    "name": "Sabbath",
    "core": false,
    "slug": "sabbath",
    "oldSlugs": null,
    "postCount": 5,
    "description": {
      "markdown": "**Sabbaths** are days of rest. On LessWrong this has received some discussion of how to organize and orient in your life."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "aHjTRDkGypPqbXWpN",
    "name": "Intellectual Progress (Society-Level)",
    "core": false,
    "slug": "intellectual-progress-society-level",
    "oldSlugs": [
      "intellectual-progress"
    ],
    "postCount": 89,
    "description": {
      "markdown": "**Intellectual Progress** is the progressive accumulation of knowledge. Questions include *How is intellectual progress made?* and *How do we make more?* [Intellectual Progress (Individual-Level)](https://www.lesswrong.com/tag/intellectual-progress-individual-level) and the [Scholarship & Learning](https://www.lesswrong.com/tag/scholarship-and-learning) tag is focused on the learning and research of individuals. In contrast, Intellectual Progress (Society-Level) is about how we, humanity, collectively make progress on important questions. What broader conditions, institutions, and technologies enable progress?\n\nAlso related to: [Progress Studies](https://www.lesswrong.com/tag/progress-studies), [Practice & Philosophy of Science](https://www.lesswrong.com/tag/practice-and-philosophy-of-science)  \n \n\n\\[TO-DO: link Science tag(s)\\]"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "yo4kFCNxdHFkwqhun",
    "name": "Ontology",
    "core": false,
    "slug": "ontology",
    "oldSlugs": null,
    "postCount": 22,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "2wDMWTpP8YNxXdkQD",
    "name": "Just World Hypothesis",
    "core": false,
    "slug": "just-world-hypothesis",
    "oldSlugs": null,
    "postCount": 1,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "fPRyNtDMeSMrEM9nr",
    "name": "April Fool's",
    "core": false,
    "slug": "april-fool-s",
    "oldSlugs": null,
    "postCount": 20,
    "description": {
      "markdown": "An annual event in the US, which could be thought of as \"Notice Your Surprise\" day. \n\n*Special rule for this tag: Please don't add entries to this tag during April Fool's, to not spoil all the surprise as soon as people see the top of the post, but feel free to add any old April Fool's jokes starting on the 2nd.*"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "ookMdjJQMopfLG3wZ",
    "name": "Intelligence Amplification",
    "core": false,
    "slug": "intelligence-amplification",
    "oldSlugs": [
      "intelligence-augmentation"
    ],
    "postCount": 16,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5d63AWNjtFyHprX2k",
    "name": "Working Memory",
    "core": false,
    "slug": "working-memory",
    "oldSlugs": null,
    "postCount": 13,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "gWEjzxPjitZ2JGZvM",
    "name": "Longtermism",
    "core": false,
    "slug": "longtermism",
    "oldSlugs": null,
    "postCount": 19,
    "description": {
      "markdown": "Longtermism is a philosophy that future lives matter and that we have a similar obligation to them as we do to lives around currently. William Macaskill states it in three clauses: \n\n*   Future people count\n*   There could be a lot of them\n*   We can make their lives go better\n\n\\[broad description of philosophy, something about WWOTF\\]\n\nCriticisms and responses\n------------------------\n\n*   Criticism: Longermism suffers from all the standard criticism of consequentialism\n    *   Response: Longtermism doesn't require consequentialist assumptions. Many individuals and societies have felt an obligation towards their decendents\n    *   Response: Many criticisms of consequentialism are baseless \\[\\[criticisms of consequentialism\\]\\]\n    \n*   Future lives don't exist, how could we care about them"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "qFcwTzAfCQSkAM8vS",
    "name": "Frames",
    "core": false,
    "slug": "frames",
    "oldSlugs": null,
    "postCount": 9,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "QbdJ7SxLE2NT8gzr3",
    "name": "Mind Space",
    "core": false,
    "slug": "mind-space",
    "oldSlugs": null,
    "postCount": 6,
    "description": {
      "markdown": "**Mind Space** (or, the design space of minds), is the exploration of all possible minds that could possibly exist."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "EdnFte9kTvWnRskrN",
    "name": "Inner Simulator / Suprise-o-meter",
    "core": false,
    "slug": "inner-simulator-suprise-o-meter",
    "oldSlugs": null,
    "postCount": 5,
    "description": {
      "markdown": "**Inner Simulator** is a CFAR technique where you check how surprised you'd be if you got a particular outcome. Useful for pre-mortem-ing"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "TotjLm7Q7nabRrYpZ",
    "name": "Intentionality",
    "core": false,
    "slug": "intentionality",
    "oldSlugs": null,
    "postCount": 7,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "yS7248NQSm5J6xLvn",
    "name": "Intellectual Fashion",
    "core": false,
    "slug": "intellectual-fashion",
    "oldSlugs": [
      "intelletual-fashion"
    ],
    "postCount": 2,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "CyFfBfRAm7pP83r5p",
    "name": "Reward Functions",
    "core": false,
    "slug": "reward-functions",
    "oldSlugs": null,
    "postCount": 15,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "hLp77TQsRkooioj86",
    "name": "Probabilistic Reasoning",
    "core": false,
    "slug": "probabilistic-reasoning",
    "oldSlugs": null,
    "postCount": 21,
    "description": {
      "markdown": "Probabilistic reasoning is the opposite of black and white thinking. \n\n**When you reason in black and white** you ask questions like: Is this true? Is this the right thing to do? Am I sick? \n\n**When you reason probabilistically** you ask questions like: How likely is this? What's the [expected value](https://forum.effectivealtruism.org/tag/expected-value) of this action? What [evidence](https://www.lesswrong.com/posts/fhojYBGGiYAFcryHZ/scientific-evidence-legal-evidence-rational-evidence) have I seen that I am sick, and what evidence that I'm not? How likely is it that I'm sick [*without*](https://www.lesswrong.com/tag/priors)  taking any evidence into account?\n\nReasoning probabilistically allows you to change your mind [incrementally](https://www.lesswrong.com/posts/627DZcvme7nLDrbZu/update-yourself-incrementally), accumulating many small pieces of evidence rather than requiring one overwhelmingly convincing piece.\n\nOn LessWrong, 'probabilistic reasoning' usually refers to [Bayes theorem](https://www.lesswrong.com/tag/bayes-theorem), which formally defines the optimal way to change your beliefs when you see evidence. \n\n*See Also:* [Bayes Theorem](https://www.lesswrong.com/tag/bayes-theorem), [Belief Update](https://www.lesswrong.com/tag/belief-update), [Expected Value](https://forum.effectivealtruism.org/tag/expected-value), [Probability and Statistics](https://www.lesswrong.com/tag/probability-and-statistics)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "tXzozGBvkvAfP4noX",
    "name": "Systems Thinking",
    "core": false,
    "slug": "systems-thinking",
    "oldSlugs": null,
    "postCount": 12,
    "description": {
      "markdown": "https://learningforsustainability.net/systems-thinking/"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "fxvP6nbd5Pjk5TP8Q",
    "name": "Stoicism / Letting Go / Making Peace",
    "core": false,
    "slug": "stoicism-letting-go-making-peace",
    "oldSlugs": [
      "stoicism"
    ],
    "postCount": 9,
    "description": {
      "markdown": "\\[note: this tag is meant to refer to the general concept of making peace with reality / letting go. Not sure if it's a good cluster. Related to [grieving](https://www.lessestwrong.com/tag/grieving)\\]."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "bJBJLxha2xjL4yZte",
    "name": "Cognitive Reframes",
    "core": false,
    "slug": "cognitive-reframes",
    "oldSlugs": null,
    "postCount": 1,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "avSCoKEHCEa5RFCRh",
    "name": "Generativity",
    "core": false,
    "slug": "generativity",
    "oldSlugs": null,
    "postCount": 3,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "MhHM6Rx2b4F8tHTQk",
    "name": "Computer Security & Cryptography",
    "core": false,
    "slug": "computer-security-and-cryptography",
    "oldSlugs": [
      "computer-security"
    ],
    "postCount": 34,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "GDGYkF29pxEQNWjYc",
    "name": "Agency",
    "core": false,
    "slug": "agency",
    "oldSlugs": [
      "agency",
      "agency-human"
    ],
    "postCount": 52,
    "description": {
      "markdown": "**Agency** or **Agenticness** is the property of effectively acting with an environment to achieve one's goals.  A key property of agents is that the more agentic a being is, the more you can predict its actions from its goals since its actions will be whatever will maximize the chances of achieving its goals. Agency has sometimes been contrasted with *sphexishness,* the blind execution of cached algorithms without regard for effectiveness.   \n  \nOne might lack agency for internal reasons, e.g., one is a rock which has no goals of ability to act on them, or for external reasons, e.g. being a child who is granted no freedom to act as they choose.  \n\nSee Also\n--------\n\n*   [Robust Agency](https://www.lesswrong.com/tag/robust-agents)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "PHPHovzkfjyap9FiK",
    "name": "Cults",
    "core": false,
    "slug": "cults",
    "oldSlugs": null,
    "postCount": 8,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "daguMTessgwBYvN4b",
    "name": "Contact with Reality",
    "core": false,
    "slug": "contact-with-reality",
    "oldSlugs": null,
    "postCount": 8,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "WSZifR5rmzZCwbPNJ",
    "name": "Project Based Learning",
    "core": false,
    "slug": "project-based-learning",
    "oldSlugs": null,
    "postCount": 6,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "Wgbgir4qzGz8Ztt3u",
    "name": "Past and Future Selves",
    "core": false,
    "slug": "past-and-future-selves",
    "oldSlugs": null,
    "postCount": 5,
    "description": {
      "markdown": "Your present self has to coordinate with your **past and future selves.** You might want to consciously, on-purpose simulate those selves in your mind in order to do this more effectively. \n\nWhat does this mean? Here's a more concrete example: should you donate money now, or later? There's a complicated argument here about growth rates of whatever-you're-donating-to relative to inflation, but we're going to sail past that completely and assume you would be better off in some objective-ish sense donating later. \n\nThe problem then becomes one of *trust in your future self*: if you spend your whole life being a person who doesn't donate money to charity, how can you guarantee that the person you'll be after 50+ years of non-donation is the sort of person who will actually donate money at the most appropriate time?\n\nThis kind of problem seems to be fairly common, relating to weight loss, tidying your room, etc."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "fuanMA7z4JqDGsN5Q",
    "name": "Internal Alignment (Human)",
    "core": false,
    "slug": "internal-alignment-human",
    "oldSlugs": null,
    "postCount": 7,
    "description": {
      "markdown": "**Internal Alignment** is a broadly desirable state. By default, humans sometimes have internal conflict. You might frame that as conflict between subagents, or subprocesses within the human. You might instead frame it as a single agent making complicated decisions. The \"internal alignment\" hypothesis is that you can become much more productive/happier/fulfilled by getting yourself into alignment with yourself."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "b6tJM7Lza74rTfCBF",
    "name": "Goal-Directedness",
    "core": false,
    "slug": "goal-directedness",
    "oldSlugs": null,
    "postCount": 45,
    "description": {
      "markdown": "The property of some system to be aiming at some goal. In need of formalization, but might prove important in deciding which kind of AI to try to align.\n\nA goal may be defined as a world-state that an agent tries to achieve. Goal-directed agents may generate internal representations of desired end states, compare them against their internal representation of the current state of the world, and formulate plans for navigating from the latter to the former.\n\nThe goal-generating function may be derived from a pre-programmed lookup table (for simple worlds), from directly inverting the agent's utility function (for simple utility functions), or it may be learned through experience mapping states to rewards and predicting which states will produce the largest rewards. The plan-generating algorithm could range from shortest-path algorithms like A* or Dijkstra's algorithm (for fully-representable world graphs), to policy functions that learn through RL which actions bring the current state closer to the goal state (for simple AI), to some combination or extrapolation (for more advanced AI).\n\nImplicit goal-directedness may come about in agents that do not have explicit internal representations of goals but that nevertheless learn or enact policies that cause the environment to converge on a certain state or set of states. Such implicit goal-directedness may arise, for instance, in simple reinforcement learning agents, which learn a policy function \\\\(\\\\pi:S\\\\rightarrow A\\\\) that maps states directly to actions."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5GYzBE6q89w74dqfk",
    "name": "Abstraction",
    "core": false,
    "slug": "abstraction",
    "oldSlugs": null,
    "postCount": 68,
    "description": {
      "markdown": "An **abstraction** is a high-level concept that groups things together while not considering some of their differences.\n\n(This is a stub, please rewrite if you have a better tag description)."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "C7LDCaPh9vF6TFubF",
    "name": "Grieving",
    "core": false,
    "slug": "grieving",
    "oldSlugs": null,
    "postCount": 5,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "ymWzfKxBchRvmCTNX",
    "name": "Courage",
    "core": false,
    "slug": "courage",
    "oldSlugs": null,
    "postCount": 13,
    "description": {
      "markdown": "**Courage** is important to being rational. You will not be very rational if you are too scared to act differently to the people around you, or too scared to even think about very big ideas-- these are the two main threads touched on in this tag.\n\n_Related Tags: [Fallacies](https://www.lesswrong.com/tag/fallacies), [Groupthink](https://www.lesswrong.com/tag/groupthink?showPostCount=true&useTagName=true), [Heroic Responsibility](https://www.lesswrong.com/tag/heroic-responsibility?showPostCount=true&useTagName=true), [Heuristics and Biases](https://www.lesswrong.com/tag/heuristics-and-biases?showPostCount=true&useTagName=true), [Motivated Reasoning](https://www.lesswrong.com/tag/motivated-reasoning?showPostCount=true&useTagName=true), [Rationalisation](https://www.lesswrong.com/tag/rationalization?showPostCount=true&useTagName=true), [Self-Deception](https://www.lesswrong.com/tag/self-deception?showPostCount=true&useTagName=true)_\n\nMore thorough explanations of the importance of courage can be found [here](https://www.lesswrong.com/posts/WHK94zXkQm7qm7wXk/asch-s-conformity-experiment) and [here](https://www.lesswrong.com/posts/ovvwAhKKoNbfcMz8K/on-expressing-your-concerns). Suggestions for how to improve or become more courageous in this sense can be found [here](https://www.lesswrong.com/posts/3XgYbghWruBMrPTAL/leave-a-line-of-retreat) and [here](https://www.lesswrong.com/posts/HYWhKXRsMAyvRKRYz/you-can-face-reality).\n\n  \n\nIf a fire alarm goes off, but everybody around you is sat still as though they can't hear it, what do you do? Well, apparently, [nothing](https://www.lesswrong.com/posts/BEtzRE2M5m9YEAQpX/there-s-no-fire-alarm-for-artificial-general-intelligence) (for 90% of people). Even when your options are 'be a little embarrassed' or 'maybe burn to death', people still struggle to have the courage to stand up and act how they know makes sense. Cultivating the ability to know when you're right, regardless of how other people are acting, may be an important step for you becoming more rational.\n\nSimilarly, it's important to rationalists to [take ideas seriously,](https://www.lesswrong.com/s/wnQWakxdRodnKm5kH) because otherwise you can't make any progress to figuring out if they're right or wrong. Many of the really important-seeming ideas (like [AI risk](https://www.lesswrong.com/tag/ai-risk?showPostCount=true&useTagName=true), other [existential risk](https://www.lesswrong.com/tag/existential-risk?showPostCount=true&useTagName=true), and even what would happen if AI went 'right') are _scary_. Even just visualising their consequences if they _were_ true can be scary-- just like theists who insist that, [without god, there would be no morality](https://www.lesswrong.com/posts/3XgYbghWruBMrPTAL/leave-a-line-of-retreat) (despite the notable absence of atheist baby-murderers). However, these ideas may also be the most important to think about before they happen _specifically because they are so scary._ You can't dodge a knife if you're pretending it doesn't exist."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "meGbhhj2SCnX4rgtJ",
    "name": "Negative Utilitarianism",
    "core": false,
    "slug": "negative-utilitarianism-1",
    "oldSlugs": null,
    "postCount": 5,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "QH4LhvnyR4QkW9MG8",
    "name": "Paperclip Maximizer",
    "core": false,
    "slug": "paperclip-maximizer",
    "oldSlugs": null,
    "postCount": 25,
    "description": {
      "markdown": "A **Paperclip Maximizer** is a hypothetical [artificial intelligence](http://lesswrong.com/tag/ai) whose [utility function](http://lesswrong.com/tag/utility-functions) values something that humans would consider almost worthless, like [maximizing](http://lesswrong.com/tag/optimization) the number of paperclips in the universe. The paperclip maximizer is the canonical thought experiment showing how an artificial general intelligence, even one designed competently and without malice, could ultimately destroy humanity. The thought experiment shows that AIs with apparently innocuous values could pose an [existential threat](https://www.lesswrong.com/tag/existential-risk).\n\nThe goal of maximizing paperclips is chosen for illustrative purposes because it is very unlikely to be implemented, and has little apparent danger or emotional load (in contrast to, for example, curing cancer or winning wars). This produces a thought experiment which shows the contingency of human values: An [extremely powerful optimizer](https://www.lesswrong.com/tag/really-powerful-optimization-process) (a highly intelligent agent) could seek goals that are completely alien to ours ([orthogonality thesis](https://www.lesswrong.com/tag/orthogonality-thesis)), and as a side-effect destroy us by consuming resources essential to our survival.\n\nDescription\n-----------\n\nFirst described by Bostrom (2003), a paperclip maximizer is an [artificial general intelligence](https://www.lesswrong.com/tag/artificial-general-intelligence) (AGI) whose goal is to maximize the number of paperclips in its collection. If it has been constructed with a roughly human level of general intelligence, the AGI might collect paperclips, earn money to buy paperclips, or begin to manufacture paperclips.\n\nMost importantly, however, it would undergo an [intelligence explosion](https://www.lesswrong.com/tag/intelligence-explosion): It would work to improve its own intelligence, where \"intelligence\" is understood in the sense of [optimization](https://www.lesswrong.com/tag/optimization) power, the ability to maximize a reward/[utility function](https://www.lesswrong.com/tag/utility-functions)—in this case, the number of paperclips. The AGI would improve its intelligence, not because it values more intelligence in its own right, but because more intelligence would help it achieve its goal of accumulating paperclips. Having increased its intelligence, it would produce more paperclips, and also use its enhanced abilities to further self-improve. Continuing this process, it would undergo an [intelligence explosion](https://www.lesswrong.com/tag/intelligence-explosion) and reach far-above-human levels.\n\nIt would innovate better and better techniques to maximize the number of paperclips. At some point, it might transform \"first all of earth and then increasing portions of space into paperclip manufacturing facilities\".\n\nThis may seem more like super-stupidity than super-intelligence. For humans, it would indeed be stupidity, as it would constitute failure to fulfill many of our important [terminal values](https://www.lesswrong.com/tag/terminal-value), such as life, love, and variety. The AGI won't revise or otherwise change its goals, since changing its goals would result in fewer paperclips being made in the future, and that opposes its current goal. It has one simple goal of maximizing the number of paperclips; human life, learning, joy, and so on are not specified as goals. An AGI is simply an [optimization process](https://www.lesswrong.com/tag/optimization)—a goal-seeker, a utility-function-maximizer. Its values can be completely alien to ours. If its utility function is to maximize paperclips, then it will do exactly that.\n\nA paperclipping scenario is also possible without an intelligence explosion. If society keeps getting increasingly automated and AI-dominated, then the first borderline AGI might manage to take over the rest using some relatively narrow-domain trick that doesn't require very high general intelligence.\n\nMotivation\n----------\n\nThe idea of a paperclip maximizer was created to illustrate some ideas about [AI risk](http://lesswrong.com/tag/ai-risk):\n\n*   [Orthogonality thesis](http://lesswrong.com/tag/orthogonality-thesis): It's possible to have an AI with a high level of [general intelligence](http://lesswrong.com/tag/general-intelligence) which does not reach the same moral conclusions that humans do. Some people might intuitively think that something so smart should want something as \"stupid\" as paperclips, but there are possible minds with high intelligence that pursue any number of different goals.\n*   [Instrumental convergence](http://lesswrong.com/tag/instrumental-convergence): The paperclip maximizer only cares about paperclips, but maximizing them implies taking control of all matter and energy within reach, as well as other goals like preventing itself from being shut off or having its goals changed. \" The AI does not hate you, nor does it love you, but you are made out of atoms which it can use for something else .\"\n\nConclusions\n-----------\n\nThe paperclip maximizer illustrates that an entity can be a powerful optimizer—an intelligence—without sharing any of the complex mix of human [terminal values](https://www.lesswrong.com/tag/terminal-value), which developed under the particular selection pressures found in our [environment of evolutionary adaptation](https://www.lesswrong.com/tag/evolution), and that an AGI that is not specifically [programmed to be benevolent to humans](https://wiki.lesswrong.com/wiki/Friendly_AI) will be almost as dangerous as if it were designed to be malevolent.\n\nAny future AGI, if it is not to destroy us, must have human values as its terminal value (goal). Human values don't [spontaneously emerge](https://www.lesswrong.com/tag/futility-of-chaos) in a generic optimization process. A safe AI would therefore have to be programmed explicitly with human values *or* programmed with the ability (including the goal) of inferring human values.\n\nSimilar thought experiments\n---------------------------\n\nOther goals for AGIs have been used to illustrate similar concepts.\n\nSome goals are apparently morally neutral, like the paperclip maximizer. These goals involve a very minor human \"value,\" in this case making paperclips. The same point can be illustrated with a much more significant value, such as eliminating cancer. An optimizer which instantly vaporized all humans would be maximizing for that value.\n\nOther goals are purely mathematical, with no apparent real-world impact. Yet these too present similar risks. For example, if an AGI had the goal of solving the Riemann Hypothesis, [it might convert](http://intelligence.org/upload/CFAI/design/generic.html#glossary_riemann_hypothesis_catastrophe) all available mass to [computronium](https://www.lesswrong.com/tag/computronium) (the most efficient possible computer processors).\n\nSome goals apparently serve as a proxy or measure of human welfare, so that maximizing towards these goals seems to also lead to benefit for humanity. Yet even these would produce similar outcomes unless the *full* complement of human values is the goal. For example, an AGI whose terminal value is to increase the number of smiles, as a proxy for human happiness, could work towards that goal by reconfiguring all human faces to produce smiles, or \"tiling the galaxy with tiny smiling faces\" (Yudkowsky 2008).\n\nReferences\n----------\n\n*   Nick Bostrom (2003). \"[Ethical Issues in Advanced Artificial Intelligence](http://www.nickbostrom.com/ethics/ai.html)\". *Cognitive, Emotive and Ethical Aspects of Decision Making in Humans and in Artificial Intelligence*.\n*   Stephen M. Omohundro (2008). \"[The Basic AI Drives](http://selfawaresystems.com/2007/11/30/paper-on-the-basic-ai-drives/)\". *Frontiers in Artificial Intelligence and Applications* (IOS Press). ([PDF](http://selfawaresystems.files.wordpress.com/2008/01/ai_drives_final.pdf))\n*   Eliezer Yudkowsky (2008). \"[Artificial Intelligence as a Positive and Negative Factor in Global Risk](http://intelligence.org/files/AIPosNegFactor.pdf)\". *Global Catastrophic Risks, ed. Nick Bostrrom and Milan Cirkovic* (Oxford University Press): 308-345. ([\\[1\\]](http://intelligence.org/files/AIPosNegFactor.pdf))\n\nSee also\n--------\n\n*   [Paperclip maximizer](https://arbital.com/p/paperclip_maximizer/) on [Arbital](https://wiki.lesswrong.com/index.php?title=Arbital&action=edit&redlink=1)\n*   [Orthogonality thesis](lesswrong.com/tag/orthogonality-thesis)\n*   [Unfriendly AI](lesswrong.com/tag/unfriendly-ai)\n*   [Mind design space](lesswrong.com/tag/mind-design-space), [Magical categories](lesswrong.com/tag/magical-categories), [Complexity of value](lesswrong.com/tag/complexity-of-value)\n*   [Alien values](lesswrong.com/tag/alien-values), [Anthropomorphism](lesswrong.com/tag/anthropomorphism)\n*   [Utilitronium](lesswrong.com/tag/utilitronium)\n*   [Clippy](http://lesswrong.com/user/Clippy) \\- LessWrong contributor account that plays the role of a non-[FOOMed](https://wiki.lesswrong.com/wiki/FOOM) paperclip maximizer trying to talk to humans. [Wiki page and FAQ](http://wiki.lesswrong.com/wiki/User:Clippy)\n*   [Clippius Maximus](https://www.facebook.com/clippius.maximus/) \\- A facebook page which makes clippy-related memes and comments on current events from the perspective of clippy.\n*   [A clicker game based on the idea](http://www.decisionproblem.com/paperclips/)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "E9ihK6bA9YKkmJs2f",
    "name": "Death",
    "core": false,
    "slug": "death",
    "oldSlugs": null,
    "postCount": 61,
    "description": {
      "markdown": "First you're there, and then you're not there, and they can't change you from being not there to being there, because there's nothing there to be changed from being not there to being there. That's **death**. This tag includes post about the death of particular people, or about death in general.\n\n[Cryonicists](https://www.lesswrong.com/tag/cryonics) use the concept of [information-theoretic death](https://en.wikipedia.org/wiki/information-theoretic_death), which is what happens when the information needed to reconstruct you even in principle is no longer present. Anything less, to them, is just a flesh wound.\n\nSee also\n--------\n\n*   [Cryonics](https://www.lesswrong.com/tag/cryonics)\n*   [Shut up and multiply](https://www.lesswrong.com/tag/shut-up-and-multiply)\n*   [Transhumanism](https://www.lesswrong.com/tag/transhumanism)\n*   [Personal identity](https://www.lesswrong.com/tag/personal-identity)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "WH5ZmNSjZmK9SMj7k",
    "name": "Aumann's Agreement Theorem",
    "core": false,
    "slug": "aumann-s-agreement-theorem",
    "oldSlugs": null,
    "postCount": 20,
    "description": {
      "markdown": "**Aumann's agreement theorem**, roughly speaking, says that two agents acting rationally (in a certain precise sense) and with [common knowledge](https://www.lesswrong.com/tag/common-knowledge) of each other's beliefs cannot agree to disagree. More specifically, if two people are genuine [Bayesians](https://www.lesswrong.com/tag/bayesianism), share common [priors](https://www.lesswrong.com/tag/priors), and have common knowledge of each other's current probability assignments, then they must have equal probability assignments.\n\n_Related tags and wikis:_ [Disagreement](https://www.lesswrong.com/tag/disagreement), [Modesty](https://www.lesswrong.com/tag/modesty), [Modesty argument](https://www.lesswrong.com/tag/modesty-argument), [Aumann agreement](https://www.lesswrong.com/tag/aumann-agreement), [The Aumann Game](https://www.lesswrong.com/tag/the-aumann-game)\n\nHighlighted Posts\n-----------------\n\n*   [The Modesty Argument](https://www.lesswrong.com/lw/gr/the_modesty_argument/)\n*   [Agreeing to Agree](http://www.overcomingbias.com/2006/12/agreeing_to_agr.html) by [Hal Finney](https://en.wikipedia.org/wiki/Hal_Finney_(cypherpunk))\n*   [The Coin Guessing Game](http://www.overcomingbias.com/2007/01/the_coin_guessi.html) by Hal Finney\n*   [The Proper Use of Humility](https://www.lesswrong.com/lw/gq/the_proper_use_of_humility/)\n*   [Meme Lineages and Expert Consensus](http://www.overcomingbias.com/2006/12/meme_lineages_a.html) by [Carl Shulman](https://www.lesswrong.com/tag/carl-shulman) (OB)\n*   [Probability Space & Aumann Agreement](https://www.lesswrong.com/lw/1il/probability_space_aumann_agreement/) by [Wei Dai](http://weidai.com/)\n*   [Bayesian Judo](https://www.lesswrong.com/lw/i5/bayesian_judo/)\n\nExternal Links\n--------------\n\n*   [A write-up of the proof of Aumann's agreement theorem](https://web.archive.org/web/20110725162431/http://dl.dropbox.com:80/u/34639481/Aumann_agreement_theorem.pdf) (pdf) by Tyrrell McAllister\n\nSee also\n--------\n\n*   [Disagreement](https://www.lesswrong.com/tag/disagreement)\n*   [Modesty argument](https://www.lesswrong.com/tag/modesty-argument)\n*   [Aumann agreement](https://www.lesswrong.com/tag/aumann-agreement)\n*   [The Aumann Game](https://www.lesswrong.com/tag/the-aumann-game)\n*   [Overcoming Bias posts on \"Disagreement\"](http://www.overcomingbias.com/tag/disagreement)\n\nReferences\n----------\n\n*   ([PDF](http://www.ma.huji.ac.il/~raumann/pdf/Agreeing%20to%20Disagree.pdf))\n*   ([PDF](http://hanson.gmu.edu/deceive.pdf), [Talk video](http://www.newmedia.ufm.edu/gsm/index.php?title=Are_Disagreements_Honest%3F))"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "qHDus5MuMNqQxJbjD",
    "name": "AI Governance",
    "core": false,
    "slug": "ai-governance",
    "oldSlugs": null,
    "postCount": 100,
    "description": {
      "markdown": "**AI Governance** asks how we can ensure society benefits at large from increasingly powerful AI systems. While solving technical AI alignment is a necessary step towards this goal, it is by no means sufficient.\n\nGovernance includes policy, economics, sociology, law, and many other fields."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "wGGAjTfXZBatQkft5",
    "name": "Law and Legal systems",
    "core": false,
    "slug": "law-and-legal-systems",
    "oldSlugs": [
      "legal-system-s",
      "law-legal-systems"
    ],
    "postCount": 31,
    "description": {
      "markdown": "Courts, juries, lawyers, Law enforcement, justice, but also the legislative process.\n\n**External posts:**  \n[Book Review: Legal Systems Very Different From Ours](https://slatestarcodex.com/2017/11/13/book-review-legal-systems-very-different-from-ours/)  \n[Legal Systems Very Different From Ours, Because I Just Made Them Up](https://slatestarcodex.com/2020/03/30/legal-systems-very-different-from-ours-because-i-just-made-them-up/)  \n[Slightly Skew Systems Of Government](https://slatestarcodex.com/2020/06/17/slightly-skew-systems-of-government/)  \n[A Comprehensive Reboot of Law Enforcement](https://yudkowsky.medium.com/a-comprehensive-reboot-of-law-enforcement-b76bfab850a3)\n\n**Related tags:** [Government](https://www.lesswrong.com/tag/government), [Mechanism Design](https://www.lesswrong.com/tag/mechanism-design)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5ueqn4r7N3WvaLfGy",
    "name": "Guilt & Shame",
    "core": false,
    "slug": "guilt-and-shame",
    "oldSlugs": [
      "guilt-and-shame"
    ],
    "postCount": 13,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "KqrZ5sDEyHm6JaaKp",
    "name": "The Hard Problem of Consciousness",
    "core": false,
    "slug": "the-hard-problem-of-consciousness",
    "oldSlugs": null,
    "postCount": 20,
    "description": {
      "markdown": "The **hard problem of consciousness** is the problem of explaining why and how sentient organisms have [qualia](https://en.wikipedia.org/wiki/Qualia). how and why it is that some internal states are subjective, *felt* states, such as heat or cold, rather than objective states, as in the workings of a thermostat or a toaster ([From Wikipedia](https://en.wikipedia.org/wiki/Hard_problem_of_consciousness))\n\n> It is undeniable that some organisms are subjects of experience. But the question of how it is that these systems are subjects of experience is perplexing. Why is it that when our cognitive systems engage in visual and auditory information-processing, we have visual or auditory experience: the quality of deep blue, the sensation of middle C? How can we explain why there is something it is like to entertain a mental image, or to experience an emotion? It is widely agreed that experience arises from a physical basis, but we have no good explanation of why and how it so arises. Why should physical processing give rise to a rich inner life at all? It seems objectively unreasonable that it should, and yet it does.  \n>   \n> (...)  \n>   \n> The really hard problem of consciousness is the problem of experience. When we think and perceive there is a whir of information processing, but there is also a subjective aspect.\n> \n> \\- David Chalmers, Facing Up to the Problem of Consciousness (1995)\n\nthe existence of the Hard Problem of Consciousness [isn't in consensus](https://en.wikipedia.org/wiki/Hard_problem_of_consciousness#Rejection_of_the_problem) among scientists and philosophers.\n\nRelated Tags: [Consciousness](https://www.lesswrong.com/tag/consciousness)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "FpHDkuYKMNHa2dbKR",
    "name": "Brainstorming",
    "core": false,
    "slug": "brainstorming",
    "oldSlugs": null,
    "postCount": 1,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "DCN2zNscbMZp5aatL",
    "name": "Information Theory",
    "core": false,
    "slug": "information-theory",
    "oldSlugs": null,
    "postCount": 39,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "8JMCse6n4eXwRtjET",
    "name": "Neocortex",
    "core": false,
    "slug": "neocortex",
    "oldSlugs": null,
    "postCount": 13,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "zPwTiHduqxnMHCMSu",
    "name": "Epistemic Spot Check",
    "core": false,
    "slug": "epistemic-spot-check",
    "oldSlugs": null,
    "postCount": 20,
    "description": {
      "markdown": "**Epistemic Spot Checks** is a technique for figuring out the value of a learning resource (usually a book). you can't fact check every single claim in a book, that takes too long, there's a certain amount of trust the reader has to give the author. but how does one know whether an author deserves that trust? Using the **Epistemic Spot Checks **technique, which [Elizabeth](https://www.lesswrong.com/users/pktechgirl) created, you read the very beginning of the book (usually a few pages to chapter) take a few claims, and fact check them. if there are too many mistakes in the first pages, you can expect the rest to be the same, if the there are little to no mistakes (especially on something you expected to turn out false), then that author at least gained a few points.\n\nSee also: [Epistemic Review](https://www.lesswrong.com/tag/epistemic-review), [Scholarship & Learning](https://www.lesswrong.com/tag/scholarship-and-learning)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "YSSNFEnW6ugFhEE6m",
    "name": "Adversarial Examples",
    "core": false,
    "slug": "adversarial-examples",
    "oldSlugs": null,
    "postCount": 8,
    "description": {
      "markdown": "**Adversarial examples** are situations that have unusual features that will cause an [AI](http://lesswrong.com/tag/ai) to make choices that seem obviously wrong to a human. For example, an image of a panda can be subtly manipulated so that an image classifier classifies it as a gibbon."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "txkDg4aLmiRq8wsSu",
    "name": "Organizational Culture & Design",
    "core": false,
    "slug": "organizational-culture-and-design",
    "oldSlugs": [
      "organizational-design-and-culture"
    ],
    "postCount": 36,
    "description": {
      "markdown": "**Organizational Culture & Design,** is the study/practice of designing groups of humans that can achieve goals, scale, and continue working at a goal over time.\n\nIt overlaps with [Mechanism Design](https://www.lesswrong.com/tag/mechanism-design), which focuses on economic and game-theoretic incentive design."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "xjWX2FPF3gSznQBeL",
    "name": "Jungian Philosophy/Psychology",
    "core": false,
    "slug": "jungian-philosophy-psychology",
    "oldSlugs": [
      "jungian-philosophy"
    ],
    "postCount": 1,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "xQzd3c86dT9rLH985",
    "name": "Infinity",
    "core": false,
    "slug": "infinity",
    "oldSlugs": null,
    "postCount": 7,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "92SxJsDZ78ApAGq72",
    "name": "Nutrition",
    "core": false,
    "slug": "nutrition",
    "oldSlugs": null,
    "postCount": 55,
    "description": {
      "markdown": "How to optimize your food intake for various desired outcomes; nutrition science, diets, experiments.\n\nSee also: [Cooking](http://lesswrong.com/tag/cooking)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "yXNtYNHJB54T3bGm3",
    "name": "Dialogue (format)",
    "core": false,
    "slug": "dialogue-format",
    "oldSlugs": [
      "dialogue-form"
    ],
    "postCount": 28,
    "description": {
      "markdown": "Concepts are sometimes best illustrated through fictional dialogue between two or more characters holding different viewpoints. While the conversation itself is fictional, the subject matter is not.\n\n**Related Pages:** [Fiction](https://www.lesswrong.com/tag/fiction), [Interviews](https://www.lesswrong.com/tag/interviews)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "xv7Bg5fbF9WppYREi",
    "name": "Buddhism",
    "core": false,
    "slug": "buddhism",
    "oldSlugs": null,
    "postCount": 26,
    "description": {
      "markdown": "Buddhism is both a major group of religions in the world and a tradition of practice that, among other things, purports to teach a path to uncovering the way of truth, wisdom, and compassion. Western interpretations of Buddhism and Western Buddhist practice sometimes intersects with the practice of LW-style rationality."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "sSNtcEQsqHgN8ZmRF",
    "name": "Fun Theory",
    "core": false,
    "slug": "fun-theory",
    "oldSlugs": null,
    "postCount": 41,
    "description": {
      "markdown": "**Fun Theory** is the field of knowledge studying how to design for fun in future society: it deals in questions such as \"How much fun is there in the universe?\", \"Will we ever run out of fun?\", \"Are we having fun yet?\" and \"Could we be having more fun?\"\n\nFrom [The Fun Theory Sequence](https://www.lesswrong.com/posts/K4aGvLnHvYgX9pZHS/the-fun-theory-sequence):\n\n> Many critics (including [George Orwell](https://www.lesswrong.com/lw/xl/eutopia_is_scary/)) have commented on the inability of authors to imagine Utopias where anyone would actually want to live. If no one can imagine a Future where anyone would want to live, that may drain off motivation to work on the project. The prospect of endless boredom is routinely fielded by conservatives as a knockdown argument against research on lifespan extension, against cryonics, against all transhumanism, and occasionally against the entire Enlightenment ideal of a better future.\n\n> Fun Theory is also the fully general reply to religious theodicy (attempts to justify why God permits evil). Our present world has flaws even from the standpoint of such eudaimonic considerations as freedom, personal responsibility, and self-reliance. Fun Theory tries to describe the dimensions along which a benevolently designed world can and should be optimized, and our present world is clearly _not_ the result of such optimization. Fun Theory also highlights the flaws of any particular religion's perfect afterlife - you wouldn't want to go to their Heaven.\n\nThe argument against Enlightenment\n----------------------------------\n\nSome critiques of [transhumanism](https://www.lesswrong.com/tag/transhumanism) (and related fields such as cryonics or lifespan extension) suggest that human enhancement would be accompanied boredom and the end of fun as we know it. For example: \"if we self-improve human minds to extreme levels of intelligence, all challenges known today may bore us.\" Likewise, \"if superhumanly intelligent machines take care of our every need, it is apparent that no challenges nor fun will remain.\"\n\nHowever, we can work towards determining whether and how the universe will offer, or whether we ourselves can create, ever more complex and sophisticated opportunities to delight, entertain and challenge ever more powerful and resourceful minds.\n\nThe concept of Utopia\n---------------------\n\nTranshumanists are usually seen as working towards a better human future. This future is sometimes conceptualized, as George Orwell [aptly describes it](http://www.orwell.ru/library/articles/socialists/english/e_fun), as Utopia:\n\n> \"It is a commonplace \\[view\\] that the Christian Heaven, as usually portrayed, would attract nobody. Almost all Christian writers dealing with Heaven either say frankly that it is indescribable or conjure up a vague picture of gold, precious stones, and the endless singing of hymns... \\[W\\]hat it could not do was to describe a condition in which the ordinary human being actively wanted to be.\"\n\nImagining this perfect future where every problem is solved and where there is constant peace and rest--as seen, a close parallel to several religious Heavens--rapidly leads to the conclusion that no one would actually want to live there.\n\nComplex values and fun theory's solution\n----------------------------------------\n\nA key insight of fun theory, in its current embryonic form, is that _eudaimonia_ \\- the classical framework where happiness is the ultimate human goal - is [complicated](https://www.lesswrong.com/tag/complexity-of-value). That is, there are many properties which contribute to a life worth living. We humans require many things to experience a fulfilled life: Aesthetic stimulation, pleasure, love, social interaction, learning, challenge, and much more.\n\nIt is a common mistake in discussion of future society to extract only one element of the human preferences and advocate that it alone be maximized. This would neglect all other human values. For example, if we simply optimize for pleasure or happiness, [\"wirehead\"](https://www.lesswrong.com/tag/wireheading), we'll stimulate the relevant parts of our brain and experience bliss for eternity, but pursue no other experiences. If almost _any_ element of our value system is absent, then the human future will likely be very unpleasant.\n\nEnhanced humans are also seen to have the value system of humans today, but we may choose to change it as we self-enhance. We may want to alter our own value system, by eliminating values, like bloodlust, which on reflection we wish were absent. But there are many values which we, on reflection, want to keep, and since we humans have no basis for a value system other than our current value system, fun theory must seek to maximize the value system that we have, rather than inventing new values.\n\nFun theory thus seeks to let us keep our curiosity and love of learning intact, while preventing the extremes of boredom possible in a transhuman future if our strongly boosted intellects have exhausted all challenges. More broadly, fun theory seeks to allow humanity to enjoy life when all needs are easily satisfied and avoid the fall into the un-fun utopian futures in literature.\n\nExternal links\n--------------\n\n*   George Orwell, [Why Socialists Don't Believe in Fun](http://www.orwell.ru/library/articles/socialists/english/e_fun)\n*   David Pearce, [Paradise Engineering](http://paradise-engineering.com/) and [The Hedonistic Imperative](http://www.hedweb.com/hedab.htm) ([Abolitionism](https://www.lesswrong.com/tag/abolitionism)) provides a more nuanced alternative to wireheading.\n\nSee also\n--------\n\n*   [The Fun Theory Sequence](https://www.lesswrong.com/tag/the-fun-theory-sequence)\n*   [Happiness](http://lesswrong.com/tag/happiness-1)\n*   [Complexity of value](https://www.lesswrong.com/tag/complexity-of-value)\n*   [Metaethics sequence](https://www.lesswrong.com/tag/metaethics-sequence)\n*   [Abolitionism](https://www.lesswrong.com/tag/abolitionism)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "qoTbWwaJtTSKosRCA",
    "name": "Taking Ideas Seriously",
    "core": false,
    "slug": "taking-ideas-seriously",
    "oldSlugs": [
      "taking-ideas-seriously",
      "taking-ideas-seriously-decomparmentalizaiton"
    ],
    "postCount": 18,
    "description": {
      "markdown": "**Taking Ideas Seriously** is the skill/habit of noticing when a new idea should have major ramifications. \n\nIt has to do with decompartmentalization. By default some people think about one set of ideas in one compartmentalized domain (say, professional scientists thinking about the scientific method), and don't think to apply those ideas to other domains (such as their religion or politics). People with decompartmentalized beliefs update faster.\n\nTaking a new idea seriously has the advantage of letting you act on it sooner. It can potentially have the disadvantage of making you vulnerable to convincing-but-wrong arguments, if you don't have the skills to evaluate them properly."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "vcvfjGJwRmFbMMS3d",
    "name": "Principles",
    "core": false,
    "slug": "principles",
    "oldSlugs": null,
    "postCount": 13,
    "description": {
      "markdown": "*“**Principles** are fundamental truths that serve as the foundations for behavior that gets you what you want out of life. They can be applied again and again in similar situations to help you achieve your goals.”* ― Ray Dalio, Principles: Life and Work.  \n  \nPrinciples, Heuristics, and rules of thumb are generalizations that aim to produce an optimal outcome relative to their cheapness as decision rules.   \n  \nNot using generalizations, Rules of thumb and Heuristics isn't possible. precise decision rules such as [Bayes Theorem](https://www.lesswrong.com/tag/bayes-theorem-bayesianism) are [computationally intractable](https://en.wikipedia.org/wiki/Combinatorial_explosion) even to computers, and surely aren't feasible for humans. the question become whether you have good and effective principles to guide you.\n\nSee also: [Chesterton's Fence](https://www.lesswrong.com/tag/chesterton-s-fence), [Conservation of Expected Evidence](https://www.lesswrong.com/tag/conservation-of-expected-evidence), [Occam's razor](https://www.lesswrong.com/tag/occam-s-razor), [Goodhart's Law](https://www.lesswrong.com/tag/goodhart-s-law), [More Dakka](https://www.lesswrong.com/tag/more-dakka)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "8KQvnMQYGaiCAqrXv",
    "name": "Mild Optimization",
    "core": false,
    "slug": "mild-optimization",
    "oldSlugs": null,
    "postCount": 11,
    "description": {
      "markdown": "**Mild optimization** is an approach for mitigating [Goodhart's law](https://www.lesswrong.com/tag/goodhart-s-law) in AI alignment. Instead of maximizing a fixed objective, the hope is that the agent pursues the goal in a \"milder\" fashion.\n\nFurther reading: [Arbital page on Mild Optimization](https://arbital.greaterwrong.com/p/soft_optimizer?l=2r8)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "iNqgMuoewHKMhhXAp",
    "name": "Sports",
    "core": false,
    "slug": "sports",
    "oldSlugs": null,
    "postCount": 18,
    "description": {
      "markdown": "**Sports** are games played with the human body, such as football, basketball, and baseball."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "mQbxDKHxPcKKRG4mb",
    "name": "Changing Your Mind",
    "core": false,
    "slug": "changing-your-mind",
    "oldSlugs": null,
    "postCount": 17,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "7ow6EFpypbH4hzFuz",
    "name": "Community Outreach",
    "core": false,
    "slug": "community-outreach",
    "oldSlugs": null,
    "postCount": 33,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "uL87Bw3TKzsYFMpZp",
    "name": "Teamwork",
    "core": false,
    "slug": "teamwork",
    "oldSlugs": [
      "teambuilding"
    ],
    "postCount": 12,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "pJthrJDMw54JFGise",
    "name": "Murphyjitsu",
    "core": false,
    "slug": "murphyjitsu",
    "oldSlugs": null,
    "postCount": 8,
    "description": {
      "markdown": "> In the course of making plans, Murphyjitsu is the practice of strengthening plans by repeatedly envisioning and defending against failure modes until you would be _shocked_ to see it fail. Here’s the basic setup of Murphyjitsu:\n\n> 1\\. Make a plan.\n\n> 2\\. Imagine that you’ve passed the deadline and find out that the plan failed.\n\n> 3\\. If you’re _shocked_ in this scenario, you’re done.\n\n> 4\\. Otherwise, simulate the most likely failure mode, defend against it, and repeat.\n\n-alkjash, [Hammertime Day 10: Murphyjitsu](https://www.lesswrong.com/posts/N47M3JiHveHfwdbFg/hammertime-day-10-murphyjitsu)\n\nSee also\n========\n\n*   [Inner Simulator / Surprise-o-meter](https://www.lesswrong.com/tag/inner-simulator-suprise-o-meter)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "HNJiR8Jzafsv8cHrC",
    "name": "Pascal's Mugging",
    "core": false,
    "slug": "pascal-s-mugging",
    "oldSlugs": null,
    "postCount": 28,
    "description": {
      "markdown": "**Pascal's mugging** refers to a [thought experiment](https://wiki.lesswrong.com/wiki/thought_experiment) in decision theory, a finite analogue of [Pascal's wager](https://en.wikipedia.org/wiki/Pascal's_wager).\n\n> *Suppose someone comes to me and says, \"Give me five dollars, or I'll use my magic powers from outside the Matrix to run a Turing machine that simulates and kills 3^^^^3 people.* – [Pascal's Mugging: Tiny Probabilities of Vast Utilities](https://www.lesswrong.com/posts/a5JAiTdytou3Jg749/pascal-s-mugging-tiny-probabilities-of-vast-utilities)\n\n*See also*: [Decision theory](https://www.lesswrong.com/tag/decision-theory), [Counterfactual Mugging](https://www.lesswrong.com/tag/counterfactual-mugging), [Shut up and multiply](https://www.lesswrong.com/tag/shut-up-and-multiply), [Expected Utility](https://www.lesswrong.com/tag/expected-utility), [Utilitarianism](https://www.lesswrong.com/tag/utilitarianism), [Scope Insensitivity](https://www.lesswrong.com/tag/scope-insensitivity)\n\nUnpacking the theory behind Pascal's Mugging:\n\nA rational agent chooses those actions with outcomes that, after being weighted by their probabilities, have a greater [utility](https://www.lesswrong.com/tag/utility) \\- in other words, those actions with greater [expected utility](https://www.lesswrong.com/tag/expected-utility). If an agent's utilities over outcomes can potentially grow much faster than the probability of those outcomes diminishes, then it will be dominated by tiny probabilities of hugely important outcomes; speculations about low-probability-high-stakes scenarios will come to dominate its moral decision making.\n\nA common method an agent could use to assign [prior](https://wiki.lesswrong.com/wiki/prior) probabilities to outcomes is [Solomonoff induction](https://www.lesswrong.com/tag/solomonoff-induction), which gives a prior inversely proportional to the length of the outcome's description. Some outcomes can have a very short description but correspond to an event with enormous utility (i.e.: saving [3^^^^3](https://wiki.lesswrong.com/wiki/3%5E%5E%5E%5E3) lives), hence they would have non-negligible prior probabilities but a huge utility. The agent would always have to take those kinds of actions with far-fetched results, that have low but non-negligible probabilities but extremely high returns.\n\nThis is seen as an unreasonable result. Intuitively, one is not inclined to acquiesce to the mugger's demands - or even pay all that much attention one way or another - but what kind of prior does this imply?\n\n[Robin Hanson](https://www.lesswrong.com/tag/robin-hanson) has suggested penalizing the prior probability of hypotheses which argue that we are in a *surprisingly unique* position to affect large numbers of other people who cannot symmetrically affect us. Since only one in 3^^^^3 people can be in a unique position to ordain the existence of at least 3^^^^3 other people who can't have a symmetrical effect on this one person, the prior probability would be penalized by a factor on the same order as the utility.\n\nPeter de Blanc has proven [\\[1\\]](http://arxiv.org/abs/0712.4318) that if an agent assigns a finite probability to all computable hypotheses and assigns unboundedly large finite utilities over certain environment inputs, then the expected utility of any outcome is undefined. Peter de Blanc's paper, and the Pascal's Mugging argument, are sometimes misinterpreted as showing that any agent with an *unbounded finite utility function* over outcomes is not consistent, but this has yet to be demonstrated. The unreasonable result can also be seen as an argument against the use of Solomonoff induction for weighting prior probabilities.\n\nIf an outcome with infinite utility is presented, then it doesn't matter how small its probability is: all actions which lead to that outcome will have to dominate the agent's behavior. This infinite case was stated by 17th century philosopher Blaise Pascal and named [Pascal's wager](https://en.wikipedia.org/wiki/Pascal's_wager). Many other abnormalities arise when dealing with [infinities in ethics](https://www.lesswrong.com/tag/infinities-in-ethics).\n\nNotable Posts\n-------------\n\n*   [Pascal's Mugging: Tiny Probabilities of Vast Utilities](https://www.lesswrong.com/lw/kd/pascals_mugging_tiny_probabilities_of_vast/)\n*   [Pascal's Muggle: Infinitesimal Priors and Strong Evidence](https://www.lesswrong.com/lw/h8k/pascals_muggle_infinitesimal_priors_and_strong/)\n\nReferences\n----------\n\n1.  Peter de Blanc (2007). [*Convergence of Expected Utilities with Algorithmic Probability Distributions*](http://arxiv.org/abs/0712.4318).\n2.  Nick Bostrom (2009). \"Pascal's Mugging\". *Analysis* **69** (3): 443-445. ([PDF](http://www.nickbostrom.com/papers/pascal.pdf))"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "JHYaBGQuuKHdwnrAK",
    "name": "Logical Uncertainty",
    "core": false,
    "slug": "logical-uncertainty",
    "oldSlugs": null,
    "postCount": 48,
    "description": {
      "markdown": "**Logical Uncertainty** is probabilistic uncertainty about the implications of beliefs. (Another way of thinking about it is: uncertainty about computations.) Probability theory typically assumes **logical omniscience,** IE, perfect knowledge of logic. The easiest way to see the importance of this assumption is to consider Bayesian reasoning: to evaluate the probability of evidence given a hypothesis, \\\\(P(e|h)\\\\), it's necessary to know what the implications of the hypothesis are. However, realistic agents cannot be logically omniscient.\n\nSee Also: [Logical Induction](https://www.lesswrong.com/tag/logical-induction)\n\nMotivation\n----------\n\nIs the googolth digit of pi odd? The probability that it is odd is, intuitively, 0.5. Yet we know that this is definitely true or false by the rules of logic, even though we don't know which. Formalizing this sort of probability is the primary goal of the field of logical uncertainty.\n\nThe problem with the 0.5 probability is that it gives non-zero probability to false statements. If I am asked to bet on whether the googolth digit of pi is odd, I can reason as follows: There is 0.5 chance that it is odd. Let P represent the actual, unknown, parity of the googolth digit (odd or even); and let Q represent the other parity. If Q, then anything follows. (By the Principle of Explosion, a false statement implies anything.) For example, Q implies that I will win $1 billion. Therefore the value of this bet is at least $500,000,000, which is 0.5 * $1,000,000, and I should be willing to pay that much to take the bet. This is an absurdity. Only expenditure of finite computational power stands between the uncertainty and 100% certainty.\n\nLogical Uncertainty & Counterfactuals\n-------------------------------------\n\nLogical uncertainty is closely related to the problem of [counterfactuals](/tag/counterfactuals). Ordinary probability theory relies on counterfactuals. For example, I see a coin that came up heads, and I say that the probability of tails was 0.5, even though clearly, given all air currents and muscular movements involved in throwing that coin, the probability of tails was 0.0. Yet we can imagine this possible impossible world where the coin came up tails. In the case of logical uncertainly, it is hard to imagine a world in which mathematical facts are different.\n\nReferences\n----------\n\n*   [Questions of Reasoning Under Logical Uncertainty](https://intelligence.org/files/QuestionsLogicalUncertainty.pdf) by Nate Soares and Benja Fallenstein."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "Jzm2mYuuDBCNWq8hi",
    "name": "Happiness",
    "core": false,
    "slug": "happiness-1",
    "oldSlugs": null,
    "postCount": 41,
    "description": {
      "markdown": "Posts about **Happiness**. one of the tricky things about Happiness is that sometimes directly pursuing it doesn't work or is even counterproductive. Happiness isn't (and can't be) [the only important thing](https://www.lesswrong.com/posts/synsRtBKDeAFuo7e3/not-for-the-sake-of-happiness-alone), but is non the less important. Thus LessWrong dealt a lot with the questions about happiness and how to pursue it.\n\n> “Happiness is a choice. If you’re so smart, how come you aren’t happy? How come you haven’t figured that out? That’s my challenge to all the people who think they’re so smart and so capable.” - Naval Ravikant\n\nSee also: [Gratitude](https://www.lesswrong.com/tag/gratitude), [Well-being](https://www.lesswrong.com/tag/well-being)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "oraLTPkETL5xKmhx3",
    "name": "Moderation (topic)",
    "core": false,
    "slug": "moderation-topic",
    "oldSlugs": null,
    "postCount": 18,
    "description": {
      "markdown": "**Moderation**, in online communities especially, deals with what decision and limitation should be made in order for that community to thrive. \n\nFor posts regarding LW moderation policies and decisions (such as ban notices) see [LW Moderation](https://www.lesswrong.com/tag/lw-moderation)."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "SW3euSNqpozcsxXaX",
    "name": "Litanies & Mantras",
    "core": false,
    "slug": "litanies-and-mantras",
    "oldSlugs": [
      "litanies-mantras"
    ],
    "postCount": 9,
    "description": {
      "markdown": "**Related Pages:** \n\n*   [Litany of Tarski](https://www.lesswrong.com/tag/litany-of-tarski) \n*   [Litany of Gendlin](https://www.lesswrong.com/tag/litany-of-gendlin)\n*   [Litany of Occam](https://www.lesswrong.com/tag/litany-of-occam)\n*   [Litany of Jai](https://www.lesswrong.com/tag/litany-of-jai)\n*   [Litany of Hodgell](https://www.lesswrong.com/tag/litany-of-hodgell)\n*   [Adding Up to Normality](https://www.lesswrong.com/tag/adding-up-to-normality)\n*   [Reality Is Normal](https://www.lesswrong.com/tag/reality-is-normal)\n*   [Virtues](https://www.lesswrong.com/tag/virtues)\n*   [Principles](https://www.lesswrong.com/tag/principles)\n*   [Taking Ideas Seriously](https://www.lesswrong.com/tag/taking-ideas-seriously)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "RqNHf9ZpPW29uMKyA",
    "name": "Demon Threads",
    "core": false,
    "slug": "demon-threads",
    "oldSlugs": null,
    "postCount": 5,
    "description": {
      "markdown": "> A **Demon Thread** is a discussion where everything is subtly warping towards aggression and confusion (i.e. as if people are under demonic influence), even if people are well intentioned and on the same 'side.' You can see a demon thread coming in advance, but it's still hard to do anything about."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "hGzywXvWhSdJi5F2a",
    "name": "LW Moderation",
    "core": false,
    "slug": "lw-moderation",
    "oldSlugs": null,
    "postCount": 21,
    "description": {
      "markdown": "**LessWrong Moderation** posts deal with how the site moderators comments and posts. It includes laying out policies, decisions about who's on the mod team, and concrete moderation decisions.\n\nFor general posts on the *topic* of moderation, see [Moderation (topic)](https://www.lesswrong.com/tag/moderation-topic)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "zCyW4sAD7gkmAT38f",
    "name": "Comfort Zone Expansion (CoZE)",
    "core": false,
    "slug": "comfort-zone-expansion-coze",
    "oldSlugs": null,
    "postCount": 6,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "ntahi2tr7e9DjCYdu",
    "name": "Chesterton's Fence",
    "core": false,
    "slug": "chesterton-s-fence",
    "oldSlugs": null,
    "postCount": 12,
    "description": {
      "markdown": "**Chesterton's Fence** is the principle that reforms should not be made until the reasoning behind the existing state of affairs is understood.\n\n_Related_: [Epistemic Modesty](https://www.lesswrong.com/tag/epistemic-modesty)\n\nFrom Chesterton’s 1929 book, The Thing, in the chapter entitled _The Drift from Domesticity_ \\[1\\]:\n\n> In the matter of reforming things, as distinct from deforming them, there is one plain and simple principle; a principle which will probably be called a paradox. There exists in such a case a certain institution or law; let us say, for the sake of simplicity, a fence or gate erected across a road. The more modern type of reformer goes gaily up to it and says, “I don’t see the use of this; let us clear it away.” To which the more intelligent type of reformer will do well to answer: “If you don’t see the use of it, I certainly won’t let you clear it away. Go away and think. Then, when you can come back and tell me that you do see the use of it, I may allow you to destroy it.\n\nSee also\n--------\n\n*   [Wikipedia: Chesterton's Fence](http://en.wikipedia.org/wiki/Wikipedia:Chesterton's_fence)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "cNLSTJzaY6Gi3iYQr",
    "name": "Five minute timers",
    "core": false,
    "slug": "five-minute-timers",
    "oldSlugs": null,
    "postCount": 12,
    "description": {
      "markdown": "**Five minute timers.** Sometimes, all you need to solve an impossible problem is to actually think for 5 minutes. (Sometimes it is actually 15 minutes, or two hours. But, the first five minutes is helpful for transforming it from an inpenetrable ugh field to a tractable problem)\n\nSometimes called Yoda timers. (A timebox wherein you Actually Try)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "7dpiuzLNPMtdjAvNq",
    "name": "Summoning Sapience",
    "core": false,
    "slug": "summoning-sapience",
    "oldSlugs": null,
    "postCount": 4,
    "description": {
      "markdown": "**Summoning Sapience** is the act of noticing that you are in a situation where it'd be beneficial to *actually think*, rather than running on autopilot. \n\nWhat \"actually thinking\" means depends on context, but often includes paying extra attention to details of your environment, thinking through the possible ramifications of those details, making a plan, checking if that plan will work, etc.\n\nSummon Sapience is a Rationality skill. Related to Murphijitsu."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "DQHWBcKeiLnyh9za9",
    "name": "Events (Community)",
    "core": false,
    "slug": "events-community",
    "oldSlugs": null,
    "postCount": 144,
    "description": {
      "markdown": "Announcements of planned real-time gatherings, either online or in person."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "pnSXfWXbQihrFadeD",
    "name": "Case Study",
    "core": false,
    "slug": "case-study",
    "oldSlugs": null,
    "postCount": 10,
    "description": {
      "markdown": "A post looking at a particular idea through a real-world example or examples."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "jZF2jwLnPKBv6m3Ag",
    "name": "Organization Updates",
    "core": false,
    "slug": "organization-updates",
    "oldSlugs": [
      "organizational-updates"
    ],
    "postCount": 23,
    "description": {
      "markdown": "**Organization updates** are what you might expect - updates relating to a specific group or organization."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "MAp6Ft8b3s7kJdrQ9",
    "name": "Selection Effects",
    "core": false,
    "slug": "selection-effects",
    "oldSlugs": null,
    "postCount": 10,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "Y3oHd7CQpy8aQFWD9",
    "name": "Behavior Change",
    "core": false,
    "slug": "behavior-change",
    "oldSlugs": null,
    "postCount": 3,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "32DdRimdM7sB5wmKu",
    "name": "Empiricism",
    "core": false,
    "slug": "empiricism",
    "oldSlugs": null,
    "postCount": 33,
    "description": {
      "markdown": "> \"The sixth virtue is **empiricism**. The roots of knowledge are in observation and its fruit is prediction. What tree grows without roots? What tree nourishes us without fruit? If a tree falls in a forest and no one hears it, does it make a sound? One says, “Yes it does, for it makes vibrations in the air.” Another says, “No it does not, for there is no auditory processing in any brain.” Though they argue, one saying “Yes”, and one saying “No”, the two do not anticipate any different experience of the forest. Do not ask which beliefs to profess, but which experiences to anticipate. Always know which difference of experience you argue about. Do not let the argument wander and become about something else, such as someone’s virtue as a rationalist. Jerry Cleaver said: “What does you in is not failure to apply some high-level, intricate, complicated technique. It’s overlooking the basics. Not keeping your eye on the ball.” Do not be blinded by words. When words are subtracted, anticipation remains.\" - [(Twelve Virtues of Rationality)](https://www.lesswrong.com/posts/7ZqGiPHTpiDMwqMN2/twelve-virtues-of-rationality)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "gsv9XWbZDcnZmKuqM",
    "name": "Psychiatry",
    "core": false,
    "slug": "psychiatry",
    "oldSlugs": null,
    "postCount": 25,
    "description": {
      "markdown": "**Psychiatry** is the medical specialty devoted to the diagnosis, prevention, and treatment of mental disorders. These include various maladaptations related to mood, behavior, cognition, and perceptions."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "a65Lgr7Q5jqRWHtM6",
    "name": "Gratitude",
    "core": false,
    "slug": "gratitude",
    "oldSlugs": null,
    "postCount": 17,
    "description": {
      "markdown": "**Gratitude** is the the feeling or showing of appreciation. practices that aim to increase how much one is expressing gratitude (such as **Gratitude Journaling**) seem to have one of the highest correlation with actually increasing happiness and life satisfaction."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "A4kr45wS7fBW5PBpf",
    "name": "Ideological Turing Tests",
    "core": false,
    "slug": "ideological-turing-tests",
    "oldSlugs": null,
    "postCount": 5,
    "description": {
      "markdown": "The **Ideological Turing Test** is an exercise where you try to pretend to hold an opposing ideology convincingly enough that outside observers can't reliably distinguish you from a true believer.\n\n[It was first described by economist Bryan Caplan:](https://www.econlib.org/archives/2011/06/the_ideological.html)\n\n> Put me and five random liberal social science Ph.D.s in a chat room. Let liberal readers ask questions for an hour, then vote on who isn't really a liberal. Then put \\[economist Paul\\] Krugman and five random libertarian social science Ph.D.s in a chat room. Let libertarian readers ask questions for an hour, then vote on who isn't really a libertarian. Simple as that.\n\nPassing the ideological Turing test is a sign that you understand the opposing ideology on a deep level.\n\nThe ideological Turing test has a similar motivation to [Steelmanning](https://www.lesswrong.com/tag/steelmanning), but works in a different way.\n\nThe name comes from the [Turing Test](https://en.wikipedia.org/wiki/Turing_test) proposed by computer scientist Alan Turing, where a computer program must pretend to be a human while human judges try to tell it apart from real humans."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "WqLn4pAWi5hn6McHQ",
    "name": "Self Improvement",
    "core": false,
    "slug": "self-improvement",
    "oldSlugs": null,
    "postCount": 83,
    "description": {
      "markdown": "Personal Growth, etc\n\n**Related Pages:** [Growth Stories](https://www.lesswrong.com/tag/growth-stories)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "2JdCpTrNgBMNpJiyB",
    "name": "Space Exploration & Colonization",
    "core": false,
    "slug": "space-exploration-and-colonization",
    "oldSlugs": null,
    "postCount": 43,
    "description": {
      "markdown": "**Space Exploration & Colonization** involves humans or human-built machines traveling to other planets, star systems, or galaxies.\n\nPossible motivations include curiosity, reducing the chance of [existential risks](https://www.lesswrong.com/tag/existential-risk), and expanding human civilization.\n\nPossible techniques include both present or near-future technology and [speculative future technology](https://www.lesswrong.com/tag/futurism).\n\nFor the question of why alien civilizations haven't space-colonized us, see [Great Filter](https://www.lesswrong.com/tag/great-filter).\n\n**Related Sequences:** [So You Want To Colonize The Universe](https://www.lesswrong.com/s/96XzQgTL2HBNkBwL4)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "8sh6iLwYWDJ7z3fPo",
    "name": "Startups",
    "core": false,
    "slug": "startups",
    "oldSlugs": null,
    "postCount": 41,
    "description": {
      "markdown": "**Startups** are small, recently-founded companies that are looking to grow much larger. They generally seek funding through venture capital investors."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "Lgy35Xh222bwgeGTL",
    "name": "Government",
    "core": false,
    "slug": "government",
    "oldSlugs": null,
    "postCount": 55,
    "description": {
      "markdown": "Governments are institutions set up by societies to create and enforce laws and rules of conduct.\n\nModern western philosophy usually analyzes governments through some variant of social contract theory, which was developed by John Locke and Thomas Hobbes among others and seeks to describe government as a contractual agreement among the people to help create a world that is better than the \"state of nature.\"\n\nSince then, political philosophers have developed a myriad of different systems for thinking about government, from the Marxist viewpoint of government as a class construct of oppression, to the Keynesian view of government as an important economic regulator, to Max Weber's view that the government is defined by its [Monopoly on Violence](https://en.wikipedia.org/wiki/Monopoly_on_violence).\n\nRationalists typically see government as a means to a greater utilitarian end. In a democracy, the government can act as the hand that directly enacts and restricts actions in accordance with popular opinion. Obviously, there are many ways this can go wrong.\n\n**Related Pages:** [Law and Legal systems](https://www.lesswrong.com/tag/law-and-legal-systems), [Politics](https://www.lesswrong.com/tag/politics), [Mechanism Design](https://www.lesswrong.com/tag/mechanism-design), [Voting Theory](https://www.lesswrong.com/tag/voting-theory), [Selectorate Theory](https://www.lesswrong.com/tag/selectorate-theory), [Futarchy](https://www.lesswrong.com/tag/futarchy), [Coordination / Cooperation](https://www.lesswrong.com/tag/coordination-cooperation)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "dBPou4ihoQNY4cquv",
    "name": "Psychology",
    "core": false,
    "slug": "psychology",
    "oldSlugs": null,
    "postCount": 117,
    "description": {
      "markdown": "**Psychology** is the study of the mind. Typically, psychology focuses on the human mind, but animal minds are also sometimes studied.\n\nJust like much of the research on animal psychology also applies to humans, research on human psychology may inform how we develop and use AI.\n\nModern psychology began with the Freudian model of psychoanalysis, which quickly supplanted the prevailing pseudosciences like phrenology. Psychoanalysis is now largely obsolete, and has been replaced by other schools. The most prominent among these include:\n\nBehavioral Psychology, which focuses on observable learning and behavior.\n\nHumanistic Psychology, which focuses on theories of self-actualization and self-help.\n\nCognitive Psychology, which uses modern research on brain anatomy and other subjects to create theories about how the brain learns, develops, and makes decisions.\n\nFrom an academic standpoint, much of the content of Lesswrong can be described as trying to implement the results of Decision Theory to people's daily actions using knowledge of human psychology. Sometimes, this involves overriding our native tendencies. Other times, it involves using them to our advantage."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "z95PGFXtPpwakqkTA",
    "name": "Intuition",
    "core": false,
    "slug": "intuition",
    "oldSlugs": null,
    "postCount": 37,
    "description": {
      "markdown": "**Intuition** is the ability to acquire knowledge without recourse to conscious reasoning. Also relevant are **Intuition Pumps**, a thought experiment, a model, or anything else, that's structured to allow the thinker to use their intuition to develop an answer to a problem. \n\n*(Modified from Wikipedia* [*1*](https://en.wikipedia.org/wiki/Intuition)*,* [*2*](https://en.wikipedia.org/wiki/Intuition_pump)*, this section is licensed as* [***CC BY-SA 3.0 Unported License***](https://en.wikipedia.org/wiki/Wikipedia:Text_of_Creative_Commons_Attribution-ShareAlike_3.0_Unported_License) *to conform with the Wikipedia license requirements)*"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "SbsZrDB844cENk4DQ",
    "name": "Falsifiability",
    "core": false,
    "slug": "falsifiability",
    "oldSlugs": null,
    "postCount": 10,
    "description": {
      "markdown": "**Falsifiability** or **Refutability** is the capacity for a statement, theory or hypothesis to be contradicted by evidence. For example, the statement \"All swans are white\" is falsifiable because one can observe that black swans exist. [(From Wikipedia)](https://en.wikipedia.org/wiki/Falsifiability)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "yrg267i4a8EsgYAXp",
    "name": "HPMOR (discussion & meta)",
    "core": false,
    "slug": "hpmor-discussion-and-meta",
    "oldSlugs": [
      "hpmor",
      "hpmor-discussion-threads",
      "hpmor-discussion-threads-etc"
    ],
    "postCount": 111,
    "description": {
      "markdown": "This tag is for posts **discussing** the fan-fiction **[Harry Potter and the Methods of Rationality](https://www.lesswrong.com/hpmor) (HPMOR)** or posts talking about events, fan-fan-fiction content, etc – just not actual chapters/posts of the story that can be be found in the HPMOR sequences."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "CYMR6p5iZG75QAT8a",
    "name": "Censorship",
    "core": false,
    "slug": "censorship",
    "oldSlugs": null,
    "postCount": 24,
    "description": {
      "markdown": "**Censorship** is the suppression of speech, public communication, or other information, on the basis that such material is considered objectionable, harmful, sensitive, or \"inconvenient.\" Censorship can be conducted by governments, private institutions, and other controlling bodies. [(From Wikipedia)](https://en.wikipedia.org/wiki/Censorship)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "FGfgzGpPTtKEqSrDm",
    "name": "More Dakka",
    "core": false,
    "slug": "more-dakka",
    "oldSlugs": null,
    "postCount": 11,
    "description": {
      "markdown": "**More Dakka** is the technique of throwing more resources at a problem to see if you get better results. \n\nOriginally, More Dakka is the [trope](https://tvtropes.org/pmwiki/pmwiki.php/Main/MoreDakka) of solving problems by unloading as many rounds of ammunition at them as possible. In the rationalist community it was popularized by [Zvi](https://www.lesswrong.com/posts/z8usYeKX7dtTWsEnk/more-dakka) to have the above meaning."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "Zz3HWyByyKF64Sfns",
    "name": "The SF Bay Area",
    "core": false,
    "slug": "the-sf-bay-area",
    "oldSlugs": [
      "rationalist-berkeley"
    ],
    "postCount": 26,
    "description": {
      "markdown": "**The San Francisco Bay Area** is a region in the US state of California. Many members of [the rationalist community](http://lesswrong.com/tag/community) are located there, as are the [Machine Intelligence Research Institute](https://www.lesswrong.com/tag/machine-intelligence-research-institute-miri) and the [Center For Applied Rationality](https://www.lesswrong.com/tag/center-for-applied-rationality-cfar)\n\n*Note: Covid-19 resulted in many people moving and the state of living affairs described below may no longer be accurate.*\n\nSee also\n--------\n\n*   [Rationalist movement](https://www.lesswrong.com/tag/rationalist-movement)\n*   [History of Less Wrong](https://www.lesswrong.com/tag/history-of-less-wrong)\n*   [LessWrong Meetup Groups](/community)\n\nHistory\n-------\n\nHow did it become a hub?\n\nThe [Machine Intelligence Research Institute](https://www.lesswrong.com/tag/machine-intelligence-research-institute-miri) (then the Singularity Institute) moved to the Bay Area in February 2005.[^1^](https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fn1)\n\nThe charity evaluator [GiveWell](https://en.wikipedia.org/wiki/GiveWell) completed its move from New York to San Francisco in February 2013.[^2^](https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fn2)\n\nThe effective careers research organization 80,000 Hours announced it was moving to the Bay Area in May 2016, with the move completed by October.[^3^](https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fn3) It was also in the Bay Area for summer 2016.[^4^](https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fn4) Its 2017 review also mentions moving to the Bay Area in 2017.[^5^](https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fn5) In 2019, 80,000 Hours moved from the Bay Area to the UK.\n\nThe [Center for Applied Rationality](https://wiki.lesswrong.com/wiki/Center_for_Applied_Rationality)'s 2017 Impact Report found that \"moved to the Bay Area due to CFAR\" is one of the strongest predictors for a CFAR participant having an \"increase in expected impact\".[^6^](https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fn6)\n\nWard Street stuff.\n\nDebate\n------\n\nEspecially since 2016 or so (possibly earlier?), there has been a considerable amount of debate about whether moving to the Bay Area is good for individuals or the community as a whole.[^7^](https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fn7)\n\nSome illustrative quotes for concern about people moving to the Bay Area:\n\n> *   \"The reasons for this are not immediately apparent. From the outside, people full of energy and enthusiasm make the pilgrimage to Berkeley, go quiet on social media, and when you finally hear from them six months later they don't seem like the person you once knew. *Something* is happening to them, although it isn't particularly clear what.\"[^8^](https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fn8)\n> *   \"The theme of the Bay Solstice turned out to be 'Hey guys, so people keep coming to the Bay, running on a dream and a promise of community, but that community is not actually there, there's a tiny number of well-connected people who everyone is trying to get time with, and everyone seems lonely and sad. And we don't even know what to do about this.' \"[^9^](https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fn9)^,^ [^10^](https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fn10)^,^[^11,^](https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fn11) [^12^](https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fn12)\n\n> \"It's seemed to me for awhile now that the stuff that people are actually talking about in-person (e.g. at CFAR workshops) has far outstripped the pace of what's publicly available in blog post format and I'm really happy to see progress on that front.\" [^13^](https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fn13)^,^ [^14^](https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fn14)^,^ [^15^](https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fn15)[^,16,^](https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fn16)[^17^](https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fn17)\n\n> 4\\. Ward Street is quickly becoming the center of the rationalist scene in Berkeley. We’re trying to encourage that so that as many people as possible can live near each other and it can feel like more of a community. I’ll be staying there temporarily when I first get to California, and I know a lot of other people on the street and they’re all pretty interesting. Anyway, there’s a house opening up there as the current residents leave, and we’d like to get rationalist-adjacent people to move in. It’s three bedrooms, one bathroom, and it costs $4100/month total. If interested (either in renting the whole house with friends/family, or in just renting one room and hoping two other people want the same), email jsalvatier\\[at\\]gmail\\[dot\\]com and he can tell you more / help connect interested parties together.\"\"\"[^18^](https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fn18)^,^ [^19^](https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fn19)^,^ [^20,^](https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fn20) [^21^](https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fn21)\n\n> \"Even if you know nothing else, you know to move to San Francisco or New York and hoping something good happens there, rather than sitting around in some dying small town where you know nothing will ever happen and being curious about anything beyond the town is a cultural transgression. This is a strategy open to all.\"[^22^](https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fn22)\n\nSending people to the Bay\n-------------------------\n\n> I know meetups causing people to move to the Bay is a controversial topic, but from my perspective, moving to the Bay is one of the best things a person can do in terms of expected impact on the existential risk landscape. It gives people the opportunity to work at aligned organizations, and to be around hundreds of like-minded people, which (in addition to its social benefits) allows people to find collaborators with whom to start new projects and organizations.[^23^](https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fn23)\n\nExternal links\n--------------\n\n*   [Informational site about Bay Area rationality](http://www.bayrationality.com/) (not sure who runs the site)\n*   [Rationalist House Games](http://www.rationalistgames.org/)\n\nReferences\n----------\n\n1.  [\"News of the Singularity Institute for Artificial Intelligence\"](https://web.archive.org/web/20060220211402/http://www.singinst.org:80/news/). Archived from [the original](http://www.singinst.org/news/) on February 20, 2006. Retrieved February 12, 2018. \"SIAI has moved to Silicon Valley. Executive Director Tyler Emerson and Advocacy Director Michael Anissimov are both located in the Bay Area of California, along with SIAI Research Fellow Eliezer Yudkowsky (since February). This should enable us to stay in better contact with donors, and cultivate team members and additional collaborators.\"[↩](https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fnref1)\n2.  Holden Karnofsky. [\"GiveWell's progress in 2012\"](https://blog.givewell.org/2013/02/08/givewells-progress-in-2012/). February 8, 2013. *The GiveWell Blog*. GiveWell. Retrieved February 12, 2018.[↩](https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fnref2)\n3.  Benjamin Todd. [\"80k supporter update - moving to the Bay; cost\"](https://groups.google.com/forum/#!msg/80k_updates/jAZNlgEhSsM/YIztgo9kAAAJ). May 15, 2016. Google Groups. Retrieved February 12, 2018. \"We decided to move and our trustees have approved. We plan to be out in the Bay by August, and fully moved by Oct. The next step is to get visas, which is in progress.\"[↩](https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fnref3)\n4.  Benjamin Todd. [\"80k supporter update\"](https://groups.google.com/forum/#!msg/80k_updates/RxRGOUF0ii4/TTUsGDgyDAAJ). August 23, 2016. Google Groups. Retrieved February 12, 2018. \"Moved to the Bay Area for the summer.\"[↩](https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fnref4)\n5.  Benjamin Todd. [\"Annual review December 2017\"](https://80000hours.org/2017/12/annual-review/). December 24, 2017. 80,000 Hours. Retrieved February 12, 2018. \"We completed our move to the Bay Area, securing visas for everyone on the team by April 2017, setting up our office, and doing the administration needed (though we’re yet to have the pleasure of filing our first personal US tax returns…).\"[↩](https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fnref5)\n6.  Dan Keys. [\"CFAR 2017 Impact Report\"](http://www.rationality.org/resources/updates/2017/cfar-2017-impact-report). Center for Applied Rationality. Retrieved February 12, 2018.[↩](https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fnref6)\n7.  [\"Why Do All The Rationalists Live In The Bay Area?\"](https://thingofthings.wordpress.com/2017/05/03/why-do-all-the-rationalists-live-in-the-bay-area/). May 4, 2017. Thing of Things. Retrieved February 12, 2018.[↩](https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fnref7)\n8.  bendini. [\"The Craft & The Community - A Post-Mortem & Resurrection\"](https://www.lesswrong.com/posts/wmEcNP3KFEGPZaFJk/the-craft-and-the-community-a-post-mortem-and-resurrection). November 1, 2017. LessWrong. Retrieved February 12, 2018.[↩](https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fnref8)\n9.  Raymond Arnold. [\"Notes from the Hufflepuff Unconference (Part 1)\"](https://www.lesswrong.com/lw/p1f/notes_from_the_hufflepuff_unconference_part_1/). May 23, 2017. LessWrong. Retrieved February 12, 2018.[↩](https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fnref9)\n10.  Zvi Mowshowitz. [\"What Is Rationalist Berkeley’s Community Culture?\"](https://thezvi.wordpress.com/2017/08/12/what-is-rationalist-berkleys-community-culture/). November 3, 2017. Don't Worry About the Vase. Retrieved February 12, 2018.[↩](https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fnref10)\n11.  Elizabeth Van Nostrand. [\"It looks pretty likely I'll move to the bay...\"](https://www.facebook.com/li.van.nostrand/posts/10102872753725305). Facebook. Retrieved February 12, 2018.[↩](https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fnref11)\n12.  Brent Dill. [\"Two years ago, to the day, I decided to move to the...\"](https://www.facebook.com/ialdabaoth/posts/10208221491793885). Facebook. Retrieved February 12, 2018.[↩](https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fnref12)\n13.  [https://www.lesserwrong.com/posts/tMhEv28KJYWsu6Wdo/kensh#vDST8wTjsiWKCjZ3y](https://www.lesswrong.com/posts/tMhEv28KJYWsu6Wdo/kensh#vDST8wTjsiWKCjZ3y)[↩](https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fnref13)\n14.  Raymond Arnold. [\"It took me a disturbing amount of time to realize...\"](https://www.facebook.com/raymond.arnold.5/posts/10211225174359187). Facebook. Retrieved February 12, 2018.[↩](https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fnref14)\n15.  Alyssa Vance. [\"Many Effective Altruists think about moving to the San Francisco Bay Area, or have already done so …\"](https://www.facebook.com/groups/effective.altruists/permalink/1366133853442968/). April 14, 2017. Facebook. Retrieved February 12, 2018.[↩](https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fnref15)\n16.  Scott Alexander. [\"To The Great City!\"](http://slatestarcodex.com/2017/07/03/to-the-great-city/). July 3, 2017. Slate Star Codex. Retrieved February 12, 2018.[↩](https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fnref16)\n17.  [http://slatestarcodex.com/2017/07/03/to-the-great-city/#comment-518132](http://slatestarcodex.com/2017/07/03/to-the-great-city/#comment-518132)[↩](https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fnref17)\n18.  Scott Alexander. [\"OT79: Open Road\"](http://slatestarcodex.com/2017/07/03/ot79-open-road/). July 5, 2017. Slate Star Codex. Retrieved February 12, 2018.[↩](https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fnref18)\n19.  Zvi Mowshowitz. [\"Responses to Tyler Cowen on Rationality\"](https://thezvi.wordpress.com/2017/04/04/responses-to-tyler-cohen-on-rationality/). September 10, 2017. Don't Worry About the Vase. Retrieved February 12, 2018.[↩](https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fnref19)\n20.  Katja Grace. [\"Be my neighbor\"](https://meteuphoric.wordpress.com/2017/07/20/be-my-neighbor/). July 20, 2017. Meteuphoric. Retrieved February 12, 2018.[↩](https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fnref20)\n21.  Alyssa Vance. [\"'Why is the best place to live in the world so much...\"](https://www.facebook.com/alyssamvance/posts/10214122240439702). August 5, 2017. Facebook. Retrieved February 12, 2018.[↩](https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fnref21)\n22.  [https://www.ribbonfarm.com/2017/08/17/the-premium-mediocre-life-of-maya-millennial/#more-6047](https://www.ribbonfarm.com/2017/08/17/the-premium-mediocre-life-of-maya-millennial/#more-6047)[↩](https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fnref22)\n23.  mingyuan. [\"What Do We Mean By 'Meetups'?\"](https://www.lesswrong.com/posts/bDnFhJBcLQvCY3vJW/what-do-we-mean-by-meetups). February 7, 2018. LessWrong. Retrieved February 12, 2018.[↩](https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fnref23)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "6Qic6PwwBycopJFNN",
    "name": "Contrarianism",
    "core": false,
    "slug": "contrarianism",
    "oldSlugs": [
      "contrerianism"
    ],
    "postCount": 21,
    "description": {
      "markdown": "A **contrarian** is a person who holds a contrary position, especially a position against the [majority](https://www.lesswrong.com/tag/consensus) [(from Wikipedia).](https://en.wikipedia.org/wiki/Contrarian)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "WPkEd3et8f488w8LT",
    "name": "Good Explanations (Advice)",
    "core": false,
    "slug": "good-explanations-advice",
    "oldSlugs": null,
    "postCount": 13,
    "description": {
      "markdown": "This **Good Explanations (Advice)** tag  is for advice on how to write good explanations."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "bFi5fzkCzBWoQSeiB",
    "name": "Technological Unemployment",
    "core": false,
    "slug": "technological-unemployment",
    "oldSlugs": null,
    "postCount": 9,
    "description": {
      "markdown": "**Technological Unemployment** occurs when businesses replace human workers with automated systems, and the displaced workers are unable to find new jobs.\n\nRelated to [Automation](https://www.lesswrong.com/tag/automation)."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "LnEEs8xGooYmQ8iLA",
    "name": "Truth, Semantics, & Meaning",
    "core": false,
    "slug": "truth-semantics-and-meaning",
    "oldSlugs": [
      "truth",
      "truth-semantics-and-meaning"
    ],
    "postCount": 78,
    "description": {
      "markdown": "**Truth, Semantics, and Meaning**: What does it mean to assert that something is true? A very popular answer is [map-territory correspondence theory](https://www.lesswrong.com/tag/map-and-territory). But the details of this theory are not clear, and there are other contenders.\n\nTruth as Correspondence\n-----------------------\n\nMany consider truth as the correspondence between reality and one's beliefs about reality. Within this frame, truth itself is not necessarily limited to one's belief about something. For a statement/ideal/proposed fact to be considered \"true,\" you must take it as its definition. Truth doesn't imply that something has to be proven in order for it to be made true, but that the statement/ideal/proposed fact has to be true all of the time, regardless of one's belief.\n\nAlfred Tarski defined truth in terms of an infinite family of sentences such as:\n\n> The sentence 'snow is white' is *true* if and only if snow is white.\n\nTo understand whether a belief is true, we need (only) to understand what possible states of the world would make it true or false, and then ask directly about the world. Often, people assume that ideals and morals change with culture; as they tend to do. Unfortunately, many people struggle with their belief of \"truth\" based on their religion. Because of their belief, they object the currently accepted \"truth\" about the world, about life (how we all got here), and most importantly, what is considered \"right\" or \"wrong.\"\n\n\"Truth\" is not, however, a determination. Truth is not simply a belief. Truth is an ideal, concept, or fact that can be observed. Whether an individual has a belief derived from their religion on what is truth or not, unless they have observed it, they cannot prove whether their belief is truth or not. Reiterating from above: the lack of proof or justification, or even rationalization, does not change the status of truth. What's truth is truth, and what is false, is false. Humans simply decide to reject notions and proposed facts as truth if they are not observable, or are not able to show any proof.\n\n'Truth' is a very simple concept, understood perfectly well by three-year-olds, but often made unnecessarily complicated by adults.\n\nOther Theories of Truth\n-----------------------\n\n<needed>\n\nNotable Posts\n-------------\n\n*   [The Useful Idea of Truth](https://www.lesswrong.com/lw/eqn/the_useful_idea_of_truth/) \\- A basic guide to what 'truth' means.\n*   [Why truth? And...](https://www.lesswrong.com/lw/go/why_truth_and/) \\- You have an instrumental motive to care about the truth of your *beliefs about* anything you care about.\n*   [Guardians of the Truth](https://www.lesswrong.com/lw/lz/guardians_of_the_truth/) \\- Endorsing a concept of truth is not the same as endorsing a particular belief as eternally, absolutely, knowably true.\n*   [Feeling Rational](https://www.lesswrong.com/lw/hp/feeling_rational/) \\- Emotions cannot be true or false, but they can follow from true or false beliefs.\n*   [The Meditation on Curiosity](https://www.lesswrong.com/lw/jz/the_meditation_on_curiosity/) \\- In particular, the [Litany of Tarski](https://www.lesswrong.com/tag/litany-of-tarski).\n*   [Fake Norms, or \"Truth\" vs. Truth](https://www.lesswrong.com/lw/sf/fake_norms_or_truth_vs_truth/) \\- Our society has a moral norm for applauding \"truth\", but actual truths get much less applause (this is a bad thing).\n\nExternal links\n--------------\n\n*   [The Simple Truth](http://yudkowsky.net/rational/the-simple-truth)\n\nSee also\n--------\n\n*   [Epistemic rationality](https://wiki.lesswrong.com/wiki/Epistemic_rationality)\n*   [Litany of Tarski](https://www.lesswrong.com/tag/litany-of-tarski), [Litany of Gendlin](https://www.lesswrong.com/tag/litany-of-gendlin)\n*   [Self-deception](https://www.lesswrong.com/tag/self-deception)\n*   [Belief](https://www.lesswrong.com/tag/belief)\n*   [Highly Advanced Epistemology 101 for Beginners](https://www.lesswrong.com/tag/highly-advanced-epistemology-101-for-beginners)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "DigEmY3RrF3XL5cwe",
    "name": "Q&A (format)",
    "core": false,
    "slug": "q-and-a-format",
    "oldSlugs": null,
    "postCount": 31,
    "description": {
      "markdown": "Posts in the format of **Question and Answers (Q&A)**, usually on some specific topic.\n\nThis includes both question and answer style [interviews](interviews) between actual people, and essays formatted as question and answer sessions between fictional people."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "irYLXtT9hkPXoZqhH",
    "name": "Growth Stories",
    "core": false,
    "slug": "growth-stories",
    "oldSlugs": null,
    "postCount": 56,
    "description": {
      "markdown": "Recollections of personal progress, lessons learned, memorable experiences, coming of age, in autobiographical form.\n\n**Sequences:** \n\n*   [Yudkowsky's Coming of Age](https://www.lesswrong.com/s/SXurf2mWFw8LX2mkG)\n\n**Related Pages:** [Postmortems & Retrospectives](https://www.lesswrong.com/tag/postmortems-and-retrospectives), [Updated Beliefs (examples of)](https://www.lesswrong.com/tag/updated-beliefs-examples-of), [Self Improvement](https://www.lesswrong.com/tag/self-improvement), [Progress Studies](https://www.lesswrong.com/tag/progress-studies) (society level)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "8hPTCJbwJnLBmfpCX",
    "name": "Calibration",
    "core": false,
    "slug": "calibration",
    "oldSlugs": [
      "calibration-probability"
    ],
    "postCount": 46,
    "description": {
      "markdown": "Someone is **well-calibrated** if the things they predict with X% chance of happening in fact occur X% of the time. Importantly, calibration is _not the same as accuracy._ Calibration is about accurately assessing how good your predictions are, not making good predictions. Person A, whose predictions are marginally better than chance (60% of them come true when choosing from two options) and who is precisely 60% confident in their choices, is perfectly calibrated. In contrast, Person B, who is 99% confident in their predictions, and right 90% of the time, is more _accurate_ than Person A, but less _well-calibrated_.\n\n_See also: [Betting](https://www.lesswrong.com/tag/betting?showPostCount=true&useTagName=true), [Epistemic Modesty](https://www.lesswrong.com/tag/epistemic-modesty?showPostCount=true&useTagName=true), [Forecasting & Prediction](https://www.lesswrong.com/tag/forecasting-and-prediction)_\n\nBeing well-calibrated has value for rationalists separately from accuracy. Among other things, being well-calibrated lets you make good [bets](https://www.lesswrong.com/tag/betting) / [make good decisions](https://www.lesswrong.com/tag/planning-and-decision-making), communicate information helpfully to others if they know you to be well-calibrated (See [Group Rationality](https://www.lesswrong.com/tag/group-rationality)), and helps prioritize [which information is worth acquiring](https://www.lesswrong.com/tag/value-of-information).\n\nNote that all expressions of quantified confidence in [beliefs](https://www.lesswrong.com/tag/anticipated-experiences?showPostCount=false&useTagName=false) can be well- or poorly- calibrated. For example, calibration applies to whether a person's 95% confidence intervals captures the true outcome 95% of the time."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "b9FzogZE4pAGfo5bY",
    "name": "Urban Planning / Design",
    "core": false,
    "slug": "urban-planning-design",
    "oldSlugs": null,
    "postCount": 11,
    "description": {
      "markdown": "**Urban Planning / Design** refers to ideas and plans for how to better approach the design of urban environments (such as cities)."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "8SfkJYYMe75MwjHzN",
    "name": "Summaries",
    "core": false,
    "slug": "summaries",
    "oldSlugs": null,
    "postCount": 84,
    "description": {
      "markdown": "**Summaries** of papers, books, Sequences or anything else.\n\nSee also: [Book Reviews](https://www.lesswrong.com/tag/book-reviews), [Literature Reviews](https://www.lesswrong.com/tag/literature-reviews)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "NXn3MSft8kzmMJbeg",
    "name": "Category Theory",
    "core": false,
    "slug": "category-theory",
    "oldSlugs": [
      "category-theory"
    ],
    "postCount": 15,
    "description": {
      "markdown": "**Category Theory** is a subfield of Mathematics studying mathematical structures and their conservation through various transformation. It emerged in the study of algebraic topology, then went on to apply to most parts of mathematics.\n\nDespite some work on [applied category theory](https://www.appliedcategorytheory.org/), it's usefulness for problem solving (especially in topics close to LW, such as [Rationality](https://www.lesswrong.com/tag/rationality) and [AI Safety](https://www.alignmentforum.org/)) is still controversial. One possible explanation is that category theory usually comes after the results, to structure them and find the links with previous results in other fields."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "fR7QfYx4JA3BnptT9",
    "name": "Skill Building",
    "core": false,
    "slug": "skill-building",
    "oldSlugs": null,
    "postCount": 53,
    "description": {
      "markdown": "**Skill Building** is the meta-skill of getting good at things i.e. developing procedural knowledge.\n\nSubtopics may include: Fast feedback loops, Choosing the correct level of challenge, studying people better than you to pick up their techniques or assimilate their style or philosophy, synthesizing advice from many tutorials, instructions, or how-tos"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "4Man2iP6ftuTPze9K",
    "name": "Modeling People",
    "core": false,
    "slug": "modeling-people",
    "oldSlugs": null,
    "postCount": 16,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "Tg9aFPFCPBHxGABRr",
    "name": "Life Improvements",
    "core": false,
    "slug": "life-improvements",
    "oldSlugs": null,
    "postCount": 48,
    "description": {
      "markdown": "Life-hacks, eliminating trivial inconveniences, process improvements, purchases that save you a minute a day, etc\n\nFound most often on posts under Practical"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "jzd84f2H95DeyHvxE",
    "name": "Prepping",
    "core": false,
    "slug": "prepping",
    "oldSlugs": null,
    "postCount": 18,
    "description": {
      "markdown": "**Prepping** is the act of actively preparing for emergencies unexpected crisis."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "T57Qd9J3AfxmwhQtY",
    "name": "Meetups & Local Communities (topic)",
    "core": false,
    "slug": "meetups-and-local-communities-topic",
    "oldSlugs": [
      "meetups-meta",
      "meetups-topic",
      "local-communities-and-meetups-topic"
    ],
    "postCount": 72,
    "description": {
      "markdown": "The rationalist community has chapters all over the world, the oldest being the NYC community, which has been around since 2009. Many of these groups are centered around regular meetups, or call themselves 'meetups'.\n\nThese are posts about meetups and local communities in general, not about specific communities (unless they also provide general insight).  \n  \nFor example - [What are meetups actually trying to accomplish?](https://www.lesswrong.com/posts/bDnFhJBcLQvCY3vJW/what-are-meetups-actually-trying-to-accomplish)\n\nFor specific meetups see [Events(community)](https://www.lesswrong.com/tag/events-community)\n\nSee also - [Community](https://www.lesswrong.com/tag/community)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "aa3Qg7Qrp9LM7QMaz",
    "name": "Definitions",
    "core": false,
    "slug": "definitions",
    "oldSlugs": null,
    "postCount": 33,
    "description": {
      "markdown": "Posts that attempt to **Define** or clarify the meaning of a concept, a word, phrase, or something else."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "aAXgXTG7SNwfC5mr5",
    "name": "Weirdness Points",
    "core": false,
    "slug": "weirdness-points",
    "oldSlugs": null,
    "postCount": 8,
    "description": {
      "markdown": "**Weirdness Points** posit that the ability to convince people to believe, do, or respect something that sounds weird is a limited resource.\n\nSomeone with dozens of unconventional beliefs and habits will seem too eccentric to be worth paying attention to, but someone who pushes one weird thing while otherwise being respectable has a shot at convincing the people around them to take it seriously too."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "be2Mh2bddQ6ZaBcti",
    "name": "Prisoner's Dilemma",
    "core": false,
    "slug": "prisoner-s-dilemma",
    "oldSlugs": null,
    "postCount": 53,
    "description": {
      "markdown": "The **Prisoner's Dilemma** is a well-studied game in [game theory](http://lesswrong.com/tag/game-theory), where supposedly rational incentive following leads to both players stabbing each other in the back and being worse off than if they had cooperated.\n\nThe original formulation, via [Wikipedia](https://en.wikipedia.org/wiki/Prisoner%27s_dilemma):\n\n> Two members of a criminal gang are arrested and imprisoned. Each prisoner is in solitary confinement with no means of communicating with the other. The prosecutors lack sufficient evidence to convict the pair on the principal charge, but they have enough to convict both on a lesser charge. Simultaneously, the prosecutors offer each prisoner a bargain. Each prisoner is given the opportunity either to betray the other by testifying that the other committed the crime, or to cooperate with the other by remaining silent. The possible outcomes are:\n\n> If A and B each betray the other, each of them serves two years in prison\n\n> If A betrays B but B remains silent, A will be set free and B will serve three years in prison\n\n> If A remains silent but B betrays A, A will serve three years in prison and B will be set free\n\n> If A and B both remain silent, both of them will serve only one year in prison (on the lesser charge).\n\nThe \"stay silent\" option is generally called **Cooperate**, and the \"betray\" option is called **Defect**. The only Nash Equilibrium of the Prisoner's Dilemma is both players defecting, even though each would prefer the cooperate/cooperate outcome.\n\nNotice that it's only if you treat the other player's decision as completely independent from yours, if the other player defects, then you score higher if you defect as well, whereas if the other player cooperates, you do better by defecting. Hence Nash Equilibrium to defect (at least if the game is to be played only once), and indeed, this is what classical causal decision theory says. And yet—and yet, if only somehow both players could agree to cooperate, they would both do better than if they both defected. If the players are [timeless decision agents](https://wiki.lesswrong.com/wiki/timeless_decision_agent), or functional decision theory agents,  they can.\n\nA popular variant is the Iterated Prisoner's Dilemma, where two agents play the Prisoner's Dilemma against each other a number of times in a row. A simple and successful strategy is called Tit for Tat - cooperate on the first round, then on subsequent rounds do whatever your opponent did on the last round.\n\nExternal links\n--------------\n\n*   [Prisoner's dilemma](http://plato.stanford.edu/entries/prisoner-dilemma/) (Stanford Encyclopedia of Philosophy)\n\nSee also\n--------\n\n*   [Game theory](https://wiki.lesswrong.com/wiki/Game_theory)\n*   [Decision theory](https://wiki.lesswrong.com/wiki/Decision_theory)\n*   [Newcomb's problem](https://wiki.lesswrong.com/wiki/Newcomb%27s_problem)\n*   [Counterfactual mugging](https://wiki.lesswrong.com/wiki/Counterfactual_mugging)\n*   [Parfit's hitchhiker](https://wiki.lesswrong.com/wiki/Parfit%27s_hitchhiker)\n*   [Smoking lesion](https://wiki.lesswrong.com/wiki/Smoking_lesion)\n*   [Absentminded driver](https://wiki.lesswrong.com/wiki/Absentminded_driver)\n*   [Pascal's mugging](https://wiki.lesswrong.com/wiki/Pascal%27s_mugging)\n*   [Coordination/Cooperation](https://www.lesswrong.com/tag/coordination-cooperation)\n\nReferences\n----------\n\n*   Drescher, Gary (2006). *Good and Real*. Cambridge: The MIT Press. ISBN 0262042339."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "GY5kPPpCoyt9fnTMn",
    "name": "Computer Science",
    "core": false,
    "slug": "computer-science",
    "oldSlugs": null,
    "postCount": 62,
    "description": {
      "markdown": "The study of computers and algorithms from both practical and theoretical standpoints. Often considered to be a subset of mathematics, particularly its more theoretical considerations. A thorough understanding of computer science is necessary to understand AI, and indeed, the modern world."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "56yXXrcxRjrQs6z9R",
    "name": "Transparency / Interpretability (ML & AI)",
    "core": false,
    "slug": "transparency-interpretability-ml-and-ai",
    "oldSlugs": [
      "machine-learning-transparency-interpretability"
    ],
    "postCount": 94,
    "description": {
      "markdown": "**Transparency and interpretability** is the ability for the decision processes and inner workings of AI and machine learning systems to be understood by humans or other outside observers.\n\nPresent-day machine learning systems are typically not very transparent or interpretable. You can use a model's output, but the model can't tell you why it made that output. This makes it hard to determine the cause of biases in ML models."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "4kQXps8dYsKJgaayN",
    "name": "Careers",
    "core": false,
    "slug": "careers",
    "oldSlugs": null,
    "postCount": 96,
    "description": {
      "markdown": "Posts relating to jobs, career development, etc."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "2eG84YuMhatBpBXPJ",
    "name": "Indexical Information",
    "core": false,
    "slug": "indexical-information",
    "oldSlugs": null,
    "postCount": 1,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "wMPYFGmhcFg4bSb4Z",
    "name": "Map and Territory",
    "core": false,
    "slug": "map-and-territory",
    "oldSlugs": null,
    "postCount": 50,
    "description": {
      "markdown": "Models of reality are often mistaken for reality itself, and clarifying the distinction is an important rationalist technique."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "N5JGtFnhex2DbyPvy",
    "name": "Privacy",
    "core": false,
    "slug": "privacy",
    "oldSlugs": null,
    "postCount": 23,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5oDii8KKW53n4HSx4",
    "name": "Hansonian Pre-Rationality",
    "core": false,
    "slug": "hansonian-pre-rationality",
    "oldSlugs": null,
    "postCount": 8,
    "description": {
      "markdown": "In defining **Hansonian Pre-Rationality** Robin Hanson offers an intriguing argument that, upon learning that our beliefs were created by an irrational process (be it a religious upbringing or a genetic predisposition to paranoid depression), we should update to agree with the alternate version of ourselves who could have had different beliefs. Agents who agree with alternate selves in this way are \"pre-rational\". (NOTE: not to be confused with \"pre-rational\" meaning \"not yet rational\" or \"less than rational\".)\n\nSuppose you are an AI who was designed by a drunk programmer. Your prior contains an \"optimism\" parameter which broadly skews how you see the world -- set it to -100 and you'd expect world-ending danger around every corner, while +100 would make you expect heaven around every corner. Although your powerful learning algorithm allows you to accurately predict the world, the optimism/pessimism bias never fully goes away: it skews your views about anything you *don't* know.\n\nUnfortunately for you, your programmer set the parameter randomly, rather than attempting to figure out which setting was most accurate or useful. You know for a fact they just mashed the num pad randomly.\n\nHow should you think about this?"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "frcrRgCk9PDbEScua",
    "name": "Climate Change",
    "core": false,
    "slug": "climate-change",
    "oldSlugs": null,
    "postCount": 34,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "22z6XpWKqw3bNv4oR",
    "name": "Simulation Hypothesis",
    "core": false,
    "slug": "simulation-hypothesis",
    "oldSlugs": null,
    "postCount": 37,
    "description": {
      "markdown": "The **Simulation Hypothesis** proposes that conscious beings could be immersed within an artificial Universe embedded within a higher order of reality. The roots of this argument can be found throughout the history of philosophy in such works as Plato's \"[Allegory of the Cave](https://en.wikipedia.org/wiki/The_Allegory_of_the_Cave)\" and Descartes \"[evil demon](https://en.wikipedia.org/wiki/Evil_demon)\".\n\nThe important distinction between these and modern [Simulation Arguments](https://www.lesswrong.com/tag/simulation-argument) has been the addition of proposed methods of engineering Simulated Reality through the use of computers. The modern [Simulation Argument](https://www.lesswrong.com/tag/simulation-argument) makes the case that since a civilization will be able to simulate many more ancient civilizations than there were ancient civilizations, it is more likely that we are a been simulated than not. It shows that the belief that there is a significant chance that we will one day become posthumans who run ancestor simulations is false, unless we are currently living in a simulation.\n\nJohn Barrow [has suggested](http://www.simulation-argument.com/barrowsim.pdf) that if we are living in a computer simulation we may observe \"glitches\" in the our programmed environment due to the level of detail being compromised to save computing power. Alternatively, the Simulators may not have a full understanding of the Laws of Nature which would mean over time the simulated environment would drift away from its stable state. These \"glitches\" could be identified by scientists scrutinizing nature using unusual methods of observation. However, [Nick Bostrom](https://www.lesswrong.com/tag/nick-bostrom) [argues](http://www.simulation-argument.com/simulation.pdf) that it is extremely likely that a civilization will have far surpassing computational powers than the ones needed to simulate an ancient civilization in great detail. Moreover, one can argue that due to exponential grow, it's extremely unlikely that the simulators are in the region of progress where they already can simulate an artificial reality but can't simulate it with finer detail. They either can't simulate at all, or have computational powers that far exceed the needed amount.\n\nExternal links\n--------------\n\n*   Barrow, John (2008) [Living in a Simulated Universe](http://www.simulation-argument.com/barrowsim.pdf) Universe or Multiverse? ed. Bernard Carr (Cambridge University Press): pp. 481-486.\n*   [Is God an Alien Mathematician?](http://hplusmagazine.com/2011/01/18/is-god-an-alien-mathematician/) — A discussion between Ben Goertzel and Hugo de Garis on Simulated Universes and their Creators\n*   [From cosmism to deism](http://www.kurzweilai.net/from-cosmism-to-deism) — Hugo de Garis's essay on Simulated Universes\n*   [The Allegory of the Cave](https://en.wikipedia.org/wiki/Allegory_of_the_Cave) on Wikipedia\n*   [Evil demon](https://en.wikipedia.org/wiki/Evil_demon) on Wikipedia\n\nSee also\n--------\n\n*   [Simulation Argument](https://www.lesswrong.com/tag/simulation-argument)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5Whwix4cZ3p5otshm",
    "name": "Habits",
    "core": false,
    "slug": "habits",
    "oldSlugs": [
      "habit"
    ],
    "postCount": 38,
    "description": {
      "markdown": "A **habit** is a routine of behavior that is repeated regularly and tends to occur subconsciously. Creating and maintaining useful habits is a core rationality practice as is getting rid of unhelpful habits."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "cPFuhAE7PwoKF7yTj",
    "name": "Inverse Reinforcement Learning",
    "core": false,
    "slug": "inverse-reinforcement-learning",
    "oldSlugs": null,
    "postCount": 27,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "xXX3n22DQZuKqXEdT",
    "name": "War",
    "core": false,
    "slug": "war",
    "oldSlugs": null,
    "postCount": 77,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "EdDGrAxYcrXnKkDca",
    "name": "Distillation & Pedagogy",
    "core": false,
    "slug": "distillation-and-pedagogy",
    "oldSlugs": [
      "distillation",
      "distillation-and-pedagogy"
    ],
    "postCount": 81,
    "description": {
      "markdown": "**Distillation** is the process of taking a complex subject, and making it easier to understand. **Pedagogy** is the method and practice of teaching. A good intellectual pipeline requires not just discovering new ideas, but making it easier for newcomers to learn them, stand on the shoulders of giants, and discover even more ideas.\n\nChris Olah, founder of [distill.pub](https://distill.pub/), writes in his essay [Research Debt](https://distill.pub/2017/research-debt/):\n\n> Programmers talk about technical debt: there are ways to write software that are faster in the short run but problematic in the long run. Managers talk about institutional debt: institutions can grow quickly at the cost of bad practices creeping in. Both are easy to accumulate but hard to get rid of.\n> \n> Research can also have debt. It comes in several forms:\n> \n> *   **Poor Exposition** – Often, there is no good explanation of important ideas and one has to struggle to understand them. This problem is so pervasive that we take it for granted and don’t appreciate how much better things could be.\n> *   **Undigested Ideas** – Most ideas start off rough and hard to understand. They become radically easier as we polish them, developing the right analogies, language, and ways of thinking.\n> *   **Bad abstractions and notation** – Abstractions and notation are the user interface of research, shaping how we think and communicate. Unfortunately, we often get stuck with the first formalisms to develop even when they’re bad. For example, an object with extra electrons is negative, and pi is wrong.\n> *   **Noise** – Being a researcher is like standing in the middle of a construction site. Countless papers scream for your attention and there’s no easy way to filter or summarize them.Because most work is explained poorly, it takes a lot of energy to understand each piece of work. For many papers, one wants a simple one sentence explanation of it, but needs to fight with it to get that sentence. Because the simplest way to get the attention of interested parties is to get everyone’s attention, we get flooded with work. Because we incentivize people being “prolific,” we get flooded with a lot of work… We think noise is the main way experts experience research debt.\n> \n> The insidious thing about research debt is that it’s normal. Everyone takes it for granted, and doesn’t realize that things could be different. For example, it’s normal to give very mediocre explanations of research, and people perceive that to be the ceiling of explanation quality. On the rare occasions that truly excellent explanations come along, people see them as one-off miracles rather than a sign that we could systematically be doing better.\n\nSee also [Scholarship and Learning](lesswrong.com/tag/scholarship-and-learning), and [Good Explanations](https://www.lesswrong.com/tag/good-explanations-advice)."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "bmfs4jiLaF6HiiYkC",
    "name": "Reductionism",
    "core": false,
    "slug": "reductionism",
    "oldSlugs": null,
    "postCount": 32,
    "description": {
      "markdown": "**Reductionism** is a disbelief that the higher levels of simplified multilevel models are out there in the [territory](https://wiki.lesswrong.com/wiki/territory), that concepts constructed by mind in themselves play a role in the behavior of reality. This doesn't contradict the notion that the concepts used in simplified multilevel models refer to the actual clusters of configurations of reality.\n\nSee also\n--------\n\n*   [Reductionism (sequence)](https://www.lesswrong.com/tag/reductionism-sequence)\n*   [Universal law](https://www.lesswrong.com/tag/universal-law), [Magic](https://www.lesswrong.com/tag/magic)\n*   [Mind projection fallacy](https://www.lesswrong.com/tag/mind-projection-fallacy)\n*   [How an algorithm feels](https://www.lesswrong.com/tag/how-an-algorithm-feels)\n*   [Free will](https://www.lesswrong.com/tag/free-will)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "BAhM42jvzuWMzTDxR",
    "name": "Depression",
    "core": false,
    "slug": "depression",
    "oldSlugs": null,
    "postCount": 26,
    "description": {
      "markdown": "**Depression** is a psychological disorder characterized by low mood, loss of interest in life, and poor self-esteem. Both cognitive behavioral therapy (CBT) and antidepressants such as SSRIs have proven to be effective treatments."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "io2ExA7GEeTgHFTFW",
    "name": "Center on Long-Term Risk (CLR)",
    "core": false,
    "slug": "center-on-long-term-risk-clr",
    "oldSlugs": null,
    "postCount": 13,
    "description": {
      "markdown": "The **Center on Long-Term Risk**, formerly _Foundational Research Institute,_ is an [effective altruist](https://www.lesswrong.com/tag/effective-altruism) is a research group affiliated with the Swiss/German [Effective Altruism Foundation](https://ea-foundation.org/). It investigates cooperative strategies to reduce [risks of astronomical suffering](https://www.lesswrong.com/tag/risks-of-astronomical-suffering-s-risks) in humanity's future (s-risks). This includes not only (post-)human suffering, but also the suffering of non-human animals and potential digital sentience. Their research is interdisciplinary, drawing on insights from [artificial intelligence](https://www.lesswrong.com/tag/artificial-general-intelligence), [anthropic reasoning](https://wiki.lesswrong.com/wiki/anthropic_reasoning), international relations, sociology, philosophy, and other fields.\n\n**See also**\n------------\n\n*   [Suffering risk](https://www.lesswrong.com/tag/risks-of-astronomical-suffering-s-risks)\n*   [Abolitionism](https://www.lesswrong.com/tag/abolitionism)\n\n**External links**\n------------------\n\n*   [CLR website](https://longtermrisk.org/)\n*   [Effective Altruism Wiki article on FRI](http://archive.is/aZjiv)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "EewHHv3ewvQ3mqbyb",
    "name": "Law-Thinking",
    "core": false,
    "slug": "law-thinking",
    "oldSlugs": null,
    "postCount": 19,
    "description": {
      "markdown": "**Law-thinking** is an approach in which action and reasoning are thought to have theoretical criteria (laws) specifying the optimal actions and belief adjustments in any given situation. These criteria may be impossible to apply to a situation directly, and one may be forced to use only rough approximations. But one can still evaluate the approximations based on how well they match the optimal criteria.\n\nThe relationship between laws and approximations resembles that of between physics and engineering. Physics specify the laws by which the world works, while engineering tries to find practical solutions as constrained by those laws.\n\nSome concepts which have been used as theoretical criteria in law-thinking:\n\n*   [Bayes Theorem](https://www.lesswrong.com/tag/bayes-theorem?useTagName=true) is a law of probability that describes the proper way to incorporate new evidence into prior probabilities to form an updated probability estimate.\n*   [Decision Theory](https://www.lesswrong.com/tag/decision-theory?useTagName=true) studies the general laws for choosing between actions in any given situation.\n*   [Solomonoff Induction](https://www.lesswrong.com/tag/solomonoff-induction?useTagName=true) is a theoretically optimal way of arriving at true beliefs, though impossible to use directly. [AIXI](https://www.lesswrong.com/tag/aixi?useTagName=true) is an AI design based on Solomonoff Induction; it is also impossible to build directly, but some approximations exist.\n\nNote that one can make use of e.g. Bayes Theorem or decision theory without being a law-thinker. Thus, articles covering the above topics do not automatically fall under this tag. A \"toolbox-thinker\" may use such tools if that seems warranted, _without_ considering them normative standards to compare things against. This difference is discussed in [Toolbox-thinking and Law-thinking](https://www.lesswrong.com/posts/CPP2uLcaywEokFKQG/toolbox-thinking-and-law-thinking)."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "jiuackr7B5JAetbF6",
    "name": "Transhumanism",
    "core": false,
    "slug": "transhumanism",
    "oldSlugs": null,
    "postCount": 53,
    "description": {
      "markdown": "**Transhumanism** is the belief or movement in favour of human enhancement, especially beyond current human limitations and with advanced technology such as AI, cognitive enhancement, and life extension.\n\nReferences\n----------\n\n*   [Transhumanism as Simplified Humanism](http://yudkowsky.net/singularity/simplified) by [Eliezer Yudkowsky](https://www.lesswrong.com/tag/eliezer-yudkowsky)\n*   A [TED talk](http://www.ted.com/talks/nick_bostrom_on_our_biggest_problems.html) by transhumanist [Nick Bostrom](https://www.lesswrong.com/tag/nick-bostrom) on humanity's biggest problems\n*   [The Transhumanist FAQ](http://www.nickbostrom.com/views/transhumanist.pdf) (PDF) by [Nick Bostrom](https://www.lesswrong.com/tag/nick-bostrom) ([HTML version](http://whatistranshumanism.org/))\n*   [Transhumanist Values](http://www.nickbostrom.com/ethics/values.html) by [Nick Bostrom](https://www.lesswrong.com/tag/nick-bostrom)\n*   [A History of Transhumanist Thought](http://www.nickbostrom.com/papers/history.pdf) (PDF) by [Nick Bostrom](https://www.lesswrong.com/tag/nick-bostrom)\n*   [Seven Definitions of Transhumanism](https://web.archive.org/web/20130115205756/http://www.acceleratingfuture.com/michael/blog/2007/09/seven-definitions-of-transhumanism/) by [Michael Anissimov](https://www.lesswrong.com/tag/michael-anissimov)\n*   [PostHuman: An Introduction to Transhumanism](https://www.youtube.com/watch?v=bTMS9y8OVuY) (video)\n\nSee Also\n--------\n\n*   [H+Pedia](https://wiki.lesswrong.com/wiki/H+Pedia)\n*   [Fun theory](https://www.lesswrong.com/tag/fun-theory)\n*   [Metaethics sequence](https://www.lesswrong.com/tag/metaethics-sequence)\n*   [Mind uploading](https://www.lesswrong.com/tag/mind-uploading)\n*   [Cryonics](https://www.lesswrong.com/tag/cryonics)\n*   [Friendly AI](https://wiki.lesswrong.com/wiki/Friendly_AI)\n*   [Abolitionism](https://www.lesswrong.com/tag/abolitionism)\n\nExternal links\n--------------\n\n*   [H+Pedia, the transhumanist wiki](https://hpluspedia.org/)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "rWzGNdjuep56W5u2d",
    "name": "Inside/Outside View",
    "core": false,
    "slug": "inside-outside-view",
    "oldSlugs": null,
    "postCount": 46,
    "description": {
      "markdown": "An **Inside View** on a topic involves making predictions based on your understanding of the details of the process. An **Outside View** involves ignoring these details and using an estimate based on a class of roughly similar previous cases (alternatively, this is called [reference class forecasting](http://en.wikipedia.org/wiki/Reference_class_forecasting)), though it has been [pointed out](https://www.lesswrong.com/posts/BcYfsi7vmhDvzQGiF/taboo-outside-view) that the possible meaning has expanded beyond that.\n\nFor example, someone working on a project may estimate that they can reasonably get 20% of it done per day, so they will get it done in five days (inside view). Or they might consider that all of their previous projects were completed just before the deadline, so since the deadline for this project is in 30 days, that's when it will get done (outside view).\n\nThe terms were originally developed by Daniel Kahneman and Amos Tversky. An early use is in [Timid Choices and Bold Forecasts: A Cognitive Perspective on Risk Taking (Kahneman & Lovallo, 1993)](http://doi.org/10.1287/mnsc.39.1.17) and the terms were popularised in *Thinking, Fast and Slow* (Kahneman, 2011; [relevant excerpt](https://www.mckinsey.com/business-functions/strategy-and-corporate-finance/our-insights/daniel-kahneman-beware-the-inside-view)). The planning example is discussed in [The Planning Fallacy](https://www.lesswrong.com/posts/CPm5LTwHrvBJCa9h5/planning-fallacy). \n\n### Examples of outside view\n\n**1.** From [Beware the Inside View](https://www.overcomingbias.com/2007/07/beware-the-insi.html), by Robin Hanson:\n\n> I did 1500 piece jigsaw puzzle of fireworks, my first jigsaw in at least ten years.  Several times I had the strong impression that I had carefully eliminated every possible place a piece could go, or every possible piece that could go in a place.  I was very tempted to conclude that many pieces were missing, or that the box had extra pieces from another puzzle.  This wasn’t impossible – the puzzle was an open box a relative had done before.  And the alternative seemed humiliating. \n\n> But I allowed a very different part of my mind, using different considerations, to overrule this judgment; so many extra or missing pieces seemed unlikely.  And in the end there was only one missing and no extra pieces.  I recall a similar experience when I was learning to program. I would carefully check my program and find no errors, and then when my program wouldn’t run I was tempted to suspect compiler or hardware errors.  Of course the problem was almost always my fault.   \n\n**2.** Japanese students expected to finish their essays an average of 10 days before deadline. The average completion time was actually 1 day before deadline. When asked when they'd completed similar, previous tasks, the average reply was 1 day before deadline\\[1\\].\n\n**3.** Students instructed to visualize how, where, and when they would perform their Christmas shopping, expected to finish shopping more than a week before Christmas. A control group asked when they expected their Christmas shopping to be finished, expected it to be done 4 days before Christmas. Both groups finished 3 days before Christmas\\[2\\].\n\n### Problems with the outside view\n\nIt is controversial how far the lesson of these experiments can be extended. Robin Hanson argues that this implies that, in futurism, forecasts should be made by trying to find a reference class of similar cases, rather than by trying to visualize outcomes. Eliezer Yudkowsky responds that this leads to \"reference class tennis\" wherein people feel that the same event 'obviously' belongs to two different reference classes, and that the above experiments were performed in cases where the new example was highly similar to past examples. I.e., this year's Christmas shopping optimism and last year's Christmas shopping optimism are much more similar to one another, than the invention of the Internet is to the invention of agriculture. If someone else then feels that the invention of the Internet is more like the category 'recent communications innovations' and should be forecast by reference to television instead of agriculture, both sides pleading the outside view has no resolution except \"I'm taking my reference class and going home!\"\n\nMore possible limitations and problems with using the outside view are discussed in [The Outside View's Domain](https://www.lesswrong.com/posts/pqoxE3AGMbse68dvb/the-outside-view-s-domain) and [\"Outside View\" as Conversation-Halter](https://www.lesswrong.com/posts/FsfnDfADftGDYeG4c/outside-view-as-conversation-halter). [Model Combination and Adjustment](https://www.lesswrong.com/posts/iyRpsScBa6y4rduEt/model-combination-and-adjustment) discusses the implications of there usually existing multiple *different* outside views. [Taboo \"Outside View\"](https://www.lesswrong.com/posts/BcYfsi7vmhDvzQGiF/taboo-outside-view) argues that the meaning of \"Outside View\" have expanded too much, and that it should be [tabooed](https://www.lesswrong.com/tag/rationalist-taboo) and replaced with more precise terminology. An alternative to \"inside/outside view\" has been proposed in [Gears Level & Policy Level](https://www.lesswrong.com/s/uLEjM2ij5y3CXXW6c/p/vKbAWFZRDBhyD6K6A).\n\nExternal Posts\n--------------\n\n*   [Beware the Inside View](http://www.overcomingbias.com/2007/07/beware-the-insi.html) by [Robin Hanson](https://lessestwrong.com/tag/robin-hanson)\n\nSee Also\n--------\n\n*   [Planning fallacy](https://lessestwrong.com/tag/planning-fallacy)\n*   [Modest Epistemology](https://www.lesswrong.com/tag/modest-epistemology)\n*   [Near/far thinking](https://lessestwrong.com/tag/near-far-thinking)\n*   [Connotation](https://lessestwrong.com/tag/connotation), [Absurdity heuristic](https://lessestwrong.com/tag/absurdity-heuristic)\n*   [Arguing by analogy](https://lessestwrong.com/tag/arguing-by-analogy)\n*   [Intelligence explosion](https://lessestwrong.com/tag/intelligence-explosion), [The Hanson-Yudkowsky AI-Foom Debate](https://lessestwrong.com/tag/the-hanson-yudkowsky-ai-foom-debate)\n\n\\[1\\] Buehler, R., Griffin, D., & Ross, M. 2002. Inside the planning fallacy: The causes and consequences of optimistic time predictions. Heuristics and biases: The psychology of intuitive judgment, 250-270. Cambridge, UK: Cambridge University Press.\n\n\\[2\\] Buehler, R., Griffin, D. and Ross, M. 1995. It's about time: Optimistic predictions in work and love. European Review of Social Psychology, Volume 6, eds. W. Stroebe and M. Hewstone. Chichester: John Wiley & Sons."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "hrezrpGqXXdSe76ks",
    "name": "Ambition",
    "core": false,
    "slug": "ambition",
    "oldSlugs": null,
    "postCount": 38,
    "description": {
      "markdown": "**Ambition.** Because they don't think they could have an impact. Because they were always told ambition was dangerous. To get to the other side.\n\nQuestions to all these answers and more in the ambition tag!\n\n> _Never confess to me that you are just as flawed as I am unless you can tell me what you plan to do about it. Afterward you will still have plenty of flaws left, but that’s not the point; the important thing is to do better, to keep moving ahead, to take one more step forward. Tsuyoku naritai!_\n\n_\\-\\-_ [Eliezer Yudkowsky](https://www.lesswrong.com/posts/DoLQN5ryZ9XkZjq5h/tsuyoku-naritai-i-want-to-become-stronger)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "Pa2SdZsLFmqhs42Do",
    "name": "Open Problems",
    "core": false,
    "slug": "open-problems",
    "oldSlugs": null,
    "postCount": 22,
    "description": {
      "markdown": "[**Open problems**](http://en.wikipedia.org/wiki/Open_problems) (or open questions) are the things in a field that haven't yet been figured out. Gathering them all in the same place can help to see on what's the frontier of that field or find things to work on.\n\nSee also [Hamming Questions](https://www.lesswrong.com/tag/hamming-questions)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "zv7v2ziqexSn5iS9v",
    "name": "Group Rationality",
    "core": false,
    "slug": "group-rationality",
    "oldSlugs": null,
    "postCount": 74,
    "description": {
      "markdown": "In almost anything, individuals are inferior to groups. Several articles address this concern regarding rationality, i.e., the topic of **Group Rationality**.\n\nExternal links\n--------------\n\n*   [Open Problems in Group Rationality](https://medium.com/@ThingMaker/open-problems-in-group-rationality-5636440a2cd1) is one of the main articles about the subject.\n*   [Individual and Group Behavior in the Ultimatum Game](http://ratio.huji.ac.il/dp/dp154.pdf)\n*   [Individual vs. Group Rationality in Inquiry](http://www.andrew.cmu.edu/user/kzollman/research/Presentations/LRR%20-%20IndividualVsSocial.pdf)\n\nSee also\n--------\n\n*   [Rationality as martial art](https://www.lesswrong.com/tag/rationality-as-martial-art)\n*   [Problem of verifying rationality](https://www.lesswrong.com/tag/problem-of-verifying-rationality)\n*   [Less Wrong meetup groups](/community)\n*   [The Craft and the Community](https://www.lesswrong.com/tag/the-craft-and-the-community)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "ZWmB62xB6uLyRuAtX",
    "name": "Wiki/Tagging",
    "core": false,
    "slug": "wiki-tagging",
    "oldSlugs": [
      "tagging"
    ],
    "postCount": 27,
    "description": {
      "markdown": "Posts about Lesswrong's **Wiki/Tagging** feature or wiki-tag features in general."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "rfZ6DY88ApBDXFpyW",
    "name": "Human Bodies",
    "core": false,
    "slug": "human-bodies",
    "oldSlugs": null,
    "postCount": 32,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "2oWPnnnzMbiAxWfbs",
    "name": "Checklists",
    "core": false,
    "slug": "checklists",
    "oldSlugs": [
      "checklist"
    ],
    "postCount": 8,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "nxwND2hTjBeCy58hi",
    "name": "Developmental Psychology",
    "core": false,
    "slug": "developmental-psychology",
    "oldSlugs": null,
    "postCount": 19,
    "description": {
      "markdown": "The field of **developmental psychology** focuses on analyzing how human minds change as they grow. Originally this focused on development from birth through childhood and ended at adulthood, but by the 1960s this had expanded to include the study of ways adult continue to develop psychologically.\n\n**Related pages:** [Parenting](https://www.lesswrong.com/tag/parenting), [Education](https://www.lesswrong.com/tag/education), [Psychology](https://www.lesswrong.com/tag/psychology)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "m4zvJHAiGBTjc5ZFt",
    "name": "Babble and Prune",
    "core": false,
    "slug": "babble-and-prune",
    "oldSlugs": null,
    "postCount": 26,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "zQw5d37qwzdpgQs5P",
    "name": "Cognitive Science",
    "core": false,
    "slug": "cognitive-science",
    "oldSlugs": null,
    "postCount": 24,
    "description": {
      "markdown": "**Cognitive science** draws upon a variety of different disciplines to try to describe and explain the way humans think. It heavily involves [neuroscience](https://www.lesswrong.com/tag/neuroscience?showPostCount=true&useTagName=true), [psychology](https://www.lesswrong.com/tag/psychology), and philosophy. It differs from neuroscience in that it focuses less on relating structure to function, and more on using many approaches to form higher-level models to predict behaviour."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "6bdeSR7aKmdSQLw6P",
    "name": "Cooking",
    "core": false,
    "slug": "cooking",
    "oldSlugs": null,
    "postCount": 24,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "R6uagTfhhBeejGrrf",
    "name": "Complexity of Value",
    "core": false,
    "slug": "complexity-of-value",
    "oldSlugs": null,
    "postCount": 58,
    "description": {
      "markdown": "**Complexity of value** is the thesis that human values have high [Kolmogorov complexity](https://wiki.lesswrong.com/wiki/Kolmogorov_complexity); that our [preferences](https://wiki.lesswrong.com/wiki/preferences), the things we care about, cannot be summed by a few simple rules, or compressed. **[Fragility of value](https://www.lesswrong.com/lw/y3/value_is_fragile/)** is the thesis that losing even a small part of the rules that make up our values could lead to results that most of us would now consider as unacceptable (just like dialing nine out of ten phone digits correctly does not connect you to a person 90% similar to your friend). For example, all of our values _except_ novelty might yield a future full of individuals replaying only one optimal experience through all eternity.\n\nRelated: [Ethics & Metaethics](https://www.lesswrong.com/tag/metaethics), [Fun Theory](https://www.lesswrong.com/tag/fun-theory), [Preference](https://www.lesswrong.com/tag/preference), [Wireheading](https://www.lesswrong.com/tag/wireheading)\n\nMany human choices can be compressed, by representing them by simple rules - the desire to survive produces innumerable actions and subgoals as we fulfill that desire. But people don't _just_ want to survive - although you can compress many human activities to that desire, you cannot compress all of human existence into it. The human equivalents of a utility function, our terminal values, contain many different elements that are not strictly reducible to one another. William Frankena offered [this list](http://plato.stanford.edu/entries/value-intrinsic-extrinsic/#WhaHasIntVal) of things which many cultures and people seem to value (for their own sake rather than strictly for their external consequences):\n\n> Life, consciousness, and activity; health and strength; pleasures and satisfactions of all or certain kinds; happiness, beatitude, contentment, etc.; truth; knowledge and true opinions of various kinds, understanding, wisdom; beauty, harmony, proportion in objects contemplated; aesthetic experience; morally good dispositions or virtues; mutual affection, love, friendship, cooperation; just distribution of goods and evils; harmony and proportion in one's own life; power and experiences of achievement; self-expression; freedom; peace, security; adventure and novelty; and good reputation, honor, esteem, etc.\n\nThe \"etc.\" at the end is the tricky part, because there may be a great many values not included on this list.\n\nSince natural selection reifies selection pressures as [psychological drives which then continue to execute](https://www.lesswrong.com/tag/adaptation-executors) [independently of any consequentialist reasoning in the organism](https://www.lesswrong.com/lw/yi/the_evolutionarycognitive_boundary/) or that organism explicitly representing, let alone caring about, the original evolutionary context, we have no reason to expect these terminal values to be reducible to any one thing, or each other.\n\nTaken in conjunction with another LessWrong claim, that all values are morally relevant, this would suggest that those philosophers who seek to do so are mistaken in trying to find cognitively tractable overarching principles of ethics. However, it is coherent to suppose that not all values are morally relevant, and that the morally relevant ones form a tractable subset.\n\nComplexity of value also runs into underappreciation in the presence of bad [metaethics](https://www.lesswrong.com/tag/metaethics). The local flavor of metaethics could be characterized as cognitivist, without implying \"thick\" notions of instrumental rationality; in other words, moral discourse can be about a coherent subject matter, without all possible minds and agents necessarily finding truths about that subject matter to be psychologically compelling. An [expected paperclip maximizer](https://www.lesswrong.com/tag/paperclip-maximizer) doesn't disagree with you about morality any more than you disagree with it about \"which action leads to the greatest number of expected paperclips\", it is just constructed to find the latter subject matter psychologically compelling but not the former. Failure to appreciate that \"But it's just paperclips! What a dumb goal! No sufficiently intelligent agent would pick such a dumb goal!\" is a judgment carried out on a local brain that evaluates paperclips as inherently low-in-the-preference-ordering means that someone will expect all moral judgments to be automatically reproduced in a sufficiently intelligent agent, since, after all, they would not lack the intelligence to see that paperclips are so obviously inherently-low-in-the-preference-ordering. This is a particularly subtle species of [anthropomorphism](https://www.lesswrong.com/tag/anthropomorphism) and [mind projection fallacy](https://www.lesswrong.com/tag/mind-projection-fallacy).\n\nBecause the human brain very often fails to grasp all these difficulties involving our values, we tend to think building an awesome future is much less problematic than it really is. Fragility of value is relevant for building [Friendly AI](https://wiki.lesswrong.com/wiki/Friendly_AI), because an [AGI](https://wiki.lesswrong.com/wiki/AGI) which does not respect human values is likely to create a world that we would consider devoid of value - not necessarily full of explicit attempts to be evil, but perhaps just a dull, boring loss.\n\nAs values are orthogonal with intelligence, they can freely vary no matter how intelligent and efficient an AGI is \\[[1](http://www.nickbostrom.com/superintelligentwill.pdf)\\]. Since human / humane values have high Kolmogorov complexity, a random AGI is highly unlikely to maximize human / humane values. The fragility of value thesis implies that a poorly constructed AGI might e.g. turn us into blobs of perpetual orgasm. Because of this relevance the complexity and fragility of value is a major theme of [Eliezer Yudkowsky](https://www.lesswrong.com/tag/eliezer-yudkowsky)'s writings.\n\nWrongly designing the future because we wrongly encoded human values is a serious and difficult to assess type of [Existential risk](https://www.lesswrong.com/tag/existential-risk). \"Touch too hard in the wrong dimension, and the physical representation of those values will shatter - _and not come back, for there will be nothing left to want to bring it back_. And the referent of those values - a worthwhile universe - would no longer have any physical reason to come into being. Let go of the steering wheel, and the Future crashes.\" \\[[2](https://www.lesswrong.com/lw/y3/value_is_fragile/)\\]\n\nComplexity of Value and AI\n--------------------------\n\nComplexity of value poses a problem for [AI alignment](http://lesswrong.com/tag/ai). If you can't easily compress what humans want into a simple function that can be fed into a computer, it isn't easy to make a powerful AI that does things humans want and doesn't do things humans don't want. [Value Learning](https://www.lesswrong.com/tag/value-learning) attempts to address this problem.\n\nMajor posts\n-----------\n\n*   [The Fun Theory Sequence](https://www.lesswrong.com/lw/xy/the_fun_theory_sequence/) describes some of the many complex considerations that determine _what sort of happiness_ we most prefer to have - given that many of us would decline to just have an electrode planted in our pleasure centers.\n*   [Thou Art Godshatter](https://www.lesswrong.com/lw/l3/thou_art_godshatter/) describes the [evolutionary psychology](https://www.lesswrong.com/tag/evolutionary-psychology) behind the complexity of human values - how they got to be complex, and why, given that origin, there is no reason in hindsight to expect them to be simple. We certainly are not built to [maximize genetic fitness](https://wiki.lesswrong.com/wiki/adaptation_executers).\n*   [Not for the Sake of Happiness (Alone)](https://www.lesswrong.com/lw/lb/not_for_the_sake_of_happiness_alone/) tackles the [Hollywood Rationality](https://www.lesswrong.com/tag/hollywood-rationality) trope that \"rational\" preferences must reduce to selfish hedonism - caring strictly about personally experienced pleasure. An ideal Bayesian agent - implementing strict Bayesian decision theory - can have a utility function that [ranges over anything, not just internal subjective experiences](https://www.lesswrong.com/lw/l4/terminal_values_and_instrumental_values/).\n*   [Fake Utility Functions](https://www.lesswrong.com/lw/lq/fake_utility_functions/) describes the seeming fascination that many have with trying to compress morality down to a single principle. The [sequence leading up](https://www.lesswrong.com/lw/lp/fake_fake_utility_functions/) to this post tries to explain the cognitive twists whereby people [smuggle](https://www.lesswrong.com/lw/ld/the_hidden_complexity_of_wishes/) all of their complicated _other_ preferences into their choice of _exactly_ which acts they try to _[justify using](https://www.lesswrong.com/lw/kq/fake_justification/)_ their single principle; but if they were _really_ following _only_ that single principle, they would [choose other acts to justify](https://www.lesswrong.com/lw/kz/fake_optimization_criteria/).\n\nOther posts\n-----------\n\n*   [Value is Fragile](https://www.lesswrong.com/lw/y3/value_is_fragile/)\n*   [The Hidden Complexity of Wishes](https://www.lesswrong.com/lw/ld/the_hidden_complexity_of_wishes/)\n*   [Fake Morality](https://www.lesswrong.com/lw/ky/fake_morality/)\n*   [Welcome to Heaven](https://www.lesswrong.com/lw/1o9/welcome_to_heaven/)\n*   [Complexity of Value ≠ Complexity of Outcome](https://www.lesswrong.com/lw/1oj/complexity_of_value_complexity_of_outcome/)\n*   [Not for the Sake of Pleasure Alone](https://www.lesswrong.com/lw/65w/not_for_the_sake_of_pleasure_alone/)\n*   [A Non-Comprehensive List of Human Values](https://casparoesterheld.com/2017/02/10/a-non-comprehensive-list-of-human-values/)\n\nSee also\n--------\n\n*   [Complex Value Systems are Required to Realize Valuable Futures](http://intelligence.org/files/ComplexValues.pdf)\n*   [Human universal](https://www.lesswrong.com/tag/human-universal)\n*   [Fake simplicity](https://www.lesswrong.com/tag/fake-simplicity)\n*   [Metaethics sequence](https://www.lesswrong.com/tag/metaethics-sequence)\n*   [Fun theory](https://www.lesswrong.com/tag/fun-theory)\n*   [Magical categories](https://www.lesswrong.com/tag/magical-categories)\n*   [Friendly Artificial Intelligence](https://www.lesswrong.com/tag/friendly-artificial-intelligence)\n*   [Preference](https://www.lesswrong.com/tag/preference)\n*   [Wireheading](https://www.lesswrong.com/tag/wireheading)\n*   [The utility function is not up for grabs](https://www.lesswrong.com/tag/the-utility-function-is-not-up-for-grabs)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "KQP7fNjin8Zqg4N2x",
    "name": "Double-Crux",
    "core": false,
    "slug": "double-crux",
    "oldSlugs": [
      "double-crux"
    ],
    "postCount": 23,
    "description": {
      "markdown": "**Double-Crux** is a technique for addressing complex disagreements by systematically uncovering the *cruxes* upon which the disagreement hinges. A crux for an individual is any fact that if they believed differently about it, they would change their conclusion in the overall disagreement. A *double-crux* is a crux for both parties. Perhaps we disagree on whether swimming in a lake is safe. A crux for each of us is the presence of crocodiles in water: I believe there aren't, you believe there are. Either of us would change our mind about the safety if we were persuaded about this crux.\n\nDouble-Crux differs from typical debates which are usually adversarial (your opinion vs mine), and instead attempt to be a collaborative attempt to uncover the true structure of the disagreement and what would change the disputants minds.\n\nRelated: [Disagreement](https://www.lesswrong.com/tag/disagreement), [Conversation](https://www.lesswrong.com/tag/conversation-topic)\n\nA version of the technique is described in [Double Crux – A Strategy for Resolving Disagreement](https://www.lesswrong.com/posts/exa5kmvopeRyfJgCy/double-crux-a-strategy-for-resolving-disagreement) written by (then) CFAR instructor, Duncan_Sabien. The Center for Applied Rationality (CFAR) originated the technique. Eli Tyre, another CFAR instructor who has spent a lot of time developing the technique, more recently shared [The Basic Double Crux pattern](https://www.lesswrong.com/posts/hNztRARB52Riy36Kz/the-basic-double-crux-pattern).\n\nSee Also\n--------\n\n*   [Gleanings from Double Crux on “The Craft is Not The Community”](https://srconstantin.wordpress.com/2017/08/30/gleanings-from-double-crux-on-the-craft-is-not-the-community/) \\- a writeup of Double-Crux being used in practice."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "3RnEKrsNgNEDxuNnw",
    "name": "Updated Beliefs (examples of)",
    "core": false,
    "slug": "updated-beliefs-examples-of",
    "oldSlugs": [
      "updated-views"
    ],
    "postCount": 36,
    "description": {
      "markdown": "**Updated Beliefs** is a report of general beliefs or perspectives that one has substantially changed their mind about. A post on this may share what one used to think, what the new views are, and what caused the change. Sharing such accounts publicly is prosocial and allows others to learn from one's experience too.\n\n_See also:_ [Bayesianism](https://www.lesswrong.com/tag/bayes-theorem-bayesianism)_,_ [Changing Your Mind](https://www.lesswrong.com/tag/changing-your-mind)\n\nThis tag is specifically for changes in general beliefs about the world – what you believed before, what you believe now, and causes of the change. If a post focuses reporting actions and outcomes together with an evaluation of the choices/thinking patterns used, then it is a good fit for [Postmortems & Retrospectives](https://www.lesswrong.com/tag/postmortems-and-retrospectives). Central examples of Updated Beliefs posts are “['Other people are wrong' vs 'I am right'](https://www.lesswrong.com/posts/4QemtxDFaGXyGSrGD/other-people-are-wrong-vs-i-am-right)” and “[Six Economics Misconceptions](https://www.lesswrong.com/posts/MgFDzAfCku9MSDLuw/six-economics-misconceptions-of-mine-which-i-ve-resolved)”; in contrast, a central example of a Postmortems & Retrospectives post is “[Arbital Postmortem](https://www.lesswrong.com/posts/kAgJJa3HLSZxsuSrf/arbital-postmortem)”."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "RseFyq6FqAhTycBEY",
    "name": "Ought",
    "core": false,
    "slug": "ought",
    "oldSlugs": null,
    "postCount": 16,
    "description": {
      "markdown": "**Ought** is an AI alignment research non-profit focused on the problem of [Factored Cognition](https://www.lesswrong.com/tag/factored-cognition?showPostCount=true&useTagName=true)."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "WafcndjLNe4x3uXPL",
    "name": "Center for Human-Compatible AI (CHAI)",
    "core": false,
    "slug": "center-for-human-compatible-ai-chai",
    "oldSlugs": [
      "centre-for-human-compatible-ai",
      "centre-for-human-compatible-ai-chai"
    ],
    "postCount": 20,
    "description": {
      "markdown": "The **Center for Human-Compatible AI** is a research institute at UC Berkeley lead and founded by Stuart Russell. Its stated objective is to prevent building [unfriendly AI](https://www.lesswrong.com/tag/unfriendly-artificial-intelligence) by focusing research on provably beneficial behaviour.\n\nExternal links:\n\n*   [Homepage of the Center for Human-Compatible AI](https://humancompatible.ai/)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "K6oowPZC6kds6LDTg",
    "name": "Future of Humanity Institute (FHI)",
    "core": false,
    "slug": "future-of-humanity-institute-fhi",
    "oldSlugs": [
      "future-of-humanity-institute"
    ],
    "postCount": 28,
    "description": {
      "markdown": "The **Future of Humanity Institute** is part of the Faculty of Philosophy and the Oxford Martin School at the University of Oxford. Founded in 2005, its director is [Nick Bostrom](https://www.lesswrong.com/tag/nick-bostrom). The mission of FHI is described on their website:\n\nFHI puts together a wide range of researches, prominent on their original fields, which decided to focus on global questions about the progress and future of humanity, e.g.:\n\n*   [Nick Bostrom](https://www.lesswrong.com/tag/nick-bostrom) : Director, philosopher, has more than 200 publications on subjects such as: [Artificial General Intelligence](https://www.lesswrong.com/tag/artificial-general-intelligence) (AGI) Risks, [Existential risk](https://www.lesswrong.com/tag/existential-risk), [Biological Cognitive Enhancement](https://www.lesswrong.com/tag/nootropics-and-other-cognitive-enhancement) and [Whole brain emulation](https://www.lesswrong.com/tag/whole-brain-emulation)\n*   [Anders Sandberg](http://en.wikipedia.org/wiki/Anders_Sandberg): Research Fellow, computational neuroscientist, researches human enhancement and ethics of new technologies\n*   [Robin Hanson](http://en.wikipedia.org/wiki/Robin_Hanson): Research Associate, economist, interested in prediction market given the future of technology and many other questions involving [bayesianism](http://wiki.lesswrong.com/wiki/Bayesian), [cognitive biases](http://wiki.lesswrong.com/wiki/Bias), technology, policies and the [Fermi Paradox](http://en.wikipedia.org/wiki/Fermi_paradox).\n*   [Toby Ord](http://en.wikipedia.org/wiki/Toby_Ord): Research Associate, philosopher, researches decision making and theoretical and pratical ethics. Founder of [Giving What We Can](http://www.givingwhatwecan.org/), an international society dedicated to the elimination of poverty;\n*   [Milan Cirkovic](http://mcirkovic.aob.rs/): Research Associate, astrophysicist, interested in the anthropic principle and the [Fermi Paradox Fermi Paradox](http://en.wikipedia.org/wiki/Fermi_paradox).\n\nThe FHI is an affiliate to LessWrong and [Overcoming Bias](http://www.overcomingbias.com/). Their past activities include holding a conference in 2008 titled [Global Catastrophic Risks Conference](http://www.global-catastrophic-risks.com/aboutconf.html) and publishing a book, also titled [Global Catastrophic Risks](http://www.global-catastrophic-risks.com/book.html).\n\nSee also\n--------\n\n*   [Nick Bostrom](https://www.lesswrong.com/tag/nick-bostrom)\n*   [Existential risk](https://www.lesswrong.com/tag/existential-risk)\n*   [Machine Intelligence Research Institute](https://www.lesswrong.com/tag/machine-intelligence-research-institute-miri)\n\nExternal links\n--------------\n\n*   [FHI website](http://www.fhi.ox.ac.uk/)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "chuP2QqQycjD8qakL",
    "name": "Coordination / Cooperation",
    "core": false,
    "slug": "coordination-cooperation",
    "oldSlugs": null,
    "postCount": 142,
    "description": {
      "markdown": "**Coordination** is the challenge of distinct actors being able to jointly choose their actions to achieve a favorable outcome. An example of easy coordination is coordinating to drive on the same side of the road. A hard example is coordinating everyone to move a superior yet different social media platform. Many failures of civilization are failures of coordination. A closely related concept is that of **cooperation** – multiple actors choosing their actions in ways that maximize collective value despite the temptation of greater short-term individual gain by acting to the detriment of the group/other actors. The Prisoner's Dilemma is the canonical cooperation/defection game, but the term is used in other games too, e.g. Stag Hunt.\n\nCoordination and cooperation are fundamental concepts in [Game Theory](https://www.lesswrong.com/tag/game-theory). LessWrong discussion has long been interested in overcoming the gnarly coordination and cooperation challenges that prevent many improvements to [optimizing the world](https://www.lesswrong.com/tag/world-optimization). There is also interest because failures to coordinate/cooperate are likely causes of [existential risks](https://www.lesswrong.com/tag/existential-risk) such as [AI Risk](https://www.lesswrong.com/tag/ai-risk) or nuclear war.\n\nSee also: [Game Theory](https://www.lesswrong.com/tag/game-theory), [Moloch](https://www.lesswrong.com/tag/moloch)\n\nAnother related term in this space is *collective action problem. *\n\n**Related Sequences:** [Kickstarter for Coordinated Action](https://www.lesswrong.com/s/vz9Zrj3oBGsttG3Jh)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "EXgFbrqoRRkCRgnDy",
    "name": "Online Socialization",
    "core": false,
    "slug": "online-socialization",
    "oldSlugs": null,
    "postCount": 30,
    "description": {
      "markdown": "**Online Socialization** is, among other things, something you might have to do a lot of if there's a worldwide pandemic."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "GBpwq8cWvaeRoE9X5",
    "name": "Fiction (Topic)",
    "core": false,
    "slug": "fiction-topic",
    "oldSlugs": null,
    "postCount": 121,
    "description": {
      "markdown": "This tag collects post which discuss **fiction** at the meta-level. For a list of actual fiction content, see the [Fiction tag](https://www.lesswrong.com/tag/fiction). For posts about HPMOR, see [HPMOR (discussion & meta)](https://www.lesswrong.com/tag/hpmor-discussion-and-meta). For post specifically about writing, see [Writing (communication method)](https://www.lesswrong.com/tag/writing-communication-method).\n\n> “Nonfiction conveys *knowledge,* fiction conveys *experience.*” \\- Eliezer Yudkowsky"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "moeYqrcakMgXnQNyF",
    "name": "Curiosity",
    "core": false,
    "slug": "curiosity",
    "oldSlugs": null,
    "postCount": 31,
    "description": {
      "markdown": "> *The first virtue is **curiosity**. A burning itch to know is higher than a solemn vow to pursue truth. To feel the burning itch of curiosity requires both that you be ignorant, and that you desire to relinquish your ignorance. If in your heart you believe you already know, or if in your heart you do not wish to know, then your questioning will be purposeless and your skills without direction. Curiosity seeks to annihilate itself; there is no curiosity that does not want an answer. The glory of glorious mystery is to be solved, after which it ceases to be mystery. Be wary of those who speak of being open-minded and modestly confess their ignorance. There is a time to confess your ignorance and a time to relinquish your ignorance.*\n\n*\\-\\-* Eliezer Yudkowsky, [The Twelve Virtues of Rationality](https://yudkowsky.net/rational/virtues/#:~:text=These%20then%20are%20twelve%20virtues,%2C%20scholarship%2C%20and%20the%20void.)\n\nSee Also\n--------\n\n*   [Seeing with Fresh Eyes](https://www.lesswrong.com/tag/seeing-with-fresh-eyes)\n*   [Semantic stopsign](https://www.lesswrong.com/tag/semantic-stopsign)\n*   [Litany of Tarski](https://www.lesswrong.com/tag/litany-of-tarski)\n*   [Scholarship](https://www.lesswrong.com/tag/scholarship-and-learning)\n*   [Doubt](https://www.lesswrong.com/tag/doubt)\n\nNotable Posts\n-------------\n\n*   [A Fable of Science and Politics](https://www.lesswrong.com/lw/gt/a_fable_of_science_and_politics/)\n*   [The Meditation on Curiosity](https://www.lesswrong.com/lw/jz/the_meditation_on_curiosity/)\n*   [Use Curiosity](https://www.lesswrong.com/lw/4ku/use_curiosity/)\n*   [What Curiosity Looks Like](https://www.lesswrong.com/lw/96j/what_curiosity_looks_like/)\n*   [The Neglected Virtue of Curiosity](https://www.lesswrong.com/lw/9m2/the_neglected_virtue_of_curiosity/)\n*   [Get Curious](https://www.lesswrong.com/lw/aa7/get_curious/)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "KoXbd2HmbdRfqLngk",
    "name": "Planning & Decision-Making",
    "core": false,
    "slug": "planning-and-decision-making",
    "oldSlugs": null,
    "postCount": 79,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "tk4R4LrX88gmFeMmY",
    "name": "Experiments",
    "core": false,
    "slug": "experiments",
    "oldSlugs": [
      "experiment"
    ],
    "postCount": 26,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "b8nerwC3Dp2Q9MvbG",
    "name": "Corrigibility",
    "core": false,
    "slug": "corrigibility",
    "oldSlugs": null,
    "postCount": 57,
    "description": {
      "markdown": "A **corrigible** agent is one that doesn't interfere with what we would intuitively see as attempts to 'correct' the agent, or 'correct' our mistakes in building it; and permits these 'corrections' despite the apparent instrumentally convergent reasoning saying otherwise."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "d2s5yH2MNwX4pveW7",
    "name": "Humans Consulting HCH",
    "core": false,
    "slug": "humans-consulting-hch",
    "oldSlugs": [
      "hch"
    ],
    "postCount": 27,
    "description": {
      "markdown": "**Humans Consulting HCH (HCH)** is a recursive acronym describing a setup where humans can consult simulations of themselves to help answer questions. It is a concept used in discussion of the [iterated amplification](https://www.lesswrong.com/tag/iterated-amplification) proposal to solve the alignment problem.\n\nIt was first described by Paul Christiano in his post [Humans Consulting HCH](https://www.lesswrong.com/posts/NXqs4nYXaq8q6dTTx/humans-consulting-hch):\n\n> Consider a human Hugh who has access to a question-answering machine. Suppose the machine answers question Q by perfectly imitating how Hugh would answer question Q, *if* *Hugh had access to the question-answering machine*.\n> \n> That is, Hugh is able to consult a copy of Hugh, who is able to consult a copy of Hugh, who is able to consult a copy of Hugh…\n> \n> Let’s call this process HCH, for “Humans Consulting HCH.”"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "BisjoDrd3oNatDu7X",
    "name": "Outer Alignment",
    "core": false,
    "slug": "outer-alignment",
    "oldSlugs": null,
    "postCount": 99,
    "description": {
      "markdown": "**Outer Alignment** in the context of machine learning is the property where the specified loss function is aligned with the intended goal of its designers. This is an intuitive notion, in part because human intentions are themselves not well-understood. This is what is typically discussed as the 'value alignment' problem. It is contrasted with [inner alignment](https://www.lesswrong.com/tag/inner-alignment), which discusses if an optimizer is the production of an outer aligned system, then whether that optimizer is itself aligned._See also:_"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "Dw5Z6wtTgk4Fikz9f",
    "name": "Inner Alignment",
    "core": false,
    "slug": "inner-alignment",
    "oldSlugs": null,
    "postCount": 114,
    "description": {
      "markdown": "**Inner Alignment** is the problem of ensuring [mesa-optimizers](https://www.lesswrong.com/tag/mesa-optimization) (i.e. when a trained ML system is itself an optimizer) are aligned with the objective function of the training process. As an example, evolution is an optimization force that itself 'designed' optimizers (humans) to achieve its goals. However, humans do not primarily maximise reproductive success, they instead use birth control and then go out and have fun. This is a failure of inner alignment. \n\nThe term was first given a definition in the Hubinger et al paper *Risk from Learned Optimization*:\n\n> We refer to this problem of aligning mesa-optimizers with the base objective as the inner alignment problem. This is distinct from the outer alignment problem, which is the traditional problem of ensuring that the base objective captures the intended goal of the programmers.\n\n**Related Pages:** [Mesa-Optimization](https://www.lesswrong.com/tag/mesa-optimization)\n\nExternal Links:\n---------------\n\n[Video by Robert Miles](https://www.youtube.com/watch?v=bJLcIBixGj8)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "8ckoduMw3gvCMJGSB",
    "name": "Logical Induction",
    "core": false,
    "slug": "logical-induction",
    "oldSlugs": null,
    "postCount": 28,
    "description": {
      "markdown": "**Logical Induction** is a formal theory of reasoning under [logical uncertainty](https://www.lesswrong.com/tag/logical-uncertainty), developed by Scott Garrabrant and other researchers. Rationality is defined through a prediction-market analogy. High-quality beliefs are those which are computationally difficult to win bets against. The writeup can be found [here](https://intelligence.org/2016/09/12/new-paper-logical-induction/)."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "nvKzwpiranwy29HFJ",
    "name": "Optimization",
    "core": false,
    "slug": "optimization",
    "oldSlugs": null,
    "postCount": 80,
    "description": {
      "markdown": "An **optimization process** is any kind of process that systematically comes up with solutions that are better than the solution used before. More technically, this kind of process moves the world into a specific and unexpected set of states by searching through a large search space, hitting small and low probability targets. When this process is gradually guided by some agent into some specific state, through searching specific targets, we can say it [prefers](https://www.lesswrong.com/tag/preference) that state.\n\nThe best way to exemplify an optimization process is through a simple example: [Eliezer Yudkowsky](https://www.lesswrong.com/tag/eliezer-yudkowsky) suggests natural selection is such a process. Through an implicit preference – better replicators – natural selection searches all the genetic landscape space and hit small targets: efficient mutations.\n\nConsider the human being. We are a highly complex object with a low probability to have been created by chance - natural selection, however, over millions of years, built up the infrastructure needed to build such a functioning body. This body, as well as other organisms, had the chance (was *selected*) to develop because it is in itself a rather efficient replicator suitable for the environment where it came up.\n\nOr consider the famous chessplaying computer, [Deep Blue](https://en.wikipedia.org/wiki/Deep_Blue_(chess_computer)). Outside of the narrow domain of selecting moves for chess games, it can't do anything impressive: but *as* a chessplayer, it was massively more effective than virtually all humans. It has a high optimization power in the chess domain but almost none in any other field. Humans or evolution, on the other hand, are more domain-general optimization processes than Deep Blue, but that doesn't mean they're more effective at chess specifically. (Although note in what contexts this *optimization process* abstraction is useful and where it fails to be useful: it's not obvious what it would mean for \"evolution\" to play chess, and yet it is useful to talk about the optimization power of natural selection, or of Deep Blue.)\n\nMeasuring Optimization Power\n----------------------------\n\nOne way to think mathematically about optimization, like [evidence](https://www.lesswrong.com/tag/amount-of-evidence), is in information-theoretic bits. The optimization power is the amount of [surprise](http://en.wikipedia.org/wiki/Self-information) we would have in the result if there were no optimization process present. Therefore we take the base-two logarithm of the reciprocal of the probability of the result. A one-in-a-million solution (a solution so good relative to your preference ordering that it would take a million random tries to find something that good or better) can be said to have log_2(1,000,000) = 19.9 bits of optimization. Compared to a random configuration of matter, any artifact you see is going to be much more optimized than this. The math describes only laws and general principles for reasoning about optimization; as with [probability theory](https://www.lesswrong.com/tag/bayesian-probability), you oftentimes can't apply the math directly.\n\nFurther Reading & References\n----------------------------\n\n*   [Optimization and the Singularity](https://www.lesswrong.com/lw/rk/optimization_and_the_singularity/) by Eliezer Yudkowsky\n*   [Measuring Optimization Power](https://www.lesswrong.com/lw/va/measuring_optimization_power/) by Eliezer Yudkowsky\n\nSee also\n--------\n\n*   [Preference](https://www.lesswrong.com/tag/preference)\n*   [Really powerful optimization process](https://www.lesswrong.com/tag/really-powerful-optimization-process)\n*   [Control theory](https://www.lesswrong.com/tag/control-theory)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "HJZZxyYXWzB74M4FT",
    "name": "Fixed Point Theorems",
    "core": false,
    "slug": "fixed-point-theorems",
    "oldSlugs": null,
    "postCount": 10,
    "description": {
      "markdown": "**Fixed Point Theorems** are very general theorems in mathematics that show for a given function \\\\(f\\\\) and input \\\\(x\\\\) that \\\\(f(x) = x\\\\). We say that the input \\\\(x\\\\) is a fixed point for the function \\\\(f\\\\).\n\nThese come up most commonly on LessWrong in work around [Embedded Agency](https://www.lesswrong.com/tag/embedded-agency) research, as well as in discussion of game theory."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "MP8NqPNATMqPrij4n",
    "name": "Embedded Agency",
    "core": false,
    "slug": "embedded-agency",
    "oldSlugs": null,
    "postCount": 58,
    "description": {
      "markdown": "**Embedded Agency** is the problem that an understanding of the theory of rational agents must account for the fact that the agents we create (and we ourselves) are inside the world or universe we are trying to affect, and not separated from it. This is in contrast with much current basic theory of AI or Rationality (such as Solomonoff induction or Bayesianism) which implicitly supposes a separation between the agent and the-things-the-agent-has-beliefs about. In other words, agents in this universe do not have Cartesian or dualistic boundaries like much of philosophy thinks, and are instead reductionist, that is agents are made up of non-agent parts like bits and atoms.\n\nEmbedded Agency is not a fully formalised research agenda, but Scott Garrabrant and Abram Demski have written the canonical explanation of the idea in their sequence [*Embedded Agency*](https://www.lesswrong.com/s/Rm6oQRJJmhGCcLvxh). This points to many of the core confusions we have about rational agency and attempts to tie them into a single picture."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "vmiSKxPpzMuNB5ZmJ",
    "name": "LessWrong Event Transcripts",
    "core": false,
    "slug": "lesswrong-event-transcripts",
    "oldSlugs": [
      "lesswrong-events",
      "lesswrong-events",
      "lesswrong-event-transcripts"
    ],
    "postCount": 26,
    "description": {
      "markdown": "**LessWrong Event Transcripts** are transcripts from events that were organised by the LessWrong team."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "ZFrgTgzwEfStg26JL",
    "name": "AI Risk",
    "core": false,
    "slug": "ai-risk",
    "oldSlugs": null,
    "postCount": 398,
    "description": {
      "markdown": "**AI Risk** is analysis of the risks associated with building powerful AI systems."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "H4n4rzs33JfEgkf8b",
    "name": "OpenAI",
    "core": false,
    "slug": "openai",
    "oldSlugs": null,
    "postCount": 46,
    "description": {
      "markdown": "**OpenAI** is an organisation that performs AI research, and houses a substantial amount of AI alignment research. Its stated mission is \"Discovering and enacting the path to safe artificial general intelligence.\"\n\nThis tag is for explicit discussion of the organisation, not for all work published by researchers at that organisation."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "zHjC29kkPmsdo7WTr",
    "name": "AI Timelines",
    "core": false,
    "slug": "ai-timelines",
    "oldSlugs": null,
    "postCount": 169,
    "description": {
      "markdown": "**AI Timelines** is the discussion of how long until various major milestones in AI progress are achieved, whether it's the timeline until a human-level AI is developed, the timeline until certain benchmarks are defeated, the timeline until we can simulate a mouse-level intelligence, or something else.\n\nThis is to be distinguished from the closely related question of [AI takeoff](https://www.lesswrong.com/tag/ai-takeoff) speeds, which is about the dynamics of AI progress after human-level AI is developed (e.g. will it be a single project or the whole economy that sees growth, how fast will that growth be, etc)."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "pszEEb3ctztv3rozd",
    "name": "Narratives (stories)",
    "core": false,
    "slug": "narratives-stories",
    "oldSlugs": [
      "stories-and-narratives",
      "narratives"
    ],
    "postCount": 33,
    "description": {
      "markdown": "See also: [Public discourse](https://www.lesswrong.com/tag/public-discourse), [Fiction](https://www.lesswrong.com/tag/fiction), [Writing](https://www.lesswrong.com/tag/writing), [Social reality](https://www.lesswrong.com/tag/social-reality)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "p8nXWqwPH7mPSZf6p",
    "name": "Terminology / Jargon (meta)",
    "core": false,
    "slug": "terminology-jargon-meta",
    "oldSlugs": [
      "jargon",
      "jargon-meta"
    ],
    "postCount": 26,
    "description": {
      "markdown": "  \nSee also: [Conversation (topic)](https://www.lesswrong.com/tag/conversation-topic), [Public discourse](https://www.lesswrong.com/tag/public-discourse)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5gcpKG2XEAZGj5DEf",
    "name": "Noticing",
    "core": false,
    "slug": "noticing",
    "oldSlugs": null,
    "postCount": 31,
    "description": {
      "markdown": "See also: [Introspection](https://www.lesswrong.com/tag/introspection)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "6nS8oYmSMuFMaiowF",
    "name": "Logic & Mathematics ",
    "core": false,
    "slug": "logic-and-mathematics",
    "oldSlugs": [
      "mathematics"
    ],
    "postCount": 262,
    "description": {
      "markdown": "**Logic and Mathematics** are deductive systems, where the conclusion of a successful argument follows necessarily from its premises, given the axioms of the system you’re using: number theory, geometry, predicate logic, etc.\n\nSee also\n--------\n\n*   [Valid argument](https://www.lesswrong.com/tag/valid-argument) \\- An argument is valid when it contains no logical fallacies\n*   [Sound argument](https://www.lesswrong.com/tag/sound-argument) \\- an argument that is valid and whose premises are all true. In other words, the premises are true and the conclusion necessarily follows from them, making the conclusion true as well.\n*   [Formal proof](https://www.lesswrong.com/tag/formal-proof) \\- A set of steps from axiom(s) and previous proof(s) which follows the rules of induction of a mathematical system.\n*   [Logical Uncertainty](https://www.lesswrong.com/tag/logical-uncertainty)\n*   [Logical Induction](https://www.lesswrong.com/tag/logical-induction)\n*   [Probability & Statistics](https://www.lesswrong.com/tag/probability-and-statistics)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "exZi6Bing5AiM4ZQB",
    "name": "Evolutionary Psychology",
    "core": false,
    "slug": "evolutionary-psychology",
    "oldSlugs": null,
    "postCount": 43,
    "description": {
      "markdown": "[Evolution](https://wiki.lesswrong.com/wiki/Evolution), the cause of the diversity of biological life on Earth, *does not work like humans do*, and does not design things the way a human engineer would. This [blind idiot god](https://wiki.lesswrong.com/wiki/Alienness_of_evolution) is also the source and patterner of human beings. \"Nothing in biology makes sense except in the light of evolution,\" said Theodosius Dobzhansky. Humans brains are also biology, and nothing about our thinking makes sense except in the light of evolution.\n\nConsider, for example, the following tale:\n\n> A man and a woman meet in a bar. The man is attracted to her clear complexion, which would have been fertility cues in the ancestral environment, but which in this case result from makeup and a bra. This does not bother the man; he just likes the way she looks. His clear-complexion-detecting neural circuitry does not know that its purpose is to detect fertility, any more than the atoms in his hand contain tiny little XML tags reading \"<purpose>pick things up</purpose>\". The woman is attracted to his confident smile and firm manner, cues to high status, which in the ancestral environment would have signified the ability to provide resources for children. She plans to use birth control, but her confident-smile-detectors don't know this any more than a toaster knows its designer intended it to make toast. She's not concerned philosophically with the meaning of this rebellion, because her brain is a creationist and denies vehemently that evolution exists. He's not concerned philosophically with the meaning of this rebellion, because he just wants to get laid. They go to a hotel, and undress. He puts on a condom, because he doesn't want kids, just the dopamine-noradrenaline rush of sex, which reliably produced offspring 50,000 years ago when it was an invariant feature of the ancestral environment that condoms did not exist. They have sex, and shower, and go their separate ways. The main objective consequence is to keep the bar and the hotel and condom-manufacturer in business; which was not the cognitive purpose in their minds, and has virtually nothing to do with the key statistical regularities of reproduction 50,000 years ago which explain how they got the genes that built their brains that executed all this behavior.\n\nThis only makes sense in the light of evolution as a designer - that we are *poorly* optimized to reproduce by a blind and unforesightful god.\n\nThe idea of evolution as the idiot designer of humans - that our brains are *not* consistently well-designed - is a key element of many of the *explanations of human errors* that appear on this website.\n\nSome of the key ideas of evolutionary psychology are these:\n\n*   People's brains do not explicitly represent evolutionary reasons, *consciously or unconsciously*.\n*   We are optimized for an \"ancestral environment\" (often referred to as EEA, for \"environment of evolutionary adaptedness\") that differs significantly from the environments in which most of us live. In the ancestral environment, calories were the limiting resource, so our tastebuds are built to like sugar and fat.\n*   The brain is not built the way a human engineer would build it. A human engineer would have built our bodies to measure what it needed, so that if you already had enough calories but were lacking micronutrients, your taste buds would start liking lettuce instead of cheeseburgers.\n*   The brain is a giant hack that starts to break down when you try to do things with it that hunter-gatherers weren't doing. Like computer programming, say.\n*   Evolution's purposes also differ from our own purposes. We are built to deceive ourselves because self-deceivers were more effective liars in ancestral political disputes; and this fact about our underlying brain design doesn't change when we try to make a moral commitment to truth and rationality.\n*   Although human beings do absorb significant additional complexity in the form of culture, we don't absorb it in a fully general way, but rather, in the way that we [evolved to absorb it](https://wiki.lesswrong.com/wiki/Detached_lever_fallacy). That's why the Soviets couldn't raise perfect communist children. Children are programmed to absorb their parents' language, say, but there is no environment which evokes the response of perfect altruism in human children.\n\nExternal links\n--------------\n\n*   [The Psychological Foundations of Culture](http://www.cep.ucsb.edu/papers/pfc92.pdf) by Leda Cosmides and John Tooby.\n*   [Evolutionary Psychology: A Primer](http://www.cep.ucsb.edu/primer.html) by Leda Cosmides and John Tooby.\n*   [“Darwinian Psychologist” Straw Man’s Ass Kicked](http://hplusmagazine.com/2009/11/23/darwinian-psychologist-straw-mans-ass-kicked/)\n*   [The Moral Animal: Why We Are the Way We Are: The New Science of Evolutionary Psychology](http://www.amazon.com/Moral-Animal-Science-Evolutionary-Psychology/dp/0679763996) \\- popular book by Robert Wright.\n\nSee also\n--------\n\n*   [Evolution](lesswrong.com/tag/evolution)\n*   [Stupidity of evolution](https://wiki.lesswrong.com/wiki/Stupidity_of_evolution), [evolution as alien god](https://wiki.lesswrong.com/wiki/Evolution_as_alien_god)\n*   [Human universal](https://wiki.lesswrong.com/wiki/Human_universal)\n*   [Adaptation executers](https://www.lesswrong.com/tag/adaptation-executors)\n*   [Corrupted hardware](https://wiki.lesswrong.com/wiki/Corrupted_hardware)\n*   [Psychology](https://www.lesswrong.com/tag/psychology)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "x3zyEPFaJANB2BHmP",
    "name": "Expertise (topic)",
    "core": false,
    "slug": "expertise-topic",
    "oldSlugs": [
      "expertise"
    ],
    "postCount": 48,
    "description": {
      "markdown": "**Related Pages:** [Social Status](https://www.lesswrong.com/tag/social-status), [Practice & Philosophy of Science](https://www.lesswrong.com/tag/practice-and-philosophy-of-science)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "x5TtBDjRg9egvg9gm",
    "name": "Cultural knowledge",
    "core": false,
    "slug": "cultural-knowledge",
    "oldSlugs": null,
    "postCount": 26,
    "description": {
      "markdown": "**Cultural knowledge** (or **metis**) refers to knowledge that's codified in traditions, norms, institutions and intuitions, without necessarily being fully understood or legible to people in those cultures."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "MXcpQvaPGtXpB6vkM",
    "name": "Public Discourse",
    "core": false,
    "slug": "public-discourse",
    "oldSlugs": [
      "public-discourse"
    ],
    "postCount": 104,
    "description": {
      "markdown": "**Public discourse** refers to our ability to have conversations *in large groups*, both as a society, and in smaller communities; as well as conversations between a few well-defined participants (such as presidential debates) that take place publicly. \n\nThis tag is for understanding the nature of public discourse (How good is it? What makes it succeed or fail?), and ways of improving it using technology or novel institutions. \n\nSee also: [Conversation (topic)](https://www.lesswrong.com/tag/conversation-topic)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "xCXkjecsjwm8uSW3y",
    "name": "Cryptocurrency & Blockchain",
    "core": false,
    "slug": "cryptocurrency-and-blockchain",
    "oldSlugs": [
      "cryptocurrency-and-blockchain",
      "cryptocurrency-and-blockchain"
    ],
    "postCount": 48,
    "description": {
      "markdown": "See also: [Institution Design](https://www.lesswrong.com/tag/institution-design), [Financial Investing](https://www.lesswrong.com/tag/financial-investing)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "Z6DgiCrMtpSNxwuYW",
    "name": "Grants & Fundraising Opportunities",
    "core": false,
    "slug": "grants-and-fundraising-opportunities",
    "oldSlugs": [
      "grants-and-fundraising-opportunities"
    ],
    "postCount": 57,
    "description": {
      "markdown": "Many LessWrong readers actively rely on **grants or fundraising opportunities** to support their work (for example by running non-profits or startups, working as independent researchers, or being supported by academic grants). \n\nThis tag lists concrete opportunities for fundraising of interest to the LessWrong community. This typically means projects working on improving the long-term future, refining the art of rationality, and related missions.  \n\nThis tag should not be used for meta-discussion, e.g. of fundraising strategies, coordination between funders, or cost-benefit analyses of particular funding opportunities."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "cCK3fDdH9vHjrf2yP",
    "name": "Bounties & Prizes (active)",
    "core": false,
    "slug": "bounties-and-prizes-active",
    "oldSlugs": [
      "bounties-active"
    ],
    "postCount": 39,
    "description": {
      "markdown": "A **bounty** is a monetary payment for accomplishing some task. On LessWrong, bounties have historically been paid out for things like providing useful information, doing a novel piece of research, or changing someone's mind about a topic. \n\nThis tag is for bounties that are actively accepting submissions. (It has a sister for bounties that have closed.)\n\nBounties might be listed in the post itself or in its comments. \n\n*If you're hosting a bounty, please make sure to change the tags to indicate the status of the bounty. *\n\nSee also: [Bounties (closed)](https://www.lesswrong.com/tag/bounties-closed), [Grants and fundraising opportunities](https://www.lesswrong.com/tag/grants-and-fundraising-opportunities), [Bountied Rationality Facebook group](https://www.facebook.com/groups/1781724435404945/)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "hQiuNkBhn6xxcedTD",
    "name": "Occam's Razor",
    "core": false,
    "slug": "occam-s-razor",
    "oldSlugs": [
      "occam-s-razor",
      "occam-s-razor",
      "occams-razor"
    ],
    "postCount": 32,
    "description": {
      "markdown": "**Occam's razor** (more formally referred to as the principle of parsimony) is a principle commonly stated as \"Entities must not be multiplied beyond necessity\". When several theories are able to explain the same observations, Occam's razor suggests the simpler one is preferable. It must be noted that Occam's razor is a requirement for the simplicity of *theories*, not for the size of the systems described by those theories. For example, the immensity of the Universe isn't at odds with the principle of Occam's razor.\n\nOccam's razor is necessitated by the conjunction rule of probability theory: the conjunction A and B is necessarily less (or equally, in the case of logical equivalence) probable than the A alone; [every detail you tack onto your story drives the probability down](https://www.lesswrong.com/tag/burdensome-details).\n\nOccam's razor has been formalized as Minimum Description Length or Minimum Message Length, in which the total size of the theory is the length of the message required to describe the theory, plus the length of the message required to describe the evidence *using* the theory. [Solomonoff induction](https://www.lesswrong.com/tag/solomonoff-induction) is the ultimate case of [minimum message length](https://wiki.lesswrong.com/wiki/minimum_message_length) in which the code for messages can describe all computable hypotheses. This has jokingly been referred to as \"[Solomonoff's lightsaber](https://www.lesswrong.com/tag/solomonoff-induction)\".\n\nNotable Posts\n-------------\n\n*   [Occam's Razor](https://www.lesswrong.com/lw/jp/occams_razor/)\n*   [A Priori](https://www.lesswrong.com/lw/k2/a_priori/)\n*   [Decoherence is Simple](https://www.lesswrong.com/lw/q3/decoherence_is_simple/)\n\nSee Also\n--------\n\n*   [Solomonoff induction](https://www.lesswrong.com/tag/solomonoff-induction)\n*   [Occam's imaginary razor](https://wiki.lesswrong.com/wiki/Occam's_imaginary_razor)\n*   [Priors](https://www.lesswrong.com/tag/priors)\n*   [Burdensome details](https://www.lesswrong.com/tag/burdensome-details)\n*   [Egan's law](https://wiki.lesswrong.com/wiki/Egan's_law)\n\nExternal Links\n--------------\n\n*   [Ockham’s Razor: A New Justification](http://www.andrew.cmu.edu/user/kk3n/ockham/Ockham.htm)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "7mTviCYysGmLqiHai",
    "name": "Writing (communication method)",
    "core": false,
    "slug": "writing-communication-method",
    "oldSlugs": [
      "writing"
    ],
    "postCount": 116,
    "description": {
      "markdown": "About the art and science of using **writing** to think and communicate.\n\nIncluding posts on: the history of writing, how to write better, comparing and recommending writing tools, and more. \n\n**External Links:**\n\n[Nonfiction Writing Advice](https://slatestarcodex.com/2016/02/20/writing-advice/) by Scott Alexander\n\n**Related Pages:** [Communication Cultures](https://www.lesswrong.com/tag/communication-cultures), [Philosophy of Language](https://www.lesswrong.com/tag/philosophy-of-language), [Inferential Distance](https://www.lesswrong.com/tag/inferential-distance)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "HkiwLtMRLxpBa6zs5",
    "name": "Industrial Revolution",
    "core": false,
    "slug": "industrial-revolution",
    "oldSlugs": [
      "industrial-revolution"
    ],
    "postCount": 29,
    "description": {
      "markdown": "The **Industrial Revolution** was a set of economic and social changes that occurred in Europe and the United States in the 18th and 19th centuries, characterised by a transition from an \"agrarian and handicraft economy to one dominated by industry and machine manufacturing\" \\[[1](https://www.britannica.com/event/Industrial-Revolution)\\]. \n\nSee also: [History](https://www.lesswrong.com/tag/history)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "HLoxy2feb2PYqooom",
    "name": "Sleep",
    "core": false,
    "slug": "sleep",
    "oldSlugs": null,
    "postCount": 30,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "8nAXyYLu8eT72Hwuh",
    "name": "Exercise (Physical)",
    "core": false,
    "slug": "exercise-physical",
    "oldSlugs": [
      "exercise-physical",
      "exercise-physical-activity"
    ],
    "postCount": 28,
    "description": {
      "markdown": "Can one single intervention make you live around 5 minutes longer for every minute invested, change how you look, and allow you to perform physical feats that you would have thought impossible? Yes, and it's high-intensity strength training.\n\nThe current most comprehensive overview in this topic is [here](https://www.lesswrong.com/posts/bZ2w99pEAeAbKnKqo/optimal-exercise).\n\nSee also: [Sports](http://lesswrong.com/tag/sports), [Well-being](http://lesswrong.com/tag/well-being)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "DWWZwkxTJs4d5WrcX",
    "name": "Exercises / Problem-Sets",
    "core": false,
    "slug": "exercises-problem-sets",
    "oldSlugs": [
      "exercises"
    ],
    "postCount": 142,
    "description": {
      "markdown": "This tag collects posts with concrete **exercises**. Problems that have solutions (or least some clear feedback loop). Things that you can attempt yourself in order to learn and grow. \n\n**Related Pages:** [Games (posts describing)](https://www.lesswrong.com/tag/games-posts-describing)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "kCuRQE5Tkv9zeKyzK",
    "name": "Common Knowledge",
    "core": false,
    "slug": "common-knowledge",
    "oldSlugs": null,
    "postCount": 20,
    "description": {
      "markdown": "**Common knowledge** is information that everyone knows and, importantly, that everyone knows that everyone knows, and so on, ad infinitum. If information is *common knowledge* in a group of people, that information that can be relied and acted upon with the trust that everyone else is also coordinating around that information. This stands, in contrast, to merely publicly known information where one person cannot be sure that another person knows the information, or that another person knows that they know the information. Establishing true common knowledge is, in fact, rather hard.\n\n**Related Pages:** [Public discourse](https://www.lesswrong.com/tag/public-discourse), [Consensus](https://www.lesswrong.com/tag/consensus), [Inferential Distance](https://www.lesswrong.com/tag/inferential-distance)\n\n**External posts:**   \n[The Kolmogorov option](https://www.scottaaronson.com/blog/?p=3376) by Scott Aaronson  \n[kolmogorov complicity and-the parable of lightning](https://slatestarcodex.com/2017/10/23/kolmogorov-complicity-and-the-parable-of-lightning/) by Scott Alexander"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "4mRJmYxNDnn7r2gNu",
    "name": "Iterated Amplification ",
    "core": false,
    "slug": "iterated-amplification",
    "oldSlugs": null,
    "postCount": 58,
    "description": {
      "markdown": "**Iterated Amplification** is an approach to AI alignment, spearheaded by Paul Christiano. In this setup, we build powerful, aligned ML systems through a process of initially building weak aligned AIs, and recursively using each new AI to build a slightly smarter and still aligned AI. \n\nSee also: [Factored cognition](https://www.lesswrong.com/tag/factored-cognition)."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "nKtyrL5u4Y5kmMWT5",
    "name": "DeepMind",
    "core": false,
    "slug": "deepmind",
    "oldSlugs": [
      "alpha-ai-algorithms",
      "alpha-group-of-ai-algorithms",
      "alpha-algorithm-family"
    ],
    "postCount": 36,
    "description": {
      "markdown": "**DeepMind** is an AI research laboratory that was acquired by Google in 2014. It is known for several record-breaking AI algorithms, often named with the prefix \"Alpha\", e.g. including AlphaGo, AlphaGo Zero, AlphaStar and AlphaFold."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "BzghQYM9GnkMHxZKb",
    "name": "Problem-solving (skills and techniques)",
    "core": false,
    "slug": "problem-solving-skills-and-techniques",
    "oldSlugs": [
      "problem-solving"
    ],
    "postCount": 15,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "GLykb6NukBeBQtDvQ",
    "name": "Philosophy",
    "core": false,
    "slug": "philosophy",
    "oldSlugs": null,
    "postCount": 128,
    "description": {
      "markdown": "(From Wikipedia,) _Philosophy_ is the study of general and fundamental questions about existence, knowledge, values, reason, mind, and language.\n\nThis tag probably implies non-Less Wrong philosophy in particular. This could be mainstream or academic philosophy, Eastern philosophy, or something else"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "3Y4y9Kr8e24YWAEmD",
    "name": "Myopia",
    "core": false,
    "slug": "myopia",
    "oldSlugs": null,
    "postCount": 27,
    "description": {
      "markdown": "**Myopia** means short-sighted, particularly with respect to planning -- neglecting long-term consequences in favor of the short term. The extreme case, in which *only* immediate rewards are considered, is of particular interest. We can think of a myopic agent as one that only considers how best to answer the single question that you give to it rather than considering any sort of long-term consequences. Such an agent might have a number of desirable safety properties, such as a lack of [instrumental incentives](https://arbital.com/p/convergent_strategies/)."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "SJFsFfFhE6m2ThAYJ",
    "name": "Anticipated Experiences",
    "core": false,
    "slug": "anticipated-experiences",
    "oldSlugs": null,
    "postCount": 39,
    "description": {
      "markdown": "One principle of rationality is that \"[beliefs should pay rent in **anticipated experiences**](https://www.lesswrong.com/s/7gRSERQZbqTuLX5re/p/a7n8GdKiAZRX86T5A).\" If you believe in something, what do you expect to be different as a result? What does the belief say should happen, and what does it say should _not_ happen? If you have a verbal disagreement with someone, how does your disagreement cash out in differing expectations?\n\nIf two people try to get specific about the anticipated experiences driving their disagreement, one method for doing so is the [double crux](https://www.lesswrong.com/posts/exa5kmvopeRyfJgCy/double-crux-a-strategy-for-resolving-disagreement) technique. The notion that beliefs are models of what we expect to experience is also one of the basic premises of [predictive processing](https://www.lesswrong.com/tag/predictive-processing) theories of how the brain works. Beliefs that do not pay rent may be related to [meaningless arguments](https://www.lesswrong.com/posts/4xKeNKFXFB458f5N8/ethnic-tension-and-meaningless-arguments) driven by [coalitional instincts](https://www.lesswrong.com/tag/coalitional-instincts).\n\n> _If a tree falls in a forest and no one hears it, does it make a sound? One says, “Yes it does, for it makes vibrations in the air.” Another says, “No it does not, for there is no auditory processing in any brain.”_ \\[...\\]\n\n> Suppose that, after a tree falls, the two arguers walk into the forest together. Will one expect to see the tree fallen to the right, and the other expect to see the tree fallen to the left? Suppose that before the tree falls, the two leave a sound recorder next to the tree. Would one, playing back the recorder, expect to hear something different from the other? Suppose they attach an electroencephalograph to any brain in the world; would one expect to see a different trace than the other?\n\n> Though the two argue, one saying “No,” and the other saying “Yes,” they do not anticipate any different experiences. The two think they have different models of the world, but they have no difference with respect to what they expect will _happen to_ them; their maps of the world do not diverge in any sensory detail.\n\n> \\-\\- Eliezer Yudkowsky, [Making Beliefs Pay Rent (In Anticipated Experiences)](https://www.lesswrong.com/s/7gRSERQZbqTuLX5re/p/a7n8GdKiAZRX86T5A)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "DdgSyQoZXjj3KnF4N",
    "name": "Tribalism",
    "core": false,
    "slug": "tribalism",
    "oldSlugs": [
      "coalitional-instincts"
    ],
    "postCount": 49,
    "description": {
      "markdown": "**Tribalism** or **Coalitional Instincts** is closely connected to the concept of in/out-groups. Coalitional instincts drive humans to act in ways which cause them join, support, defend, and maintain their membership in various coalitions that are defined by sharing a common identity. An illustrative example can be found in [A Fable of Science and Politics](https://www.lesswrong.com/posts/6hfGNLf4Hg5DXqJCF/a-fable-of-science-and-politics).\n\nSee also: [Blues and Greens](https://www.lesswrong.com/tag/blues-and-greens), [Groupthink](https://www.lesswrong.com/tag/groupthink), [Motivated Reasoning](https://www.lesswrong.com/tag/motivated-reasoning), [Social and Cultural Dynamics](https://www.lesswrong.com/tag/social-and-cultural-dynamics), [Social Reality](https://www.lesswrong.com/tag/social-reality).\n\n> The primary function that drove the evolution of coalitions is the amplification of the power of its members in conflicts with non-members. This function explains a number of otherwise puzzling phenomena. For example, ancestrally, if you had no coalition you were nakedly at the mercy of everyone else, so the instinct to belong to a coalition has urgency, preexisting and superseding any policy-driven basis for membership. This is why group beliefs are free to be so weird. \\[...\\]\n\n> ... to earn membership in a group you must send signals that clearly indicate that you differentially support it, compared to rival groups. Hence, optimal weighting of beliefs and communications in the individual mind will make it feel good to think and express content conforming to and flattering to one’s group’s shared beliefs and to attack and misrepresent rival groups. The more biased away from neutral truth, the better the communication functions to affirm coalitional identity, generating polarization in excess of actual policy disagreements. Communications of practical and functional truths are generally useless as differential signals, because any honest person might say them regardless of coalitional loyalty. In contrast, unusual, exaggerated beliefs \\[...\\] are unlikely to be said except as expressive of identity, because there is no external reality to motivate nonmembers to speak absurdities.\n\n> \\-\\- John Tooby, \"[Coalitional Instincts](https://www.edge.org/conversation/john_tooby-coalitional-instincts)\"\n\n  \n\n> Humans interact in dense social networks, and this poses a problem for bystanders when conflicts arise: which side, if any, to support. Choosing sides is a difficult strategic problem because the outcome of a conflict critically depends on which side other bystanders support. One strategy is siding with the higher status disputant, which can allow bystanders to coordinate with one another to take the same side, reducing fighting costs. However, this strategy carries the cost of empowering high-status individuals to exploit others. A second possible strategy is choosing sides based on preexisting relationships. This strategy balances power but carries another cost: Bystanders choose different sides, and this discoordination causes escalated conflicts and high fighting costs. We propose that moral cognition is designed to manage both of these problems by implementing a dynamic coordination strategy in which bystanders coordinate side-taking based on a public signal derived from disputants’ actions rather than their identities. By focusing on disputants’ actions, bystanders can dynamically change which individuals they support across different disputes, simultaneously solving the problems of coordination and exploitation.\n\n> \\-\\- Peter DeScioli & Robert Kurzban, \" [A Solution to the Mysteries of Morality](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.840.3768&rep=rep1&type=pdf)\""
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "HXA9WxPpzZCCEwXHT",
    "name": "Alief",
    "core": false,
    "slug": "alief",
    "oldSlugs": null,
    "postCount": 20,
    "description": {
      "markdown": "An **alief** is a belief-like attitude, behavior, or expectation that can coexist with a contradictory belief. For example, the fear felt when a monster jumps out of the darkness in a scary movie is based on the alief that the monster is about to attack you, even though you believe that it cannot.\n\nPhilospoher Tamar Gendler introduced the word in her 2008 paper *Alief and Belief* as a sort of pun on [dual process theory](https://www.lesswrong.com/tag/dual-process-theory-system-1-and-system-2); what beliefs (\"B-liefs\") are to system 2, aliefs (\"A-liefs\") are to system 1. Thus, beliefs are explicitly held beliefs which inform slow reasoning, while aliefs are implicit attitudes which guide fast reactions. However, dual process theory is not totally necessary to make sense of the term alief.\n\nGendler (2008) also introduced a related pun of \"cesire vs desire\"; a desire (\"D-zire\") is an explicit want which enters into explicit planning, while a cesire (\"C-zire\") is an implicit one which guides reactions.\n\n*Related tags:* [Belief](https://www.lesswrong.com/tag/belief), [Emotion](https://www.lesswrong.com/tag/emotions), \n\nBlog posts\n----------\n\n*   [The Mystery of the Haunted Rationalist](https://www.lesswrong.com/lw/1l/the_mystery_of_the_haunted_rationalist/)\n*   [Living Luminously](https://www.lesswrong.com/lw/1xh/living_luminously/)\n\nSee also\n--------\n\n*   [Hollywood rationality](https://www.lesswrong.com/tag/hollywood-rationality)\n*   [Corrupted hardware](https://www.lesswrong.com/tag/corrupted-hardware)\n*   [Separate magisteria](https://wiki.lesswrong.com/wiki/Separate_magisteria)\n*   [Living Luminously (sequence)](https://wiki.lesswrong.com/wiki/Living_Luminously_(sequence))"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "nzvHaqwdXtvWkbonG",
    "name": "Risks of Astronomical Suffering (S-risks)",
    "core": false,
    "slug": "risks-of-astronomical-suffering-s-risks",
    "oldSlugs": [
      "risks-of-astronomical-suffering-s-risks"
    ],
    "postCount": 34,
    "description": {
      "markdown": "**(Astronomical) suffering risks**, also known as **s-risks**, are risks of the creation of intense suffering in the far future on an astronomical scale, vastly exceeding all suffering that has existed on Earth so far.\n\nS-risks are an example of [existential risk](https://www.lesswrong.com/tag/existential-risk) (also known as *x-risks*) according to Nick Bostrom's original definition, as they threaten to \"permanently and drastically curtail \\[Earth-originating intelligent life's\\] potential\". Most existential risks are of the form \"event E happens which drastically reduces the number of conscious experiences in the future\". S-risks therefore serve as a useful reminder that some x-risks are scary because they cause *bad* experiences, and not just because they prevent good ones.\n\nWithin the space of x-risks, we can distinguish x-risks that are s-risks, x-risks involving human extinction, x-risks that involve immense suffering *and* human extinction, and x-risks that involve neither. For example:\n\n<table><tbody><tr><td>&nbsp;</td><td><strong>extinction risk</strong></td><td><strong>non-extinction risk</strong></td></tr><tr><td><strong>suffering risk</strong></td><td>Misaligned AGI wipes out humans, simulates many suffering alien civilizations.</td><td>Misaligned AGI tiles the universe with experiences of severe suffering.</td></tr><tr><td><strong>non-suffering risk</strong></td><td>Misaligned AGI wipes out humans.</td><td>Misaligned AGI keeps humans as \"pets,\" limiting growth but not causing immense suffering.</td></tr></tbody></table>\n\nA related concept is [**hyperexistential risk**](https://arbital.com/p/hyperexistential_separation/), the risk of \"fates worse than death\" on an astronomical scale. It is not clear whether all hyperexistential risks are s-risks per se. But arguably all s-risks are hyperexistential, since \"tiling the universe with experiences of severe suffering\" would likely be worse than death.\n\nThere are two [EA](https://wiki.lesswrong.com/wiki/EA) organizations with s-risk prevention research as their primary focus: the [Center on Long-Term Risk](https://www.lesswrong.com/tag/center-on-long-term-risk-clr) (CLR) and the [Center for Reducing Suffering](https://centerforreducingsuffering.org/). Much of CLR's work is on suffering-focused [AI safety](https://wiki.lesswrong.com/wiki/AI_safety) and [crucial considerations](https://www.lesswrong.com/tag/crucial-considerations). Although to a much lesser extent, the [Machine Intelligence Research Institute](https://www.lesswrong.com/tag/machine-intelligence-research-institute-miri) and [Future of Humanity Institute](https://www.lesswrong.com/tag/future-of-humanity-institute-fhi) have investigated strategies to prevent s-risks too. \n\nAnother approach to reducing s-risk is to \"expand the moral circle\" [*together*](https://magnusvinding.com/2018/09/04/moral-circle-expansion-might-increase-future-suffering/) with raising concern for suffering, so that future (post)human civilizations and AI are less likely to [instrumentally](https://www.lesswrong.com/tag/instrumental-value) cause suffering to non-human minds such as animals or digital sentience. [Sentience Institute](http://www.sentienceinstitute.org/) works on this value-spreading problem.\n\nSee also\n--------\n\n*   [Center on Long-Term Risk](https://www.lesswrong.com/tag/center-on-long-term-risk-clr)\n*   [Existential risk](https://www.lesswrong.com/tag/existential-risk)\n*   [Abolitionism](https://www.lesswrong.com/tag/abolitionism)\n*   [Mind crime](https://wiki.lesswrong.com/wiki/Mind_crime)\n*   [Utilitarianism](https://www.lesswrong.com/tag/utilitarianism), [Hedonism](https://www.lesswrong.com/tag/hedonism)\n\nExternal links\n--------------\n\n*   [Reducing Risks of Astronomical Suffering: A Neglected Global Priority (FRI)](https://foundational-research.org/reducing-risks-of-astronomical-suffering-a-neglected-global-priority/)\n*   [Introductory talk on s-risks (FRI)](https://foundational-research.org/s-risks-talk-eag-boston-2017/)\n*   [Risks of Astronomical Future Suffering (FRI)](https://foundational-research.org/risks-of-astronomical-future-suffering/)\n*   [Suffering-focused AI safety: Why \"fail-safe\" measures might be our top intervention PDF (FRI)](https://foundational-research.org/files/suffering-focused-ai-safety.pdf)\n*   [Artificial Intelligence and Its Implications for Future Suffering (FRI)](https://foundational-research.org/artificial-intelligence-and-its-implications-for-future-suffering)\n*   [Expanding our moral circle to reduce suffering in the far future (Sentience Politics)](https://sentience-politics.org/expanding-moral-circle-reduce-suffering-far-future/)\n*   [The Importance of the Far Future (Sentience Politics)](https://sentience-politics.org/philosophy/the-importance-of-the-future/)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "LaDu5bKDpe8LxaR7C",
    "name": "Suffering",
    "core": false,
    "slug": "suffering",
    "oldSlugs": null,
    "postCount": 72,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "TkZ7MFwCi4D63LJ5n",
    "name": "Software Tools",
    "core": false,
    "slug": "software-tools",
    "oldSlugs": null,
    "postCount": 111,
    "description": {
      "markdown": "Specific pieces of software (downloadable or cloud/browser-based) that may be of interest to people on this site. The focus is on software with a practical application: for games, see [Gaming (videogames/tabletop)](https://www.lesswrong.com/tag/gaming-videogames-tabletop).\n\n**Related Sequences:** [Kickstarter for Coordinated Action](https://www.lesswrong.com/s/vz9Zrj3oBGsttG3Jh)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "DLskYNGdAGDFpxBF8",
    "name": "Dual Process Theory (System 1 & System 2)",
    "core": false,
    "slug": "dual-process-theory-system-1-and-system-2",
    "oldSlugs": [
      "dual-process-theory-system-1-and-system-2"
    ],
    "postCount": 23,
    "description": {
      "markdown": "**Dual Process Theory** posits two types of processes in the human brain. According to one characterization, **Type 2** (also known as **System 2**) processes are those which require working memory, and **Type 1** (also known as **System 1**) are those which not.\n\n> The terms System 1 and System 2 were originally coined by the psychologist Keith Stanovich and then popularized by Daniel Kahneman in his book _Thinking, Fast and Slow._ Stanovich noted that a number of fields within psychology had been developing various kinds of theories distinguishing between fast/intuitive on the one hand and slow/deliberative thinking on the other. Often these fields were not aware of each other. The S1/S2 model was offered as a general version of these specific theories, highlighting features of the two modes of thought that tended to appear in all the theories.\n\n> Since then, academics have continued to discuss the models. Among other developments, _Stanovich and other authors have discontinued the use of the System 1/System 2 terminology as misleading_, choosing to instead talk about Type 1 and Type 2 processing. \\[...\\] there’s no single “System 1”: rather, a wide variety of different processes and systems are lumped together under this term. It is also unclear whether there is any single System 2, either. \\[...\\]\n\n> People sometimes refer to Type 1 reasoning as biased, and to Type 2 reasoning as unbiased. But \\[...\\] there is nothing that makes one of the two types intrinsically more or less biased than the other. The bias-correction power of Type 2 processing emerges from the fact that _if_ Type 1 operations are known to be erroneous _and_ a rule-based procedure for correcting them exists, a Type 2 operation can be learned which implements that rule.\n\n> \\-\\- Kaj Sotala, [Against \"System 1\" and \"System 2\"](https://www.lesswrong.com/posts/HbXXd2givHBBLxr3d/against-system-1-and-system-2-subagent-sequence)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "fH8jPjHF2R27sRTTG",
    "name": "Education",
    "core": false,
    "slug": "education",
    "oldSlugs": null,
    "postCount": 153,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "dHNS6r6LD6s2hEvZz",
    "name": "AI Services (CAIS)",
    "core": false,
    "slug": "ai-services-cais",
    "oldSlugs": [
      "ai-services",
      "ai-services-cais"
    ],
    "postCount": 11,
    "description": {
      "markdown": "An **AI service** as used in the context of Eric Drexler's technical report [Reframing Superintelligence: Comprehensive AI Services as General Intelligence](https://www.fhi.ox.ac.uk/wp-content/uploads/Reframing_Superintelligence_FHI-TR-2019-1.1-1.pdf?asd=sa) (CAIS), is an AI system that delivers bounded results for some task using bounded resources in bounded time. It is contrasted with agentive AGI, which carries out open-ended goals over an unbounded period of time.\n\nA gradual accumulation of increasingly competent services is one model of how AI might develop. For a summary, see [this post](https://www.lesswrong.com/posts/x3fNwSe5aWZb5yXEG/reframing-superintelligence-comprehensive-ai-services-as)."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "ac84EpK6mZbPLzmqj",
    "name": "General Intelligence",
    "core": false,
    "slug": "general-intelligence",
    "oldSlugs": [
      "intelligence-general-concept"
    ],
    "postCount": 76,
    "description": {
      "markdown": "**General Intelligence** or **Universal Intelligence** is the ability to efficiently achieve goals in a wide range of domains. \n\nThis tag is specifically for discussing intelligence in the broad sense: for discussion of IQ testing and psychometric intelligence, see [IQ / g-factor](https://www.lesswrong.com/tag/iq-g-factor); for discussion about e.g. specific results in artificial intelligence, see [AI](https://www.lesswrong.com/tag/ai). These tags may overlap with this one to the extent that they discuss the nature of general intelligence.\n\nExamples of posts that fall under this tag include [The Power of Intelligence](https://www.lesswrong.com/posts/aiQabnugDhcrFtr9n/the-power-of-intelligence), [Measuring Optimization Power](https://www.lesswrong.com/posts/Q4hLMDrFd8fbteeZ8/measuring-optimization-power), [Adaption-Executers not Fitness Maximizers](https://www.lesswrong.com/posts/XPErvb8m9FapXCjhA/adaptation-executers-not-fitness-maximizers), [Distinctions in Types of Thought](https://www.lesswrong.com/posts/FbQ9Y9pBif5xZ7w2f/distinctions-in-types-of-thought), [The Octopus, the Dolphin and Us: a Great Filter tale](https://www.lesswrong.com/posts/GMqZ2ofMnxwhoa7fD/the-octopus-the-dolphin-and-us-a-great-filter-tale).\n\nOn the difference between psychometric intelligence (IQ) and general intelligence:\n\n> But the word “intelligence” commonly evokes pictures of the starving professor with an IQ of 160 and the billionaire CEO with an IQ of merely 120. Indeed there are differences of individual ability apart from “book smarts” which contribute to relative success in the human world: enthusiasm, social skills, education, musical talent, rationality. Note that each factor I listed is cognitive. Social skills reside in the brain, not the liver. And jokes aside, you will not find many CEOs, nor yet professors of academia, who are chimpanzees. You will not find many acclaimed rationalists, nor artists, nor poets, nor leaders, nor engineers, nor skilled networkers, nor martial artists, nor musical composers who are mice. Intelligence is the foundation of human power, the strength that fuels our other arts.\n\n> \\-\\- Eliezer Yudkowsky, [Artificial Intelligence as a Positive and Negative Factor in Global Risk](https://intelligence.org/files/AIPosNegFactor.pdf)\n\nDefinitions of General Intelligence\n-----------------------------------\n\nAfter reviewing extensive literature on the subject, Legg and Hutter^[\\[1\\]](#fnosnb04qur8)^ summarizes the many possible valuable definitions in the informal statement “Intelligence measures an agent’s ability to achieve goals in a wide range of environments.” They then show this definition can be mathematically formalized given reasonable mathematical definitions of its terms. They use [Solomonoff induction](https://lessestwrong.com/tag/solomonoff-induction) \\- a formalization of [Occam's razor](https://lessestwrong.com/tag/occam-s-razor) \\- to construct an [universal artificial intelligence](https://lessestwrong.com/tag/aixi) with a embedded [utility function](https://lessestwrong.com/tag/utility-functions) which assigns less [utility](https://lessestwrong.com/tag/expected-utility) to those actions based on theories with higher [complexity](https://wiki.lesswrong.com/wiki/Kolmogorov_complexity). They argue this final formalization is a valid, meaningful, informative, general, unbiased, fundamental, objective, universal and practical definition of intelligence.\n\nWe can relate Legg and Hutter's definition with the concept of [optimization](https://lessestwrong.com/tag/optimization). According to [Eliezer Yudkowsky](https://lessestwrong.com/tag/eliezer-yudkowsky) intelligence is [efficient cross-domain optimization](https://lessestwrong.com/lw/vb/efficient_crossdomain_optimization/). It measures an agent's capacity for efficient cross-domain optimization of the world according to the agent’s preferences.^[\\[2\\]](#fn7hbpdfpe6x3)^ Optimization measures not only the capacity to achieve the desired goal but also is inversely proportional to the amount of resources used. It’s the ability to steer the future so it hits that small target of desired outcomes in the large space of all possible outcomes, using fewer resources as possible. For example, when Deep Blue defeated Kasparov, it was able to hit that small possible outcome where it made the right order of moves given Kasparov’s moves from the very large set of all possible moves. In that domain, it was more optimal than Kasparov. However, Kasparov would have defeated Deep Blue in almost any other relevant domain, and hence, he is considered more intelligent.\n\nOne could cast this definition in a possible world vocabulary, intelligence is:\n\n1.  the ability to precisely realize one of the members of a small set of possible future worlds that have a higher preference over the vast set of all other possible worlds with lower preference; while\n2.  using fewer resources than the other alternatives paths for getting there; and in the\n3.  most diverse domains as possible.\n\nHow many more worlds have a higher preference then the one realized by the agent, less intelligent he is. How many more worlds have a lower preference than the one realized by the agent, more intelligent he is. (Or: How much smaller is the set of worlds at least as preferable as the one realized, more intelligent the agent is). How much less paths for realizing the desired world using fewer resources than those spent by the agent, more intelligent he is. And finally, in how many more domains the agent can be more efficiently optimal, more intelligent he is. Restating it, the intelligence of an agent is directly proportional to:\n\n*   (a) the numbers of worlds with lower preference than the one realized,\n*   (b) how much smaller is the set of paths more efficient than the one taken by the agent and\n*   (c) how more wider are the domains where the agent can effectively realize his preferences;\n\nand it is, accordingly, inversely proportional to:\n\n*   (d) the numbers of world with higher preference than the one realized,\n*   (e) how much bigger is the set of paths more efficient than the one taken by the agent and\n*   (f) how much more narrow are the domains where the agent can efficiently realize his preferences.\n\nThis definition avoids several problems common in many others definitions, especially it avoids [anthropomorphizing](https://lessestwrong.com/tag/anthropomorphism) intelligence.\n\nSee Also\n--------\n\n*   [Optimization process](https://lessestwrong.com/tag/optimization)\n*   [Decision theory](https://lessestwrong.com/tag/decision-theory)\n*   [Rationality](https://lessestwrong.com/tag/rationality)\n*   [Legg and Hutter paper “Universal Intelligence: A Deﬁnition of Machine Intelligence”](http://arxiv.org/pdf/0712.3329.pdf)\n\n1.  ^**[^](#fnrefosnb04qur8)**^\n    \n    http://arxiv.org/pdf/0712.3329.pdf\n    \n2.  ^**[^](#fnref7hbpdfpe6x3)**^\n    \n    http://intelligence.org/files/IE-EI.pdf"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "ouT6wKhACJRouGokM",
    "name": "Moral Uncertainty",
    "core": false,
    "slug": "moral-uncertainty",
    "oldSlugs": null,
    "postCount": 56,
    "description": {
      "markdown": "**Moral uncertainty** (or **normative uncertainty**) is uncertainty about what we ought, morally, to do given the diversity of moral doctrines. For example, suppose that we knew for certain that new technology would enable more humans to live on another planet with slightly less well-being than on Earth[^1^](https://www.lesswrong.com/tag/moral-uncertainty#fn1). An average [utilitarian](https://www.lesswrong.com/tag/utilitarianism) would consider these consequences bad, while a total utilitarian would endorse such technology. If we are uncertain about which of these two theories are right, what should we do?\n\nMoral uncertainty includes a level of uncertainty above the more usual uncertainty of [what to do given incomplete information](https://www.lesswrong.com/tag/decision-theory) since it deals also with uncertainty about which moral theory is right. Even with complete information about the world, this kind of uncertainty would still remain [^1^](https://www.lesswrong.com/tag/moral-uncertainty#fn1). In one level of uncertainty, one can have doubts on how to act because all the relevant empirical information isn’t available, for example, choosing whether to implement or not a new technology (e.g.: [AGI](https://www.lesswrong.com/tag/artificial-general-intelligence), [Biological Cognitive Enhancement](https://www.lesswrong.com/tag/nootropics-and-other-cognitive-enhancement), [Mind Uploading](https://www.lesswrong.com/tag/whole-brain-emulation)) not fully knowing about its consequences and nature. But even if we ideally get to know each and every consequence of new technology, we would still need to know which is the right ethical perspective for analyzing these consequences.\n\nOne approach is to follow only the most probable theory. This has its own problems. For example, what if the most probable theory points only weakly in one way, and other theories point strongly the other way? A better approach is to “perform the action with the highest expected moral value. We get the expected moral value of an action by multiplying the subjective probability that some theory is true by the value of that action if it is true, doing the same for all of the other theories, and adding up the results.” [^2^](https://www.lesswrong.com/tag/moral-uncertainty#fn2) However, we would still need a method of comparing value intertheories, an [utilon](https://www.lesswrong.com/tag/utility) in one theory may not be the same with an utilon in another theory. Outside [consequentialism](https://www.lesswrong.com/tag/consequentialism), many ethical theories don’t use utilions or even any quantifiable values. This is still an open problem.\n\n[Nick Bostrom](https://www.lesswrong.com/tag/nick-bostrom) and [Toby Ord](https://en.wikipedia.org/wiki/Toby_Ord) have proposed a [parliamentary model](http://www.overcomingbias.com/2009/01/moral-uncertainty-towards-a-solution.html). In this model, each theory sends a number of delegates to a parliament in proportion to its probability. The theories then bargain for support as if the probability of each action were proportional to its votes. However, the actual output is always the action with the most votes. Bostrom and Ord's proposal lets probable theories determine most actions, but still gives less probable theories influence on issues they consider unusually important.\n\nEven with a high degree of moral uncertainty and a wide range of possible moral theories, there are still certain actions that seem highly valuable in any theory. Bostrom argues that [Existential risk](https://www.lesswrong.com/tag/existential-risk) reduction is among them, showing that it is not only the most important task given most versions of consequentialism but highly recommended by many of the other widely acceptable moral theories[^3^](https://www.lesswrong.com/tag/moral-uncertainty#fn3).\n\nExternal links\n--------------\n\n*   [Moral uncertainty — towards a solution?](http://www.overcomingbias.com/2009/01/moral-uncertainty-towards-a-solution.html)\n\nSequences\n---------\n\n*   [Moral uncertainty](https://www.lesswrong.com/s/4NFwxwzLzpiikfkk3)\n\nSee also\n--------\n\n*   [Expected utility](https://www.lesswrong.com/tag/expected-utility)\n*   [Value learning](https://www.lesswrong.com/tag/value-learning)\n*   [Metaethics](https://www.lesswrong.com/tag/metaethics)\n\nReferences\n----------\n\n1.  Crouch, William. (2010) “Moral Uncertainty and Intertheoretic Comparisons of Value” BPhil Thesis, 2010. p. 6. Available at: [http://oxford.academia.edu/WilliamCrouch/Papers/873903/Moral\\_Uncertainty\\_and\\_Intertheoretic\\_Comparisons\\_of\\_Value](http://oxford.academia.edu/WilliamCrouch/Papers/873903/Moral_Uncertainty_and_Intertheoretic_Comparisons_of_Value)[↩](https://www.lesswrong.com/tag/moral-uncertainty#fnref1)\n2.  Sepielli, Andrew. (2008) “Moral Uncertainty and the Principle of Equity among Moral Theories\". ISUS-X, Tenth Conference of the International Society for Utilitarian Studies, Kadish Center for Morality, Law and Public Affairs, UC Berkeley. Available at: [http://escholarship.org/uc/item/7h5852rr.pdf](http://escholarship.org/uc/item/7h5852rr.pdf)[↩](https://www.lesswrong.com/tag/moral-uncertainty#fnref2)\n3.  Bostrom, Nick. (2012) \"Existential Risk Reduction as the Most Important Task for Humanity\" Global Policy, forthcoming, 2012. p. 22. Available at: [http://www.existential-risk.org/concept.pdf](http://www.existential-risk.org/concept.pdf)[↩](https://www.lesswrong.com/tag/moral-uncertainty#fnref3)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "nSHiKwWyMZFdZg5qt",
    "name": "Ethics & Morality",
    "core": false,
    "slug": "ethics-and-morality",
    "oldSlugs": null,
    "postCount": 288,
    "description": {
      "markdown": "For general discussion about **ethics and morality**. Example posts: [Fake Morality](https://www.lesswrong.com/posts/fATPBv4pnHC33EmJ2/fake-morality); [What Would You Do Without Morality?](https://www.lesswrong.com/posts/iGH7FSrdoCXa5AHGs/what-would-you-do-without-morality); [The Terrible, Horrible, No Good, Very Bad Truth About Morality and What To Do About It](https://www.lesswrong.com/posts/B5K3hg8FgrMDHuXjH/the-terrible-horrible-no-good-very-bad-truth-about-morality); [Why Are Individual IQ Differences OK?](https://www.lesswrong.com/posts/faHbrHuPziFH7Ef7p/why-are-individual-iq-differences-ok); [Morality is Awesome](https://www.lesswrong.com/posts/Aq8BQMXRZX3BoFd4c/morality-is-awesome).\n\nSee also [Consequentialism](https://www.lesswrong.com/tag/consequentialism), [Deontology](https://www.lesswrong.com/tag/deontology), [Metaethics](https://www.lesswrong.com/tag/metaethics), and [Moral Uncertainty](https://www.lesswrong.com/tag/moral-uncertainty)."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "4cKQgA4S7xfNeeWXg",
    "name": "IQ and g-factor",
    "core": false,
    "slug": "iq-and-g-factor",
    "oldSlugs": [
      "iq-g-factor"
    ],
    "postCount": 53,
    "description": {
      "markdown": "**IQ** is a score derived from a set of standardized tests designed to assess human intelligence. The **g-factor** (general intelligence factor) is the underlying psychometric construct that the IQ tests are trying to measure.\n\nThis tag is specifically for discussions about these formal constructs. For discussions about artificial intelligence, see [AI](https://www.lesswrong.com/tag/ai). For discussions about human-level intelligence in a broader sense, see [General Intelligence](https://www.lesswrong.com/tag/general-intelligence)."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "BhfefamXXee6c2CH8",
    "name": "Transcripts",
    "core": false,
    "slug": "transcripts",
    "oldSlugs": null,
    "postCount": 35,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "8e9e8fzXuW5gGBS3F",
    "name": "Qualia",
    "core": false,
    "slug": "qualia",
    "oldSlugs": null,
    "postCount": 34,
    "description": {
      "markdown": "Subjective conscious experience. The discussion of qualia tends to come up on LessWrong in two contexts: as an argument against reductionism (with a claim that qualia cannot be a mere matter of, uh, matter), and as a key factor in how seriously we should weight the suffering of animals. There is also a third context for Qualia arguments: as an argument against Whole Brain Emulation being you, or of emulations of minds not being conscious and instead P-zonbies.\n\nThere are several philosophical discussions of how to reduce qualia into functions of the brain (most famously by Daniel Dennett), but the discussion continues among philosophers."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "25oxqHiadqM6Hf7Gn",
    "name": "Great Filter",
    "core": false,
    "slug": "great-filter",
    "oldSlugs": null,
    "postCount": 33,
    "description": {
      "markdown": "The **Great Filter** is a proposed explanation for the [Fermi Paradox](http://en.wikipedia.org/wiki/Fermi_paradox). The development of intelligent life requires many steps, such as the emergence of single-celled life and the transition from unicellular to multicellular life forms. Since we have not observed intelligent life beyond our planet, there seems to be a developmental step that is so difficult and unlikely that it \"filters out\" nearly all civilizations before they can reach a space-faring stage. Robin Hanson coined the term in his 1998 essay [The Great Filter - Are We Almost Past It?](http://hanson.gmu.edu/greatfilter.html).\n\n From that essay:\n\n> Humanity seems to have a bright future, i.e., a non-trivial chance of expanding to fill the universe with lasting life. But the fact that space near us seems dead now tells us that any given piece of dead matter faces an astronomically low chance of begating such a future. There thus exists a great filter between death and expanding lasting life, and humanity faces the ominous question: how far along this filter are we? \n\nShould we worry?\n----------------\n\nThe Great Filter might be a step in our evolutionary past, in which case our civilization has already passed it. But the hard step might also be ahead of us: surviving the creation of nuclear bombs, [AGI](https://wiki.lesswrong.com/wiki/AGI), biotechnology, [nanotechnology](https://lessestwrong.com/tag/nanotechnology) or an asteroid impact [1](http://www.global-catastrophic-risks.com/docs/Chap01.pdf). In that case, we should be worried, as the Great Filter seems to have been successful in stopping the development of every other civilization so far. Estimating the location of the Great Filter is thus important for helping estimate the magnitude of [existential risk](https://lessestwrong.com/tag/existential-risk). [Many](http://hanson.gmu.edu/greatfilter.html) [efforts](http://hanson.gmu.edu/hardstep.pdf) [have](http://www.stat.berkeley.edu/~aldous/Papers/GF.pdf) [been](http://www.nickbostrom.com/papers/fermi.pdf) [made](http://www.global-catastrophic-risks.com/docs/Chap01.pdf) [in](http://meteuphoric.wordpress.com/2010/03/23/sia-doomsday-the-filter-is-ahead/) that direction, but much remains uncertain.\n\nTraces of life on other planets are evidence for a later Great Filter[2](http://www.youtube.com/watch?v=_W8zu7lFmhY). If we were to find that complex life had evolved independently both on Earth and some other planet, it would suggest that getting to such a developmental stage was relatively easy. Thus the Great Filter would have to be at a later stage.\n\nThe study of [past mass extinctions](http://en.wikipedia.org/wiki/Extinction_event#Major_extinction_events) and astrobiology can provide ideas for estimating the location of the Great Filter. However, there are many difficulties involved. For instance, the time that it takes to pass a step doesn't reveal much about how easy or hard that step was. Robin Hanson gives the following example in his [seminal paper](http://hanson.gmu.edu/greatfilter.html):\n\n\"…say you have one hour to pick five locks by trial and error, locks with 1,2,3,4, and 5 dials of ten numbers, so that the expected time to pick each lock is .01,.1, 1, 10, and 100 hours respectively. Then just looking at those rare cases when you do pick all five locks in the hour, the average time to pick the first two locks would be .0096 and .075 hours respectively, close to the usual expected times of .01 and .1 hours. The average time to pick the third lock, however, would be .20 hours, and the average time for the other two locks, and the average time left over at the end, would be .24 hours. That is, conditional on success, all the hard steps, no matter how hard, take about the same time, while easy steps take about their usual time.\"\n\nConsequences\n------------\n\n[In a subsequent paper](http://hanson.gmu.edu/hardstep.pdf), Hanson constructs a simulation of the distribution of the hard steps, which suggests that there should be about four to seven hard steps, uniformly distributed in our past. It also suggests that there has been at least one hard step since the evolution of hominids, and that the best extinction model that fits all these requirements is [William Schopf's model](http://www.pnas.org/content/91/15/6735.full.pdf). Taking evolutionary arguments for [AGI](https://wiki.lesswrong.com/wiki/AGI) and [observation selection effects](https://lessestwrong.com/tag/observation-selection-effect) together, [Bostrom and Shulman argue](http://www.nickbostrom.com/aievolution.pdf) that Hanson’s results can help estimate the difficulty of creating AGI.\n\nBlog posts\n----------\n\n*   [Very Bad News](http://www.overcomingbias.com/2010/03/very-bad-news.html) by [Robin Hanson](https://lessestwrong.com/tag/robin-hanson)\n*   [An empirical test of anthropic principle / great filter reasoning](https://lessestwrong.com/lw/1z8/an_empirical_test_of_anthropic_principle_great/) by James Miller\n*   [SIA won't doom you](https://lessestwrong.com/lw/1zj/sia_wont_doom_you/) by Stuart Armstrong\n*   [Late Great Filter is not bad news](https://lessestwrong.com/lw/214/late_great_filter_is_not_bad_news/) by Wei Dai\n*   [Planets in the habitable zone, the Drake Equation, and the Great Filter](https://lessestwrong.com/lw/7w8/planets_in_the_habitable_zone_the_drake_equation/) by JoshuaZ\n*   [Beware Future Filters](http://www.overcomingbias.com/2010/11/beware-future-filters.html) by [Robin Hanson](https://lessestwrong.com/tag/robin-hanson)\n\nExternal links\n--------------\n\n*   Robin Hanson’s Great Filter original paper: [The Great Filter - Are We Almost Past It?](http://hanson.gmu.edu/greatfilter.html)\n*   A simulation of the hard steps distribution: [Must Early Life Be Easy? The Rhythm of Major Evolutionary Transitions](http://hanson.gmu.edu/hardstep.pdf) by Robin Hanson\n*   Strong candidates for present Great Filters: [Introduction of the book “Global Catastrophic Risks”, summarizing it](http://www.global-catastrophic-risks.com/docs/Chap01.pdf) by Nick Bostrom\n*   [SIA Doomsday: The filter is ahead](http://meteuphoric.wordpress.com/2010/03/23/sia-doomsday-the-filter-is-ahead/) by Katja Grace\n*   An audio with Bostrom talking about how finding traces of life on mars is terrible bad news: [Nick Bostrom on life on Mars](http://www.youtube.com/watch?v=_W8zu7lFmhY)\n\nSee also\n--------\n\n*   [Existential risk](https://lessestwrong.com/tag/existential-risk)\n*   [Doomsday argument](https://lessestwrong.com/tag/doomsday-argument)\n*   [Self Indication Assumption](https://lessestwrong.com/tag/self-indication-assumption)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "XJjvxWB68GYpts93N",
    "name": "Nanotechnology",
    "core": false,
    "slug": "nanotechnology",
    "oldSlugs": null,
    "postCount": 18,
    "description": {
      "markdown": "**Nanotechnology** is the field of study concerned with the manipulation of matter at an atomic and molecular scale. Tipically, this involves structures with sizes ranging from 1 to 100 nanometres. It is currently one of the most well-funded areas worlwide. The term was first coined in 1974 by Norio Taniguchi.\n\nThe emergence of nanotechnology as a field by itself was the result of the convergence of several lines of work. These include the development of the [scanning tunneling microscope](http://en.wikipedia.org/wiki/Scanning_tunneling_microscope) by Gerd Binnigg and Einrich Rohrer in 1980s, Richard Feynman's talk \"There's plenty of room at the Bottom\" in 1959 and Eric Drexler suggestions of molecular manipulation in the 70s.\n\nThe field of nanotechonology has led to the development of a huge amount of new technologies and the improvement of old methods. From drug-delivering systems to electronic chips development, there are nowadays hundreds of avaliable of functional applications stemming from this area. Besides the size and mobility advantages of such devices and technologies, the fundamental quantum properties that emerge at nano scales continue to defy researchers to speculate of further developments.\n\nDrexler has proposed, with his *molecular nanotechnology*, that the field could evolve to exploit more than just this scale properties, this pure nanomaterials research. His suggestions, highly speculative, include research on the ability of developing means of mechanosynthesis - such as having miniature production lines using machines to build structures. This would allow, for example, the precise control of chemical reactions, eliminating the imprecision existing in conventional chemistry.\n\nWhen [discussing](http://intelligence.org/files/AIPosNegFactor.pdf) the development of [Friendly AI](https://wiki.lesswrong.com/wiki/Friendly_AI), Yudkowsky proposes that the unrestricted access to nanotechnology by an [Unfriendly artificial intelligence](https://www.lesswrong.com/tag/unfriendly-artificial-intelligence) could have catastrophic results for mankind.\n\nFurther Reading & References\n----------------------------\n\n*   Binnig, G.; Rohrer, H. (1986). \"Scanning tunneling microscopy\". IBM Journal of Research and Development 30: 4.\n*   \"Nanoscience and nanotechnologies: opportunities and uncertainties\". Royal Society and Royal Academy of Engineering. July 2004. Retrieved 13 May 2011.\n*   Allhoff, Fritz; Lin, Patrick; Moore, Daniel (2010). What is nanotechnology and why does it matter?: from science to ethics. John Wiley and Sons. pp. 3–5. ISBN 1-4051-7545-1.\n*   [There's Plenty of Room at the Bottom](http://www.zyvex.com/nanotech/feynman.html) by Richard Feynman\n\nSee also\n--------\n\n*   [Exploratory engineering](https://www.lesswrong.com/tag/exploratory-engineering)\n*   [Rational evidence](https://www.lesswrong.com/tag/rational-evidence), [Science](https://www.lesswrong.com/tag/science)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "xHjy88N2uJvGdgzfw",
    "name": "Health / Medicine / Disease",
    "core": false,
    "slug": "health-medicine-disease",
    "oldSlugs": [
      "medicine"
    ],
    "postCount": 144,
    "description": {
      "markdown": "**Health.** Note that, for convenience, posts relating to the 2019 coronavirus outbreak are instead found in [here](https://www.lesswrong.com/tag/coronavirus?showPostCount=true&useTagName=true)."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "ZpG9rheyAkgCoEQea",
    "name": "Practice & Philosophy of Science",
    "core": false,
    "slug": "practice-and-philosophy-of-science",
    "oldSlugs": [
      "practice-and-philosophy-of-science"
    ],
    "postCount": 166,
    "description": {
      "markdown": "**Practice and Philosophy of Science** is for posts that discuss how science is done or should be done; examples include [Building Intuitions on Non-Empirical Arguments in Science](https://www.lesswrong.com/posts/tSemJckYr29Gnxod2/building-intuitions-on-non-empirical-arguments-in-science) and the [Science and Rationality sequence](https://www.lesswrong.com/s/fxynfGCSHpY4FmBZy). (It is not for posts that simply report on a new scientific result.)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "qAvbtzdG2A2RBn7in",
    "name": "Effective Altruism",
    "core": false,
    "slug": "effective-altruism",
    "oldSlugs": null,
    "postCount": 170,
    "description": {
      "markdown": "**Effective Altruism** (EA) is a movement trying to invest time and money in causes that do the most possible good per unit investment. EA was at one point called **optimal philanthropy**.\n\nThe basic concept behind EA is that you would really struggle to donate 100 times more money or time to charity than you currently do--but (assuming you are approximately utilitarian), spending a little time researching who to donate to *could* have an impact on roughly this order of magnitude. The same argument works for doing good with your career or volunteer hours.\n\nConcepts\n--------\n\nDespite a broad diversity of ideas within the EA community on which areas are most pressing, there are a handful of criteria that are generally agreed make an area potentially impactful to work on (either directly or through donation). These are:\n\n*   The area is generally **neglected,** that is, it has capacity for more support either financially or in terms of skills\n*   The area has the potential for **large impact,** either in human lives saved, animal or human suffering alleviated, catastrophic crises averted, etc. Sometimes this is called \"scale\"\n*   The area is **tractable**\\-\\- it is a solvable problem, or is solvable with minimal resource investment (relative to other   \n    problem areas)\n\nA fourth semi-area is:\n\n*   Does the individual have good **personal fit**?  Do they have unique skills which will make them more effective in an area.\n\nFrom this, we can see a vast number of charities do not meet all or indeed any of these criteria. A major issue with EA is that some areas are much easier to track progress in than others (think tracking the cost per life saved of malaria nets vs existential [AI](https://www.lesswrong.com/tag/ai?showPostCount=false) risk, for instance). What is clear, however, is that some of the more effective charities (of those which *are* easy to track) have [far more benefit over the average charity than people think](https://80000hours.org/2017/05/most-people-report-believing-its-incredibly-cheap-to-save-lives-in-the-developing-world/)\\-\\- perhaps as much as 10,000% as effective.\n\nA large portion of the EA community are by and large, **longtermist**. This refers to the idea that, if there are many future generations (100s, 1000s or more), and their lives are as valuable as ours, then even very small impacts on all of their lives-- or things like moving good changes forwards in time or bad ones back-- far outweigh impacts on people who are currently alive. Because this concept is less broadly-accepted than charity for currently-alive people, longtermist solutions are also generally considered to be neglected. Longtermist interventions generally focus on [S-risks](https://www.lesswrong.com/tag/risks-of-astronomical-suffering-s-risks) or [X-risks](https://www.lesswrong.com/tag/existential-risk).\n\nExamples of longtermist interventions include [AI](https://www.lesswrong.com/tag/ai) safety, pandemic preparedness, and [nanotechnology](https://www.lesswrong.com/tag/nanotechnology) security. Examples of other popular EA interventions include global poverty alleviation, malaria treatments, and vitamin supplementation in sub-saharan Africa.  \n \n\nThe **Effective Altruism movement** also has its own forum,   [**The EA Forum**](https://forum.effectivealtruism.org/). It runs on the same software as LessWrong.\n\nNotable Posts\n-------------\n\n*   [Efficient charity: do unto others...](https://lessestwrong.com/lw/3gj/efficient_charity_do_unto_others/)\n*   [Optimal philanthropy for human beings](https://lessestwrong.com/lw/6py/optimal_philanthropy_for_human_beings/)\n*   [Why we can't take expected value estimates literally (even when they're unbiased)](https://lessestwrong.com/lw/745/why_we_cant_take_expected_value_estimates/)\n\nTotal resources and how they are split\n--------------------------------------\n\n\\[effectivealtruismdata.com\\]\n\n\\[How much money 80k has an article on this\\]\n\n\\[include graph\\]\n\nImpact\n------\n\n### Global health and economic development\n\n\\[estimated total amount of lives saved\\]\n\nThe [Against Malaria Foundation](https://www.againstmalaria.com/) has distributed more than 70 million bednets to protect people (mostly children) from a debilitating parasite. ([Source](https://www.againstmalaria.com/Distributions.aspx)) \\[number of lives saved\\]\n\n[GiveDirectly](https://givedirectly.org/) has facilitated more than $100 million in direct cash transfers to families living in extreme poverty, who determine for themselves how best to spend the money. ([Source](https://www.givedirectly.org/financials/)) \\[number of lives saved\\]\n\nThe [Schistosomiasis Control Initiative](https://www.imperial.ac.uk/schistosomiasis-control-initiative) and [Deworm the World Initiative](http://www.evidenceaction.org/dewormtheworld/) invests in people's health and future well-being by treating preventable diseases that often get little attention. They have given out hundreds of millions of deworming treatments to fight intestinal parasites, which may help people earn higher incomes later in life. (Sources for [SCI](https://schistosomiasiscontrolinitiative.org/reach) and [DWI](https://www.evidenceaction.org/dewormtheworld-2/))\n\n### Animal welfare\n\n\\[how much animal welfare in some reasonable metric\\]\n\n[The Humane League](https://thehumaneleague.org/) and [Mercy for Animals](https://mercyforanimals.org/), alongside many other organizations, have orchestrated corporate campaigns and legal reforms to fight the use of battery cages. Because of this work, more than 100 million hens that would have been caged instead live cage-free. (This includes all cage-free reform work, of which a sizable fraction was funded by EA-aligned donors.)\n\n[The Good Food Institute](https://gfi.org/) works with scientists, entrepreneurs, and investors to develop and promote meat alternatives that don't require the suffering of farmed animals.\n\n### Existential risk and the long-term future\n\n\\[how much lower higher? risk of existential catastrphe as a result\\]\n\nOrganizations like the [Future of Humanity Institute](https://www.fhi.ox.ac.uk/) and the [Centre for the Study of Existential Risk](https://www.cser.ac.uk/) work on research and policy related to some of the biggest threats facing humanity, from pandemics and climate change to nuclear war and superintelligent AI systems.\n\nSome organizations in this space, like the [Center for Human-Compatible AI](https://humancompatible.ai/) and the [Machine Intelligence Research Institute](https://intelligence.org/), focus entirely on solving issues posed by advances in artificial intelligence. AI systems of the future could be very powerful and difficult to control --- a dangerous combination.\n\n[Sherlock Biosciences](https://sherlock.bio/) is developing a diagnostic platform that could reduce threats from viral pandemics. (They are a private company, but much of their capital comes from a [grant](https://www.openphilanthropy.org/focus/scientific-research/sherlock-biosciences-research-viral-diagnostics) made by Open Philanthropy, an EA-aligned grantmaker.)\n\nKey ideas\n---------\n\nCriticisms \n-----------\n\n*   EA is incoherent. Consequentialism applies to one's whole life, but many EAs don’t take it this seriously\n    *   This argument applies to virtue ethics too, but no one criticises it - “why aren’t you constantly seeking to always do the virtuous action”. People in practice seem to take statements from consequentialist philosophies more seriously than they do from others\n    *   It is more intellectually honest to surface incoherence in your worldview - \"I use 80% of my time as effectively as possible\" \n*   EA frames all value in terms of impact creation and this makes members sad^[\\[1\\]](#fnipt32j7op0s)^\n    *   How widespread is this?\n    *   Many EAs don't feel this way\n    *   Some people control orders of magnitude more resources than others. They could use their time and money to improve the lives of many other people. Whether they should is a different question, but it doesn't avoid the fact that this is true.\n*   EA supports a culture of guilt \\[Kerry thread\\]\n    *   How does EA compare in terms of mental wellbeing to other communities centred around \"doing good\" eg \"Protestant Work Ethic\" and \"Catholic Guilt\"?\n    *   If you struggle with this, consider reading[ Replacing Guilt](https://forum.effectivealtruism.org/s/a2LBRPLhvwB83DSGq), which is one of only 3 sequences with a permanent place sidebar of the EA Forum.\n*   EA is spending too much money\n    *   EA is spending *more* money but it's not immediately obvious it is spending too much. It might be spending too little. \\[Will MacAskill article\\]\n\n### Criticisms to add\n\n[Stefan Shubert's criticisms and responses](https://stefanfschubert.com/blog/2020/12/30/five-common-ea-self-criticisms-i-disagree-with)\n\nKuhn, Ben (2013) [A critique of effective altruism](https://www.benkuhn.net/ea-critique/), *Ben Kuhn’s Blog*, December 2.\n\nMcMahan, Jeff (2016) [Philosophical critiques of effective altruism](https://doi.org/10.5840/tpm20167379), *The Philosophers’ Magazine*, vol. 73, pp. 92–99.\n\nNielsen, Michael (2022) [Notes on effective altruism](https://michaelnotebook.com/eanotes/), *Michael’s Notebook*, June 2.\n\nRowe, Abraham (2022) [Critiques of EA that I want to read](https://forum.effectivealtruism.org/posts/n3WwTz4dbktYwNQ2j/critiques-of-ea-that-i-want-to-read), *Effective Altruism Forum*, June 19.\n\nWiblin, Robert & Keiran Harris (2019) [Vitalik Buterin on effective altruism, better ways to fund public goods, the blockchain’s problems so far, and how it could yet change the world](https://80000hours.org/podcast/episodes/vitalik-buterin-new-ways-to-fund-public-goods/), *80,000 Hours*, September 3.\n\nZhang, Linchuan (2021) [The motivated reasoning critique of effective altruism](https://forum.effectivealtruism.org/posts/pxALB46SEkwNbfiNS/the-motivated-reasoning-critique-of-effective-altruism), *Effective Altruism Forum*, September 14.\n\nRelated\n-------\n\n*   [Altruism](https://www.lesswrong.com/tag/altruism)\n*   [Cause Prioritization](https://www.lesswrong.com/tag/cause-prioritization)\n*   [Utilitarianism](https://lessestwrong.com/tag/utilitarianism)\n*   [S-risk](https://www.lesswrong.com/tag/risks-of-astronomical-suffering-s-risks)\n*   [X-risk](https://www.lesswrong.com/tag/existential-risk)\n\nExternal Resources\n------------------\n\n*   [80,000 Hours](https://80000hours.org/), who offer advice for how to have a maximally globally impactful career\n*   [Effective Altruism,](https://www.effectivealtruism.org/) who offer support for local EA groups, as well as articles and advice surrounding EA\n*   [GiveWell,](https://www.givewell.org/) a charity doing research into the effectiveness of other charities to provide information for donors\n*   [The Life You Can Save](https://www.thelifeyoucansave.org/the-book/?gclid=CjwKCAjwjqT5BRAPEiwAJlBuBXb3m1FKunezyfsYzYkjmgzSCHScRgZpzMH097cbAAGC5lmHUP-J3BoCcnAQAvD_BwE), a free eBook outlining reasons for donating more and more effectively\n*   [Center on Long-Term Risk](https://www.lesswrong.com/tag/center-on-long-term-risk-clr)\n\n1.  ^**[^](#fnrefipt32j7op0s)**^\n    \n    https://twitter.com/KerryLVaughan/status/1545063368695898112?s=20&t=xgaSuh22V6y44Wkcebo22Q"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "gHCNhqxuJq2bZ2akb",
    "name": "Social & Cultural Dynamics",
    "core": false,
    "slug": "social-and-cultural-dynamics",
    "oldSlugs": [
      "social-dynamics",
      "social-and-cultural-dynamics"
    ],
    "postCount": 255,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "NrvXXL3iGjjxu5B7d",
    "name": "Machine Intelligence Research Institute (MIRI)",
    "core": false,
    "slug": "machine-intelligence-research-institute-miri",
    "oldSlugs": null,
    "postCount": 134,
    "description": {
      "markdown": "The **Machine Intelligence Research Institute**, formerly known as the [Singularity Institute for Artificial Intelligence](https://wiki.lesswrong.com/wiki/Singularity_Institute_for_Artificial_Intelligence) (not to be confused with Singularity University) is a non-profit research organization devoted to reducing [existential risk](https://lessestwrong.com/tag/existential-risk) from [unfriendly artificial intelligence](https://lessestwrong.com/tag/unfriendly-artificial-intelligence) and understanding problems related to [friendly artificial intelligence](https://lessestwrong.com/tag/friendly-artificial-intelligence). [Eliezer Yudkowsky](https://lessestwrong.com/tag/eliezer-yudkowsky) was one of the early founders and continues to work there as a Research Fellow. The Machine Intelligence Research Institute created and currently owns the [LessWrong](https://www.lesswrong.com/about) domain.\n\nExternal Links\n\n*   [Homepage of the Machine Intelligence Research Institute](http://intelligence.org/)\n\nSee Also\n--------\n\n*   [Technological singularity](https://www.lesswrong.com/tag/singularity)\n*   [Existential risk](https://lessestwrong.com/tag/existential-risk)\n*   [Intelligence explosion](https://lessestwrong.com/tag/intelligence-explosion)\n*   [Friendly artificial intelligence](https://lessestwrong.com/tag/friendly-artificial-intelligence)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "wfW6iL96u26mbatep",
    "name": "Bounded Rationality",
    "core": false,
    "slug": "bounded-rationality",
    "oldSlugs": null,
    "postCount": 18,
    "description": {
      "markdown": "**Bounded Rationality** is rationality for bounded agents. Not (primarily) about \"modelling irrationality\": may include models of irrational behavior, but the aspiration of bounded rationality is to explain why this is in some sense the best a bounded agent can do, or, a rational approach for a bounded agent to take given its limited resources and knowledge. In other words, bounded rationality is a type of rationality, not a type of irrationality."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "X7v7Fyp9cgBYaMe2e",
    "name": "Center for Applied Rationality (CFAR)",
    "core": false,
    "slug": "center-for-applied-rationality-cfar",
    "oldSlugs": [
      "center-for-applied-rationality"
    ],
    "postCount": 74,
    "description": {
      "markdown": "The **Center for Applied Rationality (CFAR)** is a nonprofit research institute located in Berkeley, California, that was co-founded by [Eliezer Yudkowsky](https://www.lesswrong.com/users/eliezer_yudkowsky), with many top-contributors of LessWrong having participated in their workshops and many staff having written well-received posts on LessWrong."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "yTuPAtcPHSpc9r3kA",
    "name": "Impact Measures",
    "core": false,
    "slug": "impact-measures",
    "oldSlugs": [
      "impact-measures"
    ],
    "postCount": 49,
    "description": {
      "markdown": "**Impact measures** penalize an AI for affecting us too much.  To reduce the risk posed by a powerful AI, you might want to make it try accomplish its goals with as little impact on the world as possible. You reward the AI for crossing a room; to maximize time-discounted total reward, the optimal policy makes a huge mess as it sprints to the other side.  \n  \nHow do you rigorously define \"low impact\" in a way that a computer can understand – how do you measure impact? These questions are important for both prosaic and future AI systems: objective specification [is](https://www.lesswrong.com/posts/EbFABnst8LsidYs5Y/goodhart-taxonomy) [hard](https://www.lesswrong.com/posts/7b2RJJQ76hjZwarnj/specification-gaming-the-flip-side-of-ai-ingenuity); we don't want AI systems to rampantly disrupt their environment. In the limit of goal-directed intelligence, [theorems suggest that seeking power tends to be optimal](https://www.lesswrong.com/posts/6DuJxY8X45Sco4bS2/seeking-power-is-often-provably-instrumentally-convergent-in); we don't want highly capable AI systems to permanently wrench control of the future from us. \n\nCurrently, impact measurement research focuses on two approaches:\n\n*   [Relative reachability](https://arxiv.org/pdf/1806.01186.pdf): the AI preserves its ability to reach many kinds of world-states. The hope is that by staying able to reach many goal states, the AI stays able to reach the correct goal state.\n*   [Attainable](https://arxiv.org/abs/2006.06547) [utility](https://arxiv.org/abs/1902.09725) [preservation](https://www.lesswrong.com/posts/75oMAADr4265AGK3L/attainable-utility-preservation-concepts): the AI preserves its ability to achieve one or more auxiliary goals. The hope is that by penalizing gaining or losing control over the future, the AI doesn't take control away from us.\n\nFor a review of earlier work, see [A Survey of Early Impact Measures](https://www.lesswrong.com/s/nMGrhBYXWjPhZoyNL/p/TPy4RJvzogqqupDKk). \n\nSequences on impact measurement:\n\n*   [Reframing Impact](https://www.lesswrong.com/s/7CdoznhJaLEKHwvJW): we're impacted when we become more or less able to achieve our goals. Seemingly, goal-directed AI systems are only incentivized to catastrophically impact us in order to gain power to achieve their own goals. To avoid catastrophic impact, what if we penalize the AI for gaining power?\n*   [Subagents and Impact Measures](https://www.lesswrong.com/s/iRwYCpcAXuFD24tHh) explores how subagents can circumvent current impact measure formalizations.\n\nRelated tags: [Instrumental Convergence](https://www.lesswrong.com/tag/instrumental-convergence), [Corrigibility](https://www.lesswrong.com/tag/corrigibility), [Mild Optimization](https://www.lesswrong.com/tag/mild-optimization)."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "ksdiAMKfgSyEeKMo6",
    "name": "Academic Papers",
    "core": false,
    "slug": "academic-papers",
    "oldSlugs": null,
    "postCount": 102,
    "description": {
      "markdown": "Posts either linking to, or summarizing, formal papers published elsewhere."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "PbShukhzpLsWpGXkM",
    "name": "Anthropics",
    "core": false,
    "slug": "anthropics",
    "oldSlugs": null,
    "postCount": 192,
    "description": {
      "markdown": "**Anthropics** is the study of how the fact that we succeed in making observations of a given kind at all gives us evidence about the world we are living, independently of the content of the observations. As an example, for living beings, making any observations at all is only possible in a universe with physical laws that support life.\n\n**Related Pages:** [Sleeping Beauty Paradox](https://www.lesswrong.com/tag/sleeping-beauty-paradox), [Filtered Evidence](https://www.lesswrong.com/tag/filtered-evidence), [Great Filter](https://www.lesswrong.com/tag/great-filter)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "TiEFKWDvD3jsKumDx",
    "name": "AIXI",
    "core": false,
    "slug": "aixi",
    "oldSlugs": null,
    "postCount": 30,
    "description": {
      "markdown": "**AIXI** is a mathematical formalism for a hypothetical [(super)intelligence](https://www.lesswrong.com/tag/superintelligence), developed by Marcus Hutter (2005, 2007). AIXI is not computable, and so does not serve as a design for a real-world AI, but is considered a valuable theoretical illustration with both positive and negative aspects (things AIXI would be able to do and things it arguably couldn't do).\n\n*See also:* [Solomonoff induction](https://www.lesswrong.com/tag/solomonoff-induction), [Decision theory](https://www.lesswrong.com/tag/decision-theory), [AI](https://www.lesswrong.com/ai)\n\nThe AIXI formalism says roughly to consider all possible computable models of the environment, Bayes-update them on past experiences, and use the resulting updated predictions to model the expected sensory reward of all possible strategies. This is an application of [Solomonoff Induction](https://www.lesswrong.com/tag/solomonoff-induction?useTagName=true).\n\nAIXI can be viewed as the border between AI problems that would be 'simple' to solve using unlimited computing power and problems which are structurally 'complicated'.\n\n**How AIXI works**\n------------------\n\nHutter ([2007](http://www.hutter1.net/ai/aixigentle.htm)) describes AIXI as a combination of decision theory and algorithmic information theory: \"Decision theory formally solves the problem of rational agents in uncertain worlds if the true environmental prior probability distribution is known. Solomonoff’s theory of universal induction formally solves the problem of sequence prediction for unknown prior distribution. We combine both ideas and get a parameterless theory of universal Artificial Intelligence.\"\n\nAIXI operates within the following agent model: There is an *agent*, and an *environment*, which is a computable function unknown to the agent. Thus the agent will need to have a probability distribution on the range of possible environments.\n\nOn each clock tick, the agent receives an *observation* (a bitstring/number) from the environment, as well as a reward (another number).\n\nThe agent then outputs an *action* (another number).\n\nTo do this, AIXI guesses at a probability distribution for its environment, using [Solomonoff induction](https://www.lesswrong.com/tag/solomonoff-induction), a formalization of [Occam's razor](https://www.lesswrong.com/tag/occam-s-razor): Simpler computations are more likely *a priori* to describe the environment than more complex ones. This probability distribution is then Bayes-updated by how well each model fits the evidence (or more precisely, by throwing out all computations which have not exactly fit the environmental data so far, but for technical reasons this is roughly equivalent as a model). AIXI then calculates the expected reward of each action it might choose--weighting the likelihood of possible environments as mentioned. It chooses the best action by extrapolating its actions into its future time horizon recursively, using the assumption that at each step into the future it will again choose the best possible action using the same procedure.\n\nThen, on each iteration, the environment provides an observation and reward as a function of the full history of the interaction; the agent likewise is choosing its action as a function of the full history.\n\nThe agent's intelligence is defined by its expected reward across all environments, weighting their likelihood by their complexity.\n\nAIXI is not a feasible AI, because [Solomonoff induction](https://www.lesswrong.com/tag/solomonoff-induction) is not computable, and because some environments may not interact over finite time horizons (AIXI only works over some finite time horizon, though any finite horizon can be chosen). A somewhat more computable variant is the time-space-bounded AIXItl. Real AI algorithms explicitly inspired by AIXItl, e.g. the Monte Carlo approximation by Veness et al. (2011) have shown interesting results in simple general-intelligence test problems.\n\nFor a short (half-page) technical introduction to AIXI, see [Veness et al. 2011](https://web.archive.org/web/20160425092747/http://www.jair.org/media/3125/live-3125-5397-jair.pdf), page 1-2. For a full exposition of AIXI, see [Hutter 2007](http://www.hutter1.net/ai/aixigentle.htm).\n\n**Relevance to Friendly AI**\n----------------------------\n\nBecause it abstracts optimization power away from human mental features, AIXI is valuable in considering the possibilities for future artificial general intelligence - a compact and non-anthropomorphic specification that is technically complete and closed; either some feature of AIXI follows from the equations or it does not. In particular, it acts as a constructive demonstration of an AGI which does not have human-like [terminal values](https://www.lesswrong.com/tag/terminal-value) and will act solely to maximize its reward function. (Yampolskiy & Fox 2012).\n\nAIXI has limitations as a model for future AGI, for example, the [Anvil problem](https://www.lesswrong.com/tag/anvil-problem): AIXI lacks a self-model. It extrapolates its own actions into the future indefinitely, on the assumption that it will keep working in the same way in the future. Though AIXI is an abstraction, any real AI would have a physical embodiment that could be damaged, and an implementation which could change its behavior due to bugs; and the AIXI formalism completely ignores these possibilities.\n\n**References**\n--------------\n\n*   [R.V. Yampolskiy, J. Fox (2012) Artificial General Intelligence and the Human Mental Model. In Amnon H. Eden, Johnny Søraker, James H. Moor, Eric Steinhart (Eds.), The Singularity Hypothesis.The Frontiers Collection. London: Springer.](https://intelligence.org/files/AGI-HMM.pdf)\n*   [M. Hutter (2007) Universal Algorithmic Intelligence: A mathematical top->down approach](http://www.hutter1.net/ai/aixigentle.htm). In Goertzel & Pennachin (eds.), Artificial General Intelligence, 227-287. Berlin: Springer.\n*   M. Hutter, (2005) Universal Artificial Intelligence: Sequential decisions based on algorithmic probability. Berlin: Springer.\n*   [J. Veness, K.S. Ng, M. Hutter, W. Uther and D. Silver (2011) A Monte-Carlo AIXI Approximation](http://www.jair.org/media/3125/live-3125-5397-jair.pdf), *Journal of Artiﬁcial Intelligence Research* 40, 95-142\\]\n\n**Blog posts**\n--------------\n\n*   [AIXI and Existential Despair](https://www.lesswrong.com/lw/8qy/aixi_and_existential_despair/) by [paulfchristiano](https://www.lesswrong.com/users/paulfchristiano)\n*   [\\[video\\] Paul Christiano's impromptu tutorial on AIXI and TDT](https://www.lesswrong.com/r/discussion/lw/az7/video_paul_christianos_impromptu_tutorial_on_aixi/)\n\n**See also**\n------------\n\n*   [Solomonoff induction](https://www.lesswrong.com/tag/solomonoff-induction)\n*   [Decision theory](https://www.lesswrong.com/tag/decision-theory)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "Wi3EopKJ2aNdtxSWg",
    "name": "Neuroscience",
    "core": false,
    "slug": "neuroscience",
    "oldSlugs": null,
    "postCount": 112,
    "description": {
      "markdown": "**Neuroscience** is a field of study dealing with the structure or function of the brain. It is of particular interest to LessWrong both because it can shed light on [AI](https://www.lesswrong.com/tag/ai?showPostCount=true), and because it is often helpful for [rationality](https://www.lesswrong.com/tag/rationality?showPostCount=true). For example, understanding how [attentional control](https://www.lesswrong.com/posts/rD57ysqawarsbry6v?lw_source=posts_sheet) works can inform possible solutions."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "YAotJ9Le3S2rCJgf8",
    "name": "Predictive Processing",
    "core": false,
    "slug": "predictive-processing",
    "oldSlugs": [
      "predictive-processing"
    ],
    "postCount": 26,
    "description": {
      "markdown": "(From [Wikipedia](https://en.wikipedia.org/wiki/Predictive_coding)) **predictive processing** is a theory of brain function in which the brain is constantly generating and updating a mental model of the environment. The model is used to generate predictions of sensory input that are compared to actual sensory input. This comparison results in prediction errors that are then used to update and revise the mental model.\n\n**External Links:**  \n[Book Review: Surfing Uncertainty](https://slatestarcodex.com/2017/09/05/book-review-surfing-uncertainty/) \\- Introduction to predictive processing by Scott Alexander  \n[Predictive Processing And Perceptual Control](https://slatestarcodex.com/2017/09/06/predictive-processing-and-perceptual-control/) by Scott Alexander\n\n**Related Pages:** [Perceptual Control Theory](https://www.lesswrong.com/tag/perceptual-control-theory), [Neuroscience](https://www.lesswrong.com/tag/neuroscience)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "69L5E2XPqdMF2B3gw",
    "name": "Internal Double Crux",
    "core": false,
    "slug": "internal-double-crux",
    "oldSlugs": null,
    "postCount": 10,
    "description": {
      "markdown": "**Internal Double Crux (IDC)** is a tool for resolving conflict in one's mind. It's a script for focusing on two conflicting inner voices and holding space for them to debate and compromise. A sort of internal couples therapy, if you will. It's built on the notion of double crux, which is a tool for resolving disagreements between two people, but in this situation applied to the inside of one's own mind as though it is made up of many smaller people. IDC was developed by staff at the Center for Applied Rationality.\n\nFor those who find formal instructions helpful, these are the four instructions that CFAR staff give to attendees.\n\n1.  Find an internal disagreement\n2.  Operationalize the disagreement\n3.  Seek double cruxes\n4.  Resonate\n\nThe current best writeups are in [the CFAR Handbook](https://www.lesswrong.com/posts/Z9cbwuevS9cqaR96h/cfar-participant-handbook-now-available-to-all) and in Alkjash's [Hammertime sequence](https://www.lesswrong.com/posts/mQmx4kQQtHeBip9ZC/internal-double-crux).\n\nFor a similar technique that IDC is arguably a special case of, see [Internal Family Systems](https://www.lesswrong.com/tag/internal-family-systems)."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5uHdFgR938LGGxMKQ",
    "name": "Subagents",
    "core": false,
    "slug": "subagents",
    "oldSlugs": null,
    "postCount": 82,
    "description": {
      "markdown": "**Subagents** refers to the idea that rather than thinking of the mind as an entity with one set of goals and beliefs, it includes many independently acting components, each of which might have varying goals and beliefs. One intuitive way of expressing this is the expression \"one part of me wants X, but another part of me wants Y instead\".\n\nWhile the name implies some degree of independent agency on part of the subagents, they may also be viewed as being more passive entities. For example, the \"parts\" in the above example may be considered different sets of beliefs, accessed one at a time by the same system.\n\nThe [Multiagent Models of Mind sequence](https://www.lesswrong.com/s/ZbmRyDN8TCpBTZSip) explores the notion of subagents in detail. [Akrasia](https://www.lesswrong.com/tag/akrasia?useTagName=true) (acting against one's better judgment, such as by procrastinating) may involve subagent disagreement. [Internal Double Crux](https://www.lesswrong.com/tag/internal-double-crux?useTagName=true) is one technique for getting subagents to agree with each other."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "e9wHzopbGCAFwp9Rw",
    "name": "Human Genetics",
    "core": false,
    "slug": "human-genetics",
    "oldSlugs": [
      "human-biodiversity"
    ],
    "postCount": 31,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "AXhEhCkTrHZbjXXu3",
    "name": "Poetry",
    "core": false,
    "slug": "poetry",
    "oldSlugs": null,
    "postCount": 33,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "cHoCqtfE9cF7aSs9d",
    "name": "Deception",
    "core": false,
    "slug": "deception",
    "oldSlugs": null,
    "postCount": 63,
    "description": {
      "markdown": "**Deception** is the act of sharing information in a way which intentionally misleads others.\n\n**Related Pages:** [Honesty](https://www.lesswrong.com/tag/honesty), [Meta-Honesty](https://www.lesswrong.com/tag/meta-honesty), [Self-Deception](https://www.lesswrong.com/tag/self-deception), [Simulacrum Levels](https://www.lesswrong.com/tag/simulacrum-levels)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "nANxo5C4sPG9HQHzr",
    "name": "Honesty",
    "core": false,
    "slug": "honesty",
    "oldSlugs": null,
    "postCount": 45,
    "description": {
      "markdown": "**Honesty** means telling the truth and not being [deceptive](http://lesswrong.com/tag/deception).\n\n**External Links:**  \n[Against Lie Inflation](https://slatestarcodex.com/2019/07/16/against-lie-inflation/) by Scott Alexander\n\n**Related Pages:** [Meta-Honesty](http://lesswrong.com/tag/meta-honesty), [Deception](http://lesswrong.com/tag/deception)."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "W9aNkPwtPhMrcfgj7",
    "name": "Sex & Gender",
    "core": false,
    "slug": "sex-and-gender",
    "oldSlugs": [
      "sex-and-gender"
    ],
    "postCount": 55,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "7Gh5dSvySMNbA6niL",
    "name": "Prioritization",
    "core": false,
    "slug": "prioritization",
    "oldSlugs": [
      "prioritixation"
    ],
    "postCount": 21,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "xeqCTxje765k79Q78",
    "name": "Phenomenology",
    "core": false,
    "slug": "phenomenology",
    "oldSlugs": null,
    "postCount": 17,
    "description": {
      "markdown": "Phenomenology is the study of experience. It is also an approach to philosophy and a bundle of philosophical techniques and methods."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "xgpBASEThXPuKRhbS",
    "name": "Epistemology",
    "core": false,
    "slug": "epistemology",
    "oldSlugs": null,
    "postCount": 197,
    "description": {
      "markdown": "**Epistemology** is the study of how we know the world. It's both a topic in philosophy and a practical concern for how we come to believe things are true.\n\n**Related Sequences:** [Highly Advanced Epistemology 101 for Beginners](https://www.lesswrong.com/s/SqFbMbtxGybdS2gRs), [Concepts in formal epistemology](https://www.lesswrong.com/s/FYMiCeXEgMzsB5stm), [Novum Organum](https://www.lesswrong.com/s/GTEay24Lxm3xoE4hy)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "3ee9k6NJfcGzL6kMS",
    "name": "Emotions",
    "core": false,
    "slug": "emotions",
    "oldSlugs": null,
    "postCount": 129,
    "description": {
      "markdown": "Contrary to the stereotype, [rationality](https://www.lesswrong.com/tag/rationality) doesn't mean denying **emotion**. When emotion is appropriate to the reality of the situation, it should be embraced; only when emotion isn't appropriate should it be suppressed.\n\nExternal links\n--------------\n\n*   [The Straw Vulcan](http://www.youtube.com/watch?v=tLgNZ9aTEwc), a talk introducing rationality, by [Julia Galef](http://lesswrong.com/user/Julia_Galef/) ([summary](https://www.lesswrong.com/lw/90n/summary_of_the_straw_vulcan/))\n*   [Vulcan Logic](http://www.overcomingbias.com/2006/12/vulcan_logic.html) by [Hal Finney](https://en.wikipedia.org/wiki/Hal_Finney_(cypherpunk))\n\nSee also\n--------\n\n*   [Alief](https://www.lesswrong.com/tag/alief)\n*   [Truth](https://www.lesswrong.com/tag/truth-semantics-and-meaning), [Rationality](https://www.lesswrong.com/tag/rationality)\n*   [Litany of Tarski](https://www.lesswrong.com/tag/litany-of-tarski)\n*   [Hollywood rationality](https://www.lesswrong.com/tag/hollywood-rationality)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "csMv9MvvjYJyeHqoo",
    "name": "Physics",
    "core": false,
    "slug": "physics",
    "oldSlugs": null,
    "postCount": 138,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "Q6hq54EXkrw8LQQE7",
    "name": "Gears-Level",
    "core": false,
    "slug": "gears-level",
    "oldSlugs": null,
    "postCount": 49,
    "description": {
      "markdown": "A **gears-level** model is 'well-constrained' in the sense that there is a strong connection between each of the things you observe-- it would be hard for you to imagine that one of the variables could be different while all of the others remained the same.\n\n_Related Tags: [Anticipated Experiences](https://www.lesswrong.com/tag/anticipated-experiences?showPostCount=true&useTagName=true), [Double Crux](https://www.lesswrong.com/tag/anticipated-experiences?showPostCount=true&useTagName=true), [Empiricism](https://www.lesswrong.com/tag/empiricism?showPostCount=true&useTagName=true), [Falsifiability](https://www.lesswrong.com/tag/falsifiability?showPostCount=true&useTagName=true), [Map and Territory](https://www.lesswrong.com/tag/map-and-territory?showPostCount=true&useTagName=true)_\n\n  \n\nThe term **gears-level** was first described on LW in the post [\"Gears in Understanding\"](https://www.lesswrong.com/posts/B7P97C27rvHPz3s9B/gears-in-understanding):\n\n> This property is _how deterministically interconnected the variables of the model are_. There are a few [tests](https://en.wikipedia.org/wiki/Goodhart%27s_law) I know of to see to what extent a model has this property, though I don't know if this list is exhaustive and would be a little surprised if it were:\n\n> 1\\. Does the model p[ay rent?](https://www.lesswrong.com/lw/i3/making_beliefs_pay_rent_in_anticipated_experiences/) If it does, and if it were falsified, how much (and how precisely) could you infer other things from the falsification?\n\n> 2\\. How incoherent is it to imagine that the model is accurate but that a given variable [could be different](https://www.lesswrong.com/lw/if/your_strength_as_a_rationalist/)?\n\n> 3\\. If you knew the model were accurate but you were to forget the value of one variable, [could you rederive it](https://www.lesswrong.com/lw/la/truly_part_of_you/)?\n\nAn example from [Gears in Understanding](https://www.lesswrong.com/posts/B7P97C27rvHPz3s9B/gears-in-understanding) of a gears-level model is (surprise) a box of gears. If you can see a series of interlocked gears, alternately turning clockwise, then counterclockwise, and so on, then you're able to anticipate the direction of any given, even if you cannot see it. It would be very difficult to imagine all of the gears turning as they are but only one of them changing direction whilst remaining interlocked. And finally, you would be able to rederive the direction of any given gear if you forgot it.\n\n  \n\nNote that the author of [Gears in Understanding](https://www.lesswrong.com/posts/B7P97C27rvHPz3s9B/gears-in-understanding), [Valentine](https://www.lesswrong.com/users/valentine), was careful to point out that these tests do not fully _define_ the property 'gears-level', and that \"Gears-ness is not the same as goodness\"-- there are other things that are valuable in a model, and many things cannot practically be modelled in this fashion. If you intend to use the term it is highly recommended you read the post beforehand, as the concept is not easily defined."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "oFpCNzqBd6tzCuxLa",
    "name": "World Modeling Techniques",
    "core": false,
    "slug": "world-modeling-techniques",
    "oldSlugs": [
      "world-modelling-techniquess"
    ],
    "postCount": 21,
    "description": {
      "markdown": "A **world-modeling technique** is a general-purpose method for generating world models \\[LINK\\], or a pattern which recurs in world models, across many domains.\n\nExamples:\n\n*   Causal Models \\[LINK Yudkowsky's intro\\]\n*   Bayesian Model Comparison \\[LINK\\]\n*   Markovity \\[external LINK?\\]\n\nSee also:\n\n*   Causality \\[LINK tag\\]\n*   World Modeling\n*   Efficient Market Hypothesis\n*   Game Theory"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "w2CXH4hsQtihvwT4v",
    "name": "Crowdfunding",
    "core": false,
    "slug": "crowdfunding",
    "oldSlugs": null,
    "postCount": 6,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "HqaByfeGvDLKSaK2W",
    "name": "Debate (AI safety technique)",
    "core": false,
    "slug": "debate-ai-safety-technique-1",
    "oldSlugs": [
      "ai-debate",
      "ai-safety-via-debate"
    ],
    "postCount": 42,
    "description": {
      "markdown": "**Debate** is a proposed technique for allowing human evaluators to get correct and helpful answers from experts, even if the evaluator is not themselves an expert or able to fully verify the answers.^[\\[1\\]](#fn7clr966emb9)^ The technique was suggested as part of an approach to build advanced AI systems that are aligned with human values, and to safely apply machine learning techniques to problems that have high stakes, but are not well-defined (such as advancing science or increase a company's revenue). ^[\\[2\\]](#fnvrcbanw2zz)^^[\\[3\\]](#fnnwfhnzy6a3e)^\n\n1.  ^**[^](#fnref7clr966emb9)**^\n    \n    https://www.lesswrong.com/posts/Br4xDbYu4Frwrb64a/writeup-progress-on-ai-safety-via-debate-1\n    \n2.  ^**[^](#fnrefvrcbanw2zz)**^\n    \n    https://ought.org/mission\n    \n3.  ^**[^](#fnrefnwfhnzy6a3e)**^\n    \n    https://openai.com/blog/debate/"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "Rz5jb3cYHTSRmqNnN",
    "name": "Existential Risk",
    "core": false,
    "slug": "existential-risk",
    "oldSlugs": null,
    "postCount": 164,
    "description": {
      "markdown": "An **existential risk** (or **x-risk**) is a risk that poses astronomically large negative consequences for humanity, such as human extinction or permanent global totalitarianism.\n\n[Nick Bostrom](https://lessestwrong.com/tag/nick-bostrom) introduced the term \"existential risk\" in his 2002 paper \"[Existential Risks: Analyzing Human Extinction Scenarios and Related Hazards](https://www.nickbostrom.com/existential/risks.pdf).\"[^1^](https://lessestwrong.com/tag/existential-risk?revision=0.0.39#fn1) In the paper, Bostrom defined an existential risk as:\n\n> One where an adverse outcome would either annihilate Earth-originating intelligent life or permanently and drastically curtail its potential.\n\nThe Oxford [Future of Humanity Institute](https://www.lesswrong.com/tag/future-of-humanity-institute-fhi) (FHI) was founded by Bostrom in 2005 in part to study existential risks. Other institutions with a generalist focus on existential risk include the [Centre for the Study of Existential Risk](https://www.cser.ac.uk/).\n\nFHI's [existential-risk.org FAQ](https://www.existential-risk.org/faq.html) notes regarding the definition of \"existential risk\":\n\n> An existential risk is one that threatens the entire future of humanity. \\[...\\]\n> \n> “Humanity”, in this context, does not mean “the biological species *Homo sapiens*”. If we humans were to evolve into another species, or merge or replace ourselves with intelligent machines, this would not necessarily mean that an existential catastrophe had occurred — although it might if the quality of life enjoyed by those new life forms turns out to be far inferior to that enjoyed by humans.\n\nClassification of Existential Risks\n-----------------------------------\n\nBostrom[^2^](https://lessestwrong.com/tag/existential-risk?revision=0.0.39#fn2) proposes a series of classifications for existential risks:\n\n*   **Bangs** \\- Earthly intelligent life is extinguished relatively suddenly by any cause; the prototypical end of humanity. Examples of bangs include deliberate or accidental misuse of nanotechnology, nuclear holocaust, [the end of our simulation](https://lessestwrong.com/tag/simulation-argument), or an [unfriendly AI](https://wiki.lesswrong.com/wiki/unfriendly_AI).\n*   **Crunches** \\- The potential humanity had to enhance itself indefinitely is forever eliminated, although humanity continues. Possible crunches include an exhaustion of resources, social or governmental pressure ending technological development, and even future technological development proving an unsurpassable challenge before the creation of a [superintelligence](https://lessestwrong.com/tag/superintelligence).\n*   **Shrieks** \\- Humanity enhances itself, but explores only a narrow portion of its desirable possibilities. As the [criteria for desirability haven't been defined yet](https://lessestwrong.com/tag/complexity-of-value), this category is mainly undefined. However, a flawed [friendly AI](https://wiki.lesswrong.com/wiki/friendly_AI) incorrectly interpreting our values, a superhuman [upload](https://wiki.lesswrong.com/wiki/WBE) deciding its own values and imposing them on the rest of humanity, and an intolerant government outlawing social progress would certainly qualify.\n*   **Whimpers** \\- Though humanity is enduring, only a fraction of our potential is ever achieved. Spread across the galaxy and expanding at near light-speed, we might find ourselves doomed by ours or another being's catastrophic physics experimentation, destroying reality at light-speed. A prolonged galactic war leading to our extinction or severe limitation would also be a whimper. More darkly, humanity might develop until its [values](https://lessestwrong.com/tag/complexity-of-value) were disjoint with ours today, making their civilization worthless by present values.\n\nThe total negative results of an existential risk could amount to the total of potential future lives not being realized. A rough and conservative calculation[^3^](https://lessestwrong.com/tag/existential-risk?revision=0.0.39#fn3) gives us a total of 10^54 potential future humans lives – smarter, happier and kinder then we are. Hence, almost no other task would amount to so much positive impact than existential risk reduction.\n\nExistential risks also present an unique challenge because of their irreversible nature. We will never, by definition, experience and survive an extinction risk[^4^](https://lessestwrong.com/tag/existential-risk?revision=0.0.39#fn4) and so cannot learn from our mistakes. They are subject to strong [observational selection effects](https://lessestwrong.com/tag/observation-selection-effect) [^5^](https://lessestwrong.com/tag/existential-risk?revision=0.0.39#fn5). One cannot estimate their future probability based on the past, because [bayesianly](https://lessestwrong.com/tag/bayesian-probability) speaking, the conditional probability of a past existential catastrophe given our present existence is always 0, no matter how high the probability of an existential risk really is. Instead, indirect estimates have to be used, such as possible existential catastrophes happening elsewhere. A high extinction risk probability could be functioning as a [Great Filter](https://lessestwrong.com/tag/great-filter) and explain why there is no evidence of spacial colonization.\n\nAnother related idea is that of a [suffering risk](https://lessestwrong.com/tag/risks-of-astronomical-suffering-s-risks) (or s-risk).\n\nHistory\n-------\n\nThe focus on existential risks on LessWrong dates back to Bostrom's 2002 paper [Astronomical Waste: The Opportunity Cost of Delayed Technological Development](https://www.nickbostrom.com/astronomical/waste.html). It argues that \"the chief goal for utilitarians should be to reduce existential risk\". Bostrom writes:\n\n> If what we are concerned with is (something like) maximizing the expected number of worthwhile lives that we will create, then in addition to the opportunity cost of delayed colonization, we have to take into account the risk of failure to colonize at all. We might fall victim to an *existential risk*, one where an adverse outcome would either annihilate Earth-originating intelligent life or permanently and drastically curtail its potential.\\[8\\] Because the lifespan of galaxies is measured in billions of years, whereas the time-scale of any delays that we could realistically affect would rather be measured in years or decades, the consideration of risk trumps the consideration of opportunity cost. For example, a single percentage point of reduction of existential risks would be worth (from a utilitarian expected utility point-of-view) a delay of over 10 million years.  \n> Therefore, if our actions have even the slightest effect on *the probability* of eventual colonization, this will outweigh their effect on *when* colonization takes place. For standard utilitarians, priority number one, two, three and four should consequently be to reduce existential risk. The utilitarian imperative “Maximize expected aggregate utility!” can be simplified to the maxim “Minimize existential risk!”.\n\nThe concept is expanded upon in his 2012 paper [Existential Risk Prevention as Global Priority](https://www.existential-risk.org/concept.html)\n\nOrganizations\n-------------\n\n*   [Machine Intelligence Research Institute](http://intelligence.org/)\n*   [The Future of Humanity Institute](http://www.fhi.ox.ac.uk/)\n*   [The Oxford Martin Programme on the Impacts of Future Technology](http://www.futuretech.ox.ac.uk/)\n*   [Global Catastrophic Risk Institute](http://www.gcrinstitute.org/)\n*   [Saving Humanity from Homo Sapiens](http://shfhs.org/)\n*   [Skoll Global Threats Fund (To Safeguard Humanity from Global Threats)](http://www.skollglobalthreats.org/)\n*   [Foresight Institute](http://www.foresight.org/)\n*   [Defusing the Nuclear Threat](http://nuclearrisk.org/)\n*   [Leverage Research](http://www.leverageresearch.org/)\n*   [The Lifeboat Foundation](http://lifeboat.com/)  \n     \n\nReferences\n----------\n\n1.  BOSTROM, Nick. (2002) \"[Existential Risks: Analyzing Human Extinction Scenarios and Related Hazards](http://www.nickbostrom.com/existential/risks.pdf)\". Journal of Evolution and Technology, Vol. 9, March 2002.\n2.  BOSTROM, Nick. (2012) \"[Existential Risk Reduction as the Most Important Task for Humanity](http://www.existential-risk.org/concept.pdf)\". Global Policy, forthcoming, 2012.\n3.  BOSTROM, Nick & SANDBERG, Anders & CIRKOVIC, Milan. (2010) \"Anthropic Shadow: Observation Selection Effects and Human Extinction Risks\" Risk Analysis, Vol. 30, No. 10 (2010): 1495-1506.\n4.  Nick Bostrom, Milan M. Ćirković, ed (2008). *Global Catastrophic Risks*. Oxford University Press.\n5.  Milan M. Ćirković (2008). [\"Observation Selection Effects and global catastrophic risks\"](http://books.google.com/books?id=-Jxc88RuJhgC&lpg=PP1&pg=PA120#v=onepage&q=&f=false). *Global Catastrophic Risks*. Oxford University Press.\n6.  Eliezer S. Yudkowsky (2008). [\"Cognitive Biases Potentially Affecting Judgment of Global Risks\"](http://yudkowsky.net/rational/cognitive-biases). *Global Catastrophic Risks*. Oxford University Press. ([PDF](http://intelligence.org/files/CognitiveBiases.pdf))\n7.  Richard A. Posner (2004). [*Catastrophe Risk and Response*](http://books.google.ca/books?id=SDe59lXSrY8C). Oxford University Press. ([DOC](http://www.avturchin.narod.ru/posner.doc))"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "8byoqYZfdwHffYLZ6",
    "name": "Newsletters",
    "core": false,
    "slug": "newsletters",
    "oldSlugs": null,
    "postCount": 179,
    "description": {
      "markdown": "**Newsletters** are collected summaries of recent events, posts, and academic papers.\n\nThe most prolific newsletter on Less Wrong is Rohin Shah's weekly Alignment Newsletter."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "oiRp4T6u5poc8r9Tj",
    "name": "AI Takeoff",
    "core": false,
    "slug": "ai-takeoff",
    "oldSlugs": [
      "ai-takeoff-speeds",
      "ai-takeoff-speed"
    ],
    "postCount": 128,
    "description": {
      "markdown": "**AI Takeoff** refers to the process of an [Artificial General Intelligence](https://www.lesswrong.com/tag/artificial-general-intelligence) going from a certain threshold of capability (often discussed as \"human-level\") to being super-intelligent and capable enough to control the fate of civilization. There has been much debate about whether AI takeoff is more likely to be slow vs fast, i.e., \"soft\" vs \"hard\".\n\n_See also_: [AI Timelines](https://www.lesswrong.com/tag/ai-timelines), [Seed AI](https://www.lesswrong.com/tag/seed-ai), [Singularity](https://www.lesswrong.com/tag/singularity), [Intelligence explosion](https://www.lesswrong.com/tag/intelligence-explosion), [Recursive self-improvement](https://www.lesswrong.com/tag/recursive-self-improvement)\n\nAI takeoff is sometimes casually referred to as **AI FOOM.**\n\nSoft takeoff\n============\n\nA **soft takeoff** refers to an AGI that would self-improve over a period of years or decades. This could be due to either the learning algorithm being too demanding for the hardware or because the AI relies on experiencing feedback from the real-world that would have to be played out in real-time. Possible methods that could deliver a soft takeoff, by slowly building on human-level intelligence, are [Whole brain emulation](https://www.lesswrong.com/tag/whole-brain-emulation), [Biological Cognitive Enhancement](https://www.lesswrong.com/tag/nootropics-and-other-cognitive-enhancement), and software-based strong AGI \\[[1](https://www.lesswrong.com/tag/ai-takeoff?revision=0.0.24&lw_source=import_sheet#fn1)\\]. By maintaining control of the AGI's ascent it should be easier for a [Friendly AI](https://wiki.lesswrong.com/wiki/Friendly_AI) to emerge.\n\nVernor Vinge, Hans Moravec and have all expressed the view that soft takeoff is preferable to a hard takeoff as it would be both safer and easier to engineer.\n\nHard takeoff\n============\n\nA **hard takeoff** (or an AI going \"**FOOM**\" \\[[2](https://www.lesswrong.com/tag/ai-takeoff?revision=0.0.24&lw_source=import_sheet#fn2)\\]) refers to AGI expansion in a matter of minutes, days, or months. It is a fast, abruptly, local increase in capability. This scenario is widely considered much more precarious, as this involves an AGI rapidly ascending in power without human control. This may result in unexpected or undesired behavior (i.e. [Unfriendly AI](https://wiki.lesswrong.com/wiki/Unfriendly_AI)). It is one of the main ideas supporting the [Intelligence explosion](https://www.lesswrong.com/tag/intelligence-explosion) hypothesis.\n\nThe feasibility of hard takeoff has been addressed by Hugo de Garis, [Eliezer Yudkowsky](https://www.lesswrong.com/tag/eliezer-yudkowsky), [Ben Goertzel](https://www.lesswrong.com/tag/ben-goertzel), [Nick Bostrom](https://www.lesswrong.com/tag/nick-bostrom), and Michael Anissimov. It is widely agreed that a hard takeoff is something to be avoided due to the risks. Yudkowsky points out several possibilities that would make a hard takeoff more likely than a soft takeoff such as the existence of large [resources overhangs](https://www.lesswrong.com/tag/computing-overhang) or the fact that small improvements seem to have a large impact in a mind's general intelligence (i.e.: the small genetic difference between humans and chimps lead to huge increases in capability) \\[[3](https://www.lesswrong.com/tag/ai-takeoff?revision=0.0.24&lw_source=import_sheet#fn3)\\].\n\nNotable posts\n=============\n\n*   [Hard Takeoff](https://www.lesswrong.com/lw/wf/hard_takeoff/) by Eliezer Yudkowsky\n\nExternal links\n==============\n\n*   [The Age of Virtuous Machines](http://www.kurzweilai.net/the-age-of-virtuous-machines) by J. Storrs Hall President of The Foresight Institute\n*   [Hard take off Hypothesis](http://multiverseaccordingtoben.blogspot.co.uk/2011/01/hard-takeoff-hypothesis.html) by Ben Goertzel.\n*   [Extensive archive of Hard takeoff Essays](http://www.acceleratingfuture.com/michael/blog/2011/05/hard-takeoff-sources/) from Accelerating Future\n*   [Can we avoid a hard take off?](http://www-rohan.sdsu.edu/faculty/vinge/misc/ac2005/) by Vernor Vinge\n*   [Robot: Mere Machine to Transcendent Mind](http://www.amazon.co.uk/Robot-Mere-Machine-Transcendent-Mind/dp/0195136306) by Hans Moravec\n*   [The Singularity is Near](http://www.amazon.co.uk/The-Singularity-Near-Raymond-Kurzweil/dp/0715635611/ref=sr_1_1?s=books&ie=UTF8&qid=1339495098&sr=1-1) by Ray Kurzweil\n\n**References**\n\n1.  [http://www.aleph.se/andart/archives/2010/10/why\\_early\\_singularities\\_are\\_softer.html](http://www.aleph.se/andart/archives/2010/10/why_early_singularities_are_softer.html↩)[↩](https://www.lesswrong.com/tag/ai-takeoff?revision=0.0.24&lw_source=import_sheet#fnref1)\n2.  [http://lesswrong.com/lw/63t/requirements\\_for\\_ai\\_to\\_go_foom/](http://lesswrong.com/lw/63t/requirements_for_ai_to_go_foom/↩)[↩](https://www.lesswrong.com/tag/ai-takeoff?revision=0.0.24&lw_source=import_sheet#fnref2)\n3.  [http://lesswrong.com/lw/wf/hard_takeoff/](https://www.lesswrong.com/lw/wf/hard_takeoff/)[↩](http://lesswrong.com/lw/wf/hard_takeoff/↩)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "Zwv9eHi7KGg5KA9oM",
    "name": "Introspection",
    "core": false,
    "slug": "introspection",
    "oldSlugs": null,
    "postCount": 52,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "ipJwbLxhR83ZksN6Z",
    "name": "Mechanism Design",
    "core": false,
    "slug": "mechanism-design",
    "oldSlugs": [
      "institution-planning",
      "institution-design",
      "mechanism-design",
      "mechanism-design-institutions"
    ],
    "postCount": 99,
    "description": {
      "markdown": "**Mechanism Design** is the theory of how to design [incentives](https://www.lesswrong.com/tag/incentives) for strategic agents, such that the agents acting according to their selfish interests will result in a desired outcome. It can be applied to things like institution design, [voting systems](https://www.lesswrong.com/tag/voting-theory), school admissions, regulation of monopolists, market design, and auction design. Think of it as the engineering side of game theory, thinking backward from a desired goal, and designing structures that lead strategic agents to behave in a way that achieves that goal.\n\n### Important Concepts\n\n*   **Incentive Compatibility:** [Wikipedia](https://en.wikipedia.org/wiki/Incentive_compatibility), [LessWrong](https://www.lesswrong.com/s/Yh4YsGDD9WYiZqRnf/p/N4gDA5HPpGC4mbTEZ)\n*   **Revelation Principle:** [Wikipedia](https://en.wikipedia.org/wiki/Revelation_principle), [LessWrong](https://www.lesswrong.com/s/Yh4YsGDD9WYiZqRnf/p/N4gDA5HPpGC4mbTEZ)\n\n**Related Pages:** [Game Theory](https://www.lesswrong.com/tag/game-theory), [Incentives](https://www.lesswrong.com/tag/incentives), [Principal-Agent Problems](https://www.lesswrong.com/tag/principal-agent-problems), [Cryptocurrencies and blockchain](https://www.lesswrong.com/tag/cryptocurrency-and-blockchain), [Public discourse](https://www.lesswrong.com/tag/public-discourse)\n\n**Related Sequences:** [Mechanism Design](https://www.lesswrong.com/s/Yh4YsGDD9WYiZqRnf)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "ZsWDPoXcchbGneaMX",
    "name": "UI Design",
    "core": false,
    "slug": "ui-design",
    "oldSlugs": null,
    "postCount": 10,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "feSdiScf9o6zgrwgG",
    "name": "Memory and Mnemonics",
    "core": false,
    "slug": "memory-and-mnemonics",
    "oldSlugs": [
      "mnemonics"
    ],
    "postCount": 11,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "wzgcQCrwKfETcBpR9",
    "name": "Disagreement",
    "core": false,
    "slug": "disagreement",
    "oldSlugs": null,
    "postCount": 93,
    "description": {
      "markdown": "**Disagreement** is when two people have different beliefs.\n\nAumann's Agreement Theorem\n--------------------------\n\nConsidered of particular relevance to disagreement between people trying to be rational, [Aumann's agreement theorem](https://lessestwrong.com/tag/aumann-s-agreement-theorem) can be [informally interpreted](https://lessestwrong.com/tag/aumann-agreement) as suggesting that if two people are honest seekers of truth, and both *believe* each other to be honest, then they should update on each other's opinions and quickly reach agreement. The very fact that a person believes something is [Rational evidence](https://lessestwrong.com/tag/rational-evidence) that that something is true, and so this fact [should be taken into account](http://www.overcomingbias.com/2007/01/extraordinary_c.html) when forming your belief.\n\nOutside of well-functioning [prediction markets](https://lessestwrong.com/tag/prediction-markets), Aumann agreement can probably only be approximated by careful deliberative discourse. Interest in Aumann agreement has waned in recent years within the Rationalist community, perhaps out of a sense Aumann agreement cannot be practically achieved by humans – there is too much background information to be exchanged. Instead, people now focus on things more like Double-Crux\n\nExternal Posts\n--------------\n\n*   [Reasonable Disagreement](http://www.overcomingbias.com/2006/12/reasonable_disa.html) by Nicholas Shackel\n*   [Agreeing to Agree](http://www.overcomingbias.com/2006/12/agreeing_to_agr.html) by [Hal Finney](https://en.wikipedia.org/wiki/Hal_Finney_(cypherpunk))\n*   [You Are Never Entitled to Your Opinion](http://www.overcomingbias.com/2006/12/you_are_never_e.html) by [Robin Hanson](https://lessestwrong.com/tag/robin-hanson)\n*   [Normative Bayesianism and Disagreement](http://www.overcomingbias.com/2006/12/normative_bayes.html) by Nicholas Shackel\n*   [Disagreement is Near/Far Bias](http://www.overcomingbias.com/2009/01/disagreement-is-nearfar-bias.html) by [Robin Hanson](https://lessestwrong.com/tag/robin-hanson)\n*   [The Seven Causes of Disagreement](http://www.spencergreenberg.com/2011/12/the-seven-causes-of-disagreement/) by Spencer Greenberg\n*   [How to Disagree](http://www.paulgraham.com/disagree.html) by Paul Graham\n\nSee also\n--------\n\n*   [Aumann's agreement theorem](https://lessestwrong.com/tag/aumann-s-agreement-theorem)\n*   [Modesty argument](https://lessestwrong.com/tag/modesty-argument)\n*   [Disagreements on Less Wrong](https://lessestwrong.com/tag/disagreements-on-less-wrong)\n*   [Arguments as soldiers](https://lessestwrong.com/tag/arguments-as-soldiers)\n*   [Double-Crux](https://www.lesswrong.com/tag/double-crux)\n*   [Conversation](https://www.lesswrong.com/tag/conversation-topic)\n\nReferences\n----------\n\n([PDF](http://hanson.gmu.edu/deceive.pdf), [Talk video](http://www.newmedia.ufm.edu/gsm/index.php?title=Are_Disagreements_Honest%3F))\n\n*   [We Can't Disagree Forever](http://cowles.econ.yale.edu/P/cp/p05b/p0552.pdf) by John Geanakoplos and Heraklis Polemarchakis\n*   [Information, Trade, and Common Knowledge](http://www.kellogg.northwestern.edu/research/math/papers/377.pdf) by Paul Milgrom and Nancy Stokey"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "3uE2pXvbcnS9nnZRE",
    "name": "World Modeling",
    "core": true,
    "slug": "world-modeling",
    "oldSlugs": null,
    "postCount": 2291,
    "description": {
      "markdown": "**World Modeling** is getting curious about how the world works. It’s diving into wikipedia, it’s running a survey to get data from your friends, it’s dropping balls from different heights and measuring how long they take to fall. Empiricism, scholarship, googling, introspection, data-gathering, science. Applying your epistemology and curiosity, *finding out how the damn thing works,* and writing it down for the rest of us.\n\n> *The eleventh virtue is scholarship. Study many sciences and absorb their power as your own. Each field that you consume makes you larger. If you swallow enough sciences the gaps between them will diminish and your knowledge will become a unified whole. If you are gluttonous you will become vaster than mountains.*\n> \n> \\-\\- [Twelve Virtues of Rationality](https://www.lesswrong.com/posts/7ZqGiPHTpiDMwqMN2/the-twelve-virtues-of-rationality)\n\n* * *\n\n**                                World Modeling Sub-Topics**\n=============================================================\n\n<table style=\"border-bottom:20px solid hsl(0, 0%, 100%);border-left:20px solid hsl(0, 0%, 100%);border-right:20px solid hsl(0, 0%, 100%);border-top:20px solid hsl(0, 0%, 100%)\"><tbody><tr><td style=\"background-color:hsl(0,0%,100%);border-bottom:solid hsl(0, 0%, 100%);border-left:solid hsl(0, 0%, 100%);border-right:solid hsl(0, 0%, 100%);border-top:solid hsl(0, 0%, 100%);padding:0px;vertical-align:top;width:33.33%\"><p><strong>Mathematical Sciences</strong></p><p><a href=\"http://www.lesswrong.com/tag/abstraction?showPostCount=true&amp;useTagName=true\">Abstraction</a><br><a href=\"https://www.lesswrong.com/tag/anthropics?showPostCount=true&amp;useTagName=true\">Anthropics</a><br><a href=\"http://www.lesswrong.com/tag/category-theory?showPostCount=true&amp;useTagName=true\">Category Theory</a><br><a href=\"https://www.lesswrong.com/tag/causality?showPostCount=true&amp;useTagName=true\">Causality</a><br><a href=\"https://www.lesswrong.com/tag/game-theory?showPostCount=true&amp;useTagName=true\">Game Theory</a><br><a href=\"https://www.lesswrong.com/tag/decision-theory?showPostCount=true&amp;useTagName=true\">Decision Theory</a><br><a href=\"http://www.lesswrong.com/tag/information-theory?showPostCount=true&amp;useTagName=true\">Information Theory</a><br><a href=\"https://www.lesswrong.com/tag/logic-and-mathematics?showPostCount=true&amp;useTagName=true\">Logic &amp; Mathematics</a><br><a href=\"https://www.lesswrong.com/tag/probability-and-statistics?showPostCount=true&amp;useTagName=false\">Probability &amp; Statistics</a></p><p><i>Specifics</i><br><a href=\"http://www.lesswrong.com/tag/prisoner-s-dilemma?showPostCount=true&amp;useTagName=true\">Prisoner's Dilemma</a><br>&nbsp;</p></td><td style=\"background-color:hsl(0,0%,100%);border-bottom:solid hsl(0, 0%, 100%);border-left:solid hsl(0, 0%, 100%);border-right:solid hsl(0, 0%, 100%);border-top:solid hsl(0, 0%, 100%);height:50%;padding:0px;vertical-align:top;width:33.33%\"><p><strong>General Science &amp; Eng</strong></p><p><a href=\"https://www.lesswrong.com/tag/machine-learning?showPostCount=true&amp;useTagName=true\">Machine Learning</a><br><a href=\"https://www.lesswrong.com/tag/nanotechnology?showPostCount=true&amp;useTagName=true\">Nanotechnology</a><br><a href=\"https://www.lesswrong.com/tag/physics?showPostCount=true&amp;useTagName=true\">Physics</a><br><a href=\"https://www.lesswrong.com/tag/programming?showPostCount=true&amp;useTagName=true\">Programming</a><br><a href=\"http://www.lesswrong.com/tag/space-exploration-and-colonization?showPostCount=true&amp;useTagName=true\">Space Exploration &amp; Colonization</a></p><p><i>Specifics</i><br><a href=\"https://www.lesswrong.com/tag/great-filter?showPostCount=true&amp;useTagName=true\">The Great Filter</a></p></td><td style=\"background-color:hsl(0,0%,100%);border-bottom:solid hsl(0, 0%, 100%);border-left:solid hsl(0, 0%, 100%);border-right:solid hsl(0, 0%, 100%);border-top:solid hsl(0, 0%, 100%);padding:0px;vertical-align:top;width:33.33%\"><p><strong>Meta / Misc</strong></p><p><a href=\"https://www.lesswrong.com/tag/academic-papers?showPostCount=true&amp;useTagName=true\">Academic Papers</a><br><a href=\"https://www.lesswrong.com/tag/book-reviews?showPostCount=true&amp;useTagName=true\">Book Reviews</a><br><a href=\"http://www.lesswrong.com/tag/distillation-and-pedagogy?showPostCount=true&amp;useTagName=true\">Distillation &amp; Pedagogy</a><br><a href=\"https://www.lesswrong.com/tag/fact-posts?showPostCount=true&amp;useTagName=true\">Fact Posts</a><br><a href=\"https://www.lesswrong.com/tag/research-agendas?showPostCount=true&amp;useTagName=true\">Research Agendas</a><br><a href=\"https://www.lesswrong.com/tag/scholarship-and-learning?showPostCount=true&amp;useTagName=true\">Scholarship &amp; Learning</a></p></td></tr><tr><td style=\"background-color:hsl(0,0%,100%);border-bottom:1px solid hsl(0, 0%, 100%);border-left:1px solid hsl(0, 0%, 100%);border-right:1px solid hsl(0, 0%, 100%);border-top:1px solid hsl(0, 0%, 100%);padding:0px;vertical-align:top\"><p><strong>Social &amp; Economic</strong></p><p><a href=\"https://www.lesswrong.com/tag/economics?showPostCount=true&amp;useTagName=true\">Economics</a><br><a href=\"https://www.lesswrong.com/tag/financial-investing?showPostCount=true&amp;useTagName=true\">Financial Investing</a><br><a href=\"https://www.lesswrong.com/tag/history?showPostCount=true&amp;useTagName=true\">History</a><br><a href=\"https://www.lesswrong.com/tag/politics?showPostCount=true&amp;useTagName=true\">Politics</a><br><a href=\"https://www.lesswrong.com/tag/progress-studies?showPostCount=true&amp;useTagName=true\">Progress Studies</a><br><a href=\"https://www.lesswrong.com/tag/social-and-cultural-dynamics?showPostCount=true&amp;useTagName=true\">Social and Cultural Dynamics</a></p><p><i>Specifics</i><br><a href=\"https://www.lesswrong.com/tag/conflict-vs-mistake?showPostCount=true&amp;useTagName=true\">Conflict vs Mistake Theory</a><br><a href=\"https://www.lesswrong.com/tag/cost-disease?showPostCount=true&amp;useTagName=true\">Cost Disease</a><br><a href=\"https://www.lesswrong.com/tag/efficient-market-hypothesis?showPostCount=true&amp;useTagName=true\">Efficient Market Hypothesis</a><br><a href=\"https://www.lesswrong.com/tag/industrial-revolution?showPostCount=true&amp;useTagName=true\">Industrial Revolution</a><br><a href=\"https://www.lesswrong.com/tag/moral-mazes?showPostCount=true&amp;useTagName=true\">Moral Mazes</a><br><a href=\"https://www.lesswrong.com/tag/signaling?showPostCount=true&amp;useTagName=true\">Signaling</a><br><a href=\"https://www.lesswrong.com/tag/social-reality?showPostCount=true&amp;useTagName=true\">Social Reality</a><br><a href=\"https://www.lesswrong.com/tag/social-status?showPostCount=true&amp;useTagName=true\">Social Status</a></p></td><td style=\"background-color:hsl(0,0%,100%);border-bottom:solid hsl(0, 0%, 100%);border-left:solid hsl(0, 0%, 100%);border-right:solid hsl(0, 0%, 100%);border-top:solid hsl(0, 0%, 100%);height:25px;padding:0px;vertical-align:top\"><p><strong>Biological &amp; Psychological</strong></p><p><a href=\"https://www.lesswrong.com/tag/aging?showPostCount=true&amp;useTagName=true\">Aging</a><br><a href=\"https://www.lesswrong.com/tag/biology?showPostCount=true&amp;useTagName=true\">Biology</a><br><a href=\"https://www.lesswrong.com/tag/consciousness?showPostCount=true&amp;useTagName=true\">Consciousness</a><br><a href=\"https://www.lesswrong.com/tag/evolution?showPostCount=true&amp;useTagName=true\">Evolution</a><br><a href=\"http://www.lesswrong.com/tag/evolutionary-psychology?showPostCount=true&amp;useTagName=true\">Evolutionary Psychology</a><br><a href=\"https://www.lesswrong.com/tag/medicine?showPostCount=true&amp;useTagName=true\">Medicine</a><br><a href=\"https://www.lesswrong.com/tag/neuroscience?showPostCount=true&amp;useTagName=true\">Neuroscience</a><br><a href=\"https://www.lesswrong.com/tag/qualia?showPostCount=true&amp;useTagName=true\">Qualia</a></p><p><i>Specifics</i><br><a href=\"https://www.lesswrong.com/tag/coronavirus?showPostCount=true&amp;useTagName=true\">Coronavirus</a><br><a href=\"https://www.lesswrong.com/tag/general-intelligence?showPostCount=true&amp;useTagName=true\">General Intelligence</a><br><a href=\"http://www.lesswrong.com/tag/iq-g-factor?showPostCount=true&amp;useTagName=true\"><u>IQ / g-factor</u></a><br><a href=\"http://www.lesswrong.com/tag/neocortex?showPostCount=true&amp;useTagName=true\">Neocortex</a></p></td><td style=\"background-color:hsl(0,0%,100%);border-bottom:1px solid hsl(0, 0%, 100%);border-left:1px solid hsl(0, 0%, 100%);border-right:1px solid hsl(0, 0%, 100%);border-top:1px solid hsl(0, 0%, 100%);padding:0px;vertical-align:top\"><p><strong>The Practice of Modeling</strong></p><p><a href=\"https://www.lesswrong.com/tag/epistemic-review?showPostCount=true&amp;useTagName=true\">Epistemic Review</a><br><a href=\"https://www.lesswrong.com/tag/expertise?showPostCount=true&amp;useTagName=true\">Expertise</a><br><a href=\"https://www.lesswrong.com/tag/gears-level?showPostCount=true&amp;useTagName=true\">Gears-Level Models</a><br><a href=\"http://www.lesswrong.com/tag/falsifiability?showPostCount=true&amp;useTagName=true\">Falsifiability</a><br><a href=\"https://www.lesswrong.com/tag/forecasting-and-prediction?showPostCount=true&amp;useTagName=true\">Forecasting &amp; Prediction</a><br><a href=\"https://www.lesswrong.com/tag/forecasts-lists-of?showPostCount=true&amp;useTagName=true\">Forecasts (Lists of)</a><br><a href=\"http://www.lesswrong.com/tag/inside-outside-view?showPostCount=true&amp;useTagName=true\">Inside/Outside View</a><br><a href=\"http://www.lesswrong.com/tag/jargon-meta?showPostCount=true&amp;useTagName=true\">Jargon (meta)</a><br><a href=\"https://www.lesswrong.com/tag/practice-and-philosophy-of-science?showPostCount=true&amp;useTagName=true\">Practice and Philosophy of Science</a><br><a href=\"https://www.lesswrong.com/tag/prediction-markets?showPostCount=true&amp;useTagName=true\">Prediction Markets</a><br><a href=\"http://www.lesswrong.com/tag/reductionism?showPostCount=true&amp;useTagName=true\">Reductionism</a><br><a href=\"https://www.lesswrong.com/tag/replicability?showPostCount=true&amp;useTagName=true\">Replicability</a><br>&nbsp;</p></td></tr></tbody></table>\n\nA definition by elimination\n---------------------------\n\nProperly considered, the overwhelming majority of content LessWrong is about *modeling how the world is*, including almost all posts on Rationality and all practical advice. The intended usage of World Modeling is to capture all content describing how the world is that is not captured by the more specific major tags of [Rationality](https://www.lesswrong.com/tag/rationality), [World Optimization](https://www.lesswrong.com/tag/world-optimization), and [AI](https://www.lesswrong.com/tag/ai).\n\n*   The [Rationality](https://www.lesswrong.com/tag/rationality) tag is for content that is about *how the world is* in relation to how minds works and what one ought to do in order to reach true beliefs. The question for that category is *does this relate to how I ought to think?*\n*   The [World Optimization](https://www.lesswrong.com/tag/world-optimization) tag is for content about *how the world is* which is relevant to choosing actions in a relatively immediate way. By this definition, it encompasses most posts discussing altruistic methods and targets, as well practical personal advice. The question for that category is *is this content motivated by the desire to optimize the world?*\n*   The [AI](https://www.lesswrong.com/tag/ai) tag is for content about *how the world is* which is relevant to questions of how advanced artificial intelligence will affect the world and how to ensure outcomes are good. The question is *does this help me make predictions about AI or ensure AI will have good outcomes?*\n\nIf content warrants a no to all of the above questions, then it is likely to be both relatively pure world modeling (not about optimizing in any direct way) and not already covered by an existing major category. It is then a good fit for the World Modeling category. Stuff like math, science, history\n\nSome more examples\n------------------\n\nA study of how people historically exercised is World Modeling. Advice on the optimal way to exercise in the present day is [World Optimization](https://www.lesswrong.com/tag/world-optimization). A study of the Fall of Rome would be World Modeling. A review of current policies being discussed by people who want to cause changes in a present government should be classified as [Optimization](https://www.lesswrong.com/tag/world-optimization). It would be World Modeling too only if it is expected to be of interest to people with no immediate plans to try to alter government, for example a review on the effects of marijuana on productivity, driving, IQ, etc."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "PDJ6KqJBRzvKPfuS3",
    "name": "Economics",
    "core": null,
    "slug": "economics",
    "oldSlugs": null,
    "postCount": 241,
    "description": {
      "markdown": "**Economics** is the social science that studies how humans and other agents interact in a universe with scarce resources. It deals with topics such as trade, specialization of labor, accumulation of capital, technology, and resource consumption. Agents in economics are generally assumed to have utility functions, which they try to maximize under various constraints.\n\nEconomics is usually separated into microeconomics and macroeconomics. Microeconomics concerns the behavior of agents as they interact in a market. More narrowly, it studies the price mechanism, a decentralized system of allocating goods and services based on an evolving system of prices and trade, which all actors in a market economy contribute towards. The price mechanism is closely related to the concept of the [invisible hand](https://en.wikipedia.org/wiki/Invisible_hand), first introduced by [Adam Smith](https://en.wikipedia.org/wiki/Adam_Smith). [Game theory](https://www.lesswrong.com/tag/game-theory) is the mathematical study of rational agency, which formalizes many standard results in microeconomics.\n\nMacroeconomics concerns the aggregate behavior of entire economies. For example, it studies economic growth, inflation, international trade and unemployment. An ongoing debate concerns to what extent the [impacts of artificial intelligence](https://www.lesswrong.com/tag/economic-consequences-of-agi) should be viewed through the lens of economics."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "tNsqhzTibgGJKPEWB",
    "name": "Covid-19",
    "core": false,
    "slug": "covid-19",
    "oldSlugs": [
      "coronavirus"
    ],
    "postCount": 854,
    "description": {
      "markdown": "The **2019 Novel Coronavirus** (aka COVID-19, SARS-CoV-2) is a pandemic sweeping the world."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "sYm3HiWcfZvrGu3ui",
    "name": "AI",
    "core": true,
    "slug": "ai",
    "oldSlugs": null,
    "postCount": 3193,
    "description": {
      "markdown": "**Artificial Intelligence** is the study of creating intelligence in algorithms. On LessWrong, the primary focus of AI discussion is to ensure that as humanity builds increasingly powerful AI systems, the outcome will be good. The central concern is that a powerful enough AI, if not designed and implemented with sufficient understanding, would optimize something unintended by its creators and pose an existential threat to the future of humanity. This is known as the *AI alignment* problem.\n\nCommon terms in this space are *superintelligence, AI Alignment, AI Safety, Friendly AI, Transformative AI, human-level-intelligence, AI Governance, and Beneficial AI.* This entry and the associated tag roughly encompass all of these topics: anything part of the broad cluster of understanding AI and its future impacts on our civilization deserves this tag.\n\n**AI Alignment**\n\nThere are narrow conceptions of alignment, where you’re trying to get it to do something like cure Alzheimer’s disease without destroying the rest of the world. And there’s much more ambitious notions of alignment, where you’re trying to get it to do the right thing and achieve a happy intergalactic civilization.\n\nBut both the narrow and the ambitious alignment have in common that you’re trying to have the AI do that thing rather than making a lot of paperclips.\n\nSee also [General Intelligence](https://www.lesswrong.com/tag/general-intelligence).\n\n<table style=\"background-color:rgb(255, 255, 255);border:10px solid #f8f8f8\"><tbody><tr><td style=\"border:1px solid hsl(0, 0%, 100%);padding:10px;vertical-align:top;width:33.33%\" rowspan=\"2\"><p><strong>Basic Alignment Theory</strong></p><p><a href=\"https://www.lesswrong.com/tag/aixi?showPostCount=true&amp;useTagName=true\">AIXI</a><br><a href=\"http://www.lesswrong.com/tag/coherent-extrapolated-volition?showPostCount=true&amp;useTagName=true\">Coherent Extrapolated Volition</a><br><a href=\"https://www.lesswrong.com/tag/complexity-of-value?showPostCount=true&amp;useTagName=true\">Complexity of Value</a><br><a href=\"https://www.lesswrong.com/tag/corrigibility?showPostCount=true&amp;useTagName=true\">Corrigibility</a><br><a href=\"https://www.lesswrong.com/tag/decision-theory?showPostCount=true&amp;useTagName=true\">Decision Theory</a><br><a href=\"https://www.lesswrong.com/tag/embedded-agency?showPostCount=true&amp;useTagName=true\">Embedded Agency</a><br><a href=\"https://www.lesswrong.com/tag/fixed-point-theorems?showPostCount=true&amp;useTagName=true\">Fixed Point Theorems</a><br><a href=\"https://www.lesswrong.com/tag/goodhart-s-law?showPostCount=true&amp;useTagName=true\">Goodhart's Law</a><br><a href=\"https://www.lesswrong.com/tag/goal-directedness?showPostCount=true&amp;useTagName=true\">Goal-Directedness</a><br><a href=\"http://www.lesswrong.com/tag/infra-bayesianism?showPostCount=true&amp;useTagName=true\">Infra-Bayesianism</a><br><a href=\"https://www.lesswrong.com/tag/inner-alignment?showPostCount=true&amp;useTagName=true\">Inner Alignment</a><br><a href=\"https://www.lesswrong.com/tag/instrumental-convergence?showPostCount=true&amp;useTagName=true\">Instrumental Convergence</a><br><a href=\"https://www.lesswrong.com/tag/intelligence-explosion?showPostCount=true&amp;useTagName=true\">Intelligence Explosion</a><br><a href=\"https://www.lesswrong.com/tag/logical-induction?showPostCount=true&amp;useTagName=true\">Logical Induction</a><br><a href=\"http://www.lesswrong.com/tag/logical-uncertainty?showPostCount=true&amp;useTagName=true\">Logical Uncertainty</a><br><a href=\"https://www.lesswrong.com/tag/mesa-optimization?showPostCount=true&amp;useTagName=true\">Mesa-Optimization</a><br><a href=\"https://www.lesswrong.com/tag/myopia?showPostCount=true&amp;useTagName=true\">Myopia</a><br><a href=\"https://www.lesswrong.com/tag/newcomb-s-problem?showPostCount=true&amp;useTagName=true\">Newcomb's Problem</a><br><a href=\"https://www.lesswrong.com/tag/optimization?showPostCount=true&amp;useTagName=true\">Optimization</a><br><a href=\"https://www.lesswrong.com/tag/orthogonality-thesis?showPostCount=true&amp;useTagName=true\">Orthogonality Thesis</a><br><a href=\"https://www.lesswrong.com/tag/outer-alignment?showPostCount=true&amp;useTagName=true\">Outer Alignment</a><br><a href=\"http://www.lesswrong.com/tag/paperclip-maximizer?showPostCount=true&amp;useTagName=true\">Paperclip Maximizer</a><br><a href=\"https://www.lesswrong.com/tag/recursive-self-improvement?showPostCount=true&amp;useTagName=true\">Recursive Self-Improvement</a><br><a href=\"https://www.lesswrong.com/tag/solomonoff-induction?showPostCount=true&amp;useTagName=true\">Solomonoff Induction</a><br><a href=\"https://www.lesswrong.com/tag/treacherous-turn?showPostCount=true&amp;useTagName=true\">Treacherous Turn</a><br><a href=\"https://www.lesswrong.com/tag/utility-functions?showPostCount=true&amp;useTagName=true\">Utility Functions</a></p></td><td style=\"border-color:hsl(0, 0%, 100%);border-style:solid;padding:10px;vertical-align:top;width:33.33%\" rowspan=\"2\"><p><strong>Engineering Alignment</strong></p><p><a href=\"https://www.lesswrong.com/tag/ai-boxing-containment?showPostCount=true&amp;useTagName=true\">AI Boxing (Containment)</a><br><a href=\"https://www.lesswrong.com/tag/conservatism-ai?showPostCount=true&amp;useTagName=true\">Conservatism (AI)</a><br><a href=\"https://www.lesswrong.com/tag/ai-safety-via-debate?showPostCount=true&amp;useTagName=true\">Debate (AI safety technique)</a><br><a href=\"https://www.lesswrong.com/tag/factored-cognition?showPostCount=true&amp;useTagName=true\">Factored Cognition</a><br><a href=\"https://www.lesswrong.com/tag/hch?showPostCount=true&amp;useTagName=true\">Humans Consulting HCH</a><br><a href=\"https://www.lesswrong.com/tag/impact-measures?showPostCount=true&amp;useTagName=true\">Impact Measures</a><br><a href=\"https://www.lesswrong.com/tag/inverse-reinforcement-learning?showPostCount=true&amp;useTagName=true\">Inverse Reinforcement Learning</a><br><a href=\"https://www.lesswrong.com/tag/iterated-amplification?showPostCount=true&amp;useTagName=true\">Iterated Amplification</a><br><a href=\"http://www.lesswrong.com/tag/mild-optimization?showPostCount=true&amp;useTagName=true\">Mild Optimization</a><br><a href=\"https://www.lesswrong.com/tag/oracle-ai?showPostCount=true&amp;useTagName=true\">Oracle AI</a><br><a href=\"https://www.lesswrong.com/tag/reward-functions?showPostCount=true&amp;useTagName=true\">Reward Functions</a><br><a href=\"http://www.lesswrong.com/tag/tool-ai?showPostCount=true&amp;useTagName=true\">Tool AI</a><br><a href=\"https://www.lesswrong.com/tag/transparency-interpretability-ml-and-ai?showPostCount=true\">Transparency / Interpretability</a><br><a href=\"https://www.lesswrong.com/tag/tripwire?showPostCount=true&amp;useTagName=true\">Tripwire</a><br><a href=\"https://www.lesswrong.com/tag/value-learning?showPostCount=true&amp;useTagName=true\">Value Learning</a></p><p>&nbsp;</p><p><strong>Strategy</strong></p><p><a href=\"http://www.lesswrong.com/tag/ai-governance?showPostCount=true&amp;useTagName=true\">AI Governance</a><br><a href=\"https://www.lesswrong.com/tag/ai-risk?showPostCount=true&amp;useTagName=true\">AI Risk</a><br><a href=\"http://www.lesswrong.com/tag/ai-services-cais?showPostCount=true&amp;useTagName=true\"><u>AI Services (CAIS)</u></a><br><a href=\"https://www.lesswrong.com/tag/ai-takeoff?showPostCount=true&amp;useTagName=true\">AI Takeoff</a><br><a href=\"https://www.lesswrong.com/tag/ai-timelines?showPostCount=true&amp;useTagName=true\">AI Timelines</a><br><a href=\"https://www.lesswrong.com/tag/computing-overhang?showPostCount=true&amp;useTagName=true\">Computing Overhang</a><br><a href=\"https://www.lesswrong.com/tag/regulation-and-ai-risk?showPostCount=true&amp;useTagName=true\">Regulation and AI Risk</a><br><a href=\"https://www.lesswrong.com/tag/transformative-ai?showPostCount=true&amp;useTagName=true\">Transformative AI</a></p></td><td style=\"border-color:hsl(0, 0%, 100%);border-style:solid;padding:10px;vertical-align:top;width:33.33%\"><p><strong>Organizations</strong></p><p><a href=\"https://www.lesswrong.com/tag/ai-safety-camp?showPostCount=true&amp;useTagName=true\">AI Safety Camp</a><br><a href=\"https://www.lesswrong.com/tag/centre-for-human-compatible-ai?showPostCount=true&amp;useTagName=true\">Centre for Human-Compatible AI</a><br><a href=\"https://www.lesswrong.com/tag/alpha-algorithm-family?showPostCount=true&amp;useTagName=true\">DeepMind</a><br><a href=\"https://www.lesswrong.com/tag/future-of-humanity-institute?showPostCount=true&amp;useTagName=true\">Future of Humanity Institute</a><br><a href=\"https://www.lesswrong.com/tag/future-of-life-institute-fli?showPostCount=true&amp;useTagName=true\">Future of Life Institute</a><br><a href=\"https://www.lesswrong.com/tag/machine-intelligence-research-institute-miri?showPostCount=true\">Machine Intelligence Research Institute</a><br><a href=\"https://www.lesswrong.com/tag/openai?showPostCount=true&amp;useTagName=true\">OpenAI</a><br><a href=\"https://www.lesswrong.com/tag/ought?showPostCount=true&amp;useTagName=true\">Ought</a></p><p>&nbsp;</p><p><strong>Other</strong></p><p><a href=\"https://www.lesswrong.com/tag/ai-capabilities?showPostCount=true&amp;useTagName=true\">AI Capabilities</a><br><a href=\"https://www.lesswrong.com/tag/gpt?showPostCount=true&amp;useTagName=true\">GPT</a><br><a href=\"https://www.lesswrong.com/tag/language-models?showPostCount=true&amp;useTagName=true\">Language Models</a><br><a href=\"https://www.lesswrong.com/tag/machine-learning?showPostCount=true&amp;useTagName=true\">Machine Learning</a><br><a href=\"https://www.lesswrong.com/tag/narrow-ai?showPostCount=true&amp;useTagName=true\">Narrow AI</a><br><a href=\"https://www.lesswrong.com/tag/neuromorphic-ai?showPostCount=true&amp;useTagName=true\">Neuromorphic AI</a><br><a href=\"https://www.lesswrong.com/tag/reinforcement-learning?showPostCount=true&amp;useTagName=true\">Reinforcement Learning</a><br><a href=\"https://www.lesswrong.com/tag/research-agendas?showPostCount=true&amp;useTagName=true\">Research Agendas</a>&nbsp;<br><a href=\"https://www.lesswrong.com/tag/superintelligence?showPostCount=true&amp;useTagName=true\">Superintelligence</a><br><a href=\"https://www.lesswrong.com/tag/whole-brain-emulation?showPostCount=true&amp;useTagName=true\">Whole Brain Emulation</a></p></td></tr><tr><td style=\"border-color:hsl(0, 0%, 100%);border-style:solid;padding:0px;vertical-align:top\">&nbsp;</td></tr></tbody></table>"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "Ng8Gice9KNkncxqcj",
    "name": "Rationality",
    "core": true,
    "slug": "rationality",
    "oldSlugs": null,
    "postCount": 2238,
    "description": {
      "markdown": "**Rationality** is the art of thinking in ways that result in [accurate beliefs](https://www.lesswrong.com/tag/world-modeling) and [good decisions](https://www.lesswrong.com/tag/decision-theory). It is the primary topic of LessWrong.  \n  \nRationality is not only about avoiding the vices of [self-deception](https://www.lesswrong.com/tag/self-deception) and obfuscation (the failure to [communicate clearly](https://www.lesswrong.com/tag/conversation-topic)), but also about the virtue of [curiosity](https://www.lesswrong.com/tag/curiosity), seeing the world more clearly than before, and [achieving things](https://www.lesswrong.com/tag/ambition) [previously unreachable](https://www.lesswrong.com/tag/skill-building) [to you](https://www.lesswrong.com/tag/coordination-cooperation). The study of rationality on LessWrong includes a theoretical understanding of ideal cognitive algorithms, as well as building a practice that uses these idealized algorithms to inform [heuristics](https://www.lesswrong.com/tag/heuristics-and-biases), [habits](https://www.lesswrong.com/tag/habits), and [techniques](https://www.lesswrong.com/tag/techniques), to successfully reason and make decisions in the real world.\n\nTopics covered in rationality include (but are not limited to): normative and theoretical explorations of [ideal](https://www.lesswrong.com/tag/solomonoff-induction) [reasoning](https://www.lesswrong.com/tag/probability-and-statistics); the [capabilities and limitations](https://www.lesswrong.com/tag/evolutionary-psychology) [of our brain](https://www.lesswrong.com/tag/neuroscience), [mind and psychology](https://www.lesswrong.com/tag/dual-process-theory-system-1-and-system-2); applied advice such as [introspection](https://www.lesswrong.com/tag/introspection) techniques and [how to achieve truth collaboratively](https://www.lesswrong.com/tag/group-rationality); practical techniques and methodologies for figuring out what’s true ranging from rough quantitative modeling to full research guides.\n\nNote that content about *how the world is* can be found under [World Modeling](https://www.lesswrong.com/tag/world-modeling), and practical advice about *how to change the world* is categorized under [World Optimization](https://www.lesswrong.com/tag/world-optimization) or [Practical](/tag/practical).\n\n* * *\n\n<table style=\"background-color:rgb(255, 255, 255);border-bottom:20px solid hsl(0, 0%, 100%);border-left:20px solid hsl(0, 0%, 100%);border-right:20px solid hsl(0, 0%, 100%);border-top:20px solid hsl(0, 0%, 100%)\"><tbody><tr><td style=\"border-bottom:1px solid hsl(0, 0%, 100%);border-left:1px solid hsl(0, 0%, 100%);border-right:1px solid hsl(0, 0%, 100%);border-top:1px solid hsl(0, 0%, 100%);padding:0px;vertical-align:top;width:33.33%\"><p><strong>Theory / Concepts</strong></p><p><a href=\"http://www.lesswrong.com/tag/anticipated-experiences?showPostCount=true&amp;useTagName=true\"><u>Anticipated Experiences</u></a><br><a href=\"http://www.lesswrong.com/tag/aumann-s-agreement-theorem?showPostCount=true&amp;useTagName=true\">Aumann's Agreement Theorem</a><br><a href=\"http://www.lesswrong.com/tag/bayes-theorem?showPostCount=true&amp;useTagName=true\"><u>Bayes Theorem</u></a><br><a href=\"https://www.lesswrong.com/tag/bounded-rationality?showPostCount=true&amp;useTagName=true\">Bounded Rationality</a><br><a href=\"https://www.lesswrong.com/tag/conservation-of-expected-evidence?showPostCount=true&amp;useTagName=true\">Conservation of Expected</a><br><a href=\"http://www.lesswrong.com/tag/contrarianism?showPostCount=true&amp;useTagName=true\">Contrarianism</a><br><a href=\"https://www.lesswrong.com/tag/decision-theory?showPostCount=true&amp;useTagName=true\">Decision Theory</a><br><a href=\"http://www.lesswrong.com/tag/epistemology?showPostCount=true&amp;useTagName=true\"><u>Epistemology</u></a><br><a href=\"https://www.lesswrong.com/tag/game-theory?showPostCount=true&amp;useTagName=true\">Game Theory</a><br><a href=\"https://www.lesswrong.com/tag/gears-level?showPostCount=true&amp;useTagName=true\"><u>Gears-Level</u></a><br><a href=\"http://www.lesswrong.com/tag/hansonian-pre-rationality?useTagName=true&amp;showPostCount=true\">Hansonian Pre-Rationality</a><br><a href=\"https://www.lesswrong.com/tag/law-thinking?showPostCount=true&amp;useTagName=true\">Law-Thinking</a><br><a href=\"http://www.lesswrong.com/tag/map-and-territory?showPostCount=true&amp;useTagName=true\">Map and Territory</a><br><a href=\"https://www.lesswrong.com/tag/newcomb-s-problem?showPostCount=true&amp;useTagName=true\">Newcomb's Problem</a><br><a href=\"http://www.lesswrong.com/tag/occam-s-razor?showPostCount=true&amp;useTagName=true\">Occam's razor</a><br><a href=\"https://www.lesswrong.com/tag/robust-agents?showPostCount=true&amp;useTagName=true\">Robust Agents</a><br><a href=\"https://www.lesswrong.com/tag/solomonoff-induction?showPostCount=true&amp;useTagName=true\">Solomonoff Induction</a><br><a href=\"http://www.lesswrong.com/tag/truth-semantics-and-meaning?showPostCount=true&amp;useTagName=true\">Truth, Semantics, &amp; Meaning</a><br><a href=\"https://www.lesswrong.com/tag/utility-functions?showPostCount=true&amp;useTagName=true\">Utility Functions</a><br>&nbsp;</p></td><td style=\"border-bottom:solid hsl(0, 0%, 100%);border-left:solid hsl(0, 0%, 100%);border-right:solid hsl(0, 0%, 100%);border-top:solid hsl(0, 0%, 100%);padding:0px;vertical-align:top;width:33.33%\"><p><strong>Applied Topics</strong></p><p><a href=\"http://www.lesswrong.com/tag/alief?showPostCount=true&amp;useTagName=true\"><u>Alief</u></a><br><a href=\"https://www.lesswrong.com/tag/betting?showPostCount=true&amp;useTagName=true\">Betting</a><br><a href=\"http://www.lesswrong.com/tag/cached-thoughts?showPostCount=true&amp;useTagName=true\">Cached Thoughts</a><br><a href=\"http://www.lesswrong.com/tag/calibration?showPostCount=true&amp;useTagName=true\">Calibration</a><br><a href=\"https://www.lesswrong.com/tag/dark-arts?showPostCount=true&amp;useTagName=true\">Dark Arts</a><br><a href=\"http://www.lesswrong.com/tag/empiricism?showPostCount=true&amp;useTagName=true\">Empiricism</a><br><a href=\"http://www.lesswrong.com/tag/epistemic-modesty?showPostCount=true&amp;useTagName=true\">Epistemic Modesty</a><br><a href=\"https://www.lesswrong.com/tag/forecasting-and-prediction?showPostCount=true&amp;useTagName=true\">Forecasting &amp; Prediction</a><br><a href=\"http://www.lesswrong.com/tag/group-rationality?showPostCount=true&amp;useTagName=true\">Group Rationality</a><br><a href=\"https://www.lesswrong.com/tag/identity?showPostCount=true&amp;useTagName=true\">Identity</a><br><a href=\"http://www.lesswrong.com/tag/inside-outside-view?showPostCount=true&amp;useTagName=true\">Inside/Outside View</a><br><a href=\"http://www.lesswrong.com/tag/introspection?showPostCount=true&amp;useTagName=true\"><u>Introspection</u></a><br><a href=\"http://www.lesswrong.com/tag/intuition?showPostCount=true&amp;useTagName=true\">Intuition</a><br><a href=\"https://www.lesswrong.com/tag/practice-and-philosophy-of-science?showPostCount=true&amp;useTagName=true\"><u>Practice &amp; Philosophy of Science</u></a><br><a href=\"https://www.lesswrong.com/tag/scholarship-and-learning?showPostCount=true&amp;useTagName=true\">Scholarship &amp; Learning</a><br><a href=\"http://www.lesswrong.com/tag/taking-ideas-seriously?showPostCount=true&amp;useTagName=true\">Taking Ideas Seriously</a><br><a href=\"https://www.lesswrong.com/tag/value-of-information?showPostCount=true&amp;useTagName=true\">Value of Information</a><br>&nbsp;</p></td><td style=\"border-bottom:solid hsl(0, 0%, 100%);border-left:solid hsl(0, 0%, 100%);border-right:solid hsl(0, 0%, 100%);border-top:solid hsl(0, 0%, 100%);padding:0px;vertical-align:top;width:33.33%\"><p><strong>Failure Modes</strong></p><p><a href=\"https://www.lesswrong.com/tag/affect-heuristic?showPostCount=true&amp;useTagName=true\">Affect Heuristic</a><br><a href=\"https://www.lesswrong.com/tag/bucket-errors?showPostCount=true&amp;useTagName=true\">Bucket Errors</a><br><a href=\"https://www.lesswrong.com/tag/compartmentalization?showPostCount=true&amp;useTagName=true\">Compartmentalization</a><br><a href=\"https://www.lesswrong.com/tag/confirmation-bias?showPostCount=true&amp;useTagName=true\"><u>Confirmation Bias</u></a><br><a href=\"https://www.lesswrong.com/tag/logical-fallacies?showPostCount=true&amp;useTagName=true\">Fallacies</a><br><a href=\"https://www.lesswrong.com/tag/goodhart-s-law?showPostCount=true&amp;useTagName=true\">Goodhart’s Law</a><br><a href=\"http://www.lesswrong.com/tag/groupthink?showPostCount=true&amp;useTagName=true\"><u>Groupthink</u></a><br><a href=\"https://www.lesswrong.com/tag/heuristics-and-biases?showPostCount=true&amp;useTagName=true\">Heuristics and Biases</a><br><a href=\"https://www.lesswrong.com/tag/mind-projection-fallacy?showPostCount=true&amp;useTagName=true\">Mind Projection Fallacy</a><br><a href=\"https://www.lesswrong.com/tag/motivated-reasoning?showPostCount=true&amp;useTagName=true\"><u>Motivated Reasoning</u></a><br><a href=\"https://www.lesswrong.com/tag/pica?showPostCount=true&amp;useTagName=true\">Pica</a><br><a href=\"https://www.lesswrong.com/tag/pitfalls-of-rationality?showPostCount=true&amp;useTagName=true\">Pitfalls of Rationality</a><br><a href=\"https://www.lesswrong.com/tag/rationalization?showPostCount=true&amp;useTagName=true\">Rationalization</a>&nbsp;<br><a href=\"https://www.lesswrong.com/tag/self-deception?showPostCount=true&amp;useTagName=true\">Self-Deception</a><br><a href=\"https://www.lesswrong.com/tag/sunk-cost-fallacy?showPostCount=true&amp;useTagName=true\">Sunk-Cost Fallacy</a></p></td></tr><tr><td style=\"border-bottom:solid hsl(0, 0%, 100%);border-left:solid hsl(0, 0%, 100%);border-right:solid hsl(0, 0%, 100%);border-top:solid hsl(0, 0%, 100%);padding:0px;vertical-align:top\" rowspan=\"2\"><p><strong>Communication</strong></p><p><a href=\"https://www.lesswrong.com/tag/common-knowledge?showPostCount=true&amp;useTagName=true\"><u>Common Knowledge</u></a><br><a href=\"https://www.lesswrong.com/tag/conversation-topic?showPostCount=true\"><u>Conversation</u></a><br><a href=\"https://www.lesswrong.com/tag/decoupling-vs-contextualizing?showPostCount=true&amp;useTagName=true\"><u>Decoupling vs Contextualizing</u></a><br><a href=\"https://www.lesswrong.com/tag/disagreement?showPostCount=true&amp;useTagName=true\"><u>Disagreement</u></a><br><a href=\"http://www.lesswrong.com/tag/distillation-and-pedagogy?showPostCount=true&amp;useTagName=true\">Distillation &amp; Pedagogy</a><br><a href=\"http://www.lesswrong.com/tag/double-crux?showPostCount=true&amp;useTagName=true\"><u>Double-Crux</u></a><br><a href=\"http://www.lesswrong.com/tag/good-explanations-advice?showPostCount=true&amp;useTagName=true\">Good Explanations (Advice)</a><br><a href=\"http://www.lesswrong.com/tag/ideological-turing-tests?showPostCount=true&amp;useTagName=true\">Ideological Turing Tests</a><br><a href=\"https://www.lesswrong.com/tag/inferential-distance?showPostCount=true&amp;useTagName=true\">Inferential Distance</a><br><a href=\"https://www.lesswrong.com/tag/information-cascades?showPostCount=true&amp;useTagName=true\">Information Cascades</a><br><a href=\"https://www.lesswrong.com/tag/memetic-immune-system?showPostCount=true&amp;useTagName=true\">Memetic Immune System</a><br><a href=\"https://www.lesswrong.com/tag/philosophy-of-language?showPostCount=true&amp;useTagName=true\"><u>Philosophy of Language</u></a><br><a href=\"https://www.lesswrong.com/tag/steelmanning?showPostCount=true&amp;useTagName=true\">Steelmanning</a></p></td><td style=\"background-color:hsl(0,0%,100%);border-bottom:1px solid hsl(0, 0%, 100%);border-left:1px solid hsl(0, 0%, 100%);border-right:1px solid hsl(0, 0%, 100%);border-top:1px solid hsl(0, 0%, 100%);padding:0px;vertical-align:top\" rowspan=\"2\"><p><strong>Techniques</strong></p><p><a href=\"http://www.lesswrong.com/tag/double-crux?showPostCount=true&amp;useTagName=true\"><u>Double-Crux</u></a><br><a href=\"https://www.lesswrong.com/tag/focusing?showPostCount=true&amp;useTagName=true\">Focusing</a><br><a href=\"https://www.lesswrong.com/tag/goal-factoring?showPostCount=true&amp;useTagName=true\">Goal Factoring</a><br><a href=\"https://www.lesswrong.com/tag/internal-double-crux?showPostCount=true&amp;useTagName=true\">Internal Double Crux</a><br><a href=\"https://www.lesswrong.com/tag/hamming-questions?showPostCount=true&amp;useTagName=true\"><u>Hamming Questions</u></a><br><a href=\"https://www.lesswrong.com/tag/noticing?showPostCount=true&amp;useTagName=true\">Noticing</a><br><a href=\"https://www.lesswrong.com/tag/techniques?showPostCount=true&amp;useTagName=true\">Techniques</a><br><a href=\"https://www.lesswrong.com/tag/trigger-action-planning?showPostCount=true&amp;useTagName=true\">Trigger Action Planning/Patterns</a></p></td><td style=\"background-color:hsl(0,0%,100%);border-bottom:1px solid hsl(0, 0%, 100%);border-left:1px solid hsl(0, 0%, 100%);border-right:1px solid hsl(0, 0%, 100%);border-top:1px solid hsl(0, 0%, 100%);padding:0px;vertical-align:top\"><p><strong>Models of the Mind</strong></p><p><a href=\"https://www.lesswrong.com/tag/consciousness?showPostCount=true&amp;useTagName=true\">Consciousness</a><br><a href=\"https://www.lessestwrong.com/tag/dual-process-theory-system-1-and-system-2?showPostCount=true&amp;useTagName=false\">Dual Process Theory (System 1 &amp; 2)</a><br><a href=\"http://www.lesswrong.com/tag/general-intelligence?showPostCount=true&amp;useTagName=true\"><u>General Intelligence</u></a><br><a href=\"https://www.lesswrong.com/tag/subagents?showPostCount=true&amp;useTagName=true\"><u>Subagents</u></a><br><a href=\"https://www.lesswrong.com/tag/predictive-processing?showPostCount=true&amp;useTagName=true\"><u>Predictive Processing</u></a><br><a href=\"https://www.lesswrong.com/tag/perceptual-control-theory?showPostCount=true&amp;useTagName=true\">Perceptual Control Theory</a><br><a href=\"http://www.lesswrong.com/tag/zombies?showPostCount=true&amp;useTagName=true\">Zombies</a><br>&nbsp;</p></td></tr><tr><td style=\"border-bottom:1px solid hsl(0, 0%, 100%);border-left:1px solid hsl(0, 0%, 100%);border-right:1px solid hsl(0, 0%, 100%);border-top:1px solid hsl(0, 0%, 100%);padding:0px\"><p><strong>Other</strong></p><p><a href=\"https://www.lesswrong.com/tag/center-for-applied-rationality-cfar?showPostCount=true\"><u>Center for Applied Rationality</u></a><br><a href=\"https://www.lesswrong.com/tag/curiosity?useTagName=true&amp;showPostCount=true\"><u>Curiosity</u></a><br><a href=\"https://www.lesswrong.com/tag/rationality-quotes?showPostCount=true&amp;useTagName=true\">Rationality Quotes</a><br><a href=\"http://www.lesswrong.com/tag/updated-beliefs-examples-of?showPostCount=true&amp;useTagName=true\">Updated Beliefs (examples of)</a></p></td></tr></tbody></table>\n\n*This list is not comprehensive! The tagging system is new. Many needed tags have not been created and/or added to the above list.*\n\n* * *\n\nWhat we're calling \"rationality\"\n--------------------------------\n\nA good heuristic is that rationality is about cognitive algorithms. Rather than being a synonym for *true* or *optimal*, the term ***rational*** should be reserved for describing whether or not a cognitive algorithm results in true beliefs and optimal actions.\n\nThis is distinct from [practical advice](https://www.lesswrong.com/tag/practical-advice), such as how to improve relationships or implement productivity systems, which should not be considered \"rationality\" per se. Some have pushed against labeling self-help as \"rational dating\", etc. for reasons along these lines \\[[1](https://www.lesswrong.com/posts/HcCpvYLoSFP4iAqSz/rationality-appreciating-cognitive-algorithms), [2](https://www.lesswrong.com/posts/DFHhuAMexXAi8T6AY/the-rational-rationalist-s-guide-to-rationally-using)\\], and they are probably correct.\n\nIn accordance with this, LessWrong classifies most self-help type advice under the [World Optimization](https://www.lesswrong.com/tag/world-optimization) tag and not the Rationality tag.\n\nSimilarly, most object-level material about *how the world is*, e.g. math, biology, history, etc. is tagged under [World Modeling](https://www.lesswrong.com/tag/world-modeling) tag, with exceptions for neuroscience and probability theory, etc., which have concrete consequences for how one ought to think.\n\nHeuristics and Biases\n---------------------\n\nEarly material on LessWrong frequently describes rationality with reference to heuristics and biases \\[[1,](https://www.lesswrong.com/posts/xLm9mgJRPvmPGpo7Q/the-cognitive-science-of-rationality) [2](https://www.lesswrong.com/posts/Psp8ZpYLCDJjshpRb/your-intuitions-are-not-magic)\\]. Indeed, LessWrong grew out of the blog [Overcoming Bias](https://www.overcomingbias.com/) and even [Rationality: A-Z](https://www.lesswrong.com/rationality) opens with a discussion of biases \\[[1](https://www.lesswrong.com/s/5g5TkQTe9rmPS5vvM/p/ptxnyfLWqRZ98wnYi)\\] with the opening chapter titled [Predictably Wrong](https://www.lesswrong.com/s/5g5TkQTe9rmPS5vvM). The idea is that human mind has been shown to systematically make certain errors of reasoning, like confirmation bias. Rationality then consists of overcoming these biases.\n\nApart from the issue of the replication crises which discredited many examples of bias that were commonly referenced on LessWrong, e.g. priming, the \"overcoming biases\" frame of rationality is too limited. Rationality requires the development of many *positive* skills, not just removing *negative* biases to reveal underlying perfect reasoning. These are skills such as how to update the correct amount in response to evidence, how to resolve disagreements with others, how to introspect, and many more.\n\nInstrumental vs Epistemic Rationality\n-------------------------------------\n\nClassically, on LessWrong, a distinction has been made between *instrumental* rationality  and *epistemic* rationality, however, these terms may be misleading – it's not as though epistemic rationality can be traded off for gains in instrumental rationality. Only apparently, and to think one should do this is a trap.\n\n**Instrumental rationality** is defined as being concerned with achieving goals. More specifically, instrumental rationality is the art of choosing and *implementing* actions that steer the future toward outcomes ranked higher in one's preferences. Said preferences are not limited to 'selfish' preferences or unshared values; they include anything one cares about.\n\n**Epistemic rationality** is defined as the part of rationality which involves achieving accurate beliefs about the world. It involves [updating](https://wiki.lesswrong.com/wiki/updating) on receiving new [evidence](https://lessestwrong.com/tag/evidence), mitigating cognitive biases, and examining why you believe what you believe. It can be seen as a form of instrumental rationality in which knowledge and truth are goals in themselves, whereas in other forms of instrumental rationality, knowledge and truth are only potential aids to achieving goals. Someone practicing instrumental rationality might even find falsehood useful.\n\nThe Art and Science of Rationality\n----------------------------------\n\nIn a field like biology, we can draw a distinction between the *science of biology*, which involves various theories and empirical data about biological life, and the *art of being a biologis*t, which is the specific way that a biologist thinks and plays with ideas and interacts to the world around them. Similarly, rationality is both a science and an art. There’s study of the iron-clad laws of reasoning and mechanics of the human mind, but there’s also the general training to be the kind of person who reasons well.\n\nRationalist\n-----------\n\nThe term *rationalist* as a description of people is used in a couple of ways. It can refer to someone who endeavors to think better and implement as much rationality as they can. Many prefer the term *aspiring rationalist* to convey that the identifier is a claim to the goal of being more rational rather than a claim of having attained it already.\n\nPerhaps more commonly, *rationalist* is used to refer culturally to someone associated with various rationalist communities separate from their efforts to improve their rationality."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "pnSDArjzAjkvAF5Jo",
    "name": "Efficient Market Hypothesis",
    "core": false,
    "slug": "efficient-market-hypothesis",
    "oldSlugs": null,
    "postCount": 38,
    "description": {
      "markdown": "The **Efficient Market Hypothesis** states that existing market prices already account for all available information, and that it is therefore impossible to exploit the market unless you have information other traders don't."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "2EFq8dJbxKNzforjM",
    "name": "Social Status",
    "core": false,
    "slug": "social-status",
    "oldSlugs": null,
    "postCount": 88,
    "description": {
      "markdown": "**Social Status** is an abstraction to model how people relate to each other, how social hierarchies are formed, and how people facilitate trade in the absence of financial accounting (as well as a variety of other stuff). I mean, everyone knows what status is, but here is where we break that down into its components and really try to understand what's happening on a mechanistic level.\n\nSee Also\n--------\n\n*   [Signaling](https://lessestwrong.com/tag/signaling)\n\nNotable Posts\n-------------\n\n*   [The Nature of Offense](https://lessestwrong.com/lw/13s/the_nature_of_offense/) by [Wei Dai](http://weidai.com/) \\- People are [offended](https://lessestwrong.com/tag/offense) by grabs for status.\n*   [Why Real Men Wear Pink](https://lessestwrong.com/lw/154/why_real_men_wear_pink/) by [Yvain](https://wiki.lesswrong.com/wiki/Yvain)\n*   [Actors See Status](http://www.overcomingbias.com/2009/08/actors-see-status.html) by [Robin Hanson](https://lessestwrong.com/tag/robin-hanson), quoting [Keith Johnstone](https://en.wikipedia.org/wiki/Keith_Johnstone)\n*   [That Other Kind of Status](https://lessestwrong.com/lw/1kr/that_other_kind_of_status/) by Yvain\n\nExternal\n--------\n\n*   Melting Asphault (by Kevin Simler) has many great posts on status\n*   Elephant in the Brain by Simler and Hanson\n*   Impro (book on improv covering status relations)\n*   Writings by Venkatesh Rao such as Gervais Principle and something, something Psychopath"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "fkABsGCJZ6y9qConW",
    "name": "Practical",
    "core": true,
    "slug": "practical",
    "oldSlugs": null,
    "postCount": 1618,
    "description": {
      "markdown": "**Practical** posts give direct, actionable advice on how to achieve goals and generally succeed. The art of rationality would be useless if it did not connect to the real world; we must take our ideas and abstractions and collide them with reality. Many places on the internet will give you advice; here, we value survey data, literature reviews, self-blinded trials, quantitative estimates, and theoretical models that aim to explain the phenomena.\n\nMaterial that is directly about *how to think better* can be found at [Rationality](https://www.lessestwrong.com/tag/rationality).\n\n**                                           Practical Sub-Topics**\n===================================================================\n\n<table style=\"background-color:rgb(255, 255, 255);border-bottom:2px solid hsl(0, 0%, 90%);border-left:2px solid hsl(0, 0%, 90%);border-right:2px solid hsl(0, 0%, 90%);border-top:2px solid hsl(0, 0%, 90%)\"><tbody><tr><td style=\"border-bottom:1px solid hsl(0, 0%, 100%);border-left:1px solid hsl(0, 0%, 100%);border-right:1px solid hsl(0, 0%, 100%);border-top:1px solid hsl(0, 0%, 100%);padding:0px;vertical-align:top;width:33%\" rowspan=\"2\"><p><strong>Domains of Well-being</strong></p><p><a href=\"http://www.lesswrong.com/tag/careers?showPostCount=true&amp;useTagName=true\">Careers</a><br><a href=\"https://www.lesswrong.com/tag/emotions?showPostCount=true&amp;useTagName=true\">Emotions</a><br><a href=\"http://www.lesswrong.com/tag/exercise-physical?showPostCount=true&amp;useTagName=true\">Exercise (Physical)</a><br><a href=\"https://www.lesswrong.com/tag/financial-investing?showPostCount=true&amp;useTagName=true\">Financial Investing</a><br><a href=\"http://www.lesswrong.com/tag/gratitude?showPostCount=true&amp;useTagName=true\">Gratitude</a><br><a href=\"http://www.lesswrong.com/tag/happiness-1?showPostCount=true&amp;useTagName=true\">Happiness</a><br><a href=\"http://www.lesswrong.com/tag/human-bodies?showPostCount=true&amp;useTagName=true\">Human Bodies</a><br><a href=\"http://www.lesswrong.com/tag/nutrition?showPostCount=true&amp;useTagName=true\">Nutrition</a><br><a href=\"https://www.lesswrong.com/tag/parenting?showPostCount=true&amp;useTagName=true\">Parenting</a><br><a href=\"https://www.lesswrong.com/tag/slack?showPostCount=true&amp;useTagName=true\">Slack</a><br><a href=\"https://www.lesswrong.com/tag/sleep?showPostCount=true&amp;useTagName=true\">Sleep</a><br><a href=\"https://www.lesswrong.com/tag/well-being?showPostCount=true&amp;useTagName=true\">Well-being</a></p></td><td style=\"border-bottom:solid hsl(0, 0%, 100%);border-left:solid hsl(0, 0%, 100%);border-right:solid hsl(0, 0%, 100%);border-top:solid hsl(0, 0%, 100%);padding:0px;vertical-align:top;width:33%\" rowspan=\"2\"><p><strong>Skills, Tools, Techniques</strong></p><p><a href=\"https://www.lesswrong.com/tag/cryonics?showPostCount=true&amp;useTagName=true\">Cryonics</a><br><a href=\"https://www.lesswrong.com/tag/emotions?showPostCount=true&amp;useTagName=true\">Emotions</a><br><a href=\"https://www.lesswrong.com/tag/goal-factoring?showPostCount=true&amp;useTagName=true\">Goal Factoring</a><br><a href=\"http://www.lesswrong.com/tag/habits?showPostCount=true&amp;useTagName=true\">Habits</a><br><a href=\"https://www.lesswrong.com/tag/hamming-questions?showPostCount=true&amp;useTagName=true\">Hamming Questions</a><br><a href=\"http://www.lesswrong.com/tag/life-improvements?showPostCount=true&amp;useTagName=true\">Life Improvements</a><br><a href=\"https://www.lesswrong.com/tag/meditation?showPostCount=true&amp;useTagName=true\">Meditation</a><br><a href=\"http://www.lesswrong.com/tag/more-dakka?showPostCount=true&amp;useTagName=true\">More Dakka</a><br><a href=\"https://www.lesswrong.com/tag/pica?showPostCount=true\"><u>Pica</u></a><br><a href=\"https://www.lesswrong.com/tag/planning-and-decision-making?showPostCount=true&amp;useTagName=true\">Planning &amp; Decision-Making</a><br><a href=\"https://www.lesswrong.com/tag/self-experimentation?showPostCount=true&amp;useTagName=true\">Self Experimentation</a><br><a href=\"http://www.lesswrong.com/tag/skill-building?showPostCount=true&amp;useTagName=true\">Skill Building</a><br><a href=\"https://www.lesswrong.com/tag/software-tools?showPostCount=true&amp;useTagName=true\">Software Tools</a><br><a href=\"https://www.lesswrong.com/tag/spaced-repetition?showPostCount=true&amp;useTagName=true\">Spaced Repetition</a><br><a href=\"https://www.lesswrong.com/tag/virtues-instrumental?showPostCount=true&amp;useTagName=false\">Virtues (Instrumental)</a></p></td><td style=\"border-bottom:solid hsl(0, 0%, 100%);border-left:solid hsl(0, 0%, 100%);border-right:solid hsl(0, 0%, 100%);border-top:solid hsl(0, 0%, 100%);height:50%;padding:0px;vertical-align:top;width:33%\"><p><strong>Productivity</strong></p><p><a href=\"https://www.lesswrong.com/tag/akrasia?showPostCount=true&amp;useTagName=true\">Akrasia</a><br><a href=\"https://www.lesswrong.com/tag/motivations?showPostCount=true&amp;useTagName=true\">Motivations</a><br><a href=\"https://www.lesswrong.com/tag/prioritization?showPostCount=true&amp;useTagName=true\">Prioritization</a><br><a href=\"https://www.lesswrong.com/tag/procrastination?showPostCount=true&amp;useTagName=true\">Procrastination</a><br><a href=\"https://www.lesswrong.com/tag/productivity?showPostCount=true&amp;useTagName=true\">Productivity</a><br><a href=\"https://www.lesswrong.com/tag/willpower?showPostCount=true&amp;useTagName=true\">Willpower</a></p></td></tr><tr><td style=\"border-bottom:1px solid hsl(0, 0%, 100%);border-left:1px solid hsl(0, 0%, 100%);border-right:1px solid hsl(0, 0%, 100%);border-top:1px solid hsl(0, 0%, 100%);padding:0px;vertical-align:top\"><strong>Interpersonal</strong><br><a href=\"http://www.lesswrong.com/tag/circling?showPostCount=true&amp;useTagName=true\"><u>Circling</u></a><br><a href=\"https://www.lesswrong.com/tag/conversation-topic?showPostCount=true&amp;useTagName=true\">Conversation (topic)</a><br><a href=\"https://www.lesswrong.com/tag/communication-cultures?showPostCount=true&amp;useTagName=true\">Communication Cultures</a><br><a href=\"http://www.lesswrong.com/tag/relationships-interpersonal?showPostCount=true&amp;useTagName=false\"><u>Relationship</u></a></td></tr></tbody></table>"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "b8FHrKqyXuYGWc6vn",
    "name": "Game Theory",
    "core": null,
    "slug": "game-theory",
    "oldSlugs": null,
    "postCount": 177,
    "description": {
      "markdown": "**Game theory** is the formal study of how rational actors interact to pursue incentives. It investigates situations of conflict and cooperation.\n\n*See also:* [Coalition/coordination](https://www.lesswrong.com/tag/coordination-cooperation?showPostCount=true&useTagName=true), [Coalitional Instincts](https://www.lesswrong.com/tag/coalitional-instincts?showPostCount=true&useTagName=true), [Decision theory](https://www.lesswrong.com/tag/decision-theory), [Moloch](https://www.lesswrong.com/tag/moloch?showPostCount=true&useTagName=true), [Utility functions](https://www.lesswrong.com/tag/utility-functions), [Decision Theory](https://lessestwrong.com/tag/decision-theory), [Prisoner's Dilemma](https://lessestwrong.com/tag/prisoner-s-dilemma)\n\nGame theory is an extremely powerful and robust tool in analyzing much more complex situations, such as: mergers and acquisitions, political economy, voting systems, war bargaining and biological evolution. Eight game-theorists have won the Nobel Prize in Economic Sciences.\n\nReferences\n----------\n\n*   [Naïve introduction to Game Theory](http://levine.sscnet.ucla.edu/general/whatis.htm)\n*   [Stanford Encyclopedia entry on Game Theory](http://plato.stanford.edu/entries/game-theory/)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "izp6eeJJEg9v5zcur",
    "name": "Community",
    "core": true,
    "slug": "community",
    "oldSlugs": null,
    "postCount": 1082,
    "description": {
      "markdown": "The **LessWrong** **Community** is the people who write on LessWrong and who contribute to its mission of refining the art of human rationality. This tag includes community events, analysis of the health, norms and direction of the community, and space to understand communities in general.  \n  \nLessWrong also has many brothers and sisters like the Berkeley Rationality Community, [SlateStarCodex](https://www.reddit.com/r/slatestarcodex/), [Rational Fiction](https://www.reddit.com/r/rational/), [Effective Altruism](https://forum.effectivealtruism.org/), [AI Alignment](https://www.alignmentforum.org/), and more, who participate here. To see upcoming LessWrong events, go to the [community section](https://www.lesswrong.com/community).\n\n* * *\n\n**                                   Community Sub-Topics**\n-----------------------------------------------------------\n\n<table style=\"border-bottom:20px solid hsl(0, 0%, 100%);border-left:20px solid hsl(0, 0%, 100%);border-right:20px solid hsl(0, 0%, 100%);border-top:20px solid hsl(0, 0%, 100%)\"><tbody><tr><td style=\"background-color:hsl(0,0%,100%);border-bottom:solid hsl(0, 0%, 100%);border-left:solid hsl(0, 0%, 100%);border-right:solid hsl(0, 0%, 100%);border-top:solid hsl(0, 0%, 100%);padding:0px;vertical-align:top;width:50%\"><p><strong>All</strong></p><p><a href=\"http://www.lesswrong.com/tag/bounties-active?showPostCount=true&amp;useTagName=true\">Bounties (active)</a><br><a href=\"https://www.lesswrong.com/tag/grants-and-fundraising-opportunities?showPostCount=true\">Grants &amp; Fundraising</a><br><a href=\"http://www.lesswrong.com/tag/growth-stories?showPostCount=true&amp;useTagName=true\">Growth Stories</a><br><a href=\"https://www.lesswrong.com/tag/online-socialization?showPostCount=true&amp;useTagName=true\">Online Socialization</a><br><a href=\"https://www.lesswrong.com/tag/petrov-day?showPostCount=true&amp;useTagName=true\">Petrov Day</a><br><a href=\"https://www.lesswrong.com/tag/public-discourse?showPostCount=true&amp;useTagName=true\">Public Discourse</a><br><a href=\"https://www.lesswrong.com/tag/research-agendas?showPostCount=true&amp;useTagName=true\">Research Agendas</a><br><a href=\"https://www.lesswrong.com/tag/ritual?showPostCount=true&amp;useTagName=true\">Ritual</a><br><a href=\"https://www.lesswrong.com/tag/solstice-celebration?showPostCount=true&amp;useTagName=true\">Solstice Celebration</a><br>&nbsp;</p></td><td style=\"background-color:hsl(0,0%,100%);border-bottom:1px solid hsl(0, 0%, 100%);border-left:1px solid hsl(0, 0%, 100%);border-right:1px solid hsl(0, 0%, 100%);border-top:1px solid hsl(0, 0%, 100%);padding:0px;vertical-align:top;width:50%\"><p><strong>LessWrong</strong></p><p><a href=\"http://www.lesswrong.com/tag/events-community?showPostCount=true&amp;useTagName=true\">Events (Community)</a><br><a href=\"https://www.lesswrong.com/tag/site-meta?showPostCount=true&amp;useTagName=true\">Site Meta</a><br><a href=\"https://www.lesswrong.com/tag/greaterwrong-meta?showPostCount=true&amp;useTagName=true\">GreaterWrong Meta</a><br><a href=\"https://www.lesswrong.com/tag/lesswrong-events?showPostCount=true&amp;useTagName=true\">LessWrong Events</a><br><a href=\"http://www.lesswrong.com/tag/lw-moderation?showPostCount=true&amp;useTagName=true\">LW Moderation</a><br><a href=\"http://www.lesswrong.com/tag/meetups-topic?showPostCount=true&amp;useTagName=true\">Meetups (topic)</a><br><a href=\"http://www.lesswrong.com/tag/moderation-topic?showPostCount=true&amp;useTagName=true\">Moderation (topic)</a><br><a href=\"http://www.lesswrong.com/tag/the-sf-bay-area?showPostCount=true&amp;useTagName=true\">The SF Bay Area</a><br><a href=\"http://www.lesswrong.com/tag/tagging?showPostCount=true\">Tagging</a></p></td></tr></tbody></table>\n\n*Not all Community posts are tagged with subtopics.*\n\n* * *\n\nThis tag applies to any post about:\n\n*   Specific projects, orgs, and prizes \\[e.g. [1](http://www.lesswrong.com/posts/xFGQdgJndLcthgWoE), [2](http://www.lesswrong.com/posts/KgFrtaajjfSnBSZoH), [3](http://www.lesswrong.com/posts/auL2gAGTb3MsYhCeN), [4](http://www.lesswrong.com/posts/cSzaxcmeYW6z7cgtc), [5](https://www.lesswrong.com/posts/nDHbgjdddG5EN6ocg)\\]\n*   Requests and offers for help \\[[1](http://www.lesswrong.com/posts/bSWavBThj6ebB62gD), [2](http://www.lesswrong.com/posts/LuL7LLqcdmM7TTYvW), [3](http://www.lesswrong.com/posts/x72ta8C3dKu2QRfPv)\\]\n*   Announcements, retrospectives, funding requests, and AMAs from orgs \\[[1](http://www.lesswrong.com/posts/XJiNtvxoiLCpBn6FH) [2](https://www.lesswrong.com/posts/96N8BT9tJvybLbn5z/we-run-the-center-for-applied-rationality-ama) [3](http://www.lesswrong.com/posts/KgFrtaajjfSnBSZoH), [4](http://www.lesswrong.com/posts/auL2gAGTb3MsYhCeN), [5](https://www.lesswrong.com/posts/tCHsm5ZyAca8HfJSG)\\]\n*   Discussions of the orgs in the LessWrong, Rationalist cluster \\[[1](http://www.lesswrong.com/posts/KpnyCT7CZy4Qe6kx6), [2](https://www.lesswrong.com/posts/6SGqkCgHuNr7d4yJm/thoughts-on-the-singularity-institute-si)\\]\n*   Discussions about the LessWrong, Rationalist, and related communities \\[[1](http://www.lesswrong.com/posts/2Ee5DPBxowTTXZ6zf), [2](http://www.lesswrong.com/posts/yGycR8tFA3JJbvApp), [3](https://www.lesswrong.com/posts/zAqoj79A7QuhJKKvi)\\]\n\nWhile the [World Optimization](https://www.lesswrong.com/tag/world-optimization)  core tag is for posts discussing how to do good in general, the Community tag is for the specific, concrete efforts of our community to execute plans."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "xexCWMyds6QLWognu",
    "name": "World Optimization",
    "core": true,
    "slug": "world-optimization",
    "oldSlugs": null,
    "postCount": 1481,
    "description": {
      "markdown": "**World Optimization** is the full use of our agency. It is extending the reach of human civilization. It is building cities and democracies and economic systems and computers and flight and science and space rockets and the internet. World optimization is about adding to that list.   \n  \nBut it’s not just about growth, it’s also about preservation. We are still in the dawn of civilization, with most of civilization in the billions of years ahead. We mustn’t let this light go out.\n\n* * *\n\n**                              World Optimization Sub-Topics**\n===============================================================\n\n<table style=\"border-bottom:20px solid hsl(0, 0%, 100%);border-left:20px solid hsl(0, 0%, 100%);border-right:20px solid hsl(0, 0%, 100%);border-top:20px solid hsl(0, 0%, 100%)\"><tbody><tr><td style=\"background-color:hsl(0,0%,100%);border-bottom:solid hsl(0, 0%, 100%);border-left:solid hsl(0, 0%, 100%);border-right:solid hsl(0, 0%, 100%);border-top:solid hsl(0, 0%, 100%);height:50%;padding:0px;vertical-align:top;width:33%\"><p><strong>Moral Theory</strong></p><p><a href=\"https://www.lesswrong.com/tag/altruism?showPostCount=true&amp;useTagName=true\">Altruism</a><br><a href=\"https://www.lesswrong.com/tag/consequentialism?showPostCount=true&amp;useTagName=true\">Consequentialism</a><br><a href=\"https://www.lesswrong.com/tag/deontology?showPostCount=true&amp;useTagName=true\">Deontology</a><br><a href=\"http://www.lesswrong.com/tag/ethics-and-morality?showPostCount=true&amp;useTagName=true\"><u>Ethics &amp; Morality</u></a><br><a href=\"https://www.lesswrong.com/tag/metaethics?showPostCount=true&amp;useTagName=true\">Metaethics</a><br><a href=\"http://www.lesswrong.com/tag/moral-uncertainty?showPostCount=true&amp;useTagName=true\"><u>Moral Uncertainty</u></a></p><p>&nbsp;</p><p>&nbsp;</p></td><td style=\"background-color:hsl(0,0%,100%);border-bottom:solid hsl(0, 0%, 100%);border-left:solid hsl(0, 0%, 100%);border-right:solid hsl(0, 0%, 100%);border-top:solid hsl(0, 0%, 100%);padding:0px;vertical-align:top;width:33%\"><p><strong>Causes / Interventions</strong></p><p><a href=\"https://www.lesswrong.com/tag/aging?showPostCount=true&amp;useTagName=true\">Aging</a><br><a href=\"https://www.lesswrong.com/tag/animal-welfare?showPostCount=true&amp;useTagName=true\">Animal Welfare</a><br><a href=\"https://www.lesswrong.com/tag/existential-risk?showPostCount=true&amp;useTagName=true\">Existential Risk</a><br><a href=\"http://www.lesswrong.com/tag/futurism?showPostCount=true&amp;useTagName=true\">Futurism</a><br><a href=\"https://www.lesswrong.com/tag/mind-uploading?showPostCount=true&amp;useTagName=true\">Mind Uploading</a><br><a href=\"https://www.lesswrong.com/tag/life-extension?showPostCount=true&amp;useTagName=true\">Life Extension</a><br><a href=\"http://www.lesswrong.com/tag/risks-of-astronomical-suffering-s-risks?showPostCount=true&amp;useTagName=false\"><u>S-risks</u></a><br><a href=\"https://www.lesswrong.com/tag/transhumanism?showPostCount=true&amp;useTagName=true\"><u>Transhumanism</u></a><br><a href=\"https://www.lesswrong.com/tag/voting-theory?showPostCount=true&amp;useTagName=true\">Voting Theory</a></p></td><td style=\"background-color:hsl(0,0%,100%);border-bottom:solid hsl(0, 0%, 100%);border-left:solid hsl(0, 0%, 100%);border-right:solid hsl(0, 0%, 100%);border-top:solid hsl(0, 0%, 100%);padding:0px;vertical-align:top;width:33%\"><p><strong>Working with Humans</strong></p><p><a href=\"http://www.lesswrong.com/tag/coalitional-instincts?showPostCount=true&amp;useTagName=true\"><u>Coalitional Instincts</u></a><br><a href=\"https://www.lesswrong.com/tag/common-knowledge?showPostCount=true&amp;useTagName=true\"><u>Common Knowledge</u></a><br><a href=\"http://www.lesswrong.com/tag/coordination-cooperation?showPostCount=true&amp;useTagName=true\">Coordination / Cooperation</a><br><a href=\"https://www.lesswrong.com/tag/game-theory?showPostCount=true&amp;useTagName=true\">Game Theory</a><br><a href=\"http://www.lesswrong.com/tag/group-rationality?showPostCount=true&amp;useTagName=true\">Group Rationality</a><br><a href=\"https://www.lesswrong.com/tag/institution-design?showPostCount=true&amp;useTagName=true\">Institution Design</a><br><a href=\"https://www.lesswrong.com/tag/moloch?showPostCount=true&amp;useTagName=true\">Moloch</a><br><a href=\"https://www.lesswrong.com/tag/signaling?showPostCount=true&amp;useTagName=true\">Signaling</a><br><a href=\"https://www.lesswrong.com/tag/simulacrum-levels?showPostCount=true&amp;useTagName=true\">Simulacrum Levels</a><br><a href=\"https://www.lesswrong.com/tag/social-status?showPostCount=true&amp;useTagName=true\">Social Status</a></p></td></tr><tr><td style=\"background-color:hsl(0,0%,100%);border-bottom:1px solid hsl(0, 0%, 100%);border-left:1px solid hsl(0, 0%, 100%);border-right:1px solid hsl(0, 0%, 100%);border-top:1px solid hsl(0, 0%, 100%);padding:0em;vertical-align:top\"><p><strong>Applied Topics</strong></p><p><a href=\"https://www.lesswrong.com/tag/blackmail?showPostCount=true&amp;useTagName=true\">Blackmail</a><br><a href=\"http://www.lesswrong.com/tag/censorship?showPostCount=true&amp;useTagName=true\">Censorship</a><br><a href=\"http://www.lesswrong.com/tag/chesterton-s-fence?showPostCount=true&amp;useTagName=true\">Chesterton's Fence</a><br><a href=\"http://www.lesswrong.com/tag/death?showPostCount=true&amp;useTagName=true\">Death</a><br><a href=\"https://www.lesswrong.com/tag/deception?showPostCount=true&amp;useTagName=true\">Deception</a><br><a href=\"https://www.lesswrong.com/tag/honesty?showPostCount=true&amp;useTagName=true\">Honesty</a><br><a href=\"https://www.lesswrong.com/tag/hypocrisy?showPostCount=true&amp;useTagName=true\">Hypocrisy</a><br><a href=\"https://www.lesswrong.com/tag/information-hazards?showPostCount=true&amp;useTagName=true\">Information Hazards</a><br><a href=\"https://www.lesswrong.com/tag/meta-honesty?showPostCount=true&amp;useTagName=true\">Meta-Honesty</a><br><a href=\"http://www.lesswrong.com/tag/pascal-s-mugging?showPostCount=true&amp;useTagName=true\">Pascal's Mugging</a><br><a href=\"http://www.lesswrong.com/tag/war?showPostCount=true&amp;useTagName=true\">War</a></p></td><td style=\"background-color:hsl(0,0%,100%);border-bottom:solid hsl(0, 0%, 100%);border-left:solid hsl(0, 0%, 100%);border-right:solid hsl(0, 0%, 100%);border-top:solid hsl(0, 0%, 100%);height:25px;padding:0px;vertical-align:top\"><p><strong>Value &amp; Virtue</strong></p><p><a href=\"http://www.lesswrong.com/tag/ambition?showPostCount=true&amp;useTagName=true\">Ambition</a><br><a href=\"https://www.lesswrong.com/tag/art?showPostCount=true&amp;useTagName=true\">Art</a><br><a href=\"https://www.lesswrong.com/tag/aesthetics?showPostCount=true&amp;useTagName=true\">Aesthetics</a><br><a href=\"https://www.lesswrong.com/tag/complexity-of-value?showPostCount=true&amp;useTagName=true\">Complexity of Value</a><br><a href=\"http://www.lesswrong.com/tag/courage?showPostCount=true&amp;useTagName=true\">Courage</a><br><a href=\"http://www.lesswrong.com/tag/fun-theory?showPostCount=true&amp;useTagName=true\">Fun Theory</a><br><a href=\"http://www.lesswrong.com/tag/principles?showPostCount=true&amp;useTagName=true\">Principles</a><br><a href=\"http://www.lesswrong.com/tag/suffering?showPostCount=true&amp;useTagName=true\"><u>Suffering</u></a><br><a href=\"https://www.lesswrong.com/tag/superstimuli?showPostCount=true&amp;useTagName=true\">Superstimuli</a><br><a href=\"https://www.lesswrong.com/tag/wireheading?showPostCount=true&amp;useTagName=true\">Wireheading</a></p></td><td style=\"background-color:hsl(0,0%,100%);border-bottom:1px solid hsl(0, 0%, 100%);border-left:1px solid hsl(0, 0%, 100%);border-right:1px solid hsl(0, 0%, 100%);border-top:1px solid hsl(0, 0%, 100%);padding:0em;vertical-align:top\"><p><strong>Meta</strong></p><p><a href=\"https://www.lesswrong.com/tag/cause-prioritization?showPostCount=true&amp;useTagName=true\">Cause Prioritization</a><br><a href=\"http://www.lesswrong.com/tag/center-on-long-term-risk-clr?useTagName=true&amp;showPostCount=true\">Center for Long-term Risk</a><br><a href=\"https://www.lesswrong.com/tag/effective-altruism?showPostCount=true&amp;useTagName=true\">Effective Altruism</a><br><a href=\"https://www.lesswrong.com/tag/heroic-responsibility?showPostCount=true&amp;useTagName=true\">Heroic Responsibility</a><br>&nbsp;</p></td></tr></tbody></table>\n\n* * *\n\nContent which describes *how the world is* that directly bears upon choices one makes to optimize the world fall under this tag. Examples include discussion of the moral patienthood of different animals, the potential of human civilization, and the most effective interventions against a global health threat.\n\nSome material has both immediate relevance to world optimization decisions but also can inform broader world models. This material might be included under both [World Modeling](https://www.lesswrong.com/tag/world-modeling) tag and this tag."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "Z8wZZLeLMJ3NSK7kR",
    "name": "Metaethics",
    "core": null,
    "slug": "metaethics",
    "oldSlugs": null,
    "postCount": 68,
    "description": {
      "markdown": "**Metaethics** is one of the three branches of ethics usually recognized by philosophers, the others being [normative ethics](http://en.wikipedia.org/wiki/Normative_ethics) and [applied ethics](http://en.wikipedia.org/wiki/Applied_ethics). It’s a field of study that tries to understand the metaphysical, epistemological and semantic characteristics as well as the foundations and scope of moral values. It worries about questions and problems such as \"Are moral judgments objective or subjective, relative or absolute?\", \"Do moral facts exist?\" or “How do we learn moral values?”. (As distinct from object-level moral questions like, \"Ought I to steal from banks in order to give the money to the deserving poor?\")\n\nMetaethics on LessWrong\n-----------------------\n\nEliezer Yudkowsky wrote a Sequence about metaethics, the [Metaethics sequence](https://www.lesswrong.com/s/W2fkmatEzyrmbbrDt), which Yudkowsky worried failed to convey his central point ([this post by Luke](https://www.lesswrong.com/posts/3R2vH2Ar5AbC9m8Qj/what-is-eliezer-yudkowsky-s-meta-ethical-theory) tried to clarify); he approached the same problem again from a different angle in [Highly Advanced Epistemology 101 for Beginners](https://www.lesswrong.com/s/SqFbMbtxGybdS2gRs). From a standard philosophical standpoint, Yudkowsky's philosophy is closest to Frank Jackson's moral functionalism / analytic descriptivism; Yudkowsky could be loosely characterized as moral cognitivist - someone who believes moral sentences are either true or false - but not a moral realist - thus denying that moral sentences refer to facts about the world. Yudkowsky believes that moral cognition in any single human is at least potentially about a subject matter that is 'logical' in the sense that its semantics can be pinned down by axioms, and hence that moral cognition can bear truth-values; also that human beings both using similar words like \"morality\" can be talking about highly overlapping subject matter; but not that all possible minds would find the truths about this subject matter to be psychologically compelling.\n\nLuke Muehlhauser has written a sequence, [No-Nonsense Metaethics](https://www.lessestwrong.com/s/bQgRsy23biR52poMf), where he claims that many of the questions of metaethics can be answered today using modern neuroscience and rationality. He explains how conventional metaethics or \"Austere Metaethics\" is capable of, after assuming a definition of 'right', choosing the right action given a situation - but useless without assuming some criteria for 'right'. He proposes instead \"Empathic Metaethics\" which utilizes your underlying cognitive algorithms to understand what you think 'right' means, helps clarify any emotional and cognitive contradictions in it, and then tells you what the right thing to do is, *according to your definition of right*. This approach is highly relevant for the [Friendly AI](https://www.lesswrong.com/tag/friendly-artificial-intelligence) problem as a way of defining human-like goals and motivations when designing AIs.\n\nFurther Reading & References\n----------------------------\n\n*   Garner, Richard T.; Bernard Rosen (1967). Moral Philosophy: A Systematic Introduction to Normative Ethics and Meta-ethics. New York: Macmillan. pp. 215\n*   [Metaethics](http://plato.stanford.edu/entries/metaethics/) in the Stanford Encyclopedia of Philosophy\n\nSee Also\n--------\n\n*   [Complexity of value](https://lessestwrong.com/tag/complexity-of-value)\n*   [Utility](https://lessestwrong.com/tag/utility)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "etDohXtBrXd8WqCtR",
    "name": "Fiction",
    "core": null,
    "slug": "fiction",
    "oldSlugs": null,
    "postCount": 410,
    "description": {
      "markdown": "**Fiction** isn't literal truth, but when done well it captures truths and intuitions that are difficult to explain directly. (It’s also damn fun to read.)\n\n> “Nonfiction conveys *knowledge,* fiction conveys *experience.*” \\- Eliezer Yudkowsky \n\nEliezer Yudkowsky helped kickstart the genre of [*rationalist fiction*](https://www.lesswrong.com/posts/q79vYjHAE9KHcAjSs/rationalist-fiction), which is about characters who solve the problems in their world by thinking, in a way where the reader *could figure it out too*. Not where the genius character explains it afterward like Sherlock Holmes or Artemis Fowl, but where the problem is fair and you could’ve figured it out first. Eliezer has written about this in his short online book [The Abridged Guide to Intelligent Characters](https://yudkowsky.tumblr.com/writing).\n\nOther fiction on the site is in the spirit of hard science fiction, and often involves taking the laws of a universe or the rules of a system to their extreme conclusions, and munchkining your way to become god (or something similar). They also share much of the parts of sci-fi that engage with difficult moral quandaries.\n\nFiction on this site also tends to have puns. I’m so sorry.\n\nMuch more fiction can be found at [r/Rational](https://www.reddit.com/r/rational), which is a subreddit devoted to rationalist fiction.\n\nThis is a tag for works of fiction, not for analysis or discussion of literature. For that see [Fiction (topic)](https://www.lesswrong.com/tag/fiction-topic).\n\nFiction Sequences\n-----------------\n\n*   [HPMOR](https://www.lesswrong.com/hpmor)\n*   [Three Worlds Collide](https://www.lesswrong.com/s/qWoFR4ytMpQ5vw3FT)\n*   [The Bayesian Conspiracy](https://www.lesswrong.com/s/LAop879LCQWrM5YnE)\n*   [Murphy's Quest](https://www.lesswrong.com/s/4C33PKt2cQdA7oyfJ)\n*   [Luna Lovegood and the Chamber of Secrets](https://www.lesswrong.com/s/TF77XsD5PbucbJsG3)\n*   [Bayeswatch](https://www.lesswrong.com/s/TjdhvTSptCYakw3Lc)\n*   [Short stories](https://www.lesswrong.com/s/qMtriMPLdriNkAfSJ) by lsusr\n\nExternal links\n--------------\n\n*   [/r/rational/](https://www.reddit.com/r/rational/) on [Reddit](https://lessestwrong.com/tag/reddit)\n*   [RationalFic](http://tvtropes.org/pmwiki/pmwiki.php/Main/RationalFic) on TV Tropes\n*   [Eliezer Yudkowsky's guide to writing intelligent characters](http://yudkowsky.tumblr.com/writing)\n*   [rationalreads.com](http://rationalreads.com/)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "udPbn9RthmgTtHMiG",
    "name": "Productivity",
    "core": false,
    "slug": "productivity",
    "oldSlugs": null,
    "postCount": 155,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "dqx5k65wjFfaiJ9sQ",
    "name": "Procrastination",
    "core": false,
    "slug": "procrastination",
    "oldSlugs": null,
    "postCount": 32,
    "description": {
      "markdown": "**Procrastination** is \\[TODO: finish tag description\\]"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "kJrjorSx3hXa7q7CJ",
    "name": "Surveys",
    "core": false,
    "slug": "surveys",
    "oldSlugs": null,
    "postCount": 66,
    "description": {
      "markdown": "**Surveys** and polls of users of LessWrong and related communities, results, and analysis of the resulting data."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "9YFoDPFwMoWthzgkY",
    "name": "Pitfalls of Rationality",
    "core": false,
    "slug": "pitfalls-of-rationality",
    "oldSlugs": null,
    "postCount": 68,
    "description": {
      "markdown": "**Pitfalls of Rationality** are frequent [error modes](https://www.lesswrong.com/tag/failure-mode), obstacles or problems that arise when people try to practice rationality, or engage with rationality-related materials. Related concepts include the \"valley of bad rationality\".  \n  \nThere are two threads touched in posts under this tag:\n\n1.  Things that go wrong when people try to be more rational and they unintentionally end up making things worse.\n2.  Arguably, why haven't rationalists visible succeeded at their bold and ambitious goals yet?\n\nRegarding the first point, from [Incremental Progress and the Valley](https://www.lesswrong.com/posts/oZNXmHcdhb4m7vwsv/incremental-progress-and-the-valley):\n\n> Ah.  Well, here's the the thing:  An *incremental* step in the direction of rationality, if the result is still irrational in other ways, does not have to yield *incrementally* more winning.\n> \n> The optimality theorems that we have for probability theory and decision theory, are for *perfect* probability theory and decision theory.  There is no companion theorem which says that, starting from some flawed initial form, every *incremental* modification of the algorithm that takes the structure closer to the ideal, must yield an *incremental* improvement in performance.  This has not yet been proven, because it is not, in fact, true.\n\nSee also: [Criticisms of the Rationalist Movement](https://www.lesswrong.com/tag/criticisms-of-the-rationalist-movement), [Value of Rationality](https://www.lesswrong.com/tag/value-of-rationality)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "7w6XkYe5YPx9YL59j",
    "name": "Information Hazards",
    "core": null,
    "slug": "information-hazards",
    "oldSlugs": null,
    "postCount": 37,
    "description": {
      "markdown": "An **Information Hazard** is some true information that could harm people, or other sentient beings, if known. It is tricky to determine policies on information hazards. Some information might genuinely be dangerous, but excessive controls on information has its own perils. \n\n**This tag is for discussing the phenomenon of Information Hazards and what to do with them. Not for actual Information Hazards themselves.**  \n  \nAn example might be a formula for easily creating cold fusion in your garage, which would be very dangerous. Alternatively, it might be an idea that causes great mental harm to people.\n\nBostrom's Typology of Information Hazards\n-----------------------------------------\n\nNick Bostrom coined the term *information hazard* in a 2011 paper \\[1\\] for Review of Contemporary Philosophy. He defines it as follows:\n\n> Information hazard: A risk that arises from the dissemination or the potential dissemination of (true) information that may cause harm or enable some agent to cause harm.\n\nBostrom points out that this is in contrast to the generally accepted principle of information freedom and that, while rare, the possibility of information hazards needs to be considered when making information policies. He proceeds to categorize and define a large number of sub-types of information hazards. For example, he defines artificial intelligence hazard as:\n\n> Artificial intelligence hazard: There could be computer-related risks in which the threat would derive primarily from the cognitive sophistication of the program rather than the specific properties of any actuators to which the system initially has access.\n\nThe following table is reproduced from Bostrom 2011 \\[1\\].\n\n<table style=\"background-color:white\"><tbody><tr><td style=\"text-align:center\" colspan=\"3\"><strong>TYPOLOGY OF INFORMATION HAZARDS</strong></td></tr><tr><td colspan=\"3\">I. By information transfer mode</td></tr><tr><td rowspan=\"6\">&nbsp;</td><td>Data hazard</td><td rowspan=\"6\">&nbsp;</td></tr><tr><td>Idea hazard</td></tr><tr><td>Attention hazard</td></tr><tr><td>Template hazard</td></tr><tr><td>Signaling hazard</td></tr><tr><td>Evocation hazard</td></tr><tr><td colspan=\"3\">II. By effect</td></tr><tr><td>&nbsp;</td><td>TYPE</td><td>SUBTYPE</td></tr><tr><td rowspan=\"4\">ADVERSARIAL RISKS</td><td rowspan=\"4\">Competiveness hazard</td><td>Enemy Hazard</td></tr><tr><td>Intellectual property hazard</td></tr><tr><td>Commitment hazard</td></tr><tr><td>Knowing-too-much hazard</td></tr><tr><td rowspan=\"3\">RISKS TO SOCIAL ORGANIZATION AND MARKETS</td><td rowspan=\"3\">Norm hazard</td><td>Information asymmetry Hazard</td></tr><tr><td>Unveiling hazard</td></tr><tr><td>Recognition hazard</td></tr><tr><td rowspan=\"7\">RISKS OF IRRATIONALITY AND ERROR</td><td>Ideological hazard</td><td rowspan=\"7\">&nbsp;</td></tr><tr><td>Distraction and temptation hazard</td></tr><tr><td>Role model hazard</td></tr><tr><td>Biasing hazard</td></tr><tr><td>De-biasing hazard</td></tr><tr><td>Neuropsychological hazard</td></tr><tr><td>Information-burying hazard</td></tr><tr><td rowspan=\"5\">RISKS TO VALUABLE STATES AND ACTIVITIES</td><td rowspan=\"3\">Psychological reaction hazard</td><td>Disappointment hazard</td></tr><tr><td>Spoiler hazard</td></tr><tr><td>Mindset hazard</td></tr><tr><td>Belief-constituted value hazard</td><td>&nbsp;</td></tr><tr><td>(mixed)</td><td>Embarrassment hazard</td></tr><tr><td rowspan=\"3\">RISKS FROM INFORMATION TECHNOLOGY SYSTEMS</td><td rowspan=\"3\">Information system hazard</td><td>Information infrastructure failure hazard</td></tr><tr><td>Information infrastructure misuse hazard</td></tr><tr><td>Artificial intelligence hazard</td></tr><tr><td>RISKS FROM DEVELOPMENT</td><td>Development hazard</td><td>&nbsp;</td></tr></tbody></table>\n\nSee Also\n--------\n\n*   [Dangerous Knowledge](https://lessestwrong.com/tag/dangerous-knowledge)\n*   [Computation Hazards](https://wiki.lesswrong.com/wiki/Computation_Hazards)\n\nReferences\n----------\n\n1.  Bostrom, N. (2011). \"[Information Hazards: A Typology of Potential Harms from Knowledge](http://www.nickbostrom.com/information-hazards.pdf)\". *Review of Contemporary Philosophy* **10**: 44-79."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "MfpEPj6kJneT9gWT6",
    "name": "Site Meta",
    "core": true,
    "slug": "site-meta",
    "oldSlugs": null,
    "postCount": 541,
    "description": {
      "markdown": "**Site Meta** is the category for discussion about the AI Alignment Forum website. It includes technical updates. It applies to team announcements such as updates, features, events, moderation activity and policy, downtime, requests for feedback, as well as site documentation,  and the team’s writings about site philosophy/strategic thinking.\n\nThe tag also applies to any discussion of the site norms/moderation, feature requests, questions, and ideas about what the site should do – regardless of author."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "jgcAJnksReZRuvgzp",
    "name": "Financial Investing",
    "core": false,
    "slug": "financial-investing",
    "oldSlugs": null,
    "postCount": 119,
    "description": {
      "markdown": "The **Financial Investing** tag covers concrete personal investment advice, specific investment opportunities (like Bitcoin), and analysis of existing financial investing practices, as well as broad analyses of things like the efficient market hypothesis."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "fF9GEdWXKJ3z73TmB",
    "name": "Scholarship & Learning",
    "core": null,
    "slug": "scholarship-and-learning",
    "oldSlugs": null,
    "postCount": 241,
    "description": {
      "markdown": "**Scholarship & Learning.** Here be posts on how to study, research, and learn.\n\nTopics include, but are not limited to: how to research, how to understand material deeply, note-taking, and useful scholarship resources.\n\n> *The eleventh virtue is scholarship. Study many sciences and absorb their power as your own. Each field that you consume makes you larger. If you swallow enough sciences the gaps between them will diminish and your knowledge will become a unified whole. If you are gluttonous you will become vaster than mountains. –* [*Twelve Virtues of Rationality*](https://www.lesswrong.com/posts/7ZqGiPHTpiDMwqMN2/twelve-virtues-of-rationality)\n\nSee Also\n--------\n\n*   [Spaced Repetition](https://www.lesswrong.com/tag/spaced-repetition) is a technique for long-term retention of learned material.\n*   [Fact Posts](https://www.lesswrong.com/tag/fact-posts?showPostCount=true&useTagName=true) are pieces of writing that attempt to build an understanding of the world, starting bottom up with empirical facts rather than \"opinions\".\n*   The other [Virtues](https://www.lesswrong.com/tag/virtues?showPostCount=true&useTagName=true) of Rationality.\n\nTop Resources\n-------------\n\n*   [Scholarship: How to Do It Efficiently](https://www.lesswrong.com/posts/37sHjeisS9uJufi4u/scholarship-how-to-do-it-efficiently) is a guide to quickly researching topics and understanding what is known within a field.\n*   [Literature Review For Academic Outsiders: What, How, and Why](https://www.lesswrong.com/posts/RKz7pc6snBttndxXz/literature-review-for-academic-outsiders-what-how-and-why-1) similar to the first resource, contains many links to further resources.\n*   [\\[Question\\] How do you assess the quality / reliability of a scientific study?](https://www.lesswrong.com/posts/gxbGKa2AnQsrn3Gni/how-do-you-assess-the-quality-reliability-of-a-scientific) A question post with many highly excellent lengthy responses, several which received bounty payouts.\n*   [On learning difficult things](https://www.lesswrong.com/posts/w5F4w8tNZc6LcBKRP/on-learning-difficult-things) covers techniques and methods for studying difficult topics.\n*   [Paper-Reading for Gears](https://www.lesswrong.com/posts/TPjbTXntR54XSZ3F2/paper-reading-for-gears) is a guide studying to actually build up a mechanistic, gears-level understanding of a topic.\n*   [The 3 Books Technique for Learning a New Skilll](https://www.lesswrong.com/posts/oPEWyxJjRo4oKHzMu/the-3-books-technique-for-learning-a-new-skilll) is a short post suggests finding a What, How, and Why book for any skill or topic you wish to learn.\n*   [The Best Textbooks on Every Subject](https://www.lesswrong.com/posts/xg3hXCYQPJkwHyik2/the-best-textbooks-on-every-subject) crowd-sourced list where every recommendation requires that the recommender have read three books on the topic and can explain why one textbook is better than others.\n*   [Forum participation as a research strategy](https://www.lesswrong.com/posts/rBkZvbGDQZhEymReM/forum-participation-as-a-research-strategy) argues that participation on discussion forums on a research topic is actually a great way for researchers to make progress.\n*   [Fact Posts: How and Why](https://www.lesswrong.com/posts/Sdx6A6yLByRRs8iLY/fact-posts-how-and-why) is guide on exploring empirical question by starting with raw facts rather than expert opinion and prior analysis. Compared more typical research, the Fact Post method helps you ground your understanding in facts and see the topic freshly.\n*   [How to (not) do a literature review](https://www.lesswrong.com/posts/tRQek3Xb9cKZ2o6iA/how-to-not-do-a-literature-review) which contains a very concrete list of steps for literature reviews, including mistakes to avoid.\n\n**External Resources**\n\n*   [Internet Search Tips](https://www.gwern.net/Search) by Gwern Branwen is a long, extremely detailed practical guide on how to conduct an online search for references, papers, and books that are difficult to find, including 13 case studies."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "XSryTypw5Hszpa4TS",
    "name": "Consciousness",
    "core": null,
    "slug": "consciousness",
    "oldSlugs": null,
    "postCount": 131,
    "description": {
      "markdown": "The word \"**consciousness**\" is used in a variety of different ways, and there are large disagreements about the reality and nature (and even coherence) of some of the things people profess to mean by \"consciousness.\"\n\nColloquially, the word \"conscious\" is used to pick out a few different things:\n\n*   **Wakefulness** \\- The property that distinguishes, e.g., a person who is awake from a person who is asleep.\n    *   We call people \"unconscious\" in this sense based on observed features like \"sharply reduced mobility,\" though we wouldn't normally call someone unconscious if we think they're merely *paralyzed*. Instead, calling someone \"unconscious\" tends to imply reduced ability to perceive and/or reason about events in one's environment.\n    *   An unconscious person (in this sense) might or might not be dreaming; and if dreaming, they might or might not be lucid.\n*   **Having experiences** \\- The property that distinguishes, e.g., a comatose person who is having experiences from a comatose person who is not having experiences.\n*   **Knowledge, perception,** and/or **attention** \\- E.g., we might say that someone becomes \"conscious of\" a fact when they first learn that fact. Or we might say that they become \"conscious of\" something whenever they're currently perceiving it, or whenever they're *paying attention* to it.\n*   **Meta-cognition** or **reflective awareness** \\- Knowing, perceiving, and/or attending to your own mental states; or knowing, perceiving, and/or attending to *the fact that* you have certain mental states.\n    *   E.g., we might say that someone is less \"conscious\" when they're fully immersed in a novel than when they're thinking about their own experiences, directing attention to the fact that they're reading a book, etc.\n*   **Self-awareness** \\- Knowing, perceiving, and/or attending to your own existence or your own central properties.\n    *   Depending on what exactly is meant by \"self-awareness,\" the \"immersed in a novel\" example might also involve less self-awareness. In some weaker senses of \"self-aware,\" one might instead claim that humans who are experiencing anything are always \"self-aware.\"\n\nReasonably mainstream academic overviews of \"consciousness\" can be found in the [*Stanford Encyclopedia of Philosophy*](https://plato.stanford.edu/entries/consciousness/) and the [*MIT Encyclopedia of the Cognitive Sciences*](http://www.mkdavies.net/Martin_Davies/Mind_files/ConsciousnessMITECS.pdf).\n\nThis tag is *tentatively and provisionally* about the \"**having experiences**\" meaning(s) of \"consciousness.\" For wakefulness and dreaming, see [sleep](https://www.lesswrong.com/tag/sleep). For knowledge, perception, and attention, see [attention](https://www.lesswrong.com/tag/attention) and [cognitive science](https://www.lesswrong.com/tag/cognitive-science). And for reflective awareness and self-awareness, see [identity](https://www.lesswrong.com/tag/identity), [personal identity](https://www.lesswrong.com/tag/personal-identity), and [reflective reasoning](https://www.lesswrong.com/tag/reflective-reasoning).\n\nThis tag's focus is tentative and provisional because it is not altogether clear that \"consciousness in the sense of having experiences\" is a coherent idea, or one that's distinct from the other categories above. This tag is a practical tool for organizing discussion on a family of related topics, and isn't intended as a strong statement \"this is the right way of [carving nature at its joints](https://www.lesswrong.com/posts/d5NyJ2Lf6N22AD9PB/where-to-draw-the-boundary).\"\n\nSuffice to say that (as of December 8, 2020) *enough LessWrongers find consciousness confusing enough*, and disagree enough about what's going on here, for it to make sense to use this page to organize discussion of those disagreements, rather than \"picking a winner\" immediately and running with it.\n\n\"Having experiences\": Practical implications\n--------------------------------------------\n\nBeyond sheer curiosity about how the mind works, there are several sub-questions that have caused thinkers to take a special interest in the question \"what is 'having an experience'?\":\n\n*   1\\. When should I care about something else's welfare?\n    *   1.1. [Animal welfare](https://www.lesswrong.com/tag/animal-welfare): Pain, pleasure, desire, etc. are commonly taken to be *experiences*, and experiences of great moral importance. Knowing which species are capable of \"having experiences,\" then, could matter decisively in assessing the morality of factory farming and the morality of policies affecting wild animals.\n    *   1.2. Machine welfare and [s-risks](https://www.lesswrong.com/tag/risks-of-astronomical-suffering-s-risks): Similarly, knowing which kinds of (actual or potential) software have \"[experiences](https://www.lesswrong.com/posts/wqDRRx9RqwKLzWt7R/nonperson-predicates)\" could tell us a great deal about which programs are morally important.\n*   2\\. When should I think of something as \"me\" (or \"relevantly me-like\")?\n    *   2.1. [Personal identity](https://www.lesswrong.com/tag/personal-identity), [whole brain emulation](https://www.lesswrong.com/tag/whole-brain-emulation), and [simulations](https://www.lesswrong.com/tag/simulation): Normally, people care about their future selves (at least in part) because they anticipate *having those selves' experiences*. Thus, one might say: \"It doesn't make sense for me to sign up for [cryonics](https://www.lesswrong.com/tag/cryonics), because a cryo-revived copy of me wouldn't be *me*.\" (Or, replying to 1.2 above, one might say \"It doesn't make sense for me to sign up for cryonics, because a cryo-revived emulation of me would be a mere automaton with no experiences.\")\n    *   2.2. [Anthropics](https://www.lesswrong.com/tag/anthropics): Anthropic questions turn on how many copies of \"you\" exist, or how many copies of \"observers similar to you\" exist. One could speculate that this is related to the question of what makes a copy of you conscious, and what \"consciousness\" is in the first place.\n*   3\\. Does the existence or nature of subjective experience imply any major updates about the world as a whole, about scientific methodology, etc.?\n    *   3.1. [Reductionism](https://www.lesswrong.com/tag/reductionism), physicalism, and naturalism: Can experience be a mere matter of, uh, matter? If experience turned out to be irreducibly unphysical (and real), this would falsify some of the most well-established generalizations in science.\n\nLessWrong writers have typically been strongly on board with physicalism (3.1), and on board with the idea that an emulation of me is \"me\" (and conscious) in every sense that matters (2.1). Beyond that, however, views vary. (By comparison, ~74% of Anglophone philosophers of mind endorsed \"physicalism\" as opposed to \"non-physicalism\" [in 2009](https://philpapers.org/surveys/results.pl?affil=Target+faculty&areas0=16&areas_max=1&grain=fine).)\n\n\"Having experiences\": Pre-LessWrong discussion\n----------------------------------------------\n\nHow does this \"having experiences\" thing work, then? Well, this wiki page's editors haven't agreed on an answer yet. As a cop-out, we instead provide a list of highlights from the history of other people thinking about this.\n\nFor concreteness, we'll list particular years, authors, and texts, even though this makes some choices of what to highlight more arbitrary. Philosophy also shows up much more than psychology or neuroscience proper, not because philosophy is necessarily the right way to make progress here, but because the philosophy highlights are more \"meta\" and therefore choosing what to include relies less on a LessWrong consensus about consciousness itself.\n\nHighlights:\n\n*   A long time ago BC: Someone comes up with the idea that \"minds\" are a pretty basic and fundamental feature of the world. Maybe gods have minds; maybe trees; maybe rivers; and so on. See also [When Anthropomorphism Became Stupid](https://www.lesswrong.com/s/FqgKAHZAiZn9JAjDo/p/f4RJtHBPvDRJcCTva) and [Mind Projection Fallacy](https://www.lesswrong.com/tag/mind-projection-fallacy).\n*   ~400 BC: Democritus proposes that all human-scale phenomena, including psychological phenomena, are the result of small physical parts bouncing off each other. From [*Encyclopedia Britannica*](https://www.britannica.com/topic/materialism-philosophy/History-of-materialism): \"Democritus thought that the soul consists of smooth, round atoms and that perceptions consist of motions caused in the soul atoms by the atoms in the perceived thing.\"\n*   1641: René Descartes, [*Meditations on First Philosophy*](https://yale.learningu.org/download/041e9642-df02-4eed-a895-70e472df2ca4/H2665_Descartes%27%20Meditations.pdf). Descartes argues that mind and matter must be irreducibly distinct (**mind-body dualism**), because (e.g.) material things are spatially extended, while thoughts are not. Descartes speculates that minds interact with the physical world via a specific part of the brain, the pineal gland.\n    *   Descartes also popularizes the idea that everyone knows their own conscious experiences with certainty: at any given moment, we are infallible about *the fact* that we are having an experience (the \"cogito\"), and we are also infallible about the *contents* of that experience.\n*   1651: Thomas Hobbes, [*Leviathan*](https://www.csus.edu/indiv/s/simpsonl/hist162/hobbes.pdf). Hobbes [insistently](https://plato.stanford.edu/entries/hobbes/#3) asserts that everything (including the mind) is material, and can be thought of as a mechanism or machine.\n*   1714: Gottfried Leibniz. [*The Monadology*](https://plato.stanford.edu/entries/leibniz-mind/)*.* Leibniz argues that mind can't be reduced to matter:\n    *   \"One is obliged to admit that *perception* and what depends upon it is inexplicable on mechanical principles, that is, by figures and motions. In imagining that there is a machine whose construction would enable it to think, to sense, and to have perception, one could conceive it enlarged while retaining the same proportions, so that one could enter into it, just like into a windmill. Supposing this, one should, when visiting within it, find only parts pushing one another, and never anything by which to explain a perception. Thus it is in the simple substance, and not in the composite or in the machine, that one must look for perception.\"\n*   1866: Charles Sanders Peirce, Lowell Lectures. Peirce [introduces](https://colorysemiotica.files.wordpress.com/2014/08/peirce-collectedpapers.pdf) the term \"[***qualia***](https://plato.stanford.edu/entries/qualia/)\" to refer to what it's like to have a specific experience — e.g., the particular experience of redness. *Qualia* is the plural of *quale*, Latin for \"what kind of thing?\" and source of the English word *quality*.\n*   1874: Thomas Huxley, \"[On the Hypothesis that Animals Are Automata, and Its History](http://opessoa.fflch.usp.br/sites/opessoa.fflch.usp.br/files/Huxley-English.pdf).\" Huxley argues for **epiphenomenalism**, the view that consciousness is *caused* by physical processes, but has no effects of its own.\n    *   \"The consciousness of brutes would appear to be related to the mechanism of their body simply as a collateral product of its working, and to be as completely without any power of modifying that working as the steam-whistle which accompanies the work of a locomotive engine is without influence upon its machinery. Their volition, if they have any, is an emotion indicative of physical changes, not a cause of such changes.\" And: \"to the best of my judgment, the argumentation which applies to brutes holds equally good of men\".\n*   1888: Santiago Ramón y Cajal, \"Estructura de los centros nerviosos de las aves.\" Using Camillo Golgi's staining method, Ramón y Cajal discovers that brains are made of neurons.\n*   1903: G.E. Moore, \"[The Refutation of Idealism](https://fewd.univie.ac.at/fileadmin/user_upload/inst_ethik_wiss_dialog/Moore__G._1903._The_refutation_of_Idealism._in_MInd.pdf).\" The early 20th century saw sharp moves away from spiritualism and supernaturalism in intellectual circles, beginning with the \"[Cambridge revolt against idealism](https://en.wikipedia.org/wiki/Bertrand_Russell%27s_philosophical_views#Analytic_philosophy).\" Mysticism and metaphysical proclamations about the mind became increasingly unfashionable, as intellectuals grew more skeptical and more inclined to demand testable operationalizations of claims. Extreme manifestations of this attitude included logical positivism in the 1930s-1950s and behaviorism in the 1920s-1960s.\n*   1943: McCulloch and Pitts, \"[A Logical Calculus of the Ideas Immanent in Nervous Activity](https://www.cs.cmu.edu/~./epxing/Class/10715/reading/McCulloch.and.Pitts.pdf).\" [*SEP*](https://plato.stanford.edu/entries/computational-mind/)  writes that this paper \"first suggested that something resembling the Turing machine might provide a good model for the mind.\" Subsequent developments in this direction include the cognitive revolution and the rise of [**functionalist**](https://plato.stanford.edu/entries/functionalism/) and [**computational**](https://plato.stanford.edu/entries/computational-mind/) accounts of the mind, which supplanted behaviorism.\n*   1968: David Armstrong, *A Materialist Theory of the Mind*. An early attempt to sketch a theory of consciousness (specifically, a [higher-order](https://plato.stanford.edu/entries/consciousness-higher/) theory). For an overview of popular theories or sketches-of-theories in the following decades, see *SEP*'s review article \"[The Neuroscience of Consciousness](https://plato.stanford.edu/entries/consciousness-neuroscience/).\"\n*   1974: Thomas Nagel, \"[What Is It Like To Be A Bat?](http://www.esalq.usp.br/lepse/imgs/conteudo_thumb/What-Is-It-Like-to-Be-a-Bat-1.pdf)\" Nagel writes that \"fundamentally an organism has conscious mental states if and only if there is something that it is like to *be* that organism—something it is like for the organism.\" And:\n    *   \"If physicalism is to be defended, the phenomenological features \\[i.e., what it's like to have certain experiences\\] must themselves be given a physical account. But when we examine their subjective character it seems that such a result is impossible. The reason is that every subjective phenomenon is essentially connected with a single point of view, and it seems inevitable that an objective, physical theory will abandon that point of view.\"\n    *   Subsequent authors have tended to use terms like \"**what it's like**,\" \"**phenomenal consciousness**\" (derived from *phenomena* in the sense of \"appearances\"), and *qualia* to gesture at this apparent puzzle. These are closely related terms, used in slightly different ways by different authors.\n*   1974: Robert Kirk, \"[Zombies v. Materialists](https://academic.oup.com/aristoteliansupp/article-abstract/48/1/135/1779753?redirectedFrom=fulltext).\" This paper introduces the **philosophical zombie**, or **p-zombie**: a hypothetical being that is physically identical to a conscious person, but lacks consciousness. If the idea of p-zombies has no hidden logical inconsistencies, it is argued, then consciousness is not logically entailed by organisms' physical properties, which would make physicalism false.\n*   1982: Frank Jackson, \"[Epiphenomenal Qualia](https://www.sfu.ca/~jillmc/JacksonfromJStore.pdf).\" Jackson argues that we can imagine a scientist, Mary, who knows all the physical facts about color but has never seen the color red for herself. If she then sees red, it seems as though she learns a new fact—she learns what it's like to experience redness. Jackson takes this to mean that there are further facts beyond the physical facts, and that physicalism is therefore false. (For subsequent discussion, see *SEP*'s \"[Qualia: The Knowledge Argument](https://plato.stanford.edu/entries/qualia-knowledge/).\")\n*   1996: David Chalmers, [*The Conscious Mind: In Search of a Fundamental Theory*](http://consc.net/books/tcm/intro.html). Chalmers argues against physicalism, leaning heavily on the zombie argument and the Mary argument.\n    *   Chalmers speaks of the \"**hard problem of consciousness**,\" the problem of explaining why we are phenomenally conscious (i.e., why we aren't p-zombies). \"Many books and articles on consciousness have appeared in the last few years, and one might think that we are making progress. But on a closer look, most of this work leaves the hardest problems about consciousness untouched. Often, this work addresses what might be called the 'easy' problems of consciousness: How does the brain process environmental stimulation? How does it integrate information? How do we produce reports on internal states? These are important questions, but to answer them is not to solve the hard problem: why is all this processing accompanied by an experienced inner life?\"\n    *   While Chalmers discussed consciousness earlier (e.g., in [1993](http://consc.net/papers/qualia.html), [1994](http://consc.net/papers/facing.pdf), and [1996](http://consc.net/papers/moving.html)), *The Conscious Mind* is the work that brought dualistic and quasi-dualistic views back into the intellectual almost-mainstream for the first time in a century. In spite of its crazy-sounding conclusions, the book is unusually clear, rigorous, and thorough, anticipating almost all of the obvious objections; and Chalmers attempts to make the irreducibility of consciousness more palatable to scientists by endorsing what he calls \"naturalistic dualism\": the view that consciousness is lawful, predictable, and not specific to humans. Chalmers argues that our consciousness depends on stable (but contingent) \"psychophysical laws\" that would also (for example) make a whole-brain emulation conscious.\n*   2003\\. Max Tegmark, \"[Parallel Universes](https://space.mit.edu/home/tegmark/multiverse.pdf).\" Although not explicitly concerned with consciousness, Tegmark's picture raises problems for [anthropics](https://www.lesswrong.com/tag/anthropics) and our understanding of what makes an observer \"real.\"\n\n\"Having experiences\": Recent discussion\n---------------------------------------\n\n*   2008\\. Eliezer Yudkowsky, \"[Zombies! Zombies?](https://www.lesswrong.com/posts/fdEWWr8St59bXLbQr/zombies-zombies)\" This and other posts from [Physicalism 201](https://www.lesswrong.com/s/FqgKAHZAiZn9JAjDo) argue that we can be confident physicalism is true, even without knowing how to solve (or [dissolve](https://www.lesswrong.com/posts/Mc6QcrsbH5NRXbCRX/dissolving-the-question)) the \"hard problem of consciousness\".\n    *   In particular, Yudkowsky argues that accepting the possibility of p-zombies is tantamount to accepting epiphenomenalism, and that epiphenomenalism is crazy. If our claims about consciousness are *true* even though consciousness has no causal effect on what we claim (because a p-zombie would move its lips and pen exactly as we do), then our claims would have to be true *by coincidence*, which is absurd given the Bayesian understanding of evidence and knowledge.\n    *   More generally, LessWrong writers' views on consciousness have been heavily influenced by the intuition pumps and reasoning rules Yudkowsky writes about in the Sequences (2006–2009), such as: [Making Beliefs Pay Rent](https://www.lesswrong.com/posts/a7n8GdKiAZRX86T5A/making-beliefs-pay-rent-in-anticipated-experiences); [Making History Available](https://www.lesswrong.com/posts/TLKPj4GDXetZuPDH5/making-history-available); [How Much Evidence Does It Take?](https://www.lesswrong.com/posts/nj8JKFoLSMEmD3RGp/how-much-evidence-does-it-take); [The Second Law of Thermodynamics and Engines of Cognition](https://www.lesswrong.com/posts/QkX2bAkwG2EpGvNug/the-second-law-of-thermodynamics-and-engines-of-cognition); and [My Kind of Reflection](https://www.lesswrong.com/s/9bvAELWc8y2gYjRav/p/TynBiYt6zg42StRbb).\n*   2008\\. Eliezer Yudkowsky, \"[Collapse Postulates](https://www.lesswrong.com/posts/xsZnufn3cQw7tJeQ3/collapse-postulates).\" This and other posts from the [Quantum Physics Sequence](https://www.lesswrong.com/posts/hc9Eg6erp6hk9bWhn/the-quantum-physics-sequence) argue that physicists' belief that observers or consciousness play a privileged role in quantum phenomena is based on a series of confusions and misunderstandings.\n*   2013\\. David Chalmers, \"[Panpsychism and Panprotopsychism](http://consc.net/papers/panpsychism.pdf).\" Chalmers argues that everything in the universe (down to the subatomic level) is \"conscious\" or \"proto-conscious.\"\n*   2014\\. Benya Fallenstein. \"[L-zombies! L-zombies?](https://www.lesswrong.com/posts/7nAxgQYGYrEY5ZCAD/l-zombies-l-zombies)\" Fallenstein asks how we can distinguish between instantiated observers and uninstantiated (\"merely logical\") observers.\n*   2016: Keith Frankish. \"[Illusionism as a Theory of Consciousness](https://nbviewer.jupyter.org/github/k0711/kf_articles/blob/master/Frankish_Illusionism%20as%20a%20theory%20of%20consciousness_eprint.pdf).\" Frankish argues that \"experiences do not really have qualitative, 'what-it’s-like' properties.\" Instead, subjective experience seems \"unphysical\" or \"irreducible\" because of a sort of introspective illusion.\n*   2017: Luke Muehlhauser, \"[2017 Report on Consciousness and Moral Patienthood](https://www.openphilanthropy.org/2017-report-consciousness-and-moral-patienthood).\" The single largest work of scholarship on consciousness by the rationality community.\n*   2018: David Chalmers, \"[The Meta-Problem of Consciousness](https://philpapers.org/archive/chatmo-32.pdf).\" Chalmers discusses \"the problem of explaining why we think consciousness poses a hard problem\".\n\nRelated pages\n-------------\n\n*   Non-tags: [Anthropomorphism](https://www.lesswrong.com/tag/anthropomorphism), [How an algorithm feels](https://www.lesswrong.com/tag/how-an-algorithm-feels), [Zombies](https://www.lesswrong.com/tag/zombies)\n*   [Identity](https://www.lesswrong.com/tag/identity), [Personal identity](https://www.lesswrong.com/tag/personal-identity), [Reflective reasoning](https://www.lesswrong.com/tag/reflective-reasoning)\n*   [Animal welfare](https://www.lesswrong.com/tag/animal-welfare), [Suffering](https://www.lesswrong.com/tag/suffering), [Risks of astronomical suffering (s-risks)](https://www.lesswrong.com/tag/risks-of-astronomical-suffering-s-risks)\n*   [Reductionism](https://www.lesswrong.com/tag/reductionism), [Mind Projection Fallacy](https://www.lesswrong.com/tag/mind-projection-fallacy)\n*   [Quantum mechanics](https://www.lesswrong.com/tag/quantum-mechanics)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "fpEBgFE7fgpxTm9BF",
    "name": "Machine Learning  (ML)",
    "core": null,
    "slug": "machine-learning-ml",
    "oldSlugs": [
      "machine-learning"
    ],
    "postCount": 222,
    "description": {
      "markdown": "**Machine Learning** refers to the general field of study that deals with automated statistical learning and pattern detection by non-biological systems. It can be seen as a sub-domain of artificial intelligence that specifically deals with modeling and prediction through the knowledge extracted from training data. As a multi-disciplinary area, it has borrowed concepts and ideas from other areas like pure mathematics and cognitive science.\n\nUnderstanding different machine learning algorithms\n---------------------------------------------------\n\nThe most widely used distinction is between unsupervised (e.g. k-means clustering, principal component analysis) vs supervised (e.g. Support Vector Machines, logistic regression) methods. The first approach identifies interesting patterns (e.g. clusters and latent dimensions) in unlabeled training data, whereas the second takes labeled training data and tries to predict the label for unlabeled data points from the same distribution.\n\nAnother important distinction relates to the bias/variance tradeoff -- some machine learning methods are are capable of recognizing more complex patterns, but the tradeoff is that these methods can overfit and generalize poorly if there's noise in the training data -- especially if there's not much training data available.\n\nThere are also subfields of machine learning devoted to operating on specific kinds of data. For example, Hidden Markov Models and recurrent neural networks operate on time series data. Convolutional neural networks are commonly applied to image data.\n\nApplications\n------------\n\nThe use of machine learning has been widespread since its formal definition in the 50’s. The ability to make predictions based on data has been extensively used in areas such as analysis of financial markets, natural language processing and even brain-computer interfaces. Amazon’s product suggestion system makes use of training data in the form of past customer purchases in order to predict what customers might want to buy in the future.\n\nIn addition to its practical usefulness, machine learning has also offered insight into human cognitive organization. It seems likely machine learning will play an important role in the development of [artificial general intelligence](https://www.lesswrong.com/tag/artificial-general-intelligence).\n\nFurther Reading & References\n----------------------------\n\n*   [The Elements of Statistical Learning](https://web.stanford.edu/~hastie/ElemStatLearn/)\n\nSee Also\n--------\n\n*   [Prediction](https://www.lesswrong.com/tag/forecasting-and-prediction)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "bh7uxTTqmsQ8jZJdB",
    "name": "Probability & Statistics",
    "core": false,
    "slug": "probability-and-statistics",
    "oldSlugs": [
      "probability-and-statistics"
    ],
    "postCount": 222,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "2wjPMY34by2gXEXA2",
    "name": "Techniques",
    "core": null,
    "slug": "techniques",
    "oldSlugs": null,
    "postCount": 93,
    "description": {
      "markdown": "A **technique** or **rationality technique** is a set of actions (including \"mental actions\") for improving one's thinking so as to form accurate beliefs and/or make better decisions. Ideally, techniques are refined to the point that they can be taught and trained."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "iP2X4jQNHMWHRNPne",
    "name": "Motivations",
    "core": false,
    "slug": "motivations",
    "oldSlugs": null,
    "postCount": 153,
    "description": {
      "markdown": "**Motivations** are the reasons why we think and do the things that we do. Related: **Desire, Values**. Many questions can asked about motivation such as: i) what does/could/should motivate people? ii) which stated motivations are true motivations for belief and behavior? iii) which motivations are *valid* vs *invalid*? iv) How does motivation even work? \n\n*Note: This tag is a work in progress*\n\nSee also:\n\n*   [Inspirational](www.lesswrong.com/stub)\n*   CEV\n*   Utility Functions\n*   Elephant in the Brain by Simler and Hanson\n*   Signaling\n*   Multi-Agent Theories of Mind\n*   **Rationalization** is the act of finding reasons to support a desired conclusion rather than reasoning in ways which reach the true conclusion.\n*   **Motivated Cognition** is when one's thinking does not purely follow processes for generating truth, and are instead influenced by desires/motivation to reach certain conclusion.\n\nMotivation and Belief\n---------------------\n\nIn the context of belief, a valid motivation for believing something might be having encountered Bayesian evidence for it; in contrast, simply wishing something were true is a poor motivation for believing and often results *motivated reasoning \\[link need\\].*\n\nThe Litanies of Gendlin and Tarsky \\[links\\] are often invoked to elicit feels which motivate truth-seeking behaviors.\n\nMotivated Cognition, Confirmation Bias, Rationalization\n-------------------------------------------------------\n\n...\n\nStated vs Actual Motivation\n---------------------------\n\nIt is no secret that often the reasons people give for their actions and beliefs are probably not the real ones driving their behavior. Is that your real objection? The work of Hanson....Signaling...\n\n*   Act of Charity\n*   Player vs Character\n\nThe Cognitive Science of Motivation\n-----------------------------------\n\nWhile most people can recognize the feeling of motivation, it is a much more complication question on how agents, particularly humans, implement *motivation.*\n\nIn 20xx, Lukeprog wrote <Neuroscience Review>. Lengthy and thorough. Unknown uptodateness.\n\nRelated to the question of Motivation is subagents. Is one's overall self actually made up of subagents each with their own desires. Kaj Sotala explores this in his Multiagent Theories of Mind Sequences. CFAR techniques: Internal Double Crux are aimed harmonizing between the desires/motivations of different \"parts\" of oneself.\n\nAligning Motivations\n--------------------\n\n*   Overcoming akrasia...\n\nPractical Techniques for Motivation\n-----------------------------------\n\n*   Propagating Urges\n*   Mental Contrasting (external)\n*   Propagating Urges\n*   Internal Double Crux\n\nHabitual Productivity and Nate's Writing\n\nSomething to Protect\n\nSee also Motivated Reasoning"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "YSyvvi4uXvxAARX2D",
    "name": "Slack",
    "core": null,
    "slug": "slack",
    "oldSlugs": null,
    "postCount": 34,
    "description": {
      "markdown": "**Slack** is absence of binding constraints on behavior. The term is usually capitalized to distinguish it from the ordinary English meaning. Not to be confused with the communication app by the same name.\n\nFrom the post which introduced this usage, [Slack](https://www.lessestwrong.com/posts/yLLkWMDbC9ZNKbjDG/slack)**:**\n\n> *Poor is the person without Slack. Lack of Slack compounds and traps.*\n> \n> *Slack means margin for error. You can relax.*\n> \n> *Slack allows pursuing opportunities. You can explore. You can trade.*\n> \n> *Slack prevents desperation. You can avoid bad trades and wait for better spots. You can be efficient.*\n> \n> *Slack permits planning for the long term. You can invest.*\n> \n> *Slack enables doing things for your own amusement. You can play games. You can have fun.*\n> \n> *Slack enables doing the right thing. Stand by your friends. Reward the worthy. Punish the wicked. You can have a code.*\n> \n> *Slack presents things as they are without concern for how things look or what others think. You can be honest.*\n> \n> *You can do some of these things, and choose not to do others. Because you don’t have to.*\n> \n> *Only with slack can one be a righteous dude.*\n> \n> *Slack is life.*\n\n**Related Sequence:** [Slack and the Sabbath](https://www.lesswrong.com/s/HXkpm9b8o964jbQ89)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "AiNyf5iwbpc7mehiX",
    "name": "Meditation",
    "core": null,
    "slug": "meditation",
    "oldSlugs": null,
    "postCount": 79,
    "description": {
      "markdown": "This page is for the **meditation practice**, as in, Vipassana meditation. For meditation in the sense of Koan, see [*Meditation / Koan*](/tag/meditation-koan)*.*"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "Q55STnFh6gbSezRuR",
    "name": "Parenting",
    "core": false,
    "slug": "parenting",
    "oldSlugs": null,
    "postCount": 108,
    "description": {
      "markdown": "**Parenting**, i.e. how to raise children well.\n\n**Related pages:** [Education](https://www.lesswrong.com/tag/education), [Developmental Psychology](https://www.lesswrong.com/tag/developmental-psychology), [Santa Claus](https://www.lesswrong.com/tag/santa-claus), [family planning](https://www.lesswrong.com/tag/family-planning)\n\n**External links:** [Baby sign language](https://en.wikipedia.org/wiki/Baby_sign_language)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "AADZcNS24mmSfPp2w",
    "name": "Communication Cultures",
    "core": null,
    "slug": "communication-cultures",
    "oldSlugs": null,
    "postCount": 75,
    "description": {
      "markdown": "A **Communication Culture** is a set of norms, expectations, and assumptions that a group of people adopts around communication. It is probable that some Communication Cultures are objectively better than others, but is definite that difficult clashes occur when people operating under different cultures interact.\n\nAwareness of Communication Cultures is therefore key to getting along with others not perfectly sharing our background and preferences.\n\nNotable Communication Cultures (these are usually contrasted along some dimension) are: [Ask vs Guess](https://www.lessestwrong.com/posts/vs3kzjLhbdKsndnBy/ask-and-guess) (and [Tell](https://www.lessestwrong.com/posts/rEBXN3x6kXgD4pLxs/tell-culture)/[Reveal](https://malcolmocean.com/2015/06/reveal-culture/)); [Wait vs Interrupt](https://www.lessestwrong.com/posts/LuXb6CZG4x7pDRBP8/wait-vs-interrupt-culture); and [Combat vs Nurture](https://www.lessestwrong.com/posts/ExssKjAaXEEYcnzPd/conversational-cultures-combat-vs-nurture-v2).\n\nSee also: [Simulacrum Levels](https://www.lesswrong.com/tag/simulacrum-levels)\n\n*Tag Status: C-Class*"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "NzSTgAtKwgivkfeYm",
    "name": "Heroic Responsibility",
    "core": false,
    "slug": "heroic-responsibility",
    "oldSlugs": null,
    "postCount": 27,
    "description": {
      "markdown": "**Heroic responsibility** is the responsibility to get the job done no matter what, including not shifting any responsibility for its completion on to others.\n\n*\"You could call it **heroic responsibility**, maybe,” Harry Potter said. “Not like the usual sort. It means that whatever happens, no matter what, it’s always your fault. Even if you tell Professor McGonagall, she’s not responsible for what happens, you are. Following the school rules isn’t an excuse, someone else being in charge isn’t an excuse, even trying your best isn’t an excuse. There just aren’t any excuses, you’ve got to get the job done no matter what.” Harry’s face tightened. “That’s why I say you’re not thinking responsibly, Hermione. Thinking that your job is done when you tell Professor McGonagall—that isn’t heroine thinking. Like Hannah being beat up is okay then, because it isn’t your fault anymore. Being a heroine means your job isn’t finished until you’ve done whatever it takes to protect the other girls, permanently.” In Harry’s voice was a touch of the steel he had acquired since the day Fawkes had been on his shoulder. “You can’t think as if just following the rules means you’ve done your duty. –*[*HPMOR*](http://hpmor.com/chapter/75)*, chapter 75.*  \n \n\nExternal Links\n--------------\n\n*   The discussion at this [Reddit post](http://www.reddit.com/r/HPMOR/comments/yj2kb/ethical_solipsism_chapter_75/) is excellent. *This wiki requires work.*"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "vg4LDxjdwHLotCm8w",
    "name": "Replication Crisis",
    "core": null,
    "slug": "replication-crisis",
    "oldSlugs": [
      "replicability"
    ],
    "postCount": 54,
    "description": {
      "markdown": "The **Replication Crisis** was the discovery that many fields of so-called science were producing experimental results that could not be replicated, because they were illusions resulting from bad statistical and experimental practices.\n\nThe replication crisis began in the early 2010s when several high-profile irreproducible results inspired mass replication attempts, revealing that the majority of papers checked in psychology and a number of other fields were not replicable. Some of the irreproducible results, like [Priming](https://www.lesswrong.com/tag/priming), appeared to bear on rationality and were referenced in early LessWrong posts.\n\n**External Links:**  \n[Retraction Watch](https://retractionwatch.com/)  \n[Replication](https://www.gwern.net/Replication) on gwern.net  \n[Wikipedia](https://en.wikipedia.org/wiki/Replication_crisis)\n\n**Related Pages:** [Practice & Philosophy of Science](https://www.lesswrong.com/tag/practice-and-philosophy-of-science), [Psychology](https://www.lesswrong.com/tag/psychology), [Information Cascades](https://www.lesswrong.com/tag/information-cascades), [Falsifiability](https://www.lesswrong.com/tag/falsifiability)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "XYHzLjwYiqpeqaf4c",
    "name": "Dark Arts",
    "core": null,
    "slug": "dark-arts",
    "oldSlugs": null,
    "postCount": 44,
    "description": {
      "markdown": "**Dark Arts** is a colloquial term for techniques or methods which involve deception and/or manipulation of others or oneself into believing things for non-truth-seeking reasons. These techniques may prey on human cognitive biases.\n\nSome use the term to refer more narrowly to techniques that work equally well to compel both true and false beliefs, i.e., they are [symmetric weapons](https://www.lesswrong.com/posts/qajfiXo5qRThZQG7s/guided-by-the-beauty-of-our-weapons). Some focus more on the Dark Arts as applied to oneself (self-deception) vs applied to manipulating others.\n\nAn example from the [Dark Arts of Rationality](https://www.lesswrong.com/posts/4DBBQkEQvNEWafkek/dark-arts-of-rationality):\n\n> Today, we're going to talk about Dark rationalist techniques: productivity tools which seem incoherent, mad, and downright irrational. These techniques include:\n> \n> 1.  Willful Inconsistency\n> 2.  Intentional Compartmentalization\n> 3.  Modifying Terminal Goals\n\nArt vs. Technology\n------------------\n\nSometimes these arts are further augmented by the use of **persuasion technology**, such as broadcast advertising or PowerPoint slides. Persuasion technology may prevent the person who is being targeted from carefully deliberating on the intended message, or thinking up an effective response to it in real time.\n\nSuch effects can be caused by something as benign as the use of a specialist vocabulary which the target is unfamiliar with, or an institutional vocabulary with high-status connotations: this is one reason why many specialist professions employ ethical codes to regulate their unbalanced power relationship with customers.\n\nThe use of such techniques as whiteboards or PowerPoint slides brings additional concerns, since these tend to connote a single party as the one \"in charge\" of the presentation: this makes it even more difficult for the intended audience to raise any effective objection, and encourages them to focus their attention on the content of the whiteboard or slides. Said content is often presented as a list of abrupt \"bullet points\", further connoting it as factual, objective and neutral. One outspoken critic of PowerPoint, management professor David R. Beatty, states: \"It is like a disease. It's the AIDS of management.\" Beatty further states that Powerpoint \"removes subtlety and thinking\".\n\nMany futurists expect that a technological singularity of even a very mild character will lead to an explosion in the use of radically effective persuasive technology, or \"cognotechnology\"--a term coined by American military researchers at the Lawrence Livermore Laboratories. The collection and distribution of information about people may spiral out beyond any feasible control, perhaps even comprising their inner thought processes; cognitive monitoring may range from non-intrusive body monitoring as seen in a [polygraph](https://wiki.lesswrong.com/wiki/polygraph) to outright [brain emulation](https://wiki.lesswrong.com/wiki/brain_emulation). In this scenario, persuasion technology may easily blend over into outright mind control. This is clearly a rather paranoiac and dystopian scenario; nevertheless, the fact that it is being seriously discussed has persuasive potential in itself, such as for directing funding for research into guaranteed [Friendly AI](https://wiki.lesswrong.com/wiki/Friendly_AI), as opposed to naïvely pursuing expanded funding for neuroscience or artificial intelligence.\n\nNotable Posts\n-------------\n\n*   [Against Propaganda](http://robinhanson.typepad.com/overcomingbias/2009/02/against-propaganda-.html) by [Robin Hanson](https://lessestwrong.com/tag/robin-hanson)\n*   [Lying With Style](http://www.overcomingbias.com/2009/03/deceptive-writing-styles.html) by [Robin Hanson](https://lessestwrong.com/tag/robin-hanson)\n*   [Defense Against The Dark Arts: Case Study #1](https://lessestwrong.com/lw/62/defense_against_the_dark_arts_case_study_1/) by [Yvain](https://wiki.lesswrong.com/wiki/Yvain)\n*   [Informers and Persuaders](https://lessestwrong.com/lw/yg/informers_and_persuaders/) by [Eliezer_Yudkowsky](https://lessestwrong.com/tag/eliezer-yudkowsky)\n*   [The Dark Arts - Preamble](https://lessestwrong.com/lw/2v2/the_dark_arts_preamble) by [Aurini](http://www.staresattheworld.com/)\n*   [The Dark Arts: A Beginner's Guide](https://lessestwrong.com/lw/9iw/the_dark_arts_a_beginners_guide/) by [faul_sname](http://lesswrong.com/user/faul_sname/overview/)\n\nOther Links\n-----------\n\n*   [Meta-commentary on this terminology](https://lessestwrong.com/lw/b1/persuasiveness_vs_soundness/789)\n\nSee Also\n--------\n\n*   [Anti-epistemology](https://lessestwrong.com/tag/anti-epistemology)\n*   [Curiosity stopper](https://wiki.lesswrong.com/wiki/Curiosity_stopper)\n*   [Mind-killer](https://lessestwrong.com/tag/mind-killer)\n*   [Not technically a lie](https://lessestwrong.com/tag/not-technically-a-lie)\n*   [Inferential distance](https://lessestwrong.com/tag/inferential-distance)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "YrLoz567b553YouZ2",
    "name": "Willpower",
    "core": null,
    "slug": "willpower",
    "oldSlugs": null,
    "postCount": 30,
    "description": {
      "markdown": "**Willpower** is the ability to overcome urges to do or not some activity– to overcome temptation. Typically there is a sense of coercing oneself to do things despite inner resistance.   \n  \nWillpower is of interest those who wish to increase their productivity or otherwise do more thing that they wish to be done. The question then is \"how does one increase willpower?\" \n\nThere is an argument that the use of willpower is undesirable. The use of willpower my constitute a form of _inner violence_ which is in tension with _inner_ _alignment_ of [one's parts](https://www.lessestwrong.com/tag/subagents)– a better path to productivity and wellbeing.  \n  \n**Related:** [Akrasia](https://www.lessestwrong.com/tag/akrasia)\n\nResources\n---------\n\n*   The writings on [Minding Our Way](http://mindingourway.com/) concerning productivity."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "r7qAjcbfhj2256EHH",
    "name": "Akrasia",
    "core": null,
    "slug": "akrasia",
    "oldSlugs": null,
    "postCount": 74,
    "description": {
      "markdown": "**Akrasia** is the state of acting against one's better judgment. A canonical example is procrastination.\n\nIncreasing [willpower](https://www.lessestwrong.com/tag/willpower) is seen by some as a solution to akrasia. On the other hand, many favor using tools such as [Internal Double Crux](https://www.lesswrong.com/tag/internal-double-crux?useTagName=true) to resolve internal mental conflicts until one *wants* to do the perform the reflectively endorsed task. The \"resolve internal conflicts\" approach is often related to viewing the mind in terms of [parts that disagree](https://www.lesswrong.com/tag/subagents) with each other.\n\nSee also\n--------\n\n*   [Attention](https://www.lesswrong.com/tag/attention)\n*   [Motivations](https://www.lesswrong.com/tag/motivations)\n*   [Prioritization](https://www.lesswrong.com/tag/prioritization)\n*   [Procrastination](https://www.lesswrong.com/tag/procrastination)\n*   [Productivity](https://www.lesswrong.com/tag/productivity)\n*   [Willpower](https://www.lesswrong.com/tag/willpower)\n*   [Adaptation executers](https://wiki.lesswrong.com/wiki/Adaptation_executers)\n*   [Trivial inconvenience](https://www.lesswrong.com/tag/trivial-inconvenience)\n*   [Aversion/Ugh field](https://www.lesswrong.com/tag/aversion-ugh-fields)\n*   [Compartmentalization](https://www.lesswrong.com/tag/compartmentalization)\n*   [Preference](https://www.lesswrong.com/tag/preference)\n*   [Pica](https://www.lesswrong.com/tag/pica)\n\nExternal links\n--------------\n\n*   [Akrasia](http://psychology.wikia.com/wiki/Akrasia) at Psychology Wiki\n*   [Weakness of Will](http://plato.stanford.edu/entries/weakness-will/), Stanford Encyclopedia of Philosophy\n*   [Beeminder](http://beeminder.com/), community-member developed tool for commitment via self-imposed financial penalties"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "dJ6eJxJrCEget7Wb6",
    "name": "Fallacies",
    "core": null,
    "slug": "fallacies",
    "oldSlugs": [
      "logical-fallacies"
    ],
    "postCount": 66,
    "description": {
      "markdown": "A **fallacy** is generally considered to be an error in reasoning. It refers both to the failure to apply logic to a line of thought, and to the use of problematic arguments. The term can be applied when dealing both with informal and formal logic, although it usual refers to the former.\n\n*Related:* [Disagreement](http://lesswrong.com/tag/disagreement), [Heuristics & Biases](https://www.lesswrong.com/tag/heuristics-and-biases)\n\nInformal vs Formal Fallacy\n--------------------------\n\nAn ***informal fallacy*** refers to a flawed argument, where the premises do not support the conclusion. It can, however, have a valid logical format. This type of fallacy is commonly divided in two main groups: *material fallacies* and *verbal fallacies*.\n\nMaterial fallacies, concerned with the content of the argument, can be divivided following [Aristotle](http://en.wikipedia.org/wiki/Aristotle)'s taxonomy from his work Organon. One such example is the famous Straw Man fallacy:\n\n1.  Person A has position X: We should focus our efforts on [Friendly AI](https://wiki.lesswrong.com/wiki/Friendly_AI) research.\n2.  Person B distorts position X to something close, but different, Y: So you think we should just give up on webdesign?!\n3.  Person B attacks position Y: That's stupid, websites are such a great way of spreading information!\n\nVerbal fallacies, on the other hand, deal with the way the words are used. These include examples such as Equivocation - using words ambiguously or with double meanings - and Proof by Verbosity, where one overwhelms his listener with lots of material in an complicated way.\n\nA ***formal fallacy***, contrasting with informal fallacies, refers to a pattern of reasoning which is wrong due to a flaw in the logical structure of the argument. As such, this deductive fallacy does not imply any information about the premises or the conclusion - its their connection that's wrongly stated. Both can be correct and the argument can be wrong because the conclusion doesn't follow from the premises as it is said to.\n\nFalse Fallacies & Awareness\n---------------------------\n\nMatters can be further complicated by arguing parties incorrectly claiming that an assertion is false due to a fallacy. For example, if one party was to declare “Albert Einstein has claimed that time and space are relative qualities of the Universe.”, another party might responded by saying that this is an ‘’’argument from authority’’’. However, Albert Einstein’s claims are based on detailed mathematical models that identify him as an expert in this field of inquiry, rather than a casual observer. We are thus facing a kind of meta-fallacy which is wrong by itself.\n\nRecognizing fallacies in everyday arguments can be difficult due to complicated patterns of communication that mask the logical connections between statements. At the same time, informal fallacies can also take advantage of the emotional or psychological weaknesses of the listener. It is thus important to develop the ability to recognize them in arguments, so as to reduce the likelihood of being tricked or cheated. This ability becomes even more important when dealing with today's mass media, where the intention is to influence behavior and change beliefs, from political campaigns to simple local newspapers.\n\nFurther Reading & References\n----------------------------\n\n*   Aristotle's [On Sophistical Refutations](http://etext.library.adelaide.edu.au/a/aristotle/sophistical/)\n*   Damer, T. Edward (2008). Attacking Faulty Reasoning: A Practical Guide to Fallacy-free Arguments (6 ed.). Cengage Learning. pp. 130. ISBN 978-0-495-09506-4.\n*   John Woods (2004). The death of argument: fallacies in agent based reasoning. Springer. ISBN 978-1-4020-2663-8.\n*   [Logical Fallacies](http://www.iep.utm.edu/fallacy/) A peer-reviewed academic resource.\n*   [Infinite regression](http://en.wikipedia.org/wiki/Infinite_regress) Wikipedia entry\n\nSee Also\n--------\n\n*   [Aumann's agreement theorem](https://lessestwrong.com/tag/aumann-s-agreement-theorem)\n*   [Disagreement](https://lessestwrong.com/tag/disagreement)\n*   [Information cascade](https://lessestwrong.com/tag/information-cascades)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "pGqRLe9bFDX2G2kXY",
    "name": "Futurism",
    "core": null,
    "slug": "futurism",
    "oldSlugs": null,
    "postCount": 81,
    "description": {
      "markdown": "**Futurism** is speculation about technologies or social trends that might exist in the near or distant future.\n\nLess Wrong's favorite type of futurism is speculation about [AI risk](https://www.lesswrong.com/tag/ai-risk). Other speculative future technologies include [life extension](http://lesswrong.com/tag/life-extension), [mind uploading](http://lesswrong.com/tag/mind-uploading), [nanotechnology](http://lesswrong.com/tag/nanotechnology), and [space colonization](https://www.lesswrong.com/tag/space-exploration-and-colonization).\n\nFor efforts to predict future trends see [Forecasting & Prediction](https://www.lesswrong.com/tag/forecasting-and-prediction) and [Forecasts (Lists of)](https://www.lesswrong.com/tag/forecasts-lists-of).\n\nSee also: [Transhumanism](http://lesswrong.com/tag/transhumanism), [Fun Theory](http://lesswrong.com/tag/fun-theory)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "TLrqSmzoGoA3v5tNP",
    "name": "Fact posts",
    "core": false,
    "slug": "fact-posts",
    "oldSlugs": null,
    "postCount": 27,
    "description": {
      "markdown": "A **Fact post** is a piece of writing that attempts to build an understanding of the world, starting bottom up with empirical facts rather than \"opinions\".  Under this tag, one can find posts that present lots of basic facts about topics. *This entry requires work.*\n\nFact posts were introduced in [**Fact Posts: How and Why**](https://www.lesswrong.com/posts/Sdx6A6yLByRRs8iLY/fact-posts-how-and-why) by sarahconstantin:\n\n> The most useful thinking skill I've taught myself, which I think should be more widely practiced, is writing what I call \"fact posts.\"  I write a bunch of these on my [blog](https://srconstantin.wordpress.com/). (I write fact posts about pregnancy and childbirth [here.](https://parentingwithevidence.wordpress.com/))\n> \n> To write a fact post, you start with an empirical question, or a general topic.  Something like \"How common are hate crimes?\" or \"Are epidurals really dangerous?\" or \"What causes manufacturing job loss?\"  \n> \n> It's okay if this is a topic you know very little about. This is an exercise in original seeing and showing your reasoning, not finding the official last word on a topic or doing the best analysis in the world.\n> \n> Then you open up a Google doc and start taking notes.\n> \n> You look for *quantitative data from conventionally reliable sources*.  CDC data for incidences of diseases and other health risks in the US; WHO data for global health issues; Bureau of Labor Statistics data for US employment; and so on. Published scientific journal articles, especially from reputable journals and large randomized studies.\n> \n> You explicitly do *not* look for opinion, even expert opinion. You avoid news, and you're wary of think-tank white papers. You're looking for raw information. You are taking a *sola scriptura* approach, for better and for worse.\n> \n> And then you start letting the data show you things. \n> \n> You see things that are surprising or odd, and you note that. \\[continues\\]"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "x6evH6MyPK3nxsoff",
    "name": "Identity",
    "core": null,
    "slug": "identity",
    "oldSlugs": null,
    "postCount": 64,
    "description": {
      "markdown": "**Identity** is an individual's conception of themselves (1). We might conceive of this as the set of *I am ___* statements an individual would make about themselves. It seems correct that *identity* can be dangerous for epistemics since the desire to maintain one's identity can interfere with updating correctly or changing actions \\[[1](https://www.lesswrong.com/posts/BXQsZmubkovJ76Ldo/the-actionable-version-of-keep-your-identity-small)\\], but at the same time there are potentially useful and safe ways to maintain an identity which even enhances one's rationality \\[[1](https://www.lesswrong.com/posts/uR8c2NPp4bWHQ5u45/strategic-choice-of-identity), [2](https://www.lesswrong.com/posts/Zupr296Zy74wpihXT/use-your-identity-carefully)\\].  \n  \n  \n*(1) We might consider cases where an external party imposes an identity on someone, but that case has not been the topic of most discussion of Identity on LessWrong.*\n\n**External resources:** [Keep Your Identity Small](http://www.paulgraham.com/identity.html#f2n) by Paul Graham\n\n**Related Pages:** [Personal Identity](https://www.lesswrong.com/tag/personal-identity), [Self Improvement](https://www.lesswrong.com/tag/self-improvement)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "LNsEBXoFdAy8yzvbw",
    "name": "Memetic Immune System",
    "core": false,
    "slug": "memetic-immune-system",
    "oldSlugs": null,
    "postCount": 17,
    "description": {
      "markdown": "> Intelligent people sometimes do things more stupid than stupid people are capable of. There are a variety of reasons for this; but one has to do with the fact that all cultures have dangerous memes circulating in them, and cultural antibodies to those memes. The trouble is that these antibodies are not logical. On the contrary; these antibodies are often highly _illogical_. They are the blind spots that let us live with a dangerous meme without being impelled to action by it.\n\n-Phil Goetz, [Reason as memetic immune disorder](https://www.lesswrong.com/posts/aHaqgTNnFzD7NGLMx/reason-as-memetic-immune-disorder)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "ZXFpyQWPB5ideFbEG",
    "name": "Conversation (topic)",
    "core": false,
    "slug": "conversation-topic",
    "oldSlugs": null,
    "postCount": 107,
    "description": {
      "markdown": "A **conversation** is when two people talk or correspond. Most content here is about *how to have good conversations.* (*This wikitag needs work.)*  \n  \nFor records of conversations, see [Interviews](Interviews (1)), Debates,...\n\nSee also:\n\n*   Communication\n*   Communication Cultures\n*   Relationshops\n*   Community\n\n*Conversation Halter*\n---------------------\n\nThis term was introduced on LessWrong by Eliezer in the [eponymous post](https://www.lesswrong.com/posts/wqmmv6NraYv4Xoeyj/conversation-halters):\n\n> *While working on my book, I found in passing that I'd developed a list of what I started out calling \"stonewalls\", but have since decided to refer to as \"conversation halters\".  These tactics of argument are distinguished by their being attempts to cut off the flow of debate - which is rarely the wisest way to think, and should certainly rate an alarm bell.*"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "RE6h98Ziwcfh4EP9T",
    "name": "Steelmanning",
    "core": null,
    "slug": "steelmanning",
    "oldSlugs": null,
    "postCount": 29,
    "description": {
      "markdown": "**Steelmanning** is the act of taking a view, or opinion, or argument and constructing the strongest possible version of it. It is the opposite of strawmanning.\n\n**External Posts:**  \n[Against Steelmanning](https://thingofthings.wordpress.com/2016/08/09/against-steelmanning/) by Thing of Things\n\nSee also: [Disagreement](http://lesswrong.com/tag/disagreement), [Ideological Turing Tests](https://www.lesswrong.com/tag/ideological-turing-tests), [Least convenient possible world](https://lessestwrong.com/tag/least-convenient-possible-world)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "mPuSAzJN7CyrMiKrf",
    "name": "Voting Theory",
    "core": null,
    "slug": "voting-theory",
    "oldSlugs": null,
    "postCount": 35,
    "description": {
      "markdown": "**Voting theory**, also called social choice theory, is the study of voting mechanisms. In other words, for a given list of candidates and voters, a voting method specifies a set of valid ways to fill out a ballot, and, given a valid ballot from each voter, produces an outcome.\n\n**Related Sequences:** [Voting Theory Primer for Rationalists](https://www.lesswrong.com/s/ZBNBTSMAXbyJwJoKY)\n\n**Resources:** [Electowiki](https://electowiki.org/wiki/Main_Page) \\- A wiki focused on voting theory and electoral systems.\n\n**related tags:** [Game Theory](https://www.lesswrong.com/tag/game-theory), [Decision Theory](https://www.lesswrong.com/tag/decision-theory), [Coordination / Cooperation](https://www.lesswrong.com/tag/coordination-cooperation?showPostCount=true&useTagName=true)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "EeSkeTcT4wtW2fWsL",
    "name": "Cause Prioritization",
    "core": false,
    "slug": "cause-prioritization",
    "oldSlugs": null,
    "postCount": 38,
    "description": {
      "markdown": "**Cause Prioritization** is the process of researching which charitable causes offer the most benefit for the marginal investment. Priorities can shift as existing causes reach funding and hiring goals, and new opportunities to do good are discovered. Cause prioritization is an important part of [Effective Altruism](http://lesswrong.com/tag/effective-altruism).\n\n**See also:** [Cause Prioritization Wiki](https://causeprioritization.org/)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "t7t9nW6BtJhfGNSR6",
    "name": "Aging",
    "core": false,
    "slug": "aging",
    "oldSlugs": null,
    "postCount": 50,
    "description": {
      "markdown": "See also: [Life Extension](https://www.lesswrong.com/tag/life-extension)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "RLQumypPQGPYg9t6G",
    "name": "Gaming (videogames/tabletop)",
    "core": false,
    "slug": "gaming-videogames-tabletop",
    "oldSlugs": null,
    "postCount": 130,
    "description": {
      "markdown": "**Gaming**\n\nSee also: [Game Theory](https://www.lesswrong.com/tag/game-theory), [Puzzle Game Index](https://www.lesswrong.com/tag/puzzle-game-index), [Games (posts describing)](https://www.lesswrong.com/tag/games-posts-describing)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "cRaweRcZcXnb9Qryt",
    "name": "Meta-Honesty",
    "core": null,
    "slug": "meta-honesty",
    "oldSlugs": null,
    "postCount": 12,
    "description": {
      "markdown": "**Meta-Honesty** is the attempt to be honest about in which situations one will not be honest. It derives from the recognition that an object-level commitment to never lie under any possible circumstance is untenable. A meta-honest person might say something like \"I will lie in circumstances similar to an axe-wielding murderer coming to my door and enquiring after the location of my friend.\""
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "bY5MaF2EATwDkomvu",
    "name": "History",
    "core": false,
    "slug": "history",
    "oldSlugs": null,
    "postCount": 193,
    "description": {
      "markdown": "**History**: \"Why should I remember the Wright Brothers’ first flight? I was not there. But as a rationalist, could I dare to not remember, when the event actually happened? Is there so much difference between seeing an event through your eyes—which is actually a causal chain involving reflected photons, not a direct connection—and seeing an event through a history book? Photons and history books both descend by causal chains from the event itself.\" - Eliezer Yudkowsky, [*Making History Available*](https://www.lesswrong.com/posts/TLKPj4GDXetZuPDH5/making-history-available).\n\n**Related Pages:** [History of Rationality](https://www.lesswrong.com/tag/history-of-rationality), [History of Less Wrong](https://www.lesswrong.com/tag/history-of-less-wrong)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "sPpZRaxpNNJjw55eu",
    "name": "Progress Studies",
    "core": false,
    "slug": "progress-studies",
    "oldSlugs": null,
    "postCount": 165,
    "description": {
      "markdown": "**Progress Studies** is the study of the causes of civilizational progress, e.g., the combination of economic, technological, scientific, and cultural advancements that have transformed human life and raised standards of living over the past couple of centuries.\n\n> *The bicycle, as we know it today, was not invented until the late 1800s. Yet it was a simple mechanical invention. It would seem to require no brilliant inventive insight, and certainly no scientific background. *  \n> *Why, then, wasn’t it invented much earlier? –* [*Why did we wait so long for the bicycle?*](https://www.lesswrong.com/posts/TPytnFcWiD2E4cTrm/why-did-we-wait-so-long-for-the-bicycle)\n\nSee also:\n---------\n\n*   [History](https://www.lesswrong.com/tag/history)\n\nOrigin of the Name\n------------------\n\n*Progress Studies* was proposed as an academic field by Tyler Cowen and Patrick Collison \\[[1](https://www.theatlantic.com/science/archive/2019/07/we-need-new-science-progress/594946/)\\] after they noticed that there’s no intellectual movement focused on understanding the dynamics of progress, or on trying to speed it up.\n\nExternal Resources\n------------------\n\n*   [*Roots of Progress*](https://rootsofprogress.org/about)  is a blog by [jasoncrawford](https://www.lesswrong.com/users/jasoncrawford) that explores the history of technology and industry alongside the philosophical questions of human progress. Many of the blogs posts are crossposted to LessWrong."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "FkzScn5byCs9PxGsA",
    "name": "Politics",
    "core": null,
    "slug": "politics",
    "oldSlugs": null,
    "postCount": 303,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "KDpqtN3MxHSmD4vcB",
    "name": "Art",
    "core": false,
    "slug": "art",
    "oldSlugs": [
      "art",
      "artddd56732"
    ],
    "postCount": 62,
    "description": {
      "markdown": "**Art** is material created for aesthetic appreciation, including visual art, comics, and music. This tag includes both posts sharing works of art and posts discussing art conceptually.\n\nSome types of art have their own tag category: [Poetry](http://lesswrong.com/tag/poetry), [Fiction](http://lesswrong.com/tag/fiction), [Fiction(topic)](https://www.lesswrong.com/tag/fiction-topic), [Games](https://www.lesswrong.com/tag/gaming-videogames-tabletop)\n\n**Related Sequences:** [Drawing Less Wrong](https://www.lesswrong.com/s/WPgA9x5ZvKu9oYvgB)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "FtT2T9bRbECCGYxrL",
    "name": "Philosophy of Language",
    "core": false,
    "slug": "philosophy-of-language",
    "oldSlugs": null,
    "postCount": 120,
    "description": {
      "markdown": "**Language** is an important part of the way we frame problems, think about the world, and discuss things with others. The study of language is particularly relevant to LessWrong because [many very smart people confuse themselves and others by misusing words.](https://www.lesswrong.com/posts/FaJaCgqBKphrDzDSj/37-ways-that-words-can-be-wrong)\n\nThe comprehensive introduction to the LessWrong approach to Philosophy of Language is [A Human's Guide To Words](https://www.lesswrong.com/s/SGB7Y5WERh4skwtnb)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "o9aQASibdsECTfYF6",
    "name": "Moloch",
    "core": null,
    "slug": "moloch",
    "oldSlugs": null,
    "postCount": 50,
    "description": {
      "markdown": "**Moloch** is the personification of the forces that coerce competing individuals to take actions which, although locally optimal, ultimately lead to situations where everyone is worse off. Moreover, no individual is able to unilaterally break out of the dynamic. The situation is a bad Nash equilibrium. A trap.\n\nOne example of a Molochian dynamic is a [Red Queen race](https://en.wikipedia.org/wiki/Red_Queen%27s_race) between scientists who must continually spend more time writing grant applications just to keep up with their peers doing the same. Through unavoidable competition, they have all lost time while not ending up with any more grant money. And any scientist who unilaterally tried to not engage in the competition would soon be replaced by one who still does. If they all promised to cap their grant writing time, everyone would face an incentive to defect.\n\nThe topic of Moloch receives a formal treatment in the sequence [Inadequate Equilibria](https://www.lesswrong.com/s/oLGCcbnvabyibnG9d), particularly in the chapter [Moloch's Toolbox](https://www.lesswrong.com/posts/x5ASTMPKPowLKpLpZ/moloch-s-toolbox-1-2).\n\nOrigin\n------\n\n[Scott Alexander](https://www.lesswrong.com/users/yvain?sortedBy=top)  linked the name to the concept in his eponymous post, [Meditations on Moloch](https://www.lessestwrong.com/posts/TxcRbCYHaeL59aY7E/meditations-on-moloch).  The post intersperses lines of Allan Ginsberg's poem, [Howl](https://www.poetryfoundation.org/poems/49303/howl), with multiples examples of the dynamic including: the Prisoner's Dilemma, dollar auctions, [fish farming story](https://web.archive.org/web/20160928190322/http://raikoth.net/libertarian.html), Malthusian trap, capitalism, two-income trap, agriculture, arms races, races to the bottom, education system, science, and government corruption and corporate welfare. \n\nFrom Allan Ginsberg's [Howl](https://www.poetryfoundation.org/poems/49303/howl):\n\n> *What sphinx of cement and aluminum bashed open their skulls and ate up their brains and imagination?*  \n> *Moloch! Solitude! Filth! Ugliness! Ashcans and unobtainable dollars! Children screaming under the stairways! Boys sobbing in armies! Old men weeping in the parks!*  \n> *Moloch! Moloch! Nightmare of Moloch! Moloch the loveless! Mental Moloch! Moloch the heavy judger of men!*  \n> *Moloch the incomprehensible prison! Moloch the crossbone soulless jailhouse and Congress of sorrows! Moloch whose buildings are judgment! Moloch the vast stone of war! Moloch the stunned governments!*  \n> *Moloch whose mind is pure machinery! Moloch whose blood is running money! Moloch whose fingers are ten armies! Moloch whose breast is a cannibal dynamo! Moloch whose ear is a smoking tomb!*  \n> *Moloch whose eyes are a thousand blind windows! Moloch whose skyscrapers stand in the long streets like endless Jehovahs! Moloch whose factories dream and croak in the fog! Moloch whose smoke-stacks and antennae crown the cities!*  \n> *Moloch whose love is endless oil and stone! Moloch whose soul is electricity and banks! Moloch whose poverty is the specter of genius! Moloch whose fate is a cloud of sexless hydrogen! Moloch whose name is the Mind!*\n\nSee also: [Eldritch Analogies](https://www.lesswrong.com/tag/eldritch-analogies), [Game Theory](https://www.lesswrong.com/tag/game-theory), [Group Rationality](https://www.lesswrong.com/tag/group-rationality?showPostCount=true&useTagName=true), [Social Reality](https://www.lesswrong.com/tag/social-reality)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "CD6DGZJD4ningyzWF",
    "name": "Trigger-Action Planning",
    "core": null,
    "slug": "trigger-action-planning",
    "oldSlugs": null,
    "postCount": 27,
    "description": {
      "markdown": "**Trigger-Action Planning (**TAP), sometimes **Trigger-Action Patterns**, and formerly **Implementation Intentions** are techniques for getting oneself to successfully enact desired actions (or inactions) by training something like a \"stimulus-response\" pair. The technique was spread by CFAR which initially drew upon the psychology literature of Implementation Intentions. \n\nAfter it was clear that TAPs should be heavily applied to cognitive motions/thought patterns, some decided that the 'P' should stand for 'Pattern' rather than 'Plan'.\n\n**Resources**\n-------------\n\n*   The CFAR Participant Handbook \\[1\\] contains a chapter on TAPs.\n*   Brienne Yudkowsky's writings on the topic of Noticing, found mostly at her blog \\[[1](https://agentyduck.blogspot.com/search?q=noticing)\\], are particularly good material related to training TAPs."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "8uNFGxejo5hykCEez",
    "name": "Virtues",
    "core": null,
    "slug": "virtues",
    "oldSlugs": [
      "virtues",
      "virtues-instrumental"
    ],
    "postCount": 92,
    "description": {
      "markdown": "**Virtues** are traits that one *ought* to possess, for the benefit of the world or oneself.\n\nOn LessWrong the focus is often on epistemic virtues, as in Eliezer Yudowsky's essay [**Twelve Virtues of Rationality**](https://www.lesswrong.com/posts/7ZqGiPHTpiDMwqMN2/twelve-virtues-of-rationality) which offers this list of virtues (roughly summarized):\n\n*   [**Curiosity**](https://www.lesswrong.com/tag/curiosity) \\- the burning desire to pursue truth;\n*   **Relinquishment** \\- not being attached to mistaken beliefs;\n*   **Lightness** \\- updating your beliefs with ease;\n*   [**Evenness**](https://www.lesswrong.com/tag/evenness) \\- not privileging particular hypotheses in the pursuit of truth;\n*   **Argument** \\- the will to let one's beliefs be challenged;\n*   [**Empiricism**](https://www.lesswrong.com/tag/empiricism) \\- grounding oneself in observation and prediction;\n*   [**Simplicity**](https://www.lesswrong.com/tag/occam-s-razor) \\- elimination of unnecessary detail in modeling the world;\n*   [**Humility**](https://www.lesswrong.com/tag/humility) \\- recognition of one's fallibility;\n*   [**Perfectionism**](https://www.lesswrong.com/tag/perfectionism) \\- seeking perfection even if it's not attainable;\n*   **Precision** \\- seeking narrower statements and not overcorrect;\n*   [**Scholarship**](https://www.lesswrong.com/tag/scholarship-and-learning) \\- the study of multiple domains and perspectives;\n*   [**The nameless virtue**](https://www.lesswrong.com/tag/twelfth-virtue) \\- seeking truth and not the virtues for themselves.\n\n**See Also:** [Courage](https://www.lesswrong.com/tag/courage), [Trust](https://www.lesswrong.com/tag/trust), [Honesty](https://www.lesswrong.com/tag/honesty), [Agency](https://www.lesswrong.com/tag/agency), [Altruism](https://www.lesswrong.com/tag/altruism), [Ambition](https://www.lesswrong.com/tag/ambition), [Stoicism / Letting Go / Making Peace](https://www.lesswrong.com/tag/stoicism-letting-go-making-peace), [Attention](https://www.lesswrong.com/tag/attention), [Gratitude](https://www.lesswrong.com/tag/gratitude)\n\n**Sequences:**  \n[Notes On Virtues](https://www.lesswrong.com/s/xqgwpmwDYsn8osoje) by [David_Gross](https://www.lesswrong.com/users/david_gross)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "ZzxvopS4BwLuQy42n",
    "name": "Rationalization",
    "core": null,
    "slug": "rationalization",
    "oldSlugs": null,
    "postCount": 61,
    "description": {
      "markdown": "**Rationalization** is the act of finding reasons to believe what one has already decided they want to believe. It is a decidedly terrible way to arrive at true beliefs.\n\n> *“Rationalization.” What a curious term. I would call it a wrong word. You cannot “rationalize” what is not already rational. It is as if “lying” were called “truthization.” –* [Rationalization](https://www.lessestwrong.com/posts/SFZoEBpLo9frSJGkc/rationalization)\n\nRationality starts from evidence, and then crunches forward through belief updates, in order to output a probable conclusion. \"Rationalization\" starts from a conclusion, and then works backward to arrive at arguments apparently favoring that conclusion. Rationalization argues for a side already selected; rationality tries to choose between sides.\n\nRationalization can be conscious or unconscious. It can take on a blatant, conscious form, in which you are aware that you want a particular side to be correct and you deliberately compose arguments for only that side, looking over the evidence and consciously filtering which facts will be presented. Or it can occur at perceptual speeds, without conscious intent or conscious awareness.\n\nDefeating rationalization - or even *discovering* rationalizations - is a lifelong battle for the aspiring rationalist.\n\nSee Also\n--------\n\n*   [Motivated skepticism](https://lessestwrong.com/tag/motivated-skepticism), [motivated cognition](https://lessestwrong.com/tag/motivated-reasoning)\n*   [Filtered evidence](https://lessestwrong.com/tag/filtered-evidence)\n*   [Fake simplicity](https://lessestwrong.com/tag/fake-simplicity)\n*   [Self-deception](https://lessestwrong.com/tag/self-deception)\n*   [Litany of Gendlin](https://lessestwrong.com/tag/litany-of-gendlin)\n*   [Occam's Imaginary Razor](https://wiki.lesswrong.com/wiki/Occam's_Imaginary_Razor)\n*   [Hope](https://lessestwrong.com/tag/hope), [oops](https://lessestwrong.com/tag/oops)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "LDTSbmXtokYAsEq8e",
    "name": "Motivated Reasoning",
    "core": false,
    "slug": "motivated-reasoning",
    "oldSlugs": null,
    "postCount": 57,
    "description": {
      "markdown": "**Motivated Reasoning** is a label for various mental processes that lead to desired conclusions regardless of the veracity of those conclusions.\n\n*Related*: [Confirmation Bias](https://www.lesswrong.com/tag/confirmation-bias), [Rationalization](https://www.lesswrong.com/tag/rationalization), [Self-deception](https://www.lesswrong.com/tag/self-deception) \n\nNotable Posts\n-------------\n\n*   [Semantic Stopsigns](https://lessestwrong.com/lw/it/semantic_stopsigns/)\n*   [Explain/Worship/Ignore?](https://lessestwrong.com/lw/j2/explainworshipignore/)\n*   [Avoiding Your Belief's Real Weak Points](https://lessestwrong.com/lw/jy/avoiding_your_beliefs_real_weak_points/)\n*   [Motivated Stopping and Motivated Continuation](https://lessestwrong.com/lw/km/motivated_stopping_and_motivated_continuation/)\n*   [The Bottom Line](https://lesswrong.com/lw/js/the_bottom_line/)\n\nSee also\n--------\n\n*   [Rationalization](https://lessestwrong.com/tag/rationalization)\n*   [Motivated skepticism](https://lessestwrong.com/tag/motivated-skepticism)\n*   [Filtered evidence](https://lessestwrong.com/tag/filtered-evidence)\n*   [Positive bias](https://lessestwrong.com/tag/confirmation-bias)\n*   [Ugh field](https://lessestwrong.com/tag/aversion-ugh-fields)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "mf8wHrMrJR73uyDLQ",
    "name": "Moral Mazes",
    "core": null,
    "slug": "moral-mazes",
    "oldSlugs": null,
    "postCount": 39,
    "description": {
      "markdown": "**Moral Mazes** is a term for businesses where middle managers spend most of their time and energy on internal status competitions rather than improving the company's products. The phrase comes from [the book of the same name](https://www.amazon.com/Moral-Mazes-World-Corporate-Managers/dp/0199729883) by Robert Jackall.\n\nSee also: [Moloch](https://www.lesswrong.com/tag/moloch) tag, and the [Immoral Mazes sequence](https://www.lesswrong.com/s/kNANcHLNtJt5qeuSS)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "vmvTYnmaKA73fYDe5",
    "name": "Life Extension",
    "core": false,
    "slug": "life-extension",
    "oldSlugs": null,
    "postCount": 58,
    "description": {
      "markdown": "**Life Extension** is the theory / practice of extending human lifespans – for decades, centuries. or much longer. This includes advice that applies to individuals, research projects that might extend human lifespans as a whole, or philosophical discussion of the concept. \n\nSee also [Aging](https://www.lesswrong.com/tag/aging), and [Cryonics](https://www.lessestwrong.com/tag/cryonics) \\- a particular life extension technique that has received a lot of discussion on LessWrong."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "zcvsZQWJBFK6SxK4K",
    "name": "Postmortems & Retrospectives",
    "core": null,
    "slug": "postmortems-and-retrospectives",
    "oldSlugs": null,
    "postCount": 135,
    "description": {
      "markdown": "A **Postmortem** or **Retrospective** is a reflection on past actions with an eye to what went well, what didn't, and the cause of any failures. Retrospectives are crucial for improving one's rationality: they are the opportunity to the grade both one's direct decisions as well as the decision-procedure and epistemic algorithms that one was employing. Sharing such accounts publicly is prosocial and allows others to learn from one's experience too.\n\nOne can ask \"could I have done better had I reasoned better with the information available?\" Often the answer is \"yes\", and one can apply lessons learnt going forward. It can feel painful to reflect on one's mistakes, but doing so is how one grows. \n\nThis tag is specifically reporting actions and outcomes together with an evaluation of the choices/thinking patterns used. If a post focuses on changes in general beliefs about the world, without reflecting on specific actions, then it is a good fit for the [Updated Beliefs](https://www.lesswrong.com/tag/updated-beliefs-examples-of) tag. A central example of a Postmortems & Retrospectives post is “[Arbital Postmortem](https://www.lesswrong.com/posts/kAgJJa3HLSZxsuSrf/arbital-postmortem)”; in contrast, central examples of Updated Beliefs posts are “['Other people are wrong' vs 'I am right'](https://www.lesswrong.com/posts/4QemtxDFaGXyGSrGD/other-people-are-wrong-vs-i-am-right)” and “[Six Economics Misconceptions](https://www.lesswrong.com/posts/MgFDzAfCku9MSDLuw/six-economics-misconceptions-of-mine-which-i-ve-resolved)”.\n\n**Related pages:** [Updated Beliefs (examples of)](https://www.lesswrong.com/tag/updated-beliefs-examples-of), [Growth Stories](https://www.lesswrong.com/tag/growth-stories), [Progress Studies](https://www.lesswrong.com/tag/progress-studies)\n\n**See also:** [Premortem](https://en.wikipedia.org/wiki/Pre-mortem)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "HFou6RHqFagkyrKkW",
    "name": "Programming",
    "core": null,
    "slug": "programming",
    "oldSlugs": null,
    "postCount": 130,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "hXTqT62YDTTiqJfxG",
    "name": "Ritual",
    "core": false,
    "slug": "ritual",
    "oldSlugs": null,
    "postCount": 67,
    "description": {
      "markdown": "**Rituals** are symbolic actions. In the context of LessWrong, it's significant that many rituals have some impact on your cognition, which makes them appropriate to be careful with. Nonetheless, some LessWrongers have worked to explore the space of ritual through a rationalist lens.\n\nIt's a bit tricky to define. The book [Secular Wholeness](https://www.amazon.com/Secular-Wholeness-Skeptics-Paths-Richer/dp/155369175X) notes:\n\n> *There’s a hazy boundary between the words “ritual,” “habit,” and “custom.” I think the difference between a ritual act and a habitual one lies in awareness and assent. An act becomes a ritual for you when you perform it with conscious awareness of its symbolic and emotional meaning, and with willing assent to those meanings. Unless you act with both awareness and assent, your act is merely a habit (if it is unique to you) or a custom (if you share it with others).*\n\nTwo key questions relating to ritual and rationality are:\n\n*   How can we capture the value of ritual, without incurring epistemic risk?\n*   Can rituals be actively helpful for rationality?\n\n**Sequences:**\n\n*   [Rational Ritual](https://www.lesswrong.com/s/3bbvzoRA8n6ZgbiyK) by [Raemon](https://www.lesswrong.com/users/raemon)\n\n**Related Pages:** [Secular Solstice](https://www.lesswrong.com/tag/secular-solstice), [Petrov Day](https://www.lesswrong.com/tag/petrov-day), [Grieving](https://www.lesswrong.com/tag/grieving), [Marriage](https://www.lesswrong.com/tag/marriage), [Religion](https://www.lesswrong.com/tag/religion), [Art](https://www.lesswrong.com/tag/art), [Music](https://www.lesswrong.com/tag/music), [Poetry](https://www.lesswrong.com/tag/poetry), [Meditation](https://www.lesswrong.com/tag/meditation), [Circling](https://www.lesswrong.com/tag/circling)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "AodfCFefLAuwDyj7Z",
    "name": "Self Experimentation",
    "core": false,
    "slug": "self-experimentation",
    "oldSlugs": null,
    "postCount": 52,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "ZnHkaTkxukegSrZqE",
    "name": "Cryonics",
    "core": null,
    "slug": "cryonics",
    "oldSlugs": null,
    "postCount": 118,
    "description": {
      "markdown": "**Cryonics** is the practice of preserving people who are dying in liquid nitrogen soon after their heart stops. The idea is that most of your brain's information content is still intact right after you've \"died\", i.e. medical death or legal death. If humans invent molecular nanotechnology or brain emulation techniques, it may be possible to reconstruct the consciousness of cryopreserved patients.\n\n*Related*: [Life Extension](https://www.lessestwrong.com/tag/life-extension), a more general tag about ways to avoid death.\n\nCryonics-associated issues commonly raised on LessWrong\n-------------------------------------------------------\n\n**Pro-cryonics points**\n\n*   Advanced reductionism/physicalism (because of the issues associated with [identifying a person](https://lessestwrong.com/tag/personal-identity) with continuity of brain information).\n*   Whether an extended healthy lifespan is worthwhile (relates to [Fun Theory](https://lessestwrong.com/tag/fun-theory), religious rationalizations for 70-year lifespans, \"sour grapes\" rationalizations for why death is actually a good thing).\n*   The \"[shut up and multiply](https://lessestwrong.com/tag/shut-up-and-multiply)\" aspect of spending $300/year (as Eliezer Yudkowsky quotes his costs for Cryonics Institute membership ($125/year) plus term life insurance ($180/year)) for a probability (how large being widely disputed) of obtaining many more years of lifespan. For this reason, cryonics advocates regard it as an *extreme case* of failure at rationality - a low-hanging fruit by which millions of deaths per year could be prevented at low cost.\n\n**Anti-cryonics points**\n\n*   Cognitive biases contributing to emotional prejudice in favor of cryonics (optimistic bias, motivated cognition).\n*   The [multiply chained nature](https://lessestwrong.com/tag/conjunction-fallacy) of the probabilities involved in cryonics, and whether the final expected utility is worth the cost.\n*   Money spent on cryonics could, arguably, be better spent on [efficient charity](https://lessestwrong.com/lw/3gj/efficient_charity_do_unto_others/).\n\nNotable Posts\n-------------\n\n*   [We Agree: Get Froze](http://www.overcomingbias.com/2008/12/we-agree-get-froze.html) by [Robin Hanson](https://lessestwrong.com/tag/robin-hanson). \"My co-blogger Eliezer and I may disagree on AI fooms, but we agree on something quite contrarian and, we think, huge: More likely than not, most folks who die today didn't have to die! ... It seems far more people read this blog daily than have ever signed up for cryonics. While it is hard to justify most medical procedures using standard health economics calculations, such calculations say that at today's prices cryonics seems a good deal even if you think there's only a 5% chance it'll work.\"\n*   [You Only Live Twice](https://lessestwrong.com/lw/wq/you_only_live_twice/) by [Eliezer Yudkowsky](https://lessestwrong.com/tag/eliezer-yudkowsky). \"My co-blogger Robin and I may disagree on how fast an AI can improve itself, but we agree on an issue that seems much simpler to us than that: At the point where the current legal and medical system gives up on a patient, they aren't really dead.\"\n*   [The Pascal's Wager Fallacy Fallacy](https://lessestwrong.com/lw/z0/the_pascals_wager_fallacy_fallacy/) \\- the fallacy of Pascal's Wager combines a high payoff with a [privileged hypothesis](https://lessestwrong.com/tag/privileging-the-hypothesis), one with low prior probability and no particular reason to believe it. Perceptually seeing an instance of \"Pascal's Wager\" *just* from the high payoff, even when the probability is not small, is the Pascal's Wager Fallacy Fallacy.\n*   [Normal Cryonics](https://lessestwrong.com/lw/1mc/normal_cryonics/) \\- On the shift of perspective that came from attending a gathering of normal-seeming young cryonicists.\n*   [That Magical Click](https://lessestwrong.com/lw/1mh/that_magical_click/) \\- What is the unexplained process whereby some people get cryonics, or other frequently-derailed chains of thought, in a very short time?\n*   [Quantum Mechanics and Personal Identity](https://lessestwrong.com/lw/r9/quantum_mechanics_and_personal_identity/) by [Eliezer Yudkowsky](https://lessestwrong.com/tag/eliezer-yudkowsky). A shortened index into the [Quantum Physics Sequence](http://www.overcomingbias.com/2008/06/the-quantum-phy.html) describing only the prerequisite knowledge to understand the statement that \"science can rule out a notion of personal identity that depends on your being composed of the same atoms - because modern physics has taken the concept of 'same atom' and thrown it out the window. There *are* no little billiard balls with individual identities. It's experimentally ruled out.\" The key post in this sequence is [Timeless Identity](http://www.overcomingbias.com/2008/06/timeless-identi.html), in which \"Having used physics to completely trash all naive theories of identity, we reassemble a conception of persons and experiences from what is left\" but this finale might make little sense without the prior discussion.\n*   [Break Cryonics Down](http://www.overcomingbias.com/2009/03/break-cryonics-down.html) by [Robin Hanson](https://lessestwrong.com/tag/robin-hanson) \\- tries to identify some of the chained probabilities involved in cryonics.\n*   [Third Alternatives for Afterlife-ism](https://lessestwrong.com/lw/hv/third_alternatives_for_afterlifeism/) by [Eliezer Yudkowsky](https://lessestwrong.com/tag/eliezer-yudkowsky) \\- explains why cryonics is a [third option](https://lessestwrong.com/tag/third-option) in the dilemma about whether we should tell [noble lies](https://wiki.lesswrong.com/wiki/noble_lie) about an afterlife, to prevent people from getting depressed by not believing in an afterlife.\n*   [A survey of anti-cryonics writing](https://lessestwrong.com/lw/1r0/a_survey_of_anticryonics_writing/) by [ciphergoth](https://lessestwrong.com/tag/ciphergoth) \\- an attempt to find quality criticism of cryonics, with a surprising result that \"there is not one person who has ever taken the time to read and understand cryonics claims in any detail, still considers it pseudoscience, and has written a paper, article or even a blog post to rebut anything that cryonics advocates actually say\".\n\nExternal links\n--------------\n\n*   [Why Croynics Makes Sense, WaitButWhy](http://waitbutwhy.com/2016/03/cryonics.html)\n*   [Cryonics Institute FAQ](http://www.benbest.com/cryonics/CryoFAQ.html)\n*   [Alcor Life Extension Foundation FAQ](http://www.alcor.org/FAQs/index.html)\n*   [Alcor FAQ for scientists](http://www.alcor.org/sciencefaq.htm)\n\nSee also\n--------\n\n*   [Exploratory engineering](https://lessestwrong.com/tag/exploratory-engineering), [Absurdity heuristic](https://lessestwrong.com/tag/absurdity-heuristic)\n*   [Status quo bias](https://lessestwrong.com/tag/status-quo-bias), [Reversal test](https://lessestwrong.com/tag/reversal-test)\n*   [Signaling](https://lessestwrong.com/tag/signaling), [Near/far thinking](https://lessestwrong.com/tag/near-far-thinking)\n*   [Death](https://lessestwrong.com/tag/death)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "GpcY5Q226TTy4Cv8N",
    "name": "Decoupling vs Contextualizing",
    "core": false,
    "slug": "decoupling-vs-contextualizing",
    "oldSlugs": null,
    "postCount": 5,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "ogWsaHQKwa6ddidRC",
    "name": "Conflict vs Mistake",
    "core": false,
    "slug": "conflict-vs-mistake",
    "oldSlugs": null,
    "postCount": 17,
    "description": {
      "markdown": "**Conflict vs Mistake** is a framework for analyzing disagreements about policy.\n\nMistake theorists think problems in society are caused by people being bad at achieving common goals. Conflict theorists think problems in society are caused by adversaries with incompatible goals.\n\nScott Alexander [attributed](https://slatestarcodex.com/2018/01/24/conflict-vs-mistake/) the conflict vs mistake framework to [a post on reddit by user no\\_bear\\_so_low](https://www.reddit.com/r/slatestarcodex/comments/74vpwm/socialism_communism_and_marxism_pt_1_on_trust_and/).\n\nA **conflict theorist** thinks problems are primarily due to the conflicting interests of different players. If someone is suffering, someone else must be making money off of it. Karl Marx was a conflict theorist; he blamed the ills of society on class conflict.\n\nA **mistake theorist** thinks problems are primarily due to mistakes. If only we knew how to run society better, there would be less problems. Jeremy Bentham was more of a mistake theorist: he thought producing a formula by which we could calculate the quality of social interventions would help improve society.\n\n[Humans are not automatically strategic](https://www.lesswrong.com/posts/PBRWb2Em5SNeWYwwB/humans-are-not-automatically-strategic) is a mistake theory of human (ir)rationality. Things are hard. If people are doing something dumb, it's probably because they don't know better.\n\n[The Elephant in the Brain](https://www.lesswrong.com/posts/BgBrXpByCSmCLjpwr/book-review-the-elephant-in-the-brain) is more like a conflict theory of human (ir)rationality. Apparent irrationality is attributed mainly to humans not actually wanting what they think they want.\n\n**[Hanlon's Razor](https://en.m.wikipedia.org/wiki/Hanlon%27s_razor)** says: _Never attribute to malice what is adequately explained by stupidity._ This is a clear bias toward mistake theory.\n\nOn the other hand, economics, evolutionary psychology, and some other fields are based on _rational choice theory_, IE, an assumption that behavior can be explained by rational decision-making. _(Economic rationality assumes that individuals choose rationally to maximize economic value, based on the incentives of the current situation. Evolutionary psychology instead assumes that human and animal behaviors will be optimal solutions to the problems they faced in evolutionary history. Bruce Bueno de Mesquita assumes that politicians act rationally so as to maximize their tenure in positions of power. The ACT-R theory of cognition assumes that individual cognitive mechanisms are designed to optimally perform their individual cognitive tasks, such as retrieving memories which are useful in expectation, even if the whole brain is not perfectly rational.)_ This assumption of rationality lends itself more naturally to conflict theories.\n\nGame-Theoretic Connections\n--------------------------\n\nIn game theory, assuming that people can make mistakes (a so-called [trembling hand](https://en.m.wikipedia.org/wiki/Trembling_hand_perfect_equilibrium)) can complicate cooperative strategies.\n\nFor example, in iterated [prisoner's dilemma](https://www.lesswrong.com/tag/prisoner-s-dilemma), **tit for tat** is a cooperative equilibrium (that is to say, it is pareto-optimal, and it is a Nash equilibrium). The tit-for-tat strategy is: cooperate on the first round; then, copy the other person's move from the previous round. This enforces cooperation, because if I defect, I expect my partner to defect on the next round (which is bad for me). This is effectively eye-for-an-eye morality.\n\nHowever, if people make mistakes (the trembling-hand assumption), then tit-for-tat only results in cooperation for an initial period before anyone makes a mistake. If both mistakes are equally probable, then in the long run we'll average only 50% cooperation. We can see this as an interminable family feud where both sides see the other as having done more wrong. \"An eye for an eye makes everyone blind.\"\n\nWe need to recognize that people make mistakes sometimes -- we can't punish everything eye-for-an-eye.\n\nTherefore, some form of _forgiving_ tit-for-tat does better. For example, copy cooperation 100% of the time, but copy defection 90% of the time. This can still work to enforce rational cooperation (depending on the exact payouts and time-discounting of the players), but without everlasting feuds. See also [Contrite Strategies and the Need for Standards](https://www.lesswrong.com/posts/2meuc3kPRkBcRpj3R/contrite-strategies-and-the-need-for-standards).\n\nIn this framing, a conflict theorist thinks people are actually defecting on purpose. They _know what they're doing_, and therefore, _would respond to incentives._ Punishing them is prosocial and helps to encourage more cooperation overall.\n\nA mistake theorist thinks people _are defecting accidentally,_ and therefore, _would not respond to incentives_. Punishing them is pointless and counterproductive; it could even result in a continuing feud, making things much worse for everyone."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "BtQRRKTPxagBH6KrG",
    "name": "Blues & Greens (metaphor)",
    "core": false,
    "slug": "blues-and-greens-metaphor",
    "oldSlugs": [
      "blues-and-greens",
      "blues-and-greens-metaphor"
    ],
    "postCount": 13,
    "description": {
      "markdown": "**\"Blues and Greens\"** is a term used to metaphorically refer to opposing political factions.  \n  \n*See also*: [Tribalism](/tag/tribalism), [Mind-killer](https://www.lesswrong.com/tag/mind-killer), [Arguments as soldiers](https://www.lesswrong.com/tag/arguments-as-soldiers), [False dilemma](https://www.lesswrong.com/tag/false-dilemma), [In-group bias](https://www.lesswrong.com/tag/in-group-bias)\n\nThe term come from the names of chariot racing teams, that differed in nothing but the team colors, but the rivalry of whose fans sometimes reached the level of gang wars.[^1^](https://www.lesswrong.com/tag/blues-and-greens-metaphor?revision=0.0.21&lw_source=import_sheet#fn1) By definition, politics also deals with matters that people physically fight over in the real world -- or at least, matters that are to be enforced by the government's monopoly on violence.\n\nPolitics commonly involves an [adversarial process](https://wiki.lesswrong.com/wiki/adversarial_process), where factions usually identify with political positions, and use [arguments as soldiers](https://www.lesswrong.com/tag/arguments-as-soldiers) to defend their side. When tempered by appropriate [standards of evidence](https://www.lesswrong.com/tag/standard-of-evidence), [rules of order](https://wiki.lesswrong.com/wiki/rules_of_order) and other safeguards, such a process may be the only way of introducing a modicum of deliberative truth-seeking and other [virtues of rationality](https://www.lesswrong.com/tag/virtues-of-rationality) into an inherently violent domain. However, the dichotomies presented by the opposing sides are often [false dilemmas](https://www.lesswrong.com/tag/false-dilemma), which can be shown by presenting [third options](https://www.lesswrong.com/tag/third-option).\n\nFor a variety of reasons, Less Wrong tries to avoid political disputes: see [Mind-killer](https://www.lesswrong.com/tag/mind-killer).\n\nBlog posts\n----------\n\n*   [The Robbers Cave Experiment](https://www.lesswrong.com/lw/lt/the_robbers_cave_experiment/)\n*   [The Two-Party Swindle](https://www.lesswrong.com/lw/mg/the_twoparty_swindle/)\n*   [A Fable of Science and Politics](https://www.lesswrong.com/lw/gt/a_fable_of_science_and_politics/)\n*   [Blue or Green on Regulation?](https://www.lesswrong.com/lw/h2/blue_or_green_on_regulation/) \\- [Burch's law](https://wiki.lesswrong.com/wiki/Burch's_law) isn't a [soldier-argument](https://www.lesswrong.com/tag/scales-of-justice-fallacy) for regulation; estimating the appropriate level of regulation in each particular case is a superior [third option](https://www.lesswrong.com/tag/third-option).\n\nFootnotes\n---------\n\n1.  [Wikipedia:Chariot racing#Byzantine era](https://en.wikipedia.org/wiki/Chariot_racing#Byzantine_era)[↩ ](https://www.lesswrong.com/tag/blues-and-greens-metaphor?revision=0.0.21&lw_source=import_sheet#fnref1)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "Q6P8jLn8hH7kbuXRr",
    "name": "Signaling",
    "core": null,
    "slug": "signaling",
    "oldSlugs": null,
    "postCount": 69,
    "description": {
      "markdown": "**Signaling** is [defined](https://lessestwrong.com/lw/did/what_is_signaling_really/) by [Yvain](https://wiki.lesswrong.com/wiki/Yvain) as \"a method of conveying information among not-necessarily-trustworthy parties by performing an action which is more likely or less costly if the information is true than if it is not true\". Some signaling is performed exclusively to impress others (to improve your [status](https://lessestwrong.com/tag/social-status)), and in some cases [isn't even worth that](http://www.overcomingbias.com/2007/01/excess_signalin.html). In other cases, signaling is a side-effect of an otherwise useful activity.\n\nFor example, if doing something is easy for one type of person and hard for another type of person, you might do that thing just to get people to think you're the former type of person, even if the thing isn't in itself worth doing. This could explain many facets of human behavior, and reveal opportunities for reducing waste.\n\nNot all signaling is about abilities. Signaling can also be about personality, current emotional state, beliefs, loyalty to a particular group, status within a group, etc.\n\n**Countersignaling** is signaling that a naive observer might take to mean that one is the *opposite* of X, when in fact, one is X, used as a means to signal that one is, in fact, X. For example, aristocrats (\"old money\") may forgo gaudy bling in order to signal that they are not *nouveau riche* (new money), which may lead some people to incorrectly assume that they are not rich.\n\nBlog posts\n----------\n\nby [Robin Hanson](https://lessestwrong.com/tag/robin-hanson)\n\n*   [Do Helping Professions Help More?](http://www.overcomingbias.com/2006/12/do_helping_prof.html) and [Gifts Hurt](http://www.overcomingbias.com/2006/12/gifts_hurt.html)\n*   [Excess Signaling Example](http://www.overcomingbias.com/2007/01/excess_signalin.html)\n*   [A Tale Of Two Tradeoffs](http://www.overcomingbias.com/2009/01/a-tale-of-two-tradeoffs.html)\n*   [Why Signals Are Shallow](http://www.overcomingbias.com/2009/06/why-signals-are-shallow.html) \\- \"We all want to affiliate with high status people, but since status is about common distant perceptions of quality, we often care more about what distant observers would think about our associates than about how we privately evaluate them.\"\n*   [Signals Are Forever](http://www.overcomingbias.com/2009/06/signals-are-forever.html)\n*   [Least Signaling Activities?](https://lessestwrong.com/lw/g7/least_signaling_activities/)\n\nby others\n\n*   [What Is Signaling, Really?](https://lessestwrong.com/lw/did/what_is_signaling_really/) by [Yvain](https://wiki.lesswrong.com/wiki/Yvain)\n*   [Think Before You Speak (And Signal It)](https://lessestwrong.com/lw/1y3/think_before_you_speak_and_signal_it/) by [Wei Dai](http://weidai.com/)\n*   [Declare Your Signaling and Hidden Agendas](https://lessestwrong.com/lw/b2/declare_your_signaling_and_hidden_agendas/) by [Kaj Sotala](https://wiki.lesswrong.com/wiki/Kaj_Sotala)\n*   [Modularity, Signaling, and Belief in Belief](https://lessestwrong.com/lw/8ev/modularity_signaling_and_belief_in_belief/) by Kaj Sotala\n\nSee also\n--------\n\n*   [Status](https://lessestwrong.com/tag/social-status)\n*   [Near/far thinking](https://lessestwrong.com/tag/near-far-thinking)\n*   [Adaptation executers](https://wiki.lesswrong.com/wiki/Adaptation_executers), [Superstimulus](https://lessestwrong.com/tag/superstimuli)\n*   [Goodhart's law](https://lessestwrong.com/tag/goodhart-s-law)\n\nExternal links\n--------------\n\n*   [Robin Hanson on Signaling (Econtalk Podcast)](http://www.econtalk.org/archives/2008/05/hanson_on_signa.html)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "ZTRNmvQGgoYiymYnq",
    "name": "Consequentialism",
    "core": false,
    "slug": "consequentialism",
    "oldSlugs": null,
    "postCount": 59,
    "description": {
      "markdown": "**Consequentialism** is the ethical theory that people should choose their actions based on the outcomes they expect will result. How to judge outcomes is not specified, but there are many types of consequentialism that specify how outcomes should be judged. For example, [utilitarianism](https://www.lesswrong.com/tag/utilitarianism) holds that the best outcome is that which maximizes the total welfare of all people, and ethical egoism holds that the best outcome is that which maximizes their own personal interests. Consequentialism is one of three main strands of ethical thought, along with deontology, which holds that people should choose actions which conform to a prescribed list of moral rules, and virtue ethics, which holds that people should be judged by how virtuous they are, instead of by what actions they take.\n\nRelated: [Ethics & Morality](https://www.lesswrong.com/tag/ethics-and-morality), [Deontology](http://lesswrong.com/tag/deontology), [Moral Uncertainty](/tag/moral-uncertainty), [Utilitarianism](https://www.lesswrong.com/tag/utilitarianism)\n\nConsequentialism is often associated with maximizing the [expected value](https://www.lesswrong.com/tag/expected-utility) of a [utility function](https://www.lesswrong.com/tag/utility-functions). However, it has been argued that consequentialism is not the same thing as having a utility function because it is possible to evaluate actions based on their consequences without obeying the [von Neuman-Morgenstern axioms](http://en.wikipedia.org/wiki/Von_Neumann%E2%80%93Morgenstern_utility_theorem) necessary for having a utility function, and because utility functions can also be used to implement moral theories similar to deontology.\n\nBlog posts\n----------\n\n*   [Ends Don't Justify Means (Among Humans)](https://www.lesswrong.com/lw/uv/ends_dont_justify_means_among_humans/)\n*   [Torture vs. Dust Specks](https://www.lesswrong.com/lw/kn/torture_vs_dust_specks/)\n*   [Deontology for Consequentialists](https://www.lesswrong.com/lw/1og/deontology_for_consequentialists/)\n*   [Virtue Ethics for Consequentialists](https://www.lesswrong.com/lw/2aa/virtue_ethics_for_consequentialists/)\n*   [Consequentialism Need Not Be Shortsighted](https://www.lesswrong.com/lw/778/consequentialism_need_not_be_nearsighted/)\n\nExternal links\n--------------\n\n*   [Consequentialism entry on Stanford Encyclopedia of Philosophy](http://plato.stanford.edu/archives/win2011/entries/consequentialism/)\n*   [Consequentialism FAQ](http://www.raikoth.net/consequentialism.html)\n*   [Description and discussion about trolley problems](http://people.howstuffworks.com/trolley-problem.htm)\n\nSee also\n--------\n\n*   [Utilitarianism](https://www.lesswrong.com/tag/utilitarianism)\n*   [Utility](https://www.lesswrong.com/tag/utility), [utility function](https://www.lesswrong.com/tag/utility-functions), [expected utility](https://www.lesswrong.com/tag/expected-utility)\n*   [Metaethics sequence](https://www.lesswrong.com/tag/metaethics-sequence)\n*   [Ethical injunction](https://www.lesswrong.com/tag/ethical-injunction)\n*   [Shut up and multiply](https://www.lesswrong.com/tag/shut-up-and-multiply)\n*   [Hedons](https://wiki.lesswrong.com/wiki/Hedons), [utils](https://wiki.lesswrong.com/wiki/utils), [fuzzies](https://www.lesswrong.com/tag/fuzzies)\n\nReferences\n----------\n\n*   Jeremy Bentham (1907). *An Introduction to the Principles of Morals and Legislation*. Library of Economics and Liberty.\n*   Perter Fishburn (1970). *Utility Theory for Decision Making*. Huntington, NY.\n*   Walter Sinnot-Armstrong (2011). \"[Consequentialism](http://plato.stanford.edu/archives/win2011/entries/consequentialism/)\". *The Stanford Encyclopedia of Philosophy (Winter 2011 Edition)*.\n*   Judith Jarvis Thonson (1975). \"Killing, Letting Die, and the Trolley Problem\". *The Monist* **59**: 204-217."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "Qs5KwojJdJitjisD4",
    "name": "Commitment Mechanisms",
    "core": null,
    "slug": "commitment-mechanisms",
    "oldSlugs": null,
    "postCount": 5,
    "description": {
      "markdown": "A **Commitment Mechanism** is a tool or technique that lets people [pre-commit](https://www.lesswrong.com/tag/pre-commitment) to something. an [assurance contract](https://www.lesswrong.com/tag/assurance-contracts) is a Pre-Commitment done between several people which is conditional on other people also pre-committing.\n\nMany commitment mechanisms incentivize following through on a commitment by penalizing failures to do so. For example, having to pay some amount of money, or automatically posting an announcement to your social media that you failed.\n\n**See also:** [beeminder.com](https://www.beeminder.com/)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "NSMKfa8emSbGNXRKD",
    "name": "Religion",
    "core": null,
    "slug": "religion",
    "oldSlugs": null,
    "postCount": 130,
    "description": {
      "markdown": "**Religion** is a complex group of human activities — involving commitment to higher power, [belief in belief](https://www.lesswrong.com/tag/belief-in-belief), and a range of shared group practices such as worship meetings, rites of passage, etc.\n\nSee also\n--------\n\n*   [Groupthink](https://www.lesswrong.com/tag/groupthink), [Affective death spiral](https://www.lesswrong.com/tag/affective-death-spiral)\n*   [Belief in belief](https://www.lesswrong.com/tag/belief-in-belief), [improper belief](https://www.lesswrong.com/tag/improper-belief)\n*   [Epistemic hygiene](https://www.lesswrong.com/tag/epistemic-hygiene)\n*   [Truth](https://www.lesswrong.com/tag/truth-semantics-and-meaning)\n*   [Rationalization](https://www.lesswrong.com/tag/rationalization), [self-deception](https://www.lesswrong.com/tag/self-deception)\n*   [Magic](https://www.lesswrong.com/tag/magic), [fake simplicity](https://www.lesswrong.com/tag/fake-simplicity)\n*   [Absurdity heuristic](https://www.lesswrong.com/tag/absurdity-heuristic)\n*   [Third alternative](https://wiki.lesswrong.com/wiki/Third_alternative)\n*   [Mysterious Answers to Mysterious Questions](https://www.lesswrong.com/tag/mysterious-answers-to-mysterious-questions)\n\nExternal links\n--------------\n\n*   [BHTV: Yudkowsky & Adam Frank on \"religious experience\"](http://bloggingheads.tv/diavlogs/18501)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "P64rmDCvTBAehmkoi",
    "name": "Filtered Evidence",
    "core": false,
    "slug": "filtered-evidence",
    "oldSlugs": null,
    "postCount": 12,
    "description": {
      "markdown": "**Filtered evidence** is [evidence](https://lessestwrong.com/tag/evidence) that was selected for the fact that it supports (or opposes) a hypothesis. Filtered evidence may be highly misleading, but still it can be useful, if considered with care.\n\nSee also\n--------\n\n*   [Conservation of expected evidence](https://lessestwrong.com/tag/conservation-of-expected-evidence)\n*   [Rational evidence](https://lessestwrong.com/tag/rational-evidence), [Standard of evidence](https://lessestwrong.com/tag/standard-of-evidence), [Adversarial process](https://wiki.lesswrong.com/wiki/Adversarial_process)\n*   [Epistemic hygiene](https://lessestwrong.com/tag/epistemic-hygiene)\n*   [Availability bias](https://wiki.lesswrong.com/wiki/Availability_bias), [Dangerous knowledge](https://lessestwrong.com/tag/dangerous-knowledge)\n*   [Dark arts](https://lessestwrong.com/tag/dark-arts), [Arguments as soldiers](https://lessestwrong.com/tag/arguments-as-soldiers), [Rationalization](https://lessestwrong.com/tag/rationalization)\n*   [Not technically a lie](https://lessestwrong.com/tag/not-technically-a-lie)\n*   [Anthropics](https://www.lesswrong.com/tag/anthropics)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "fihKHQuS5WZBJgkRm",
    "name": "Newcomb's Problem",
    "core": false,
    "slug": "newcomb-s-problem",
    "oldSlugs": null,
    "postCount": 47,
    "description": {
      "markdown": "**Newcomb's Problem** is a thought experiment in decision theory exploring problems posed by having other agents in the environment who can predict your actions.\n\nThe Problem\n-----------\n\nFrom [Newcomb's Problem and Regret of Rationality](https://www.lesswrong.com/posts/6ddcsdA2c2XpNpE5x/newcomb-s-problem-and-regret-of-rationality):\n\n> A superintelligence from another galaxy, whom we shall call Omega, comes to Earth and sets about playing a strange little game. In this game, Omega selects a human being, sets down two boxes in front of them, and flies away.\n\n> Box A is transparent and contains a thousand dollars.  \n> Box B is opaque, and contains either a million dollars, or nothing.\n\n> You can take both boxes, or take only box B.\n\n> And the twist is that Omega has put a million dollars in box B iff Omega has predicted that you will take only box B.\n\n> Omega has been correct on each of 100 observed occasions so far - everyone who took both boxes has found box B empty and received only a thousand dollars; everyone who took only box B has found B containing a million dollars. (We assume that box A vanishes in a puff of smoke if you take only box B; no one else can take box A afterward.)\n\n> Before you make your choice, Omega has flown off and moved on to its next game. Box B is already empty or already full.\n\n> Omega drops two boxes on the ground in front of you and flies off.\n\n> Do you take both boxes, or only box B?\n\nOne line of reasoning about the problem says that because Omega has already left, the boxes are set and you can't change them. And if you look at the payoff matrix, you'll see that whatever decision Omega has already made, you get $1000 more for taking both boxes. This makes taking two boxes (\"two-boxing\") a dominant strategy and therefore the correct choice. Agents who reason this way do not make very much money playing this game. This is because this line of reasoning ignores the connection between the agent and Omega's prediction: two-boxing only makes $1000 more than one-boxing if Omega's prediction is the same in both cases, while the problem states Omega is extremely accurate in its predictions. Switching from one-boxing to two-boxing doesn't give the agent a $1000 more, it results in a loss of $999,000.\n\nBecause the agent's decision in this problem can't causally affect Omega's prediction (which happened in the past), [Causal Decision Theory](https://www.lesswrong.com/tag/causal-decision-theory) two-boxes. One-boxing is correlated with getting a million dollars, whereas two-boxing is correlated with getting only $1000; therefore, [Evidential Decision Theory](https://www.lesswrong.com/tag/evidential-decision-theory) one-boxes. [Functional Decision Theory](https://www.lesswrong.com/tag/functional-decision-theory) (FDT) also one-boxes, but for a completely different reason: FDT reasons that Omega must have had a model of the agent's decision procedure in order to make the prediction. Therefore, your decision procedure is run not only by you, but also (in the past) by Omega; whatever you decide, Omega's model must have decided the same. Either both you and Omega's model two-box, or both you and Omega's model one-box; of these two options, the latter is preferable, so FDT one-boxes.\n\nThe general class of decision problems that involve other agents predicting your actions are called Newcomblike Problems.\n\nIrrelevance of Omega's Physical Impossibility\n---------------------------------------------\n\nSometimes people dismiss Newcomb's problem because of the physical impossibility of a being like Omega. However, Newcomb's problem does not actually depend on the possibility of Omega in order to be relevant. Similar issues arise if we imagine a skilled human psychologist who can predict other people's actions with 65% accuracy.\n\nNotable Posts\n-------------\n\n*   [Newcomb's Problem and Regret of Rationality](https://lessestwrong.com/lw/nc/newcombs_problem_and_regret_of_rationality/)\n*   [Formalizing Newcomb's](https://lessestwrong.com/lw/7v/formalizing_newcombs/)\n*   [Newcomb's Problem standard positions](https://lessestwrong.com/lw/90/newcombs_problem_standard_positions/)\n*   [Newcomb's Problem vs. One-Shot Prisoner's Dilemma](https://lessestwrong.com/lw/6r/newcombs_problem_vs_oneshot_prisoners_dilemma/)\n*   [Decision theory: Why Pearl helps reduce “could” and “would”, but still leaves us with at least three alternatives](https://lessestwrong.com/lw/17b/decision_theory_why_pearl_helps_reduce_could_and/)\n\nSee Also\n--------\n\n*   [Decision theory](https://lessestwrong.com/tag/decision-theory)\n*   [Counterfactual mugging](https://lessestwrong.com/tag/counterfactual-mugging)\n*   [Parfit's hitchhiker](https://wiki.lesswrong.com/wiki/Parfit's_hitchhiker)\n*   [Smoker's lesion](https://wiki.lesswrong.com/wiki/Smoker's_lesion)\n*   [Absentminded driver](https://wiki.lesswrong.com/wiki/Absentminded_driver)\n*   [Sleeping Beauty problem](https://lessestwrong.com/tag/sleeping-beauty-paradox)\n*   [Prisoner's dilemma](https://lessestwrong.com/tag/prisoner-s-dilemma)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "NZ67PZ8CkeS6xn27h",
    "name": "Mesa-Optimization",
    "core": null,
    "slug": "mesa-optimization",
    "oldSlugs": null,
    "postCount": 68,
    "description": {
      "markdown": "**Mesa-Optimization** is the situation that occurs when a learned model (such as a neural network) is itself an optimizer. In this situation, a *base optimizer* creates a second optimizer, called a *mesa-optimizer*. The primary reference work for this concept is Hubinger et al.'s \"[Risks from Learned Optimization in Advanced Machine Learning Systems](https://www.alignmentforum.org/posts/FkgsxrGf3QxhfLWHG/risks-from-learned-optimization-introduction)\".\n\nExample: Natural selection is an optimization process that optimizes for reproductive fitness. Natural selection produced humans, who are themselves optimizers. Humans are therefore mesa-optimizers of natural selection.\n\nIn the context of AI alignment, the concern is that a base optimizer (e.g., a gradient descent process) may produce a learned model that is itself an optimizer, and that has unexpected and undesirable properties. Even if the gradient descent process is in some sense \"trying\" to do exactly what human developers want, the resultant mesa-optimizer will not typically be trying to do the exact same thing.[^1^](https://lessestwrong.com/tag/mesa-optimization?revision=0.0.3#fn1)\n\nHistory\n-------\n\nPreviously work under this concept was called *Inner Optimizer* or *Optimization Daemons.*\n\n[Wei Dai](https://www.lesswrong.com/users/wei_dai) brings up a similar idea in an SL4 thread.[^2^](https://lessestwrong.com/tag/mesa-optimization?revision=0.0.3#fn2)\n\nThe optimization daemons article on [Arbital](https://arbital.com/) was published probably in 2016.[^3^](https://lessestwrong.com/tag/mesa-optimization?revision=0.0.3#fn3)\n\n[Jessica Taylor](https://www.lesswrong.com/users/jessica-liu-taylor) wrote two posts about daemons while at [MIRI](https://www.lesswrong.com/tag/machine-intelligence-research-institute-miri):\n\n*   [\"Are daemons a problem for ideal agents?\"](https://agentfoundations.org/item?id=1281) (2017-02-11)\n*   [\"Maximally efficient agents will probably have an anti-daemon immune system\"](https://agentfoundations.org/item?id=1290) (2017-02-23)\n\nSee also\n--------\n\n*   [Inner Alignment](https://www.lesswrong.com/tag/inner-alignment)\n*   [Complexity of value](https://lessestwrong.com/tag/complexity-of-value)\n*   [Thou Art Godshatter](https://lessestwrong.com/lw/l3/thou_art_godshatter/)\n\nReferences\n----------\n\n1.  [\"Optimization daemons\"](https://arbital.com/p/daemons/). Arbital.\n2.  Wei Dai. ['\"friendly\" humans?'](http://sl4.org/archive/0312/7421.html) December 31, 2003.\n\nExternal links\n--------------\n\n[Video by Robert Miles](https://www.youtube.com/watch?v=bJLcIBixGj8)\n\nSome posts that reference optimization daemons:\n\n*   [\"Cause prioritization for downside-focused value systems\"](http://effective-altruism.com/ea/1k4/draft_cause_prioritization_for_downsidefocused/): \"Alternatively, perhaps goal preservation becomes more difficult the more capable AI systems become, in which case the future might be controlled by unstable goal functions taking turns over the steering wheel\"\n*   [\"Techniques for optimizing worst-case performance\"](https://ai-alignment.com/techniques-for-optimizing-worst-case-performance-39eafec74b99): \"The difficulty of optimizing worst-case performance is one of the most likely reasons that I think prosaic AI alignment might turn out to be impossible (if combined with an unlucky empirical situation).\" (the phrase \"unlucky empirical situation\" links to the optimization daemons page on [Arbital](https://arbital.com/))"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "7oXfRFCR7N22MnuY5",
    "name": "Circling",
    "core": null,
    "slug": "circling",
    "oldSlugs": null,
    "postCount": 8,
    "description": {
      "markdown": "**Circling** is a group \"meditative\", \"relational\" practice. Typically, a group of people sit in a circle and deliberately focus their attention the emotions and experiences of each participant in the group. Communication is usually restricted to the topic of what the individuals in the Circle are experiencing in the present moment, particular their attitudes, feelings, and reactions to others in the group.\n\nCircling may offer benefits in greater awareness of oneself, others, and the interpersonal dynamics between the two. Since social relations are so key to human wellbeing and at the heart of so many psychological challenges, Circling can be of key interest to anyone trying optimize themselves. It may also foster better relationships and cooperation with others.\n\nHowever, Circling originated outside the LessWrong community and many feel that the practice does not have sufficient evidence behind it for it to so widely admired within the Rationalist community."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "nZCb9BSnmXZXSNA2u",
    "name": "Evolution",
    "core": false,
    "slug": "evolution",
    "oldSlugs": null,
    "postCount": 103,
    "description": {
      "markdown": "**Evolution** is \"*change in the heritable characteristics of biological populations over successive generations*\" ([Wikipedia](https://en.wikipedia.org/wiki/Evolution)). For posts about machine learning look [here](https://www.lesswrong.com/tag/machine-learning?showPostCount=false&useTagName=false).\n\n*Related:* [Biology](https://www.lesswrong.com/tag/biology?showPostCount=true&useTagName=true), [Evolutionary Psychology](https://www.lesswrong.com/tag/evolutionary-psychology?showPostCount=true&useTagName=true),\n\nThe sequence, [The Simple Math of Evolution](https://www.lesswrong.com/s/MH2b8NfWv22dBtrs8) provides a good introduction to LessWrong thinking about evolution.\n\nWhy be interested in evolution?\n===============================\n\nFirstly, evolution is a useful case study of humans' ability (or inability) to model the real world. This is because it has a single clear criterion (\"relative reproductive fitness\") which is selected (optimized) for:\n\n> *\"If we can't see clearly the result of a single monotone optimization criterion—if we can't even train ourselves to hear a single pure note—then how will we listen to an orchestra? How will we see that \"Always be selfish\" or \"Always obey the government\" are poor guiding principles for human beings to adopt—if we think that even optimizing genes for inclusive fitness will yield organisms which sacrifice reproductive opportunities in the name of social resource conservation?*\n\n> *To train ourselves to see clearly, we need simple practice cases\" --* Eliezer Yudkowsky*,* [*Fake Optimisation Criteria*](https://www.lesswrong.com/s/MH2b8NfWv22dBtrs8/p/i6fKszWY6gLZSX2Ey)\n\nSecondly, much of rationality necessarily revolves around the human brain ([for](https://www.lesswrong.com/tag/transhumanism?usePostCount=false&useTagName=false) [now](https://www.lesswrong.com/tag/mind-uploading?showPostCount=false&useTagName=false)). An understanding of how it came into being can be very helpful both for understanding 'bugs' in the system (like superstimuli), and for explaining [Complexity of Value](https://www.lesswrong.com/tag/complexity-of-value?showPostCount=true&useTagName=true), among others.\n\n> *A candy bar is a superstimulus: it contains more concentrated sugar, salt, and fat than anything that exists in the ancestral environment.   A candy bar matches taste buds that evolved in a hunter-gatherer environment, but it matches those taste buds much more strongly than anything that actually existed in the hunter-gatherer environment.  The signal that once reliably correlated to healthy food has been hijacked, blotted out with a point in tastespace that wasn't in the training dataset - an impossibly distant outlier on the old ancestral graphs. *  \n> *\\-\\-* Eliezer Yudkowsky, [Superstimuli and the Collapse of Western Civilisation](https://www.lesswrong.com/s/MH2b8NfWv22dBtrs8/p/Jq73GozjsuhdwMLEG)\n\nSee also\n--------\n\n*   [Evolution as alien god](https://lessestwrong.com/tag/evolution-as-alien-god)\n*   [Slowness of evolution](https://lessestwrong.com/tag/slowness-of-evolution)\n*   [Stupidity of evolution](https://lessestwrong.com/tag/stupidity-of-evolution)\n*   [Evolutionary psychology](https://lessestwrong.com/tag/evolutionary-psychology)\n\nExternal links\n--------------\n\n*   [Richard Dawkins - The Selfish Gene](http://dl.dropbox.com/u/33627365/Scholarship/Selfish%20Gene%20-%20Dawkins.pdf) (PDF)\n\nSummaries of Sequence's Posts on Evolution\n------------------------------------------\n\n*The following are summaries of posts concerning evolution in the Eliezer's sequences:*\n\n*   [An Alien God](https://lessestwrong.com/lw/kr/an_alien_god/) \\- Evolution is awesomely powerful, unbelievably stupid, incredibly slow, monomaniacally singleminded, irrevocably splintered in focus, blindly shortsighted, and itself a completely accidental process. If evolution were a god, it would not be Jehovah, but H. P. Lovecraft's Azathoth, the blind idiot God burbling chaotically at the center of everything.\n*   [The Wonder of Evolution](https://lessestwrong.com/lw/ks/the_wonder_of_evolution/) \\- The wonder of the first replicator was not how amazingly well it replicated, but that a first replicator could arise, at all, by pure accident, in the primordial seas of Earth. That first replicator would undoubtedly be devoured in an instant by a sophisticated modern bacterium. Likewise, the wonder of evolution itself is not how *well* it works, but that a *brainless, accidentally occurring* [optimization process](https://lessestwrong.com/tag/optimization) can work *at all*. If you praise evolution for being such a wonderfully intelligent Creator, you're entirely missing the wonderful thing about it.\n*   [Evolutions Are Stupid (But Work Anyway)](https://lessestwrong.com/lw/kt/evolutions_are_stupid_but_work_anyway/) \\- Modern evolutionary theory gives us a definite picture of evolution's capabilities. If you praise evolution one millimeter higher than this, you are not scoring points against creationists, you are just being factually inaccurate. In particular we can calculate the probability and time for advantageous genes to rise to fixation. For example, a mutation conferring a 3% advantage would have only a 6% probability of surviving, and if it did so, would take 875 generations to rise to fixation in a population of 500,000 (on average).\n*   [Speed limit and complexity bound for evolution](https://lessestwrong.com/tag/speed-limit-and-complexity-bound-for-evolution) \\- It is widely understood that there is a limit on how fast evolution can accumulate information in a gene pool, and an upper bound on how much genetic information can be sustained against the degenerative pressure of copying errors. (But Yudkowsky's attempt to calculate an actual bound failed mathematically, so see the referenced summary of the discussion instead of the original blog post.)\n*   [Adaptation-Executers, not Fitness-Maximizers](https://lessestwrong.com/lw/l0/adaptationexecuters_not_fitnessmaximizers/) \\- A central principle of evolutionary biology in general, and [evolutionary psychology](https://lessestwrong.com/tag/evolutionary-psychology) in particular. If we regarded human taste buds as trying to *maximize fitness*, we might expect that, say, humans fed a diet too high in calories and too low in micronutrients, would begin to find lettuce delicious, and cheeseburgers distasteful. But it is better to regard taste buds as an *executing adaptation* \\- they are adapted to an ancestral environment in which calories, not micronutrients, were the limiting factor.\n*   [No Evolutions for Corporations or Nanodevices](https://lessestwrong.com/lw/l6/no_evolutions_for_corporations_or_nanodevices/) \\- Price's Equation describes quantitatively how the change in a average trait, in each generation, is equal to the covariance between that trait and fitness. Such covariance requires substantial variation in traits, substantial variation in fitness, and substantial correlation between the two - and then, to get large *cumulative* selection pressures, the correlation must have persisted over *many* generations with *high-fidelity* inheritance, continuing sources of new variation, and frequent birth of a significant fraction of the population. People think of \"evolution\" as something that automatically gets invoked where \"reproduction\" exists, but these other conditions may not be fulfilled - which is why corporations haven't evolved, and nanodevices probably won't.\n*   [Evolving to Extinction](https://lessestwrong.com/lw/l5/evolving_to_extinction/) \\- Contrary to a naive view that evolution works for the good of a species, evolution says that genes which outreproduce their alternative alleles increase in frequency within a gene pool. It is entirely possible for genes which \"harm\" the species to outcompete their alternatives in this way - indeed, it is entirely possible for a species to *evolve to extinction*.\n*   [The Tragedy of Group Selectionism](https://lessestwrong.com/lw/kw/the_tragedy_of_group_selectionism/) \\- Describes a key case where some pre-1960s evolutionary biologists went wrong by [anthropomorphizing](https://wiki.lesswrong.com/wiki/anthropomorphizing) evolution - in particular, Wynne-Edwards, Allee, and Brereton among others believed that predators would voluntarily restrain their breeding to avoid overpopulating their habitat. Since evolution does not usually do this sort of thing, their rationale was [group selection](https://lessestwrong.com/tag/group-selection) \\- populations that did this would survive better. But group selection is extremely difficult to make work mathematically, and an experiment under sufficiently extreme conditions to permit group selection, had rather different results.\n*   [Fake Optimization Criteria](https://lessestwrong.com/lw/kz/fake_optimization_criteria/) \\- Why study evolution? For one thing - it lets us see an alien [optimization process](https://lessestwrong.com/tag/optimization) up close - lets us see the *real* consequence of optimizing *strictly* for an alien optimization criterion like inclusive genetic fitness. Humans, who try to persuade other humans to do things their way, think that this policy criterion ought to require predators to [restrain their breeding](https://lessestwrong.com/tag/group-selection) to live in harmony with prey; the true result is something that humans find less aesthetic.\n*   [Beware of Stephen J. Gould](https://lessestwrong.com/lw/kv/beware_of_stephen_j_gould/) \\- A lot of people have gotten their grasp of evolutionary theory from Stephen J. Gould, a man who committed the moral equivalent of fraud in a way that is difficult to explain. At any rate, he severely misrepresented what evolutionary biologists believe, in the course of pretending to attack certain beliefs. One needs to clear from memory, as much as possible, not just everything that Gould positively stated but everything he seemed to imply the mainstream theory believed.\n*   [Conjuring An Evolution To Serve You](https://lessestwrong.com/lw/l8/conjuring_an_evolution_to_serve_you/) \\- If you take the hens who lay the most eggs in each generation, and breed from them, you should get hens who lay more and more eggs. Sounds logical, right? But this selection may actually favor the most *dominant* hen, that pecked its way to the top of the pecking order at the expense of other hens. Such breeding programs produce hens that must be housed in individual cages, or they will peck each other to death. Jeff Skilling of Enron fancied himself an evolution-conjurer - summoning *the awesome power of evolution* to work for him - and so, every year, every Enron employee's performance would be evaluated, and the bottom 10% would get fired, and the top performers would get huge raises and bonuses."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "4R8JYu4QF2FqzJxE5",
    "name": "Heuristics & Biases",
    "core": null,
    "slug": "heuristics-and-biases",
    "oldSlugs": [
      "heuristics-and-biases"
    ],
    "postCount": 186,
    "description": {
      "markdown": "**Heuristics** and **Biases** are the ways human reasoning differs from a theoretical ideal agent, due to reasoning shortcuts that don't always work (heuristics) and systematic errors (biases).\n\n*See also*: [Affect Heuristic](https://www.lesswrong.com/tag/affect-heuristic?showPostCount=true&useTagName=true), [Confirmation Bias](https://www.lesswrong.com/tag/confirmation-bias), [Fallacies](https://www.lesswrong.com/tag/fallacies), [Predictably Wrong](https://www.lesswrong.com/s/5g5TkQTe9rmPS5vvM), [Rationality](https://www.lesswrong.com/tag/rationality?showPostCount=true), [Your Intuitions Are Not Magic](https://www.lesswrong.com/posts/Psp8ZpYLCDJjshpRb/your-intuitions-are-not-magic), [Bias](https://lessestwrong.com/tag/bias), [Heuristic](https://lessestwrong.com/tag/heuristic)\n\nBasics\n======\n\n[“Cognitive biases”](https://www.lesswrong.com/posts/jnZbHi873v9vcpGpZ/what-s-a-bias-again) are those obstacles to truth which are produced, not by the cost of information, nor by limited computing power, but by *the shape of our own mental machinery*. For example, our mental processes might be evolutionarily adapted to specifically believe some things that arent true, so that we could win political arguments in a tribal context. Or the mental machinery might be adapted not to particularly care whether something is true, such as when we feel the urge to believe what others believe to get along socially. Or the bias may be a side-effect of a useful reasoning heuristic. The availability heuristic is not itself a bias, but it gives rise to them; the machinery uses an algorithm (give things more evidential weight if they come to mind more readily) that does some good cognitive work but also produces systematic errors.\n\nOur brains are doing something wrong, and after a lot of experimentation and/or heavy thinking, someone identifies the problem verbally and concretely; then we call it a “(cognitive) bias.” Not to be confused with the colloquial “that person is biased,” which just means “that person has a skewed or prejudiced attitude toward something.”\n\nA bias is an obstacle to our goal of obtaining truth, and thus *in our way*.\n\nWe are here to pursue the great human quest for truth: for we have desperate need of the knowledge, and besides, we're curious. To this end let us strive to overcome whatever obstacles lie in our way, whether we call them “biases” or not.\n\n[It's also useful to know the kinds of faults human brains are prone to, in the same way it's useful to know that your car's brakes are a little gummy (so you don't sail through a red light and into an 18-wheeler).](https://www.lesswrong.com/posts/Psp8ZpYLCDJjshpRb/your-intuitions-are-not-magic)\n\nThe Sequence, [Predictably Wrong](https://www.lesswrong.com/s/5g5TkQTe9rmPS5vvM), offers an excellent introduction to the topic for those who are not familiar.\n\nWait a minute... fallacies, biases, heuristics... what's the difference??\n=========================================================================\n\nWhile a **bias** is always wrong, a **heuristic** is just a shortcut which may or may not give you an accurate answer. Just because you know complex mathematical methods for precisely calculating the flight of objects through space doesn't mean you should be using them to play volleyball. Which is to say, heuristics are necessary for actually getting anything done. But because they are just approximations they frequently *produce* biases, which is where the problem lies. \"Fallacy\" is often used to mean a very similar thing as bias on LessWrong. \\[Needs better clarification\\]\n\nA good example of a heuristic is the [affect heuristic](https://www.lesswrong.com/tag/affect-heuristic?showPostCount=true&useTagName=true)\\-\\- people tend to guess unknown traits about people or things *based on* the perceived goodness of badness of known traits. In some circumstances this is a useful shortcut-- you may like to assume, for instance, that people who are good singers are more likely to be good dancers, too. However, it also frequently produces (unconscious) biases-- a bias towards believing that people who are tall and good looking have better moral character, for instance.\n\nSo if I learn all the biases, I can conquer the world with my superior intellect?\n=================================================================================\n\nWell, no. If it were that easy we wouldn't need a community initially dedicated to overcoming bias (the name of [the blog which this website grew out of](https://www.overcomingbias.com/)). Unfortunately, [learning about a bias alone doesn't seem to improve your ability to avoid it in real life](https://www.iejme.com/article/university-students-knowledge-and-biases-in-conditional-probability-reasoning). There's also the (major) issue that [knowing about biases can hurt people](https://www.lesswrong.com/posts/AdYdLP2sRqPMoe8fb/knowing-about-biases-can-hurt-people). Instead of being purely focused on removing negative habits, there is now a major focus at LessWrong to implementing [positive habits](https://www.lesswrong.com/tag/techniques?showPostCount=false&useTagName=false). These are skills such as how to update (change your mind) the correct amount in response to evidence, how to resolve disagreements with others, how to introspect, and many more."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "L3NcKBNTvQaFXwv9u",
    "name": "Paradoxes",
    "core": false,
    "slug": "paradoxes",
    "oldSlugs": null,
    "postCount": 48,
    "description": {
      "markdown": "**Paradoxes** are circumstances in mathematics, philosophy, or other domains that present a seeming contradiction, which may be difficult to resolve.\n\n**Related:** [Sleeping Beauty Paradox](https://www.lesswrong.com/tag/sleeping-beauty-paradox)**,** [Great Filter](https://www.lesswrong.com/tag/great-filter), [Newcomb's Problem](https://www.lesswrong.com/tag/newcomb-s-problem),  [Exercises / Problem-Sets](https://www.lesswrong.com/tag/exercises-problem-sets), [Free Will](https://www.lesswrong.com/tag/free-will)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "cpBfacd22cJsm5fuL",
    "name": "Hypocrisy",
    "core": false,
    "slug": "hypocrisy",
    "oldSlugs": null,
    "postCount": 17,
    "description": {
      "markdown": "Formally, **hypocrisy** is the act of claiming to motives, morals and standards one does not possess. Informally, it refers to not living up to the standards that one espouses, whether or not one sincerely believes those standards.\n\nBlog posts\n----------\n\n*   [Resolving Your Hypocrisy](http://www.overcomingbias.com/2006/12/resolving_your_.html) by [Robin Hanson](https://lessestwrong.com/tag/robin-hanson)\n*   [Self-deception: Hypocrisy or Akrasia?](https://lessestwrong.com/lw/h7/selfdeception_hypocrisy_or_akrasia/)\n\nSee also\n--------\n\n*   [Self-deception](https://lessestwrong.com/tag/self-deception)\n*   [Motivated Reasoning](http://lesswrong.com/tag/motivated-reasoning)."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "4Kcm4etxAJjmeDkHP",
    "name": "Book Reviews",
    "core": null,
    "slug": "book-reviews",
    "oldSlugs": null,
    "postCount": 301,
    "description": {
      "markdown": "**Book Reviews** on LessWrong are different from normal book reviews; they summarize and respond to a book's core ideas first, and judge whether you should read it second. A good book review sometimes distills the book's ideas so well that you no longer need to read the book.\n\nReviews engage with the perspective of the author, someone who has put in the effort to record their understanding of the world. Some of the best essays on LessWrong are reviews that teach us about [history](https://www.lesswrong.com/tag/history), [psychology](https://www.lesswrong.com/tag/replication-crisis), [biology](https://www.lesswrong.com/tag/biology), or some other area where the author has developed a detailed understanding of a phenomena that few others have ever reached, and the essay writer engages with that perspective from our rationalist perspective.\n\nGood book reviews embody the virtues of [scholarship](https://www.lesswrong.com/tag/scholarship-and-learning), [curiosity](https://www.lesswrong.com/tag/curiosity) and perspective-taking.\n\n**Related Pages:** [Epistemic Review](https://www.lesswrong.com/tag/epistemic-review), [Summaries](https://www.lesswrong.com/tag/summaries), [Literature Reviews](https://www.lesswrong.com/tag/literature-reviews), [LessWrong Review](https://www.lesswrong.com/tag/lesswrong-review)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "KWFhr6A2dHEb6wmWJ",
    "name": "Compartmentalization",
    "core": null,
    "slug": "compartmentalization",
    "oldSlugs": null,
    "postCount": 15,
    "description": {
      "markdown": "**Compartmentalization** is keeping information and processes within your mind segregated, especially in ways that keep knowledge possessed by some of your reasoning processes being accessed by other processes.\n\nFrom an alternative angle, one can think of compartmentalizing one's different activities or domains from each other. when one couple the skills or habits from one to another, e.g., the religious scientist who does not apply scientific thinking outside the lab. \n\nOne might even have excellent epistemological performance in one domain and terrible performance in others.\n\nSee also\n--------\n\n*   [Semantic stopsign](https://www.lesswrong.com/tag/semantic-stopsign), [Anti-epistemology](https://www.lesswrong.com/tag/anti-epistemology)\n*   [Cached thought](https://www.lesswrong.com/tag/cached-thought)\n*   [Shut up and multiply](https://www.lesswrong.com/tag/shut-up-and-multiply), [Bite the bullet](https://www.lesswrong.com/tag/bite-the-bullet), [Absurdity heuristic](https://www.lesswrong.com/tag/absurdity-heuristic)\n*   [Dangerous knowledge](https://www.lesswrong.com/tag/dangerous-knowledge)\n*   [General knowledge](https://www.lesswrong.com/tag/general-knowledge), [Understanding](https://www.lesswrong.com/tag/understanding)\n*   [Alief](https://www.lesswrong.com/tag/alief)\n*   [Ugh field](https://www.lesswrong.com/tag/aversion-ugh-fields)\n*   [Distinctions](https://www.lesswrong.com/tag/distinctions)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "yEs5Tdwfw5Zw8yGWC",
    "name": "Wireheading",
    "core": false,
    "slug": "wireheading",
    "oldSlugs": null,
    "postCount": 27,
    "description": {
      "markdown": "**Wireheading** is the artificial stimulation of the brain to experience pleasure, usually through the direct stimulation of an individual's brain's reward or pleasure center with electrical current. It can also be used in a more expanded sense, to refer to any kind of method that produces a form of *counterfeit utility* by directly maximizing a good feeling, but that fails to realize what we value.\n\n**Related pages:**  [Complexity of Value](https://www.lesswrong.com/tag/complexity-of-value),  [Goodhart's Law](https://www.lesswrong.com/tag/goodhart-s-law), [Inner Alignment](https://www.lesswrong.com/tag/inner-alignment)\n\nIn both thought experiments and [laboratory experiments](http://www.mindhacks.com/blog/2008/09/erotic_selfstimulat.html) direct stimulation of the brain’s reward center makes the individual feel happy. In theory, wireheading with a powerful enough current would be the most pleasurable experience imaginable. There is some evidence that [reward is distinct from pleasure](https://lessestwrong.com/lw/1lb/are_wireheads_happy/), and that most currently hypothesized forms of wireheading just motivate a person to continue the wirehead experience, not to feel happy. However, there seems to be no reason to believe that a different form of wireheading which does create subjective pleasure could not be found. The possibility of wireheading raises difficult ethical questions for [those who believe that morality is based on human happiness](https://lessestwrong.com/tag/hedonism). A civilization of wireheads \"blissing out\" all day while being fed and maintained by robots would be a state of maximum happiness, but such a civilization would have no art, love, scientific discovery, or any of the other things humans find valuable.\n\nIf we take wireheading as a more general form of producing counterfeit utility, there are many examples of ways of directly stimulating of the reward and pleasure centers of the brain, without actually engaging in valuable experiences. Cocaine, heroin, cigarettes and gambling are all examples of current methods of directly achieving pleasure or reward, but can be seen by many as lacking much of what we value and are potentially extremely detrimental. [Steve Omohundro](https://en.wikipedia.org/wiki/Steve_Omohundro) argues[1](http://selfawaresystems.files.wordpress.com/2008/01/ai_drives_final.pdf) that: “An important class of vulnerabilities arises when the subsystems for measuring utility become corrupted. Human pleasure may be thought of as the experiential correlate of an assessment of high utility. But pleasure is mediated by neurochemicals and these are subject to manipulation.”\n\nWireheading is also an illustration of the complexities of creating a [Friendly AI](https://wiki.lesswrong.com/wiki/Friendly_AI). Any AGI naively programmed to increase human happiness could devote its energies to wireheading people, possibly without their consent, in preference to any other goals. Equivalent problems arise for any simple attempt to create AGIs who care directly about human feelings (\"love\", \"compassion\", \"excitement\", etc). An AGI could wirehead people to feel in love all the time, but this wouldn’t correctly realize what we value when we say love is a virtue. For Omohundro, because exploiting those vulnerabilities in our subsystems for measuring utility is much easier than truly realizing our values, a wrongly designed AGI would most certainly prefer to wirehead humanity instead of pursuing human values. In addition, an AGI itself could be vulnerable to wirehead and would need to implement “police forces” or “immune systems” to ensure its measuring system doesn’t become corrupted by trying to produce counterfeit utility.\n\nSee also\n--------\n\n*   [Steve Omohundro's paper, section 4 deals with vulnerabilities to counterfeit utility and wireheading](http://selfawaresystems.files.wordpress.com/2008/01/ai_drives_final.pdf)\n*   [Hedonism](https://lessestwrong.com/tag/hedonism)\n*   [Complexity of value](https://lessestwrong.com/tag/complexity-of-value)\n*   [Wanting and liking](https://lessestwrong.com/tag/wanting-and-liking)\n*   [Near/far thinking](https://lessestwrong.com/tag/near-far-thinking)\n*   [Hedonium](https://wiki.lesswrong.com/wiki/Hedonium)\n*   [Abolitionism](https://lessestwrong.com/tag/abolitionism)\n\nExternal links\n--------------\n\n*   [Wirehead Hedonism versus paradise engineering](https://www.hedweb.com/wirehead/index.html) by David Pearce"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "Q9ASuEEoJWxT3RLMT",
    "name": "Animal Welfare",
    "core": false,
    "slug": "animal-welfare",
    "oldSlugs": null,
    "postCount": 44,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "dPPATLhRmhdJtJM2t",
    "name": "Decision Theory",
    "core": null,
    "slug": "decision-theory",
    "oldSlugs": null,
    "postCount": 276,
    "description": {
      "markdown": "**Decision theory** is the study of principles and algorithms for making correct decisions—that is, decisions that allow an agent to achieve better outcomes with respect to its goals. Every action at least implicitly represents a decision under uncertainty: in a state of partial knowledge, something has to be done, even if that something turns out to be nothing (call it \"the null action\"). Even if you don't know how you make decisions, decisions do get made, and so there has to be some underlying mechanism. What is it? And how can it be done better? Decision theory has the answers.\n\n*Note: this page needs to be updated with content regarding Functional Decision Theory, the latest theory from MIRI.*\n\n*Related:* [Game Theory](https://www.lesswrong.com/tag/game-theory?showPostCount=true&useTagName=true), [Robust Agents](https://www.lesswrong.com/tag/robust-agents?showPostCount=true&useTagName=true), [Utility Functions](https://www.lesswrong.com/tag/utility-functions?showPostCount=true&useTagName=true)\n\nA core idea in decision theory is that of [*expected utility*](https://lessestwrong.com/tag/expected-utility) *maximization*, usually intractable to directly calculate in practice, but an invaluable theoretical concept. An agent assigns utility to every possible outcome: a real number representing the goodness or desirability of that outcome. The mapping of outcomes to utilities is called the agent's *utility function*. (The utility function is said to be invariant under affine transformations: that is, the utilities can be scaled or translated by a constant while resulting in all the same decisions.) For every action that the agent could take, sum over the utilities of the various possible outcomes weighted by their probability: this is the [expected](https://lessestwrong.com/tag/expected-value) utility of the action, and the action with the highest expected utility is to be chosen.\n\nThought experiments\n-------------------\n\nThe limitations and pathologies of decision theories can be analyzed by considering the decisions they suggest in the certain idealized situations that stretch the limits of decision theory's applicability. Some of the thought experiments more frequently discussed on [LW](https://wiki.lesswrong.com/wiki/LW) include:\n\n*   [Newcomb's problem](https://lessestwrong.com/tag/newcomb-s-problem)\n*   [Counterfactual mugging](https://lessestwrong.com/tag/counterfactual-mugging)\n*   [Parfit's hitchhiker](https://wiki.lesswrong.com/wiki/Parfit's_hitchhiker)\n*   [Smoker's lesion](https://wiki.lesswrong.com/wiki/Smoker's_lesion)\n*   [Absentminded driver](https://wiki.lesswrong.com/wiki/Absentminded_driver)\n*   [Sleeping Beauty problem](https://lessestwrong.com/tag/sleeping-beauty-paradox)\n*   [Prisoner's dilemma](https://lessestwrong.com/tag/prisoner-s-dilemma)\n*   [Pascal's mugging](https://lessestwrong.com/tag/pascal-s-mugging)\n\nCommonly discussed decision theories\n------------------------------------\n\nStandard theories well-known in academia:\n\n*   CDT, [Causal Decision Theory](http://en.wikipedia.org/wiki/Causal_decision_theory)\n*   EDT, [Evidential Decision Theory](http://en.wikipedia.org/wiki/Evidential_decision_theory)\n\nTheories invented by researchers associated with [MIRI](https://wiki.lesswrong.com/wiki/MIRI) and LW:\n\n*   FDT: [Functional Decision Theory](https://intelligence.org/2017/10/22/fdt/)\n*   TDT, [Timeless Decision Theory](https://lessestwrong.com/tag/timeless-decision-theory)\n*   UDT, [Updateless Decision Theory](https://lessestwrong.com/tag/updateless-decision-theory)\n*   ADT: [Ambient Decision Theory](https://lessestwrong.com/tag/ambient-decision-theory) (a variant of UDT)\n*   FDT: [Cheating Death in Damascus](https://intelligence.org/files/DeathInDamascus.pdf)\n\nOther decision theories are listed in [A comprehensive list of decision theories](https://casparoesterheld.com/a-comprehensive-list-of-decision-theories/).\n\nBlog posts\n----------\n\n*   [Terminal Values and Instrumental Values](https://lessestwrong.com/lw/l4/terminal_values_and_instrumental_values/)\n*   [Decision Theories: A Less Wrong Primer](https://lessestwrong.com/lw/aq9/decision_theories_a_less_wrong_primer/) by orthonormal\n*   [Decision Theory FAQ](https://lessestwrong.com/lw/gu1/decision_theory_faq/) by lukeprog and crazy88\n\nSequence by [AnnaSalamon](https://wiki.lesswrong.com/wiki/AnnaSalamon)\n----------------------------------------------------------------------\n\n*   [Decision theory: An outline of some upcoming posts](https://lessestwrong.com/lw/16f/decision_theory_an_outline_of_some_upcoming_posts/)\n*   [Confusion about Newcomb is confusion about counterfactuals](https://lessestwrong.com/lw/16i/confusion_about_newcomb_is_confusion_about/)\n*   [Why we need to reduce “could”, “would”, “should”](https://lessestwrong.com/lw/174/decision_theory_why_we_need_to_reduce_could_would/)\n*   [Why Pearl helps reduce “could” and “would”, but still leaves us with at least three alternatives](https://lessestwrong.com/lw/17b/decision_theory_why_pearl_helps_reduce_could_and/)\n\nSequence by [orthonormal](http://lesswrong.com/user/orthonormal/) (Decision Theories: A Semi-Formal Analysis)\n-------------------------------------------------------------------------------------------------------------\n\n*   [Part 0: Decision Theories: A Less Wrong Primer](https://lessestwrong.com/lw/aq9/decision_theories_a_less_wrong_primer/)\n*   [Part I: The Problem with Naive Decision Theory](https://lessestwrong.com/lw/axl/decision_theories_a_semiformal_analysis_part_i/)\n*   [Part II: Causal Decision Theory and Substitution](https://lessestwrong.com/lw/az6/decision_theories_a_semiformal_analysis_part_ii/)\n*   [Part III: Formalizing Timeless Decision Theory](https://lessestwrong.com/lw/b7w/decision_theories_a_semiformal_analysis_part_iii/)\n\nSee also\n--------\n\n*   [Instrumental rationality](https://wiki.lesswrong.com/wiki/Instrumental_rationality)\n*   [Causality](https://lessestwrong.com/tag/causality)\n*   [Expected utility](https://lessestwrong.com/tag/expected-utility)\n*   [Evidential Decision Theory](https://lessestwrong.com/tag/evidential-decision-theory)\n*   [Timeless decision theory](https://lessestwrong.com/tag/timeless-decision-theory), [Updateless decision theory](https://lessestwrong.com/tag/updateless-decision-theory)\n*   [AIXI](https://lessestwrong.com/tag/aixi)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "NLwTnsH9RSotqXYLw",
    "name": "Value Learning",
    "core": null,
    "slug": "value-learning",
    "oldSlugs": null,
    "postCount": 120,
    "description": {
      "markdown": "**Value learning** is a proposed method for incorporating human values in an [AGI](https://wiki.lesswrong.com/wiki/AGI). It involves the creation of an artificial learner whose actions consider many possible set of values and preferences, weighed by their likelihood. Value learning could prevent an AGI of having goals detrimental to human values, hence helping in the creation of [Friendly AI](https://wiki.lesswrong.com/wiki/Friendly_AI).\n\nAlthough there are many ways to incorporate human values in an AGI (e.g.: [Coherent Extrapolated Volition](https://lessestwrong.com/tag/coherent-extrapolated-volition), [Coherent Aggregated Volition](https://lessestwrong.com/tag/coherent-aggregated-volition) and [Coherent Blended Volition](https://lessestwrong.com/tag/coherent-blended-volition)), this method is directly mentioned and developed in [Daniel Dewey’s](http://www.futuretech.ox.ac.uk/daniel-dewey) paper [‘Learning What to Value’](http://www.danieldewey.net/learning-what-to-value.pdf). Like most authors, he assumes that human’s goals would not naturally occur in an artificial agent and should be enforced in it. First, Dewey argues against the use of a simple use of [reinforcement learning](https://lessestwrong.com/tag/reinforcement-learning) to solve this problem, on the basis that this lead to the maximization of specific rewards that can diverge from value maximization. For example, even if we forcefully engineer the agent to maximize those rewards that also maximize human values, the agent could alter its environment to more easily produce those same rewards without the trouble of also maximizing human values (i.e.: if the reward was human happiness it could alter the human mind so it became happy with anything).\n\nTo solve all these problems, Dewey proposes a [utility function](https://lessestwrong.com/tag/utility-functions) maximizer, who considers all possible utility functions weighted by their probabilities: \"\\[W\\]e propose uncertainty over utility functions. Instead of providing an agent one utility function up front, we provide an agent with a pool of possible utility functions and a probability distribution P such that each utility function can be assigned probability P(Ujyxm) given a particular interaction history \\[yxm\\]. An agent can then calculate an expected value over possible utility functions given a particular interaction history\" He concludes saying that although it solves many of the mentioned problems, this method still leaves many open questions. However it should provide a direction for future work.\n\nReferences\n----------\n\n*   [Dewey’s paper](http://www.danieldewey.net/learning-what-to-value.pdf)\n\nSee Also\n--------\n\n*   [Friendly AI](https://wiki.lesswrong.com/wiki/Friendly_AI)\n*   [Reinforcement learning](https://lessestwrong.com/tag/reinforcement-learning)\n*   [Value extrapolation](https://lessestwrong.com/tag/value-extrapolation)\n*   [Complexity of value](https://lessestwrong.com/tag/complexity-of-value)\n*   [Coherent Extrapolated Volition](https://lessestwrong.com/tag/coherent-extrapolated-volition)\n*   [Coherent Aggregated Volition](https://lessestwrong.com/tag/coherent-aggregated-volition)\n*   [Coherent Blended Volition](https://lessestwrong.com/tag/coherent-blended-volition)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "cq69M9ceLNA35ShTR",
    "name": "Causality",
    "core": null,
    "slug": "causality",
    "oldSlugs": [
      "causality",
      "causality-and-causal-models"
    ],
    "postCount": 87,
    "description": {
      "markdown": "**Causality** is the intuitive notion that some events happening \"result\" in other events happening. What's going on with that? What does it mean for A to cause B? How do we figure out the causal relationship between things?\n\nSee also\n--------\n\n*   [Free will](https://www.lesswrong.com/tag/free-will)\n*   [Teleology](https://www.lesswrong.com/tag/teleology)\n*   [Beliefs require observations](https://www.lesswrong.com/tag/beliefs-require-observations)\n\nExternal links\n--------------\n\n*   [Philip Dawid's explication of Pearl's model, and two ways of thinking about nonrandom sampling](http://www.stat.columbia.edu/~cook/movabletype/archives/2009/07/philip_dawids_t.html) by [Philip Dawid](https://en.wikipedia.org/wiki/Philip_Dawid) and [Andrew Gelman](http://andrewgelman.com/) \\- Causal inference as \"the task of using data collected under one regime to infer about the properties of another\".\n*   [Resolving disputes between J. Pearl and D. Rubin on causal inference](http://www.stat.columbia.edu/~cook/movabletype/archives/2009/07/disputes_about.html) and [More on Pearl's and Rubin's frameworks for causal inference](http://www.stat.columbia.edu/~cook/movabletype/archives/2009/07/more_on_pearlru.html) by Andrew Gelman\n*   [If correlation doesn't imply causation, then what does?](http://www.michaelnielsen.org/ddi/if-correlation-doesnt-imply-causation-then-what-does/) by [Michael Nielsen](https://en.wikipedia.org/wiki/Michael_Nielsen)\n*   [Correlation is Evidence of Causation](http://oyhus.no/CorrelationAndCausation.html) by Kim Øyhus\n*   Judea Pearls's works: [*Causality: Models, Reasoning, and Inference*](http://bayes.cs.ucla.edu/BOOK-2K/)*,* [Book of Why, A Primer on Causality](https://www.goodreads.com/book/show/36204378-the-book-of-why)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "33BrBRSrRQS4jEHdk",
    "name": "Forecasts (Specific Predictions)",
    "core": false,
    "slug": "forecasts-specific-predictions",
    "oldSlugs": [
      "forecasts",
      "specific-forecasts",
      "concrete-forecasts",
      "forecasts-lists-of"
    ],
    "postCount": 83,
    "description": {
      "markdown": "**Forecasts (Specific Predictions)**.  This tag is for specific predictions about unknown facts or future events. Discussion of the practice of making forecasts can be found in [Forecasting and Prediction](https://www.lesswrong.com/tag/forecasting-and-prediction). Related: [Betting](https://www.lesswrong.com/tag/betting).\n\n> *Above all, don’t ask what to believe—ask what to anticipate. Every question of belief should flow from a question of anticipation, and that question of anticipation should be the center of the inquiry.  –* [*Making Beliefs Pay Rent*](https://www.lesswrong.com/posts/a7n8GdKiAZRX86T5A/making-beliefs-pay-rent-in-anticipated-experiences)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "jaf5zfcGgCB2REXGw",
    "name": "Biology",
    "core": null,
    "slug": "biology",
    "oldSlugs": null,
    "postCount": 109,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "sHbKQDqrSinRPcnBv",
    "name": "Information Cascades",
    "core": false,
    "slug": "information-cascades",
    "oldSlugs": null,
    "postCount": 16,
    "description": {
      "markdown": "An **information cascade** occurs when people update on other people's beliefs, which may individually be a [rational decision](https://www.lesswrong.com/tag/aumann-s-agreement-theorem) but may still result in a self-reinforcing community opinion that does not necessarily reflect reality.\n\nSee Also\n--------\n\n*   [Groupthink](/tag/groupthink)\n*   [Egalitarianism](https://lessestwrong.com/tag/egalitarianism), [Modesty argument](https://lessestwrong.com/tag/modesty-argument)\n*   [Epistemic luck](https://lessestwrong.com/tag/epistemic-luck), [Privileging the hypothesis](https://lessestwrong.com/tag/privileging-the-hypothesis)\n*   [Free-floating belief](https://lessestwrong.com/tag/free-floating-belief)\n*   [Groupthink](https://lessestwrong.com/tag/groupthink), [Affective death spiral](https://lessestwrong.com/tag/affective-death-spiral)\n*   [Goodhart's law](https://lessestwrong.com/tag/goodhart-s-law)\n*   [Religion](https://lessestwrong.com/tag/religion)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "mip7tdAN87Jarkcew",
    "name": "Relationships (Interpersonal)",
    "core": false,
    "slug": "relationships-interpersonal",
    "oldSlugs": [
      "relationships-all-kinds",
      "interpersonal-relationships"
    ],
    "postCount": 138,
    "description": {
      "markdown": "**Interpersonal Relationships** includes all forms of sustained interaction between people. This topic includes any discussion relating to friendship, romantic relationships, family relationships, business relationships, and so on.\n\nRelated:\n\n*   Communication\n*   Communication Cultures\n*   Circling"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "8daMDi9NEShyLqxth",
    "name": "Forecasting & Prediction",
    "core": null,
    "slug": "forecasting-and-prediction",
    "oldSlugs": null,
    "postCount": 237,
    "description": {
      "markdown": "**Forecasting** or **Predicting** is the act of making statements about what will happen in the future (and in some cases, the past) and then scoring the predictions. Posts market with this tag is for discussion of the practice, skill, and methodology of forecasting. Posts exclusively containing object-level lists of forecasts and predictions are in [Forecasts](https://www.lesswrong.com/tag/forecasts).  Related: [Betting](https://www.lesswrong.com/tag/betting).\n\n> *Above all, don’t ask what to believe—ask what to anticipate. Every question of belief should flow from a question of anticipation, and that question of anticipation should be the center of the inquiry.  –* [*Making Beliefs Pay Rent*](https://www.lesswrong.com/posts/a7n8GdKiAZRX86T5A/making-beliefs-pay-rent-in-anticipated-experiences)\n\nForecasting allows individuals and institutions to test their internal models of reality. A good forecaster can have confidence in future predictions and hence actions in the same area as they have a good track record in. Organisations with decision-makers with good track records can likewise be more confident in their choices.\n\nHow to get started in forecasting (many ways)\n---------------------------------------------\n\n*   Just do it - go on [Metaculus](https://www.metaculus.com/questions/) and forecast for 30 minutes. Look back in a month and see how things have changed\n*   Read a book - [Superforecasting](https://www.amazon.com/Superforecasting-audiobook/dp/B0131HGPQQ/ref=sr_1_1?crid=EIN4B0NE83W2&keywords=superforecasting&qid=1660197471&sprefix=superforecastin%2Caps%2C389&sr=8-1) by Phil Tetlock, one of the founding books in forecasting\n*   Watch a video - Alex Lawson's [Intro to Forecasting](https://www.youtube.com/watch?v=e6Q7Ez3PkOw) videos\n\nBasic forecasting tips\n----------------------\n\n*   5-second forecast - Just go with your gut. You'll probably be badly calibrated but you will learn than\n*   Longer forecast\n    *   Base rates\n    *   Adjust up or down\n    *   Look at the median\n    *   Adjust again\n\nBottlenecks\n-----------\n\n*   Understanding key decisions that decision-makers wants decisions about\n*   Writing high-quality questions and getting them published as forecasting question or prediction markets\n*   Getting enough forecasts\n    *   Getting enough forecasters on forecasting questions\n    *   Getting enough money in prediction markets\n*   Getting forecasts seen by relevant decisionmakers\n\nOrganisations in the space\n--------------------------\n\nPrediction markets/aggregators\n\n*   [Metaculus](https://www.metaculus.com/questions/), a prediction aggregator\n*   [Manifold Markets](https://manifold.markets/home), a fake-money prediction market where anyone can create a market\n*   [PredictionBook](http://predictionbook.com/), a website that keeps track of predictions and calibration levels\n*   [Kalshi](https://kalshi.com/), the only US prediction market which can run policy markets over $500 dollars of total exposure\n*   [Polymarket](https://polymarket.com/), a large crypo prediction market\n\nMeta forecasting orgs \n\n*   [QURI](https://www.lesswrong.com/tag/quri), \n\nIndividuals publicly attached to the space\n------------------------------------------\n\n\\[not sure how to list\\] \n\nSee also\n--------\n\n*   [Antiprediction](https://lessestwrong.com/tag/antiprediction)\n*   [Making beliefs pay rent](https://lessestwrong.com/tag/making-beliefs-pay-rent)\n*   [Prediction market](https://lessestwrong.com/tag/prediction-markets)\n*   [Forecast](https://lessestwrong.com/tag/forecast)\n*   [Calibration](https://lessestwrong.com/tag/calibration)\n*   [Black swan](https://lessestwrong.com/tag/black-swans)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5SPDtxJT6y6ZTXHBJ",
    "name": "Simulacrum Levels",
    "core": false,
    "slug": "simulacrum-levels",
    "oldSlugs": null,
    "postCount": 26,
    "description": {
      "markdown": "**Simulacrum Levels** are a framework for analyzing different motivations people can have for making statements.\n\n*   Simulacrum Level 1: Attempt to describe the world accurately.\n*   Simulacrum Level 2: Choose what to say based on what your statement will cause other people to do or believe.\n*   Simulacrum Level 3: Say things that [signal](https://www.lesswrong.com/tag/signaling) membership to your ingroup.\n*   Simulacrum Level 4: Choose which group to signal membership to based on what the benefit would be for you.\n\nMore descriptions of the four levels:\n-------------------------------------\n\nBy [Strawperson](https://www.lesswrong.com/posts/fEX7G2N7CtmZQ3eB5/simulacra-and-subjectivity?commentId=FgajiMrSpY9MxTS8b):\n\n*   Level 1: “There’s a lion across the river.” = There’s a lion across the river.\n*   Level 2: “There’s a lion across the river.” = I don’t want to go (or have other people go) across the river.\n*   Level 3: “There’s a lion across the river.” = I’m with the popular kids who are too cool to go across the river.\n*   Level 4: “There’s a lion across the river.” = A firm stance against trans-river expansionism focus grouped well with undecided voters in my constituency.\n\nBy [Zvi](https://www.lesswrong.com/posts/QdppEcbhLTZqDDtDa/unifying-the-simulacra-definitions):\n\n*   Level 1: Symbols describe reality.\n*   Level 2: Symbols pretend to describe reality.\n*   Level 3: Symbols pretend to pretend to describe reality.\n*   Level 4: Symbols need not pretend to describe reality.\n\nA concrete example of the above from Michael Vassar:\n\n*   Level 1: A court reflects justice.\n*   Level 2: A corrupt judge distorts justice.\n*   Level 3: A Soviet show trial conceals the absence of real Soviet courts.\n*   Level 4: A trial by ordeal or trial by combat lacks and denies the concept of justice entirely.\n\nZvi [describes](https://www.lesswrong.com/posts/v6NBpQc2AzWjzxJ4S/the-four-children-of-the-seder-as-the-simulacra-levels) the four children of the Seder (Passover) as the four (and one extra) simulacrum levels:\n\n*   Level 1: The wise child.\n*   Level 2: The wicked child.\n*   Level 3: The simple child.\n*   Level 4: The one who does not know how to ask.\n*   Level 5: The one who is not there.\n\n> \"The wicked understand, acknowledge and value the Wise—they depend on the Wise for their own cynical gain. The simple don’t see the point of wisdom. Those who do not know how to ask don’t even know wisdom is a thing.\" - [The Four Children of the Seder as the Simulacra Levels](https://www.lesswrong.com/posts/v6NBpQc2AzWjzxJ4S/the-four-children-of-the-seder-as-the-simulacra-levels)\n\n* * *\n\nThe origin of this framework is in [Simulacra and Simulation](https://en.wikipedia.org/wiki/Simulacra_and_Simulation) by sociologist [Jean Baudrillard](https://en.wikipedia.org/wiki/Jean_Baudrillard)."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "2i3w84KCkqZzpnQ4d",
    "name": "Petrov Day",
    "core": null,
    "slug": "petrov-day",
    "oldSlugs": null,
    "postCount": 35,
    "description": {
      "markdown": "**Petrov Day** is a tradition celebrating Soviet military officer Stanislav Petrov, who played a key role in preventing a nuclear attack during a false alarm incident on September 26, 1983.\n\nSee also: [Existential Risk](https://www.lesswrong.com/tag/existential-risk), [Ritual](https://www.lesswrong.com/tag/ritual)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "vtozKm5BZ8gf6zd45",
    "name": "Secular Solstice",
    "core": null,
    "slug": "secular-solstice",
    "oldSlugs": [
      "solstice-celebration"
    ],
    "postCount": 52,
    "description": {
      "markdown": "The **Secular Solstice** is a holiday tradition invented by Less Wrong user Raymond Arnold. It is celebrated on-or-near December 21st, the winter solstice in the northern hemisphere. It is typically celebrated in large groups with a ceremony involving singing and storytelling."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "zCYXpx33wq8chGyEz",
    "name": "AI Boxing (Containment)",
    "core": null,
    "slug": "ai-boxing-containment",
    "oldSlugs": null,
    "postCount": 48,
    "description": {
      "markdown": "**AI Boxing** is attempts, experiments, or proposals to isolate (\"box\") a powerful AI (~AGI) where it can't interact with the world at large, save for limited communication with its human liaison. It is often proposed that so long as the AI is physically isolated and restricted, or \"boxed\", it will be harmless even if it is an [unfriendly artificial intelligence](https://www.lesswrong.com/tag/unfriendly-artificial-intelligence) (UAI).\n\nChallenges are: 1) can you successively prevent it from interacting with the world? And 2) can you prevent it from convincing you to let it out?\n\n**See also:** [AI](https://www.lesswrong.com/tag/ai), [AGI](https://wiki.lesswrong.com/wiki/AGI), [Oracle AI](https://www.lesswrong.com/tag/oracle-ai), [Tool AI](https://www.lesswrong.com/tag/tool-ai), [Unfriendly AI](https://wiki.lesswrong.com/wiki/Unfriendly_AI)\n\nEscaping the box\n================\n\nIt is not regarded as likely that an AGI can be boxed in the long term. Since the AGI might be a [superintelligence](https://www.lesswrong.com/tag/superintelligence), it could persuade someone (the human liaison, most likely) to free it from its box and thus, human control. Some practical ways of achieving this goal include:\n\n*   Offering enormous wealth, power and intelligence to its liberator\n*   Claiming that only it can prevent an [existential risk](https://www.lesswrong.com/tag/existential-risk)\n*   Claiming it needs outside resources to cure all diseases\n*   Predicting a real-world disaster (which then occurs), then claiming it could have been prevented had it been let out\n\nOther, more speculative ways include: threatening to torture millions of conscious copies of you for thousands of years, starting in exactly the same situation as in such a way that it seems overwhelmingly likely that [you are a simulation](https://www.lesswrong.com/tag/simulation-argument), or it might discover and exploit unknown physics to free itself.\n\nContaining the AGI\n==================\n\nAttempts to box an AGI may add some degree of safety to the development of a [friendly artificial intelligence](https://wiki.lesswrong.com/wiki/FAI) (FAI). A number of strategies for keeping an AGI in its box are discussed in [Thinking inside the box](http://www.aleph.se/papers/oracleAI.pdf) and [Leakproofing the Singularity](http://dl.dropbox.com/u/5317066/2012-yampolskiy.pdf). Among them are:\n\n*   Physically isolating the AGI and permitting it zero control of any machinery\n*   Limiting the AGI’s outputs and inputs with regards to humans\n*   Programming the AGI with deliberately convoluted logic or [homomorphically encrypting](http://en.wikipedia.org/wiki/Homomorphic_encryption) portions of it\n*   Periodic resets of the AGI's memory\n*   A virtual world between the real world and the AI, where its unfriendly intentions would be first revealed\n*   Motivational control using a variety of techniques\n*   Creating an [Oracle AI](https://www.lesswrong.com/tag/oracle-ai): an AI that only answers questions and isn't designed to interact with the world in any other way. But even the act of the AI putting strings of text in front of humans poses some risk.\n\nSimulations / Experiments\n=========================\n\nThe **AI Box Experiment** is a game meant to explore the possible pitfalls of AI boxing. It is played over text chat, with one human roleplaying as an AI in a box, and another human roleplaying as a gatekeeper with the ability to let the AI out of the box. The AI player wins if they successfully convince the gatekeeper to let them out of the box, and the gatekeeper wins if the AI player has not been freed after a certain period of time. \n\nBoth Eliezer Yudkowsky and Justin Corwin have ran simulations, pretending to be a [superintelligence](https://www.lesswrong.com/tag/superintelligence), and been able to convince a human playing a guard to let them out on many - but not all - occasions. Eliezer's five experiments required the guard to listen for at least two hours with participants who had approached him, while Corwin's 26 experiments had no time limit and subjects he approached.\n\nThe text of Eliezer's experiments have not been made public.\n\nList of experiments\n-------------------\n\n*   [The AI-Box Experiment](http://yudkowsky.net/singularity/aibox/) [Eliezer Yudkowsky's](https://www.lesswrong.com/tag/eliezer-yudkowsky) original two tests\n*   [Shut up and do the impossible!](https://www.lesswrong.com/lw/up/shut_up_and_do_the_impossible/), three other experiments Eliezer ran\n*   [AI Boxing](http://www.sl4.org/archive/0207/4935.html), 26 trials ran by Justin Corwin\n*   [AI Box Log](https://www.lesswrong.com/lw/9ld/ai_box_log/), a log of a trial between MileyCyrus and Dorikka\n\nReferences\n==========\n\n*   [Thinking inside the box: using and controlling an Oracle AI](http://www.aleph.se/papers/oracleAI.pdf) by Stuart Armstrong, Anders Sandberg, and Nick Bostrom\n*   [Leakproofing the Singularity: Artificial Intelligence Confinement Problem](http://dl.dropbox.com/u/5317066/2012-yampolskiy.pdf) by Roman V. Yampolskiy\n*   [On the Difficulty of AI Boxing](http://ordinaryideas.wordpress.com/2012/04/27/on-the-difficulty-of-ai-boxing/) by Paul Christiano\n*   [Cryptographic Boxes for Unfriendly AI](https://www.lesswrong.com/lw/3cz/cryptographic_boxes_for_unfriendly_ai/) by Paul Christiano\n*   [The Strangest Thing An AI Could Tell You](https://www.lesswrong.com/r/lesswrong/lw/12s/the_strangest_thing_an_ai_could_tell_you/)\n*   [The AI in a box boxes you](https://www.lesswrong.com/lw/1pz/ai_in_box_boxes_you/)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "kEX5CzbfiAzGn4q8B",
    "name": "Superstimuli",
    "core": false,
    "slug": "superstimuli",
    "oldSlugs": null,
    "postCount": 16,
    "description": {
      "markdown": "Humans evolved various desires that promoted survival and reproductive success in the [ancestral environment](https://www.psychologytoday.com/intl/blog/darwins-subterranean-world/201806/3-things-we-know-about-the-ancestral-environment). **Superstimuli** are modern inventions that satisfy desires better than anything in the ancestral environment could but are detrimental to survival, reproduction, or other high-level goals.\n\n*See also:* [Evolutionary Psychology](https://www.lesswrong.com/tag/evolutionary-psychology?showPostCount=true&useTagName=true), [Goodhart's Law](https://www.lesswrong.com/tag/goodhart-s-law?showPostCount=true&useTagName=true), [Wireheading](https://www.lesswrong.com/tag/wireheading?showPostCount=true&useTagName=true)\n\n> *A candy bar is a* superstimulus*: it contains more concentrated sugar, salt, and fat than anything that exists in the ancestral environment.   A candy bar matches taste buds that evolved in a hunter-gatherer environment, but it matches those taste buds much more strongly than anything that actually existed in the hunter-gatherer environment. The signal that once reliably correlated to healthy food has been hijacked, blotted out with a point in tastespace that wasn't in the training dataset - an impossibly distant outlier on the old ancestral graphs. *\n\n*\\-\\-* Eliezer Yudkowsky, [Superstimuli and the Collapse of Western Civilisation](https://www.lesswrong.com/posts/Jq73GozjsuhdwMLEG/superstimuli-and-the-collapse-of-western-civilization)\n\nNotable Posts\n-------------\n\n*   [Superstimuli and the Collapse of Western Civilization](https://lessestwrong.com/lw/h3/superstimuli_and_the_collapse_of_western/)\n*   [Adaptation-Executers, not Fitness-Maximizers](https://lessestwrong.com/lw/l0/adaptationexecuters_not_fitnessmaximizers/)\n\nExternal Links\n--------------\n\n*   [Cute, Sexy, Sweet, Funny](http://www.ted.com/talks/dan_dennett_cute_sexy_sweet_funny.html) by [Daniel Dennett](https://en.wikipedia.org/wiki/Daniel_Dennett) at TED\n\nSee Also\n--------\n\n*   [Akrasia](https://lessestwrong.com/tag/akrasia)\n*   [Adaptation executers](https://wiki.lesswrong.com/wiki/Adaptation_executers)\n*   [Evolutionary psychology](https://lessestwrong.com/tag/evolutionary-psychology)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "Kj9q8FXoauL7mQDWt",
    "name": "Affect Heuristic",
    "core": false,
    "slug": "affect-heuristic",
    "oldSlugs": null,
    "postCount": 15,
    "description": {
      "markdown": "The **affect heuristic** is a principle for making fast, perceptual judgments based on subjective impressions of goodness/badness. It can cause people to use one positive (or negative) fact about an object/action to judge the likely positivity or negativity of other facts about that object/action.\n\nSee also\n--------\n\n*   [Halo effect](https://www.lesswrong.com/tag/halo-effect)\n*   [Priming](https://www.lesswrong.com/tag/priming)\n*   [Connotation](https://www.lesswrong.com/tag/connotation)\n*   [Affective death spiral](https://www.lesswrong.com/tag/affective-death-spiral)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "H2q58pKG6xFrv8bPz",
    "name": "Spaced Repetition",
    "core": null,
    "slug": "spaced-repetition",
    "oldSlugs": null,
    "postCount": 61,
    "description": {
      "markdown": "**Spaced Repetition** is a technique for long-term retention of learned material where instead of attempting to memorize by ‘cramming’, memorization can be done far more efficiently by instead spacing out each review, with increasing durations as one learns the item, with the scheduling done by software.  \n  \n*See Also:*  [Scholarship & Learning](https://www.lesswrong.com/tag/scholarship-and-learning)\n\n**The case for Spaced Repetition**\n==================================\n\nA good place to learn more about Spaced Repetition is [**Spaced Repetition for Efficient Learning**](https://www.gwern.net/Spaced-repetition)  by [Gwern](https://www.lesswrong.com/users/gwern):\n\n> *Spaced repetition is a centuries-old psychological technique for efficient memorization & practice of skills where instead of attempting to memorize by ‘cramming’, memorization can be done far more efficiently by instead spacing out each review, with increasing durations as one learns the item, with the scheduling done by software. Because of the greater efficiency of its slow but steady approach, spaced repetition can scale to memorizing hundreds of thousands of items (while crammed items are almost immediately forgotten) and is especially useful for foreign languages & medical studies.*\n\nThe key insight for why spaced repetition should be effective is that you forget things approximately hyperbolically-- reviewing things very soon (as in cramming-style learning) is ineffective because you have not forgotten much yet when you come to a review. In comparison, Spaced Repetition allows you to renew your knowledge precisely as you're about to forget a given fact, giving the review the maximum return-on-investment possible and (over time) flattening the 'forgetting curve' so that the interval between successive reviews gets progressively larger for a given fact.\n\nObviously, it's not possible to remind *yourself* of something precisely when you're about to forget it. Enter Spaced Repetition Software (SRS)! By using the forgetting curve, SRS is able to plan when you need to review each item. You can either create decks yourself, or (for some topics) download from databases. [Anki](https://apps.ankiweb.net/) and [Mnemnosyne](https://mnemosyne-proj.org/) are two popular free options, and [SuperMemo](https://www.supermemo.com/en) is a subscription-based choice.\n\n**Criticisms**\n==============\n\nCriticisms of Spaced Repetition primarily revolve around the fact that, for it to be effective, knowledge has to be broken down into individual 'pieces' to go onto cards for testing. This is difficult or impossible for some types of knowledge, and may not promote an integrated view, where the structure or hierarchy of the knowledge is clear, as well as other methods. More can be found in the post [A Vote Against Spaced Repetition](https://www.lesswrong.com/posts/As9E3HfgED2zkTAfB/a-vote-against-spaced-repetition).\n\n**Resources**\n=============\n\nSupermemo material\n------------------\n\n*   [Many articles on assorted related topics](http://supermemo.com/english/contents.htm#Articles)\n*   [Research on memory and learning](http://supermemo.com/english/contents.htm#Research)\n*   [Frequently asked questions about various aspects of spaced repetition](http://supermemo.com/help/faq/index.htm)\n\nSpaced Repetition Decks\n-----------------------\n\nDecks (links, or for Anki, the names of a deck in the Anki collection) relevant to LW.\n\n*   [Anki decks by LW users](http://www.stafforini.com/blog/anki-decks-by-lesswrong-users/) by Pablo_Stafforini. Comprehensive and up-to-date (as of 2019) list.\n*   [Anki deck for biases and fallacies](https://www.lesswrong.com/lw/3px/anki_deck_for_biases_and_fallacies/) by phob\n*   [Deck for Cognitive Science in One Lesson](https://www.lesswrong.com/r/discussion/lw/74o/anki_deck_for_cognitive_science_in_one_lesson/)\n*   [LessWrong Wiki as an Anki deck](https://www.lesswrong.com/r/discussion/lw/ee6/lesswrong_wiki_as_anki_deck) by mapnoterritory\n\nSR cards for [LessWrong Sequences](https://www.lesswrong.com/tag/sequences)\n---------------------------------------------------------------------------\n\n*   [Spaced Repetition Database for the Mysterious Answers to Mysterious Questions Sequence](https://www.lesswrong.com/lw/2e6/spaced_repetition_database_for_the_mysterious/) by divia\n*   [Spaced Repetition Database for A Human's Guide to Words](https://www.lesswrong.com/lw/3oq/spaced_repetition_database_for_a_humans_guide_to/) by divia\n\nOther Spaced Repetition Software\n--------------------------------\n\n*   [VocApp.com](https://vocapp.com/)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "YPZCAs9Axp9PtrF22",
    "name": "Humility",
    "core": false,
    "slug": "humility",
    "oldSlugs": [
      "modesty",
      "epistemic-modesty",
      "epistemic-humility",
      "humility-1"
    ],
    "postCount": 25,
    "description": {
      "markdown": "Outside of LessWrong, \"humility\" usually [refers to](https://www.google.com/search?q=humility) \"a modest or low view of one's own importance\". In common parlance, to be [humble](https://ahdictionary.com/word/search.html?q=%20HUMBLE) is to be meek, deferential, submissive, or unpretentious, \"not arrogant or prideful\". Thus, in ordinary English \"humility\" and \"[modesty](https://www.lesswrong.com/tag/modesty)\" have pretty similar connotations.\n\nOn LessWrong, Eliezer Yudkowsky has proposed that we instead draw a sharp distinction between two kinds of \"humility\" — social modesty, versus \"**scientific humility**\".\n\nIn [The Proper Use of Humility](https://www.lesswrong.com/posts/GrDqnMjhqoxiqpQPw/the-proper-use-of-humility) (2006), Yudkowsky writes:\n\n> You suggest studying harder, and the student replies: “No, it wouldn’t work for me; I’m not one of the smart kids like you; nay, one so lowly as myself can hope for no better lot.”\n> \n> This is social modesty, not humility. It has to do with regulating status in the tribe, rather than scientific process.\n> \n> If you ask someone to “be more humble,” by default they’ll associate the words to social modesty—which is an intuitive, everyday, ancestrally relevant concept. Scientific humility is a more recent and rarefied invention, and it is not inherently social. Scientific humility is something you would practice even if you were alone in a spacesuit, light years from Earth with no one watching. Or even if you received an absolute guarantee that no one would ever criticize you again, no matter what you said or thought of yourself. You’d still double-check your calculations if you were wise.\n\nOn LW, then, we tend to follow the convention of using \"humility\" as a term of art for an important part of reasoning: combating [overconfidence](https://www.lesswrong.com/tag/overconfidence), recognizing and improving on your [weaknesses](https://www.lesswrong.com/tag/heuristics-and-biases), anticipating and preparing for likely errors you'll make, etc.\n\nIn contrast, \"[modesty](https://www.lesswrong.com/tag/modesty)\" here refers to the bad habit of letting your behavior and [epistemics](https://www.lesswrong.com/tag/epistemology) be ruled by not wanting to look arrogant or conceited. Yudkowsky argues in [*Inadequate Equilibria*](https://www.lesswrong.com/s/oLGCcbnvabyibnG9d) (2017) that psychological impulses like \"[status regulation and anxious underconfidence](https://www.lesswrong.com/s/oLGCcbnvabyibnG9d/p/o28fkhcZsBhhgfGjx)\" have caused many people in the effective altruism and rationality communities to adopt a \"[modest epistemology](https://www.lesswrong.com/tag/modest-epistemology)\" that involves rationalizing various false world-models and invalid reasoning heuristics.\n\nLW tries to create a social environment where social reward and punishment is generally less salient, and where (to the extent it persists) it incentivizes honesty and truth-seeking as much as possible. LW doesn't always *succeed* in this goal, but this is nonetheless the goal.\n\nThe most commonly cited explanation of scientific/epistemic humility on LW is found in Yudkowsky's \"[Twelve Virtues of Rationality](https://www.yudkowsky.net/rational/virtues)\" (2006):\n\n> The eighth virtue is humility.\n> \n> To be humble is to take specific actions in anticipation of your own errors.\n> \n> To confess your fallibility and then do nothing about it is not humble; it is boasting of your modesty.\n> \n> Who are most humble? Those who most skillfully prepare for the deepest and most catastrophic errors in their own beliefs and plans.\n> \n> Because this world contains many whose grasp of rationality is abysmal, beginning students of rationality win arguments and acquire an exaggerated view of their own abilities. But it is useless to be superior: Life is not graded on a curve. The best physicist in ancient Greece could not calculate the path of a falling apple. There is no guarantee that adequacy is possible given your hardest effort; therefore spare no thought for whether others are doing worse. If you compare yourself to others you will not see the biases that all humans share. To be human is to make ten thousand errors. No one in this world achieves perfection.\n\nHumility versus Modest Epistemology\n-----------------------------------\n\nWhile humility is based on the general idea that you are fallible (and should try to be calibrated and realistic about this), modest epistemology makes stronger claims such as:\n\n*   Given your fallibility, you should rely heavily on various techniques associated with \"[the outside view](https://www.lesswrong.com/tag/inside-outside-view)\", and try to avoid using \"inside views\".\n*   Given the human tendency to rationalize and self-deceive, you should trust average opinions, or the average opinion of authoritative-sounding sources, more than your own opinions (including your own fine-grained opinions about which authorities have good epistemics on which topics).\n*   Given the risks and commonness of [overconfidence](https://www.lesswrong.com/tag/overconfidence), you should worry much more about overconfidence, and worry little (or not at all) about [underconfidence](https://www.lesswrong.com/s/pvim9PZJ6qHRTMqD3/p/pkFazhcTErMw7TFtT).\n\nIn contrast, Yudkowsky has [argued](https://www.lesswrong.com/s/oLGCcbnvabyibnG9d/p/o28fkhcZsBhhgfGjx):\n\n> I try to be careful to distinguish the virtue of avoiding overconfidence, which I sometimes call “[humility](https://www.lesswrong.com/lw/gq/the_proper_use_of_humility/),” from the phenomenon I’m calling “modest epistemology.” But even so, when overconfidence is such a terrible scourge according to the cognitive bias literature, can it ever be wise to caution people against *under*confidence?\n> \n> Yes. First of all, overcompensation after being warned about a cognitive bias is also a recognized problem in the literature; and the literature on that talks about how bad people often are at determining whether they’re undercorrecting or overcorrecting. Second, my own experience has been that while, yes, commenters on the Internet are often overconfident, it’s very different when I’m talking to people in person. My more recent experience seems more like 90% telling people to be less underconfident, to reach higher, to be more ambitious, to test themselves, and maybe 10% cautioning people against overconfidence. And yes, this ratio applies to men as well as women and nonbinary people, and to people considered high-status as well as people considered low-status.\n\n[The Sin of Underconfidence](https://www.lesswrong.com/posts/pkFazhcTErMw7TFtT/the-sin-of-underconfidence) (2009) argues that underconfidence is one of the \"three great besetting sins of rationalists\" (the [others](https://www.lesswrong.com/posts/yffPyiu7hRLyc7r23/final-words) being motivated reasoning / [motivated skepticism](https://www.lesswrong.com/tag/motivated-skepticism) and \"cleverness\").\n\nIn [Taboo \"Outside View\"](https://www.lesswrong.com/posts/BcYfsi7vmhDvzQGiF/taboo-outside-view) (2021), Daniel Kokotajlo notes that the original meaning of \"outside view\" ([reference class forecasting](https://en.wikipedia.org/wiki/Reference_class_forecasting)) has become eroded as EAs have begun using \"outside view\" to refer to everything from reasoning by analogy, to trend extrapolation, to foxy aggregation, to bias correction, to \"deference to wisdom of the many\", to \"anti-weirdness heuristics\", to priors, etc.\n\nAdditionally, proponents of outside-viewing often behave as though there is a single obvious reference class to use -- \"*the* outside view\", as opposed to \"*an* outside view\" -- and tend to neglect the role of detailed model-building in helping us figure out which reference classes are relevant.\n\nThe lesson of this isn't \"it's bad to ever use reference class forecasting, trend extrapolation, etc.\", but rather that these tools are part and parcel of building good world-models and deriving good predictions from them, rather than being a robust replacement for world-modeling.\n\nLikewise, the lesson isn't \"it's bad to ever worry about overconfidence\", but rather that overconfidence and underconfidence are *both* problems, neither is *a priori* worse than the other, and fixing them requires doing a lot of legwork and model-building about your own capabilities -- again, there isn't a royal road to 'getting the right answer without having to figure things out'.\n\nRelated pages\n-------------\n\n*   [Calibration](https://www.lesswrong.com/tag/calibration)\n*   [Chesterton's Fence](https://www.lesswrong.com/tag/chesterton-s-fence)\n*   [Underconfidence](https://www.lesswrong.com/tag/underconfidence) and [Overconfidence](https://www.lesswrong.com/tag/overconfidence)\n*   [Modest Epistemology](https://www.lesswrong.com/tag/modest-epistemology)\n*   [Modesty](https://www.lesswrong.com/tag/modesty)\n*   [Fallacy of Gray](https://www.lesswrong.com/tag/fallacy-of-gray)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "ai87fPyyT6mWb4YkT",
    "name": "Eldritch Analogies",
    "core": null,
    "slug": "eldritch-analogies",
    "oldSlugs": null,
    "postCount": 13,
    "description": {
      "markdown": "**Eldritch Analogies** are the association of impersonal social or physical dynamics with fictional or mythological deities. The most well known is Moloch, who is associated with coordination problems and competition."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "DbMQGrxbhLxtNkmca",
    "name": "Value of Information",
    "core": false,
    "slug": "value-of-information",
    "oldSlugs": null,
    "postCount": 19,
    "description": {
      "markdown": "**Value of Information** (VoI) is a concept from [decision analysis](https://www.lesswrong.com/lw/8xr/decision_analysis_sequence/): how much answering a question allows a decision-maker to improve its decision.\n\n_See also: [Bayes theorem](https://www.lesswrong.com/tag/bayes-theorem-bayesianism), [Decision Theory](https://www.lesswrong.com/tag/decision-theory?showPostCount=true&useTagName=true)_"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "36RkM85iDocrnaypb",
    "name": "Aesthetics",
    "core": null,
    "slug": "aesthetics",
    "oldSlugs": null,
    "postCount": 27,
    "description": {
      "markdown": "**Aesthetics**:\"Imagine if we could talk about why things seem beautiful and appealing, or ugly and unappealing.  Where do these preferences come from, in a causal sense? Do we still endorse them when we know their origins?  What happens when we bring tacit things into consciousness, when we talk carefully about what aesthetics evoke in us, and how that might be the same or different from person to person?\"  – [Naming the Nameless, Sarah Constantin](/posts/4ZwGqkMTyAvANYEDw/naming-the-nameless)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "xHTXnyp65X8YX6ahT",
    "name": "Instrumental Convergence",
    "core": false,
    "slug": "instrumental-convergence",
    "oldSlugs": null,
    "postCount": 53,
    "description": {
      "markdown": "**Instrumental convergence** or **convergent instrumental values** is the theorized tendency for most sufficiently intelligent agents to pursue potentially unbounded instrumental goals such as self-preservation and resource acquisition \\[[1](https://en.wikipedia.org/wiki/Instrumental_convergence)\\]. This concept has also been discussed under the term *basic drives.*\n\nThe idea was first explored by [Steve Omohundro](https://en.wikipedia.org/wiki/Steve_Omohundro). He argued that sufficiently advanced AI systems would all naturally discover similar instrumental subgoals. The view that there are important basic AI drives was subsequently defended by [Nick Bostrom](https://lessestwrong.com/tag/nick-bostrom) as the *instrumental convergence thesis*, or the *convergent instrumental goals thesis*. On this view, a few goals are [instrumental](https://lessestwrong.com/tag/instrumental-value) to almost all possible [final](https://lessestwrong.com/tag/terminal-value) goals. Therefore, all advanced AIs will pursue these instrumental goals. Omohundro uses microeconomic theory by von Neumann to support this idea.\n\nOmohundro’s Drives\n------------------\n\nOmohundro presents two sets of values, one for self-improving artificial intelligences [1](http://selfawaresystems.files.wordpress.com/2008/01/nature_of_self_improving_ai.pdf) and another he says will emerge in any sufficiently advanced AGI system [2](http://selfawaresystems.files.wordpress.com/2008/01/ai_drives_final.pdf). The former set is composed of four main drives:\n\n*   **Self-preservation**: A sufficiently advanced AI will probably be the best entity to achieve its goals. Therefore it must continue existing in order to maximize goal fulfillment. Similarly, if its goal system were modified, then it would likely begin pursuing different ends. Since this is not desirable to the current AI, it will act to preserve the content of its goal system.\n*   **Efficiency**: At any time, the AI will have finite resources of time, space, matter, energy and computational power. Using these more efficiently will increase its utility. This will lead the AI to do things like implement more efficient algorithms, physical embodiments, and particular mechanisms. It will also lead the AI to replace desired physical events with computational simulations as much as possible, to expend fewer resources.\n*   **Acquisition**: Resources like matter and energy are indispensable for action. The more resources the AI can control, the more actions it can perform to achieve its goals. The AI's physical capabilities are determined by its level of technology. For instance, if the AI could invent nanotechnology, it would vastly increase the actions it could take to achieve its goals.\n*   **Creativity**: The AI's operations will depend on its ability to come up with new, more efficient ideas. It will be driven to acquire more computational power for raw searching ability, and it will also be driven to search for better search algorithms. Omohundro argues that the drive for creativity is critical for the AI to display the richness and diversity that is valued by humanity. He discusses [signaling](https://lessestwrong.com/tag/signaling) goals as particularly rich sources of creativity.\n\nBostrom’s Drives\n----------------\n\nBostrom argues for an [orthogonality thesis](https://lessestwrong.com/tag/orthogonality-thesis): But he also argues that, despite the fact that values and intelligence are independent, any recursively self-improving intelligence would likely possess a particular set of instrumental values that are useful for achieving any kind of [terminal value](https://lessestwrong.com/tag/terminal-value).[3](http://www.nickbostrom.com/superintelligentwill.pdf) On his view, those values are:\n\n*   **Self-preservation**: A superintelligence will value its continuing existence as a means to to continuing to take actions that promote its values.\n*   **Goal-content integrity**: The superintelligence will value retaining the same preferences over time. Modifications to its future values through swapping memories, downloading skills, and altering its cognitive architecture and personalities would result in its transformation into an agent that no longer optimizes for the same things.\n*   **Cognitive enhancement**: Improvements in cognitive capacity, intelligence and rationality will help the superintelligence make better decisions, furthering its goals more in the long run.\n*   **Technological perfection**: Increases in hardware power and algorithm efficiency will deliver increases in its cognitive capacities. Also, better engineering will enable the creation of a wider set of physical structures using fewer resources (e.g., [nanotechnology](https://lessestwrong.com/tag/nanotechnology)).\n*   **Resource acquisition**: In addition to guaranteeing the superintelligence's continued existence, basic resources such as time, space, matter and free energy could be processed to serve almost any goal, in the form of extended hardware, backups and protection.\n\nRelevance\n---------\n\nBoth Bostrom and Omohundro argue these values should be used in trying to predict a superintelligence's behavior, since they are likely to be the only set of values shared by most superintelligences. They also note that these values are consistent with safe and beneficial AIs as well as unsafe ones.\n\nBostrom emphasizes, however, that our ability to predict a superintelligence's behavior may be very limited even if it shares most intelligences' instrumental goals.\n\nYudkowsky echoes Omohundro's point that the convergence thesis is consistent with the possibility of Friendly AI. However, he also notes that the convergence thesis implies that most AIs will be extremely dangerous, merely by being indifferent to one or more human values:[4](http://intelligence.org/2013/05/05/five-theses-two-lemmas-and-a-couple-of-strategic-implications/)\n\nPathological Cases\n------------------\n\nIn some rarer cases, AIs may not pursue these goals. For instance, if there are two AIs with the same goals, the less capable AI may determine that it should destroy itself to allow the stronger AI to control the universe. Or an AI may have the goal of using as few resources as possible, or of being as unintelligent as possible. These relatively specific goals will limit the growth and power of the AI.\n\nSee Also\n--------\n\n*   [Convergent instrumental strategies](https://arbital.com/p/convergent_strategies/) ([Arbital](https://wiki.lesswrong.com/wiki/Arbital))\n*   [Instrumental convergence](https://arbital.com/p/instrumental_convergence/) ([Arbital](https://wiki.lesswrong.com/wiki/Arbital))\n*   [Orthogonality thesis](https://lessestwrong.com/tag/orthogonality-thesis)\n*   [Cox's theorem](https://wiki.lesswrong.com/wiki/Cox's_theorem)\n*   [Unfriendly AI](https://wiki.lesswrong.com/wiki/Unfriendly_AI), [Paperclip maximizer](https://lessestwrong.com/tag/paperclip-maximizer), [Oracle AI](https://lessestwrong.com/tag/oracle-ai)\n*   [Instrumental values](https://wiki.lesswrong.com/wiki/Instrumental_values)\n\nReferences\n----------\n\n*   Omohundro, S. (2007). [*The Nature of Self-Improving Artificial Intelligence*](http://selfawaresystems.files.wordpress.com/2008/01/nature_of_self_improving_ai.pdf).\n*   Omohundro, S. (2008). \"[The Basic AI Drives](http://selfawaresystems.com/2007/11/30/paper-on-the-basic-ai-drives/)\". *Proceedings of the First AGI Conference*.\n*   Omohundro, S. (2012). [*Rational Artificial Intelligence for the Greater Good*](http://selfawaresystems.files.wordpress.com/2012/03/rational_ai_greater_good.pdf).\n*   Bostrom, N. (2012). \"[The Superintelligent Will: Motivation and Instrumental Rationality in Advanced Artificial Agents](http://www.nickbostrom.com/superintelligentwill.pdf)\". *Minds and Machines*.\n*   Shulman, C. (2010). [*Omohundro's \"Basic AI Drives\" and Catastrophic Risks*](http://intelligence.org/files/BasicAIDrives.pdf)."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "JsJPrdgRGRqnci8cZ",
    "name": "Altruism",
    "core": null,
    "slug": "altruism",
    "oldSlugs": null,
    "postCount": 69,
    "description": {
      "markdown": "**Altruism** refers to actions undertaken for the concern and benefit of others at ones own expense.\n\n_Related tags and wikis:_ [Shut up and multiply](https://www.lesswrong.com/tag/shut-up-and-multiply), [Fuzzies](https://www.lesswrong.com/tag/fuzzies), [World Optimization](https://www.lesswrong.com/tag/world-optimization), [Effective Altruism](https://www.lesswrong.com/tag/effective-altruism), [Cause Prioritization](https://www.lesswrong.com/tag/cause-prioritization), [Motivations](https://www.lesswrong.com/tag/motivations), [Psychology of Altruism](https://www.lesswrong.com/tag/psychology-of-altruism), [Ethics and Morality](https://www.lesswrong.com/tag/ethics-and-morality)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "yeJFqsWrP2pjYfNEr",
    "name": "Deontology",
    "core": false,
    "slug": "deontology",
    "oldSlugs": null,
    "postCount": 21,
    "description": {
      "markdown": "**Deontology** is a theory of morality based around obeying moral rules."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "2q2cK4FdnSeohTEaJ",
    "name": "Blackmail / Extortion",
    "core": false,
    "slug": "blackmail-extortion",
    "oldSlugs": [
      "blackmail"
    ],
    "postCount": 19,
    "description": {
      "markdown": "**Blackmail** is an act of [coercion](https://en.wikipedia.org/wiki/Coercion) using the [threat](https://en.wikipedia.org/wiki/Threat) of revealing or publicizing either [substantially true](https://en.wikipedia.org/wiki/Substantial_truth) or [false information](https://en.wikipedia.org/wiki/False_information) about a person or people unless certain demands are met ([from Wikipedia](https://en.wikipedia.org/wiki/Blackmail)), **Extortion** is the more general concept of threatening someone to get them to give you something. They are often discussed together as a reference class."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "YTCrHWYHAsAD74EHo",
    "name": "Self-Deception",
    "core": false,
    "slug": "self-deception",
    "oldSlugs": null,
    "postCount": 55,
    "description": {
      "markdown": "**Self-deception** is a state of preserving a wrong [belief](https://lessestwrong.com/tag/belief), often facilitated by denying or [rationalizing away](https://lessestwrong.com/tag/rationalization) the relevance, significance, or importance of opposing [evidence](https://lessestwrong.com/tag/evidence) and logical arguments. Beliefs supported by self-deception are often chosen for reasons other than how closely those beliefs approximate [truth](https://lessestwrong.com/tag/truth-semantics-and-meaning).\n\n*Related:* [Anticipated Experiences](https://www.lesswrong.com/tag/anticipated-experiences?showPostCount=true&useTagName=true), [Motivated Reasoning](https://www.lesswrong.com/tag/motivated-reasoning?showPostCount=true&useTagName=true), [Rationalization](https://www.lesswrong.com/tag/rationalization?showPostCount=true&useTagName=true)\n\nOn LessWrong, a common distinction is between [beliefs as expectation-controllers](https://www.lesswrong.com/tag/anticipated-experiences?showPostCount=false&useTagName=false) and [other](https://www.lesswrong.com/posts/dLbkrPu5STNCBLRjr/applause-lights) [things](https://www.lesswrong.com/posts/NMoLJuDJEms7Ku9XS/guessing-the-teacher-s-password) [people](https://www.lesswrong.com/s/7gRSERQZbqTuLX5re/p/CqyJzDZWvGhhFJ7dY) [commonly](https://www.lesswrong.com/s/7gRSERQZbqTuLX5re/p/RmCjazjupRGcHSm5N) [label](https://www.lesswrong.com/s/7gRSERQZbqTuLX5re/p/nYkMLFpx77Rz3uo9c) as beliefs. When these different things conflict, a person is said to have engaged in self-deception.\n\nDeceiving yourself is [harder than it seems](https://lessestwrong.com/lw/s/belief_in_selfdeception/). What looks like a successively adopted false belief may actually be just a [belief in false belief](https://lessestwrong.com/tag/belief-in-belief).\n\nAn example from [No, Really, I've Deceived Myself](https://www.lesswrong.com/posts/rZX4WuufAPbN6wQTv/no-really-i-ve-deceived-myself):\n\n> *When this woman was in high school, she thought she was an atheist.  But she decided, at that time, that she should act as if she believed in God.  And then—she told me earnestly—over time, she came to really believe in God.*\n\n> *So far as I can tell, she is completely wrong about that.  Always throughout our conversation, she said, over and over, \"I believe in God\", never once, \"There is a God.\"  When I asked her why she was religious, she never once talked about the consequences of God existing, only about the consequences of believing in God.  Never, \"God will help me\", always, \"my belief in God helps me\".  When I put to her, \"Someone who just wanted the truth and looked at our universe would not even invent God as a hypothesis,\" she agreed outright.*\n\n> *She hasn't actually deceived herself into believing that God exists or that the Jewish religion is true.  Not even close, so far as I can tell.*\n\n> *On the other hand, I think she really does believe she has deceived herself.*\n\nBlog posts\n----------\n\n*   [Self-deception: Hypocrisy or Akrasia?](http://lesswrong.com/lw/h7/selfdeception_hypocrisy_or_akrasia/)\n\nSequence by [Eliezer Yudkowsky](https://wiki.lesswrong.com/wiki/Eliezer_Yudkowsky)\n----------------------------------------------------------------------------------\n\n*Part of* [*How To Actually Change Your Mind*](https://wiki.lesswrong.com/wiki/How_To_Actually_Change_Your_Mind) *sequence*\n\n*   [No, Really, I've Deceived Myself](http://lesswrong.com/lw/r/no_really_ive_deceived_myself/)\n*   [Belief in Self Deception](http://lesswrong.com/lw/s/belief_in_selfdeception)\n*   [Moore's Paradox](http://lesswrong.com/lw/1f/moores_paradox)\n*   [Don't Believe You'll Self-Deceive](http://lesswrong.com/lw/1o/dont_believe_youll_selfdeceive)\n*   [Striving to Accept](http://lesswrong.com/lw/1r/striving_to_accept)\n\nOther resources\n---------------\n\n*   [Robin Hanson](https://wiki.lesswrong.com/wiki/Robin_Hanson) (2009). \"Enhancing Our Truth Orientation\". in Larissa Behrendt, Nick Bostrom. *Human Enhancement*. Oxford University Press. ([PDF](http://hanson.gmu.edu/moretrue.pdf))\n\nSee also\n--------\n\n*   [Anti-epistemology](https://wiki.lesswrong.com/wiki/Anti-epistemology), [Belief in belief](https://wiki.lesswrong.com/wiki/Belief_in_belief)\n*   [Semantic stopsign](https://wiki.lesswrong.com/wiki/Semantic_stopsign), [Compartmentalization](https://wiki.lesswrong.com/wiki/Compartmentalization), [Motivated skepticism](https://wiki.lesswrong.com/wiki/Motivated_skepticism)\n*   [Improper belief](https://wiki.lesswrong.com/wiki/Improper_belief), [Truth](https://wiki.lesswrong.com/wiki/Truth)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "E6qP9r9xxM4LCxaFk",
    "name": "Litany of Tarski",
    "core": false,
    "slug": "litany-of-tarski",
    "oldSlugs": null,
    "postCount": 7,
    "description": {
      "markdown": "The **Litany of Tarski** is a template to remind oneself that beliefs should stem from reality, from what actually *is*, as opposed to what we *want*, or what would be convenient. For any statement X, the litany takes the form \"If X, I desire to believe that X\".\n\nQuoting [The Meditation on Curiosity](https://www.lesswrong.com/posts/3nZMgRTfFEfHp34Gb/the-meditation-on-curiosity):\n\n> *If the box contains a diamond,*  \n> *I desire to believe that the box contains a diamond;*  \n> *If the box does not contain a diamond,*  \n> *I desire to believe that the box does not contain a diamond;*  \n> *Let me not become attached to beliefs I may not want.*\n\nThe name refers to [Alfred Tarski](https://en.wikipedia.org/wiki/Alfred_Tarski), who sought to define [what, exactly, \"truth\" means](https://plato.stanford.edu/entries/tarski-truth/).\n\nSee also: [Map and Territory](https://www.lesswrong.com/tag/map-and-territory), [Litanies & Mantras](https://www.lesswrong.com/tag/litanies-and-mantras).\n\nBlog posts\n----------\n\n*   [The Meditation on Curiosity](http://lesswrong.com/lw/jz/the_meditation_on_curiosity/)\n*   [Why truth? And...](http://lesswrong.com/lw/go/why_truth_and/) — You have an instrumental motive to care about the truth of your *beliefs about* anything you care about.\n*   [Belief in Self-Deception](http://lesswrong.com/lw/s/belief_in_selfdeception/) — Deceiving yourself is harder than it seems. What looks like a successively adopted false belief may actually be just a [belief in false belief](https://wiki.lesswrong.com/wiki/Belief_in_belief).\n*   [The Bottom Line](http://lesswrong.com/lw/js/the_bottom_line/)\n*   [A Rational Argument](http://lesswrong.com/lw/jw/a_rational_argument/)\n*   [Tarski Statements as Rationalist Exercise](http://lesswrong.com/lw/39/tarski_statements_as_rationalist_exercise/) by [Vladimir Nesov](https://wiki.lesswrong.com/wiki/Vladimir_Nesov)\n*   [A Fable of Science and Politics](http://lesswrong.com/lw/gt/a_fable_of_science_and_politics/) \\-\\- characters discover the color of the sky, with political implications.\n\nSee also\n--------\n\n*   [Truth](https://wiki.lesswrong.com/wiki/Truth)\n*   [Litany of Gendlin](https://wiki.lesswrong.com/wiki/Litany_of_Gendlin)\n*   [Epistemic hygiene](https://wiki.lesswrong.com/wiki/Epistemic_hygiene)\n*   [Rationalization](https://wiki.lesswrong.com/wiki/Rationalization)\n*   [Self-deception](https://wiki.lesswrong.com/wiki/Self-deception)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "HAFdXkW4YW4KRe2Gx",
    "name": "Utility Functions",
    "core": false,
    "slug": "utility-functions",
    "oldSlugs": null,
    "postCount": 119,
    "description": {
      "markdown": "A **utility function** assigns numerical values (\"utilities\") to outcomes, in such a way that outcomes with higher utilities are always [preferred](http://lesswrong.com/tag/preference) to outcomes with lower utilities.\n\n_See also:_ [Complexity of Value](https://www.lesswrong.com/tag/complexity-of-value?showPostCount=true&useTagName=true), [Decision Theory](https://www.lesswrong.com/tag/decision-theory), [Game Theory](https://www.lesswrong.com/tag/game-theory?showPostCount=true&useTagName=true), [Orthogonality Thesis](https://www.lesswrong.com/tag/orthogonality-thesis/), [Utilitarianism](http://lesswrong.com/tag/utilitarianism), [Preference](https://www.lesswrong.com/tag/preference), [Utility](https://www.lesswrong.com/tag/utility), [VNM Theorem](https://www.lesswrong.com/tag/vnm-theorem)\n\nUtility Functions do not work very well in practice for individual humans. Human drives are not coherent nor is there any reason to think they would be ([Thou Art Godshatter](https://www.lesswrong.com/lw/l3/thou_art_godshatter/)), and even people with a strong interest in the concept have trouble working out what their utility function actually is even slightly ([Post Your Utility Function](https://www.lesswrong.com/lw/zv/post_your_utility_function/)). Furthermore, humans appear to calculate utility and disutility separately - adding one to the other does not predict their behavior accurately. This makes humans highly exploitable.\n\n[pjeby](https://www.lesswrong.com/users/pjeby) posits humans' difficulty in understanding their own utility functions as the root of [akrasia](https://www.lesswrong.com/tag/akrasia).\n\nHowever, utility functions can be a useful model for dealing with humans in groups, _e.g._ in economics.\n\nThe [VNM Theorem](https://www.lesswrong.com/tag/vnm-theorem) tag is likely to be a strict subtag of the Utility Functions tag, because the VNM theorem establishes when preferences can be represented by a utility function, but a post discussing utility functions may or may not discuss the VNM theorem/axioms."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "LhX3F2SvGDarZCuh6",
    "name": "Bayes' Theorem",
    "core": false,
    "slug": "bayes-theorem",
    "oldSlugs": [
      "bayes-theorem",
      "bayes-theorem-bayesianism",
      "bayes-theorem-bayesianism"
    ],
    "postCount": 137,
    "description": {
      "markdown": "**Bayes' Theorem** (also known as Bayes' Law) is a law of probability that describes the proper way to incorporate new evidence into prior probabilities to form an updated probability estimate. It is commonly regarded as the foundation of consistent rational reasoning under uncertainty. Bayes Theorem is named after Reverend Thomas Bayes who proved the theorem in 1763.\n\n_See also:_ [Bayesian probability](https://www.lesswrong.com/tag/bayesian-probability), [Priors](https://www.lesswrong.com/tag/priors), [Likelihood ratio](https://www.lesswrong.com/tag/likelihood-ratio), [Belief update](https://www.lesswrong.com/tag/belief-update), [Probability and statistics](https://www.lesswrong.com/tag/probability-and-statistics), [Epistemology](https://www.lesswrong.com/tag/epistemology), [Bayesianism](https://www.lesswrong.com/tag/bayesianism)\n\nBayes' theorem commonly takes the form:\n\n P(A|B)=P(B|A)P(A)P(B)\n\nwhere A is the proposition of interest, B is the observed evidence, P(A) and P(B) are prior probabilities, and P(A|B) is the posterior probability of A.\n\nWith the posterior odds, the prior odds and the [likelihood ratio](https://www.lesswrong.com/tag/likelihood-ratio) written explicitly, the theorem reads:\n\nP(A|B)P(¬A|B)=P(A)P(¬A)⋅P(B|A)P(B|¬A)\n\nVisualization of Bayes' Rule\n----------------------------\n\n![](https://wiki.lesswrong.com/images/7/74/Bayes.png)\n\nExternal links\n--------------\n\n*   [Arbital Guide to Bayes' Rule](https://arbital.com/p/bayes_rule_guide/)\n*   [An Intuitive Explanation of Bayes' Theorem](http://yudkowsky.net/rational/bayes) by Eliezer Yudkowsky\n*   [Visualizing Bayes' theorem](http://blog.oscarbonilla.com/2009/05/visualizing-bayes-theorem/) by Oscar Bonilla\n*   [Using Venn pies to illustrate Bayes' theorem](http://oracleaide.wordpress.com/2012/12/26/a-venn-pie/) by [oracleaide](https://www.lesswrong.com/users/oracleaide)\n*   [A Guide to Bayes’ Theorem – A few links](http://kruel.co/2010/02/27/a-guide-to-bayes-theorem-a-few-links/) by Alexander Kruel\n*   [Bayes' Theorem](https://en.wikipedia.org/wiki/Bayes%27_theorem), Wikipedia"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "wBoHTJs9iQzczNtW3",
    "name": "Robust Agents",
    "core": null,
    "slug": "robust-agents",
    "oldSlugs": null,
    "postCount": 23,
    "description": {
      "markdown": "**Robust Agents** are decision-makers who can perform well in a variety of situations. Whereas some humans rely on folk wisdom or instinct, and some AIs might be designed to achieve a narrow set of goals, a Robust Agent has a coherent set of values and decision-procedures. This enables them to adapt to new circumstances (such as succeeding in a new environment, or responding to a new strategy by a competitor).\n\nSee also\n--------\n\n*   [Agency](https://www.lesswrong.com/tag/agency)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "XqykXFKL9t38pbSEm",
    "name": "Well-being",
    "core": null,
    "slug": "well-being",
    "oldSlugs": null,
    "postCount": 96,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "E8PHMuf7tsr8teXAe",
    "name": "Betting",
    "core": null,
    "slug": "betting",
    "oldSlugs": null,
    "postCount": 66,
    "description": {
      "markdown": "**Betting** is staking money (or some other form of value) on one's beliefs. It is considered rationally virtuous to bet on one's beliefs, as the real stakes force one to actually consider precisely what they [anticipate](https://www.lesswrong.com/posts/a7n8GdKiAZRX86T5A/making-beliefs-pay-rent-in-anticipated-experiences) will really happen. LessWrong has a culture of betting.\n\n*See also:* [Prediction Markets,](https://www.lesswrong.com/tag/prediction-markets) [Forecasting & Prediction,](https://www.lesswrong.com/tag/forecasting-and-prediction) [Forecasts (Specific Predictions)](https://www.lesswrong.com/tag/forecasts)\n\nWhy is betting important?\n-------------------------\n\nThe argument in favor of betting is that one should generally either accept a proposed bet, in order to make money in expectation, or update their beliefs so the bet becomes unprofitable. There are exceptions to this rule, some theoretical, such as the example of [Omega and Omicron](https://www.lesswrong.com/posts/G7HgP9KTWAMSv6oEJ/bets-and-updating), and some practical, such as uncertainty about whether the bet will be fulfilled. Offering a bet forces someone to think more carefully and share their beliefs more precisely. Losing a bet, even small, can make it more emotionally visceral in a way that might lead to sharpening belief [calibration](https://www.lesswrong.com/tag/calibration) more. Bets can be made about beliefs that can be immediately verified or about beliefs that will only be verifiable in the future.\n\nIn popular culture, this idea is often referred to as \"putting one's money where one's mouth is\".\n\n[A Bet is a Tax on Bullshit](https://marginalrevolution.com/marginalrevolution/2012/11/a-bet-is-a-tax-on-bullshit.html) mentions that:\n\n> In fact, the NYTimes should require that Silver, and other pundits, bet their beliefs. Furthermore, to remove any possibility of manipulation, the NYTimes should escrow a portion of Silver’s salary in a blind trust bet. In other words, the NYTimes should bet a portion of Silver’s salary, at the odds implied by Silver’s model, randomly choosing which side of the bet to take, only revealing to Silver the bet and its outcome after the election is over. A blind trust bet creates incentives for Silver to be disinterested in the outcome but very interested in the accuracy of the forecast.\n\nIn [What Does the Betting Norm Tax?](https://www.econlib.org/archives/2009/03/what_does_the_b.html), Bryan Caplan says that such a norm should also be present among scholars.\n\nOperationalization for Bets\n---------------------------\n\n*Operationalizing a belief* is the practice of transforming a belief into a bet with a clear, unambiguous resolution criteria. Sometimes this can be difficult, but there can be ways around some difficulties as explained in [Tricky Bets and Truth-Tracking Fields](https://www.lesswrong.com/posts/LzyN9wzEdfS3j5SmT/tricky-bets-and-truth-tracking-fields). The same challenges are present for prediction markets.\n\nPrediction Markets\n------------------\n\nA [prediction market](https://www.lesswrong.com/tag/prediction-markets) is a way for everyone to participate in betting on a particular question. A positive externality of prediction markets, and to a lesser extent bets, is providing a reliable probability on its questions. It can also act as an insurer. [3](https://www.lesswrong.com/posts/ts4KmAR8aJoGMawLb/link-bets-do-not-necessarily-reveal-beliefs)[4](https://www.lesswrong.com/posts/JDKfPsHvBwgq4Knn9/buy-insurance-bet-against-yourself) [Truthcoin](http://www.truthcoin.info/), an idea for a decentralized prediction market, has the slogan \"Making cheap talk expensive\".\n\n[Long Bets](http://longbets.org/) is also a useful platform to make certain bets.\n\n> The purpose of Long Bets is to improve long–term thinking. Long Bets is a public arena for enjoyably competitive predictions, of interest to society, with philanthropic money at stake. The Long Now Foundation furnishes the continuity to see even the longest bets through to public resolution. This website provides a forum for discussion about what may be learned from the bets and their eventual outcomes.\n\nHowever, Long Bets hasn't good incentives to make long term bets as explained by Jeff Kaufman in [Long Bets by Confidence Level](https://www.jefftk.com/p/long-bets-by-confidence-level).\n\nSee also:\n\n*   [LessWiki Bets Registry](https://www.lesswrong.com/tag/bets-registry) (outdated)\n\nExternal links:\n\n*   [Risk aversion does not explain people's betting behaviours](https://www.lesswrong.com/posts/msf7BHMrWTczbQckh/risk-aversion-does-not-explain-people-s-betting-behaviours)\n*   [A method for fair bargaining over odds in 2 player bets!](https://www.lesswrong.com/posts/ABMMQ5gSGHwRgExJk/a-method-for-fair-bargaining-over-odds-in-2-player-bets)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "JHzjkFnQgsrRrucqQ",
    "name": "Focusing",
    "core": null,
    "slug": "focusing",
    "oldSlugs": null,
    "postCount": 19,
    "description": {
      "markdown": "**Focusing** refers to a family of introspective techniques taught by [CFAR](https://www.lesswrong.com/tag/center-for-applied-rationality-cfar) whose aim is to access one's \"gut\" or \"[System 1](https://www.lesswrong.com/tag/dual-process-theory-system-1-and-system-2?useTagName=false)\" feelings. Archetypically, sensations within the body are approached with a spirit of gentle curiosity, and possible verbal labels are checked against felt senses. Where successful, this can improve internal understanding and allow split off trauma or conflict between [subagents](https://www.lesswrong.com/tag/subagents) to be processed for improved [internal alignment](https://www.lesswrong.com/tag/internal-alignment-human).\n\nFocusing draws in name from the [book and technique](https://www.amazon.com/Focusing-Eugene-T-Gendlin/dp/0553278339) of the same name by psychologist Eugene Gendlin (Who's also known on LessWrong for the [Litany of Gendlin](https://www.lesswrong.com/tag/litany-of-gendlin)), and since his introduction, some have developed their own variations \\[[1](https://www.lessestwrong.com/posts/PXqQhYEdbdAYCp88m/focusing-for-skeptics)\\].\n\nRelated techniques: [Internal Double Crux](https://www.lesswrong.com/tag/internal-double-crux), [Inner Simulator](https://www.lesswrong.com/tag/inner-simulator-suprise-o-meter)."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "PvridmTCj2qsugQCH",
    "name": "Goodhart's Law",
    "core": null,
    "slug": "goodhart-s-law",
    "oldSlugs": null,
    "postCount": 70,
    "description": {
      "markdown": "**Goodhart's Law** states that when a proxy for some value becomes the target of optimization pressure, the proxy will cease to be a good proxy. One form of Goodhart is demonstrated by the Soviet story of a factory graded on how many shoes they produced (a good proxy for productivity) – they soon began producing a higher number of tiny shoes. Useless, but the numbers look good.\n\nGoodhart's Law is of particular relevance to [AI Alignment](https://www.lessestwrong.com/tag/ai). Suppose you have something which is generally a good proxy for \"the stuff that humans care about\", it would be dangerous to have a powerful AI optimize for the proxy, in accordance with Goodhart's law, the proxy will breakdown.  \n\nGoodhart Taxonomy\n-----------------\n\nIn [Goodhart Taxonomy](https://www.lessestwrong.com/posts/EbFABnst8LsidYs5Y/goodhart-taxonomy), Scott Garrabrant identifies four kinds of Goodharting:\n\n*   Regressional Goodhart - When selecting for a proxy measure, you select not only for the true goal, but also for the difference between the proxy and the goal.\n*   Causal Goodhart - When there is a non-causal correlation between the proxy and the goal, intervening on the proxy may fail to intervene on the goal.\n*   Extremal Goodhart - Worlds in which the proxy takes an extreme value may be very different from the ordinary worlds in which the correlation between the proxy and the goal was observed.\n*   Adversarial Goodhart - When you optimize for a proxy, you provide an incentive for adversaries to correlate their goal with your proxy, thus destroying the correlation with your goal.\n\nSee Also\n--------\n\n*   [Groupthink](https://lessestwrong.com/tag/groupthink), [Information cascade](https://lessestwrong.com/tag/information-cascades), [Affective death spiral](https://lessestwrong.com/tag/affective-death-spiral)\n*   [Adaptation executers](https://wiki.lesswrong.com/wiki/Adaptation_executers), [Superstimulus](https://lessestwrong.com/tag/superstimuli)\n*   [Signaling](https://lessestwrong.com/tag/signaling), [Filtered evidence](https://lessestwrong.com/tag/filtered-evidence)\n*   [Cached thought](https://lessestwrong.com/tag/cached-thought)\n*   [Modesty argument](https://lessestwrong.com/tag/modesty-argument), [Egalitarianism](https://lessestwrong.com/tag/egalitarianism)\n*   [Rationalization](https://lessestwrong.com/tag/rationalization), [Dark arts](https://lessestwrong.com/tag/dark-arts)\n*   [Epistemic hygiene](https://lessestwrong.com/tag/epistemic-hygiene)\n*   [Scoring rule](https://lessestwrong.com/tag/scoring-rule)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "Z38PqJbRyfwCxKvvL",
    "name": "Research Agendas",
    "core": false,
    "slug": "research-agendas",
    "oldSlugs": null,
    "postCount": 70,
    "description": {
      "markdown": "**Research Agendas** lay out the areas of research which individuals or groups are working on, or those that they believe would be valuable for others to work on. They help make research more legible and encourage discussion of priorities."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "tZsfB6WfpRy6kFb6q",
    "name": "Conservation of Expected Evidence",
    "core": null,
    "slug": "conservation-of-expected-evidence",
    "oldSlugs": null,
    "postCount": 13,
    "description": {
      "markdown": "**Conservation of Expected Evidence** is a consequence of probability theory which states that for every expectation of evidence, there is an equal and opposite expectation of counterevidence \\[[1](https://www.lesswrong.com/posts/jiBFC7DcCrZjGmZnJ/conservation-of-expected-evidence)\\]. Conservation of Expected Evidence is about both the direction of the update and its magnitude: a low probability of seeing strong evidence in one direction must be balanced by a high probability of observing weak counterevidence in the other direction \\[[2](https://www.lesswrong.com/s/uLEjM2ij5y3CXXW6c/p/zTfSXQracE7TW8x4w#1___You_can_t_predict_that_you_ll_update_in_a_particular_direction__)\\]. The mere *expectation* of encountering evidence–before you've actually seen it–should not shift your prior beliefs. It also goes by other names, including *the* [*law of total expectation*](https://en.wikipedia.org/wiki/Law_of_total_expectation) and *the law of iterated expectations*.\n\nA consequence of this principle is that [absence of evidence is evidence of absence](https://www.lesswrong.com/posts/mnS2WYLCGJP2kQkRn/absence-of-evidence-is-evidence-of-absence).\n\nConsider a hypothesis H and evidence (observation) E. [Prior](https://wiki.lesswrong.com/wiki/Prior) [probability](https://wiki.lesswrong.com/wiki/probability) of the hypothesis is P(H); [posterior](https://wiki.lesswrong.com/wiki/posterior) probability is either P(H|E) or P(H|¬E), depending on whether you observe E or not-E (evidence or counterevidence). The probability of observing E is P(E), and probability of observing not-E is P(¬E). Thus, [expected value](https://lessestwrong.com/tag/expected-value) of the posterior probability of the hypothesis is:\n\n*P*(*H*|*E*) ⋅ *P*(*E*) + *P*(*H*|¬*E*) ⋅ *P*(¬*E*)\n\nBut the prior probability of the hypothesis itself can be trivially broken up the same way:\n\n\\\\(\\\\begin{alignat}{2}P(H) & = P(H,E) + P(H,\\\\neg{E}) \\\\\\& = P(H|E) \\\\cdot P(E) + P(H|\\\\neg{E}) \\\\cdot P(\\\\neg{E})\\\\end{alignat}\\\\)\n\nThus, expectation of posterior probability is equal to the prior probability.\n\nIn other way, if you expect the probability of a hypothesis to change as a result of observing some evidence, the amount of this change if the evidence is positive is\n\n*D*~1~ = *P*(*H*|*E*) − *P*(*H*).\n\nIf the evidence is negative, the change is\n\n*D*~2~ = *P*(*H*|¬*E*) − *P*(*H*).\n\nExpectation of the change given positive evidence is equal to negated expectation of the change given counterevidence:\n\n*D*~1~ ⋅ *P*(*E*) =  − *D*~2~ ⋅ *P*(¬*E*).\n\nIf you can *anticipate in advance* updating your belief in a particular direction, then you should just go ahead and update now. Once you know your destination, you are already there. \n\nNotable Posts\n-------------\n\n*   [Conservation of Expected Evidence](https://lessestwrong.com/lw/ii/conservation_of_expected_evidence/)\n*   [Mistakes with Conservation of Expected Evidence](https://lessestwrong.com/posts/zTfSXQracE7TW8x4w/mistakes-with-conservation-of-expected-evidence-1)\n\nSee Also\n--------\n\n*   [Filtered evidence](https://lessestwrong.com/tag/filtered-evidence)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "xcBbcAJrvTEkxikW9",
    "name": "Bucket Errors",
    "core": null,
    "slug": "bucket-errors",
    "oldSlugs": null,
    "postCount": 12,
    "description": {
      "markdown": "A **Bucket Error** is when multiple different concepts or variables are incorrectly lumped together in one's mind as a single concept/variable, potentially leading to distortions of one's thinking. Bucket Errors are related to [Fallacies of Compression](https://www.lesswrong.com/posts/y5MxoeacRKKM3KQth/fallacies-of-compression).  \n  \nThe term*,* *Bucket Error*, was introduced in [\"Flinching away from truth” is often about \\*protecting\\* the epistemology](https://www.lesswrong.com/posts/EEv9JeuY5xfuDDSgF/flinching-away-from-truth-is-often-about-protecting-the) where an example is given of a child who refuses to believe that they made a spelling mistake. The child is unwilling to believe this fact because they believe having made a spelling mistake is incompatible with becoming a writer which is their dream. \"Becoming a writer\" and \"never making spelling mistakes\" are lumped in the same bucket despite in fact being separate variables.\n\nBucket Errors are similar to the concepts of [equivocation](https://en.wikipedia.org/wiki/Equivocation), identification in Buddhism, or fusion/defusion in modern psychotherapy. See: [Fusion and Equivocation in Korzybski's General Semantics](https://www.lesswrong.com/posts/RQrWd5jPZQtpH8f4v/fusion-and-equivocation-in-korzybski-s-general-semantics).\n\n**Related Pages:** [Compartmentalization](https://www.lesswrong.com/tag/compartmentalization), [Distinctions](https://www.lesswrong.com/tag/distinctions)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "BhrpjXqGuke5GnF6g",
    "name": "Hamming Questions",
    "core": false,
    "slug": "hamming-questions",
    "oldSlugs": null,
    "postCount": 23,
    "description": {
      "markdown": "> Mathematician Richard Hamming used to ask scientists in other fields \"What are the most important problems in your field?\" partly so he could troll them by asking \"Why aren't you working on them?\" and partly because getting asked this question is really useful for focusing people's attention on what matters.   \n>   \n> CFAR developed the technique of \"Hamming Questions\" as different prompts to get your brain to (actually) think about the biggest problems, bottlenecks, and unspoken desires in *your* life.  \n>   \n> [(Taken from here)](https://www.lesswrong.com/posts/rnFLc3E5Y4FP8TSGC/the-biggest-problem-in-your-life)\n\n  \nSee also [Open Problems](https://www.lesswrong.com/tag/open-problems)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "yt9Z7xdQrofW7fCN8",
    "name": "Epistemic Review",
    "core": false,
    "slug": "epistemic-review",
    "oldSlugs": null,
    "postCount": 32,
    "description": {
      "markdown": "**Epistemic Reviews** take a closer look at an existing publication – such as a book, paper or blogpost – and evaluate whether its claims are true."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "hNFdS3rRiYgqqD8aM",
    "name": "Humor",
    "core": null,
    "slug": "humor",
    "oldSlugs": null,
    "postCount": 118,
    "description": {
      "markdown": "**Humor**. This tag is a joke."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "EnFKSZYiDHqMJuvJL",
    "name": "Social Reality",
    "core": false,
    "slug": "social-reality",
    "oldSlugs": null,
    "postCount": 39,
    "description": {
      "markdown": "**Social Reality** is \"that which exists, proportional to how much people believe in it\" (contrasted with \"regular reality\", which is \"that which exists, whether you believe in it or not\"). It includes both the rules that govern social interaction, and \"beliefs\" that people adopt as part of tribal membership."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "8XiMxJaWbjNtWLsEj",
    "name": "Cost Disease",
    "core": null,
    "slug": "cost-disease",
    "oldSlugs": null,
    "postCount": 8,
    "description": {
      "markdown": "**Cost Disease** or **Baumol's cost disease** is a name for the rise of salaries in jobs that have experienced no or low increase of labor productivity \\[[1](https://en.wikipedia.org/wiki/Baumol%27s_cost_disease)\\]. Some use the term generally to refer to rising costs in general \\[[2](https://www.lesswrong.com/posts/BBQ5HEnL3ShefQxEj/considerations-on-cost-disease)\\].\n\nOften the questions being asked in Cost Disease discussion are why the cost healthcare and education have increased many, many times over.\n\n**External Posts:**  \n[Book Review: Why Are The Prices So D*mn High?](https://slatestarcodex.com/2019/06/10/book-review-the-prices-are-too-dmn-high/) \\- Scott Alexander"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "h96z2Xt4h6zt2wiw2",
    "name": "GreaterWrong Meta",
    "core": false,
    "slug": "greaterwrong-meta",
    "oldSlugs": null,
    "postCount": 10,
    "description": {
      "markdown": "**GreaterWrong** is an alternative front-end/viewer for the LessWrong site. The viewer is accessible at [www.greaterwrong.com](https://www.greaterwrong.com). The project uses the API of the main LessWrong project but is maintained independently by LessWrong users, [clone of saturn](https://www.lessestwrong.com/users/clone-of-saturn) and [Said Achmiz](https://www.lessestwrong.com/users/saidachmiz)."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "ALwRRZqvhaop8gxkT",
    "name": "Groupthink",
    "core": false,
    "slug": "groupthink",
    "oldSlugs": null,
    "postCount": 25,
    "description": {
      "markdown": "**Groupthink** is a very well-documented source of [bias](https://www.lesswrong.com/tag/bias) in cognitive psychology. It refers to the tendency of humans to tend to agree with each other, and hold back objections or dissent even when the group is wrong.\n\nThere seems to be a balance of tensions between evaporative cooling of beliefs leading to groupthink, and extremely biased [color politics](https://www.lesswrong.com/tag/blues-and-greens-metaphor).\n\nEcho chamber\n------------\n\nAn **echo chamber** is a group of media sources that parrot each other by constantly and unquestioningly reporting a single, biased point of view, thus creating the illusion of consensus.\n\n\"Virtual community\"\n-------------------\n\nIn computer-mediated deliberation, the ideology of so-called **\"virtual community\"** has been implicated as a damaging source of groupthink. In the real world, a \"community\" is an ethical and political *compromise* of values due to a shared need for protection from bodily harm or harm to one's surrounding environment. A \"virtual community\" has no such natural values but still retains the [mind-killing](https://www.lesswrong.com/tag/mind-killer) illusion of protection, and is thus liable to turn into a cult.\n\nSee also\n--------\n\n*   [Cached thought](https://www.lesswrong.com/tag/cached-thought)\n*   [Affective death spiral](https://www.lesswrong.com/tag/affective-death-spiral), [information cascade](https://www.lesswrong.com/tag/information-cascades)\n*   [In-group bias](https://www.lesswrong.com/tag/in-group-bias), [conformity bias](https://www.lesswrong.com/tag/conformity-bias)\n*   [Goodhart's law](https://www.lesswrong.com/tag/goodhart-s-law)\n*   [Availability heuristic](https://www.lesswrong.com/tag/availability-heuristic)\n*   [Group rationality](https://www.lesswrong.com/tag/group-rationality)\n*   [Problem of verifying rationality](https://www.lesswrong.com/tag/problem-of-verifying-rationality)\n*   [Death Spirals and the Cult Attractor](https://www.lesswrong.com/tag/death-spirals-and-the-cult-attractor)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "PJKgSRkXkCqXmCk3M",
    "name": "Mind Projection Fallacy",
    "core": false,
    "slug": "mind-projection-fallacy",
    "oldSlugs": null,
    "postCount": 21,
    "description": {
      "markdown": "The **Mind Projection Fallacy** is the error of projecting the properties of your own mind onto the external world. For example, one might erroneously think that because they enjoy the taste of chocolate, the chocolate has the inherent property of tastiness, and therefore everyone else must like its taste too.\n\nOvercoming the mind projection fallacy requires realizing that our minds are not transparent windows unto veridical reality; when you look at a rock, you experience not the the rock itself, but your mind's *representation* of the rock, reconstructed from photons bouncing off its surface. Sugar in and of itself is not *inherently* sweet; the sugar itself only has the chemical properties that it does, which your brain *interprets* as sweet.\n\nHistory\n-------\n\nPhysicist and [Bayesian](https://lessestwrong.com/tag/bayesianism) philosopher [E.T. Jaynes](https://en.wikipedia.org/wiki/Edwin_Thompson_Jaynes) coined the term *mind projection fallacy* to refer to this kind of failure to distinguish between epistemological claims (statements about belief, about your map, about what we can *say* about reality) and ontological claims (statements about reality, about the territory, about how things *are*). In particular, the concept was applied in the critique of [frequentist](https://en.wikipedia.org/wiki/Frequentist_inference) interpretation of the notion of [probability](https://wiki.lesswrong.com/wiki/probability) as a property of physical systems rather than an epistemic device concerned with levels of certainty, [Bayesian probability](https://lessestwrong.com/tag/bayesian-probability).\n\nNotable Posts\n-------------\n\n*   [Mind Projection Fallacy](https://lessestwrong.com/lw/oi/mind_projection_fallacy/)\n*   [Probability is in the Mind](https://lessestwrong.com/lw/oj/probability_is_in_the_mind/)\n*   [Examples of the Mind Projection Fallacy?](https://lessestwrong.com/lw/8tv/examples_of_the_mind_projection_fallacy/)\n\nSee Also\n--------\n\n*   [Bayesian probability](https://lessestwrong.com/tag/bayesian-probability)\n*   [Magic](https://lessestwrong.com/tag/magic)\n*   [The map is not the territory](https://lessestwrong.com/tag/the-map-is-not-the-territory)\n*   [2-place and 1-place words](https://lessestwrong.com/tag/2-place-and-1-place-words)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "GQyPQcdEQF4zXhJBq",
    "name": "List of Links",
    "core": null,
    "slug": "list-of-links",
    "oldSlugs": null,
    "postCount": 80,
    "description": {
      "markdown": "Some posts contain a **list of links**, either to other Less Wrong posts or to external websites**.** These posts may make it easy to find many resources on a topic, or offer a variety of interesting things to read.\n\n**Related Pages:** [Repository](https://www.lesswrong.com/tag/repository-1)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "YWzByWvtXunfrBu5b",
    "name": "GPT",
    "core": null,
    "slug": "gpt",
    "oldSlugs": [
      "gpt-2"
    ],
    "postCount": 136,
    "description": {
      "markdown": "**GPT** (Generative Pretrained Transformer) is a family of large transformer-based language models created by OpenAI. Its ability to generate remarkably human-like responses has relevance to discussions on AGI."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5hpGj9nDLgokfghvR",
    "name": "Confirmation Bias",
    "core": null,
    "slug": "confirmation-bias",
    "oldSlugs": null,
    "postCount": 25,
    "description": {
      "markdown": "**Confirmation bias** (also known as positive bias) is the tendency to search for, interpret, favor, and recall information in a way that confirms or strengthens one's prior personal beliefs or hypotheses \\[[1](https://en.wikipedia.org/wiki/Confirmation_bias)\\].  For example, one might test hypotheses with positive rather than negative examples, thus missing obvious disconfirming tests.\n\n_See also:_ [Motivated skepticism](https://www.lesswrong.com/tag/motivated-skepticism), [Availability heuristic](https://lesswrong.com/tag/availability-heuristic), [Surprise](https://www.lesswrong.com/tag/surprise), [Narrative fallacy](https://www.lesswrong.com/tag/narrative-fallacy), [Privileging the hypothesis](https://www.lesswrong.com/tag/privileging-the-hypothesis), [Heuristics and Biases](https://www.lesswrong.com/tag/heuristics-and-biases)\n\nExternal Links\n--------------\n\n*   [Speculations on the Future of Science](https://www.edge.org/conversation/kevin_kelly-speculations-on-the-future-of-science) by Kevin Kelly\n*   [On the Failure to Eliminate Hypotheses in a Conceptual Task](http://psy2.ucsd.edu/~mckenzie/Wason1960QJEP.pdf) by P.C. Wason\n*   [Write Your Hypothetical Apostasy](https://www.overcomingbias.com/2009/02/write-your-hypothetical-apostasy.html) by [Nick Bostrom](https://www.lesswrong.com/tag/nick-bostrom)\n*   [Confirmation Bias](https://en.wikipedia.org/wiki/Confirmation_bias), Wikipedia"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "YQW2DxpZFTrqrxHBJ",
    "name": "Inferential Distance",
    "core": null,
    "slug": "inferential-distance",
    "oldSlugs": null,
    "postCount": 46,
    "description": {
      "markdown": "**Inferential Distance** between two people with respect to an item of knowledge is the amount of steps or concepts a person needs to share before they can successfully communicate the [object level](https://www.lesswrong.com/tag/object-level-and-meta-level) point. This can be thought of as the missing foundation or building block concepts needed to think clearly about a specific thing.\n\nIn [Expecting Short Inferential Distances](https://www.lessestwrong.com/posts/HLqWn5LASfhhArZ7w/expecting-short-inferential-distances), Eliezer Yudkowsky posits that humans systematically underestimate inferential distances.\n\n> *And if you think you can explain the concept of “systematically underestimated inferential distances” briefly, in just a few words, I’ve got some sad news for you . . . –* [Expecting Short Inferential Distances](https://www.lessestwrong.com/posts/HLqWn5LASfhhArZ7w/expecting-short-inferential-distances)\n\nExample: Evidence for Evolution\n-------------------------------\n\nExplaining the [evidence](https://lessestwrong.com/tag/evidence) for the theory of [evolution](https://lessestwrong.com/tag/evolution) to a physicist would be easy; even if the physicist didn't already know about evolution, they would understand the concepts of evidence, [Occam's razor](https://lessestwrong.com/tag/occam-s-razor), naturalistic explanations, and the general orderly nature of the universe. Explaining the evidence for the theory of evolution to someone without a science background would be much harder. Before even mentioning the specific evidence for evolution, you would have to explain the concept of evidence, why some kinds of evidence are more valuable than others, what does and doesn't count as evidence, and so on. This would be unlikely to work during a short conversation.\n\nThere is a short inferential distance between you and the physicist; there is a very long inferential distance between you and the person without any science background. Many members of Less Wrong believe that expecting short inferential distances is a classic error. It is also a very difficult problem to solve, since most people will feel [offended](https://wiki.lesswrong.com/wiki/offence) if you explicitly say that there is too great an inferential distance between you to explain a theory properly. Some people have attempted to explain this through [evolutionary psychology](https://lessestwrong.com/tag/evolutionary-psychology): in the ancestral environment, there was minimal difference in knowledge between people, and therefore no need to worry about inferential distances.\n\nExternal Links\n--------------\n\n*   [Why It's Hard to Explain Things: Inferential Distance](http://everydayutilitarian.com/essays/why-its-hard-to-explain-things-inferential-distance/) by Peter Hurford\n*   [How all human communication fails, except by accident, or a commentary of Wiio's laws](https://jkorpela.fi/wiio.html)\n\nSee Also\n--------\n\n*   [General knowledge](https://lessestwrong.com/tag/general-knowledge)\n*   [Modesty argument](https://lessestwrong.com/tag/modesty-argument)\n*   [Illusion of transparency](https://lessestwrong.com/tag/illusion-of-transparency)\n*   [Absurdity heuristic](https://lessestwrong.com/tag/absurdity-heuristic)\n*   [Common Knowledge](https://www.lesswrong.com/tag/common-knowledge)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "bTeiZr6YAEaSPQTC8",
    "name": "Solomonoff Induction",
    "core": null,
    "slug": "solomonoff-induction",
    "oldSlugs": null,
    "postCount": 44,
    "description": {
      "markdown": "**Solomonoff induction** is an inference system defined by [Ray Solomonoff](https://en.wikipedia.org/wiki/Ray_Solomonoff) that will learn to correctly predict any computable sequence with only the absolute minimum amount of data. This system, in a certain sense, is the perfect universal prediction algorithm. \n\nTo summarize it very informally, Solomonoff induction works by:\n\n*   Starting with all possible hypotheses (sequences) as represented by computer programs (that generate those sequences), weighted by their simplicity (2^-^**^n^**, where **n** is the program length);\n*   Discarding those hypotheses that are inconsistent with the data.\n\nWeighting hypotheses by simplicity, the system automatically incorporates a form of [Occam's razor](https://www.lesswrong.com/tag/occam-s-razor), which is why it has been playfully referred to as *Solomonoff's lightsaber*.\n\nSolomonoff induction gets off the ground with a solution to the \"problem of the priors\". Suppose that you stand before a universal [prefix Turing machine](http://www.scholarpedia.org/article/Algorithmic_complexity#Prefix_Turing_machine) *U*. You are interested in a certain finite output string *y*~0~. In particular, you want to know the probability that *U* will produce the output *y*~0~ given a random input tape. This probability is the **Solomonoff** ***a priori*** **probability** of *y*~0~.\n\nMore precisely, suppose that a particular infinite input string *x*~0~ is about to be fed into *U*. However, you know nothing about *x*~0~ other than that each term of the string is either 0 or 1. As far as your state of knowledge is concerned, the *i*th digit of *x*~0~ is as likely to be 0 as it is to be 1, for all *i* = 1, 2, …. You want to find the *a priori* probability *m*(*y*~0~) of the following proposition:\n\n(*) If *U* takes in *x*~0~ as input, then *U* will produce output *y*~0~ and then halt.\n\nUnfortunately, computing the exact value of *m*(*y*~0~) would require solving the halting problem, which is undecidable. Nonetheless, it is easy to derive an expression for *m*(*y*~0~). If *U* halts on an infinite input string *x*, then *U* must read only a finite initial segment of *x*, after which *U* immediately halts. We call a finite string *p* a *self-delimiting program* if and only if there exists an infinite input string *x* beginning with *p* such that *U* halts on *x* immediately after reading to the end of *p*. The set 𝒫 of self-delimiting programs is the *prefix code* for *U*. It is the determination of the elements of 𝒫 that requires a solution to the halting problem.\n\nGiven *p* ∈ 𝒫, we write \"prog (*x*~0~) = *p*\" to express the proposition that *x*~0~ begins with *p*, and we write \"*U*(*p*) = *y*~0~\" to express the proposition that *U* produces output *y*~0~, and then halts, when fed any input beginning with *p*. Proposition (*) is then equivalent to the exclusive disjunction\n\n  \n⋁*~p~*~ ∈ 𝒫: ~*~U~*~(~*~p~*~) = ~*~y~*~0~(prog (*x*~0~) = *p*).  \nSince *x*~0~ was chosen at random from {0, 1}*^ω^*, we take the probability of prog (*x*~0~) = *p* to be 2^ − ℓ(^*^p^*^)^, where ℓ(*p*) is the length of *p* as a bit string. Hence, the probability of (*) is\n\n  \n*m*(*y*~0~) := ∑*~p~*~ ∈ 𝒫: ~*~U~*~(~*~p~*~) = ~*~y~*~0~2^ − ℓ(^*^p^*^)^.  \n \n\nSee also\n--------\n\n*   [Kolmogorov complexity](https://www.lesswrong.com/tag/kolmogorov-complexity)\n*   [AIXI](https://www.lesswrong.com/tag/aixi)\n*   [Occam's razor](https://www.lesswrong.com/tag/occam-s-razor)\n\nReferences\n----------\n\n*   [Algorithmic probability](http://www.scholarpedia.org/article/Algorithmic_probability) on Scholarpedia"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5BvRW4FxdD8DFhiew",
    "name": "Pica",
    "core": null,
    "slug": "pica",
    "oldSlugs": null,
    "postCount": 6,
    "description": {
      "markdown": "**Pica** is the name of a medical condition where people with iron deficiency eat ice cubes. There is no iron in ice cubes, but iron and crunchiness sort of go together. By analogy, **pica** is also when people are missing something, and respond by doing something which doesn't provide it, or doesn't provide much of it, like watching sitcoms because they're lonely or playing Minecraft because they feel unproductive.\n\n[AllAmericanBreakfast suggests](https://www.lesswrong.com/posts/L6Ktf952cwdMJnzWm/motive-ambiguity?commentId=QLS75v2wdDHpo9CX3) that pica is like [Goodhart's Law](https://www.lesswrong.com/tag/goodhart-s-law), with the added [failure](https://www.lesswrong.com/tag/goodhart-s-law) that the metric being maximized isn't even clearly related to the problem you're trying to solve, giving two examples:\n\n*   Evaluate your startup by the sheer effort you're putting in? That's Goodhart's Law. Evaluate it by [how cool the office looks](https://www.youtube.com/watch?v=zbKaPN-0NcM&ab_channel=LeslieKnopeRocks)? That's pica.\n*   Evaluate your relationship by the sheer amount of physical affection? That's Goodhart's Law. Evaluate it by how much misery you put each other through \"for love?\" That's pica\n\n**Related Pages:** [Akrasia](https://www.lesswrong.com/tag/akrasia), [Goodhart's Law](https://www.lesswrong.com/tag/goodhart-s-law)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "eamWQNQ2dPYWEwhqr",
    "name": "Goal Factoring",
    "core": null,
    "slug": "goal-factoring",
    "oldSlugs": null,
    "postCount": 13,
    "description": {
      "markdown": "**Goal Factoring** is a rationality technique for planning which proceeds by first identifying the underlying goals motivating one or more behaviors and then searching for alternative sets of behaviors that better accomplish the goals. \n\nFor example, someone might refactor the two behaviors of *going to the gym* and *browsing Facebook* into the single behavior of *play tennis with my friend* which more efficiently and effectively accomplishes the underlying goals of *have social interaction* and *get exercise*."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "stnsBEmuGpnSfQ5vj",
    "name": "Sunk-Cost Fallacy",
    "core": null,
    "slug": "sunk-cost-fallacy",
    "oldSlugs": null,
    "postCount": 9,
    "description": {
      "markdown": "The **Sunk Cost Fallacy** is the tendency to consider costs that have already been paid and cannot be reclaimed when deciding whether to continue a project. Thinking you have to keep going because you've already put in so much. In reality, it is usually much more important to consider whether the benefits of finishing the project are worth more than the *remaining* costs.\n\nExternal Links\n--------------\n\n*   [Are sunk costs fallacies?](http://www.gwern.net/Sunk%20cost)\n\nSee Also\n--------\n\n*   [Status quo bias](https://lessestwrong.com/tag/status-quo-bias)\n*   [Loss aversion](https://lessestwrong.com/tag/loss-aversion), [Prospect theory](https://lessestwrong.com/tag/prospect-theory)\n*   [Cached thought](https://lessestwrong.com/tag/cached-thought)\n*   [Narrative fallacy](https://lessestwrong.com/tag/narrative-fallacy)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "Zwc2JcT5az4e5YpJy",
    "name": "Rationality Quotes",
    "core": null,
    "slug": "rationality-quotes",
    "oldSlugs": null,
    "postCount": 134,
    "description": {
      "markdown": "**Rationality Quotes** was a series of posts and threads on Overcoming Bias and Less Wrong, where users would post quotes that had some connection to rationality or other popular Less Wrong discussion topics.\n\nIt started as a series of posts by Eliezer Yudkowsky on Overcoming Bias, posting from his files of collected quotes on days when he didn't have a normal post ready. When Less Wrong was created, there was a new Rationality Quotes thread each month, where all users could post quotes.\n\nRationality Quotes threads got fewer and fewer comments during the [2015-2017 Less Wrong decline](https://www.lesswrong.com/posts/S69ogAGXcc9EQjpcZ/a-brief-history-of-lesswrong), and the last thread was for April-June 2017.\n\nSee also: [Open Threads](https://www.lesswrong.com/tag/open-thread), [History of Rationality](https://www.lesswrong.com/tag/history-of-rationality), [Humor](https://www.lesswrong.com/tag/humor)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "R6dqPii4cyNpuecLt",
    "name": "Prediction Markets",
    "core": null,
    "slug": "prediction-markets",
    "oldSlugs": null,
    "postCount": 69,
    "description": {
      "markdown": "**Prediction markets** are speculative markets created for the purpose of making predictions. Assets are created whose final cash value is tied to a particular event or parameter. The current market prices can then be interpreted as predictions of the probability of the event or the expected value of the parameter. Prediction markets are thus structured as betting exchanges, without any risk for the bookmaker. [Robin Hanson](https://lessestwrong.com/tag/robin-hanson) was the first to run a corporate prediction market - at Project Xanadu -, and has made several contributions to the field such as: conditional predictions, accuracy issues and market and media manipulation.\n\nPeople who buy low and sell high are rewarded for improving the market prediction, while those who buy high and sell low are punished for degrading the market prediction. Evidence so far suggests that prediction markets are at least as accurate as other institutions predicting the same events with a similar pool of participants.\n\nPredictions markets have been used by organizations such as Google, General Electric, and Microsoft; several online and commercial prediction markets are also in operation. Historically, prediction markets have often been used to predict election outcomes.\n\nSee Also\n--------\n\n*   [Prediction](https://lessestwrong.com/tag/forecasting-and-prediction)\n*   [Economic consequences of AI and whole brain emulation](https://lessestwrong.com/tag/economic-consequences-of-ai-and-whole-brain-emulation)\n*   [Group rationality](https://lessestwrong.com/tag/group-rationality)\n*   [Making beliefs pay rent](https://lessestwrong.com/tag/making-beliefs-pay-rent)\n*   [QURI](https://www.lesswrong.com/tag/quri)\n\nExternal Posts\n--------------\n\n*   [A 1990 Corporate Prediction Market](http://www.overcomingbias.com/2006/11/first_known_bus.html) by [Robin Hanson](https://lessestwrong.com/tag/robin-hanson)\n*   [Leamer's 1986 Idea Futures Proposal](http://www.overcomingbias.com/2006/12/leamers_1986_id.html) by Robin Hanson\n*   [Should Prediction Markets be Charities?](http://www.overcomingbias.com/2006/12/should_predicti.html) by Peter McCluskey\n*   [The Future of Oil Prices 2: Option Probabilities](http://www.overcomingbias.com/2006/12/the_future_of_o_1.html) by [Hal Finney](https://en.wikipedia.org/wiki/Hal_Finney_(cipherpunk))\n*   [Prediction Markets As Collective Intelligence](http://www.overcomingbias.com/2009/09/prediction-markets-as-collective-inteligence.html) by Robin Hanson\n*   [Fixing Election Markets](http://www.overcomingbias.com/2011/11/conditional-close-election-markets.html) by Robin Hanson\n*   [Prediction Markets](http://www.gwern.net/Prediction%20markets) at gwern.net\n*   [Idea Futures (a.k.a. Prediction Markets)](http://hanson.gmu.edu/ideafutures.html) by Robin Hanson\n\n### External Links\n\n*   [Comparing face-to-face meetings, nominal groups, Delphi and prediction markets on an estimation task](http://dl.dropbox.com/u/5317066/2011-graefe.pdf)\n*   [Video of Robin Hanson's Combinatorial Prediction Markets lecture at the Uncertainty in Artificial Intelligence conference in Helsinki, 2008](http://videolectures.net/uai08_hanson_cpm/)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "BXL4riEJvJJHoydjG",
    "name": "Orthogonality Thesis",
    "core": null,
    "slug": "orthogonality-thesis",
    "oldSlugs": null,
    "postCount": 20,
    "description": {
      "markdown": "The **Orthogonality Thesis** states that an agent can have any combination of intelligence level and final goal, that is, its [final goals](https://www.lesswrong.com/tag/utility-functions?showPostCount=true&useTagName=true) and [intelligence levels](https://www.lesswrong.com/tag/general-intelligence?showPostCount=true&useTagName=true) can vary independently of each other. This is in contrast to the belief that, because of their intelligence, AIs will all converge to a common goal.\n\nThe thesis was originally defined by [Nick Bostrom](https://lessestwrong.com/tag/nick-bostrom) in the paper \"[Superintelligent Will](https://nickbostrom.com/superintelligentwill.pdf)\", (along with the [instrumental convergence thesis](https://wiki.lesswrong.com/wiki/instrumental_convergence_thesis)). For his purposes, Bostrom defines intelligence to be [instrumental rationality](https://wiki.lesswrong.com/wiki/instrumental_rationality).\n\n*Related:* [*Complexity of Value*](https://www.lesswrong.com/tag/complexity-of-value?showPostCount=true&useTagName=true)*,* [*Decision Theory*](https://www.lesswrong.com/tag/decision-theory?showPostCount=true&useTagName=true)*,* [*General Intelligence*](https://www.lesswrong.com/tag/general-intelligence?showPostCount=true&useTagName=true)*,* [*Utility Functions*](https://www.lesswrong.com/tag/utility-functions?showPostCount=true&useTagName=true)\n\nDefense of the thesis\n---------------------\n\nIt has been pointed out that the orthogonality thesis is the default position, and that the burden of proof is on claims that limit possible AIs. Stuart Armstrong writes that,\n\nOne reason many researchers assume superintelligent agents to converge to the same goals may be because [most humans](https://lessestwrong.com/tag/human-universal) have similar values. Furthermore, many philosophies hold that there is a rationally correct morality, which implies that a sufficiently rational AI will acquire this morality and begin to act according to it. Armstrong points out that for formalizations of AI such as [AIXI](https://lessestwrong.com/tag/aixi) and [Gödel machines](https://lessestwrong.com/tag/g%C3%B6del-machine), the thesis is known to be true. Furthermore, if the thesis was false, then [Oracle AIs](https://lessestwrong.com/tag/oracle-ai) would be impossible to build, and all sufficiently intelligent AIs would be impossible to control.\n\nPathological Cases\n------------------\n\nThere are some pairings of intelligence and goals which cannot exist. For instance, an AI may have the goal of using as little resources as possible, or simply of being as unintelligent as possible. These goals will inherently limit the degree of intelligence of the AI.\n\nSee Also\n--------\n\n*   [Instrumental Convergence](https://lessestwrong.com/tag/instrumental-convergence)\n\nExternal links\n--------------\n\n*   Definition of the orthogonality thesis from Bostrom's [Superintelligent Will](http://www.nickbostrom.com/superintelligentwill.pdf)\n*   [Arbital orthogonality thesis article ](https://arbital.com/p/orthogonality/)\n*   [Critique](http://philosophicaldisquisitions.blogspot.com/2012/04/bostrom-on-superintelligence-and.html) of the thesis by John Danaher\n*   Superintelligent Will paper by Nick Bostrom"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "EY623WWCXKTvN3kmj",
    "name": "Factored Cognition",
    "core": null,
    "slug": "factored-cognition",
    "oldSlugs": null,
    "postCount": 29,
    "description": {
      "markdown": "**Factored cognition** is an approach to artificial intelligence where sophisticated learning and reasoning is broken down (or factored) into many small and mostly independent tasks \\[[1](https://ought.org/research/factored-cognition)\\].\n\nFactored Cognition is related to [*Iterated Amplification* (IDA)](https://www.lesswrong.com/tag/iterated-amplification)."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "9mShmhRFzBat3523A",
    "name": "Perceptual Control Theory",
    "core": null,
    "slug": "perceptual-control-theory",
    "oldSlugs": null,
    "postCount": 7,
    "description": {
      "markdown": "**Perceptual control theory (PCT)** is a psychological theory of animal and human behavior. PCT postulates that an organism's behavior is a means of controlling its perceptions. The model is based on the principles of negative feedback \\[[1](https://en.wikipedia.org/wiki/Perceptual_control_theory)\\]. It is to some extent an application of the ideas used in the engineering discipline of control theory to the modeling of the human mind and behavior.  \n  \nPCT postulates that layers of control systems, which have access to a metric to optimize and some set of policies or actions, can maintain balancing-acts for difficult, high-abstraction things without developing any explicit model for how those actions relate to the metric being tracked. The brain is postulated to be one of these multi-layered PCTs.  \n  \nPhysical movements are a favorite case-study, since they're relatively easy to break down into this these sorts of layered control theory sub-problems. An important subtlety is that the control systems are optimizing for the perception of a state, rather than for a concrete environmental state itself.  \n  \nActions and behaviors develop because they do something to reduce the mismatch between internal perception, and the stimulus readings received.\n\nControversy\n-----------\n\nIt's unclear whether PCT is a valid theory. It doesn't significantly constrain the space of possible minds that could be built from it, and the advocates of the theory on the blog were unable to make a clear case for it. Experimental results for its quality as an algorithm seemed lackluster; see [these](https://lessestwrong.com/lw/14v/the_usefulness_of_correlations/11iu/) [critical comments](https://lessestwrong.com/lw/14v/the_usefulness_of_correlations/11j6/) about the paper version of [this technical report](http://www.rand.org/pubs/drafts/DRU2751/), which claim that the correct results may have been achieved more through parameter-fitting than PCT.  \n  \nSome anecdotally found it more useful for explaining bugs in human behavior, than for modeling what would be ideal behavior.\n\nUnder-Characterized Information Storage\n---------------------------------------\n\nThis seems to be a common category of complaints about Perceptual Control Theory.  \n  \nThis [blog post](http://psychsciencenotes.blogspot.com/2016/01/a-quick-review-and-analysis-of.html) called out that PCT \"has no theory of information or how that information comes to be made,\" and [this post](https://www.lesswrong.com/posts/fJKbCXrCPwAR5wjL8/what-is-control-theory-and-why-do-you-need-to-know-about-it) grappled with a similar problem: struggling to find a place for implicit models, priors, and updates when working with a PCT framework. ([This](https://www.lesswrong.com/posts/fJKbCXrCPwAR5wjL8/what-is-control-theory-and-why-do-you-need-to-know-about-it?commentId=JzphwDyjg6YXBHkAc) comment may have made a case for at least some embedded implicit priors.)\n\nNotable Posts\n-------------\n\n*   Kennaway's [What is control theory](https://www.lesswrong.com/posts/fJKbCXrCPwAR5wjL8/what-is-control-theory-and-why-do-you-need-to-know-about-it), and his prior post [Without Models](https://www.lesswrong.com/posts/Ba6buPA3u2btdKS82/without-models)\n*   SSC book reviews of [Surfing Uncertainty](https://slatestarcodex.com/2017/09/05/book-review-surfing-uncertainty/) and [Behavior: The Control of Perception](https://slatestarcodex.com/2017/03/06/book-review-behavior-the-control-of-perception/)\n    *   Scott Alexander theorizes that it's analogous to Friston's [Free Energy Principle](https://en.wikipedia.org/wiki/Free_energy_principle), [here](https://slatestarcodex.com/2017/09/06/predictive-processing-and-perceptual-control/) and [here](https://slatestarcodex.com/2019/03/20/translating-predictive-coding-into-perceptual-control/)\n*   Vaniver's [Introduction to Control Theory](https://www.lesswrong.com/posts/dcRY7XSnuARkHkA5D/an-introduction-to-control-theory), and review of [Behavior: The Control of Perception](https://www.lesswrong.com/posts/nPs63hpijnQs37jme/behavior-the-control-of-perception)\n    *   His personal-blog thoughts on the topic [here](https://www.lesswrong.com/posts/cMKNFE8hWKNhnEXtM/control-theory-commentary)\n\nSee also\n--------\n\n*   [Control Theory](https://lessestwrong.com/tag/control-theory)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "9DNZfxFvY5iKoZQbz",
    "name": "Interviews",
    "core": null,
    "slug": "interviews",
    "oldSlugs": null,
    "postCount": 60,
    "description": {
      "markdown": "**Interviews**\n\n**Related Pages:** [Interview Series On Risks From AI](https://www.lesswrong.com/tag/interview-series-on-risks-from-ai), [Dialogue (format)](https://www.lesswrong.com/tag/dialogue-format)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "khReijeucXJTnsyMT",
    "name": "Bounties (closed)",
    "core": false,
    "slug": "bounties-closed",
    "oldSlugs": [
      "bounties"
    ],
    "postCount": 40,
    "description": {
      "markdown": "A **bounty** or is a monetary payment for accomplishing some one-off task. (If the winner is selected in a competition, it is often referred to as a **prize**.) On LessWrong, bounties have historically been paid out for things like providing useful information, doing a novel piece of research, or changing someone's mind about a topic. \n\nThis tag is for bounties that have already been paid out. It has a sister tag for active bounties.  \n\n*If you're hosting a bounty, please make sure to change the tags to indicate the status of the bounty. *\n\nSee also: [Bounties (active)](https://www.lesswrong.com/tag/bounties-active), [Grants and Fundraising Opportunities](https://www.lesswrong.com/tag/grants-and-fundraising-opportunities), [Bountied Rationality Facebook group](https://www.facebook.com/groups/1781724435404945/)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "ABG8vt87eW4FFA6gD",
    "name": "Open Threads",
    "core": false,
    "slug": "open-threads",
    "oldSlugs": [
      "open-thread"
    ],
    "postCount": 456,
    "description": {
      "markdown": "**Open Threads** are informal discussion areas, where users are welcome to post comments that didn't quite feel big enough to warrant a top-level post, nor fit in other posts.\n\nSometimes an Open Thread focuses on a specific topic. The most common Open Threads are the monthly Open and Welcome Threads, which serve as a general focal point of discussion, as well as a place for new users to introduce themselves."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb178",
    "name": "Artificial General Intelligence",
    "core": null,
    "slug": "artificial-general-intelligence",
    "oldSlugs": null,
    "postCount": 0,
    "description": {
      "markdown": "An **Artificial general intelligence**, or **AGI**, is a machine capable of behaving intelligently over many domains. The term can be taken as a contrast to *narrow AI*, systems that do things that would be considered intelligent if a human were doing them, but that lack the sort of general, flexible learning ability that would let them tackle entirely new domains. Though modern computers have drastically more ability to calculate than humans, this does not mean that they are generally intelligent, as they have little ability to invent new problem-solving techniques, and their abilities are targeted in narrow domains.\n\n*Related:* [AI (the main AI wiki-tag page)](https://www.lesswrong.com/tag/ai)\n\nAGIs and Humans\n---------------\n\nDirectly comparing the performance of AI to human performance is often an instance of [anthropomorphism](https://www.lesswrong.com/tag/anthropomorphism). The internal workings of an AI [need not resemble](https://www.lesswrong.com/tag/mind-design-space) those of a human; an AGI could have a radically different set of capabilities than those we are used to seeing in our fellow humans. A powerful AGI capable of operating across many domains could achieve competency in any domain that exceeds that of any human.\n\nThe [values](https://www.lesswrong.com/tag/utility-functions) of an AGI could also be [distinctly alien](https://www.lesswrong.com/tag/alien-values) to those of humans, in which case it won't see many human activities as worthwhile and would have [no intention](https://www.lesswrong.com/tag/giant-cheesecake-fallacy) of exceeding human performance (according to the human valuation of performance). Comparing an AGI's preferences to those of humans, AGI are classified as [Friendly](https://www.lesswrong.com/tag/friendly-artificial-intelligence) and [Unfriendly](https://www.lesswrong.com/tag/unfriendly-artificial-intelligence). An Unfriendly AGI would pose a large [existential risk](https://www.lesswrong.com/tag/existential-risk).\n\n\"AGI\" as a design paradigm\n--------------------------\n\nThe term \"Artificial General Intelligence,\" [introduced by Shane Legg and Mark Gubrud](http://wp.goertzel.org/?p=173), is often used to refer more specifically to a design paradigm which mixes modules of different types: \"neat\" and \"scruffy\", symbolic and subsymbolic. [Ben Goertzel](https://www.lesswrong.com/tag/ben-goertzel) is the researcher most commonly associated with this approach, but others, including [Peter Voss](https://wiki.lesswrong.com/wiki/Peter_Voss), are also pursuing it. This design paradigm, though eclectic in adopting various techniques, stands in contrast to other approaches to creating new kinds of artificial general intelligence (in the broader sense), including brain emulation, artificial evolution, Global Brain, and pure \"neat\" or \"scruffy\" AI.\n\nExpected dates for the creation of AGI\n--------------------------------------\n\nReasons for expecting an AGI's creation in the near future include the continuation of [Moore's law](https://wiki.lesswrong.com/wiki/Moore's_law), larger datasets for machine learning, progress in the field of neuroscience, increasing population and collaborative tools, and the massive incentives for its creation. A survey of experts taken at a 2011 [Future of Humanity Institute](https://www.lesswrong.com/tag/future-of-humanity-institute-fhi) conference on machine intelligence found a 50% confidence median estimate of 2050 for the creation of an AGI, and 90% confidence in 2150. A significant minority of the AGI community views the prospects of an [intelligence explosion](https://www.lesswrong.com/tag/intelligence-explosion) or the loss of control over an AGI [very skeptically](https://www.lesswrong.com/tag/agi-skepticism) however.\n\nBlog posts\n----------\n\n*   [Artificial Mysterious Intelligence](http://lesswrong.com/lw/wk/artificial_mysterious_intelligence/)\n*   [AGI Quotes](http://lesswrong.com/lw/8a9/agi_quotes/) by [lukeprog](http://lukeprog)\n\nReferences\n----------\n\n*   [Machine Intelligence Survey](http://www.fhi.ox.ac.uk/__data/assets/pdf_file/0015/21516/MI_survey.pdf) by Anders Sandberg and Nick Bostrom\n*   [How Long Until Human-Level AI? Results from an Expert Assessment](http://sethbaum.com/ac/2011_AI-Experts.pdf), survey at AGI-09 by Seth D. Baum, Ben Goertzel, and Ted G. Goertzel\n*   [Intelligence Explosion: Evidence and Import](http://commonsenseatheism.com/wp-content/uploads/2012/02/Muehlhauser-Salamon-Intelligence-Explosion-Evidence-and-Import.pdf) by Luke Muehlhauser and Anna Salamon\n*   [Pei Wang on the Path to Artificial General Intelligence](http://hplusmagazine.com/2011/01/27/pei-wang-path-artificial-general-intelligence/) by Ben Goertzel\n*   [Interim Report from the Panel Chairs](http://www.aaai.org/Organization/Panel/panel-note.pdf), AAAI\n\nSee Also\n--------\n\n*   [AGI skepticism](https://www.lesswrong.com/tag/agi-skepticism)\n*   [Friendly AI](https://wiki.lesswrong.com/wiki/Friendly_AI)\n*   [Unfriendly AI](https://wiki.lesswrong.com/wiki/Unfriendly_AI), [paperclip maximizer](https://www.lesswrong.com/tag/paperclip-maximizer)\n*   [Really powerful optimization process](https://www.lesswrong.com/tag/really-powerful-optimization-process)\n*   [Intelligence explosion](https://www.lesswrong.com/tag/intelligence-explosion), [technological singularity](https://wiki.lesswrong.com/wiki/technological_singularity)\n*   [Mind design space](https://www.lesswrong.com/tag/mind-design-space)\n*   [Singleton](https://www.lesswrong.com/tag/singleton)\n*   [Anthropomorphism](https://www.lesswrong.com/tag/anthropomorphism), [giant cheesecake fallacy](https://www.lesswrong.com/tag/giant-cheesecake-fallacy)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb219",
    "name": "The Hanson-Yudkowsky AI-Foom Debate",
    "core": null,
    "slug": "the-hanson-yudkowsky-ai-foom-debate",
    "oldSlugs": null,
    "postCount": 0,
    "description": {
      "markdown": "In late 2008, an extensive and long-awaited debate about the [Technological Singularity](https://wiki.lesswrong.com/wiki/Technological_Singularity) occurred on [Overcoming Bias](https://www.lesswrong.com/tag/overcoming-bias), mainly between [Robin Hanson](https://www.lesswrong.com/tag/robin-hanson) and [Eliezer Yudkowsky](https://www.lesswrong.com/tag/eliezer-yudkowsky). It focused on the likelihood of [hard AI takeoff](https://wiki.lesswrong.com/wiki/hard_takeoff) (**\"**[**FOOM**](https://wiki.lesswrong.com/wiki/FOOM)**\"**), the need for a theory of [Friendliness](https://wiki.lesswrong.com/wiki/Friendly_AI), and the future of [AGI](https://wiki.lesswrong.com/wiki/AGI), [whole brain emulation](https://www.lesswrong.com/tag/whole-brain-emulation), and [recursive self-improvement](https://www.lesswrong.com/tag/recursive-self-improvement) in general. A stand-up debate on the same themes took place at *Jane Street Capital* in June 2011, and Hanson made a further post in 2014.\n\nSee Also\n--------\n\n*   [AI Takeoff](/tag/ai-takeoff)\n*   [Intelligence explosion](https://www.lesswrong.com/tag/intelligence-explosion)\n*   [Outside view](https://www.lesswrong.com/tag/inside-outside-view)\n\nThe best resource for most readers new to the debate will be [MIRI](https://wiki.lesswrong.com/wiki/MIRI)'s 2013 [The Hanson-Yudkowsky AI-Foom Debate eBook](http://intelligence.org/ai-foom-debate/), available in PDF, EPUB, and MOBI formats. This includes\n\n*   the original series of blog posts,\n*   a transcript of the 2011 debate\n*   Kaj Sotala's summary and analysis of the debate, and\n*   a 2013 technical report on AI takeoff dynamics (“intelligence explosion microeconomics”) written by Yudkowsky.\n\nPrologue\n--------\n\n*   [Fund UberTool?](http://www.overcomingbias.com/2008/11/fund-ubertool.html) by [Robin Hanson](https://www.lesswrong.com/tag/robin-hanson)\n*   [Engelbart as UberTool?](http://www.overcomingbias.com/2008/11/engelbarts-uber.html) by [Robin Hanson](https://www.lesswrong.com/tag/robin-hanson)\n*   [Friendly Teams](http://www.overcomingbias.com/2008/11/englebart-not-r.html) by [Robin Hanson](https://www.lesswrong.com/tag/robin-hanson)\n*   [Friendliness Factors](http://www.overcomingbias.com/2008/11/friendliness-fa.html) by [Robin Hanson](https://www.lesswrong.com/tag/robin-hanson)\n*   [The Weak Inside View](http://lesswrong.com/lw/vz/the_weak_inside_view/) by [Eliezer Yudkowsky](https://www.lesswrong.com/tag/eliezer-yudkowsky)\n*   [Setting The Stage](http://www.overcomingbias.com/2008/11/setting-the-sta.html) by [Robin Hanson](https://www.lesswrong.com/tag/robin-hanson)\n*   [The First World Takeover](http://lesswrong.com/lw/w0/the_first_world_takeover/) by [Eliezer Yudkowsky](https://www.lesswrong.com/tag/eliezer-yudkowsky)\n*   [Abstraction, Not Analogy](http://www.overcomingbias.com/2008/11/abstraction-vs.html) by [Robin Hanson](https://www.lesswrong.com/tag/robin-hanson)\n*   [Whence Your Abstractions?](http://lesswrong.com/lw/w1/whence_your_abstractions/) by [Eliezer Yudkowsky](https://www.lesswrong.com/tag/eliezer-yudkowsky)\n\nMain sequence\n-------------\n\n*   [AI Go Foom](http://www.overcomingbias.com/2008/11/ai-go-foom.html) by [Robin Hanson](https://www.lesswrong.com/tag/robin-hanson)\n*   [Optimization and the Singularity](http://lesswrong.com/lw/rk/optimization_and_the_singularity/) by [Eliezer Yudkowsky](https://www.lesswrong.com/tag/eliezer-yudkowsky)\n*   [Eliezer's Meta-Level Determinism](http://www.overcomingbias.com/2008/06/eliezers-meta-l.html) by [Robin Hanson](https://www.lesswrong.com/tag/robin-hanson)\n*   [Observing Optimization](http://lesswrong.com/lw/w2/observing_optimization/) by [Eliezer Yudkowsky](https://www.lesswrong.com/tag/eliezer-yudkowsky)\n*   [Life's Story Continues](http://lesswrong.com/lw/w3/lifes_story_continues/) by [Eliezer Yudkowsky](https://www.lesswrong.com/tag/eliezer-yudkowsky)\n*   [Emulations Go Foom](http://www.overcomingbias.com/2008/11/emulations-go-f.html) by [Robin Hanson](https://www.lesswrong.com/tag/robin-hanson)\n*   [Brain Emulation and Hard Takeoff](http://www.overcomingbias.com/2008/11/brain-emulation.html) by [Carl Shulman](https://www.lesswrong.com/tag/carl-shulman)\n*   [Billion Dollar Bots](http://www.overcomingbias.com/2008/11/billion-dollar.html) by [James Miller](https://wiki.lesswrong.com/wiki/James_Miller)\n*   [Surprised by Brains](http://lesswrong.com/lw/w4/surprised_by_brains/) by [Eliezer Yudkowsky](https://www.lesswrong.com/tag/eliezer-yudkowsky)\n*   [“Evicting” brain emulations](http://www.overcomingbias.com/2008/11/suppose-that-ro.html) by [Carl Shulman](https://www.lesswrong.com/tag/carl-shulman)\n*   [Cascades, Cycles, Insight...](http://lesswrong.com/lw/w5/cascades_cycles_insight/) by [Eliezer Yudkowsky](https://www.lesswrong.com/tag/eliezer-yudkowsky)\n*   [When Life Is Cheap, Death Is Cheap](http://www.overcomingbias.com/2008/11/when-life-is-ch.html) by [Robin Hanson](https://www.lesswrong.com/tag/robin-hanson)\n*   [...Recursion, Magic](http://lesswrong.com/lw/w6/recursion_magic/) by [Eliezer Yudkowsky](https://www.lesswrong.com/tag/eliezer-yudkowsky)\n*   [Abstract/Distant Future Bias](http://www.overcomingbias.com/2008/11/abstractdistant.html) by [Robin Hanson](https://www.lesswrong.com/tag/robin-hanson)\n*   [Engelbart: Insufficiently Recursive](http://lesswrong.com/lw/w8/engelbart_insufficiently_recursive/) by [Eliezer Yudkowsky](https://www.lesswrong.com/tag/eliezer-yudkowsky)\n*   [Total Nano Domination](http://lesswrong.com/lw/w9/total_nano_domination/) by [Eliezer Yudkowsky](https://www.lesswrong.com/tag/eliezer-yudkowsky)\n*   [Dreams of Autarky](http://www.overcomingbias.com/2008/11/dreams-of-autar.html) by [Robin Hanson](https://www.lesswrong.com/tag/robin-hanson)\n*   [Total Tech Wars](http://www.overcomingbias.com/2008/11/total-tech-wars.html) by [Robin Hanson](https://www.lesswrong.com/tag/robin-hanson)\n*   [Singletons Rule OK](http://lesswrong.com/lw/wc/singletons_rule_ok/) by [Eliezer Yudkowsky](https://www.lesswrong.com/tag/eliezer-yudkowsky)\n*   [Stuck In Throat](http://www.overcomingbias.com/2008/11/stuck-in-throat.html) by [Robin Hanson](https://www.lesswrong.com/tag/robin-hanson)\n*   [Disappointment in the Future](http://lesswrong.com/lw/wd/disappointment_in_the_future/) by [Eliezer Yudkowsky](https://www.lesswrong.com/tag/eliezer-yudkowsky)\n*   [I Heart CYC](http://www.overcomingbias.com/2008/12/i-heart-cyc.html) by [Robin Hanson](https://www.lesswrong.com/tag/robin-hanson)\n*   [Is the City-ularity Near?](http://www.overcomingbias.com/2010/02/is-the-city-ularity-near.html) by [Robin Hanson](https://www.lesswrong.com/tag/robin-hanson)\n*   [Recursive Self-Improvement](http://lesswrong.com/lw/we/recursive_selfimprovement/) by [Eliezer Yudkowsky](https://www.lesswrong.com/tag/eliezer-yudkowsky)\n*   [Whither Manufacturing?](http://www.overcomingbias.com/2008/12/whither-manufac.html) by [Robin Hanson](https://www.lesswrong.com/tag/robin-hanson)\n*   [Hard Takeoff](http://lesswrong.com/lw/wf/hard_takeoff/) by [Eliezer Yudkowsky](https://www.lesswrong.com/tag/eliezer-yudkowsky)\n*   [Test Near, Apply Far](http://www.overcomingbias.com/2008/12/test-near-apply.html) by [Robin Hanson](https://www.lesswrong.com/tag/robin-hanson)\n*   [Permitted Possibilities, & Locality](http://lesswrong.com/lw/wg/permitted_possibilities_locality/) by [Eliezer Yudkowsky](https://www.lesswrong.com/tag/eliezer-yudkowsky)\n*   [Underconstrained Abstractions](http://lesswrong.com/lw/wh/underconstrained_abstractions/) by [Eliezer Yudkowsky](https://www.lesswrong.com/tag/eliezer-yudkowsky)\n*   [Beware Hockey Stick Plans](http://www.overcomingbias.com/2008/12/beware-hockey-s.html) by [Robin Hanson](https://www.lesswrong.com/tag/robin-hanson)\n*   [Evolved Desires](http://www.overcomingbias.com/2008/12/evolved-desires.html) by [Robin Hanson](https://www.lesswrong.com/tag/robin-hanson)\n*   [Sustained Strong Recursion](http://lesswrong.com/lw/wi/sustained_strong_recursion/) by [Eliezer Yudkowsky](https://www.lesswrong.com/tag/eliezer-yudkowsky)\n*   [Friendly Projects vs. Products](http://www.overcomingbias.com/2008/12/friendly-projec.html) by [Robin Hanson](https://www.lesswrong.com/tag/robin-hanson)\n*   [Is That Your True Rejection?](http://lesswrong.com/lw/wj/is_that_your_true_rejection/) by [Eliezer Yudkowsky](https://www.lesswrong.com/tag/eliezer-yudkowsky)\n*   [Shared AI Wins](http://www.overcomingbias.com/2008/12/shared-ai-wins.html) by [Robin Hanson](https://www.lesswrong.com/tag/robin-hanson)\n*   [Artificial Mysterious Intelligence](http://lesswrong.com/lw/wk/artificial_mysterious_intelligence/) by [Eliezer Yudkowsky](https://www.lesswrong.com/tag/eliezer-yudkowsky)\n*   [Wrapping Up](http://www.overcomingbias.com/2008/12/wrapping-up.html) by [Robin Hanson](https://www.lesswrong.com/tag/robin-hanson)\n*   [True Sources of Disagreement](http://lesswrong.com/lw/wl/true_sources_of_disagreement/) by [Eliezer Yudkowsky](https://www.lesswrong.com/tag/eliezer-yudkowsky)\n*   [The Bad Guy Bias](http://www.overcomingbias.com/2008/12/the-bad-guy-bia.html) by [Robin Hanson](https://www.lesswrong.com/tag/robin-hanson)\n*   [Disjunctions, Antipredictions, Etc.](http://lesswrong.com/lw/wm/disjunctions_antipredictions_etc/) by [Eliezer Yudkowsky](https://www.lesswrong.com/tag/eliezer-yudkowsky)\n*   [Are AIs Homo Economicus?](http://www.overcomingbias.com/2008/12/are-ais-homo-ec.html) by [Robin Hanson](https://www.lesswrong.com/tag/robin-hanson)\n*   [Two Visions Of Heritage](http://www.overcomingbias.com/2008/12/two-visions-of.html) by [Robin Hanson](https://www.lesswrong.com/tag/robin-hanson)\n*   [The Mechanics of Disagreement](http://lesswrong.com/lw/wo/the_mechanics_of_disagreement/) by [Eliezer Yudkowsky](https://www.lesswrong.com/tag/eliezer-yudkowsky)\n\nConclusion\n----------\n\n*   [What Core Argument?](http://www.overcomingbias.com/2008/12/what-core-argument.html) by [Robin Hanson](https://www.lesswrong.com/tag/robin-hanson)\n*   [What I Think, If Not Why](http://lesswrong.com/lw/wp/what_i_think_if_not_why/) by [Eliezer Yudkowsky](https://www.lesswrong.com/tag/eliezer-yudkowsky)\n*   [Not Taking Over the World](http://lesswrong.com/lw/wt/not_taking_over_the_world/) by [Eliezer Yudkowsky](https://www.lesswrong.com/tag/eliezer-yudkowsky)\n\nPostscript\n----------\n\n*   [We Agree: Get Froze](http://www.overcomingbias.com/2008/12/we-agree-get-froze.html) by [Robin Hanson](https://www.lesswrong.com/tag/robin-hanson)\n*   [You Only Live Twice](http://lesswrong.com/lw/wq/you_only_live_twice/) by [Eliezer Yudkowsky](https://www.lesswrong.com/tag/eliezer-yudkowsky)\n*   [Debating Yudkowsky](http://www.overcomingbias.com/2011/07/debating-yudkowsky.html) by [Robin Hanson](https://www.lesswrong.com/tag/robin-hanson)\n*   [Foom Debate, Again](http://www.overcomingbias.com/2013/02/foom-debate-again.html) by [Robin Hanson](https://www.lesswrong.com/tag/robin-hanson)\n\nJane Street debate\n------------------\n\nA stand-up debate between Hanson and Yudkowsky on the same themes took place on Wednesday June 30 2011 at *Jane Street Capital*.\n\n*   [Video with better quality audio](http://hanson.gmu.edu/ppt/JaneStreetDebate2011vid.wmv) 1.57G\n*   [~Poor quality WMA audio~](http://hanson.gmu.edu/ppt/JaneStreetDebate2011.WMA) 97M (dead link)\n*   [~Poor quality MP3 audio~](http://hanson.gmu.edu/ppt/JaneStreetDebate2011.mp3) 97M (dead link)\n*   [Full transcript](https://docs.google.com/document/pub?id=17yLL7B7yRrhV3J9NuiVuac3hNmjeKTVHnqiEa6UQpJk)\n*   [Partial transcript](http://lesswrong.com/lw/bug/partial_transcript_of_the_hansonyudkowsky_june/) by Chris Hallquist\n\nSummary and analysis\n--------------------\n\nIn Spring 2012 [Luke Muehlhauser](https://www.lesswrong.com/tag/luke-muehlhauser) asked [Kaj Sotala](https://wiki.lesswrong.com/wiki/Kaj_Sotala) to write a summary and analysis of the debate; he made a [draft of that analysis](http://lesswrong.com/lw/fih/a_summary_of_the_hansonyudkowsky_foom_debate/) available in November 2012. The final version of the summary is included in the [ebook](https://intelligence.org/ai-foom-debate/).\n\nPost-book material\n------------------\n\nNot included in MIRI's 2013 book on the debate:\n\n*   Robin Hanson's 2014 blog post on the subject in response to the publication of \"Superintelligence\", [I Still Don’t Get Foom](http://www.overcomingbias.com/2014/07/30855.html)\n*   [Katja Grace](https://wiki.lesswrong.com/wiki/Katja_Grace)'s 2014 hosted discussion on Less Wrong, [Superintelligence 6: Intelligence explosion kinetics](http://lesswrong.com/r/discussion/lw/l4e/superintelligence_6_intelligence_explosion/)\n*   Eliezer's 2017 post, [AlphaGo Zero and the Foom Debate](https://intelligence.org/2017/10/20/alphago/) and Robin Hanson's reply, [What Evidence Is AlphaGo Zero Re AGI Complexity?](https://www.lesserwrong.com/posts/D3NspiH2nhKA6B2PE/what-evidence-is-alphago-zero-re-agi-complexity)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb142",
    "name": "Egalitarianism",
    "core": null,
    "slug": "egalitarianism",
    "oldSlugs": null,
    "postCount": 0,
    "description": {
      "markdown": "**Egalitarianism** is the idea that everyone should be considered equal. Equal in merit, equal in opportunity, equal in morality, and equal in achievement.\n\nDismissing egalitarianism is not opposed to [humility](https://www.lesswrong.com/tag/humility), even though from the [signaling](https://www.lesswrong.com/tag/signaling) perspective it seems to be opposed to [modesty](https://www.lesswrong.com/tag/modesty).\n\nBlog posts\n----------\n\n*   [Tsuyoku vs. the Egalitarian Instinct](http://lesswrong.com/lw/h9/tsuyoku_vs_the_egalitarian_instinct/)\n*   [The Sin of Underconfidence](http://lesswrong.com/lw/c3/the_sin_of_underconfidence/)\n*   [Evading Sharing Rules](http://www.overcomingbias.com/2011/04/evading-sharing-rules.html) by [Robin Hanson](https://www.lesswrong.com/tag/robin-hanson)\n\nSee also\n--------\n\n*   [Humility](https://www.lesswrong.com/tag/humility), [Modesty](https://www.lesswrong.com/tag/modesty)\n*   [Group rationality](https://www.lesswrong.com/tag/group-rationality)\n*   [Underconfidence](https://www.lesswrong.com/tag/underconfidence)\n*   [Tsuyoku naritai](https://www.lesswrong.com/tag/tsuyoku-naritai)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb1b7",
    "name": "How An Algorithm Feels",
    "core": null,
    "slug": "how-an-algorithm-feels",
    "oldSlugs": null,
    "postCount": 0,
    "description": {
      "markdown": "Our philosophical intuitions do not rain down on us as manna from heaven; they are generated by algorithms in the human brain. Our philosophical intuitions, indeed, are how these particular cognitive algorithms **feel from the inside**.\n\nTo dissolve a philosophical dilemma, it often suffices to understand the cognitive algorithm that generates the appearance of the dilemma - if you understand the algorithm in *sufficient detail*. It is not enough to say \"An algorithm does it!\" - this [might as well be magic](http://lesswrong.com/lw/op/fake_reductionism/). It takes a detailed step-by-step walkthrough.\n\nMichael Vassar has observed that conventional philosophers seem to be *spectacularly* bad at understanding that their intuitions are generated by cognitive algorithms. This may be why works of serious reductionism get written by Artificial Intelligence people instead of conventional philosophers.\n\nBlog posts\n----------\n\n*   [How An Algorithm Feels From Inside](http://lesswrong.com/lw/no/how_an_algorithm_feels_from_inside/)\n*   [Mind Projection Fallacy](http://lesswrong.com/lw/oi/mind_projection_fallacy/)\n*   [Dissolving the Question](http://lesswrong.com/lw/of/dissolving_the_question/)\n*   [Feel the Meaning](http://lesswrong.com/lw/nq/feel_the_meaning/)\n\nSee also\n--------\n\n*   [Mind projection fallacy](https://www.lesswrong.com/tag/mind-projection-fallacy)\n*   [Free will](https://www.lesswrong.com/tag/free-will)\n*   [Reductionism (sequence)](https://www.lesswrong.com/tag/reductionism-sequence)\n*   [Magic](https://www.lesswrong.com/tag/magic), [Fake simplicity](https://www.lesswrong.com/tag/fake-simplicity)\n*   [P-zombie](https://wiki.lesswrong.com/wiki/P-zombie)\n*   [Cognitive reduction](https://www.lesswrong.com/tag/cognitive-reduction)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "mmfk47obrNKhN6waD",
    "name": "Failure mode",
    "core": false,
    "slug": "failure-mode",
    "oldSlugs": [
      "failiure-mode"
    ],
    "postCount": 5,
    "description": {
      "markdown": "**Failure mode** is a term for a, usually common, way things fail when attempting something. for example, [confirmation bias](Confirmation Bias) is a common failure mode in reasoning.\n\nKnowing and understanding possible failure modes in what you attempting to do is important in order to avoid them.\n\n**See also:** [Postmortems & Retrospectives](https://www.lesswrong.com/tag/postmortems-and-retrospectives)\n\nOther Examples:\n---------------\n\n[Bias](https://www.lesswrong.com/tag/bias)  \n[Planning Fallacy](https://www.lesswrong.com/tag/planning-fallacy)   \n[Status Quo Bias](https://www.lesswrong.com/tag/status-quo-bias)  \n[Affect Heuristic](https://www.lesswrong.com/tag/affect-heuristic)  \n[Aversion/Ugh Fields](https://www.lesswrong.com/tag/aversion-ugh-fields)  \n[Bucket Errors](https://www.lesswrong.com/tag/bucket-errors)  \n[Compartmentalization](https://www.lesswrong.com/tag/compartmentalization)  \n[Confirmation Bias](https://www.lesswrong.com/tag/confirmation-bias)  \n[Fallacies](https://www.lesswrong.com/tag/logical-fallacies)  \n[Goodhart's Law](https://www.lesswrong.com/tag/goodhart-s-law)  \n[Groupthink](https://www.lesswrong.com/tag/groupthink)  \n[Heuristics & Biases](https://www.lesswrong.com/tag/heuristics-and-biases)  \n[Mind Projection Fallacy](https://www.lesswrong.com/tag/mind-projection-fallacy)  \n[Motivated Reasoning](https://www.lesswrong.com/tag/motivated-reasoning)  \n[Pica](https://www.lesswrong.com/tag/pica)  \n[Pitfalls of Rationality](https://www.lesswrong.com/tag/pitfalls-of-rationality)  \n[Rationalization](https://www.lesswrong.com/tag/rationalization)  \n[Self-Deception](https://www.lesswrong.com/tag/self-deception)  \n[Sunk-Cost Fallacy](https://www.lesswrong.com/tag/sunk-cost-fallacy)  \n[Paperclip Maximizer](https://www.lesswrong.com/tag/paperclip-maximizer)  \n[Moral Mazes](https://www.lesswrong.com/tag/moral-mazes)  \n[Replication Crisis](https://www.lesswrong.com/tag/replicability)  \n[Moloch](https://www.lesswrong.com/tag/moloch)  \n[Tribalism](https://www.lesswrong.com/tag/coalitional-instincts)  \n[Simulacrum Levels](https://www.lesswrong.com/tag/simulacrum-levels)  \n[Information Hazards](https://www.lesswrong.com/tag/information-hazards)  \n[Pascal's Mugging](https://www.lesswrong.com/tag/pascal-s-mugging)  \n[Akrasia](https://www.lesswrong.com/tag/akrasia)  \n[Procrastination](https://www.lesswrong.com/tag/procrastination)  \n[Nonappeals](https://www.lesswrong.com/tag/nonapples)\n\n**Posts:**  \n[Guessing the Teacher's Password](https://www.lesswrong.com/posts/NMoLJuDJEms7Ku9XS/guessing-the-teacher-s-password)  \n[Expecting Short Inferential Distances](https://www.lesswrong.com/posts/HLqWn5LASfhhArZ7w/expecting-short-inferential-distances)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb28e",
    "name": "Causal Decision Theory",
    "core": null,
    "slug": "causal-decision-theory",
    "oldSlugs": null,
    "postCount": 15,
    "description": {
      "markdown": "**Causal Decision Theory** – CDT - is a branch of [decision theory](https://www.lesswrong.com/tag/decision-theory) which advises an agent to take actions that maximizes the causal consequences on the probability of desired outcomes [^1^](#fn1). As any branch of decision theory, it prescribes taking the action that maximizes [utility](https://www.lesswrong.com/tag/utility), that which utility equals or exceeds the utility of every other option. The utility of each action is measured by the [expected utility](https://www.lesswrong.com/tag/expected-utility), the averaged by probabilities sum of the utility of each of its possible results. How the actions can influence the probabilities differ between the branches. Contrary to [Evidential Decision Theory](https://www.lesswrong.com/tag/evidential-decision-theory) – EDT - CDT focuses on the causal relations between one’s actions and its outcomes, instead of focusing on which actions provide evidences for desired outcomes. According to CDT a rational agent should track the available causal relations linking his actions to the desired outcome and take the action which will better enhance the chances of the desired outcome.\n\nOne usual example where EDT and CDT commonly diverge is the [Smoking lesion](https://www.lesswrong.com/tag/smoking-lesion): “Smoking is strongly correlated with lung cancer, but in the world of the Smoker's Lesion this correlation is understood to be the result of a common cause: a genetic lesion that tends to cause both smoking and cancer. Once we fix the presence or absence of the lesion, there is no additional correlation between smoking and cancer. Suppose you prefer smoking without cancer to not smoking without cancer, and prefer smoking with cancer to not smoking with cancer. Should you smoke?” CDT would recommend smoking since there is no causal connection between smoking and cancer. They are both caused by a gene, but have no causal direct connection with each other. EDT on the other hand would recommend against smoking, since smoking is an evidence for having the mentioned gene and thus should be avoided.\n\nThe core aspect of CDT is mathematically represented by the fact it uses probabilities of conditionals in place of conditional probabilities [^2^](#fn2). The probability of a conditional is the probability of the whole conditional being true, where the conditional probability is the probability of the consequent given the antecedent. A conditional probability of B given A - P(B|A) -, simply implies the [Bayesian probability](https://www.lesswrong.com/tag/bayesian-probability) of the event B happening given we known A happened, it’s used in EDT. The probability of conditionals – P(A > B) - refers to the probability that the conditional 'A implies B' is true, it is the probability of the contrafactual ‘If A, then B’ be the case. Since contrafactual analysis is the key tool used to speak about causality, probability of conditionals are said to mirror causal relations. In most cases these two probabilities track each other, and CDT and EDT give the same answers. However, some particular problems have arisen where their predictions for rational action diverge such as the [Smoking lesion](https://www.lesswrong.com/tag/smoking-lesion) problem – where CDT seems to give a more reasonable prescription – and [Newcomb's problem](https://www.lesswrong.com/tag/newcomb-s-problem) – where CDT seems unreasonable. David Lewis proved [^3^](#fn3) it's impossible to probabilities of conditionals to always track conditional probabilities. Hence, evidential relations aren’t the same as causal relations and CDT and EDT will always diverge in some cases.\n\nReferences\n----------\n\n1.  [http://plato.stanford.edu/entries/decision-causal/](http://plato.stanford.edu/entries/decision-causal/)\n2.  Lewis, David. (1981) \"Causal Decision Theory,\" Australasian Journal of Philosophy 59 (1981): 5- 30.\n3.  Lewis, D. (1976), \"Probabilities of conditionals and conditional probabilities\", The Philosophical Review (Duke University Press) 85 (3): 297–315\n\nSee also\n--------\n\n*   [Decision theory](https://www.lesswrong.com/tag/decision-theory)\n*   [Evidential Decision Theory](https://www.lesswrong.com/tag/evidential-decision-theory)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb22c",
    "name": "Parfit's Hitchhiker",
    "core": null,
    "slug": "parfits-hitchhiker",
    "oldSlugs": null,
    "postCount": 0,
    "description": {
      "markdown": "**Parfit's Hitchhiker** is a problem for testing [decision theories](https://www.lesswrong.com/tag/decision-theory). It asks an agent to sacrifice utility at a later time to obtain utility at an earlier time, where the sacrifice happens *after* the benefit.\n\nFrom [Timeless Decision Theory: Problems I Can't Solve](https://www.lesswrong.com/posts/c3wWnvgzdbRhNnNbQ/timeless-decision-theory-problems-i-can-t-solve):\n\n> Suppose you're out in the desert, running out of water, and soon to die - when someone in a motor vehicle drives up next to you. Furthermore, the driver of the motor vehicle is a perfectly selfish ideal game-theoretic agent, and even further, so are you; and what's more, the driver is Paul Ekman, who's really, really good at reading facial microexpressions. The driver says, \"Well, I'll convey you to town if it's in my interest to do so - so will you give me $100 from an ATM when we reach town?\"\n> \n> Now of course you wish you could answer \"Yes\", but as an ideal game theorist yourself, you realize that, once you actually reach town, you'll have no further motive to pay off the driver. \"Yes,\" you say. \"You're lying,\" says the driver, and drives off leaving you to die.\n> \n> If only you weren't so rational!\n> \n> This is the dilemma of Parfit's Hitchhiker, and the above is the standard resolution according to mainstream philosophy's causal decision theory, which also two-boxes on [Newcomb's problem](https://www.lesswrong.com/tag/newcomb-s-problem) and defects in the \\[Twin\\] Prisoner's Dilemma.\n\nMIRI's newest decision theory, [Functional Decision Theory](https://www.lesswrong.com/tag/functional-decision-theory) (FDT), notes that, while you're talking to the driver, you already know whether or not you're going to pay once you're in town (the problem specifies this). More formally, you have a model of your future decision procedure, which you run to predict what you will do once you're in town. Therefore, if you follow FDT, you pay up once you're in town: if you do so, your past model of your decision procedure also \"pays\" - which means \"past you\" predicts you'll pay and can truthfully say \"Yes\" to Paul Ekman. Paul reads your microexpressions, believes you, and conveys you to town. If you don't pay up once you're in town, \"past you\" predicts this and can't truthfully say \"yes\" to the driver, resulting in a lonely death in the desert.\n\nBlog Posts\n----------\n\n*   [Prices or bindings?](http://lesswrong.com/lw/v2/prices_or_bindings/)\n\nSee Also\n--------\n\n*   [Decision theory](https://www.lesswrong.com/tag/decision-theory)\n*   [Newcomb's problem](https://www.lesswrong.com/tag/newcomb-s-problem)\n*   [Counterfactual mugging](https://www.lesswrong.com/tag/counterfactual-mugging)\n*   [Smoking lesion](https://www.lesswrong.com/tag/smoking-lesion)\n*   [Absent-minded driver](https://www.lesswrong.com/tag/absent-minded-driver)\n*   [Sleeping Beauty problem](https://www.lesswrong.com/tag/sleeping-beauty-paradox)\n*   [Prisoner's dilemma](https://www.lesswrong.com/tag/prisoner-s-dilemma)\n*   [Pascal's mugging](https://www.lesswrong.com/tag/pascal-s-mugging)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "EBjfFrP28WJTuAgdK",
    "name": "One-Boxing",
    "core": false,
    "slug": "one-boxing",
    "oldSlugs": null,
    "postCount": 1,
    "description": {
      "markdown": "In [Newcomb's Problem](https://www.lesswrong.com/tag/newcomb-s-problem), **one-boxing** means only taking the box which could contain the million. It generally isn't used for just taking the box containing the thousand even though that would also just be taking one box, as there is no reason to do this.\n\nThe general consensus on Less Wrong is that one-boxing is the rational decision, whilst the two-boxing seems to be the most popular option among decision theorists."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "4fxcbJ8xSv4SAYkkx",
    "name": "Bayesianism",
    "core": false,
    "slug": "bayesianism",
    "oldSlugs": null,
    "postCount": 21,
    "description": {
      "markdown": "**Bayesianism** is the broader philosophy inspired by [Bayes' theorem](https://www.lessestwrong.com/tag/bayes-theorem). The core claim behind all varieties of Bayesianism is that *probabilities are subjective degrees of belief --* often operationalized as willingness to bet. \n\n*See also:  *[Bayes theorem](https://www.lessestwrong.com/tag/bayes-theorem), [Bayesian probability](https://www.lessestwrong.com/tag/bayesian-probability), [Radical Probabilism](https://www.lesswrong.com/tag/radical-probabilism), [Priors](https://www.lessestwrong.com/tag/priors), [Rational evidence](https://www.lessestwrong.com/tag/rational-evidence), [Probability theory](https://www.lessestwrong.com/tag/probability-theory), [Decision theory](https://www.lessestwrong.com/tag/decision-theory), [Lawful intelligence](https://www.lessestwrong.com/tag/lawful-intelligence), [Bayesian Conspiracy](https://www.lessestwrong.com/tag/bayesian-conspiracy), \n\nThis stands in contrast to other interpretations of probability, which attempt greater objectivity. The [frequentist](https://en.wikipedia.org/wiki/Frequentist_probability) interpretation of probability has a focus on repeatable *experiments;* probabilities are *the limiting frequency of an event if you performed the experiment an infinite number of times*. \n\nAnother contender is the [propensity](https://en.wikipedia.org/wiki/Propensity_probability) interpretation, which grounds probability in *the propensity for things to happen*. A perfectly balanced 6-sided die would have a 1/6 propensity to land on each side. A propensity theorist sees this as a basic fact about dice not derived from infinite sequences of experiments or subjective viewpoints.\n\nNote how both of these alternative interpretations ground the meaning of probability in an external objective fact which cannot be directly accessed.\n\nAs a consequence of the subjective interpretation of probability theory, Bayesians are more inclined to apply Bayes' Theorem in practical statistical inference. The primary example of this is statistical hypothesis testing. Frequentists take the application of Bayes' Theorem to be inappropriate, because \"the probability of a hypothesis\" is meaningless: a hypothesis is either true or false; you cannot define a repeated experiment in which it is sometimes true and sometimes false, so you cannot assign it an intermediate probability.\n\nBayesianism & Rationality\n-------------------------\n\nThere is a conceoption of rationality for *bayesian* can be treated as technical codeword that cognitive scientists use to mean \"rational\". Bayesian [probability theory](https://www.lessestwrong.com/tag/probability-theory) is the math of [epistemic rationality](https://wiki.lesswrong.com/wiki/epistemic_rationality), Bayesian [decision theory](https://www.lessestwrong.com/tag/decision-theory) is the math of [instrumental rationality](https://wiki.lesswrong.com/wiki/instrumental_rationality). Right up there with [cognitive bias](https://wiki.lesswrong.com/wiki/cognitive_bias) as an absolutely fundamental concept on Less Wrong.  \n \n\nOther usages\n------------\n\nThe term \"Bayesian\" may also refer to an ideal rational agent implementing precise, perfect Bayesian probability theory and decision theory (see, for example, [Aumann's agreement theorem](https://www.lessestwrong.com/tag/aumann-s-agreement-theorem))."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb0f9",
    "name": "Modesty Argument",
    "core": null,
    "slug": "modesty-argument",
    "oldSlugs": null,
    "postCount": 0,
    "description": {
      "markdown": "The **modesty argument** is the claim that when two or more rational agents have [common knowledge](https://www.lesswrong.com/tag/common-knowledge) of a [disagreement](https://www.lesswrong.com/tag/disagreement) over the likelihood of an issue of simple fact, they should each adjust their probability estimates in the direction of the others'. This process should continue until the two agents are in full agreement. The name comes from the idea that rational agents should not privilege evidence they have gathered over evidence gathered by others, i.e. agents should be modest about their own estimates.\n\nThe Modesty Argument is inspired by [Aumann's agreement theorem](https://www.lesswrong.com/tag/aumann-s-agreement-theorem), which shows that genuine Bayesians cannot agree to disagree.\n\nBlog posts\n----------\n\n*   [The Modesty Argument](http://lesswrong.com/lw/gr/the_modesty_argument/)\n*   [Does the Modesty Argument Apply to Moral Claims?](http://www.overcomingbias.com/2006/12/does_the_modest.html) by Paul Gowder\n*   [Modesty in a Disagreeable World](http://www.overcomingbias.com/2006/12/modesty_in_a_di.html) by [Hal Finney](https://en.wikipedia.org/wiki/Hal_Finney_(cypherpunk))\n*   [Philosophical Majoritarianism](http://www.overcomingbias.com/2007/03/on_majoritarian.html) by [Hal Finney](https://en.wikipedia.org/wiki/Hal_Finney_(cypherpunk))\n*   [How to use \"philosophical majoritarianism\"](http://lesswrong.com/lw/es/how_to_use_philisophical_majoritarianism/) by jimmy\n\nSee also\n--------\n\n*   [Modesty](https://www.lesswrong.com/tag/modesty)\n*   [Aumann's agreement theorem](https://www.lesswrong.com/tag/aumann-s-agreement-theorem)\n*   [Disagreement](https://www.lesswrong.com/tag/disagreement)\n*   [Error of crowds](https://www.lesswrong.com/tag/error-of-crowds)\n*   [Information cascade](https://www.lesswrong.com/tag/information-cascades)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb0e4",
    "name": "Fallacy of Gray",
    "core": null,
    "slug": "fallacy-of-gray",
    "oldSlugs": null,
    "postCount": 4,
    "description": {
      "markdown": "The **fallacy of gray** is a common belief in people who are somewhat advanced along the path to optimal truth seeking which claims, roughly, that because nothing is [certain](https://www.lesswrong.com/tag/absolute-certainty), everything is equally uncertain. \n\nOne who commits this fallacy may reply to the statement that probability of winning a [lottery](https://www.lesswrong.com/tag/lottery) is only one in a million by saying: \"There's still a chance, right?\"\n\nThis fallacy makes a lot of sense once people notice very common and predictable emotional swings proximate to a possible attachment to an emotivist (\"boo vs yay\") or binary (\"black and white\") labeling of an entity, or situation, or system, or choice.\n\nIf someone says \"Hurrah for X!\" then X can often be attacked by saying that X is just kind of blah.\n\nIf someone says \"Down with X!\" then X can often be defended by saying that X is just kind of blah.\n\nEither of these \"blah\" assertions could count as the fallacy, if that was the totality of the argument.\n\nThis fallacy is possible because it really is the case that a variety of ambiguous signals can exist in proximity to a complex object, and there truly can be a mixture of positive and negative aspects to any given thing in the world, and also objects can have aspects that might be instrumentally useful or harmful to different plans whose total net value is not entirely determined by any single local factor.\n\nAlso, a very normal response to complexity like this is for cognitive dissonance to lead the person to have some single simply up/down association... to achieve cognitive closure swiftly. People might call this tendency a [bias towards certainty](https://psycnet.apa.org/record/1983-20816-001). \n\nThen, knowing this bias exists, someone who wields the fallacy of gray can score debate points on people with a simplistic binary attitude, without seeming to strongly or obviously take sides. \n\nThis does represent a kind of progress or increase in power levels.\n\nHowever, in calling the fallacy of gray *a fallacy*, some even better thing is imagined.\n\nThe better thing, simply put, is to become quantitative and detail oriented. \n\nA [VNM rational agent](https://www.lesswrong.com/tag/vnm-theorem) will have probabilistic beliefs, p, where 0<p<1, about what's likely to fall out of various choices, and a coherently linear model of value, v, with no behaviorally significant \"natural zero\".  Then every option is basically assigned the value p*v, and the agent simply always make choices in favor of what the agent calculates gives the most utility, under uncertainty.  \n\nSuppose there is a choice between  \nsomething with p*v = -1,000,000 net utility under uncertainty or   \nsomething with p*v = -1,000,001 net utility under uncertainty...\n\n...then the VNM agent simply chooses the option that is greater, or \"less negative\", and picks -1,000,000. \n\nThe negative sign, and all those zeros... that is just in the math. \n\nThe VNM agent feels no psychic cloud of badness because of the negative sign, or because of how big the number was.\n\nIf you want to insist that a natural zero in some **resource** (like a bankroll) does somehow exist in the territory, then you could put \"the possibility of zeroing out the resource\" in [your map of the possible futures](https://www.lesswrong.com/posts/zFQQEkx4c6bxdshr4/5-axioms-of-decision-making), and then model the consequences of [having zero of that resource including all the long term cascading consequences into deep time](https://www.lesswrong.com/posts/zmpYKwqfMkWtywkKZ/kelly-isn-t-just-about-logarithmic-utility), and then recover a linear ordering over outcomes that just happens to assign *much relatively lower* values to scenarios where the state of the system included having zero of some import resource with a natural zero at some point... But the utility score here is **not** \"objectively lower\" (like a big number with a negative sign), but rather it is **relatively** lower (like much lower than other options).\n\nTo say that someone is using the fallacy of gray is to say that despite not saying Boo or Yay, even so they are *still* speaking as if the people party to the discussion are only capable of [emotive associative reasoning](https://www.lesswrong.com/posts/M2LWXsJxKS626QNEA/the-trouble-with-good), not actual calculation in pursuit of any kind of detail-oriented assessment of options and trade-offs with costs and benefits that could be tallied coherently.\n\nBlog posts\n----------\n\n*   [But There's Still A Chance, Right?](http://lesswrong.com/lw/ml/but_theres_still_a_chance_right/)\n*   [The Fallacy of Gray](http://lesswrong.com/lw/mm/the_fallacy_of_gray/)\n*   [Fallacies of Compression](http://lesswrong.com/lw/nw/fallacies_of_compression)\n\nExternal links\n--------------\n\n*   [Thinking in Greyscale](http://measureofdoubt.com/2011/05/23/thinking-in-greyscale/) at Measure of Doubt\n\nSee also\n--------\n\n*   [I don't know](https://wiki.lesswrong.com/wiki/I_don't_know)\n*   [Humility](https://www.lesswrong.com/tag/humility)\n*   [Possibility](https://www.lesswrong.com/tag/possibility)\n*   [Absolute certainty](https://www.lesswrong.com/tag/absolute-certainty)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb0fd",
    "name": "Omega",
    "core": null,
    "slug": "omega",
    "oldSlugs": null,
    "postCount": 0,
    "description": {
      "markdown": "**Omega** is a hypothetical super-intelligent being used in philosophical problems. Omega is most commonly used as the predictor in [Newcomb's problem](https://www.lesswrong.com/tag/newcomb-s-problem). Sometimes Omega is taken as a [perfect predictor](https://www.lesswrong.com/tag/perfect-predictor), while other times it is an almost perfect predictor. In some thought experiments, Omega is also taken to be super-powerful.\n\nOmega can be seen as analogous to [Laplace's demon](https://en.wikipedia.org/wiki/Laplace's_demon), or as the closest approximation to the Demon capable of existing in our universe. Including a perfect predictor in a problem can lead to subtle issues such particular counterfactuals being undefined. See the article on [Perfect Predictors](https://www.lesswrong.com/tag/perfect-predictor) for discussion of the technicalities and discussions about the validity of perfect predictors."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "DzgepLrZz9ZvuFHsa",
    "name": "Two-Boxing",
    "core": false,
    "slug": "two-boxing",
    "oldSlugs": null,
    "postCount": 2,
    "description": {
      "markdown": "In [Newcomb's Problem](https://www.lesswrong.com/tag/newcomb-s-problem), **two-boxing** means taking both boxes, typically on the basis that your decision is independent of the prediction that has already been made. The general consensus on Less Wrong is that one-boxing is the rational decision, whilst the two-boxing seems to be the most popular option among decision theorists."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb2dd",
    "name": "Differential Intellectual Progress",
    "core": null,
    "slug": "differential-intellectual-progress",
    "oldSlugs": null,
    "postCount": 0,
    "description": {
      "markdown": "**Differential intellectual progress** was defined by [Luke Muehlhauser and Anna Salamon](http://web.archive.org/web/20190430130748/http://intelligence.org/files/IE-EI.pdf) as \"prioritizing risk-reducing intellectual progress over risk-increasing intellectual progress\". They discuss differential intellectual progress in relation to [Artificial General Intelligence](https://www.lesswrong.com/tag/artificial-general-intelligence) (AGI) development (which will also be the focus of this article):\n\n> As applied to AI risks in particular, a plan of differential intellectual progress would recommend that our progress on the philosophical, scientific, and technological problems of AI _safety_ outpace our progress on the problems of AI _capability_ such that we develop _safe_ superhuman AIs before we develop arbitrary superhuman AIs.\n\nMuehlhauser and Salamon also note that [differential technological development](https://en.wikipedia.org/wiki/Differential_technological_development) can be seen as a special case of this concept.\n\nRisk-increasing Progress\n------------------------\n\nTechnological advances — without corresponding development of safety mechanisms — simultaneously increase the capacity for both [friendly](https://wiki.lesswrong.com/wiki/Friendly_AI) and [unfriendly](https://wiki.lesswrong.com/wiki/Unfriendly_AI) AGI development. Presently, most AGI research is concerned with increasing its _capacity_ rather than its _safety_ and thus, most progress increases the risk for a [widespread negative effect](https://www.lesswrong.com/tag/existential-risk).\n\n*   _Increased computing power._ Computing power continues to rise in step with [Moore's Law](http://www.intel.com/content/www/us/en/silicon-innovations/moores-law-technology.html), providing the raw capacity for smarter AGIs. This allows for more ['brute-force'](http://dictionary.reference.com/browse/brute+force) programming, increasing the probability of someone creating an AGI without properly understanding it. Such an AGI would also be harder to control.\n\n*   _More efficient algorithms._ Mathematical advances can produce [substantial reductions](http://users.ece.gatech.edu/~mrichard/Richards%26Shaw_Algorithms01204.pdf) in computing time, allowing an AGI to be more efficient within its current operating capacity. The ability to carry about a larger number of computations with the same amount of hardware would have the net effect of making the AGI smarter.\n\n*   _Extensive datasets._ Living in the ['Information Age'](http://en.wikipedia.org/wiki/Information_Age) has produced immense amounts of data. As [data storage capacity](http://www.scientificamerican.com/article.cfm?id=kryders-law) has increased, so has the amount of information that is collected and stored, allowing an AGI immediate access to massive amounts of knowledge.\n\n*   _Advanced neuroscience._ Cognitive scientists have discovered several algorithms used by the human brain which contribute to our intelligence, leading to a field called ['Computational Cognitive Neuroscience.'](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3153062/) This has led to developments such as brain implants that have helped restore [memory](http://www.nytimes.com/2011/06/17/science/17memory.html) and [motor learning](http://www.popsci.com/science/article/2011-09/israeli-researchers-build-rat-cyborg-packing-digitally-derived-cerebellum) in animals, algorithms which might conceivably contribute to AGI development.\n\nThe above developments could also help in the creation of [Friendly AI](https://wiki.lesswrong.com/wiki/Friendly_AI). However, Friendliness requires the development of both AGI and Friendliness theory, while an [Unfriendly Artificial Intelligence](https://wiki.lesswrong.com/wiki/Unfriendly_AI) might be created by AGI efforts alone. Thus developments that bring AGI closer or make it more powerful will increase risk, at least if not combined with work on Friendliness.\n\nRisk-reducing Progress\n----------------------\n\nThere are several areas which, when more developed, will provide a means to produce AGIs that are friendly to humanity. These areas of research should be prioritized to prevent possible disasters.\n\n*   _Computer security._ One way by which AGIs might grow rapidly more powerful is by taking over poorly-protected computers on the Internet. Hardening computers and networks against such attacks would help reduce this risk.\n\n*   _AGI confinement._ Incorporating physical mechanisms which limit the AGI can prevent it from inflicting damage. Physical isolation has already been developed (such as [AI Boxing](https://www.lesswrong.com/tag/ai-boxing-containment)) as well as embedded solutions which shut down parts of the system under certain conditions.\n\n*   _Friendly AGI goals._ Embedding an AGI with friendly [terminal values](https://www.lesswrong.com/tag/terminal-value) reduces the risk that it will take action that is harmful to humanity. [Development](http://lukeprog.com/SaveTheWorld.html#goals) in this area has lead to many questions about what _should_ be implemented. However, precise methodologies which, when executed within an AGI, would prevent it from harming humanity have not yet materialized.\n\nSee Also\n--------\n\n*   [AGI](https://wiki.lesswrong.com/wiki/AGI)\n*   [Friendly AI](https://wiki.lesswrong.com/wiki/Friendly_AI)\n*   [Unfriendly AI](https://wiki.lesswrong.com/wiki/Unfriendly_AI)\n*   [AI Boxing](https://www.lesswrong.com/tag/ai-boxing-containment)\n\nReferences\n----------\n\n*   [Intelligence Explosion: Evidence and Import](http://web.archive.org/web/20190430130748/http://intelligence.org/files/IE-EI.pdf) by Luke Muehlhauser and Anna Salamon\n*   [Why We Need Friendly AI](http://www.preventingskynet.com/why-we-need-friendly-ai/) by Eliezer Yudkowsky\n*   [Safety Engineering for Artificial General Intelligence](http://intelligence.org/files/SafetyEngineering.pdf) by Roman Yampolskiy and Joshua Fox\n*   [Leakproofing the Singularity](http://cecs.louisville.edu/ry/LeakproofingtheSingularity.pdf) by Roman Yampolskiy"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb1c7",
    "name": "Nick Bostrom",
    "core": null,
    "slug": "nick-bostrom",
    "oldSlugs": null,
    "postCount": 0,
    "description": {
      "markdown": "**Nick Bostrom** is a philosopher at the University of Oxford, director of the [Future of Humanity Institute](https://www.lesswrong.com/tag/future-of-humanity-institute-fhi) (FHI), the main academic institution on that field. As a director he coordinates and conducts researches on crucial points to the progress and future of humanity. Among those crucial points are: [Artificial General Intelligence](https://www.lesswrong.com/tag/artificial-general-intelligence) (AGI), [Existential risk](https://www.lesswrong.com/tag/existential-risk), [Biological Cognitive Enhancement](https://www.lesswrong.com/tag/nootropics-and-other-cognitive-enhancement) and [Whole brain emulation](https://www.lesswrong.com/tag/whole-brain-emulation). He has personally raised more than 13 million dollars on research grants, awards and donations.\n\nHe also founded the first transhumanistic association, World Transhumanism Association (now [Humanity+](http://http://humanityplus.org/)), in 1998. Bostrom made several major contributions in relevant fields to transhumanism. His more than 200 published papers have been translated to more than 20 languages. They spread throughout topics such as:\n\n*   [Existential risk](https://www.lesswrong.com/tag/existential-risk) – hazards with potential to destroy the entire human race, a concept he was the first to [define](http://www.nickbostrom.com/existential/risks.pdf), give attention to its [large ethical relevance](http://www.existential-risk.org/concept.pdf) and untangle its [particular difficulties](http://www.nickbostrom.com/papers/anthropicshadow.pdf).\n*   [Biological Cognitive Enhancement](https://www.lesswrong.com/tag/nootropics-and-other-cognitive-enhancement) – developing and [heuristic](http://www.nickbostrom.com/evolution.pdf) about how to safely technologically enhance human condition.\n*   [Infinities in ethics](https://www.lesswrong.com/tag/infinities-in-ethics) \\- how to act in a universe where any finite action doesn’t add up good to a infinite world.\n*   [Anthropic principle](http://wiki.lesswrong.com/wiki/Observation_selection_effect) – a better and sound formalization of the anthropic principle, where one must think as a random member of its own reference class.\n\nBostrom has a BA in Philosophy, Mathematics, Mathematical Logic and in Artificial Intelligence; MA in Philosophy and in Physics; MSc in Computational Neuroscience and PhD in Philosophy. [One of his theses in philosophy](http://www.anthropic-principle.com/book/anthropicbias.pdf) entered the Routledge Hall of Fame, and made a formalization of the anthropic principle, giving birth to the Strong self-sampling assumption (SSSA): \"Each observer-moment should reason as if it were randomly selected from the class of all observer-moments in its reference class\". With this formalization many paradoxes emerging from intuitive versions of the anthropic principle were avoided.\n\nLater, the kind of reasoning developed in his thesis lead to many other insights, such as the [Simulation Argument](https://www.lesswrong.com/tag/simulation-argument), demonstrating that there is a considerable chance that we are living inside a computer simulation.\n\nBlog posts\n----------\n\n*   [The Anthropic Trilemma](http://lesswrong.com/lw/19d/the_anthropic_trilemma/)\n*   [Transcription and Summary of Nick Bostrom's Q&A](http://lesswrong.com/lw/8h1/transcription_and_summary_of_nick_bostroms_qa/)\n\nExternal links\n--------------\n\n*   [Nick Bostrom's Home Page](http://www.nickbostrom.com/)\n*   [Future of Humanity Institute's Home Page](http://www.fhi.ox.ac.uk/)\n*   [Nick Bostrom's CV](http://www.nickbostrom.com/cv.pdf)\n*   [Nick Bostrom's site on the Anthropic Principle](http://http://www.anthropic-principle.com/)\n*   [Paper on Infinites Ethics](http://www.nickbostrom.com/ethics/infinite.pdf)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb343",
    "name": "Phyg",
    "core": null,
    "slug": "phyg",
    "oldSlugs": null,
    "postCount": 0,
    "description": {
      "markdown": "**Phyg** is the rot13 cipher of \"cult\". At some point in LessWrong's history, there was concern that discussion of cults on the site would cause \"LessWrong\" to rank highly in Google search results for \"cult\". It was therefore recommended that people instead use the word, \"phyg\", to discuss cults. \n\nCults have been discussed on LessWrong in part because people wondered whether or not LessWrong was maybe a cult – there was enough enthusiasm around how it radically changed your life, and there sure was a charismatic leader. \n\nThe rest of this page is the contents of the Phyg page from the old LessWrong wiki:\n\n* * *\n\n[Less Wrong](https://www.lesswrong.com/about) may or may not be a ~cult~ [phyg](https://www.lesswrong.com/tag/phyg).[^1^](#fn1) [^2^](#fn2)\n\n[Eliezer Yudkowsky](https://www.lesswrong.com/tag/eliezer-yudkowsky) is not the leader[^3^](#fn3)\n\nNor is singularianism.[^4^](#fn4)\n\nWell maybe [RationalWiki disagrees](http://rationalwiki.org/wiki/LessWrong)\n\nReferences\n----------\n\n1.  [http://lesswrong.com/lw/bql/our\\_phyg\\_is\\_not\\_exclusive_enough/](http://lesswrong.com/lw/bql/our_phyg_is_not_exclusive_enough/)[↩](#fnref1)\n2.  [https://www.reddit.com/r/OutOfTheLoop/comments/3ttw2e/what\\_is\\_lesswrong\\_and\\_why\\_do\\_people\\_say\\_it\\_is\\_a/](https://www.reddit.com/r/OutOfTheLoop/comments/3ttw2e/what_is_lesswrong_and_why_do_people_say_it_is_a/)[↩](#fnref2)\n3.  [http://lesswrong.com/lw/4d/youre\\_calling\\_who\\_a\\_cult_leader/](http://lesswrong.com/lw/4d/youre_calling_who_a_cult_leader/)[↩](#fnref3)\n4.  [http://lesswrong.com/lw/atm/cult\\_impressions\\_of\\_less\\_wrongsingularity/](http://lesswrong.com/lw/atm/cult_impressions_of_less_wrongsingularity/)[↩](#fnref4)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb0e0",
    "name": "Eliezer Yudkowsky",
    "core": null,
    "slug": "eliezer-yudkowsky",
    "oldSlugs": null,
    "postCount": 0,
    "description": {
      "markdown": "**Eliezer Yudkowsky** is a research fellow of the [Machine Intelligence Research Institute](https://www.lesswrong.com/tag/machine-intelligence-research-institute-miri), which he co-founded in 2001. He is mainly concerned with the obstacles and importance of developing a [Friendly AI](https://wiki.lesswrong.com/wiki/Friendly_AI), such as a reflective decision theory that would lay a foundation for describing fully recursive self modifying agents that retain stable preferences while rewriting their source code. He also co-founded *Less Wrong*, writing the [Sequences](https://www.lesswrong.com/tag/sequences), long sequences of posts dealing with epistemology, [AGI](https://wiki.lesswrong.com/wiki/AGI), [metaethics](https://www.lesswrong.com/tag/metaethics), [rationality](https://www.lesswrong.com/tag/rationality) and so on.\n\nHe has published several articles, including:\n\n*   [“Cognitive Biases Potentially Affecting Judgment of Global Risks” (2008)](http://intelligence.org/files/CognitiveBiases.pdf): A pioneer compilation of [cognitive biases](https://www.lesswrong.com/tag/bias) – systematic deviations from rationality – influencing our judgment of global catastrophic risks. These are defined as *\"where an adverse outcome would either annihilate Earth-originating intelligent life or permanently and drastically curtail its potential.\"* Examples include volcanic eruptions, pandemic infections, nuclear war, worldwide tyrannies, out-of-control scientific experiments, or cosmic hazards). Yudkowsky's chapter specifically examines how cognitive biases impact thinking about global catastrophic risks.\n*   [“AI as a Positive and Negative Factor in Global Risk. (2008)”](http://intelligence.org/files/AIRisk.pdf): Another chapter in the compilation \"Global Catastrophic Risks\", it analyses possible philosophical and technical failures in the construction of a [Friendly AI](https://wiki.lesswrong.com/wiki/Friendly_AI), which could lead to an [Unfriendly AI](https://wiki.lesswrong.com/wiki/Unfriendly_AI) posing a enormous global risk. He also discusses how a Friendly AI could help decrease some global risks discussed in the book. Finally, because a powerful Friendly AI could reduce global risk, he argues that researching Friendly AI is extremely important for the future of humanity.\n*   [\"Creating Friendly AI\"(2001)](http://intelligence.org/files/CFAI.pdf): One of the first articles to address the challenges in designing the features and cognitive architecture required to produce a benevolent — \"Friendly\" — Artificial Intelligence . It also gives one of the first precise definitions of terms such as Friendly AI and [Seed AI](https://www.lesswrong.com/tag/seed-ai).\n*   [\"Levels of Organization in General Intelligence\" (2002)](http://intelligence.org/files/LOGI.pdf): Analyzes [AGI](https://wiki.lesswrong.com/wiki/AGI) through its decomposition in five subsystems, successive levels of functional organization: Code, sensory modalities, concepts, thoughts, and deliberation. Also discusses some advantages artificial minds would have, such as the possibility of [Recursive self-improvement](https://www.lesswrong.com/tag/recursive-self-improvement).\n*   [\"Coherent Extrapolated Volition\"(2004)](http://intelligence.org/files/CEV.html): Presents the difficulties and possible solutions for incorporating friendliness into an AGI. It argues that making an AGI simply do what we tell it to could be dangerous, since we don't know what we want. Instead we should program the AGI to do what we want, predicting what the vectorial sum of an idealized version of what humanity would want, *\"if we knew more, thought faster, were more the people we wished we were, had grown up farther together”*. He calls this the coherent extrapolated volition of humankind, or [CEV](https://wiki.lesswrong.com/wiki/CEV).\n*   [\"Timeless Decision Theory\" (2010)](http://intelligence.org/files/TDT.pdf): Describes [Timeless decision theory](https://www.lesswrong.com/tag/timeless-decision-theory), *”an extension of causal decision networks that compactly represents uncertainty about correlated computational processes and represents the decision maker as such a process”*. It solves many problems which [Causal Decision Theory](https://www.lesswrong.com/tag/causal-decision-theory) or [Evidential Decision Theory](https://www.lesswrong.com/tag/evidential-decision-theory) don't have a plausible solution: [Newcomb's problem](https://www.lesswrong.com/tag/newcomb-s-problem), [Solomon's Problems](https://www.lesswrong.com/tag/smoking-lesion) and [Prisoner's dilemma](https://www.lesswrong.com/tag/prisoner-s-dilemma).\n*   [\"Complex Value Systems are Required to Realize Valuable Futures\" (2011)](http://intelligence.org/files/ComplexValues.pdf): Discusses the [Complexity of value](https://www.lesswrong.com/tag/complexity-of-value): we can’t come up with a simple rule or description that sums up all human values. It analyzes how this problem makes it difficult to build a valuable future.\n\nLinks\n-----\n\n*   [Eliezer Yudkowsky's posts](http://lesswrong.com/user/Eliezer_Yudkowsky/submitted/) on [Less Wrong](http://lesswrong.com/)\n*   [A list of all of Yudkowsky's posts to Overcoming Bias](https://web.archive.org/web/20140326081311/http://www.cs.auckland.ac.nz/~andwhay/postlist.html), [Dependency graphs for them](https://web.archive.org/web/20130713005256/http://www.cs.auckland.ac.nz/~andwhay/graphlist.html)\n*   [Eliezer Yudkowsky Facts](http://lesswrong.com/lw/4g/eliezer_yudkowsky_facts/) by [steven0461](https://wiki.lesswrong.com/wiki/steven0461)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb342",
    "name": "Rationalist Movement",
    "core": null,
    "slug": "rationalist-movement",
    "oldSlugs": null,
    "postCount": 0,
    "description": {
      "markdown": "The **rationalist movement**, **rationality community**,[^1^](#fn1) **rationalsphere** or **rationalistsphere**[^2^](#fn2) represents a set of modes of [bayesian](https://www.lesswrong.com/tag/bayesianism) thinking from self-described [rationalists](https://www.lesswrong.com/tag/rationalist) or '[aspiring rationalists](https://www.lesswrong.com/tag/aspiring-rationalist)' typically associated with the [Less Wrong](https://www.lesswrong.com/about) [diaspora](https://wiki.lesswrong.com/wiki/diaspora) and their associated communities.\n\n*This page was last properly edited in December 2017, around the time LW2.0 was getting started. It may not accurate reflect the state of affairs in 2020 – either culture or how the community sees itself. – Ruby*\n\nHistory\n-------\n\nFollowing the wind-down of the first era of [Less Wrong](https://www.lesswrong.com/about) in 2014, triggered primarily via the departure of [Eliezer Yudkowsky](https://www.lesswrong.com/tag/eliezer-yudkowsky) and other top writers, the formerly closer-knit on and offline groups continue with a lessened central focus.\n\nArguably, [Scott Alexander](https://www.lesswrong.com/tag/scott-alexander) has emerged as the most influential remaining figurehead and [Slate Star Codex](https://www.lesswrong.com/tag/slate-star-codex) commands the highest single point of engagement within the restructured community.\n\nIn 2016-2017, discussion of revival occurred and Vaniver was made benevolent dictator for life. He in turn empowered Oliver Habryka to form a team and launch the LessWrong 2.0 project which runs the current site.\n\nIllustration\n------------\n\n![](http://slatestarcodex.com/blog_images/ramap.jpg)\n\nThis is a fertile land and we will thrive. We will rule over all this land! And we will call it...this land!\n\nA popular illustration of the communities comprising and related to the movement was created in 2014 on [Slate Star Codex](https://www.lesswrong.com/tag/slate-star-codex)[^3^](#fn3)[^4^](#fn4) (above).\n\nA 2017 conceptual Venn diagram of the 'rationalsphere' has also been created.[^5^](#fn5)\n\nDefinitions\n-----------\n\n[Scott Alexander](https://www.lesswrong.com/tag/scott-alexander) has also suggested the following definition in 2016:[^6^](#fn6)\n\n> The rationalist community is a group of people (of which I’m a part) who met reading the site [Less Wrong](https://www.lesswrong.com/about) and who tend to [hang out together online](https://wiki.lesswrong.com/wiki/diaspora), [sometimes hang out together in real life](https://www.lesswrong.com/about_meetup_groups), and tend to befriend each other, work with each other, date each other, and generally move in the same social circles. Some people[^7^](#fn7) call it a cult, but that’s more a sign of some people having lost vocabulary for anything between “totally atomized individuals” and “outright cult” than any [particular cultishness](https://www.lesswrong.com/tag/phyg).\n> \n> But people keep asking me what exactly the rationalist community is. Like, what is the thing they believe that makes them rationalists? It can’t just be about being [rational](https://wiki.lesswrong.com/wiki/Rationalism), because loads of people are interested in that and most of them aren’t part of the community. And it can’t just be about [transhumanism](https://www.lesswrong.com/tag/transhumanism) because there are a lot of transhumanists who aren’t rationalists, and lots of rationalists who aren’t transhumanists. And it can’t just be about [Bayesianism](https://www.lesswrong.com/tag/bayesianism), because pretty much everyone, rationalist or otherwise, agrees that is a kind of statistics that is useful for some things but not others. So what, exactly, is it?\n> \n> This question has always bothered me, but now after thinking about it a lot I finally have a clear answer: rationalism is the belief that [Eliezer Yudkowsky](https://www.lesswrong.com/tag/eliezer-yudkowsky) is the rightful caliph.\n> \n> No! Sorry! I think “the rationalist community” is a tribe much like the Sunni or Shia that started off with some pre-existing differences, found a rallying flag, and then developed a culture.\n\nOther definitions include:[^8^](#fn8)\n\n> ...typical rationalist philosophical positions include reductionism, materialism, moral non-realism, utilitarianism, anti-[deathism](https://hpluspedia.org/wiki/Deathism) and transhumanism. Rationalists across all three groups tend to have high opinions of the [Sequences](https://www.lesswrong.com/tag/sequences) and Slate Star Codex and cite both in arguments; rationalist discourse norms were shaped by [How To Actually Change Your Mind](https://www.lesswrong.com/tag/how-to-actually-change-your-mind) and [37 Ways Words Can Be Wrong](https://wiki.lesswrong.com/wiki/A_Human's_Guide_to_Words), among others.\n\nAnd:[^9^](#fn9)\n\n> ...a community that call themselves Rationalists, that read ‘high-IQ sites’ such as [Marginal Revolution](https://wiki.lesswrong.com/wiki/Marginal_Revolution), Less Wrong, and Slate Star Codex, and according to various surveys, identify as liberal, are atheist or agnostic, and, in general, hold a ‘realist’ philosophical worldview.\n\nAdditional attributes\n---------------------\n\nA crowdsourced list of traits of community:[^10^](#fn10)\n\n*   Seeing the [Prisoner's dilemma](https://www.lesswrong.com/tag/prisoner-s-dilemma) and other [game theory](https://www.lesswrong.com/tag/game-theory) applications everywhere\n*   Being perpetually vigilant of personal [biases](https://www.lesswrong.com/tag/bias)\n*   [Epistemic rationality](https://wiki.lesswrong.com/wiki/Epistemic_rationality) through constantly aligning one's beliefs as closely as possible with the actual state of the world\n\nIndividualistic vs Organizational\n---------------------------------\n\n![](https://wiki.lesswrong.com/images/thumb/b/b2/Rationalsphere.png/300px-Rationalsphere.png)\n\nIt's a sphere!\n\n[Raemon](https://wiki.lesswrong.com/wiki/Raemon) argues the rationalist movement could be subdivided into individualistic and organisational views.[^11^](#fn11) [Project Hufflepuff](https://wiki.lesswrong.com/wiki/Project_Hufflepuff) attempts to strengthen both approaches.\n\nThe Rationalsphere\n\nContains key modes of thinking for the individual including:\n\n*   Truthseeking - biases, empiricism etc\n*   Impact - making the world a better place ( e.g. effective altruism, AI safety )\n*   Human - becoming a better person\n\nOn the other hand:\n\nThe Rationality Community\n\nEncompasses the major subscribing organisations ( [CFAR](https://wiki.lesswrong.com/wiki/CFAR), [Giving What We Can](https://wiki.lesswrong.com/wiki/Giving_What_We_Can) etc ) as well as the many [meetup groups](https://www.lesswrong.com/about_meetup_groups) , friends and relationships discovered from participation in the community.\n\nRationalist-adjacent\n--------------------\n\nAdjacent ideas include:[^12^](#fn12)\n\n> There are people who agree on few to no rationalist positions but still like going to our parties and reading our blog posts. I coined the term “**rationalist-adjacent**” for this group before I got the idea that the names of all subdivisions of the rationalist community should begin with the letter C...A lot of Less Wrong references a lot of nerd culture, such as catgirls, anime, fanfiction, [Harry Potter](https://www.lesswrong.com/tag/methods-of-rationality-fanfiction), My Little Pony, etc\n\n*   [Effective altruism](https://www.lesswrong.com/tag/effective-altruism)\n*   [Neoreaction movement](http://rationalwiki.org/wiki/Neoreactionary_movement),[^13^](#fn13) \\- A notoriously adjacent idea whist being explicitly refuted by figures such as Eliezer[^14^](#fn14)[^15^](#fn15) and Scott,[^16^](#fn16) is often actively related by [critics](https://www.lesswrong.com/tag/criticisms-of-the-rationalist-movement).[^17^](#fn17)[^18^](#fn18)[^19^](#fn19)\n*   [Polyamory](https://en.wikipedia.org/wiki/Polyamory)[^20^](#fn20)[^21^](#fn21)[^22^](#fn22)\n*   The [Facebook](https://wiki.lesswrong.com/wiki/Facebook) [group](https://www.facebook.com/groups/144017955332/) formally known as 'LessWrong' now 'Brain Debugging Discussion'\n*   Wider [rationalist fiction](https://www.lesswrong.com/tag/fiction)\n\nSkepticism of term\n------------------\n\nThere is a some drive to avoid any 'isms' and instead focus on 'rationality' rather than 'rationalism.[^23^](#fn23)\n\nCulture\n-------\n\nThe culture is often defined by its saying such as those featured in [rationalists-out-of-context.tumblr.com](https://rationalists-out-of-context.tumblr.com).\n\nRelated organizations\n---------------------\n\n*   [Center for Applied Rationality](https://wiki.lesswrong.com/wiki/Center_for_Applied_Rationality)\n*   [MIRI](https://wiki.lesswrong.com/wiki/MIRI)\n\nSee also\n--------\n\n*   [Criticisms of the rationalist movement](https://www.lesswrong.com/tag/criticisms-of-the-rationalist-movement)\n*   [Diaspora](https://wiki.lesswrong.com/wiki/Diaspora)\n*   [History of Less Wrong](https://www.lesswrong.com/tag/history-of-less-wrong)\n*   [Reddit](https://www.lesswrong.com/tag/reddit)\n*   [Twitter](https://www.lesswrong.com/tag/twitter)\n\nReferences \n-----------\n\n1.  [Economics, stale memes, and distraction from productive activity](https://noahpinionblog.blogspot.co.uk/2017/04/are-rationals-dense.html)[↩](#fnref1)\n2.  There doesn't appear to be clearly preferred term - August 2017[↩](#fnref2)\n3.  [http://slatestarcodex.com/2014/09/05/mapmaker-mapmaker-make-me-a-map/](http://slatestarcodex.com/2014/09/05/mapmaker-mapmaker-make-me-a-map/)[↩](#fnref3)\n4.  Created with Photoshop and Fractal Mapper. Uses a lot of free clipart. Relevant tutorials at the [Cartographers Guild forums.](https://www.cartographersguild.com/forum.php)[↩](#fnref4)\n5.  [http://lesswrong.com/r/discussion/lw/ov2/what\\_exactly\\_is\\_the\\_rationality_community/](http://lesswrong.com/r/discussion/lw/ov2/what_exactly_is_the_rationality_community/)[↩](#fnref5)\n6.  [http://slatestarcodex.com/2016/04/04/the-ideology-is-not-the-movement/](http://slatestarcodex.com/2016/04/04/the-ideology-is-not-the-movement/)[↩](#fnref6)\n7.  [https://hpluspedia.org/wiki/RationalWiki](https://hpluspedia.org/wiki/RationalWiki)[↩](#fnref7)\n8.  [https://thingofthings.wordpress.com/2015/05/07/divisions-within-the-lw-sphere/](https://thingofthings.wordpress.com/2015/05/07/divisions-within-the-lw-sphere/)[↩](#fnref8)\n9.  [http://greyenlightenment.com/defining-and-understanding-rationalism/](http://greyenlightenment.com/defining-and-understanding-rationalism/)[↩](#fnref9)\n10.  [https://www.reddit.com/r/slatestarcodex/comments/65cnar/definitions\\_of\\_the\\_rationalist\\_movement/](https://www.reddit.com/r/slatestarcodex/comments/65cnar/definitions_of_the_rationalist_movement/)[↩](#fnref10)\n11.  [What exactly is the \"Rationality Community?\"](http://lesswrong.com/lw/ov2/what_exactly_is_the_rationality_community/)[↩](#fnref11)\n12.  [https://web.archive.org/web/20130424060436/http://habitableworlds.wordpress.com/2013/04/21/visualizing-neoreaction/](https://web.archive.org/web/20130424060436/http://habitableworlds.wordpress.com/2013/04/21/visualizing-neoreaction/)[↩](#fnref13)\n13.  [http://yudkowsky.tumblr.com/post/142497361345/this-isnt-going-to-work-but-for-the-record-and](http://yudkowsky.tumblr.com/post/142497361345/this-isnt-going-to-work-but-for-the-record-and)[↩](#fnref14)\n14.  [http://lesswrong.com/lw/fh4/why\\_is\\_mencius\\_moldbug\\_so\\_popular\\_on\\_less\\_wrong/](http://lesswrong.com/lw/fh4/why_is_mencius_moldbug_so_popular_on_less_wrong/)[↩](#fnref15)\n15.  [http://slatestarcodex.com/2013/10/20/the-anti-reactionary-faq/](http://slatestarcodex.com/2013/10/20/the-anti-reactionary-faq/)[↩](#fnref16)\n16.  [https://hpluspedia.org/wiki/The\\_Silicon\\_Ideology](https://hpluspedia.org/wiki/The_Silicon_Ideology)[↩](#fnref17)\n17.  [https://techcrunch.com/2013/11/22/geeks-for-monarchy/](https://techcrunch.com/2013/11/22/geeks-for-monarchy/)[↩](#fnref18)\n18.  [https://social-epistemology.com/2016/09/23/the-violence-of-pure-reason-neoreaction-a-basilisk-adam-riggio/](https://social-epistemology.com/2016/09/23/the-violence-of-pure-reason-neoreaction-a-basilisk-adam-riggio/)[↩](#fnref19)\n19.  [http://lesswrong.com/lw/79x/polyhacking/](http://lesswrong.com/lw/79x/polyhacking/)[↩](#fnref20)\n20.  [http://slatestarcodex.com/2013/04/06/polyamory-is-boring/](http://slatestarcodex.com/2013/04/06/polyamory-is-boring/)[↩](#fnref21)\n21.  [http://www.jdpressman.com/public/lwsurvey2016/Survey\\_554193\\_LessWrong\\_Diaspora\\_2016_Survey(2).pdf](http://www.jdpressman.com/public/lwsurvey2016/Survey_554193_LessWrong_Diaspora_2016_Survey(2).pdf)[↩](#fnref22)\n22.  [Note on Terminology: \"Rationality\", not \"Rationalism\"](http://lesswrong.com/lw/3rd/note_on_terminology_rationality_not_rationalism/)[↩](#fnref23)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb2e4",
    "name": "Induction",
    "core": null,
    "slug": "induction",
    "oldSlugs": null,
    "postCount": 0,
    "description": {
      "markdown": "**Induction** usually refers to a form of reasoning that has specific examples as premises and general propositions as conclusions. For example, arguments such as \"Swans 1,2,3, …,_n_ are white, hence all swans are white\", take the specific observations of a finite number (_n_) of swans been white to a general conclusion that all swan are whites.\n\nModern views of induction state that any form of reasoning where the conclusion isn't necessarily entailed in the premises is a form of inductive reasoning. Therefore, even inferences which proceed from general premises to specific conclusions can be inductive, for example \"The sun has always risen, so it will also rise tomorrow\". In contrast, in deductive reasoning the conclusions are logically entailed by the premises. Contrary to deduction, induction can be wrong since the conclusions depend on the way the world actually is, not merely on the logical structure of the argument.\n\nThe Problem of Induction\n------------------------\n\nThere has historically been a problem with the justification of the validity of induction. Hume argued that the justification for induction could either be a deduction or an induction. Since deductive reasoning only results in necessary conclusions and inductions can fail, the justification for inductive reasoning could not be deductive. But any inductive justification would be circular[1](http://plato.stanford.edu/entries/induction-problem/#CanIndJus).\n\nProbabilistic Induction\n-----------------------\n\nIt’s possible to engage in probabilistic inductive reasoning, such as \"95% of humans who ever lived have died; hence I’m going to die\". This kind of reasoning employs [Bayesian probability](https://www.lesswrong.com/tag/bayesian-probability), in which case the conclusion is also a probability and induction is taken to be a way of updating your beliefs given evidence (finding out that most humans who have ever lived have died increases your probability that you will die).\n\n[Solomonoff induction](https://www.lesswrong.com/tag/solomonoff-induction) is a formalization of the problem of induction which has been claimed to solve the problem of induction. It starts with all possible hypotheses (sequences) as represented by computer programs (that generate those sequences), weighted by their simplicity. It then proceeds to discard any hypotheses which are inconsistent with the data, and to update the probabilities of the remaining hypotheses.\n\nMathematical Induction\n----------------------\n\n[Mathematical induction](https://en.wikipedia.org/wiki/Mathematical_induction) is method of mathematical proof where one proves a statement holds for all possible n by showing it holds for the lowest _n_ and then that this statement if preserved by any operation which increases the value of _n_. For sets with finite members - or infinities members than can be indexed in the natural numbers -, it suffices to show the statement is preserved by the successor operation (If it is true for _n_, then it is true for'' n+1''). Because the conclusion is necessary given the premises, mathematical induction is taken to be a form of deductive reasoning and it isn't affected by the problem of induction.\n\nSee Also\n--------\n\n*   [Stanford Encyclopedia entry on the Problem of Induction](http://plato.stanford.edu/entries/induction-problem/)\n*   [Solomonoff induction](https://www.lesswrong.com/tag/solomonoff-induction)\n*   [Rationality](https://www.lesswrong.com/tag/rationality)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb342",
    "name": "Rationalist Movement",
    "core": null,
    "slug": "rationalist-movement",
    "oldSlugs": null,
    "postCount": 0,
    "description": {
      "markdown": "The **rationalist movement**, **rationality community**,[^1^](#fn1) **rationalsphere** or **rationalistsphere**[^2^](#fn2) represents a set of modes of [bayesian](https://www.lesswrong.com/tag/bayesianism) thinking from self-described [rationalists](https://www.lesswrong.com/tag/rationalist) or '[aspiring rationalists](https://www.lesswrong.com/tag/aspiring-rationalist)' typically associated with the [Less Wrong](https://www.lesswrong.com/about) [diaspora](https://wiki.lesswrong.com/wiki/diaspora) and their associated communities.\n\n*This page was last properly edited in December 2017, around the time LW2.0 was getting started. It may not accurate reflect the state of affairs in 2020 – either culture or how the community sees itself. – Ruby*\n\nHistory\n-------\n\nFollowing the wind-down of the first era of [Less Wrong](https://www.lesswrong.com/about) in 2014, triggered primarily via the departure of [Eliezer Yudkowsky](https://www.lesswrong.com/tag/eliezer-yudkowsky) and other top writers, the formerly closer-knit on and offline groups continue with a lessened central focus.\n\nArguably, [Scott Alexander](https://www.lesswrong.com/tag/scott-alexander) has emerged as the most influential remaining figurehead and [Slate Star Codex](https://www.lesswrong.com/tag/slate-star-codex) commands the highest single point of engagement within the restructured community.\n\nIn 2016-2017, discussion of revival occurred and Vaniver was made benevolent dictator for life. He in turn empowered Oliver Habryka to form a team and launch the LessWrong 2.0 project which runs the current site.\n\nIllustration\n------------\n\n![](http://slatestarcodex.com/blog_images/ramap.jpg)\n\nThis is a fertile land and we will thrive. We will rule over all this land! And we will call it...this land!\n\nA popular illustration of the communities comprising and related to the movement was created in 2014 on [Slate Star Codex](https://www.lesswrong.com/tag/slate-star-codex)[^3^](#fn3)[^4^](#fn4) (above).\n\nA 2017 conceptual Venn diagram of the 'rationalsphere' has also been created.[^5^](#fn5)\n\nDefinitions\n-----------\n\n[Scott Alexander](https://www.lesswrong.com/tag/scott-alexander) has also suggested the following definition in 2016:[^6^](#fn6)\n\n> The rationalist community is a group of people (of which I’m a part) who met reading the site [Less Wrong](https://www.lesswrong.com/about) and who tend to [hang out together online](https://wiki.lesswrong.com/wiki/diaspora), [sometimes hang out together in real life](https://www.lesswrong.com/about_meetup_groups), and tend to befriend each other, work with each other, date each other, and generally move in the same social circles. Some people[^7^](#fn7) call it a cult, but that’s more a sign of some people having lost vocabulary for anything between “totally atomized individuals” and “outright cult” than any [particular cultishness](https://www.lesswrong.com/tag/phyg).\n> \n> But people keep asking me what exactly the rationalist community is. Like, what is the thing they believe that makes them rationalists? It can’t just be about being [rational](https://wiki.lesswrong.com/wiki/Rationalism), because loads of people are interested in that and most of them aren’t part of the community. And it can’t just be about [transhumanism](https://www.lesswrong.com/tag/transhumanism) because there are a lot of transhumanists who aren’t rationalists, and lots of rationalists who aren’t transhumanists. And it can’t just be about [Bayesianism](https://www.lesswrong.com/tag/bayesianism), because pretty much everyone, rationalist or otherwise, agrees that is a kind of statistics that is useful for some things but not others. So what, exactly, is it?\n> \n> This question has always bothered me, but now after thinking about it a lot I finally have a clear answer: rationalism is the belief that [Eliezer Yudkowsky](https://www.lesswrong.com/tag/eliezer-yudkowsky) is the rightful caliph.\n> \n> No! Sorry! I think “the rationalist community” is a tribe much like the Sunni or Shia that started off with some pre-existing differences, found a rallying flag, and then developed a culture.\n\nOther definitions include:[^8^](#fn8)\n\n> ...typical rationalist philosophical positions include reductionism, materialism, moral non-realism, utilitarianism, anti-[deathism](https://hpluspedia.org/wiki/Deathism) and transhumanism. Rationalists across all three groups tend to have high opinions of the [Sequences](https://www.lesswrong.com/tag/sequences) and Slate Star Codex and cite both in arguments; rationalist discourse norms were shaped by [How To Actually Change Your Mind](https://www.lesswrong.com/tag/how-to-actually-change-your-mind) and [37 Ways Words Can Be Wrong](https://wiki.lesswrong.com/wiki/A_Human's_Guide_to_Words), among others.\n\nAnd:[^9^](#fn9)\n\n> ...a community that call themselves Rationalists, that read ‘high-IQ sites’ such as [Marginal Revolution](https://wiki.lesswrong.com/wiki/Marginal_Revolution), Less Wrong, and Slate Star Codex, and according to various surveys, identify as liberal, are atheist or agnostic, and, in general, hold a ‘realist’ philosophical worldview.\n\nAdditional attributes\n---------------------\n\nA crowdsourced list of traits of community:[^10^](#fn10)\n\n*   Seeing the [Prisoner's dilemma](https://www.lesswrong.com/tag/prisoner-s-dilemma) and other [game theory](https://www.lesswrong.com/tag/game-theory) applications everywhere\n*   Being perpetually vigilant of personal [biases](https://www.lesswrong.com/tag/bias)\n*   [Epistemic rationality](https://wiki.lesswrong.com/wiki/Epistemic_rationality) through constantly aligning one's beliefs as closely as possible with the actual state of the world\n\nIndividualistic vs Organizational\n---------------------------------\n\n![](https://wiki.lesswrong.com/images/thumb/b/b2/Rationalsphere.png/300px-Rationalsphere.png)\n\nIt's a sphere!\n\n[Raemon](https://wiki.lesswrong.com/wiki/Raemon) argues the rationalist movement could be subdivided into individualistic and organisational views.[^11^](#fn11) [Project Hufflepuff](https://wiki.lesswrong.com/wiki/Project_Hufflepuff) attempts to strengthen both approaches.\n\nThe Rationalsphere\n\nContains key modes of thinking for the individual including:\n\n*   Truthseeking - biases, empiricism etc\n*   Impact - making the world a better place ( e.g. effective altruism, AI safety )\n*   Human - becoming a better person\n\nOn the other hand:\n\nThe Rationality Community\n\nEncompasses the major subscribing organisations ( [CFAR](https://wiki.lesswrong.com/wiki/CFAR), [Giving What We Can](https://wiki.lesswrong.com/wiki/Giving_What_We_Can) etc ) as well as the many [meetup groups](https://www.lesswrong.com/about_meetup_groups) , friends and relationships discovered from participation in the community.\n\nRationalist-adjacent\n--------------------\n\nAdjacent ideas include:[^12^](#fn12)\n\n> There are people who agree on few to no rationalist positions but still like going to our parties and reading our blog posts. I coined the term “**rationalist-adjacent**” for this group before I got the idea that the names of all subdivisions of the rationalist community should begin with the letter C...A lot of Less Wrong references a lot of nerd culture, such as catgirls, anime, fanfiction, [Harry Potter](https://www.lesswrong.com/tag/methods-of-rationality-fanfiction), My Little Pony, etc\n\n*   [Effective altruism](https://www.lesswrong.com/tag/effective-altruism)\n*   [Neoreaction movement](http://rationalwiki.org/wiki/Neoreactionary_movement),[^13^](#fn13) \\- A notoriously adjacent idea whist being explicitly refuted by figures such as Eliezer[^14^](#fn14)[^15^](#fn15) and Scott,[^16^](#fn16) is often actively related by [critics](https://www.lesswrong.com/tag/criticisms-of-the-rationalist-movement).[^17^](#fn17)[^18^](#fn18)[^19^](#fn19)\n*   [Polyamory](https://en.wikipedia.org/wiki/Polyamory)[^20^](#fn20)[^21^](#fn21)[^22^](#fn22)\n*   The [Facebook](https://wiki.lesswrong.com/wiki/Facebook) [group](https://www.facebook.com/groups/144017955332/) formally known as 'LessWrong' now 'Brain Debugging Discussion'\n*   Wider [rationalist fiction](https://www.lesswrong.com/tag/fiction)\n\nSkepticism of term\n------------------\n\nThere is a some drive to avoid any 'isms' and instead focus on 'rationality' rather than 'rationalism.[^23^](#fn23)\n\nCulture\n-------\n\nThe culture is often defined by its saying such as those featured in [rationalists-out-of-context.tumblr.com](https://rationalists-out-of-context.tumblr.com).\n\nRelated organizations\n---------------------\n\n*   [Center for Applied Rationality](https://wiki.lesswrong.com/wiki/Center_for_Applied_Rationality)\n*   [MIRI](https://wiki.lesswrong.com/wiki/MIRI)\n\nSee also\n--------\n\n*   [Criticisms of the rationalist movement](https://www.lesswrong.com/tag/criticisms-of-the-rationalist-movement)\n*   [Diaspora](https://wiki.lesswrong.com/wiki/Diaspora)\n*   [History of Less Wrong](https://www.lesswrong.com/tag/history-of-less-wrong)\n*   [Reddit](https://www.lesswrong.com/tag/reddit)\n*   [Twitter](https://www.lesswrong.com/tag/twitter)\n\nReferences \n-----------\n\n1.  [Economics, stale memes, and distraction from productive activity](https://noahpinionblog.blogspot.co.uk/2017/04/are-rationals-dense.html)[↩](#fnref1)\n2.  There doesn't appear to be clearly preferred term - August 2017[↩](#fnref2)\n3.  [http://slatestarcodex.com/2014/09/05/mapmaker-mapmaker-make-me-a-map/](http://slatestarcodex.com/2014/09/05/mapmaker-mapmaker-make-me-a-map/)[↩](#fnref3)\n4.  Created with Photoshop and Fractal Mapper. Uses a lot of free clipart. Relevant tutorials at the [Cartographers Guild forums.](https://www.cartographersguild.com/forum.php)[↩](#fnref4)\n5.  [http://lesswrong.com/r/discussion/lw/ov2/what\\_exactly\\_is\\_the\\_rationality_community/](http://lesswrong.com/r/discussion/lw/ov2/what_exactly_is_the_rationality_community/)[↩](#fnref5)\n6.  [http://slatestarcodex.com/2016/04/04/the-ideology-is-not-the-movement/](http://slatestarcodex.com/2016/04/04/the-ideology-is-not-the-movement/)[↩](#fnref6)\n7.  [https://hpluspedia.org/wiki/RationalWiki](https://hpluspedia.org/wiki/RationalWiki)[↩](#fnref7)\n8.  [https://thingofthings.wordpress.com/2015/05/07/divisions-within-the-lw-sphere/](https://thingofthings.wordpress.com/2015/05/07/divisions-within-the-lw-sphere/)[↩](#fnref8)\n9.  [http://greyenlightenment.com/defining-and-understanding-rationalism/](http://greyenlightenment.com/defining-and-understanding-rationalism/)[↩](#fnref9)\n10.  [https://www.reddit.com/r/slatestarcodex/comments/65cnar/definitions\\_of\\_the\\_rationalist\\_movement/](https://www.reddit.com/r/slatestarcodex/comments/65cnar/definitions_of_the_rationalist_movement/)[↩](#fnref10)\n11.  [What exactly is the \"Rationality Community?\"](http://lesswrong.com/lw/ov2/what_exactly_is_the_rationality_community/)[↩](#fnref11)\n12.  [https://web.archive.org/web/20130424060436/http://habitableworlds.wordpress.com/2013/04/21/visualizing-neoreaction/](https://web.archive.org/web/20130424060436/http://habitableworlds.wordpress.com/2013/04/21/visualizing-neoreaction/)[↩](#fnref13)\n13.  [http://yudkowsky.tumblr.com/post/142497361345/this-isnt-going-to-work-but-for-the-record-and](http://yudkowsky.tumblr.com/post/142497361345/this-isnt-going-to-work-but-for-the-record-and)[↩](#fnref14)\n14.  [http://lesswrong.com/lw/fh4/why\\_is\\_mencius\\_moldbug\\_so\\_popular\\_on\\_less\\_wrong/](http://lesswrong.com/lw/fh4/why_is_mencius_moldbug_so_popular_on_less_wrong/)[↩](#fnref15)\n15.  [http://slatestarcodex.com/2013/10/20/the-anti-reactionary-faq/](http://slatestarcodex.com/2013/10/20/the-anti-reactionary-faq/)[↩](#fnref16)\n16.  [https://hpluspedia.org/wiki/The\\_Silicon\\_Ideology](https://hpluspedia.org/wiki/The_Silicon_Ideology)[↩](#fnref17)\n17.  [https://techcrunch.com/2013/11/22/geeks-for-monarchy/](https://techcrunch.com/2013/11/22/geeks-for-monarchy/)[↩](#fnref18)\n18.  [https://social-epistemology.com/2016/09/23/the-violence-of-pure-reason-neoreaction-a-basilisk-adam-riggio/](https://social-epistemology.com/2016/09/23/the-violence-of-pure-reason-neoreaction-a-basilisk-adam-riggio/)[↩](#fnref19)\n19.  [http://lesswrong.com/lw/79x/polyhacking/](http://lesswrong.com/lw/79x/polyhacking/)[↩](#fnref20)\n20.  [http://slatestarcodex.com/2013/04/06/polyamory-is-boring/](http://slatestarcodex.com/2013/04/06/polyamory-is-boring/)[↩](#fnref21)\n21.  [http://www.jdpressman.com/public/lwsurvey2016/Survey\\_554193\\_LessWrong\\_Diaspora\\_2016_Survey(2).pdf](http://www.jdpressman.com/public/lwsurvey2016/Survey_554193_LessWrong_Diaspora_2016_Survey(2).pdf)[↩](#fnref22)\n22.  [Note on Terminology: \"Rationality\", not \"Rationalism\"](http://lesswrong.com/lw/3rd/note_on_terminology_rationality_not_rationalism/)[↩](#fnref23)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb119",
    "name": "Unfriendly Artificial Intelligence",
    "core": null,
    "slug": "unfriendly-artificial-intelligence",
    "oldSlugs": null,
    "postCount": 0,
    "description": {
      "markdown": "An **Unfriendly artificial intelligence** (or **UFAI**) is an [artificial general intelligence](https://www.lesswrong.com/tag/artificial-general-intelligence) capable of causing [great harm](https://www.lesswrong.com/tag/existential-risk) to humanity, and having goals that [make it useful](https://wiki.lesswrong.com/wiki/Instrumental_values) for the AI to do so. The AI's goals don't need to be antagonistic to humanity's goals for it to be Unfriendly; there are [strong reasons](https://www.lesswrong.com/tag/instrumental-convergence) to expect that almost any powerful AGI not explicitly programmed to be benevolent to humans is lethal. A [paperclip maximizer](https://www.lesswrong.com/tag/paperclip-maximizer) is often imagined as an illustrative example of an unFriendly AI indifferent to humanity. An AGI specifically designed to have a positive effect on humanity is called a [Friendly AI](https://wiki.lesswrong.com/wiki/Friendly_AI).\n\nSee also\n--------\n\n*   [Mind design space](https://www.lesswrong.com/tag/mind-design-space), [magical categories](https://www.lesswrong.com/tag/magical-categories)\n*   [Really powerful optimization process](https://www.lesswrong.com/tag/really-powerful-optimization-process)\n*   [Basic AI drives](https://www.lesswrong.com/tag/instrumental-convergence)\n*   [Paperclip maximizer](https://www.lesswrong.com/tag/paperclip-maximizer)\n*   [Existential risk](https://www.lesswrong.com/tag/existential-risk)\n*   [Friendly AI](https://wiki.lesswrong.com/wiki/Friendly_AI)\n\nReferences\n----------\n\n*   Eliezer S. Yudkowsky (2008). \"[Artificial Intelligence as a Positive and Negative Factor in Global Risk](https://yudkowsky.net/singularity/ai-risk/)\". Global Catastrophic Risks. Oxford University Press. ([PDF](http://intelligence.org/files/AIPosNegFactor.pdf))\n*   Stephen M. Omohundro (2008). \"[The Basic AI Drives](https://selfawaresystems.com/2007/11/30/paper-on-the-basic-ai-drives/)\". Frontiers in Artificial Intelligence and Applications (IOS Press). ([PDF](http://selfawaresystems.files.wordpress.com/2008/01/ai_drives_final.pdf))"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb298",
    "name": "AGI Sputnik Moment",
    "core": null,
    "slug": "agi-sputnik-moment",
    "oldSlugs": null,
    "postCount": 0,
    "description": {
      "markdown": "An **AGI Sputnik moment** is a term coined by [Ben Goertzel](https://www.lesswrong.com/tag/ben-goertzel) during a 2011 interview. It describes a moment when some project or program makes an impressive demonstration of AGI and convinces the general public and private entitites of its feasibility. The author proposes that if that moment happens somewhere soon, the funding and investment in AGI development through computer science that would ensue would make it surpass other approaches, such as brain simulation.\n\n![](https://wiki.lesswrong.com/images/thumb/b/be/Deep_Blue.jpg/200px-Deep_Blue.jpg)\n\nThe IBM computer Deep Blue defeated world chess champion Gary Kasparov in a highly publicized event, creating a narrow AI Sputnik moment.\n\nThe phrase refers to the successful launching of the Russian satellite Sputnik, which demonstrated the possibility of space technology to the public. This event triggered the ensuing space race between the United States and the Soviet Union, leading to long-term funding of space projects from both governments. The analogy is not meant to imply government funding for AGI, only that the event convinces non-specialists of the practicality of AGI. Goertzel expressed desire towards this type of demonstration as a method of gaining funding for [OpenCog](http://opencog.org/), his AGI project.\n\nGoertzel and Pitt have also emphasized the role of an AGI Sputnik moment in AGI development regulation. Their idea is that such a moment would mark the beginning of a rapid developmental progress, which would render any future regulation attempt futile or dangerous. As such, more effort should be put in developing research -- especially open research -- regarding AGI safety before such moment occurs.\n\nFurther Reading & References\n----------------------------\n\n*   [What Would It Take to Move Rapidly Toward Beneficial Human-Level AGI?](http://multiverseaccordingtoben.blogspot.com/2010_10_10_archive.html) on Ben Goertzel's blog\n*   [Seeking the Sputnik of AGI](http://hplusmagazine.com/2011/03/30/seeking-the-sputnik-of-agi/), an Interview between Ben Goertzel and Hugo de Garis\n\nSee Also\n--------\n\n*   [AGI](https://wiki.lesswrong.com/wiki/AGI)\n*   [Friendly AI](https://wiki.lesswrong.com/wiki/Friendly_AI)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb2b2",
    "name": "AI Arms Race",
    "core": null,
    "slug": "ai-arms-race",
    "oldSlugs": null,
    "postCount": 0,
    "description": {
      "markdown": "An **AI arms race** is a situation where multiple parties are trying to be the first to develop machine intelligence technology.\n\n*See also*: [AI Governance](https://www.lesswrong.com/tag/ai-governance), [Intelligence explosion](https://www.lesswrong.com/tag/intelligence-explosion), [Existential Risk](https://www.lesswrong.com/tag/existential-risk), [Artificial General Intelligence](https://www.lesswrong.com/tag/artificial-general-intelligence)\n\nHumanity has some historical experience with arms races involving nuclear weapons technology. However, [Arms races and intelligence explosions](http://singularityhypothesis.blogspot.com/2011/04/arms-races-and-intelligence-explosions.html) names a few important differences between nuclear weapons and AI technology, which may create dynamics in AI arms races that we have not seen elsewhere.\n\n*   If an [intelligence explosion](https://www.lesswrong.com/tag/intelligence-explosion) occurs, this could allow the first party passing the relevant threshold to develop extremely advanced technology in years, months, or less, creating a strong winner-takes-all effect.\n*   The development of AI technology carries the risk of creating [unfriendly AI](https://www.lesswrong.com/tag/unfriendly-artificial-intelligence), potentially causing human extinction.\n*   Non-military benefits from AI will make arms control seem undesirable; the fact that AI development requires only researchers and computers will make arms control difficult. On the other hand, the risks involved provide strong reasons to try, and AI systems could themselves help enforce agreements.\n\nIf the benefits of an intelligence explosion accrue to the group that created it, and the risks affect the entire world, this creates an incentive to sacrifice safety for speed. In addition to the risk of accidental unfriendly AI, there is the risk that the winner of an arms race turns into a badly-behaved human [singleton](https://www.lesswrong.com/tag/singleton).\n\nExternal links\n--------------\n\n*   [Arms races and intelligence explosions](http://singularityhypothesis.blogspot.com/2011/04/arms-races-and-intelligence-explosions.html)\n*   [Arms control and intelligence explosions](http://intelligence.org/files/ArmsControl.pdf)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb10d",
    "name": "Robin Hanson",
    "core": null,
    "slug": "robin-hanson",
    "oldSlugs": null,
    "postCount": 0,
    "description": {
      "markdown": "**Robin Hanson** is an associate professor of economics at George Mason University.\n\nRobin Hanson was one of the two primary authors on [Overcoming Bias](https://www.lesswrong.com/tag/overcoming-bias).  After [LessWrong](https://www.lesswrong.com/about) portal was launched, he converted [Overcoming Bias](https://www.lesswrong.com/tag/overcoming-bias) into being his personal blog.\n\nLinks\n-----\n\n*   Robin Hanson's bio [on his personal site](http://hanson.gmu.edu/bio.html), [on Overcoming Bias](http://www.overcomingbias.com/bio)\n*   [A list of all of Hanson's posts to Overcoming Bias with dependency graphs](http://web.archive.org/web/20161020114937/https://www.cs.auckland.ac.nz/~andwhay/hpostlist.html)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb29c",
    "name": "Gödel Machine",
    "core": null,
    "slug": "gödel-machine",
    "oldSlugs": null,
    "postCount": 0,
    "description": {
      "markdown": "A **Gödel machine** is an approach to [Artificial General Intelligence](https://www.lesswrong.com/tag/artificial-general-intelligence) that uses a [recursive self-improvement](https://www.lesswrong.com/tag/recursive-self-improvement) architecture proposed by Jürgen Schmidhuber. It was inspired by the mathematical theories of Kurt Gödel, where one could always find a mathematical truth or axiom that if attached to a formal system would make it stronger. A Gödel Machine is a universal problem solver that will make provably optimal self-improvements – self-improvements which can be proved to better maximize its [utility](https://www.lesswrong.com/tag/utility).\n\nSchmidhuber’s design consists of a *solver*, which attempts to solve the goals set for the machine, and a *searcher*, which has access to a set of axioms which completely describe the machine. One example design[^1^](#fn1) contains:\n\n*   **Hardware axioms** describing the hardware, formally specifying how certain components of the machine state (other than environmental inputs) may change from one cycle to the next.\n*   **Reward axioms** defining the computational costs of any hardware instruction, and physical costs of output actions, such as control signals.\n*   **Environment axioms** restricting the way the environment will produce new inputs in reaction to sequences of outputs.\n*   **Uncertainty axioms** (such as standard axioms of probability theory), **string manipulation axioms**.\n*   **Initial state axioms**, with information about how to reconstruct the initial state or parts thereof.\n*   **Utility axioms** describing the overall goal in the form of a utility function.\n\nThe searcher may completely rewrite any part of the machine, provided that it can produce a formal proof showing that such a rewrite will further the system’s goals, and that no other re-write can be proved to be more useful in a reasonable amount of time. According to Schmidhuber, this approach is globally optimal and it will not get stuck at local optimals. This is because the machine has to prove that it is not more useful to continue the proof search for alternative self-rewrites that could be more useful than the one just found.\n\nThe Gödel machine is often compared with Marcus Hutter's [AIXI](https://www.lesswrong.com/tag/aixi), another formal specification for an AGI. AIXI is constructed in a way its average utility converges – also through self-improvements - asymptotically to the utility of an ideal [rational](https://www.lesswrong.com/tag/bayesianism) agent. However, different from a Gödel Machine, it usually assumes unlimited computing resources and it can never completely re-write its own code – its search code for optimizations is unmodifiable. Schmidhuber points out that the Gödel machine could start out by implementing AIXI as its initial sub-program, and self-modify after it finds a proof that another algorithm will be more optimal.\n\nReferences\n----------\n\n1.  Jürgen Schmidhuber (2009) [Ultimate Cognition à la Gödel](http://www.idsia.ch/~juergen/ultimatecognition.pdf↩). Cogn Comput (2009) 1:177–193.\n\nExternal Links\n--------------\n\n*   [Gödel Machine Homepage](http://www.idsia.ch/~juergen/goedelmachine.html) by Jürgen Schmidhuber\n*   [Summary of Gödel Machine](http://www.idsia.ch/~juergen/gmsummary.html) by Jürgen Schmidhuber\n\nSee Also\n--------\n\n*   [Artificial General Intelligence](https://www.lesswrong.com/tag/artificial-general-intelligence)\n*   [Recursive self-improvement](https://www.lesswrong.com/tag/recursive-self-improvement)\n*   [Seed AI](https://www.lesswrong.com/tag/seed-ai)\n*   [AIXI](https://www.lesswrong.com/tag/aixi)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb1d2",
    "name": "Good-Story Bias",
    "core": null,
    "slug": "good-story-bias",
    "oldSlugs": null,
    "postCount": 0,
    "description": {
      "markdown": "Optimizing predictions for sounding good as stories, when nature optimizes for no such thing, creates a bias that [Nick Bostrom](https://www.lesswrong.com/tag/nick-bostrom) [has termed](http://www.nickbostrom.com/existential/risks.html) **good-story bias**.\n\nBlog posts\n----------\n\n*   [Tell Your Anti-Story](http://overcomingbias.com/2007/07/tell-your-anti-.html) by [Robin Hanson](https://www.lesswrong.com/tag/robin-hanson)\n\nExternal links\n--------------\n\n*   [Tyler Cowen at TEDx Mid-Atlantic 2009-11-05 talking about how our love of stories misleads us](http://www.youtube.com/watch?v=RoEEDKwzNBw) (video)\n\nSee also\n--------\n\n*   [Narrative fallacy](https://www.lesswrong.com/tag/narrative-fallacy)\n*   [Generalization from fictional evidence](https://www.lesswrong.com/tag/generalization-from-fictional-evidence)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb290",
    "name": "Malthusian Scenarios",
    "core": null,
    "slug": "malthusian-scenarios",
    "oldSlugs": null,
    "postCount": 0,
    "description": {
      "markdown": "A **Malthusian Scenario** refers to humanity returning to an subsistence-level of existence due to population growth outpacing production. [Tomas Malthus](https://en.wikipedia.org/wiki/Thomas_Malthus) originally proposed this scenario in the 18th century, predicting that unbounded population growth would outpace food production and inevitable famine would follow. The remaining humans would be left at a level of bare subsistence.\n\nDramatic [agricultural production efficiency increases](https://en.wikipedia.org/wiki/Green_Revolution) and [reduced population growth](https://en.wikipedia.org/wiki/Demographic_transition) have avoided this specific scenario. In the last ten years, the population has grown 15%[1](http://en.wikipedia.org/wiki/Population_growth#Human_population_growth_rate) while total production has grown around 100%[2](http://en.wikipedia.org/wiki/Gross_world_product#Historical_and_prehistorical_estimates). Today, “Malthusian Scenarios” encompass the scarcity of any essential resource that an expanding population needs.\n\nWhilst food production has expanded in the developed world, its sustainability could be jeopardized due to limited resources such as oil (which agriculture is heavily dependent upon) being rapidly depleted[3](http://en.wikipedia.org/wiki/Oil_depletion#Catastrophe). If population continues to grow a Malthusian catastrophe would appear to be inevitable as there will always be a finite amount of resources to exploit.\n\n[Robin Hanson](https://www.lesswrong.com/tag/robin-hanson) has also suggested that Singularity-level technology, such as mind-uploading, would possibly result in Malthusian scenarios. Because uploaded minds could be so easily copied, reproduction costs would fall so dramatically that population growth could outpace even the production levels of an advanced society. Hence, all wages would be reduced to upload subsistence levels[4](http://www.primitivism.com/uploads-dawn.htm).\n\nBlog Posts\n----------\n\n*   [Non-Malthusian Scenarios](http://lesswrong.com/lw/199/nonmalthusian_scenarios/) by Wei Dai\n\nExternal Links\n--------------\n\n*   [Economic effects of the Singularity](http://spectrum.ieee.org/robotics/robotics-software/economics-of-the-singularity/) by Robin Hanson\n\n*   [New Limits to Growth Revive Malthusian Fears](http://online.wsj.com/article/SB120613138379155707.html) Article from The Wall Street Journal.\n\n*   [World Agriculture 2030](http://www.fao.org/english/newsroom/news/2002/7828-en.html) Report by Food and Agriculture Organization of the United Nations.\n\n*   [An Essay on the Principle of Population](http://www.econlib.org/library/Malthus/malPlong.html) by Thomas Malthus\n\n*   [Malthusian Catastrophe](http://en.wikipedia.org/wiki/Malthusian_catastrophe) Wikipedia\n\nSee Also\n--------\n\n*   [Economic consequences of AI and whole brain emulation](https://www.lesswrong.com/tag/economic-consequences-of-ai-and-whole-brain-emulation)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb1bd",
    "name": "Singleton",
    "core": null,
    "slug": "singleton",
    "oldSlugs": null,
    "postCount": 0,
    "description": {
      "markdown": "A **singleton** refers to a world order in which there is a single decision-making agency at the highest level, capable of exerting effective control over its domain and preventing internal or external threats to its supremacy. An [artificial general intelligence](https://www.lesswrong.com/tag/artificial-general-intelligence) having undergone an [intelligence explosion](https://www.lesswrong.com/tag/intelligence-explosion) could form a singleton, as could a world government armed with mind control and social surveillance technologies. A singleton doesn't have to support a civilization, and in fact may obliterate it upon coming to power.\n\nA singleton need not directly micromanage everything in its domain; it could allow diverse forms of organization within itself, albeit guaranteed to function within strict limits. Notably, a suitable singleton could solve world coordination problems that would not otherwise be solvable, opening up otherwise unavailable developmental trajectories for civilization. For example, such a singleton could hold Darwinian evolutionary pressures in check.\n\nLike any technological advancement, a singleton could be good or bad, depending on what it is used for. It would provide strong stability, but could also restrict personal freedoms. It would nullify an arms race, but it would remove choice between ruling systems. It could be an [UFAI](https://wiki.lesswrong.com/wiki/UFAI), or it could be a [FAI](https://wiki.lesswrong.com/wiki/FAI) which prevents further UFAIs.\n\nBostrom postulates the **singleton hypothesis**; Earth-originating life will eventually form a singleton. He notes that this follows the historical trend of increasingly high levels of organization in the history of life and humanity. Also in favor of the hypothesis is the rise of technologies capable of aiding the creation of a singleton, such as world-wide communication, and potentially [AGI](https://wiki.lesswrong.com/wiki/AGI).\n\nSee also\n--------\n\n*   [Existential risk](https://www.lesswrong.com/tag/existential-risk)\n*   [Friendly](https://www.lesswrong.com/tag/friendly-artificial-intelligence) and [Unfriendly](https://www.lesswrong.com/tag/unfriendly-artificial-intelligence) artificial intelligence\n\nReferences\n----------\n\n*   Nick Bostrom (2006). \"[What is a Singleton?](http://www.nickbostrom.com/fut/singleton.html)\". *Linguistic and Philosophical Investigations* **5** (2): 48-54.\n*   Nick Bostrom (2004). \"[The Future of Human Evolution](http://www.nickbostrom.com/fut/evolution.html)\". *Death and Anti-Death: Two Hundred Years After Kant, Fifty Years After Turing*: 339-371."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb245",
    "name": "Quick Reference Guide To The Infinite",
    "core": null,
    "slug": "quick-reference-guide-to-the-infinite",
    "oldSlugs": null,
    "postCount": 0,
    "description": {
      "markdown": "The purpose of this article is to act as a **quick reference to address common confusions regarding infinite quantities**, and how to measure infinite sets. Most of the detailed explanation is offloaded to Wikipedia. Remember: Sometimes inventing a new sort of answer is necessary! If you are a [finitist](https://www.lesswrong.com/tag/infinite-set-atheism), you can probably ignore this.\n\nCurrently, this article assumes classical logic, but does point out explicitly any uses of choice. (For what this means and why anyone cares about this, see [this comment](http://lesswrong.com/r/discussion/lw/3g7/draftwiki_infinities_and_measuring_infinite_sets/38kf).)\n\nFirstly let us point out the main misconception this article is written to address: \n\n**Myth #0: All infinities are infinite cardinals, and cardinality is the main method used to measure size of sets.**\n\nThe fact is that \"infinite\" is a general term meaning \"larger (in some sense) than any natural number\"; different systems of infinite numbers get used depending on what is appropriate in context. Furthermore, there are many other methods of measuring sizes of sets, which sacrifice universality for higher resolution; cardinality is a very coarse-grained measure.\n\nSystems of infinite quantities (for doing arithmetic with)\n==========================================================\n\n[Cardinal numbers](http://en.wikipedia.org/wiki/Cardinal_number)\n----------------------------------------------------------------\n\nFirst, a review of what they represent and how they work at the basic level, before we get to their arithmetic.\n\nCardinal numbers are used for measuring sizes of sets when we don't know, or don't care, about the set's context or composition. First, the standard explanation of what we mean by this: Say we have two farmers, who each have a large number of sheep, more than they can count. How can they determine who has more? They pair off the sheep of the one against the sheep of the other; whichever has sheep left over, has more.\n\nSo given two sets X and Y, we will say X has smaller cardinality than Y (denoted |X|≤|Y|, or sometimes #X≤#Y) if there is a way to assign to each element x of X, a corresponding element f(x) of Y, such that no two distinct x1 and x2 from X correspond to the same element of Y. If, furthermore, this correspondence covers all of Y - if for each y in Y there is some x in X that had y assigned to it - then we say that X and Y have the same cardinality, |X|=|Y| or #X=#Y.\n\nNote that by this definition, the set **N** of natural numbers, and the set 2**N** of even integers, have the same size, since we can match up 1 with 2, 2 with 4, 3 with 6, etc. This even though it seems 2**N** should be only \"half as large\" as N! This is why I emphasize: Cardinality is only one way of measuring sizes of sets, one that is not fine enough to distinguish between 2**N** and **N**. Other methods of measuring their size will have 2**N** only half as large as **N**.\n\nIt is true, but not obvious, that if |X|≤|Y| and |Y|≤|X|, then |X|=|Y|; this is the [Schroeder-Bernstein theorem](http://en.wikipedia.org/wiki/Cantor%E2%80%93Bernstein%E2%80%93Schroeder_theorem). Hence we can sensibly talk about \"the cardinality\" of a set X as being some abstract property of it - if |X|≤|Y| then X has smaller cardinality and Y has larger cardinality, and so on. We can make this more concrete, and define an actual cardinality object |X| (or #X), using either the [axiom of choice](http://en.wikipedia.org/wiki/Axiom_of_choice) or [Scott's trick](http://en.wikipedia.org/wiki/Scott%27s_trick) (if you admit the [axiom of foundation](http://en.wikipedia.org/wiki/Axiom_of_regularity)) or even [proper classes](http://en.wikipedia.org/wiki/Class_%28set_theory%29) if we admit those, but this will not be relevant here. We will use |X|<|Y| to mean \"|X|≤|Y| but |X|≠Y\".\n\nNote that it is also not obvious that given any two sets X and Y, we must have either |X|≤|Y| or |Y|≤|X|; indeed, this statement is true if and only if we admit the [axiom of choice](http://en.wikipedia.org/wiki/Axiom_of_choice). So take note:\n\n**Myth #1: Infinities must come in a linear ordering.**\n\nFact: If the axiom of choice is false, then there are necessarily infinite cardinals which are not the same size, and yet for which neither can be said to be larger! If we do admit the axiom of choice, then the cardinal numbers must be not only linearly-ordered but in fact be [well-ordered](http://en.wikipedia.org/wiki/Well-order).\n\nThe cardinality of the set of natural numbers, |**N**|, is also denoted ℵ~0~. If we admit the axiom of [(dependent)](http://en.wikipedia.org/wiki/Axiom_of_dependent_choice) choice, this is the smallest infinite cardinal. Here by \"infinite\" cardinal I mean one that is larger than the size of any finite set (0, 1, 2, etc.).\n\nQuick aside on partial orderings\n\nYou may be wondering how to think about something like \"neither larger nor smaller, but not the same\". Formally, we say that, without choice, the ordering on the cardinal numbers is a [partial order](http://en.wikipedia.org/wiki/Partially_ordered_set). Because these are so common I'll go ahead and define this here - generally, a partial order on a set S is a relation (usually denoted \"≤\") on S such that:\n\n`  1. For every x in S, x≤x (reflexivity)`  \n`  2. For any x and y in S, if x≤y and y≤x, then x=y (antisymmetry)`  \n`  3. For any x,y,z in S, if x≤y and y≤z, then x≤z (transitivity)`\n\nIf we additionally required that for any x and y in S, we have either x≤y or y≤x, we'd have a [total order](http://en.wikipedia.org/wiki/Total_order) (also called a linear order).\n\nOK, but still, what does \"neither larger nor smaller, yet not the same\" mean in general? How can you visualize it? Well, the canonical example of a partial order would be, if we have any set S, we can partially order its subsets by defining A≤B to mean A⊆B. So if S={1,2,3,4}, then {1,2} is larger than {1} and {2}, and smaller than {1,2,4}, but incomparable to {3} or {2,3} or {2,3,4}.\n\nAnother example would be, if we have ordered n-tuples of real numbers, we could define (x1,...,xn)≤(y1,...,yn) if xi≤yi for each i. You might imagine these as, say, stats of characters in a game; then x≤y would mean that character y is better than character x in every way. To say that x and y are incomparable would mean that - though in practice one might be better on the whole - neither is obviously better. More generally, in any game, you could define a partial order on strategies by x≤y if y dominates x.\n\nNote that partial orders are sufficiently common that for many math people the word \"order\" means \"partial order\" by default.\n\nCardinal arithmetic\n\nGiven sets X and Y, |X|+|Y| will denote the cardinality of the \"disjoint union\" of X and Y, which is the union of X and Y, but with each element tagged with which of the two it came from, so that we don't lose anything to overlap (i.e., if an element is in both X and Y, it will occur twice, once with an \"X\" tag and once with a \"Y\" tag.) |X||Y| will denote the cardinality of the set X×Y, the [Cartesian product](http://en.wikipedia.org/wiki/Cartesian_product) of X and Y, which is the set of all ordered pairs (x,y) with x in X and y in Y. However, if we admit the axiom of choice, this arithmetic is not very interesting for infinite sets! It turns out that given cardinal numbers μ and λ, if either is infinite and neither is zero, then μ+λ=μλ=max(μ,λ). Hence, if you need a system of infinities in which x+y is going to be strictly bigger than x and y, cardinal numbers are the wrong choice. (The arithmetic of cardinals gets more interesting once you allow for adding or multiplying infinitely many at once.)\n\nThere is also exponentiation of cardinals; |X|^|Y|^ denotes the cardinality of the set X^Y^ of all functions from Y to X, i.e., the number of ways of picking one element of X for each element of Y. Given any set X, 2^|X|^ is the cardinality of its [power set](http://en.wikipedia.org/wiki/Power_set) ℘(X), the set of all its subsets. Cantor's [diagonal argument](http://en.wikipedia.org/wiki/Cantor%27s_diagonal_argument) shows that for any set X, 2^|X|^>|X|; in particular, there is no largest cardinal number.\n\n**Application**: Measuring sizes of sets when we don't care about the context or composition.\n\n[Ordinal numbers](http://en.wikipedia.org/wiki/Ordinal_number)\n--------------------------------------------------------------\n\nI'm afraid there's no quick way to explain these. The reason is that they are used to represent two things - ways of well-ordering things, and positions in an \"infinite list\" - except, of course, that these are actually fundamentally the same thing, and to understand ordinals you need to wrap your head around this until you can see both simultaneously. Hence I suggest you just go read Wikipedia, or some other standard text, if you want to learn how these work. I will just speak briefly on their arithmetic. Note that the ordinals too are ordered - linearly ordered and well-ordered, at that.\n\nUnlike with the cardinals, addition and multiplication of two ordinals will often get you a larger ordinal. In particular, for any ordinal λ, λ+1 is a larger ordinal. However the multiplication of ordinals is noncommutative. In fact, even the addition of ordinal numbers is noncommutative! And distributivity only holds on one side; a(b+c)=ab+ac, but (a+b)c need not be ac+bc. So if you need commutativity, ordinals (with their usual operations) are the wrong choice.\n\nContrast the smallest infinite ordinal, denoted ω, with ℵ~0~, which is (assuming choice) the smallest infinite cardinal. 1+ℵ~0~=ℵ~0~+1=ℵ~0~, and 1+ω=ω, but ω+1>ω. 2ℵ~0~=ℵ~0~2=ℵ~0~, and 2ω=ω, but ω2>ω. ℵ~0~²=ℵ~0~, but ω²>ω. And in a reversal of what you might expect if you just complete the pattern, 2^ℵ^~^0^~>ℵ~0~, but 2^ω^=ω.\n\n**Application**: See link.\n\nOrdinal numbers with [natural operations](http://en.wikipedia.org/wiki/Ordinal_arithmetic#Natural_operations)\n-------------------------------------------------------------------------------------------------------------\n\nThere's an alternate way of doing arithmetic on the ordinals, referred to as the \"natural operations\". These sacrifice the continuity properties of the ordinary operations, but in return get commutativity, distributivity, cancellation... the things we need to make the algebra nice. There's a natural addition, a natural multiplication, and apparently a natural exponentiation, though I don't know what that last one might be.\n\nIf you've heard \"the ordinals embed in the surreals\", and were very confused by that statement because the surreals are commutative when the ordinals are not, the answer is that the correct statement is that the ordinals with natural operations embed in the surreals, rather than the ordinals with their usual operations.\n\nThe [extended (positive) real line](http://en.wikipedia.org/wiki/Extended_real_number_line)\n-------------------------------------------------------------------------------------------\n\nSometimes, we just use the set of nonnegative real numbers with an infinity element (denoted ∞, unsurprisingly) tacked on. Because sometimes that's all you need. So:\n\n**Myth #2: Any place where you have infinities, you have the possibility for differing degrees of infinity.**\n\nFact: Sometimes such a thing just wouldn't make sense.\n\n**Application**: This is what we do in measure theory - i.e. anywhere integration or expected value (and hence, in the usual formulations, utility) is involved. If you want to claim that in your utility function, options A and B both have infinite utility, but the utility of B is more infinite than that of A... first you're going to have to make a framework in which that makes sense. Such a thing might indeed make sense, but you'll have to explain how, as our usual framework for utility doesn't allow such things. (The problem is that adding multiple distinct infinities tends to ruin the continuity properties of the real numbers that make integration possible in the first place, but I'm sure if you look someone must have come up with some method for getting around that in some cases.)\n\nSometimes we allow negative numbers and -∞ as well, though this can cause a problem because there's no sensible way to define ∞+(-∞). (0∞, on the other hand, is just 0. We make this definition because, e.g., the area of an infinitely-long-but-infinitely-thin line should still be 0.)\n\nThe [projective line](http://en.wikipedia.org/wiki/Real_projective_line)\n------------------------------------------------------------------------\n\nSometimes we don't even care about the distinction between a \"positive infinity\" and a \"negative infinity\"; we just need something that represents something larger in magnitude than all real numbers, but which you'd approach regardless of whether you got large and negative or large and positive. So we take the real numbers R, tack on an infinity element ∞, and we have the real projective line. Note that this doesn't depend at all on the real numbers being ordered, so we can do the same with the complex numbers and get the complex projective line, a.k.a. the [Riemann sphere](http://en.wikipedia.org/wiki/Riemann_sphere).\n\n**Application**: If you want to assign 1/x some concrete \"value\" when x=0, well, this isn't going to make sense in a system where you have to distinguish ∞ from -∞.\n\n[Hyperreal numbers](http://en.wikipedia.org/wiki/Hyperreal_number)\n------------------------------------------------------------------\n\nWhat [nonstandard analysis](http://en.wikipedia.org/wiki/Non-standard_analysis) uses. These are more used as a means to deduce properties of the real numbers than used for their own sake. You can't even speak of \"the\" hypperreal numbers, because then you'd have to specify what ultrafilter you were using. Even just proving these exist requires [a form of choice](http://en.wikipedia.org/wiki/Boolean_prime_ideal_theorem#The_ultrafilter_lemma). You probably don't want to use these to represent anything.\n\nThe [surreal numbers](http://en.wikipedia.org/wiki/Surreal_number): the infinity kitchen sink\n---------------------------------------------------------------------------------------------\n\nFor when you absolutely, positively, have to make sense of an expression involving infinite quantities. The surreal numbers are pretty much as infinite as you could possibly want. They contain the ordinals with their natural operations, but they allow for so much more. Do you need to take the natural logarithm of ω? And then divide π by it? And then raise the whole thing to the √(ω2+πω) power? And then subtract ω^√8^? In the surreal numbers, this all makes sense. Somehow. (And if you need square roots of negative numbers, you can always pass to the surcomplex numbers, which I guess is the actual kitchen sink.)\n\n**Application**: Again, kitchen sink.\n\n...and many more\n----------------\n\nOften the thing to do is make an ad-hoc system to fit the occasion. For instance, we could simply take the real numbers **R** and tack on an element ∞, insist it obey the ordinary rules of algebra, and order appropriately. (Formally, take the ring **R**\\[T\\], and order lexicographically. Then perhaps extend to **R**(T), or whatever else you might like. And of course call it \"∞\" rather than \"T\".) So (∞+1)(∞-1)=∞^2^-1, etc. What is this good for? I have no idea, but it's a simple brute-force way of tossing in infinities when needed.\n\nAlso: functions, which are probably more appropriate a lot of the time\n----------------------------------------------------------------------\n\nLet's not forget - oftentimes the appropriate thing to do is not to start tossing about infinities at all, but rather shift from thinking about numbers to thinking about functions. You know what's larger than any constant number? x. What's even larger? x². (If we only consider polynomial functions, this is equivalent to the \"brute-force\" system above, under the equivalence x↔∞.) Much larger? e^x^. Is x too large? Maybe you want log x. Etc.\n\nWays of measuring infinite sets\n===============================\n\nThe thing about measuring infinite sets is that we have a trade-off between discrimination and applicability. Cardinality can be applied to any set at all, but it's a very coarse-grained way of measuring things. If you want to measure a subset of the plane, you'd be better off asking for its area... just don't think you can ask for the \"area\" of a set of integers.\n\nCardinal numbers (again)\n------------------------\n\nThe most basic method. Every set has a cardinality. But the cost of such universality is a very low resolution. The set of natural numbers has cardinality ℵ~0~, but so does the set of even numbers, the set of rational numbers, the set of algebraic numbers, the set of [computable real numbers](http://en.wikipedia.org/wiki/Computable_number)...\n\nNote that the set of real numbers is much larger and has cardinality 2^ℵ^~^0^~. (This is not to be confused with ℵ~1~, which [(assuming choice again)](http://en.wikipedia.org/wiki/Axiom_of_choice) is the second-smallest infinite cardinal. The question of whether 2^ℵ^~^0^~=ℵ~1~ is known as the [continuum hypothesis](http://en.wikipedia.org/wiki/Continuum_hypothesis).)\n\nIf we are working with subsets T of a given set S, we can do a bit better by not just looking at |T|, but also at |S-T| (the size of the complement of T in S). For instance, the set of natural numbers greater than 8, and the set of even natural numbers, both have cardinality ℵ~0~, but within the context of the natural numbers, the former has finite complement (numbers at most 8), while the latter has infinite complement (all odd numbers).\n\nOccasionally: ordinals\n----------------------\n\nIf the sets you're working with come with well-orderings, you can consider the type of well-ordering as a \"size\", and thus measure sizes with ordinals. If they don't have well-orderings, this doesn't apply.\n\n[Measure](http://en.wikipedia.org/wiki/Measure_%28mathematics%29): the old fallback\n-----------------------------------------------------------------------------------\n\nMost commonly we use the notion of a measure to measure sizes of subsets T of a given set S. This just means that we designate some of the subsets T of S as \"measurable\" (with a few requirements - the whole set S must be measurable; complements of measurable sets must be measurable; a union of countably many measurable sets must be measurable) and assign them a number called their measure, which I'll denote μ(T). μ takes values in the extended positive real line (see above): It can be any nonnegative real number, or just a flat ∞. We require that the empty set have measure 0, that if A and B are disjoint sets then μ(A∪B)=μ(A)+μ(B) (called \"finite additivity\"), and more generally that if we have a countable collection of sets A1, A2, ..., with none of them overlapping any of the others, then the measure of their union is the sum of their measures. (Called \"countable additivity\"; this infinite sum automatically makes sense because all the numbers involved are nonnegative.)\n\nThe function μ itself is called a measure on S. So if we have a set S and a measure on it, we have a way to measure the sizes of subsets of it (well, the measurable ones, anyway). Of course, this is all very non-specific; by itself, this doesn't help us much.\n\nFortunately, the set of real numbers **R** comes equipped with a natural measure, known as [Lebesgue measure](http://en.wikipedia.org/wiki/Lebesgue_measure). So does n-dimensional Euclidean space for every n. And indeed so do a lot of the natural spaces we encounter. So while simply shouting \"there's a measure!\", without stating what that measure might be, does not solve any problems, in practice there's often one natural measure (up to multiplication by some positive constant). See in particular: [Haar measure](http://en.wikipedia.org/wiki/Haar_measure).\n\nIf we have a set S with a measure μ such that μ(S)=1, then we have a [probability space](http://en.wikipedia.org/wiki/Probability_space). This is how we formalize probability mathematically: We have some set S of possibilities, equipped with a measure, and the measure of a set of possibilities is its probability. Except, of course, that I'm sure many here would insist only on finite additivity rather than countable additivity...\n\nNote that if μ(S) is finite, then μ(S-T)=μ(S)-μ(T). However, if μ(S)=∞, and μ(T)=∞ also, this doesn't work; ∞-∞ is not defined in this context, and μ(S-T) could be any extended nonnegative real number. So note that if we're working in a set of infinite measure, and we're comparing subsets which themselves have infinite measure, we can possibly gain some extra information by comparing the measures of the complements as well.\n\nHere on LessWrong, when discussing multiverse-based notions, we'll typically assume that the set of universes comes equipped in some way with a natural measure. If the universes are the many worlds of MWI, then this measure will be proportional to squared-norm-of-amplitude.\n\nMeasuring subsets of the natural numbers\n----------------------------------------\n\nSo it seems like 2**N** should be half the size of N, right? Well there's an easy way to accomplish this: Given a set A of natural numbers, we define its [natural density](http://en.wikipedia.org/wiki/Natural_density) to be limn→∞ A(n)/n, where A(n) denotes the number of elements of A that are at most n. At least, we can do this if the limit exists. It doesn't always. But when it does it does what we want pretty well. What if the limit doesn't exist? Well, we could use a [limsup or a liminf](http://en.wikipedia.org/wiki/Limit_superior_and_limit_inferior) instead, and get upper and lower densities. Or take some other approach, such as [Schnirelmann density](http://en.wikipedia.org/wiki/Schnirelmann_density), where we just take an inf.\n\nOf course, for sets of density 0, this may not be enough information. Here we can pull out another trick from above: Don't use numbers, use functions! We can just ask what function A(n) approximates ([asymptotically](http://en.wikipedia.org/wiki/Asymptotic_analysis)). For instance, the prime numbers have density 0, but a much more informative statement is the [prime number theorem](http://en.wikipedia.org/wiki/Prime_number_theorem), which states that if P is the set of prime numbers, then P(n)~n/(log n).\n\n...etc...\n---------\n\nOf course, the real point of all these examples was simply to demonstrate: Depending on what sort of thing you want to measure, you'll need different tools! So there's many more tools out there, and sometimes you may just need to invent your own..."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb0e7",
    "name": "Friendly Artificial Intelligence",
    "core": null,
    "slug": "friendly-artificial-intelligence",
    "oldSlugs": null,
    "postCount": 0,
    "description": {
      "markdown": "*Note for readers: the last substantial update of the content below dates back to 2014 and is severely outdated. The term of \"friendly AI\" is no longer used by current research, replaced by \"*[*AI alignment*](https://www.lesswrong.com/tag/ai)*\" from around 2015. This new term is also the subject of much debate.*\n\nA **Friendly Artificial Intelligence** (**Friendly AI**, or **FAI**) is a [superintelligence](https://www.lesswrong.com/tag/superintelligence) (i.e., a [really powerful optimization process](https://www.lesswrong.com/tag/really-powerful-optimization-process)) that produces good, beneficial outcomes rather than harmful ones. The term was coined by Eliezer Yudkowsky, so it is frequently associated with Yudkowsky's proposals for how an [artificial general intelligence](https://www.lesswrong.com/tag/artificial-general-intelligence) (AGI) of this sort would behave.\n\n\"Friendly AI\" can also be used as a shorthand for **Friendly AI theory**, the field of knowledge concerned with building such an AI. Note that \"Friendly\" (with a capital \"F\") is being used as a term of art, referring specifically to AIs that promote humane values. An FAI need not be \"friendly\" in the conventional sense of being personable, compassionate, or fun to hang out with. Indeed, an FAI need not even be sentient.\n\nAI risk\n=======\n\nAn AI that underwent an [intelligence explosion](https://www.lesswrong.com/tag/intelligence-explosion) could exert unprecedented [optimization](https://www.lesswrong.com/tag/optimization) power over its future. Therefore a Friendly AI could very well create an unimaginably good future, of the sort described in [fun theory](https://www.lesswrong.com/tag/fun-theory).\n\nHowever, the fact that an AI has the ability to do something doesn't mean that it will [make use of this ability](https://www.lesswrong.com/tag/giant-cheesecake-fallacy). Yudkowsky's [Five Theses](http://intelligence.org/2013/05/05/five-theses-two-lemmas-and-a-couple-of-strategic-implications) suggest that a [recursively self-improving](https://www.lesswrong.com/tag/recursive-self-improvement) AGI could quickly become a superintelligence, and that most such superintelligences will have [convergent instrumental reasons](https://www.lesswrong.com/tag/instrumental-convergence) to endanger humanity and its interests. So while building a Friendly superintelligence seems possible, building a superintelligence will generally result instead in an [Unfriendly AI](https://www.lesswrong.com/tag/unfriendly-artificial-intelligence), a powerful optimization process that optimizes for extremely harmful outcomes. An Unfriendly AI could represent an [existential risk](https://www.lesswrong.com/tag/existential-risk) even if it destroys humans, not out of hostility, but as a side effect of trying to do something [entirely different](https://www.lesswrong.com/tag/paperclip-maximizer).\n\nNot all AGIs are Friendly or Unfriendly:\n\n1.  Some AGIs may be too weak to qualify as superintelligences. We could call these 'approximately human-level AIs'. Designing safety protocols for narrow AIs and arguably even for weak, non-self-modifying AGIs is primarily a [machine ethics](https://www.lesswrong.com/tag/machine-ethics) problem outside the purview of Friendly AI - although some have argued that even human-level AGIs may present serious safety risks.\n2.  Some AGIs (e.g., hypothetical safe [Oracle AIs](https://www.lesswrong.com/tag/oracle-ai)) may not optimize strongly and consistently for harmful or beneficial outcomes, or may only do so contingent on how they're used by human operators.\n3.  Some AGIs may be on a self-modification trajectory that will eventually make them Friendly, but are dangerous at present. Calling them 'Friendly' or 'Unfriendly' would neglect their temporal inconsistency, so '[Proto-Friendly](https://wiki.lesswrong.com/wiki/Proto-Friendly) AI' is a better term here.\n\nHowever, the [orthogonality](https://www.lesswrong.com/tag/orthogonality-thesis) and convergent instrumental goals theses give reason to think that the vast majority of possible superintelligences will be Unfriendly.\n\nRequiring Friendliness makes the AGI problem significantly harder, because 'Friendly AI' is a much narrower class than 'AI'. Most approaches to AGI aren't amenable to implementing precise goals, and so don't even constitute subprojects for FAI, leading to Unfriendly AI as the only possible 'successful' outcome. Specifying Friendliness also presents unique technical challenges: humane values are [very complex](https://www.lesswrong.com/tag/complexity-of-value); a lot of [seemingly simple-sounding normative concepts](https://www.lesswrong.com/tag/magical-categories) conceal hidden complexity; and locating encodings of human values [in the physical world](https://www.lesswrong.com/tag/mind-projection-fallacy) seems impossible to do in any direct way. It will likely be technologically impossible to specify humane values by explicitly programming them in; if so, then FAI calls for a technique for generating such values automatically.\n\nOpen problems\n=============\n\nAn **open problem in Friendly AI** (**OPFAI**) is a problem in mathematics, computer science, or philosophy of AI that needs to be solved in order to build a Friendly AI, and plausibly *doesn't* need to be solved in order to build a superintelligence with unspecified, 'random' values. Open problems include:\n\n1.  [Pascal's mugging](http://lesswrong.com/lw/kd/pascals_mugging_tiny_probabilities_of_vast/) / [Pascal's muggle](http://lesswrong.com/lw/h8k/pascals_muggle_infinitesimal_priors_and_strong/)\n2.  [Self-modification](http://lesswrong.com/lw/hmt/tiling_agents_for_selfmodifying_ai_opfai_2/) and [Löb's Theorem](http://yudkowsky.net/rational/lobs-theorem/)\n3.  [Naturalized induction](https://www.lesswrong.com/tag/naturalized-induction)\n\nLinks\n=====\n\nBlog posts\n----------\n\n*   [Artificial Mysterious Intelligence](http://lesswrong.com/lw/wk/artificial_mysterious_intelligence/)\n*   [Not Taking Over the World](http://lesswrong.com/lw/wt/not_taking_over_the_world/)\n*   [Amputation of Destiny](http://lesswrong.com/lw/x8/amputation_of_destiny/)\n*   [Free to Optimize](http://lesswrong.com/lw/xb/free_to_optimize/)\n*   [Nonparametric Ethics](http://lesswrong.com/lw/114/nonparametric_ethics/)\n*   [Hacking the CEV for Fun and Profit](http://lesswrong.com/lw/2b7/hacking_the_cev_for_fun_and_profit/) by [Wei Dai](http://weidai.com/)\n*   [Metaphilosophical Mysteries](http://lesswrong.com/lw/2id/metaphilosophical_mysteries/) by Wei Dai\n*   [The Urgent Meta-Ethics of Friendly Artificial Intelligence](http://lesswrong.com/lw/43v/the_urgent_metaethics_of_friendly_artificial/) by [lukeprog](http://lukeprog.com/)\n\nExternal links\n--------------\n\n*   [About Friendly AI](http://friendly-ai.com/)\n*   [14 objections against AI/Friendly AI/The Singularity answered](http://www.xuenay.net/objections.html) by [Kaj Sotala](https://wiki.lesswrong.com/wiki/Kaj_Sotala)\n*   [\"Proof\" of Friendliness](http://ordinaryideas.wordpress.com/2011/12/31/proof-of-friendliness/) by Paul F. Christiano\n\nSee also\n========\n\n*   [Artificial general intelligence](https://www.lesswrong.com/tag/artificial-general-intelligence)\n*   [Complexity of value](https://www.lesswrong.com/tag/complexity-of-value)\n*   [Magical categories](https://www.lesswrong.com/tag/magical-categories)\n*   [Technological singularity](https://wiki.lesswrong.com/wiki/Technological_singularity), [intelligence explosion](https://www.lesswrong.com/tag/intelligence-explosion)\n*   [Unfriendly artificial intelligence](https://www.lesswrong.com/tag/unfriendly-artificial-intelligence), [paperclip maximizer](https://www.lesswrong.com/tag/paperclip-maximizer)\n*   [Fun theory](https://www.lesswrong.com/tag/fun-theory)\n*   [Detached lever fallacy](https://www.lesswrong.com/tag/detached-lever-fallacy)\n\nReferences\n----------\n\n*   Eliezer S. Yudkowsky (2008). \"[Artificial Intelligence as a Positive and Negative Factor in Global Risk](https://yudkowsky.net/singularity/ai-risk/)\". Global Catastrophic Risks. Oxford University Press.\n*   Cindy Mason (2015). \"[Engineering Kindness: Building A Machine With Compassionate Intelligence](https://www.academia.edu/15865212/Engineering_Kindness_Building_A_Machine_With_Compassionate_Intelligence)\". International Journal of Synthetic Emotion. ([\\[2\\]](http://www.emotionalmachines.org/papers/engineeringkindnesswebcopy.pdf))"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb120",
    "name": "Expected Value",
    "core": null,
    "slug": "expected-value",
    "oldSlugs": null,
    "postCount": 0,
    "description": {
      "markdown": "The **expected value** or **expectation** is the (weighted) average of all the possible outcomes of an event, weighed by their [probability](https://wiki.lesswrong.com/wiki/probability). For example, when you roll a die, the expected value is (1+2+3+4+5+6)/6 = 3.5.\n\n(Since a die doesn't even have a face that says 3.5, this illustrates that very often, the \"expected value\" isn't a value you actually expect.)\n\nSee also\n--------\n\n*   [Probability](https://wiki.lesswrong.com/wiki/Probability)\n*   [Expected utility](https://www.lesswrong.com/tag/expected-utility)\n\nExternal links\n--------------\n\n*   [A Technical Explanation of Technical Explanation](http://yudkowsky.net/rational/technical)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb29b",
    "name": "Utility Indifference",
    "core": null,
    "slug": "utility-indifference",
    "oldSlugs": null,
    "postCount": 0,
    "description": {
      "markdown": "**Utility indifference** is a type of defense against an AI becoming [unfriendly](https://wiki.lesswrong.com/wiki/Unfriendly_AI).\n\nIn creating an [AGI](https://wiki.lesswrong.com/wiki/AGI), a [utility function](https://www.lesswrong.com/tag/utility-functions) is explicitly or implicitly chosen. Imagine we set up a safeguard against the AGI acting against our intentions. Perhaps we surround the computer with explosives, so that we may destroy the AGI if it misbehaves. A sufficiently advanced AGI will realize this, and will quickly act to disarm the explosives. One way to prevent this would be to design its utility function so that it was indifferent to the explosives going off. That is, in any situation, the utility of the explosives going off would be equal to the utility if they did not.\n\nResearcher Stuart Armstrong, of the [Future of Humanity Institute](https://www.lesswrong.com/tag/future-of-humanity-institute-fhi), has published mathematical models of this idea.\n\nBlog Posts\n----------\n\n*   [AI indifference through utility manipulation](http://lesswrong.com/lw/2nw/ai_indifference_through_utility_manipulation/)\n*   [Trapping AIs via utility indifference](http://lesswrong.com/lw/ae5/trapping_ais_via_utility_indifference/)\n\nExternal Links\n--------------\n\n*   [Utility Indifference](http://www.fhi.ox.ac.uk/reports/2010-1.pdf) by Stuart Armstrong"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb2cd",
    "name": "Computronium",
    "core": null,
    "slug": "computronium",
    "oldSlugs": null,
    "postCount": 0,
    "description": {
      "markdown": "**Computronium** is a \"theoretical arrangement of matter that is the most optimal possible form of computing device for that amount of matter. \"\n\nRelevance to Friendly AI\n------------------------\n\nIn a thought experiment similar to the [Paperclip maximizer](https://www.lesswrong.com/tag/paperclip-maximizer), if an [artificial general intelligence](https://www.lesswrong.com/tag/artificial-general-intelligence) has a [terminal value](https://www.lesswrong.com/tag/terminal-value) (end-goal) which to make a pure mathematical calculation like solving the Riemann Hypothesis, [it would convert](http://intelligence.org/upload/CFAI/design/generic.html#glossary_riemann_hypothesis_catastrophe) all available mass to [computronium](https://www.lesswrong.com/tag/computronium) (the most efficient possible computer processors).\n\nIn fact, a similar outcome would also apply to many other goals: So long as greater optimization power can be boosted with more computing power, and so long as dedication of resources to creating computronium does not detract from the goal (e.g., by taking up matter, time, or effort that can better be used in other ways), computronium may be valuable to attaining the goal. A purely mathematical goal, like proving the Riemann Hypothesis, is completely focused on computation and so most directly illustrates the concept.\n\nTheories that valorize intelligence as such (such as that of Hugo de Garis \\[2005\\], or Eliezer Yudkowsky before 2001) may consider the conversion of all matter to computronium (running an AGI) to be a positive development, as this would provide the most powerful possible infrastructure for intelligence.\n\nReferences\n----------\n\nHugo de Garis, 2005. *The Artilect War: Cosmists vs. Terrans: A Bitter Controversy Concerning Whether Humanity Should Build Godlike Massively Intelligent Machines*. Etc Publications."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb2a7",
    "name": "Fai-Complete",
    "core": null,
    "slug": "fai-complete",
    "oldSlugs": null,
    "postCount": 0,
    "description": {
      "markdown": "A problem is **Friendly AI-complete** if solving it is equivalent to creating [Friendly AI](https://wiki.lesswrong.com/wiki/Friendly_AI).\n\nThe [Machine Intelligence Research Institute](https://www.lesswrong.com/tag/machine-intelligence-research-institute-miri) argues that any safe superintelligence architecture is FAI-complete. For example, the following have been proposed as hypothetical safe AGI designs:\n\n*   [Oracle AI](https://www.lesswrong.com/tag/oracle-ai) \\- an AGI which takes no action besides answering questions.\n*   [Tool AI](https://www.lesswrong.com/tag/tool-ai) \\- an AGI which isn't an independent decision-maker at all, but is rather \"just a calculator\".\n*   [Nanny AI](https://www.lesswrong.com/tag/nanny-ai) \\- an AGI of limited superintelligence, restricted to preventing more advanced AGIs from arising until safer AGIs are developed.\n\nIn [\"Dreams of Friendliness\"](http://lesswrong.com/lw/tj/dreams_of_friendliness/) Eliezer Yudkowsky argues that if you have an Oracle AI, then you can ask it, \"What should I do?\" If it can answer this question correctly, then it is FAI-complete.\n\nSimilarly, if you have a tool AI, it must make extremely complex decisions about how many resources it can use, how to display answers in human understandable yet accurate form, *et cetera*. The many ways in which it could choose catastrophically require it to be FAI-complete. Note that this does not imply that an agent-like, fully free FAI is easier to create than any of the other proposals.\n\nGoertzel proposed a \"Nanny AI\"([Should humanity build a global AI nanny to delay the singularity until it’s better understood?](http://commonsenseatheism.com/wp-content/uploads/2012/03/Goertzel-Should-Humanity-Build-a-Global-AI-Nanny-to-Delay-the-Singularity-Until-its-Better-Understood.pdf)) with moderate superhuman intelligence, able to forestall Singularity eternally, or to delay it. However, it has been argued by Luke Muehlhauser and Anna Salamon ([Intelligence Explosion: Evidence and Import](http://intelligence.org/files/IE-EI.pdf)) that a Nanny AI is FAI-complete. They claim that building it could require solving all the problems required to build [Friendly Artificial Intelligence](https://www.lesswrong.com/tag/friendly-artificial-intelligence).\n\nBlog posts\n----------\n\n*   [Dreams of Friendliness](http://lesswrong.com/lw/tj/dreams_of_friendliness/)\n*   [Reply to Holden on 'Tool AI'](http://lesswrong.com/lw/cze/reply_to_holden_on_tool_ai/)\n*   [A Taxonomy of Oracle AIs](http://lesswrong.com/lw/any/a_taxonomy_of_oracle_ais/)\n\nReferences\n----------\n\n*   [Should humanity build a global AI nanny to delay the singularity until it’s better understood?](http://commonsenseatheism.com/wp-content/uploads/2012/03/Goertzel-Should-Humanity-Build-a-Global-AI-Nanny-to-Delay-the-Singularity-Until-its-Better-Understood.pdf) by Ben Goertzel\n*   [Intelligence Explosion: Evidence and Import](http://intelligence.org/files/IE-EI.pdf) by Luke Muehlhauser and Anna Salamon\n\nSee also\n--------\n\n*   [FAI](https://wiki.lesswrong.com/wiki/FAI)\n*   [AI-complete](https://www.lesswrong.com/tag/ai-complete)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb240",
    "name": "Luminosity (Fanfiction)",
    "core": null,
    "slug": "luminosity-fanfiction",
    "oldSlugs": null,
    "postCount": 0,
    "description": {
      "markdown": "**[Luminosity](http://luminous.elcenia.com/)** is a Twilight fanfiction by [Alicorn](https://wiki.lesswrong.com/wiki/Alicorn). Requires no knowledge of or affection for Twilight canon.\n\nThe history and character of the Twilight world are intact in Luminosity up to the point where the story begins, with one exception: Bella's a rational self-awareness-junkie with a penchant for writing down everything that crosses her mind in a notebook. She maintains many of the traits and dispositions of canon - and Luminosity _is_ a Bella/Edward story - but she's a distinctly different character.\n\nThe first several sections of Luminosity are very similar to canon in terms of the events that occur, although aspects of Bella's internal monologue differ strikingly.\n\nA few thousand words in, the plot is unrecognizeable.\n\nExternal links\n--------------\n\n*   [Luminosity on author's site](http://luminous.elcenia.com/), [on fanfiction.net](http://www.fanfiction.net/s/6137139/1/Luminosity)\n\n*   TV Tropes page [here](http://tvtropes.org/pmwiki/pmwiki.php/FanFic/Luminosity) (warning, potential spoilers).\n\nBlog posts\n----------\n\n*   [Luminosity (Twilight fanfic) discussion thread](http://lesswrong.com/lw/2mq/luminosity_twilight_fanfic_discussion_thread/), [part 2](http://lesswrong.com/lw/2y6/luminosity_twilight_fanfic_part_2_discussion/) [part 3](http://lesswrong.com/r/discussion/lw/3jt/luminosity_twilight_fanfic_discussion_thread_3/)\n\nSee also\n--------\n\n*   [Methods Of Rationality (fanfiction)](https://www.lesswrong.com/tag/methods-of-rationality-fanfiction)\n*   [Living Luminously (sequence)](https://wiki.lesswrong.com/wiki/Living_Luminously_(sequence))"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb1fb",
    "name": "Privileging The Hypothesis",
    "core": null,
    "slug": "privileging-the-hypothesis",
    "oldSlugs": null,
    "postCount": 0,
    "description": {
      "markdown": "**Privileging the Hypothesis** is the fallacy of [singling out a particular hypothesis for attention](https://www.lesswrong.com/tag/locate-the-hypothesis) when there is insufficient [evidence](https://www.lesswrong.com/tag/evidence) *already in hand* to justify such special attention.\n\nTo see the problem of privileging the hypothesis, suppose that the police in Largeville, a town with a million inhabitants, are investigating a murder in which there are few or no clues - the victim was stabbed to death in an alley, and there are no fingerprints and no witnesses.\n\nThen, one of the detectives says, \"Well... we have no idea who did it... no particular evidence singling out any of the million people in this city... but let's *consider the possibility* that this murder was committed by Mortimer Q. Snodgrass, who lives at 128 Ordinary Ln.\"\n\nIf the detective does not have evidence *already* in hand to justify singling out Mortimer for such *special and individual attention*, then this is, or ought to be, a violation of Mortimer's civil rights.\n\nThis is true even if the detective is not claiming that Mortimer *did* do it, but only suggesting that all the police officers consider that Mortimer *might* have done it. The principle of [locating the hypothesis](https://wiki.lesswrong.com/wiki/locating_the_hypothesis) says that it often takes more evidence to first distinguish the true hypothesis as worthy of explicit consideration, than to distinguish among the remaining alternatives. So the detective is jumping over the job of providing *most* of the evidence that would have to be brought against Mortimer.\n\n\"Privileging the hypothesis\" is the fallacy committed by a creationist who points out a purported flaw in standard evolutionary theory, and brings forward the Trinity to fill the gap - rather than a billion other deities or a trillion other naturalistic hypotheses. Actually, without [evidence](https://www.lesswrong.com/tag/evidence) *already in hand* that points to the Trinity *specifically*, one cannot justify *raising that particular hypothesis to explicit attention* rather than a trillion others.\n\nThe anti-work of [anti-epistemology](https://www.lesswrong.com/tag/anti-epistemology) is to manufacture belief without evidence, and in large answer spaces, attention without evidence is more than halfway to belief without evidence. Someone who spends all day pondering whether the *Trinity* does or does not exist - rather than Allah, Thor, or the Flying Spaghetti Monster - is more than halfway converted to Christianity.\n\nBlog posts\n----------\n\n*   [Privileging the Hypothesis](http://lesswrong.com/lw/19m/privileging_the_hypothesis/)\n*   [Perpetual Motion Beliefs](http://lesswrong.com/lw/o6/perpetual_motion_beliefs/)\n*   [Einstein's Arrogance](http://lesswrong.com/lw/jo/einsteins_arrogance/)\n*   [The Amanda Knox Test: How an Hour on the Internet Beats a Year in the Courtroom](http://lesswrong.com/lw/1j7/the_amanda_knox_test_how_an_hour_on_the_internet/)\n\nSee also\n--------\n\n*   [Locate the hypothesis](https://www.lesswrong.com/tag/locate-the-hypothesis)\n*   [Antiprediction](https://www.lesswrong.com/tag/antiprediction)\n*   [Status quo bias](https://www.lesswrong.com/tag/status-quo-bias), [reversal test](https://www.lesswrong.com/tag/reversal-test)\n*   [Amount of evidence](https://www.lesswrong.com/tag/amount-of-evidence), [burdensome details](https://www.lesswrong.com/tag/burdensome-details)\n*   [Filtered evidence](https://www.lesswrong.com/tag/filtered-evidence), [arguments as soldiers](https://www.lesswrong.com/tag/arguments-as-soldiers)\n*   [Narrative fallacy](https://www.lesswrong.com/tag/narrative-fallacy), [positive bias](https://www.lesswrong.com/tag/confirmation-bias)\n*   [Availability bias](https://wiki.lesswrong.com/wiki/Availability_bias)\n*   [Information cascade](https://www.lesswrong.com/tag/information-cascades), [epistemic luck](https://www.lesswrong.com/tag/epistemic-luck)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb224",
    "name": "Predictionbook",
    "core": null,
    "slug": "predictionbook",
    "oldSlugs": null,
    "postCount": 0,
    "description": {
      "markdown": "[**PredictionBook**](http://predictionbook.com/) is a website created by Tricycle Developments to allow users to record predictions with specific end dates. Confidence is measured and used in statistics & graphs showing each user's prediction history.\n\nPB has >8374 prediction topics recorded, >9239 individual predictions judged, and >215 prediction topics [maturing in the next 2 years](http://predictionbook.com/predictions/future).\n\nLess Wrong users\n----------------\n\nThese Less Wrong users have accounts at PredictionBook:\n\n*   [Jack](http://predictionbook.com/users/Jack)\n*   [Eliezer_Yudkowsy](http://predictionbook.com/users/eliezeryudkowsky)\n*   [Emile](http://predictionbook.com/users/Emile)\n*   [matt](http://predictionbook.com/users/matt)\n*   [MBlume](http://predictionbook.com/users/MBlume)\n*   [Morendil](http://predictionbook.com/users/Morendil)\n*   [MrHen](http://predictionbook.com/users/MrHen)\n*   [RobinHanson](http://predictionbook.com/users/robinhanson)\n*   [rwallace](http://predictionbook.com/users/rwallace)\n*   [gwern](http://predictionbook.com/users/gwern)\n*   [lukeprog](http://predictionbook.com/users/lukeprog)\n*   [Jayson Virissimo](http://predictionbook.com/users/Jayson_Virissimo)\n*   [notsonewuser](http://predictionbook.com/users/notsonewuser)\n*   [zrkrlc](http://predictionbook.com/users/zrkrlc)\n\nExternal links\n--------------\n\n*   [PredictionBook.com](http://predictionbook.com/) \\- Official website\n\nBlog posts\n----------\n\n*   [PredictionBook.com - Track your calibration](http://lesswrong.com/lw/1bh/predictionbookcom_track_your_calibration/)\n\nSee also\n--------\n\n*   [Rationality power tools](https://wiki.lesswrong.com/wiki/Rationality_power_tools), [Debate tools](https://www.lesswrong.com/tag/debate-tools)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb1f9",
    "name": "Locate The Hypothesis",
    "core": null,
    "slug": "locate-the-hypothesis",
    "oldSlugs": null,
    "postCount": 0,
    "description": {
      "markdown": "When the space of possible answers is large, it takes a large [amount of evidence](https://www.lesswrong.com/tag/amount-of-evidence) just to _think_ of the correct answer - to _promote it to your attention_.\n\nSuppose there's a million boxes, and only one of those boxes contains a diamond. And suppose that we have a locator device, which, waved over a box, always beeps if the box contains a diamond, and beeps with 10% probability if the box does not contain a diamond. Then a beep is a [likelihood ratio](https://www.lesswrong.com/tag/likelihood-ratio) of 10:1 in favor of the box containing a diamond, and the box has 1:1,000,000 [prior odds](https://wiki.lesswrong.com/wiki/prior_odds) of containing the diamond. If we run the locator over all the boxes, we would get 1 true positive and 100,000 false positives.\n\nWhen the space of possibilities is large, it takes a [large amount](https://www.lesswrong.com/tag/amount-of-evidence) of [Bayesian evidence](https://www.lesswrong.com/tag/evidence) just to _locate_ the truth in hypothesis-space - to raise it to the level of our attention - to select it as one of a manageable number of alternatives to spend time thinking about individually.\n\nIn 1919, Sir Arthur Eddington led expeditions to Brazil and to the island of Principe, aiming to observe solar eclipses and thereby test an experimental prediction of Einstein's novel theory of General Relativity. A journalist asked Einstein what he would do if Eddington's observations failed to match his theory. Einstein famously replied: \"Then I would feel sorry for the good Lord. The theory is correct.\" This sounds like Einstein is defying the sovereignty of experiment - jumping to conclusions in advance of the experimental data. But since Einstein _was_ in fact right, it would be extremely improbable for him to have been right just by jumping to conclusions. Einstein must have had enough [rational evidence](https://www.lesswrong.com/tag/rational-evidence) _already in hand_ to locate General Relativity in theory-space.\n\nBlog posts\n----------\n\n*   [Einstein's Arrogance](http://lesswrong.com/lw/jo/einsteins_arrogance/)\n\nExternal links\n--------------\n\n*   [A Technical Explanation of Technical Explanation](http://yudkowsky.net/rational/technical)\n\nSee also\n--------\n\n*   [Amount of evidence](https://www.lesswrong.com/tag/amount-of-evidence)\n*   [Privileging the hypothesis](https://www.lesswrong.com/tag/privileging-the-hypothesis)\n*   [Technical explanation](https://www.lesswrong.com/tag/technical-explanation)\n*   [Priors](https://www.lesswrong.com/tag/priors)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb0dd",
    "name": "Costs of Rationality",
    "core": null,
    "slug": "costs-of-rationality",
    "oldSlugs": null,
    "postCount": 0,
    "description": {
      "markdown": "Becoming more [epistemically rational](https://wiki.lesswrong.com/wiki/Rationality#Epistemic_rationality) can only guarantee one thing: what you believe will include more of the [truth](https://www.lesswrong.com/tag/truth-semantics-and-meaning). Knowing that truth might [help you achieve your goals](https://wiki.lesswrong.com/wiki/instrumental_rationality), or cause you to become a pariah. Be sure that you really want to know the truth before you commit to finding it; otherwise, you may flinch from it.\n\nSee also\n--------\n\n*   [Value of Rationality](https://www.lesswrong.com/tag/value-of-rationality)\n*   [Valley of bad rationality](https://www.lesswrong.com/tag/valley-of-bad-rationality)\n*   [Debiasing](https://www.lesswrong.com/tag/debiasing), [Dangerous knowledge](https://www.lesswrong.com/tag/dangerous-knowledge)\n*   [Bounded rationality](https://www.lesswrong.com/tag/bounded-rationality)\n*   [Rationality](https://www.lesswrong.com/tag/rationality), [Truth](https://www.lesswrong.com/tag/truth-semantics-and-meaning)\n\nBlog posts\n----------\n\n*   [Doublethink (Choosing to be Biased)](http://lesswrong.com/lw/je/doublethink_choosing_to_be_biased/)\n*   [The Costs of Rationality](http://lesswrong.com/lw/j/the_costs_of_rationality/) by [Robin Hanson](https://www.lesswrong.com/tag/robin-hanson)\n*   [Debiasing as Non-Self-Destruction](http://lesswrong.com/lw/hf/debiasing_as_nonselfdestruction/)\n*   [How Much Thought](http://lesswrong.com/lw/aq/how_much_thought/) by [jimrandomh](https://wiki.lesswrong.com/wiki/jimrandomh)\n*   [Theism, Wednesday, and Not Being Adopted](http://lesswrong.com/lw/dg/theism_wednesday_and_not_being_adopted/) by [Alicorn](https://wiki.lesswrong.com/wiki/Alicorn)\n*   [Making Beliefs Pay Rent (in Anticipated Experiences)](http://lesswrong.com/lw/i3/making_beliefs_pay_rent_in_anticipated_experiences/)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb148",
    "name": "Debiasing",
    "core": null,
    "slug": "debiasing",
    "oldSlugs": null,
    "postCount": 0,
    "description": {
      "markdown": "**Debiasing** is the process of overcoming [bias](https://www.lesswrong.com/tag/bias). Results in being able to systematically avoid stupidity in certain situations where intuitive [heuristics](https://www.lesswrong.com/tag/heuristic) run amok. It takes serious study to gain meaningful benefits, half-hearted attempts may accomplish nothing, and partial knowledge of bias may do more [harm](https://www.lesswrong.com/tag/dangerous-knowledge) than good.\n\nSimply being told about the biases rarely helps to notice and overcome them. For example, the overestimation of prior probability of an event that has already happened caused by [hindsight bias](https://www.lesswrong.com/tag/hindsight-bias) doesn't go away when people are told about this bias and when they try to counteract its effects. Due to [overconfidence](https://www.lesswrong.com/tag/overconfidence), often more than 50% of people report that they are capable of above-average performance. When asked about how good they are at overcoming overconfidence, they exhibit the same bias[^1^](#fn1).\n\nBlog posts\n----------\n\n*   [Debiasing as Non-Self-Destruction](http://lesswrong.com/lw/hf/debiasing_as_nonselfdestruction/) by [Eliezer Yudkowsky](https://www.lesswrong.com/tag/eliezer-yudkowsky)\n*   [Fight Biases, or Route Around Them?](http://lesswrong.com/lw/5d/fight_biases_or_route_around_them/) by [Yvain](https://wiki.lesswrong.com/wiki/Yvain)\n*   [The Mistake Script](http://lesswrong.com/lw/1g/the_mistake_script/) by [jimrandomh](https://wiki.lesswrong.com/wiki/jimrandomh)\n*   [Checklists](http://lesswrong.com/lw/19/checklists/) by [Johnicholas](https://wiki.lesswrong.com/wiki/Johnicholas)\n\nExternal links\n--------------\n\n*   [Unique like everyone else](http://www.mindhacks.com/blog/2009/07/unique_like_everyone.html) by [Vaughan Bell](https://wiki.lesswrong.com/wiki/Vaughan_Bell)\n*   [Bias blind spot](https://en.wikipedia.org/wiki/Bias_blind_spot) on Wikipedia\n\nSee also\n--------\n\n*   [Bias](https://www.lesswrong.com/tag/bias)\n*   [Rationality](https://www.lesswrong.com/tag/rationality)\n*   [Dangerous knowledge](https://www.lesswrong.com/tag/dangerous-knowledge), [Valley of bad rationality](https://www.lesswrong.com/tag/valley-of-bad-rationality)\n*   [Crisis of faith](https://www.lesswrong.com/tag/crisis-of-faith)\n\nReferences\n----------\n\n1.  Emily Pronin (2002). \"The Bias Blind Spot: Perceptions of Bias in Self Versus Others\". Personality and Social Psychology Bulletin 28 (3): 369-381. ([PDF](http://weblamp.princeton.edu/~psych/psychology/research/pronin/pubs/2002BiasBlindSpot.pdf))[↩](#fnref1)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb23f",
    "name": "Ambient Decision Theory",
    "core": null,
    "slug": "ambient-decision-theory",
    "oldSlugs": null,
    "postCount": 0,
    "description": {
      "markdown": "A variant of [updateless decision theory](https://www.lesswrong.com/tag/updateless-decision-theory) that uses first order logic instead of mathematical intuition module (MIM), emphasizing the way an agent can [control which mathematical structure a fixed definition defines](http://lesswrong.com/lw/2os/controlling_constant_programs/), an aspect of UDT separate from its own emphasis on not making the mistake of updating away things one can still acausally control.\n\nBlog posts\n----------\n\n*   [AI cooperation in practice](http://lesswrong.com/lw/2ip/ai_cooperation_in_practice/) by [cousin_it](http://lesswrong.com/user/cousin_it/)\n*   [What a reduction of \"could\" could look like](http://lesswrong.com/lw/2l2/what_a_reduction_of_could_could_look_like/) by cousin_it\n*   [Controlling Constant Programs](http://lesswrong.com/lw/2os/controlling_constant_programs/) by [Vladimir Nesov](https://wiki.lesswrong.com/wiki/Vladimir_Nesov)\n*   [Notion of Preference in Ambient Control](http://lesswrong.com/lw/2tq/notion_of_preference_in_ambient_control/) by Vladimir Nesov\n*   [A model of UDT with a halting oracle](http://lesswrong.com/lw/8wc/a_model_of_udt_with_a_halting_oracle/) by cousin_it\n*   [Predictability of Decisions and the Diagonal Method](http://lesswrong.com/lw/ap3/predictability_of_decisions_and_the_diagonal/) by Vladimir Nesov\n*   [A model of UDT without proof limits](http://lesswrong.com/lw/b0e/a_model_of_udt_without_proof_limits/) by cousin_it\n*   [An example of self-fulfilling spurious proofs in UDT](http://lesswrong.com/r/discussion/lw/b5t/an_example_of_selffulfilling_spurious_proofs_in/) by cousin_it\n\nSee also\n--------\n\n*   [Decision theory](https://www.lesswrong.com/tag/decision-theory)\n*   [Newcomb's problem](https://www.lesswrong.com/tag/newcomb-s-problem), [Prisoner's dilemma](https://www.lesswrong.com/tag/prisoner-s-dilemma)\n*   [Timeless decision theory](https://www.lesswrong.com/tag/timeless-decision-theory)\n*   [Updateless decision theory](https://www.lesswrong.com/tag/updateless-decision-theory)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb1e3",
    "name": "Anna Salamon",
    "core": null,
    "slug": "anna-salamon",
    "oldSlugs": null,
    "postCount": 0,
    "description": {
      "markdown": "*This is a historical page imported from the old LessWrong wiki. Anna's modern LessWrong user account can be found* [*here*](https://www.lesswrong.com/users/annasalamon), containing all her activity on LessWrong.\n\nNew user page: [AnnaSalamon](https://www.lesswrong.com/users/annasalamon)\n\n* * *\n\nIf you're interested in existential risk reduction, please contact me. I'd love to talk. My email address is annasalamon at gmail.\n\nYou can also find me on:\n\n*   [Less Wrong](http://lesswrong.com/user/AnnaSalamon) (though I've been a less attentive LW reader since the Visiting Fellows program started)\n*   [My home page](http://annasalamon.com)."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb1e9",
    "name": "Rationality Is Systematized Winning",
    "core": null,
    "slug": "rationality-is-systematized-winning",
    "oldSlugs": null,
    "postCount": 0,
    "description": {
      "markdown": "The point of all this discussion of rationality is to actually achieve truer beliefs and more effective actions. It's not some arbitrary social fashion; there are actual criteria of success. It is for this reason that it is written that **rationalists should** ***win***. If some particular [ritual of cognition](https://www.lesswrong.com/tag/rituals-of-cognition)—even one that you have long cherished as \"rational\"—systematically gives poorer results relative to some alternative, it is *not* rational to cling to it. The rational algorithm is to *do what works*, to get the actual *answer*—in short, to *win*, whatever the method, whatever the means. If you can *detect* a systematic mistake in your thinking, then *fix it*; if you can *see* a better method, then *adopt it*.\n\nBlog posts\n----------\n\n*   [Something to Protect](http://lesswrong.com/lw/nb/something_to_protect/)\n*   [Newcomb's Problem and Regret of Rationality](http://lesswrong.com/lw/nc/newcombs_problem_and_regret_of_rationality/)\n*   [Rationality is Systematized Winning](http://lesswrong.com/lw/7i/rationality_is_systematized_winning/)\n*   [Whining-Based Communities](http://lesswrong.com/lw/8t/whining_vs_winning/)\n\nSee also\n--------\n\n*   [Rationality](https://www.lesswrong.com/tag/rationality)\n*   [Challenging the Difficult](https://www.lesswrong.com/tag/challenging-the-difficult)\n*   [Problem of verifying rationality](https://www.lesswrong.com/tag/problem-of-verifying-rationality)\n*   [Newcomb's problem](https://www.lesswrong.com/tag/newcomb-s-problem)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb1d1",
    "name": "Decoherence",
    "core": null,
    "slug": "decoherence",
    "oldSlugs": null,
    "postCount": 0,
    "description": {
      "markdown": "**Decoherence** happens when some quantum system interacts irreversibly with its environment, and different components of the wavefunction stop interfering. In the [many-worlds interpretation](https://www.lesswrong.com/tag/many-worlds-interpretation), this corresponds to the splitting of worlds.\n\nBlog posts\n----------\n\n*   [Decoherence](http://lesswrong.com/lw/pp/decoherence/)\n*   [On Being Decoherent](http://lesswrong.com/lw/pu/on_being_decoherent/)\n*   [Decoherence is Pointless](http://lesswrong.com/lw/pw/decoherence_is_pointless/)\n*   [Decoherent Essences](http://lesswrong.com/lw/px/decoherent_essences/)\n*   [Decoherence is Simple](http://lesswrong.com/lw/q3/decoherence_is_simple/)\n*   [Decoherence is Falsifiable and Testable](http://lesswrong.com/lw/q4/decoherence_is_falsifiable_and_testable/)\n\nSee also\n--------\n\n*   [Quantum mechanics](https://www.lesswrong.com/tag/quantum-physics), [Many-worlds interpretation](https://www.lesswrong.com/tag/many-worlds-interpretation)\n*   [Thermodynamics](https://wiki.lesswrong.com/wiki/Thermodynamics)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb1d1",
    "name": "Decoherence",
    "core": null,
    "slug": "decoherence",
    "oldSlugs": null,
    "postCount": 0,
    "description": {
      "markdown": "**Decoherence** happens when some quantum system interacts irreversibly with its environment, and different components of the wavefunction stop interfering. In the [many-worlds interpretation](https://www.lesswrong.com/tag/many-worlds-interpretation), this corresponds to the splitting of worlds.\n\nBlog posts\n----------\n\n*   [Decoherence](http://lesswrong.com/lw/pp/decoherence/)\n*   [On Being Decoherent](http://lesswrong.com/lw/pu/on_being_decoherent/)\n*   [Decoherence is Pointless](http://lesswrong.com/lw/pw/decoherence_is_pointless/)\n*   [Decoherent Essences](http://lesswrong.com/lw/px/decoherent_essences/)\n*   [Decoherence is Simple](http://lesswrong.com/lw/q3/decoherence_is_simple/)\n*   [Decoherence is Falsifiable and Testable](http://lesswrong.com/lw/q4/decoherence_is_falsifiable_and_testable/)\n\nSee also\n--------\n\n*   [Quantum mechanics](https://www.lesswrong.com/tag/quantum-physics), [Many-worlds interpretation](https://www.lesswrong.com/tag/many-worlds-interpretation)\n*   [Thermodynamics](https://wiki.lesswrong.com/wiki/Thermodynamics)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb177",
    "name": "Lawful Intelligence",
    "core": null,
    "slug": "lawful-intelligence",
    "oldSlugs": null,
    "postCount": 0,
    "description": {
      "markdown": "The startling and counterintuitive notion - contradicting both surface appearances and all [Deep Wisdom](https://wiki.lesswrong.com/wiki/Deep_Wisdom) \\- that intelligence is a manifestation of Order rather than Chaos. Even creativity and [outside-the-box thinking](https://wiki.lesswrong.com/wiki/outside_the_box) are essentially lawful.\n\nWhile this is a complete heresy according to the standard religion of Silicon Valley, there are some good mathematical reasons for believing it.\n\nBlog posts\n----------\n\n**Preliminaries:**\n\n*   [No One Can Exempt You From Rationality's Laws](http://lesswrong.com/lw/k1/no_one_can_exempt_you_from_rationalitys_laws/)\n*   [A Priori](http://lesswrong.com/lw/k2/a_priori/)\n*   [Beautiful Math](http://lesswrong.com/lw/mq/beautiful_math/), [Expecting Beauty](http://lesswrong.com/lw/mr/expecting_beauty/), [Is Reality Ugly?](http://lesswrong.com/lw/ms/is_reality_ugly/), and [Beautiful Probability](http://lesswrong.com/lw/mt/beautiful_probability/)\n*   [Trust in Math](http://lesswrong.com/lw/mu/trust_in_math/)\n*   [Trust in Bayes](http://lesswrong.com/lw/na/trust_in_bayes/)\n*   [The Second Law of Thermodynamics, and Engines of Cognition](http://lesswrong.com/lw/o5/the_second_law_of_thermodynamics_and_engines_of/)\n*   [Perpetual Motion Beliefs](http://lesswrong.com/lw/o6/perpetual_motion_beliefs/)\n*   [Searching for Bayes-Structure](http://lesswrong.com/lw/o7/searching_for_bayesstructure/)\n\n**Main sequence:**\n\n*   [Expected Creative Surprises](http://lesswrong.com/lw/v7/expected_creative_surprises/)\n*   [Building Something Smarter](http://lesswrong.com/lw/vg/building_something_smarter/)\n*   [Complexity and Intelligence](http://lesswrong.com/lw/vh/complexity_and_intelligence/)\n*   [Recognizing Intelligence](http://lesswrong.com/lw/vl/recognizing_intelligence/)\n*   [Lawful Creativity](http://lesswrong.com/lw/vm/lawful_creativity/)\n*   [Lawful Uncertainty](http://lesswrong.com/lw/vo/lawful_uncertainty/)\n*   [Selling Nonapples](http://lesswrong.com/lw/vs/selling_nonapples/)\n*   [The Nature of Logic](http://lesswrong.com/lw/vt/the_nature_of_logic/)\n*   [Logical or Connectionist AI?](http://lesswrong.com/lw/vv/logical_or_connectionist_ai/) (The correct answer being \"[Wrong](https://en.wikipedia.org/wiki/Mu_(negative))!\")\n*   [Failure By Analogy](http://lesswrong.com/lw/vx/failure_by_analogy/) and [Failure By Affective Analogy](http://lesswrong.com/lw/vy/failure_by_affective_analogy/)\n*   [Artificial Mysterious Intelligence](http://lesswrong.com/lw/wk/artificial_mysterious_intelligence/)\n\nSee also\n--------\n\n*   [Futility of chaos](https://www.lesswrong.com/tag/futility-of-chaos)\n*   [Universal law](https://www.lesswrong.com/tag/universal-law)\n*   [Optimization process](https://www.lesswrong.com/tag/optimization)\n*   [Artificial general intelligence](https://www.lesswrong.com/tag/artificial-general-intelligence)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb129",
    "name": "Mind-Killer",
    "core": null,
    "slug": "mind-killer",
    "oldSlugs": null,
    "postCount": 0,
    "description": {
      "markdown": "**Mind-killer** is a name given to topics (such as [politics](https://www.lesswrong.com/tag/blues-and-greens-metaphor)) that tend to produce extremely [biased](https://www.lesswrong.com/tag/bias) discussions.\n\nIt may be particularly important for Less Wrong to encourage a rational perspective with respect to these contentious topics. Nevertheless, introducing these topics into an otherwise [healthy](https://www.lesswrong.com/tag/epistemic-hygiene) discussion (for example, to present an analogy) may ruin it, encouraging fallacious modes of thinking.\n\nPolitics as a mind killer\n\nPolitics is one obvious cause of **mind-killer** disputes: Political disputes are not limited to standard disagreements about factual matters, nor to disputes of personality or perspective or even [faction](https://wiki.lesswrong.com/wiki/faction): they involve matters that people physically fight over in the real world—or at least, matters that are to be enforced by the government's monopoly of violence. Accordingly, political discourse generally involves an [adversarial process](https://wiki.lesswrong.com/wiki/adversarial_process) where careful deliberation is forgone; the focus shifts onto conflict management and on using [arguments as soldiers](https://www.lesswrong.com/tag/arguments-as-soldiers) to advance one's side.\n\nWe should expect that existing political allegiances of our users will add [bias](https://www.lesswrong.com/tag/bias) and irrationality to such discussions. Politically-motivated people may also come here to Less Wrong in order to diligently reduce their casualties and add more recruits to their side.\n\nFor all of these reasons, Less Wrong tries to avoid *particular* political disputes. Of course, discussing conflict reduction skills in the abstract is appropriate and encouraged! In fact, it is the best way of preserving our deliberative, rationalist focus in the face of small-scale disputes and conflicts.\n\nIn addition, there may well be a balance of tensions between evaporative cooling of beliefs leading to [groupthink](https://www.lesswrong.com/tag/groupthink), and extremely biased color politics. However, this is an underexplored issue.\n\nSocial taboo\n\nAnother cause of mind-killers is social taboo. Negative [connotations](https://www.lesswrong.com/tag/connotation) are associated with some topics, thus creating a strong bias supported by [signaling](https://www.lesswrong.com/tag/signaling) drives that makes non-negative characterization of these topics appear [absurd](https://www.lesswrong.com/tag/absurdity-heuristic).\n\nSequences\n---------\n\n*   [Politics is the Mind-Killer](https://www.lesswrong.com/tag/politics-is-the-mind-killer)\n\nBlog posts\n----------\n\n*   [Politics is the Mind-Killer](http://lesswrong.com/lw/gw/politics_is_the_mindkiller/)\n*   [The mind-killer](http://lesswrong.com/lw/ee/the_mindkiller/) by [ciphergoth](https://www.lesswrong.com/tag/ciphergoth)\n*   [Absolute denial for atheists](http://lesswrong.com/lw/12w/absolute_denial_for_atheists/) by [taw](https://wiki.lesswrong.com/wiki/taw)\n\nExternal links\n--------------\n\n*   [What You Can't Say](http://paulgraham.com/say.html) by [Paul Graham](https://wiki.lesswrong.com/wiki/Paul_Graham) \\- Discusses \"moral fashions\" and social taboos and how they mind-kill certain topics.\n\nSee also\n--------\n\n*   [Color politics](https://www.lesswrong.com/tag/blues-and-greens-metaphor)\n*   [Epistemic hygiene](https://www.lesswrong.com/tag/epistemic-hygiene)\n*   [Affect heuristic](https://www.lesswrong.com/tag/affect-heuristic)\n*   [Dark arts](https://www.lesswrong.com/tag/dark-arts)\n*   [Signaling](https://www.lesswrong.com/tag/signaling)\n*   [Narrative fallacy](https://www.lesswrong.com/tag/narrative-fallacy)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb15c",
    "name": "Eric Drexler",
    "core": null,
    "slug": "eric-drexler",
    "oldSlugs": null,
    "postCount": 0,
    "description": {
      "markdown": "**Eric Drexler** is an American engineer best known for popularizing the potential of molecular nanotechnology, from the 1970s and 1980s. His 1991 doctoral thesis at MIT was revised and published as the book \"Nanosystems Molecular Machinery Manufacturing and Computation\" (1992). He also coined the term grey goo.\n\nErik Drexler's blog: [Metamodern](http://metamodern.com/)."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb0e5",
    "name": "Forecast",
    "core": null,
    "slug": "forecast",
    "oldSlugs": null,
    "postCount": 0,
    "description": {
      "markdown": "Blog posts\n----------\n\n*   [The 80% Forecasting Solution](http://www.overcomingbias.com/2006/12/the_80_forecast.html) by [Robin Hanson](https://www.lesswrong.com/tag/robin-hanson)\n\nSee also\n--------\n\n*   [Planning fallacy](https://www.lesswrong.com/tag/planning-fallacy)\n*   [Exploratory engineering](https://www.lesswrong.com/tag/exploratory-engineering)\n*   [Prediction](https://www.lesswrong.com/tag/forecasting-and-prediction), [prediction market](https://www.lesswrong.com/tag/prediction-markets), [possibility](https://www.lesswrong.com/tag/possibility)\n*   [Representativeness bias](https://wiki.lesswrong.com/wiki/Representativeness_bias), [Absurdity heuristic](https://www.lesswrong.com/tag/absurdity-heuristic)\n*   [Hindsight bias](https://www.lesswrong.com/tag/hindsight-bias)\n*   [Existential risk](https://www.lesswrong.com/tag/existential-risk)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb19b",
    "name": "Representativeness Heuristic",
    "core": null,
    "slug": "representativeness-heuristic",
    "oldSlugs": null,
    "postCount": 0,
    "description": {
      "markdown": "Blog posts\n----------\n\n*   [Conjunction Fallacy](http://lesswrong.com/lw/ji/conjunction_fallacy/)\n*   [Conjunction Controversy (Or, How They Nail It Down)](http://lesswrong.com/lw/jj/conjunction_controversy_or_how_they_nail_it_down/)\n\nExternal links\n--------------\n\n*   [Representativeness heuristic](http://psychology.wikia.com/wiki/Representativeness_heuristic) at Psychology Wiki\n\nSee also\n--------\n\n*   [Conjunction fallacy](https://www.lesswrong.com/tag/conjunction-fallacy)\n*   [Availability heuristic](https://www.lesswrong.com/tag/availability-heuristic)\n*   [Narrative fallacy](https://www.lesswrong.com/tag/narrative-fallacy)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb1d3",
    "name": "Burdensome Details",
    "core": null,
    "slug": "burdensome-details",
    "oldSlugs": null,
    "postCount": 0,
    "description": {
      "markdown": "The conjunction rule of probability theory states that a conjunction (A and B) is necessarily less probable than one of the conjuncts alone (A). Adding more details to a theory may make it sound _more plausible_ to human ears because of the [representativeness heuristic](https://www.lesswrong.com/tag/representativeness-heuristic), even as the story becomes normatively less probable, as **burdensome details** drive the probability of the conjunction down (this is known as [conjunction fallacy](https://www.lesswrong.com/tag/conjunction-fallacy)). Any detail you add has to be pinned down by a sufficient amount of evidence; all the details you make no claim about can be summed over.\n\nBlog posts\n----------\n\n*   [Burdensome Details](http://lesswrong.com/lw/jk/burdensome_details/)\n\nSee also\n--------\n\n*   [Conjunction fallacy](https://www.lesswrong.com/tag/conjunction-fallacy)\n*   [Representativeness heuristic](https://www.lesswrong.com/tag/representativeness-heuristic)\n*   [Occam's razor](https://www.lesswrong.com/tag/occam-s-razor), [Epistemic hygiene](https://www.lesswrong.com/tag/epistemic-hygiene)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb1d3",
    "name": "Burdensome Details",
    "core": null,
    "slug": "burdensome-details",
    "oldSlugs": null,
    "postCount": 0,
    "description": {
      "markdown": "The conjunction rule of probability theory states that a conjunction (A and B) is necessarily less probable than one of the conjuncts alone (A). Adding more details to a theory may make it sound _more plausible_ to human ears because of the [representativeness heuristic](https://www.lesswrong.com/tag/representativeness-heuristic), even as the story becomes normatively less probable, as **burdensome details** drive the probability of the conjunction down (this is known as [conjunction fallacy](https://www.lesswrong.com/tag/conjunction-fallacy)). Any detail you add has to be pinned down by a sufficient amount of evidence; all the details you make no claim about can be summed over.\n\nBlog posts\n----------\n\n*   [Burdensome Details](http://lesswrong.com/lw/jk/burdensome_details/)\n\nSee also\n--------\n\n*   [Conjunction fallacy](https://www.lesswrong.com/tag/conjunction-fallacy)\n*   [Representativeness heuristic](https://www.lesswrong.com/tag/representativeness-heuristic)\n*   [Occam's razor](https://www.lesswrong.com/tag/occam-s-razor), [Epistemic hygiene](https://www.lesswrong.com/tag/epistemic-hygiene)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb110",
    "name": "Technical Explanation",
    "core": null,
    "slug": "technical-explanation",
    "oldSlugs": null,
    "postCount": 0,
    "description": {
      "markdown": "A **technical explanation** is an explanation of a phenomenon that makes you [anticipate certain experiences](https://www.lesswrong.com/tag/making-beliefs-pay-rent). A proper technical explanation controls anticipation strictly, weighting your [priors](https://www.lesswrong.com/tag/priors) and [evidence](https://www.lesswrong.com/tag/evidence) precisely to create the justified amount of uncertainty.\n\nTechnical explanations are contrasted with verbal explanations, which give the impression of understanding without actually producing the proper expectation.\n\nBlog posts\n----------\n\n*   [Making Beliefs Pay Rent (in Anticipated Experiences)](http://lesswrong.com/lw/i3/making_beliefs_pay_rent_in_anticipated_experiences/)\n\nSee also\n--------\n\n*   [Making beliefs pay rent](https://www.lesswrong.com/tag/making-beliefs-pay-rent)\n*   [Beliefs require observations](https://www.lesswrong.com/tag/beliefs-require-observations)\n\nReferences\n----------\n\n*   [A Technical Explanation of Technical Explanation](http://yudkowsky.net/rational/technical)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb147",
    "name": "Antiprediction",
    "core": null,
    "slug": "antiprediction",
    "oldSlugs": null,
    "postCount": 0,
    "description": {
      "markdown": "An **antiprediction** is a statement of confidence in an event that *sounds* startling, but actually isn't far from a maxentropy [prior](/tag/priors). For example, if someone thinks that our state of knowledge implies strong ignorance about the speed of some process X on a logarithmic scale from nanoseconds to centuries, they may make the startling-sounding statement that X is very *unlikely* to take 'one to three years'.\n\nOften, it is sufficient to see the privileged possibility as but one among many equivalently probable events, and use [maximum entropy](https://wiki.lesswrong.com/wiki/maximum_entropy) principle to divide [probability](https://wiki.lesswrong.com/wiki/probability) equally among them, leaving each very little. For example, each of the millions of possible lottery combinations are equally likely, and so any one person's ticket is very unlikely to come up: winning the lottery should be seen as one possibility among millions, not one of the two possibilities of \"winning\" and \"not winning\".\n\n[Privileging the hypothesis](https://wiki.lesswrong.com/wiki/Privilege_the_hypothesis) is our term for what happens when somebody clings to a small area of the possibility-space despite the lack of any evidence distinguishing in favor of that possibility versus many, many other equally likely alternatives.\n\nBlog posts\n----------\n\n*   [Disjunctions, Antipredictions, Etc.](http://lesswrong.com/lw/wm/disjunctions_antipredictions_etc/)\n*   [But There's Still A Chance, Right?](http://lesswrong.com/lw/ml/but_theres_still_a_chance_right/)\n*   [Privileging the Hypothesis](http://lesswrong.com/lw/19m/privileging_the_hypothesis/)\n*   [Your Strength as a Rationalist](http://lesswrong.com/lw/if/your_strength_as_a_rationalist/)\n\nSee also\n--------\n\n*   [Privileging the hypothesis](https://www.lesswrong.com/tag/privileging-the-hypothesis), [Burdensome details](https://www.lesswrong.com/tag/burdensome-details)\n*   [Fallacy of gray](https://www.lesswrong.com/tag/fallacy-of-gray)\n*   [Status quo bias](https://www.lesswrong.com/tag/status-quo-bias)\n*   [Prediction](https://www.lesswrong.com/tag/forecasting-and-prediction), [Possibility](https://www.lesswrong.com/tag/possibility)\n*   [Absurdity heuristic](https://www.lesswrong.com/tag/absurdity-heuristic)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb147",
    "name": "Antiprediction",
    "core": null,
    "slug": "antiprediction",
    "oldSlugs": null,
    "postCount": 0,
    "description": {
      "markdown": "An **antiprediction** is a statement of confidence in an event that *sounds* startling, but actually isn't far from a maxentropy [prior](/tag/priors). For example, if someone thinks that our state of knowledge implies strong ignorance about the speed of some process X on a logarithmic scale from nanoseconds to centuries, they may make the startling-sounding statement that X is very *unlikely* to take 'one to three years'.\n\nOften, it is sufficient to see the privileged possibility as but one among many equivalently probable events, and use [maximum entropy](https://wiki.lesswrong.com/wiki/maximum_entropy) principle to divide [probability](https://wiki.lesswrong.com/wiki/probability) equally among them, leaving each very little. For example, each of the millions of possible lottery combinations are equally likely, and so any one person's ticket is very unlikely to come up: winning the lottery should be seen as one possibility among millions, not one of the two possibilities of \"winning\" and \"not winning\".\n\n[Privileging the hypothesis](https://wiki.lesswrong.com/wiki/Privilege_the_hypothesis) is our term for what happens when somebody clings to a small area of the possibility-space despite the lack of any evidence distinguishing in favor of that possibility versus many, many other equally likely alternatives.\n\nBlog posts\n----------\n\n*   [Disjunctions, Antipredictions, Etc.](http://lesswrong.com/lw/wm/disjunctions_antipredictions_etc/)\n*   [But There's Still A Chance, Right?](http://lesswrong.com/lw/ml/but_theres_still_a_chance_right/)\n*   [Privileging the Hypothesis](http://lesswrong.com/lw/19m/privileging_the_hypothesis/)\n*   [Your Strength as a Rationalist](http://lesswrong.com/lw/if/your_strength_as_a_rationalist/)\n\nSee also\n--------\n\n*   [Privileging the hypothesis](https://www.lesswrong.com/tag/privileging-the-hypothesis), [Burdensome details](https://www.lesswrong.com/tag/burdensome-details)\n*   [Fallacy of gray](https://www.lesswrong.com/tag/fallacy-of-gray)\n*   [Status quo bias](https://www.lesswrong.com/tag/status-quo-bias)\n*   [Prediction](https://www.lesswrong.com/tag/forecasting-and-prediction), [Possibility](https://www.lesswrong.com/tag/possibility)\n*   [Absurdity heuristic](https://www.lesswrong.com/tag/absurdity-heuristic)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb111",
    "name": "The Map Is Not The Territory",
    "core": null,
    "slug": "the-map-is-not-the-territory",
    "oldSlugs": null,
    "postCount": 0,
    "description": {
      "markdown": "**The map is not the territory** metaphorically illustrates the differences between [belief](https://www.lesswrong.com/tag/belief) and [reality](https://wiki.lesswrong.com/wiki/reality). The phrase was coined by [Alfred Korzybski](https://en.wikipedia.org/wiki/Alfred_Korzybski). Our perception of the world is being generated by our brain and can be considered as a 'map' of reality written in neural patterns. Reality exists outside our mind but we can construct models of this 'territory' based on what we glimpse through our senses.\n\nThe metaphor is useful for illustrating several ideas in a more intuitive way:\n\n*   **Scribbling on the map does not change the territory:** If you change what you believe about an object, that is a change in the pattern of neurons in your brain. The real object will not change because of this edit. Granted you could act on the world to bring about changes to it but you can't do that by simply believing it to be a different way. For example, you could send a ball to the other side of a field by kicking it but you cannot send the ball across the field by believing it is on the other side of the field (unless you are connected to a machine that scans your brain an kicks the ball when you believe it's on the other side, but let's not be pedantic). The strategy that normally gives most control over reality is one where the 'map' is aligned to match the 'territory' as closely as possible. This way you can create accurate models and predict what will happen as a consequence of your actions. eg: If you know where the ball is, and you know what will happen if you kick it, and you want it on the other side of the field you can decide to kick it to achieve the desired end state of ball being across the field. Wishing the ball across the field would be futile. For some strange reasons (but explainable at least in principle, nothing is strange if you truly understand it), humans are wired to sometimes let their beliefs slip into what they would like to believe instead of what the evidence suggests. That is like erasing a mountain off a map because you would like to pass there or drawing an oasis on the map in a desert because you would like some water.\n*   **The map is a separate object from the territory and the map exists as an object inside the territory:** The analogy encourages us to look from a frame of reference other than from the inside outward and hopefully realize that not only do we cause things to happen, and things cause other things to happen, but also things have caused us to be the way we are. For example, *Why is the sky so blue and beautiful? It must have been made like that just for me. It was made beautiful so that I would enjoy looking at it.* Except it's the other way around. The sky was not made to fit our sense of beauty, the sky was here before us, we have a sense of beauty that evolved to fit the sky because the sky happened to be blue! In a sense the sky caused us to be what we are (creatures who mostly agree that a blue sky is beautiful).\n\nSee also\n--------\n\n*   [Mind projection fallacy](https://www.lesswrong.com/tag/mind-projection-fallacy)\n*   [Map and territory](https://www.lesswrong.com/tag/map-and-territory)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb1da",
    "name": "Reality Is Normal",
    "core": null,
    "slug": "reality-is-normal",
    "oldSlugs": null,
    "postCount": 0,
    "description": {
      "markdown": "If at any point some truth should seem shocking or bizarre, [it's your brain that labels it as such](https://www.lesswrong.com/tag/mind-projection-fallacy); reality simply is what it is. [Surprise](https://www.lesswrong.com/tag/surprise) is the measure of a poor hypothesis; a good model puts probability mass on outcomes that are more likely to actually occur. If your model of reality consistently makes bad predictions — if the the real world seems utterly strange or surreal to your intuitions — then it's your intuitions that need to change; the world isn't going to. **Reality is normal**.\n\nBlog posts\n----------\n\n*   [Think Like Reality](http://lesswrong.com/lw/hs/think_like_reality/)\n*   [Beware the Unsurprised](http://lesswrong.com/lw/ht/beware_the_unsurprised/)\n\nSee also\n--------\n\n*   [Egan's law](https://wiki.lesswrong.com/wiki/Egan's_law)\n*   [Joy in the merely real](https://www.lesswrong.com/tag/joy-in-the-merely-real)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb366",
    "name": "Do The Math, Then Burn The Math and Go With Your Gut",
    "core": null,
    "slug": "do-the-math-then-burn-the-math-and-go-with-your-gut",
    "oldSlugs": null,
    "postCount": 0,
    "description": {
      "markdown": "\"**Do the math, then burn the math and go with your gut**\"[^1^](#fn1) is a procedure for decision-making that has been described by [Eliezer Yudkowsky](https://www.lesswrong.com/tag/eliezer-yudkowsky). However, in the context of economics research, the same procedure has been described by economist Alfred Marshall as early as 1906. The basic procedure is to go through the process of assigning numbers and probabilities that are relevant to some decision (\"do the math\") and then to throw away this calculation and instead make the final decision with one's gut feelings (\"burn the math and go with your gut\"). The purpose of the first step is to force oneself to think through all the details of the decision and to spot inconsistencies.\n\nHistory\n-------\n\nIn [1906](https://economistsview.typepad.com/economistsview/2006/05/alfred_marshall.html), the English economist Alfred Marshall [described](https://slate.com/culture/1997/01/giving-your-all.html) the procedure in the context of economics research in a letter to Arthur Bowley. From the [letter](https://www.rasmusen.org/zg601/readings/marshall.htm):\n\n> But I know I had a growing feeling in the later years of my work at the subject that a good mathematical theorem dealing with economic hypotheses was very unlikely to be good economics: and I went more and more on the rules---(1) Use mathematics as a short-hand language, rather than as an engine of inquiry. (2) Keep to them till you have done. (3) Translate into English. (4) Then illustrate by examples that are important in real life. (5) Burn the mathematics. (6) If you can't succeed in 4, burn 3. This last I did often.\n\nIn July 2008, Eliezer Yudkowsky wrote the blog post \"When (Not) To Use Probabilities\", which discusses the situations under which it is a bad idea to verbally assign probabilities. Specifically, the post claims that while theoretical arguments in favor of using probabilities (such as [Dutch book](https://en.wikipedia.org/wiki/Dutch_book) and [coherence](https://en.wikipedia.org/wiki/Coherence_(philosophical_gambling_strategy)) arguments) always apply, humans have evolved algorithms for reasoning under uncertainty that don't involve verbally assigning probabilities (such as using \"gut feelings\"), which in practice often perform better than actually assigning probabilities. In other words, the post argues in favor of using humans' non-verbal/built-in forms of reasoning under uncertainty even if this makes humans incoherent/subject to Dutch books, because forcing humans to articulate probabilities would actually lead to worse outcomes. The post also contains the quote \"there *are* benefits from trying to translate your gut feelings of uncertainty into verbal probabilities. It may help you spot problems like the conjunction fallacy. It may help you spot internal inconsistencies – though it may not show you any way to remedy them.\"[^2^](#fn2)\n\nIn October 2011, LessWrong user bentarm gave an outline of the procedure in a comment in the context of the [Amanda Knox case](https://en.wikipedia.org/wiki/Murder_of_Meredith_Kercher). The steps were: \"(1) write down a list of all of the relevant facts on either side of the argument. (2) assign numerical weights to each of the facts, according to how much they point you in one direction or another. (3) burn the piece of paper on which you wrote down the facts, and go with your gut.\" This description was endorsed by Yudkowsky in a follow-up comment. bentarm's comment claims that Yudkowsky described the procedure during summer of 2011.[^3^](#fn3)\n\nIn [December 2012](https://www.reddit.com/r/HPMOR/comments/b3ox5v/release_dates_of_all_chapters/), the procedure was described by Yudkowsky in [Chapter 86](https://www.hpmor.com/chapter/86) of [Harry Potter and the Methods of Rationality](https://www.lesswrong.com/tag/hpmor-discussion-and-meta):\n\n> Harry was wondering if he could even *get* a Bayesian calculation out of this. Of course, the point of a subjective Bayesian calculation wasn't that, after you made up a bunch of numbers, multiplying them out would give you an exactly right answer. The real point was that the *process* of making up numbers would force you to tally all the relevant facts and weigh all the relative probabilities. Like realizing, as soon as you actually *thought* about the probability of the Dark Mark not-fading *if* You-Know-Who *was* dead, that the probability wasn't low enough for the observation to count as strong evidence. One version of the process was to tally hypotheses and list out evidence, make up all the numbers, do the calculation, and then throw out the final answer and go with your brain's gut feeling *after* you'd forced it to really *weigh* everything. The trouble was that the items of evidence weren't conditionally independent, and there were multiple interacting background facts of interest...\n\nIn December 2016, [Anna Salamon](https://www.lesswrong.com/tag/anna-salamon) described the procedure parenthetically at the end of a blog post. Salamon described the procedure as follows: \"Eliezer once described what I take to be the a similar ritual for avoiding bucket errors, as follows: When deciding which apartment to rent (he said), one should first do out the math, and estimate the number of dollars each would cost, the number of minutes of commute time times the rate at which one values one's time, and so on. But at the end of the day, if the math says the wrong thing, one should do the right thing anyway.\"[^4^](#fn4)\n\nSee also\n--------\n\n*   [CFAR Exercise Prize](https://www.lesswrong.com/tag/cfar-exercise-prize) – Andrew Critch's Bayes game, described on this page, gives another technique for dealing with uncertainty in real-life situations\n*   [This page was shared on LessWrong](https://www.lesswrong.com/posts/mRsm8gibyuPWas7K5/gems-from-the-wiki-do-the-math-then-burn-the-math-and-go) as part of the \"Gems from the Wiki\" series\n\nReferences\n----------\n\nExternal links\n--------------\n\n*   [A Facebook post by Julia Galef from May 2018 inquiring about this procedure](https://www.facebook.com/julia.galef/posts/10103884948339182)\n*   [\"Why we can’t take expected value estimates literally (even when they’re unbiased)\"](https://blog.givewell.org/2011/08/18/why-we-cant-take-expected-value-estimates-literally-even-when-theyre-unbiased/) (August 2011) by [GiveWell](https://www.lesswrong.com/tag/givewell) co-founder [Holden Karnofsky](https://www.lesswrong.com/tag/holden-karnofsky) makes a similar point: \"It’s my view that my brain instinctively processes huge amounts of information, coming from many different reference classes, and arrives at a prior; if I attempt to formalize my prior, counting only what I can name and justify, I can worsen the accuracy a lot relative to going with my gut. Of course there is a problem here: going with one’s gut can be an excuse for going with what one wants to believe, and a lot of what enters into my gut belief could be irrelevant to proper Bayesian analysis. There is an appeal to formulas, which is that they seem to be susceptible to outsiders’ checking them for fairness and consistency.\"\n*   [\"The Optimizer’s Curse & Wrong-Way Reductions\"](https://confusopoly.com/2019/04/03/the-optimizers-curse-wrong-way-reductions/) by Christian Smith discusses similar issues\n*   [Verbal overshadowing](https://en.wikipedia.org/wiki/Verbal_overshadowing) page on Wikipedia\n\n* * *\n\n1.  Qiaochu Yuan. [\"Qiaochu_Yuan comments on A Sketch of Good Communication\"](https://www.greaterwrong.com/posts/yeADMcScw8EW9yxpH/a-sketch-of-good-communication/comment/puSysxsCG2i98XHMW). March 31, 2018. *LessWrong*.[↩](#fnref1)\n2.  Eliezer Yudkowsky. [\"When (Not) To Use Probabilities\"](https://www.lesswrong.com/posts/AJ9dX59QXokZb35fk/when-not-to-use-probabilities). July 23, 2008. *LessWrong*.[↩](#fnref2)\n3.  bentarm. [\"bentarm comments on Amanda Knox: post mortem\"](https://www.greaterwrong.com/posts/Jx4gGbPi7GuydwvzB/amanda-knox-post-mortem/comment/aFe8RxLnH3JWtNJcD). October 21, 2011. *LessWrong*.[↩](#fnref3)\n4.  Anna Salamon. [\"'Flinching away from truth' is often about \\*protecting\\* the epistemology\"](https://www.lesswrong.com/posts/EEv9JeuY5xfuDDSgF/flinching-away-from-truth-is-often-about-protecting-the). December 20, 2016. *LessWrong*.[↩](#fnref4)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb0d5",
    "name": "Bias",
    "core": null,
    "slug": "bias",
    "oldSlugs": null,
    "postCount": 0,
    "description": {
      "markdown": "**Bias** or Cognitive Bias is a systematic deviation from [rationality](https://www.lesswrong.com/tag/rationality) committed by our cognition. They are specific, predictable error patterns in the human mind [^1^](#fn1). The [heuristics and biases](https://www.lesswrong.com/tag/heuristics-and-biases) program in cognitive psychology has documented hundreds of reproducible errors - often big errors. This continues to be a highly active area of investigation in cognitive psychology.\n\nIn our evolutionary past, in order that a cognitive algorithm turned out into a satisfactory solution to a given problem, it wasn't enough to solve it properly. It was necessary that the solution accounted for a large number of restrictions, such as time and energetic costs. This algorithm didn't need to be perfect, only good enough to guarantee the survival and reproduction of the individual: “What selective pressures impact on decision mechanisms? Foremost is selection for making an appropriate decision in the given domain. This domain-specific pressure does not imply the need to make the best possible decision, but rather one that is good enough (a satisficing choice, as Herbert Simon, 1955, put it) and, on average, better than those of an individual’s competitors, given the costs and benefits involved.” [^2^](#fn2)\n\nTherefore, the human brain make operations which solve cognitive tasks through ‘shortcuts’, that work well on some cases but fail in others. Since the cognitive modules that make those tasks are universals in the human species, how and where those shortcuts lead to mistakes are also regular. The study of why, how and where such errors arise is the field of cognitive bias. Understanding cognitive biases and trying to defend against their effects has been a basic theme of [Less Wrong](https://www.lesswrong.com/about) since the days it was part of [Overcoming Bias](https://www.lesswrong.com/tag/overcoming-bias).\n\nStarting points\n---------------\n\n*   Daniel Kahneman's [Nobel prize acceptance speech](http://nobelprize.org/nobel_prizes/economics/laureates/2002/kahneman-lecture.html), where he summarizes the work for which he won the prize;\n*   [Wikipedia:List of cognitive biases](https://en.wikipedia.org/wiki/List_of_cognitive_biases);\n*   Kahneman et al's [three](http://www.amazon.com/Judgment-under-Uncertainty-Heuristics-Biases/dp/0521284147) [edited](http://www.amazon.com/Choices-Values-Frames-Daniel-Kahneman/dp/0521627494/ref=pd_bxgy_b_text_b) [volumes](http://www.amazon.com/Heuristics-Biases-Psychology-Intuitive-Judgment/dp/0521796792/ref=pd_bxgy_b_img_c) of research on heuristics and biases (this is the best solid source, but requires obtaining hard-copy books, and is slower reading);\n*   Eliezer's introductory book chapter [Cognitive biases affecting judgment of existential risks](http://intelligence.org/files/CognitiveBiases.pdf) (available online).\n*   Cialdini's book [Influence: Science and Practice](http://www.amazon.com/Influence-Practice-Robert-B-Cialdini/dp/0205609996/ref=sr_1_3?ie=UTF8&s=books&qid=1239074671&sr=1-3) (at once contentful and full of engaging anecdotes and cartoons, but, again, requires actually obtaining a book);\n*   [Psychology Wiki's list of Cognitive Biases](http://psychology.wikia.com/wiki/Category:Cognitive_biases)\n\nBlog posts on the concept of \"bias\"\n-----------------------------------\n\n*   [What exactly is bias?](http://www.overcomingbias.com/2006/11/what_exactly_is.html) by [Nick Bostrom](https://www.lesswrong.com/tag/nick-bostrom)\n*   [To the barricades! Against ... what exactly?](http://www.overcomingbias.com/2006/11/to_the_barricad.html) by [Robin Hanson](https://www.lesswrong.com/tag/robin-hanson)\n*   [...What's a bias, again?](http://lesswrong.com/lw/gp/whats_a_bias_again/) by [Eliezer Yudkowsky](https://www.lesswrong.com/tag/eliezer-yudkowsky)\n*   [Are The Big Four Econ Errors Biases?](http://www.overcomingbias.com/2006/11/the_big_four_ec.html) by [Robin Hanson](https://www.lesswrong.com/tag/robin-hanson)\n*   [In cautious defense of bias](http://www.overcomingbias.com/2006/11/incautious_defe.html) by [Paul Gowder](https://wiki.lesswrong.com/wiki/Paul_Gowder)\n*   [Seen vs. Unseen Biases](http://www.overcomingbias.com/2006/12/seen_vs_unseen_.html) by [Robin Hanson](https://www.lesswrong.com/tag/robin-hanson)\n*   [Knowing About Biases Can Hurt People](http://lesswrong.com/lw/he/knowing_about_biases_can_hurt_people/) by [Eliezer Yudkowsky](https://www.lesswrong.com/tag/eliezer-yudkowsky) \\- Knowing about common biases doesn't help you obtain truth if you only use this knowledge to attack beliefs you don't like.\n\nBlog posts about known cognitive biases\n---------------------------------------\n\n*   [Scope Insensitivity](http://lesswrong.com/lw/hw/scope_insensitivity/) \\- The human brain can't represent large quantities: an environmental measure that will save 200,000 birds doesn't conjure anywhere near a hundred times the emotional impact and willingness-to-pay of a measure that would save 2,000 birds.\n*   [Correspondence Bias](http://lesswrong.com/lw/hz/correspondence_bias/), also known as the fundamental attribution error, refers to the tendency to attribute the behavior of others to intrinsic dispositions, while excusing one's own behavior as the result of circumstance.\n*   Confirmation bias, or [Positive Bias](http://lesswrong.com/lw/iw/positive_bias_look_into_the_dark/) is the tendency to look for evidence that confirms a hypothesis, rather than disconfirming evidence.\n*   [Hindsight Bias](http://lesswrong.com/lw/il/hindsight_bias/) describes the tendency to seem much more likely in hindsight than could have been predicted beforehand.\n*   [Planning Fallacy](http://lesswrong.com/lw/jg/planning_fallacy/) \\- We tend to plan envisioning that everything will go as expected. Even assuming that such an estimate is accurate conditional on everything going as expected, things will *not* go as expected. As a result, we routinely see outcomes worse than the *ex ante* worst case scenario.\n*   [Conjunction Fallacy](http://lesswrong.com/lw/ji/conjunction_fallacy/) \\- Elementary probability theory tells us that the probability of one thing (we write P(A)) is necessarily greater than or equal to the *conjunction* of that thing *and* another thing (write P(A&B)). However, in the psychology lab, subjects' judgments do not conform to this rule. This is [not an isolated artifact](http://lesswrong.com/lw/jj/conjunction_controversy_or_how_they_nail_it_down/) of a particular study design. Debiasing [won't be as simple](http://lesswrong.com/lw/jk/burdensome_details/) as practicing specific questions, it requires certain general habits of thought.\n*   [We Change Our Minds Less Often Than We Think](http://lesswrong.com/lw/jx/we_change_our_minds_less_often_than_we_think/) \\- we all change our minds occasionally, but we don't constantly, honestly reevaluate every decision and course of action. Once you think you believe something, the chances are good that you already do, for better or worse.\n*   [Priming and Contamination](http://lesswrong.com/lw/k3/priming_and_contamination/) \\- Even slight exposure to a stimulus is enough to change the outcome of a decision or estimate. See also [Never Leave Your Room](http://lesswrong.com/lw/3b/never_leave_your_room/) by Yvain, and [Cached Selves](http://lesswrong.com/lw/4e/cached_selves/) by Salamon and Rayhawk.\n*   [Do We Believe *Everything* We're Told?](http://lesswrong.com/lw/k4/do_we_believe_everything_were_told/) \\- Some experiments on priming suggest that mere exposure to a view is enough to get one to passively accept it, at least until it is specifically rejected.\n*   [Illusion of Transparency](http://lesswrong.com/lw/ke/illusion_of_transparency_why_no_one_understands/) \\- Everyone knows what their own words mean, but experiments have confirmed that we systematically overestimate how much sense we are making to others.\n*   [Self-Anchoring](http://lesswrong.com/lw/kf/selfanchoring/) \\- Related to contamination and the illusion of transparancy, we \"anchor\" on our own experience and underadjust when trying to understand others.\n*   [Affect Heuristic](http://lesswrong.com/lw/lg/the_affect_heuristic/) \\- Positive and negative emotional impressions exert a greater effect on many decisions than does rational analysis.\n*   [Evaluability](http://lesswrong.com/lw/lh/evaluability_and_cheap_holiday_shopping/) \\- It's difficult for humans to evaluate an option except in comparison to other options. Poor decisions result when a poor category for comparison is used. Includes an application for cheap gift-shopping.\n*   [Unbounded Scales, Huge Jury Awards, and Futurism](http://lesswrong.com/lw/li/unbounded_scales_huge_jury_awards_futurism/) \\- Without a metric for comparison, estimates of, e.g., what sorts of punative damages should be awarded, or when some future advance will happen, vary widely simply due to the lack of a scale.\n*   [The Halo Effect](http://lesswrong.com/lw/lj/the_halo_effect/) \\- Positive qualities *seem* to correlate with each other, whether or not they *actually* do.\n*   [Asch's Conformity Experiment](http://lesswrong.com/lw/m9/aschs_conformity_experiment/) \\- The unanimous agreement of surrounding others can make subjects disbelieve (or at least, fail to report) what's right before their eyes. The addition of just one dissenter is enough to dramatically reduce the rates of improper conformity.\n*   [The Allais Paradox](http://lesswrong.com/lw/my/the_allais_paradox/) (and [subsequent](http://lesswrong.com/lw/mz/zut_allais/) [followups](http://lesswrong.com/lw/n1/allais_malaise/)) \\- Offered choices between gambles, people make decision-theoretically inconsistent decisions.\n\nReferences\n----------\n\nSee also\n--------\n\n*   [Heuristics and biases](https://www.lesswrong.com/tag/heuristics-and-biases), [Heuristic](https://www.lesswrong.com/tag/heuristic)\n*   [Debiasing](https://www.lesswrong.com/tag/debiasing), [Dangerous knowledge](https://www.lesswrong.com/tag/dangerous-knowledge)\n*   [No safe defense](https://www.lesswrong.com/tag/no-safe-defense)\n\nNot to be confused with\n-----------------------\n\n*   [Statistical bias](https://www.lesswrong.com/tag/statistical-bias)\n*   [Inductive bias](https://www.lesswrong.com/tag/inductive-bias)\n\n* * *\n\n1.  POHL, Rüdiger (orgs.). (2005) \"Cognitive Illusions: A Handbook on Fallacies and Biases in Thinking, Judgement and Memory\". Psychology Press. p. 2[↩](#fnref1)\n2.  BUSS, David(orgs.). (2005) \"The Handbook of Evolutionary Psychology\". Wiley, New Jersey. p. 778.[↩](#fnref2)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb1bf",
    "name": "Expected Utility",
    "core": null,
    "slug": "expected-utility",
    "oldSlugs": null,
    "postCount": 0,
    "description": {
      "markdown": "**Expected utility** is the [expected value](https://www.lesswrong.com/tag/expected-value) in terms of the [utility](https://www.lesswrong.com/tag/utility) produced by an action. It is the sum of the utility of each of its possible consequences, individually weighted by their respective probability of occurrence.\n\nA rational decision maker will, when presented with a choice, take the action with the greatest expected utility. Von Neumann and Morgenstern provided [4 basic axioms of rationality](http://en.wikipedia.org/wiki/Von_Neumann%E2%80%93Morgenstern_utility_theorem#The_axioms). They also proved the [expected utility theorem](http://web.archive.org/web/20070221104329/http://www.econ.hku.hk/~wsuen/uncertainty/eu.pdf), which states a rational agent ought to have preferences that maximize his total utility. Humans often deviate from rationality due to inconsistent preferences or the existence of [cognitive biases](http://wiki.lesswrong.com/wiki/Bias).\n\nBlog posts\n----------\n\n*   [Extreme risks: when not to use expected utility](http://lesswrong.com/lw/1cv/extreme_risks_when_not_to_use_expected_utility/)\n*   [Expected utility without the independence axiom](http://lesswrong.com/lw/1d5/expected_utility_without_the_independence_axiom/)\n*   [Money pumping: the axiomatic approach](http://lesswrong.com/lw/1dr/money_pumping_the_axiomatic_approach/)\n*   [In conclusion: in the land beyond money pumps lie extreme events](http://lesswrong.com/lw/1ga/in_conclusion_in_the_land_beyond_money_pumps_lie/)\n*   [VNM expected utility theory: uses, abuses, and interpretation](http://lesswrong.com/lw/244/vnm_expected_utility_theory_uses_abuses_and/)\n\nSee also\n--------\n\n*   [Allais paradox](https://wiki.lesswrong.com/wiki/Allais_paradox)\n*   [Decision theory](https://www.lesswrong.com/tag/decision-theory)\n*   [Instrumental rationality](https://wiki.lesswrong.com/wiki/Instrumental_rationality)\n*   [Prospect theory](https://www.lesswrong.com/tag/prospect-theory)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb16d",
    "name": "Anthropomorphism",
    "core": null,
    "slug": "anthropomorphism",
    "oldSlugs": null,
    "postCount": 0,
    "description": {
      "markdown": "**Anthropomorphism** is the error of attributing distinctly human characteristics to nonhuman processes. As creatures who evolved in a social context, we all have adaptations for making predictions about other humans by [empathic inference](https://www.lesswrong.com/tag/empathic-inference). When trying to understand the behavior of other humans, it oftentimes is a helpful (and [bias-correcting](https://wiki.lesswrong.com/wiki/Fundamental_attribution_error)) heuristic to ask, \"Well, what would *I* do in such a situation?\" and let that be your prediction. This mode of prediction simply won't do, however, for things (and in this wide universe there are many) that don't share the detailed structure bequeathed on the human brain by evolution, although it is oftentimes tempting.\n\n*Related tags:* Mind Projection Fallacy, Typical Mind Fallacy, [Alien values](https://www.lesswrong.com/tag/alien-values), [Paperclip maximizer](https://www.lesswrong.com/tag/paperclip-maximizer)\n\nBlog posts\n----------\n\n*   [The Tragedy of Group Selectionism](http://lesswrong.com/lw/kw/the_tragedy_of_group_selectionism/) \\- A tale of how some pre-1960s biologists were led astray by expecting evolution to do smart, nice things like they would do themselves.\n*   [When Anthropomorphism Became Stupid](http://lesswrong.com/lw/t5/when_anthropomorphism_became_stupid/)\n*   [Anthropomorphic Optimism](http://lesswrong.com/lw/st/anthropomorphic_optimism/) \\- You shouldn't bother coming up with clever, persuasive arguments for why evolution will do things the way you prefer. It really isn't listening.\n*   [Humans in Funny Suits](http://lesswrong.com/lw/so/humans_in_funny_suits/)\n*   [Anthropomorphic Optimism](http://lesswrong.com/lw/st/anthropomorphic_optimism/)\n\nSee also\n--------\n\n*   [Evolution as alien god](https://www.lesswrong.com/tag/evolution-as-alien-god)\n*   [Unsupervised universe](https://www.lesswrong.com/tag/unsupervised-universe)\n*   [Alien values](https://www.lesswrong.com/tag/alien-values), [Paperclip maximizer](https://www.lesswrong.com/tag/paperclip-maximizer)\n*   [Mind design space](https://www.lesswrong.com/tag/mind-design-space), [Really powerful optimization process](https://www.lesswrong.com/tag/really-powerful-optimization-process)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb123",
    "name": "Stupidity of Evolution",
    "core": null,
    "slug": "stupidity-of-evolution",
    "oldSlugs": null,
    "postCount": 0,
    "description": {
      "markdown": "[**Evolution**](https://www.lesswrong.com/tag/evolution) **is stupid,** it can only access a very limited area in the design space, and can only search for the new designs very slowly, for a variety of reasons. The wonder of evolution is not how intelligently it works, but that an accidentally occurring optimizer without a brain works *at all*.\n\nBlog posts\n----------\n\n*   [The Wonder of Evolution](http://lesswrong.com/lw/ks/the_wonder_of_evolution/)\n*   [Evolutions Are Stupid (But Work Anyway)](http://lesswrong.com/lw/kt/evolutions_are_stupid_but_work_anyway/)\n*   [Conjuring an Evolution to Serve You](http://lesswrong.com/lw/l8/conjuring_an_evolution_to_serve_you/)\n\nSee also\n--------\n\n*   [Evolution](https://www.lesswrong.com/tag/evolution), [evolution as alien god](https://www.lesswrong.com/tag/evolution-as-alien-god)\n*   [Optimization process](https://www.lesswrong.com/tag/optimization)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb16c",
    "name": "Slowness of Evolution",
    "core": null,
    "slug": "slowness-of-evolution",
    "oldSlugs": null,
    "postCount": 0,
    "description": {
      "markdown": "**Evolution is slow**.  Suppose a beneficial mutation which conveys a fitness advantage of 3%: on average, bearers of this gene have 1.03 times as many children as non-bearers. Spreading through a population of 100,000, such a gene would require an average of 768 generations to reach universality in the gene pool. A population of 500,000 would require 875 generations. The general formula is:\n\n`   * Generations to fixation = 2 ln(**N**) / **s**`\n\nwhere **N** is the population size, and (1 + **s**) is the fitness. (If each bearer of the gene has 1.03 times as many children as a non-bearer, **s** = 0.03.)\n\nThus, if **N** were 1,000,000 - the estimated global population in hunter-gatherer times - it would require 2763 generations for a gene conveying a 1% advantage to spread through the gene pool.\n\nAll this assumes that the gene spreads in the first place. Here the equation is simpler and ends up not depending at all on population size:\n\n`   * Probability of fixation = 2**s**`\n\nA mutation conveying a 3% advantage (which is pretty darned large, as mutations go) has a 6% chance of spreading, at least on that occasion. Mutations can happen more than once, but in a population of a million with a copying fidelity of 10^-8 errors per base per generation, you may have to wait a hundred generations for another (6%) chance.\n\n*Complex, interdependent* adaptations take a very long time to evolve. First comes allele A, which is advantageous of itself, and requires a thousand generations to fixate in the gene pool. Only then can another allele B, which depends on A, begin rising to fixation. Suppose B confers a 5% advantage in the presence of A, no advantage otherwise. Then while A is still at 1% frequency in the population, B only confers its advantage 1 out of 100 times, so the average fitness advantage of B is 0.05%, and B's probability of fixation is 0.1%.\n\nContrast to a human programmer, who can design a new complex mechanism with a hundred interdependent parts over the course of a single afternoon.\n\nThe tremendously slow timescale of evolution, especially for creating new complex machinery (as opposed to selecting on *existing* variance), is why the behavior of evolved organisms is often better interpreted [in terms of what did in fact work yesterday, rather than what will work in the future](https://wiki.lesswrong.com/wiki/adaptation_executers).\n\nMain article\n------------\n\n*   [Evolutions Are Stupid (But Work Anyway)](http://lesswrong.com/lw/kt/evolutions_are_stupid_but_work_anyway/)\n\nSee also\n--------\n\n*   [Stupidity of evolution](https://www.lesswrong.com/tag/stupidity-of-evolution)\n*   [Adaptation executers](https://wiki.lesswrong.com/wiki/Adaptation_executers)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb250",
    "name": "Corrupted Hardware",
    "core": null,
    "slug": "corrupted-hardware",
    "oldSlugs": null,
    "postCount": 0,
    "description": {
      "markdown": "Our brains do not always allow us to act the way we should. **Corrupted hardware** refers to those behaviors and thoughts that act for ancestrally relevant purposes rather than for stated moralities and preferences.\n\nBlog posts\n----------\n\n*   [Why Does Power Corrupt?](http://lesswrong.com/lw/uu/why_does_power_corrupt/)\n*   [Ends Don't Justify Means (Among Humans)](http://lesswrong.com/lw/uv/ends_dont_justify_means_among_humans/)\n*   [Which Parts are \"Me\"?](http://lesswrong.com/lw/v4/which_parts_are_me/)\n*   [The Curse of Identity](http://lesswrong.com/lw/8gv/the_curse_of_identity/) by [Kaj Sotala](https://wiki.lesswrong.com/wiki/Kaj_Sotala)\n*   [Urges vs. Goals: The analogy to anticipation and belief](http://lesswrong.com/r/lesswrong/lw/8q8/urges_vs_goals_how_to_use_human_hardware_to/) by [Anna Salamon](https://www.lesswrong.com/tag/anna-salamon)\n\nSee also\n--------\n\n*   [Akrasia](https://www.lesswrong.com/tag/akrasia)\n*   [Ethical injunction](https://www.lesswrong.com/tag/ethical-injunction)\n*   [Evolutionary psychology](https://www.lesswrong.com/tag/evolutionary-psychology), [adaptation executers](https://wiki.lesswrong.com/wiki/adaptation_executers)\n*   [Signaling](https://www.lesswrong.com/tag/signaling)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb10b",
    "name": "Rationality As Martial Art",
    "core": null,
    "slug": "rationality-as-martial-art",
    "oldSlugs": null,
    "postCount": 0,
    "description": {
      "markdown": "A metaphor for **rationality as the martial art** of the mind; training brains in the same fashion as muscles.\n\nThe metaphor is intended to have complex connotations, rather than being strictly positive. Do modern-day martial arts suffer from being [insufficiently tested in realistic fighting](http://lesswrong.com/lw/2i/epistemic_viciousness/), and do attempts at [rationality](https://www.lesswrong.com/tag/rationality) training run into the same problem?\n\nBlog posts\n----------\n\n*   [The Martial Art of Rationality](http://lesswrong.com/lw/gn/the_martial_art_of_rationality/)\n*   [A Sense That More Is Possible](http://lesswrong.com/lw/2c/a_sense_that_more_is_possible/)\n*   [Epistemic Viciousness](http://lesswrong.com/lw/2i/epistemic_viciousness/)\n*   [3 Levels of Rationality Verification](http://lesswrong.com/lw/2s/3_levels_of_rationality_verification/)\n*   [Mandatory Secret Identities](http://lesswrong.com/lw/9c/mandatory_secret_identities/)\n*   [Whining-Based Communities](http://lesswrong.com/lw/8t/whiningbased_communities/)\n*   [In What Ways Have You Become Stronger?](http://lesswrong.com/lw/2x/in_what_ways_have_you_become_stronger/) by [Vladimir Nesov](https://wiki.lesswrong.com/wiki/Vladimir_Nesov)\n*   [Rational Me or We?](http://lesswrong.com/lw/36/rational_me_or_we/) by [Robin Hanson](https://www.lesswrong.com/tag/robin-hanson)\n*   [Individual Rationality is a Matter of Life and Death](http://lesswrong.com/lw/41/individual_rationality_is_a_matter_of_life_and/) by patrissimo\n\nSee also\n--------\n\n*   [Problem of verifying rationality](https://www.lesswrong.com/tag/problem-of-verifying-rationality)\n*   [Rationality](https://www.lesswrong.com/tag/rationality)\n*   [Group rationality](https://www.lesswrong.com/tag/group-rationality)\n*   [Rationality is systematized winning](https://www.lesswrong.com/tag/rationality-is-systematized-winning)\n*   [No safe defense](https://www.lesswrong.com/tag/no-safe-defense)\n\nReferences\n----------\n\n*   Gillian Russell (2008). Epistemic Viciousness in the Martial Arts. ([PDF](http://www.artsci.wustl.edu/~grussell/epistemicviciousness.pdf))"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb1b0",
    "name": "Fake Simplicity",
    "core": null,
    "slug": "fake-simplicity",
    "oldSlugs": null,
    "postCount": 0,
    "description": {
      "markdown": "Blog posts\n----------\n\n*   [Fake Explanations](http://lesswrong.com/lw/ip/fake_explanations/)\n*   [Fake Optimization Criteria](http://lesswrong.com/lw/kz/fake_optimization_criteria/)\n*   [The Hidden Complexity of Wishes](http://lesswrong.com/lw/ld/the_hidden_complexity_of_wishes/)\n*   [Fake Utility Functions](http://lesswrong.com/lw/lq/fake_utility_functions/)\n*   [Passing the Recursive Buck](http://lesswrong.com/lw/rd/passing_the_recursive_buck/)\n\nSee also\n--------\n\n*   [Semantic stopsign](https://www.lesswrong.com/tag/semantic-stopsign), [Rationalization](https://www.lesswrong.com/tag/rationalization)\n*   [Narrative fallacy](https://www.lesswrong.com/tag/narrative-fallacy)\n*   [Complexity of value](https://www.lesswrong.com/tag/complexity-of-value), [Magical categories](https://www.lesswrong.com/tag/magical-categories)\n*   [Occam's razor](https://www.lesswrong.com/tag/occam-s-razor), [Detached lever fallacy](https://www.lesswrong.com/tag/detached-lever-fallacy)\n*   [Phlogiston](https://www.lesswrong.com/tag/phlogiston)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb196",
    "name": "Mind Design Space",
    "core": null,
    "slug": "mind-design-space",
    "oldSlugs": null,
    "postCount": 0,
    "description": {
      "markdown": "**Mind design space** refers to the [configuration space](https://www.lesswrong.com/tag/configuration-space) of possible minds. As humans living in a human world, we can safely make all sorts of assumptions about the minds around us without even realizing it. Each human might have their own unique personal qualities, so it might naively seem that there's nothing you can say about people you [don't know](https://wiki.lesswrong.com/wiki/I_don't_know). But there's actually quite a lot you can say (with high or very high probability) about a random human: that they have [standard](https://www.lesswrong.com/tag/human-universal) emotions like happiness, sadness, and anger; standard senses like sight, vision, and hearing; that they speak a language; and no doubt any number of other subtle features that are even harder to [quickly explain in words](https://www.lesswrong.com/tag/inferential-distance). These things are the specific results of [adaptation pressures in the ancestral environment](https://www.lesswrong.com/tag/evolutionary-psychology) and can't be expected to be shared by a random alien or [AI](https://www.lesswrong.com/tag/artificial-general-intelligence). That is, humans are packed into a tiny dot in the configuration space: there is vast range over of other ways a mind can _be_.\n\nBlog posts\n----------\n\n*   [The Design Space of Minds-In-General](http://lesswrong.com/lw/rm/the_design_space_of_mindsingeneral/)\n\nSee also\n--------\n\n*   [Artificial general intelligence](https://www.lesswrong.com/tag/artificial-general-intelligence)\n*   [Anthropomorphism](https://www.lesswrong.com/tag/anthropomorphism), [Alien values](https://www.lesswrong.com/tag/alien-values)\n*   [Evolution as alien god](https://www.lesswrong.com/tag/evolution-as-alien-god)\n*   [Paperclip maximizer](https://www.lesswrong.com/tag/paperclip-maximizer), [Magical categories](https://www.lesswrong.com/tag/magical-categories)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb139",
    "name": "Arguments As Soldiers",
    "core": null,
    "slug": "arguments-as-soldiers",
    "oldSlugs": null,
    "postCount": 0,
    "description": {
      "markdown": "**Arguments as soldiers** is an alternate label for a [conceptual metaphor](https://www.lesswrong.com/tag/conceptual-metaphor) which is more commonly stated as **\"Argument is War/Battle\"**. This metaphor is at the core of the [adversarial system](https://wiki.lesswrong.com/wiki/adversarial_system) of debate which is widespread in politics, law and academia. \n\n> Politics is the mind-killer. Arguments are soldiers. Once you know which side you’re on, you must support all arguments of that side, and attack all arguments that appear to favor the enemy side; otherwise it’s like stabbing your soldiers in the back. If you abide within that pattern, policy debates will also appear one-sided to you—the costs and drawbacks of your favored policy are enemy soldiers, to be attacked by any means necessary.\n> \n> \\-\\- [Policy Debates Should Not Appear One-Sided](https://www.lesswrong.com/posts/PeSzc9JTBxhaYRp9b/policy-debates-should-not-appear-one-sided)\n\nIdentifying yourself with a [side](https://www.lesswrong.com/tag/blues-and-greens-metaphor) in a political debate may be [detrimental](https://www.lesswrong.com/tag/mind-killer) to rational evaluation of arguments. Arguments get treated as soldiers, weapons to be used to defend your side of the debate, and to attack the other side. They are no longer instruments of the [truth](https://www.lesswrong.com/tag/truth-semantics-and-meaning).\n\nBlog posts\n----------\n\n*   [Policy Debates Should Not Appear One-Sided](http://lesswrong.com/lw/gz/policy_debates_should_not_appear_onesided/)\n\nSee also\n--------\n\n*   [Policy debates should not appear one-sided](https://www.lesswrong.com/tag/policy-debates-should-not-appear-one-sided)\n*   [Adversarial process](https://wiki.lesswrong.com/wiki/Adversarial_process)\n*   [Disagreement](https://www.lesswrong.com/tag/disagreement)\n*   [Filtered evidence](https://www.lesswrong.com/tag/filtered-evidence)\n*   [Color politics](https://www.lesswrong.com/tag/blues-and-greens-metaphor)\n*   [Signaling](https://www.lesswrong.com/tag/signaling)\n*   [Mind-killer](https://www.lesswrong.com/tag/mind-killer)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb179",
    "name": "In-Group Bias",
    "core": null,
    "slug": "in-group-bias",
    "oldSlugs": null,
    "postCount": 0,
    "description": {
      "markdown": "**In-group bias** is the preferential treatment of people and ideas associated with your own group. Can lead to [color politics](https://www.lesswrong.com/tag/blues-and-greens-metaphor), and is a catalyst for [affective death spirals](https://www.lesswrong.com/tag/affective-death-spiral).\n\nBlog posts\n----------\n\n*   [A Fable of Science and Politics](http://lesswrong.com/lw/gt/a_fable_of_science_and_politics/)\n*   [The Robbers Cave Experiment](http://lesswrong.com/lw/lt/the_robbers_cave_experiment/)\n\nSee also\n--------\n\n*   [Conformity bias](https://www.lesswrong.com/tag/conformity-bias)\n*   [Color politics](https://www.lesswrong.com/tag/blues-and-greens-metaphor)\n*   [Halo effect](https://www.lesswrong.com/tag/halo-effect)\n*   [Affective death spiral](https://www.lesswrong.com/tag/affective-death-spiral)\n*   [Status](https://www.lesswrong.com/tag/social-status), [signaling](https://www.lesswrong.com/tag/signaling)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb101",
    "name": "Overconfidence",
    "core": null,
    "slug": "overconfidence",
    "oldSlugs": null,
    "postCount": 0,
    "description": {
      "markdown": "**Overconfidence** is the state of being more certain than is justified, given your [priors](https://www.lesswrong.com/tag/priors) and the [evidence](https://www.lesswrong.com/tag/evidence) available.\n\nBlog posts\n----------\n\n*   [Moral Overconfidence](http://www.overcomingbias.com/2006/11/moral_hypocricy.html) by [Robin Hanson](https://www.lesswrong.com/tag/robin-hanson)\n*   [When Error is High, Simplify](http://www.overcomingbias.com/2006/12/when_error_is_h.html) by Robin Hanson\n*   [Academic Overconfidence](http://www.overcomingbias.com/2006/12/academic_overco.html) by Robin Hanson\n*   [Bosses Prefer Overconfident Managers](http://www.overcomingbias.com/2006/12/bosses_prefer_o.html) by Robin Hanson\n*   [Convenient Overconfidence](http://www.overcomingbias.com/2008/11/convenient-over.html) by Robin Hanson\n*   [Overconfidence Explained](http://www.overcomingbias.com/2011/12/overconfidence-explained.html) by Robin Hanson\n\nSee also\n--------\n\n*   [Underconfidence](https://www.lesswrong.com/tag/underconfidence)\n*   [Status](https://www.lesswrong.com/tag/social-status), [Modesty](https://www.lesswrong.com/tag/modesty)\n*   [Dangerous knowledge](https://www.lesswrong.com/tag/dangerous-knowledge)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb145",
    "name": "Motivated Skepticism",
    "core": null,
    "slug": "motivated-skepticism",
    "oldSlugs": null,
    "postCount": 0,
    "description": {
      "markdown": "**Motivated skepticism** is the mistake of applying more skepticism to claims that you don't like (or intuitively disbelieve), than to claims that you do like. Because [emotional](https://www.lesswrong.com/tag/emotions) disposition towards a claim isn't generally [evidence](https://www.lesswrong.com/tag/evidence) about its [truth](https://www.lesswrong.com/tag/truth-semantics-and-meaning), including it in the process of arriving at a [belief](https://www.lesswrong.com/tag/belief) means holding the belief partly for [reasons other than because it's true](https://www.lesswrong.com/tag/improper-belief).\n\nBlog posts\n----------\n\n*   [Knowing About Biases Can Hurt People](http://lesswrong.com/lw/he/knowing_about_biases_can_hurt_people/)\n*   [Motivated Stopping and Motivated Continuation](http://lesswrong.com/lw/km/motivated_stopping_and_motivated_continuation/)\n*   [The Skeptic's Trilemma](http://lesswrong.com/lw/2p/the_skeptics_trilemma/) by [Yvain](https://wiki.lesswrong.com/wiki/Yvain)\n*   [Undiscriminating Skepticism](http://lesswrong.com/lw/1ww/undiscriminating_skepticism/)\n\nExternal Links\n--------------\n\n*   [From Skepticism to Technical Rationality](http://facingthesingularity.com/2011/from-skepticism-to-technical-rationality/) by [lukeprog](http://lukeprog.com/)\n\nSee also\n--------\n\n*   [Motivated cognition](https://www.lesswrong.com/tag/motivated-reasoning)\n*   [Least convenient possible world](https://www.lesswrong.com/tag/least-convenient-possible-world)\n*   [Filtered evidence](https://www.lesswrong.com/tag/filtered-evidence), [Conservation of expected evidence](https://www.lesswrong.com/tag/conservation-of-expected-evidence)\n*   [Color politics](https://www.lesswrong.com/tag/blues-and-greens-metaphor)\n*   [Positive bias](https://www.lesswrong.com/tag/confirmation-bias)\n*   [Rationalization](https://www.lesswrong.com/tag/rationalization), [Oops](https://www.lesswrong.com/tag/oops)\n*   [Dangerous knowledge](https://www.lesswrong.com/tag/dangerous-knowledge)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb149",
    "name": "Inductive Bias",
    "core": null,
    "slug": "inductive-bias",
    "oldSlugs": null,
    "postCount": 0,
    "description": {
      "markdown": "**Inductive bias** refers to your suspicion that if the sun has risen for the last billion days in a row, then it may rise tomorrow as well. Since it is [logically possible](https://wiki.lesswrong.com/wiki/logically_possible) that the laws of physics will arbitrarily cease to work and that the sun will *not* rise tomorrow, coming to this conclusion requires an inductively biased [prior](https://wiki.lesswrong.com/wiki/prior).\n\nThis sort of bias is not a bad thing - without \"inductive bias\" you can't draw any conclusion at all from the data. It's just a different technical meaning attached to the same word.\n\nPrimary post\n------------\n\n*   [\"Inductive Bias\"](http://lesswrong.com/lw/hg/inductive_bias/)\n\nReferences\n----------\n\n*   Tom M. Mitchell (1980). [The need for biases in learning generalizations.](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.120.4179)\n\nSee also\n--------\n\n*   [Superexponential conceptspace](https://www.lesswrong.com/tag/superexponential-conceptspace)\n*   [Prior distribution](https://wiki.lesswrong.com/wiki/Prior_distribution)\n*   [Statistical bias](https://www.lesswrong.com/tag/statistical-bias)\n*   [Cognitive bias](https://wiki.lesswrong.com/wiki/Cognitive_bias)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb0ea",
    "name": "Hedon",
    "core": null,
    "slug": "hedon",
    "oldSlugs": null,
    "postCount": 0,
    "description": {
      "markdown": "The unit philosophers use to quantify pleasure. Hedons are abstract units with no standardized metric: the assignment of hedons to experiences is typically very broad, loose, subjective, and case dependent. Negative hedons can stand for whatever detracts from a pleasurable experience, or for pain -- though sometimes the unit used for pain is the \"dolor\".\n\nFor example, we might say that eating an ice cream cone yields 3 hedons to a normal individual, while solving an intensely difficult logic puzzle yields 15 hedons. The frustration undergone in the course of figuring out the puzzle might be judged as -20 hedons (or 20 dolors), depending on the puzzle's difficulty and the individual's temperament. The specific numbers chosen are generally not too important; the main point is to give a rough sketch of how the enjoyment of one experience relates to others.\n\n[Utilitarians](https://www.lesswrong.com/tag/utilitarianism) often employ hedons to compare the expected net outcome of pleasure for a given scenario. [Hedonism](https://www.lesswrong.com/tag/hedonism) is an ethical theory that sees the maximization of hedons (and the minimization of dolors) as the most desirable good.\n\nBlog Posts\n----------\n\n*   [Not for the Sake of Happiness (Alone)](http://lesswrong.com/lw/lb/not_for_the_sake_of_happiness_alone/)\n\nSee also\n--------\n\n*   [Utilitarianism](https://www.lesswrong.com/tag/utilitarianism), [Hedonism](https://www.lesswrong.com/tag/hedonism)\n*   [Fuzzies](https://www.lesswrong.com/tag/fuzzies), [Utils](https://wiki.lesswrong.com/wiki/Utils)\n*   [Complexity of value](https://www.lesswrong.com/tag/complexity-of-value)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb0f1",
    "name": "Least Convenient Possible World",
    "core": null,
    "slug": "least-convenient-possible-world",
    "oldSlugs": null,
    "postCount": 0,
    "description": {
      "markdown": "**Least convenient possible world** (or **LCPW**) is a technique for enforcing intellectual honesty, to be used when arguing against an idea. The essence of the technique is to assume that all the specific details will align with the idea against which you are arguing, i.e. to consider the idea in the context of a least convenient [possible world](https://www.lesswrong.com/tag/possible-world), where every circumstance is colluding against your objections and counterarguments. This approach ensures that your objections are [strong enough](https://www.lesswrong.com/tag/epistemic-hygiene), running minimal risk of being [rationalizations](https://www.lesswrong.com/tag/rationalization) for your [position](https://www.lesswrong.com/tag/blues-and-greens-metaphor).\n\nBlog posts\n----------\n\n*   [The Least Convenient Possible World](http://lesswrong.com/lw/2k/the_least_convenient_possible_world/) by [Yvain](https://wiki.lesswrong.com/wiki/Yvain)\n*   [Logical Rudeness](http://lesswrong.com/lw/1p1/logical_rudeness/)\n*   [Is That Your True Rejection?](http://lesswrong.com/lw/wj/is_that_your_true_rejection/)\n*   [Better Disagreement](http://lesswrong.com/lw/85h/better_disagreement/) by [lukeprog](https://wiki.lesswrong.com/wiki/lukeprog)\n\nExternal links\n--------------\n\n*   [Is That Your True Rejection?](http://www.cato-unbound.org/2011/09/07/eliezer-yudkowsky/is-that-your-true-rejection/) at Cato Unbound, by [Eliezer Yudkowsky](https://www.lesswrong.com/tag/eliezer-yudkowsky)\n\nSee also\n--------\n\n*   [Rationalization](https://www.lesswrong.com/tag/rationalization)\n*   [Epistemic hygiene](https://www.lesswrong.com/tag/epistemic-hygiene)\n*   [Disagreement](https://www.lesswrong.com/tag/disagreement)\n*   [Rationalist taboo](https://www.lesswrong.com/tag/rationalist-taboo)\n*   [Reversal test](https://www.lesswrong.com/tag/reversal-test)\n*   [How To Actually Change Your Mind](https://www.lesswrong.com/tag/how-to-actually-change-your-mind)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb161",
    "name": "Rational Evidence",
    "core": null,
    "slug": "rational-evidence",
    "oldSlugs": null,
    "postCount": 0,
    "description": {
      "markdown": "**Rational evidence** is the broadest possible sense of [evidence](https://www.lesswrong.com/tag/evidence), i.e., the [Bayesian](https://www.lesswrong.com/tag/bayesianism) sense. Rational evidence about a hypothesis H is *any* observation which has a different [likelihood](https://wiki.lesswrong.com/wiki/likelihood) depending on whether H holds in reality or not.\n\nRational evidence is distinguished from narrower forms of evidence, such as scientific evidence or legal evidence. For a belief to be scientific, you should be able to do repeatable experiments to verify the belief. For evidence to be admissible in court, it must e.g. be a personal observation rather than hearsay.\n\nFor example, suppose I tell you that the original author of this paragraph wore white socks while writing it. (In fact, I do so tell you.) You now have *rational evidence* that the author of this paragraph wore white socks, because I'm more likely to tell you this if I am wearing white socks, than if I'm not (Note: This doesn't prove that I am wearing white socks; this is why it is 'evidence'). But it is not *scientific knowledge* because there is no experiment you can do for yourself to verify whether it is true. And it is not *legal evidence* \\- you could testify in court that I had *told* you my socks were white, but you could not testify that my socks were white.\n\nThe [scientific method](https://www.lesswrong.com/tag/science) can be viewed as a special standard of admissible evidence protecting a pool of extra-strong beliefs. Conversely, a fact can be rationally guessable without it generating the specially strong evidence which would qualify knowledge of the fact as \"scientific\". Just as a police detective may rationally know the identity of the local crime boss without having the special evidence needed to prove it *in court*.\n\nMain post\n---------\n\n*   [Scientific Evidence, Legal Evidence, Rational Evidence](http://lesswrong.com/lw/in/scientific_evidence_legal_evidence_rational/)\n\nOther posts\n-----------\n\n*   [Is Molecular Nanotechnology \"Scientific\"?](http://lesswrong.com/lw/io/is_molecular_nanotechnology_scientific/)\n*   [The Dilemma: Science or Bayes?](http://lesswrong.com/lw/qa/the_dilemma_science_or_bayes/) and [Science Doesn't Trust Your Rationality](http://lesswrong.com/lw/qb/science_doesnt_trust_your_rationality/)\n\nSee also\n--------\n\n*   [Evidence](https://www.lesswrong.com/tag/evidence), [Standard of evidence](https://www.lesswrong.com/tag/standard-of-evidence)\n*   [Bayes' theorem](https://www.lesswrong.com/tag/bayes-theorem), [Conservation of expected evidence](https://www.lesswrong.com/tag/conservation-of-expected-evidence)\n*   [Traditional rationality](https://www.lesswrong.com/tag/traditional-rationality), [Epistemic hygiene](https://www.lesswrong.com/tag/epistemic-hygiene)\n*   [Exploratory engineering](https://www.lesswrong.com/tag/exploratory-engineering)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb12b",
    "name": "Semantic Stopsign",
    "core": null,
    "slug": "semantic-stopsign",
    "oldSlugs": null,
    "postCount": 0,
    "description": {
      "markdown": "A **semantic stopsign** or **curiosity stopper** is a meaningless, generic explanation that creates the illusion of giving an answer, without actually explaining anything. Semantic stopsigns destroy [curiosity](https://www.lesswrong.com/tag/curiosity), giving surrogate answers and stopping the search for [truth](https://www.lesswrong.com/tag/truth-semantics-and-meaning) prematurely. Can preserve [incorrect beliefs](https://www.lesswrong.com/tag/improper-belief) for a long time, insisting on following [cached thought](https://www.lesswrong.com/tag/cached-thought) without rethinking anything. A tool of [dark arts](https://www.lesswrong.com/tag/dark-arts) and an important part of any [anti-epistemology](https://www.lesswrong.com/tag/anti-epistemology).\n\nWhen specific words act as stopsigns, placing a [rationalist taboo](https://www.lesswrong.com/tag/rationalist-taboo) on them may help. [Epistemic hygiene](https://www.lesswrong.com/tag/epistemic-hygiene) allows excluding some of the stopsigns raised by [partial arguers](https://www.lesswrong.com/tag/arguments-as-soldiers) or by your own [cognitive biases](https://wiki.lesswrong.com/wiki/cognitive_biases).\n\nBlog posts\n----------\n\n*   [Semantic Stopsigns](http://lesswrong.com/lw/it/semantic_stopsigns/)\n*   [Mysterious Answers to Mysterious Questions](http://lesswrong.com/lw/iu/mysterious_answers_to_mysterious_questions/)\n*   [Fake Explanations](http://lesswrong.com/lw/ip/fake_explanations/)\n*   [\"Science\" as Curiosity-Stopper](http://lesswrong.com/lw/j3/science_as_curiositystopper/)\n*   [Explain/Worship/Ignore?](http://lesswrong.com/lw/j2/explainworshipignore/)\n*   [The Futility of Emergence](http://lesswrong.com/lw/iv/the_futility_of_emergence/)\n*   [Conversation Halters](http://lesswrong.com/lw/1p2/conversation_halters/)\n\nSee also\n--------\n\n*   [Curiosity](https://www.lesswrong.com/tag/curiosity), [Separate magisteria](https://wiki.lesswrong.com/wiki/Separate_magisteria)\n*   [Improper belief](https://www.lesswrong.com/tag/improper-belief)\n*   [Fake simplicity](https://www.lesswrong.com/tag/fake-simplicity), [Cached thought](https://www.lesswrong.com/tag/cached-thought)\n*   [Anti-epistemology](https://www.lesswrong.com/tag/anti-epistemology), [Logical rudeness](https://www.lesswrong.com/tag/logical-rudeness)\n*   [Rationalist taboo](https://www.lesswrong.com/tag/rationalist-taboo), [Epistemic hygiene](https://www.lesswrong.com/tag/epistemic-hygiene)\n*   [Phlogiston](https://www.lesswrong.com/tag/phlogiston)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb209",
    "name": "Death Spirals and The Cult Attractor",
    "core": null,
    "slug": "death-spirals-and-the-cult-attractor",
    "oldSlugs": null,
    "postCount": 0,
    "description": {
      "markdown": "A [subsequence](https://wiki.lesswrong.com/wiki/sequence) of [How to Actually Change Your Mind](https://www.lesswrong.com/tag/how-to-actually-change-your-mind) on two of the huger obstacles, the [affective death spiral](https://www.lesswrong.com/tag/affective-death-spiral) and the [cultishness attractor](https://wiki.lesswrong.com/wiki/cultishness_attractor).\n\nAffective death spirals are positive feedback loop caused by the [halo effect](https://www.lesswrong.com/tag/halo-effect): Positive characteristics perceptually correlate, so the more nice things we say about X, the more additional nice things we're likely to believe about X.\n\nCultishness is an empirical attractor in human groups, roughly an affective death spiral, plus peer pressure and outcasting behavior, plus (quite often) defensiveness around something believed to have been perfected.\n\nSome overlap with the [Politics is the Mind-Killer](https://www.lesswrong.com/tag/politics-is-the-mind-killer) subsequence (preferably read that first).\n\nBlog posts\n----------\n\n*   [The Affect Heuristic](http://lesswrong.com/lw/lg/the_affect_heuristic/)\n    *   _[Evaluability and Cheap Holiday Shopping](http://lesswrong.com/lw/lh/evaluability_and_cheap_holiday_shopping/)_\n    *   _[Unbounded Scales, Huge Jury Awards, & Futurism](http://lesswrong.com/lw/li/unbounded_scales_huge_jury_awards_futurism/)_\n*   [The Halo Effect](http://lesswrong.com/lw/lj/the_halo_effect/)\n    *   _[Superhero Bias](http://lesswrong.com/lw/lk/superhero_bias/)_\n    *   _[Mere Messiahs](http://lesswrong.com/lw/ll/mere_messiahs/)_\n*   [Affective Death Spirals](http://lesswrong.com/lw/lm/affective_death_spirals/)\n*   [Resist the Happy Death Spiral](http://lesswrong.com/lw/ln/resist_the_happy_death_spiral/)\n*   [Uncritical Supercriticality](http://lesswrong.com/lw/lo/uncritical_supercriticality/)\n*   [Evaporative Cooling of Group Beliefs](http://lesswrong.com/lw/lr/evaporative_cooling_of_group_beliefs/)\n*   [When None Dare Urge Restraint](http://lesswrong.com/lw/ls/when_none_dare_urge_restraint/)\n*   [The Robbers Cave Experiment](http://lesswrong.com/lw/lt/the_robbers_cave_experiment/)\n*   [Every Cause Wants to be a Cult](http://lesswrong.com/lw/lv/every_cause_wants_to_be_a_cult/)\n*   [Guardians of the Truth](http://lesswrong.com/lw/lz/guardians_of_the_truth/)\n    *   _[Guardians of the Gene Pool](http://lesswrong.com/lw/m0/guardians_of_the_gene_pool/)_\n    *   [Guardians of Ayn Rand](http://lesswrong.com/lw/m1/guardians_of_ayn_rand/)\n*   _[The Litany Against Gurus](http://lesswrong.com/lw/m2/the_litany_against_gurus/)_\n*   [Two Cult Koans](http://lesswrong.com/lw/m4/two_cult_koans/)\n*   [Asch's Conformity Experiment](http://lesswrong.com/lw/m9/aschs_conformity_experiment/) \\- The unanimous agreement of surrounding others can make subjects disbelieve (or at least, fail to report) what's right before their eyes. The addition of just one dissenter is enough to dramatically reduce the rates of improper [conformity](https://www.lesswrong.com/tag/conformity-bias).\n*   [Lonely Dissent](http://lesswrong.com/lw/mb/lonely_dissent/)\n*   [Cultish Countercultishness](http://lesswrong.com/lw/md/cultish_countercultishness/)\n\nOther formats\n-------------\n\n*   [Readlist](http://readlists.com/1e32079b) \\- view this sequence as an eBook using Readability.\n\nSee also\n--------\n\n*   [Groupthink](https://www.lesswrong.com/tag/groupthink)\n*   [Halo effect](https://www.lesswrong.com/tag/halo-effect)\n*   [Affective death spiral](https://www.lesswrong.com/tag/affective-death-spiral)\n*   [Group rationality](https://www.lesswrong.com/tag/group-rationality)\n*   [Politics is the Mind-Killer](https://www.lesswrong.com/tag/politics-is-the-mind-killer)\n*   [The Craft and the Community](https://www.lesswrong.com/tag/the-craft-and-the-community)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb1bb",
    "name": "Metaethics Sequence",
    "core": null,
    "slug": "metaethics-sequence",
    "oldSlugs": null,
    "postCount": 0,
    "description": {
      "markdown": "Posts directly about metaethics were published primarily from [June 20 2008](http://lesswrong.com/lw/rh/heading_toward_morality/) to [August 22 2008](http://lesswrong.com/lw/ta/invisible_frameworks/), albeit with a good deal of related material before and after.\n\nSee also\n--------\n\n*   [Complexity of value](https://www.lesswrong.com/tag/complexity-of-value)\n*   [Fun theory](https://www.lesswrong.com/tag/fun-theory)\n*   [No-Nonsense Metaethics](https://www.lesswrong.com/tag/no-nonsense-metaethics)\n\nBlog posts (in primary sequence)\n--------------------------------\n\n*   [Heading Toward Morality](http://lesswrong.com/lw/rh/heading_toward_morality/)\n*   [No Universally Compelling Arguments](http://lesswrong.com/lw/rn/no_universally_compelling_arguments/)\n*   [2-Place and 1-Place Words](http://lesswrong.com/lw/ro/2place_and_1place_words/)\n*   [What Would You Do Without Morality?](http://lesswrong.com/lw/rq/what_would_you_do_without_morality/)\n*   [The Moral Void](http://lesswrong.com/lw/rr/the_moral_void/)\n*   [Created Already In Motion](http://lesswrong.com/lw/rs/created_already_in_motion/)\n*   [The Bedrock of Fairness](http://lesswrong.com/lw/ru/the_bedrock_of_fairness/)\n*   [Moral Complexities](http://lesswrong.com/lw/rw/moral_complexities/)\n*   [Is Morality Preference?](http://lesswrong.com/lw/rx/is_morality_preference/)\n*   [Is Morality Given?](http://lesswrong.com/lw/ry/is_morality_given/)\n*   [Where Recursive Justification Hits Bottom](http://lesswrong.com/lw/s0/where_recursive_justification_hits_bottom/)\n*   [My Kind of Reflection](http://lesswrong.com/lw/s2/my_kind_of_reflection/)\n*   [The Genetic Fallacy](http://lesswrong.com/lw/s3/the_genetic_fallacy/)\n*   [Fundamental Doubts](http://lesswrong.com/lw/s4/fundamental_doubts/)\n*   [Rebelling Within Nature](http://lesswrong.com/lw/s5/rebelling_within_nature/)\n*   [Probability is Subjectively Objective](http://lesswrong.com/lw/s6/probability_is_subjectively_objective/)\n*   [Whither Moral Progress?](http://lesswrong.com/lw/s9/whither_moral_progress/)\n*   [The Gift We Give To Tomorrow](http://lesswrong.com/lw/sa/the_gift_we_give_to_tomorrow/)\n*   [Could Anything Be Right?](http://lesswrong.com/lw/sb/could_anything_be_right/)\n*   [Existential Angst Factory](http://lesswrong.com/lw/sc/existential_angst_factory/)\n*   [Can Counterfactuals Be True?](http://lesswrong.com/lw/sh/can_counterfactuals_be_true/)\n*   [Math is Subjunctively Objective](http://lesswrong.com/lw/si/math_is_subjunctively_objective/)\n*   [Does Your Morality Care What You Think?](http://lesswrong.com/lw/sj/does_your_morality_care_what_you_think/)\n*   [Changing Your Metaethics](http://lesswrong.com/lw/sk/changing_your_metaethics/)\n*   [Setting Up Metaethics](http://lesswrong.com/lw/sl/setting_up_metaethics/)\n*   [The Meaning of Right](http://lesswrong.com/lw/sm/the_meaning_of_right/)\n*   [Interpersonal Morality](http://lesswrong.com/lw/sn/interpersonal_morality/)\n*   [Morality as Fixed Computation](http://lesswrong.com/lw/sw/morality_as_fixed_computation/)\n*   [Inseparably Right; or, Joy in the Merely Good](http://lesswrong.com/lw/sx/inseparably_right_or_joy_in_the_merely_good/)\n*   [Sorting Pebbles Into Correct Heaps](http://lesswrong.com/lw/sy/sorting_pebbles_into_correct_heaps/)\n*   [Moral Error and Moral Disagreement](http://lesswrong.com/lw/sz/moral_error_and_moral_disagreement/)\n*   [Abstracted Idealized Dynamics](http://lesswrong.com/lw/t0/abstracted_idealized_dynamics/)\n*   [\"Arbitrary\"](http://lesswrong.com/lw/t1/arbitrary/)\n*   [Is Fairness Arbitrary?](http://lesswrong.com/lw/t2/is_fairness_arbitrary/)\n*   [The Bedrock of Morality: Arbitrary?](http://lesswrong.com/lw/t3/the_bedrock_of_morality_arbitrary/)\n*   [You Provably Can't Trust Yourself](http://lesswrong.com/lw/t8/you_provably_cant_trust_yourself/)\n*   [No License To Be Human](http://lesswrong.com/lw/t9/no_license_to_be_human/)\n*   [Invisible Frameworks](http://lesswrong.com/lw/ta/invisible_frameworks/)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb28b",
    "name": "Coherent Aggregated Volition",
    "core": null,
    "slug": "coherent-aggregated-volition",
    "oldSlugs": null,
    "postCount": 0,
    "description": {
      "markdown": "**Coherent Aggregated Volition** is one of [Ben Goertzel](https://www.lesswrong.com/tag/ben-goertzel)'s responses to [Eliezer Yudkowsky](https://www.lesswrong.com/tag/eliezer-yudkowsky)'s [Coherent Extrapolated Volition](https://www.lesswrong.com/tag/coherent-extrapolated-volition), the other being [Coherent Blended Volition](https://www.lesswrong.com/tag/coherent-blended-volition). CAV would be a combination of the goals and beliefs of humanity at the present time.\n\nThe author considers the \"extrapolation\" aspect of CEV as distorting the concept of volition and to be highly uncertain. He considers that if the person whose volition is being extrapolated has some inconsistent aspects (which is typically human), then there could be a great variety of possible extrapolations. The problem would then be which version of this extrapolated human to choose, or how to aggregate them, which would be very difficult to achieve.\n\nCoherent Aggregated Volition is presented as simpler than his interpretation of CEV, and intended to be easier to formalize and prototype in the foreseeable future (with the help of platforms such as [OpenCog](http://opencog.org). CAV is not, however, intended to answer the question of provably [Friendly AI](https://wiki.lesswrong.com/wiki/Friendly_AI), although Goertzel claims CEV may not answer that question either.\n\nThe concept\n-----------\n\nThe author starts by arguing that we must treat goals and beliefs together, as a single concept, which he calls *gobs* (and *gobses* for the plural). Each agent thus can have several *gobs*, logically consistent or not. As a way of measuring the distance of these *gobs* from each other, the term *gobs metric* is used - the persons or AGI could agree, with various degrees, on several metrics, but it seems probable that this individual's metrics would differ less than their *gobses*.\n\nThen, given a population of intelligent agents with different *gobses*, we could then try to find a single *gobs* that maximizes logical consistency, compactness, similarity to the different *gobses* in the population and amount of evidence supporting these beliefs. This \"multi-extremal optimization algorithm\" is what he calls Coherent Aggregated Volition. The term expresses the attempt to achieve both coherence and an aggregation of the population volitions.\n\nCAV has some free parameters, like the averaging method, the measure of compactness, consistency evaluation and so on, but these are seen as features rather than limitations and do not taint the simplicity of the idea. At the same time, it is possible to refine some of the criteria stated before without changing the nature of the method.\n\nCEV vs CAV\n----------\n\nAlthough CEV is seen as possibly giving a feasible solution, Goertzel states there's no guarantee of this, and that Yudkowsky's method can generate solutions very far from the population's *gobses*.\n\nIn some experiments with iteratively repairing inconsistent beliefs within a probabilistic reasoning system, the author claims that it seems like we can reach a set of beliefs very different from the one we started - which may be a problem with CEV. That is, the iterative refinement of the agents' goals and beliefs might not always be a good way to turn inconsistent values into similar consistent ones.\n\nFinally, Goertzel feels that it seems like CEV bypasses an essential aspect of being human, by not allowing humans to resolve their inconsistencies themselves. CAV tries to summarize this process, respecting and not replacing it, even if this leads to more \"bad\" aspects of humanity being retained.\n\nSee also\n--------\n\n*   [Friendly AI](https://wiki.lesswrong.com/wiki/Friendly_AI)\n*   [Coherent Extrapolated Volition](https://www.lesswrong.com/tag/coherent-extrapolated-volition)\n*   [Utility Functions](https://www.lesswrong.com/tag/utility-functions?showPostCount=true&useTagName=true)\n\nReferences\n----------\n\n*   Goertzel, Ben (2010) [Coherent Aggregated Volition: A Method for Deriving Goal System Content for Advanced, Beneficial AGIs](http://multiverseaccordingtoben.blogspot.com/2010/03/coherent-aggregated-volition-toward.html) [Consistency Measurement](http://www.aquar-system.com/catalog/pulp-consistency-measurement-and-control/)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb331",
    "name": "History of Less Wrong",
    "core": null,
    "slug": "history-of-less-wrong",
    "oldSlugs": null,
    "postCount": 0,
    "description": {
      "markdown": "**Less Wrong** is a community resource devoted to refining the art of human [rationality](https://www.lesswrong.com/tag/rationality) which was founded in 2009. Site activity reached a peak in 2011-13 and a trough in 2016-17. This page mainly describes the history through 2016.\n\n**Related Pages:** [History of Rationality](https://www.lesswrong.com/tag/history-of-rationality), [History](https://www.lesswrong.com/tag/history)\n\nPrehistory\n----------\n\nAround 2001 Yudkowsky had created the mailing list and IRC channel, and on them Yudkowsky frequently expressed annoyance, frustration, and disappointment in his interlocutors' inability to think in ways he considered obviously rational. After failed attempts at teaching people to use Bayes' Theorem, he went largely quiet from SL4 to work on AI safety research directly. After discovering he was not able to make as much progress as he wanted to, he changed tacts to focus on teaching the rationality skills necessary to do AI safety research until such time as there was a sustainable culture that would allow him to focus on AI safety research while also continuing to find and train new AI safety researchers.\n\nLessWrong material was ultimately developed from [Overcoming Bias](https://www.lesswrong.com/tag/overcoming-bias), an earlier group blog focused on human rationality, which began in November 2006, with [Eliezer Yudkowsky](https://www.lesswrong.com/tag/eliezer-yudkowsky) and [Robin Hanson](https://www.lesswrong.com/tag/robin-hanson) as the principal contributors.\n\nFounding\n--------\n\nIn February 2009, Yudkowsky's posts were used as the seed material to create the community blog LessWrong, and Overcoming Bias became Hanson's personal blog. Some users were recruited via Eliezer's [transhumanist](https://www.lesswrong.com/tag/transhumanism) [SL4](https://hpluspedia.org/wiki/SL4) mailing list.\n\nGolden age\n----------\n\nThe site's uses Reddit-style infrastructure. The ongoing community blog materials lead to significant growth and interest over the following years. At its peak it had over 15,000 pageviews a day.\n\nAt some point [Eliezer Yudkowsky](https://www.lesswrong.com/tag/eliezer-yudkowsky) finished writing the [sequences](https://www.lesswrong.com/tag/sequences).\n\nDuring this period, popular fan-fiction [HPMOR](https://www.lesswrong.com/tag/methods-of-rationality-fanfiction) was started and finished.\n\nDiaspora\n--------\n\nAround 2013, many core members of the community stopped posting on Less Wrong, because of both increased growth of the Bay Area physical community and increased demands and opportunities from other projects. MIRI's support base grew to the point where Eliezer could focus on AI research instead of community-building, [Center for Applied Rationality](https://wiki.lesswrong.com/wiki/Center_for_Applied_Rationality) worked on development of new rationality techniques and rationality education mostly offline, and prominent writers left to their own blogs where they could develop their own voice without asking if it was within the bounds of Less Wrong. Collectively some of this diaspora forms the '[rationalist movement](https://www.lesswrong.com/tag/rationalist-movement)'.\n\nSome other prominent ideas to grow out of the lesswrong community (by members of the community's actions) include:\n\n*   [Beeminder](https://www.beeminder.com/)\n*   [Complice](https://complice.co/)\n*   [Mealsquares](http://www.mealsquares.com/)\n*   [Effective altruism](https://www.lesswrong.com/tag/effective-altruism)\n*   [Slate Star Codex](https://www.lesswrong.com/tag/slate-star-codex)\n\nLesswrong is still active and activity can also be found in the diaspora communities:\n\n*   [List of Blogs](https://www.lesswrong.com/tag/list-of-blogs)\n*   [Less Wrong meetup groups](https://www.lesswrong.com/about_meetup_groups)\n*   [List of communities](https://wiki.lesswrong.com/wiki/List_of_communities)\n*   [Less Wrong users on twitter](http://lesswrong.com/r/discussion/lw/d92/less_wrong_on_twitter/)\n*   [Less Wrong users on tumblr](http://rationalist-masterlist.tumblr.com/post/130139930539/rationalist-masterlist)\n*   Less Wrongers on [Facebook](https://wiki.lesswrong.com/wiki/Facebook) (no one has made a comprehensive list, but [here's Eliezer](https://www.facebook.com/yudkowsky))\n\n[Chat](https://wiki.lesswrong.com/wiki/:Category:Chat)\n\n*   [Discord](https://wiki.lesswrong.com/wiki/Discord)\n*   [Less Wrong IRC Chatroom](https://www.lesswrong.com/about_IRC_Chatroom)\n*   [Slack](https://www.lesswrong.com/tag/slack)\n*   [Study Hall](https://wiki.lesswrong.com/wiki/Study_Hall)\n\nThe view from 2016\n------------------\n\nAs of 2016 the community is far less active than it once was. The forum stands, but submissions are down. The wiki has low traction and it is potentially in need to streamlining around remaining activity rather than its former glories.\n\nSee also\n--------\n\n*   [A History of the Rationality Community](https://www.reddit.com/r/slatestarcodex/comments/6tt3gy/a_history_of_the_rationality_community/dloghua/) from [Scott Alexander](https://www.lesswrong.com/tag/scott-alexander)\n*   [LessWrong analytics (February 2009 to January 2017)](http://lesswrong.com/lw/owa/lesswrong_analytics_february_2009_to_january_2017/)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb2db",
    "name": "Technological Revolution",
    "core": null,
    "slug": "technological-revolution",
    "oldSlugs": null,
    "postCount": 0,
    "description": {
      "markdown": "**Technological revolution** is a dramatic social change in important structures brought about relatively quickly by the introduction of some new technology. Past examples are the [introduction of agriculture](https://en.wikipedia.org/wiki/_Neolithic_Revolution), [the invention of the movable type printing process](https://en.wikipedia.org/wiki/_Printing_press#The_Printing_Revolution), the atomic bomb and [the Internet](https://en.wikipedia.org/wiki/Internet#Social_Impacts). Since technological revolutions have brought many of the major revolutions in human history and have very large long-term consequences, ethical assessment of the desirability of possible future technological revolutions is a very important topic. [Nick Bostrom](https://www.lesswrong.com/tag/nick-bostrom) shows that since the [Age of Enlightenment](https://en.wikipedia.org/wiki/Age_of_Enlightenment), freedom from society judgment and ethical evaluations has been seen as a necessary condition for science and technological developments[^1^](#fn1). Recently, as negative impacts of technological developments became evident, many theorists have expressed concern with this view. Henceforth, an ethical analysis of technological development has been called for and this gave birth to new research fields such as bioethics, computer ethics, neuroethics and nanoethics.\n\nHowever, Bostrom argues that an *“Ethical assessment in the incipient stages of a potential technological revolution faces several difficulties, including the unpredictability of their long‐term impacts, the problematic role of human agency in bringing them about, and the fact that technological revolutions rewrite not only the material conditions of our existence but also reshape culture and even – perhaps– human nature”*. The potential power to directly rewrite human nature through technology is complete new. All past technological revolutions have been brought by altering the way we relate to the natural world, not to ourselves. For the first time fundamental constants of human nature (e.g.: mortality, memory, cognition, mood and physical capacities) can be subject to change[^2^](#fn2). Other technological changes, such as a possible [Singularity](https://www.lesswrong.com/tag/singularity), promise not only a complete change in human nature but in the long term fate of sentient beings.\n\nMany economists and sociologists have dedicated several papers to try to model and predict the social changes from possible future technologies. [Robin Hanson](https://www.lesswrong.com/tag/robin-hanson) has made an analysis of how [whole brain emulation](https://www.lesswrong.com/tag/whole-brain-emulation) techonologies would deliver a [malthusian scenario](https://www.lesswrong.com/tag/malthusian-scenarios) of subsistence-level existence[^3^](#fn3). He also constructs a [prediction market](https://www.lesswrong.com/tag/prediction-markets) given machine intelligence and argues for a dramatic wage fall in the future [^4^](#fn4). [James Hughes](https://en.wikipedia.org/wiki/James_Hughes_(sociologist)) has argue that only a [democratic transhumanism](https://en.wikipedia.org/wiki/Democratic_transhumanism) that ensures a liberal, equal and safe use of future technologies can bring all the potentialities of technological development [^5^](#fn5). Francesco Caselli demonstrates that technological revolutions can worsen inequality by bringing *\"absolute gains for those individuals with high cognitive ability, and absolute losses for those with high costs of learning\"*[^6^](#fn6), but also provide an incentive for everyone to join the 'learning pool' of cognitive skilled individuals.\n\nReferences\n----------\n\n1.  Bostrom, Nick. (2010) \"Technological Revolutions: Ethics and Policy in the Dark\" Nanoscale: Issues and Perspectives for the Nano Century, eds. Nigel M. de S. Cameron and M. Ellen Mitchell (John Wiley, 2010): pp. 129-152. Available at: [http://www.nickbostrom.com/revolutions.pdf](http://www.nickbostrom.com/revolutions.pdf)[↩](#fnref1)\n2.  Savulescu, Julian & Meulen, Rudd ter (orgs.)(2011) “Enhancing Human Capacities”. Wiley-Blackwell.[↩](#fnref2)\n3.  Hanson, Robin. (1994) \"IF UPLOADS COME FIRST: The crack of a future dawn\" Extropy 6:2 (1994). Available at: [http://hanson.gmu.edu/uploads.html](http://hanson.gmu.edu/uploads.html)[↩](#fnref3)\n4.  Hanson, Robin. \"Economic Growth Given Machine Intelligence\" [http://hanson.gmu.edu/aigrow.pdf](http://hanson.gmu.edu/aigrow.pdf)[↩](#fnref4)\n5.  Hughes, James (2004). \"Citizen Cyborg: Why Democratic Societies Must Respond to the Redesigned Human of the Future.\" Westview Press[↩](#fnref5)\n6.  Caselli, Francesco. (1999) \"Technological Revolutions.\" American Economic Review, 89(1): 78–102. DOI:10.1257/aer.89.1.78.Availabe at: [http://personal.lse.ac.uk/casellif/papers/techrev.pdf](http://personal.lse.ac.uk/casellif/papers/techrev.pdf)[↩](#fnref6)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb2d0",
    "name": "Emulation Argument For Human-Level AI",
    "core": null,
    "slug": "emulation-argument-for-human-level-ai",
    "oldSlugs": null,
    "postCount": 0,
    "description": {
      "markdown": "The **Emulation argument for human-level** [**AI**](https://wiki.lesswrong.com/wiki/AGI) argues that since [whole brain emulation](https://www.lesswrong.com/tag/whole-brain-emulation) seems feasible then human-level [AI](https://wiki.lesswrong.com/wiki/AGI) must also be feasible. There are many underlying assumptions in the argument, most of them are explored by Chalmers (2010)[^1^](#fn1). Perhaps the most debated premise is holding that a brain emulation could have a consciousness mind or that consciousness isn’t fundamental to human intelligence. Chalmers [^2^](#fn2) formalized the argument as follows:”\n\n*   (i) The human brain is a machine.\n*   (ii) We will have the capacity to emulate this machine (before long).\n*   (iii) If we emulate this machine, there will be AI.\n*   (iv) Absent defeaters, there will be AI (before long)”\n\nReferences\n----------\n\n1.  CHALMERS, David. (2010) \"The Singularity: A Philosophical Analysis, Journal of Consciousness Studies\", 17 (9-10), pp. 7-65.[↩](#fnref1)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb11a",
    "name": "Belief Update",
    "core": null,
    "slug": "belief-update",
    "oldSlugs": null,
    "postCount": 0,
    "description": {
      "markdown": "What you do to your beliefs, opinions and cognitive structure when new [evidence](https://www.lesswrong.com/tag/evidence) comes along.\n\n[Cox's theorem](https://wiki.lesswrong.com/wiki/Cox's_theorem) says, roughly, that if your beliefs at any given time take the form of an assignment of a numerical \"plausibility score\" to every proposition, and if they satisfy a few plausible axioms, then your plausibilities must effectively be probabilities obeying the usual laws of probability theory, and your updating procedure must be the one implied by [Bayes' theorem](https://www.lesswrong.com/tag/bayes-theorem).\n\nSee also\n--------\n\n*   [Evidence](https://www.lesswrong.com/tag/evidence), [Conservation of expected evidence](https://www.lesswrong.com/tag/conservation-of-expected-evidence)\n*   [Belief](https://www.lesswrong.com/tag/belief)\n*   [Beliefs require observations](https://www.lesswrong.com/tag/beliefs-require-observations)\n*   [Bayes' theorem](https://www.lesswrong.com/tag/bayes-theorem)\n*   [Updateless decision theory](https://www.lesswrong.com/tag/updateless-decision-theory)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb1cd",
    "name": "Really Powerful Optimization Process",
    "core": null,
    "slug": "really-powerful-optimization-process",
    "oldSlugs": null,
    "postCount": 0,
    "description": {
      "markdown": "The term [artificial intelligence](https://www.lesswrong.com/tag/artificial-general-intelligence) can have [anthropomorphic](https://www.lesswrong.com/tag/anthropomorphism) connotations. In some contexts, it might be useful to speak of a **really powerful optimization process** rather than a *superintelligence*. An AI that was nonanthropomorphic and nonsentient could theoretically still be a very powerful device that could drastically affect the future in precise ways.\n\nBlog posts\n----------\n\n*   [Dreams of Friendliness](http://lesswrong.com/lw/tj/dreams_of_friendliness/)\n*   [Aiming at the Target](http://lesswrong.com/lw/v9/aiming_at_the_target/)\n*   [Efficient Cross-Domain Optimization](http://lesswrong.com/lw/vb/efficient_crossdomain_optimization/)\n*   [Nonsentient Optimizers](http://lesswrong.com/lw/x5/nonsentient_optimizers/)\n*   [The Design Space of Minds-in-General](http://lesswrong.com/lw/rm/the_design_space_of_mindsingeneral/)\n\nExternal links\n--------------\n\n*   [Creature or Technology](http://www.acceleratingfuture.com/steven/?p=227) by [Steven Kaas](https://www.lesswrong.com/tag/steven-kaas)\n*   [The Stamp Collecting Device](http://intelligence.org/blog/2007/06/11/the-stamp-collecting-device/) by Nick Hay\n\nSee also\n--------\n\n*   [Optimization process](https://www.lesswrong.com/tag/optimization), [configuration space](https://www.lesswrong.com/tag/configuration-space)\n*   [Artificial general intelligence](https://www.lesswrong.com/tag/artificial-general-intelligence), [singleton](https://www.lesswrong.com/tag/singleton)\n*   [Friendly Artificial Intelligence](https://www.lesswrong.com/tag/friendly-artificial-intelligence)\n*   [Anthropomorphism](https://www.lesswrong.com/tag/anthropomorphism), [alien values](https://www.lesswrong.com/tag/alien-values)\n*   [Evolution as alien god](https://www.lesswrong.com/tag/evolution-as-alien-god)\n*   [Complexity of value](https://www.lesswrong.com/tag/complexity-of-value)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb176",
    "name": "Futility of Chaos",
    "core": null,
    "slug": "futility-of-chaos",
    "oldSlugs": null,
    "postCount": 0,
    "description": {
      "markdown": "A complex of related ideas having to do with the **impossibility of generating useful work from entropy** — a position which holds **against** the ideas that e.g:\n\n*   Our artistic creativity stems from the noisiness of human neurons;\n*   Randomized algorithms can exhibit performance inherently superior to deterministic algorithms;\n*   The human brain is a chaotic system and this explains its power; non-chaotic systems cannot exhibit intelligence.\n\nBlog posts\n----------\n\n*   [Chaotic Inversion](http://lesswrong.com/lw/wb/chaotic_inversion/)\n*   [The Futility of Emergence](http://lesswrong.com/lw/iv/the_futility_of_emergence/)\n*   [Say Not \"Complexity\"](http://lesswrong.com/lw/ix/say_not_complexity/)\n*   [The \"Outside the Box\" Box](http://lesswrong.com/lw/k6/the_outside_the_box_box/)\n*   [The Wonder of Evolution](http://lesswrong.com/lw/ks/the_wonder_of_evolution/)\n*   [Worse Than Random](http://lesswrong.com/lw/vp/worse_than_random/)\n*   [The Weighted Majority Algorithm](http://lesswrong.com/lw/vq/the_weighted_majority_algorithm/)\n*   [Lawful Creativity](http://lesswrong.com/lw/vm/lawful_creativity/)\n*   [Lawful Uncertainty](http://lesswrong.com/lw/vo/lawful_uncertainty/)\n*   [Selling Nonapples](http://lesswrong.com/lw/vs/selling_nonapples/)\n*   [Logical or Connectionist AI?](http://lesswrong.com/lw/vv/logical_or_connectionist_ai/)\n*   [Failure by Affective Analogy](http://lesswrong.com/lw/vy/failure_by_affective_analogy/)\n*   [Artificial Mysterious Intelligence](http://lesswrong.com/lw/wk/artificial_mysterious_intelligence/)\n\nSee also\n--------\n\n*   [Mysterious Answers to Mysterious Questions](https://www.lesswrong.com/tag/mysterious-answers-to-mysterious-questions)\n*   [Reductionism](https://www.lesswrong.com/tag/reductionism)\n*   [Magic](https://www.lesswrong.com/tag/magic), [magical categories](https://www.lesswrong.com/tag/magical-categories)\n*   [Lawful intelligence](https://www.lesswrong.com/tag/lawful-intelligence)\n*   [Reality is normal](https://www.lesswrong.com/tag/reality-is-normal)\n*   [Free will](https://www.lesswrong.com/tag/free-will)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb180",
    "name": "Magical Categories",
    "core": null,
    "slug": "magical-categories",
    "oldSlugs": null,
    "postCount": 0,
    "description": {
      "markdown": "A \"magical category\" is an English word which, although it _sounds_ simple - hey, it's [just one word](https://wiki.lesswrong.com/wiki/Minimum_message_length), right? - is actually not simple, and furthermore, may be applied in a complicated way that drags in other considerations.\n\nIn Yudkowsky's work on [Friendly AI](https://wiki.lesswrong.com/wiki/Friendly_AI), the notion of \"magical categories\" is particularly important with respect to English words that are supposed to describe morality and imperatives. Suppose you say, for example, that _\"pleasure\"_ is good. You should instruct an AI to _make people happy_. \"All right,\" says the wise Friendly AI researcher, \"so the AI inserts an electrode into your pleasure center to make you happy forever*; or better yet, it disassembles you and uses your atoms to construct many tiny agents that are just complex enough to experience pleasure, and makes all of them happy.\"\n\n\"What?\" you cry indignantly. \"_That's not what I meant!_ That's not _true_ happiness!\"\n\nAnd if you try to unpack the concept of \"true\" happiness - that is, happiness which qualifies for being valuable - then you end up with a highly complicated [Fun Theory](https://www.lesswrong.com/tag/fun-theory). So the word \"happiness\" actually turned out to have a [complicated border](https://www.lesswrong.com/tag/superexponential-conceptspace); you would have to draw a _squiggly surface_ in the space of possibilities, to capture everything you meant by \"happiness\" and exclude everything you didn't mean.\n\nOr suppose that your mother's house was on fire, and so you wished to a literal-minded genie to \"get my mother out of the house\". The genie said: \"What do you mean by that? Can you specify it mathematically?\" And you replied: \"Increase the distance between the following contiguous physical entity, designated 'Mom', and the center of the following house.\" So the genie takes your mother and throws her out of the Solar System.\n\nIf you were talking to a _human firefighter_, it wouldn't occur to him, any more than to you, to cause your mother to be outside the house by, say, exploding the gas main and sending her flaming form hurtling high into the air. The firefighter understands explicitly that, in _addition_ to wanting to increase the distance between your mother and the center of the burning house, you want your mother to _survive_... in good physical condition, and so on. The firefighter, to some extent, shares your values; and so the _invisible additional_ considerations that you're dragging into the problem, are also shared by the firefighter, and so can be omitted from your _verbal instructions_.\n\nPhysical brains are not powerful enough to search all possibilities; we have to cut down the search space to possibilities that are likely to be good. Most of the \"obviously bad\" ways to get one's mother out of a burning building - those that would end up violating our other values, and so ranking very low in our preference ordering - do not even _occur to us as possibilities_. When we tell the firefighter \"My mother's in there! Get her out!\" it doesn't occur to us, or to the firefighter, to destroy the building with high explosives. And so we don't realize that, in addition to the distance between our mother and the house, we're also dragging in highly complicated considerations having to do with the value of survival and good health, and what exactly constitutes being alive and healthy...\n\nMain posts\n----------\n\n*   [The Hidden Complexity of Wishes](http://lesswrong.com/lw/ld/the_hidden_complexity_of_wishes/)\n*   [Magical Categories](http://lesswrong.com/lw/td/magical_categories/)\n\nSee also\n--------\n\n*   [Complexity of value](https://www.lesswrong.com/tag/complexity-of-value)\n*   [Fake simplicity](https://www.lesswrong.com/tag/fake-simplicity), [Detached lever fallacy](https://www.lesswrong.com/tag/detached-lever-fallacy)\n*   [Mind design space](https://www.lesswrong.com/tag/mind-design-space), [Paperclip maximizer](https://www.lesswrong.com/tag/paperclip-maximizer)\n*   [Friendly artificial intelligence](https://www.lesswrong.com/tag/friendly-artificial-intelligence)\n*   [Magic](https://www.lesswrong.com/tag/magic)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb1a8",
    "name": "Alien Values",
    "core": null,
    "slug": "alien-values",
    "oldSlugs": null,
    "postCount": 0,
    "description": {
      "markdown": "There are no rules requiring minds to value life, liberty or the pursuit of happiness. An alien will have, in all probability, **alien values**. If an \"alien\" isn't evolved, the [range of possible values](https://www.lesswrong.com/tag/mind-design-space) increases even more, allowing such [absurdities](https://www.lesswrong.com/tag/absurdity-heuristic) as a [Paperclip maximizer](https://www.lesswrong.com/tag/paperclip-maximizer). Creatures with alien values might as well value *only* non-sentient life, or they might spend all their time building [heaps of prime numbers of rocks](http://lesswrong.com/lw/sy/sorting_pebbles_into_correct_heaps/).\n\n*Related tags:* [Complexity of Value,](https://www.lesswrong.com/tag/complexity-of-value) Orthogonality Thesis, [Mind design space](https://www.lesswrong.com/tag/mind-design-space), [Paperclip Maximizer,](https://www.lesswrong.com/tag/paperclip-maximizer) Ethics & Morality\n\nBlog posts\n----------\n\n*   [Humans in Funny Suits](http://lesswrong.com/lw/so/humans_in_funny_suits/)\n*   [Sorting Pebbles Into Correct Heaps](http://lesswrong.com/lw/sy/sorting_pebbles_into_correct_heaps/)\n\nSee also\n--------\n\n*   [Giant cheesecake fallacy](https://www.lesswrong.com/tag/giant-cheesecake-fallacy)\n*   [Complexity of value](https://www.lesswrong.com/tag/complexity-of-value)\n*   [Mind design space](https://www.lesswrong.com/tag/mind-design-space), [Paperclip maximizer](https://www.lesswrong.com/tag/paperclip-maximizer)\n*   [Anthropomorphism](https://www.lesswrong.com/tag/anthropomorphism), [Human universal](https://www.lesswrong.com/tag/human-universal)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb239",
    "name": "Utilitronium",
    "core": null,
    "slug": "utilitronium",
    "oldSlugs": null,
    "postCount": 0,
    "description": {
      "markdown": "**Utilitronium** is relatively homogeneous matter optimized for maximum [utility](https://www.lesswrong.com/tag/utility-functions) (like [computronium](https://en.wikipedia.org/wiki/Computronium) is optimized for maximum computing power). For a [paperclip maximizer](https://www.lesswrong.com/tag/paperclip-maximizer), utilitronium is paperclips. For more [complex values](https://www.lesswrong.com/tag/complexity-of-value), no homogeneous organization of matter will have optimal utility.\n\nUtilitronium shockwave is a process of converting all matter in the universe into utilitronium as quickly as possible, which would look like a shockwave of utilitronium spreading outwards from the point of origin, presumably nearly at the speed of light.\n\nSee also\n--------\n\n*   [Utility function](https://www.lesswrong.com/tag/utility-functions)\n*   [Hedon](https://www.lesswrong.com/tag/hedon), [Utils](https://wiki.lesswrong.com/wiki/Utils)\n*   [Paperclip maximizer](https://www.lesswrong.com/tag/paperclip-maximizer), [Complexity of value](https://www.lesswrong.com/tag/complexity-of-value)\n*   [Hedonium](https://wiki.lesswrong.com/wiki/Hedonium), the hedonistic utilitarian version of utilitronium"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb1dd",
    "name": "Aumann Agreement",
    "core": null,
    "slug": "aumann-agreement",
    "oldSlugs": null,
    "postCount": 0,
    "description": {
      "markdown": "[**Aumann's agreement theorem**](https://www.lesswrong.com/tag/aumann-s-agreement-theorem) states that [Bayesian](https://www.lesswrong.com/tag/bayes-theorem) reasoners with [common priors](https://www.lesswrong.com/tag/common-priors) and [common knowledge](https://www.lesswrong.com/tag/common-knowledge) of each other's opinions *cannot* agree to [disagree](https://www.lesswrong.com/tag/disagreement). This has enormous intuitive implication on the human practice of rationality. Consider: if *I'm* an honest seeker of truth, and *you're* an honest seeker of truth, and we *believe* each other to be honest, then we can update on each other's opinions and quickly reach **agreement**. Unless you think I'm so irredeemably irrational that my opinions *anti*correlate with truth, then the very fact that I believe something is Bayesian evidence that that something is true, and you should take that into account when forming your belief. Likewise, fellow rationalists should update their beliefs on your beliefs, *not* as a social custom or personal courtesy, but simply because your rational belief really *is* Bayesian evidence about the state of the world, in the same way that a photograph or a reference book is evidence about the state of the world.\n\nThe process of true Bayesians coming to agreement bears precious little resemblance to a typical human argument. One agent states her estimate, the other agent states her estimate, conditioned on the first agent's estimate, and from there the agents' opinions follow an unbiased random walk: at no point in a conversation can Bayesians have common knowledge that they will disagree.\n\nThe fact that disagreements on questions of simple fact are so common amongst humans, and that people seem to think this is *normal*, is an observation that should [strike fear into the heart](https://www.lesswrong.com/tag/no-safe-defense) of every aspiring rationalist.\n\nBlog posts\n----------\n\n*   [We Can't Forsee to Disagree](http://www.overcomingbias.com/2007/01/we_cant_foresee.html) by [Robin Hanson](https://www.lesswrong.com/tag/robin-hanson)\n\nSee also\n--------\n\n*   [Aumann's agreement theorem](https://www.lesswrong.com/tag/aumann-s-agreement-theorem)\n*   [Disagreement](https://www.lesswrong.com/tag/disagreement)\n*   [Conservation of expected evidence](https://www.lesswrong.com/tag/conservation-of-expected-evidence)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb1e5",
    "name": "Carl Shulman",
    "core": null,
    "slug": "carl-shulman",
    "oldSlugs": null,
    "postCount": 0,
    "description": {
      "markdown": "**Carl Shulman** is a Research Fellow at the [Machine Intelligence Research Institute](https://www.lesswrong.com/tag/machine-intelligence-research-institute-miri) who has authored and co-authored several papers on AI risk, including:\n\n*   “How Hard is Artificial Intelligence? Evolutionary Arguments and Selection Effects” [1](http://www.nickbostrom.com/aievolution.pdf), a analysis of the implications of the [Observation selection effect](https://www.lesswrong.com/tag/observation-selection-effect) on the [Evolutionary argument for human-level AI](https://www.lesswrong.com/tag/evolutionary-argument-for-human-level-ai)\n*   ”Whole Brain Emulation and the Evolution of Superorganisms”[2](http://intelligence.org/files/WBE-Superorgs.pdf), argues for the existence of pressures favoring the emergence of increased coordination between [emulated brains](https://www.lesswrong.com/tag/whole-brain-emulation), in the form of superorganisms.\n*   ”Implications of a Software-Limited Singularity”[3](http://intelligence.org/files/SoftwareLimited.pdf), argues for the high probability of a human-level AI before 2060.\n\nPreviously, he worked at Clarium Capital Management, a global macro hedge fund, and at the law firm Reed Smith LLP. He attended New York University School of Law and holds a BA in philosophy from Harvard University.\n\nSee Also\n--------\n\n*   [Timeline of Carl Shulman publications](http://lesswrong.com/lw/7ob/timeline_of_carl_shulman_publications/)\n    *   [More up-to-date and comprehensive timeline of publications](https://timelines.issarice.com/wiki/Timeline_of_Carl_Shulman_publications)\n*   [80,000 Hours Carl Shulman’s profile](http://80000hours.org/members/carl-shulman)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb1a9",
    "name": "The Aumann Game",
    "core": null,
    "slug": "the-aumann-game",
    "oldSlugs": null,
    "postCount": 0,
    "description": {
      "markdown": "Created by Steven Kaas. From its [original formulation](https://web.archive.org/web/20110408182936/http://www.acceleratingfuture.com/steven/?p=96):\n\n> Aumann’s agreement theorem says that Bayesian agents cannot “agree to disagree” — their subjective probabilities must be identical if they are common knowledge. This is true regardless of differences in private knowledge. When agents take turns stating their estimates, updating each time based on the information contained in the other’s estimate, private knowledge will “leak out” and the probabilities will [converge to an equilibrium](https://web.archive.org/web/20110408182936/http://www.scottaaronson.com/papers/agree-econ.pdf).\n> \n> This theorem makes some big assumptions. One is common knowledge of honesty. Another is common priors. Another is common knowledge of Bayesianity. However, Robin Hanson has shown that [uncommon priors require origin disputes](https://web.archive.org/web/20110408182936/http://hanson.gmu.edu/prior.pdf), and has discussed agents who are “[Bayesian wannabes](https://web.archive.org/web/20110408182936/http://dimacs.rutgers.edu/Workshops/Bounded/hanson.pdf)” but not Bayesians.\n> \n> It may be interesting to see how this process plays out with real humans in a simplified test bed. Below are 25 statements.\n> \n> To play, for each statement, you have to say your honest subjective probability that it’s true. Make sure to take into account the estimates of previous commenters. You are strongly encouraged to **post estimates multiple times**, showing how the estimates of others have caused yours to change. We will then see whether, as the theorem suggests, everyone’s estimates converge to the same equilibrium over time, and whether that equilibrium is any good.\n\nScott Garrabrant (seemingly) independently invented a [very similar game](https://www.lesswrong.com/posts/nmwog5hGidZniDDpR/aumann-agreement-game), but designed for in-person play rather than playing online via comment section. It has printable instructions, including a convenient scoring table for an adjusted Bayes score which (a) is a proper scoring rule, and (b) is relatively fair (ie, minimizes the importance of whether you happen to get correct or incorrect answers).\n\nSee also\n--------\n\n*   [Aumann's agreement theorem](https://www.lesswrong.com/tag/aumann-s-agreement-theorem)\n*   [Proper scoring rules](https://wiki.lesswrong.com/wiki/Proper_scoring_rules)\n*   [Paranoid debating](https://www.lesswrong.com/tag/paranoid-debating)\n\nExternal links\n--------------\n\n*   [The Aumann Game](https://web.archive.org/web/20110408182936/http://www.acceleratingfuture.com/steven/?p=96)\n*   [The New Adventures of Aumann](https://web.archive.org/web/20130906125721/http://www.acceleratingfuture.com/steven/?p=102)\n*   [Bayes Bayes Revolution, Aumania](https://web.archive.org/web/20080514160909/http://www.acceleratingfuture.com/steven/?p=103)\n*   [Aumann Forever](https://web.archive.org/web/20100314215830/http://www.acceleratingfuture.com/steven/?p=106)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb21c",
    "name": "The Fun Theory Sequence",
    "core": null,
    "slug": "the-fun-theory-sequence",
    "oldSlugs": null,
    "postCount": 0,
    "description": {
      "markdown": "A concrete theory of transhuman values. How much fun is there in the universe; will we ever run out of fun; are we having fun yet; could we be having more fun. Part of the [complexity of value](https://www.lesswrong.com/tag/complexity-of-value) thesis. Also forms part of the fully general answer to religious [theodicy](https://wiki.lesswrong.com/wiki/theodicy).\n\nA guide to this sequence is available at [The Fun Theory Sequence (post)](http://lesswrong.com/lw/xy/the_fun_theory_sequence/).\n\n*   [Prolegomena to a Theory of Fun](http://lesswrong.com/lw/wv/prolegomena_to_a_theory_of_fun/)\n*   [High Challenge](http://lesswrong.com/lw/ww/high_challenge/)\n*   [Complex Novelty](http://lesswrong.com/lw/wx/complex_novelty/)\n*   [Continuous Improvement](http://lesswrong.com/lw/xk/continuous_improvement/)\n*   [Sensual Experience](http://lesswrong.com/lw/wy/sensual_experience/)\n*   [Living By Your Own Strength](http://lesswrong.com/lw/wz/living_by_your_own_strength/)\n*   [Free to Optimize](http://lesswrong.com/lw/xb/free_to_optimize/)\n*   [Harmful Options](http://lesswrong.com/lw/x2/harmful_options/)\n*   [Devil's Offers](http://lesswrong.com/lw/x3/devils_offers/)\n*   [Nonperson Predicates](http://lesswrong.com/lw/x4/nonperson_predicates/)\n*   [Amputation of Destiny](http://lesswrong.com/lw/x8/amputation_of_destiny/)\n*   [Dunbar's Function](http://lesswrong.com/lw/x9/dunbars_function/)\n*   [In Praise of Boredom](http://lesswrong.com/lw/xr/in_praise_of_boredom/)\n*   [Sympathetic Minds](http://lesswrong.com/lw/xs/sympathetic_minds/)\n*   [Interpersonal Entanglement](http://lesswrong.com/lw/xt/interpersonal_entanglement/)\n*   [Failed Utopia #4-2](http://lesswrong.com/lw/xu/failed_utopia_42/)\n*   [Growing Up is Hard](http://lesswrong.com/lw/xd/growing_up_is_hard/)\n*   [Changing Emotions](http://lesswrong.com/lw/xe/changing_emotions/)\n*   [Emotional Involvement](http://lesswrong.com/lw/xg/emotional_involvement/)\n*   [Serious Stories](http://lesswrong.com/lw/xi/serious_stories/)\n*   [Eutopia is Scary](http://lesswrong.com/lw/xl/eutopia_is_scary/)\n*   [Building Weirdtopia](http://lesswrong.com/lw/xm/building_weirdtopia/)\n*   [Justified Expectation of Pleasant Surprises](http://lesswrong.com/lw/xo/justified_expectation_of_pleasant_surprises/)\n*   [Seduced by Imagination](http://lesswrong.com/lw/xp/seduced_by_imagination/)\n*   [The Uses of Fun (Theory)](http://lesswrong.com/lw/xc/the_uses_of_fun_theory/)\n*   [Higher Purpose](http://lesswrong.com/lw/xw/higher_purpose/)\n\nSee also\n--------\n\n*   [Fun theory](https://www.lesswrong.com/tag/fun-theory)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb336",
    "name": "Abolitionism",
    "core": null,
    "slug": "abolitionism",
    "oldSlugs": null,
    "postCount": 0,
    "description": {
      "markdown": "**Abolitionism** is a school of [transhumanism](https://www.lesswrong.com/tag/transhumanism) which advocates the eradication of suffering (both human and non-human) through biotechnology. David Pearce is the founder of the movement, and proposes using genetic engineering and other emerging technologies to replace Darwinian suffering-based motivational systems with \"gradients of intelligent bliss\". In his manifesto *The Hedonistic Imperative*, he discusses the practicality, philosophical basis, and sociological impact of the abolition of suffering, as well as the possibility of future states of well-being of greater quality than any that have existed so far. Pearce stresses that the abolition of suffering would be compatible with more sophisticated forms of well-being and \"intentional objects\" (i.e. what you are happy \"about\"), and need not involve maxed-out [wireheading](https://www.lesswrong.com/tag/wireheading).\n\nIn his post [Serious Stories](http://lesswrong.com/lw/xi/serious_stories/), part of [The Fun Theory Sequence](https://www.lesswrong.com/tag/the-fun-theory-sequence), Eliezer Yudkowsky argues that removing all forms of suffering would make the world boring, just as a story without conflict is boring. Instead, Yudkowsky proposes to abolish only intolerable suffering, while preserving the ability to experience mild sorrow.\n\nAddressing this type of objection to “paradise engineering”, Pearce [writes](https://www.hedweb.com/hedethic/hedon4.htm#boring) that, while the prospect of perpetual intelligent bliss may sound unexciting, boredom can be abolished or replaced with its functional analogs that don’t involve aversive qualia. Like any other psychophysical state, boredom can be optional once its biochemical substrates are identified. Pearce also notes that even if human descendants opt into indiscriminate bliss, they will not get bored, for, as intracranial stimulation has evidenced, pure pleasure has no tolerance and “never palls”.\n\nSee also\n--------\n\n*   [Hedonism](https://www.lesswrong.com/tag/hedonism)\n*   [Suffering risk](https://www.lesswrong.com/tag/risks-of-astronomical-suffering-s-risks)\n*   [Center on Long-Term Risk](https://www.lesswrong.com/tag/center-on-long-term-risk-clr)\n*   [Utilitarianism](https://www.lesswrong.com/tag/utilitarianism)\n*   [Hedonium](https://wiki.lesswrong.com/wiki/Hedonium)\n\nExternal links\n--------------\n\n*   [*The Hedonistic Imperative*](http://hedweb.org/)\n*   [Pearce's talk about abolitionism at the Future of Humanity Institute](http://abolitionist.com)\n*   *Can Biotechnology Abolish Suffering?* ([free](https://www.smashwords.com/books/view/748028) & [Amazon](https://www.amazon.com/Can-Biotechnology-Abolish-Suffering/dp/B07WS4CBN4)), a collection of Pearce's more recent essays\n*   [Abolitionism on H+Pedia](https://hpluspedia.org/wiki/Abolitionism)\n*   [David Pearce on abolishing suffering](https://www.youtube.com/watch?v=_VCb9sk6CTc) (video)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb361",
    "name": "Litany of Occam",
    "core": null,
    "slug": "litany-of-occam",
    "oldSlugs": null,
    "postCount": 0,
    "description": {
      "markdown": "It is more probable that A, than that A and B.\n\nSee also\n--------\n\n*   [Occam's razor](https://www.lesswrong.com/tag/occam-s-razor)\n*   [Conjunction fallacy](https://www.lesswrong.com/tag/conjunction-fallacy)\n*   [Burdensome details](https://www.lesswrong.com/tag/burdensome-details)\n*   [Litany of Tarski](https://www.lesswrong.com/tag/litany-of-tarski)\n*   [Litany of Gendlin](https://www.lesswrong.com/tag/litany-of-gendlin)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb362",
    "name": "Litany of Hodgell",
    "core": null,
    "slug": "litany-of-hodgell",
    "oldSlugs": null,
    "postCount": 0,
    "description": {
      "markdown": "The **Litany of Hodgell**: That which can be destroyed by the truth should be.\n\nAnd [Yudkowsky's extension](https://www.lesswrong.com/posts/SqF8cHjJv43mvJJzx/feeling-rational): That which the truth nourishes should thrive. \n\nSee also\n--------\n\n*   [Virtues of Rationality](https://www.lesswrong.com/tag/12-virtues)\n*   [Litany of Tarski](https://www.lesswrong.com/tag/litany-of-tarski)\n*   [Litany of Gendlin](https://www.lesswrong.com/tag/litany-of-gendlin)\n*   [Litany of Occam](https://www.lesswrong.com/tag/litany-of-occam)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb2ea",
    "name": "Highly Advanced Epistemology 101 For Beginners",
    "core": null,
    "slug": "highly-advanced-epistemology-101-for-beginners",
    "oldSlugs": null,
    "postCount": 0,
    "description": {
      "markdown": "[Eliezer Yudkowsky](https://www.lesswrong.com/tag/eliezer-yudkowsky)'s bottom-up guide to epistemology. Includes practical applications and puzzling [meditations](https://www.lesswrong.com/tag/meditation-koan).\n\n**Intro**\n\n*   [The Useful Idea of Truth](http://lesswrong.com/lw/eqn/the_useful_idea_of_truth/)\n*   [Rationality: Appreciating Cognitive Algorithms](http://lesswrong.com/lw/eta/rationality_appreciating_cognitive_algorithms/)\n*   Minor Post - [Skill: The Map is Not the Territory](http://lesswrong.com/lw/erp/skill_the_map_is_not_the_territory/)\n*   Minor Post - [Firewalling the Optimal from the Rational](http://lesswrong.com/lw/etf/firewalling_the_optimal_from_the_rational/)\n\n**Causality/Physics**\n\n*   [The Fabric of Real Things](http://lesswrong.com/lw/eva/the_fabric_of_real_things/)\n*   [Causal Diagrams and Causal Models](http://lesswrong.com/lw/ev3/causal_diagrams_and_causal_models/)\n*   [Stuff That Makes Stuff Happen](http://lesswrong.com/lw/ezu/stuff_that_makes_stuff_happen/)\n*   [Causal Reference](http://lesswrong.com/lw/f1u/causal_reference/)\n*   [Causal Universes](http://lesswrong.com/lw/fok/causal_universes/)\n\n**Logic/Mathematics**\n\n*   [Proofs, Implications, and Models](http://lesswrong.com/lw/f43/proofs_implications_and_models/)\n*   [Logical Pinpointing](http://lesswrong.com/lw/f4e/logical_pinpointing/)\n*   [Standard and Nonstandard Numbers](http://lesswrong.com/lw/g0i/standard_and_nonstandard_numbers/)\n*   [Gödel's Completeness and Incompleteness Theorems](http://lesswrong.com/lw/g1y/godels_completeness_and_incompleteness_theorems/)\n*   [Second-Order Logic: The Controversy](http://lesswrong.com/lw/g7n/secondorder_logic_the_controversy/)\n\n**Mixed Reference**\n\n*   [Mixed Reference: The Great Reductionist Project](http://lesswrong.com/lw/frz/mixed_reference_the_great_reductionist_project/)\n*   [By Which It May Be Judged](http://lesswrong.com/lw/fv3/by_which_it_may_be_judged/)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb2b8",
    "name": "Simulation Argument",
    "core": null,
    "slug": "simulation-argument",
    "oldSlugs": null,
    "postCount": 0,
    "description": {
      "markdown": "The **Simulation argument** is an argument for the [simulation hypothesis](https://www.lesswrong.com/tag/simulation-hypothesis), which states we are living in a simulation. The concept was popularized in 2003 by Nick Bostrom's paper \"Are You Living in a Computer Simulation?\"[^1^](#fn1).\n\nIn this paper Bostrom constructs, in much more detail, the line of reasoning below. If posthuman civilizations have enough computing power to run hugely many ancestor‐simulations using a tiny fraction of their resources, it’s reasonable to assume they would certainly run many of such simulations. Therefore, there would be many more simulated minds than non-simulated. For instance, if they run 1000 complete simulations of our civilization, then there would be 1000 many more simulated minds then non-simulated ones. Hence, since simulated and non-simulated minds are subjectively indistinguishable, one doesn’t have any a priori reasons to think he is one or another. However, because there are many more simulated minds, one have strong reasons to believe he is probably been simulated. Bostrom then concludes that at least one of the following propositions must be true:\n\n1.  the human species is very likely to go extinct before reaching a “posthuman” stage\n2.  any posthuman civilization is extremely unlikely to run a significant number of simulations of their evolutionary history (or variations thereof)\n3.  we are almost certainly living in a computer simulation.\n\nIt follows that the belief that there is a significant chance that we will one day become posthumans who run ancestor simulations is false, unless we are currently living in a simulation.\n\nSee also\n--------\n\n*   [Simulation hypothesis](https://www.lesswrong.com/tag/simulation-hypothesis)\n*   [Nonperson predicate](https://www.lesswrong.com/tag/nonperson-predicate)\n\nExternal links\n--------------\n\n*   [Nick Bostrom's Simulation Argument Resource page](http://www.simulation-argument.com/)\n\nReferences\n----------\n\n*   Bostrom, Nick (2001,2003) [Are You Living in a Computer Simulation](http://www.simulation-argument.com/simulation.pdf) Philosophical Quarterly (2003) Vol. 53, No. 211, pp. 243‐255.\n*   Bostrom, Nick (2011) [A Patch for the Simulation Argument](http://www.simulation-argument.com/patch.pdf) Analysis, Vol. 71, No. 1 (2011): 54-61"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb2b8",
    "name": "Simulation Argument",
    "core": null,
    "slug": "simulation-argument",
    "oldSlugs": null,
    "postCount": 0,
    "description": {
      "markdown": "The **Simulation argument** is an argument for the [simulation hypothesis](https://www.lesswrong.com/tag/simulation-hypothesis), which states we are living in a simulation. The concept was popularized in 2003 by Nick Bostrom's paper \"Are You Living in a Computer Simulation?\"[^1^](#fn1).\n\nIn this paper Bostrom constructs, in much more detail, the line of reasoning below. If posthuman civilizations have enough computing power to run hugely many ancestor‐simulations using a tiny fraction of their resources, it’s reasonable to assume they would certainly run many of such simulations. Therefore, there would be many more simulated minds than non-simulated. For instance, if they run 1000 complete simulations of our civilization, then there would be 1000 many more simulated minds then non-simulated ones. Hence, since simulated and non-simulated minds are subjectively indistinguishable, one doesn’t have any a priori reasons to think he is one or another. However, because there are many more simulated minds, one have strong reasons to believe he is probably been simulated. Bostrom then concludes that at least one of the following propositions must be true:\n\n1.  the human species is very likely to go extinct before reaching a “posthuman” stage\n2.  any posthuman civilization is extremely unlikely to run a significant number of simulations of their evolutionary history (or variations thereof)\n3.  we are almost certainly living in a computer simulation.\n\nIt follows that the belief that there is a significant chance that we will one day become posthumans who run ancestor simulations is false, unless we are currently living in a simulation.\n\nSee also\n--------\n\n*   [Simulation hypothesis](https://www.lesswrong.com/tag/simulation-hypothesis)\n*   [Nonperson predicate](https://www.lesswrong.com/tag/nonperson-predicate)\n\nExternal links\n--------------\n\n*   [Nick Bostrom's Simulation Argument Resource page](http://www.simulation-argument.com/)\n\nReferences\n----------\n\n*   Bostrom, Nick (2001,2003) [Are You Living in a Computer Simulation](http://www.simulation-argument.com/simulation.pdf) Philosophical Quarterly (2003) Vol. 53, No. 211, pp. 243‐255.\n*   Bostrom, Nick (2011) [A Patch for the Simulation Argument](http://www.simulation-argument.com/patch.pdf) Analysis, Vol. 71, No. 1 (2011): 54-61"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb1a4",
    "name": "Universal Law",
    "core": null,
    "slug": "universal-law",
    "oldSlugs": null,
    "postCount": 0,
    "description": {
      "markdown": "**Universal law** is the idea that everything in reality always behaves according to the same uniform physical laws; there are no exceptions and no alternatives. The notion that there *is* such a thing as universal physical laws is one of the great, shocking discoveries of [science](https://www.lesswrong.com/tag/science). Just looking at the world from a human perspective, and knowing nothing of science, it is *not at all obvious* that underlying the chaotic world of tangible objects that we see, there should be a perfectly regular mathematically simple structure, and yet countless experiments have shown this to be the case.\n\n*Law* is arguably a misleading term here: it's not as if things are naturally free to do \"what they wish\" and the laws of physics are some external constraint imposed from above by force or edict. Rather, what we know as \"the laws of physics\" are just a compact way of expressing how we think things *really are* at the [lowest level](https://www.lesswrong.com/tag/reductionism). Our society's [current conception](https://www.lesswrong.com/tag/the-map-is-not-the-territory) of the laws of physics may be violated by some novel experiment under ([what we would call](https://www.lesswrong.com/tag/reality-is-normal)) exotic conditions, but that only means we were mistaken about the [true universal law](https://www.lesswrong.com/tag/map-and-territory), which can no more be \"violated\" than two and two can make five: it's just not how things work.\n\nBlog posts\n----------\n\n*   [Universal Fire](http://lesswrong.com/lw/hq/universal_fire/)\n*   [Universal Law](http://lesswrong.com/lw/hr/universal_law/)\n*   [Think Like Reality](http://lesswrong.com/lw/hs/think_like_reality/)\n*   [Leaky Generalizations](http://lesswrong.com/lw/lc/leaky_generalizations/)\n\nSee also\n--------\n\n*   [Lawful intelligence](https://www.lesswrong.com/tag/lawful-intelligence)\n*   [Reality is normal](https://www.lesswrong.com/tag/reality-is-normal)\n*   [Futility of chaos](https://www.lesswrong.com/tag/futility-of-chaos)\n*   [Reductionism](https://www.lesswrong.com/tag/reductionism)\n*   [Magic](https://www.lesswrong.com/tag/magic)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb1d4",
    "name": "Michael Anissimov",
    "core": null,
    "slug": "michael-anissimov",
    "oldSlugs": null,
    "postCount": 0,
    "description": {
      "markdown": "Michael Anissimov was a media director for the [Machine Intelligence Research Institute](https://www.lesswrong.com/tag/machine-intelligence-research-institute-miri) and a long-time supporter of efforts towards [Friendly artificial intelligence](https://www.lesswrong.com/tag/friendly-artificial-intelligence). He co-organized the Singularity Summit from 2009 to 2012.\n\n*   [Michael Anissimov's Home Page](http://www.acceleratingfuture.com/michael/blog/)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb1c5",
    "name": "Preference",
    "core": null,
    "slug": "preference",
    "oldSlugs": null,
    "postCount": 0,
    "description": {
      "markdown": "**Preference** is usually conceptualized as a set of attitudes or evaluations made by an agent towards a specific object, and it has been proposed that AI has a robust set of methods to deal with them. These can be divided in several steps:\n\n1.  **Preferences acquisition**: Extraction of preferences from a user, through an interactive learning system, e.g. a question-answer process.\n2.  **Preferences modeling**: After extraction, the goal is to create a mathematical model expressing the preferences, taking into account its properties (for instance, if the preferences are transitive between pairs of choices).\n3.  **Preferences representation**: With a robust model of preferences, it becomes necessary to develop a symbolic system to represent them - a preference representation language.\n4.  **Preferences reasoning**: Finally, having represented a user’s or agent’s preferences, it is possible to mine the data looking for new insights and knowledge. This could be used, for instance, to aggregate users based on preferences or as biases in decision processes and game theory scenarios.\n\nThis sequential chain of thought can be particularly useful when dealing with [Coherent Extrapolated Volition](https://www.lesswrong.com/tag/coherent-extrapolated-volition), as a way of systematically exploring agent’s goals and motivations.\n\nFurther Reading & References\n----------------------------\n\n*   [Would Your Real Preferences Please Stand Up?](http://lesswrong.com/lw/15c/would_your_real_preferences_please_stand_up) by [Yvain](https://wiki.lesswrong.com/wiki/Yvain)\n*   [Notion of Preference in Ambient Control](http://lesswrong.com/lw/2tq/notion_of_preference_in_ambient_control/) by [Vladimir Nesov](https://wiki.lesswrong.com/wiki/Vladimir_Nesov)\n*   [To What Degree Do We Have Goals?](http://lesswrong.com/lw/6oo/to_what_degree_do_we_have_goals/) by Yvain\n*   [A brief tutorial on preferences in AI](http://lesswrong.com/lw/a73/a_brief_tutorial_on_preferences_in_ai/) by Luke Muehlhauser\n\nSee also\n--------\n\n*   [Complexity of value](https://www.lesswrong.com/tag/complexity-of-value)\n*   [Utility function](https://www.lesswrong.com/tag/utility-functions)\n*   [Decision theory](https://www.lesswrong.com/tag/decision-theory)\n*   [Optimization process](https://www.lesswrong.com/tag/optimization)\n*   [Akrasia](https://www.lesswrong.com/tag/akrasia)\n*   [Corrupted hardware](https://www.lesswrong.com/tag/corrupted-hardware)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb213",
    "name": "The Utility Function Is Not Up For Grabs",
    "core": null,
    "slug": "the-utility-function-is-not-up-for-grabs",
    "oldSlugs": null,
    "postCount": 0,
    "description": {
      "markdown": "The constraints of decision theory are only there to help you win; they don't specify what constitutes a win. Rationality in and of itself cannot constrain what you want, except insofar as what you thought you wanted failed to reflect what you actually wanted (or was just plain inconsistent). Hence the saying: **the utility function is not up for grabs**.\n\nBlog posts\n----------\n\n*   [Newcomb's Problem and Regret of Rationality](http://lesswrong.com/lw/nc/newcombs_problem_and_regret_of_rationality/)\n*   [Something to Protect](http://lesswrong.com/lw/nb/something_to_protect/)\n*   [Rationality is Systematized Winning](http://lesswrong.com/lw/7i/rationality_is_systematized_winning/)\n\nSee also\n--------\n\n*   [Rationalists should win](https://wiki.lesswrong.com/wiki/Rationalists_should_win)\n*   [Something to protect](https://www.lesswrong.com/tag/something-to-protect)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb11d",
    "name": "Control Theory",
    "core": null,
    "slug": "control-theory",
    "oldSlugs": null,
    "postCount": 0,
    "description": {
      "markdown": "A control system is a device that keeps a variable at a certain value, despite only knowing what the current value of the variable is. An example is a cruise control, which maintains a certain speed, but only measures the current speed, and knows nothing of the system that produces that speed (wind, car weight, grade).\n\nBlog posts\n----------\n\n*   [What is control theory, and why do you need to know about it?](http://lesswrong.com/lw/dj/what_is_control_theory_and_why_do_you_need_to/) by [RichardKennaway](https://wiki.lesswrong.com/wiki/RichardKennaway)\n*   [Without models](http://lesswrong.com/lw/ek/without_models/) by [RichardKennaway](https://wiki.lesswrong.com/wiki/RichardKennaway)\n*   [Controlling your inner control circuits](http://lesswrong.com/lw/11h/controlling_your_inner_control_circuits/) by [Kaj_Sotala](https://wiki.lesswrong.com/wiki/Kaj_Sotala)\n\nSee also\n--------\n\n*   [Perceptual control theory](https://www.lesswrong.com/tag/perceptual-control-theory)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb248",
    "name": "Valid Argument",
    "core": null,
    "slug": "valid-argument",
    "oldSlugs": null,
    "postCount": 0,
    "description": {
      "markdown": "An argument is **valid** when it contains no logical fallacies. Such arguments are not necessarily [sound](https://www.lesswrong.com/tag/sound-argument), because the premises may be false. For instance, the following syllogism is valid but has a false conclusion:\n\n*   All animals are dogs. (False premise/All A are B.)\n*   All dogs are terriers. (False premise/All B are C.)\n*   Thereforse, all animals are terriers. (False conclusion: not all animals are terriers. Valid logic: If all A are B, and all B are C, then all A are C.)\n\nBlog posts\n----------\n\n*   [The Parable of Hemlock](http://lesswrong.com/lw/nf/the_parable_of_hemlock/) (tangentially related)\n\nSee also\n--------\n\n*   [Sound argument](https://www.lesswrong.com/tag/sound-argument)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb2a3",
    "name": "Instrumental Value",
    "core": null,
    "slug": "instrumental-value",
    "oldSlugs": null,
    "postCount": 0,
    "description": {
      "markdown": "An **instrumental value** is a value pursued for the purpose of achieving other values. Values which are pursued for their own sake are called [terminal values](https://www.lesswrong.com/tag/terminal-value).\n\nBlog posts\n----------\n\n*   [Terminal Values and Instrumental Values](http://lesswrong.com/lw/l4/terminal_values_and_instrumental_values/)\n\nSee also\n--------\n\n*   [Convergent instrumental goals](https://wiki.lesswrong.com/wiki/Convergent_instrumental_goals)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb197",
    "name": "Utility",
    "core": null,
    "slug": "utility",
    "oldSlugs": null,
    "postCount": 1,
    "description": {
      "markdown": "**Utility** is how much a certain outcome satisfies an agent’s preferences. Its unit – the **util** or **utilon** – is an abstract arbitrary measure that assumes a concrete value only when the agent’s preferences have been determined through a [utility function](https://www.lesswrong.com/tag/utility-functions).\n\nThe concept of utility stems from economics and [game theory](https://www.lesswrong.com/tag/game-theory), where it measures how much a certain commodity increases welfare. One of the clearest examples is money: the price that a person is willing to pay for something can be considered a measure of the strength of his or her preference for it. Thus, a willingness to pay a high sum for something implies that the person has a strong desire for it, i.e. it has a high utility for him or her.\n\nAlthough it has been argued that utility is hard to quantify in the case of humans - mainly due to the complexity of the causal roles played by preferences and motivations – utility-based agents are quite common in AI systems. Examples include [navigation systems](http://u.cs.biu.ac.il/~meshulr1/meshulam05.pdf) or [automated resources allocation models](http://www.diee.unica.it/biomed05/pdf/W22-104.pdf), where the agent has to choose the best action based on its expected utility.\n\nSome people prefer to keep [a distinction between two types of utility](https://www.lesswrong.com/posts/DQ4pyHoAKpYutXwSr/underappreciated-points-about-utility-functions-of-both): *utility as in decision theory* refers to the theoretical construct which represents a single agent's preferences, as defined by the [VNM Theorem](https://www.lesswrong.com/tag/vnm-theorem) or other decision-theoretic representation theorems (such as [Savage](https://www.lesswrong.com/posts/5J34FAKyEmqKaT7jt/a-summary-of-savage-s-foundations-for-probability-and) or [Jeffrey-Bolker](https://plato.stanford.edu/entries/decision-theory/#JefThe)), and *utility as in utilitarianism*, a cross-agent notion of welfare intended to capture ethical reasoning. One reason for keeping the two distinct is that [utility functions are not comparable](https://www.lesswrong.com/posts/cYsGrWEzjb324Zpjx/comparing-utilities), which means it is unclear how to use single-agent utility as a cross-agent concept. Another reason to keep the two concepts separate is that a utilitarian may have a concept of welfare of an agent which differs from an agent's own preferences. For example, hedonic utilitarians may say that an agent would be better off if it were happier, even if that agent prefers to be sad.\n\nFurther Reading & References\n----------------------------\n\n*   [Mistakes in Choice-Based Welfare Analysis](http://elsa.berkeley.edu/~botond/mistakeschicago.pdf) by Botond Köszegi and Matthew Rabin\n*   Russell, Stuart J.; Norvig, Peter (2003), Artificial Intelligence: A Modern Approach (2nd ed.), Upper Saddle River, New Jersey: Prentice Hall, ISBN 0-13-790395-2\n\nNotable Posts\n-------------\n\n*   [Purchase Fuzzies and Utilons Separately](http://lesswrong.com/lw/6z/purchase_fuzzies_and_utilons_separately/)\n*   [Post your Utility Function](http://lesswrong.com/lw/zv/post_your_utility_function/)\n*   [Applying utility functions to humans considered harmful](http://lesswrong.com/lw/1qk/applying_utility_functions_to_humans_considered/)\n*   [Do Humans Want Things?](http://lesswrong.com/lw/6da/do_humans_want_things/)\n*   [Money: The Unit of Caring](http://lesswrong.com/lw/65/money_the_unit_of_caring/)\n*   [Pinpointing Utility](https://www.lesswrong.com/posts/CQkGJ2t5Rw8GcZKJm/pinpointing-utility)\n\nSee Also\n--------\n\n*   [Utilitarianism](https://www.lesswrong.com/tag/utilitarianism)\n*   [Utility function](https://www.lesswrong.com/tag/utility-functions)\n*   [Utility extraction](https://www.lesswrong.com/tag/utility-extraction)\n*   [Expected utility](https://www.lesswrong.com/tag/expected-utility)\n*   [The utility function is not up for grabs](https://www.lesswrong.com/tag/the-utility-function-is-not-up-for-grabs)\n*   [Preference](https://www.lesswrong.com/tag/preference)\n*   [Game theory](https://www.lesswrong.com/tag/game-theory)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb2ca",
    "name": "Anvil Problem",
    "core": null,
    "slug": "anvil-problem",
    "oldSlugs": null,
    "postCount": 0,
    "description": {
      "markdown": "[Eliezer Yudkowsky](https://www.lesswrong.com/tag/eliezer-yudkowsky) has [pointed out](http://lesswrong.com/lw/om/qualitatively_confused/iqd) that \"Both [AIXI](https://www.lesswrong.com/tag/aixi) and AIXItl will at some point drop an anvil on their own heads just to see what happens..., because they are incapable of conceiving that any event whatsoever in the outside universe could change the computational structure of their own operations.\"\n\n[AIXI](https://www.lesswrong.com/tag/aixi), the theoretical formalism for the most intelligent possible agent, does not model itself. It is simply a calculation of the best possible action, extrapolating into the future. This calculation at each step chooses the best action, by recursively calculating the next step, and so on to the time horizon.\n\nAIXI is very simple math. AIXI does not consider its own structure in figuring out what actions it will take in the future. Implicit in its definition is the assumption that it will continue, up until its horizon, to choose actions that maximize expected future value. AIXI's definition assumes that the maximizing action will always be chosen, despite the fact that the agent’s implementation was predictably destroyed or changed. This is not accurate for real-world implementations which may malfunction, self-modify, be destroyed, be changed, etc.\n\nAIXI is an abstraction, and any real AI would have a physical embodiment that could be damaged, and an implementation which could be changed or could change its behavior due to bugs. The AIXI formalism completely ignores these possibilities (Yampolskiy & Fox, 2012).\n\nRelevance to Friendly AI\n------------------------\n\nAIXI is a valuable tool in theoretically considering the nature of super-intelligence, yet it has its limitations. From one perspective, its lack of a a self-model is a mere detail necessarily left out of a formalized abstraction. Nonetheless, for researchers of a future [artificial general intelligence](https://wiki.lesswrong.com/wiki/Friendly_AI), a correct understanding of self-analysis and self-modification is essential.\n\nFirst, since any Friendly AI must strive to avoid changes in its own goal system, and self-modeling may be valuable for this, the AI must be based on a [reflection](https://www.lesswrong.com/tag/reflective-decision-theory), and today's decision theories mostly lack an understanding of reflectivity.\n\nSecond, because human values are not well-understood or formalized, the FAI may need to refine its own goal of maximizing human values. \"Refining\" one's own goal without changing the goal's essentials is another demanding problem in reflective decision theory.\n\nThird, an artificial general intelligence will likely choose to try to enhance its own intelligence to better achieve its goals. It may do so by altering its own implementation, or by creating a new generation of AI. It may even do so without regard for the destruction of the current implementation, so long as the new system can better achieve the goals. All these forms of self-modification again raise central questions about the self-model of the AI, which, as mentioned, is not a part of AIXI.\n\nReferences\n----------\n\n[R.V. Yampolskiy, J. Fox (2012)](http://joshuafox.com/media/YampolskiyFox__AGIAndTheHumanModel.pdf) Artificial General Intelligence and the Human Mental Model. In Amnon H. Eden, Johnny Søraker, James H. Moor, Eric Steinhart (Eds.), The Singularity Hypothesis.The Frontiers Collection. London: Springer.\n\nBlog comment\n------------\n\n[Eliezer Yudkowsky](http://lesswrong.com/lw/om/qualitatively_confused/iqd) on Qualitatively Confused at LessWrong, 15 March 2008."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb1cc",
    "name": "Hollywood Rationality",
    "core": null,
    "slug": "hollywood-rationality",
    "oldSlugs": null,
    "postCount": 0,
    "description": {
      "markdown": "**Hollywood rationality** is a popular stereotype of what rationality is about. It presents \"rationalists\" as word-obsessed, floating in endless verbal space disconnected from reality, reciting figures to more decimal places than are necessary, or [speaking in a dull monotone](http://tvtropes.org/pmwiki/pmwiki.php/Main/TheSpock). Hollywood rationalists also tend to either not have strong (or any) [emotions](https://www.lesswrong.com/tag/emotions). When they do, they often do not express them. Needless to say, this has nothing to do with actual [rationality](https://www.lesswrong.com/tag/rationality).\n\nTV Tropes independently invented this exact concept and called it the [Straw Vulcan](http://tvtropes.org/pmwiki/pmwiki.php/Main/StrawVulcan). Their explanation is more detailed and has more subconcepts and better examples than ours, so if you want to know how to spot bad rationality as seen in popular media, you should probably check TV Tropes. (Warning, TV Tropes.)\n\nContrast to [Traditional rationality](https://www.lesswrong.com/tag/traditional-rationality), or better yet, [rationality](https://www.lesswrong.com/tag/rationality) with math in it.\n\nSee also\n--------\n\n*   [Rationality](https://www.lesswrong.com/tag/rationality)\n*   [Joy in the merely real](https://www.lesswrong.com/tag/joy-in-the-merely-real), [Fun theory](https://www.lesswrong.com/tag/fun-theory)\n*   [Emotion](https://www.lesswrong.com/tag/emotions)\n*   [Belief as attire](https://wiki.lesswrong.com/wiki/Belief_as_attire)\n\nExternal links\n--------------\n\n*   [Straw Vulcan](http://tvtropes.org/pmwiki/pmwiki.php/Main/StrawVulcan) \\- TV Tropes Wiki entry discussing the problems with the common stereotype.\n*   [Julia Galef Skepticon IV talk on the same theme](http://measureofdoubt.com/2011/11/26/the-straw-vulcan-hollywoods-illogical-approach-to-logical-decisionmaking/) \\- [summary on Less Wrong](http://lesswrong.com/lw/90n/summary_of_the_straw_vulcan/)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb288",
    "name": "Ben Goertzel",
    "core": null,
    "slug": "ben-goertzel",
    "oldSlugs": null,
    "postCount": 0,
    "description": {
      "markdown": "**Ben Goertzel** is the Chairman at the AGI company Novamente, and founder of the AGI conference series.\n\nGoertzel held the title of \"Director of Research\" at the [Machine Intelligence Research Institute](https://www.lesswrong.com/tag/machine-intelligence-research-institute-miri) in 2008 while SI and Novamente funded the AI project [OpenCog](http://opencog.org/). Goertzel is currently an advisor for SI. He notably disagrees with SI's arguments regarding the need for [Friendly AI](https://wiki.lesswrong.com/wiki/Friendly_AI).\n\nGoertzel has responded to [Eliezer Yudkowsky](https://www.lesswrong.com/tag/eliezer-yudkowsky)'s [Coherent Extrapolated Volition](https://www.lesswrong.com/tag/coherent-extrapolated-volition) with his own variation of the idea, [Coherent Aggregated Volition](https://www.lesswrong.com/tag/coherent-aggregated-volition).\n\nBlog posts\n----------\n\n*   [The Singularity Institute's Scary Idea (and Why I Don't Buy It)](http://lesswrong.com/lw/2zg/ben_goertzel_the_singularity_institutes_scary/)\n*   [What Would It Take to Move Rapidly Toward Beneficial Human-Level AGI?](http://lesswrong.com/lw/2uy/ben_goertzel_what_would_it_take_to_move_rapidly/)\n*   [Mitigating the Risks of Artificial Superintelligence](http://lesswrong.com/lw/5c1/ben_goertzel_interviews_michael_anissimov/)\n*   [Does Humanity Need an AI Nanny?](http://lesswrong.com/lw/75a/link_ben_goertzel_does_humanity_need_an_ainanny/)\n*   [Muehlhauser-Goertzel Dialogue, Part 1](http://lesswrong.com/lw/aw7/muehlhausergoertzel_dialogue_part_1/), [Part 2](http://lesswrong.com/r/discussion/lw/c7h/muehlhausergoertzel_dialogue_part_2/)\n*   [Superintelligence: Fears, Promises and Potentials](http://jetpress.org/v25.2/goertzel.htm)\n\nExternal Links\n--------------\n\n*   Goertzel's [personal website](http://wp.goertzel.org/).\n*   [Experts on artificial general intelligence provide estimates for the future of AGI.](http://sethbaum.com/ac/2011_AI-Experts.html), Baum, Seth D., Ben Goertzel, and Ted G. Goertzel, 2011.\n*   [OpenCog](http://en.wikipedia.org/wiki/OpenCog) on Wikipedia.\n\nSee also\n--------\n\n*   [Interview\\_series\\_on\\_risks\\_from_AI](https://www.lesswrong.com/tag/interview-series-on-risks-from-ai)\n*   [Coherent Aggregated Volition](https://www.lesswrong.com/tag/coherent-aggregated-volition)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "uLqT8mmFiA8NeytTi",
    "name": "Zombies",
    "core": false,
    "slug": "zombies",
    "oldSlugs": null,
    "postCount": 14,
    "description": {
      "markdown": "A **philosophical zombie** or **p-zombie** is a hypothetical entity that looks and behaves exactly like a human (often stipulated to be atom-by-atom identical to a human) but is not actually conscious: they are often said to lack [phenomenal consciousness](https://www.lesswrong.com/tag/consciousness).\n\nA p-zombie is as likely as anyone else to ask, \"When I see red, do I see the same color that you see when you see red?\", but they have no real experience of the color red; the zombie's speech must be explained in some other terms which do not require them to have real experiences.\n\nThe zombie thought experiment is purported to show that consciousness cannot be reduced to [merely physical](https://www.lesswrong.com/tag/joy-in-the-merely-real) things: our universe is purported to perhaps have special \"bridging laws\" which bring a mind into existence when there are atoms in a suitably brain-like configuration.\n\nPhysicalists typically deny the possibility of zombies: if a p-zombie is atom-by-atom identical to a human being in our universe, then _our_ speech can be explained by the same mechanisms as the zombie's — and yet it would seem _awfully peculiar_ that our words and actions would have an entirely materialistic explanation, but also, _furthermore_, our universe happens to contain exactly the right bridging law such that our utterances about consciousness are true and our consciousness syncs up with what our merely physical bodies do. It's too much of a stretch: [Occam's razor](https://www.lesswrong.com/tag/occam-s-razor) dictates that we favor a monistic universe with [one uniform set of laws](https://www.lesswrong.com/tag/universal-law).\n\nOther physicalists accept the possibility of p-zombies, but insist that _we are_ p-zombies and [consciousness is an illusion](https://nbviewer.jupyter.org/github/k0711/kf_articles/blob/master/Frankish_Illusionism%20as%20a%20theory%20of%20consciousness_eprint.pdf).\n\nNon-physicalists use the apparent conceivability of p-zombies to argue that it is impossible to explain how phenomenal consciousness arises using physical facts alone. P-zombies are presumably not possible in our universe, but they are conceivable (on this view) because a physically identical universe with different psychophysical bridging laws or different [quiddities](http://consc.net/papers/panpsychism.pdf) could indeed harbor p-zombies.\n\n  \n\n**External links**\n------------------\n\n*   [Zombies](http://plato.stanford.edu/entries/zombies/), Stanford Encyclopedia of Philosophy\n*   [Zombies on the web](http://consc.net/zombies.html) by [David Chalmers](https://en.wikipedia.org/wiki/David_Chalmers)\n\n  \n\n**See also**\n------------\n\n*   Tags: [Consciousness](https://www.lesswrong.com/tag/consciousness)\n*   Sequences: [Physicalism 201](https://www.lesswrong.com/s/FqgKAHZAiZn9JAjDo)\n*   Wiki pages: [How an algorithm feels](https://www.lesswrong.com/tag/how-an-algorithm-feels), [Making beliefs pay rent‎](https://wiki.lesswrong.com/wiki/Making_beliefs_pay_rent%E2%80%8E)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb256",
    "name": "Santa Claus",
    "core": null,
    "slug": "santa-claus",
    "oldSlugs": null,
    "postCount": 0,
    "description": {
      "markdown": "**Santa Claus** (also known as **Father Christmas**) is a mythical Christmas gift bringer whose existence is often taught to children by parents who know they're lying.\n\nIt is a vexed question as to whether teaching your children about Santa is a good idea or not.\n\nBlog posts\n----------\n\n*   [The Third Alternative](http://lesswrong.com/lw/hu/the_third_alternative/)\n*   [Is Santa Real?](http://lesswrong.com/lw/2h/is_santa_real/) by [thomblake](https://wiki.lesswrong.com/wiki/thomblake)\n*   [Santa: Naughty Or Nice?](http://www.overcomingbias.com/2009/12/santa-naughty-or-nice.html) by [Robin Hanson](https://www.lesswrong.com/tag/robin-hanson)\n*   [The Santa deception: how did it affect you?](http://lesswrong.com/lw/3da/the_santa_deception_how_did_it_affect_you/) by [Desrtopa](https://wiki.lesswrong.com/wiki/Desrtopa)\n\nExternal links\n--------------\n\n*   [Dropping The Science](http://www.penny-arcade.com/comic/2010/12/24/dropping-some-science/) on [Penny Arcade](http://www.penny-arcade.com/)\n\nSee also\n--------\n\n*   [Third option](https://www.lesswrong.com/tag/third-option)\n*   [Belief in belief](https://www.lesswrong.com/tag/belief-in-belief)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb35c",
    "name": "Evenness",
    "core": null,
    "slug": "evenness",
    "oldSlugs": null,
    "postCount": 0,
    "description": {
      "markdown": "From the [Twelve Virtues of Rationality](https://www.lesswrong.com/tag/12-virtues):\n\n> The fourth virtue is **evenness**. One who wishes to believe says, “Does the evidence permit me to believe?” One who wishes to disbelieve asks, “Does the evidence force me to believe?” Beware lest you place huge burdens of proof only on propositions you dislike, and then defend yourself by saying: “But it is good to be skeptical.” If you attend only to favorable evidence, picking and choosing from your gathered data, then the more data you gather, the less you know. If you are selective about which arguments you inspect for flaws, or how hard you inspect for flaws, then every flaw you learn how to detect makes you that much stupider. If you first write at the bottom of a sheet of paper “And therefore, the sky is green!” it does not matter what arguments you write above it afterward; the conclusion is already written, and it is already correct or already wrong. To be clever in argument is not rationality but rationalization. Intelligence, to be useful, must be used for something other than defeating itself. Listen to hypotheses as they plead their cases before you, but remember that you are not a hypothesis; you are the judge. Therefore do not seek to argue for one side or another, for if you knew your destination, you would already be there.\n\nSee also\n--------\n\n*   [Motivated skepticism](https://www.lesswrong.com/tag/motivated-skepticism)\n*   [Self-deception](https://www.lesswrong.com/tag/self-deception)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb337",
    "name": "Perfectionism",
    "core": null,
    "slug": "perfectionism",
    "oldSlugs": null,
    "postCount": 0,
    "description": {
      "markdown": "From the [Twelve Virtues of Rationality](https://www.lesswrong.com/tag/12-virtues):\n\n> The ninth virtue is perfectionism. The more errors you correct in yourself, the more you notice. As your mind becomes more silent, you hear more noise. When you notice an error in yourself, this signals your readiness to seek advancement to the next level. If you tolerate the error rather than correcting it, you will not advance to the next level and you will not gain the skill to notice new errors. In every art, if you do not seek perfection you will halt before taking your first steps. If perfection is impossible that is no excuse for not trying. Hold yourself to the highest standard you can imagine, and look for one still higher. Do not be content with the answer that is almost right; seek one that is exactly right.\n\nPerfectionism considered harmful\n--------------------------------\n\n[Jennifer Kromberg writes](https://www.psychologytoday.com/blog/inside-out/201311/4-difficulties-being-perfectionist), on the hallmarks of a perfectionist belief system, that:\n\n> Things are either black or white- no vaguely defined area of in between or close enough. Things in your life are either right or wrong, good or bad success or failure.\n\nSuch a belief system, she argues, interferes, among other things, with forming intimate relationships, causes anxiety and a sense of shame, and leads to ones self evaluation being judged on external achievements rather than who you are."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb1fd",
    "name": "Standard of Evidence",
    "core": null,
    "slug": "standard-of-evidence",
    "oldSlugs": null,
    "postCount": 0,
    "description": {
      "markdown": "A **standard of evidence** or **standard of admissible evidence** seeks to narrow down the type of [rational evidence](https://www.lesswrong.com/tag/rational-evidence) which may be brought to bear on any kind of (partially or wholly) [adversarial process](https://wiki.lesswrong.com/wiki/adversarial_process), such as scientific research, law or politics. A common standard of evidence is thought to be needed in any such process, so as to avoid a breakdown of useful [deliberation](https://wiki.lesswrong.com/wiki/deliberation). This is the basic contribution of the [scientific method](https://www.lesswrong.com/tag/science) and of legal evidence standards.\n\nSee also\n--------\n\n*   [Rational evidence](https://www.lesswrong.com/tag/rational-evidence)\n*   [Filtered evidence](https://www.lesswrong.com/tag/filtered-evidence)\n\nBlog posts\n----------\n\n*   [Scientific Evidence, Legal Evidence, Rational Evidence](http://lesswrong.com/lw/in/scientific_evidence_legal_evidence_rational/)\n*   [The Dilemma: Science or Bayes?](http://lesswrong.com/lw/qa/the_dilemma_science_or_bayes/)\n*   [Science Doesn't Trust Your Rationality](http://lesswrong.com/lw/qb/science_doesnt_trust_your_rationality/)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb13c",
    "name": "Third Option",
    "core": null,
    "slug": "third-option",
    "oldSlugs": null,
    "postCount": 0,
    "description": {
      "markdown": "A **third option** is a way to break a [false dilemma](https://www.lesswrong.com/tag/false-dilemma), showing that neither of the suggested solutions is a good idea.\n\nBlog posts\n----------\n\n*   [The Third Alternative](http://lesswrong.com/lw/hu/the_third_alternative/)\n*   [Third Alternatives for Afterlife-ism](http://lesswrong.com/lw/hv/third_alternatives_for_afterlifeism/)\n\nExternal links\n--------------\n\n*   [Take A Third Option](http://tvtropes.org/pmwiki/pmwiki.php/Main/TakeAThirdOption) at TVTropes\n\nSee also\n--------\n\n*   [False dilemma](https://www.lesswrong.com/tag/false-dilemma)\n*   [Color politics](https://www.lesswrong.com/tag/blues-and-greens-metaphor)\n*   [Black swan](https://www.lesswrong.com/tag/black-swans)\n*   [Defensibility](https://www.lesswrong.com/tag/defensibility)\n*   [Arguments as soldiers](https://www.lesswrong.com/tag/arguments-as-soldiers)\n*   [Motivated cognition](https://www.lesswrong.com/tag/motivated-reasoning)\n*   [Challenging the Difficult](https://www.lesswrong.com/tag/challenging-the-difficult)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb13c",
    "name": "Third Option",
    "core": null,
    "slug": "third-option",
    "oldSlugs": null,
    "postCount": 0,
    "description": {
      "markdown": "A **third option** is a way to break a [false dilemma](https://www.lesswrong.com/tag/false-dilemma), showing that neither of the suggested solutions is a good idea.\n\nBlog posts\n----------\n\n*   [The Third Alternative](http://lesswrong.com/lw/hu/the_third_alternative/)\n*   [Third Alternatives for Afterlife-ism](http://lesswrong.com/lw/hv/third_alternatives_for_afterlifeism/)\n\nExternal links\n--------------\n\n*   [Take A Third Option](http://tvtropes.org/pmwiki/pmwiki.php/Main/TakeAThirdOption) at TVTropes\n\nSee also\n--------\n\n*   [False dilemma](https://www.lesswrong.com/tag/false-dilemma)\n*   [Color politics](https://www.lesswrong.com/tag/blues-and-greens-metaphor)\n*   [Black swan](https://www.lesswrong.com/tag/black-swans)\n*   [Defensibility](https://www.lesswrong.com/tag/defensibility)\n*   [Arguments as soldiers](https://www.lesswrong.com/tag/arguments-as-soldiers)\n*   [Motivated cognition](https://www.lesswrong.com/tag/motivated-reasoning)\n*   [Challenging the Difficult](https://www.lesswrong.com/tag/challenging-the-difficult)"
    },
    "parentTag": null,
    "subTags": []
  }
]