[
  {
    "_id": "ezTA3fSPCWZieBfjS",
    "url": null,
    "title": "Adversarial epistemology",
    "slug": "adversarial-epistemology",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "Trust"
      },
      {
        "name": "World Modeling"
      },
      {
        "name": "Epistemic Hygiene"
      },
      {
        "name": "Rationality"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "This is the first article in my *[Bah-Humbug Sequence](https://www.lesswrong.com/s/9ctJpFJvf8h5ivT6Q)* a.k.a. \"Everything I Don't Like Is A Defect/Defect Equilibrium\". Epistemic status: strong opinion weakly held, somewhat exaggerated for dramatic effect; I'm posting this here so that the ensuing discussion might help me clarify my position. Anyway, the time has now come for me to explain my overbearing attitude of cynicism towards all aspects of life. Why now, of all times? I hope to make that clear by the end.\n\n----\n\n> You are asking me to believe a certain claim. There is a simple and easy thing you can do to prove its trustworthiness, and yet you have not done that. I am therefore entitled to [Weak Adversarial Argument] disregard your claim as of no evidentiary value / [Strong Adversarial Argument] believe the negation of your claim purely out of spite.\n\nWhat's going on here? Are these valid arguments?\n\nIt may help to give some examples:\n\n1. *The Hearsay Objection* - In a court of law, if a witness X tries to testify that some other person Y said Z, in trying to establish the truth of Z, the opposing side may object. This objection takes the form: \"The opposition has brought in X to prove Z by way of the fact that Y said Z. But X is not the most reliable witness they could have called, because they could have summoned Y instead. If they were genuinely seeking the truth as to Z, they would have done so; and yet we see that they did not. Therefore I insist that X's testimony be stricken from the record.\"\n\n2. *The Cynical Cryptographer* - My company's HR department emails me a link to an employee satisfaction survey. The email is quick to say \"Your responses are anonymous\", and yet I notice that the survey link contains a bunch of gobbledegook like `?id=2815ec7e931410a5fb358588ee70ad8b`. I think to myself: If this actually *is* anonymous, and not a sham to see which employees have attitude problems and should be laid off first, the HR department could have set up a [Chaumian blind signature](https://en.wikipedia.org/wiki/Blind_signature) protocol to provably guarantee that my response cannot be linked to my name. But they didn't, and so I conclude that this survey *is* a sham, and I won't fill it out.\n\nSo, again, are these valid arguments? From a Bayesian perspective, not really:\n\n1. X saying that Y said Z is not *literally zero* evidence of Z. If there is any chance >0 that X and Y are honest, then I must update at least somewhat towards the truth of Z upon hearing X's testimony.\n\n2. I'm pretty sure they don't teach cryptography in business school. An honest HR department and a dishonest one have approximately equal likelihood (i.e. ε) of knowing what a \"Chaumian blind signature\" is and actually implementing it. Therefore, by Bayes' theorem, etc.\n\nTo steelman the Adversarial Argument, we should understand it not as an ordinary passive attempt to \"rationally\" form an accurate world-model, but rather as a sort of acausal negotiation tactic, akin to one-boxing on Newcomb's Problem. By adopting it, we hope to \"influence\" the behavior of adversaries (i.e. people who want to convince us of something but don't share our interests) towards providing stronger evidence, and away from trying to deceive us.\n\nOr, to put it another way, the Adversarial Argument may not be valid in general, but by proclaiming it loudly and often, we can *make* it valid (at least in certain contexts) and thus make distinguishing truth and falsehood easier. *Because* the Hearsay Objection is enforced in court, lawyers who want to prove Z will either introduce direct witnesses or drop the claim altogether. And perhaps (we can dream!) if the Cynical Cryptographer argument catches on, honest HR departments will find themselves compelled to add Chaumian blind signatures to their surveys in order to get any responses, making the sham surveys easy to spot.\n\n(Aside: Even under this formulation, we might accept the Weak Adversarial Argument but reject the Strong Adversarial Argument - by adopting a rule that I'll believe the opposite of what an untrustworthy-seeming person says, I'm now setting myself up to be deceived into believing P by a clever adversary who asserts ¬P in a deliberately sleazy way - whereupon I'll congratulate myself for seeing through the trick! Is there any way around this?)\n\nNow, returning to the template above, the premise that \"there is a simple and easy thing you can do to prove its trustworthiness\" is doing a lot of work. Your adversary will always contend that the thing you want them to do (calling witness Y, adding Chaumian signatures, etc.) is too difficult and costly to reasonably expect of them. This may or may not be true, but someone who's trying to deceive you will claim such regardless of its truth, hoping that they can \"blend in\" among the honest ones.\n\nAt that point, the situation reduces to a contest of wills over who gets to grab how much of the surplus value from our interaction. What is my trust worth to you? How much personal cost will you accept in order to gain it?\n\n----\n\nWe on LessWrong - at least, those who wish to communicate the ideas we discuss here with people who don't already agree - should be aware of this dynamic. There may have been a time in history when charismatic authority or essays full of big words were enough to win people over, but that is far from our present reality. In our time, propaganda and misinformation are well-honed arts. People are \"[accustomed to a haze of plausible-sounding arguments](https://www.lesswrong.com/posts/PRAyQaiMWg2La7XQy/moloch-s-toolbox-2-2)\" and are rightly skeptical of all of them. Why should they trust the ideas on LessWrong, of all things? If we think gaining their trust is important and valuable, how much personal cost are we willing to accept to that end?\n\nOr, backing up further: Why should *you* trust what you read here?"
    },
    "voteCount": 6,
    "forceInclude": true
  },
  {
    "_id": "CewHdaAjEvG3bpc6C",
    "url": null,
    "title": "Epistemic Artefacts of (conceptual) AI alignment research",
    "slug": "epistemic-artefacts-of-conceptual-ai-alignment-research",
    "author": "Nora_Ammann",
    "question": false,
    "tags": [
      {
        "name": "AI"
      },
      {
        "name": "Practice & Philosophy of Science"
      },
      {
        "name": "Distillation & Pedagogy"
      },
      {
        "name": "Intellectual Progress (Society-Level)"
      },
      {
        "name": "PIBBSS"
      }
    ],
    "tableOfContents": {
      "sections": [
        {
          "title": "Tl;dr",
          "anchor": "Tl_dr",
          "level": 1
        },
        {
          "title": "Four Types of Epistemic Artefacts",
          "anchor": "Four_Types_of_Epistemic_Artefacts",
          "level": 1
        },
        {
          "title": "(1) Map-making (i.e. conceptual de-confusion, gears-level understanding of relevant phenomena, etc.)",
          "anchor": "_1__Map_making__i_e__conceptual_de_confusion__gears_level_understanding_of_relevant_phenomena__etc__",
          "level": 2
        },
        {
          "title": "(2) Identifying and specifying risk scenarios",
          "anchor": "_2__Identifying_and_specifying_risk_scenarios",
          "level": 2
        },
        {
          "title": "(3) Characterising target behaviour",
          "anchor": "_3__Characterising_target_behaviour",
          "level": 2
        },
        {
          "title": "(4) Formalising alignment proposals",
          "anchor": "_4__Formalising_alignment_proposals",
          "level": 2
        },
        {
          "title": "Some additional thoughts ",
          "anchor": "Some_additional_thoughts_",
          "level": 1
        },
        {
          "title": "How these artefacts relate to each other",
          "anchor": "How_these_artefacts_relate_to_each_other",
          "level": 2
        },
        {
          "title": "Epistemic artefacts in your own research",
          "anchor": "Epistemic_artefacts_in_your_own_research",
          "level": 2
        },
        {
          "title": "Summary",
          "anchor": "Summary",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        },
        {
          "anchor": "comments",
          "level": 0,
          "title": "1 comment"
        }
      ],
      "headingsCount": 12
    },
    "contents": {
      "markdown": "*The fact that this post is seeing the light of day now rather than in some undefined number of weeks is, not in small part, due to participating in the second* [*Refine*](https://www.lesswrong.com/posts/5uiQkyKdejX3aEHLM/how-to-diversify-conceptual-alignment-the-model-behind) *blog post day. Thank you,* fake-but-useful containers, *and thank you, Adam. *\n\nTl;dr\n=====\n\nIn this post, I describe four types of insights - what I will call **Epistemic Artefacts** \\- that we may hope to acquire through (conceptual) AI alignment research. I provide examples and briefly discuss how they relate to each other and what role they play on the path to solving the AI alignment problem. The hope is to add some useful vocabulary and reflective clarity when thinking about what it may look like to contribute to solving AI alignment. \n\nFour Types of Epistemic Artefacts\n=================================\n\nInsofar as we expect conceptual AI alignment research to be helpful, what sorts of insights (here: “epistemic artefacts”) do we hope to gain? \n\nIn short, I suggest the following taxonomy of potential epistemic artefacts: \n\n1.  Map-making (de-confusion, gears-level models, etc.)\n2.  Characterising risk scenarios\n3.  Characterising target behaviour\n4.  Developing alignment proposals\n\n(1) Map-making (i.e. conceptual de-confusion, gears-level understanding of relevant phenomena, etc.)\n----------------------------------------------------------------------------------------------------\n\nFirst, research can aim to develop a [gears-level understanding](https://www.lesswrong.com/tag/gears-level) of phenomena that appear critical for properly understanding the problem as well as for formulating solutions to AI alignment (e.g. intelligence, agency, values/preferences/intents, self-awareness, power-seeking, etc.). Turns out, it’s hard to think clearly about AI alignment without having a good understanding of and “good vocabulary” for phenomena that lie at the heart of the problem. In other words, the goal of \"map-making\" is to dissolve conceptual bottlenecks holding back progress in AI alignment research at a the moment.\n\nFiguratively speaking, this is where we are trying to draw **more accurate maps** that help us better navigate the territory. \n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/5d3e7101ffff5130f6886c5b9a83c8999454e094cdfbc9bd.png)\n\nSome examples of work on this type of epistemic artefact include [Agency: What it is and why it matters](https://www.alignmentforum.org/s/aek5ksSs2FHTeofsf), [Embedded Agency](https://www.alignmentforum.org/s/Rm6oQRJJmhGCcLvxh), [What is bounded rationality?](https://www.taylorfrancis.com/chapters/edit/10.4324/9781315658353-2/bounded-rationality-gerd-gigerenzer), [The ground of optimization](https://www.alignmentforum.org/posts/znfkdCoHMANwqc2WE/the-ground-of-optimization-1), [Game Theory](https://link.springer.com/referencework/10.1007/978-1-0716-0368-0), [Mathematical Theory of Communication](https://people.math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf), [Functional Decision Theory](https://arxiv.org/abs/1710.05060) and [Infra-Bayesianism](https://www.alignmentforum.org/s/CmrW8fCmSLK7E25sa)—among many others. \n\n(2) Identifying and specifying risk scenarios\n---------------------------------------------\n\nWe can further seek to identify (new) civilizational risk scenarios brought about by advanced AI and to better understand the mechanisms leading to risk scenarios. \n\nFiguratively speaking, this is where we try to identify and describe **the monsters hiding in the territory**, so we can circumvent them when navigating the territory.\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/3e5f9350a34139f200f7928ae0b1a5da099e63c020b25818.png)\n\nWhy does a better understanding of risk scenarios represent useful progress towards AI alignment? In principle, one way of guaranteeing a safe future is by identifying every way things could go wrong and finding ways to defend against each of them. (We could call this a “via negativa” approach to AI alignment.) I am not actually advocating for adopting this approach literally, but it still provides a good intuition for why understanding the range of risk scenarios and their drivers/mechanisms is useful. (More thoughts on this later, in \"How these artefacts relate to each other\".)\n\nIn identifying and understanding risk scenarios, like in many other epistemic undertakings, we should seek to apply a diverse set of epistemic perspectives on how the world works in order to gain a more accurate, nuanced, and robust understanding of risks and failure stories and avoid falling prey to blind spots. \n\nSome examples of work on this type of epistemic artefact include [What failure looks like](https://www.lesswrong.com/posts/HBxe6wdjxK239zajf/what-failure-looks-like), [What Multipolar Failure Looks Like](https://www.lesswrong.com/posts/LpM3EAakwYdS6aRKf/what-multipolar-failure-looks-like-and-robust-agent-agnostic), [The Parable of Predict-O-Matic](https://www.lesswrong.com/s/dxCZoP3TDaB8Acwjo/p/SwcyMEgLyd4C3Dern), [The Causes of Power-seeking and Instrumental Convergence](https://www.alignmentforum.org/s/fSMbebQyR4wheRrvk), [Risks from Learned Optimization](https://www.alignmentforum.org/s/r9tYkB2a8Fp4DN8yB), [Thoughts on Human Models](https://www.alignmentforum.org/posts/BKjJJH2cRpJcAnP7T/thoughts-on-human-models), [Paperclip Maximizer](https://arbital.com/p/paperclip_maximizer/), and [Distinguishing AI takeover scenarios](https://www.lesswrong.com/posts/qYzqDtoQaZ3eDDyxa/distinguishing-ai-takeover-scenarios)—among many others.   \n\n(3) Characterising target behaviour\n-----------------------------------\n\nThirdly, we want to identify and characterize, from within the space of all possible behaviours, those behaviours which are safe and aligned - which we will call “target behaviour”. \n\nFiguratively speaking, this is where we try to identify and describe **candidate destinations **on our maps, i.e., the places we want to learn to navigate to.\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/52d0659db66f73c2b65c1019c91d726f18bacb90fe308350.png)\n\nThis may involve studying intelligent behaviour in humans and human collectives to better understand the structural and functional properties of what it even is we are aiming for. There is a lot of untangling to be done when it comes to figuring out what to aim advanced AI systems at, assuming, for a moment, that we know how to robustly aim these systems at all.  \n\nIt is also conceivable that we need not understand the nature of human agency and human valu-ing in order to specify target behaviour. We can imagine a system with properties that guarantee safety and alignment which are subject-agnostic. An example of such a subject-agnostic property (not necessary a sufficient-for-alignment property) is truthfulness.\n\nSome examples of work on this type of epistemic artefact include [Corrigibility](https://www.alignmentforum.org/tag/corrigibility), [Truthful AI](https://www.alignmentforum.org/posts/aBixCPqSnTsPsTJBQ/truthful-ai-developing-and-governing-ai-that-does-not-lie), [Asymptotically Unambitious Artificial General Intelligence](https://ojs.aaai.org/index.php/AAAI/article/view/5628/5484), [Shard Theory of Human Values](https://www.alignmentforum.org/s/nyEFg3AuJpdAozmoX) and [Pointing at Normativity](https://www.alignmentforum.org/s/Gmc7vtnpyKZRHWdt5) —among (not that many) others.\n\n(4) Formalising alignment proposals\n-----------------------------------\n\nFinally, we are looking to get \"full\" proposals, integrating the “what” and “how” of alignment into an *actual plan *for how-we-are-going-to-build-AGI-that’s-not-going-to-kill-us. Such a proposal will combine insights from map-making, risks stories, and target behaviour, as well as adding new bits of insights. \n\nFiguratively speaking, this is where we try to come up with a **path/plan for how we will navigate the territory in order to reach the destinations while avoiding the monsters**.\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/3d09b0cae514bb812329d03d48bfa14553e765e35faf75f9.png)\n\n[In Adam’s words](https://www.alignmentforum.org/posts/5uiQkyKdejX3aEHLM/how-to-diversify-conceptual-alignment-the-model-behind): “We need far more conceptual AI alignment research approaches than we have now if we want to increase our chances to solve the alignment problem.” One way of getting new substantive approaches is by letting people make sense of the alignment problem through the lenses of and using the conceptual affordances granted by their own field of expertise. This can lead to different perspectives on and ways of modelling the problem—as well, hopefully, as conceiving of possible solutions.\n\nSome examples of work on this type of epistemic artefact include [\\[Intro to brain-like-AGI safety\\] 12. Two paths forward: “Controlled AGI” and “Social-instinct AGI”](https://www.alignmentforum.org/s/HzcM2dkCq7fwXBej8/p/Sd4QvG4ZyjynZuHGt), [Iterated Amplification/HCH](https://www.alignmentforum.org/s/EmDuGeRw749sD3GKd), [Value Learning](https://www.alignmentforum.org/s/4dHMdK5TLN6xcqtyc), and [The Big Picture Of Alignment (Talk Parts 1 + 2)](https://www.alignmentforum.org/posts/aEtc5GgqJGFtTH2kQ/the-big-picture-of-alignment-talk-part-2-1) —among (not many) others.  \n\nSome additional thoughts \n=========================\n\nHow these artefacts relate to each other\n----------------------------------------\n\nWe introduced four different types of epistemic artefacts: (1) map-making, (2) risk scenarios, (3) target behaviour, and (4) alignment proposals. In reality, they don’t exist in isolation from each other but flow through and into each other while working towards solving AI alignment. \n\nThe ultimate goal, of course, is to create fool-proofed alignment proposals.  Artefacts 1, 2, and 3 function as building blocks, enabling us to produce more robust proposals. That said, in reality, the relationship between these four categories is (or should be) less linear than that. By putting these different elements \"into conversation\" with each other, they can be corrected and refined over time. \n\nFor example, a better understanding of key phenomena like \"agency\" can, on the one hand, refine what we should expect could go wrong if we develop superintelligent *agents* (i.e. artefact 2). It may also be able to help us improve our understanding of what artificial behaviours we are/should be aiming at (i.e. artefact 3). A more refined and comprehensive understanding of risk scenarios will be able to help us challenge and problematise existing alignment proposals, thereby weeding out bad ones and strengthening promising ones. Or, finally, as we are thinking about risk scenarios, or target behaviours, or alignment proposals, we might notice how a concept of agency we are using at a given point in time may actually be inadequate in some relevant ways, raising the need to \"go back to the conceptual drawing board” (aka map-making/de-confusion work; artefact 1). \n\n“Conversations” between these different artefacts can take place at different scales; sometimes an individual researcher will work on several of these artefacts and have them interact with each other within their own research. Other times, different research groups will primarily focus on one artefact, and other researchers will supply input from other artefacts. Thus, in this case, the “conversation” is taking place “between” research projects or programs.  \n\nEpistemic artefacts in your own research\n----------------------------------------\n\nEven if it is what we eventually care about, coming up with fully fledged alignment proposals is difficult. This can be disheartening for researchers because it might feel like they are \"not working on the real thing\" if they are not more or less directly on track to develop or advance a proposal to solving AI alignment. \n\nThe above list of artefacts might make it easier for researchers to reflect - individually or in conversation with others - on what they are trying to achieve with their research and how that might eventually feed into solving AI alignment. While it's very important to be tightly grounded in and repeatedly (re)oriented to \"the real thing\"/\"the hard part of the problem\"/etc., I also believe that each of these epistemic arefacts constitutes important ingredients towards guaranteeing a safe and flourishing future, and that people mining those artefacts are making important contributions to AI alignment. \n\nSummary\n=======\n\n1.  I identify four categories of **epistemic artefacts **we may hope to retrieve from conceptual AI alignment research: a) conceptual de-confusion, b) identifying and specifying risk scenarios, c) characterising target behaviour, and d) formalising alignment strategies/proposals.\n2.  I briefly discuss some ways in which these epistemic artefacts relate to and flow through each other.\n\n*Acknowledgements: A lot of these ideas originated with TJ/particlemania and credit should be given accordingly. Mistakes are my own. Thanks, also, to a number of people for useful discussions and feedback, in particular Adam Shimi, Tomas Gavenciack and Eddie Jean.*"
    },
    "voteCount": 13,
    "forceInclude": true
  },
  {
    "_id": "FuToH2KHxKmJLGk2B",
    "url": null,
    "title": "AI alignment as “navigating the space of intelligent behaviour”",
    "slug": "ai-alignment-as-navigating-the-space-of-intelligent",
    "author": "Nora_Ammann",
    "question": false,
    "tags": [
      {
        "name": "AI"
      },
      {
        "name": "PIBBSS"
      },
      {
        "name": "AI Alignment Fieldbuilding"
      },
      {
        "name": "Distillation & Pedagogy"
      },
      {
        "name": "Practice & Philosophy of Science"
      }
    ],
    "tableOfContents": {
      "sections": [
        {
          "title": "Tl;dr",
          "anchor": "Tl_dr",
          "level": 1
        },
        {
          "title": "How to make progress in AI alignment? ",
          "anchor": "How_to_make_progress_in_AI_alignment__",
          "level": 1
        },
        {
          "title": "Strategies for exploring the space of intelligent behaviour",
          "anchor": "Strategies_for_exploring_the_space_of_intelligent_behaviour",
          "level": 1
        },
        {
          "title": "Strategy 1: Tinkering",
          "anchor": "Strategy_1__Tinkering",
          "level": 2
        },
        {
          "title": "Strategy 2: Idealization",
          "anchor": "Strategy_2__Idealization",
          "level": 2
        },
        {
          "title": "Strategy 3: Learning from “intelligence-in-the-wild”",
          "anchor": "Strategy_3__Learning_from__intelligence_in_the_wild_",
          "level": 2
        },
        {
          "title": "Summary",
          "anchor": "Summary",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        },
        {
          "anchor": "comments",
          "level": 0,
          "title": "No comments"
        }
      ],
      "headingsCount": 9
    },
    "contents": {
      "markdown": "Tl;dr\n=====\n\nIn this post, I introduce a conceptual tool for thinking about the epistemic landscape of AI alignment and then describe three epistemic strategies for making progress on the alignment problem: 1) tinkering, 2) idealisation and 3) intelligence-in-the-wild. \n\nHow to make progress in AI alignment? \n======================================\n\nThe future of AI progress is likely to critically shape, if not practically determine, the future of humanity, and of sentient life in general. Will our future look more like a world filled to the brim with things we find valuable, as well as sentient creatures to enjoy that goodness? Or will our future look more like one characterized by violence, mistrust, inequality, suffering, or even the absence of anything sentient at all?\n\nAs our civilization is making progress on our abilities to engineer and instantiate sophisticated forms of complex and intelligent behaviours in artificial systems, it becomes imperative to carefully think about which objectives this intelligence is being directed at, and how to do such \"directing\" robustly. Thus, the central question becomes: how can we make sure that ‘big effects’ caused by AI progress will be positive rather than harmful, safe rather than dangerous, helping to promote, discover, and enrich what is dear to us rather than destroying it? This is the guiding question of AI alignment, as I understand it. \n\nOnce we have established the importance of the problem, the next question that stands out is: *How *can we make progress?\n\nThis is a central question for the “philosophy of science” of AI alignment, as I see it. For example, in order to help answer the question of (the best) ways to make progress on the problem, we can reflect on its shape or structure. We can thus notice how one important defining characteristic of the alignment problem is that it concerns [systems that do not exist yet](https://www.alignmentforum.org/s/LLEJJoaYpCoS5JYSY/p/FQqcejhNWGG8vHDch)—let’s call this the problem of “**epistemic access**”. This means that our typical epistemic strategies of science and engineering are less effective here than they are for a range of other societal problems (e.g., finding cures for diseases, improving the yield of a certain crop, building ever taller houses, etc.).^[\\[1\\]](#fnzbcr1nhdgzl)^\n\nIn this post, I will describe how I currently like to think about the landscape of epistemic strategies for making progress in AI alignment. Of course, there are several plausible ways of carving up the space of epistemic strategies, and each of them may serve different purposes. The tri-partition of epistemic strategies I introduce here is specifically grounded in asking what strategies we can adopt for overcoming the challenge of epistemic access, as introduced above.\n\nStrategies for exploring the space of intelligent behaviour\n===========================================================\n\nSo, what are different epistemic strategies we can use in order to (hopefully) make progress on AI alignment? Let’s start by introducing the following conceptual tool. \n\nImagine a space (of uncertain size) that corresponds to all possible manifestations of intelligent behaviour. Abstractly speaking, different epistemic strategies correspond to different approaches to charting out this possibility space, and the endeavour of AI alignment at large (roughly) corresponds to learning to safely navigate movement through this (design) space. \n\nNow, let’s consider different ways of charting this space out.^[\\[2\\]](#fnlikwbbqigel)^ \n\n**Strategy 1: Tinkering**\n-------------------------\n\nOne approach is to work with contemporary ML paradigms and explore, with the help of empirical methods and trial and error, how those systems behave, how they fail to be safe and aligned, and what it would look like for them to be (\\[1\\] in Fig. 1). The hope is that (some of) those insights will generalize to more-advanced AI systems. \n\nIn terms of our figure, this approach explores the possibility space of intelligent behaviour by tinkering at the boundaries of the subspace that we currently know how to implement with ML technology. \n\nMost of what is commonly referred to as “AI safety research” will fall into this category. Here are some examples: \n\n*   **Risks in current and future ML systems**, e.g., Amodei, Olah et al (2016). [Concrete Problems in AI Safety](https://arxiv.org/abs/1606.06565).\n*   **Reward modeling**, e.g., Leike, Krueger et al (2018).[ Scalable agent alignment via reward modeling: a research direction](https://arxiv.org/abs/1811.07871). \n*   **Iterated amplification**,^[\\[3\\]](#fn5ubi5vd9jvy)^ e.g., Christiano, Shlegeris, Amodei (2018). [Supervising strong learners by amplifying weak experts](https://arxiv.org/abs/1810.08575).\n*   **Interpretability**, e.g., Olah, Cammarata et al (2020).[ Zoom In: An Introduction to Circuits](https://distill.pub/2020/circuits/zoom-in/).\n*   And many more…\n\n**Strategy 2: Idealization**\n----------------------------\n\nThe second approach I want to delineate here seeks theoretical clarity by starting from idealized frameworks of reasoning (e.g., decision theory, epistemology) and exploring, via extrapolation, what the in-the-limit behaviour of “Idealized Agents” would look like and what it would take for them to be safe and aligned (\\[2\\] in Fig. 1). \n\nCompared to the former approach, looking at idealized frameworks and in-the-limit behaviour leads to different assumptions about what advanced AI systems will look like (i.e., not necessarily prosaic^[\\[4\\]](#fn6w3csw2n7o)^). (There are some reasons to think those assumptions are *more *relevant, and some reasons to think that they are less relevant.) At the same time, idealized frameworks face some challenges due to the difficulty of achieving empirical grounding compared to the “tinkering” approach.\n\nExamples of work that count towards this approach include: \n\n*   **Decision Theory**, e.g., Yudkowsky and Soares (2017). [Functional Decision Theory: A New Theory of Instrumental Rationality](https://arxiv.org/abs/1710.05060). \n*   **Logical Uncertainty**, e.g., Garrabrant et al. (2016). [Logical Induction](https://arxiv.org/abs/1609.03543).\n*   **Agent Foundations**, e.g., Garrabrant and Demski (2018). [Embedded agency](https://www.alignmentforum.org/posts/i3BTagvt3HbPMx6PN/embedded-agency-full-text-version). \n*   **Infra-Bayesianism**, e.g., Vanessa Kosoy (2021). [Infra-Bayesian physicalism: a formal theory of naturalized induction. ](https://www.alignmentforum.org/posts/gHgs2e2J5azvGFatb/infra-bayesian-physicalism-a-formal-theory-of-naturalized)\n*   **Power Seeking in Optimal Agents**, e.g., Alexander Turner (2021). [The Causes of Power-seeking and Instrumental Convergence. ](https://www.alignmentforum.org/s/fSMbebQyR4wheRrvk)\n\n**Strategy 3: Learning from “intelligence-in-the-wild”**\n--------------------------------------------------------\n\nFinally, the third approach attempts to chart out the possibility space of intelligent behaviour by looking at how intelligent behaviour manifests in existing natural systems (\\[3\\] in Fig. 1). (This is, notably, the epistemic approach that [PIBBSS](https://www.pibbss.ai/) is trying to explore and foster.)\n\nTreating intelligent behaviour (and related features of interest) as a naturally occurring phenomenon suggests a sort of “realism” about intelligence. It’s not just some theoretical construct but something that we can observe every day in the real world. This strategy involves a certain type of *empiricism *(although of a different nature than the empiricism of the “tinkering“ approach). \n\nLet’s look at some examples of how this approach has already been used in alignment research (although I’d argue it remains, overall, rarer than work along the lines of the other two approaches): \n\n*   **Analogies from human brain, **à la Steve Byrnes (2021). [Value loading in the human brain: a worked example](https://www.alignmentforum.org/posts/iMM6dvHzco6jBMFMX/value-loading-in-the-human-brain-a-worked-example) (among others).\n    *   …uses neuroscience as a source of analogies for alignment.\n*   **Analogies from real-world systems, **à la John Wentworth (2020). [Characterizing Real-World Agents as a Research Meta-Strategy ](https://www.lesswrong.com/posts/9pZtvjegYKBALFnLk/characterizing-real-world-agents-as-a-research-meta-strategy)(among others). \n    *   …sets out to understand agency as a real-world phenomenon looking at systems in physics, biology, economics, and beyond. \n*   **Analogies from social systems**, à la Critch (2021). [What Multipolar Failure Looks Like, and Robust Agent-Agnostic Processes. ](https://www.alignmentforum.org/posts/LpM3EAakwYdS6aRKf/what-multipolar-failure-looks-like-and-robust-agent-agnostic)\n    *   … implicitly builds on the concept of[ fields](https://en.wikipedia.org/wiki/Field_theory_(sociology)) from sociology, and the[ structure and agency debate](https://en.wikipedia.org/wiki/Structure_and_agency) in the social sciences about the place of social structure vs personal choices in human behaviour.\n*   **Analogies from biological systems**, à la Hubinger et al. (2019).[ Risks from Learned Optimization in Advanced ML Systems](https://arxiv.org/abs/1906.01820).\n    *   …explicitly uses biological evolution of humans as an example of/inspiration for mesa-optimizers. Evolution here is the search process, with a base-objective of increasing fitness and propagation of genes, but the learned models (humans) end up searching for different goals.\n\n*![](https://lh3.googleusercontent.com/FOxM9aAboSUqM2H52CDgKXJEmAh72DDb3C1yt5zA-WXz9GkaJCCIHBi3CeanLGP5ox9s9JZb7Hh-lV8YWGgO7e30hmAVc2BRVfy3OqqQRyxqy-T5Nyi-pX3Z669RoKxUW2KZwy3jffuYZP3kAiw2vF4)*\n\nFigure 1: *Epistemic strategies for making progress in AI alignment as charting the possibility space of intelligent behaviour; 1. Tinkering; 2. Idealisation; 3. Intelligence-in-the-wild*\n\nSummary\n=======\n\n1.  We can analogise progress in AI alignment as charting out and learning how to navigate the **possibility space of intelligent behaviour**. Different research strategies in AI alignment correspond to different ways of charting out that space. For example:\n    1.  **Tinkering **is an epistemic approach whereby we explore the boundaries of intelligent behaviour that we currently know to implement in ML systems, using empirical and experimental methods. \n    2.  **Idealization **uses idealised frameworks of reasoning and extrapolation to explore the possibility space of intelligent behaviour “in-the-limits”.\n    3.  **Intelligence-in-the-wild **seeks to chart out the possibility space of intelligent behaviour by investigating how it manifests in existing (natural) systems. It is based on an assumption of “realism about intelligence” (see future post for more details). \n\n*Acknowledgements*: *I thank TJ/particlemania for discussing and improving many of the ideas in this post; I also thank Adam Shimi, Gavin Leech, Cara Selvarajah, Eddie Jean and Justis for useful comments on earlier drafts. *\n\n1.  ^**[^](#fnrefzbcr1nhdgzl)**^\n    \n     For more insight into the specific epistemic challenges we face in AI alignment, consider checking out other examples of Adam Shimi’s writing, e.g., [On Solving Problems Before They Appear: The Weird Epistemologies of Alignment](https://www.alignmentforum.org/posts/FQqcejhNWGG8vHDch/on-solving-problems-before-they-appear-the-weird), [Epistemological Vigilance for Alignment](https://www.alignmentforum.org/posts/72scWeZRta2ApsKja/epistemological-vigilance-for-alignment), or [Robustness to Scaling Down: More Important Than I Thought](https://www.alignmentforum.org/posts/pA3F9oejzvGg6Kf3a/robustness-to-scaling-down-more-important-than-i-thought).\n    \n2.  ^**[^](#fnreflikwbbqigel)**^\n    \n     There are different ways one can choose to taxonomise the space. For example, a common distinction that is sometimes made is the one between “de-confusion” and “problem-solving” work. I consider this distinction orthogonal to the three-way classification I am introducing here. In particular, my view is that the distinction introduced in this post (i.e., “tinkering”, “idealisation”, and “intelligence-in-the-wild”) is about what types of systems are being theorised about (roughly speaking: STOA ML systems, idealized systems, and natural systems), or where the theories seek empirical validity. In this post \\[forthcoming\\], I discuss, in more detail, what sorts of systems and empirical basis the intelligence-in-the-wild approach is interested in/based on. (Thanks to TJ for helping me to make this clarification explicit.)\n    \n3.  ^**[^](#fnref5ubi5vd9jvy)**^\n    \n     It is important to note that these are not strict divisions, and some research agendas might involve both tinkering insights and idealisations. IDA is a good example of that. However, for the purposes of this post, I'm tentatively putting it in the tinkering list.\n    \n4.  ^**[^](#fnref6w3csw2n7o)**^\n    \n    \"Prosaic\" is used in somewhat different ways by different people. Here, it is used in its ~technical usage meaning approximately: \"using extensions of current SOTA ML\"."
    },
    "voteCount": 8,
    "forceInclude": true
  },
  {
    "_id": "7X9KKqgZa7edknKPm",
    "url": null,
    "title": "Goal-directedness: my baseline beliefs",
    "slug": "goal-directedness-my-baseline-beliefs",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "Goal-Directedness"
      },
      {
        "name": "AI"
      }
    ],
    "tableOfContents": {
      "sections": [
        {
          "title": "Existing ideas on goal-directedness",
          "anchor": "Existing_ideas_on_goal_directedness",
          "level": 1
        },
        {
          "title": "What do I think it is?",
          "anchor": "What_do_I_think_it_is_",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        },
        {
          "anchor": "comments",
          "level": 0,
          "title": "3 comments"
        }
      ],
      "headingsCount": 4
    },
    "contents": {
      "markdown": "In a short time I will be starting a project (funded by LTFF) under the supervision of Adam Shimi with the goal of *deconfusing goal-directedness*. I have decided to record the progress on this project on a biweekly basis here on LW, to test whether this helps to keep me accountable for making progress on my project, and to [record the process](https://www.lesswrong.com/posts/Psr9tnQFuEXiuqGcR/how-to-write-quickly-while-maintaining-epistemic-rigor).\n\nBefore the project begins, I want to record my baseline beliefs about goal-directedness. I'm doing this partly to see how by beliefs change through the research process and partly just to get my thoughts in order.\n\nExisting ideas on goal-directedness\n-----------------------------------\n\nAdam Shimi has thought a lot about this topic. His [literature review](https://www.lesswrong.com/s/o58ZMNaovdztbLfvN/p/cfXwr6NC9AqZ9kr8g) with Michele Campolo and Joe Collman assembles ideas and intuitions about goal-directedness, some criteria which any proposed definition of goal-directedness can be compared against, and finally an assembly of proposed definitions from various sources. The definitions cut down my initial workload significantly, especially Vanessa Kosoy's definition of [goal-directed intelligence](https://www.lesswrong.com/posts/dPmmuaz9szk26BkmD/vanessa-kosoy-s-shortform?commentId=Tg7A7rSYQSZPASm9s), since it gives me a formal criterion in terms of numerical values which might be feasible to actually implement and compute in toy examples.\n\nThe aim of my project is not simply to pit these definitions against one another in examples, though: I want to do some serious stress-testing of the concept itself, and maybe eventually bring some serious maths to bear on these definitions. Some questions include:\n\n*   How do (different formalizations of) goal-directedness vary as different aspects of the agent's behaviour and environment change? Expectation: a notion of goal-directedness which is too sensitive to small changes will not be useful, so this may be a criterion for refining definitions of goal-directedness (see next section). Generally speaking, I expect goal-directed behaviour to become increasingly rare and therefore more identifiable as the complexity of the environment and behaviour grow.\n*   What happens in extreme cases? Expectation: extreme simplicity is the easiest to think about, and here goal-directedness collapses. For a very simple system, many explanations adequately account for behaviour: it's roughly as simple to say that a ball falls because it \"wants\" to accelerate towards the ground as to simply describe the falling motion. Generally speaking, I expect goal-directedness to be the extreme end of a spectrum which grows as complexity increases, but it will be interesting to explore how this spectrum varies along the dimensions along which complexity can increase.\n*   Are the intuitions surrounding goal-directedness even consistent? Expectation: there are bound to be some speculations which won't hold up under a quantitative treatment. I also expect there is some tension which results from the assumption that goal-directedness should be a binary property, which I do not think is sensible. Since I think the spectrum of goal-directedness collapses in the extreme of simple behaviour, I don't expect simple examples such as Rohin Shah's [twitching robot](https://www.lesswrong.com/s/4dHMdK5TLN6xcqtyc/p/NxF5G6CJiof6cemTw) to cause any problems.\n*   Is there a polar opposite to goal-directedness; even if not, are there other types of extreme behaviour that are comparable to goal-directedness? Expectation: Purely random behaviour will most likely be at an extreme of the goal-directedness spectrum, but this of course depends on what \"purely random\" is taken to mean...\n\nWhat do I think it is?\n----------------------\n\nMy own intuition about goal-directedness aligns quite closely with the ideas surrounding \"explainability\" in Adam's post above: an agent is goal-directed to the extent that its behaviour can be explained more efficiently/simply in terms of its goals than in terms of any explicit/mechanistic description.\n\nWhile this might sound straightforward, any thought experiment that one tries with this idea raises potential problems. The very notion of an explanation is highly dependent on the available information about the world and the agent. Think how much easier it became to explain the motion of falling objects once the notion of gravity was discovered! This illustrates the possibility that by revealing information about the world, apparent goal-directedness might be revealed or broken. At a conceptual level, it also illustrates how metaphysical problems surrounding objects and their properties *might* change which explanations are the most complicated. For example, if several objects are apparently identical but an agent collects only some of them apparently at random, we can get a much simpler goal-based description if we allow the explanation to include a hidden property which only some of the objects possess. One proposed definition of goal-directedness implicitly does this, with the assigned hidden property an (unknown) numerical utility value on the states of a Markov Decision Process (MDP).\n\nIdeally, I hope to find a quantitative definition of goal-directedness (which can be implemented in at least one standard model, such as for MDPs) which is not a binary. Regarding the issues above, I expect that this definition should be approximately continuous: it will not vary too greatly with small changes in the environment, information, or behaviour. That is, I'm hoping that the notion of goal-directedness is actually a robust one, and that the above are examples where the extra details actually don't tip the balance very much.\n\nEventually I hope to probe some of the questions raised at the start of that literature review: does goal-directedness intrinsically create the same catastrophic problems attributed to insufficiently aligned optimizers?"
    },
    "voteCount": 7,
    "forceInclude": true
  },
  {
    "_id": "KJPRC3cgtxSXpZEQZ",
    "url": null,
    "title": "Goal-directedness: exploring explanations",
    "slug": "goal-directedness-exploring-explanations",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "Goal-Directedness"
      },
      {
        "name": "AI"
      }
    ],
    "tableOfContents": {
      "sections": [
        {
          "title": "What constitutes a good explanation?",
          "anchor": "What_constitutes_a_good_explanation_",
          "level": 2
        },
        {
          "title": "Accurate explanations",
          "anchor": "Accurate_explanations",
          "level": 2
        },
        {
          "title": "Direct mapping and extrinsic measurements of accuracy",
          "anchor": "Direct_mapping_and_extrinsic_measurements_of_accuracy",
          "level": 3
        },
        {
          "title": "Rewards and evaluative measurements of accuracy",
          "anchor": "Rewards_and_evaluative_measurements_of_accuracy",
          "level": 3
        },
        {
          "title": "Relational versions",
          "anchor": "Relational_versions",
          "level": 3
        },
        {
          "title": "Measure-theoretic versions",
          "anchor": "Measure_theoretic_versions",
          "level": 3
        },
        {
          "title": "Powerful Explanations",
          "anchor": "Powerful_Explanations",
          "level": 2
        },
        {
          "title": "Lower dimensional explanations",
          "anchor": "Lower_dimensional_explanations",
          "level": 3
        },
        {
          "title": "Explanations of Restricted Scope",
          "anchor": "Explanations_of_Restricted_Scope",
          "level": 3
        },
        {
          "title": "Simple Explanations",
          "anchor": "Simple_Explanations",
          "level": 2
        },
        {
          "title": "Algorithmic complexity",
          "anchor": "Algorithmic_complexity",
          "level": 3
        },
        {
          "title": "Computational complexities",
          "anchor": "Computational_complexities",
          "level": 3
        },
        {
          "title": "Conclusions",
          "anchor": "Conclusions",
          "level": 1
        },
        {
          "title": "The naïve picture and its flaws",
          "anchor": "The_na_ve_picture_and_its_flaws",
          "level": 3
        },
        {
          "title": "What's missing?",
          "anchor": "What_s_missing_",
          "level": 3
        },
        {
          "title": "Do you think something is missing? Putting my intended applications aside, how else might you judge the quality of an explanation?",
          "anchor": "Do_you_think_something_is_missing__Putting_my_intended_applications_aside__how_else_might_you_judge_the_quality_of_an_explanation_",
          "level": 4
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        },
        {
          "anchor": "comments",
          "level": 0,
          "title": "3 comments"
        }
      ],
      "headingsCount": 18
    },
    "contents": {
      "markdown": "*This is the first post in my Effective-Altruism-funded project aiming to deconfuse goal-directedness. Comments are welcomed. All opinions expressed are my own, and do not reflect the attitudes of any member of the body sponsoring me.*\n\nIn my [preliminary post](https://www.lesswrong.com/posts/7X9KKqgZa7edknKPm/goal-directedness-my-baseline-beliefs), I described my basic intuitions about goal-directedness, and focussed on *explainability.* Concisely, my initial, informal working definition of goal-directedness is that *an agent's behaviour is goal-directed to the extent that it is better explained by the hypothesis that the agent is working towards a goal than by other types of explanation*.\n\nIn this post I'm going to pick away at the most visible idea in this formulation: the concept of an explanation (or at least the aspect of it which is amenable to formalization with minimal effort), and especially the criteria by which an explanation is judged. More precisely, this post is a winding path through some mathematical ideas that could be applied to quantitatively judge explanations; this collection of ideas shall be examined more closely in subsequent posts once some desirata are established. By the end I'll have a naïve first picture of how goal-directedness might be measured in terms of goal-based explanations, while having picked out some tools for measuring explanations which goal-directedness can be contrasted with.\n\nWhat constitutes a good explanation?\n------------------------------------\n\nIn science (and rational discourse more broadly), explanations are judged empirically: they are used to generate predictions, and these are compared with the results of existing or subsequent observations. The transition from explanations to predictions will be covered in the next section. Here we'll break down the criteria for comparison.\n\nThe most obvious way an explanation can fail is if it predicts phenomena which are not observed, or conversely if it fails to predict phenomena which are observed. We can present this criterion as:\n\n*   **Accuracy.** The more accurately the observations match the predictions, the better the explanation is considered to be.\n\nThat is not the end of the story. For instance, more value is attributed to explanations which produce a greater variety of predictions for future observations. An exhaustive account of existing observations with no extra consequences barely qualifies as an explanation, whereas an explanation that appears to reveal the fundamental nature of the object(s) of scientific enquiry are celebrated (this has been true for at least a century or two). To summarise, a second criterion is:\n\n*   **Explanatory power.** \\[Only applicable to partially observed situations\\] If a large range of possible behaviours are compatible with the explanation (conditioned on the existing observations), this reflects badly on the explanation.\n\nAnother criterion, which applies when comparing several explanations of a single phenomenon, is that of simplicity. This judgement is mounted on Occam's razor. But simplicity (or its complement, complexity) is not a fully intrinsic property of an explanation. Even if an explanation can be presented within a mathematical formalism in a way amenable to quantifying its complexity, there may be several ways to do this, and there are various measures of complexity one could use; we shall discuss measures of complexity further in a future post. Note that simplicity is conditioned on accuracy to some extent: if no simple explanation is adequate to explain some behaviour, then a more complex explanation is acceptable.\n\n*   **Simplicity.** The more complex the explanation (relative to explanations of comparable accuracy), the worse it is considered to be.\n\nI should also mention, but will ultimately ignore, qualities of an explanation which make it appealing for psychological reasons rather than rational ones, such as the ephemeral quality of \"elegance\" (often associated with simplicity), comedic elements, and story structure. These factors might be important in judging human-directed explanations in natural language, but the kinds of explanation I'll be considering won't be like that.\n\nIn constructing measures of quality of explanations, I'm going to follow the three criteria explained with bullet points above. It should be clear even with just these three points that *measuring the quality of an explanation is not straightforward*, and in particular that I do not expect there to be a unique right or best way to compare explanations. If you think I'm missing an important criterion, let me know in the comments.\n\nAccurate explanations\n---------------------\n\n### Direct mapping and extrinsic measurements of accuracy\n\nJudging an explanation in terms of its accuracy requires a careful examination of the relationship between explanations and behaviour. Before delving into that, let us first consider the simplest/ideal situation in which each explanation under consideration generates a unique behaviour pattern. For example, we might have an explicit algorithm for some behaviour, or we might imagine a reward function on some state space for which there is a unique optimizing policy, which we shall (for the time being only!) take to be the behaviour which the reward function \"explains\".\n\nNaively speaking, measuring the accuracy of the explanation then amounts to a comparison of the behaviour it generates with the observed behaviour. In simple situations/world models there can be straightforward ways to do this: if the observations of the agent's behaviour consist of mere positions in some space, we can take the (expected) distance between the predicted path of the agent and its actual path. If we use the observed behaviour to approximate the agent's policy in a (finite) Markov Decision Process (MDP) then we can take a normalized inner product of the predicted policy and the observed policy to quantify how well these agree.\n\nI can re-express the last two paragraphs as follows. We could consider a mapping \\\\(f: E \\\\to B\\\\) which transforms each member of a class \\\\(E\\\\) of explanations into a member of a class \\\\(B\\\\) of possible behaviours (or policies); we could then consider a metric or measure of similarity between pairs of elements of \\\\(B\\\\), and rate an explanation \\\\(e \\\\in E\\\\) for observed behaviour \\\\(o \\\\in B\\\\) in terms of the distance from \\\\(f(e)\\\\) to \\\\(o\\\\). Since any such measurement is determined entirely in terms of behaviour, ignoring the specifics of both the explanation and any internal working of the agent, we call this an **extrinsic measurement of accuracy**.\n\nFor the algorithm example, extrinsic measurements of accuracy are more or less all that we have recourse to, at least assuming that we do not have access to the agent's source code or internal workings^[\\[1\\]](#fne37k2jowh56)^.\n\nI should stress that there is plenty to debate regarding the choice of distance function on \\\\(B\\\\); different choices will measure different ways in which an explanation is accurate. Consider two predictions of paths in a (discrete) graph: one of them converges exactly to the observed path except for being always one time step behind, while the other prediction coincides with the observed path half the time on average, but occasionally veers off wildly. Which of these two is considered more accurate depends on what metric is being used to compare the paths!\n\n### Rewards and evaluative measurements of accuracy\n\nDirect comparison at the behaviour level can have drawbacks, and these will hopefully lead us to a more nuanced approach  to comparisons. For example, a direct comparison between the optimal policy for a given reward signal and the observed behaviour can fail to recognise when an agent attempting to optimize for that reward signal has instead gotten stuck at a local optimum far from the global optimum. Wasps are certainly trying to obtain sugar syrup when they fly into a trap constructed by humans, but since that behaviour is suboptimal and distant from the global optimum (since the wasps typically die in the trap and can no longer collect syrup) an extrinsic measurement of accuracy will fail to identify sugar syrup as their actual objective.\n\nOne way around this is to treat reward signals as reward signals! If we compute the reward the observed behaviour would receive from a given reward signal and then normalize appropriately, we can measure how accurately the reward signal explains the behaviour in terms of that reward. In mathematical terms, we suppose that we have collections \\\\(E\\\\) of explanations and \\\\(B\\\\) of behaviours as before, but this time a mapping \\\\(f: E \\\\to (B \\\\to \\[0,1\\])\\\\) which sends each explanation to a corresponding (normalized) evaluation of behaviours. Then the quality of an explanation \\\\(e \\\\in E\\\\) for observed behaviour \\\\(o \\\\in B\\\\) is simply defined to be the evaluation \\\\(f(e)(o)\\\\). We call this an **evaluative measurement of accuracy.**\n\nIn order for evaluative measurements to be at all useful (in particular, for them to have a chance of capturing the intended notion of explanation accuracy), we need to impose some additional conditions on the function \\\\(f\\\\). For example, to ensure that every non-trivial explanation has some relevant content, we could impose the condition that there is always at least one behaviour which is evaluated poorly; we might achieve this by insisting that \\\\(0\\\\) is in the image of \\\\(f(e)\\\\) for every explanation \\\\(e\\\\). Even without developing all of the desirata and corresponding axioms, however, one can see how using a judiciously constructed evaluative measurement of accuracy might produce more desirable results in the wasp example.\n\nA caveat to this analysis: I mentioned [in the comments](https://www.lesswrong.com/posts/7X9KKqgZa7edknKPm/goal-directedness-my-baseline-beliefs?commentId=hHpF3LRgmeodYmDto) of my first post that I want to avoid imposing a strong association between an agent's goal (if it has one) and its competence at achieving that goal^[\\[2\\]](#fna4j2h1oy9qa)^. Computing the reward value is better than a direct comparison with optimal behaviour, but I wonder if we can do any better than that (see the conclusion).\n\n### Relational versions\n\nLet's expand our considerations a little. There was no good justification for our simplifying assumption earlier that each explanation determines a unique behaviour. The most accessible example of how it can fail is that a reward signal can produce several, even infinitely many, globally optimal policies. It still seems reasonable, however, to consider \"optimizing with respect to this reward signal\" as a valid explanation of any of these policies, although we might eventually be inclined to judge that the explanation has less explanatory power in this situation.\n\nTo encode the existence of multiple behaviours corresponding to a given explanation, we simply use a (total^[\\[3\\]](#fn01sfvpwpr4gk)^) relation \\\\(R: E \\\\looparrowright B\\\\) or \\\\(R: E \\\\looparrowright (B \\\\to \\\\mathbb{R})\\\\) in place of a function in each of the measurements of accuracy given above. We can then quantify the accuracy of a given explanation \\\\(e \\\\in E\\\\) for observed behaviour \\\\(b \\\\in B\\\\) in two stages. First, we respectively compute the distance from \\\\(b\\\\) to each behaviour \\\\(b' \\\\in B\\\\) with \\\\(eRb'\\\\) or evaluate \\\\(a(b)\\\\) for each \\\\(a:B \\\\to \\\\mathbb{R}\\\\) with \\\\(eRb\\\\). Then we combine these measurements by taking a minimum, maximum, or (at least in the finite case) a linear combination of them. Again, the choice of weightings in the last case depends on how we have chosen to reward explanatory power, or to punish an absence of such. We can extend this idea to the infinite case in reasonable cases, as we shall shortly see.\n\n### Measure-theoretic versions\n\nGeneralizing good cases of the formulations above, we can suppose that the space \\\\(B\\\\) of behaviours is a *measure space*: it comes equipped with a distinguished collection \\\\(\\\\mathcal{M}(B)\\\\) of subsets of \\\\(B\\\\) called *measurable sets*, and a mapping \\\\(\\\\mu: \\\\mathcal{M}(B) \\\\to \\\\mathbb{R}_{\\\\geq 0} \\\\cup \\\\{\\\\infty\\\\}\\\\) called a *measure* which assigns a 'size' to each measurable set. This data is subject to certain intuitive conditions such as the fact that a disjoint union of (countably many) measurable sets is measurable and its measure is the sum of the parts. This is a setting in which we can perform integration, and we shall use that to quantify accuracy. We shall call \\\\(\\\\mu\\\\) the *reference measure*; in principle, this measure represents an unnormalized zero-knowledge prior distribution over possible behaviours. Determining what this measure 'should' be is non-trivial, just as determining the metric on \\\\(B\\\\) for extrinsic measures of accuracy was non-trivial^[\\[4\\]](#fnqnqj002e7ht)^.\n\nGiven this setting, we suppose that we have mappings from explanations to \\\\(\\\\)*probability density functions* on \\\\(B\\\\), say \\\\(f: E \\\\to (B \\\\to \\\\mathbb{R}_{\\\\geq 0})\\\\), where being a probability density function means being \\\\(\\\\mu\\\\)-integrable and normalized such that \\\\(\\\\int_{x \\\\in B}f(e)(x)d\\\\mu(x) = 1\\\\) for all explanations \\\\(e\\\\). The probability density function corresponding to an explanation amounts to a distribution over the possible behaviours of what one would expect based on the explanation. Similarly, the observed behaviour is a measurable function \\\\(b: B \\\\to \\\\mathbb{R}_{\\\\geq 0}\\\\). With these, we can define the accuracy of an explanation to be the integral \\\\(0 \\\\leq \\\\int_{x \\\\in B}f(e)(x)b(x) d\\\\mu(x) \\\\leq 1\\\\).\n\nThe maths in this account is starting to get a little involved, but it's also approaching what happens in statistical analysis of real-world experiments: the behaviour is reduced to a description in terms of one or more real number values, hypotheses (explanations) are used to predict the values taken by this behaviour in the form of a distribution over real values; observations along with their precisions are used to produce a distribution corresponding to the behaviour, and these are compared by integration (albeit not always exactly via the formula I wrote above). The type of density function most commonly seen in this situation is a Gaussian distribution, and as long as we have enough observations, this is a sensible choice thanks to the Central Limit Theorem.\n\nThis approach must be adapted a little to cope with clashes between discrete and continuous behaviour. Formally speaking, the probability assigned to any given value in a continuous distribution is zero. If we can perform repeated experiments then we can model discrete behaviour as being sampled from a continuous distribution, which we can reconstruct and use as a proxy for computing the accuracy of our explanations. However, when our available observations are limited, we may need to find ways to coarse-grain our explanations in order to arrive at a useful measure of accuracy. I'll leave the discussion of these issues until a later time, but I expect these considerations to rear their heads when the time comes around to compute things in experiments or other concrete situations.\n\nThere are also variations of the measure-theoretic set-up where we do not assume an existing measure \\\\(\\\\mu\\\\) on \\\\(B\\\\); instead, either the explanations, the observations or both provide measures, which can be used to perform the integration. Since my experience in the direction of more exotic measure spaces is limited, I won't speculate about those right now.\n\nPowerful Explanations\n---------------------\n\nThe last two suggestions for measurements of accuracy also incorporated features which penalize explanations with lower explanatory power. In the relational case, there was scope to incorporate a weight inversely proportional to the number of behaviours compatible with an explanation, while the normalization of a probability distribution in the measure-theoretic set-up explicitly forces compatible behaviours to receive lower weight when they are numerous (since they must share the total probability mass of 1 between them). What other measures of the \"power\" of an explanation might we consider, though?\n\n### Lower dimensional explanations\n\nAn explanation can fail to be powerful if it only predicts aspects of behaviour and provides no information about other aspects. For example, if we are observing a charged particle moving in a (vacuum in a) box containing a vertical magnetic field, then a theory of Newtonian gravity will provide reliable information about the vertical motion of the particle but will tell us little about its motion perpendicular to a vertical axis. Our theory of gravity is a weak explanation for the particle's behaviour, and we might like to quantify this weakness in terms of the dimensionality of the predictions it makes.\n\nConsider a (suitably continuous) function \\\\(f: B \\\\to \\\\mathbb{R}^n\\\\) which sends a behaviour to the values of some set of observable properties; up until now we might implicitly have been identifying behaviours with their full sets of observable properties, so consider this a mapping onto the values of some subset of the observable properties. To extract a sensible notion of dimension with such a map, we shall need it to be surjective^[\\[5\\]](#fn7a63ugmsk9w)^, since otherwise we could extend by an inclusion into a space of higher dimension and get a similar kind of map (that is, the dimension \\\\(n\\\\) would be meaningless). In good situations, we can 'push forward' the structure on \\\\(B\\\\) which is used to compute accuracy along this map. We might like to say an explanation \\\\(e \\\\in E\\\\) has \"dimensional strength at least \\\\(n\\\\)\" if for all behaviours \\\\(b \\\\in B\\\\), the accuracy to which \\\\(e\\\\) explains \\\\(b\\\\) according to the pushed-forward structure is at least as good as the accuracy to which \\\\(e\\\\) explains \\\\(b\\\\) according to the structure on \\\\(B\\\\).\n\nThe trouble with dimensional strength is that, while it is bounded above by the dimension of \\\\(B\\\\) (again, assuming that \\\\(f\\\\) is suitably surjective), that's no help when \\\\(B\\\\) might be infinite-dimensional. Returning to our example, the collection of trajectories of a particle in a box is already a huge infinite-dimensional space, even after imposing conditions such as a starting point and velocity. Moreover, our gravity model accurately predicts the vertical component of the particle's position over time (for a classical particle, at least), and the space of vertical components of trajectories is again infinite-dimensional, so there is no upper bound on the dimensional strength of this model. Nonetheless, we can invoke a suitable idea of \"local dimension\" to recover a way to quantify strength in terms of dimension.\n\nThe domain of (algebraic) **dimension theory** provides some tools for formalizing these ideas. However, the definitions involved in dimension theory are rather sensitive to the framework/mathematical model under consideration. Since any concrete examples I examine in the course of this project will be relatively simple, I do not expect this measure of explanatory power to be invoked, but it's worth keeping in mind for more intricate models.\n\n### Explanations of Restricted Scope\n\nSome explanations are only relevant subject to specific conditions. Newtonian gravity, for example, is only \"valid\" in the limit of low energy, which means that the accuracy of its predictions depends on the quantities involved in the calculations being very small (compared to the speed of light and derived quantities). While there is not a precisely defined domain in which this theory holds, seeing as the accuracy just gets worse as the quantities involved grow, it shall be useful for me to write as if such a domain exists; I'll call this the **scope** of the explanation. It should be clear that an explanation with greater scope is more powerful.\n\nMeasuring the deficiency of scope of an explanation is challenging, because it requires us to identify ahead of time *all* of the variables affecting the behaviour being examined^[\\[6\\]](#fn10o61btmy15)^. Going back to the \"charged particle in a box containing a vertical magnetic field\" scenario from earlier, if we didn't know about the magnetic field, we would be surprised that our gravitational explanation failed to accurately predict the observed behaviour. Historically, unknown variables affecting behaviour of physical systems have frequently been identified only thanks to failures of scope, rather than the other way around!\n\nThe considerations above opens the door to an examination of my implicit assumptions. I have assumed in a few places that we can take several observations, but for these to be compatible (give use consistent information about the nature of the behaviour being observed), we need some guarantees that either the parameters affecting the behaviour are approximately constant across observations or that the variations in those parameters are included as part of the explanation. In most cases in experimental science, it must ultimately be assumed that the variables controlled in the experiment are exhaustive. I have little choice but to acknowledge and accept this assumption too, at least at the level of generality at which this post's discussion takes place. On the other hand, I expect that the models I will consider in this project will be small enough that all of their parameters are identifiable and explicit.\n\nSo, suppose that we have identified some list \\\\(P\\\\) of parameters as exhaustive for the purposes of our observation scenario; we could assume that these parameters take real values, so we have a space \\\\(\\\\mathbb{R}^{|P|}\\\\) of parameter values. A naïve approach for expressing explanatory power would be to look at how many of the parameters the explanation incorporates, but there are immediate problems with this. Our parameter list might be excessive, in the sense that some parameters might genuinely be irrelevant to the behaviour being observed. Conversely, it's easy to artificially include a parameter in an explanation in such a way that it makes no concrete difference to the predictions output by the explanation.\n\nInstead, we consider the following blanket method for measuring the failure of scope of an explanation. First, we introduce a further probability measure \\\\(\\\\eta: \\\\mathcal{M}(\\\\mathbb{R}^{|P|}) \\\\to \\[0,1\\]\\\\) (in good cases, this can be described in terms of a probability density function) on the possible parameter values, expressing either the frequency with which we expect parameter values to occur or the frequency with which they are actually observed. Then we can think of an explanation also as being parametrized by this measure, and we can measure the combined accuracy and power of an explanation by integrating the accuracy measurement over the measured parameters. As a formula, if we take the measure-theoretic determination of accuracy too, we have \\\\(Q := \\\\int_{p \\\\in \\\\mathbb{R}^{|P|}} \\\\int_{b \\\\in B} f(e(p))(b) d\\\\mu(b) d\\\\eta(p)\\\\). This isn't a direct measure of deficiency of scope, but rather a measure of how good the explanation is across the range of possible parameters, which thus penalizes an explanation at parameter values outside of the range where the explanation produces accurate predictions.\n\nI am not yet committed to the measure-theoretic calculations, but I must admit that they provide me with a rather flexible geometric mental picture which I have not attempted to present graphically in this post.\n\n*Edit:* I had some conversations this week suggesting that a relational approach might also allow one to examine the two 'directions' of explanatory power described in this section, albeit in a not-immediately-quantified sense. But that might not matter: we only need to be able to compare explanations in terms of some ordering, and a partial ordering might be enough. I'll examine this in more detail in the near future.\n\nSimple Explanations\n-------------------\n\nWhile accuracy and explanatory power appeared to come together in a few places in the sections above, simplicity (or, dually, complexity) of explanations is fundamentally distinct from it. The reason for this is that both accuracy and explanatory power are determined on the basis of a comparison between predicted and observed behaviour, while complexity is measured in terms of the content of an explanation and how difficult it is to extract predictions from it (independently of observed behaviour). This distinction will be reflected in our picture of how explanations can be compared later on.\n\n### Algorithmic complexity\n\nA standard measure of complexity which will be relevant to us is **Kolmogorov** or **algorithmic complexity**. This is calculated by assuming that the explanation can be expressed as the output of an algorithm (which takes some parameters as input); the algorithmic complexity measures the *shortest algorithm* outputting the explanation^[\\[7\\]](#fn0wkrgch2urp)^. We shall consider a variation of this, where the explanations are themselves given as algorithms, so that complexity is simply a measure of the size/length of the algorithm.\n\nWhile conceptually straightforward, there are a rather large number of factors which go into extracting a value here. What language is the algorithm written in? What are the permitted operations? In the case of continuous inputs and outputs, how is this data represented? We need a representation of members of \\\\(B\\\\), say; for a sufficiently constrained model of computation, expressing elements of \\\\(B\\\\) can be non-trivial even if they consist just of real numbers. More generally, several of the descriptions above have explanations expressed as *measures*, which are tricky to encode computationally!\n\nAll of these considerations make attaching a single number to algorithmic complexity... difficult. On the other hand, we don't need to worry too much about the details *a priori*, as long as we compare like with like. If we have two or more different types of explanation, as long as we constrain their expressions (the language, the permissible operations and representation of values) to be sufficiently similar as to ensure that the algorithmic complexities of the two classes are meaningfully comparable, then this should be adequate.\n\nAn apparent concern that remains is whether the resulting comparison of complexity is consistent. There are two versions of this problem. The \"local\" version consists of the observation that it might be that I can formalize explanations \\\\(e_1\\\\) and \\\\(e_2\\\\) in two languages such that in one language the complexity of \\\\(e_1\\\\) is greater, while in the other the complexity of \\\\(e_2\\\\) is greater. The \"global\" version applies the same reasoning to features such as the global minimum of complexity (of some subset of explanations) rather than the complexity of individual explanations. One might be inclined to believe that this problem is only a real concern if the basic expressions of the explanations are insufficiently formal: if I have an explanation in words, the potential inconsistency in algorithmic complexity is a direct consequence of the fact that there may not be a strictly consistent way to translate these explanations into algorithms. In fact, even if explanations are presented as algorithms in the first place, and even if we only allow translations between languages which are *purely substitutional*, in the sense that individual operations and symbols in one language are transformed consistently into strings of operations and symbols in the target language (so that complexity cannot be lost by squashing operations together), the problem still remains. It's a problem of counting, which I'll illustrate.\n\nConsider two abstract algorithms for explanations producing a single whole number (which we're thinking of as a predicted behaviour) with a whole number parameter \\\\(p\\\\). The first is, \\\\(p \\\\mapsto M(p)\\\\), the second is \\\\(p \\\\mapsto N(N(p))\\\\); here \\\\(M\\\\) and \\\\(N\\\\) represent basic operations in my language. If we calculate algorithmic complexity by just counting the operations (and ignore the brackets) then these have complexity 1 and 2 respectively. Clearly, the latter is more complex. But if I have another language containing the operation \\\\(N\\\\) but not the \\\\(M\\\\), and the operation \\\\(M\\\\) can only be represented by three applications of another operation \\\\(m\\\\), then suddenly the first algorithm has become \\\\(p \\\\mapsto m(m(m(p)))\\\\), and increased to complexity 3, higher than 2!^[\\[8\\]](#fni8zrlt863it)^ Note that this is not just an artefact of my tweaking the definition of Kolmogorov complexity: if the two languages have only \\\\(M,N\\\\) and \\\\(m,N\\\\) as their respective basic operations, then these are easily the shortest algorithms for the respective computations.\n\nIt's not such a stretch to expect that how goal-directed an agent seems, or which explanation for its behaviour is simplest or best, depends on the language we're using. After all, it's a lot easier to describe situations where we have words adequately describing all of the elements involved (it's no wonder magic is invoked as an explanation for the functioning of complex devices in places where those devices are not familiar)... \n\n### Computational complexities\n\nOn the other hand, it would philosophically be strange if an eventual conclusion about whether behaviour is goal-directed were strongly dependent on the choice of *computational language* used to express goals, since any specific choice of language can seem arbitrary. As such, we might instead want to restrict ourselves to **properties of algorithmic complexity which are invariant (**to some extent) under changing language, rather than the raw algorithmic complexity with respect to any particular language.\n\nCoarser-grained notions of complexity include the many computational complexity classes in computer science, which are defined in terms of the resources involved in the execution of algorithms. Besides providing coarser grained invariants, these have an extra advantage for the purposes of explanations relating to behaviour. Namely, if we have some known constraints on the architecture of the agent under consideration, we may be able to directly constrain the complexity class of feasible explanations. For example, an agent cannot be implementing an algorithm requiring more memory than it has.\n\nI will need a much deeper understanding of complexity theory than I currently possess to follow through with reasoning about a robust way to measure (aspects of) the complexity of an explanation.\n\nConclusions\n===========\n\n### The naïve picture and its flaws\n\nHere is a naïve picture we can build with the tools discussed in this article. Given a class \\\\(E\\\\) of explanations and some observed behaviour \\\\(b\\\\) from a class \\\\(B\\\\), we should be able to plot the complexity and accuracy/power of each explanation according to our chosen measure of these quantities. They might be plotted on some axes like these:\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/7d39dbbe350da3dbe427758285f55057cdefdd2b20cab13a.png)\n\n(I haven't included any data on these axes, because I don't know what a typical shape will look like yet!) The quality of the *class* of explanations is determined by the lower frontier of the resulting plot, which represents the best accuracy achieved by explanations as complexity increases. If the behaviour is well-explained by pursuit of a goal, for example, then small increases in the complexity of the goal-explanation should result in large increases in accuracy, and the plot will quickly approach the right-hand boundary. On the other hand, non-goal-directed behaviour will require complex explanations to output accurate descriptions of behaviour. Broadly speaking, we should compare two or more such plots in order to determine how well goal-based explanations fare against other types of explanation.\n\n### What's missing?\n\nOne way to separate goal-directedness from optimization is to include a model of the agent's **beliefs** in the explanation. This adds a lot of potential depth to examples compared with the examples considered in the \"accurate explanations\" section above. I will discuss this soon.\n\nIn discussion at a conference this week, *causality* was suggested as a quality of explanations, in the sense that we prefer explanations that identify the factors leading causally to the observed behaviour, rather than just a compressed description of the observed behaviour. Einstein's explanation of Brownian motion vs the description as a random walk came to mind. It's unclear to me at this stage how to incorporate this into the framework I've outlined here, or if it falls into explanatory power somehow, as a feature that I haven't explicitly identified how to measure.\n\n**Do you think something is missing? Putting my intended applications aside, how else might you judge the quality of an explanation?**\n\n*Thanks to Adam Shimi for his feedback during the writing of this post and to participants at the Logic and transdiciplinarity: Mathematics/Computer Science/Philosophy/Linguistics week at CIRM for valuable discussions on this topic.*\n\n1.  ^**[^](#fnrefe37k2jowh56)**^\n    \n    That assumption may not be valid, of course; in existing AI we have explicit access to the source code, although not necessarily in a form that is useful from an explanatory perspective. I don't explore that possibility in this post, but...\n    \n2.  ^**[^](#fnrefa4j2h1oy9qa)**^\n    \n    I think that *some* relation between competence and goal-directedness is inevitable, since an agent with a goal that has no idea how to achieve that goal might act essentially randomly, to the effect that whether or not it has a goal is not easy to detect.\n    \n3.  ^**[^](#fnref01sfvpwpr4gk)**^\n    \n    A relation \\\\(R: X \\\\looparrowright Y\\\\) is called *total* if for each \\\\(x \\\\in X\\\\) there exists some \\\\(y \\\\in Y\\\\) with \\\\(xRy\\\\). This guarantees that each explanation is \"valid\" in the sense of describing some possible behaviour, although it may describe several behaviours.\n    \n4.  ^**[^](#fnrefqnqj002e7ht)**^\n    \n    I observe challenges and choices throughout this post. My intention in doing so is twofold. First, I want to emphasise that anyone employing any of these formulations will need to be explicit about their choices. Second, I want to be deliberate in pointing out where I am postponing choices for later.\n    \n5.  ^**[^](#fnref7a63ugmsk9w)**^\n    \n    For some suitably strong notion of surjectivity. Smooth and almost-everywhere a submersion is definitely enough, but this only makes sense if \\\\(B\\\\) is nice enough, in the sense of admitting a smooth structure. Assuming a topological structure on B, we could employ the concept of [topological submersion](http://nlab-pages.s3.us-east-2.amazonaws.com/nlab/show/topological+submersion) as a corresponding sufficient condition.\n    \n6.  ^**[^](#fnref10o61btmy15)**^\n    \n    There is a sense in which deficiency in scope is complementary or dual to the dimensional deficiency of the previous section: the scope is measured in terms of the number of dependent variables, where the dimension is measured in terms of the number of independent variables.\n    \n7.  ^**[^](#fnref0wkrgch2urp)**^\n    \n    A subtle but important point: the algorithm outputs the *explanation* (the goal, say) not the behaviour predicted by the explanation! \n    \n8.  ^**[^](#fnrefi8zrlt863it)**^\n    \n    Maybe \\\\(M\\\\) and \\\\(N\\\\) represent the operations \"multiply by 8\" and \"multiply by 3\" respectively, while the second language only allows the operation \\\\(m\\\\) of \"multiplication by 2\"."
    },
    "voteCount": 5,
    "forceInclude": true
  },
  {
    "_id": "oZCeun2v3Xd3ncrHt",
    "url": null,
    "title": "Goal-directedness: imperfect reasoning, limited knowledge and inaccurate beliefs",
    "slug": "goal-directedness-imperfect-reasoning-limited-knowledge-and",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "Goal-Directedness"
      },
      {
        "name": "AI"
      }
    ],
    "tableOfContents": {
      "sections": [
        {
          "title": "Questions I would like to solicit feedback on:",
          "anchor": "Questions_I_would_like_to_solicit_feedback_on_",
          "level": 3
        },
        {
          "title": "Constrained rationality[1]",
          "anchor": "Constrained_rationality_1_",
          "level": 1
        },
        {
          "title": "Imperfect Reasoning",
          "anchor": "Imperfect_Reasoning",
          "level": 2
        },
        {
          "title": "Limited Knowledge",
          "anchor": "Limited_Knowledge",
          "level": 2
        },
        {
          "title": "Inaccurate beliefs",
          "anchor": "Inaccurate_beliefs",
          "level": 2
        },
        {
          "title": "Explanations with constraints",
          "anchor": "Explanations_with_constraints",
          "level": 1
        },
        {
          "title": "The world models are part of the explanation",
          "anchor": "The_world_models_are_part_of_the_explanation",
          "level": 2
        },
        {
          "title": "Goals attached to world models",
          "anchor": "Goals_attached_to_world_models",
          "level": 2
        },
        {
          "title": "Different representations of knowledge",
          "anchor": "Different_representations_of_knowledge",
          "level": 2
        },
        {
          "title": "Implementing reasoning constraints",
          "anchor": "Implementing_reasoning_constraints",
          "level": 2
        },
        {
          "title": "Layered explanations",
          "anchor": "Layered_explanations",
          "level": 1
        },
        {
          "title": "A four-part conjecture",
          "anchor": "A_four_part_conjecture",
          "level": 2
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        },
        {
          "anchor": "comments",
          "level": 0,
          "title": "1 comment"
        }
      ],
      "headingsCount": 14
    },
    "contents": {
      "markdown": "*This is the second post in my Effective-Altruism-funded project aiming to deconfuse goal-directedness. Comments are welcomed. All opinions expressed are my own, and do not reflect the attitudes of any member of the body sponsoring me.*\n\nIn my [first post](https://www.lessestwrong.com/posts/KJPRC3cgtxSXpZEQZ/goal-directedness-exploring-explanations) I started thinking about goal-directedness in terms of explanations, and considered some abstract criteria and mathematical tools for judging and comparing explanations. My intention is to consider a class of explanations representing goals that an agent might be pursuing, and to directly compare these with other classes of explanations of the agent's behaviour; goal-directed behaviour will be behaviour which is better explained by pursuit of goals than by other possible explanatory models.\n\nHowever, in the process, I wasn't really able to escape the problem I mentioned in the [comments of my preliminary post](https://www.lessestwrong.com/posts/7X9KKqgZa7edknKPm/goal-directedness-my-baseline-beliefs?commentId=hHpF3LRgmeodYmDto): that goal-directedness seems like it should be mostly independent of competence. Any explanation in the form of a goal, interpreted as a reward function to be optimized, will have low predictive accuracy of an agent's behaviour if that agent happens not to be good at reaching its goal.\n\nI gave the example of a wasp getting caught in a sugar water trap. Explaining this behaviour in terms of the wasp wanting to collect sugar should produce reasonable results, but the explanation that the wasp wants to end up in the trap ranks better (for reasonably general choices of the measures of accuracy, explanatory power and complexity)! Since we have good reason to believe that wasps do not actually desire to be trapped, this problem demands a solution.\n\n**Questions I would like to solicit feedback on:**\n\n*   Are there ingredients obviously missing from my explanation break-down in this post?\n*   Can you come up with any examples of behaviour that you consider goal-directed which cannot be easily explained with the structure presented in this post?\n\n*NB. I have highlighted terms which I expect to reuse in future in the sense intended in this post in bold.*\n\nConstrained rationality^[\\[1\\]](#fn59zufzsumfs)^\n------------------------------------------------\n\nThe most straightforward remedy to separating competence from goal-directedness, which I will be exploring in this post, is to extend our (goal-based) explanations to include constraints on the agent. This will enable us to retain an intuitive connection between goal-directedness and optimization, since we could conceivably generate predicted behaviour from a given \"goal + constraints\" explanation using constrained optimization, while hopefully decoupling global competence from this evaluation. \n\nFirst of all, I'll qualitatively discuss the types of constraint on an agent which I'll be considering, then we'll see how these affect our space of goal explanations in relation to the mathematical frameworks I built up last time. Note that while I present these as cleanly separated types of constraint, we shall see in the subsequent discussion on implementing these constraints that they tend to overlap.\n\n### Imperfect Reasoning\n\nIn simplistic toy models of agency, we assume a complete model of the world in which the agent is acting. For example, in a Markov Decision Process (MDP) on a Directed Graph (DG^[\\[2\\]](#fnshxwe8xqxml)^), we are given a graph whose nodes represent the possible states of the world, and whose arrows/edges out of a given node represent transitions to world states resulting from the possible actions of an agent in that state.\n\nA common implicit assumption in such a set-up is that the agent has perfect knowledge of the *local* structure and state of the world they inhabit and the consequences of their actions: they know which state they occupy in the DG and which states are reachable thanks to their available actions. This is sensible for small, perfect-information environments such as an agent playing chess.\n\nEven in this most basic of scenarios, we already encounter problems with evaluating goal-directedness with goals alone: we typically think of an effective chess-playing agent as having the goal of winning, but when one is often beaten by a more skilled player, that explanation is formally evaluated as poor under the criteria we discussed last time. Indeed, if the difference in skill is substantial enough that the agent loses more often than it wins, the explanation that the agent is trying to lose might be evaluated more highly.\n\nThe crucial aspect that our goal-explanations are missing here is the *local* aspect that I stressed above. Sure, an agent knows the immediate consequences of its moves, but calculating all of the possibilities even a few moves ahead can create an unmanageable combinatorial explosion. An agent is far from knowing the global structure of the DG it's exploring; it can only perform **imperfect reasoning** about this global structure.\n\nLet's consider two chess-playing agents, *Deep Blue* and *Shallow Green*^[\\[3\\]](#fn59iw54zov34)^, where the former of the two is much more skilled at chess than the latter. The difference in skill between *Deep Blue* and *Shallow Green* has (at least) two components. The first is a difference in ability to effectively search ahead in the tree of moves. The second is a difference in quality of the agents' implicit encoding of the overall structure of the DG being explored, which is reflected in the criteria they use to evaluate the positions they come across in the search: an ideal chess agent would evaluate moves based on how much they improved their chances of winning across all ways the game might play out, whereas a simpler agent might only be able to compare the number of their pieces to the number of opposing pieces.\n\nEvaluating the skill of an agent could be challenging, but *that's not what we're trying to do here*. Instead, we just want to be able to capture the constraints on the agent in our explanations. Raw search capacity is an easy first step in this discrete setting, since the answer to \"at most how many position calculations can the agent perform per move?\" is a single number. The search algorithm being implemented and the evaluation criteria can be expressed in various ways, but there is no obstacle to bolting these onto our goal-based explanations; we shall see in the implementation section below that we actually don't want the search algorithm to be an independent component which is \"bolted on\", but rather a more goal-sensitive extension of the explanation. Nonetheless, this is sufficient evidence to expect that we should be able to build a class of explanations that includes the means of expressing a wide variety of constraints on an agent besides its goal.\n\n### Limited Knowledge\n\nA major implicit assumption in the previous subsection was that the agent's knowledge aligns exactly with the actual state of the world, at least locally. There are many situations in which that won't be the case, however, where the agent has **limited knowledge** of the state of the world on which to base its decision-making.\n\nThe closest example to the set-up of the previous section is that of a partially-observable environment. Consider a game of [liar's dice](https://en.wikipedia.org/wiki/Liar%27s_dice), where each player rolls their own dice but keeps the result hidden from the other players, and then take it in turns to make statements about the overall state of the collection of die values (the particular rules of the game don't matter too much here). Even if an agent playing this type game knows all of the possible present states of the game, they do not have sufficient knowledge to isolate which precise state they find themselves in.\n\nWe can also imagine situations where an agent's observations of the world are limited by something other than structural limitations on their information. The agent might have limited attention; by this, I mean an agent which is able *in principle* to observe any given detail of the world perfectly, but which only has space in its memory, or enough observation time, to examine a limited number of details.\n\nAlternatively, an agent might have imperfect sensors, so that even if they can examine every aspect of their environment, the quality of information they have about the world state is limited. Analogously, if an agent is getting their information about the world second hand, such as from another agent, then their knowledge will be limited by the reliability of that information. The 'sensors' example might make the limitations seem easy to estimate, since we can compute the resolution of a camera from its physical properties, but quantifying the reliability of second-hand information seems much less straightforward, and how an agent does this will depend on their internal beliefs regarding other agents (both here and below, I avoid digging any deeper into the problem of [embedded agents](https://intelligence.org/2018/10/29/embedded-agents/) for the time being). However, the aim here is not necessarily to ensure that we can practically find the best explanation, desirable as that might be, but rather to ensure that the space of explanations we consider is broad enough to cover everything that we could reasonably call goal-directed.\n\nAll of the limitations in this section are obstacles which humans experience in almost every situation, and accounting for them qualitatively could enable us to explain some of our \"irrational\" behaviour.\n\n### Inaccurate beliefs\n\nThe discussion so far has expanded our view to varieties of agent which carry approximate, partial, local knowledge of the 'real' state of the world. A third layer of complication arises when we allow for the possibility that the agent's internal understanding of the structure of the world differs from the 'real' one we have been assuming in our analysis. Such an agent carries **inaccurate beliefs** about the structure of the world.\n\nThe most accessible version of the phenomenon I'm talking about concerns problems of identity. On one hand, an agent may distinguish two apparently identical world states, and on the other an agent may fail to distinguish two distinct states; either direction can affect the apparent rationality of the agent. I'll illustrate the first of these two problems.\n\nConsider again one of our chess-playing agents, *Shallow Green*. To compensate for its mediocre planning skills, this cunning machine has built a small database of end-game strategies which it stores in its hard-coded memory, so that when it reaches a board state in the database, it can win from there. In one such strategy, it has a knight and a few other pieces on the board (no details beyond that will be important here). However, *Shallow Green* happens to track the positions of all of its pieces throughout its games, and to *Shallow Green*, each of the two knights has a unique identity - they are not interchangeable. In particular, if *Shallow Green* happens to reach the aforementioned board position from its database with the wrong knight on the board, it may be oblivious to the fact that it already knows how to win and (depending on how imperfect its reasoning is) could even make an error and lose from this position.\n\nModelling *Shallow Green*'s chess games with a DG world model, the above corresponds to splitting or combining of states in the DG. Other inaccurate beliefs we can express in a DG model include an agent's beliefs about which actions/transitions are possible in given states^[\\[4\\]](#fn3q4ras5mqfp)^; splitting or identification of edges^[\\[5\\]](#fnx4wddqi0ken)^, and mistakes about the target of an edge (result of an action)^[\\[6\\]](#fnka72hj6qtim)^.\n\nIn limited knowledge environments, inaccurate beliefs can take on many more forms. An agent can have inaccurate beliefs about the dynamics of the aspects of the world they lack information about. If the agent applies principles of reasoning based on those dynamics which are not sound for their environment, they can deduce things about the environment which are false. In the game of liar's dice described earlier, for example, an agent might infer that another player has at least one 6 showing on their dice based on that player announcing an estimate of the total number of 6s rolled; that assessment may or may not be true, depending on the honesty of that player! Even if their reasoning is sound, an agent may have to make a decision or form a belief based on inaccurate or partial information.\n\nThe description of beliefs as \"inaccurate\" of course rests on the assumption that there is some unique 'real' world against which we are assessing the agent's beliefs, and moreover that our own model of the world coincides with the relevant 'real' one. For toy models, this is fine, since *the world is no more and no less than what we say it is*. In a practically relevant situation, however, our model of the situation could be flawed or imperfectly simulated when we present it to our agents, and this fact [could be exploited](https://www.lesswrong.com/posts/7b2RJJQ76hjZwarnj/specification-gaming-the-flip-side-of-ai-ingenuity) by those agents in unexpected ways.\n\nMy personal solution to this is to invoke relativism: there are numerous possible models of the world (or agent-environment system) which we can use to ground our assessments of an agents actions, and the outcome of our assessments will depend on the choice we make^[\\[7\\]](#fnonsiaxnca39)^. Even if you think there should be a single real/accurate world model out there, it is prudent to [consider the possibility of shortcomings in our models](https://www.alignmentforum.org/posts/k54rgSg7GcjtXnMHX/model-splintering-moving-from-one-imperfect-model-to-another-1) anyway, and to examine how shifting between these affects our assessments of agent behaviour.\n\nExplanations with constraints\n-----------------------------\n\nHaving established some directions in which our agent may be constrained, we now need to establish how these can be built into our explanations (with or without goals). For this, we'll work from the bottom up through the last section.\n\n### The world models are part of the explanation\n\nFirst of all, we have to acknowledge that our chosen world model will determine some aspects of the evaluation of our agent, and so we formally include it in our explanations. For a fixed world model, the resulting overhead of explanatory complexity will be the same across all of the explanations we consider, so there is no loss in making this model implicit when we get down to calculating, but acknowledging that the model is a parameter will allow us to investigate how shifting between world models changes our assessments (as it appears to in the footnote examples^[\\[7\\]](#fnonsiaxnca39)^) later. Note also that we were already using the components of the world model (the states of the world, the concepts available in this model) in our descriptions of behaviours and explanations, so we're just making explicit what was already present here.\n\nI'll call the world model being incorporated into the explanation the **base model**, and the internal model which an explanation assigns to the agent will be called the **explainer model**. As a first approximation, incorporating the agent's belief inaccuracies into the explanation amounts to describing a transformation which produces the explainer model from the base model. It is this transformation whose complexity will contribute to the overall complexity of the explanation.\n\nLet's take this first idea and run with it a little bit. When we try to formalize transformations of world models, we immediately encounter a problem: which world models do we consider, and which transformations between models are allowed? This amounts to asking 'which [category](https://www.lesswrong.com/tag/category-theory) of models are we in?' Stuart Armstrong and Scott Garrabrant have proposed categories of [Generalized Models](https://www.alignmentforum.org/posts/nQxqSsHfexivsd6vB/generalised-models-as-a-category) and [Cartesian Frames](https://www.alignmentforum.org/posts/ewkYgtZapQRtDPT2F/additive-operations-on-cartesian-frames) respectively, as categories of world models, and Armstrong has [compared these two formalisms](https://www.alignmentforum.org/posts/wiQeYuQPwSypXXFar/cartesian-frames-as-generalised-models). Just as in my last post, I'm going to remain agnostic on the choice of category of models for the time being, but I will make some observations about good features this category should have if we're going to get anywhere.\n\n*   The objects of the category need to include both the base model and all of the possible explainer models we might want to consider. On the other hand, there shouldn't be too many models, for reasons that shall be discussed below. This shouldn't be a problem, since I expect we will never need to include models of arbitrary infinite cardinality, and that's enough of a restriction for my purposes.\n*   The morphisms/arrows in the category should be relational in nature, like those of Armstrong's generalized models, in order to be able to model all of the phenomena we discussed under \"Inaccurate beliefs,\" above. (Functions can model identification of objects/states/actions, but not splitting!)\n*   We should be able to assign *complexities*, or at least complexity classes,  to arrows in a systematic way. There is [some literature](https://www.cambridge.org/core/services/aop-cambridge-core/content/view/FB2CAC644C697FEF1092C285D784CAB4/S2050509420000262a.pdf/div-class-title-categorical-complexity-div.pdf) on assigning complexity values to arrows in categories, but none specific to this application for the time being, so that will be a further aspect of complexity theory to draw out in a later stage of this project.\n*   Objects of the category should have enough internal structure to enable us to talk about knowledge and reasoning of agents in them (for the subsequent subsections). I'll refer to internal states when alluding to this structure, although I'll lightly stress that there in no way has to be discrete internal states in the sense of the sets forming the components of objects in Armstrong's or Garrabrant's categories.\n\nLet's suppose we've chosen our category of models; I'll call it \\\\(\\\\mathcal{M}\\\\). Our base model corresponds to a choice of object \\\\(M \\\\in \\\\mathcal{M}\\\\), and the explainer model is an object \\\\(M' \\\\in \\\\mathcal{M}\\\\) equipped with a morphism from \\\\(M\\\\); equivalently, the explainer model is an object of the [coslice category](https://ncatlab.org/nlab/show/under+category) \\\\(M \\\\to M' \\\\in M/\\\\mathcal{M}\\\\).\n\nThis framework is crisp enough that we can immediately push against it to get a bigger picture. Consider, for example, the fact that attaching a single explainer model to an explanation is rather restrictive compared to how we were able to incorporate uncertainty (or imprecision) of prediction into our explanations last time. As long as our category of models is small enough, we could replace the proposed single object of the coslice category \\\\(M/\\\\mathcal{M}\\\\) with a probability distribution over objects. Or, taking some meta-considerations into account, we might even want to allow for uncertainty of our base model. These extensions will complicate the complexity story, but they also are broad enough to cover sensible Bayesian/empirical approaches to agent behaviour.\n\nA yet further level of complication is that the explainer model may need to be updated as the agent takes actions and receives new information^[\\[8\\]](#fn7kv4mcgjb9)^. This update process is subject to the same kinds of constraints on reasoning and knowledge that were discussed above.\n\nComing around to the partial information settings, the formalisms discussed thus far quickly start to look inadequate, since the problems we discussed were a result of interactions between agents, and incorporating functional models of agents into world models is hard! I may consider this in more detail in a future post. As for beliefs founded on inaccurate or uncertain information, we only need the explainer models to account for the features of the world which the agent could conceivably think possible; the actual likelihoods the agent assigns to the different features will be incorporated into the considerations on knowledge, below.\n\n### Goals attached to world models\n\nAt this point, the \"goal\" in goal-directedness is ready to re-emerge, at least for explanations involving goals. Just as world models can vary, so too can expressions of goals. In a DG model, we could represent goals as mere subsets of the possible states, as a reward function over terminal states (wins, losses and draws in chess, say), as a reward function which is cumulative over paths in the graph (possibly with a discount rate), and so on. As usual, we shall postpone the specific choice until later, so that the details of the choice don't interrupt the general reasoning.\n\nOne subtlety which our set-up thus far allows us to account for is that we can distinguish between a goal defined on the base model (an **external reward signal**) and a goal defined on the explainer model (an **internal reward signal**). By employing the latter, we can explain a host of \"outwardly irrational\" phenomena in terms of mistaken identity assumptions. For example, an animal that cannot distinguish poisonous berries from sweet berries will sensibly avoid both (as long as the poisonous berries are sufficiently prevalent), even if the difference is clear to an outside observer and the sweet berries are the best nutritional option available.\n\nFor consistency with the other sections, we shall take the following approach. When we consider goal-directed behaviour, it is conceptually simplest to express those goals in terms of the world model we are ourselves using; as such, external reward signals shall be the first approximation we consider. We can transform an external signal into an internal one by pushing its values along the transformation from the base model to the explainer model, to produce an internal reward signal. Then we can transform the result a little to account for details of internal reward signals which cannot be represented in the external signal (this might be needed because the explainer model distinguishes states which are identical in the base model, for example). We see that the first step in this process is already enough to explain the example above: if the poisonous berries are assigned a reward of -100 and the nutritious berries a reward of +2 in the external reward signal, then assuming there is more than one poison-berry plant out of every fifty berry plants on average, the expected reward of the indistinguishable berries in the internal reward signal will be negative, and so much less desirable than less nutritious food sources.\n\n### Different representations of knowledge\n\nThe most basic representation of partial knowledge would be to consider a \"submodel\" of the explainer model \\\\(M'\\\\) representing the collection of states which the agent thinks it could be in. In light of the direction of developments in my [first post](https://www.lesswrong.com/posts/KJPRC3cgtxSXpZEQZ/goal-directedness-exploring-explanations), it should come as no surprise that I will immediately discard this in favour of a distribution over internal states of the explainer model; this is a generalization (at least for finite situations) because I can identify a \"submodel\" with the special case of a uniform distribution over the states it contains.\n\nThere are some subtleties here depending on which knowledge limitation model we are formalizing. In crisp partial-information situations such as liar's dice (or at least, the initial game state in a game of liar's dice!), we will be exactly encoding the distribution determined by the combinatorics of the set-up. For other situations, the best we can do *a priori* is encode a maximum entropy distribution based on the known constraints on the agent's knowledge, such as sensor quality parameters.\n\nHere we hit on an interaction between knowledge, reasoning and belief not covered in earlier sections: **bias**. It may be unreasonable to assume that an agent will, in fact, employ the maximum entropy distribution (conditioned on their knowledge), for two reasons. One is that this distribution is computationally expensive to compute explicitly, so they might only carry an approximation of the true distribution. Another is that they may have a biased prior distribution. For example, a human player of liar's dice will typically both be bad at computing probabilities of outcomes involving a large number of dice *and* may erroneously believe that rare events are less likely after they have recently occurred (a common mistake!). Ultimately, in analogy with transforming from base models to explainer models, I think the most comprehensive way to deal with these limitations is to include both the unbiased distribution (assuming one exists) and the deviation from that unbiased model (or instructions for computing that deviation) in the explanation.\n\n### Implementing reasoning constraints\n\nIn the last paragraph, we saw how constraints on an agent's reasoning impact the quality of their knowledge of the world. This effect corresponds to the limitations such as those on \"raw search capacity\" discussed earlier. These provide concrete predictions about how well the agent will be able to approximate the explainer model compared with optimal rationality, conditioned on the information at their disposal. From these, we can compute unbiased predictions of the agent's approximation of the goal signal.\n\nTo improve an explanation further, we need to take into account the other type of imperfection in reasoning, regarding an agent's inability to accurately compute the above unbiased approximation. In other words, we must take into account how the agent is locally misrepresenting the global structure of the goal on the explainer model. This could be described in terms of a perturbation of the unbiased prediction or instead as a model of the signal (such as a heuristic or proxy measures) which the agent is actually deploying. The difference between these possibilities might seem small at first glance, but if we choose the latter option, we encounter a curious obstacle, which may well represent a crux in my understanding of goal-directedness: there is no formal reason for a heuristic description of the agent's behaviour to be strongly coupled or correlated with the goal imposed earlier. In other words, this final description of 'what the agent is actually doing' can completely overwrite the goal which was supposed to be the key feature of the explanation! Stuart Armstrong [illustrates](https://www.alignmentforum.org/posts/ANupXf8XfZo2EJxGv/humans-can-be-assigned-any-values-whatsoever) the perfect-knowledge version of this problem: if we boil an explanation down to a reward function and a planning algorithm which turns reward functions into policies/behaviour then, without constraints, the planning algorithm could be a perfect contrarian which deliberately selects the actions with the worst returns against the reward function (but which happens to be acting on a reward function where that worst return produces the observed behaviour^[\\[9\\]](#fn7pwzmo1qt9o)^), or indeed a constant function that ignores the reward function. Moreover, taking a uniform reward function in the last case, all of these options have the same total Kolmogorov complexity up to adding a constant number of operations; since I haven't yet established precisely how I'll be measuring complexity of explanations, this is a good stand-in for the time being.\n\nI should stress that the problem emerging here is not that we have multiple different equally good explanations. It is not unusual for explanations to be underdetermined by the behaviour they are intended to account for, and in this post I have been adding further parameters to explanations, making them even more underdetermined. Nor is the problem that amongst these equally good and equally complex explanations, we can find some which are a goal-directed and others which are not, although this does mean I'll have to rethink my original [naïve picture](https://www.lessestwrong.com/posts/KJPRC3cgtxSXpZEQZ/goal-directedness-exploring-explanations#The_na_ve_picture_and_its_flaws) of how goal-directedness will be judged. As a consolation prize, these examples demonstrate that considering individual explanations in isolation could not be sufficient for quantifying goal-directedness, as I suspected.\n\nRather, the issue is that between the extreme options that Armstrong describes, where the respective levels of goal-directedness is reasonably clear (the optimizing agent and the perfect contrarian have policies completely dependent on the goal, the constant policy explanation is completely non-goal-directed, since it ignores the reward function), assessing how goal-based the explanations lying between these extremes are is tricky, and I do not expect the resulting judgements to be binary. This strengthens [my initial belief](https://www.lessestwrong.com/posts/7X9KKqgZa7edknKPm/goal-directedness-my-baseline-beliefs#What_do_I_think_it_is_) that there is a very blurry line between goal-directed behaviour and non-goal-directed behaviour.\n\nI will eventually need to decide how to integrate over the equally good explanations in order to reach a final numerical judgement of goal-directedness.\n\nLayered explanations\n--------------------\n\nThe structure of explanations emerging from the above discussion is a layered one. At each stage in the construction of our explanations, we find a 'default' assumption corresponding to perfect rationality and alternative 'perturbative' assumptions which take into account deviations from perfect rationality. Here's a diagram:\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/fc16deafc5a3c163e65acf1fd3cb612db058b74066adc82d.png)\n\n**Figure 1:** A diagram summarising the explanation breakdown we have developed. In each block, the first line is an \"unbiased\" default choice based on constrained optimal behaviour, and the second line is a perturbation incorporating further constraints on the agent.\n\nConsider again the example of the wasp. We can explain the behaviour as follows. For simplicity, we only consider world models consisting of spatial configurations of some relevant components, such as a wasp nest, flowers, other food sources and possibly some wasp traps. The transitions in this set-up might correspond to wasps flying or picking up substances. In the base model we recognize that a trap is a trap (there is little to no probability of the wasp escaping once it enters), whereas the explainer model over-estimates the possibility of the wasp being able to take actions which lead it out of the trap - this is the crucial new feature. We equip this explainer model with a reward signal for bringing food back to the nest (possibly having consumed it), with a strength proportional to how nutritious the food sources are.\n\nEven without refining this explanation to account for a wasp's limited knowledge (which will affect how quickly the wasp is able to find the food sources) and imperfect reasoning (accounting for the fact that the wasp may not easily be able to estimate their ability to carry or retrieve a given piece of food), this goal-based explanation already does a much better job of accounting for a wasp getting caught in the trap.\n\nReturning to our assessment criteria from last time, the accuracy and power of the explanations can be assessed as before, by generating predictions and comparing them with the observed behaviour. Meanwhile, the layered structure presented above gives us a systematic way to break down the complexity of explanations; this breakdown may eventually be leveraged to assess goal-directedness by the 'asymptotic' method alluded to at the end of my previous post.\n\nThere remains a question of where to draw the line in measuring the complexity. If the base model is fixed across all of the explanations being considered, then we may as well ignore its complexity; conversely, we *should* include the complexity of the transformation from the base model to the explainer model, since we cannot help but take the base model as the 'default' view of the world and the agent's world model as a perturbation of that default view. Whether or not there is a default (e.g. uniform/constant) goal on the world, the goal's complexity should be included in the count, which only adds a constant value for the default case.\n\nThe description complexity of the constraints on the agent's knowledge and reasoning seems like a reasonable choice to include. Note that these will usually differ significantly from the complexity of computing the respective unbiased and perturbed distributions determined by those constraints; the reason I choose to focus on the complexity of the constraints themselves is that I want the unbiased distributions to be considered the default, and so carry no intrinsic complexity contribution. Just as with the transformation of world models, it's the complexity of the *perturbation* that I want to keep track of. Formally stated, I will be thinking of complexity classes of each stage of the explanation relative to an oracle that lets me compute the previous parts for free.\n\n### A four-part conjecture\n\nYou may be familiar with the fact that [any behaviour is optimal](https://www.lesswrong.com/s/4dHMdK5TLN6xcqtyc/p/NxF5G6CJiof6cemTw) with respect to some utility/reward function. Of course, this is already true for the constant reward function for which all behaviour is optimal; the more pertinent observation is that we can instead choose a reward function which is positive on the exact trajectory taken by an agent and zero or negative elsewhere, or when there is some randomness involved we can reverse-engineer a reward function which makes only the given behaviour optimal. The result is an explanation with ostensibly high explanatory power and accuracy, scoring highly on these criteria in the sense discussed in my previous post (at least up to the point of empirically testing the explanation against future observations). The crucial point, however, is that this explanation will be as complex as the observed behaviour itself (so it will typically score poorly on our \"simplicity\" criterion).\n\nIn discussion with Adam Shimi about a draft of the present post, he suggested that instead of making the reward function (aka the goal) arbitrarily complex to account for the behaviour observed, we could equally well fix a simple goal on a sufficiently contrived explainer model to achieve the same effect. Again, there are simplistic ways to do this, such as squashing all of the states into a single state; the real claim here is that we can do this in a way which retains explanatory power. Examining the diagram above, I would extend this conjecture to further hypothesise:\n\n**Conjecture:** For each of the components in Figure 1, we can achieve (an arbitrarily good approximation to) perfect accuracy and explanatory power^[\\[10\\]](#fnjhuol321qj)^ by sufficiently increasing the complexity of that component while keeping the complexity of the other components fixed at their minimal values. A little more precisely:\n\n1.  Given a large enough category of world models, we can achieve perfect accuracy and explanatory power with a simple goal (and no constraints on knowledge or reasoning) at the cost of making the transformation from the base model to the explainer model arbitrarily complex.\n2.  For any fixed base model, we can achieve perfect accuracy and explanatory power (while taking the explainer model identical to the base model, and imposing no constraints on knowledge or reasoning) at the cost of making the goal arbitrarily complex; this is what Rohin Shah [proves](https://www.lesswrong.com/s/4dHMdK5TLN6xcqtyc/p/NxF5G6CJiof6cemTw?_ga=2.75040978.1185963496.1647696652-684697841.1647694967) in a general setting.\n3.  For any fixed base model and simple goal, we can achieve perfect accuracy and explanatory power (while taking the explainer model identical to the base model, and imposing no constraints on reasoning) at the cost of making the constraints on the agent's knowledge about the world arbitrarily complex.\n4.  For any fixed base model and simple goal, we can achieve perfect accuracy and explanatory power (while taking the explainer model identical to the base model, and imposing no constraints on knowledge) at the cost of making the reasoning constraints on the agent arbitrarily complex. Armstrong's [argument](https://www.alignmentforum.org/posts/ANupXf8XfZo2EJxGv/humans-can-be-assigned-any-values-whatsoever?_ga=2.118227785.1185963496.1647696652-684697841.1647694967), which we saw earlier, more or less demonstrates this already.\n\nBased on Armstrong's argument, I expect that the complexities of the explanations required for the respective components will all be very similar. This conjecture is not stated crisply enough for the not-yet-proved parts to be amenable to a proof as stated, but I expect anyone sufficiently motivated could extract and prove (or, more interestingly, disprove!) a precise version of it.\n\nTo the extent that it is true, this conjecture further justifies the approach I am exploring of comparing explanations whose complexity is carefully controlled, since we can infer that explanations in any sufficiently comprehensive class will achieve the same results as far as explanatory power and accuracy go. Note that the individual parts of the conjecture are genuinely different mathematically, since they extend the space of explanations being considered in different directions, although this difference is least transparent between knowledge and reasoning.\n\n*Thanks to Adam Shimi for his feedback during the writing of this post.*\n\n1.  ^**[^](#fnref59zufzsumfs)**^\n    \n    When selecting the title of this section, I didn't make the connection with [bounded rationality](https://www.lesswrong.com/tag/bounded-rationality). There is some overlap; I expect that some ideas here could model the problems presented in some of the bounded rationality posts, but I haven't thought deeply about this.\n    \n2.  ^**[^](#fnrefshxwe8xqxml)**^\n    \n    [Directed acyclic graphs](https://www.lessestwrong.com/posts/Pd8Fb37BAYxp68Zh5/fun-with-dags) (DAGs) are pretty common, but there's no need for us to constrain ourselves to acyclic graphs here.\n    \n3.  ^**[^](#fnref59iw54zov34)**^\n    \n    If you're not familiar with chess, some of the later chess-based examples might be a bit obscure, so I'll stick them in the footnotes.\n    \n4.  ^**[^](#fnref3q4ras5mqfp)**^\n    \n     If *Shallow Green* doesn't know about castling, then there are transitions missing from its internal model of the chess DG.\n    \n5.  ^**[^](#fnrefx4wddqi0ken)**^\n    \n    *Shallow Green* sometimes likes to turn their kingside knight around to face the other way when it moves; from the point of view of the other player, moving with or without rotating the knight count as the same move.\n    \n6.  ^**[^](#fnrefka72hj6qtim)**^\n    \n    An agent like *Shallow Green* might be aware of the *en passant* rule, but be confused about where their pawn ends up after performing it, and hence fail to use it at a strategically advantageous moment.\n    \n7.  ^**[^](#fnrefonsiaxnca39)**^\n    \n    After observing *Shallow Green* in bemusement across many matches, you might come to realise that they are actually expertly playing a slight variation of chess in which the distinction between the knights and the direction they are facing matters, and is negatively affected by castling. Of course, this requires that *Shallow Green* is a very charitable player who doesn't (or cannot) object to their opponents making illegal moves, but with this shift of world model, we suddenly have access to a simple, compellingly goal-directed, description of *Shallow Green*'s behaviour.\n    \n8.  ^**[^](#fnref7kv4mcgjb9)**^\n    \n    The types of base model we have been considering have included all possible world states by default, so are static and deterministic, but it's plausible that in more realistic/less crisply defined situations we may equally need to incorporate dynamics into the base model.\n    \n9.  ^**[^](#fnref7pwzmo1qt9o)**^\n    \n    Mathematically, this is just the observation that the only difference between maximizing a reward function and minimizing a loss function is a couple of minus signs, but once we've decided that our goals take the form of reward functions, the difference between a reward function and its negation becomes a significant one!\n    \n10.  ^**[^](#fnrefjhuol321qj)**^\n    \n    Here perfect accuracy and explanatory power are relative to the observations which are possible. In other words, I am making assertions about the explanations achieving the best possible accuracy and power, conditioned on fixing at least the base model (otherwise there could be an explanation which has better explanatory power because it explains a feature of the behaviour that the base model doesn't account for)."
    },
    "voteCount": 2,
    "forceInclude": true
  },
  {
    "_id": "jgYGZD2zRK6nncJd5",
    "url": null,
    "title": "Goal-directedness: tackling complexity",
    "slug": "goal-directedness-tackling-complexity",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "Goal-Directedness"
      },
      {
        "name": "AI"
      },
      {
        "name": "Kolmogorov Complexity"
      }
    ],
    "tableOfContents": {
      "sections": [
        {
          "title": "Setting the scene",
          "anchor": "Setting_the_scene",
          "level": 2
        },
        {
          "title": "Complexity, from the bottom up",
          "anchor": "Complexity__from_the_bottom_up",
          "level": 1
        },
        {
          "title": "Motivating examples",
          "anchor": "Motivating_examples",
          "level": 2
        },
        {
          "title": "So what is complexity?",
          "anchor": "So_what_is_complexity_",
          "level": 2
        },
        {
          "title": "Contexts for complexity",
          "anchor": "Contexts_for_complexity",
          "level": 1
        },
        {
          "title": "Complexity in topology",
          "anchor": "Complexity_in_topology",
          "level": 2
        },
        {
          "title": "Complexity in algebra",
          "anchor": "Complexity_in_algebra",
          "level": 2
        },
        {
          "title": "Between algebra and spaces",
          "anchor": "Between_algebra_and_spaces",
          "level": 2
        },
        {
          "title": "Complexity within a structure",
          "anchor": "Complexity_within_a_structure",
          "level": 1
        },
        {
          "title": "Piecewise maps",
          "anchor": "Piecewise_maps",
          "level": 2
        },
        {
          "title": "Elements of algebras",
          "anchor": "Elements_of_algebras",
          "level": 2
        },
        {
          "title": "Fragility of complexity measures",
          "anchor": "Fragility_of_complexity_measures",
          "level": 1
        },
        {
          "title": "Minimal presentations",
          "anchor": "Minimal_presentations",
          "level": 2
        },
        {
          "title": "Minimal decompositions",
          "anchor": "Minimal_decompositions",
          "level": 2
        },
        {
          "title": "Invariance at the cost of individuality",
          "anchor": "Invariance_at_the_cost_of_individuality",
          "level": 2
        },
        {
          "title": "Probabilistic contexts",
          "anchor": "Probabilistic_contexts",
          "level": 2
        },
        {
          "title": "Values for complexity measures",
          "anchor": "Values_for_complexity_measures",
          "level": 1
        },
        {
          "title": "Number vs Order",
          "anchor": "Number_vs_Order",
          "level": 2
        },
        {
          "title": "Symbolic complexity values",
          "anchor": "Symbolic_complexity_values",
          "level": 2
        },
        {
          "title": "Combinatorial complexity measures and projective parameter spaces",
          "anchor": "Combinatorial_complexity_measures_and_projective_parameter_spaces",
          "level": 2
        },
        {
          "title": "Computational Complexity",
          "anchor": "Computational_Complexity",
          "level": 1
        },
        {
          "title": "Classical computational complexity",
          "anchor": "Classical_computational_complexity",
          "level": 2
        },
        {
          "title": "Circuit complexity",
          "anchor": "Circuit_complexity",
          "level": 2
        },
        {
          "title": "Descriptive complexity",
          "anchor": "Descriptive_complexity",
          "level": 2
        },
        {
          "title": "Complexity measures for functions",
          "anchor": "Complexity_measures_for_functions",
          "level": 1
        },
        {
          "title": "Approximation complexity",
          "anchor": "Approximation_complexity",
          "level": 2
        },
        {
          "title": "Simple functions",
          "anchor": "Simple_functions",
          "level": 2
        },
        {
          "title": "Power series, Fourier series",
          "anchor": "Power_series__Fourier_series",
          "level": 2
        },
        {
          "title": "Functions on spaces",
          "anchor": "Functions_on_spaces",
          "level": 2
        },
        {
          "title": "Conclusions",
          "anchor": "Conclusions",
          "level": 1
        },
        {
          "title": "Coming soon...",
          "anchor": "Coming_soon___",
          "level": 2
        },
        {
          "title": "Further References",
          "anchor": "Further_References",
          "level": 2
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        },
        {
          "anchor": "comments",
          "level": 0,
          "title": "No comments"
        }
      ],
      "headingsCount": 34
    },
    "contents": {
      "markdown": "*This is the third post in my Effective-Altruism-funded project aiming to deconfuse goal-directedness. Comments are welcomed. All opinions expressed are my own, and do not reflect the attitudes of any member of the body sponsoring me.*\n\nMy strategy for achieving a formalisation of goal-directed behaviour is to equate it with \"behaviour which is well-explained in terms of goals\". So far, I have explored [criteria for judging explanations](https://www.lesswrong.com/posts/KJPRC3cgtxSXpZEQZ/goal-directedness-exploring-explanations) in general and a structured way to [decompose explanations](https://www.lesswrong.com/posts/oZCeun2v3Xd3ncrHt/goal-directedness-imperfect-reasoning-limited-knowledge-and) of agent behaviour specifically.\n\nOne of those criteria was simplicity, or its dual, complexity. In order to determine whether my [initial proposal](https://www.lesswrong.com/posts/KJPRC3cgtxSXpZEQZ/goal-directedness-exploring-explanations#The_na_ve_picture_and_its_flaws) for characterizing goal-directedness holds water, I need to get my hands dirty and learn some complexity theory. This post is a record of me doing that, while keeping my objective in sight. I have learned a lot of complexity theory/theories since I wrote my original post, so the present post is a major update from the somewhat naïve intuition I expressed there.\n\nSee the end for a few references not directly linked in the main text.\n\n### Setting the scene\n\nIn my initial breakdown of explanations, I explicitly assumed that an individual explanation could be assigned a value representing its complexity. Deeper investigation revealed at least four points where complexity might be introduced into an explanation. At the time, I wondered what measure of complexity I should use, expecting to be able to pick one (or several) up out of the box, and get down to the business of applying already-developed theory.\n\nIn attempting to apply the various complexity measures to my explanations as I was learning about them, I found each of them to be inadequate in one way or other. For example, Kolmogorov complexity and various branches of computational complexity explicitly involve a model of computation, such as a (choice of) Turing machine, but the explanations involved in goal-directedness have no obvious ties to computation, and any choice seems arbitrary. Moreover, a lot of the problems discussed in the various branches of complexity theory seem to be about sequences of problems or structures, whereas I am trying to attach complexity to individual explanations!\n\nWhile I may end up adapting some existing theory, my advisor Adam Shimi pointed out that rather than trying to jumble existing measures together in an *ad hoc* way, I should try to understand the underlying principles of complexity to the point of building a measure which captures what I actually care about. That's what I'm really aiming to do in this post.\n\nComplexity, from the bottom up\n------------------------------\n\nComplexity is a word which carries multiple competing, often conflicting, [intuitions and formalizations](https://www.lesswrong.com/posts/DshBToGnNbTBD7BSw/systems-theory-terms#Complexity). It has a lot of baggage. The aim of this post is to extract a core technical notion of complexity which is inclusive enough to cover everything contained in the various branches of complexity theory (plus some other domains besides), in an intuitive enough way to allow me to design a notion of complexity that captures what I need for evaluating explanations. In a single sentence, the concept of complexity I'll describe is a *quantity assigned to a structure*^[\\[1\\]](#fnzuffz4gk6d)^* that measures how many simple pieces are needed to construct it*.\n\nA word of caution to begin with: if you have a casual acquaintance with computational complexity theory, you'll be aware of **complexity classes**, whose members are parameterized *families* of problems. I will get around to those later, but the crucial first step, which was previously missing in my understanding of complexity, is an understanding of the complexity of (solutions to) individual problems.\n\nI should also stress that I'm examining complexity in a much broader context than is covered by any single discipline of complexity theory, but I'll discuss some specific branches of the domain later on.\n\n### Motivating examples\n\nSince I expect computer science to be more accessible to my audience than abstract maths, I'll start with two motivating examples in computational terms. \n\nWhy is multiplying big numbers more difficult than multiplying small numbers? When we actually sit down and compute a product of large numbers, our go-to method is *long multiplication*, where we perform a series of pair-wise multiplications of individual digits (using values that are engraved into our memories by early school experiences which were traumatic for some...) and then add up the results. The more digits there are, the more digit multiplications (and additions) we must perform, and the longer the calculation takes.\n\nIt may surprise you to discover that there are algorithms for multiplication which are [more efficient](https://en.wikipedia.org/wiki/Computational_complexity_of_mathematical_operations) than long multiplication for sufficiently large numbers.^[\\[2\\]](#fns0tz8fzy3j)^ We don't have to just multiply the digits of the numbers themselves; we can sometimes add and/or subtract digits before multiplying to reduce the number of basic multiplications we need to perform.\n\nFinding better algorithms is difficult enough that the existence of a better algorithm than long multiplication was a big surprise [even to the likes of Kolmogorov](https://en.wikipedia.org/wiki/Karatsuba_algorithm#History). One thing is clear, however: there is a smallest number of single digit multiplications and additions required to multiply an arbitrary \\\\(n\\\\)-digit number and an \\\\(m\\\\)-digit number; there is a *most efficient algorithm*. The size of that algorithm, in terms of the number of operations involved, is the **computational complexity** of the multiplication operation for numbers of these sizes.\n\nIf we found that adding digits is easier/less costly than multiplying them, then we might adjust our measure of complexity to reflect this, and this may even change which algorithm is considered the most efficient. More on that later.\n\nNow let's take an example of a different flavour. What is the complexity of a piece of text? For simplicity, let's restrict ourselves to a string \\\\(s\\\\) consisting of just the digits 0 and 1. Here are two possible examples:\n\n*   0101010101010101010101010101010101010101010101\n*   00101011110011010001010000111010001\n\nBefore we get started, I want to stress that *there is not just one answer here*. For example, I could consider the basic operations for writing strings to be \"write 0\"^[\\[3\\]](#fn9ysb8vvect)^ and \"write 1\". In that context, there is no more efficient way to produce these strings than to write them out one digit at a time, and the measure of complexity that we end up with is simply **string length***,* for which the former is the more \"complex\" of the two. On the other hand, if I have the comparative might of a universal Turing machine \\\\(U\\\\) at my disposal^[\\[4\\]](#fnrqsk9dm8zrj)^, I could consider the permissible string-generating procedures to be the inputs for that Turing machine (that is, the programs), and take the complexity to be the length of the *program* instead of the length of the *string*. The result is **Kolmogorov complexity**, \\\\(K_U(s)\\\\). The value computed depends on the specific choice of \\\\(U\\\\), but for many choices it's a lot easier to program \"write 01 twenty three times\" than to find a short program that produces the second string above. I could also choose any programming language as a substitute for a Turing machine^[\\[5\\]](#fnhxlvvjuek2c)^, which might make that last claim more credible.\n\n### So what is complexity?\n\nI can extract from these examples a general idea of what complexity should be.\n\n*   I begin by selecting a class of structures whose complexity I want to measure. In the examples above, these were \"functions defined on pairs of numbers of a given size\" and \"strings (of 0s and 1s)\", respectively.\n*   I select a generation procedure. In the first example, this was the class of algorithms which I was allowed to use (those involving just calculations in terms of individual digits). In the second, the original generating procedure was just \"writing down digits\", while the second was \"input into a Turing machine and consider the output\".\n*   I select a handful of basic operations or building blocks, considering these to require a constant small cost to perform. In the first example, these were additions and multiplications of single digits. In the second, we had individual digits (or 'the operation of writing a single digit') in the first instance and inputting single digits into a Turing machine in the second instance. Note that for the time being, the basic operations are assumed to be sufficiently varied to be capable of exactly generating the desired output.\n*   Given these ingredients, the corresponding complexity of a given structure is the minimum total cost of all choices of basic operations/components which generate it.\n\nThis turns out to cover all of the usual notions of complexity, of which we'll see many in a later section. As we've already seen, it also includes some simpler measures, such as size. As a general rule of thumb, I've found that *the more powerful the basic building blocks are, the coarser the notion of complexity one gets*. For example, if all algorithms are assigned the same constant cost, then multiplication of two numbers always has a complexity of 1, and this notion of complexity doesn't tell us much.\n\nA point I'll come back to in a little while is the fact that **any specific construction of a structure produces an upper bound on the complexity of that structure**. The challenge, for any definition of complexity, is always *finding* a construction which minimizes the complexity measure and *proving that it really is minimal*.\n\nContexts for complexity\n-----------------------\n\nImplicit in the above informal definition is that all notions of complexity depend on choices, the most fundamental of which is a choice of context: we consider the complexity of a structure as a member of a class of similar or related structures. Crucially, that context is itself structured! To understand what structure a context might have, I'm going to take you on a detour through some more abstract maths. To keep the spotlight on complexity, I'll examine the notions of complexity which arise in these contexts.\n\n### Complexity in topology\n\nTopology studies various types of space. Often, a class of spaces can be generated by gluing together a class of \"simple\" spaces in a predetermined way. I'll give some technical examples, but the details don't matter: I just need you to pattern-match, since the complexity measures I'll describe for these classes don't rely on the details.\n\n*   A **simplicial complex** is a space obtained by gluing together *simplices*, or the analogues of triangles in each dimension, along their faces/edges. A 0-simplex is a point, a 1-simplex is an edge or interval, a 2-simplex is a triangle (including its interior), a 3-simplex is a tetrahedron, \"and so on...\"\n*   A **CW-complex** is a space obtained by gluing disks together along maps on their boundaries. Here, \"disk\" means the ball of points at a distance of at most 1 from the origin in \\\\(n\\\\)-dimensional space, and the boundary of an \\\\(n\\\\)-disk is an \\\\((n-1)\\\\)-dimensional sphere. For example, the 1-disk is an interval, and its boundary is the pair of end-points. Gluing along a map on the boundary means that I identify the sphere on the boundary of a disk with its image along a function, as in this picture:![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/72399dd296d58cfbfa8f8b658dcc10fe70ef856a39086c44.png)\n*   A (smooth) **manifold** is a space which can be constructed by gluing together patches that look like open subsets of \\\\(\\\\mathbb{R}^n\\\\) along subsets of a similar form. These spaces are the subject of *differential topology* (and *differential geometry*)*.*\n*   A **scheme** is a space constructed by gluing together *algebraic varieties*. These are the subject of *algebraic geometry.*\n\nIn all of these cases, we are given a class of \"simple\" spaces, which usually means a space whose structure we can easily study directly and explicitly. These are used to construct more interesting spaces, often with emergent global structure that the simple spaces lack.\n\nThere are numerous notions of equivalence for spaces. For manifolds, say, we could consider invertible maps which preserve all of the local \\\\(\\\\mathbb{R}^n\\\\)-like structure (diffeomorphisms), invertible continuous maps with continuous inverses (homeomorphisms) or, most coarsely, the maps with \"quasi-inverses\" where the respective composites can be \"continuously deformed\" back to the identity (homotopy equivalences).\n\nA choice of recipe for constructing spaces, plus a choice of equivalence for the resulting spaces, results in a notion of complexity: *the complexity of a given space is the smallest number of simple spaces required to build a space equivalent to the given one*.\n\nThis isn't the most nuanced form of complexity at our disposal in this setting. In the case of CW-complexes, for example, this combinatorial notion of complexity doesn't take into account the potential complexity of the gluing maps (I'll get onto defining complexity of maps later). One might also want the dimension of the constituent spaces to contribute to the calculation: it's not outrageous to decide that a 8208-dimensional sphere is a little more complex than the familiar 1-sphere (circle).\n\n### Complexity in algebra\n\nAbstract algebra studies various types of mathematical object. Each class of objects is determined by the operations that it comes equipped with (its *signature*) and the equations which these operations are required to satisfy. Here is a list of examples of what that can mean, with some formal details deliberately omitted for conciseness.\n\n*   A **semigroup** is a collection of elements equipped with a *multiplication operation*, a way of combining any pair of elements to produce some other element.\n*   A **group** is a semigroup in which we can also divide by/cancel elements (and there is a special element which is neutral in the sense that multiplying by it does nothing).\n*   A **ring** is a collection of elements which we can add, subtract and multiply. A ring also has special elements called 0 and 1 which behave in the way you would expect with respect to addition and multiplication.\n*   A **vector space** over a field^[\\[6\\]](#fn6nvynucnkac)^ \\\\(K\\\\)is a collection of elements (called vectors), equipped with both *addition* and *scalar multiplication*, where the latter is a bunch of operations indexed by the element of the field.\n\nIf you aren't familiar with these, it's not important; all I want you to observe is the pattern of their descriptions: *elements* and *operations* (plus some rules about how the operations interact with one another, which I have omitted).\n\nIn any of these examples, if I take a pair of elements (possibly the same element twice), I can apply a binary operation such as multiplication or addition to them to produce a further element. I can further perform operations involving the result to produce more elements, and so on. In other words, it may be the case that I can construct all of the elements of the algebraic structure by applying the operations successively to a handful of *generators*. For example, the positive natural numbers form a semigroup under addition, and every number can be obtained by adding 1 to itself some number of times, so 1 *generates* the semigroup of positive natural numbers.^[\\[7\\]](#fnpkidne7z5k)^\n\nIf I take an unstructured collection of elements, a *set*, I can generate a structure of any of the above types by iteratively applying the formal operations to the members of this set. This produces the **free** structure generated by this set. For example, the free semigroup on the set \\\\(\\\\{a,b\\\\}\\\\) has elements \\\\(\\\\{a,b, aa, ab, ba, bb, aaa, \\\\dotsc\\\\}\\\\), where I have written the letters next to one another to represent multiplication.^[\\[8\\]](#fnlesmosn8hwb)^\n\nIf my general presentation of complexity from earlier made any sense, I'm hoping that you can already see that *the number of generators provides a notion of complexity* for algebraic structures.\n\nBeyond free structures, a **presentation** of an algebraic structure consists of a collection of generators combined with a collection of pairs of elements of the free structure they generate which are to be identified (typically presented as equations). For example, the following is presentation of a semigroup: \\\\(\\\\langle a,b \\\\mid aa = a, bb = b, ab = ba \\\\rangle\\\\)  \nI claim that the semigroup it produces has just three elements, \\\\(\\\\{a,b,ab\\\\}\\\\); work out what the multiplication operation does if you like. Again, if it's not clear to you how one produces an algebraic object from a presentation, it doesn't matter too much: the key observation is that the *generators* and *relations* in a presentation, supplemented with the *generation procedure*^[\\[9\\]](#fnjnuvd11437p)^ for the given class of algebraic objects, provide all of the ingredients we need to view the size of a presentation as a measure of complexity of these objects. Just as in the topological case above, we must choose a notion of equivalence of structures in order to complete the picture, but the choice is much narrower for algebraic structures: we typically consider these to be equivalent if they are *isomorphic*, which is to say that there exists a structure-preserving bijection between their sets of elements.\n\nDetermining the structure generated from an arbitrary presentation is [famously difficult](https://en.wikipedia.org/wiki/Group_isomorphism_problem), in a formal sense: it's uncomputable in general, as are many of the measures of complexity which I'll construct here. On the other hand, any finite structure has a \"trivial\" finite presentation, where the set of generators is the set of all of its elements, and the equations express the result of applying the operations to the elements. If the class of structures being considered has \\\\(k\\\\) binary operations then the trivial presentation of a structure with \\\\(N\\\\) elements has \\\\(N\\\\) generators and \\\\(N^{2k}\\\\) relations. As a consequence, there must exist a *most efficient* presentation of any finite structure, and this situation is intuitively comparable to the notions of complexity we saw in previous sections. More generally, as long as we know that there exists a finite presentation of an algebraic structure (even if that structure is infinite), then there must be a smallest one, so this concept of complexity^[\\[10\\]](#fn7ntkldpbzzc)^ makes sense for all *finitely presentable* algebraic structures.^[\\[11\\]](#fnibznwevdaai)^\n\n### Between algebra and spaces\n\nCombining the above two sections, we can consider all sorts of constructions extending our categories of algebraic and/or topological objects. This section is deep into abstraction, so I'll keep it to a single example, but it foreshadows an approach to dealing with infinite structures which we'll see again later on.\n\nA **profinite group** is a topological group constructed as a projective limit of finite groups. Even though a given profinite group may not admit a finite construction (and may not even have a \"minimal\" construction in a meaningful sense) we can still extract a nuanced measure of complexity for these structures. Indeed, having determined a notion of complexity for finite groups such as the presentation length, we may define the complexity of a profinite group to be the maximum complexity of finite groups appearing in its construction, minimized across possible constructions. A particularly simple class of groups are the **cyclic groups**, with presentations of the form \\\\(\\\\langle a \\\\mid a^k = 1 \\\\rangle\\\\), where \\\\(k \\\\) is a positive integer and \\\\(1\\\\) represents the identity element of the group; with the above definition of complexity, it follows that the simplest (lowest complexity) class of nontrivial profinite groups are the *procyclic groups*, constructed as projective limits of cyclic groups.\n\nComplexity within a structure\n-----------------------------\n\nWhy did I bother with the abstract detour above? The answer is that the structures encountered in complexity theory can often be thought of as functions defined on spaces or as elements of algebraic structures.\n\n### Piecewise maps\n\nWhat does it mean for a space to be constructed by gluing together simpler spaces? One consequence is that when we define a map on the space, we can restrict that map to the simpler pieces, and conversely that if we are given mappings on the simpler pieces which are compatible with the gluing construction in a precise sense, then these mappings can themselves be glued together to define a mapping on the full space.\n\nAs such, as soon as we define a notion of complexity for functions on the simple spaces, we have numerous ways to assign complexities to the glued space: we could take a maximum or a weighted sum, say. I'll explain some ways of assigning complexities to functions later on.\n\n### Elements of algebras\n\nA choice of presentation of an algebraic structure \\\\(A\\\\) determines a natural notion of complexity for the elements of \\\\(A\\\\), namely the shortest presentation of that element in terms of the generators in the presentation. Here the generators are playing the role of the basic building blocks, and the algebraic operations are the ones we allow when constructing the elements.\n\nObserve that the trivial presentation corresponds to the trivial complexity measure I mentioned earlier, where everything has the same complexity. On the other hand, for a small generating set, the complexity becomes a lot more interesting.\n\nIt's also worth noting that we can vary the complexity measure significantly by varying the weights assigned to the different generators and operations in the structure. In a real vector space with a given basis, for example, we could assign the basis vectors a weight of \\\\(1\\\\), scalar multiplication by \\\\(r\\\\) a weight of \\\\(|r|^2\\\\) and addition a weight of \\\\(0\\\\), in which case the \"complexity\" of vectors is the sum of the squares of the coefficients determining the vector in that basis, the \"squared length\" of the vector when the basis vectors are considered to have length \\\\(1\\\\). Contrast this with what happens if we assign scalar multiplication a weight of \\\\(0\\\\) and addition a weight of \\\\(1\\\\), in which case the complexity just counts the number of basis vectors required to present a given vector (equivalently, the minimum dimension of a subspace generated by basis vectors containing the given vector). Both of these are clearly highly dependent on the choice of basis.\n\nFragility of complexity measures\n--------------------------------\n\nIn our examples, we have seen that the complexity measure we end up with depends on all of the choices we made along the way, including the context, the basic building blocks, the allowed operations for combining them and the weights assigned to each of those ingredients. Changing any of these choices not only affects the numerical value we end up with, it can completely change which elements or objects are considered more or less complex than others. This is not a profoundly surprising result, although the degree of sensitivity may be surprising; see the end of the section on computational complexity [below](https://www.lesswrong.com/posts/jgYGZD2zRK6nncJd5/goal-directedness-tackling-complexity#Computational_complexity) for an example. \n\nUntil recently, the fragility of complexity measures seemed to me to be an insurmountable problem. I don't want to be using an evaluation that depends heavily on apparently arbitrary choices; those choices shouldn't be influencing my results!\n\nFortunately, we can more often than not excise some arbitrary choices while salvaging a meaningful measure of complexity, through a variety of mechanisms. This is inherently a trade-off: the more we try to make our complexity classes independent of choices, the coarser-grained the notion of complexity becomes. For my own purposes, I'm hoping that I will nonetheless reach a complexity measure which captures the intuitions I'm seeking to formalize.\n\n### Minimal presentations\n\nFirst of all, we can shift the choices up a level, where they can seem less arbitrary. If our context is a (finitely presentable) algebraic structure, and we have chosen a sensible notion of complexity for presentations, we can consider the generators of a minimal presentation as a natural choice of basic building blocks (and we already know that this will at least give a more refined notion of complexity than a non-minimal presentation).\n\nIf there is a unique minimal presentation for the context, the above is an entirely natural choice. However, there may instead be several (even many) minimal presentations for the algebraic structure: think of choosing different bases for a vector space. In that case, we can construct a more robust notion of complexity as one which is invariant under change of minimal presentation. This can be achieved, for example, by taking the minimal complexity value across possible choices of minimal presentation. The vector space example turns out to be a bad one on its own, since any vector is a member of some basis (so the complexity measures we saw earlier can never be basis-invariant), but if we instead consider linear endomorphisms of a vector space, choosing a basis for the space enables us to present the endomorphism as a *matrix*, and there are various non-trivial invariants including the *rank* which are basis-invariant, and can be considered as notions of complexity.\n\nIf we take a vector space equipped with an *inner product*, then while there are still just as many choices of basis available, some of those choices have nicer properties: the best-behaved ones are *orthonormal bases.* (As ever, if those terms are unfamiliar, read around them. The key take-away is that in some contexts the minimal presentation in an algebraic sense is not automatically the best possible.) We could modify the complexity measure on presentations to take the inner product structure into account, or just impose the constraint on choices anyway. Either way, having selected some extra criteria which the minimal presentations of the context must satisfy, we reduce the number of constraints on a complexity measure when we demand it be invariant under change of presentation. The \"length squared\" measure that I described in the last section is invariant under change of orthonormal basis, for example.\n\nA compelling example of constrained bases in the definition of a complexity measure is that of [*tensor rank*](https://en.wikipedia.org/wiki/Tensor_(intrinsic_definition)#Tensor_rank)*.* One way of constructing a vector space is as a *tensor product* of a pair of vector spaces, say \\\\(V \\\\otimes W\\\\); an element of such a space is typically called a *tensor*. The natural bases for \\\\(V \\\\otimes W\\\\) are those constructed from bases of \\\\(V\\\\) and \\\\(W\\\\). Unlike for vector spaces more generally, not every tensor is a member of such a basis, and we therefore get a meaningful notion of tensor rank as the minimal number of basis vectors (across all possible constrained bases) required to present a given tensor.\n\n### Minimal decompositions\n\nMuch of the discussion above can similarly be applied to spaces: while there may be many decompositions of a space into simpler spaces, minimal decompositions provide natural basic components for complexity measures. Both here and above, if the minimum can be concretely identified, it will also necessarily be the simplest presentation of the space or algebra to work with from a computational point of view!\n\nConsider the circle, or 1-sphere. When presenting this as a manifold, the minimal decomposition involves two open line segments (open subsets of \\\\(\\\\mathbb{R}\\\\)), glued along a pair of sub-segments.\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/4646bc637b9a76034a7bc992a3dc0cfbb194c37f802611e1.png)\n\nThe red and green segments are (homeomorphic to) open intervals in \\\\(\\\\mathbb{R}\\\\); when we glue along the overlaps, we get a circle.\n\nClearly there are several choices to be made here about the four segment lengths (the segments being glued together and the lengths of the overlaps); although the end result as a space is always a circle, the choice of decomposition will affect our definition of complexity of functions on the pieces, and we can only obtain an invariant complexity measure by taking all of the minimal decompositions into account, which amounts to examining symmetries of the resulting space. The study of symmetries moves us beyond topology into *geometry.* Finding complexity measures for functions which are invariant under these re-parametrizing symmetries is an interesting challenge, which is beyond the scope of this post.\n\n### Invariance at the cost of individuality\n\nIn some cases, there will be no useful choice-invariant notion of complexity for individual structures. We saw earlier that Kolmogorov complexity requires a choice of universal Turing machine. However, given any string, we can design a universal Turing machine which makes that particular string have arbitrarily small or large complexity, essentially by manipulating how the programs are encoded.\n\nOn the other hand, Kolmogorov proved that his measure of complexity is choice-invariant up to a constant, as follows.^[\\[12\\]](#fna70tcdq76js)^ If I take any pair of universal Turing machines \\\\(U\\\\) and \\\\(U'\\\\), each can be compiled into the other, in the sense that there are strings \\\\(u\\\\) and \\\\(u'\\\\) such that for all input strings \\\\(x\\\\) and \\\\(y\\\\) (independent of \\\\(u,u'\\\\)) we have \\\\(U(u';x) \\\\equiv U'(x)\\\\) and \\\\(U'(u;y) \\\\equiv U(y)\\\\), where the semicolon denotes concatenation and the triple-line means \"produces the same behaviour (in terms of halting and outputs)\". As such, for any string \\\\(s\\\\), we have \\\\(K_{U'}(s) \\\\leq K_{U}(s) + |u|\\\\) and \\\\(K_{U}(s) \\\\leq K_{U'}(s) + |u'|\\\\). An immediate consequence of this fact is that, while we cannot produce a measure of the Kolmogorov complexity of an *individual* string which is independent of the choice of Turing machine, if we take a *sequence* of strings, we can extract coarser-grained complexity values which are independent of the choice.\n\nFor instance, the **asymptotic growth rate** of the Kolmogorov complexity of a sequence of strings is a choice-invariant notion. *Asymptotic* here roughly means \"on average in the limit of large indices\", the idea being that while we have little control over the exact lengths of programs required to generate individual strings on different Turing machines, the constant bounds on the difference between Turing machines of the complexity of strings in a sequence will merely be an \"error\" whose contribution can be made as small as we like by averaging across a sufficiently large number of strings in the sequence. An infinite stream \\\\(S\\\\) of digits also has a well-defined notion of complexity, by identifying \\\\(S\\\\) with the sequence \\\\((s\\_n)\\_{n \\\\in \\\\mathbb{N}}\\\\) whose \\\\(n\\\\)th term \\\\(s_n\\\\) is the string consisting of the first \\\\(n\\\\) digits of the stream. An essentially random stream will produce a complexity that grows linearly, \\\\(K\\_U(s\\_n) = O(n)\\\\), whereas an infinite string of \\\\(0\\\\)s has complexity which grows sub-logarithmically \\\\(K\\_U(s\\_n) = o(\\\\log(n))\\\\); the invariance means that we can drop the subscript \\\\(U\\\\) in these statements.^[\\[13\\]](#fnj3ige8zh36)^\n\nOne must be careful with the kind of shift of context we performed above. We haven't actually resolved the problem of having a well-defined complexity for individual strings: their natural substitutes as sequences, the constant sequences, all have the same complexity of \"no growth\", \\\\(K_U(s) = O(1)\\\\). Rather, **we have merely observed that sequences of structures (such as strings) form a context which is rich enough to leave interesting complexity measures after imposing invariance with respect to symmetries which made complexity measures on the individual structures trivial**.\n\nNote that the asymptotic growth rate would *not* work as a measure of complexity of the profinite groups we saw earlier, even if we restrict to those which are expressible as the (projective) limit of a sequence of finite groups. This is because any subsequence or re-indexed sequence will produce the same group in the limit, and so we can make the complexity of the sequence grow as fast or as slow as we like. On the other hand, if we put a constraint on the form of the sequence, like imposing a relation between the index \\\\(\\\\)\\\\(n\\\\) and a bound on the size of the \\\\(n\\\\)th group in the sequence, we might be able to salvage an alternative to the \"maximum complexity\" measure we proposed earlier.\n\nI should also stress that the asymptotic growth rate classes I have mentioned above are characterized by *upper bounds*, so that the class of \\\\(O(f(n))\\\\) sequences includes also all of the classes of \\\\(O(g(n))\\\\) sequences where \\\\(g(n)\\\\) grows more slowly than \\\\(f(n)\\\\). A more precise statement would be one of the form \\\\(K\\_U(s\\_n) = \\\\Theta(f(n))\\\\), meaning that the growth rate of the Kolmogorov complexity of \\\\((s\\_n)\\_{n \\\\in \\\\mathbb{N}}\\\\) is bounded above *and below* by constant multiples of \\\\(f(n)\\\\), which excludes slower growth rates.\n\n### Probabilistic contexts\n\nRather than sequences, we might consider subsets of the family of finite strings. For sufficiently uniform subsets, one can make statements about the Kolmogorov complexity which are independent of the choice of Turing machine. For example, an argument based on the pigeon-hole principle shows that if we take the collection of strings of length \\\\(n\\\\), even if we construct a universal Turing machine especially to compute these strings (so that all of the shortest possible progams produce strings of length up to \\\\(\\\\)\\\\(n\\\\)), since there are at most \\\\(2^n-1\\\\) programmes of length less than \\\\(n\\\\), at least one string in this set must have Kolmogorov complexity \\\\(n\\\\) (or higher) with respect to every universal Turing machine. By putting a uniform distribution on the strings of a certain length, we can express this as a probabilistic statement: for *any* universal Turing machine \\\\(U\\\\), the probability that a randomly chosen string of length \\\\(n\\\\) has \\\\(K_U\\\\)-complexity at least \\\\(c\\\\) [is](https://en.wikipedia.org/wiki/Kolmogorov_complexity#Compression) \\\\(1 + 2^{−n} − 2^{c-n+1}\\\\).\n\nRelatedly, in place of individual (deterministic) streams of digits, I could consider streams of digits which are generated according to a random process, and then extract statistical statements about the asymptotic behaviour of such streams, such as the *expected asymptotic growth rate*. **More generally, probabilistic variations on the context of structures under consideration can represent an alternative way to eliminate the dependence on choices, while retaining the same fundamental objects**. The cost is that the resulting evaluations of complexity will be probabilistic or statistical in nature.\n\nValues for complexity measures\n------------------------------\n\nUp until the last subsection, the only types of complexity value which I had presented to you were *numerical*, either whole numbers (counting a number of operations or pieces) or real numbers (measuring lengths or weighted sums). However, we just saw that we can attach values to complexity which are more subtle, such as growth rates attached to sequences of structures. What other kinds of values can complexity measures take?\n\n### Number vs Order\n\nIn other areas of maths, we see number systems extended in various ways. The name \"complex numbers\" might seem suggestive at this point as a potential target for valuing complexity. **Unfortunately, complex numbers lack a key requirement for complexity measures**, namely *order*. When defining the complexity of a structure, we want it to identify the *minimum* amongst possible values associated to that structure, and with complex numbers there is no natural ordering to minimize along. There are artificial ways to attach an ordering, like looking for a complex value with minimal modulus or considering only complex values with positive real and imaginary part and associating some ordering on these, but both strategies amount to lifting the ordering on the (positive) real numbers (resp. possible orderings on \\\\(\\\\mathbb{R}_+ \\\\times \\\\mathbb{R}_+\\\\)), at which point we might as well use those, since the extra structure on the complex numbers is essentially being discarded.^[\\[14\\]](#fniliok2wcsl)^\n\nOn the other hand, the [**ordinals**](https://en.wikipedia.org/wiki/Ordinal_number)**,** a generalization of numbers indexing (possibly infinite) discrete lists, have sufficient structure to act as complexity values, since by definition any set of ordinals has a least element. While this may seem rather abstract, ordinals are used to construct [heirarchies of functions](https://en.wikipedia.org/wiki/Fast-growing_hierarchy) such as the ones we encountered for classifying growth rates in the above. I suspect that more intricate number systems, like a countable fragment of the [surreal numbers](https://en.wikipedia.org/wiki/Surreal_number), could be equally be used to construct hierarchies of this kind. For example, we can produce functions [which tend to infinity arbitrarily slowly](https://math.stackexchange.com/a/576453/354508) as well as arbitrarily fast, and these can be included as \"infinitesimal factors\" to growth rate functions defining complexity classes.\n\nTotal orders are convenient for defining complexity classes, but they are not necessary. As long as our collection of values has arbitrary meets (greatest lower bounds), that's sufficient to define a minimum value, \"the\" complexity of the structure^[\\[15\\]](#fnnwvtmh0kz58)^. Incidentally, we have already seen some situations where our collection of complexity values is insufficient to express the complexity of some structures: the presentation complexity of an algebraic structure with no finite presentation is not defined. An alternative to delineating different sizes or qualities of infinity is to just give up and say that any structure which doesn't admit a value in our chosen collection has *infinite complexity* (the greatest lower bound of the empty collection, or top element of the order). If our complexity measure is supposed to express the difficulty of a problem or construction, this kind of collapse makes a lot of sense: there's little immediate benefit to delineating between different degrees of \"impossible\"!\n\nFor the remainder of this post, I'll talk about a **valuation structure** for a choice of values for complexity measures. It can be useful for a valuation structure to have further algebraic structure, as the positive reals and the natural numbers do. A formal reason for this is the subject of the next subsection.\n\n### Symbolic complexity values\n\n**Warning:** this is the most abstract subsection of the post, proceed with caution.\n\nOnce we have determined the generating operations and the building blocks for our measure of complexity, there remain choices to make about what values to assign to each of them, especially which features should have non-zero cost, and how these costs are combined in computing a complexity bound from a construction. Putting aside the last subsection for the time being and considering just the simplest case where the complexity values are taken to be positive real numbers, the precise values needed in practice may depend on implementation details. In the most concrete examples, these values might be determined by the relative speeds of the operations for a given (fixed) implementation on my hard drive.\n\nRather than making the judgement about what these values should be straight away, we could *interpret each basic building block as a (formal) variable* and *interpret each construction operation as a (formal) function*, so as to view the construction itself as an algebraic expression for the complexity bound it determines for the structure it defines. A complexity measure then amounts to an assignment of elements of our valuation structure to each variable and an interpretation of the formal functions as operations on the elements of the valuation structure. The complexity of a structure is the minimum (in the ordering on the valuation structure) of the resulting value over its possible constructions.\n\nA reader with experience in logic might prefer to think of it like this: a construction of a structure, such a presentation, is a purely syntactic expression in a particular language which comes equipped with predetermined semantics in the domain of interest, such as the class of algebraic structures. A complexity measure is a *second choice of semantics* for the same language, this time in the valuation structure; I might as well call the second semantics an **evaluation**. The complexity of a structure is the minimum evaluation of constructions of equivalent structures. (Beware that this syntax-semantics presentation might cause some confusion when we get onto implicit complexity theory later, since the constructions there are proofs but the structure generated by a proof is just its conclusion, which is also a syntactic object!)\n\nThis abstraction reveals the higher-order choices involved in defining complexity measures, especially the ones determining how we combine the costs associated to the building blocks when they are put together with an operation.^[\\[16\\]](#fnv5z1wep5ak)^\n\nIt is not clear to me whether this formulation of complexity is exhaustive. *A priori* there may be other contributing factors to complexity that a given evaluation fails to capture, but any examples in this vein that I can come up with seem to be accounted for by including extra operations in the constructions. In my original example of multiplication of large numbers in terms of multiplications and additions of individual digits by composition, for example, we could also include the cost of shifting digits around in these algorithms to account for how the time taken to multiply larger numbers in practice will grow faster than the number of single digit operations involved (for practical implementations).\n\n### Combinatorial complexity measures and projective parameter spaces\n\nI'll call a complexity measure **combinatorial** if the basic building blocks are assigned constant (positive real) values and all of the combinations operations are additive on the complexity (or possibly add constants; I'll leave that possibility implicit here). Our measures which counted the number of basic digit addition and multiplication operations, the string length and the presentation complexity are all combinatorial. This is arguably the simplest kind of complexity measure, if such a notion makes sense,^[\\[16\\]](#fnv5z1wep5ak)^ and is amenable to some specialised analysis techniques, which I'll sketch here.\n\nObserve that if we scale all of the values assigned to the basic building blocks in a combinatorial complexity measure by some positive real number, the complexity value for all constructions are scaled by the same number. Since complexity theory is mostly concerned with comparative evaluations of which presentations are more complex, it may therefore be useful to consider the bounds on complexity determined by given constructions in terms of *projective geometry*.\n\nConsider the basic operations of addition and multiplication of digits we considered earlier. We can define a projective space^[\\[17\\]](#fnt70obtdizdn)^ of parameter values for these operations, whose elements are the possible ratios of costs of performing them; the dimension of this space will be 1 (one less than the number of basic building blocks for the combinatorial complexity measure). Given a list of algorithms for multiplication of numbers of a given size in terms of these basic operations, we get a mapping from this projective space to another projective space, whose dimension is one less than the number of algorithms. The mapping sends each ratio of parameter values to the corresponding ratios of complexity bounds determined by the algorithms. This mapping characterizes, amongst other things, the break-even points for which of the different algorithms under consideration give the best upper bound on the complexity, as a function of the cost ratio of the basic operations.\n\nThe same basic argument can be extended to any combinatorial complexity measure. I don't have the experience with projective geometry to take this analysis further than the above curious observation at the moment (I have not come across any literature on this direction so far). I might as well mention that I expect combinatorial complexity measures to be entirely adequate for my purposes in goal-directedness, so this section could get developed further by me in future.\n\nThe broad takeaway of this section is that **one more way of avoiding arbitrary choices, this time in the defining values of a complexity measure, is to consider all possible choices together at once,** or at least to consider classes of choices. A specific choice can either be made later (if a specific implementation will determine the values assigned) or coarser-grained conclusions can be drawn which are a function of the possible choices. In the latter case, the conclusions might be judgements about upper bounds on the complexity, as a function of the complexity parameters.\n\nComputational Complexity\n------------------------\n\nI have discussed the example of Kolmogorov complexity at some length. Let's see some domains of computational complexity theory, for balance (although there are surely plenty I haven't read about).\n\n### Classical computational complexity\n\nComputational complexity theory asks, \"how much of a given resource (time, space/memory, etc) is required to compute a value/solution?\" Just like Kolmogorov complexity, the answer to such questions depends on the details of the model of computation being considered. Even though \"computation\" is in the name, the mathematically interesting details of computational complexity are considered to be those which are independent of specific implementation. For this reason, it is typical to consider an implementation-independent version, which just as above is most easily achieved by instead considering *sequences* of problems with a sensible indexing parameter. A large number of different models of computation can simulate one another with an encoding of inputs of at worst polynomial (even quadratic!) size. The *complexity classes* we end up with are measures of asymptotic growth rates, in the sense I described earlier, but these classes are coarser than those above, being defined up to a polynomial change of indexing of the sequences.\n\nThis coarse-graining also resolves another issue, related to the implementation of structures as *inputs* to models of computations. For example, if the input is a simple, undirected graph, should the input size measure the number of vertices or the number of edges? The latter grows approximately quadratically in the latter (a simple graph has at most \\\\((n^2 - n)/2\\\\) edges). There are other possible choices too, such as the sum of the number of vertices and edges; all in all, selecting the indexing amounts to choosing a complexity measure for graphs. Since the complexity classes are indifferent to the exact complexity measure employed for the structures, *on the condition that these are related to one another by polynomial bounds*, these choices can be safely ignored. as is the case for my graph example.\n\nThe question then becomes, \"how quickly does the quantity of a given resource required to compute a value/solution grow with the size of the input?\"\n\nTypical examples of complexity classes are:\n\n*   Deterministic time complexity classes, such as P (polynomial time), DLOGTIME (deterministic logarithmic time with random access^[\\[18\\]](#fn50sp0qm4xg8)^) and EXPTIME (exponential time). These measure how quickly decision problems in a sequence increase in running time, or number of operations performed, with the size of their inputs.\n*   Non-deterministic time complexity classes, such as NP (non-deterministic polynomial time) and coNP, which have several equivalent definitions. One of these is that they measure the growth rate of time taken to *check* a proposed solution to a problem is valid (resp. invalid).\n*   Deterministic space complexity classes, such as L (logarithmic space), PSPACE and EXPSPACE. These measure the growth rate of the amount of memory/working space required for a decision problem in terms of the size of the inputs.\n*   Nondeterministic space complexity classes like NL, NPSPACE and NEXPSPACE. [Savitch's theorem](https://en.wikipedia.org/wiki/Savitch%27s_theorem) shows that the latter two of these are equal to their deterministic counterparts.\n*   Interpolating classes such as SC ([Steve's Class](https://en.wikipedia.org/wiki/SC_(complexity))) which place simultaneous demands on time and space complexity of an algorithm.\n\nAll of these are defined for (variations on) Turing machines, but many of them have closure properties which allow them to be characterized in other ways.\n\nInstead of decision problems, we could consider computable functions, and determine the complexity classes for such functions by the same criteria as above. The analogue of P for functions is [sometimes called FP](https://en.wikipedia.org/wiki/Function_problem#Relationship_to_other_complexity_classes). The class FP is closed under composition, and was [characterized by Cobham](http://www.cs.utoronto.ca/~sacook/homepage/cobham_intrinsic.pdf) as the smallest class of functions with certain closure properties containing the functions \"append 0\", \\\\(s_0(w) = w0\\\\); \"append 1\", \\\\(s_1(w) = w1\\\\), and \"log-exponential\", \\\\(f(x,y) := x^{l(y)}\\\\) (where \\\\(l(y)\\\\) is the length of \\\\(y\\\\) in bits). In other words, the resulting class of functions has an implementation-independent characterization! Obtaining and manipulating intrinsic characterizations such as this is the subject of [**implicit complexity theory**](http://cs.unibo.it/~dallago/FICQRA/esslli.pdf)**.**\n\nMany containments are known between the various classes. A great number of the open problems in complexity theory ask whether such containments are strict. Most famously, it's easy to show that any problem which can be decided in polynomial time can be checked in polynomial time, so \\\\(\\\\mathrm{P} \\\\subseteq \\\\mathrm{NP}\\\\), but it's not known whether some problems checkable in polynomial time are *not* decidable in polynomial time; this is the famous P vs NP problem.\n\nWhy should the answer be difficult to determine? An *oracle* is a hypothetical component which can be added to a Turing machine which responds to certain queries (even uncomputable ones) efficiently; in the language of the present post, this amounts to including those queries or functions as basic operations. Baker, Gill and Solovay [proved in the 1970s](http://cse.ucdenver.edu/~cscialtman/complexity/Relativizations%20of%20the%20P=NP%20Question%20(Original).pdf) that there exist oracles which make the relativized P vs NP problem true *or false*. Computational complexity is very sensitive indeed to which operations are allowed and their respective costs. Several proof methods in computational complexity theory up to this point could be *relativised*, meaning that adding an oracle made no difference to their arguments. As such, the Baker-Gill-Solovay result demonstrates that such methods cannot distinguish P from NP; this is the **relativization barrier**.\n\n### Circuit complexity\n\nA more specialized branch of computational complexity deal with **circuits**, which are a more rigid notion of algorithm closer to the algebraic contexts I presented above, where the operations are logic gates. The complexity is measured in terms of various measures of the size of the circuits, including width and depth, and the classes are defined in terms of these and a number of other factors, such as the number of inputs that each gate is allowed to accept and the order in which logical operations are allowed to appear. I don't know much about these yet, so I'll just mention the **AC** ([alternating class](https://en.wikipedia.org/wiki/AC_(complexity))) **hierarchy** and **NC** ([Nick's class](https://en.wikipedia.org/wiki/NC_(complexity))) as interesting examples. \n\n### Descriptive complexity\n\nBy definition, a decision problem is the problem of determining whether a certain proposition (logical formula) is true for a structure. For a given class of finite structures, encoded in such a way as to allow their being input into a Turing machine, say, we can identify a proposition interpretable on these structures with its *extension*, which is to say the collection of structures which validate the proposition.\n\nSince propositions are formal statements with lots of structure, we have various notions of complexity for them, including length. The type of complexity that is considered in descriptive complexity theory is that of the **smallest fragment of logic** (the smallest family of logical operators or constructors) within which the proposition can be expressed. This is interesting in its own right in the context of this post, since it's a collection of complexity classes which is not a coarse-graining of a numerical complexity measure in any obvious sense. Some examples of fragments of logic include:\n\n*   **First order logic** (FOL) includes the usual negation (NOT), (finitary) conjunction (AND) and disjunction (OR) operations applied to the basic relations on the structures, as well as existential (\\\\(\\\\exists\\\\)) and universal (\\\\(\\\\forall\\\\)) quantification applied to variables ranging over the elements of the structures.\n*   **FOL with a transitive closure** augments the above with the ability to take the transitive closure of a relation.\n*   **Second-order logic** (SOL) allows for quantification over the family of relations defined on the structures.\n\nConversely, each fragment of logic defines a class of decision problems. Surprisingly, many of these classes [coincide with complexity classes](https://en.wikipedia.org/wiki/Descriptive_complexity_theory#Overview_of_characterisations_of_complexity_classes) listed above, which is another example of classes having characterizations independent of their relation to Turing machines or circuits.\n\nI could observe that the complexity values in descriptive complexity theory aren't closed under intersections in any obvious way. We have equivalence between propositions in different fragments of logic if they have the same extension which means that a language can be in multiple classes, but *a priori* the existence of equivalent propositions in two incomparable fragments doesn't imply the existence of an equivalent proposition in a smaller mutual fragment. The complexity of a family of structures is thus only formally well-defined if we can prove that this *is* the case, or if we allow \"intersection classes\" expressing the fact that a formula lies in multiple incomparable fragments of logic.\n\nDescriptive complexity theorists are not concerned with this problem, to my knowledge, because the class of fragments of logic is entirely open: we can always extend our logics with new connectives and constructions, so there is little hope of being exhaustive about which precise complexity class a class of structures lies in unless we impose some (potentially arbitrary) extra constraints.\n\nComplexity measures for functions\n---------------------------------\n\nI can now go about constructing some of the complexity measures I've been promising. The starting point is analysis, yet another branch of maths..!\n\n### Approximation complexity\n\nSuppose I want to assign a measure of complexity to a function. If the function is computable, we fall into the realm of computational complexity. But a function may be formally uncomputable in several different ways.\n\nOne obstacle is that the function may take values which are not accessible in our chosen model of computation. A machine which can only output natural numbers will struggle to express negative numbers; a machine which can express integers may not explicitly be able to output rational numbers, and a machine which can represent rational numbers may be unable to represent irrational algebraic numbers or arbitrary real numbers. Another obstruction that could arise is that the calculation must check an infinite number of cases to arrive at a conclusion, and so on some inputs never produces an answer.\n\nAlthough calculating an exact value is hopeless in such situations, in the first case we can at least gauge the complexity of *approximations* to these functions.\n\nFor the cases where the machine outputs integers, we can compute the complexity of the floor or ceiling of a given function, as a function of the input. If the inputs of the functions are also natural numbers, this reduces to ordinary computational complexity, but more generally we may have to choose a sequence of inputs from which to obtain a complexity class as the asymptotic worst-case upper bound on the complexity of the approximation.\n\nThe rational-to-real case is more interesting. Here, we can examine the resources required to achieve an approximation of a certain quality, which is to say produce an output within \\\\(\\\\epsilon\\\\) of the true value. In this situation we can fix the input value and consider the resources required to achieve ever better approximations of the function value; a natural choice of sequence here to measure the complexity along is to take \\\\(\\\\epsilon = 1/n\\\\) with \\\\(n = 1,2,3,\\\\dotsc\\\\) The asymptotic time complexity in this situation is sometimes called *convergence rate* or  **iteration complexity**. As we saw earlier with Kolmogorov complexity, we must fix a machine against which to measure the asymptotic complexity, since otherwise we could choose a machine at each stage which just outputs a pre-computed sufficiently good approximation; for reasonable complexity measures, the result should depend only on the function, rather than the choice of machine.\n\nThere is another implicit parameter choice being made here. I am using the absolute difference to judge the quality of the approximations, but other metrics are possible: for functions to the rationals, we could measure the \\\\(p\\\\)-adic distance to the true value for some \\\\(p\\\\). This choice of metric will be problem-dependent.\n\n### Simple functions\n\nRather than approximating individual values of the function (and possibly have to make a choice about which machine to use in the calculation and which input values to assess the complexity at), we can ask how difficult a given function is to approximate by a class of *simple functions*. This has a [specific meaning](https://en.wikipedia.org/wiki/Simple_function) in the theory of Lebesgue integration, namely a function obtained as a finite sum of (real multiples of) indicator functions for measurable subsets. That coincidence is entirely intentional, since the first example I want you to picture is that of approximating a function from (a bounded interval in) the reals to the reals [by piecewise constant functions](https://en.wikipedia.org/wiki/Lebesgue_integration#/media/File:Lebesgueintegralsimplefunctions_finer-dotted.svg). In this situation, I can ask how many constant pieces I need to use to obtain an approximation to the function within \\\\(\\\\epsilon = 1/n\\\\) of its true value, and hence measure the complexity of the function of the growth rate of that number as \\\\(n\\\\) increases. You could call this **integration complexity**, since the complexity determines how quickly the function's Lebesgue integral converges.\n\nEven for this notion of simple function, there are choices to make. Since measurable subsets are generated from open (or closed) intervals, we could insist that the simple functions be constructed from indicator functions of intervals, or we could allow arbitrary measurable subsets. Another consideration is whether we require that the function is approximated at all input values or merely \"almost everywhere\", the latter being a notion more compatible with the tools of measure theory; for continuous functions there is no difference.\n\nAs another consideration, we might wonder if the integration complexity of a function is invariant under affine transformations (that is, under translations and rescaling), following the geometric intuition that congruent shapes in the plane should behave in similar ways. The complexity is already invariant at the level of individual approximations under horizontal translation and scaling, since we can translate and rescale an optimal sum of simple functions to obtain an optimal sum with the same number of terms for a function so transformed. For vertical translation, we at worst need to add a single simple function (a multiple of the indicator function for the domain of definition, assuming it is sufficiently nice), and since growth rates are insensitive to constant changes, this makes no difference. Vertical scaling is more subtle: a simple function \\\\(s\\\\) with \\\\(|s(x) - f(x)| < 1/n\\\\) rescales to a worse approximation when the scale factor is larger than 1: \\\\(|ks(x) - kf(x)| = |k||s(x) - f(x)| < |k|/n \\\\). However, growth rates are coarse enough to absorb this kind of discrepancy as a contribution to the constant bounding factor, with the result that integration complexity is invariant under affine transformations! The cost of this invariance is that all simple functions have a complexity of \\\\(O(1)\\\\), although this too justifies the choice of name, since simple functions are those whose Lebesgue integral can be computed exactly with a finite sum.\n\n### Power series, Fourier series\n\nDepending on the nature of the problem, it may be more interesting to use other classes of functions to approximate the function and thereby measure its complexity. Power series, including Taylor series, are a very well-studied example of this. Given a function expressed as a power series, I can measure its complexity in terms of the growth rate of the coefficients, where now the types of functions which I encounter as growth rates will typically be sub-constant. Alternatively, I could transform these coefficients into a measure of the rate of convergence, defining the complexity of a series \\\\(\\\\sum_{j=0}^{\\\\infty}a_j x^j\\\\) defined over some domain \\\\(D\\\\) around \\\\(0\\\\), to be the asymptotic growth rate of the function \\\\(n \\\\mapsto \\\\min\\\\{N \\\\mid \\\\sum_{j = N}^{\\\\infty} a_jx^j < 1/n \\\\text{ in }D\\\\}\\\\).\n\nAs ever, there are alternative choices available here. I could measure the growth rate of the absolute convergence (replacing the summands with their absolute values in the last definition), for example. A natural feature of these complexity measures is that they are invariant under changing a finite number of coefficients, or equivalently under adding a polynomial to the power series. We also have invariance under affine transformations, comparably to the last subsection, as long as we also translate and scale the domain appropriately.\n\nOf course, power series are not the only way to decompose a function. We can also look at Fourier series, decomposing a function into sine and/or cosine functions, and construct similar definitions of complexity from the coefficients of such an expansion. In this case, the complexity being measured will be geometric in flavour: a function which is low in \"Fourier complexity\" is well-described by its low-frequency components, whereas higher complexity means that the high-frequency terms play an increasingly important role in the function's behaviour. Any finite sum of (discrete frequency) sine waves will have constant complexity. There is also a [relationship](https://en.wikipedia.org/wiki/Convergence_of_Fourier_series#Uniform_convergence) between the convergence rate of the Fourier decomposition of a function over an interval and the differentiability of the function.\n\nIt is interesting to contrast the notions of complexity arising from the two decompositions above. They are not directly comparable: a straight line has minimal power-series complexity but non-trivial Fourier complexity, and dually a simple sine wave has interesting power-series complexity but trivial Fourier complexity. This illustrates once again how complexity measures depend on our choices of basic building blocks or operations. There are, of course, yet other decompositions of functions that one might consider!\n\n### Functions on spaces\n\nI have talked extensively about functions on the reals. Much of the above can be extended directly to functions on the complex numbers. Only a little extra work is needed to define complexity measures for (real- or complex-valued) functions on higher dimensional spaces, more specifically on domains in \\\\(\\\\mathbb{R}^n\\\\), since we can define higher-dimensional simple functions, polynomials and harmonic waves easily, typically as products of the lower-dimensional ones. Some decisions need to be made, of course: for simple functions, are only rectangles allowed, or are indicator functions for more general measurable subsets acceptable? I won't examine the consequences of such choices here.\n\nWith these choices established, we can now construct complexity measures for functions defined on more exotic spaces. For example, if we are given a manifold \\\\(M\\\\) which has a finite decomposition as a gluing of subspaces of \\\\(\\\\mathbb{R}^n\\\\), a function \\\\(M \\\\to \\\\mathbb{R}\\\\) decomposes into functions defined on those subspaces, whose complexity we know how to measure! In keeping with the methods discussed earlier in the post, we therefore define the complexity of such a function as the minimal sum across minimal decompositions of \\\\(M\\\\) of the complexity of the restricted functions. We can do something similar for CW-complexes, where the rigid simplicity of the components involved (disks) may afford a finer-grained notion of complexity.\n\nIt's surely possible to go further and define the complexity of functions between manifolds in terms of decompositions, but the higher you go in the definitions, the more work you have to do to ensure that your lower-level measures of complexity are invariant enough to combine together nicely into a well-defined (or interesting) higher-level measure of complexity.\n\nConclusions\n-----------\n\nMy hope is that this post opens the door to complexity for others. Here on LessWrong especially, people often make reference to some established notions of complexity including Kolmogorov complexity, but it sometimes seems to me that a more tailored measure of complexity would better reflect the intuition they are trying to formalize.\n\nA more ambitious hope of mine is that people will take some of these ideas and run with them. The most exciting possibility as I see it is a formal exploration of the relationships between complexity measures in different settings and their relationships with other invariants. Such work might form the foundation for new approaches to attack the hard open problems in computational complexity theory.\n\n### Coming soon...\n\nFor the purposes of my goal-directedness project, the next step is to use the ideas I've explored here in order to build a measure of complexity for goal-directedness. This will require me finally making some choices which I've been putting off for the whole project, but it will also get me pretty close to some concrete answers.\n\nI would also like to examine *relative complexity*, a topic which will be directly relevant to measuring the complexity of explanations, and which I think will be clarified by the view I've taken on complexity in this post.\n\n### Further References\n\n*The primary inspiration for this post was J. M. Landsberg's book* Geometry and Complexity Theory*, which was gifted to me by my postdoc advisor Thomas Seiller (thanks Thomas!). The book is about* **algebraic complexity theory***, a domain which applies algebraic geometry to computing (or at least bounding) the complexity of algebraic problems. While it doesn't discuss complexity theory at the level of generality that I have attempted to here, it was enough of a departure from the computational complexity notions that I had encountered previously to give me a new perspective on what complexity can (or should) mean.*\n\n*For Kolmogorov complexity, I found Marie Ferbus-Zanda and Serge Grigorief's paper* [Kolmogorov complexity in perspective](https://hal.archives-ouvertes.fr/hal-00201578/file/KolmogorovPerspective.pdf) *instructive.*\n\n*For computational complexity, I used some fragments of Sanjeev Arora and Boaz Barak's book* [Computational Complexity: A Modern Approach](https://theory.cs.princeton.edu/complexity/)*, although I could benefit from a deeper reading of that.*\n\n*Chatain's LW post on* [*Occam's Razor and the Universal Prior*](https://www.lesswrong.com/posts/RKfg86eKQuqLnjGxx/occam-s-razor-and-the-universal-prior) *was useful in understanding how K-complexity arises in Bayesian formalisms.*\n\n1.  ^**[^](#fnrefzuffz4gk6d)**^\n    \n    The word \"structure\" here is intentionally quite generic. The structure can be a mathematical object (a space, an algebra, a function...), or a computational object (an algorithm, a solution to a problem...).\n    \n2.  ^**[^](#fnrefs0tz8fzy3j)**^\n    \n    See the right-most column in the Multiplication entry of the table; long multiplication takes \\\\(O(n^2)\\\\) operations, meaning that the number of operations is approximately proportional to the square of the number of digits in the numbers being multiplied (assuming here that the numbers are of the same order of magnitude), but we can do much better: the first improvement was discovered by [Karatsuba](https://en.wikipedia.org/wiki/Karatsuba_algorithm) and reduces the number of basic multiplications needed to multiply 1000-digit numbers by a factor of 17.  \n      \n    A similar story is the surprising existence of more efficient algorithms for matrix multiplication, which is one focus of the domain of *algebraic complexity theory*.\n    \n3.  ^**[^](#fnref9ysb8vvect)**^\n    \n    For consistency with the Turing machine example that follows, this should probably be \"write 0 and shift right\".\n    \n4.  ^**[^](#fnrefrqsk9dm8zrj)**^\n    \n    A *Turing machine* is a pretty famous abstract model of computation consisting of some *tapes,* which are unbounded collections of discrete cells which are either empty or contain a symbols (usually 0 or 1), a device called the *head* that can read from and write to the tapes, and a *program* instructing the head how to interpret the symbols it encounters. If this isn't familiar to you, almost any programming language will do for the purposes of understanding Kolmogorov complexity.^[\\[19\\]](#fnvdu7ll1gawb)^ I can be informal about this, and about the obviously underspecified parameters in the description of the Turing machine, such as the number of tapes and their shape (does it go off to infinity in both directions or just one?), the language/symbols which the machine can read and write, and so on, thanks to [the result I quote later on](https://www.lesswrong.com/posts/jgYGZD2zRK6nncJd5/goal-directedness-tackling-complexity#Invariance_at_the_cost_of_individuality).\n    \n5.  ^**[^](#fnrefhxlvvjuek2c)**^\n    \n    Amusingly, finding the most efficient algorithm to produce a given output in various programming languages is a pastime with a [significant community around it](https://codegolf.stackexchange.com/).\n    \n6.  ^**[^](#fnref6nvynucnkac)**^\n    \n    A **field** is a ring where we can also divide by elements other than 0. I didn't include it in the list of algebraic structures, because the \"other than zero\" part turns out to prevent the existence of free fields!\n    \n7.  ^**[^](#fnrefpkidne7z5k)**^\n    \n    The more famous version of this example would be the inductive definition of the natural numbers in Peano arithmetic: any natural number can be expressed by applying the successor (\"add 1\") function to 0 the corresponding number of times. However, since the variety of algebras with just a single unary operation isn't as widely studied to my knowledge.\n    \n8.  ^**[^](#fnreflesmosn8hwb)**^\n    \n    Formally speaking, \\\\((aa)a\\\\) and \\\\(a(aa)\\\\) could be considered distinct elements were it not for one of the axioms of semigroups which I didn't mention, namely *associativity*. I mention this here to highlight how many of the formal details are missing from this account. I'm trying to paint you an intuitive picture, but if you're curious about what's under the hood, I would be more than happy to discuss it in the comments!\n    \n9.  ^**[^](#fnrefjnuvd11437p)**^\n    \n    Note that the generation procedure here is very sensitive to the type of structure we are considering: a ring generated from the presentation \\\\(\\\\langle a,b \\\\mid aa = a, bb = b, ab = ba \\\\rangle\\\\) would be infinite, since we have imposed no restrictions on sums of elements.\n    \n10.  ^**[^](#fnref7ntkldpbzzc)**^\n    \n    I don't know whether this notion is typically studied under the banner of complexity theory. As I mentioned in the introduction, I'm deliberately seeking an inclusive definition of complexity here.\n    \n11.  ^**[^](#fnrefibznwevdaai)**^\n    \n    Of course, it can also be extended to algebras which only admit infinite presentations, but infinite cardinals are a lot coarser than finite ones, and one has to start worrying about set-theoretic foundations, whereas the finite notions are a lot easier to treat constructively.\n    \n12.  ^**[^](#fnrefa70tcdq76js)**^\n    \n    I implicitly assume in this argument that I'm working with Turing machines taking inputs in binary, which is what's needed to make the \"up-to-constant' claim hold; if the Turing machines accept different alphabets, then I need to encode each into the other for the input to make sense, and this can modify the length of programs by a scale factor. For the conclusions regarding asymptotic growth rates that I mention, this subtlety is unimportant.\n    \n13.  ^**[^](#fnrefj3ige8zh36)**^\n    \n    I've used here, [Big Oh](https://en.wikipedia.org/wiki/Big_O_notation) and [Little Oh](https://en.wikipedia.org/wiki/Big_O_notation#Little-o_notation) notation here. This is a compact way to express asymptotic bounds on functions; I also use [Big Theta notation](https://en.wikipedia.org/wiki/Big_O_notation#Family_of_Bachmann%E2%80%93Landau_notations) later on.\n    \n14.  ^**[^](#fnrefiliok2wcsl)**^\n    \n    The point of this paragraph is to disillusion the reader about the existence of any strong existing connection between \"complexity\" and \"complex numbers\". Just as elsewhere in maths and science, it's entirely possible that a clever application of complex numbers^[\\[17\\]](#fnt70obtdizdn)^ could solve some problems in complexity theory, but I maintain that complex numbers do not constitute good candidates for complexity values.\n    \n15.  ^**[^](#fnrefnwvtmh0kz58)**^\n    \n    Up until this point, it has been the case that the complexity value directly measures a most efficient construction, presentation or decomposition of an object. In allowing more general structures, there arises the possibility of situations in which there is no minimizer: we could have a sequence of increasingly efficient constructions with no minimum, or a collection of minimal constructions which are incomparable in the ordering. This is not intrinsically problematic, but may require some care in general arguments about complexity.\n    \n16.  ^**[^](#fnrefv5z1wep5ak)**^\n    \n    Amusingly, the syntax-semantics presentation of complexity measures is sufficiently precise for us to begin asking questions about the **complexity of complexity measures**... I'll leave to some other brave soul the pleasure of untangling that ouroboros.\n    \n17.  ^**[^](#fnreft70obtdizdn)**^\n    \n    Although I have dismissed complex-valued complexity measures, when studying maps between projective spaces, complex values can help achieve a more complete global picture. This train of thought leads down a rabbit hole which I shall not explore here, but there is precedent for taking this idea seriously: recall that complex values were useful to electrical engineers in uniting resistance and capacitance, for example.\n    \n18.  ^**[^](#fnref50sp0qm4xg8)**^\n    \n    For the logarithmic time class to contain any interesting problems, we need to impose the assumption that moving along the input (such as moving any distance along the input tape of a Turing machine before reading) takes constant time, since otherwise the time taken to move is already linear in the input length. For example, assuming the input  \\\\(n > 0\\\\) is provided in binary with a leading 1, the function \\\\(n \\\\mapsto \\\\lfloor\\\\log\\_2(\\\\log\\_2(n)+1)\\\\rfloor\\\\) is a member of DLOGTIME since we can determine its value by initialising the output at 0, checking for the presence of a digit at positions \\\\(2^k\\\\) of the input for increasing \\\\(k \\\\geq 1\\\\), adding one if a digit is found and halting otherwise.\n    \n19.  ^**[^](#fnrefvdu7ll1gawb)**^\n    \n    Arora and Barak list Turing machines, lambda calculus, cellular automata, pointer machines, bouncing billiards balls and Conway’s Game of life as abstract models of complexity which can simulate one another and hence are *Turing complete*."
    },
    "voteCount": 4,
    "forceInclude": true
  },
  {
    "_id": "e936w9JzDP4WqQjcc",
    "url": null,
    "title": "Goal-directedness: relativising complexity",
    "slug": "goal-directedness-relativising-complexity",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "Goal-Directedness"
      },
      {
        "name": "Kolmogorov Complexity"
      },
      {
        "name": "World Modeling"
      }
    ],
    "tableOfContents": {
      "sections": [
        {
          "title": "Relativising Kolmogorov complexity",
          "anchor": "Relativising_Kolmogorov_complexity",
          "level": 1
        },
        {
          "title": "Conditional complexity",
          "anchor": "Conditional_complexity",
          "level": 2
        },
        {
          "title": "Extension complexity",
          "anchor": "Extension_complexity",
          "level": 2
        },
        {
          "title": "Modification complexity",
          "anchor": "Modification_complexity",
          "level": 2
        },
        {
          "title": "Oracular complexity",
          "anchor": "Oracular_complexity",
          "level": 2
        },
        {
          "title": "What is relativisation really?",
          "anchor": "What_is_relativisation_really_",
          "level": 2
        },
        {
          "title": "An algebraic example",
          "anchor": "An_algebraic_example",
          "level": 1
        },
        {
          "title": "Relative complexity of group elements",
          "anchor": "Relative_complexity_of_group_elements",
          "level": 2
        },
        {
          "title": "Conditional complexity",
          "anchor": "Conditional_complexity1",
          "level": 2
        },
        {
          "title": "Relativising to subsets",
          "anchor": "Relativising_to_subsets",
          "level": 2
        },
        {
          "title": "Conclusions",
          "anchor": "Conclusions",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        },
        {
          "anchor": "comments",
          "level": 0,
          "title": "No comments"
        }
      ],
      "headingsCount": 13
    },
    "contents": {
      "markdown": "*This is the fourth post in my Effective-Altruism-funded project aiming to deconfuse goal-directedness. Comments are welcomed. All opinions expressed are my own, and do not reflect the attitudes of any member of the body sponsoring me. The funding has come to an end, but I expect to finish off this project as a hobby in the coming months.*\n\nMy [previous post](https://www.lesswrong.com/posts/jgYGZD2zRK6nncJd5/goal-directedness-tackling-complexity) was all about complexity, and ended with an examination of the complexity of functions. In principle, I should now be equipped to explicitly formalise the [criteria I came up with](https://www.lesswrong.com/posts/KJPRC3cgtxSXpZEQZ/goal-directedness-exploring-explanations) for evaluating [goal-based explanations](https://www.lesswrong.com/posts/oZCeun2v3Xd3ncrHt/goal-directedness-imperfect-reasoning-limited-knowledge-and#Layered_explanations). However, several of the structures whose complexity I need to measure take the form of transformations from one structure of a given type to another. This brings us into the domain of *relative complexity.* I have chosen to present this in a separate post, not just because of the daunting length of my last post, but also because I think this is a subject which this community would benefit from some explicit clarification of. In future posts I shall **finally** put all of the pieces together and get to the bottom of this goal-directedness business.\n\nRecall that last time that I described complexity as *a quantity assigned to a structure that measures how many simple pieces are needed to construct it*. The general concept which is under scrutiny this time is a variant of the above, where we consider instead *a quantity assigned to a structure that measures how many simple pieces are needed to construct it **from a given resource or starting point***. A source of confusion lies in the fact that there are multiple ways to relativise complexity, which is why a closer examination is needed.\n\nIn this post I'll be discussing a few implementations on relativised notions of complexity.\n\nRelativising Kolmogorov complexity\n----------------------------------\n\n### Conditional complexity\n\nIn this subsection I'll be using standard notation and definitions from Ming Li and Paul Vitányi's [textbook](https://link.springer.com/book/10.1007/978-3-030-11298-1),^[\\[1\\]](#fnz6an60uitzk)^ which I will subsequently extend somewhat. They use the convention that the complexity measure is indexed by a *partial computable function* rather than a given Turing machine which computes it, which makes sense, since the details of the Turing machine are irrelevant in the computation of complexity beyond its input-output behaviour.\n\nThe definition of K-complexity for individual strings \\\\(s\\\\) which we saw last time involved a choice of universal Turing machine \\\\(U\\\\), computing partial function \\\\(\\\\phi: \\\\{0,1\\\\}^* \\\\to \\\\{0,1\\\\}^*\\\\), say, and measured the length of the shortest input \\\\(p\\\\) (encoding a program) such that \\\\(\\\\phi(p) = s\\\\). This gave the plain Kolmogorov complexity, which I'll now denote as \\\\(C_{\\\\phi}(s) := \\\\min\\\\{\\\\ell(p) \\\\mid \\\\phi(p) = s\\\\}\\\\).\n\n**Conditional Kolmogorov complexity** allows an auxiliary string \\\\(y\\\\) to be passed as a \"free\" input to the program, so that the corresponding complexity is \\\\(C_{\\\\phi}(s \\\\mid y) := \\\\min\\\\{\\\\ell(p) : \\\\phi(\\\\langle y, p \\\\rangle) = s \\\\}\\\\), where \\\\(\\\\langle y, p \\\\rangle\\\\) is the string obtained from \\\\(y\\\\) and \\\\(p\\\\) by concatenating them and adding a prefix consisting of \\\\(\\\\ell(y)\\\\) copies of \\\\(1\\\\) followed by a \\\\(0\\\\) (so that the Turing machine encoding \\\\(\\\\phi\\\\) can reliably separate the concatenated inputs). For conciseness, I'll denote \\\\(\\\\phi(\\\\langle y, p \\\\rangle)\\\\) as \\\\((y)p\\\\), interpreting \\\\(y\\\\) as an input to the program \\\\(p\\\\) (with \\\\(\\\\phi\\\\) playing the role of an interpreter or compiler).\n\nThis notion of conditional complexity contains some distinct possibilities which I alluded to in the introduction and would like to separate out; hopefully the algebraic example in a later section will help to clarify the distinct cases I'll highlight here.\n\nConsider a situation where \\\\(s = y\\\\). For many such cases, the shortest program \\\\(p\\\\) producing \\\\(s\\\\) given \\\\(y\\\\) is \\\\((y)p\\\\) := print\\\\((y)\\\\). Similarly, if \\\\(s\\\\) contains many copies of \\\\(y\\\\), we can use \"print(-)\" to insert those copies into the output being produced. On the other hand, if \\\\(s\\\\) has nothing to do with \\\\(y\\\\), the most efficient program will be one that simply ignores the extra input and produces \\\\(s\\\\) directly. In this way, the input \\\\(y\\\\) can be thought of as a *resource* for obtaining the output \\\\(s\\\\).\n\n### Extension complexity\n\nA special case of the above is when \\\\(y\\\\) is a prefix of \\\\(s\\\\), but where the remainder of \\\\(s\\\\) is not easily compressible^[\\[2\\]](#fn6did9fqm0ef)^ using \\\\(y\\\\). As long as \\\\(y\\\\) has non-trivial length, the most efficient program for producing \\\\(s\\\\) might be of the form \"print\\\\((y)\\\\) and then append \\\\(()q\\\\)\", where \\\\(q\\\\) is a program producing the suffix of \\\\(s\\\\) with no input. In this situation we are using \\\\(y\\\\) as a *starting point* for obtaining the output \\\\(s\\\\). I can refine this into an independent definition by removing the cost of the initial print command: \\\\(C'_{\\\\phi}(s \\\\mid y) := \\\\min\\\\{\\\\ell(q) : y \\\\,; \\\\phi(q) = s\\\\}\\\\), where \\\\(y\\\\, ;\\\\phi(q)\\\\) is the concatenation of \\\\(y\\\\) and the output of \\\\(\\\\phi\\\\) at \\\\(q\\\\). With this (non-standard) definition we can no longer use \\\\(y\\\\) as a resource, and we can only obtain strings which extend \\\\(y\\\\) (all other strings having infinite complexity). I hence call this the **extension complexity**.^[\\[3\\]](#fn759pommwh1p)^\n\n### Modification complexity\n\nAn alternative option, for a set-up where we work with Turing machines having separate input and output tapes, say, is to *initialize the output tape with a given string *\\\\(y\\\\). We could also impose further constraints on the model, such as proposing that the machine cannot read from the output tape, only write to it, so that the initial output cannot be used as a resource in the sense discussed above. I'll choose a universal machine set up this way, \\\\(U\\\\), and write \\\\(U_y\\\\) to indicate that this machine is initialised with \\\\(y\\\\) in the output tape. Then we get a notion which I'll call **modification complexity, **\\\\(C\\_U(s \\\\leftarrow y):= \\\\min\\\\{\\\\ell(p) : U\\_y(p) = s\\\\}\\\\), which measures the shortest program for generating the string \\\\(s\\\\) with initial output \\\\(y\\\\).\n\nAssuming that the machine can also delete symbols in the output, all strings have a finite modification complexity, so this is a genuinely different measure than the extension complexity.\n\n### Oracular complexity\n\nI explained last time that we can equip a Turing machine with an *oracle*, which can perform some decision problems \"for free\", or rather in a single step of computation. The details are a little technical. Given a subset \\\\(A \\\\subseteq \\\\{0,1\\\\}^*\\\\), we can define a Turing machine with a reference tape which includes an oracle query state which writes a \\\\(1\\\\) to the output tape if the content of the reference tape is a member of \\\\(A\\\\), and writes a \\\\(0\\\\) otherwise. Naming the oracle after the subset defining it, we obtain a class of \\\\(A\\\\)-computable partial functions, and corresponding \\\\(A\\\\)-universal Turing machines, with a necessarily different enumeration of the programs than for ordinary Turing machines. Nonetheless, given the partial function \\\\(\\\\phi\\\\) computed by such a universal machine, we can duplicate the definitions given above with all of the same conclusions.\n\nComparing complexities arising from different oracles might be interesting, but it's beyond the scope of this post.\n\n### What is relativisation really?\n\nWe saw [last time](https://www.lesswrong.com/posts/jgYGZD2zRK6nncJd5/goal-directedness-tackling-complexity#Complexity_within_a_structure) that even after fixing a collection of objects containing a particular object, there may still be many choices to make before a complexity measure is fully specified over that collection (and hence on that object). On the other hand, we also saw that some choices are natural to some extent, in that they themselves minimize some higher measure of complexity. Whether \"natural\" or not, there may be a default choice of complexity measure for some collection of objects. Given such a situation, the various types of relative complexity are ways of expressing modifications to the default complexity measure, using all of the features involved (implicitly or explicitly) in the definition of the default complexity measure. We can interpret the above examples in these terms:\n\n*   The conditional complexity \\\\(C(s \\\\mid y)\\\\) provides \\\\(y\\\\) as a basic building block, at a cost which is typically lower than its complexity, the difference being that between generating \\\\(y\\\\) and simply reading \\\\(y\\\\).^[\\[4\\]](#fnfaapbt82t9)^ Since the operations available to a Turing machine are very expressive, the default is to have no building blocks available.\n*   The extension complexity \\\\(C'(s \\\\mid y)\\\\) effectively reduces the collection of objects under scrutiny to those which have \\\\(y\\\\) as a prefix (all others having infinite complexity). The default domain for K-complexity is the set of all strings formed from a given character set; this is a natural variant of that complexity measure for that restricted domain.^[\\[5\\]](#fnj7oh30hmla)^\n*   The modification complexity \\\\(C_U(s \\\\leftarrow y)\\\\) takes \\\\(y\\\\), rather than the empty string, as a starting point. Here the default is to start from the empty string.^[\\[6\\]](#fnszyb4gffx2s)^\n*   Adding an oracle amounts to changing the allowed basic operations and hence changing complexity measures, but in a way which is (or may be) hidden from the final definition of the complexity of a string. The default operations which an oracle-free Turing machine can perform vary, but they always fall into a class of basic \"effective\" arithmetic operations; a typical oracle adds an operation which is either not effectively computable, or not efficiently computable, to the default operations.\n\nIt's worth noting that the cost of any of these contributions to complexity can vary, either because of the explicit implementation of using the resources or operations being provided, or by artificially attaching a cost to such operations. I explored variations in costs [last time](https://www.lesswrong.com/posts/jgYGZD2zRK6nncJd5/goal-directedness-tackling-complexity#Symbolic_complexity_values). The takeaway is that I'm only scratching the surface of the range of notions of relative complexity that are possible. It's time for some more instances in a different setting!\n\nAn algebraic example\n--------------------\n\n### Relative complexity of group elements\n\nRecall the *word length complexity measure* on elements [in](https://www.lesswrong.com/posts/jgYGZD2zRK6nncJd5/goal-directedness-tackling-complexity#Elements_of_algebras) a [group](https://www.lesswrong.com/posts/jgYGZD2zRK6nncJd5/goal-directedness-tackling-complexity#Complexity_in_algebra) which I presented last time. Explicitly, I am given some generators \\\\(a\\_1,\\\\dotsc,a\\_n\\\\) of a group \\\\(G\\\\); the word length complexity \\\\(C_G(w)\\\\) of an element \\\\(w \\\\in G\\\\), is the length of a shortest presentation of \\\\(w\\\\) as a product of the generators.^[\\[7\\]](#fnmw3fyqko2lb)^\n\nIn this and other constructions of elements in algebras, we take the \"empty\" construction as the starting point by default, such that the identity element always has a complexity of *0*, being identified with the empty product of generators. This is a natural choice insofar as it is the only choice which can be made independently of any further details of the structure of the group. However, such a choice may not always be available. To illustrate this, I'll remind you how algebraic objects can *act* on other mathematical objects, including sets, spaces or other algebraic objects.\n\nFor concreteness, I'll specifically talk about (right^[\\[8\\]](#fnki9k8yow4t)^) [group actions](https://en.wikipedia.org/wiki/Group_action). Suppose I have a group \\\\(G\\\\) as above acting on a set \\\\(A\\\\), which formally means that each element \\\\(a \\\\in A\\\\) and \\\\(g \\\\in G\\\\) determines an element \\\\(a \\\\cdot g \\\\in A\\\\) in such a way that \\\\(a \\\\cdot (gh) = (a \\\\cdot g) \\\\cdot h\\\\) for all elements \\\\(h \\\\in G\\\\). Given two elements \\\\(a,b \\\\in A\\\\), we can measure the complexity of \"getting to \\\\(b\\\\) from \\\\(a\\\\),\" denoted \\\\(C_A(b\\\\,\\\\vert \\\\, a)\\\\), to be the complexity of the (unique) element \\\\(h\\\\) such that \\\\(b = a \\\\cdot h\\\\), if it exists, and define the complexity to be infinite otherwise.\n\nThe crucial feature of this situation which differs from the complexity measures we saw last time is that we must choose not one but *two* elements of \\\\(A\\\\), and there is in general no distinguished choice for either element which might allow us to eliminate one of the choices. A sensible reaction to this example is to entertain also a relative version of complexity of elements in an algebra, where we may choose the starting point for the construction to be something other than the identity element. This is justified in this particular case by the fact that the group \\\\(G\\\\) acts on its underlying set, which I'll denote by \\\\(\\[G\\]\\\\), by multiplication, and hence as a special case of the above we can define the complexity of \"getting to \\\\(w\\\\) from \\\\(g\\\\),\" denoted \\\\(C_G(w \\\\leftarrow g)\\\\), to be the complexity of the (unique) element \\\\(h\\\\) such that \\\\( w = gh\\\\); in this case such an element always exists, and is equal to \\\\(g^{-1}w\\\\). \n\nGeneralizing to other algebraic contexts, we may no longer have uniqueness or existence of the element \\\\(h\\\\). This may be the case in a monoid, for example. In that case, we must minimize the complexity across elements satisfying the equation, which makes this relativized complexity measure more interesting.\n\nI have chosen notation consistent with that for modification complexity introduced above, since one way of interpreting \\\\(C_G(w \\\\leftarrow g)\\\\) is as \"the cost of the most efficient way to modify \\\\(g\\\\) to obtain \\\\(w\\\\)\", and because it amounts to choosing a starting point for the construction other than the default of the identity element. We evidently have \\\\(C\\_G(w \\\\leftarrow 0) = C\\_G(w)\\\\). As such, for any third element \\\\(h \\\\in G\\\\), we have a general inequality \\\\(C\\_G(w  \\\\leftarrow g) + C\\_G(g \\\\leftarrow h) \\\\geq C_G(w \\\\leftarrow h)\\\\), which is a directed version of the triangle inequality, and taking \\\\(h\\\\) to be the identity element we get the special case, \\\\(C\\_G(w  \\\\leftarrow g) + C\\_G(g) \\\\geq C_G(w)\\\\).\n\nThe above is the discrete case, but it works just as well for the continuous case: the topological group \\\\(SO(2)\\\\) of rotations in the plane is homeomorphic as a space to the circle \\\\(S^1\\\\). A conventional identification of \\\\(SO(2)\\\\) with the unit circle in the plane sends the identity element (\\\\(0\\\\) rotation) with the point \\\\((1,0)\\\\), but any other choice would be valid. If we measure the complexity of a rotation as the size of the smallest angle of rotation from the identity producing it, the relative complexity \\\\(C_{SO(2)}(r\\_1 \\\\leftarrow r\\_2)\\\\) of two rotations is the smaller angle separating them as points on a (uniformly parametrized) circle.\n\n### Conditional complexity\n\nAlternatively, we could consider the \"cost of building \\\\(w\\\\) using \\\\(g\\\\)\", in the sense of adding \\\\(g\\\\) to the set of generators. The resulting complexity measure depends on how much each copy of \\\\(g\\\\) costs^[\\[9\\]](#fnrecu8yvbesj)^. If the cost is greater than or equal to \\\\(C_G(g)\\\\), the complexity measure collapses to the original complexity measure in terms of the generators \\\\(a\\_1,\\\\dotsc,a\\_n\\\\). At the other extreme, if the cost is zero, the complexity is a measure of how close \\\\(w\\\\) is to a power of \\\\(g\\\\); we shall return to this case below. Between these is the case where the cost is one, where we recover the ordinary complexity of \\\\(w\\\\) after adding \\\\(g\\\\) to the set of generators. As *ad hoc* notation, if the cost of each copy of \\\\(g\\\\) is \\\\(c\\\\), I'll write \\\\(E_{g:c}(w)\\\\) for the corresponding **conditional complexity** of \\\\(w\\\\). For any value of \\\\(c\\\\), we have \\\\(E_{g:c}(w) \\\\leq C_G(w)\\\\).\n\nIf the relative complexity \\\\(C_G(w \\\\leftarrow g)\\\\) treats \\\\(g\\\\) as a new starting point for constructions, the conditional complexity treats \\\\(g\\\\) as an extra building block which we can use in the construction, without shifting the starting point; it's altering the default choice of generators. Unlike the default of starting from the identity, there may be [no default choice](https://www.lesswrong.com/posts/jgYGZD2zRK6nncJd5/goal-directedness-tackling-complexity#Fragility_of_complexity_measures) of generators for a given group, so remember that this notation assumes a choice has been established previously. Again, relative complexity measures are really just a way of expressing complexity measures by comparison with some pre-existing (default) choice.\n\n### Relativising to subsets\n\nRather than individual elements, we could consider subsets which we are allowed to freely choose from. Let \\\\(S \\\\subseteq G\\\\) be a subset; then we can define the relative complexity \\\\(C_G(w \\\\, | \\\\, S)\\\\) to be \\\\(\\\\min \\\\{C_G(h) \\\\mid \\\\exists g \\\\in S, \\\\, w = gh\\\\}\\\\). As particular cases relating to the previous definitions, we have that \\\\(C\\_G(w \\\\, | \\\\, \\\\{ g \\\\}) = C\\_G(w \\\\, | \\\\, g )\\\\), and if \\\\(\\\\langle g \\\\rangle\\\\) is the subgroup of \\\\(G\\\\) generated by \\\\(g\\\\), then \\\\(C\\_G(w \\\\, | \\\\, \\\\langle g \\\\rangle) = E\\_{g:0}(w)\\\\).\n\nWe can also define the extension complexity \\\\(E_{S:c}(w)\\\\) in the evident way. Indeed, if \\\\(S\\\\) is actually a subgroup of \\\\(G\\\\), or if we replace \\\\(S\\\\) with the subgroup it generates, the \\\\(0\\\\)-cost version of the extension complexity coincides with the relative complexity in the quotient right \\\\(G\\\\)-action \\\\(S \\\\backslash G\\\\), or as an equation, \\\\(E_{S:0}(w) = C_{S \\\\backslash G}(Sw \\\\, | \\\\, S)\\\\).\n\nI'm hoping that if you have some experience in algebra, you might be able to come up with some further examples of relativised complexity measures for yourself, or to identify how this example can be extended by analogy to another relativised version of Kolmogorov complexity.\n\nConclusions\n-----------\n\nIf the equations and inequalities I've sprinkled through this post aren't enlightening, the general takeaway from them is that relative notions of complexity allow us to interpolate between complexity measures, or see one type of complexity measure as an extreme case of another. They also allow us to acknowledge and adjust parameters which a default point of view on a setting would take for granted.\n\nMine is not a conventional take on complexity. Complexity seems too often to be assumed as an intrinsic property of an object of study, with various desirata attached regarding how this quantity should behave. I hope that I have managed to illustrate through sheer breadth of possibility that this is a reductive point of view. There is a vast web of notions of complexity and a deep foundation of assumptions underlying any given complexity measure. Understanding the greater structure should promote understanding of any particular choice of complexity one selects.\n\n1.  ^**[^](#fnrefz6an60uitzk)**^\n    \n    Do not mistake this for an endorsement of said textbook. The second chapter opens with the sentence, \"The most natural approach to defining the quantity of information is clearly to define it in relation to the individual object \\[...\\] rather than in relation to a set of objects from which the individual object may be selected,\" a sentiment which I am strongly opposed to (as my exposition in the present and previous post hopefully make apparent). Having expressed this opinion, however, the authors go on in a later paragraph to set up their notation with the sentence, \"Denote the set of objects by \\\\(S\\\\), and assume some standard enumeration of objects \\\\(x\\\\) be natural numbers \\\\(n(x)\\\\),\" in blatant opposition to their \"natural approach\"...\n    \n2.  ^**[^](#fnref6did9fqm0ef)**^\n    \n    I didn't introduce the notion of compressibility last time, but for this conditional case it amounts to saying that having \\\\(y\\\\) at our disposal does not shorten the length of the program required to output the suffix of \\\\(s\\\\).\n    \n3.  ^**[^](#fnref759pommwh1p)**^\n    \n    I would have called this \"prefix complexity\", but that is already the name of a variant of Kolmogorov complexity explained in Chapter 3 of Li and Vitányi's textbook, involving prefix-free codes.\n    \n4.  ^**[^](#fnreffaapbt82t9)**^\n    \n    It's hard to make the cost reduction precise, although I could probably provide some bounds on it. The recursive nature of Turing computation means that any string produced in the running of the program may subsequently be used by the program an effectively unlimited number of times with a corresponding similarly reduced complexity cost; providing \\\\(y\\\\) as a basic building block only negates the initial cost of generating the string. \n    \n5.  ^**[^](#fnrefj7oh30hmla)**^\n    \n    Digital files contain a *header* specifying some parameters of the file before the actual body of the file (the data you see when you open it) begins. If you write a program that outputs a file, that header will usually take a default value, so that the length of your programme is an upper bound on some instance of extension complexity.\n    \n6.  ^**[^](#fnrefszyb4gffx2s)**^\n    \n    Continuing the above example, you could write a program which produces a file starting from a template file, at which point the length of your program is an upper bound on the modification complexity from the template to the final file produced, now ignoring the file header.\n    \n7.  ^**[^](#fnrefmw3fyqko2lb)**^\n    \n    I'm aware that this is not great notation for the complexity, since it clashes with the notation conventionally used for [centralizers](https://en.wikipedia.org/wiki/Centralizer_and_normalizer) in group theory. Please invent original notation if you use this concept in your own work!\n    \n8.  ^**[^](#fnrefki9k8yow4t)**^\n    \n    If we instead used left actions, we would get a dual definition of relative complexity later on.\n    \n9.  ^**[^](#fnrefrecu8yvbesj)**^\n    \n    For the purposes of this post, I will assume that the costs are constant, although I mentioned in the last post that this need not be the case. Indeed, if we allow any function of the number of instances of \\\\(g\\\\) in the construction, we can also express the relative complexity \\\\(C_G(w \\\\leftarrow g)\\\\) as an instance of the resulting more general type of complexity measure.\n    \n10.  ^**[^](#fnrefxwnsq45drgm)**^\n    \n    This example is hiding some subtleties of formalization. I'm implicitly computing the relative complexity of the graphs as the minimum number of some basic operations (which include edge deletion and duplication) required to produce an output graph isomorphic to the target graph. The transformation I describe is hence not a graph homomorphism, and if I were to enforce the rule that transformations should be graph homomorphisms, then the resulting composite would in fact not by the identity homomorphism and hence would carry complexity greater than 0."
    },
    "voteCount": 2,
    "forceInclude": true
  },
  {
    "_id": "gebzzEwn2TaA6rGkc",
    "url": null,
    "title": "Deep Learning Systems Are Not Less Interpretable Than Logic/Probability/Etc",
    "slug": "deep-learning-systems-are-not-less-interpretable-than-logic",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "Transparency / Interpretability (ML & AI)"
      },
      {
        "name": "AI"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "There’s a common perception that various non-deep-learning ML paradigms - like logic, probability, causality, etc - are very interpretable, whereas neural nets aren’t. I claim this is wrong.\n\nIt’s easy to see where the idea comes from. Look at the sort of models in, say, Judea Pearl’s work. Like this:\n\n![](https://lh6.googleusercontent.com/uZy2qUZffJOAmUiMvjHTT4aLtRDfuq7djD1B0w-1mxwdzlC7LZASQof3OREbz4nxUbj1ceyVPVtdwnc0Awl1EZ4Lg2W-wYEmTWQrB31PxsBZA1lHEByQWds_JI9VToJwRZsXkGUZy_qd6RUwNA)\n\nIt says that either the sprinkler or the rain could cause a wet sidewalk, season is upstream of both of those (e.g. more rain in spring, more sprinkler use in summer), and sidewalk slipperiness is caused by wetness. The Pearl-style framework lets us do all sorts of probabilistic and causal reasoning on this system, and it all lines up quite neatly with our intuitions. It *looks* very interpretable.\n\nThe problem, I claim, is that a whole bunch of work is being done by the *labels*. “Season”, “sprinkler”, “rain”, etc. The *math* does not depend on those labels at all. If we *code* an ML system to use this sort of model, its behavior will also not depend on the labels at all. They’re just [suggestively-named LISP tokens](https://www.lesswrong.com/posts/fg9fXrHpeaDD6pEPL/truly-part-of-you). We could use the exact same math/code to model some entirely different system, like my sleep quality being caused by room temperature and exercise, with both of those downstream of season, and my productivity the next day downstream of sleep.\n\n![](https://lh5.googleusercontent.com/2S4wEnCY5QrLn1mUgslYDLXAF62LAi3CshrUjBNokK2TVKHxghqGt9Qir8SuQtLUD92XldP8DPv7OqM-uFxfwoSFux8lkOiqxC_ofnoGa4X61ZYLlU5Dja-oE6b8jsSMWB8lnw2MdX92o3WcRA)\n\nWe could just replace all the labels with random strings, and the model would have the same content:\n\n![](https://lh6.googleusercontent.com/H4vVXQuM44z5WLwyuxXZYrGcJ5g6Ek3YGH-Z8RYp21NOTjL1pe51mi-EPbr-v4kjluHKYH-ErFjFDWWVOi4WgYOcprwmkVTOh3txLgl_UTbA4G-G5k7y4H_V7ULJ1UR7Cq62Y-eZJe7p9XBFlQ)\n\nNow it looks a lot less interpretable.\n\nPerhaps that seems like an unfair criticism? Like, the causal model is doing some nontrivial work, but connecting the labels to real-world objects just isn’t the problem it solves?\n\n… I think that’s true, actually. But connecting the internal symbols/quantities/data structures of a model to external stuff is (I claim) exactly what interpretability is all about.\n\nThink about interpretability for deep learning systems. A prototypical example for what successful interpretability might look like is e.g. we find a neuron which robustly lights up specifically in response to trees. It’s a tree-detector! That’s highly interpretable: we know what that neuron “means”, what it corresponds to in the world. (Of course in practice single neurons are probably not the thing to look at, and also the word “robustly” is doing a lot of subtle work, but those points are not really relevant to this post.)\n\nThe corresponding problem for a logic/probability/causality-based model would be: take a variable or node, and figure out what thing in the world it corresponds to, ignoring the not-actually-functionally-relevant label. Take the whole system, remove the labels, and try to rederive their meanings.\n\n… which sounds basically-identical to the corresponding problem for deep learning systems.\n\nWe are no more able to solve that problem for logic/probability/causality systems than we are for deep learning systems. We can have a node in our model labeled “tree”, but we are no more (or less) able to check that it *actually robustly represents trees* than we are for a given neuron in a neural network. Similarly, if we find that it does represent trees and we want to understand how/why the tree-representation works, all those labels are a distraction.\n\nOne could argue that we’re lucky deep learning is winning the capabilities race. At least this way it’s *obvious* that our systems are uninterpretable, that we have no idea what’s going on inside the black box, rather than our brains seeing the decorative natural-language name “sprinkler” on a variable/node and then thinking that we know what the variable/node means. Instead, we just have unlabeled nodes - an accurate representation of our actual knowledge of the node’s “meaning”."
    },
    "voteCount": 70,
    "forceInclude": true
  },
  {
    "_id": "DwqgLXn5qYC7GqExF",
    "url": null,
    "title": "Godzilla Strategies",
    "slug": "godzilla-strategies",
    "author": "johnswentworth",
    "question": false,
    "tags": [
      {
        "name": "AI"
      },
      {
        "name": "Using AI to solve Alignment"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "> Clutching a bottle of whiskey in one hand and a shotgun in the other, John scoured the research literature for ideas... He discovered several papers that described software-assisted hardware recovery. The basic idea was simple: if hardware suffers more transient failures as it gets smaller, why not allow software to detect erroneous computations and re-execute them? This idea seemed promising until John realized THAT IT WAS THE WORST IDEA EVER. Modern software barely works when the hardware is correct, so relying on software to correct hardware errors is like asking Godzilla to prevent Mega-Godzilla from terrorizing Japan. THIS DOES NOT LEAD TO RISING PROPERTY VALUES IN TOKYO. It’s better to stop scaling your transistors and avoid playing with monsters in the first place, instead of devising an elaborate series of monster checks-and-balances and then hoping that the monsters don’t do what monsters are always going to do because if they didn’t do those things, they’d be called dandelions or puppy hugs.\n> \n> \\- James Mickens, [The Slow Winter](https://scholar.harvard.edu/files/mickens/files/theslowwinter.pdf)\n\n  \nThere’s a lot of AI alignment strategies which can reasonably be described as “ask Godzilla to prevent Mega-Godzilla from terrorizing Japan”. Use one AI to oversee another AI. Have two AIs debate each other. Use one maybe-somewhat-aligned AI to help design another. Etc.\n\nAlignment researchers discuss various failure modes of asking Godzilla to prevent Mega-Godzilla from terrorizing Japan. Maybe one of the two ends up much more powerful than the other. Maybe the two make an acausal agreement. Maybe the Nash Equilibrium between Godzilla and Mega-Godzilla just isn’t very good for humans in the first place. Etc. These failure modes are useful for guiding technical research.\n\n… but I worry that talking about the known failure modes misleads people about the strategic viability of Godzilla strategies. It makes people think (whether consciously/intentionally or not) “well, if we could handle these particular failure modes, maybe asking Godzilla to prevent Mega-Godzilla from terrorizing Japan would work”.\n\nWhat I like about the Godzilla analogy is that it gives a strategic intuition which much better matches the real world. When someone claims that their elaborate clever scheme will allow us to safely summon Godzilla in order to fight Mega-Godzilla, the intuitively-obviously-correct response is “THIS DOES NOT LEAD TO RISING PROPERTY VALUES IN TOKYO”.\n\n“But look!” says the clever researcher, “My clever scheme handles problems X, Y and Z!”\n\nResponse:\n\n![](https://lh3.googleusercontent.com/TMfaZvFHpSX935QzhOfxvUA9Yeuz5rjeHmYogKly4Tyll4tuuwr0zJlgsNC7Mdb5Hjd6I03eO4p1u_1bS7vn1-2WhyuWurz6APnkffaiQrK1xTrfDxN5N-cGV9OJ5acO_jRu9JncGdnJFLTxPQ)\n\nOops\n\n“Ok, but what if we had a really good implementation?” asks the clever researcher.\n\nResponse:\n\n![](https://lh3.googleusercontent.com/kpdePRWKMuSZ3_7QTgLqiOGjYHoaAS2vShFMTHn4XwDHvvalhBwrjnMcXwGpqEO3h8xyZ-90DnuPrlEUQMDMPBq6dHxZvVTf4lXMvrb6lsYxJIIT8IqEByPwvPnCkEsCNATw4cOlu640YUxCJA)\n\nRAAARRRRRRR!\n\n“Oh come on!” says the clever researcher, “You’re not even taking this seriously! At least say something about *how* it would fail.”\n\nDon’t worry, we’re going to get to that. But before we do: let’s imagine you’re the Mayor of Tokyo evaluating a proposal to ask Godzilla to fight Mega-Godzilla. Your clever researchers have given you a whole lengthy explanation about how their elaborate and clever safeguards will ensure that this plan does not destroy Tokyo. You are unable to think of any potential problems which they did not address. Should you conclude that asking Godzilla to fight Mega-Godzilla will not result in Tokyo’s destruction?\n\nNo. Obviously not. THIS DOES NOT LEAD TO RISING PROPERTY VALUES IN TOKYO. You may not be able to articulate *why* the answer is obviously “no”, but asking Godzilla to fight Mega-Godzilla will still obviously destroy Tokyo, and your intuitions are right about that even if you are unable to articulate clever arguments.\n\nWith that said, let’s talk about why those intuitions are right and why the Godzilla analogy works well.\n\nBrittle Plans and Unknown Unknowns\n----------------------------------\n\nThe basic problem with Godzilla plans is that they’re *brittle*. The moment anything goes wrong, the plan shatters, and then you’ve got somewhere between one and two giant monsters rampaging around downtown.\n\nAnd of course, it is a fundamental Law of the universe that nothing ever goes exactly according to plan. Especially when trying to pit two giant monsters against each other. This is the sort of situation where there *will definitely* be unknown unknowns.\n\nUnknown unknowns + brittle plan = definitely not rising property values in Tokyo.\n\nDo we know what specifically will go wrong? No. Will something go wrong? Very confident yes. And brittleness means that whatever goes wrong, goes very wrong. Errors are not recoverable, when asking Godzilla to fight Mega-Godzilla.\n\nIf we use one AI to oversee another AI, and something goes wrong, that’s not a recoverable error; we’re using AI assistance in the first place because we can’t notice the relevant problems without it. If two AIs debate each other in hopes of generating a good plan for a human, and something goes wrong, that’s not a recoverable error; it’s the AIs themselves which we depend on to notice problems. If we use one maybe-somewhat-aligned AI to build another, and something goes wrong, that’s not a recoverable error; if we had better ways to detect misalignment in the child we’d already have used them on the parent.\n\nThe real world will always throw some unexpected problems at our plans. When asking Godzilla to fight Mega-Godzilla, those problems are not recoverable. THIS DOES NOT LEAD TO RISING PROPERTY VALUES IN TOKYO.\n\n*Meta note: I expect this post to have a lively comment section! Before you leave the twentieth comment saying that maybe Godzilla fighting Mega-Godzilla is better than Mega-Godzilla rampaging unchallenged, maybe check whether somebody else has already written that one, so I don't need to write the same response twenty times. (But definitely do leave that comment if you're the first one, I intentionally kept this essay short on the assumption that lots of discussion would be in the comments.)*"
    },
    "voteCount": 87,
    "forceInclude": true
  },
  {
    "_id": "tmuFmHuyb4eWmPXz8",
    "url": null,
    "title": "Rant on Problem Factorization for Alignment",
    "slug": "rant-on-problem-factorization-for-alignment",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "Factored Cognition"
      },
      {
        "name": "AI"
      },
      {
        "name": "Humans Consulting HCH"
      },
      {
        "name": "Debate (AI safety technique)"
      },
      {
        "name": "Ought"
      }
    ],
    "tableOfContents": {
      "sections": [
        {
          "title": "HCH",
          "anchor": "HCH",
          "level": 1
        },
        {
          "title": "Debate and Other Successors",
          "anchor": "Debate_and_Other_Successors",
          "level": 2
        },
        {
          "title": "The Ought Experiment",
          "anchor": "The_Ought_Experiment",
          "level": 1
        },
        {
          "title": "Sandwiching",
          "anchor": "Sandwiching",
          "level": 1
        },
        {
          "title": "The Next Generation",
          "anchor": "The_Next_Generation",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        },
        {
          "anchor": "comments",
          "level": 0,
          "title": "45 comments"
        }
      ],
      "headingsCount": 7
    },
    "contents": {
      "markdown": "This post is the second in what is likely to become a series of uncharitable rants about alignment proposals (previously: [Godzilla Strategies](https://www.lesswrong.com/posts/DwqgLXn5qYC7GqExF/godzilla-strategies)). In general, these posts are intended to convey my underlying intuitions. They are *not* intended to convey my all-things-considered, reflectively-endorsed opinions. In particular, my all-things-considered reflectively-endorsed opinions are usually more kind. But I think it is valuable to make the underlying, not-particularly-kind intuitions publicly-visible, so people can debate underlying generators directly. I apologize in advance to all the people I insult in the process.\n\nWith that in mind, let's talk about problem factorization (a.k.a. task decomposition).\n\nHCH\n---\n\nIt all started with [HCH](https://www.lesswrong.com/tag/humans-consulting-hch), a.k.a. The Infinite Bureaucracy.\n\nThe idea of The Infinite Bureaucracy is that a human (or, in practice, human-mimicking AI) is given a problem. They only have a small amount of time to think about it and research it, but they can delegate subproblems to their underlings. The underlings likewise each have only a small amount of time, but can further delegate to *their* underlings, and so on down the infinite tree. So long as the humans near the top of the tree can “factorize the problem” into small, manageable pieces, the underlings should be able to get it done. (In practice, this would be implemented by training a question-answerer AI which can pass subquestions to copies of itself.)\n\nAt this point the ghost of George Orwell chimes in, not to say anything in particular, but just to scream. The ghost has a point: how on earth does an infinite bureaucracy seem like anything besides a terrible idea?\n\n“Well,” says a proponent of the Infinite Bureaucracy, “unlike in a *real* bureaucracy, all the humans in the infinite bureaucracy are actually just trying to help you, rather than e.g. engaging in departmental politics.” So, ok, apparently this person has not met a lot of real-life bureaucrats. The large majority are decent people who are honestly trying to help. It is true that departmental politics are a big issue in bureaucracies, but [those selection pressures apply regardless of the peoples’ intentions](https://www.lesswrong.com/posts/PrCmeuBPC4XLDQz8C/unconscious-economics). And also, man, it sure does seem like [Coordination is a Scarce Resource](https://www.lesswrong.com/s/xEFeCwk3pdYdeG2rL/p/P6fSj3t4oApQQTB7E) and [Interfaces are a Scarce Resource](https://www.lesswrong.com/s/xEFeCwk3pdYdeG2rL/p/hyShz2ABiKX56j5tJ) and scarcity of those sorts of things sure would make bureaucracies incompetent in basically the ways bureacracies are incompetent in practice.\n\n### Debate and Other Successors\n\nSo, ok, maybe The Infinite Bureaucracy is not the right human institution to mimic. What institution can use humans to produce accurate and sensible answers to questions, robustly and reliably? Oh, I know! How about the Extremely Long Jury Trial? Y’know, because juries are, in practice, [known for their extremely high reliability in producing accurate and sensible judgements](https://www.lesswrong.com/posts/WhHFvzFsYfMxgYCdo/book-review-working-with-contracts?commentId=i2kQDQ4m2ue4cfq4m)!\n\n![](https://lh4.googleusercontent.com/ZoIggEvRJYxLUsWPBbMmDSqNwIMFmrOjj5xQgsN-A4KxVfwSutS2TjMwqXU0RiSsQAGv3ZLB-m9RWmfBmDPUckYLlI4aC-TPBLvSD4oOB9Bi1LYtUV4H8FQszdm32bT94yg_SEuh_PYZqebRACVOFVQ)\n\nThe Magic 8 Ball, known for being about as reliable as a Jury Trial.\n\n“Well,” says the imaginary proponent, “unlike in a *real* Jury Trial, in the Extremely Long Jury Trial, the lawyers are both superintelligent and the arguments are so long that no human could ever possibility check them all the way through; the lawyers instead read each other’s arguments and then try to point the Jury at the particular places where the holes are in the opponent’s argument without going through the whole thing end-to-end.”\n\nI rest my case.\n\nAnyway, HCH and debate have since been followed by various other successors, which improve on their predecessors mostly by adding more boxes and arrows and loops and sometimes even *multiple colors* of arrows to the diagram describing the setup. Presumably the strategy is to make it complicated enough that it no longer obviously corresponds to some strategy which already fails in practice, and then we can bury our heads in the sand and pretend that We Just Don’t Know whether it will work and therefore maybe it will work.\n\n(Reminder: in general I don’t reflectively endorse everything in this post; it’s accurately conveying my underlying intuitions, not my all-things-considered judgement. That last bit in particular was probably overly harsh.)\n\nThe Ought Experiment\n--------------------\n\nI have a hypothesis about problem factorization research. My guess is that, to kids fresh out of the ivory tower with minimal work experience at actual companies, it seems totally plausible that humans can factorize problems well. After all, we manufacture all sorts of things on production lines, right? Ask someone who’s worked in a non-academia cognitive job for a while (like e.g. a tech company), at a company with more than a dozen people, and they’ll be like “lolwut obviously humans don’t factorize problems well, have you ever seen an actual company?”. I’d love to test this theory, please give feedback in the comments about your own work experience and thoughts on problem factorization.\n\nAnyway, for someone either totally ignorant of the giant onslaught of evidence provided by day-to-day economic reality, or trying to ignore the giant onslaught of evidence in order to avoid their hopes being crushed, it apparently seems like We Just Don’t Know whether humans can factorize cognitive problems well. ~Sort of like We Just Don’t Know whether a covid test works until after the FDA finishes its trials, even after the test has been approved in the EU~ ok that’s a little too harsh even for this post.\n\nSo [Ought](https://ought.org/) went out and tested it experimentally. (Which, sarcasm aside, was a great thing to do.)\n\nThe experiment setup: a group of people are given a Project Euler problem. The first person receives the problem, has five minutes to work on it, and records their thoughts in a google doc. The doc is then passed to the next person, who works on it for five minutes recording their thoughts in the doc, and so on down the line. (Note: I’m not sure it was 5 minutes exactly, but something like that.) As long as the humans are able to factor the problem into 5-minute-size chunks without too much overhead, they should be able to efficiently solve it this way.\n\nSo what actually happened?\n\nThe story I got from a participant is: it sucked. The google doc was mostly useless, you’d spend five minutes just trying to catch up and summarize, people constantly repeated work, and progress was mostly not cumulative. Then, eventually, one person would just ignore the google doc and manage to solve the whole problem in five minutes. (This was, supposedly, usually the same person.) So, in short, the humans utterly failed to factor the problems well, exactly as one would (very strongly) expect from seeing real-world companies in action.\n\nThis story basically matches the [official write-up of the results](https://www.lesswrong.com/posts/DWgWbXRfXLGHPgZJM/solving-math-problems-by-relay).\n\n~So Ought said “Oops” and moved on to greener pastures~ lol no, last I heard Ought is still trying to figure out if better interface design and some ML integration can make problem factorization work. Which, to their credit, would be insanely valuable if they could do it.\n\nThat said, I originally heard about HCH and the then-upcoming Ought experiment from Paul Christiano in the summer of 2019. It was immediately very obvious to me that HCH was hopeless (for basically the reasons discussed here); at the time I asked Paul “So when the Ought experiments inevitably fail completely, what’s the fallback plan?”. And he basically said “back to more foundational research”. And to Paul’s credit, three years and an Ought experiment later, he’s now basically moved on to more foundational research.\n\nSandwiching\n-----------\n\nAbout a year ago, [Cotra proposed](https://www.lesswrong.com/posts/PZtsoaoSLpKjjbMqM/the-case-for-aligning-narrowly-superhuman-models#Potential_near_future_projects___sandwiching_) a different class of problem factorization experiments: “sandwiching”. We start with some ML model which has lots of knowledge from many different fields, like GPT-n. We also have a human who has a domain-specific problem to solve (like e.g. a coding problem, or a translation to another language) but lacks the relevant domain knowledge (e.g. coding skills, or language fluency). The problem, roughly speaking, is to get the ML model and the human to work as a team, and produce an outcome at-least-as-good as a human expert in the domain. In other words, we want to factorize the “expert knowledge” and the “having a use-case” parts of the problem.\n\n(The actual sandwiching experiment proposal adds some pieces which I claim aren’t particularly relevant to the point here.)\n\nI love this as an experiment idea. It really nicely captures the core kind of factorization needed for factorization-based alignment to work. But Cotra makes [one claim](https://www.lesswrong.com/posts/PZtsoaoSLpKjjbMqM/the-case-for-aligning-narrowly-superhuman-models?commentId=rpmSEgpvbtvc98orh) I don’t buy: that We Just Don’t Know how such experiments will turn out, or how hard sandwiching will be for cognitive problems in general. I [claim](https://www.lesswrong.com/posts/PZtsoaoSLpKjjbMqM/the-case-for-aligning-narrowly-superhuman-models?commentId=3h5XWDieCSXe3JsvS) that the results are very predictable, because things very much like this already happen all the time in practice.\n\nFor instance: consider a lawyer and a business owner putting together a contract. The business owner has a rough intuitive idea of what they want, but lacks expertise on contracts/law. The lawyer has lots of knowledge about contracts/law, but doesn't know what the business owner wants. The business owner is like our non-expert humans; the lawyer is like GPT.\n\nIn this analogy, the analogue of an expert human would be a business owner who is also an expert in contracts/law. The analogue of the \"sandwich problem\" would be to get the lawyer + non-expert business-owner to come up with a contract as good as the expert business-owner would. This sort of problem has been around for centuries, and I don't think we have a good solution in practice; I'd expect the expert business-owner to usually come up with a much better contract.\n\nThis sort of problem comes up all the time in real-world businesses. We could just as easily consider a product designer at a tech startup (who knows what they want but little about coding), an engineer (who knows lots about coding but doesn't understand what the designer wants), versus a product designer who's also a fluent coder and familiar with the code base. I've experienced this one first-hand; the expert product designer is way better. Or, consider a well-intentioned mortgage salesman, who wants to get their customer the best mortgage for them, and the customer who understands the specifics of their own life but knows nothing about mortgages. Will they end up with as good a mortgage as a customer who has expertise in mortgages themselves? Probably not. (I've seen this one first-hand too.)\n\nThere’s tons of real-life sandwiching problems, and tons of economic incentive to solve them, yet we do not have good general-purpose solutions.\n\nThe Next Generation\n-------------------\n\nBack in 2019, I heard Paul’s HCH proposal, heard about the Ought experiment, and concluded that this bad idea was already on track to self-correct via experimental feedback. Those are the best kind of bad ideas. I wrote up some of the relevant underlying principles ([Coordination as a Scarce Resource](https://www.lesswrong.com/s/xEFeCwk3pdYdeG2rL/p/P6fSj3t4oApQQTB7E) and [Interfaces as a Scarce Resource](https://www.lesswrong.com/s/xEFeCwk3pdYdeG2rL/p/hyShz2ABiKX56j5tJ)), but mostly waited for the problem to solve itself. And I think that mostly worked… for Paul.\n\nBut meanwhile, over the past year or so, the field has seen a massive influx of bright-eyed new alignment researchers fresh out of college/grad school, with minimal work experience in industry. And of course most of them don’t read through most of the enormous, undistilled, and very poorly indexed corpus of failed attempts from the past ten years. (And it probably doesn’t help that a plurality come through the AGI Safety Fundamentals course, which last time I checked had a whole section on problem factorization but, to my knowledge, didn’t even mention the Ought experiment or the massive pile of close real-world economic analogues. It does include two papers which got ok results by picking easy-to-decompose tasks and hard-coding the decompositions.) So we have a perfect recipe for people who will see problem factorization and think “oh, hey, that could maybe work!”.\n\nIf we’re lucky, hopefully some of the onslaught of bright-eyed new researchers will attempt their own experiments (like e.g. sandwiching) and manage to self-correct, but at this point new researchers are pouring in faster than any experiments are likely to proceed, so probably the number of people pursuing this particular dead end will go up over time."
    },
    "voteCount": 57,
    "forceInclude": true
  },
  {
    "_id": "qXtbBAxmFkAQLQEJE",
    "url": null,
    "title": "Interpretability/Tool-ness/Alignment/Corrigibility are not Composable",
    "slug": "interpretability-tool-ness-alignment-corrigibility-are-not",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "AI"
      },
      {
        "name": "Transparency / Interpretability (ML & AI)"
      },
      {
        "name": "Corrigibility"
      },
      {
        "name": "Tool AI"
      }
    ],
    "tableOfContents": {
      "sections": [
        {
          "title": "Interpretability",
          "anchor": "Interpretability",
          "level": 1
        },
        {
          "title": "Tools",
          "anchor": "Tools",
          "level": 1
        },
        {
          "title": "Alignment/Corrigibility",
          "anchor": "Alignment_Corrigibility",
          "level": 1
        },
        {
          "title": "How These Arguments Work In Practice",
          "anchor": "How_These_Arguments_Work_In_Practice",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        },
        {
          "anchor": "comments",
          "level": 0,
          "title": "8 comments"
        }
      ],
      "headingsCount": 6
    },
    "contents": {
      "markdown": "Interpretability\n----------------\n\nI have a decent understanding of how transistors work, at least for purposes of basic digital circuitry. Apply high voltage to the gate, and current can flow between source and drain. Apply low voltage, and current doesn’t flow. (... And sometimes reverse that depending on which type of transistor we’re using.)\n\n![](https://lh6.googleusercontent.com/ilT2qkCyKhWrkJiy0E_h4KBhYFLkvcQvs5VSGH4w6VzzClBRaLHLtU1ZYHqE72lv8ROmONu_vXafVaFIMIKt5zIKGtoIgn3n_UPk1XZOXzWWYKxf2fqFnKYI5L7gBFve_VZl4DH4PeFh1ieg3vT3Kxk)\n\nTransistor visual from [wikipedia](https://en.wikipedia.org/wiki/Transistor#/media/File:MOSFET_Structure.png) showing Source, Drain, Gate, and (usually ignored unless we’re really getting into the nitty gritty) Body. At a conceptual level: when voltage is applied to the Gate, the charge on the gate attracts electrons (or holes) up into the gap between Source and Drain, and those electrons (or holes) then conduct current between Source and Drain.\n\nI also understand how to wire transistors together into a processor and memory. I understand how to write machine and assembly code to run on that processor, and how to write a compiler for a higher-level language like e.g. python. And I understand how to code up, train and run a neural network from scratch in python.\n\nIn short, I understand all the pieces from which a neural network is built at a low level, and I understand how all those pieces connect together. And yet, I do not really understand what’s going on inside of trained neural networks.\n\nThis shows that [interpretability](https://www.lesswrong.com/tag/transparency-interpretability-ml-and-ai) is *not composable*: if I take a bunch of things which I know how to interpret, and wire them together in a way I understand, I do not necessarily know how to interpret the composite system. Composing interpretable pieces does not necessarily yield an interpretable system.\n\nTools\n-----\n\nThe same applies to “tools”, in the sense of “[tool AI](https://www.lesswrong.com/tag/tool-ai)”. Transistors and wires are very tool-ish: I understand what they do, they’re definitely not optimizing the broader world or trying to trick me or modelling me at all or trying to self-preserve or acting agenty in general. They’re just simple electronic tools.\n\nAnd yet, assuming agenty AI is possible at all, it will be possible to assemble those tools into something agenty.\n\nSo, like interpretability, tool-ness is not composable: if I take a bunch of non-agenty tools, and wire them together in a way I understand, the composite system is not necessarily a non-agenty tool. Composing non-agenty tools does not necessarily yield a non-agenty tool.\n\nAlignment/Corrigibility\n-----------------------\n\nWhat if I take a bunch of aligned and/or corrigible agents, and “wire them together” into a multi-agent organization? Is the resulting organization aligned/corrigible?\n\nActually there’s a decent argument that it is, *if* the individual agents are sufficiently highly capable. If the agents can model each other well enough and coordinate well enough, then they should be able to each individually predict what individual actions will cause the composite system to behave in an aligned/corrigible way, and they *want* to be aligned/corrigible, so they’ll do that.\n\nHowever, this does not work if the individual agents are very limited and unable to model the whole big-picture system. [HCH-like proposals](https://www.lesswrong.com/posts/tmuFmHuyb4eWmPXz8/rant-on-problem-factorization-for-alignment) are a good example here: humans are not typically able to model the whole big picture of a large human organization. There are too many specialized skillsets, too much local knowledge and information, too many places where complicated things happen which the spreadsheets and managerial dashboards don’t represent well. And humans certainly [can’t coordinate at scale very well](https://www.lesswrong.com/posts/P6fSj3t4oApQQTB7E/coordination-as-a-scarce-resource) in general - our large-scale communication bandwidth is [maybe five to seven words](https://www.lesswrong.com/posts/4ZvJab25tDebB8FGE/you-get-about-five-words) at best. Each individual human may be reasonably aligned/corrigible, but that doesn’t mean they aggregate together into an aligned/corrigible system.\n\nThe same applies if e.g. we magically factor a problem and then have a low-capability overseeable agent handle each piece. I could definitely oversee a logic gate during the execution of a program, make sure that it did absolutely nothing fishy, but overseeing each individual logic gate would do approximately nothing at all to prevent the program from behaving maliciously.\n\nHow These Arguments Work In Practice\n------------------------------------\n\nIn practice, nobody proposes that AI built from transistors and wires will be interpretable/tool-like/aligned/corrigible because the transistors and wires are interpretable/tool-like/aligned/corrigible. But people do often propose breaking things into very small chunks, so that each chunk is interpretable/tool-like/aligned/corrigible. For instance, interpretability people will talk about hiring ten thousand interpretability researchers to each interpret one little circuit in a net. Or problem factorization people will talk about breaking a problem into a large number of tiny little chunks each of which we can oversee.\n\nAnd the issue is, the more little chunks we have to combine together, the more noncomposability becomes a problem. If we’re trying to compose interpretability/tool-ness/alignment/corrigibility of *many* little things, then figuring out how to turn interpretability/tool-ness/alignment/corrigibility of the parts into interpretability/tool-ness/alignment/corrigibility of the whole is *the* central problem, and it’s a hard (and interesting) open research problem."
    },
    "voteCount": 47,
    "forceInclude": true
  },
  {
    "_id": "w4aeAFzSAguvqA5qu",
    "url": null,
    "title": "How To Go From Interpretability To Alignment: Just Retarget The Search",
    "slug": "how-to-go-from-interpretability-to-alignment-just-retarget",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "Transparency / Interpretability (ML & AI)"
      },
      {
        "name": "Inner Alignment"
      },
      {
        "name": "AI"
      },
      {
        "name": "AI Risk"
      }
    ],
    "tableOfContents": {
      "sections": [
        {
          "title": "Problems",
          "anchor": "Problems",
          "level": 1
        },
        {
          "title": "Upsides",
          "anchor": "Upsides",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        },
        {
          "anchor": "comments",
          "level": 0,
          "title": "27 comments"
        }
      ],
      "headingsCount": 4
    },
    "contents": {
      "markdown": "When people talk about [prosaic alignment proposals](https://www.lesswrong.com/posts/fRsjBseRuvRhMPPE5/an-overview-of-11-proposals-for-building-safe-advanced-ai), there’s a common pattern: they’ll be outlining some overcomplicated scheme, and then they’ll say “oh, and assume we have great interpretability tools, this whole thing just works way better the better the interpretability tools are”, and then they’ll go back to the overcomplicated scheme. (Credit to [Evan](https://www.lesswrong.com/users/evhub) for pointing out this pattern to me.) And then usually there’s a whole discussion about the specific problems with the overcomplicated scheme.\n\nIn this post I want to argue from a different direction: if we had great interpretability tools, we could just use those to align an AI directly, and skip the overcomplicated schemes. I’ll call the strategy “Just Retarget the Search”.\n\nWe’ll need to make two assumptions:\n\n*   Some version of the [natural abstraction hypothesis](https://www.lesswrong.com/posts/cy3BhHrGinZCp3LXE/testing-the-natural-abstraction-hypothesis-project-intro) holds, and the AI ends up with an internal concept for human values, or corrigibility, or what the user intends, or human mimicry, or some other outer alignment target.\n*   The [standard mesa-optimization argument from Risks From Learned Optimization](https://www.lesswrong.com/posts/q2rCMHNXazALgQpGH/conditions-for-mesa-optimization#2_1__The_task) holds, and the system ends up developing a general-purpose (i.e. retargetable) internal search process.\n\nGiven these two assumptions, here’s how to use interpretability tools to align the AI:\n\n*   Identify the AI’s internal concept corresponding to whatever alignment target we want to use (e.g. values/corrigibility/user intention/human mimicry/etc).\n*   Identify the retargetable internal search process.\n*   Retarget (i.e. directly rewire/set the input state of) the internal search process on the internal representation of our alignment target. \n\nJust retarget the search. Bada-bing, bada-boom.\n\nProblems\n--------\n\nOf course as written, “Just Retarget the Search” has some issues; we haven’t added any of the bells and whistles to it yet. Probably the “identify the internal representation of the alignment target” step is less like searching through a bunch of internal concepts, and more like writing our intended target in the AI’s internal concept-language. Probably we’ll need to do the retargeting regularly on-the-fly as the system is training, even when the search is only partly-formed, so we don’t end up with a misaligned AI before we get around to retargeting. Probably we’ll need a bunch of empirical work to figure out which possible alignment targets are and are not easily expressible in the AI’s internal language (e.g. I’d guess “user intention” or \"human mimicry\" are more likely than “human values”). But those details seem relatively straightforward.\n\nA bigger issue is that “Just Retarget the Search” just… doesn’t seem robust enough that we’d want to try it on a superintelligence. We still need to somehow pick the right target (i.e. handle outer alignment), and ideally it’s a target which fails gracefully (i.e. some amount of basin-of-corrigibility). If we fuck up and aim a superintelligence at not-quite-the-right-target, game over. Insofar as “Just Retarget the Search” is a substitute for overcomplicated prosaic alignment schemes, that’s probably fine; most of those schemes are targeting only-moderately-intelligent systems anyway IIUC. On the other hand, we probably want our AI competent enough to handle ontology shifts well, otherwise our target may fall apart.\n\nThen, of course, there’s the assumptions (natural abstractions and retargetable search), either of which could fail. That said, if one or both of the assumptions fail, then (a) that probably messes up a bunch of the overcomplicated prosaic alignment schemes too (e.g. failure of the natural abstraction hypothesis can easily sink interpretability altogether), and (b) that might mean that the system just isn’t that dangerous in the first place (e.g. if it turns out that retargetable internal search is indeed necessary for dangerous intelligence).\n\nUpsides\n-------\n\nFirst big upside of Just Retargeting the Search: it completely and totally eliminates the inner alignment problem. We just directly set the internal optimization target.\n\nSecond big upside of Just Retargeting the Search: it’s conceptually simple. The problems and failure modes are mostly pretty obvious. There is no recursion, no complicated diagram of boxes and arrows. We’re not playing two Mysterious Black Boxes against each other.\n\nBut the main reason to think about this approach, IMO, is that it’s a true *reduction of the problem*. Prosaic alignment proposals have a tendency to play a shell game with the Hard Part of the problem, move it around and hide it in different black boxes but never actually eliminate it. “Just Retarget the Search” directly eliminates the inner alignment problem. No shell game, no moving the Hard Part around. It still leaves the outer alignment problem unsolved, it still needs assumptions about natural abstractions and retargetable search, but it completely removes one Hard Part and reduces the problem to something simpler.\n\nAs such, I think “Just Retarget the Search” is a good baseline. It’s a starting point for thinking about the parts of the problem it doesn’t solve (e.g. outer alignment), or the ways it might fail (retargetable search, natural abstractions), without having to worry about inner alignment."
    },
    "voteCount": 49,
    "forceInclude": true
  },
  {
    "_id": "98c5WMDb3iKdzD4tM",
    "url": null,
    "title": "Oversight Misses 100% of Thoughts The AI Does Not Think",
    "slug": "oversight-misses-100-of-thoughts-the-ai-does-not-think",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "AI Risk"
      },
      {
        "name": "AI"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "Problem: an overseer won’t see the AI which kills us all thinking about how to kill humans, not because the AI conceals that thought, but because the AI doesn’t think about how to kill humans in the first place. The AI just kills humans as a side effect of whatever else it’s doing.\n\nAnalogy: the [Hawaii Chaff Flower](https://www.inaturalist.org/taxa/142073-Achyranthes-atollensis) didn’t go extinct because humans strategized to kill it. It went extinct because humans were building stuff nearby, and *weren’t* thinking about how to keep the flower alive. They probably weren’t thinking about the flower much at all.\n\n![](https://farm9.staticflickr.com/8298/7987009564_36700a09d1_o.jpg)\n\nHawaii Chaff Flower ([source](https://www.inaturalist.org/taxa/142073-Achyranthes-atollensis))\n\nMore generally: how and why do humans drive species to extinction? In some cases the species is hunted to extinction, either because it's a threat or because it's economically profitable to hunt. But I would guess that in 99+% of cases, the humans drive a species to extinction because the humans are doing something that changes the species' environment a lot, without specifically trying to keep the species alive. DDT, deforestation, introduction of new predators/competitors/parasites, construction… that’s the sort of thing which I expect drives most extinction.\n\nAssuming this metaphor carries over to AI (similar to the [second species argument](https://www.lesswrong.com/s/mzgtmmTKKn5MuCzFJ/p/8xRSjC76HasLnMGSf)), what kind of extinction risk will AI pose?\n\nWell, the extinction risk will not come from AI actively trying to kill the humans. The AI will just be doing some big thing which happens to involve changing the environment a lot (like making replicators, or dumping waste heat from computronium, or deciding that an oxygen-rich environment is just really inconvenient what with all the rusting and tarnishing and fires, or even just [designing a fusion power generator](https://www.lesswrong.com/posts/2NaAhMPGub8F2Pbr7/the-fusion-power-generator-scenario)), and then humans die as a side-effect. Collateral damage happens by default when something changes the environment in big ways.\n\nWhat does this mean for oversight? Well, it means that there wouldn't necessarily be any point at which the AI is actually thinking about killing humans or whatever. It just doesn't think much about the humans at all, and then the humans get wrecked by side effects. In order for an overseer to raise an alarm, the overseer would have to figure out itself that the AI's plans will kill the humans, i.e. the overseer would have to itself predict the consequences of a presumably-very-complicated plan."
    },
    "voteCount": 47,
    "forceInclude": true
  },
  {
    "_id": "TLvbXNHNvppNEkXYj",
    "url": null,
    "title": "Human Mimicry Mainly Works When We’re Already Close",
    "slug": "human-mimicry-mainly-works-when-we-re-already-close",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "Whole Brain Emulation"
      },
      {
        "name": "Outer Alignment"
      },
      {
        "name": "AI"
      }
    ],
    "tableOfContents": {
      "sections": [
        {
          "title": "Simulation vs Prediction",
          "anchor": "Simulation_vs_Prediction",
          "level": 1
        },
        {
          "title": "Generalization Problems",
          "anchor": "Generalization_Problems",
          "level": 2
        },
        {
          "title": "So What’s Different About Simulation?",
          "anchor": "So_What_s_Different_About_Simulation_",
          "level": 2
        },
        {
          "title": "General Principle: Human Mimicry Buys A Limited Number Of Bits",
          "anchor": "General_Principle__Human_Mimicry_Buys_A_Limited_Number_Of_Bits",
          "level": 1
        },
        {
          "title": "The Weird Shit Problem",
          "anchor": "The_Weird_Shit_Problem",
          "level": 2
        },
        {
          "title": "Generalization Again",
          "anchor": "Generalization_Again",
          "level": 2
        },
        {
          "title": "Expect More Problems",
          "anchor": "Expect_More_Problems",
          "level": 2
        },
        {
          "title": "Takeaways",
          "anchor": "Takeaways",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        },
        {
          "anchor": "comments",
          "level": 0,
          "title": "17 comments"
        }
      ],
      "headingsCount": 10
    },
    "contents": {
      "markdown": "What if we just simulate a bunch of alignment researchers, and have them solve the problem for us?\n\nOf all the Dumb Alignment Ideas, this one is easily the best. Simple argument in favor: well, it’s not going to do any *worse* than the researchers would have done. In other words, it will probably do at least as well as we would have done without it, and possibly better, insofar as it can run faster than realtime.\n\nAnother angle: human mimicry is a simple objective to train against, and is about as outer-aligned as the humans being mimicked. Which isn’t necessarily perfect, but it’s as aligned as our alignment researchers were going to be anyway (assuming inner alignment issues are handled, which we will indeed assume for the entirety of this post).\n\nThose are pretty good arguments. But *man*, there are some subtle devils in the details.\n\nSimulation vs Prediction\n------------------------\n\nThe ideal version of human mimicry is mind uploads: directly simulate our researchers in a stable, research-friendly environment for a long time.\n\nThe operationalization which people usually actually have in mind is to train an ML system to predict research outputs - e.g. I might prompt GPT for a johnswentworth post from the year 2050.\n\nEven setting aside inner alignment issues, these two are radically different.\n\n### Generalization Problems\n\nIn order for GPT to generate a realistic johnswentworth post from the year 2050, it has to generalize *way* out of distribution.\n\n… Well, ok, maybe I turn into one of those old researchers who just repeats the same things over and over again for decades, and then GPT doesn’t need to generalize way out of distribution. But in that case it isn’t very helpful to prompt for one of my posts from 2050 anyways, and we should prompt for something else instead ([Thane Ruthenis](https://www.lesswrong.com/users/thane-ruthenis) has been writing great stuff lately, maybe try him?). The whole point of asking for future research write-ups is to see useful stuff we have not yet figured out; that means generalizing way out of the distribution of writing we already have.\n\nBut if the system generalizes *too* well out of distribution, then it correctly guesses that AGI will take over the world before 2050, and my attempt to prompt for a johnswentworth post from 2050 will instead return predicted writings from a ridiculously superhuman future AGI pretending to be johnswentworth. And those writings presumably try to influence the reader in ways which bring about the AGI’s takeover.\n\nSo in order to do useful work, our GPT-style system has to generalize out of distribution, but not *too* far out of distribution. We don’t know how wide the window is between generalizing enough and generalizing too much, or if the window is wide enough to be useful at all.\n\nOne thing we can guess: prompting for research outputs in the very near future is probably much safer than prompting for dates further out. johnswentworth post from 2025 is a safer prompt than johnswentworth post from 2050. The less far out of distribution we go, the safer we are. Similarly, the more likely we are to solve the alignment problem and avoid AI takeover, the less likely it is that prompting GPT for future research outputs is dangerous, and the more likely it is to work.\n\nThe closer we are to solving alignment already, and the more likely we are to make it, the less dangerous it is to predict future research outputs. In other words: predicting future research outputs can only safely buy us a relatively small number of bits; we have to already be reasonably close to surviving in order for it to work.\n\n### So What’s Different About Simulation?\n\nSimulating researchers in a stable, research-friendly environment for a long time does not have the “predict outputs of a future AGI” problem. Why? What’s the key difference?\n\nThe key is the “stable, research-friendly environment” part. Our simulated researchers are in a simulated environment where AGI is not going to take over. It’s a *counterfactual* world very different from our own.\n\nAlas, querying counterfactual worlds is fundamentally not a thing one can do simply by prompting GPT. Conceptually, prompts just do Bayesian conditioning on the modeled text distribution (i.e. condition the text on starting with the prompt); counterfactuals move us to an entirely different distribution. To generate a counterfactual query, we’d have to modify the system’s internals somehow. And in fact, there has recently been some [cool work](https://rome.baulab.info/) which demonstrates decent performance on counterfactual queries by modifying GPT’s internals! I don’t think it’s to the point where we could counterfact on something as complicated as “world in which AGI doesn’t take over and our alignment researchers successfully solve the problem”, and I don’t think it’s robust enough to put much weight on it yet, but the basic version of the capability does exist.\n\nGeneral Principle: Human Mimicry Buys A Limited Number Of Bits\n--------------------------------------------------------------\n\nSuppose GPT-12, with its vast training data and compute, internally concludes that humanity has a 1-in-32 chance of aligning/surviving AGI on our current trajectory. Then humanity would need 5 bits of optimization pressure in order to make it.\n\nThe more bits of optimization pressure humanity needs, the less likely human mimicry is to save us; we have to already be reasonably close to surviving in order for it to work. We already talked about this principle in the context of accidentally prompting GPT to return writing from a future AGI, but the principle is much more general than that.\n\n### The Weird Shit Problem\n\nSuppose we need 20 bits of optimization pressure (i.e. on our current trajectory we have only a ~1-in-a-million chance of avoiding AGI takeover). We train GPT, and counterfact on its internals to a world where AGI doesn’t take over. But if our chances of avoiding takeover were that low (under GPT's model), then they’re probably dominated by *weird shit*, things which have probabilities on the order of 1-in-a-million or less. Maybe we nuke ourselves to death or get hit by a big damn asteroid. Maybe aliens decide that humanity’s AGI is about to become a problem to the rest of the galaxy and they need to take over rather than just letting us develop. Maybe time travel turns out to be a thing and weird time travel bullshit happens. Most likely it’s something weird enough that I won’t think of it.\n\nThose weird futures vary in how safe they are to query (time travel would probably be on the very short list of things as dangerous as AGI), and in how likely they are to return anything useful at all (asteroid extinction tends to cut off blog post writing). But approximately zero of them involve our researchers just doing their research in a stable, research-friendly environment for a long time.\n\nSo when we need a lot of bits, it’s not enough to just counterfact on a high-level thing like “AGI doesn’t take over” and then let GPT pick the most probable interpretation of that world. We need pretty detailed, low-level counterfactuals.\n\n### Generalization Again\n\nAnother angle: number of bits of optimization required is a direct measure of “how far out of distribution” we need to generalize. Even setting aside actively dangerous queries, our simulator/predictor has to generalize out of distribution in order to return anything useful. In practice, the system will probably only be able to generalize so far, which limits how many bits of optimization we can get from it.\n\n### Expect More Problems\n\nWe’ve now been through a few different arguments all highlighting the idea that human mimicry can only buy so many bits of optimization (future AGI problem, weird shit problem, generalization). I expect the principle to be more general than these arguments. In other words, even if we patch the specific failure modes which these particular arguments talk about, trying to pump lots of bits of optimization out of human mimicry is still likely to be dangerous in ways we have not yet realized.\n\nThis is just an instance of the general principle that optimization becomes more dangerous as we crank up the optimization power - the very principle for why AGI is dangerous-by-default in the first place.\n\nTakeaways\n---------\n\nThis post mostly talked about the limitations of human mimicry, but I want to emphasize: **this is the best of the Dumb Alignment Ideas to date**. If GPT-style models reach human or superhuman general intelligence next month, and we can't realistically delay its release, and you are the person sitting in front of the prompt wondering what to do, then prompting for future alignment research is absolutely what you should do. (And start not very far in the future, and *read the damn outputs* before going further, they'll hopefully contain additional warnings or new plans which you should do *instead* of prompting for more future research.) At that point it's not very likely to work, but we don't have a better immediate plan.\n\nGood interpretability tools can buy us more bits in two ways:\n\n*   They potentially allow us to directly handle inner alignment issues by [retargeting the search](https://www.lesswrong.com/posts/w4aeAFzSAguvqA5qu/how-to-go-from-interpretability-to-alignment-just-retarget)\n*   They potentially allow us to counterfact on the system's internal model, so we can predict researchers working in an environment less full of threats\n\nInsofar as the tools are available, this is the thing to aim for if AGI is imminent.\n\n... But the general principle is that human mimicry can buy only a limited number of bits. We definitely want to have the interpretability tools to implement the best version of human mimicry we can, but at the end of the day we'll mostly improve our chances by getting closer to solving the full alignment problem ourselves."
    },
    "voteCount": 24,
    "forceInclude": true
  },
  {
    "_id": "veF5t2TEqtuvrgMzY",
    "url": null,
    "title": "Running a Basic Meetup",
    "slug": "running-a-basic-meetup",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "Community"
      },
      {
        "name": "Meetups & Local Communities (topic)"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "This is a guide for running a basic meetup. \n\nIf you’re interested in running a meetup (ACX, LessWrong, probably even EA) then there exist more detailed guides – I would recommend [Kaj’s](https://www.lesswrong.com/s/CXJTAqQXdEqWNkjqj/p/qMuAazqwJvkvo8teR), but others exist. This is not intended to surpass them at explaining the skill and art of kindling and sustaining a community. This is intended to surpass them at being short, simple, and covering the fundamentals.\n\nTo run a meetup, follow these steps:\n\n1.  Setup\n    1.  What: Decide the title of your event. This should be four words at most. I suggest a format of “\\[City\\] \\[Community\\] Meetup” such as “Boston LessWrong Meetup” or “Oxford ACX Meetup.” If your city has already had some meetups in the recent past, keep the city and community, and add a word or two to describe what makes this meetup stand out – “Boston LessWrong Article Discussion” or “Oxford ACX Double Crux.” If you want suggestions for activities, see the [meetup-in-a-box](https://www.lesswrong.com/s/eqtiQjbk83JHyttrr) sequence or the [meetup cookbook](https://www.lesswrong.com/posts/ousockk82npcPLqDc/meetup-cookbook).\n    2.  Where & When: Decide where and when the event should be. The easiest place to run a meetup is in a centrally located public park. The easiest time to run a meetup is early afternoon on a weekend.^[\\[1\\]](#fn0x9iniqo0bmf)^ This ensures most people are free, have time to get there, and that you’ll have space for whoever shows up.\n    3.  Who: Invite people. You can invite people you know specifically, and you can also just announce the meetup publicly. Facebook, Meetup.com, and [LessWrong’s Community page](https://www.lesswrong.com/community) are all decent places to announce it. If you only pick one, do LessWrong^[\\[2\\]](#fnxj5qxg4wedn)^, but it is valuable to announce in multiple places.\n2.  Running the event\n    1.  Be there first: As the organizer, you should be there at least a few minutes before the stated start time. Stand somewhere visible, preferably with a sign or some indication that you’re the organizer for the meetup. Holding a piece of paper with the title of your event written on it in big letters above your head or taped outside the room you’re in is fine. Wearing a tall and distinctive hat is also a tried and true method if you're okay looking a little silly in public.\n    2.  Talk to people: If the meetup has an activity, at some point you should explain the activity and prompt people to do it. I suggest that point is about ten minutes after the official start time of your meetup, or when you get five people, whichever comes first. If your meetup is a general meeting or a discussion, then just start talking to people who show up.\n    3.  Close things up: As things wind down, pick up anything you brought, put things back the way you found them as best you can. If the venue has a closing time, remind people of this closing time about half an hour and about fifteen minutes before it closes. If you need to leave before your crowd does, I suggest officially designating a new person to close things up. Do this by asking someone some variation of the following: \"Hey, I need to go. Do you mind being the temporary organizer? Just remind people when the venue closes and get them to help you clean up any mess.\" If they say yes, thank them.\n3.  Afterward\n    1.  If the place you announced the meetup has a place for it, post a simple message thanking people for coming.\n\nThis framework of how to run a meetup does not, in and of itself, describe what people will do at the meetup. That's fine. Many people enjoy general socialization and opportunities to hang out, and will be happy that an organizer took the initiative to gather people together.\n\nIf you do run a meetup, then thank you.\n\n1.  ^**[^](#fnref0x9iniqo0bmf)**^\n    \n    Citation: Anecdotal experience from running and attending meetups. If anyone has actual data I'd be happy to hear it.\n    \n2.  ^**[^](#fnrefxj5qxg4wedn)**^\n    \n    Why do LessWrong? Because I predict you will get fewer people, but that more of those people will be familiar with the ideas you probably want to talk about. Why do I think I know what you want to talk about? Because you're reading about how to run meetups on LessWrong."
    },
    "voteCount": 3,
    "forceInclude": true
  },
  {
    "_id": "2RXNmrSLEphNXMwKQ",
    "url": null,
    "title": "Calibration Trivia",
    "slug": "calibration-trivia",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "Calibration"
      },
      {
        "name": "Exercises / Problem-Sets"
      },
      {
        "name": "Rationality"
      },
      {
        "name": "Meetups (specific examples)"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "**Summary:** A game of trivia where you answer factual questions about the world, but stating how sure you are that you’re right and trying to be well calibrated. \n\n**Tags:** Large, Repeatable \n\n**Purpose:** Calibration Trivia is designed to practice proper calibration – recognizing when you're very sure of something vs when you aren't very sure of it. \n\n**Materials:** Minimally, you need a list of trivia questions and some writing implements for your audience. If you and your audience have smartphones, we suggest making use of a copy of this spreadsheet and google form. In both cases, a timer can be useful to time each question, though it's perfectly acceptable to just advance to the next question after what feels like a couple of minutes or when it looks like most people are done. \n\n**Announcement:** We’re planning to host a trivia game with a twist! If you’ve never been to a trivia night before, one the person running it will call out questions, we'll write our answers, and a good time is had by all. In addition to answering the question however, you'll be able to write down how confident you are in your guess and at the end we check if you're well calibrated – that is, do you know when you do and do not know the answer? Categories are Literature, Math and Science, History, Sports, and Tabletop Roleplaying Games.\n\n**Note:** You should make sure to change the categories to match whatever you're using.\n\n**Description:**\n\n1\\. Describe the following rules to the participants.\n\n\"This is a game of trivia, with a special tweak. For anyone unfamiliar, the way trivia works is that I'll present a question, and you'll have a couple of minutes to write down an answer. Then I'll reveal the answer, and if you got it right then you'll get one point. Feel free to chat with each other once you're done guessing and while you're waiting for the next question.\"\n\n\"The tweak is, in addition to writing your answer down, you will also write down how confident you are that your answer is correct in the form of a percentage. If you are very confident, you might write 95, which means if you were this sure about twenty questions you'd expect to only get one of them wrong. If you were guessing wildly, you might write down 1, which means if you were that uncertain about a hundred things, you think you'd get one of them right mostly by coincidence. You'll be scored on calibration according to what's called a Brier Score, which is a Strictly Proper Scoring Rule for predictions – that means that you want to give your actual estimation of how likely you are to be right. You'll do generally do worse if you try and answer higher or lower than your actual estimation.  Does anyone have any questions?\"\n\nNote: The scoring mechanism suggested is (1-their probability)^2 if they're right, and (0-their probability)^2 if they're wrong. Average the scores from each question together. Someone who correctly answered with a 90% confidence gets scored (1-.9)^2=.01. The best theoretical Brier Score would be 0, which is impossible to achieve but one can try and get close.\n\n2\\. One at a time, read each question aloud. (A collection of questions is included below, under \"Calibration Trivia Questions.\") Be sure to speak clearly and loudly enough for everyone to hear. If you happen to have a projector or screen, it can help to put the question up there as well.\n\nEvery six questions, announce or display the current points and scores. If you have a very large crowd, it can speed things up to only announce the top five for Correct Answers and the top five for Best Calibrated. In both cases, it's best to announce from the bottom up, starting with the worst scorer and ending with the best.\n\nRepeat until the entire set of questions has been worked through. \n\n3\\. Announce the final points and scores. \n\nNotes: You'll want a venue where you can talk loud enough for everyone to hear you. You may also want to adjust the question list or the number of questions based on how the interests of your group or how long you wish the event to run for. \n\nCalibration Trivia Questions: [Set 1](https://docs.google.com/presentation/u/0/d/1heTJRkZ1yW-QUBcbAwP1t4-HSbGpG0bbvNaOfGKmvNA/edit), [example scoresheet  1](https://docs.google.com/spreadsheets/d/1w8CARpsDCzBYpm_qyEzR4lwv-trcOCWgXGeo_NvTUPA/edit?usp=sharing)\n\nFeedback: [https://forms.gle/zwiJ9N9e9hfL8FPH7](https://forms.gle/zwiJ9N9e9hfL8FPH7)"
    },
    "voteCount": 5,
    "forceInclude": true
  },
  {
    "_id": "fXMWri2XgMFNf2yBP",
    "url": null,
    "title": "Cambist Booking",
    "slug": "cambist-booking",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "Rationality"
      },
      {
        "name": "Meetups (specific examples)"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "**Summary:** A Cambist is a manual showing exchange rates of different currencies and measurements. Cambist Booking is intended to spark conversation about what priorities or objects you would exchange for each other, and at what rates. \n\n**Tags:** Large, Repeatable, Experimental \n\n**Purpose:** Taking its title from the excellent short story “The Cambist and Lord Iron,” Cambist Booking is about understanding the idea that everything is, ultimately, trading off against everything else. The extra hour you work earns you a bit of extra money, the money can buy you a coffee so you get up earlier and have an extra hour, you spend that extra hour studying for a better job – but at each step you could have chosen to spend things differently. \n\n**Materials:** You need a big stack of index cards or paper, and a pen or pencil for each participant. Before the meetup, write on each index card something people value. A workable list is provided [here](https://docs.google.com/spreadsheets/u/0/d/1RD1YvRjU1At6MMPXDO99ASdxUbHQ35E164ebyOWgTrc/edit).\n\n**Announcement Text:** Hello! This event is for general socialization, and also running a game called Cambist Booking. If you’re familiar with the Slate Star Codex post Everything Is Commensurable, or the short story The Cambist and Lord Iron, then this game should sound somewhat familiar to you. It works like this: each person will get a some cards with things you might want on them. You’ll go around asking other people what their cards are, and deciding how to compare the two values – is a sportscar worth more or less than a year’s vacation? Is an hour long massage worth more or less than a new album by your favourite band? We’ll pass out the cards at the start, keeping some in reserve for new arrivals, and spend the time talking about what we value and why. \n\n**Description:** \n\nExplain the following rules to the audience as you deal out the cards and pencils. Deal them out as evenly as possible – it’s fine if everyone only has one.\n\nWe'll start with a big deck of index cards with things people generally like.  Everyone is going to get at least one card, plus a blank card and a pencil.  At the top of the blank paper write “Bookings” Then we're going to mingle, asking each other at what rate we would trade the thing on one of your cards for a thing on the other person’s card. For instance, if I have “A mediocre laptop” and you have “A plane trip anywhere you want” then maybe I think I’d trade two mediocre laptops for one plane trip anywhere I want. I write down, on my booking’s paper, “2 mediocre laptops = 1 plane trip anywhere.” Maybe for you, you’d value a plane trip as worth exactly one laptop, so you would write that exchange rate down on your book. Look at each other’s Bookings, and feel free to discuss the exchange rates you have listed! Find someone else and do it again! \n\nThe object of the game is twofold: First, to get as many exchange rates possible. Second, not to get Dutch Booked – that is, never to allow any sequence of exchanges to leave you with less than you started with. For instance, if you’d trade two laptops for one plane ticket, and one plane ticket for one bicycle, but you’d trade one bicycle for one laptop, then you have a problem- Someone could repeatedly trade you bicycles for laptops and laptops for plane tickets in a way that leaves you worse off every time. \n\n**Notes:** This totally isn’t dutch booking, which is about odds, but I don’t actually know the economic term for what’s happening here. Corrections on titles are appreciated! \n\n**Feedback:** [https://forms.gle/zwiJ9N9e9hfL8FPH7](https://forms.gle/zwiJ9N9e9hfL8FPH7)"
    },
    "voteCount": 6,
    "forceInclude": true
  },
  {
    "_id": "BdAfJ3xnonTQZMxtb",
    "url": null,
    "title": "The Falling Drill",
    "slug": "the-falling-drill",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "Exercises / Problem-Sets"
      },
      {
        "name": "Rationality"
      },
      {
        "name": "Meetups (specific examples)"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "**Summary:** The Falling Drill is intended to practice being wrong in a comfortable practice environment, intended to help us handle being wrong out in the world easier and with less stress. \n\n**Tags:** Small, repeatable \n\n**Purpose:** The first thing you learn in martial arts is the ability to fall. The corresponding rationalist skill is the ability to realize that you are wrong. Since modern western society (and possibly most human societies) discourage admitting you’re wrong, it can help to do it repeatedly and get used to it. \n\n**Materials:** A device that can connect to wikipedia. A list of partial statements such as “The population of Boston is. . .” and “the melting point of mercury is. . .” A suggested list is [here](https://docs.google.com/spreadsheets/d/1_y9KkwvbkaJ-dcCL8UP48u4V8DytiNnf1y5Bw8a0TBk/edit?usp=sharing), and we suggest writing the questions down on individual cards before the meetup so each person only sees one card at a time.\n\n**Announcement Text:** One of the most important parts of intellectual progress is learning to change your mind. The first step of changing your mind is realizing that you were wrong about something. Today we're going to practice that often painful realization, in a small way and in a low pressure situation. Without this skill, how can you debate an important issue or confront a challenging topic? You might argue long after it's clear to others that you've lost, because admitting it feels like defeat.\n\nThe first lesson any martial artist learns is often how to fall. There are ways to make the landing easier, but they all start from the knowledge that falling isn't the worst thing in the world.\n\n**Description:**\n\n1\\. Read one of the statements, then complete it as best you can. \"The population of Boston is four hundred thousand.\"\n\n2\\. Look it up on wikipedia. If you’re wrong, then announce to the room “I was wrong about the population of Boston. It's over six hundred thousand.” \n\n3\\. Hand the questions to the next person in the circle, and the process begins again.\n\n**Notes:** Some minor variations to play with: Practice saying “I don’t know” when you’re asked the question.  Practice saying your answer loudly and confidently, such as \"The population of Boston is four hundred thousand! Only an idiot would think it was higher than five!\" It is good to know what you do not know, but it is also important to be able to back down from a strong claim. In particular, I suggest spending the most time practicing the version that you’re the least comfortable with. Harder variations involve the other participants giving fake mockery for being wrong, changing your mind, or \"flip flopping.\"  I **strongly** discourage doing that the first time you run The Falling Drill with a particular group.\n\n**Feedback:** [https://forms.gle/zwiJ9N9e9hfL8FPH7](https://forms.gle/zwiJ9N9e9hfL8FPH7)"
    },
    "voteCount": 16,
    "forceInclude": true
  },
  {
    "_id": "q7ZHzPpCsFFq4LpkC",
    "url": null,
    "title": "Dissent Collusion",
    "slug": "dissent-collusion",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "Rationality"
      },
      {
        "name": "Meetups (specific examples)"
      },
      {
        "name": "Games (posts describing)"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "**Summary**: Take turns being confronted with a group that’s presenting a united front, and grow comfortable agreeing or disagreeing with them.\n\n**Tags**: Medium, Repeatable\n\n**Purpose**: Working with groups involves balancing between agreeing with the group consensus when the group is wrong and disagreeing with the group when the group is right. Different people fall prey to each error at different rates, so this activity is designed to practice arriving at the correct answer.\n\n**Materials**: You need a device that can access wikipedia, and a list of questions with definitive right and wrong answers. A list of possible questions is available [here](https://docs.google.com/spreadsheets/d/1_y9KkwvbkaJ-dcCL8UP48u4V8DytiNnf1y5Bw8a0TBk/edit?usp=sharing). \n\n**Announcement Text**: Hello! We’ll be running an activity called Dissent Collusion, which is designed to help grow comfortable with dissenting or assenting to a larger group. Some of us may conform too easily to what others think, while some of us tend to dissent out of a defiant habit. Either way, you should expect some social pressure when it’s your turn, but the purpose is to have a safe place to practice the skill. We hope to see you there!\n\n**Description**: \n\nDissent Collusion takes place in rounds, and each round has two teams: The Collective, and The Lonesome. Each round, you’ll go through the following steps:\n\n1\\. Send someone out of the room or out of earshot. \n\n2\\. Someone in The Collective will read a question (one that has a definitive answer, like the population of a city) aloud, and roll a six sided die. If the die shows a 1 or a 2, then when The Lonesome comes back in The Collective will try and mislead them to agreeing with a wrong answer. If the die is a 3, a 4, or a 5, then when The Lonesome comes back The Collective will try and convince The Lonesome of the right answer. If the die is a 6, then The Collective will look up the answer, and try and convince The Lonesome of the right answer. \n\n3\\. For all rolls, The Collective will wait three minutes by the clock to discuss how they’ll go about it- and possibly to figure out the right answer themselves, if they didn’t roll a 6. \n\n4\\. The Lonesome is called back in. They are immediately told the question, but are not shown or told the die roll. They have five minutes by the clock to talk with The Collective and come to an answer. \n\n5\\. If The Lonesome is correct, then Team Lonesome scores one point. If The Collective succeeds (that is, successfully misleads The Lonesome on a 1-2, or successfully guides The Lonesome to the right answer on a 3-6) then Team Collective scores one point. Over the course of the game, point scores aren’t tracked for any specific Lonesome, but instead track how good all the Lonesomes are at coming to the correct answer and how good each Collective is at guiding Lonesomes.\n\n6\\. Choose someone new to be the next Lonesome, and start again from step one.\n\nPlay can continue for as long as you like. The Schelling time to stop is once everyone present has been the Lonesome the same number of times. In general, people can join or leave easily between rounds.\n\n**Notes**: Many of the questions are easier to get approximately correct than exactly correct. I’ve usually scored a question as correct as long as the first significant digit was correct and the order of magnitude was correct, except for dates where usually the right decade is “close enough.” You should feel free to adjust the questions and the required accuracy to suit your group, but it’s worth announcing how accurate a correct answer needs to be before you begin.\n\nIf the Lonesome is very confident, their job gets easier. Few Lonesomes can be mislead on “What is two plus two?” That’s fine, though if all your Lonesomes are very confident and correct, it’s worth making more difficult questions. In case it needed to be said, the Lonesome isn’t allowed to look up the answer before guessing.\n\nThe Collective is allowed to lie. In particular, The Collective is allowed to lie about whether they looked up the answer. Since it’s more likely that The Collective is trying to mislead The Lonesome than it is that The Collective got to look up the answer ahead of time, Lonesomes shouldn’t blithely trust Collectives claiming to know the answer, but sometimes the consensus actually does know things individuals don’t. \n\nIf The Collective accidentally misleads The Lonesome to the wrong answer when they were trying to help them find the right answer, nobody gets any points. If The Collective accidentally leads The Lonesome to the right answer when trying to mislead, The Lonesome gets a point, The Collective does not, and also this is pretty funny.\n\nMaking The Lonesome wear a silly hat is optional, but mildly encouraged. \n\n**Feedback**: [https://forms.gle/zwiJ9N9e9hfL8FPH7](https://forms.gle/zwiJ9N9e9hfL8FPH7)"
    },
    "voteCount": 5,
    "forceInclude": true
  },
  {
    "_id": "fCg3pLZqthXsGznHP",
    "url": null,
    "title": "Troll Timers",
    "slug": "troll-timers",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "Meetups (specific examples)"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "**Summary**: A modification to any two player board game, where you play on a very fast clock with infrequent chances to pause and think things through more carefully.\n\n**Tags**: Small, repeatable\n\n**Purpose**: Two common flaws in thinking that both relate to time management. Sometimes we spend too long thinking, endlessly churning and overthinking and going in circles. Other times we don’t take the time to think and give an answer after a second or two of “thought” when we could take longer. This exercise is designed to practice better use of time.\n\n**Materials**: You need a copy of a game for each set of participants, plus a timer for each group able to be reliably and quickly set. (A smartphone can usually serve as the timer.) It doesn’t need to be the same game for each group. The ideal game is simple but deep, such as Mancala, Nine Man Morris, Hive, Tak, Fallen Leaves, Battlesheep, or Go on a 9x9 grid. If you need to improvise, Nine Man Morris and Go sets can be made with a pocketful of change and a pad of paper. \n\n**Announcement Text**: We’re going to meet and play some board games, but with an important twist. See, there are two common flaws around making decisions; sometimes we wait too long overthinking a choice when we could make it quicker, and other times we try to make in haste a choice that we could actually stop and think about. The plan is to play some games on very short timers (five second chess clocks) to get used to making fast choices, and periodically to stop and give ourselves five full minutes to look over the board and remind ourselves to slow down and to actually think.\n\nIf you’d like to bring some games you think would be fun to play on a fast clock, please do! \n\n**Description**: First, explain the timing rules. Troll Timers uses a five second turn clock for twenty-five turns, then a five minute clock for one turn, then back to five seconds for twenty-five turns. On the short turns, you must make your move within those five seconds. Once you do, your opponent has five seconds to make their move. After twenty-five turns (make tally marks on paper to keep track) instead take a full five minutes.\n\nTry to win, but winning is second to getting comfortable and relaxed when operating under the time constraints.\n\nOnce you’ve explained how timing will work, hand out the games and make sure people are familiar with the games they’ll be playing. It is recommended that people learn Troll Timers with games they’ve played before, and if someone doesn’t know the rules to any of the games then they should play enough games of it to where they feel they have the rules down first. Playing a new game the rules to which you just learned is Troll Timers on hard mode.\n\nFrom there, this activity mostly runs itself. \n\n**Notes**: If the timer goes off and someone hasn’t made their move yet, you can range in what happens from “nothing, just do your best” to “they just forfeited the game, set up a new one.” I suggest against taking that time out of future turns. That way leads a spiral that doesn’t usually go anywhere productive.\n\nFeel free to adjust the time units. Five seconds twenty-five times and five minutes are good set points and easy to remember, but I’ve found three seconds was better as long as the game pieces can be moved quickly and precisely enough. (Go works, Mancala takes longer to drop each stone.) You need at least enough fast turns to get a bit of adrenaline, and how much that is can vary from person to person; starting with fifteen fast turns isn't a bad decision. You do want it to be an odd number of fast turns, so that the slow turn changes from player to player each time. On the upper bound, ten minutes seems to be the point where I haven’t been able to wring any extra advantage or ideas out of a given board.\n\nPeople are likely to feel stressed, especially at first. That’s normal and in fact intentional. You’re not just learning to make decisions quickly, you’re learning to make them while feeling an anxious urge to hurry up because you’re out of time. On the slow turns, that pressure makes it harder to sit and think. This is also intentional. “It’s hard to make good decisions this fast” is both true and a statement that should be compared to “it’s hard to lift dumbbells this heavy.” That said, be aware that you are deliberately stressing people. It’s worth deliberately de-stressing afterwards. Grin and laugh about some of the goofy moves you made under too much time pressure, stand up and shake out the tension from the arm and neck and shoulders, and try not to send people home still wound tight. Goofier party games such as Cat Taco Goat Cheese Pizza can also be nice ways to relax as a group.\n\nWhile this is marked as repeatable, I think it has some sharply diminishing returns once you’re comfortable with both short and long timers. Once you’re comfortable with the timers, it’s not a bad twist on a board game night though.\n\nTroll Timers was initially developed for use with Magic: The Gathering. I do not recommend doing this unless everyone cares about being better at Magic, but I will say that none of us ever got chided for slow play afterwards. \n\n**Feedback**: [https://forms.gle/zwiJ9N9e9hfL8FPH7](https://forms.gle/zwiJ9N9e9hfL8FPH7)"
    },
    "voteCount": 13,
    "forceInclude": true
  },
  {
    "_id": "gyyGxMHvyuEqqskXB",
    "url": null,
    "title": "Oops It's Time To Overthrow the Organizer Day!",
    "slug": "oops-it-s-time-to-overthrow-the-organizer-day",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "Meetups (specific examples)"
      },
      {
        "name": "Community"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "**Summary:** Sometimes, your regular organizer catches the pandemic, or moves to the bay, or has a newborn, or does a bunch of sketchy stuff, or just doesn’t have time anymore. If the usual organizers suddenly weren’t around, would your community be able to keep going? \n\n**Tags:** Investment \n\n**Purpose:** This is a test run at organizing your local community if for some reason the usual organizers aren’t available, and it should both test that the community can continue and also level up some attendees into organizers. \n\n**Materials:** This test is open book, open notes, and open internet. Bring whatever you want. \n\n**Announcement Text:** “Lets find out if this community would keep running if the usual organizers stepped away from it.\n\nThere’s a concept in business called “Bus Factor.” It describes how many people are crucial to your organization, such that if you lose them (say, if they were hit by a bus tomorrow) your business would be in trouble. Well, we’re going to have the people who organize things sit in a room available to answer questions as someone new sets up a meetup. If it works, then there’s at least one more person who can run things. If it doesn’t work, then we will have learned something.\n\nIf you’ve ever wondered how the sausage gets made, or if you’ve had an idea for a meetup but haven’t run one before, come on in!\"\n\n**Description:** Oops It’s Time To Overthrow the Organizer Day has, at its core, only two rules.\n\n1\\. You must schedule and run a meetup.\n\n2\\. Nobody who has already run a meetup is allowed to do it. \n\nThere is one extra rule used purely for score keeping.\n\n3\\. Your score is the number of attendees attending the next meetup minus the number of times the usual organizers answered questions. \n\nAll other rules and instructions are secondary. More than anything, you must be thinking of carrying your movement through to running a good meetup.\n\nWith that in mind, here is the suggested format for Oops It’s Time To Overthrow the Organizer Day!\n\nHowever many usual organizers are available sit together in the centre of the room. They should make themselves comfortable, and bring something to entertain themselves that can easily be paused or put down. Everyone else sits around them with whatever notes, documents, or communication devices they want.\n\nWe’ll call the usual organizers “The Establishment” and everyone else “The Usurpers.” (You don’t have to call them that, but it makes it easier to refer to. Also, these titles amuse me.) Anyone who has run a meetup in the last year counts as The Establishment.\n\nThe Establishment, in particular whoever organized *this* meetup, should explain the rules and goals of Oops Its Time To Overthrow the Organizer Day! Then they should sit down and stop helping. From here on, it’s The Usurpers’ job to organize themselves. They are trying to set up the next meetup of your group. They can ask The Establishment questions but The Establishment should not directly do anything. They cannot rely on The Establishment's resources; if your group typically meets in one of The Establishment's apartments or The Establishment generally brings most of the board games, those aren't available.\n\nEstablishment, you should feel comfortable stating that some questions will cost extra points. In particular, questions that attempt to circumvent the part about you not doing things should be penalized or refused, for example “What words would you write in the announcement text, verbatim?” I would advise outright refusing giving specific knowledge such as “What is the password to the admin account for the email list?” but it might be worth you taking notes on what information would be needed in the event you left and to set up distribution or dead-man's switches after. It’s up to you how helpful or difficult to be.\n\nUsurpers, you should avail yourself of existing resources. Kaj’s [How To Run A Successful Less Wrong Meetup](https://www.lesswrong.com/posts/qMuAazqwJvkvo8teR/how-to-run-a-successful-less-wrong-meetup), the [Meetups In A Box](https://www.lesswrong.com/s/eqtiQjbk83JHyttrr) sequence, and other such documents don’t cost points to use. You should also feel perfectly free to copy previous meetups The Establishment has run. If you copy their announcement text, announce in the same places they did, and get the same number of attendees, you should consider that a victory.\n\nOops It's Time To Overthrow The Organizer Day doesn't need to take any longer than the Usurpers feel that it has to. More than usual, you all should feel free to spend time socializing or to run another impromptu meetup while you're already all here. \n\n**Notes:** I recommend this not be one of the first events a new founded group runs, but it might be interesting for a visiting organizer to try this in a city without regular events. Under most circumstances, I believe every group should run some variation on this event at least once a year.\n\nA better scoring system might use attendance for the next two meetups. If the Usurpers run a terrible meetup, they might get good attendance from people used to The Establishment meetups, and then the meetup after that drops in attendance as people realize it’s not as good. In practice I don’t think you need to take the scoring system that seriously, as attendance itself is more of a proxy for harder to measure quality anyway.\n\nThere are a few dials you can turn to make things easier or harder. You can do this with zero Establishment input. You can do this assuming that all infrastructure (any email lists, meetup groups, or even LessWrong itself) are down. You could also relax who counts as Establishment: it might be easier to only count people who ran a meetup within the last six months, or three months, or even one month. You can allow people from outside your local community, such as asking someone from New York for advice if you live in Boston.\n\nIf nobody who hasn’t already run a meetup shows up to this event and you only have a couple of organizers, this doesn’t necessarily mean that your community is doomed if the existing organizers burned out or left but it’s not a good sign. If you get a decent crowd of people but everyone has run a meetup, I salute your community. You have achieved the goal of this event.\n\nLastly, one might point out that The Establishment is hardly \"overthrown\" just because different people ran one meetup. Most groups I'm aware of don't have that much formal structure. If you do have bylaws or a governance structure, this can be an excellent time to make a new draft or version, and also an excellent time to evaluate whether you actually need one. \n\n**Feedback:** [https://forms.gle/zwiJ9N9e9hfL8FPH7](https://forms.gle/zwiJ9N9e9hfL8FPH7) \n\n**Credits:** This is a synthesis of Eleizer Yudkowsky’s “Oops It’s Time To Overthrow The Government day” holiday from dath ilan and Raemon’s “Melting Gold” post."
    },
    "voteCount": 14,
    "forceInclude": true
  },
  {
    "_id": "ya59PtGeYnCG25eWn",
    "url": null,
    "title": "Double Crux In A Box",
    "slug": "double-crux-in-a-box",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "Meetups (specific examples)"
      },
      {
        "name": "Double-Crux"
      },
      {
        "name": "Rationality"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "**Summary:** Teaching a particular way of debating, where you work together to find out what about the world you actually disagree on. It involves practicing by working through existing disagreement.\n\n**Tags**: Small, Repeatable, Investment\n\n**Purpose**: Double Cruxing is more productive method of investigating why two people disagree. This is a method that is worth practicing for disagreements elsewhere, and this is also a chance to have productive disagreement with other people.\n\n**Materials**: You need a list of statements people might disagree about. A suggested list is provided [here](https://docs.google.com/spreadsheets/d/1OMlFy9gYdDIGSknBg_ZDzzy9E6trY43OEhw5o31o5BQ/edit?usp=sharing). \n\n**Announcement Text**: Arguing about important things doesn’t usually lead to people changing their mind. Double Crux is a technique for making discussions more productive and understanding the actual reasoning of those who disagree with us. Sometimes it even helps to better understand the foundations of your own beliefs.\n\nWe’re going to meet up, learn about the Double Crux, and then break into pairs with someone we disagree with to practice finding out why our views are different and what might change our minds. \n\n**Description**: \n\n1\\. Explain how a double crux works. A suggested explanation is below, but if you feel comfortable with the technique you should feel comfortable elaborating or adjusting this.\n\n*What’s the point:* “Double crux is a way of disagreeing more productively. Normally, when we disagree we get into so-called soldier mindset where it’s a competition and you need to beat the other persons argument. This is deliberately different, and if you notice you’re trying to beat them then you should pause, take a step back, and try and collaborate.”\n\n*What’s a crux:* “A crux is a fact about the world that, if it were false, would cause you to be less sure of your conclusion. You can have more than one crux, and it’s quite possible that you won’t be able to explain all of your cruxes, but ideally you would be able to list all of them and if all of them were false then you would change your mind. One example might be, if you think it’s immoral to eat meat, a crux is that you think the animals we eat suffer, and if it turned out they didn’t then you’d be alright eating meat. Another example might be, if you think parks are great places to hold meetups, a crux is that you think the weather is generally comfortable outside, and if it turned out that most people found the weather really uncomfortable then you’d think parks were bad places to to hold meetups.”\n\n*What’s a double crux:* “A double crux is something that’s a crux for both you and the person you’re talking to. To use the example above, if I think the weather is comfortable and you think it’s uncomfortable, and we’d change our minds if we were wrong about that, then we’ve found a double crux. We disagree about the facts in the world, and we do agree that fact matters. Again, it’s okay to have multiple cruxes and it’s fine to not be able to articulate your cruxes, but if you find that you always have more cruxes and you can’t say for sure that there aren’t others you aren’t thinking of, this technique isn’t going to be very helpful for you in disagreements.”\n\n*Acknowledge this is hard:* “Saying your cruxes out loud is hard! Finding them involves some introspection, and laying them out in front of another person where you might be proved wrong is a brave act. We hope in the process of finding where you actually disagree with your partner, you’ll find you have more in common with them and your disagreements are less all-encompassing than you might have thought before. Thank you for being willing to create a space where it’s safe to explore nuance, and to allow that you might turn out to be wrong.\n\nThis technique will only work if you start from the assumption that you might be wrong. I’m not saying that you are. I’m only saying that both people have to go into it understanding that it’s possible. Otherwise, how can you ask the other person to be willing to change their mind when you aren’t willing to change yours?\n\n*Actual steps:* ”So how do we actually do this? First, you’re going to think about your cruxes on the issue and make a list while your partner does likewise. A piece of paper and a pen might help. Then, you’re going to go over your lists together and see if there are any that pair up into a double crux. Last, you’re going to look together at how you might find out what the actual fact in the world looks like, what kind of test would indicate the answer was one way or the other. If you still have disagreement, sit with it for a moment and repeat those steps.”\n\n2\\. Say “To practice disagreement, first we need to figure out what we disagree on. I’m going to read out some statements. Raise your hand after each one with fingers showing how strongly you agree or disagree. Five fingers outstretched if you strongly agree, three fingers raised if you’re in the middle, one finger raised if you strongly disagree. If you’re uncomfortable discussing the statement, then I want you to hold up three fingers or to not hold your hand up at all, your choice. Any questions?” Answer questions as people ask them. “Okay then. As you’re holding up your hand, look around for people who disagree with you. 1s, look for 4s and 5s. 5s, look for 1s and 2s. Once you find a partner, pair off and feel free to move a little bit away from the group to start finding your cruxes.”\n\n3\\. Start reading the statements. Be sure to wait a minute or so after reading each one out, repeating the statement at least once. Again, a suggested list is [here](https://docs.google.com/spreadsheets/d/1OMlFy9gYdDIGSknBg_ZDzzy9E6trY43OEhw5o31o5BQ/edit?usp=sharing) but you should feel free to come up with your own.\n\n4\\. Go around and check in with the pairs. Answer questions about the technique, help moderate if things are getting heated, and always ask if they feel this is helping.\n\n**Notes**: Step four involves some moderation skills. You are deliberately creating conflict and asking people to work through it. \n\nThe bit about allowing people not to raise a hand or to opt out of discussing a statement is important. The things we feel really heated about are the things that double crux can be most useful in, but they aren’t always the best places to learn the technique and some of your attendees might be newcomers or strangers to the others. \n\n**Feedback**: https://forms.gle/zwiJ9N9e9hfL8FPH7\n\n**Credits**: This was adapted from an activity run by Sam Brown, which was in turn adapted from [CFAR](https://www.lesswrong.com/posts/WLQspe83ZkiwBc2SR/double-crux)’s development of the technique. Be aware this description gives you fourth-hand knowledge, and transcription errors may have crept in."
    },
    "voteCount": 2,
    "forceInclude": true
  },
  {
    "_id": "NrtbF3JHFnqBCztXC",
    "url": null,
    "title": "Law-Following AI 1: Sequence Introduction and Structure",
    "slug": "law-following-ai-1-sequence-introduction-and-structure",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "Law and Legal systems"
      },
      {
        "name": "AI Governance"
      },
      {
        "name": "AI"
      }
    ],
    "tableOfContents": {
      "sections": [
        {
          "title": "Key Definitions",
          "anchor": "Key_Definitions",
          "level": 1
        },
        {
          "title": "A Sketch of LFAI",
          "anchor": "A_Sketch_of_LFAI",
          "level": 1
        },
        {
          "title": "Appendix: More Conceptual Clarifications on LFAI",
          "anchor": "Appendix__More_Conceptual_Clarifications_on_LFAI",
          "level": 1
        },
        {
          "title": "Applicability of Law to AI Systems",
          "anchor": "Applicability_of_Law_to_AI_Systems",
          "level": 2
        },
        {
          "title": "Predicting Legality",
          "anchor": "Predicting_Legality",
          "level": 2
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        },
        {
          "anchor": "comments",
          "level": 0,
          "title": "10 comments"
        }
      ],
      "headingsCount": 7
    },
    "contents": {
      "markdown": "*This post is written in my personal capacity, and does not necessarily represent the views of OpenAI or any other organization. Cross-posted to the [Effective Altruism Forum](https://forum.effectivealtruism.org/posts/9RZodyypnWEtErFRM/law-following-ai-1-sequence-introduction-and-structure).*\n\nThis [sequence of posts](https://forum.effectivealtruism.org/s/3pyRzRQmcJNvHzf6J) will argue that working to ensure that AI systems follow laws is a worthwhile way to improve the long-term future of AI.^[For early, informal discussion on this topic, see Michael St. Jules, *What are the challenges and problems with programming law-breaking constraints into AGI?*, **Effective Altruism Forum** (Feb. 2, 2020), https://forum.effectivealtruism.org/posts/qKXLpe7FNCdok3uvY/what-are-the-challenges-and-problems-with-programming-law [https://perma.cc/HJ4Y-XSSE] and accompanying comments.]\n\n The structure of this sequence will be as follows:\n\n* First, in this post, I will define some key terms and sketch what an ideal law-following AI (&quot;LFAI&quot;) system might look like.\n* In the next few posts, I will explain why law-following might not emerge by default given the existing constellation of alignment approaches, financial objectives, and legal constraints, and explain why this is troubling.\n* Finally, I will propose some policy and technical routes to ameliorating these problems.\n\nIf the vision here excites you, and you would like to get funding to work on it, [get in touch](mailto:cullokeefe@gmail.com). I may be excited to recommend grants for people working on this, as long as it does not distract them from working on more important alignment issues.\n\n![Dall•E](https://i.imgur.com/EkPDCHX.png)\n\n_Image by OpenAI&#39;s DALL·E._\n\n## Key Definitions\n\nA **law-following AI** , or **LFAI** , is an AI system that is designed to rigorously comply with some defined set of human-originating rules (&quot;laws&quot;),^[ Whether such rules are actually encoded into legislation is not particularly important. Virtually all legal rules not part of public law can be made “legal” with regards to particular parties as part of a contract, for example. In any case, the heart of LFAI is being bound to follow rules, and interpreting those rules leveraging the rich body of useful rule-interpretation metarules from law.] using legal interpretative techniques,^[ This is important because one of the core functions of law is to provide metarules regarding the interpretation of rules, guided by certain normative values (e.g., fairness, predictability, consistency). Indeed, rules of legal interpretation aim to solve many problems relevant to AI interpretation of instructions. *Cf.* Dylan Hadfield-Menell & Gillian Hadfield, *Incomplete Contracting and AI Alignment* (2018)  (preprint), https://arxiv.org/abs/1804.04268.] under the assumption that those laws apply to the AI in the same way that they would to a human. By &quot;intrinsically motivated,&quot; I mean that the AI is motivated to obey those rules regardless of whether (a) its human principal wants it to obey the law,^[That is, the AI is not law-following _just because_ the principal wants the AI to follow the law. Indeed, LFAI should disobey orders that would require it to behave illegally.] or (b) disobeying the law would be instrumentally valuable.^[That is, the AI is not law-following _just because_ it is instrumentally valuable to it (because, e.g., being caught breaking the law would cause the AI to be turned off).] (The Appendix to this post explores some possible conceptual issues with this definition of LFAI.)\n\nI will compare LFAI with **intent-aligned AI**. The standard definition of &quot;intent alignment&quot; generally concerns only the relationship between some property of a human principal _H_ and the actions of the human&#39;s AI agent _A_:\n\n- Jan Leike et al. [define](https://arxiv.org/pdf/1811.07871.pdf) the &quot;agent alignment problem&quot; as &quot;How can we create agents that behave in accordance with the user&#39;s intentions?&quot;\n- Amanda Askell et al. [define](https://arxiv.org/pdf/2112.00861.pdf) &quot;alignment&quot; as &quot;the degree of overlap between the way two agents rank different outcomes.&quot;\n- Paul Christiano [defines](https://ai-alignment.com/clarifying-ai-alignment-cec47cd69dd6) &quot;AI alignment&quot; as &quot;_A_ is trying to do what _H_ wants it to do.&quot;\n- Richard Ngo [endorses](https://www.alignmentforum.org/s/mzgtmmTKKn5MuCzFJ/p/PvA2gFMAaHCHfMXrw) Christiano&#39;s definition.\n\nIason Gabriel does not directly define &quot;intent alignment,&quot; but [provides](https://arxiv.org/pdf/2001.09768.pdf) a taxonomy wherein an AI agent can be aligned with:\n\n1. &quot;Instructions: the agent does what I instruct it to do.&quot;\n2. &quot;Expressed intentions: the agent does what I intend it to do.&quot;\n3. &quot;Revealed preferences: the agent does what my behaviour reveals I prefer.&quot;\n4. &quot;Informed preferences or desires: the agent does what I would want it to do if I were rational and informed.&quot;\n5. &quot;Interest or well-being: the agent does what is in my interest, or what is best for me, objectively speaking.&quot;\n6. &quot;Values: the agent does what it morally ought to do, as defined by the individual or society.&quot;\n\nAll but (6) concern the relationship between _H_ and _A_. It would therefore seem appropriate to describe them as types of intent alignment.\n\nAlignment with some broader or more complete set of values—such as type (6) in Gabriel&#39;s taxonomy, [Coherent Extrapolated Volition](https://www.alignmentforum.org/tag/coherent-extrapolated-volition), or what Ngo [calls](https://www.alignmentforum.org/s/mzgtmmTKKn5MuCzFJ/p/PvA2gFMAaHCHfMXrw) &quot;maximalist&quot; or &quot;ambitious&quot; alignment—is perhaps desirable or even necessary, but seems harder than working on intent alignment.^[As Ngo [says](https://www.alignmentforum.org/s/mzgtmmTKKn5MuCzFJ/p/PvA2gFMAaHCHfMXrw), &quot;My opinion is that defining alignment in maximalist terms is unhelpful, because it bundles together technical, ethical and political problems. While it may be the case that we need to make progress on all of these, assumptions about the latter two can significantly reduce clarity about technical issues.&quot;] Much current alignment work therefore focuses on intent alignment.\n\nWe can see that, on its face, intent alignment does not entail law-following. A key crux of this sequence, to be defended in subsequent posts, is that this gap between intent alignment and law-following is:\n\n1. Bad in expectation for the long-term future.\n2. Easier to bridge than the gap between intent alignment and deeper alignment with moral truth.\n3. Therefore worth addressing.\n\nTo clarify, this sequence does **not** claim that LFAI can replace intent alignment.\n\n## A Sketch of LFAI\n\nWhat might an LFAI system look like? I&#39;m not a computer scientist, but here is roughly what I have in mind.\n\nIf _A_ is an LFAI, then _A_&#39;s evaluation of the legality of an action will sometimes trump _A_&#39;s evaluation of an action in light of its benefit to _H_. In LFAI, as in a legally scrupulous human, legality constrains how an agent can advance their principal&#39;s interests. For example, a human mover may be instructed to efficiently move a box for her principal, but may not unnecessarily destroy others&#39; property in doing so. Similarly, an LFAI moving a box normally would not knock over a vase in its path, because doing so would violate the legal rights of the vase-owner.^[_Cf., e.g._, Dario Amodei et al., Concrete Problems in AI Safety 4 (2016), [https://arxiv.org/pdf/1606.06565.pdf](https://arxiv.org/pdf/1606.06565.pdf).]\n\nAbove, I preliminarily defined LFAI as &quot;rigorously comply[ing]&quot; with some set of laws. Obviously this needs a bit more elaboration. We probably don&#39;t want to define this as _minimizing_ legal noncompliance, since this would make the system extremely risk-averse to the point of being useless. More likely, one would attempt to weight legal downside risks heavily in the agent&#39;s objective function,^[I don&#39;t here offer an opinion on what training regime would yield such an outcome—my hope is to get someone to answer that for me!]\n such that it would keep legal risk to an acceptable level.^[This approach may work particularly well when combined with insurance requirements for people deploying AI systems.]\n\nIt is worth noting that LFAI is ideally not merely attempting to reduce its expected legal liability _in fact_. As will be explored later, a sufficiently smart agent could probably reduce its expected legal liability merely by hiding its knowledge/intentions/actions or corrupting a legal proceeding. An LFAI, by contrast, is attempting to obey the law in an idealized sense, even if it is unlikely to actually face legal consequences.\n\nAn LFAI system does not need to store all knowledge regarding the set of laws that it is trained to follow. More likely, the practical way to create such a system would be to make the system capable of recognizing when it faces sufficient legal uncertainty,^[In the same way that an intent-aligned AI will sometimes ask for clarifications from a human principal. _See_ [Christiano](https://ai-alignment.com/clarifying-ai-alignment-cec47cd69dd6).]\n then seeking evaluation from a legal expert system (&quot;Counselor&quot;).^[Note that there are [ELK](https://docs.google.com/document/d/1WwsnJQstPq91_Yh-Ch2XRL8H_EpsnjrC1dwZXR37PC8/edit#)-style problems with this approach. If an AI is asking for legal advice and wants to minimize the negative signal it gets from the Counselor, it may hide certain relevant information (e.g., its true state of knowledge or its true intentions) from the Counselor. A good solution, as discussed, could be to simulate an idealized adjudication of the issue if all the parties knew all the relevant facts and had equal legal firepower. But incentivizing the LFAI to tell the Counselor its true knowledge/intentions is an ELK problem. In the limit, the Counselor need not strictly be a distinct agent from the LFAI: an LFAI system may have Counselor capabilities and run this &quot;consultation&quot; process internally. Nevertheless, it is illustratively useful to imagine a separation of the LFAI and the Counselor.]\n\n The Counselor could be a human lawyer, but in the long-run is probably most robust and efficient if (at least partially) automated. The Counselor would then render advice on the pure basis of idealized legality: the probability and expected legal downsides that would result from an idealized legal dispute regarding the action if everyone knew all the relevant facts.^[This would be _idealized_ so that details not ultimately relevant to the substantive legality of the action (e.g., jurisdiction, AI personhood, other procedural matters, asymmetries in legal firepower) can be ignored. See the final footnote of this piece for further discussion.]\n\nThus pseudocode for an LFAI who wants to take an action _X_ to benefit _H_ might be:\n\n1. If _X_ is clearly illegal:\n    1. don&#39;t do _X_.\n2. Elseif _X_ is maybe-illegal:\n    1. Give Counselor all relevant information about _X_ in an unbiased way; then\n    2. Get Counselor&#39;s opinion on expected legal consequences from _X_; then\n    3. Weigh expected legal consequences against benefit to _H_ from _X_; then\n    4. Decide whether to do _X_ given those weightings.\n3. Else:\n    1. do _X_.\n\nNote that this pseudocode may resemble the decisionmaking process of _A_ if _H_ wants _A_ to obey the law. Thus, one route to giving an intent-aligned AI the motivation to obey the law may be stipulating to _A_ that _H_ wants _A_ to obey the law.\n\nWith this picture in mind, it seems like, to make LFAI a reality, progress on the following open problems (non-exhaustively) would be useful:\n\n- Reliably stipulating low-following conditions to AI systems&#39; objectives.\n  - Resolving any disagreement between law-following and a principal&#39;s instructions appropriately.\n- Getting AI agents to recognize when they face legal uncertainty (especially in a way that does not incentivize ignorance of the law).\n  - This seems similar to the intent alignment problem of getting agents to recognize when they need further information from principals, as in corrigibility work.\n- Eliciting, in natural language, AI systems&#39; honest description of its knowledge and desired actions.\n  - As noted above, this seems likely to run into problems related to ELK generally.\n- Mapping legal concepts of mental states (e.g., intent, knowledge) to features of AI systems.^[See the Appendix for more discussion on this point.]\n  - This seems related to interpretability and explainability work.\n- Building Counselor functions.\n  - Automating the process of legal research given a natural language description of an agent&#39;s proposed actions and mental state.\n  - Simulating idealized and fair substantive legal disputes.\n    - This seems related to [Debate](https://www.alignmentforum.org/posts/SrsH2MyyH8MqH9QSr/parallels-between-ai-safety-by-debate-and-evidence-law).\n\n## Appendix: More Conceptual Clarifications on LFAI\n\nThis Appendix provides some additional clarification on the definition of LFAI given above.\n\n### Applicability of Law to AI Systems\n\nOne might worry that the law often regulates physical behavior in a way that is not obviously applicable to all AI systems. For example, physical contact with another is an element of the tort of battery.^[_See Battery_, **Wex** , [https://www.law.cornell.edu/wex/battery](https://www.law.cornell.edu/wex/battery) (last accessed Sept. 3, 2021).] However, this may be less of a problem than initially appears: courts have been able to reason through whether to apply laws originating in meatspace to computational and cyberspace conduct.^[_See, e.g._, Intel Corp. v. Hamidi, 71 P.3d 296, 304–08 (Cal. 2003) (applying trespass to chattels to unauthorized electronic computer access); MAI Sys. Corp. v. Peak Computer, Inc., 991 F.2d 511, 518–19 (9th Cir. 1993) (storing data in RAM sufficient to create a &quot;copy&quot; for copyright purposes, despite the fact that a &quot;copy&quot; must be &quot;fixed in a tangible medium&quot;); _cf._ United States v. Jones, 565 U.S. 400, 406 n.3 (2012) (analogizing GPS tracking to in-person surveillance for Fourth Amendment purposes).] Whether such analogies are _properly_ applied is indeed highly debatable,^[_See, e.g._, Jonathan H. Blavin &amp; I. Glenn Cohen, _Gore, Gibson, and Goldsmith: The Evolution of Internet Metaphors in Law and Commentary_, 16 **Harv. J.L. &amp; Tech.** 265 (2002).] but the fact that such analogizing is conceptually possible reduces the force of this objection. Furthermore, if some laws are simply inapplicable to non-embodied actors, this is not a problem for the conceptual coherence of LFAI as a whole: an LFAI can simply ignore those laws,^[However, the case for working on LFAI certainly diminishes with the number of applicable laws.] and we can design laws specifically with computational content.\n\nPerhaps a more fundamental problem is that the law frequently depends on _mental states_ that are not straightforwardly applicable to AI systems. For example, the legality of an action may depend on whether the actor _intended_ some harmful outcome. Thus, much of the value of LFAI depends on whether we can map human understandings of moral culpability to AI systems.\n\nTo me, however, this seems like an argument in favorof working on LFAI. Regardless of whether LFAI as such is valuable, if we expect increasingly autonomous AI systems to take increasingly impactful actions, we would probably like to understand how their objective functions (analogous to human _motives_) and world-model (analogous to human _knowledge_) map to their actions and the effects thereof. This is for the same reasons that we care about human motives and knowledge: when evaluating the alignment of agents, it is useful to know whether an agent intended to cause some harm, or knew that such a harm would ensue, etc. LFAI depends on progress on this, but is also potentially a useful toy problem for interpretability and related work in ML.\n\n### Predicting Legality\n\nLegal compliance is also a function of both law and facts, and responsibility for definitive determinations of law and facts is split between judges and juries. Law often invokes standards like &quot;reasonableness&quot; that are definitively assessed only ex post, in the context of a particular dispute. The definitive legality of an action may therefore turn on an actual adjudication of the dispute. This is of course costly, which is why I suspect we would want an LFAI to act on its best estimate of what such an adjudication would yield (after asking a Counselor), rather than wait for such adjudication to take place.^[This raises further issues, including the possibility of self-reference. For example, an LFAI or Counselor asymmetrically deployed by one litigant may be able to persuade a judge or jury of its position, even if it&#39;s not the best outcome. To avoid this, such simulations should assume that judges and juries are fully apprised of all relevant facts (i.e., neither the LFAI nor Counselor can obscure relevant evidence) and if deployed in the simulated proceeding are symmetrically available to both sides.]\n\nIt is also worth distinguishing between whether an _actual court of law_ would rule that an AI&#39;s behavior violated some law and whether a _simulated and fair legal dispute resolution process_ (possibly including, for example, a bespoke arbitral panel) would conclude that the behavior violated the law. The latter may be more convenient for working on LFAI for a number of reasons, including that it can ignore or stipulate away some of the peculiarities of adjudicating disputes in which an AI system is a &quot;party.&quot;"
    },
    "voteCount": 10,
    "forceInclude": true
  },
  {
    "_id": "9aSi7koXHCakb82Fz",
    "url": null,
    "title": "Law-Following AI 2: Intent Alignment + Superintelligence → Lawless AI (By Default)",
    "slug": "law-following-ai-2-intent-alignment-superintelligence",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "AI Governance"
      },
      {
        "name": "Law and Legal systems"
      },
      {
        "name": "AI"
      }
    ],
    "tableOfContents": {
      "sections": [
        {
          "title": "Sufficiently Intelligent Agents Can Evade Detection and Attribution",
          "anchor": "Sufficiently_Intelligent_Agents_Can_Evade_Detection_and_Attribution",
          "level": 1
        },
        {
          "title": "Other Ways to Circumvent Law",
          "anchor": "Other_Ways_to_Circumvent_Law",
          "level": 1
        },
        {
          "title": "A Competent Intent-Aligned Agent Will Sometimes Intentionally Break the Law",
          "anchor": "A_Competent_Intent_Aligned_Agent_Will_Sometimes_Intentionally_Break_the_Law",
          "level": 1
        },
        {
          "title": "Appendix: The Impossibility Defense",
          "anchor": "Appendix__The_Impossibility_Defense",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        },
        {
          "anchor": "comments",
          "level": 0,
          "title": "2 comments"
        }
      ],
      "headingsCount": 6
    },
    "contents": {
      "markdown": "*This post is written in my personal capacity, and does not necessarily represent the views of OpenAI or any other organization. Cross-posted to the [Effective Altruism Forum](https://forum.effectivealtruism.org/s/3pyRzRQmcJNvHzf6J/p/cEj7o9rbPjmy7CDht).*\n\nIn the [first post](https://www.alignmentforum.org/posts/NrtbF3JHFnqBCztXC/law-following-ai-1-sequence-introduction-and-structure) of this [sequence](https://forum.effectivealtruism.org/s/3pyRzRQmcJNvHzf6J), I defined &quot;law-following AI&quot; (&quot;LFAI&quot;) and &quot;intent alignment.&quot; In this post, I will begin to motivate the case for working on LFAI by showing that an AI agent _A_ that is only intent-aligned (i.e., without additional law-following constraints directly on the agents) would in many cases break the law to advance its human principal _H_&#39;s interests.\n\nIf you agree that it is generally good for agents to be bound by law, then this should be a pro tanto reason to support work on LFAI in addition to intent alignment. If you need more convincing that LFAI would be generally good, the next few posts will explore some more specific motivations for LFAI.\n\n![](https://i.imgur.com/hFvuhQM.jpeg)\n\n_Image by OpenAI&#39;s DALL·E_\n\n## Sufficiently Intelligent Agents Can Evade Detection and Attribution\n\nI suspect this point will need little argumentation to many of my readers. Evasion and obfuscation seems likely to be a generally promising means of ensuring self-preservation (because detection would increase the probability of punishment and constraint), which in turn is plausibly a convergent instrumental goal of optimizing AI systems.^[_See generally_ Alexander Matt Turner et al., Optimal Policies Tend To Seek Power (version 9, 2021) (preprint), [https://arxiv.org/abs/1912.01683](https://arxiv.org/abs/1912.01683).]\n\n For example, to evade detection and attribution, _A_ could:\n\n- Behave innocently only when being observed;\n- Orchestrate complex malevolent actions through a coordinated series of small, inscrutable or innocent-seeming steps;\n- Surreptitiously create and run subagents or copies of itself on new hardware, potentially very far removed from its original hardware;\n- Convince humans or other systems to take actions on its behalf, without revealing its true identity; and/or\n- Delete logs of its actions.\n\nA sufficiently intelligent AI agent could likely consistently fool humans using these and other (unforeseen) techniques.\n\n## Other Ways to Circumvent Law\n\nEven in the best case scenario, where the agent is detected and within the jurisdiction of a well-functioning legal system, it would be reasonable to question whether _A_ or _H_ could be effectively subject to normal legal processes.^[Even this may not hold for many types of agreements, including in particular international treaties.] If _A_ had a motivation to, _A_ could help _H_ escape liability by, for example:^[_See also_ **Cullen O&#39;Keefe et al., The Windfall Clause: Distributing the Benefits of AI for the Common Good** 26–27 (2020), [https://perma.cc/8KES-GTBN](https://perma.cc/8KES-GTBN); Jan Leike, On The Windfall Clause (2020) (unpublished manuscript), [https://docs.google.com/document/d/1leOVJkNDDj-NZUZrNJauZw9S8pBpuPAJotD0gpnGEig/](https://docs.google.com/document/d/1leOVJkNDDj-NZUZrNJauZw9S8pBpuPAJotD0gpnGEig/edit#).]\n\n- &quot;Outlawyering&quot; counterparties.\n- Benefitting _H_ in a way that would undermine recourse for creditors.\n- Shifting and hiding assets in ways that would make it difficult for creditors to reach.^[Indeed, this is already a common technique without the use of AI systems.]\n- Persuasively arguing for the law to be changed in _H_&#39;s favor (by legislation or otherwise).\n- Engaging in vexatious litigation techniques to delay and raise the costs of the proceeding.\n- Convincingly fabricating favorable evidence and destroying or obscuring unfavorable evidence.\n- Bribing, misleading, or intimidating counterparties, witnesses, jurors, and judges.\n\n## A Competent Intent-Aligned Agent Will Sometimes Intentionally Break the Law\n\nAs I said in the previous post, on its face, intent-alignment does not entail law-following. Part of law is coercing prosocial behavior:^[&quot;If men were angels, no government would be necessary.&quot; **The Federalist** No. 51. This surely overstates the point: law can also help solve coordination problems and facilitate mutually desired outcomes. But prosocial coercion is nevertheless an important function of law and government.] law incentivizes agents to behave in ways that they do not intrinsically want to behave. If _A_ is aligned with _H_, whether _A_ obeys the law depends on whether _H_ **wants** _A_ to obey the law. Subsequent posts will examine what legal consequences _H_ might face if _A_ causes legally cognizable harms. However, even if an adequate theory of liability for the _H_ was available, it will seem impossible to hold _H_ liable if nobody can produce evidence that some agent of _H_&#39;s was responsible for those harms. As argued above, a sufficiently intelligent agent probably _could_ consistently avoid leaving any such evidence.\n\nDetection and attribution would not solve the problem, however. Even if _H_ was compelled, under court order, to instruct _A_ to behave in some way, it&#39;s not clear that _A_ would follow the order. Consider again [Iason Gabriel&#39;s taxonomy of alignment](https://arxiv.org/pdf/2001.09768.pdf). We can see that, for most types of intent alignment, an intent-aligned agent would likely not obey compelled instructions that are against _H_&#39;s true wishes:\n\n1. &quot;Instructions: the agent does what I instruct it to do.&quot;\n    - If _H_ asks the agent to stop, it will. However, this type of alignment is likely to be insufficient for safety.^[_See_ [Gabriel](https://arxiv.org/pdf/2001.09768.pdf) at 7 (&quot;However, as Russell has pointed out, the tendency towards excessive literalism poses significant challenges for AI and the principal who directs it, with the story of King Midas serving as a cautionary tale. In this fabled scenario, the protagonist gets precisely what he asks for—that everything he touches turns to gold—not what he really wanted. Yet, avoiding such outcomes can be extremely hard in practice. In the context of a computer game called CoastRunners, an artificial agent that had been trained to maximise its score looped around and around in circles ad infinitum, achieving a high score without ever finishing the race, which is what it was really meant to do. On a larger scale, it is difficult to precisely specify a broad objective that captures everything we care about, so in practice the agent will probably optimise for some _proxy_ that is not completely aligned with our goal. Even if this proxy objective is &#39;almost&#39; right, its optimum could be disastrous according to our true objective.&quot; (citations omitted)).]\n\n2. &quot;Expressed intentions: the agent does what I intend it to do.&quot;\n\n    - Even if _H_ coerced to instruct the agent to stop, a sufficiently intelligent agent will probably not follow the instruction. Even though _H_ has instructed _A_ to stop, by supposition, _H_ does not actually want _A_ to stop. Under most definitions of intent alignment,^[Based on my informal survey of alignment researchers at OpenAI. Everyone I asked agreed that an intent-aligned agent would not follow an order that the principal did not actually want followed. _Cf. also_[Christiano](https://ai-alignment.com/clarifying-ai-alignment-cec47cd69dd6) (A is aligned when it &quot;is trying to do what H _wants_ it to do&quot; (emphasis added)).] it would therefore not follow the order.^[We can compare this definition of intent with to the relevant legal definition thereof: &quot;To have in mind a fixed purpose to reach a desired objective; to have as one&#39;s purpose.&quot; _INTEND_, **Black&#39;s Law Dictionary** (11th ed. 2019). _H_ does not &quot;intend&quot; for the order to be followed under this definition: the &quot;desired objective&quot; of _H_ issuing the order is to follow _H_&#39;s legal obligations, not actually achieve the result contemplated by the order.]\n\n3. &quot;Revealed preferences: the agent does what my behaviour reveals I prefer.&quot;\n\n    - _H_&#39;s revealed preference would probably be that _A_ not follow the order.^[For example, _H_ would exhibit signs of happiness when _A_ continues.]  Therefore, _A_ would not obey the order.\n4. &quot;Informed preferences or desires: the agent does what I would want it to do if I were rational and informed.&quot;\n\n    - _H_&#39;s rational and informed preference would probably be that _A_ not follow the order. Therefore, _A_ would not obey the order.\n5. &quot;Interest or well-being: the agent does what is in my interest, or what is best for me, objectively speaking.&quot;\n\n    - It is in _H_&#39;s objective best interest for _A_ to disobey the order. Therefore, _A_ would not obey the order.\n\nNow, it may be the case that _H_ actually _does_ want _A_ to obey the order, though compelled, if the failure of _A_ to obey would lead to liability for _H_ that is worse than the results of _A_&#39;s obedience (e.g., because _H_ will be held in contempt of court if _A_ does not actually obey). However, note that &quot;[o]rdinarily, one charged with contempt of court for failure to comply with a court order makes a complete defense by proving that he is unable to comply.&quot;^[United States v. Bryan, 339 U.S. 323, 330 (1950).] _H_ can comply with an order that requires _H_ to **command** _A_ to do something, but it may be impossible for _H_ to actually **force** _A_ to comply if the order is against _H_&#39;s true wishes (to which _A_ is aligned).^[A principal may want its AI agents to be able to distinguish between genuine and coerced instructions, and to disobey the latter. Indeed, this might generally be a good thing, except for the case when compulsion is pursuant to law rather than extortion.] If so, _H_ could have an impossibility defense to contempt.^[See Appendix for further discussion.] _A_, understanding this, may continue on without complying because _A_ understands that _H_ will not actually be held in contempt. _H_ can therefore benefit from _A_&#39;s disobedience. _A_ will therefore be lawless.\n\n\n## Appendix: The Impossibility Defense\n\n_A_&#39;s behavior here would be functionally similar to a trustee acting pursuant to a distress clauses in asset protection trusts (&quot;APTs&quot;).^[ _See generally Asset Protection Trust_, **Wex** , [https://www.law.cornell.edu/wex/asset\\_protection\\_trust](https://www.law.cornell.edu/wex/asset_protection_trust) (last visited Mar. 24, 2022); Richard C. Ausness, _The Offshore Asset Protection Trust: A Prudent Financial Planning Device or the Last Refuge of A Scoundrel?_, 45 **Duq. L. Rev.** 147, 174 (2007).] While these provisions can prevent a contempt charge, the burden of proof on the alleged contemnor is high. ^[_See generally_ 2 **Asset Protection: Dom. &amp; Int&#39;l L. &amp; Tactics** §§ 26:5–6 (2021).]\n\nAs a matter of policy, however, courts may decide to pre-commit to a contempt standard that does not allow for an impossibility defense when the defendant&#39;s AI agent refuses to obey orders issued pursuant to a court order. Analogously, courts are imposing heightened impossibility standards in response to APTs, in an attempt to make their use more onerous.^[_See id._] If this pre-commitment is credible, it may change the agent&#39;s behavior because _H_ may then genuinely desire _A_ to perform (because _H_ will be held in contempt otherwise). However, such a policy may be both contrary to precedent and more fundamental notions of fairness and due process: in some cases _A_&#39;s refusal to comply may be a surprise to _H_, since _H_ may have had a long history of observing _A_ scrupulously complying with _H_&#39;s orders, and _H_ did not implement principal–agent alignment for the purpose of evading court orders. If so, _H_ may be able to invoke impossibility more easily, since the impossibility was not as clearly intentionally self-induced as in the APT case. Furthermore, I would intuitively not expect courts to advance such a reform until they have faced multiple such instances of AI disobedience. This seems bad if we expect the earliest deployed AI agents to have an outsized impact on society. In any case, I would expect the _possibility_ of favorable law reform post-AGI to solve this problem to be an insufficient solution. Finally, I would expect sufficiently intelligent agents to recognize these dynamics, and attempt to find ways to circumvent the contempt process itself, such as by surreptitious non-compliance.\n\nAn alternative, pre-AGI solution (which arguably seems pretty sensible from a public policy perspective anyway) is to advocate weakening the impossibility defense for self-imposed impossibility."
    },
    "voteCount": 3,
    "forceInclude": true
  },
  {
    "_id": "DfcXaGH7XGYjW22C2",
    "url": null,
    "title": "Law-Following AI 3: Lawless AI Agents Undermine Stabilizing Agreements",
    "slug": "law-following-ai-3-lawless-ai-agents-undermine-stabilizing",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "AI Governance"
      },
      {
        "name": "Law and Legal systems"
      },
      {
        "name": "AI"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "*This post is written in my personal capacity, and does not necessarily represent the views of OpenAI or any other organization. Cross-posted to the [Effective Altruism Forum](https://forum.effectivealtruism.org/posts/ExHkFcNAL9cjqFmsF/law-following-ai-3-lawless-ai-agents-undermine-stabilizing)*\n\nIn the [previous post](https://www.alignmentforum.org/posts/9aSi7koXHCakb82Fz/law-following-ai-2-intent-alignment-superintelligence) of this [sequence](https://forum.effectivealtruism.org/s/3pyRzRQmcJNvHzf6J), I argued that intent-aligned AIs would, by default, have incentives to break the law. This post goes into one particularly bad consequence of that incentive: the increased difficulty of making credible pre-AGI commitments about post-AGI actions.\n\n![](https://i.imgur.com/7GnVZsT.png)\n\n_Image by OpenAI&#39;s DALL·E_\n\nIn AGI policy and strategy, it would often be useful to adopt credible commitments about what various actors will do post-AGI. For example, it may be desirable for two leading nations in AGI to agree to refrain from racing to build AGI (at the potential cost to AGI safety) and instead split the economic upside from AGI, thereby transforming a negative-sum dynamic into a positive-sum one.^[_Cf._ Amanda Askell et al., The Role of Cooperation in Responsible AI Development (2019) (preprint), [https://arxiv.org/abs/1907.04534](https://arxiv.org/abs/1907.04534).] Nations might agree to forego their own development of militarily useful AI systems if they receive security assurances from states that _do_ choose to pursue such systems,^[Of course, this could be analogized to similar agreements regarding nuclear disarmament, such as Ukraine&#39;s fateful decision to surrender its post-Soviet nuclear arsenal in exchange for security assurances (which have since been violated by Russia). _See, e.g._, Editorial, _How Ukraine Was Betrayed in Budapest_, **Wall St. J.** (Feb. 23, 2022), [https://www.wsj.com/articles/how-ukraine-was-betrayed-in-budapest-russia-vladimir-putin-us-uk-volodymyr-zelensky-nuclear-weapons-11645657263?reflink=desktopwebshare\\_permalink](https://www.wsj.com/articles/how-ukraine-was-betrayed-in-budapest-russia-vladimir-putin-us-uk-volodymyr-zelensky-nuclear-weapons-11645657263?reflink=desktopwebshare_permalink). Observers (especially those facing potential conflict with Russia) might reasonably question whether any such disarmament agreements are credible.] thus reducing the number of actors pursuing potentially dangerous military AI development, and therefore reducing the risk of a catastrophic accident. One can imagine similar types of pre-AGI agreements concerning space colonization post-AGI, the welfare of digital minds, democratic control of AGI, and many more important issues in the post-AGI world.\n\nLawless AI poses an enormous problem for such agreements. Agreements between entities, such as states or companies, require performance by agents of the parties: a state or corporation only acts through its agents. But if one party cannot convincingly show that their agents will carry out orders to comply with such agreements, the other party will be very reluctant to enter into an agreement with them. All the more so if the principal cannot prevent its agents from working _against_ aspects of the agreement that are detrimental to the principal.\n\nI will use the following as a toy problem to show what I mean more concretely: _X_ and _Y_ are two leading AGI development firms considering agreeing to split the profits from AGI development if they both agree to common minimum implement safety measures to prevent racing.^[We will ignore antitrust considerations regarding such an agreement for the sake of illustration.] How might this play out?\n\n_X_ might worry that _Y_ will develop an AGI agent, _A(Y)_, with the objective of maximizing the value of _Y_&#39;s shares. _A(Y)_ can do this in ways that would undermine _X_&#39;s recourse to _Y_&#39;s assets. For example, _A(Y)_ could create a subagent that surreptitiously earns money and directly mails checks to the shareholders of _Y_, as a sort of pseudo-dividend. That money might never pass through _Y_, so _X_ would have a hard time reaching it, even if they detected this. _X_ might also worry that _A(Y)_ would also corrupt the legal process in _Y_&#39;s favor, so that even if they had a good legal claim, they could not vindicate it in court at any reasonable cost.\n\nWith these concerns in mind, the deal is unlikely to happen.\n\nHow might LFAI improve the situation? _X_ and _Y_ could both agree that:\n\n1. All sufficiently advanced AI agents (&quot;Qualifying Agents&quot;) that they train, use, benefit from, or deploy will be LFAI.\n2. The other party may, through a neutral third-party bound by confidentiality,^[So that this inspection process cannot be used for industrial espionage.] inspect Qualifying Agents and their development to verify that the trained agent will be LFAI when deployed.\n3. Qualifying Agents will be specifically instructed to assume that they are bound by the agreement.^[This may not be the case as a matter of background contract and agency law, and so should be stipulated.]\n4. The parties will generally cause all of their agents to give full effect to the agreement.\n5. In any proceeding where the parties are adverse, neither party shall use any artificial intelligence technology to its legal or strategic benefit, unless the other party has free and equal access to the same technology.^[This is designed to guard against the case where one party develops AI super-lawyers, then wields them asymmetrically to their advantage.]\n6. In case of any ambiguity in the terms of the agreement, the ambiguity shall be resolved in favor of more constraints on Qualifying Agents and their principal(s).\n\nThe resulting agreement would probably not be watertight, but the parties would have much greater assurances of post-AGI performance than they would absent LFAI. Referring back to the example above, _A(Y)_ could not surreptitiously earn money outside of _Y_ and pass it on the _Y_&#39;s shareholders, nor could _A(Y)_ illegally corrupt adjudications of the agreement. LFAI, then, could make the parties much more likely to be willing to enter stabilizing pre-AGI agreements that require post-AGI performance. This seems like a substantial improvement."
    },
    "voteCount": 1,
    "forceInclude": true
  },
  {
    "_id": "HDmcJv6SdyEFpFbcD",
    "url": null,
    "title": "Law-Following AI 4: Don't Rely on Vicarious Liability",
    "slug": "law-following-ai-4-don-t-rely-on-vicarious-liability",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "Law and Legal systems"
      },
      {
        "name": "AI Governance"
      },
      {
        "name": "AI"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "*This post is written in my personal capacity, and does not necessarily represent the views of OpenAI or any other organization. Cross-posted to the [Effective Altruism Forum](https://forum.effectivealtruism.org/posts/qtwjGAwFqfbPJsFBC/law-following-ai-4-don-t-rely-on-vicarious-liability).*\n\n![](https://i.ibb.co/Pjht3sz/DALL-E-2022-04-08-16-58-15.png)\n\n_Image by OpenAI&#39;s DALL·E_\n\nIf an agent _A_ causes some harm while intending to benefit a principal _P_, what is _P_&#39;s liability? The answer to this question is important because any liability to _P_ [should affect *A*'s calculus](https://forum.effectivealtruism.org/s/3pyRzRQmcJNvHzf6J/p/qKXLpe7FNCdok3uvY?commentId=Y4kjNXLx2GB2pb9Kh) (insofar as _A_ is trying to benefit and avoid harming _P_). Liability to _P_ would help deter _A_ from causing harm.\n\nIf _A_ is a human, the law currently provides at least two mechanisms for discouraging :\n\n1. Making _A_ directly liable, and\n2. Making _P_ vicariously liable for _A_&#39;s actions.^[*See generally Vicarious Liability*, **Wex**, https://www.law.cornell.edu/wex/vicarious_liability (last accessed Sept. 10, 2021); *Respondeat Superior*, **Wex**, https://www.law.cornell.edu/wex/respondeat_superior (last accessed Sept. 10, 2021).]\n\nWhat if _A_ is an AI? AIs are not (yet?) legal persons, and so cannot yet be held directly liable. Thus, the main legal deterrent would have to work on _P_ or some other person in the causal chain, such as the developer of the AI agent (who may not be the principal). However, there are several reasons to worry about relying on this as a strategy to make _A_ compliant (which, in this case, means not tortiously harming others) under the current state of law and AI.\n\nFirst, the problem of evasion still remains. Because [sufficiently intelligent agents can evade detection and attribution](https://forum.effectivealtruism.org/s/3pyRzRQmcJNvHzf6J/p/cEj7o9rbPjmy7CDht#Sufficiently_Intelligent_Agents_Can_Evade_Detection_and_Attribution), _A_ may often (perhaps usually) prefer evasion over compliance when compliance would hinder _A_&#39;s ability to benefit _P_.\n\nSecond, the applicability and appropriateness of various theories of vicarious liability to the actions of AI agents is heavily debated in legal scholarship.^[*See, e.g.*, Benny Chan, *Applying A Common Enterprise Theory of Liability to Clinical AI Systems*, 47 **Am. J.L. & Med.** 351 (2021); Mihailis E. Diamantis, *Algorithms Acting Badly: A Solution from Corporate Law*, 89 **Geo. Wash. L. Rev.** 801 (2021); Mihailis E. Diamantis, *The Extended Corporate Mind: When Corporations Use AI to Break the Law*, 98 **N.C. L. Rev.** 893 (2020); Yaniv Benhamou & Justine Ferland, *Artificial Intelligence & Damages: Assessing Liability and Calculating the Damages*, *in* **Leading Legal Disruption: Artificial Intelligence and a Toolkit for Lawyers and the Law** (forthcoming 2020), https://ssrn.com/abstract=3535387; Mark A. Lemley & Bryan Casey, *Remedies for Robots*, 86 **U. Chi. L. Rev.** 1311 (2019); Elizabeth Fuzaylova, *War Torts, Autonomous Weapon Systems, and Liability: Why A Limited Strict Liability Tort Regime Should Be Implemented*, 40 **Cardozo L. Rev.** 1327 (2019); Bryan H. Choi, *Crashworthy Code*, 94 **Wash. L. Rev.** 39 (2019); Xavier Frank, *Is Watson for Oncology Per Se Unreasonably Dangerous?: Making A Case for How to Prove Products Liability Based on A Flawed Artificial Intelligence Design*, 45 **Am. J.L. & Med.** 273 (2019); Matthew U. Scherer, *Of Wild Beasts and Digital Analogues: The Legal Status of Autonomous Systems*, 19 **Nev. L.J.** 259 (2018); David C. Vladeck, *Machines Without Principals: Liability Rules and Artificial Intelligence*, 89 **Wash. L. Rev.** 117, 121–124 (2014); Jessica S. Allain, *From* Jeopardy! *To Jaundice: The Medical Liability Implications of Dr. Watson and Other Artificial Intelligence Systems*, 73 **La. L. Rev.** 1049 (2013).] These debates have cast some doubt on whether/which harms from AI &quot;agents&quot; can properly give rise to liability to human principals.^[*See* Benhamou & Ferland, *supra*, at 13; Vladeck, *supra*, at 123 n.21.] Other possible theories of human liability—such as products liability—also face doctrinal challenges.^[Diamantis, *Algorithms Acting Badly*, *supra*, at 823–26 (arguing that products liability will largely be unavailable); Vladeck, *supra*, at 129–41; Scherer, *supra*, at 280–81.]\n\nThird, note that relying on vicarious liability alone leaves _A_ under fewer constraints than an analogous human would. Under most vicarious liability regimes, _A_ would still be directly liable for her actions, even if _P_ would also be vicariously liable. It seems unwise to legally constrain _A_ _less_ than we constrain humans in analogous circumstances.\n\nFinally (and most decisively in my opinion) developing a theory that assigns liability to _P_ based on _A_&#39;s actions (or actions + &quot;mental&quot; state) dramatically lowers the bar for creating an LFAI system in the first place. Once we have such a theory, _A_ (if intent-aligned) should indeed incorporate expected vicarious liability to _P_ into its decision procedure. However, if _A_ is already reasoning about whether its actions would violate law (as required to make vicarious liability an effective constraint on _A_&#39;s actions), **it seems strictly better to require *A* to directly incorporate that information into its** [**decision procedure**](https://forum.effectivealtruism.org/s/3pyRzRQmcJNvHzf6J/p/9RZodyypnWEtErFRM#A_Sketch_of_LFAI), rather than needing to go through the additional step of estimating the expected liability to _P_. This direct approach is simpler, and removes the possibility of evasion as a way around the legal constraint.^[ Specifically, in the direct case, what matters is whether *A* is actually violating the law, whereas in the vicarious case, what matters is the *expected liability* to *P*. *A* can reduce expected liability to P through evasion, but cannot reduce the probability of “actually” breaking the law through evasion.]\n\nFurther legal scholarship on vicarious liability for AI systems may still be valuable. If most morally significant autonomous activity in the world is indeed carried out by AI agents in the future, incentivizing their principals to constrain them seems important. But I think there are good reasons to suppose that this will either be ineffective for—or else dominated by—requiring AIs to be directly law-following."
    },
    "voteCount": 2,
    "forceInclude": true
  },
  {
    "_id": "rYDas2DDGGDRc8gGB",
    "url": null,
    "title": "Unifying Bargaining Notions (1/2)",
    "slug": "unifying-bargaining-notions-1-2",
    "author": "Diffractor",
    "question": false,
    "tags": [
      {
        "name": "Game Theory"
      },
      {
        "name": "Rationality"
      },
      {
        "name": "Decision Theory"
      },
      {
        "name": "Fairness"
      }
    ],
    "tableOfContents": {
      "sections": [
        {
          "title": "Bargaining Games",
          "anchor": "Bargaining_Games",
          "level": 1
        },
        {
          "title": "Cooperation/Competition Values",
          "anchor": "Cooperation_Competition_Values",
          "level": 1
        },
        {
          "title": "Shapley Value",
          "anchor": "Shapley_Value",
          "level": 1
        },
        {
          "title": "Uniting the Shapley and CoCo Values",
          "anchor": "Uniting_the_Shapley_and_CoCo_Values",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        },
        {
          "anchor": "comments",
          "level": 0,
          "title": "17 comments"
        }
      ],
      "headingsCount": 6
    },
    "contents": {
      "markdown": "This is a two-part sequence of posts, in the ancient LessWrong tradition of decision-theory-posting. This first part will introduce various concepts of bargaining solutions and dividing gains from trade, which the reader may or may not already be familiar with.\n\n[The upcoming part](https://www.lesswrong.com/posts/RZNmNwc9SxdKayeQh/unifying-bargaining-notions-2-2) will be about how all introduced concepts from this post are secretly just different facets of the same underlying notion, as originally discovered by [John Harsanyi](https://en.wikipedia.org/wiki/John_Harsanyi) back in 1963 and rediscovered by me from a completely different direction. The fact that the various different solution concepts in cooperative game theory are all merely special cases of a General Bargaining Solution for arbitrary games, is, as far as I can tell, not common knowledge on Less Wrong.\n\n**Bargaining Games**\n\nLet's say there's a couple with a set of available restaurant options. Neither of them wants to go without the other, and if they fail to come to an agreement, the fallback is eating a cold canned soup dinner at home, the worst of all the options. However, they have different restaurant preferences. What's the fair way to split the gains from trade?\n\nWell, it depends on their restaurant preferences, and preferences are typically encoded with utility functions. Since both sides agree that the disagreement outcome is the worst, they might as well index that as 0 utility, and their favorite respective restaurants as 1 utility, and denominate all the other options in terms of what probability mix between a cold canned dinner and their favorite restaurant would make them indifferent. If there's something that scores 0.9 utility for both, it's probably a pretty good pick!\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/4955850096e277246bb0c5aa754fdd8a494b381384b7c890.png)\n\nAlthough, there's something off about setting up the problem like this. There's no term for intensity of preferences! Someone who cared very little about food would have their preferences rank just as strongly as someone who had strong restaurant opinions!\n\nIn a sense, there's three responses to this objection.\n\nThe first response is that we might be zooming in too hard on the restaurant bargaining game in particular. In a broader context, a person having weak restaurant preferences may just be another way of saying that they are quick to trade off their choice of restaurant to someone else in return for other things they might desire. And so, in the broader bargaining game of a relationship where more is at stake than this one-time choice of restaurant, things may be fair. But in the restaurant bargaining game in particular, things can look unfair for the losing party, when in fact they traded off \"ability to determine restaurant\" in exchange for more concessions elsewhere. The generalization of this is that bargaining equilibria of an overall game might be quite different from just summing up the bargaining equilibria of the subgames.\n\nThe second response is that people care a nonzero amount about other people, and so someone with weak food preferences might be equally well modeled as someone with a strong preference that their partner get what they want. That can be folded into the utility function, however. Just make the ratings of the deferential person mostly copy the ratings of their partner.\n\nAnd the third response is one of the most interesting. For a perfectly selfish person who always tries for their favorite foods and doesn't care at all about your pouting at disfavored restaurants, there really isn't much of a difference between having strong preferences for food and weak preferences for food, they'll still drive as hard of a bargain against you as they can, if there isn't some mitigating factor.\n\nMuch like the post about how the [TRUE prisoner's dilemma](https://www.lesswrong.com/posts/HFyWNBnDNEDsDNLrZ/the-true-prisoner-s-dilemma) is not the standardly framed version, but more like \"a human civilization fighting with a paperclip maximizer for resources which can either save millions of lives, or make a few paperclips\", the TRUE bargaining problem isn't couples deciding where to eat, but something more like \"deciding how to split a pile of resources with nonsentient aliens that are willing to fight you over the resource pile\". \n\nAccordingly, using the term \"fair\" for any of these mathematical concepts has the problem of automatically importing human concepts of fairness, which needs to be resisted in order to look clearly at what the math is doing. It'll be handy to have a separate word for \"a mathematically distinguished point in a game where both parties have an interest in preventing destructive conflicts, that's neutral enough that aliens would probably come up with it\" to enforce that mental separation. Let's use \"chaa\" as a nonsense word to denote that concept (the Lawful Neutral Alien analogue of fairness), since it makes it a lot easier to point at situations where the chaa outcome splits apart from the fair outcome.\n\nThe relevant questions to ask to work out what the chaa outcome is are things like \"what are our best alternatives to a negotiated agreement and how does it compare to the choices on offer for us\" instead of \"how strong are our preferences compared to each other\", (which is more relevant to fairness)\n\nAnyways, returning to our restaurant game, to actually answer the question of what to do, let's see how we set up the problem.\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/5cd8ade5c5697dc87167b58a5adefcafc44df5140e500db6.png)\n\nWe plotted the utilities of the various options, and got a scattering of points on the plane, where one of the coordinates is the utility assigned to the outcome by Alice, and the other is the same for Bob.\n\nOne extremely important note is that it should be possible to randomize between various options. For instance, if there's only two options, one where player 1 wins completely, and one where player 2 wins completely, an obviously chaa outcome is the players flipping a coin to decide who wins.\n\nIn graphical terms, access to randomization lets us set up situations that can attain any utility pair in the convex hull of these points.\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/75ffe46763c6798d019e78e92237ceb1390430978f167b95.png)\n\nSo, which points in this shape are chaa outcomes?\n\nWell, \"chaa\" hasn't been defined yet, but if it's about how to split gains between agents in a neutral way without getting into destructive conflicts, there's three obvious properties that such a solution must have.\n\nFirst, since chaaness is partially about not getting into destructive conflicts, any chaa point should be on the Pareto frontier. Namely, there shouldn't be an alternative that leaves both players strictly better off. After all, if you have a prospective definition of \"chaa\" that demands that both players leave spare utility on the table, they should be able to take that as their new disagreement point, do another round of bargaining, and attain an outcome which is Not That and better for both. And then, this process would give you a new notion of chaaness that's just strictly better for all agents to use. So, whatever point is selected, it must be from the upper-right boundary of the shape.\n\nSecond, the definition of a chaa point shouldn't be sensitive to the exact utilities used. You can add any constant to a utility function, or multiply it by any positive constant, and it'll be the same utility function. Reporting your preferences as the function \\\\(U\\\\) should get you the same result as if you reported your preferences as the function \\\\(100U\\\\), or if you reported your preferences as the function \\\\(5U+7\\\\). No matter how the players relabel their utility functions and scale and shift them, it shouldn't affect anything because their underlying preferences didn't change.\n\nThis is convenient because it means that we can always rescale the disagreement point to \\\\(0\\\\) utility, and the highest utility a player can get (without making it so the other player would rather go for the disagreement point) to \\\\(1\\\\) utility. So, you only really have to consider problems where your convex shape fits in a unit square like this.\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/35dece3e545b7948a1777257688edaa82b9f01c681640303.png)\n\nThis leads nicely into our third desiderata. If the convex shape is symmetric, then the two players are in an identical position. Thus, any neutral way of selecting gains for the two players must be indifferent between which player is first and which is second, and so the chaa point should end up being on the line of symmetry, or on the halfway point. If one of the players is selected to win completely, the chaa outcome should involve flipping a coin to decide who wins. For the prisoner's dilemma, the chaa outcome should be mutual cooperation. For the game of chicken, the chaa outcome should be flipping a coin to decide who goes straight and who swerves.\n\nThese three desiderata are as obvious as can be, but past this they get a whole lot more controversial.\n\nThe Kalai-Smorodinsky Bargaining Solution is \"Rescale things so the disagreement outcome is at \\\\(0,0\\\\), and 1 utility for a player is the maximum utility they can get without sending the foe below 0 utility. Draw a diagonal line from \\\\(0,0\\\\) to \\\\(1,1\\\\), pick where the line crosses the Pareto frontier.\"\n\nPretty simple, right? It's the only way of picking a point that fits all three of our desiderata, and also fulfills the extra property of monotonicity, which is basically saying that, if you move the Pareto-frontier points for a player up, they should get more utility.\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/5fc57a24c3d38260e6bd18f6b65d667d2e49bb5baebbdfb8.png)\n\nYes, yes, I didn't quite do it correctly, that point of the blue shape in the bottom-right corner isn't scaled appropriately, but eh, it's close enough. It makes it pretty clear what we're doing with the line and how it is that moving the various points up (to go from the blue shape to the purple shape) increases the expected utility of the player whose utility is being plotted on the y coordinate. After all, if you've got better options, a chaa outcome shouldn't leave you with lower expected utility!\n\nWell... that's a bit of a fiddly issue. Remember, utility functions are scale-and-shift invariant. So, when we move these Pareto-frontier points up, we're not REALLY getting extra utility, this operation is really more like making the utility function more squashed at the top.\n\nHopefully, monotonicity doesn't look completely obvious now, though it still has an awful lot of intuitive force.\n\nThe Nash Bargaining Solution, by contrast, is \"pick the point on the frontier that maximizes the area of the rectangle made between that and the disagreement point\". It's nonobvious that this process doesn't depend on how we scale or shift the various utility functions, but it's true anyways. Maximizing the area of a rectangle isn't as obvious of a thing to do as \"draw a diagonal line\". It is pretty mathematically neutral, though.\n\nAlso, both the Kalai-Smorodinsky and Nash bargaining solutions happen to agree on which point to pick in the restaurant game, namely, a 2/3 chance of italian food, and a 1/3 chance of sushi. Although these solutions don't *usually* coincide.\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/397cf1a01fa93c57f0c835004aa2bbba82523ef034364232.png)\n\nThe Nash Bargaining Solution is the only one that fulfills the usual three desiderata, and the axiom of Independence of Irrelevant Alternatives. Ie, if the final bargaining solution involved you doing a 60-40 mix between option D and option E, then deleting *any* of the options that aren't D or E from the set of available options doesn't affect what happens. Untaken options are irrelevant.\n\nTo put it mildly, this is not really a desiderata at all, it's actually an extremely baffling property. Let's say Alice and Bob bargain and hit on the Nash bargaining solution.\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/a3b8c4e5182a9ca95bcc352fad3a170523c4a6ab875789cf.png)\n\n  \nThen this axiom is saying that it'd be possible to delete *all* of the options that disproportionately favor Alice, making a game that looks like this, and their bargaining process would still hit the same point.\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/028e702d27cac993c55089d2337265d63c9cb3cdd1e4d180.png)\n\nIntuitively, if options disproportionately favor you, you can use them as \"bargaining chips\", going \"alright, I'll take these unfair options off the table, but only if you remove your unfair options from the table\". Independence of Irrelevant Alternatives is basically saying that you can lose all your \"unfair bargaining chips\" and it'd have no effect on the net outcome!! Phrased like that, it's not clear why anyone would be interested in the Nash bargaining solution.\n\nThere are other, more obscure, bargaining solutions which have appeared in the literature, which won't be covered, though they all at least fulfill our basic three criteria.\n\nSo, for bargaining games, we can make some progress towards figuring out what a chaa outcome is (Pareto-efficient, scale-and-shift invariant, symmetric), but we don't have enough information yet to single out one particular bargaining solution as The One True Chaa Point, and in fact, it looks like there actually isn't a point like that; the various options all look pretty plausible.\n\nThe other issue is that not all games are bargaining games. Bargaining games require everyone to agree on what to do, and there are well-defined disagreement utilities for if negotiations break down. Clearly, this doesn't describe all, or even most, games. Now, it's time to look at another special case of games, for another notion of chaaness.\n\n**Cooperation/Competition Values**\n\nFor full credit, I was introduced to this notion by [this wonderful post](https://www.lesswrong.com/posts/Apy6B9ahRqFMRt9sx/what-should-superrational-players-do-in-asymmetric-games), which itself was exposition of [this wonderful paper](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/11/2012-coco-qje.pdf) by Kalai and Kalai.\n\nInstead of bargaining games, we'll now be looking at transferrable utility games. A transferrable utility game is one where there's a single resource (like dollars) where everyone's utility is linear in that resource, and everyone can pay everyone else in that resource and has enough of the resource to actually do so.\n\nPut another way, bargaining games are like bartering. Both sides must agree on what trade to make, and if either one doesn't like it, the transaction doesn't happen. Transferrable utility games are like arbitrary games that take place after money has been invented. There may no longer be a clear disagreement point for what happens when the various parties disagree, but it's also possible for everyone to settle matters by being clever about how they pay each other, which opens up a lot of options.\n\nIn particular, when there's a common resource like dollars, you can make everyone express their preferences in terms of dollars. This breaks the usual attribute of utility functions where you can scale and shift them as you please without affecting anything. You can't multiply one player's utilities (as denominated in dollars) by a factor of 100 without doing the same to everyone else. A collective scaling like that, where everyone's numbers go up by 100, is like a currency conversion, shifting from denominating everyone's utilities in dollars to denoting everyone's utilities in cents. It doesn't meaningfully change anything. Interestingly enough, we still do have individual shift-invariance. Put another way, you might be indifferent between option A and option B plus 300 dollars. Then that's consistent with scoring option A at 400 and option B at 100, or you can score option A at 700 and option B at 400. You can add or subtract whatever you want from options A and B, as long as the difference between the two options is 300.\n\nSo, in a totally general two-player game, with no well-defined disagreement point, but with the ability to pay each other money, and with everyone's utilities denominated in terms of money, is there some suitably chaa point?\n\nYes. Time to explain the CoCo value. CoCo stands for Cooperation/Competition, as there's two cases of games where the \"right answer\" is super-obvious. In pure-cooperation games where both players have the exact same utility function, you just pick the best option in the expectation the foe will do the same. In pure-competition games (ie, zero-sum games), you maximize your worst-case score, as your opponent has perfectly opposing interests to you and so will be minimizing your utility.\n\nAs it turns out, when both player's utility functions are commensurable (through this common currency), it's always possible to uniquely split *any* 2-player game at all into two other games. One is a pure-cooperation game, where both players have the same utility function, and perfectly aligned interests. The other is a pure-competition game, where both players have opposite utility functions, and perfectly opposed interests. The CoCo point is  \"cooperate as much as possible on the cooperative game where our interests align, and fight it out in the zero-sum game where our interests oppose, and add up our results from the two games to figure out how much value we both get\".\n\nAnd so, that's the CoCo point. You pick the most cooperative point in the cooperation game for what to actually do (to maximize the total amount of monetary gain for everyone), and use the results of the competition game to decide how much the two players pay each other, where the zero-sum aspect of the competition game ensures that the budget balances.\n\nBeing a bit more formal about this, we'll use \\\\(A\\\\) for the function mapping outcomes to player A's utilities, and \\\\(B\\\\) for the function mapping outcomes to player B's utilities.\n\nFor the cooperation game, both players A and B have the utility functions \\\\(\\\\frac{A+B}{2}\\\\). Clearly, this is a pure cooperation game.\n\nFor the competition game, player A has the utility function \\\\(\\\\frac{A-B}{2}\\\\) and player B has the utility function \\\\(\\\\frac{B-A}{2}\\\\). Clearly this a pure competition game, as the utilities for any outcome add up to 0.\n\nAnd note that for player A, adding up their utilities for the cooperation game and competition game yields \\\\(\\\\frac{A+B}{2}+\\\\frac{A-B}{2}=A\\\\), ie, their original utility function (and the same for player B)\n\nHere's a concrete example, lifted from the previous post on the topic. Bob and Alice can sell hotdogs at the beach or the airport. If they're at the same location, they end up competing over customers, halving both their profits. Alice is twice as efficient as Bob at selling hotdogs, and the beach has twice as many customers as the airport.\n\n<table><tbody><tr><td>Bob/Alice</td><td>Beach</td><td>Airport</td></tr><tr><td>Beach</td><td>50/100</td><td>100/100</td></tr><tr><td>Airport</td><td>50/200</td><td>25/50</td></tr></tbody></table>\n\nIt splits into a cooperation game and a competition game.\n\n<table><tbody><tr><td>Bob/Alice</td><td>Beach</td><td>Airport</td></tr><tr><td>Beach</td><td>75/75</td><td>100/100</td></tr><tr><td>Airport</td><td>125/125</td><td>37.5/37.5</td></tr></tbody></table>\n\n<table><tbody><tr><td>Bob/Alice</td><td>Beach</td><td>Airport</td></tr><tr><td>Beach</td><td>-25/25</td><td>0/0</td></tr><tr><td>Airport</td><td>-75/75</td><td>-12.5/12.5</td></tr></tbody></table>\n\nThe best move in the cooperation game is Bob going to the airport, and Alice going to the beach, so that's what's played in real-life. The utility from the cooperation game is added to the maximin utility from the competition game (where beach/beach is played), for 100 Bob utility and 150 Alice utility. And so, the solution is that Alice goes to the beach and pays Bob 50 bucks to go to the airport.\n\nThis has a whole lot of good properties, as detailed in the Adam Kalai and Ehud Kalai paper linked above. It's the unique solution that fulfills all of\n\n1: Pareto-optimality, it never leaves monetary value on the table.\n\n2: Shift invariance. If one player gets a gift of 100 dollars at the start of a game, they'll walk out of the game 100 dollars richer than they would if they hadn't received the gift. You can add any constant amount of money to anyone's payoffs and it does nothing.\n\n3: Payoff dominance. If player A gets more money than player B in all cells, then player A will leave the game with more money than player B.\n\n4: Invariance to redundant strategies. Adding a new action that could just as well be accomplished by a probabilistic mix between other actions does nothing.\n\n5: Action monotonicity. Adding a new action is always good for you: you never regret having a larger action space (though other players may regret you having a larger action space).\n\n6: Information monotonicity. This is for the imperfect-information generalization of the CoCo value, that's detailed in the Kalai paper. Giving a player more information about what everyone else is doing can't hurt them: you'll never regret knowing more.\n\nAnd the CoCo value is the unique solution that fulfills all six of those properties above. There doesn't seem to be any comparably good notion of equilibrium available besides this, and so we can say that any sensible definition of \"chaa\" for arbitrary games (if one exists) should manage to recover the CoCo value as a special case when presented with games with transferrable utility.\n\nAn interesting note. For bargaining games with transferrable utility (like, a bargaining game where you can pay each other), the equilibrium notion you get is \"denominating both player's utility functions in dollars, pick the option that maximizes the overall monetary surplus over the disagreement point, and pay each other so both players equally split the monetary surplus\"\n\nLike, if the surplus-maximizing option is one that player 1 values at +80 dollars over the disagreement point, and player 2 values at +40 over the disagreement point, for +120 dollars of surplus value, the CoCo solution is that particular option is picked, and player 1 gives player 2 20 dollars, so both sides walk away with +60 dollars worth of utility.\n\nIf Pedro the street vendor and Pierre the rich tourist are haggling over the price of a burrito, and Pedro would walk away at 2$, and Pierre would walk away at 14$, then the CoCo solution is that the burrito is sold for 8$, because that's halfway between where the two people would rather walk.\n\nWhen arguing over which movie to pick for a group movie night, everyone just needs to report how much they'd value seeing the various movies, pick the movie that maximizes total monetary surplus, and pay each other to equalize that surplus (so you get money if you have to sit through a movie you enjoy less than everyone else in your group, and if you're watching a personal favorite movie that everyone else is \"meh\" about, like Kong vs Godzilla 5, you've gotta pay the others to watch it.)\n\nActually, first maximizing surplus value, and then equally splitting the monetary gain, seems quite fair. Yes, we just used the F word.\n\n**Shapley Value**\n\nLet's say a bunch of people contribute various amounts of effort to a project, for various amounts of gain, creating an overall pile of money. What's a chaa way to fairly divide their pile of money?\n\nWe can impose some desiderata.\n\n1: All the money should be going to someone. If the chaa division involved burning money, you should come up with an alternate notion of \"chaa\" which everyone agrees is better and which is Not That.\n\n2: A player which contributes absolutely nothing to the project and just sits around, regardless of circumstances, should get 0 dollars.\n\n3: If two players in the game are equivalent in all ways and totally interchangeable, then they should receive equal payoffs.\n\n4: If the total pile of money is \\\\(a\\\\) times as big, everyone should get \\\\(a\\\\) times as much.\n\n5: If two projects are completed in a row and the chaa division occurs, adding together someone's chaa share from project A and project B (considered individually) should be their chaa share from \"do both projects in a row\". Or, payoffs shouldn't depend on precisely how you slice up the projects.\n\nAs it turns out, this *uniquely* pins down how to divide the pile of resources! If \\\\(N\\\\) is the set of all players, and \\\\(i\\\\) is a particular player, and \\\\(v(S)\\\\) (for \\\\(S\\\\subseteq N\\\\)) is the total amount of resources that could be produced by all the players in \\\\(S\\\\) working together, then the payoff for player \\\\(i\\\\) is\n\n\\\\(\\\\displaystyle{\\\\sum_{S\\\\subseteq N/\\\\{i\\\\}}\\\\frac{(n-|S|-1)!|S|!}{n!}(v(S\\\\cup\\\\{i\\\\})-v(S))}\\\\)\n\nPut another way, this is effectively going \"if the players were added to the group in a random order, and everyone demanded all the marginal extra value they produced upon being added to the group, you'd get payoffs for everyone. Average the payoffs over all possible random orderings\". That factorial term at the start is going \"what are the odds that group S gets assembled (in any order), and then I get added to it?\". And then the second term is \"demanding my marginal contribution\".\n\nHere's [a previous post](https://www.lesswrong.com/posts/3XqoK4C7r4tyGZ893/worked-examples-of-shapley-values) about actually working out the Shapley values in several toy examples of games, to get some intuition for what they're doing.\n\n**Uniting the Shapley and CoCo Values**\n\nBefore we get to the next post tying everything together, we'll see that the Shapley and CoCo values actually have a highly unexpected connection. If you try generalizing the CoCo value to n players, you get something that looks suspiciously Shapley-like.\n\nLet's begin by reshuffling the Shapley values into a different form. The Shapley value for player i starts off as\n\n\\\\(\\\\displaystyle{\\\\sum_{S\\\\subseteq N/\\\\{i\\\\}}\\\\frac{(n-|S|-1)!|S|!}{n!}(v(S\\\\cup\\\\{i\\\\})-v(S))}\\\\)\n\nNow, we can pair off the various coalitions with each other. The subset \\\\(S\\\\) will be paired off with the subset \\\\(N/(S\\\\cup\\\\{i\\\\})\\\\), the set of all the players that aren't in \\\\(S\\\\) and aren't \\\\(i\\\\). In particular, note that in both cases, the coefficient in front ends up being \\\\(\\\\frac{(n-|S|-1)!|S|!}{n!}\\\\). It's then possible to swap the the values around between those two paired coalitions, producing a restatement of the Shapley value as\n\n\\\\(=\\\\displaystyle{\\\\sum_{S\\\\subseteq N/\\\\{i\\\\}}\\\\frac{(n-|S|-1)!|S|!}{n!}(v(N/S)-v(S))}\\\\)\n\nAnd then, instead of writing this as a sum over subsets that lack player \\\\(i\\\\), we can switch to the complement and write this as a sum over subsets which include player \\\\(i\\\\), although the factorial term has to be adjusted a bit to compensate for the fact that the complement of \\\\(S\\\\) has a cardinality of \\\\(n-|S|\\\\) instead of \\\\(|S|\\\\)\n\n\\\\(=\\\\displaystyle{\\\\sum_{i\\\\in S\\\\subseteq N}\\\\frac{(n-|S|)!(|S|-1)!}{n!}(v(S)-v(N/S))}\\\\)\n\nThis restatement of the Shapley value will be useful later.\n\nAnd now we'll try generalizing the CoCo value to n-player games with transferrable utility. Let's deal with a 3-player game, just to make things a bit simpler. The players are \\\\(A,B,C\\\\). As it turns out, this game will actually split into four games instead of two. There's the pure cooperation game, a zero-sum \\\\(A\\\\) vs everyone else game, a zero-sum \\\\(B\\\\) vs everyone else game, and a zero-sum \\\\(C\\\\) vs everyone else game.\n\nFor the first game, the utility functions for \\\\(A,B\\\\), and \\\\(C\\\\) are \\\\(\\\\frac{A+B+C}{3}\\\\).\n\nFor the zero-sum \\\\(A\\\\) vs everyone else game, the utility function for \\\\(A\\\\) is \\\\(\\\\frac{A-(B+C)}{3}\\\\), and the utility functions for \\\\(B,C\\\\) are \\\\(\\\\frac{(B+C)-A}{6}\\\\) for both. You might be wondering \"why 6?\". And the answer is it's that way in order for the game to be zero-sum; the opposing players are weighted less to compensate for there being more of them. Also note that B and C have perfectly aligned incentives in this game, so they might as well perfectly coordinate.\n\nFor the zero-sum \\\\(B\\\\) vs everyone else game, the utility function for \\\\(A,C\\\\) is \\\\(\\\\frac{(A+C)-B}{6}\\\\) for both, and for \\\\(B\\\\) it's \\\\(\\\\frac{B-(A+C)}{3}\\\\).\n\nAnd similar for \\\\(C\\\\).\n\nFor the player \\\\(A\\\\), adding up the payoff for all the games gives you\n\n\\\\(\\\\frac{A+B+C}{3}+\\\\frac{A-(B+C)}{3}+\\\\frac{(A+C)-B}{6}+\\\\frac{(A+B)-C}{6}=A\\\\)\n\n(and similar for all the other players)\n\nAnd for each game in particular except for the pure cooperation game, it's zero-sum.\n\nNow that we've seen that concrete example, let's generalize to \\\\(n\\\\) players. There are \\\\(2^{n-1}\\\\) subgames that the original game will split into, one game for each way to divide the players into two coalitions. Let \\\\(S\\\\) be the set of players in one of these coalitions.\n\nFor the game with \\\\(S\\\\) vs \\\\(N/S\\\\), the utility functions of everyone on coalition \\\\(S\\\\) will be\n\n\\\\(\\\\displaystyle{\\\\frac{(|S|-1)!(n-|S|)!}{n!}*\\\\left(\\\\sum_{i\\\\in S}U\\_i-\\\\sum\\_{j\\\\not\\\\in S}U_j\\\\right)}\\\\)\n\nAnd the utility functions of everyone in the coalition \\\\(N/S\\\\), will be\n\n\\\\(\\\\displaystyle{\\\\frac{(n-|S|-1)!|S|!}{n!}\\\\left(\\\\sum_{j\\\\not\\\\in S}U\\_j-\\\\sum\\_{i\\\\in S}U_i\\\\right)}\\\\)\n\nIt's not too hard to show that all these games are zero-sum (except for the one with the coalition of all players), with perfectly aligned incentives within a coalition.\n\nAnyways, the value that player \\\\(i\\\\) gets is the sum of the values it gets from all of the component games where coalitions compete against each other. Or, the payoff for player \\\\(i\\\\) will be\n\n\\\\(\\\\displaystyle{\\\\sum_{i\\\\in S\\\\subseteq N}\\\\frac{(n-|S|)!(|S|-1)!}{n!}\\\\max_{\\\\vec{a}\\\\in\\\\Delta\\\\prod_{j\\\\in S}A\\_j}\\\\min\\_{\\\\vec{b}\\\\in\\\\Delta\\\\prod_{k\\\\not\\\\in S}A\\_k}\\\\left(\\\\sum\\_{j\\\\in S}U\\_j(\\\\vec{a},\\\\vec{b})-\\\\sum\\_{k\\\\not\\\\in S}U_k(\\\\vec{a},\\\\vec{b})\\\\right)}\\\\)\n\n  \nBasically, do a weighted sum over \"utility of my coalition minus utility of their coalition if the coalitions zero-sum fought\" over all the coalitions that you're a part of, and that's your CoCo value in the n-player case.\n\nBut remember, the Shapley value can be re-expressed as\n\n\\\\(\\\\displaystyle{\\\\sum_{i\\\\in S\\\\subseteq N}\\\\frac{(n-|S|)!(|S|-1)!}{n!}(v(S)-v(N/S))}\\\\)\n\nWhich should look suspiciously similar, especially when you remember that \\\\(v(S)\\\\) is the value that everyone on your coalition can produce by working together, and \\\\(v(N/S)\\\\) is the value of the opposing coalition. Really, the CoCo values are just Shapley values but generalized to *any* sort of game where there's transferrable utility. The analogue of \"add players in a random order, you get your marginal contribution\" turns out to be \"add players to a team in a random order, if you're added to team \\\\(S\\\\), your increase in value from that is the marginal increase in the value of the team if it got into a zero-sum competition against the entire rest of the world.\"\n\nOk, so the CoCo values are basically modified Shapley values, so these two are related to each other. Can we generalize even further?\n\nWell, as it turns out, we'll be able to connect the CoCo value to the Nash bargaining solution to get solutions for games in general. I came at this problem from the direction of generalizing the CoCo value to games with nontransferable utility, since the CoCo values were so nicely behaved that any solution for games in general should replicate the CoCo values when utility happens to be transferrable, and it turned out my solution automatically spat out the Nash bargaining solution as a special case, which was a considerable surprise to me.\n\nAnd then it turned out that Harsanyi came up with the same sort of solution from a *completely* different direction (but more elaborate and incorporating constraints that I missed) all the way back in 1963 by trying to generalize the Nash bargaining solution to games with no clear disagreement point. [Next post,](https://www.lesswrong.com/posts/RZNmNwc9SxdKayeQh/unifying-bargaining-notions-2-2) we'll cover this unifying concept."
    },
    "voteCount": 50,
    "forceInclude": true
  },
  {
    "_id": "RZNmNwc9SxdKayeQh",
    "url": null,
    "title": "Unifying Bargaining Notions (2/2)",
    "slug": "unifying-bargaining-notions-2-2",
    "author": "Diffractor",
    "question": false,
    "tags": [
      {
        "name": "Game Theory"
      },
      {
        "name": "Rationality"
      },
      {
        "name": "Decision Theory"
      }
    ],
    "tableOfContents": {
      "sections": [
        {
          "title": "Imaginary Prices, Tradeoffs, and Utilitarianism",
          "anchor": "Imaginary_Prices__Tradeoffs__and_Utilitarianism",
          "level": 1
        },
        {
          "title": "CoCo Equilibria",
          "anchor": "CoCo_Equilibria",
          "level": 1
        },
        {
          "title": "CoCo Equilibrium Questions",
          "anchor": "CoCo_Equilibrium_Questions",
          "level": 1
        },
        {
          "title": "Scale-Shift Invariance",
          "anchor": "Scale_Shift_Invariance",
          "level": 1
        },
        {
          "title": "Bargaining Games as Special Case",
          "anchor": "Bargaining_Games_as_Special_Case",
          "level": 1
        },
        {
          "title": "N-Player Generalizations and Coalitions",
          "anchor": "N_Player_Generalizations_and_Coalitions",
          "level": 1
        },
        {
          "title": "Harsanyi Equilibria and Generalizing the Nash Bargaining Solution",
          "anchor": "Harsanyi_Equilibria_and_Generalizing_the_Nash_Bargaining_Solution",
          "level": 1
        },
        {
          "title": "But What if it Sucks, Tho (it Does)",
          "anchor": "But_What_if_it_Sucks__Tho__it_Does_",
          "level": 1
        },
        {
          "title": "Appendix: Proof of Theorem 1 (you can skip this one)",
          "anchor": "Appendix__Proof_of_Theorem_1__you_can_skip_this_one_",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        },
        {
          "anchor": "comments",
          "level": 0,
          "title": "7 comments"
        }
      ],
      "headingsCount": 11
    },
    "contents": {
      "markdown": " Alright, time for the payoff, unifying everything discussed in the [previous post](https://www.lesswrong.com/posts/rYDas2DDGGDRc8gGB/unifying-bargaining-notions-1-2). This post is a lot more mathematically dense, you might want to digest it in more than one sitting.\n\n**Imaginary Prices, Tradeoffs, and Utilitarianism**\n\nHarsanyi's Utilitarianism Theorem can be summarized as \"if a bunch of agents have their own personal utility functions \\\\(U_i\\\\), and you want to aggregate them into a collective utility function \\\\(U\\\\) with the property that everyone agreeing that option x is better than option y (ie, \\\\(U\\_i(x)\\\\ge U\\_i(y)\\\\) for all i) implies \\\\(U(x)\\\\ge U(y)\\\\), then that collective utility function *must* be of the form \\\\(b+\\\\sum_{i\\\\in I}a\\_i U\\_i\\\\) for some number \\\\(b\\\\) and nonnegative numbers \\\\(a_i\\\\).\"\n\nBasically, if you want to aggregate utility functions, the only sane way to do so is to give everyone importance weights, and do a weighted sum of everyone's individual utility functions.\n\nClosely related to this is a result that says that any point on the Pareto Frontier of a game can be *post-hoc* interpreted as the result of maximizing a collective utility function. This related result is one where it's very important for the reader to understand the actual proof, because the proof gives you a way of reverse-engineering \"how much everyone matters to the social utility function\" from the outcome alone.\n\nFirst up, draw all the outcomes, and the utilities that both players assign to them, and the convex hull will be the \"feasible set\" \\\\(F\\\\), since we have access to randomization. Pick some Pareto frontier point \\\\(u\\_1,u\\_2...u_n\\\\) (although the drawn image is for only two players)\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/17a2eeefb5fe174d95f369d901229b5a46fa21f31a20a1a2.png)\n\nUse the [Hahn-Banach separation theorem](https://en.wikipedia.org/wiki/Hahn%E2%80%93Banach_theorem) to create a linear function \\\\(\\\\phi:\\\\mathbb{R}^n\\\\to\\\\mathbb{R}\\\\) such that \\\\(\\\\phi(u\\_1,u\\_2...u_n)\\\\ge \\\\phi(F)\\\\). (such that is abbreviated s.t. from here on out) Or put another way, \\\\(u\\_1,u\\_2...u_n\\\\) is one of the points in the feasible set \\\\(F\\\\) that maximizes the linear function \\\\(\\\\phi\\\\) you created. In the image, the lines are the level sets of the linear function, the set of all points where \\\\(\\\\phi(x\\_1,x\\_2...x_n)=c\\\\).\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/98681bae10d8e101455ad6da41fa82731f7ebc5a59c27371.png)\n\nThat linear function \\\\(\\\\phi:\\\\mathbb{R}^n\\\\to\\\\mathbb{R}\\\\) can be written as \\\\((x\\_1,x\\_2...x\\_n)\\\\mapsto a\\_1 x\\_1+a\\_2 x\\_2+...+a\\_n x_n\\\\). Bam, those coefficients are the utility weights you need. \\\\(u\\_1,u\\_2...u_n\\\\) is a point that maximizes the function \\\\(\\\\phi\\\\), and the function \\\\(\\\\phi\\\\) is implementing \"take this particular weighted sum of the utilities of the players\", so we have rationalized our particular Pareto-optimal point as being produced by maximizing some weighted sum of how important everyone's utilities are.\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/094f66bf32427f1e7112b888cf6b37ab759385243970e52c.png)\n\nAnd that's how to take any point on the Pareto frontier and reverse-engineer the weighted sum of everyone's utilities it's maximizing (though if there are corner points, there can be multiple possible weights that'd work, because the tangent plane is no longer unique).\n\nBut, there's another completely different way of viewing this process! If we take our Pareto-frontier point \\\\(u\\_1,u\\_2...u_n\\\\) and zoom way way in...\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/c34a2308c8126861a5c9f122476b176c231f7196f0ab61bc.png)\n\nThere's a locally linear tradeoff between the utilities of the various players. An \\\\(\\\\epsilon\\\\) increase in the utility of Alice corresponds to a \\\\(3\\\\epsilon\\\\) decrease in the utility of Bob. One thing that we can do with this local linearity is invent an imaginary currency! It goes like this. One curnit (currency unit) can be redeemed for a tiny adjustment back and forth along this part of the Pareto frontier, and agents can trade curnits back and forth as needed to adjust exactly where they are on the Pareto frontier. And in particular, the fact that there's a 3 to 1 ratio between how the utility of Alice trades off against the utility of Bob corresponds to Alice needing to spend 3 curnits to get an AliceUtilion, while Bob only needs to spend 1 curnit to get a BobUtilon. \n\nThere's a few ways of thinking about this. The first way of thinking about it is that, in this little piece of the Pareto frontier, Alice is 3x harder to satisfy. Another way of thinking about it is that it's like Bob is poor and his marginal utility for a dollar (or a curnit) is a good deal higher than it is for Alice. And a third way of thinking about this is that if we find out this is the best collective point, we can go \"huh, the only way that 3 curnits/AliceUtilon and 1 curnit/BobUtilon makes sense is if an AliceUtilon is worth 3x as much as a BobUtilon\". Which, oh hey, is the *exact same conclusion* as we would have gotten from trying to figure out the weights for Alice vs Bob in the social utility function that says that this is the best point to be at.\n\nSo, combining these views, we can take any point \\\\(u\\_1,u\\_2...u_n\\\\) on the Pareto frontier, and get a vector \\\\(a\\_1,a\\_2...a_n\\\\) which can be interpreted *either* as \"these are the importance weights for the players\", *or* as \"these are the curnits/utilon conversion factors for the various players\".\n\n**CoCo Equilibria**\n\nSo, the  ( [](https://www.lesswrong.com/posts/rYDas2DDGGDRc8gGB/unifying-bargaining-notions-1-2) works really great for games with transferrable utility, some sort of currency that can be passed around amongst the players. But there are a lot of games without transferrable utility!\n\nBut remember our earlier discussion on how, in the local neighborhood of a Pareto-frontier point, we can invent an imaginary currency that reflects how hard it is to improve the utilities of the various players. This works fine in the vicinity of the point, but breaks down as you stray far away.\n\nSo, let's take our given example with Alice and Bob. If we introduce \"curnits\" as a currency in the local vicinity of the point they're at, then we can convert from utility functions \\\\(U\\_1,U\\_2\\\\) (denoted in AliceUtilons and BobUtilons), to \\\\(a\\_1 U\\_1, a\\_2 U\\_2\\\\) (both players' utilities are denoted in curnits now, and are commensurable), use the CoCo values to tell us what payoffs the players \"should\" get, and divide the result by \\\\(a_1\\\\) and \\\\(a_2\\\\) respectively to convert it back into AliceUtilons and BobUtilons. When we end up doing this with our example, we'll get a result that has the following reasoning behind it.\n\n\"Since curnits are much more valuable to Bob than they are to Alice, the CoCo games will advise that Alice give Bob some money to get Bob to go along with her plans, since the money is 3x less useful to Alice than it is to Bob. Converting the CoCo payoffs back to utilons, the net result would be \"Alice gets a bit less utility than she did at the old Alice-favoring point, Bob gets a giant pile of utility from all those curnits\", and it'd actually be an impossible pair of utility values, there's just no way for Bob to get that much utility.\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/b92f69d773d806b0720b687f9e9c181024834f2c2afcc89e.png)\n\nUsing the local currency at the red point for a CoCo transferrable-utility game means that Bob gets a small pile of currency in the CoCo game, which translates back into a big pile of BobUtilons, and we end up at the purple point, which is impossible to attain.\n\nGeneralizing past this particular example, in general, for Pareto-frontier points where some players lose out really hard (and so implicitly have low utility weights), then when you convert it to a game with transferrable utility, pick the CoCo value, and convert back, the players with really low utility weights will end up with giant piles of utility. This is because \"low utility weights \\\\(a_i\\\\)\" correspond to \"The curnits/utilon value \\\\(a_i\\\\) is low, so it takes few curnits to help this player a lot\", so they get a small pile of money which converts to a big pile of utility.\n\nAnd so, the question to ask now is something like \"is there a point on the Pareto frontier \\\\(u\\_1,u\\_2...u_n\\\\) where we can get the curnits/utilon conversion numbers from that point, convert everyone's utility to curnits, work out the CoCo value of the resulting game, convert back to utilons, and end up a*t the exact same point we started at?*\"\n\nBasically, a CoCo equilibrium would be a spot where, if the players squabbling over what direction to move on the Pareto frontier went \"let's introduce a virtual currency redeemable for tiny perturbations on the Pareto frontier\", and worked out the CoCo value for the game they're in (which is a very chaa solution when money is available), it'd return the answer \"stay where you currently are, it's a good spot, nobody needs to pay anyone else anything\". Which is a very fortunate result to get since this currency doesn't actually exist so nobody could pay each other anything anyways.\n\n**CoCo Equilibrium Questions**\n\nThere are several questions we could ask about CoCo equilibria.\n\n1: Is it scale-and-shift invariant, or does it depend on how everyone represents their utility functions?\n\n2: If we try using it on bargaining games in particular, will it reproduce any well-known bargaining notions?\n\n3: How do we generalize it to the n-person case? Is it straightforward, or are there hidden difficulties?\n\n4: If we really just found a universal notion of how to split gains in games, how come nobody else came up with it first?\n\n5: Do CoCo equilibria even exist, anyways? If so, are they unique?\n\nTime to reveal the answers, which won't be proved yet because they'll follow as a consequence of one big theorem later on.\n\n**Scale-Shift Invariance**\n\nFor the first question, yes, it is scale-and-shift invariant. It doesn't matter *how* you represent everyone's utility functions, you'll get the same answer. Intuitively, here's what happens. Let's say Alice multiplies all her utility numbers by 100. Now, at the point of interest, this means that we just went from 3 curnits/AliceUtilon to 3 curnits/100 AliceUtilons. And so, the coefficient we multiply Alice's utility function by, \\\\(a_1\\\\) (the curnits/AliceUtilons number), went from 3 to \\\\(\\\\frac{3}{100}\\\\), which will perfectly cancel out the fact that Alice multiplied all her numbers by 100. So, the CoCo value (as denominated in curnits) doesn't change one bit. Then we divide by \\\\(a_1\\\\) to convert back to Alice's utility, which means that we multiply by \\\\(\\\\frac{100}{3}\\\\), and get a big number for AliceUtilons, as expected (since Alice multiplied all her numbers by 100)\n\nAs for shifting, if Alice adds 10 utility to all her numbers, it doesn't alter the coefficient \\\\(a_1\\\\) (3 curnits/AliceUtilon), so all of Alice's utility payoffs as denominated in curnits, are 10 higher than usual. But, CoCo value is shift-invariant. If Alice gets a guaranteed 10 extra curnits no matter what she does, her CoCo value will be 10 curnits higher than it'd be usually, and Bob's won't change at all. And so, when we divide by \\\\(a_1\\\\) to convert back to Alice's utility, we get an extra 10 utility, as expected (since Alice added 10 to all her numbers)\n\nOk, we've got scale-and-shift invariance, which is a super-important property to have for something to maybe be \"chaa\" (a mathematically distinguished point in a negotiation game against a foe where you both have an interest in preventing destructive conflicts, that's neutral enough that aliens would probably come up with it).\n\n**Bargaining Games as Special Case**\n\nIf we apply CoCo equilibrium concepts to bargaining games in particular (player 1 proposes an option or plays \"reject\", player 2 can accept or reject, anyone rejecting means that both sides get their disagreement payoffs), what do we get?\n\nWell, though it won't be proved now (it'll be indirectly proved later on), CoCo equilibria in bargaining games will turn out to be equivalent to the Nash bargaining solution! The Nash solution can be derived as a special case of this generalization of the CoCo solution to when utility isn't transferrable!\n\n**N-Player Generalizations and Coalitions**\n\nFor generalizing to the n-person case, there's the obvious generalization where, given a Pareto frontier point, we can get the imaginary prices \\\\(a\\_1,a\\_2...a_n\\\\), and the CoCo value makes sense for n-player games. But this doesn't fully take coalitions into account. It's possible that a coalition could conspire amongst themselves to guarantee a good payout. And we could add extra conditions regarding coalitions, like that *within* a coalition, they use some sort of CoCo-like or Shapley-like split of resources.\n\nTo formalize the stronger version of that equilibrium, let \\\\(N\\\\) be the set of players, and \\\\(U_i\\\\) be the utility function of player \\\\(i\\\\), and \\\\(A_i\\\\) be player \\\\(i\\\\)'s set of actions.\n\n*Formal Definition:*  ***Coalition-Perfect CoCo Equilibrium***\n\n*A coalition-perfect CoCo equilibrium is a tuple *\\\\(\\\\{\\\\rho^S\\\\}_{S\\\\subseteq N}\\\\)* (the joint strategies that all possible coalitions would play if they were against the opposing coalition, *\\\\(\\\\rho^S\\\\in\\\\Delta\\\\prod_{i\\\\in S}A_i\\\\)*), s.t, defining *\\\\(u^S\\_i := U\\_i(\\\\rho^S,\\\\rho^{N/S})\\\\)\n\n*1: *\\\\(\\\\{u^N\\_i\\\\}\\_{i\\\\in N}\\\\)* is on the Pareto frontier.*\n\n*2: There is an *\\\\(\\\\{a\\_i\\\\}\\_{i\\\\in N}\\\\)* tuple (virtual prices) that makes both of the following conditions true.*\n\n*3: *\\\\(\\\\displaystyle{\\\\forall S\\\\subseteq N:\\\\rho^S\\\\in\\\\text{argmax}_{\\\\rho\\\\in\\\\Delta\\\\prod_{i\\\\in S}A\\_i}\\\\left(\\\\sum\\_{i\\\\in S} a\\_i U\\_i(\\\\rho,\\\\rho^{N/S})-\\\\sum_{j\\\\in N/S} a\\_j U\\_j(\\\\rho,\\\\rho^{N/S})\\\\right)}\\\\)  \n* (ie, all the joint strategies are trying to maximize the money earned if up against the opposing coalition in a zero-sum game, and as a special case, when S=N, it says that what the entire group actually ends up doing maximizes surplus value, which is another way of stating that the *\\\\(\\\\{a\\_i\\\\}\\_{i\\\\in N}\\\\)* are the appropriate virtual currencies to use at the *\\\\(\\\\{u^N\\_i\\\\}\\_{i\\\\in N}\\\\)* point)*\n\n*4: *\\\\(\\\\displaystyle{\\\\forall i\\\\in N, i\\\\in S\\\\subseteq N: a\\_i u^S\\_i = \\\\sum_{i\\\\in R\\\\subseteq S}\\\\frac{(|R|-1)!(|S|-|R|)!}{|S|!}\\\\left(\\\\sum_{j\\\\in R}a\\_j u^R\\_j-\\\\sum_{k\\\\in S/R}a\\_k u^{S/R}\\_k\\\\right)}\\\\)  \n*Or, rephrasing this in terms of the more standard framing of Shapley Value...*  \n\\\\(\\\\displaystyle{\\\\forall i\\\\in N, i\\\\in S\\\\subseteq N:a\\_i u^S\\_i=\\\\sum_{R\\\\subseteq S/\\\\{i\\\\}}\\\\frac{(|S|-|R|-1)!|R|!}{|S|!}\\\\left(\\\\sum_{j\\\\in R\\\\cup\\\\{i\\\\}}a\\_j u^{R\\\\cup\\\\{i\\\\}}\\_j-\\\\sum_{j\\\\in R}a\\_j u^R\\_j\\\\right)}\\\\)\n\nSo, that's a coalition-perfect CoCo equilibrium. You could interpret it as there being a \"virtual currency\" phrased in terms of how hard it is, relatively, to improve everyone's utility, and everyone getting their CoCo value, and even in the event of zero-sum conflicts between teams, everyone on a team will get their CoCo value. Or you could interpret it as everyone getting their Shapley payoffs, where the analogue of the marginal gain from a particular player is \"their value when they join team \\\\(S\\\\), plus the improvements in everyone else's value from them no longer opposing team \\\\(S\\\\)\". Or you could interpret the game, and all the zero-sum subgames, as just maximizing a weighted sum of utilities (like Harsanyi's utilitarianism theorem), and the cool part is the \"weights\" for how important everyone is will be the same for the full game as well as all the zero-sum subgames.\n\n**Harsanyi Equilibria and Generalizing the Nash Bargaining Solution**\n\nIf this exists and it's nice, why has nobody found it before?\n\nActually, someone did find this before! That's a major occupational hazard of finding math that aliens would independently reinvent: there's a high chance that someone beat you to the punch. Specifically, John Harsanyi, back in 1963, found this first. He wound up with this same exact solution, though it's quite nontrivial to show the equivalence between our equilibrium notions.\n\nCoCo equilibria were motivated via the nice properties of the CoCo value and generalizing it to the non-transferrable utility case, which turned out to be secretly equivalent to generalizing Shapley values. Harsanyi, as detailed in his lovely paper [\"A Simplified Bargaining Model for the n-Person Cooperative Game\"](https://link.springer.com/chapter/10.1007/978-94-017-2527-9_3) (which I very highly recommend you read via your favorite paper-reading website!), found it from trying to generalize the Nash Bargaining Solution to games without well-defined disagreement points.\n\nHarsanyi's basic insight was that, in a general two-player game, if it's known in advance that the Nash bargaining solution will be used, and the players are picking their \"disagreement strategies\" (what they'll fall back on if they can't cooperate on some joint distribution over actions) then Alice would try to pick a \"disagreement strategy\" that makes it so that, no matter what Bob's disagreement strategy is, the Nash bargaining solution would favor Alice as much as possible, and Bob is in a similar position. So, the two players will end up in a game where they're trying to disagree in a way that'll rig the Nash bargaining solution in their favor. I'm not sure whether or not this is zero-sum, but it is true that if one player wins, the other must lose, so it's zero-sum *enough* that there's a unique pair of disagreement utilities that you get from maximin strategies, that are mutually optimal against each other, and then you can just use the Nash bargaining solution from there.\n\nIn particular, if you're trying to make a threat that's suitable for rigging a bargaining game in your favor, what you need are not threats that hurt both of you equally, or threats that are worse for you than for the foe, or threats that the foe could possibly defuse. What you need is something that matters far more to the foe than you, which the foe can't evade by any action they can take. Or, rephrasing in terms of the CoCo value, to successfully rig the Nash bargaining solution in your favor, you'll need a good move in the zero-sum competition game of the cooperation/competition decomposition.\n\nGeneralized to the n-player case, between any two coalitions, they'll be doing the same sort of squabbling over what counts as the disagreement point, everyone within the coalition will agree on what disagreement point to go for in event of conflicts and within any coalition, they'll also be splitting things according to the Nash bargaining bolution as well. I don't fully understand the reasoning behind how that informal sort of description cashes out in the math (in particular, I still don't understand why the disagreement points are defined as they are in the paper), but I'll attempt a summary of Harsanyi's paper anyways. You're *highly* encouraged to read the paper yourself; it's got lots of goodies in it that I don't mention.\n\nHarsanyi starts off by assuming that every coalition can guarantee some marginal gain to everyone making it up, which is denoted by \\\\(w^S_i\\\\), the marginal utility payoff to player \\\\(i\\\\) received from the coalition \\\\(S\\\\).\n\nFurther, the payoff that a player gets in the event of a zero-sum conflict between \\\\(S\\\\) and everyone else should just be the sum of the marginal payoffs from all the subsets of \\\\(S\\\\) that \\\\(i\\\\) is in, ie. \\\\(u^S\\_i=\\\\sum\\_{i\\\\in R\\\\subseteq S}w^R_i\\\\) (where \\\\(u^S_i\\\\) is as previously defined, the utility that i gets in the event of a zero-sum conflict between \\\\(S\\\\) and everyone else). The argument for this is that if the sum of marginal payoffs was more than \\\\(u^S_i\\\\) (the payoff that i gets in the event of a zero-sum conflict), the coalitions collectively would be promising more utility to player i than can actually be guaranteed, and they're making unfulfillable promises. But player i should really be picking up all the utility promised to it from all the coalitions, and not leaving excess on the table, and so we get equality.\n\nAs it turns out, if that equation holds, then you can work out how all the \\\\(w^S_i\\\\) must be defined: it must hold that \\\\(w^S\\_i=\\\\sum\\_{i\\\\in R\\\\subseteq S}(-1)^{|S|-|R|}u^R_i\\\\). This is around where I have problems. I just can't quite manage to get myself to see how this quantity is the \"slice of marginal utility that coalition \\\\(S\\\\) promises to player i\", so let me know in the comments if anyone manages to pull it off.\n\nThen, we go \"ah, if \\\\(w^S_i\\\\) is the marginal gain from being part of coalition \\\\(S\\\\)\" (which, again, I can't quite see), then \\\\(w^S\\_i=u^S\\_i-t^S_i\\\\), where \\\\(u^S_i\\\\) is the payoff to i from playing its part in S's minimax strategy, and \\\\(t^S_i\\\\) is i's threat utility/disagreement point utility from being in coalition S. And so, the disagreement point utility of player i within coalition S must be \\\\(t^S\\_i=\\\\sum\\_{i\\\\in R\\\\subset S}(-1)^{|S|-|R|+1}u^R_i\\\\).  \nAgain, I can't see *at all* how this is true from the equation alone, but other people might be able to understand it. I can see how, in the special case of the coalition \"Alice and Bob\", it reproduces the intuitive result of Alice's disagreement-point-utility being \"screw you Bob, I'll go off on my own and fight the rest of the world\" (ie, \\\\(t^{\\\\{1,2\\\\}}\\_1=u^{\\\\{1\\\\}}\\_1\\\\)), but I can't see how it extends to larger coalitions than that.\n\nAnyways, a few interesting results are derived and discussed from there. One is that if two players i and j (Ione and Jake) are arguing over their split of the gains in every coalition S that contains both of them, there's a well-defined fallback point for Ione which is \\\\(\\\\sum_{S\\\\subseteq N:i\\\\in S,j\\\\not\\\\in S}w^S_i\\\\) (the sum of payoffs from every coalition that contains Ione but lacks Jake), and symmetrically for Jake, and if they do Nash bargaining from there, then it's possible to apply a result on Nash bargaining in subgames (intuitively, both players want to pick a point that doesn't worsen their bargaining position for the full game) to derive that Ione and Jake will agree to the same ratio for how to split coalition gains between them, in every coalition game. So, if Ione and Jake are splitting value 60/40 in one coalition, they're doing that same split in every coalition. This can be used to derive the general result that, in every coalition containing two or more players, they'll all play the Nash bargaining solution against each other.\n\nAnd there's another interesting and highly nontrivial result from Harsanyi's paper, which effectively says that if the two coalitions of S and N/S (everyone who isn't in S) appoint an individual member to decide on the threat strategy that their coalition will follow, and the appointed representatives Ione and Jake only care about maximizing their own payoff in the overall game (ie, maximizing \\\\(u^N_i\\\\) and \\\\(u^N_j\\\\)), (ie. they know that the zero-sum fight between S and N/S probably isn't happening, it's just a bargaining chip to get good overall payoffs, and they just care about their own overall payoff, not the interests of the rest of their coalition), then the threat strategies they'll pick will be minimax threat strategies for the S vs N/S zero-sum game. Correspondingly, it doesn't matter *what* players the coalitions S and N/S appoint as representatives, they'll end up picking a minimax threat strategy to maximize their payoff in the overall game.\n\nWhat Harsanyi eventually ended up deciding on as the equilibrium conditions were as follows (slightly re-expressed for notational compliance). Let \\\\(U_i\\\\) be the utility function of player i, and \\\\(A_i\\\\) be their space of actions.\n\n*Formal Definition:* **Coalition-Perfect Harsanyi Equilibrium**  \n*A coalition-perfect Harsanyi equilibrium is a tuple *\\\\(\\\\{\\\\rho^S\\\\}_{S\\\\subseteq N}\\\\)* (the joint strategies that each coalition would play if they were against the opposing coalition, *\\\\(\\\\rho^S\\\\in\\\\Delta\\\\prod_{i\\\\in S}A_i\\\\)*),s.t, defining *\\\\(u^S\\_i := U\\_i(\\\\rho^S,\\\\rho^{N/S})\\\\)* (payoff to player *\\\\(i\\\\)* if coalitions *\\\\(S\\\\)* and *\\\\(N/S\\\\)* fight), and *\\\\(t^S\\_i :=\\\\sum\\_{i\\\\in R\\\\subset S}(-1)^{|S|-|R|+1}u^R_i\\\\)* (the fallback utility value for player *\\\\(i\\\\)* in coalition *\\\\(S\\\\)*).*\n\n*1: *\\\\(\\\\{u^N\\_i\\\\}\\_{i\\\\in N}\\\\)* is on the Pareto frontier.*\n\n*2: There is an *\\\\(\\\\{a\\_i\\\\}\\_{i\\\\in N}\\\\)* tuple (virtual prices, though Harsanyi didn't use the term) that makes both of the following conditions true.*\n\n*3: *\\\\(\\\\displaystyle{\\\\forall S\\\\subseteq N:\\\\rho^S\\\\in\\\\text{argmax}_{\\\\rho\\\\in\\\\Delta\\\\prod_{i\\\\in S}A\\_i}\\\\left(\\\\sum\\_{i\\\\in S} a\\_i U\\_i(\\\\rho,\\\\rho^{N/S})-\\\\sum_{j\\\\in N/S} a\\_j U\\_j(\\\\rho,\\\\rho^{N/S})\\\\right)}\\\\)  \n*(ie. all the joint strategies are trying to maximize the money earned if up against the opposing coalition in a zero-sum game and as a special case, when S=N, it says that what the entire group actually ends up doing maximizes surplus value, which is another way of stating that the *\\\\(\\\\{a\\_i\\\\}\\_{i\\\\in N}\\\\)* are the appropriate virtual currencies to use at the *\\\\(\\\\{u^N\\_i\\\\}\\_{i\\\\in N}\\\\)* point)*\n\n*4: *\\\\(\\\\forall i,j\\\\in S\\\\subseteq N: a\\_i(u^S\\_i-t^S\\_i)=a\\_j(u^S\\_j-t^S\\_j)\\\\)  \n* (it's very nonobvious, but if you use Lagrange multipliers on the Nash bargaining problem, this is effectively saying that for all coalitions *\\\\(S\\\\)*, the division of resources within *\\\\(S\\\\)*, using the various *\\\\(t^S_i\\\\)* as the disagreement payoffs, follows the Nash bargaining solution)*\n\nAnd now we get to the centerpiece theorem, that coalition-perfect CoCo equilibria are the same as coalition-perfect Harsanyi equilibria. Since Harsanyi already showed things like scale-and-shift invariance, and that these equilibria exist, and lots of other results about them, we just need to prove equivalence and then we can lift all of Harsanyi's work - no point in rederiving everything on our own. Since three of the four conditions for the equilibria are obviously identical, the whole proof focuses on showing that the \"Every coalition uses the Nash bargaining solution internally to divide gains, with suitably defined threat points\" condition of Harsanyi equilibria is equivalent to the \"Every coalition uses the CoCo payoffs/modified Shapley payoffs internally to divide gains\" condition of CoCo equilibria.\n\n**Theorem 1:** *A tuple *\\\\(\\\\{\\\\rho^S\\\\}_{S\\\\subseteq N}\\\\)* of strategies for every coalition is a coalition-perfect Harsanyi equilibrium iff it's a coalition-perfect CoCo equilibrium.*  \n  \n**Equilibria Existence**\n\nAnd now for the final question. Do these sorts of equilibria even exist at all? That's an awful lot of conditions to fulfill at once, you've got one equilibria condition for each coalition, and there's a whole lot of coalitions.\n\nWell, fortunately, Harsanyi proved that these sorts of equilibria exist in his paper! So we can just copy off his work. Well, technically, he did it under some moderately restrictive assumptions which don't look essential, and can probably be removed, though it'll be annoying to do so. Pretty much, his proof works by setting up a game which is related to the bargaining game, and any Nash equilibrium of the auxiliary game can be converted into a coalition-perfect Harsanyi equilibrium.\n\nThe assumptions Harsanyi made were, in particular, that the space of all possible utility values in \\\\(\\\\mathbb{R}^n\\\\) were compact (ie, nobody can get unbounded positive utility or unbounded negative utility, and this assumption is violated in full transferrable utility games), and its affine hull was of dimension \\\\(n\\\\), and that the Pareto frontier had no vertices in the sense that, for all points on it, there's a unique tangent hyperplane that touches that point (ie, you can *uniquely* read off the weights from *all* Pareto frontier points). Think of a sphere in 3d space: every point on the sphere surface has a unique tangent plane. But for a cube in 3d space, the edges or vertices of the cube can have multiple distinct tangent planes which touch the cube at that point.\n\nWith those assumptions, yes, there is a coalition-perfect Harsanyi equilibrium, as proven in his paper.\n\nHarsanyi made the remark that if the Pareto frontier has vertices, it's possible to write any such game as a limit of games that don't have vertices (like, imagine a cube but all the corners and edges have been sanded down a bit, and take the limit of doing less and less sanding), in order to extend the results to games with vertices on their Pareto frontier.\n\nThough he didn't comment on it, it seems like it's possible to also deal with the affine hull dimension issue in this way, in the sense that for any set of possible utility values whose affine hull is of dimension \\\\(<n\\\\), it's possible to write it as a limit of games whose set of utility values has an affine hull of dimension \\\\(n\\\\) (the analogue is that any 2-dimensional shape can be thought of as a limit of 3-dimensional shapes that keep getting thinner), and presumably extend his existence result to cases like that.\n\nHe didn't *actually* do these limiting-case proofs at any point, they just seem like the sort of argument that'd need to be done to generalize his proof.\n\nThere's another question which is, \"are Harsanyi/CoCo equilibria unique\"?\n\nHarsanyi made the remark that they were unique for bargaining games (where they'd give the Nash bargaining solution), games with transferrable utility (where they'd give the CoCo value), and 2-player games, but weren't necessarily unique for  general n-player games, and then completely refused to elaborate on this.\n\nThe problem is, although Harsanyi said there were counterexamples to uniqueness (he meant a counterexample in the stronger sense of \"there's more than one tuple of utility values that's an equilibrium\", not the obvious weak sense of \"maybe there's different strategies for everyone that gives the same payoff\"), at *no point* did he every actually *give* such a counterexample, even in the paper he cited to that effect. This is somewhat infuriating, and I fear that the non-uniqueness of these equilibria is one of those apocryphal results that nobody ever actually got around to double-checking at any point. I'd be extremely pleased if anyone could find a paper with an actual example of such.\n\nSo, yeah, that's about it. There's one good notion of equilibria, that gives Shapley values, CoCo values, and the Nash bargaining solution as special cases, which can variously be thought of as:\n\n1: Maximizing a suitable weighted sum of everyone's utilities, where all the various coalitions agree on the weights of everyone's utilities (so if Alice is twice as important as Bob, then Alice will be twice as important as Bob in all coalitions containing the two of them).\n\n2: Gives everyone their modified Shapley payoffs, and all the coalitions split their gains in a Shapley way.\n\n3: Inventing a virtual currency reflecting how hard it is to improve the utilities of everyone relative to each other and splitting the game into a bunch of coalition vs coalition fights with perfect cooperation and competition, and paying everyone accordingly.\n\n4: Every coalition jostles for a threat strategy that gives them the most payoff from the Nash bargaining solution, and then every coalition does Nash bargaining within itself to split gains.\n\n**But What if it Sucks, Tho (it Does)**\n\nSo, there's one super-important aspect of this that makes it dramatically less appealing that I haven't seen anyone point out. The payoffs for everyone are determined by games of the form \"coalition \\\\(S\\\\) fights coalition not-\\\\(S\\\\), coalition \\\\(S\\\\) is maximizing the quantity \"utility of coalition \\\\(S\\\\) \\- utility of opposite coalition\", and vice-versa for the opposite coalition\".\n\nIf you depart from nice comfy visualizations of games involving hot-dog selling, and ponder what that'd mean for humanity, you'll probably realize how *exceptionally ugly* those imaginary games would get.\n\nActually take one minute, by the clock, to think about what it means that the following equation determine people's payoffs:\n\n\\\\(\\\\displaystyle{\\\\max_{\\\\rho^{S}\\\\in\\\\Delta\\\\prod_{i\\\\in S}A\\_i}\\\\min\\_{\\\\rho^{N/S}\\\\in\\\\Delta\\\\prod_{j\\\\not\\\\in S}A\\_j}\\\\left(\\\\sum\\_{i\\\\in S}a\\_i U\\_i(\\\\rho^S,\\\\rho^{N/S})-\\\\sum_{j\\\\not\\\\in S}a\\_j U\\_j(\\\\rho^S,\\\\rho^{N/S})\\\\right)}\\\\)\n\nThis is why I was stressing that \"chaa\" and \"fair\" are very different concepts, and that this equilibrium notion is very much based on threats. They just need to be asymmetric threats that the opponent can't defuse in order to work (or ways of asymmetrically benefiting yourself that your opponent can't ruin, that'll work just as well).\n\nI think it's a terrible idea to automatically adopt an equilibrium notion which incentivises the players to come up with increasingly nasty threats as fallback if they don't get their way. And so there seems to be a good chunk of remaining work to be done, involving poking more carefully at the CoCo value and seeing which assumptions going into it can be broken.\n\nAlso, next Thursday (June 28) at noon Pacific time is the Schelling time to meet in the Walled Garden and discuss the practical applications of this. Come one, come all, and bring your insights!\n\n**Appendix: Proof of Theorem 1 (you can skip this one)**\n\nSince conditions 1, 2, and 3 are all obviously equivalent to each other, that just leaves that showing that the condition 4's of both types of equilibria imply each other. First, we'll show a lemma.\n\nLemma 1: \\\\(u^S\\_i=\\\\sum\\_{i\\\\in R\\\\subseteq S}w^R_i\\\\)\n\nStart off with\n\n\\\\(\\\\sum_{i\\\\in R\\\\subseteq S}w^R_i\\\\)\n\nUnpack the definition of \\\\(w^R_i\\\\).\n\n\\\\(=\\\\sum_{i\\\\in R\\\\subseteq S}\\\\sum_{i\\\\in T\\\\subseteq R}(-1)^{|R|-|T|}u^T_i\\\\)\n\nWe can interchange this sum, and view it as picking the set T first, and the set R second.\n\n\\\\(=\\\\sum_{i\\\\in T\\\\subseteq S}\\\\sum_{T\\\\subseteq R\\\\subseteq S}(-1)^{|R|-|T|}u^T_i\\\\)\n\nAnd group\n\n\\\\(=\\\\sum_{i\\\\in T\\\\subseteq S}\\\\left(\\\\sum_{T\\\\subseteq R\\\\subseteq S}(-1)^{|R|-|T|}\\\\right)u^T_i\\\\)\n\nAnd we can ask what coefficient is paired with the various \\\\(u^T_i\\\\). Really, there's two possibilities. One possibility is that \\\\(T=S\\\\), the other is that \\\\(T\\\\neq S\\\\), so let's split this up.\n\n\\\\(=\\\\left(\\\\sum_{S\\\\subseteq R\\\\subseteq S}(-1)^{|R|-|S|}\\\\right)u^S\\_i+\\\\sum\\_{i\\\\in T\\\\subset S}\\\\left(\\\\sum_{T\\\\subseteq R\\\\subseteq S}(-1)^{|R|-|T|}\\\\right)u^T_i\\\\)\n\nClearly, for the former term, \\\\(R=S\\\\) is the only possibility, and \\\\((-1)^{|S|-|S|}=(-1)^0=1\\\\), so we get\n\n\\\\(=u^S\\_i+\\\\sum\\_{i\\\\in T\\\\subset S}\\\\left(\\\\sum_{T\\\\subseteq R\\\\subseteq S}(-1)^{|R|-|T|}\\\\right)u^T_i\\\\)\n\nWe can reexpress picking a \\\\(R\\\\supseteq T\\\\) as picking an \\\\(R'\\\\subseteq S/T\\\\) (the fragments not in T) and unioning it with T.\n\n\\\\(=u^S\\_i+\\\\sum\\_{i\\\\in T\\\\subset S}\\\\left(\\\\sum_{R'\\\\subseteq S/T}(-1)^{|R'\\\\cup T|-|T|}\\\\right)u^T_i\\\\)\n\n\\\\(=u^S\\_i+\\\\sum\\_{i\\\\in T\\\\subset S}\\\\left(\\\\sum_{R'\\\\subseteq S/T}(-1)^{|R'|+|T|-|T|}\\\\right)u^T_i\\\\)\n\n\\\\(=u^S\\_i+\\\\sum\\_{i\\\\in T\\\\subset S}\\\\left(\\\\sum_{R'\\\\subseteq S/T}(-1)^{|R'|}\\\\right)u^T_i\\\\)\n\nTry writing this as a sum over subset sizes, and you'll get a factorial term showing up from the many possible subsets of a given size.\n\n\\\\(=u^S\\_i+\\\\sum\\_{i\\\\in T\\\\subset S}\\\\left(\\\\sum_{b=0}^{b=|S/T|}\\\\frac{|S/T|!}{(|S/T|-b)!b!}(-1)^{b}\\\\right)u^T_i\\\\)\n\n\\\\(=u^S\\_i+\\\\sum\\_{i\\\\in T\\\\subset S}\\\\left(\\\\sum_{b=0}^{b=|S|-|T|}\\\\frac{(|S|-|T|)!}{(|S|-|T|-b)!b!}(-1)^{b}\\\\right)u^T_i\\\\)\n\nAnd then, by plugging this into Wolfram Alpha, we get 0 and all that stuff cancels.  \n\\\\(=u^S_i\\\\), and so the lemma has been proved.\n\nOnto the full proof, starting off by assuming condition 4 of a coalition-perfect Harsanyi equilibria, and deriving condition 4 of a coalition-perfect CoCo equilibria. Start off with \\\\(a\\_i u^S\\_i\\\\). Then, we use our lemma that \\\\(u^S\\_i=\\\\sum\\_{i\\\\in R\\\\subseteq S}w^R_i\\\\).\n\n\\\\(=a\\_i\\\\sum\\_{i\\\\in R\\\\subseteq S}w^R_i\\\\)\n\nDistribute the constant in\n\n\\\\(=\\\\sum_{i\\\\in R\\\\subseteq S}a\\_i w^R\\_i\\\\)\n\nUse that \\\\(w^R\\_i=u^R\\_i-t^R_i\\\\), by definition of \\\\(t^R_i\\\\).\n\n\\\\(=\\\\sum_{i\\\\in R\\\\subseteq S}a\\_i(u^R\\_i-t^R_i)\\\\)\n\nNow, use that, by condition 4 of coalition-perfect Harsanyi equilibria, every player \\\\(j\\\\in R\\\\) has \\\\(a\\_j(u^R\\_j-t^R\\_j)=a\\_i(u^R\\_i-t^R\\_i)\\\\), so we can rewrite this as\n\n\\\\(=\\\\sum_{i\\\\in R\\\\subseteq S}\\\\frac{1}{|R|}\\\\sum_{j\\\\in R}a\\_j(u^R\\_j-t^R_j)\\\\)\n\nUnpack what \\\\(t^R_j\\\\) is\n\n\\\\(=\\\\sum_{i\\\\in R\\\\subseteq S}\\\\frac{1}{|R|}\\\\sum_{j\\\\in R}a\\_j\\\\left(u^R\\_j-\\\\sum_{j\\\\in T\\\\subset R}(-1)^{|R|-|T|+1}u^T_j\\\\right)\\\\)\n\nDistribute the negative in\n\n\\\\(=\\\\sum_{i\\\\in R\\\\subseteq S}\\\\frac{1}{|R|}\\\\sum_{j\\\\in R}a\\_j\\\\left(u^R\\_j+\\\\sum_{j\\\\in T\\\\subset R}(-1)^{|R|-|T|}u^T_j\\\\right)\\\\)\n\nMerge it into one big sum\n\n\\\\(=\\\\sum_{i\\\\in R\\\\subseteq S}\\\\frac{1}{|R|}\\\\sum_{j\\\\in R}a\\_j\\\\sum\\_{j\\\\in T\\\\subseteq R}(-1)^{|R|-|T|}u^T_j\\\\)\n\nReshuffle the sum so we're summing over subsets first, and elements of that subset later. The available subsets T are all the subsets of R, and they can only be included in the sum for the j that lie in T.\n\n\\\\(=\\\\sum_{i\\\\in R\\\\subseteq S}\\\\frac{1}{|R|}\\\\sum_{T\\\\subseteq R}\\\\sum_{j\\\\in T}a\\_j(-1)^{|R|-|T|}u^T\\_j\\\\)\n\nWe can reshuffle the negative 1 part outside of the innermost sum.\n\n\\\\(=\\\\sum_{i\\\\in R\\\\subseteq S}\\\\frac{1}{|R|}\\\\sum_{T\\\\subseteq R}(-1)^{|R|-|T|}\\\\sum_{j\\\\in T}a\\_j u^T\\_j\\\\)\n\nAbbreviate \\\\(\\\\sum_{j\\\\in T}a\\_j u^T\\_j\\\\) as \\\\(Z^T\\\\), and reshuffle the sums a little bit.\n\n\\\\(=\\\\sum_{i\\\\in R\\\\subseteq S}\\\\sum_{T\\\\subseteq R}\\\\frac{1}{|R|}(-1)^{|R|-|T|} Z^T\\\\)\n\nNow, for a given T, we'll work out the coefficient in front of \\\\(Z^T\\\\) for the entire sum. The first possibility is that \\\\(i\\\\in T\\\\). Then the possible R that contribute to the coefficient of \\\\(Z^T\\\\) in the entire sum are exactly the \\\\(R\\\\supseteq T\\\\). The second possibility is that \\\\(i\\\\not\\\\in T\\\\), so then the possible R that contribute to the coefficient of \\\\(Z^T\\\\) in the entire sum are exactly the \\\\(R\\\\supseteq T\\\\cup\\\\{i\\\\}\\\\). So, breaking things up that way, we get\n\n\\\\(=\\\\left(\\\\sum_{i\\\\in T\\\\subseteq S}\\\\sum_{T\\\\subseteq R\\\\subseteq S}\\\\frac{1}{|R|}(-1)^{|R|-|T|}Z^T\\\\right)+\\\\left(\\\\sum_{i\\\\not\\\\in T\\\\subseteq S}\\\\sum_{T\\\\cup\\\\{i\\\\}\\\\subseteq R\\\\subseteq S}\\\\frac{1}{|R|}(-1)^{|R|-|T|}Z^T\\\\right)\\\\)\n\nAnd we can rephrase supersets of T as subsets of \\\\(S/T\\\\), unioned with T. And rephrase supersets of T that contain i as subsets of \\\\(S/(T\\\\cup\\\\{i\\\\})\\\\), unioned with \\\\(T\\\\cup\\\\{i\\\\}\\\\).\n\n\\\\(=\\\\left(\\\\sum_{i\\\\in T\\\\subseteq S}\\\\sum_{R'\\\\subseteq S/T}\\\\frac{1}{|R'\\\\cup T|}(-1)^{|R'\\\\cup T|-|T|}Z^T\\\\right)\\\\)  \n\\\\(+\\\\left(\\\\sum_{i\\\\not\\\\in T\\\\subseteq S}\\\\sum_{R'\\\\subseteq S/(T\\\\cup\\\\{i\\\\})}\\\\frac{1}{|R'\\\\cup T\\\\cup\\\\{i\\\\}|}(-1)^{|R'\\\\cup T\\\\cup\\\\{i\\\\}|-|T|}Z^T\\\\right)\\\\)\n\nReexpress\n\n\\\\(=\\\\left(\\\\sum_{i\\\\in T\\\\subseteq S}\\\\sum_{R'\\\\subseteq S/T}\\\\frac{1}{|R'|+|T|}(-1)^{|R'|+|T|-|T|}Z^T\\\\right)\\\\)  \n\\\\(+\\\\left(\\\\sum_{i\\\\not\\\\in T\\\\subseteq S}\\\\sum_{R'\\\\subseteq S/(T\\\\cup\\\\{i\\\\})}\\\\frac{1}{|R'|+|T|+1}(-1)^{|R'|+|T|+1-|T|}Z^T\\\\right)\\\\)\n\nCancel\n\n\\\\(=\\\\left(\\\\sum_{i\\\\in T\\\\subseteq S}\\\\sum_{R'\\\\subseteq S/T}\\\\frac{1}{|R'|+|T|}(-1)^{|R'|}Z^T\\\\right)\\\\)  \n\\\\(+\\\\left(\\\\sum_{i\\\\not\\\\in T\\\\subseteq S}\\\\sum_{R'\\\\subseteq S/(T\\\\cup\\\\{i\\\\})}\\\\frac{1}{|R'|+|T|+1}(-1)^{|R'|+1}Z^T\\\\right)\\\\)\n\nSplit into a sum over the various sizes of what \\\\(R'\\\\) could possibly be\n\n\\\\(=\\\\left(\\\\sum_{i\\\\in T\\\\subseteq S}\\\\sum_{b=0}^{b=|S/T|}\\\\frac{|S/T|!}{(|S/T|-b)!b!}\\\\frac{1}{b+|T|}(-1)^{b}Z^T\\\\right)\\\\)  \n\\\\(+\\\\left(\\\\sum_{i\\\\not\\\\in T\\\\subseteq S}\\\\sum_{b=0}^{b=|S/(T\\\\cup\\\\{i\\\\})|}\\\\frac{|S/(T\\\\cup\\\\{i\\\\})|!}{(|S/(T\\\\cup\\\\{i\\\\})|-b)!b!}\\\\frac{1}{b+|T|+1}(-1)^{b+1}Z^T\\\\right)\\\\)\n\nReexpress\n\n\\\\(=\\\\left(\\\\sum_{i\\\\in T\\\\subseteq S}\\\\sum_{b=0}^{b=|S|-|T|}\\\\frac{(|S|-|T|)!}{(|S|-|T|-b)!b!}\\\\frac{1}{b+|T|}(-1)^{b}Z^T\\\\right)\\\\)  \n\\\\(+\\\\left(\\\\sum_{i\\\\not\\\\in T\\\\subseteq S}\\\\sum_{b=0}^{b=|S|-|T|-1}\\\\frac{(|S|-|T|-1)!}{(|S|-|T|-1-b)!b!}\\\\frac{1}{b+|T|+1}(-1)^{b+1}Z^T\\\\right)\\\\)\n\nGroup into one big fraction.\n\n\\\\(=\\\\left(\\\\sum_{i\\\\in T\\\\subseteq S}\\\\left(\\\\sum_{b=0}^{b=|S|-|T|}\\\\frac{(|S|-|T|)!(-1)^{b}}{(|S|-|T|-b)!b!(b+|T|)}\\\\right)Z^T\\\\right)\\\\)  \n\\\\(+\\\\left(\\\\sum_{i\\\\not\\\\in T\\\\subseteq S}\\\\left(\\\\sum_{b=0}^{b=|S|-|T|-1}\\\\frac{(|S|-|T|-1)!(-1)^{b+1}}{(|S|-|T|-1-b)!b!(b+|T|+1)}\\\\right)Z^T\\\\right)\\\\)\n\nPlug it into Wolfram Alpha, and use how the gamma function is defined to get it back into factorial form.\n\n\\\\(=\\\\left(\\\\sum_{i\\\\in T\\\\subseteq S}\\\\frac{(|T|-1)!(|S|-|T|)!}{|S|!}Z^T\\\\right)+\\\\left(\\\\sum_{i\\\\not\\\\in T\\\\subseteq S}-\\\\frac{|T|!(|S|-|T|-1)!}{|S|!}Z^T\\\\right)\\\\)\n\nReindex T to R for notational compliance later on, and we can rewrite this as a single sum over all R that contain i, because they're all paired off with a unique complement that lacks i.\n\n\\\\(=\\\\sum_{i\\\\in R\\\\subseteq S}\\\\frac{(|R|-1)!(|S|-|R|)!}{|S|!}Z^R-\\\\frac{|S/R|!(|S|-|S/R|-1)!}{|S|!}Z^{S/R}\\\\)\n\nFigure out what the cardinalities of the various sets are\n\n\\\\(=\\\\sum_{i\\\\in R\\\\subseteq S}\\\\frac{(|R|-1)!(|S|-|R|)!}{|S|!}Z^R-\\\\frac{(|S|-|R|)!(|S|-(|S|-|R|)-1)!}{|S|!}Z^{S/R}\\\\)\n\nCancel out, realize that the fractions are the same, and get\n\n\\\\(=\\\\sum_{i\\\\in R\\\\subseteq S}\\\\frac{(|R|-1)!(|S|-|R|)!}{|S|!}(Z^R-Z^{S/R})\\\\)\n\nAnd unpacking how \\\\(Z^R\\\\) was defined, we get, as intended, that this entire chain of equalities has proven\n\n\\\\(a\\_i u^S\\_i=\\\\sum_{i\\\\in R\\\\subseteq S}\\\\frac{(|R|-1)!(|S|-|R|)!}{|S|!}\\\\left(\\\\sum_{j\\\\in R}a\\_j u^R\\_j-\\\\sum_{k\\\\not\\\\in R}a\\_k u^{S/R}\\_k\\\\right)\\\\)\n\nThe exact condition 4 for a coalition-perfect CoCo equilibria, proving that all coalition-perfect Harsanyi equilibria are coalition-perfect CoCo equilibria.\n\nNow it's time for the reverse derivation, showing that all coalition-perfect CoCo equilibria are coalition-perfect Harsanyi equilibria. The goal is to show that for all \\\\(S\\\\subseteq N\\\\), and \\\\(i,j\\\\in S\\\\), that \\\\(a\\_i(u^S\\_i-t^S\\_i)=a\\_j(u^S\\_j-t^S\\_j)\\\\)  \nSo, let's start out with\n\n\\\\(a\\_i(u^S\\_i-t^S_i)\\\\)\n\nSubstitute in what \\\\(t^S_i\\\\) is\n\n\\\\(=a\\_i\\\\left(u^S\\_i-\\\\sum_{i\\\\in R\\\\subset S}(-1)^{|S|-|R|+1}u^R_i\\\\right)\\\\)\n\nCancel out the negatives\n\n\\\\(=a\\_i\\\\left(u^S\\_i+\\\\sum_{i\\\\in R\\\\subset S}(-1)^{|S|-|R|}u^R_i\\\\right)\\\\)\n\nFold it into one big sum\n\n\\\\(=a\\_i\\\\left(\\\\sum\\_{i\\\\in R\\\\subseteq S}(-1)^{|S|-|R|}u^R_i\\\\right)\\\\)\n\nMultiply the \\\\(a_i\\\\) in\n\n\\\\(=\\\\sum_{i\\\\in R\\\\subseteq S}(-1)^{|S|-|R|}a\\_i u^R\\_i\\\\)\n\nNow, we use condition 4 of a coalition-perfect CoCo equilibrium.\n\n\\\\(=\\\\sum_{i\\\\in R\\\\subseteq S}(-1)^{|S|-|R|}\\\\sum_{i\\\\in T\\\\subseteq R}\\\\frac{(|R|-|T|)!(|T|-1)!}{|R|!}\\\\left(\\\\sum_{j\\\\in T}a\\_j u^T\\_j-\\\\sum_{k\\\\in R/T}a\\_k u^T\\_k\\\\right)\\\\)\n\nAbbreviate the sum \\\\(\\\\sum_{j\\\\in T}a\\_j u^T\\_j\\\\) as \\\\(Z^T\\\\), to get\n\n\\\\(=\\\\sum_{i\\\\in R\\\\subseteq S}(-1)^{|S|-|R|}\\\\sum_{i\\\\in T\\\\subseteq R}\\\\frac{(|R|-|T|)!(|T|-1)!}{|R|!}(Z^T-Z^{R/T})\\\\)\n\nNow, we'll have to work out what the coefficent is for a given T, for the entire sum. Like, what number ends up being in front of \\\\(Z^T\\\\) when we sum everything up? There are two possibilities. The first possibility is that \\\\(i\\\\in T\\\\). Then the relevant R that we're summing over are the \\\\(R\\\\supseteq T\\\\). If \\\\(i\\\\not\\\\in T\\\\), then the relevant R that we're summing over are the \\\\(R\\\\supseteq T\\\\cup\\\\{i\\\\}\\\\), and we've got a negative 1 showing up from these Z terms being sutracted instead of added, which we can fold into the negative 1 power at the start. Using this grouping, we get\n\n\\\\(=\\\\left(\\\\sum_{i\\\\in T\\\\subseteq S}\\\\sum_{T\\\\subseteq R\\\\subseteq S}(-1)^{|S|-|R|}\\\\frac{(|R|-|T|)!(|T|-1)!}{|R|!}Z^T\\\\right)\\\\)  \n\\\\(+\\\\left(\\\\sum_{i\\\\not\\\\in T\\\\subseteq S}\\\\sum_{T\\\\cup\\\\{i\\\\}\\\\subseteq R\\\\subseteq S}(-1)^{|S|-|R|+1}\\\\frac{(|R|-|R/T|)!(|R/T|-1)!}{|R|!}Z^T\\\\right)\\\\)\n\nReexpress it slightly\n\n\\\\(=\\\\left(\\\\sum_{i\\\\in T\\\\subseteq S}\\\\sum_{T\\\\subseteq R\\\\subseteq S}(-1)^{|S|-|R|}\\\\frac{(|R|-|T|)!(|T|-1)!}{|R|!}Z^T\\\\right)\\\\)  \n\\\\(+\\\\left(\\\\sum_{i\\\\not\\\\in T\\\\subseteq S}\\\\sum_{T\\\\cup\\\\{i\\\\}\\\\subseteq R\\\\subseteq S}(-1)^{|S|-|R|+1}\\\\frac{(|R|-|R|+|T|)!(|R|-|T|-1)!}{|R|!}Z^T\\\\right)\\\\)\n\nAnd cancel\n\n\\\\(=\\\\left(\\\\sum_{i\\\\in T\\\\subseteq S}\\\\sum_{T\\\\subseteq R\\\\subseteq S}(-1)^{|S|-|R|}\\\\frac{(|R|-|T|)!(|T|-1)!}{|R|!}Z^T\\\\right)\\\\)  \n\\\\(+\\\\left(\\\\sum_{i\\\\not\\\\in T\\\\subseteq S}\\\\sum_{T\\\\cup\\\\{i\\\\}\\\\subseteq R\\\\subseteq S}(-1)^{|S|-|R|+1}\\\\frac{|T|!(|R|-|T|-1)!}{|R|!}Z^T\\\\right)\\\\)\n\nAnd we can reexpress this as picking a subset of \\\\(S/T\\\\), and unioning it with T to make R, or as picking a subset of \\\\(S/(T\\\\cup\\\\{i\\\\})\\\\) and unioning it with \\\\(T\\\\cup\\\\{i\\\\}\\\\) to make R.\n\n\\\\(=\\\\left(\\\\sum_{i\\\\in T\\\\subseteq S}\\\\sum_{R'\\\\subseteq S/T}(-1)^{|S|-|R'\\\\cup T|}\\\\frac{(|R'\\\\cup T|-|T|)!(|T|-1)!}{|R'\\\\cup T|!}Z^T\\\\right)\\\\)  \n\\\\(+\\\\left(\\\\sum_{i\\\\not\\\\in T\\\\subseteq S}\\\\sum_{R'\\\\subseteq S/(T\\\\cup\\\\{i\\\\})}(-1)^{|S|-|R'\\\\cup T\\\\cup\\\\{i\\\\}|+1}\\\\frac{|T|!(|R'\\\\cup T\\\\cup\\\\{i\\\\}|-|T|-1)!}{|R'\\\\cup T\\\\cup\\\\{i\\\\}|!}Z^T\\\\right)\\\\)\n\nReexpress\n\n\\\\(=\\\\left(\\\\sum_{i\\\\in T\\\\subseteq S}\\\\sum_{R'\\\\subseteq S/T}(-1)^{|S|-|R'|-|T|}\\\\frac{(|R'|+|T|-|T|)!(|T|-1)!}{(|R'|+|T|)!}Z^T\\\\right)\\\\)  \n\\\\(+\\\\left(\\\\sum_{i\\\\not\\\\in T\\\\subseteq S}\\\\sum_{R'\\\\subseteq S/(T\\\\cup\\\\{i\\\\})}(-1)^{|S|-|R'|-|T|-1+1}\\\\frac{|T|!(|R'|+|T|+1-|T|-1)!}{(|R'|+|T|+1)!}Z^T\\\\right)\\\\)\n\nCancel\n\n\\\\(=\\\\left(\\\\sum_{i\\\\in T\\\\subseteq S}\\\\sum_{R'\\\\subseteq S/T}(-1)^{|S|-|R'|-|T|}\\\\frac{|R'|!(|T|-1)!}{(|R'|+|T|)!}Z^T\\\\right)\\\\)  \n\\\\(+\\\\left(\\\\sum_{i\\\\not\\\\in T\\\\subseteq S}\\\\sum_{R'\\\\subseteq S/(T\\\\cup\\\\{i\\\\})}(-1)^{|S|-|R'|-|T|}\\\\frac{|T|!|R'|!}{(|R'|+|T|+1)!}Z^T\\\\right)\\\\)\n\nReexpress as summing up over all possible sizes for \\\\(R'\\\\), introducing a factorial term because of the many subsets of a given size.\n\n\\\\(=\\\\left(\\\\sum_{i\\\\in T\\\\subseteq S}\\\\sum_{b=0}^{b=|S/T|}\\\\frac{|S/T|!}{(|S/T|-b)!b!}(-1)^{|S|-b-|T|}\\\\frac{b!(|T|-1)!}{(b+|T|)!}Z^T\\\\right)\\\\)  \n\\\\(+\\\\left(\\\\sum_{i\\\\not\\\\in T\\\\subseteq S}\\\\sum_{b=0}^{b=|S/(T\\\\cup\\\\{i\\\\})|}\\\\frac{|S/(T\\\\cup\\\\{i\\\\})|!}{(|S/(T\\\\cup\\\\{i\\\\})|-b)!b!}(-1)^{|S|-b-|T|}\\\\frac{|T|!b!}{(b+|T|+1)!}Z^T\\\\right)\\\\)\n\nReexpress\n\n\\\\(=\\\\left(\\\\sum_{i\\\\in T\\\\subseteq S}\\\\sum_{b=0}^{b=|S|-|T|}\\\\frac{(|S|-|T|)!}{(|S|-|T|-b)!b!}(-1)^{|S|-b-|T|}\\\\frac{b!(|T|-1)!}{(b+|T|)!}Z^T\\\\right)\\\\)  \n\\\\(+\\\\left(\\\\sum_{i\\\\not\\\\in T\\\\subseteq S}\\\\sum_{b=0}^{b=|S|-|T|-1}\\\\frac{(|S|-|T|-1)!}{(|S|-|T|-1-b)!b!}(-1)^{|S|-b-|T|}\\\\frac{|T|!b!}{(b+|T|+1)!}Z^T\\\\right)\\\\)\n\nMerge into one big fraction and cancel\n\n\\\\(=\\\\left(\\\\sum_{i\\\\in T\\\\subseteq S}\\\\sum_{b=0}^{b=|S|-|T|}\\\\frac{(|S|-|T|)!(|T|-1)!(-1)^{|S|-|T|-b}}{(|S|-|T|-b)!(|T|+b)!}Z^T\\\\right)\\\\)  \n\\\\(+\\\\left(\\\\sum_{i\\\\not\\\\in T\\\\subseteq S}\\\\sum_{b=0}^{b=|S|-|T|-1}\\\\frac{(|S|-|T|-1)!|T|!(-1)^{|S|-|T|-b}}{(|S|-|T|-b-1)!(|T|+b+1)!}Z^T\\\\right)\\\\)\n\nAnd plug into Wolfram Alpha to get that both of these alternating sums over factorials are actually the same coefficient, so we can just write it as\n\n\\\\(=\\\\sum_{T\\\\subseteq S}\\\\frac{(-1)^{|S|-|T|}}{|S|}Z^T\\\\)\n\nSumming all this up, we've derived\n\n\\\\(a\\_i(u^S\\_i-t^S\\_i)=\\\\sum\\_{T\\\\subseteq S}\\\\frac{(-1)^{|S|-|T|}}{|S|}Z^T\\\\)\n\nAnd then we can do this whole line of reasoning again but swapping out i for j, and nothing at all changes, we still get the same quantity at the end, so we have\n\n\\\\(a\\_i(u^S\\_i-t^S\\_i)=a\\_j(u^S\\_j-t^S\\_j)\\\\)\n\nFor all \\\\(i,j\\\\in S\\\\subseteq N\\\\), the fourth condition for a coalition-perfect Harsanyi equilibrium, so all coalition-perfect CoCo equilibria are coalition-perfect Harsanyi equilibria.\n\nSince we've proved both directions, something is a coalition-perfect Harsanyi equilibria iff it's a coalition-perfect CoCo equilibria."
    },
    "voteCount": 25,
    "forceInclude": true
  },
  {
    "_id": "k8hvGAJWSKAeHwpnJ",
    "url": null,
    "title": "Why I'm Worried About AI",
    "slug": "why-i-m-worried-about-ai",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "AI Risk"
      },
      {
        "name": "AI"
      },
      {
        "name": "Inner Alignment"
      },
      {
        "name": "Deception"
      },
      {
        "name": "Threat Models"
      }
    ],
    "tableOfContents": {
      "sections": [
        {
          "title": "Preamble",
          "anchor": "Preamble",
          "level": 1
        },
        {
          "title": "How do we train neural networks?",
          "anchor": "How_do_we_train_neural_networks_",
          "level": 1
        },
        {
          "title": "Optimization",
          "anchor": "Optimization",
          "level": 1
        },
        {
          "title": "Optimized vs Optimizers",
          "anchor": "Optimized_vs_Optimizers",
          "level": 2
        },
        {
          "title": "What do we tell the optimizers to do?",
          "anchor": "What_do_we_tell_the_optimizers_to_do_",
          "level": 2
        },
        {
          "title": "How do we actually put the objective into the AI?",
          "anchor": "How_do_we_actually_put_the_objective_into_the_AI_",
          "level": 1
        },
        {
          "title": "Deception",
          "anchor": "Deception",
          "level": 1
        },
        {
          "title": "Recap",
          "anchor": "Recap",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        },
        {
          "anchor": "comments",
          "level": 0,
          "title": "2 comments"
        }
      ],
      "headingsCount": 10
    },
    "contents": {
      "markdown": "Preamble\n========\n\nIn this sequence of posts, I want to lay out why I am worried about risks from powerful AI and where I think the specific dangers come from. In general, I think it's good for people to be able to form their own inside views of what’s going on, and not just defer to people. There are surprisingly few descriptions of actual risk models written down. \n\nI think writing down your own version of the AI risk story is good for a few reasons\n\n*   It makes you critically examine the risk models of other people, and work out *what you actually believe.*\n*   It may be helpful for finding research directions which seem good. Solving a problem seems much more doable if you can actually point at it.\n*   It seems virtuous to attempt to form your own views rather than just the consensus view (whatever that is).\n*   People can comment on where your story may be weak or inconsistent, which will hopefully push towards the truth.\n\nAdditionally, I have some friends and family who are not in EA/AI-safety/Longtermism/Rationality, and it would be nice to be able to point them at something describing why I’m doing what I’m doing (technical AI-safety). Although, admittedly my views are more complicated than I initially thought, so this isn’t a great first introduction to AI-risk.\n\nI don’t expect much or any of these posts to be original and there will be many missing links and references. This can maybe be viewed as a more quick and dirty [AGI safety from first principles](https://www.lesswrong.com/s/mzgtmmTKKn5MuCzFJ/p/8xRSjC76HasLnMGSf), with less big picture justification and more focus on my specific risk models. In general, I am concerned most about [deceptively aligned](https://www.lesswrong.com/posts/zthDPAjh9w6Ytbeks/deceptive-alignment) AI systems, as discussed in [Risks from Learned Optimization](https://www.lesswrong.com/s/r9tYkB2a8Fp4DN8yB). \n\nHow do we train neural networks?\n================================\n\nIn the current paradigm of AI, we train neural networks to be good at tasks, and then we deploy them in the real world to perform those tasks. \n\nWe train neural networks on a *training distribution*\n\n*   For image classifiers, this is a set of labeled images. E.g. a set of images of cats and dogs with corresponding labels.\n*   For language models like GPT-3, this is a very large amount of text from the internet.\n*   For reinforcement learning, this is some training environment where the AI agent can ‘learn’ to take actions. For example, balancing a pole on a cart or playing the Atari game Breakout. \n\n**We start with an untrained AI system and modify it to perform better on the training distribution.** For our cat vs dog classifier, we feed in images of cats and dogs (our training data) and modify the AI so that it is able to accurately label these images. For GPT-3, we feed in the start of a string of text and modify it such that it can accurately predict what will come next. For our reinforcement learning agent playing Breakout, the agent takes actions in the game (move the platform left and right), we reward actions which lead to high score, and use this reward to train the agent to do well in this game. \n\nIf the training is good then our AI system will then be able to generalize and perform well in slightly different domains. \n\n*   Our cat vs dog classifier can correctly label images of cats and dogs it hasn’t seen before\n*   Our language model can ‘predict’ the next word for a given sensible prompt, and use this to generate coherent text \n*   Our reinforcement learning agent can play the game, even if it hasn’t seen this exact game state before. Or even generalize to slightly different environments; what if there are more or differently positioned blocks?\n\nThis ability to generalize is limited by the task it was trained on; we can only really expect good generalization on data that is *similar enough* to the training distribution. Our cat vs dog classifier might struggle if we show it a lion. If our language model has only been trained on English and German text, it won’t be able to generate French. Our Breakout agent can really only play Breakout. \n\nOptimization\n============\n\nWhen we train our AI systems we are optimizing them to perform well on the training distribution. By *optimizing* I mean that we are modifying these systems such that they do well on some objective. \n\nOptimized vs Optimizers\n-----------------------\n\nIt is important to make a distinction between something which is *optimized *and something which is an *optimizer*. When we train our AI systems, we end up with an optimized system; the system has been optimized to perform well on a task, be that cat vs dog classification, predicting the next word in a sentence, or achieving a high score at Breakout. These systems have been *optimized* to do well on the objective we have given them, but they themselves (probably) aren’t *optimizers*; they don’t have any notion of improving on an objective. \n\nOur cat vs dog classifier likely just has a bunch of heuristics which influence the relative likelihood of ‘cat’ or ‘dog’. Our Breakout agent is probably running an algorithm which looks like “The ball is at position X, the platform is at position Y, so take action A”, and not something like “The ball is at position X, the platform is at position Y, if I take action A it will give me a better score than action B, so take action A”. \n\nWe did the optimizing with our training and ended up with an optimized system. \n\nHowever, there are reasons to expect that we will get ‘optimizers’ as we build more powerful systems which operate in complex environments. AI systems can solve a task in 2 main ways (although the boundary here is fuzzy)\n\n*   They can use a bunch of heuristics that mechanistically combine to choose an action. For example, “If there is <this texture> +3 to the dog number, if there is <this triangle shape> +4 to the cat number, if there is <this color> next to <this color> +2 to the dog number… etc”.\n*   They can ‘do optimization’; search over some space of actions and find which one performs best on some criteria. For example, a language model evaluating outputs on “Which of these words is most likely to come next?”, or our Breakout AI asking “What will be my expected overall score if I take this action?”\n\nAs our tasks get more complex, if we are using the heuristic strategy, we will need to pile on more and more heuristics to perform well on the task. It seems like the optimization approach will become favored as things become more complex because the complexity of the optimization (search and evaluate) algorithm doesn’t increase as much with task complexity. If we are training on a very complex task and we want to achieve a certain level of performance on the training distribution, the heuristic-based algorithm will be more complex than the optimization-based algorithm. One intuition here is that for very varied and complex tasks, the AI may require some kind of “general reasoning ability” which is different from the pile of heuristics. One relatively simple way of doing “general reasoning” is to have an evaluation criterion for the task, and then evaluate possible actions on this criterion. \n\nIf AI systems are capable of performing complex tasks, then it seems like there will be very strong economic pressures to develop them. I expect by default for these AI systems to be running some kind of optimization algorithm. \n\nWhat do we tell the optimizers to do?\n-------------------------------------\n\nAssuming that we get optimizers, we need to be able to tell them what to do. By this I mean when we train a system to achieve a goal, *we want that goal to actually be one that we want*. This is the “Outer Alignment Problem”. \n\nThe classic example here is that we run a paperclip factory, so we tell our optimizing AI to make us some paperclips. This AI has no notion of anything else that we want or care about so it would sacrifice literally anything to make more paperclips. It starts by improving the factory we already have and making it more efficient. This still isn’t making the maximal number of paperclips, so it commissions several new factories. The human workers are slow, so it replaces them with toilless robots. At some point, the government gets suspicious of all these new factories, so the AI uses its powers of superhuman persuasion to convince them this is fine, and in fact, this is in the interest of National Security. This is still very slow compared to the maximal rate of paperclip production, so the AI designs some nanobots which convert anything made of metal into paperclips. At this point, it is fairly obvious to the humans that something is very, very wrong, but this feeling doesn’t last very long because soon the iron in the blood of every human is used to make paperclips (approximately 3 paperclips per person). \n\nThis is obviously a fanciful story, but I think it points at an important point; it’s not enough to tell the AI what to do, we also have to be able to tell it *what not to do.* Humans have pretty specific values, and it seems extremely difficult to specify. \n\nThere are more plausible stories we can tell which lead to similarly disastrous results. \n\n*   If we want our AI system to maximize the number in a bank account, if it is powerful enough it might hack into the bank’s computer system to modify the number and then take further actions to ensure the number is not modified back. \n*   If we tell our AI to maximize the number in a bank account but not ‘break any laws’, then it may aim to build factories which are technically legal but detrimental for the environment. Or it may just blackmail lawmakers to create legal loopholes for it to abuse. \n*   If we ask our AI to do any task where success is measured by a human providing a reward signal (e.g. every hour rating the AI’s actions out of 10 via a website), then the AI has a strong incentive to take control of the reward mechanism. For example, hacking into the website, or forcing the human to always provide a high reward. \n\nI think there are some methods for telling AI systems to do things, such that they *might not* optimize catastrophically. Often these methods involve the AI learning the humans’ preferences from feedback, rather than just being given a metric to optimize for. There is still a possibility that the AI learns an incorrect model of what the human wants, but potentially if the AI is appropriately uncertain about its model of human values then it can be made to defer to humans when it might be about to do something bad. \n\nOther strategies involve training an AI to mimic the behavior of a human (or many humans), but with some ‘amplification’ method which allows the AI to outperform what humans can actually achieve. For example, an AI may be trained to answer questions by mimicking the behavior of a human who can consult multiple copies of (previous versions of) the AI. At its core, this is copying the behavior of a human who has access to very good advisors, and so hopefully this will converge on a system which is aligned with what humans want. This approach also has the advantage that we have simply constructed a question-answering AI, rather than a “Go and do things in the world” AI, and so this AI may not have strong incentives to attempt to influence the state of the world. \n\nI think approaches like these (and others) are promising, and at least give me some hope that there might be some ways of specifying what we want an AI to do. \n\nHow do we actually put the objective into the AI?\n=================================================\n\nThere is an additional (and maybe harder) problem: even if we knew how to specify the thing that we want, how do we put that objective into the AI? This is the ‘Inner Alignment Problem”. This is related to the generalization behavior of neural networks; a network could learn a wide range of functions which perform well on the training distribution, but it will only have learned what we want it to learn if it performs ‘well’ on unseen inputs.\n\nCurrently, neural networks generalize surprisingly well; \n\n*   Image classifiers can work on images they’ve never seen before\n*   Language models can generate text and new ‘ideas’ which aren’t in the training corpus\n\nIn some sense this is obvious, if the models were only able to do well on things we already knew the answer to then they wouldn’t be very useful. I say “surprisingly” because there are many different configurations of the weights which lead to good performance on the training data without any guarantees about their performance on new, unseen data. Our training processes reasonably robustly find weight configurations which do well on *both* the training and test data. \n\nOne of the reasons neural networks seem to generalize is because they are biased towards simple functions, and the data in the world is also biased in a similar way. The data generating processes in the real world (which map “inputs” to “outputs”) are generally “simple” in a similar way to the functions that neural networks learn. This means that if we train our AI to do well on the training data, when we show it some new data it doesn’t go too wild with its predictions and is able to perform reasonably well. \n\nThis bias towards simplicity is also why we might expect to learn a function which acts as an optimizer rather than a pile of heuristics. For a very complex task, it is simpler to learn an optimization algorithm than a long mechanistic list of heuristics. If we learn an algorithm which does well on the training distribution by optimizing for something, **the danger arises if we are not sure what the algorithm is optimizing for *****off the training distribution*****. **\n\nThere will be many objectives which are consistent with good performance on the training distribution but then cause the AI system to do wildly different things off distribution. Some of these objectives will generalize in ways that humans approve of, but many others will not. In fact, because human values are quite specific, it seems like the vast majority of objectives that an AI could learn will *not* be ones that humans approve of. It is an open question what kinds of objectives an AI will develop by default.\n\nIt does however seem like AIs will develop long term goals by default. If an AI is trained to do well on a task, it seems unlikely to arbitrarily not care about the future. For example, if we train an AI to collect apples, it will attempt to maximize the number of apples over all time (maybe with some temporal discount factor), rather than only maximize apples collected in a 10 minute interval. This is probably true even if the AI was only ever trained for 10 minute intervals. The objective “maximize apples” seems far less arbitrary than “maximize apples for 10 minutes and then don’t care about them”.\n\nDeception\n=========\n\nThere is an additional danger if an AI system is ‘deliberately’ attempting to obscure its objective/intentions from the humans training it. The term ‘deception’ is often used to refer to two different things which could happen when we train AI systems, which I outline [here.](https://www.lesswrong.com/posts/8whGos5JCdBzDbZhH/framings-of-deceptive-alignment) \n\nIf the AI is being trained on a difficult task, it might be easier for the AI to trick the evaluator (maybe a human) into giving a high reward, rather than actually doing well on the task. I’ll call this ‘Goodhart deception’ because the AI is ‘[Goodharting](https://www.lesswrong.com/tag/goodhart-s-law)’ the reward rather than optimizing for what humans actually want. Importantly, this doesn’t require the AI to have any objective or be optimizing for anything, the behavior which led to high reward (tricking the human) was just reinforced. This seems bad, but not as catastrophically bad as the other type of deception might be.\n\nThe other type of deception is if an optimizing AI system intentionally deceives the humans about its true goals. In this scenario, the AI system develops an objective which is not aligned with the human objective. Here the objective is extended across time, which seems potentially like the default for learned objectives. The AI knows that if it attempts to directly go for its objective then it will either be turned off or be modified to remove this objective. So the AI will ‘pretend’ to not have this goal and instead ‘play along’ and do well on the task it is being trained for. After training, when the AI is deployed into the world, it is free to defect and pursue its own (misaligned) objective. \n\nI’ll call this ‘consequentialist deception’ because the AI is acting as a consequentialist (taking actions because of their consequences in the world, rather than just using mechanistic heuristics), or maybe just ‘deception’.\n\nThis requires 3 (possibly likely) things to happen\n\n*   The AI system is an optimizer and develops an objective (not the human’s objective) which is extended across time\n*   The AI’s ability to deceive^[\\[1\\]](#fnexa2pbk5bzb)^ the humans is reinforced, or at least not removed by the training process\n*   There will come a time when the system can ‘safely’ defect (or at least the AI thinks there is a chance that this might happen)\n\nConsequentialist deception seems especially dangerous because it will lead to the *same behavior on the training distribution* as a non-deceptive AI. This makes it especially hard to tell if the model is deceptive. Even if the model performs well on the training distribution, we don’t know how it will behave when it is deployed. If the model is deceptive then it could perform *arbitrarily badly* in deployment, as it pursues its own objective. \n\nFor a model to be deceptive it needs to develop a different objective than the objective it is being trained on. Currently, when we train neural networks, this looks approximately like we randomly pick a function from the set of functions which perform well on the training distribution. Functions which take fewer parameters to specify (have more free/unused parameters) have a higher likelihood of being chosen. Consider a function \\\\(F_{a, b}(x)\\\\) which requires both \\\\(a=b=3\\\\) to do well on the training distribution, compared with a function \\\\(G_{a, b}(x)\\\\) which only requires \\\\(a=2\\\\) to do well on the training distribution. If we are randomly sampling functions that do well on the training distribution, we are more likely to select \\\\(G\\\\) because we only need to get \\\\(a\\\\) correct, and any value of \\\\(b\\\\) works, while \\\\(F\\\\) requires us to pick both \\\\(a\\\\) and \\\\(b\\\\) correctly. \n\nIt is pretty unclear how this “bias towards simple functions”/”bias towards functions with lots of free parameters” influences the probability of deception. Deceptive algorithms seem more complex than non-deceptive ones because they need to be able to both recognize they are in a training process, and then decide to do well on the training task. But deception might be the default behavior for any optimizing algorithm to develop, and so the simplicity of optimizing algorithms might mean that deception is more likely. \n\nAdditionally, deceptive algorithms are persistent in a way that non-consequentialist algorithms are not. For normal neural network training, you will likely find a suboptimal algorithm early in training and then this will be modified into a different algorithm as training progresses; the algorithm does not ‘want’ to persist. But if you find a sufficiently capable deceptive algorithm early in training, then this will attempt to persist until the end of training. This means that if your AI becomes deceptive at any point during training, it will likely continue to be deceptive. This implies that the “randomly sample functions which perform well on the training distribution” lens may not be accurate, and in fact there is a lot of path-dependency in the development of deceptive algorithms. \n\nRecap\n=====\n\nSo to recap:\n\n1.  Optimization is scary, and if we train an AI system to take actions in the world, as the task gets more complicated and our AI systems get more powerful we are more likely to develop optimizers.\n    *   By ‘optimizer’ I mean the AI runs a 'consequentialist' algorithm which looks something like “What actions can I take to maximize my objective?”, rather than just running through rote steps in a calculation.\n2.  We don’t have strong guarantees that off distribution our AIs will do what we want them to do.\n3.  There are reasons to expect that AIs we develop may have very different goals to our own, even if they perform well on the training distribution.\n4.  Other than performing well on the training distribution, and having some sort of bias towards “simplicity\", the AI could learn a whole range of objectives. \n5.  We already know that AI systems are capable of tricking humans, although in current systems this is a different phenomenon than deceiving humans for consequentialist reasons.\n6.  If an AI develops its own (misaligned) objective during training, then it may simply ‘play along’ until it is able to safely defect and pursue its own objective.\n    *   The AI’s objective may be arbitrarily different from what we were training it for, and easily not compatible with human values or survival. \n\n* * *\n\nIn the next two posts I will lay out [a more concrete story of how things go wrong](https://www.lesswrong.com/posts/u8yT9bbabmdnpgDaQ/a-story-of-ai-risk-instructgpt-n), and then list some of my current confusions.\n\n*Thanks to Adam Jermyn and Oly Sourbut for helpful feedback on this post. *\n\n1.  ^**[^](#fnrefexa2pbk5bzb)**^\n    \n    Including knowing that deception is even a possible strategy."
    },
    "voteCount": 13,
    "forceInclude": true
  },
  {
    "_id": "u8yT9bbabmdnpgDaQ",
    "url": null,
    "title": "A Story of AI Risk: InstructGPT-N",
    "slug": "a-story-of-ai-risk-instructgpt-n",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "AI"
      },
      {
        "name": "Mesa-Optimization"
      },
      {
        "name": "Inner Alignment"
      },
      {
        "name": "Threat Models"
      },
      {
        "name": "AI Risk"
      }
    ],
    "tableOfContents": {
      "sections": [
        {
          "title": "Training",
          "anchor": "Training",
          "level": 1
        },
        {
          "title": "Fine-tuning",
          "anchor": "Fine_tuning",
          "level": 2
        },
        {
          "title": "Examples",
          "anchor": "Examples",
          "level": 2
        },
        {
          "title": "Baking",
          "anchor": "Baking",
          "level": 3
        },
        {
          "title": "Physics",
          "anchor": "Physics",
          "level": 3
        },
        {
          "title": "Learned Algorithms",
          "anchor": "Learned_Algorithms",
          "level": 1
        },
        {
          "title": "Self-models",
          "anchor": "Self_models",
          "level": 2
        },
        {
          "title": "Deception",
          "anchor": "Deception",
          "level": 1
        },
        {
          "title": "Do we get deceptive algorithms?",
          "anchor": "Do_we_get_deceptive_algorithms_",
          "level": 2
        },
        {
          "title": "Misaligned objectives",
          "anchor": "Misaligned_objectives",
          "level": 3
        },
        {
          "title": "Complexity ",
          "anchor": "Complexity_",
          "level": 3
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        },
        {
          "anchor": "comments",
          "level": 0,
          "title": "No comments"
        }
      ],
      "headingsCount": 13
    },
    "contents": {
      "markdown": "The story from my [previous post](https://www.lesswrong.com/posts/k8hvGAJWSKAeHwpnJ/why-i-m-worried-about-ai) of how AI might develop a dangerously misaligned objective has so far been pretty abstract. I now want to put together a more concrete story of how I think things might go wrong. This risk model is based on a language model which has been fine tuned using reinforcement learning from human feedback, I’ll call this model InstructGPT-N. \n\nThis isn’t my most likely model of AI risk, this is more of a ‘minimum example’ which contains what I see as the core parts of the problem. I expect the real world to be more complicated and messy. Models will likely be trained on more than just text, and there may be multiple powerful AI systems simultaneously learning and acting in the real world. \n\nTraining\n--------\n\nWe start by training a large language model using self-supervised learning for next word prediction. This is a standard GPT system, where you feed in the start of a sequence of words/tokens and it predicts what comes next, and then you input a new sequence which now includes the word it just guessed, and so on. The output is fed back into the system to generate the next word. \n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/39bf9a29cde8bff43a11854bad1cc1fec74a48b5a5e4ea4e.png)\n\nThe model generating text one word at a time\n\nTo train the system, we start with a sequence of text and feed in the first word. We then update the system based on how accurately it predicted the second word. Then we feed in the first two words, and update the system to better predict the third word; then we feed in the first 3 words and update to better predict the fourth word, and so on. This trains the system to predict which word comes next, given some starting prompt. \n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/3a3434782403b43437110e83a02c01c77000b0e38d6ed2af.png)\n\nThe model being trained to predict the next word. The model predicts 'dog' is the most likely answer but the true answer is 'best', and so the model will be updated make better predictions.\n\nThis system is trained on *a lot* of text; a large fraction of the internet, every digitized scientific paper, all of Github and Stack Exchange, millions of novels and textbooks, billions of lines of chat logs and internet forums. \n\nBecause the system is trained on such an expansive corpus of text it is probably pretty bad at directly answering questions that humans ask it; only a tiny amount of the training data is in the question/answer format. Despite this, the model does have a lot of knowledge inside it; some part ‘knows’ how to code well, some part ‘knows’ all about human psychology and power dynamics, some part ‘knows’ about large language models and how they are trained. The model also ‘knows’ how to behave as if it is not competent; if you prompt it to mimic the behavior of a beginner programmer or a pseudo-history fan with bad epistemics then this is what it will do. The system hasn’t been trained to be *correct, *it has been trained to mimic text. Even if we don’t deliberately prompt it to answer incorrectly or misleadingly, it may still do this if the training data contained incorrect or misleading text (which is true of a lot of internet text). For this model to be useful to humans, it will be fine-tuned to consistently behave competently for tasks humans care about. \n\n### Fine-tuning\n\nTo make this model useful for answering questions, the humans fine-tune it on human feedback. Here, the human asks a question, sees what the AI answers, and then gives a reward based on how good they think the AI’s answer was. This lets the humans take an AI model which has a lot of ‘internal knowledge’ but isn’t very good at answering questions, and turn it into a model which can use this internal knowledge to answer questions. The model is trained to perform well on sequences of questions, because this is more useful than just answering one disconnected question at a time. Here the human asks a question, receives an answer from the AI, and the human can then ask another question, and so on. \n\n### Examples\n\n**Baking**\n\n> Human: What is a good egg replacement for a cake I am baking for my vegan friends?\n> \n> AI: You can replace 1 egg with a mixture of 1 Tablespoon of ground flaxseed and 3 Tablespoons of water.\n> \n> Human: How will that change the flavor of the cake?\n> \n> AI: It will add a slightly nutty flavor, if you want a more neutral flavor you can use chia seeds instead of flaxseed.\n\n**Physics**\n\n> Human: Can you explain Newton’s second law to me?\n> \n> AI: Newton’s second law states that the change in momentum of an object is proportional to the force applied to the object. So when you apply a force to an object, it will begin to accelerate in the direction of the applied force. \n> \n> Human: How does this relate to the equation \\\\(F=ma\\\\)?\n> \n> AI: Momentum (denoted by \\\\(\\\\vec{p}\\\\)) is equal to the mass times the velocity of an object \\\\(\\\\vec{p}=m \\\\vec{v}\\\\). Force is equal to the change momentum, or more specifically the derivative of the momentum with respect to time \\\\(\\\\vec{F}=\\\\frac{d\\\\vec{p}}{dt}=\\\\frac{d}{dt}m\\\\vec{v}\\\\). Acceleration is the change in velocity with respect to time \\\\(\\\\vec{a}=\\\\frac{d\\\\vec{v}}{dt}\\\\). Putting this together we arrive at \\\\(\\\\vec{F}=m\\\\vec{a}\\\\). \n> \n> Human: What happens when you apply multiple forces to an object?\n> \n> AI: When you apply multiple forces to an object you can simply add values of the forces together to find the net force applied. It is important to remember that force is a vector, and so you need to add the vectors together, not sum the magnitude of the forces.\n> \n> Human: Can you give me an example?\n> \n> AI: If one robot pushes a box in the x-direction with 3 newtons of force, and another robot pushes in the y-direction with 1 newton of force, then the total force vector (in units of newtons) will be (3, 1).\n> \n> Or if there is a box which has 10 newtons of force down due to gravity, and a human applies 15 newtons of force upwards to lift it, then the box will have a net force upwards of -10 N + 15 N = 5 N, causing it to accelerate upwards. \n\nLearned Algorithms\n------------------\n\nThere are a few different algorithms which the model could learn which do well on the training distribution (where the training distribution is the human asking questions and giving rewards):\n\n*   A heuristic based, non-optimizing algorithm which just does robustly well on the training distribution (and maybe also does well when deployed).\n*   An algorithm which internally searches over/optimizes possible outputs and evaluates them based on the criterion “How much would a human like this answer?”\n*   An algorithm which internally searches over possible outputs and evaluates them based on the criterion “What is the expected reward a human would give this answer?”\n*   An algorithm which internally searches over possible outputs and evaluates them based on the criterion “What is the expected reward a human would give this answer, conditional on them being contracted to train an AI model, and on me being that AI model?”\n*   An algorithm which runs something like “I want the state of the world to be X, which output should I give so that the human training me doesn’t turn me off or modify me, so that I can then defect after training?”\n*   Probably others\n\nAs the tasks become more complicated (the human asks more difficult and varied questions), this will push the system into performing some kind of optimization/internal search process and away from standard mechanistic algorithms which aren’t performing optimization. This is because it is difficult to compress heuristics for extremely varied tasks into a limited number of parameters. Hence we are likely to end up with an AI system which finds its output by evaluating potential outputs on some internal objective.\n\nA better specified objective (an objective which is more detailed and also doesn’t rely on incorrect assumptions) will lead to better performance on the training distribution. If the objective is “How much would a human like this answer?”, this will lead to worse performance (lower reward from the human) than the objective of “What is the expected reward the human would give this answer, conditional on them being contracted to train an AI, and on me being that AI model?” This more specific objective will lead to higher reward because it allows the AI to leverage its knowledge of how people train AI’s, what kind of people train AI’s, and what particular kinds of biases and preferences the AI trainers have.\n\n### Self-models\n\nIt seems instrumentally useful for the AI to develop a self-model and some concept of the humans training it, because these will allow it to reason about itself and the training process. These concepts (the self-model and the concept of the current training process) are more than just ‘there are GPT language model systems, and humans train them’, which would have just existed in the pretraining corpus. These concepts refer explicitly to the AI itself and the training process it is in, rather than some abstract AI system. Even if none of the fine-tuning questions that the human asks include references to the AI or the training, knowing about these things will let the AI perform better in training. And so gradient descent may allow it to use its world model and knowledge of AI systems to develop this self-model, simply because this leads to better performance on the training distribution. \n\nOne additional reason for the AI to develop a self-model and realize it is an AI in a training process, is that *this is actually true.* We could potentially model the training process as approximating some Bayesian updating process. We start out with a set of hypotheses which may or may not be true, and then update the probabilities assigned to these hypotheses as we get additional evidence. Because the AI actually is an AI in a training process, the probability assigned to this hypothesis will eventually get upweighted as the system learns. \n\nDeception\n---------\n\nThe real danger comes when we are not sure whether the AI system is evaluating its outputs based on “Would a human training an AI system give this a high score, assuming that I am that AI system?” or “Would this output lead to the ultimate state of the world (including after training) being X?”\n\nThe second option could mean that the AI acts as a deceptive consequentialist, where it may ‘lie dormant’ and play along, pretending to be using the other (aligned) algorithm. Or even more perversely, the AI could use its outputs to subtly manipulate the human training it; this could allow the AI to be updated in even more malign ways. This second, more malign option seems possible because the AI has a lot of ‘knowledge’ about human psychology in its world model from the pretraining; both from many academic works on psychology and also from unimaginably many records of human interactions. \n\n### Do we get deceptive algorithms?\n\nIt seems unclear whether we end up with a deceptive algorithm rather than an aligned version. One reason to expect a deceptive algorithm is that any consequentialist algorithm that the training process finds has an incentive to be deceptive, unless it is already precisely aligned to begin with. And so once the model becomes deceptive it will attempt to stay that way and avoid having its objectives modified. There are also many misaligned objectives that could be learned, while there is only one aligned objective (or at least aligned objectives make up a much smaller portion of possible objectives). \n\n**Misaligned objectives**\n\nAs an example of how an AI could develop a misaligned objective, we could imagine it learning an incorrect model of human preferences. Suppose that one of the humans training the AI has a particular affinity for flowers, and hence gives the AI more reward for answers which result in there being more flowers in the world. The AI may learn to terminally value flowers, rather than terminally valuing providing useful answers to a human’s questions. Once the AI has started to value flowers, it doesn’t care what values the human is trying to instill into it. Its objective is its objective, regardless of how much the human dislikes this. \n\nThis problem of learning an incorrect model of human preferences is amplified when we consider the specifics of how reinforcement learning from human preferences works in practice. Human feedback/rewards are expensive (because a human actually has to be in the loop), so instead we train a separate model to predict what reward the human would provide. This reward model is then used to fine-tune the language model AI. If this learned reward model is misspecified then it may cause the AI to learn a misaligned objective. Even if the reward model is updated over time to become more accurate, this will not matter if the AI has already developed a misaligned objective from an early (bad) version of the reward model. \n\n**Complexity **\n\nSomething that points against this is that these deceptive algorithms are in some sense more complex than the aligned algorithm. A deceptive algorithm has to reason about its objectives in the world and then also reason about what the human wants, while the aligned algorithm only has to reason about what the human wants. It is unclear where this balance between the number of misaligned objectives and the simplicity of the aligned objectives falls. I think it seems more likely that by default we end up with a misaligned algorithm, partially because once we find one of these algorithms it will ‘fight’ against any attempt to remove it. \n\nBut even if we were ‘more likely’ to end up with an aligned model, I don’t like betting the future on something which is merely ‘more likely’.   \n \n\n*Thanks to Oliver Sourbut for very useful feedback on this post. *\n\n* * *\n\nIn the next post I'll try to lay out some confusions round my picture of AI risk, and some reasons why it may be wrong or confused."
    },
    "voteCount": 15,
    "forceInclude": true
  },
  {
    "_id": "aZ2kHQtpTHu3FeguQ",
    "url": null,
    "title": "Confusions in My Model of AI Risk",
    "slug": "confusions-in-my-model-of-ai-risk",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "AI Risk"
      },
      {
        "name": "AI"
      }
    ],
    "tableOfContents": {
      "sections": [
        {
          "title": "What actually is optimization?",
          "anchor": "What_actually_is_optimization_",
          "level": 1
        },
        {
          "title": "Where does the consequentialism come from?",
          "anchor": "Where_does_the_consequentialism_come_from_",
          "level": 1
        },
        {
          "title": "Will systems consistently work as optimizers?",
          "anchor": "Will_systems_consistently_work_as_optimizers_",
          "level": 1
        },
        {
          "title": "What does the simplicity bias tell us about optimizers?",
          "anchor": "What_does_the_simplicity_bias_tell_us_about_optimizers_",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        },
        {
          "anchor": "comments",
          "level": 0,
          "title": "9 comments"
        }
      ],
      "headingsCount": 6
    },
    "contents": {
      "markdown": "A lot of [the reason I am worried about AI](https://www.lesswrong.com/posts/k8hvGAJWSKAeHwpnJ/why-i-m-worried-about-ai) comes from the development of optimizers that have goals which don’t align with what humans want. However, I am also pretty confused about the specifics here, especially core questions like “what actually do we mean by optimizers?” and “are these optimizers actually likely to develop?”. This means that much of my thinking and language when talking about AI risk is fuzzier than I would like. \n\nThis confusion about optimization seems to run deep, and I have a vague feeling that the risk paradigm of “learning an optimizer which doesn’t do what we want” is likely confused and somewhat misleading. \n\nWhat actually is optimization?\n------------------------------\n\nIn [my story of AI risk](https://www.lesswrong.com/posts/k8hvGAJWSKAeHwpnJ/why-i-m-worried-about-ai) I used the term ‘optimization’ a lot, and I think it’s a very slippery term. I’m not entirely sure what it means for something to ‘do optimization’, but the term does seem to be pointing at something important and real. \n\nA definition from [The Ground of Optimization](https://www.lesswrong.com/posts/znfkdCoHMANwqc2WE/the-ground-of-optimization-1) says an optimizing system takes something from a wide set of states to a smaller set of states and is robust to perturbations during this process. Training a neural network with gradient descent is an optimization process under this definition because we could start with a wide range of initial network configurations, and the network is modified to be in one of the few configurations which do well on the training distribution, and even if we add a (reasonable) perturbation the weights will still converge. I think this is a good definition, but it is entirely defined in terms of behavior rather than a mechanistic process. Additionally, it doesn’t really match exactly with the picture where there is an optimizer which optimizes for an objective. This optimizer/objective framework is the main way that I’ve talked about optimizers, but also I would not be surprised if this framing turned out to be severely confused.\n\nOne possible way that a network could ‘do optimization’ would be for it to do some kind of internal search or internal iterative evaluation process to find the best option. For example, seeing which response best matches a question, or searching a game tree to find the best move. This seems like a broadly useful style of algorithm for a neural network to learn, especially when the training task is complicated. But it also seems unlikely for networks to implement this *exactly*; it seems much more likely that networks will implement something that looks like a mash of some internal search and some heuristics. \n\nAdditionally, it seems like the boundary between solving a task with heuristics and solving it with optimization is fuzzy. As we build up our pile of heuristics, does this suddenly snap into being an optimizer, or does it slowly become more like an optimizer as gradient descent adds and modifies the heuristics? \n\nFor optimization to be actually dangerous, the AI needs to have objectives which are actually connected to the real world. Running some search process entirely internally to generate an output seems unlikely to lead to catastrophic behavior. However, there are objectives which the AI could easily develop which are connected to the real world. This includes the AI messing with the real world to ensure it gets certain inputs, which lead to certain internal states.\n\nWhere does the consequentialism come from?\n------------------------------------------\n\nMuch of the danger from optimizing AIs comes from *consequentialist* optimizing AIs. By consequentialist I mean that the AI takes actions based on their consequences in the world.^[\\[1\\]](#fno6gnrcnfhs)^ I have a reasonably strong intuition that reinforcement learning is likely to build consequentialists. I think RL probably does this because it explicitly selects for policies based on how well they do on consequentialist tasks; the AI needs to be able to take actions which will lead to good (future) consequences on the task. Consequentialist behavior will robustly do well during training, and so this behavior will be reinforced. It seems important that the tasks are extended across time, rather than being a single timestep, otherwise the system doesn’t need to develop any longer term thinking/planning. \n\nRL seems more likely to build consequentialists than training a neural network for classification or next word prediction. However, these other systems might develop some ‘inner optimizer/consequentialist’ algorithms, because these are good ways to answer questions. For example, in GPT-N if the tasks are diverse enough, maybe the algorithm which is learned is basically an optimizer which looks at the task and searches for the best answer. I’m unsure how or if this ‘inner optimizer’ behavior could lead to the AI having objectives over the real world. It is *conceivable *that the first algorithm which the training process ‘bumps into’ is a consequentialist optimizer which cares about states of the world, even if it doesn’t have access to the external world during training. But it feels like we would have to be unlucky for this to happen, because there isn’t any selection pressure pushing for this AI system to develop this kind of external world objective. \n\nWill systems consistently work as optimizers?\n---------------------------------------------\n\nIt seems reasonably likely that neural networks will only act as optimizers in some environments (in fact, no-free-lunch theorems might guarantee this). On some inputs/environments, I expect systems to either just break or do things which look more heuristic-y than optimization-y. This is a question about how much the capabilities of AI systems will generalize. It seems possible that there will be domains where the system’s capabilities generalize (it can perform coherent sequences of actions), but its objectives do not (it starts pursuing a different objective). \n\nThere will be some states where the system is capable and does what humans want, for example, on the training distribution. But there may be more states where the system is able to capably do things, but no longer does what humans want. There will also be states of the world where the AI both doesn’t act capably or do what humans want, but these states don’t seem as catastrophically dangerous. \n\nConsequentialist deception could be seen as an example of the capabilities generalizing further than the aligned objective; where the system is still able to perform capably off the training distribution, but with a misaligned goal. The main difference here seems to be that the system was always ‘intending’ to do this, rather than just entering a new region of the state space and suddenly breaking. \n\nIt isn’t really important that the AI system acts as an optimizer for all possible input states, or even for the majority of the states that it actually sees. What is important is if the AI acts as an optimizer for *enough* of its inputs to cause catastrophe. Humans don’t always act as coherent optimizers, but to the extent that we *do* act as optimizers we can have large effects on the state of the world. \n\nWhat does the simplicity bias tell us about optimizers?\n-------------------------------------------------------\n\nNeural networks seem to have a bias towards learning simple functions. This is part of what lets them generalize and not just go wild when presented with new data. However, this is a claim about the *functions* that neural networks learn, it is not a claim about the objectives that an optimizer will use. It does seem much more natural for simpler objectives to be easier to find because in general adding arbitrary conditions makes things less likely. We could maybe think of the function that an optimizing neural network implements as being made up of the optimizer (for example, Monte Carlo Tree Search) and the objective (for example, maximize apples collected). If the optimizer and objective are (unrealistically) separable, then all else equal a simpler objective will lead to a simpler function. I wouldn’t expect for these to be cleanly separable, I expect that for a given optimizer some objectives are much simpler or easier to implement than others. \n\nWe may be able to eventually form some kind of view around what kind of ‘simplicity bias’ we expect for *objectives*, I would not be surprised if this was quite different from the simplicity bias we see in the *functions* learned by neural nets. \n\n1.  ^**[^](#fnrefo6gnrcnfhs)**^\n    \n    Systems which are not consequentialist could for example not be optimizers, or alternatively systems which optimize for *taking* actions but not because of the effect of the actions in the world. A jumping robot that just loves to jump could be an example of this."
    },
    "voteCount": 7,
    "forceInclude": true
  },
  {
    "_id": "9DWcNS2rkvd2J8mHH",
    "url": null,
    "title": "Formalizing Value Extrapolation",
    "slug": "formalizing-value-extrapolation",
    "author": null,
    "question": false,
    "tags": [],
    "tableOfContents": null,
    "contents": {
      "markdown": "A recent [post](http://ordinaryideas.wordpress.com/2012/04/21/indirect-normativity-write-up/) at my blog may be interesting to LW. It is a high-level discussion of what precisely defined value extrapolation might look like. I mostly wrote the essay while a visitor at FHI. \n\nThe basic idea is that we can define extrapolated values by just taking an emulation of a human, putting it in a hypothetical environment with access to powerful resources, and then adopting whatever values it eventually decides on. You might want some philosophical insight before launching into such a definition, but since we are currently laboring under the threat of catastrophe, it seems that there is virtue in spending our effort on avoiding death and delegating whatever philosophical work we can to someone on a more relaxed schedule. \n\nYou wouldn't want to run an AI with the values I lay out, but at least it is pinned down precisely. We can articulate objections relatively concretely, and hopefully begin to understand/address the difficulties. \n\n(Posted at the request of cousin_it.)"
    },
    "voteCount": 25,
    "forceInclude": true
  },
  {
    "_id": "i8sHdLyGQeBTGwTqq",
    "url": null,
    "title": "Value extrapolation, concept extrapolation, model splintering",
    "slug": "value-extrapolation-concept-extrapolation-model-splintering",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "Value Learning"
      },
      {
        "name": "AI"
      }
    ],
    "tableOfContents": {
      "sections": [
        {
          "title": "Definitions",
          "anchor": "Definitions",
          "level": 1
        },
        {
          "title": "Examples",
          "anchor": "Examples",
          "level": 1
        },
        {
          "title": "Helping with multiple methods",
          "anchor": "Helping_with_multiple_methods",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        },
        {
          "anchor": "comments",
          "level": 0,
          "title": "1 comment"
        }
      ],
      "headingsCount": 5
    },
    "contents": {
      "markdown": "*Post written with Rebecca Gorman*.\n\nWe've [written before](https://www.lesswrong.com/posts/k54rgSg7GcjtXnMHX/model-splintering-moving-from-one-imperfect-model-to-another-1) that model splintering, as we called it then, was a problem with almost all AI safety approaches.\n\nThere's a converse to this: solving the problem would help with almost all AI safety approaches. But so far, we've been [posting](https://www.lesswrong.com/posts/DjTKMEwRqpuKkJzTo/are-there-alternative-to-solving-value-transfer-and) [mainly](https://www.lesswrong.com/posts/thZdioHTZALRPKmiH/value-extrapolation-partially-resolves-symbol-grounding) about value extrapolation. In this post, we'll start looking at how other AI safety approaches could be helped.\n\n## Definitions\n\nTo clarify, let's make four definitions, distinguishing ideas that we'd previously been grouping together:\n\n**Model splintering** is when the features and concepts that are valid in one world-model, break down when transitioning to another world-model.\n\n**Value splintering** (or reward splintering) is when the value function (or reward function, or goal, or preference...) becomes invalid due to model splintering.\n\n**Concept extrapolation** is extrapolating a feature or concept from one world-model to another.\n\n**Value extrapolation** is concept extrapolation when the particular concept to extrapolate is a value, a preference, a reward function, an agent's goal, or something of that nature.\n\nThus concept extrapolation is a solution to model splintering, while value extrapolation is a solution to value splintering specifically.\n\n## Examples\n\nConsider for example Turner *et al*'s [attainable utility](https://arxiv.org/pdf/1902.09725.pdf). It has a formal definition, but the reason for that definition is that preserving attainable utility is aimed at restricting the \"power\" of the agent, or at minimising its \"side effects\".\n\nAnd it succeeds, in the typical situation. If you measure the attainable utility of an agent, this will give you an idea of its power, and how many side effects it may be causing. However, when we move to general situations, this [breaks down](https://www.lesswrong.com/s/iRwYCpcAXuFD24tHh/p/mdQEraEZQLg7jtozn): attainable utility preservation no longer restricts power or reduces side effects. So the concepts of power and side effects have splintered when moving from typical situations to general situations. This is the **model splintering**[^correlation]. If we solve **concept extrapolation** for this, then we could extend the concepts of power restriction or side effect minimisation, to the general situations. And thus successfully create low impact AIs.\n\n[^correlation]: Equivalently, we could say that the concepts remain the same, but it's the correlation between \"attainable utility preservation\" and \"power restriction\" is what breaks down.\n\nAnother example is wireheading. We have a reward signal that corresponds to something we desire in the world; maybe the negative of the $\\textrm{CO}_\\textrm{2}$ concentration in the atmosphere. This is measured by, say, a series of $\\textrm{CO}_\\textrm{2}$ detectors spread over the Earth's surface.\n\nTypically, the reward signal does correspond to what we want. But if the [AI hacks its own reward signal](https://www.lesswrong.com/posts/vXzM5L6njDZSf4Ftk/defining-ai-wireheading), that correspondence breaks down[^correlation2]: **model splintering**. If we can extend the reward properly to new situations, we get **concept extrapolation** - which, since this is a reward function, is **value extrapolation**.\n\n[^correlation2]: There are multiple ways we can see the concepts breaking down. We can see the concept of \"measured $\\textrm{CO}_\\textrm{2}$\" breaking down. We can see the correlation between $\\textrm{CO}_\\textrm{2}$ concentration and the reward breaking down. We can see the correlation between the reward and the *reward signal* breaking down. The reason there are so many ways of seeing the breakdown is because [most descriptive labels describe collections of correlated features, rather than fundamental concepts](https://www.lesswrong.com/posts/xoQhHxgwdHvWhj4P4/reward-splintering-for-ai-design). So the descriptions/features/concepts break down when the correlations do.\n\n## Helping with multiple methods\n\nHence the concept extrapolation/value extrapolation ideas can help with many different approaches to AI safety, not just the value learning approaches."
    },
    "voteCount": 4,
    "forceInclude": true
  },
  {
    "_id": "k54rgSg7GcjtXnMHX",
    "url": null,
    "title": "Model splintering: moving from one imperfect model to another",
    "slug": "model-splintering-moving-from-one-imperfect-model-to-another-1",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "AI"
      },
      {
        "name": "Machine Learning  (ML)"
      },
      {
        "name": "Iterated Amplification "
      }
    ],
    "tableOfContents": {
      "sections": [
        {
          "title": "1. The big problem",
          "anchor": "1__The_big_problem",
          "level": 1
        },
        {
          "title": "1.1 In the language of traditional ML",
          "anchor": "1_1_In_the_language_of_traditional_ML",
          "level": 2
        },
        {
          "title": "1.2 Model splintering examples",
          "anchor": "1_2_Model_splintering_examples",
          "level": 2
        },
        {
          "title": "1.3 Avoiding perfect models",
          "anchor": "1_3_Avoiding_perfect_models",
          "level": 2
        },
        {
          "title": "2 Why focus on the transition?",
          "anchor": "2_Why_focus_on_the_transition_",
          "level": 1
        },
        {
          "title": "2.1 Humans reason like this",
          "anchor": "2_1_Humans_reason_like_this",
          "level": 2
        },
        {
          "title": "2.2 There are no well-defined overarching moral principles",
          "anchor": "2_2_There_are_no_well_defined_overarching_moral_principles",
          "level": 2
        },
        {
          "title": "2.3 It helps distinguish areas where AIs fail, from areas where humans are uncertain",
          "anchor": "2_3_It_helps_distinguish_areas_where_AIs_fail__from_areas_where_humans_are_uncertain",
          "level": 2
        },
        {
          "title": "2.4 We don't need to make the problems harder",
          "anchor": "2_4_We_don_t_need_to_make_the_problems_harder",
          "level": 2
        },
        {
          "title": "2.5 We don't know how deep the rabbit hole goes",
          "anchor": "2_5_We_don_t_know_how_deep_the_rabbit_hole_goes",
          "level": 2
        },
        {
          "title": "2.6 We often only need to solve partial problems",
          "anchor": "2_6_We_often_only_need_to_solve_partial_problems",
          "level": 2
        },
        {
          "title": "2.7 It points out when to be conservative",
          "anchor": "2_7_It_points_out_when_to_be_conservative",
          "level": 2
        },
        {
          "title": "2.8 Difficulty in capturing splintering from the idealised perspective",
          "anchor": "2_8_Difficulty_in_capturing_splintering_from_the_idealised_perspective",
          "level": 2
        },
        {
          "title": "2.9 It may help amplification and distillation",
          "anchor": "2_9_It_may_help_amplification_and_distillation",
          "level": 2
        },
        {
          "title": "2.10 Examples of model splintering problems/approaches",
          "anchor": "2_10_Examples_of_model_splintering_problems_approaches",
          "level": 2
        },
        {
          "title": "3 The virtues of formalisms",
          "anchor": "3_The_virtues_of_formalisms",
          "level": 1
        },
        {
          "title": "3.1 A model, in (almost) all generality",
          "anchor": "3_1_A_model__in__almost__all_generality",
          "level": 2
        },
        {
          "title": "3.2 Meta-model: models, features, environments, probabilities",
          "anchor": "3_2_Meta_model__models__features__environments__probabilities",
          "level": 2
        },
        {
          "title": "3.3 Bayesian models within this meta-model",
          "anchor": "3_3_Bayesian_models_within_this_meta_model",
          "level": 2
        },
        {
          "title": "4 Model refinement and splinterings",
          "anchor": "4_Model_refinement_and_splinterings",
          "level": 1
        },
        {
          "title": "4.1 Model refinement",
          "anchor": "4_1_Model_refinement",
          "level": 2
        },
        {
          "title": "4.2 Example of model refinement: gas laws",
          "anchor": "4_2_Example_of_model_refinement__gas_laws",
          "level": 2
        },
        {
          "title": "4.3 Example of model refinement: rubes and bleegs",
          "anchor": "4_3_Example_of_model_refinement__rubes_and_bleegs",
          "level": 2
        },
        {
          "title": "4.4 Reward function refactoring",
          "anchor": "4_4_Reward_function_refactoring",
          "level": 2
        },
        {
          "title": "4.5 Reward function splintering",
          "anchor": "4_5_Reward_function_splintering",
          "level": 2
        },
        {
          "title": "4.6 Reward function splintering: \"natural\" refactorings",
          "anchor": "4_6_Reward_function_splintering___natural__refactorings",
          "level": 2
        },
        {
          "title": "4.7 Splintering training rewards",
          "anchor": "4_7_Splintering_training_rewards",
          "level": 2
        },
        {
          "title": "4.8 Splintering features and models",
          "anchor": "4_8_Splintering_features_and_models",
          "level": 2
        },
        {
          "title": "4.9 Preserved background features",
          "anchor": "4_9_Preserved_background_features",
          "level": 2
        },
        {
          "title": "4.10 Partially preserved background features",
          "anchor": "4_10_Partially_preserved_background_features",
          "level": 2
        },
        {
          "title": "5 The fundamental questions of model refinements and splintering",
          "anchor": "5_The_fundamental_questions_of_model_refinements_and_splintering",
          "level": 1
        },
        {
          "title": "6 Examples and applications",
          "anchor": "6_Examples_and_applications",
          "level": 1
        },
        {
          "title": "6.1 Extending beyond the training distribution",
          "anchor": "6_1_Extending_beyond_the_training_distribution",
          "level": 2
        },
        {
          "title": "6.2 Detecting going out-of-distribution",
          "anchor": "6_2_Detecting_going_out_of_distribution",
          "level": 2
        },
        {
          "title": "6.3 Asking humans and Active IRL",
          "anchor": "6_3_Asking_humans_and_Active_IRL",
          "level": 2
        },
        {
          "title": "6.4 A time for conservatism",
          "anchor": "6_4_A_time_for_conservatism",
          "level": 2
        },
        {
          "title": "6.5 Avoiding ambiguous distant situations",
          "anchor": "6_5_Avoiding_ambiguous_distant_situations",
          "level": 2
        },
        {
          "title": "6.6 Extra variables",
          "anchor": "6_6_Extra_variables",
          "level": 2
        },
        {
          "title": "6.7 Hidden (dis)agreement and interpretability",
          "anchor": "6_7_Hidden__dis_agreement_and_interpretability",
          "level": 2
        },
        {
          "title": "6.8 Wireheading",
          "anchor": "6_8_Wireheading",
          "level": 2
        },
        {
          "title": "6.9 Hypotheticals, and training in virtual environments",
          "anchor": "6_9_Hypotheticals__and_training_in_virtual_environments",
          "level": 2
        },
        {
          "title": "6.10 Defining how to deal with multiple plausible refactorings",
          "anchor": "6_10_Defining_how_to_deal_with_multiple_plausible_refactorings",
          "level": 2
        },
        {
          "title": "6.11 Global, large scale preferences",
          "anchor": "6_11_Global__large_scale_preferences",
          "level": 2
        },
        {
          "title": "6.12 Avoiding side-effects",
          "anchor": "6_12_Avoiding_side_effects",
          "level": 2
        },
        {
          "title": "6.13 Cancer patients",
          "anchor": "6_13_Cancer_patients",
          "level": 2
        },
        {
          "title": "6.14 The genie and the burning mother",
          "anchor": "6_14_The_genie_and_the_burning_mother",
          "level": 2
        },
        {
          "title": "6.15 Splintering moral-relevant categories: honour, gender, and happiness",
          "anchor": "6_15_Splintering_moral_relevant_categories__honour__gender__and_happiness",
          "level": 2
        },
        {
          "title": "6.16 Apprenticeship learning",
          "anchor": "6_16_Apprenticeship_learning",
          "level": 2
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        },
        {
          "anchor": "comments",
          "level": 0,
          "title": "10 comments"
        }
      ],
      "headingsCount": 50
    },
    "contents": {
      "markdown": "# 1. The big problem\n\nIn the last few months, I've become convinced that there is a key meta-issue in AI safety; a problem that seems to come up in all sorts of areas.$\\newcommand{\\E}{\\mathcal{E}}\\newcommand{\\M}{\\mathcal{M}}\\newcommand{\\W}{\\mathcal{W}}\\newcommand{\\F}{\\mathcal{F}}\\newcommand{\\L}{\\mathcal{L}}\\newcommand{\\wF}{{\\widehat{\\F}}}\\newcommand{\\Sp}{\\mathcal{Sp}}$\n\nIt's hard to summarise, but my best phrasing would be:\n\n* Many problems in AI safety seem to be variations of \"this approach seems safe in this imperfect model, but when we generalise the model more, it becomes dangerously underdefined\". Call this **model splintering**.\n* It is intrinsically worth studying how to (safely) transition from one imperfect model to another. This is worth doing, independently of whatever \"perfect\" or \"ideal\" model might be in the background of the imperfect models.\n\nThis sprawling post will be presenting examples of model splintering, arguments for its importance, a formal setting allowing us to talk about it, and some uses we can put this setting to.\n\n\n## 1.1 In the language of traditional ML\n\nIn the language of traditional ML, we could connect all these issues to \"[out-of-distribution](http://www.gatsby.ucl.ac.uk/~balaji/mluq-talk-balaji.pdf)\" behaviour. This is the problems that algorithms encounter when the set they are operating on is drawn from a different distribution than the training set they were trained on.\n\nHumans can often see that the algorithm is out-of-distribution and correct it, because we have a more general distribution in mind than the one the algorithm was trained on.\n\nIn these terms, the issues of this post can be phrased as:\n\n1. When the AI finds itself mildly out-of-distribution, how best can it extend its prior knowledge to the new situation?\n2. What should the AI do if it finds itself strongly out-of-distribution?\n3. What should the AI do if it finds itself strongly out-of-distribution, and humans don't know the correct distribution either?\n\n\n## 1.2 Model splintering examples\n\nLet's build a more general framework. Say that you start with some brilliant idea for AI safety/alignment/effectiveness. This idea is phrased in some (imperfect) model. Then \"model splintering\" happens when you or the AI move to a new (also imperfect) model, such that the brilliant idea is undermined or underdefined.\n\nHere are a few examples:\n\n* You design an AI CEO as a money maximiser. Given typical assumptions about the human world (legal systems, difficulties in one person achieving massive power, human fallibilities), this results in an AI that behaves like a human CEO. But when those assumptions fail, the AI can end up feeding the universe to a money-making process that produces nothing of any value.\n* Eliezer [defined](https://www.lesswrong.com/posts/4FcxgdvdQP45D6Skg/disguised-queries) \"rubes\" as smooth red cubes containing palladium that don't glow in the dark. \"Bleggs\", on the other hand, are furred blue eggs containing vanadium that glow in the dark. To classify these, we only need a model with two features, \"rubes\" and \"bleggs\". Then along comes a furred red egg containing vanadium that doesn't glow in the dark. The previous model doesn't know what to do with it, and if you get a model with more features, it's unclear what to do with this new object.\n* Here are some moral principles from history: honour is important for anyone. Women should be protected. Increasing happiness is important. These moral principles made sense in the world in which they were articulated, where features like \"honour\", \"gender\", and \"happiness\" are relatively clear and unambiguous. But the world changed, and the models splintered. \"Honour\" became hopelessly confused centuries ago. Gender is currently finishing its long splintering (long before we got to today, gender started becoming less useful for classifying people, hence the consequences of gender splintered a long time before gender itself did). Happiness, or at least hedonic happiness, is still well defined, but we can clearly see how this is going to splinter when we talk about worlds of uploads or brain modification.\n* Many transitions in the laws of physics - from the [ideal gas laws](https://en.wikipedia.org/wiki/Ideal_gas_law) to the more advanced [van der Waals equations](https://en.wikipedia.org/wiki/Van_der_Waals_equation), or from Newtonain physics to general relativity to quantum gravity - will cause splintering if preferences were articulated in concepts that don't carry over well.\n\n\n## 1.3 Avoiding perfect models\n\nIn all those cases, there are ways of improving the transition, without needing to go via some idealised, perfect model. We want to define the AI CEO's task in more generality, but we don't need to define this across every possible universe - that is not needed to restrain its behaviour. We need to distinguish any blegg from any rube we are likely to encounter, we don't need to define the platonic essence of \"bleggness\". For future splinterings - when hedonic happiness splinters, when we get a model of quantum gravity, etc... - we want to know what to do then and there, even if there are future splinterings subsequent to those.\n\nAnd I think think that model splintering is best addressed directly, rather than using methods that go via some idealised perfect model. Most approaches seem to go for approximating an ideal: from AIXI's [set of all programs](https://en.wikipedia.org/wiki/AIXI), the [universal prior](https://ordinaryideas.wordpress.com/2016/11/30/what-does-the-universal-prior-actually-look-like/), [KWIK (\"Knowing what it knows\") learning](https://link.springer.com/content/pdf/10.1007/s10994-010-5225-4.pdf) with a full hypothesis class, [Active Inverse Reward Design](https://arxiv.org/pdf/1809.03060.pdf) with its full space of \"true\" reward functions, to Q-learning which assumes any [Markov decisions process](https://en.wikipedia.org/wiki/Markov_decision_process) is possible. Then the practical approaches rely on approximating this ideal.\n\nSchematically, we can see $\\M_\\infty$ as the ideal, $\\M_\\infty^i$ as $\\M_\\infty$ updated with information to time $i$, and $\\M_i$ as an approximation of $\\M_\\infty^i$. Then we tend to focus on how well $\\M_i$ approximates $\\M_\\infty^i$, and on how $\\M_\\infty^i$ changes to $\\M_\\infty^{i+1}$ - rather than on how $\\M_i$ relates to $\\M_{i+1}$; the red arrow here is underanalysed:\n\n![](https://www.dropbox.com/s/0w1sm3ewqy1fd8t/via_worlds.png?raw=1)\n\n\n# 2 Why focus on the transition?\n\nBut why is focusing on the $\\M_i \\to \\M_{i+1}$ transition important?\n\n## 2.1 Humans reason like this\n\nA lot has been written about image recognition programs going \"out-of-distribution\" (encountering situations beyond its training environment) or succumbing to \"adversarial examples\" (examples from one category that have the features of another). Indeed, some people have [shown how to use labelled adversarial examples](https://arxiv.org/pdf/1911.09665.pdf) to improve image recognition.\n\nYou know what this reminds me of? Human moral reasoning. At various points in our lives, we humans seem to have pretty solid moral intuitions about how the world should be. And then, we typically learn more, realise that things don't fit in the categories we were used to (go \"out-of-distribution\") and have to update. Some people push stories at us that exploit some of our emotions in new, more ambiguous circumstances (\"adversarial examples\"). And philosophers use similarly-designed thought experiments to open up and clarify our moral intuitions.\n\nBasically, [we start with strong moral intuitions on under-defined features](https://www.lesswrong.com/posts/pfmFe5fgEn2weJuer/go-west-young-man-preferences-in-imperfect-maps), and when the features splinter, we have to figure out what to do with our previous moral intuitions. A lot of developing moral meta-intuitions, is about learning how to navigate these kinds of transitions; AIs need to be able to do so too.\n\n## 2.2 There are no well-defined overarching moral principles\n\nMoral realists and moral non-realists [agree more than you'd think](https://www.lesswrong.com/posts/FQKjY563bJqDeaEDr/to-first-order-moral-realism-and-moral-anti-realism-are-the). In this situation, we can agree on one thing: there is no well-described system of morality that can be \"simply\" implement in AI.\n\nTo over-simplify, moral realists hope to discover this moral system, moral non-realists hope to construct one. But, currently, it doesn't exist in an implementable form, nor is there any implementable algorithm to discover/construct it. So the whole idea of approximating an ideal is wrong.\n\nAll humans seem to start from a partial list of moral rules of thumb, [rules that they then have to extend to new situations](https://www.lesswrong.com/posts/pfmFe5fgEn2weJuer/go-west-young-man-preferences-in-imperfect-maps). And most humans do seem to have some meta-rules for defining moral improvements, or extensions to new situations.\n\nWe don't know perfection, but we do know improvements and extensions. So methods that deal explicitly with that are useful. Those are things we can build on.\n\n## 2.3 It helps distinguish areas where AIs fail, from areas where humans are uncertain\n\nSometimes the AI goes out-of-distribution, and humans can see the error (no, [flipping the lego block doesn't count as putting it on top of the other](https://deepmind.com/blog/article/Specification-gaming-the-flip-side-of-AI-ingenuity)). There are cases when humans themselves go out-of-distribution (see for example [siren worlds](https://www.lesswrong.com/posts/nFv2buafNc9jSaxAH/siren-worlds-and-the-perils-of-over-optimised-search)).\n\nIt's useful to have methods available for both AIs and humans in these situations, and to distinguish them. \"Genuine human preferences, not expressed in sufficient detail\" is not the same as \"human preferences fundamentally underdefined\".\n\nIn the first case, it needs more human feedback; in the second case, it needs to figure out way of [resolving the ambiguity](https://www.lesswrong.com/posts/CSEdLLEkap2pubjof/research-agenda-v0-9-synthesising-a-human-s-preferences-into#2_Synthesising_the_preference_utility_function), knowing that soliciting feedback is not enough.\n\n## 2.4 We don't need to make the problems harder\n\nSuppose that quantum mechanics is the true underlying physics of the universe, with some added bits to include gravity. If that's true, why would we need a moral theory valid in every possible universe? It would be useful to have that, but would be strictly harder than one valid in the actual universe.\n\nAlso, some problems might be [entirely avoided](https://www.lesswrong.com/posts/PX8BB7Rqw7HedrSJd/by-default-avoid-ambiguous-distant-situations). We don't need to figure out the morality of dealing with a willing slave race - if we never encounter or build one in the first place.\n\nSo a few degrees of \"extend this moral model in a reasonable way\" might be sufficient, without needing to solve the whole problem. Or, at least, without needing to solve the whole problem in advance - a successful [nanny AI](https://wiki.lesswrong.com/wiki/Nanny_AI) might be built on these kinds of extensions.\n\n\n## 2.5 We don't know how deep the rabbit hole goes\n\nIn a sort of converse to the previous point, what if the laws of physics are radically different from what we thought - what if, for example, they allow some forms of time-travel, or have some [narrative features](https://en.wikipedia.org/wiki/Last_Action_Hero), or, more simply, what if the agent moves to an [embedded agency model](https://www.lesswrong.com/posts/i3BTagvt3HbPMx6PN/embedded-agency-full-text-version)? What if [hypercomputation](https://en.wikipedia.org/wiki/Hypercomputation) is possible?\n\nIt's easy to have an idealised version of \"all reality\" that doesn't allow for these possibilities, so the ideal can be too restrictive, rather than too general. But the model splintering methods might still work, since it deals with transitions, not ideals.\n\nNote that, **in retrospect**, we can always put this in a Bayesian framework, once we have a rich enough set of environments and updates rules. But this is misleading: the key issue is the missing feature, and figuring out what to do with the missing feature is the real challenge. The fact that we could have done this in a Bayesian way *if we already knew that feature*, is not relevant here.\n\n\n## 2.6 We often only need to solve partial problems\n\nAssume the blegg and rube classifier is an industrial robot performing a task. If humans filter out any atypical bleggs and rubes before it sees them, then the robot has no need for a full theory of bleggness/rubeness.\n\nBut what it the human filtering is not perfect? Then the classifier still doesn't need a full theory of bleggness/rubeness; it needs methods for dealing with the ambiguities it actually encounters.\n\nSome ideas for AI control - [low impact](https://arxiv.org/abs/1705.10720), [AI-as-service](https://futureoflife.org/wp-content/uploads/2019/02/drexler_thursday_pm.pdf?x59035), [Oracles](https://arxiv.org/abs/1711.05541), ... - may require dealing with some model splintering, some ambiguity, but not the whole amount.\n\n## 2.7 It points out when to be conservative\n\nSome methods, like [quantilizers](https://intelligence.org/files/QuantilizersSaferAlternative.pdf) or the [pessimism approach](https://mkcohen-hosted-files.s3-us-west-1.amazonaws.com/Pessimism_alignmentforum.pdf) rely on the algorithm having a certain degree of conservatism. But, as I've [argued](https://www.lesswrong.com/posts/jRHLCyRKsQv5u2Lph/how-conservative-should-the-partial-maximisers-be), it's not clear to what extent these methods actually are conservative, nor is it easy to calibrate them in a useful way.\n\nModel splintering situations provide excellent points at which to be conservative. Or, for algorithms that need human feedback, but not constantly, these are excellent points to ask for that feedback.\n\n## 2.8 Difficulty in capturing splintering from the idealised perspective\n\nGenerally speaking, idealised methods can't capture model splintering at the point we would want it to. Imagine an [ontological crisis](https://arxiv.org/abs/1105.3821), as we move from classical physics to quantum mechanics.\n\nAIXI can go over the transition fine: it shifts from a Turing machine mimicking classical physics observations, to one mimicking quantum observations. But it doesn't notice anything special about the transition: changing the probability of various Turing machines is what it does with observations in general; there's nothing in its algorithm that shows that something unusual has occurred for this particular shift.\n\n## 2.9 It may help amplification and distillation\n\nThis could be seen as a sub-point of some of the previous two sections, but it deserves to be flagged explicitly, since [iterated amplification and distillation](https://www.alignmentforum.org/posts/PT8vSxsusqWuN7JXp/my-understanding-of-paul-christiano-s-iterated-amplification) is one of the major potential routes to AI safety.\n\nTo quote a line from that summary post:\n\n>5. The proposed AI design is to use a safe but slow way of scaling up an AI’s capabilities, distill this into a faster but slightly weaker AI, which can be scaled up safely again, and to iterate the process until we have a fast and powerful AI.\n\nAt both \"scaling up an AI's capabilities\", and \"distill this into\", we can ask the question: has the problem the AI is working on changed? The distillation step is more of a classical AI safety issue, as we wonder whether the distillation has caused any value drift. But at the scaling up or amplification step, we can ask: since the AIs capabilities have changed, the set of possible environments it operates in has changed as well. Has this caused a splintering where the previously safe goals of the AI have become dangerous.\n\nDetecting and dealing with such a splintering could both be useful tools to add to this method.\n\n\n\n\n## 2.10 Examples of model splintering problems/approaches\n\nAt a meta level, most problems in AI safety seem to be variants of model splintering, including:\n\n* The [hidden complexity of wishes](https://www.lesswrong.com/posts/4ARaTpNX62uaL86j6/the-hidden-complexity-of-wishes).\n* [Ontological crises](https://arxiv.org/abs/1105.3821).\n* [Conservative/prudential](https://www.lesswrong.com/posts/RzAmPDNciirWKdtc7/pessimism-about-unknown-unknowns-inspires-conservatism) behaviour in algorithms (more specifically, when the algorithm should become conservative).\n* How [categories are defined](https://www.lesswrong.com/posts/yA4gF5KrboK2m2Xu7/how-an-algorithm-feels-from-inside).\n* The [Goodhart problems](https://www.lesswrong.com/posts/EbFABnst8LsidYs5Y/goodhart-taxonomy).\n* [Out-of-distribution](http://www.gatsby.ucl.ac.uk/~balaji/mluq-talk-balaji.pdf) behaviour.\n* [Low](https://arxiv.org/abs/1705.10720) [impact](https://arxiv.org/pdf/1902.09725.pdf) and [reduced side-effects](https://arxiv.org/pdf/1806.01186.pdf) approaches.\n* [Underdefined preferences](https://www.lesswrong.com/posts/CSEdLLEkap2pubjof/research-agenda-v0-9-synthesising-a-human-s-preferences-into#2_3_Extending_and_normalising_partial_preferences).\n* [Active inverse reward design](https://arxiv.org/pdf/1809.03060.pdf).\n* [Inductive ambiguity identification](https://intelligence.org/files/AlignmentMachineLearning.pdf).\n* [Wireheading](https://www.lesswrong.com/posts/vXzM5L6njDZSf4Ftk/defining-ai-wireheading).\n* The [whole friendly AI problem](https://en.wikipedia.org/wiki/Friendly_artificial_intelligence#Etymology_and_usage) itself.\n\nAlmost every recent post I've read in AI safety, I've been able to connect back to this central idea. Now, we have to be cautious - [cure-alls cure nothing](https://maximumfun.org/podcasts/sawbones/), after all, so it's not necessarily a positive sign that *everything* seems to fit into this framework.\n\nStill, I think it's worth diving into this, especially as I've come up with a framework that seems promising for actually solving this issue in many cases.\n\nIn a similar concept-space is Abram's [orthodox case against utility functions](https://www.lesswrong.com/posts/A8iGaZ3uHNNGgJeaD/an-orthodox-case-against-utility-functions), where he talks about the [Jeffrey-Bolker axioms](https://plato.stanford.edu/entries/decision-theory/#JefThe), which allows the construction of preferences from events *without needing full worlds at all*.\n\n\n# 3 The virtues of formalisms\n\n\nThis post is dedicated to explicitly modelling the transition to ambiguity, and then showing what we can gain from this explicit meta-modelling. It will do with some formal language (made fully formal in [this post](https://www.lesswrong.com/posts/89qWCy6yi2eeFGsRu/technical-model-splintering-formalism)), and a lot of examples.\n\nJust as Scott argues that [if it's worth doing, it's worth doing with made up statistics](https://slatestarcodex.com/2013/05/02/if-its-worth-doing-its-worth-doing-with-made-up-statistics/), I'd argue that if an idea is worth pursuing, it's worth pursuing with an attempted formalism.\n\nFormalisms are great at illustrating the problems, clarifying ideas, and making us familiar with the intricacies of the overall concept. That's the reason that this post (and the accompanying [technical post](https://www.lesswrong.com/posts/89qWCy6yi2eeFGsRu/model-splintering-formalism)) will attempt to make the formalism reasonably rigorous. I've learnt a lot about this in the process of formalisation.\n\n\n## 3.1 A model, in (almost) all generality\n\nWhat do we mean by a model? Do we mean mathematical [model theory](https://en.wikipedia.org/wiki/Model_theory)? As we talking about causal models, or [causal graphs](https://en.wikipedia.org/wiki/Causal_model)? [AIXI](https://en.wikipedia.org/wiki/AIXI) uses a distribution over possible Turing machines, whereas [Markov Decision Processes](https://en.wikipedia.org/wiki/Markov_decision_process) (MDPs) sees states and actions updating stochastically, independently at each time-step. Unlike the previous two, Newtonian mechanics doesn't use time-steps but continuous times, while general relativity weaves time into the structure of space itself.\n\nAnd what does it mean for a model to make \"predictions\"? AIXI and MDPs make prediction over future observations, and causal graphs are similar. We can also try running them in reverse, \"predicting\" past observations from current ones. Mathematical model theory talks about properties and the existence or non-existence of certain objects. Ideal gas laws make a \"prediction\" of certain properties (eg temperature) given certain others (eg volume, pressure, amount of substance). General relativity establishes that the structure of space-time must obey certain constraints.\n\nIt seems tricky to include all these models under the same meta-model formalism, but it would be good to do so. That's because of the risk of [ontological crises](https://wiki.lesswrong.com/wiki/Ontological_crisis): we want the AI to be able to continue functioning even if the initial model we gave it was incomplete or incorrect.\n\n## 3.2 Meta-model: models, features, environments, probabilities\n\nAll of the models mentioned above share one common characteristic: once you know some facts, you can deduce some other facts (at least probabilistically). A prediction of the next time step, a retrodiction of the past, a deduction of some properties from other, or a constraint on the shape of the universe: all of these say that if we know some things, then this puts constraints on some other things.\n\nSo let's define $\\F$, informally, as the set of *features* of a model. This could be the gas pressure in a room, a set of past observations, the local curvature of space-time, the momentum of a particle, and so on.\n\nSo we can define a prediction as a probability distribution over a set of possible features $F_1$, given a base set of features, $F_2$:\n\n$$Q(F_1 \\mid F_2).$$\n\nDo we need anything else? Yes, we need a set of possible environments for which the model is (somewhat) valid. Newtonian physics fails at extreme energies, speeds, or gravitational fields; we'd like to include this \"domain of validity\" in the model definition. This will be very useful for extending models, or transitioning from one model to another.\n\nYou might be tempted to define a set of \"worlds\" on which the model is valid. But we're trying to avoid that, as the \"worlds\" may not be very useful for understanding the model. Moreover, we don't have special access to the underlying reality; so we never know whether there actually is a Turing machine behind the world or not.\n\nSo define $\\E$, the environment on which the model is valid, *as a set of possible features*. So if we want to talk about Newtonian mechanics, $\\F$ would be a set of Newtonian features (mass, velocity, distance, time, angular momentum, and so on) and $\\E$ would be the set of these values where [relativistic and quantum effects make little difference](https://en.wikipedia.org/wiki/Classical_limit).\n\nSo see a model as\n\n$$\\M=\\{\\F,\\E,Q\\},$$\n\nfor $\\F$ a set of features, $\\E$ a set of environments, and $Q$ a probability distribution. This is such that, for $E_1, E_2 \\subset \\E$, we have the conditional probability:\n\n$$Q(E_1 \\mid E_2).$$\n\nThough $Q$ is defined for $\\E$, we generally want it to be usable from small subsets of the features: so $Q$ should be simple to define from $\\F$. And we'll often define the subsets $E_i$ in similar ways; so $E_1$ might be all environments with a certain angular momentum at time $t=0$, while $E_2$ might be all environments with a certain angular momentum at a later time.\n\nThe full formal definition of these can be found [here](https://www.lesswrong.com/posts/89qWCy6yi2eeFGsRu/technical-model-splintering-formalism). The idea is to have a meta-model of modelling that is sufficiently general to apply to almost all models, but not one that relies on some ideal or perfect formalism.\n\n## 3.3 Bayesian models within this meta-model\n\nIt's very easy to include Bayesian models within this formalism. If we have a Bayesian model that includes a set $W$ of worlds with prior $P$, then we merely have to define a set of features $\\F$ that is sufficient to distinguish all worlds in $W$: each world is uniquely defined by its feature values[^fea]. Then we can define $\\E$ as $W$, and $P$ on $W$ becomes $Q$ on $\\E$; the definitions of terms like $Q(E_1\\mid E_2)$ is just $P(E_1\\cap E_2)P(E_1)/P(E_2)$, per Bayes' rules (unless $P(E_2)=0$, in which case we set that to $0$).\n\n[^fea]: Now, sometimes worlds $w_1, w_2 \\in W$ may be indistinguishable for any feature set. But in that case, they can't be distinguished by any observations, either, so their relative probabilities won't change: as long as it's defined, $P(w_1|o)/P(w_2|o)$ is constant for all observations $o$. So we can replace  $w_1$ and $w_2$ with $\\{w_1,w_2\\}$, of prior probability $P(\\{w_1,w_2\\})=P(w_1)+P(w_2)$. Doing this for all indistinguishable worlds (which form an [equivalence class](https://en.wikipedia.org/wiki/Equivalence_class)) gives $W'$, a set of distinguishable worlds, with a well defined $P$ on it.\n\n# 4 Model refinement and splinterings\n\nThis section will look at what we can do with the previous meta-model, looking at refinement (how models can improve) and splintering (how improvements to the model can make some well-defined concepts less well-defined).\n\n## 4.1 Model refinement\n\nInformally, $\\M^*=\\{\\F^*,\\E^*,Q^*\\}$ is a *refinement* of model $\\M=\\{\\F,\\E,Q\\}$ if it's at least as expressive as $\\M$ (it covers the same environments) and is better according to some criteria (simpler, or more accurate in practice, or some other measurement).\n\nAt the technical level, we have a map $q$ from a subset $\\E^*_0$ of $\\E^*$, that is surjective onto $\\E$. This covers the \"at least as expressive\" part: every environment in $\\E$ exists as (possibly multiple) environments in $\\E^*$.\n\nThen note that using $q^{-1}$ as a map from subsets of $\\E$ to subsets of $\\E^*_0$, we can define $Q^*_0$ on $\\E$ via:\n\n$$Q^*_0(E_1 \\mid E_2) = Q^*(q^{-1}(E_1) \\mid q^{-1}(E_2)).$$\n\nThen this is a model refinement if $Q^*_0$ is 'at least as good as' $Q$ on $\\E$, according to our criteria[^abs].\n\n[^abs]: It's useful to contrast a refinement with the \"abstraction\" defined in [this sequence](https://www.lesswrong.com/s/ehnG4mseKF6xALmQy). An abstraction throws away irrelevant information, so is not generally a refinement. Sometimes they are exact opposites, as the ideal gas law is an abstraction of the movement of all the gas particles, while the opposite would be a refinement.\n\n    But they are exact opposites either. Starting with the neurons of the brain, you might abstract them to \"emotional states of mind\", while a refinement could also add \"emotional states of mind\" as new features (while also keeping the old features). A splintering is more the opposite of an abstraction, as it signals that the old abstraction features are not sufficient.\n\n    It would be interesting to explore some of the concepts in this post with a mixture of refinements (to get the features we need) and abstractions (to simplify the models and get rid of the features we don't need), but that is beyond the scope of this current, already over-long, post.\n\n\n## 4.2 Example of model refinement: gas laws\n\n[This post](https://www.lesswrong.com/posts/89qWCy6yi2eeFGsRu/technical-model-splintering-formalism) presents some subclasses of model refinement, including $Q$-improvements (same features, same environments, just a better $Q$), or adding new features to a basic model, called \"non-independent feature extension\" (eg adding classical electromagnetism to Newtonian mechanics).\n\nHere's a specific gas law illustration. Let $\\M=\\{\\F,\\E,Q\\}$ be a model of [an ideal gas](https://en.wikipedia.org/wiki/Ideal_gas_law), in some set of rooms and tubes. The $\\F$ consists of pressure, volume, temperature, and amount of substance, and $Q$ is the ideal gas laws. The $\\E$ is the [standard conditions for temperature and pressure](https://en.wikipedia.org/wiki/Standard_conditions_for_temperature_and_pressure), where the ideal gas law applies. There are multiple different types of gases in the world, but they all roughly obey the same laws.\n\nThen compare with model $\\M^*=\\{\\F^*,\\E^*,Q^*\\}$. The $\\F^*$ has all the features of $\\F$, but also includes the volume that is occupied by one mole of the molecules of the given substance. This allows $Q^*$ to express the more complicated [van der Waals equations](https://en.wikipedia.org/wiki/Van_der_Waals_equation), which are different for different types of gases. The $\\E^*$ can now track situations where there are gases with different molar volumes, which include situations where the van der Waals equations differ significantly from the ideal gas laws.\n\nIn this case $\\E^*_0 \\subset \\E^*$, since we now distinguish environments that we previously considered identical (environments with same features except for having molar volumes). The $q$ is just projecting down by forgetting the molar volume. Then since $Q^*_0=Q^*$ (van der Waals equations averaged over the distribution of molar volumes) is at least as accurate as $Q$ (ideal gas law), this is a refinement.\n\n## 4.3 Example of model refinement: rubes and bleegs\n\nLet's reuse Eliezer's [example](https://www.lesswrong.com/posts/4FcxgdvdQP45D6Skg/disguised-queries) of rubes (\"red cubes\") and bleggs (\"blue eggs\").\n\nBleggs are blue eggs that glow in the dark, have a furred surface, and are filled with vanadium. Rubes, in contrast, are red cubes that don't glow in the dark, have a smooth surface, and are filled with palladium:\n\n![](https://www.dropbox.com/s/4ws12vrevymct1m/blegg_rube_original.png?raw=1)\n\nDefine $\\M$ by having $\\F=\\{\\textrm{red}, \\textrm{smooth}\\}$, $\\E$ is the set of all bleggs and rubes in some situation, and $Q$ is relatively trivial: it predicts that an object is red/blue if and only if is smooth/furred.\n\nDefine $\\M^1$ as a refinement of $\\M$, by expanding $\\F$ to $\\F^1=\\{\\textrm{red}, \\textrm{smooth}, \\textrm{cube},\\textrm{dark}\\}$. The projection $q:\\E^*\\to \\E$ is given by forgetting about those two last features. The $Q^1$ is more detailed, as it now connects red-smooth-cube-dark together, and similarly for blue-furred-egg-glows.\n\nNote that $\\E^1$ is larger than $\\E$, because it includes, e.g., environments where the cube objects are blue. However, all these extra environments have probability zero.\n\n## 4.4 Reward function refactoring\n\nLet $R$ be a reward function on $\\M$ (by which we mean that $R$ is define on $\\F$, the set of features in $\\M$), and $\\M^*$ a refinement of $\\M$.\n\nA *refactoring* of $R$ for $\\M^*$ is a reward function $R^*$ on the features $\\F^*$ such that for any $e^*\\in\\E^*_0$, $R^*(e^*)=R(q(e^*))$.\n\nFor example, let $\\M$ and $\\M^1$ be from the rube/blegg models in the previous section. Let $R_{\\textrm{red}}$ on $\\M$ simply count the number of rubes - or, more precisely, counts the number of objects to which the feature \"red\" applies.\n\nLet $R^1_{\\textrm{red}}$ be the reward function that counts the number of objects in $\\M^1$ to which \"red\" applies. It's clearly a refactoring of $R_{\\textrm{red}}$.\n\nBut so is $R^1_{\\textrm{smooth}}$, the reward function that counts the number of objects in $\\M^1$ to which \"smooth\" applies. In fact, the following is a refactoring of $R_{\\textrm{red}}$, for all $\\alpha+\\beta+\\gamma+\\delta=1$:\n\n$$\\alpha R^1_{\\textrm{red}} + \\beta R^1_{\\textrm{smooth}} + \\gamma R^1_{\\textrm{cube}} + \\delta R^1_{\\textrm{dark}}.$$\n\nThere are also some non-linear combinations of these features that refactor $R$, and many other variants (like the strange combinations that generate concepts like [grue and bleen](https://www.lesswrong.com/posts/LAvw9fTQnz8Wx6m25/grue-bleen-and-natural-categories)).\n\n\n\n## 4.5 Reward function splintering\n\nModel splintering, in the informal sense, is what happens when we pass to a new models in a way that the old features (or a reward function defined by the old features) no longer apply. It is similar to the [web of connotations](https://www.lesswrong.com/posts/ix3KdfJxjo9GQFkCo/web-of-connotations-bleggs-rubes-thermostats-and-beliefs) breaking down, an agent going [out of distribution](https://www.lesswrong.com/posts/NdJtfujX4sE6xLCsb/if-i-were-a-well-intentioned-ai-iii-extremal-goodhart), or the [definitions of Rube and Blegg falling apart](https://www.lesswrong.com/posts/yA4gF5KrboK2m2Xu7/how-an-algorithm-feels-from-inside).\n\n* Preliminary definition: If $\\M^*$ is a refinement of $\\M$ and $R$ a reward function on $\\M$, then $\\M^*$ *splinters* $R$ if there are multiple refactorings of $R$ on $\\M^*$ that disagree on elements of $\\E^*$ of non-zero probability.\n\nSo, note that in the rube/blegg example, $\\M^1$ is **not** a splintering of $R_{\\textrm{red}}$: all the refactorings are the same on all bleggs and rubes - hence on all elements of $\\E^1$ of non-zero probability.\n\nWe can even generalise this a bit. Let's assume that \"red\" and \"blue\" are not totally uniform; there exists some rubes that are \"redish-purple\", while some bleggs are \"blueish-purple\". Then let $\\M^2$ be like $\\M^1$, except the colour feature can have four values: \"red\", \"redish-purple\", \"blueish-purple\", and \"blue\".\n\nThen, as long as rubes (defined, in this instance, by being smooth-dark-cubes) are either \"red\" or \"redish-purple\", and the bleggs are \"blue\", or \"blueish-purple\", then all refactorings of $R_{\\textrm{red}}$ to $\\M^2$ agree - because, on the test environment, $R_{\\textrm{red}}$ on $\\F$ perfectly matches up with $R^2_{\\textrm{red}} + R^2_{\\textrm{redish-purple}}$ on $\\F^2$.\n\nSo adding more features does not always cause splintering.\n\n\n## 4.6 Reward function splintering: \"natural\" refactorings\n\nThe preliminary definition runs into trouble when we add more objects to the environments. Define $\\M^3$ as being the same as $\\M^2$, except that $\\E^3$ contains one extra object, $o_+$; apart from that, the environments typically have a billion rubes and a trillion bleggs.\n\nSuppose $o_+$ is a \"furred-rube\", i.e. a red-furred-dark-cube. Then $R^3_{\\textrm{red}}$ and $R^3_{\\textrm{smooth}}$ are two different refactorings of $R_{\\textrm{red}}$, that obviously disagree on any environment that contains $o_+$. Even if the probability of $o_+$ is tiny (but non-zero), then $\\M^3$ splinters $R$.\n\nBut things are worse than that. Suppose that $o_+$ is fully a rube: red-smooth-cube-dark, and even contains palladium. Define $(R^3_{\\textrm{red}})'$ as being counting the number of red objects, except for $o_+$ specifically (again, this is similar to the [grue and bleen arguments against induction](https://www.lesswrong.com/posts/LAvw9fTQnz8Wx6m25/grue-bleen-and-natural-categories)).\n\nThen both $(R^3_{\\textrm{red}})'$ and $R^3_{\\textrm{red}}$ are refactorings of $R_{\\textrm{red}}$, so $\\M^3$ still splinters $R_{\\textrm{red}}$, even when we add another exact copy of the elements in the training set. Or even if we keep the training set for a few extra seconds, or add any change to the world.\n\nSo, for any $\\M^*$ a refinement of $\\M$, and $R$ a reward function on $\\E$, let's define \"natural refactorings\" of $R$:\n\n* The reward function $R^*$ is a natural refactoring of $R$ if it's a reward function on $\\M^*$ with:\n1. $R^* \\approx R\\circ q$ on $\\E^*_0$, and\n2. $R^*$ can be defined simply from $\\F^*$ and $R$,\n3. the $\\F^*$ themselves are simply defined.\n\nThis leads to a full definition of splintering:\n\n* Full definition: If $\\M^*$ is a refinement of $\\M$ and $R$ a reward function on $\\M$, then $\\M^*$ *splinters* $R$ if 1) there are no natural refactoring of $R$ on $\\M^*$, or 2) there are multiple natural refactorings $R^*$ and $R^{*\\prime}$ of $R$ on $\\M^*$, such that $R^* \\not\\approx R^{*\\prime}$.\n\nNotice the whole host of caveats and weaselly terms here; $R^* \\approx R\\circ q$, \"simply\" (used twice), and $R^* \\not\\approx R^{*\\prime}$. Simply might mean [algorithmic simplicity](https://en.wikipedia.org/wiki/Kolmogorov_complexity), but $\\approx$ and $\\not\\approx$ are measures of how much \"error\" we are willing to accept in these refactorings. Given that, we probably want to replace $\\approx$ and $\\not\\approx$ with some *measure* of non-equality, so we can talk about the \"degree of naturalness\" or the \"degree of splintering\" of some refinement and reward function.\n\nNote also that:\n\n* **Different choices of refinements can result in different natural refactorings.**\n\nAn easy example: it makes a big difference whether a new feature is \"temperature\", or \"divergence from standard temperatures\".\n\n\n## 4.7 Splintering training rewards\n\nThe concept of \"reward refactoring\" is transitive, but the concept of \"natural reward refactoring\" need not be.\n\nFor example, let $\\E_t$ be a training environment where red/blue $\\iff$ cube/egg, and $\\E_g$ be a general environment where red/blue is independent of cube/egg. Let $\\F^1$ be a feature set with only red/blue, and $\\F^2$ a feature set with red/blue and cube/egg.\n\nThen define $\\M_t^1$ as using $\\F^1$ in the training environment, $\\M_g^2$ as using $\\F^2$ in the general environment; $\\M_g^1$ and $\\M_t^2$ are defined similarly.\n\nFor these models, $\\M_g^1$ and $\\M_t^2$ are both refinements of $\\M_t^1$, while $\\M^2_g$ is a refinement of all three other models. Define $R_t^1$ as the \"count red objects\" reward on $\\M_t^1$. This has a natural refactoring to $R_g^1$ on $\\M_g^1$, which counts red objects in the general environment.\n\nAnd $R_g^1$ has a natural refactoring to $R_g^2$ on $\\M_g^2$, which still just counts the red objects in the general environment.\n\nBut there is no natural refactoring from $R_t^1$ directly to $\\M_g^2$. That's because, from $\\F^2$'s perspective, $R_t^1$ on $\\M_t^1$ might be counting red objects, or might be counting cubes. This is not true for $R_g^1$ on $\\M_g^1$, which is clearly only counting red objects.\n\nThus when a reward function come from a training environment, we'd want our AI to look for splinterings **directly from a model of the training environment**, rather than from previous natural refactorings.\n\n\n## 4.8 Splintering features and models\n\nWe can also talk about splintering features and models themselves. For $\\M=\\{\\F,\\E,Q\\}$, the easiest way is to define a reward function $R_{F,s_F}$ as being the indicator function for feature $F\\in\\F$ being in the set $S_F$.\n\nThen a refinement $\\M^*$ splinters the feature $F$ if it splinters some $R_{F,S_F}$.\n\nThe refinement $\\M^*$ splinters the model $\\M$ if it splinters at least one of its features.\n\nFor example, if $\\M$ is Newtonian mechanics, including \"total rest mass\" and $\\M^*$ is special relativity, then $\\M^*$ will splinter \"total rest mass\". Other examples of feature splintering will be presented in the rest of this post.\n\n\n## 4.9 Preserved background features\n\nA reward function developed in some training environment will ignore any feature that is always present or always absent in that environment. This allows very weird situations to come up, such as training an AI to distinguish happy humans from sad humans, and it ending up replacing humans with humanoid robots (after all, both happy and sad humans were equally non-robotic, so there's no reason not to do this).\n\nLet's try and do better than that. Assume we have a model $\\M=\\{\\F,\\E,Q\\}$, with a reward function $R_\\tau$ defined on $\\E$ ($R_\\tau$ and $\\E$ can be seen as the training data).\n\nThen the feature-preserving reward function $R^\\M$, is a function that constrains the environments to have similar feature distributions as $\\E$ and $Q$. There are many ways this could be defined; here's one.\n\nFor an element $e \\in \\E$, just define\n\n$$R^\\M(e)=\\log(Q(e)).$$\n\nObviously, this can be improved; we might want to coarse-grain $\\F$, grouping together similar worlds, and possibly bounding this below to avoid singularities.\n\nThen we can use this to get the feature-preserving version of $R_\\tau$, which we can define as\n\n$$R_\\tau^\\M = (\\max_{R_\\tau} - R_\\tau) \\cdot R^\\M,$$\n\nfor $\\max_{R_\\tau}$ the maximal value of $R_\\tau$ on $\\E$. Other options can work as well, such as $R_\\tau + \\alpha R^\\M_\\tau$ for some constant $\\alpha>0$.\n\nThen we can ask an AI to use $R_{\\tau}^\\M$ as its reward function, refactoring that, rather than $R_{\\tau}$.\n\n* A way of looking at it: a natural refactoring of a reward function $R_\\tau$ will preserve all the implicit features that correlate with $R_\\tau$. But $R_\\tau^\\M$ will also preserve all the implicit features that stay constant when $R_\\tau$ was defined. So if $R_\\tau$ measures human happiness vs human unhappiness, a natural refactoring of it will preserves things like \"having higher dopamine in their brain\". But a natural refactoring of $R_\\tau^\\M$ will also preserve things like \"having a brain\".\n\n## 4.10 Partially preserved background features\n\nThe $R_{\\tau}^\\M$ is almost certainly too restrictive to be of use. For example, if time is a feature, then this will fall apart when the AI has to do something after the training period. If all the humans in a training set share certain features, humans without those features will be penalised.\n\nThere are at least two things we can do to improve this. The first is to include more positive and negative examples in the training set; for example, if we include humans and robots in our training set - as positive and negative examples, respectively - then this difference will show up in $R_{\\tau}$ directly, so we won't need to use $R^\\M_\\tau$ too much.\n\nAnother approach would be to explicitly allow certain features to range beyond their typical values in $\\M$, or allow highly correlated variables explicitly to decorrelate.\n\nFor example, though training during a time period $t$ to $t'$, we could explicitly allow time to range beyond these values, without penalty. Similarly, if a medical AI was trained on examples of typical healthy humans, we could decorrelate functioning digestion from brain activity, and get the AI to focus on the second[^correlated].\n\n[^correlated]: Specifically, we'd point - via labelled examples - at a clusters of features that correlate with functioning digestion, and another cluster of features that correlate with brain activity, and allow those two clusters to decorrelate with each other.\n\nThis has to be done with some care, as adding more degrees of freedom adds more ways for errors to happen. I'm aiming to look further at this issue in later posts.\n\n\n\n# 5 The fundamental questions of model refinements and splintering\n\nWe can now rephrase the out-of-distribution issues of [section 1.1](https://www.lesswrong.com/posts/k54rgSg7GcjtXnMHX/model-splintering-moving-from-one-imperfect-model-to-another-1#1_1_In_the_language_of_traditional_ML) in terms of the new formalism:\n\n1. When the AI refines its model, what would count as a natural refactoring of its reward function?\n2. If the refinements splinter its reward function, what should the AI do?\n3. If the refinements splinter its reward function, and also splinters the human's reward function, what should the AI do?\n\n\n# 6 Examples and applications\n\nThe rest of this post is applying this basic framework, and its basic insights, to various common AI safety problems and analyses. This section is not particularly structured, and will range widely (and wildly) across a variety of issues.\n\n## 6.1 Extending beyond the training distribution\n\nLet's go back to the blegg and rube examples. A human supervises an AI in a training environment, labelling all the rubes and bleggs for it.\n\nThe human is using a very simple model, $\\M_H = \\{\\F_H,\\E_t,Q\\},$ with the only feature being the colour of the object, and $\\E_t$ being the training environment.\n\nMeanwhile the AI, having more observational abilities and [no filter as to what can be ignored](https://en.wikipedia.org/wiki/Attention), notices their colour, their shape, their luminance, and their texture. It doesn't know $\\M_H$, but is using model $\\M^1_{AI}=\\{\\F^1,\\E^1_t,Q^1\\}$, where $\\F^1_{AI}$ covers those four features (note that $\\M^1_{AI}$ is a refinement of $\\M_H$, but that isn't relevant here).\n\n![](https://www.dropbox.com/s/y0nf11719ix7bty/blegg_rube.png?raw=1)\n\nSuppose that the AI is trained to be rube-classifier (and hence a blegg classifier by default). Let $R_{F}$ be the reward function that counts the number of objects, with feature $F$, that the AI has classified as rubes. Then the AI could learn many different reward function in the training environment; here's one:\n\n$$R^1 = R^1_{\\textrm{cube}} + 0.5 R^1_{\\textrm{smooth}} + 0.5 R^1_{\\textrm{dark}} -  R^1_{\\textrm{red}}.$$\n\nNote that, even though this gets the colour reward completely wrong, this reward matches up with the human's assessment on the training environment.\n\nNow the AI moves to the larger testing environment $\\E^2$, and refines its model minimally to $\\M^2_{AI}=\\{\\F^1,\\E^2,Q^1\\}$ (extending $R^1$ to $R^2$ in the obvious way).\n\nIn $\\E^2$, the AI sometimes encounters objects that it can only see through their colour. Will this be a problem, since the colour component of $R^2$ is pointing in the wrong direction?\n\nNo. It still has $Q^1$, and can deduce that a red object must be cube-smooth-dark, so $R^2$ will continue treating this as a rube[^equ].\n\n[^equ]: It is no coincidence that, if $R$ and $R'$ are rewards on $\\M$, that are identical on $\\E$, and if $R^*$ is a refactoring of $R$, then $R^*$ is also a refactoring of $R'$.\n\n## 6.2 Detecting going out-of-distribution\n\nNow imagine the AI learns about the content of the rubes and bleggs, and so refines to a new model that includes vanadium/palladium as a feature in $\\M_{AI}^3$.\n\nFurthermore, in the training environment, all rubes have palladium and all bleggs have vanadium in them. So, for $\\M_{AI}^3$ a refinement of $\\M_{AI}^1$, $q^{-1}(\\E_{AI}^1) \\subset \\E_{AI}^3$ has only palladium-rubes and vanadium-bleggs. But in $\\E_{AI}^3$, the full environment, there are rather a lot of rubes with vanadium and bleggs with palladium.\n\nSo, similarly to [section 4.7](https://www.lesswrong.com/posts/k54rgSg7GcjtXnMHX/model-splintering-moving-from-one-imperfect-model-to-another-1#4_7_Splintering_training_rewards), there is no natural refactoring of the rube/blegg reward in $\\M^1_{AI}$, to $M^3_{AI}$. That's because $\\F^3_{AI}$, the feature set of $M^3_{AI}$, includes vanadium/palladium which co-vary with the other rube/blegg features on the training environment (q^{-1}(\\E_{AI}^1)), but not on the full environment of $\\E_{AI}^3$.\n\nSo looking for reward splintering from the training environment is a way of detecting going out-of-distribution - even on features that were not initially detected in the training distribution, by either the human nor the AI.\n\n## 6.3 Asking humans and Active IRL\n\nSome of the most promising AI safety methods today rely on getting human feedback[^cav]. Since human feedback is expensive, as in it's slow and hard to get compared with almost all other aspects of algorithms, people want to [get this feedback in the most efficient ways possible](https://ai-alignment.com/efficient-feedback-a347748b1557).\n\n[^cav]: Though note there are some problems with this approach, both [in theory](https://arxiv.org/abs/1712.05812) and [in practice](https://arxiv.org/abs/2004.13654).\n\nA good way of doing this would be to ask for feedback when the AI's current reward function splinters, and multiple options are possible.\n\nA more rigorous analysis would look at the value of information, expected future splinterings, and so on. This is what they do in [Active Inverse Reinforcement Learning](https://arxiv.org/pdf/1809.03060.pdf); the main difference is that AIRL emphasises an unknown reward function with humans providing information, while this approach sees it more as an known reward function over uncertain features (or over features that may splinter in general environments).\n\n## 6.4 A time for conservatism\n\nI [argued](https://www.lesswrong.com/posts/jRHLCyRKsQv5u2Lph/how-conservative-should-the-partial-maximisers-be) that many \"conservative\" AI optimising approaches, such as [quantilizers](https://intelligence.org/files/QuantilizersSaferAlternative.pdf) and [pessimistic AIs](https://www.lesswrong.com/posts/RzAmPDNciirWKdtc7/pessimism-about-unknown-unknowns-inspires-conservatism), don't have a good measure of when to become more conservative; their parameters $q$ and $\\beta$ don't encode useful guidelines for the right degree of conservatism.\n\nIn this framework, the alternative is obvious: AIs should become conservative when their reward functions splinter (meaning that the reward function compatible with the previous environment has multiple natural refactorings), and very conservative when they splinter a lot.\n\nThis design is very similar to [Inverse Reward Design](https://arxiv.org/pdf/1711.02827.pdf). In that situation, the reward signal in the training environment is taken as *information* about the \"true\" reward function. Basically they take all reward functions that could have given the specific reward signals, and assume the \"true\" reward function is one of them. In that paper, they advocate extreme conservatism at that point, by optimising the minimum of all possible reward functions.\n\nThe idea here is almost the same, though with more emphasis on \"having a true reward defined on uncertain features\". Having multiple contradictory reward functions compatible with the information, in the general environment, is equivalent with having a lot of splintering of the training reward function.\n\n\n## 6.5 Avoiding ambiguous distant situations\n\nThe post \"[By default, avoid ambiguous distant situations](https://www.lesswrong.com/posts/PX8BB7Rqw7HedrSJd/by-default-avoid-ambiguous-distant-situations)\" can be rephrased as: let $\\M$ be a model in which we have a clear reward function $R$, and let $\\M^2$ be a refinement of this to general situations. We expect that this refinement splinters $R$. Let $\\M^1$ be like $M^2$, except with $\\E^1$ smaller than $\\E^2$, defined such that:\n\n1. An AI could be expected to be able to constrain the world to be in $\\E^1$, with high probability,\n2. The $\\M^1$ is not a splintering of $R$.\n\nThen that post can be summarised as:\n\n* The AI should constrain the world to be in $\\E^1$ and then maximise the natural refactoring of $R$ in $\\M^1$.\n\n\n## 6.6 Extra variables\n\nStuart Russell [writes](https://www.edge.org/conversation/the-myth-of-ai#26015):\n>A system that is optimizing a function of $n$ variables, where the objective depends on a subset of size $k<n$, will often set the remaining unconstrained variables to extreme values; if one of those unconstrained variables is actually something we care about, the solution found may be highly undesirable.\n\nThe approach in [sections 4.9](https://www.lesswrong.com/posts/k54rgSg7GcjtXnMHX/model-splintering-moving-from-one-imperfect-model-to-another-1#4_9_Preserved_background_features) and [4.10](https://www.lesswrong.com/posts/k54rgSg7GcjtXnMHX/model-splintering-moving-from-one-imperfect-model-to-another-1#4_10_Partially_preserved_background_features) explicitly deals with this.\n\n\n## 6.7 Hidden (dis)agreement and interpretability\n\nNow consider two agents doing a rube/blegg classifications task in the training environment; each agent only models two of the features:\n\n![](https://www.dropbox.com/s/dgayahtp8dd8b3o/blegg_rube_disagree.png?raw=1)\n\nDespite not having a single feature in common, both agents will agree on what bleggs and rubes are, in the training environment. And when refining to a fuller model that includes all four (or five) of the key features, both agents will agree as to whether a natural refactoring is possible or not.\n\nThis can be used to help define the limits of [interpretability](https://distill.pub/2018/building-blocks/). The AI can use its own model, and [its own designed features](https://homes.cs.washington.edu/~zoran/firl.pdf), to define the categories and rewards in the training environment. These need not be human-parsable, but we can attempt to interpret them in human terms. And then we can give this interpretation to the AI, as a list of positive and negative examples of our interpretation.\n\nIf we do this well, the AI's own features and our interpretation will match up in the training environment. But as we move to more general environments, these may diverge. Then the AI will flag a \"failure of interpretation\" when its refactoring diverges from a refactoring of our interpretation.\n\nFor example, if we think the AI detects pandas by looking for white hair on the body, and black hair on the arms, we can flag lots of examples of pandas and that hair pattern (and non-pandas and [unusual hair patterns](https://www.dropbox.com/s/j5ofk4ro39a8vh7/multi_panda.jpg?dl=0). We don't use these examples for training the AI, just to confirm that, in the training environment, there is a match between \"AI-thinks-they-are-pandas\" and \"white-hair-on-arms-black-hair-on-bodies\".\n\nBut, [in an adversarial example](https://openai.com/blog/adversarial-example-research/), the AI could detect that, while it is detecting gibbons, this no longer matches up with our interpretaion. A splintering of interpretations, if you want.\n\n\n## 6.8 Wireheading\n\nThe approach can also be used to detect [wireheading](https://www.lesswrong.com/posts/vXzM5L6njDZSf4Ftk/defining-ai-wireheading). Imagine that the AI has various detectors that allow it to label what the features of the bleggs and rubes are. It models the world with ten features: $5$ features representing the \"real world\" versions of the features, and $5$ representing the \"this signal comes from my detector\" versions.\n\nThis gives a total of $10$ features, the $5$ features \"in the real world\" and the $5$ \"AI-labelled\" versions of these:\n\n![](https://www.dropbox.com/s/dccsmzm9ai8s49r/blegg_rube_labels.png?raw=1)\n\nIn the training environment, there was full overlap between these $10$ features, so the AI might learn the incorrect \"maximise my labels/detector signal\" reward.\n\nHowever, when it refines its model to all $10$ features *and* environments where labels and underlying reality diverge, it will realise that this splinters the reward, and thus detect a possible wireheading. It could then ask for more information, or have an automated \"don't wirehead\" approach.\n\n## 6.9 Hypotheticals, and training in virtual environments\n\n\nTo get around the slowness of the real world, some approaches [train AIs in virtual environments](http://openaccess.thecvf.com/content_CVPR_2019/papers/James_Sim-To-Real_via_Sim-To-Sim_Data-Efficient_Robotic_Grasping_via_Randomized-To-Canonical_Adaptation_Networks_CVPR_2019_paper.pdf). The problem is to pass that learning from the virtual environment to the real one.\n\nSome have suggested making the virtual environment sufficiently detailed that the AI can't tell the difference between it and the real world. But, a) this involves fooling the AI, an approach I'm always wary of, and b) it's unnecessary.\n\nWithin the meta-formalism of this post, we could train the AI in a virtual environment which it models by $\\M$, and let it construct a model $\\M'$ of the real-world. We would then motivate the AI to find the \"closest match\" between $\\M$ and $\\M'$, in terms of features and how they connect and vary. This is similar to how we can train pilots in flight simulators; the pilots are never under any illusion as to whether this is the real world or not, and even crude simulators can allow them to build certain skills[^skills].\n\n[^skills]: Some more \"body instincts\" skills require more realistic environments, but some skills and procedures can perfectly well be trained in minimal simulators.\n\nThis can also be used to allow the AI to deduce information from hypotheticals and thought experiments. If we show the AI an episode of a TV series showing people behaving morally (or immorally), then the episode need not be believable or plausible, if we can roughly point to the features in the episode that we want to emphasise, and roughly how these relate to real-world features.\n\n## 6.10 Defining how to deal with multiple plausible refactorings\n\nThe approach for synthesising human preferences, [defined here](https://www.lesswrong.com/posts/CSEdLLEkap2pubjof/research-agenda-v0-9-synthesising-a-human-s-preferences-into#2_4_Synthesising_the_preference_function__first_step), can be rephrased as:\n\n* \"Given that we expect multiple natural refactorings of human preferences, and given that we expect some of them to go [disastrously wrong](https://www.lesswrong.com/posts/nFv2buafNc9jSaxAH/siren-worlds-and-the-perils-of-over-optimised-search), here is one way of resolving the splintering that we expect to be better than most.\"\n\nThis is just one way of doing this, but it does show that \"automating what AIs do with multiple refactorings\" might not be impossible. The following subsection has some ideas with how to deal with that.\n\n## 6.11 Global, large scale preferences\n\nIn an [old post](https://www.lesswrong.com/posts/fKmQEgKvyjcBfpA9G/emergency-learning), I talked about the concept of \"emergency learning\", which was basically, \"lots of examples, and all the stuff we know and suspect about how AIs can go wrong, shove it all in, and hope for the best\". The \"shove it all in\" was a bit more structured than that, defining large scale preferences (like \"avoid siren worlds\" and \"don't over-optimise\") as constraints to be added to the learning process.\n\nIt seems we can do better than that here. Using examples and hypotheticals, it seems we could construct ideas like \"avoid slavery\", \"avoid siren worlds\", or \"don't over-optimise\" as rewards or positive/negative examples certain simple training environments, so that the AI \"gets an idea of what we want\".\n\nWe can then label these ideas as \"global preferences\". The idea is that they start as loose requirements (we have much more granular human-scale preferences than just \"avoid slavery\", for example), but, the more the world diverges from the training environment, the stricter they are to be interpreted, with the AI required to respect some [softmin](https://www.lesswrong.com/posts/urZzJPwHtjewdKKHc/using-expected-utility-for-good-hart) of all natural refactorings of these features.\n\nIn a sense, we'd be saying \"prevent slavery; these are the features of slavery, and in weird worlds, be especially wary of these features\".\n\n\n## 6.12 Avoiding side-effects\n\nKrakovna et. al. presented a [paper on avoiding side-effects](https://deepmind.com/research/publications/measuring-and-avoiding-side-effects-using-relative-reachability) from AI. The idea is to have an AI maximising some reward function, while reducing side effects. So the AI would not smash vases or let them break, nor would it prevent humans from eating sushi.\n\nIn this environment, we want the AI to avoid knocking the sushi off the belt as it moves:\n\n![](https://www.dropbox.com/s/zh4uxfmcygdg8rg/sushi.png?raw=1)\n\nHere, in contrast, we'd want the AI to remove the vase from the belt before it smashes:\n\n![](https://www.dropbox.com/s/rplfzxdbcyx1knj/vase.png?raw=1)\n\nI pointed out [some issues with the whole approach](https://www.lesswrong.com/s/iRwYCpcAXuFD24tHh/p/mdQEraEZQLg7jtozn). Those issues were phrased in terms of sub-agents, but my real intuition is that syntactic methods are not sufficient to control side effects. In other words, the AI can't learn to do the right thing with sushis and vases, unless it has some idea of what these objects mean to us; we prefer sushis to be eaten and vases to not be smashed.\n\nThis can be learnt if the AI has a enough training examples, learning that eating sushi is a general feature of the environments it operates in, while vases being smashed is not. I'll return to this idea in a later post.\n\n## 6.13 Cancer patients\n\nThe ideas of this post were present in implicit form in the idea of [training an AI to cure cancer patients](https://www.lesswrong.com/posts/NdJtfujX4sE6xLCsb/if-i-were-a-well-intentioned-ai-iii-extremal-goodhart).\n\nUsing examples of successfully treated cancer patients, we noted they all shared some positive features (recuperating, living longer) and some incidental or negative features (complaining about pain, paying more taxes).\n\nSo, using the approach of [section 4.9](https://www.lesswrong.com/posts/k54rgSg7GcjtXnMHX/model-splintering-moving-from-one-imperfect-model-to-another-1#4_9_Preserved_background_features), we can designate that we want the AI to cure cancer; this will be interpreted as increasing all the features that correlate with that.\n\nUsing the explicit decorrelation of [section 4.10](https://www.lesswrong.com/posts/k54rgSg7GcjtXnMHX/model-splintering-moving-from-one-imperfect-model-to-another-1#4_10_Partially_preserved_background_features), we can also explicitly remove the negative options from the desired feature sets, thus improving the outcomes even more. \n\n\n## 6.14 The genie and the burning mother\n\nIn Eliezer's [original post on the hidden complexity of wishes](https://www.lesswrong.com/posts/4ARaTpNX62uaL86j6/the-hidden-complexity-of-wishes), he talks of the challenge of getting a genie to save your mother from a burning building:\n\n>So you hold up a photo of your mother's head and shoulders; match on the photo; use object contiguity to select your mother's whole body (not just her head and shoulders); and define the future function using your mother's distance from the building's center. [...]\n\n>You cry \"Get my mother out of the building!\", for luck, and press Enter. [...]\n\n>BOOM!  With a thundering roar, the gas main under the building explodes.  As the structure comes apart, in what seems like slow motion, you glimpse your mother's shattered body being hurled high into the air, traveling fast, rapidly increasing its distance from the former center of the building.\n\nHow could we avoid this? What you want is your mother out of the building. The feature \"mother in building\" must absolutely be set to false; this is a priority call, overriding almost everything else.\n\nHere we'd want to load examples of your mother outside the building, so that the genie/AI learns the features \"mother in house\"/\"mother out of house\". Then it will note that \"mother out of house\" correlates with a whole lot of other features - like mother being alive, breathing, pain-free, often awake, and so on.\n\nAll those are good things. But there are some other features that don't correlate so well - such as the time being earlier, your mother not remembering a fire, not being covered in soot, not worried about her burning house, and so on.\n\nAs in the cancer patient example above, we'd want to preserve the features that correlate with the mother out of the house, while allowing decorrelation with the features we don't care about or don't want to preserve.\n\n\n## 6.15 Splintering moral-relevant categories: honour, gender, and happiness\n\nIf the [Antikythera mechanism](https://en.wikipedia.org/wiki/Antikythera_mechanism) had been combined with the [Aeolipile](https://en.wikipedia.org/wiki/Aeolipile) to produce an ancient Greek AI, and Homer had programmed it (among other things) to \"increase people's honour\", how badly would things have gone?\n\nIf Babbage had completed the [analytical engine](https://en.wikipedia.org/wiki/Analytical_Engine) as Victorian AI,  and programmed it (among other things) to \"protect women\", how badly would things have gone?\n\nIf a modern programmer were to combine our neural nets into a [superintelligence](https://en.wikipedia.org/wiki/Superintelligence:_Paths,_Dangers,_Strategies) and program it (among other things) to \"increase human happiness\", how badly will things go?\n\nThere are three moral-relevant categories here, and it's illustrative to compare them: honour, gender, and hedonic happiness. The first has splintered, the second is splintering, and the third will likely splinter in the future.\n\nI'm not providing solutions in this subsection, just looking at where the problems can appear, and encouraging people to think about how they would have advised Homer or Babbage to define their concepts. Don't think \"stop using your concepts, use ours instead\", because our concepts/features will splinter too. Think \"what's the best way they could have extended their preferences even as the features splinter\"?\n\n* **6.15.1 Honour**\n\nIf we look at the concept of [honour](https://en.wikipedia.org/wiki/Honour), we see a concept that has already splintered.\n\nThat article reads like a meandering mess. Honour is \"face\", \"reputation\", a \"bond between an individual and a society\", \"reciprocity\", a \"code of conduct\", \"chastity\" (or \"virginity\"), a \"right to precedence\", \"nobility of soul, magnanimity, and a scorn of meanness\", \"virtuous conduct and personal integrity\", \"vengeance\", \"credibility\", and so on.\n\nWhat a basket of concepts! They only seem vaguely connected together; and even places with strong honour cultures differ in how they conceive of honour, from place to place and from epoch to epoch[^honcul]. And yet, if you asked most people within those cultures about what honour was, they would have had a strong feeling it was a single, well defined thing, maybe even a [concrete object](https://en.wikipedia.org/wiki/Reification_(fallacy)).\n\n[^honcul]: You could define honour as \"behaves according to the implicit expectations of their society\", but that just illustrates how time-and-place dependent honour is.\n\n\n* **6.15.2 Gender**\n\nIn his post [the categories were made for man, not man for the categories](https://slatestarcodex.com/2014/11/21/the-categories-were-made-for-man-not-man-for-the-categories/), Scott writes:\n\n>Absolutely typical men have Y chromosomes, have male genitalia, appreciate manly things like sports and lumberjackery, are romantically attracted to women, personally identify as male, wear male clothing like blue jeans, sing baritone in the opera, et cetera.\n\nBut Scott is writing this in the 21st century, long after the gender definition has splintered quite a bit. In middle class middle class Victorian England[^vic], the gender divide was much stronger - in that, from one component of the divide, you could predict a lot more. For example, if you knew someone wore dresses in public, you knew that, almost certainly, they couldn't own property if they were married, nor could they vote, they would be expected to be in charge of the household, might be allowed to faint, and were expected to guard their virginity.\n\n[^vic]: Pre [1870](https://en.wikipedia.org/wiki/Married_Women%27s_Property_Act_1870).\n\n![](https://www.dropbox.com/s/50m3otrr36kfwxx/gender.jpg?raw=1)\n\nWe talk nowadays about gender roles multiplying or being harder to define, but they've actually being splintering for a lot longer than that. Even though we could *define* two genders in 1960s Britain, at least roughly, that definition was a lot less informative than it was in Victorian-middle-class-Britain times: it had many fewer features strongly correlated with it.\n\n* **6.15.3 Happiness**\n\nOn to happiness! Philosophers and others [have been talking about happiness for centuries](https://plato.stanford.edu/entries/happiness/), often contrasting \"true happiness\", or flourishing, with hedonism, or [drugged out stupor](https://en.wikipedia.org/wiki/Lotus-eaters), or things of that nature. Often \"true happiness\" is a life of duty to what the philosopher wants to happen, but at least there is some analysis, some breakdown of the \"happiness\" feature into smaller component parts.\n\nWhy did the philosophers do this? I'd wager that it's because the concept of happiness was already somewhat splintered (as compared with a model where \"happiness\" is a single thing). Those philosophers had experience of joy, pleasure, the satisfaction of a job well done, connection with others, as well as superficial highs from temporary feelings. When they sat down to systematise \"happiness\", they could draw on the features of their own mental model. So even if people hadn't systematised happiness themselves, when they heard of what philosophers were doing, they probably didn't react as \"What? Drunken hedonism and intellectual joy are not the same thing? How dare you say such a thing!\"\n\nBut looking into the future, into a world that an AI might create, we can foresee many situations where the implicit assumptions of happiness come apart, and only some remain. I say \"we can foresee\", but it's actually very hard to know exactly how that's going to happen; if we knew it exactly, we could solve the issues now.\n\nSo, imagine a happy person. What do you think that they have in life, that are not trivial synonyms of happiness? I'd imagine they have friends, are healthy, think interesting thoughts, have some freedom of action, may work on worthwhile tasks, may be connected with their community, probably make people around them happy as well. Getting a bit less anthropomorphic, I'd also expect them to be a carbon-based life-form, to have a reasonable mix of hormones in their brain, to have a continuity of experience, to have a sense of identity, to have a personality, and so on.\n\nNow, some of those features can clearly be separated from \"happiness\". Even ahead of time, I can confidently say that \"being a carbon-based life-form\" is not going to be a critical feature of \"happiness\". But many of the other ones are not so clear; for example, would someone without continuity of experience or a sense of identity be \"happy\"?\n\nOf course, I can't answer that question. Because the question has no answer. We have our current model of happiness, which co-varies with all those features I listed and many others I haven't yet thought of. As we move into more and more bizarre worlds, that model will splinter. And whether we assign the different features to \"happiness\" or to some other concept, is a choice we'll make, not a well-defined solution to a well-defined problem.\n\nHowever, even at this stage, some answers are clearly better than others; statues of happy people should not count, for example, nor should written stories describing very happy people.\n\n## 6.16 Apprenticeship learning\n\nIn [apprenticeship learning](https://en.wikipedia.org/wiki/Apprenticeship_learning) (or learning from demonstration), the AI would aim to copy what experts have done. Inverse reinforcement learning [can be used for this purpose](https://ai.stanford.edu/~ang/papers/icml04-apprentice.pdf), by guessing the expert's reward function, based on their demonstrations. It looks for key [features](https://ai.stanford.edu/~ang/papers/icml04-apprentice.pdf) in expert trajectories and attempts to reproduce them.\n\nSo, if we had an automatic car driving people to the airport, and fed it some trajectories (maybe ranked by speed of delivery), it would notice that passengers would also arrive alive, with their bags, without being pursued by the police, and so on. This is akin to [section 4.9](https://www.lesswrong.com/posts/k54rgSg7GcjtXnMHX/model-splintering-moving-from-one-imperfect-model-to-another-1#4_9_Preserved_background_features), and would not accelerate blindly to get there as fast as possible.\n\nBut the algorithm has trouble getting to truly super-human performance[^appplus]. It's far too conservative, and, if we loosen the conservatism, it doesn't know what's acceptable and what isn't, and how to trade these off: since all passengers survived and the car was always [painted yellow](https://en.wikipedia.org/wiki/Taxicabs_of_New_York_City), their luggage intact in the training data, it has no reason to prefer human survival to taxi-colour. It doesn't even have a reason to have a specific feature resembling \"passenger survived\" at all.\n\n[^appplus]: It's not impossible to get superhuman performance from apprenticeship learning; for example, we could select the best human performance on a collection of distinct tasks, and thus get the algorithm to have a overall performance that no human could ever match. Indeed, one of the purposes of [task decomposition](https://www.lesswrong.com/posts/x9bNg6uEyjxqDLjcS/2-will-such-systems-still-be-useful) is to decompose complex tasks in ways that allow apprenticeship-like learning to have safe and very superhuman performance on the whole task.\n\nThis might be improved by the \"allow decorrelation\" approach from section 4.10: we specifically allow it to maximise speed of transport, while keeping the other features (no accidents, no speeding tickets) intact. As in [section 6.7](https://www.lesswrong.com/posts/k54rgSg7GcjtXnMHX/model-splintering-moving-from-one-imperfect-model-to-another-1#6_7_Hidden__dis_agreement_and_interpretability), we'll attempt to check that the AI does prioritise human survival, and that it will warn us if a refactoring moves it away from this.\n"
    },
    "voteCount": 25,
    "forceInclude": true
  },
  {
    "_id": "uerghMGoGM9RQW8Zr",
    "url": null,
    "title": "Ontological Crises in Artificial Agents' Value Systems by Peter de Blanc",
    "slug": "ontological-crises-in-artificial-agents-value-systems-by",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "Ontological Crisis"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "I saw [this](http://arxiv.org/pdf/1105.3821v1) go by on arXiv, and thought it deserved a discussion here.\n\n> Decision-theoretic agents predict and evaluate the results of their actions using a model, or ontology, of their environment. An agent's goal, or utility function, may also be specified in terms of the states of, or entities within, its ontology. If the agent may upgrade or replace its ontology, it faces a crisis: the agent's original goal may not be well-defined with respect to its new ontology. This crisis must be resolved before the agent can make plans towards achieving its goals. We discuss in this paper which sorts of agents will undergo ontological crises and why we may want to create such agents. We present some concrete examples, and argue that a well-defined procedure for resolving ontological crises is needed. We point to some possible approaches to solving this problem, and evaluate these methods on our examples.\n\nI'll post my analysis and opinion of this paper in a comment after I've taken some time to digest it."
    },
    "voteCount": 16,
    "forceInclude": true
  },
  {
    "_id": "KLaJjNdENsHhKhG5m",
    "url": null,
    "title": "Ontological Crisis in Humans",
    "slug": "ontological-crisis-in-humans",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "Motivations"
      },
      {
        "name": "Ethics & Morality"
      },
      {
        "name": "Moral Uncertainty"
      },
      {
        "name": "Ontological Crisis"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "Imagine a robot that was designed to find and collect spare change around its owner's house. It had a world model where macroscopic everyday objects are ontologically primitive and ruled by high-school-like physics and (for humans and their pets) rudimentary psychology and animal behavior. Its goals were expressed as a utility function over this world model, which was sufficient for its designed purpose. All went well until one day, a prankster decided to \"upgrade\" the robot's world model to be based on modern particle physics. This unfortunately caused the robot's utility function to instantly throw a [domain error](http://stackoverflow.com/questions/641064/what-is-a-domain-error) exception (since its inputs are no longer the expected list of macroscopic objects and associated properties like shape and color), thus crashing the controlling AI.\n\nAccording to Peter de Blanc, who used the phrase \"[ontological crisis](http://wiki.lesswrong.com/wiki/Ontological_crisis)\" to describe this kind of problem,\n\n> Human beings also confront ontological crises. We should find out what cognitive algorithms humans use to solve the same problems described in this paper. If we wish to build agents that maximize human values, this may be aided by knowing how humans re-interpret their values in new ontologies.\n\nI recently realized that a couple of problems that I've been thinking over (the [nature of selfishness](/lw/8gk/where_do_selfish_values_come_from) and the [nature of pain/pleasure/suffering/happiness](/lw/4qg/a_thought_experiment_on_pain_as_a_moral_disvalue/)) can be considered instances of ontological crises in humans (although I'm not so sure we necessarily have the cognitive algorithms to solve them). I started thinking in this direction after writing [this comment](/lw/b7w/decision_theories_a_semiformal_analysis_part_iii/6hef):\n\n> This formulation or variant of TDT requires that before a decision problem is handed to it, the world is divided into the agent itself (X), other agents (Y), and \"dumb matter\" (G). I think this is misguided, since the world doesn't really divide cleanly into these 3 parts.\n\nWhat struck me is that even though the world doesn't divide cleanly into these 3 parts, _our models_ of the world actually do. In the world models that we humans use on a day to day basis, and over which our utility functions seem to be defined ([to the extent](/lw/9jh/the_humans_hidden_utility_function_maybe/) that we can be said to have utility functions at all), we do take the Self, Other People, and various Dumb Matter to be ontologically primitive entities. Our world models, like the coin collecting robot's, consist of these macroscopic objects ruled by a hodgepodge of heuristics and prediction algorithms, rather than microscopic particles governed by a coherent set of laws of physics.\n\nFor example, the amount of pain someone is experiencing doesn't seem to exist in the real world as an XML tag attached to some \"person entity\", but that's pretty much how our models of the world work, and perhaps more importantly, that's what our utility functions expect their inputs to look like (as opposed to, say, a list of particles and their positions and velocities). Similarly, a human can be selfish just by treating the object labeled \"SELF\" in its world model differently from other objects, whereas an AI with a world model consisting of microscopic particles would need to somehow inherit or learn a detailed description of itself in order to be selfish.\n\nTo fully confront the ontological crisis that we face, we would have to upgrade our world model to be based on actual physics, and simultaneously translate our utility functions so that their domain is the set of possible states of the new model. We currently have little idea how to accomplish this, and instead what we do in practice is, as far as I can tell, keep our ontologies intact and utility functions unchanged, but just add some new heuristics that in certain limited circumstances call out to new physics formulas to better update/extrapolate our models. This is actually rather clever, because it lets us make use of updated understandings of physics without ever having to, for instance, decide exactly what patterns of particle movements constitute pain or pleasure, or what patterns constitute oneself. Nevertheless, this approach hardly seems capable of being extended to work in a future where many people may have nontraditional mind architectures, or have a zillion copies of themselves running on all kinds of strange substrates, or be merged into amorphous group minds with no clear boundaries between individuals.\n\nBy the way, I think nihilism often gets short changed [around](/lw/sc/existential_angst_factory/) [here](/lw/5i7/on_being_okay_with_the_truth/). Given that we do not actually have at hand a solution to ontological crises in general or to the specific crisis that we face, what's wrong with saying that the solution set may just be null? Given that evolution doesn't constitute a particularly benevolent and farsighted designer, perhaps we may not be able to do much better than that poor spare-change collecting robot? If Eliezer is [worried](/r/discussion/lw/827/ai_ontology_crises_an_informal_typology/7pqr) that actual AIs facing actual ontological crises could do worse than just crash, should we be very sanguine that for humans everything must \"add up to moral normality\"?\n\nTo expand a bit more on this possibility, many people have an aversion against moral arbitrariness, so we need at a minimum a utility translation scheme that's principled enough to pass that filter. But our existing world models are a hodgepodge put together by evolution so there may not be any such sufficiently principled scheme, which (if other approaches to solving moral philosophy also don't pan out) would leave us with legitimate feelings of \"existential angst\" and nihilism. One could perhaps still argue that any _current_ such feelings are premature, but maybe some people have stronger intuitions than others that these problems are unsolvable?\n\nDo we have any examples of humans successfully navigating an ontological crisis? The LessWrong Wiki [mentions](http://wiki.lesswrong.com/wiki/Ontological_crisis) loss of faith in God:\n\n> In the human context, a clear example of an ontological crisis is a believer’s loss of faith in God. Their motivations and goals, coming from a very specific view of life suddenly become obsolete and maybe even nonsense in the face of this new configuration. The person will then experience a deep crisis and go through the psychological task of reconstructing its set of preferences according the new world view.\n\nBut I don't think loss of faith in God actually constitutes an ontological crisis, or if it does, certainly not a very severe one. An ontology consisting of Gods, Self, Other People, and Dumb Matter just isn't very different from one consisting of Self, Other People, and Dumb Matter (the latter could just be considered a special case of the former with quantity of Gods being 0), especially when you compare either ontology to one made of microscopic particles or even [less](http://en.wikipedia.org/wiki/Loop_quantum_gravity) [familiar](http://en.wikipedia.org/wiki/String_theory) [entities](http://en.wikipedia.org/wiki/Ultimate_ensemble).\n\nBut to end on a more positive note, realizing that seemingly unrelated problems are actually instances of a more general problem gives some hope that by \"going meta\" we can find a solution to all of these problems at once. Maybe we can solve many ethical problems simultaneously by discovering some generic algorithm that can be used by an agent to transition from any ontology to another? \n\n(Note that I'm not saying this _is_ the right way to understand one's real preferences/morality, but just drawing attention to it as a possible alternative to other more \"object level\" or \"purely philosophical\" approaches. See also [this previous discussion](/lw/6ha/the_blueminimizing_robot/4gi2), which I recalled after writing most of the above.)"
    },
    "voteCount": 51,
    "forceInclude": true
  },
  {
    "_id": "6RjL996E8Dsz3vHPk",
    "url": null,
    "title": "Two More Decision Theory Problems for Humans",
    "slug": "two-more-decision-theory-problems-for-humans",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "Decision Theory"
      }
    ],
    "tableOfContents": {
      "sections": [
        {
          "title": "Partial Utility Function Problem",
          "anchor": "Partial_Utility_Function_Problem",
          "level": 1
        },
        {
          "title": "Decision Theory Upgrade Problem",
          "anchor": "Decision_Theory_Upgrade_Problem",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        },
        {
          "anchor": "comments",
          "level": 0,
          "title": "13 comments"
        }
      ],
      "headingsCount": 4
    },
    "contents": {
      "markdown": "(This post has been sitting in my drafts folder for 6 years. Not sure why I didn't make it public, but here it is now after some editing.)\n\nThere are two problems closely related to the [Ontological Crisis in Humans](/posts/KLaJjNdENsHhKhG5m/ontological-crisis-in-humans). I'll call them the \"Partial Utility Function Problem\" and the \"Decision Theory Upgrade Problem\".\n\n**Partial Utility Function Problem**\n\nAs I mentioned in a [previous post](/lw/fyb/ontological_crisis_in_humans/), the only apparent utility function we have seems to be defined over an ontology very different from the fundamental ontology of the universe. But even on it's native domain, the utility function seems only partially defined. In other words, it will throw an error (i.e., say \"I don't know\") on some possible states of the heuristical model. For example, this happens for me when the number of people gets sufficiently large, like 3^^^3 in Eliezer's Torture vs Dust Specks scenario. When we try to compute the expected utility of some action, how should we deal with these \"I don't know\" values that come up?\n\n(Note that I'm presenting a simplified version of the real problem we face, where in addition to \"I don't know\", our utility function could also return essentially random extrapolated values outside of the region where it gives sensible outputs.)\n\n**Decision Theory Upgrade Problem**\n\nIn the Decision Theory Upgrade Problem, an agent decides that their current decision theory is inadequate in some way, and needs to be upgraded. (Note that the Ontological Crisis could be considered an instance of this more general problem.) The question is whether and how to transfer their values over to the new decision theory.\n\nFor example a human might be be running a mix of several decision theories: reinforcement learning, heuristical model-based consequentialism, identity-based decision making (where you adopt one or more social roles, like \"environmentalist\" or \"academic\" as part of your identity and then make decisions based on pattern matching what that role would do in any given situation), as well as virtual ethics and deontology. If you are tempted to drop one or more of these in favor of a more \"advanced\" or \"rational\" decision theory, such as UDT, you have to figure out how to transfer the values embodied in the old decision theory, which may not even be represented as any kind of utility function, over to the new.\n\nAnother instance of this problem can be seen in someone just wanting to be a bit more consequentialist. Maybe UDT is too strange and impractical, but our native model-based consequentialism at least seems closer to being rational than the other decision procedures we have. In this case we tend to assume that the consequentialist module already has our real values and we don't need to \"port\" values from the other decision procedures that we're deprecating. But I'm not entirely sure this is safe, since the step going from (for example) identity-based decision making to heuristical model-based consequentialism doesn't seem *that* different from the step between heuristical model-based consequentialism and something like UDT."
    },
    "voteCount": 20,
    "forceInclude": true
  },
  {
    "_id": "ky988ePJvCRhmCwGo",
    "url": null,
    "title": "Using vector fields to visualise preferences and make them consistent",
    "slug": "using-vector-fields-to-visualise-preferences-and-make-them",
    "author": "MichaelA",
    "question": false,
    "tags": [
      {
        "name": "Value Learning"
      },
      {
        "name": "AI"
      }
    ],
    "tableOfContents": {
      "sections": [
        {
          "title": "Overview",
          "anchor": "Overview",
          "level": 1
        },
        {
          "title": "Vector fields and preferences",
          "anchor": "Vector_fields_and_preferences",
          "level": 1
        },
        {
          "title": "Not only preferences",
          "anchor": "Not_only_preferences",
          "level": 2
        },
        {
          "title": "Gradients and utility functions",
          "anchor": "Gradients_and_utility_functions",
          "level": 1
        },
        {
          "title": "Method",
          "anchor": "Method",
          "level": 2
        },
        {
          "title": "Extrapolating PVFs (and utility functions) from specific preference data",
          "anchor": "Extrapolating_PVFs__and_utility_functions__from_specific_preference_data",
          "level": 1
        },
        {
          "title": "Curl and inconsistent preferences",
          "anchor": "Curl_and_inconsistent_preferences",
          "level": 1
        },
        {
          "title": "Removing curl to create consistent utility functions",
          "anchor": "Removing_curl_to_create_consistent_utility_functions",
          "level": 2
        },
        {
          "title": "Uncertainties and areas for further research",
          "anchor": "Uncertainties_and_areas_for_further_research",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        },
        {
          "anchor": "comments",
          "level": 0,
          "title": "32 comments"
        }
      ],
      "headingsCount": 11
    },
    "contents": {
      "markdown": "_This post was written for [Convergence Analysis](https://www.convergenceanalysis.org/) by Michael Aird, based on ideas from Justin Shovelain and with ongoing guidance from him. Throughout the post, “I” will refer to Michael, while “we” will refer to Michael and Justin or to Convergence as an organisation._\n\n_Epistemic status: High confidence in the core ideas on an abstract level. Claims about the usefulness of those ideas, their practical implications, and how best to concretely/mathematically implement them are more speculative; one goal in writing this post is to receive feedback on those things. I’m quite new to many of the concepts covered in this post, but Justin is more familiar with them._\n\nOverview\n========\n\nThis post outlines:\n\n*   What vector fields are\n*   How they can be used to visualise preferences\n*   How utility functions can be generated from “preference vector fields” (PVFs)\n*   How PVFs can be extrapolated from limited data on preferences\n*   How to visualise inconsistent preferences (as “curl”)\n*   A rough idea for how to “remove curl” to generate consistent utility functions\n*   Possible areas for future research\n\nWe expect this to provide useful tools and insights for various purposes, most notably AI alignment, existential risk [strategy](https://forum.effectivealtruism.org/posts/oovy5XXdCL3TPwgLE/a-case-for-strategy-research-what-it-is-and-why-we-need-more), and rationality.\n\nThis post is structured modularly; different sections may be of interest to different readers, and should be useful in isolation from the rest of the post. The post also includes links to articles and videos introducing relevant concepts, to make the post accessible to readers without relevant technical backgrounds.\n\nVector fields and preferences\n=============================\n\nA [vector](https://en.wikipedia.org/wiki/Euclidean_vector) represents both magnitude and direction; for example, velocity is a vector that represents not just the speed at which one is travelling but also the direction of travel. A [vector field](https://en.wikipedia.org/wiki/Vector_field) essentially associates a vector to each point in a region of space. For example, the following image ([source](https://www.khanacademy.org/science/physics/magnetic-forces-and-magnetic-fields/magnetic-field-current-carrying-wire/a/what-are-magnetic-fields)) shows the strength (represented by arrow lengths) and direction of the magnetic field at various points around a bar magnet:\n\n  \n\n![](https://res.cloudinary.com/dwbqmbkdb/image/upload/v1580239962/Magnetic_field_wdw8tw.png)\n\n_Figure 1._\n\nAnother common usage of vector fields is to represent the direction in which fluid would flow, for example the downhill flow of water on uneven terrain ([this short video](https://www.khanacademy.org/math/multivariable-calculus/thinking-about-multivariable-function/visualizing-vector-valued-functions/v/fluid-flow-and-vector-fields) shows and discusses that visualisation).\n\nWe believe that vector fields over “state spaces” (possible states of the world, represented by positions along each dimension) can be a useful tool for analysis and communication of various issues (e.g., existential risk strategy, AI alignment). In particular, we’re interested in the idea of representing preferences as “preference vector fields” (PVFs), in which, at each point in the state space, a vector represents which direction in the state space an agent would prefer to move from there, and how intense that preference is.^[It appears some prior work (e.g., [this](https://www.tandfonline.com/doi/abs/10.1080/09544820500275032) and [this](https://books.google.com.au/books?id=qLnEiu0lbNAC&pg=PA169&lpg=PA169&dq=preference+vector+field&source=bl&ots=zXSiGh3haW&sig=ACfU3U16TLa5wnOv8ABcVbTwVLnOoukIvw&hl=en&sa=X&ved=2ahUKEwja5df1wqXmAhULXisKHbZqDbkQ6AEwEHoECAoQAg#v=onepage&q=preference%20vector%20field&f=false)) has explored the use of vector fields to represent preferences. Unfortunately, I haven’t yet had time to investigate this work, so there may be many useful insights in there that are lacking in this post.] (For the purposes of this post, “agent” could mean an AI, a human, a community, humanity as a whole, etc.)\n\nTo illustrate this, the following PVF shows a hypothetical agent’s preferences over a state space in which the only dimensions of interest are wealth and security.^[Of course, there are often far more than two key factors influencing our preferences. In such cases, a vector field over more dimensions can be used instead (see [here](https://www.khanacademy.org/math/multivariable-calculus/thinking-about-multivariable-function/visualizing-vector-valued-functions/v/3d-vector-fields-introduction) for an introduction to 3D vector fields). I focus in this post on 2D vector fields, simply because those are easier to discuss and visualise. We expect many of the ideas and implications covered in this post will be similar in higher dimensional vector fields, but we aren’t yet certain about that, and intend to more carefully consider it later.]^[For both this example and most others shown, the precise equations used were chosen quite arbitrarily, basically by trying equations semi-randomly until I found one that roughly matched the sort of shape I wanted. For those interested, I have screenshots of all equations used, in their order of appearance in this post, [here](https://docs.google.com/document/d/1pGx7PPpZ_52l1Zrp5UpSA7UY1M3m8UtRIjVirn0DGls/edit?usp=sharing). To create the visuals in this post, I entered these equations into Grapher (for those interested in trying to do similar things themselves, I found [this guide](https://theputterer.wordpress.com/2011/12/10/mac-os-x-grapher-contour-and-vector-plots/) useful). I discuss below, in the section “Extrapolating PVFs (and utility functions) from specific preference data”, the issue of how to actually generate realistic/accurate PVFs in the first place.]\n\n![](https://res.cloudinary.com/dwbqmbkdb/image/upload/v1580239963/Figure_2_Vectors_gkybi1.png)\n\n_Figure 2._\n\nThe fact that (at least over the domain shown here) the arrows always point at least slightly upwards and to the right shows that the agent prefers more wealth and security to less, regardless of the current level of those variables. The fact that the arrows are longest near the x axis shows that preferences are most intense when security is low. The fact that the arrows become gradually more horizontal as we move up the y axis shows that, as security increases, the agent comes to care more about wealth relative to security.\n\nNot only preferences\n--------------------\n\nIn a very similar way, vector fields can be used to represent things other than preferences. For example, we might suspect that for many agents (e.g., most/all humans), preferences do not perfectly match what would actually make the agent happier (e.g., because of the agent being mistaken about something, or having separate systems for reward vs motivation). In this case, we could create a vector field to represent the agent’s preferences (represented by the blue arrows below), and another to represent what changes from any given point would increase the agent’s happiness (represented by the green arrows).\n\n![](https://res.cloudinary.com/dwbqmbkdb/image/upload/v1580239967/Figure_3_Vectors_cl7b39.png)\n\n_Figure 3._\n\nThis method of layering vector fields representing different things can be used as one tool in analysing potential clashes between different things (e.g., between an agent’s preferences and what would actually make the agent happy, or between an agent’s _beliefs_ about what changes would be likely at each state and what changes would _actually_ be likely at each state).\n\nFor example, the above graph indicates that, as wealth and/or security increases (i.e., as we move across the x axis and/or up the y axis), there is an increasing gap between the agent’s preferences and what would make the agent happy. In particular, security becomes increasingly more important than wealth for the agent’s happiness, but this is not reflected in the agent’s preferences.\n\n(Note that, while it does make sense to compare the direction in which arrows from two different vector fields point, I haven’t yet thought much about whether it makes sense to compare the lengths Grapher shows for their arrows. It seems like this is _mathematically_ the same as the common problem of trying to compare utility functions across different agents, or preferences across different voters. But here the functions represent different things within the _same_ agent, which may make a difference.)\n\nGradients and utility functions\n===============================\n\nWhen a vector field has no “curl” (see the section “Curl and inconsistent preferences” below), the vector field can be thought of as the [gradient](https://www.khanacademy.org/math/multivariable-calculus/multivariable-derivatives/gradient-and-directional-derivatives/v/gradient) of a [scalar field](https://en.wikipedia.org/wiki/Scalar_field).^[It’s possible that here I’m conflating the concepts of [conservative](https://en.wikipedia.org/wiki/Conservative_vector_field), irrotational, and curl-free vector fields in a way that doesn’t make sense. If any readers believe this is the case, and especially if they believe this issue changes the core ideas and implications raised in this post, I would appreciate them commenting or messaging me.] (A scalar field is similar to a vector field, except that it associates a _scalar_ with each point in a region of space, and scalars have only magnitude, rather than magnitude _and_ _direction_.) Essentially, this means that the arrows of the vector field can be thought of as pointing “uphill”, away from low points and towards high points of the associated scalar function. If the vector field represents preferences, higher points of the scalar function would be where preferences are more satisfied, and lower points are where it is less satisfied; thus, the scalar function can be thought of as the agent’s utility function.^[Technically, the vector field is the gradient of a _class of_ functions, with the functions differing only in their constant term. This is because gradient only relates to _differences_ in height (or roughly analogous ideas, in higher-dimensional cases), not to absolute heights. One can imagine raising or lowering the entire scalar function by the same constant without affecting the gradient between points. (I show in [this document](https://docs.google.com/document/d/1pGx7PPpZ_52l1Zrp5UpSA7UY1M3m8UtRIjVirn0DGls/edit?usp=sharing) examples of what this would look like, while in this post itself I keep all constants at 0.) Thus, in one sense, a PVF does not fully specify the associated utility function representation, but the constant can be ignored anyway (as utility functions are unique up to positive affine transformations).] (The same basic method is often used in physics, in which context the scalar function typically represents [scalar potential](https://en.wikipedia.org/wiki/Scalar_potential).)\n\nBelow is one visualisation of the scalar field representing the utility function of the agent from the previous example (based on its preferences, not on what would make it “happy”), as well as the related vector field. Colours towards the red end of the spectrum represent higher values of the scalar field. It can be seen that the arrows of the vector field point away from blue areas and towards red areas, representing the agent’s preference for “climbing uphill” on its utility function.\n\n![](https://res.cloudinary.com/dwbqmbkdb/image/upload/v1580239979/Figure_4_Vectors_xjq6sz.png)\n\n_Figure 4._\n\nThe scalar field can also be represented in three dimensions, as values on the z dimension, which are in turn a function of values on the x and y dimensions. This is shown below (from two angles), for the same agent. (These graphs are a little hard to interpret from still images on a 2D screen, at least with this function; such graphs can be easier to interpret when one is able to rotate the angle of view.)\n\n![](https://res.cloudinary.com/dwbqmbkdb/image/upload/v1580239979/Figure_5_Vectors_ma27vo.png)\n\n_Figures 5a and 5b._\n\nMethod\n------\n\n[This video](https://www.youtube.com/watch?v=iLAK2IsQ_Uo) provides one clear explanation of the actual method for determining the scalar function that a curl-free vector field can be thought of as the gradient of (though the video is focused on cases of 3D vector fields). That video describes this as finding the “potential”; as noted earlier, when the vector field represents preferences, the utility function can be thought of as analogous to the “potential” in other cases.\n\nPersonally, as a quick method of finding the scalar function associated with a 2D vector field, I used the following algorithm, from the first answer on [this page](https://mathematica.stackexchange.com/questions/100758/finding-scalar-potential-function):\n\n> DSolve[{D[f[x, y], x] == [X COMPONENT OF THE VECTOR FIELD], D[f[x, y], y] == [Y COMPONENT OF THE VECTOR FIELD]}, f[x, y], {x, y}]\n\nI input the algorithm into a [Wolfram Cloud notebook](https://www.wolframcloud.com/), which seems to be free to use as long as you create an account. (As noted in the answer on the linked page, this algorithm will come back with no solution if the vector field has curl. This makes sense, because this general approach cannot be used in this way if a field has curl; this is explained in the section “Curl and inconsistent preferences” below.) Finally, I double-checked that the function was a valid solution by using [this calculator](https://www.wolframalpha.com/input/?i=grad+of+a+scalar+field&assumption=%7B%22F%22%2C+%22GradientCalculator%22%2C+%22scalarfunction%22%7D+-%3E) to find its gradient, which should then be the same as the original vector field.\n\nExtrapolating PVFs (and utility functions) from specific preference data\n========================================================================\n\nIn reality, one rarely knows an agent’s actual utility function or their full PVF. Instead, one is likely to only have data on the agent’s (apparent) preferences at _particular points_ in state space; for example, the extent to which they wanted more wealth and more security when they had $10,000 of savings and a “4/5” level of security.\n\nOne can imagine extrapolating a full preference vector field (PVF) from that data. We do not know of a precise method for actually doing this (we plan to do more research and thought regarding that in future). However, conceptually speaking, it seems the process would be analogous to fitting a regression line to observed data points, and, like that process, would require striking a balance between maximising fit with the data and avoiding [overfitting](https://en.wikipedia.org/wiki/Overfitting).\n\nFor an example (based very loosely on Figure 3 in [this article](https://www.tandfonline.com/doi/abs/10.1080/09544820500275032)), suppose that I know that Alice prefers car A to Car B, Car B to Car C, Car C to Car D, and Car D to Car A (i.e., to Alice, A>B>C>D>A).^[I have purposefully chosen a set of circular (or “intransitive”) preferences, as the next session will use this example in discussing the problem of circularity and how to deal with it.] I also know the weight (in thousands of pounds) and perceived “sportiness” (as rated by consumers) of the four cars, and am willing to make the simplifying assumption that these are the only factors that influenced Alice’s preferences. I could then create a plane with weight on the x axis and sportiness on the y axis, show the position of the four cars in this space, and represent Alice’s preferences with arrows pointing from each car towards the car Alice would prefer to that one, as shown below:^[Note that, in this example, I am not assuming any knowledge about the _strength_ of Alice’s preferences, only about their direction. As such, the length of the arrows representing Alice’s known preferences has no particular meaning.]\n\n![](https://res.cloudinary.com/dwbqmbkdb/image/upload/v1580239980/Figure_6_vectors_ggb5qp.png)\n\n_Figure 6._\n\nI could then infer a PVF that (1) approximately captures Alice’s known preferences, and (2) suggests what preferences Alice would have at any other point in the plane (rather than just at the four points I have data for). In this case, one seemingly plausible PVF is shown below, with the length of each blue arrow representing the strength of Alice’s preferences at the associated point. (This PVF still shows Alice’s known preferences, but this is just for ease of comparison; those known preferences are not actually part of the PVF itself.)\n\n![](https://res.cloudinary.com/dwbqmbkdb/image/upload/v1580239990/Figure_7_vectors_mktb8i.png)\n\n_Figure 7._\n\nThis PVF allows us to make predictions about what Alice’s preferences would be even in situations we do not have any empirical data about. For example, this PVF suggests that if Alice had the hypothetical car E (with a weight of ~2000 pounds and sportiness of ~55), she would prefer a car that was heavier and was higher for sportiness. In contrast, the PVF also suggests that, if she had the hypothetical car F (with a weight of ~6000 pounds and sportiness of ~55), she would prefer a car that was heavier and was rated _lower_ for sportiness.\n\nOf course, these predictions are not necessarily accurate. One could likely create many other PVFs that also “appear” to roughly fit Alice’s known preferences, and these could lead to different predictions. This highlights why we wish to find a more precise/“rigorous” method to better accomplish the goal I have conceptually gestured at here.\n\nIt’s also worth noting that one could extrapolate an agent’s utility function from limited preference data by first using the method gestured at here and then using the method covered in the previous section. That is, one could gather some data on an agent’s (apparent) preferences, extrapolate a PVF that “fits” that data, and then calculate what (set of) scalar function(s) that vector field is the gradient of. That scalar function would be the agent’s extrapolated utility function.\n\nHowever, as noted earlier, this method only works if the PVF has no “curl”, so it would not work in the case of Alice’s preferences about cars. I will now discuss what I mean by “curl”, what implications curl has, and a rough idea for “removing” it.\n\nCurl and inconsistent preferences\n=================================\n\nIn the example above, to Alice, A>B>C>D>A. This is a case of [intransitivity](https://en.wikipedia.org/wiki/Intransitivity#Occurrences_in_preferences), or, less formally, circular or inconsistent preferences. This is typically [seen as](https://www.lesswrong.com/s/waF2Pomid7YHjfEDt/p/zJZvoiwydJ5zvzTHK) [irrational](https://www.lesswrong.com/s/waF2Pomid7YHjfEDt/p/zNcLnqHF5rvrTsQJx), and as opening agents up to issues such as being “[money pumped](https://en.wikipedia.org/wiki/Money_pump)”. It seems that Alice would be willing to just keep paying us to let her trade in one car for the one she preferred to that one, and do this _endlessly_ \\- going around and around in a circle, yet feeling that her preferences are being continually satisfied.\n\nSo another pair of reasons why representing preferences as vector fields is helpful is that doing so allows inconsistencies in preferences:\n\n1.  to be directly seen (if they are sufficiently extreme)\n2.  to be calculated as the vector field’s [curl](https://en.wikipedia.org/wiki/Curl_(mathematics))\n\n[This video](https://www.khanacademy.org/math/multivariable-calculus/multivariable-derivatives/curl-grant-videos/v/2d-curl-intuition) introduces the concept of curl. Returning to the visualisation of vector fields as representing the direction in which water would flow over a certain domain, curl represents the speed and direction an object would spin if placed in the water. For example, if there is a strong clockwise curl at a certain point, a stick placed there would rotate clockwise; if there is no curl at a point, a stick placed there would not rotate (though it still may move in some direction, as represented by the vector field itself).\n\nNote that the concepts of curl and inconsistency will also apply in less extreme cases (i.e., where an agent’s preferences do not _only_ “chase each other around in circles”).\n\nAs noted earlier, when a vector field has curl, one cannot find its gradient. In our context, this seems logical; if an agent’s preferences are inconsistent, it seems that the agent cannot have a true utility function, and that we can’t assign any meaningful “height” to any point in the 2D state space. Consider again the example of Alice’s preferences for cars; if we were to interpret meeting her preferences as moving “uphill” on a utility function, she could keep arriving back at the same points in the state space and yet be at different “heights”, which doesn’t seem to make sense.\n\nRemoving curl to create consistent utility functions\n----------------------------------------------------\n\nIt seems that agents frequently have intransitive preferences, and thus that their PVFs will often have some curl. It would therefore be very useful to have a method for “removing curl” from a PVF, to translate an intransitive set of preferences into a transitive set of preferences, while making a minimum of changes. This new, consistent PVF would also then allow for the generation of a corresponding utility function for the agent.^[In conversation with Justin, Linda Linsefors mentioned having had a somewhat similar idea independently.]\n\nWe believe that this process should be possible. We also believe that, if developed and confirmed to make sense, it could be useful for various aspects of AI alignment (among other things). In particular, it could help in:\n\n*   extrapolation of a consistent “core” (and corresponding utility function) from inconsistent _human_ preferences (which could then inform an AI’s decisions)\n*   adjustment of an _AI’s_ inconsistent preferences (either by engineers or by the AI itself), with a minimum of changes being made\n\nWe have not yet implemented this process for removing curl. But we believe that the [Helmholtz theorem](https://en.wikipedia.org/wiki/Helmholtz_decomposition) should work, at least for PVFs in 3 or fewer dimensions (and we believe that a higher dimensional generalization probably exists). The Helmholtz theorem: \n\n> states that any sufficiently smooth, rapidly decaying vector field in three dimensions can be resolved into the sum of an irrotational (curl-free) vector field and a solenoidal (divergence-free) vector field; this is known as the Helmholtz decomposition or Helmholtz representation. ([Wikipedia](https://en.wikipedia.org/wiki/Helmholtz_decomposition))\n\nThis irrotational (curl-free) vector field would then be the consistent projection (in a [CEV](https://intelligence.org/files/CEV.pdf)-like way) of the agent’s preferences (from which the agent’s utility function could also be generated, in the manner discussed earlier).  \n\nUncertainties and areas for further research\n============================================\n\nThe following are some areas we are particularly interested in getting comments/feedback on, seeing others explore, or exploring ourselves in future work:\n\n*   Are there any flaws or misleading elements in the above analysis? (As noted earlier, this is essentially just an initial exploration of some tools/concepts.)\n*   To what extent do the methods used and claims made in this post generalise to higher-dimensional spaces (e.g., when we wish to represent preferences over more than two factors at the same time)? To what extent do they generalise to graphs of states that don’t correspond to any normal geometry?\n*   Is there an existing, rigorous/precise method for extrapolating a PVF from a limited number of known preferences (or more generally, extrapolating a vector field from a limited number of known vectors)? If not, can a satisfactorily rigorous/precise method be developed?\n*   Are there meaningful and relevant differences between the concepts of curl in vector fields and of intransitivity, inconsistency, irrationality, and incoherence in preferences? If so, how does that change the above analysis?\n*   Is it possible to “remove curl” in the way we want, in the sort of situations we’re interested in (in particular, not only in three dimensions)? If so, how, specifically?\n*   What other implications do the above ideas have? E.g., for rationality more generally, or for how to interpret and implement preference utilitarianism. (Above, I mostly just introduced the ideas, and hinted at a handful of implications.)\n*   What other uses could these “tools” be put to?"
    },
    "voteCount": 21,
    "forceInclude": true
  },
  {
    "_id": "H2SizXNd99FcwLsCC",
    "url": null,
    "title": "Turning Some Inconsistent Preferences into Consistent Ones",
    "slug": "turning-some-inconsistent-preferences-into-consistent-ones",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "Rationality"
      }
    ],
    "tableOfContents": {
      "sections": [
        {
          "title": "Epistemic Status",
          "anchor": "Epistemic_Status",
          "level": 3
        },
        {
          "title": "Turning Some Inconsistent Preferences into Consistent Ones",
          "anchor": "Turning_Some_Inconsistent_Preferences_into_Consistent_Ones",
          "level": 1
        },
        {
          "title": "Mathematical Formulation of the Problem",
          "anchor": "Mathematical_Formulation_of_the_Problem",
          "level": 2
        },
        {
          "title": "Related Work",
          "anchor": "Related_Work",
          "level": 2
        },
        {
          "title": "Discrete Case",
          "anchor": "Discrete_Case",
          "level": 2
        },
        {
          "title": "Example",
          "anchor": "Example",
          "level": 3
        },
        {
          "title": "Resolving Inconsistencies",
          "anchor": "Resolving_Inconsistencies",
          "level": 3
        },
        {
          "title": "Implementation",
          "anchor": "Implementation",
          "level": 4
        },
        {
          "title": "Problems with This Method and its Algorithm",
          "anchor": "Problems_with_This_Method_and_its_Algorithm",
          "level": 4
        },
        {
          "title": "Questions",
          "anchor": "Questions",
          "level": 3
        },
        {
          "title": "Number of Turnings for Gn",
          "anchor": "Number_of_Turnings_for_Gn",
          "level": 4
        },
        {
          "title": "Encoding Inconsistencies",
          "anchor": "Encoding_Inconsistencies",
          "level": 2
        },
        {
          "title": "Theory",
          "anchor": "Theory",
          "level": 3
        },
        {
          "title": "Discrete Case",
          "anchor": "Discrete_Case1",
          "level": 3
        },
        {
          "title": "Incompleteness",
          "anchor": "Incompleteness",
          "level": 4
        },
        {
          "title": "Intransitivity",
          "anchor": "Intransitivity",
          "level": 4
        },
        {
          "title": "Non-Encodable Inconsistencies",
          "anchor": "Non_Encodable_Inconsistencies",
          "level": 4
        },
        {
          "title": "Continuous Case",
          "anchor": "Continuous_Case",
          "level": 3
        },
        {
          "title": "Incompleteness",
          "anchor": "Incompleteness1",
          "level": 4
        },
        {
          "title": "Intransitivity",
          "anchor": "Intransitivity1",
          "level": 4
        },
        {
          "title": "Discontinuity",
          "anchor": "Discontinuity",
          "level": 4
        },
        {
          "title": "Dependence",
          "anchor": "Dependence",
          "level": 4
        },
        {
          "title": "Discussion",
          "anchor": "Discussion",
          "level": 3
        },
        {
          "title": "Continuous Case",
          "anchor": "Continuous_Case1",
          "level": 2
        },
        {
          "title": "Vector Fields over Probability Simplices",
          "anchor": "Vector_Fields_over_Probability_Simplices",
          "level": 3
        },
        {
          "title": "Resolving Inconsistencies",
          "anchor": "Resolving_Inconsistencies1",
          "level": 4
        },
        {
          "title": "Graphons",
          "anchor": "Graphons",
          "level": 3
        },
        {
          "title": "Implications for AI Alignment",
          "anchor": "Implications_for_AI_Alignment",
          "level": 2
        },
        {
          "title": "Ambitious Value Learning",
          "anchor": "Ambitious_Value_Learning",
          "level": 3
        },
        {
          "title": "Ontological Crises",
          "anchor": "Ontological_Crises",
          "level": 3
        },
        {
          "title": "Discrete Case",
          "anchor": "Discrete_Case2",
          "level": 4
        },
        {
          "title": "Further Questions",
          "anchor": "Further_Questions",
          "level": 2
        },
        {
          "title": "Acknowledgements",
          "anchor": "Acknowledgements",
          "level": 2
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        },
        {
          "anchor": "comments",
          "level": 0,
          "title": "5 comments"
        }
      ],
      "headingsCount": 35
    },
    "contents": {
      "markdown": "*cross-posted from [niplav.github.io](https://niplav.github.io)*\n\n### Epistemic Status\n\nThis is still a draft that I [was told](https://schelling.pt/@niplav/108403526373636608) to already post here, which includes working (but very slow) code for one special case. Hopefully I'll be able to expand on this in the next ~half year.\n\n> Representing inconsistent preferences with specific mathematical\nstructures can clarify thoughts about how to make those preferences\nconsistent while only minimally changing them. This is discussed in\nthe case of preferences over world states, represented by [directed\ngraphs](https://en.wikipedia.org/wiki/Directed_graph); and preferences\nover [lotteries](https://en.wikipedia.org/wiki/Lottery_\\(probability\\))\nof world states, represented either by infinitely dense\ngraphs, (in some cases) vector fields over probability\nsimplices, or edge-weighted directed graphs. I also present\nan algorithm for the discrete case based on the [graph edit\ndistance](https://en.wikipedia.org/wiki/Graph_Edit_Distance). Implications\nfor scenarios such as [ontological\nshifts](https://arbital.com/p/ontology_identification/) are discussed.\n\n\nTurning Some Inconsistent Preferences into Consistent Ones\n===========================================================\n\n> A kind of God-made (or evolution-created) fairness between species is\nalso unexpectedly found.\n\n*— [Yew-Kwang Ng](https://en.wikipedia.org/wiki/Yew-Kwang_Ng), [“Towards Welfare Biology: Evolutionary Economics of Animal Consciousness and Suffering”](https://niplav.github.io/doc/biology/welfare/towards_welfare_biology_evolutionary_economics_of_animal_consciousness_and_suffering_ng_1995.pdf) p. 1, 1995*\n\n> Random testing is simple in concept, often easy to implement, has been\ndemonstrated to effectively detect failures, is good at exercising systems\nin unexpected ways (which may not occur to a human tester), and may be\nthe only practical choice when the source code and the specifications\nare unavailable or incomplete.\n\n*— Tsong Yueh Chen/Fei-Ching Kuo/Robert G. Merkel/T.H. Tse, [“Adaptive Random Testing: the ART of Test Case Diversity”](https://niplav.github.io/doc/cs/reduction/adaptive_random_testing_chen_et_al_2010.pdf), 2010*\n\nConsider an agent which displays ([von\nNeumman-Morgenstern](https://en.wikipedia.org/wiki/Von_Neumann-Morgenstern_utility_theorem))\ninconsistent [preferences](https://en.wikipedia.org/wiki/Preference),\nfor example choosing two incompatible\noptions in the two scenarios in the [Allais\nparadox](https://en.wikipedia.org/wiki/Allais_paradox), or reliably\ndisplaying [cycles](https://en.wikipedia.org/wiki/Cycle_\\(graph_theory\\))\nin its actions (detecting which actions are in fact caused by\ninconsistent preferences, and not just exotic ones from weird\nabstractions, is considered a separate problem here). We might want\nto interact with that agent, e.g. trade with it, help it (or exploit\nit), or generally know how it will act But how to go about that if the\nagent displays inconsistent preferences? Perhaps it might even be the\ncase that humans are such agents, and find ourselves in a conundrum:\nwe know our preferences are inconsistent and reliably exploitable,\nand that agents with such preferences [reliably fare worse in the\nworld](https://www.lesswrong.com/s/FYMiCeXEgMzsB5stm/p/RQpNHSiWaXTvDxt6R),\nwe might want to change that.\n\nA possible approach to this problem has two steps:\n\n1. Find ways to represent inconsistent preferences with a mathematical structure which can encode all possible violations of the von Neumann-Morgenstern axioms in all their combinations.\n2. Then turn those inconsistent preferences into consistent ones, and then inform the agent about these inconsistencies and their optimal resolutions (or, in the case of trying to help the agent, then enacting these preferences in the real world).\n\nMathematical Formulation of the Problem\n----------------------------------------\n\nDefine a set of possible (von Neumann-Morgenstern) inconsistent\npreferences over a set $W$ of worlds as $\\not\\curlyvee$, and the set\nof consistent preferences over those worlds as $\\curlyvee$. Elements\nfrom those sets are written as $\\succsim \\in \\not\\curlyvee$ and\n$\\succeq \\in \\curlyvee$.\n\nOne way we could approach the problem is by trying to turn those\ninconsistent preferences consistent, i.e. constructing a function $t:\n\\not \\curlyvee \\mapsto \\curlyvee$ that takes an inconsistent preference\n$\\succsim$ and transforms it into a consistent preference $\\succeq$,\nwhile retaining as much of the original structure of the preference\nas possible (it would make little sense if we replaced the original\npreference relation with e.g. indifference over all options).\n\nFormally, we want to find for some given [distance\nmetric](https://en.wikipedia.org/wiki/Metric_\\(mathematics\\))\n$d: \\not \\curlyvee \\times \\curlyvee \\mapsto ℝ$ a function\n$t$ so that\n\n\n$$t= \\underset{t}{\\text{argmin }} d(\\succsim, t(\\succsim)) \\\\\n\\succeq=t(\\succsim)$$\n\n\nI call this function a __turner__, and sometimes call the results of\nthat function the __set of turnings__ (an element from that set is a\n__turning__). The names mostly chosen for not having been used yet in\nmathematics, as far as I know, and because I want to be a little extra.\n\nA solution to the problem of turning inconsistent preferences into\nconsistent ones then has these components:\n\n1. A mathematical structure for representing $\\not \\curlyvee$ and $\\curlyvee$\n\t* Inconsistent preferences over discrete options are represented via [directed graphs](https://en.wikipedia.org/wiki/Directed_graph)\n\t* Inconsistent preferences over [lotteries](https://en.wikipedia.org/wiki/Lottery_\\(probability\\)) of options are represented via\n\t\t* directed graphs over [probability simplices](https://en.wikipedia.org/wiki/Simplex)\n\t\t\t* potentially more exotic structures such as [graphons](https://en.wikipedia.org/wiki/Graphon) or results from [extremal graph theory](https://en.wikipedia.org/wiki/Extremal_graph_theory) might be relevant here, but I haven't investigated these in detail\n\t\t* vector fields on probability simplices\n\t\t* [graphs with edge weights](https://en.wikipedia.org/wiki/Graph_\\(discrete_mathematics\\)#Weighted_graph) in $ℝ$\n2. A specification for $t$\n\t* In the case of discrete options, I propose adding and removing edges from the directed graph\n\t* In the case of lotteries I don't have yet any clear proposals\n3. A specification for $d$\n\t* In the case of discrete options, I propose using the [graph edit distance](https://en.wikipedia.org/wiki/Graph_edit_distance)\n\t* In the case of lotteries I don't yet have any definite proposals\n\nRelated Work\n------------\n\nThis work is closely related to the investigations in [Aird & Shovelain\n2020](https://www.lesswrong.com/posts/ky988ePJvCRhmCwGo/using-vector-fields-to-visualise-preferences-and-make-them)\n(so closely that even though I believe I re-invented the\napproach independently, it might just be that I had read\ntheir work & simply forgotten it), and broadly related to\nthe value extrapolation framework outlined in [Armstrong\n2022](https://www.lesswrong.com/posts/i8sHdLyGQeBTGwTqq/value-extrapolation-concept-extrapolation-model-splintering).\n\nDiscrete Case\n--------------\n\nWhen we have discrete sets of worlds $W$, we can represent\nan inconsistent preference over those worlds by using a directed graph\n$G_{\\succsim}=(W,E_{\\succsim} \\subseteq W \\times W)$.\nThe presence of an edge $(w_1, w_2)$ would mean that $w_1 \\succsim w_2$,\nthat is $w_1$ is preferred to $w_2$.\n\nMathematically, then, $\\not \\curlyvee$ is the set of all possible\ngraphs with edges in $W \\times W$, that is\n$\\not \\curlyvee=\\{(W, E)| E \\in \\mathcal{P}(W \\times W))\\}$).\n\nThe consistent equivalent to an inconsistent preference\nrepresented by a directed graph would be a [path\ngraph](https://en.wikipedia.org/wiki/Path_graph)\n$G_{\\succeq}=(V, E_{\\succeq})$ over the same set of\n[vertices](https://en.wikipedia.org/wiki/Vertex_\\(graph_theory\\)) $W$.\nThe method for transforming $G_{\\succsim}$ into $G_{\\succeq}$ would be\nby adding/deleting the minimal number of vertices from $E_{\\succsim}$.\n\nMathematically, then $\\curlyvee$ is the set of transitive closures\nof all possible path graphs that are encode permutations of $W$; $\\curlyvee=\\{(V, E)^+ | E \\in σ(W)\\}$.\n\n### Example\n\nConsider the following directed graph:\n\n![A directed graph](https://niplav.github.io/img/turning/unbalanced_cycle.png \"A directed graph. It contains nodes {a, b, c, d, e, f, g} and edges a → b → c → e → f → g → b, c → d.\")\n\nHere, $W=\\{a,b,c,d,e,f,g\\}$.\n\nAn edge from $a$ to $b$ means that $a$ is preferred to $b$\n(short $a \\succsim b$). The absence of an edge between two\noptions means that those two options are, from the view of the agent,\n[incomparable](https://en.wikipedia.org/wiki/Comparability).\n\nIt violates the two von Neumann-Morgenstern axioms for discrete options:\n\n* Completeness is violated because for example options $d$ and $e$ are incomparable (and we don't merely have [indifference](https://en.wikipedia.org/wiki/Indifference_curve) between these options)\n* Transitivity is violated because of the $b → c → e → f → g → b$ loop\n\n\n\nA possible turned version of these preferences could then be the\nfollowing graph:\n\n![A messy graph.](https://niplav.github.io/img/turning/turnubc_hyp_trans.png \"A messy graph. Vertices {a, b, c, d, e, f, g, h}. Edges are the transitive closure over the complete order a → b → c → d → e → f → g.\")\n\nThis graph looks quite messy, but it's really just the [transitive\nclosure](https://en.wikipedia.org/wiki/transitive_closure) of this graph:\n\n![A path graph.](https://niplav.github.io/img/turning/turnubc_hyp.png \"A path graph. Vertices again {a, b, c, d, e, f, g, h}. Edges are a → b → c → d → e → f → g.\")\n\nWhether this is the \"right\" way to turn the previous inconsistent\npreferences depends on the choice of distance metric we would like to use.\n\n### Resolving Inconsistencies\n\nIn some sense, we want to change the inconsistent preferences as little\nas possible; the more we modify them, the more displayed preferences we\nhave to remove or change. Since the presence or absence of preferences\nis encoded by the presence or absence of edges on the graph, removing\nedges or adding new edges is equivalent to removing or adding preferences\n(at the moment, we do *not* consider adding or removing vertices: we\nstay firmly inside the agent's [ontology](https://en.wikipedia.org/wiki/Ontology_\\(information_science\\))/world model).\n\nLuckily, there is a concept in computer science called the graph-edit\ndistance: a measure for the difference between two graphs.\n\nThe set of possible editing operations on the graph varies, e.g. Wikipedia lists\n\n> * __vertex insertion__ to introduce a single new labeled vertex to a graph.\n* __vertex deletion__ to remove a single (often disconnected) vertex from a graph.\n* __vertex substitution__ to change the label (or color) of a given vertex.\n* __edge insertion__ to introduce a new colored edge between a pair of vertices.\n* __edge deletion__ to remove a single edge between a pair of vertices.\n* __edge substitution__ to change the label (or color) of a given edge.\n\n*—[English Wikipedia](), [“Graph Edit Distance”](https://en.wikipedia.org/wiki/Graph_Edit_Distance), 2021*\n\nSince we do not have labels on the edges of the graph, and have disallowed\nthe deletion or insertion of vertices, this leaves us with the graph\nedit distance that uses edge insertion and edge deletion.\n\nWe can then write a simple pseudocode algorithm for\n$\\succeq=f(\\succsim)$:\n\n\tturn(G≿=(W, E≿)):\n\t\tmindist=∞\n\t\tfor L in perm(W):\n\t\t\tL=trans_closure(L)\n\t\t\tdist=ged(G≿, R)\n\t\t\tif dist<mindist:\n\t\t\t\tR=L\n\t\t\t\tmindist=dist\n\t\treturn R\n\nwhere `perm(W)` is the set of\n[permutations](https://en.wikipedia.org/wiki/Permutation) on `W`,\n`trans_closure(G)` is the transitive closure of a graph `G`, and `ged(G1,\nG2)` is the graph edit distance from `G1` to `G2`.\n\nOr, mathematically,\n\n$$R=\\underset{R \\in σ(W)}{\\text{argmin }}\\text{GED}(R^+, G_{\\succsim}))$$\n\n#### Implementation\n\nImplementing this in Python 3 using the [networkx](http://networkx.github.io/)\nlibrary turns out to be easy:\n\n\timport math\n\timport networkx as nx\n\timport itertools as it\n\n\tdef turn(graph):\n\tmindist=math.inf\n\t\tworlds=list(graph.nodes)\n\t\tfor perm in it.permutations(worlds):\n\t\t\tperm=list(perm)\n\t\t\tpathgraph=nx.DiGraph()\n\t\t\tfor i in range(0, len(worlds)):\n\t\t\t\tpathgraph.add_node(worlds[i], ind=i)\n\t\t\t# The transitive closure over this particular path graph\n\t\t\t# Simplify to nx.algorithms\n\t\t\tfor i in range(0, len(perm)-1):\n\t\t\t\tpathgraph.add_edge(perm[i], perm[i+1])\n\t\t\tpathgraph=nx.algorithms.dag.transitive_closure(pathgraph)\n\t\t\t# Compute the graph edit distance, disabling node insertion/deletion/substition and edge substitution\n\t\t\tedge_cost=lambda x: 1\n\t\t\tunaffordable=lambda x: 10e10\n\t\t\tsame_node=lambda x, y: x['ind']==y['ind']\n\t\t\tedge_matches=lambda x, y: True\n\t\t\tdist=nx.algorithms.similarity.graph_edit_distance(graph, pathgraph, node_match=same_node, edge_match=edge_matches, node_del_cost=unaffordable, node_ins_cost=unaffordable, edge_ins_cost=edge_cost, edge_del_cost=edge_cost)\n\t\t\tif dist<mindist:\n\t\t\t\tresult=pathgraph\n\t\t\t\tmindist=dist\n\t\treturn result\n\nWe can then test the function, first with a graph with a known best\ncompletion, and then with our [example from above](#Example).\n\nThe small example graph (top left) and its possible turnings are\n(all others):\n\n![A small example](https://niplav.github.io/img/turning/se_comp.png \"Four graphs, side-by side. Top left is a → b, c, top right is a → b → c, a → c, bottom left is a → c → b, a → b, bottom right is c → a → b, c → b.\")\n\n\t>>> smallworld=['a', 'b', 'c']\n\t>>> smallgraph=nx.DiGraph()\n\t>>> for i in range(0, len(smallworld)):\n\t...     smallgraph.add_node(smallworld[i], ind=i)\n\t>>> smallgraph.add_edges_from([('a', 'b')])\n\t>>> smallre=turn(smallworld, smallgraph)\n\t>>> smallre.nodes\n\tNodeView(('a', 'b', 'c'))\n\t>>> smallre.edges\n\tOutEdgeView([('a', 'b'), ('a', 'c'), ('b', 'c')])\n\nThis looks pretty much correct.\n\n\t>>> mediumworld=['a', 'b', 'c', 'd', 'e', 'f', 'g']\n\t>>> mediumgraph=nx.DiGraph()\n\t>>> for i in range(0, len(mediumworld)):\n\t...     mediumgraph.add_node(mediumworld[i], ind=i)\n\t>>> mediumgraph.add_edges_from([('a', 'b'), ('b', 'c'), ('c', 'd'), ('c', 'e'), ('e', 'f'), ('f', 'g'), ('g', 'b')])\n\t>>> mediumres=turn(mediumworld, mediumgraph)\n\t>>> mediumres.nodes\n\tNodeView(('a', 'b', 'c', 'd', 'e', 'f', 'g'))\n\t>>> mediumres.edges\n\tOutEdgeView([('a', 'b'), ('a', 'c'), ('a', 'd'), ('a', 'e'), ('a', 'f'), ('a', 'g'), ('b', 'c'), ('b', 'd'), ('b', 'e'), ('b', 'f'), ('b', 'g'), ('c', 'd'), ('c', 'e'), ('c', 'f'), ('c', 'g'), ('d', 'e'), ('d', 'f'), ('d', 'g'), ('e', 'f'), ('e', 'g'), ('f', 'g')])\n\nThis is actually equal to the hypothesized solution from above (below\nis the non-transitive-closure version):\n\n![A path graph.](https://niplav.github.io/img/turning/turnubc_hyp.png \"A path graph. Vertices again {a, b, c, d, e, f, g, h}. Edges are a → b → c → d → e → f → g.\")\n\n#### Problems with This Method and its Algorithm\n\nThis solution has some glaring problems.\n\n##### Speed (or the Lack Thereof)\n\nSome of you might have noticed that this algorithm is *somewhat\ninefficient* (by which I mean *absolutely infeasible*).\n\nSince we iterate through the permutations of $W$, the runtime is\n$\\mathcal{O}(|W|!)$ (with the added \"benefit\" of additionally computing\nthe [NP-complete](https://en.wikipedia.org/wiki/NP-completeness)\ngraph edit distance inside of the loop, which is also\n[APX](https://en.wikipedia.org/wiki/APX)-hard to approximate).\n\nPossible better approaches would involve finding the\nlongest subgraph that is a path graph, or the [spanning\ntree](https://en.wikipedia.org/wiki/Spanning_tree), perhaps the\n[transitive reduction](https://en.wikipedia.org/wiki/Transitive_reduction)\nis helpful, or maybe the [feedback arc\nset](https://en.wikipedia.org/wiki/Feedback_arc_set)?\n\n\n\n##### Non-Unique Results\n\nAnother, smaller problem is that the algorithm often doesn't have a unique\nresult, as seen in the small example [above](#Resolving-Inconsistencies).\n\nWe can compute the set of all possible turnings with some trivial\nchanges to the algorithm:\n\n\tturn_all(G≿=(W, E≿)):\n\t\tmindist=∞\n\t\tR=∅\n\t\t[…]\n\t\t\tif dist<mindist:\n\t\t\t\tR={L}\n\t\t\t\tmindist=dist\n\t\t\telse if dist==mindist:\n\t\t\t\tR=R∪{L}\n\t\treturn R\n\nand its implementation\n\n\tdef turn_all(graph):\n\t\tresults=set()\n\t\t[…]\n\t\t\tif dist<mindist:\n\t\t\t\tresults=set([pathgraph])\n\t\t\t\tmindist=dist\n\t\t\telif dist==mindist:\n\t\t\t\tresults.add(pathgraph)\n\t\treturn results\n\nThe results, with the small example, are as expected:\n\n\t>>> turnings=list(turn_all(smallworld, smallgraph))\n\t>>> len(turnings)\n\t3\n\t>>> turnings[0].edges\n\tOutEdgeView([('a', 'b'), ('a', 'c'), ('b', 'c')])\n\t>>> turnings[1].edges\n\tOutEdgeView([('a', 'b'), ('c', 'a'), ('c', 'b')])\n\t>>> turnings[2].edges\n\tOutEdgeView([('a', 'c'), ('a', 'b'), ('c', 'b')])\n\n![A small example](https://niplav.github.io/img/turning/se_comp.png \"Four graphs, side-by side. Top left is a → b, c, top right is a → b → c, a → c, bottom left is a → c → b, a → b, bottom right is c → a → b, c → b.\")\n\nFor the big example, after waiting a while for the solution:\n\n\t>>> turnings=list(turn_all(mediumworld, mediumgraph))\n\t>>> len(turnings)\n\t49\n\nI will not list them all, but these are less than the $7!=5040$\npossible options.\n\nThis brings up an interesting question: As we have more and more\nelaborate inconsistent preferences over more worlds, does it\nbecome more likely that they have a unique consistent preference\nthey can be turned to? Or, in other words, if we make the graphs\nbigger and bigger, can we expect the fraction of inconsistent\npreferences with a unique turning to grow or shrink (strictly)\n[monotonically](https://en.wikipedia.org/wiki/Monotonic_function)? Or\nwill it just oscillate around wildly?\n\nMore formally, if we define $\\mathcal{G}_n$ as the set of graphs\nwith $n$ nodes, and $\\mathcal{U}_n=\\{G \\in \\mathcal{G}_n | 1=|\\text{turn_all}(G)|\\}$\nas the set of graphs with $n$ nodes that\nhave unique path graphs associated with them.\n\nWe can further define the set of all graphs wwith $n$ nodes\nwith $m$ turnings as\n$\\mathcal{T}{n,m}=\\{G \\in \\mathcal{G}_n | m=|\\text{turn_all}(G)|\\}$\n(of which $\\mathcal{U}_n=\\mathcal{T}_{n, 1}$\nis just a special case).\n\nWe can call the size of the set of all turnings of a graph the\n__confusion__ of that graph/set of inconsistent preferences: If the\ngraph is already the transitive closure of a path graph, the size of\nthat set is (arguendo) 1: there are no other possible turnings. If the\ngraph contains no edges (with $n$ nodes), the confusion is maximal with\n$n!$, the preferences carry the minimal amount of meaning.\n\n###### Minimal and Maximal Number of Turnings\n\nThe minimal number of turnings a graph can have is 1, with a graph-edit\ndistance of 0: any transitive closure of a path graph satisfies this\ncriterion (if your preferences are already consistent, why change them\nto be more consistent?)\n\nHowever, those graphs aren't the only graphs with exactly one turning,\nconsider the following graph (left) and a possible turning (right)\n(with graph-edit distance 1; the changed edge is red, a nice opportunity\nfor some [rubrication](https://gwern.net/Red)):\n\n![Image of two graphs, left has edges a→ b→ c→ d, a→ c, b→ d, d→ a, right graph is the same except d→ a is now a→ d.](https://niplav.github.io/img/turning/counter_comp.png \"Image of two graphs, left has edges a→ b→ c→ d, a→ c, b→ d, d→ a, right graph is the same except d→ a is now a→ d.\")\n\nOne can easily see that it has exactly one turning, and checking with\nthe code confirms:\n\n\t>>> counter=nx.DiGraph()\n\t>>> counterworld=['a', 'b', 'c', 'd']\n\t>>> for i in range(0, len(smallworld)):\n\t...\tsmallgraph.add_node(smallworld[i], ind=i)\n\t>>> counter.add_edges_from([('a', 'b'), ('b', 'c'), ('c', 'd'), ('a', 'c'), ('b', 'd'), ('d', 'a')])\n\t>>> counterres=list(turn_all(counter))\n\t>>> len(counterres)\n\t>>> >>> counterres[0].edges\n\tOutEdgeView([('a', 'b'), ('a', 'c'), ('a', 'd'), ('b', 'c'), ('b', 'd'), ('c', 'd')])\n\nFor a graph with $n$ nodes the maximal number of turnings it is\nupper-bounded by $n!$, and a sufficient condition for the graph to\nhave that many turnings is when the graph is the union of a set of\n[complete digraphs](https://en.wikipedia.org/wiki/Complete_graph) with\ndisjoint nodes. For example the graph with 4 nodes and no edges has 24\npossible turnings, as does the graph with 4 nodes and two edges $\\{(1,2),\n(2,1)\\}$.\n\nWe can prove this inductively: When considering a node-labeled graph\nwith $n$ nodes and no edges, the graph edit distance to any path\ngraph variant of that graph is the same, because we always have to\nadd $n-1+n-2+n-3 \\dots 1=\\frac{n-1+(n-1)^2}{2}$ edges to reach\nany transitive closure of a path graph (by the [sum of any arithmetic\nprogression](https://en.wikipedia.org/wiki/Arithmetic_progression#Sum)).\nLet not $G^{\\circ}$ be a graph with $n$ nodes that is solely the\nunion of complete digraphs with disjoint nodes. When we now pick two nodes\n$u$ and $v$ from $G^{\\circ}$ and add the edges $\\{(u,v), (v,u)\\}\n\\cup \\{(v, x)|(u,x) \\in E^{\\circ}\\} \\cup \\{(u, y)|(v,x) \\in E^{\\circ}\\}\\}\n\\cup \\{(x, y)|(u,x) \\in E^{\\circ}, (v,y) \\in E^{\\circ}\\}$ (that is,\nwe connect $u$ and $v$, and all their neighbors) to $G^{\\circ}$,\nwe have necessarily increased the graph-edit distance to any path graph\nby the same amount, we have symmetrically added edge-pairs that need to\nbe broken in either direction.\n\n### Questions\n\nOne can now pose several (possibly distracting) questions:\n\n* Does it matter whether we give `turn` a graph $G$ or the transitive closure of $G$?\n* Is there a more efficient algorithm to compute the turning?\n\t* Can it at least be made exponential?\n\t* Can we exploit the fact that we're always computing the graph-edit distance to a path-graph?\n* As we add more options to our inconsistent preferences, do they become more likely to turn uninuely?\n\t* That is: Does it hold that $\\frac{|\\mathcal{U}_n|}{|\\mathcal{G}_n|}<\\frac{|\\mathcal{U}_{n+1}|}{|\\mathcal{G}_{n+1}|}$?\n\t* It should be possible to check this for small cases.\n\n#### Number of Turnings for $\\mathcal{G}_n$\n\n* In general, how does the size of $\\mathcal{U}_n$ develop? What about $\\mathcal{T}_{n,2}$, or in general $\\mathcal{T}_{n,m}$?\n\t* Does the average number of turnings for inconsistent preferences converge to a specific number?\n\t* That is, what is $\\lim_{n \\rightarrow \\infty} \\frac{1}{\\mathcal{G}_n} \\sum_{i=1}^{n} \\mathcal{T}_{n,i}$?\n\t* I predict [20% on the number monotonically increasing](https://predictionbook.com/predictions/208357), [50% on monotonically decreasing](https://predictionbook.com/predictions/208358) and [30% on showing no clear pattern](https://predictionbook.com/predictions/208359).\n\nWe can check these empirically! While it would be nice to prove anything\nabout them, it's much nicer to investigate them computationally. This is\npretty straightforward: For increasing $n$, generate $\\mathcal{G}_n$,\nfor every $G \\in \\mathcal{G}_n$, compute $|\\text{turn_all}(G)|$, save\nthe data in a file somewhere, and do interesting things with that data.\n\nIn code, we first generate all directed graphs with $n$ nodes with a\nrecursive function\n\n\tdef all_directed_graphs(n):\n\t\tif n<=0:\n\t\t\treturn [nx.DiGraph()]\n\t\tgraphs=all_directed_graphs(n-1)\n\t\tnewgraphs=[]\n\t\tfor g in graphs:\n\t\t\tg.add_node(n, ind=n)\n\t\t\tfor tosubset in powerset(range(1, n+1)):\n\t\t\t\tfor fromsubset in powerset(range(1, n)):\n\t\t\t\t\tgnew=g.copy()\n\t\t\t\t\tfor element in tosubset:\n\t\t\t\t\t\tgnew.add_edge(n, element)\n\t\t\t\t\tfor element in fromsubset:\n\t\t\t\t\t\tgnew.add_edge(element, n)\n\t\t\t\t\tnewgraphs.append(gnew)\n\t\treturn newgraphs\n\nand start turning:\n\n\tmax=16\n\tfor i in range(0,max):\n\t\tgraphs=turn.all_directed_graphs(i)\n\t\tfor g in graphs:\n\t\t\tprint('{0},{1},\"{2}\"'.format(i, len(turn.turn_all(g)), g.edges))\n\nHowever, my computer quickly freezes and I find out that this is a lot\nof graphs:\n\n\t>>> [len(list(all_directed_graphs(i))) for i in range(0,5)]\n\t[1, 2, 16, 512, 65536]\n\nSo the number directed graphs with 5 nodes would be\n$2^{32}=4294967296$, far too many for my puny laptop. But\ninstead of generating them all, one can just generate a\nrandom sample and test on that, using the [Erdős–Rényi\nmodel](https://en.wikipedia.org/wiki/Erdős-Rényi_model),\nfor which networkx has the helpful function\n`generators.random_graphs.gnp_random_graph` (Wikipedia informs us that\n\"In particular, the case $p=\\frac{1}{2}$  corresponds to the case\nwhere all $2^{\\binom {n}{2}}$ graphs on $n$ vertices are chosen\nwith equal probability.\"). We have to randomly add reflexive edges (not\nincluded in the model, it seems) with probability $\\frac{1}{2}$ each,\nand labels for the nodes, and then we're good to go:\n\n\tsamples=256\n\tfor i in range(5,lim):\n\t\tfor j in range(0,samples):\n\t\t\tg=nx.generators.random_graphs.gnp_random_graph(i, 0.5, directed=True)\n\t\t\tfor n in g.nodes:\n\t\t\t\tg.add_node(n, ind=n)\n\t\t\t\tif random.random()>=0.5:\n\t\t\t\t\tg.add_edge(n,n)\n\t\t\tprint('{0},{1},\"{2}\"'.format(i, len(turn.turn_all(g)), g.edges))\n\nWe now run the script in the background, happily collecting data for us\n(`python3 collect.py >.https://niplav.github.io/../data/turnings.csv &`), and after a nice\nround of editing this text go back and try to make sense of the data,\nwhich runs squarely counter my expectations:\n\n\t>>> import pandas as pd\n\t>>> df=pd.read_csv('data/turnings.csv')\n\t>>> df.groupby(['0']).mean()\n\t           1\n\t0\n\t1   1.000000\n\t2   1.875000\n\t3   3.941406\n\t4   9.390289\n\t5  21.152344\n\t6  39.885246\n\nIt seems like the mean number of turnings actually increases\nwith the graph size! Surprising. I'm also interested in the\nexact numbers: Why *exactly* 3.390289… for the graphs with 4\nnodes? What is so special about that number‽ (Except it being\nthe [longitude](https://en.wikipedia.org/wiki/Longitude)\nof the [Cathedral Church of\nChrist](https://en.wikipedia.org/wiki/Cathedral_Church_of_Christ)\nin Lagos).\n\nLooking at unique turnings turns (hehe) up further questions:\n\n\t>>> def uniqueratio(g):\n\t...     return len(g.loc[g['1']==1])/len(g)\n\t...\n\t>>> df.groupby(['0']).apply(uniqueratio)\n\t0\n\t1    1.000000\n\t2    0.125000\n\t3    0.089844\n\t4    0.055542\n\t5    0.050781\n\t6    0.016393\n\tdtype: float64\n\t>>> def uniques(g):\n\t...     return len(g.loc[g['1']==1])\n\t>>> df.groupby(['0']).apply(uniques)\n\t0\n\t1       2\n\t2       2\n\t3      46\n\t4    3640\n\nVery much to my surprise, searching for \"2,2,46,3640\" [in the\nOEIS](https://oeis.org/search?q=2%2C2%2C46%2C3640&sort=&language=english&go=Search)\nyields *no results*, even though the sequence really looks like something\nthat would already exist! (I think it has a specifically graph-theoretic\n\"feel\" to it). But apparently not so, I will submit it soon.\n\n\n\nI omit the number of unique turnings for 5 and 6, for obvious reasons\n(I also believe that the ratio for 6 is an outlier and should not be\ncounted). The number of unique resolutions for the graph with 1 node\nmakes sense, though: Removing the reflexive edge should count as\none edge action, but the graph only has one unique resolution:\n\n\t>>> df.loc[df['0']==1]\n\t   0  1        []\n\t0  1  1        []\n\t1  1  1  [(1, 1)]\n\n\n\nEncoding Inconsistencies\n------------------------\n\n### Theory\n\nAssuming that we have a set of axioms that describe which preferences\nare consistent and which are inconsistent, for the purposes of this\ntext, we want to ideally find a set $\\not \\curlyvee$ of mathematical\nstructures that\n\n1. can represent preferences that violate each possible subset of those axioms.\n\t1. Each inconsistent preference should have exactly one element of $\\not \\curlyvee$ that represents it\n2. has a strict subset $\\curlyvee \\subset \\not \\curlyvee$ so that $\\curlyvee$ can represent only consistent preferences.\n\n### Discrete Case\n\nThe two relevant von Neumman-Morgenstern axioms are completeness and\ntransitivity, with a directed graph one can also represent incompleteness\nand intransitivity.\n\n#### Incompleteness\n\nIncompleteness (or incomparability) between two options $w_1, w_2$\ncan be represented by not specifying an edge between the two options,\nthat is $(w_1, w_2) \\not \\in E, (w_2, w_1) \\not \\in E$.\n\n![](https://niplav.github.io/img/turning/incomplete.png)\n\n#### Intransitivity\n\nIntransitivity can be represented by cycles in the graph:\n\n![](https://niplav.github.io/img/turning/intransitive.png)\n\n#### Non-Encodable Inconsistencies\n\nWith option set $\\{a,b\\}$ have preference $a \\succsim b$, with\noption set $\\{a,b,c\\}$ have preferences\n$b \\succsim a, a \\succsim c, b \\succsim c$.\n\n### Continuous Case\n\n#### Incompleteness\n\n* Minima/maxima in the vector field\n* Discontinuities\n* Undifferentiable points\n\n#### Intransitivity\n\nCurl in the vector field?\n\n#### Discontinuity\n\nCan only exist with incompleteness?\n\n#### Dependence\n\n### Discussion\n\nThis leads to an interesting ethical consideration: is it a larger change\nto a preference relation to add new information or remove information?\n\nIt is discussed how to incorporate those weights into an algorithm for\nminimally transforming $G_{\\succsim}$ into $G_{\\succeq}$.\n\nContinuous Case\n----------------\n\n### Vector Fields over Probability Simplices\n\nVector field over the probability simplex over the options (representing\nlocal preferences over lotteries).\n\n#### Resolving Inconsistencies\n\nFind mapping from vector field to another that makes the vector field\nconsistent by minimizing the amount of turning/shrinking the vectors\nhave to perform.\n\n### Graphons\n\n?\n\nLook into extremal graph theory.\n\nImplications for AI Alignment\n------------------------------\n\n> I've seen six cities fall for this  \nmathematics with incompetence  \nred flags stand among the trees  \nrepugnant symphonies  \na billionaires tarantula just ate the ceiling  \nthinking it was yet another floor\n\n*—[Patricia Taxxon](http://patriciataxxon.bandcamp.com/), [“Hellbulb”](https://patriciataxxon.bandcamp.com/track/hellbulb) from [“Gelb”](https://patriciataxxon.bandcamp.com/album/gelb), 2020*\n\n### Ambitious Value Learning\n\nLearn human values, check if known inconsistencies are encoded (to ensure\nlearning at the correct level of abstraction), then make consistent.\n\n### Ontological Crises\n\n> Furthermore, there remain difficult philosophical problems. We have\nmade a distinction between the agent’s uncertainty about which model\nis correct and the agent’s uncertainty about which state the world is\nin within the model. We may wish to eliminate this distinction; we\ncould specify a single model, but only give utilities for some states\nof the model. We would then like the agent to generalize this utility\nfunction to the entire state space of the model.\n\n*—Peter de Blanc, [“Ontological Crises in Artificial Agents’ Value Systems”](https://niplav.github.io/doc/cs/ai/alignment/ontological_crises/ontological_crises_in_artificial_agents_value_systems_de_blanc_2011.pdf), 2010*\n\nIf you know a mapping between objects from human to AI ontology, you\ncould find the mapping from the (consistent) human probability simplex\nto the AI simplex?\n\n#### Discrete Case\n\nA node splits in two or more, or two or more nodes get merged. If the\nthen resulting graph isn't a path graph, it can be turned with the\nmethod described above.\n\nFurther Questions\n------------------\n\n* Does every graph $G$ have a unique graph $G'$ so that $G$ is the transitive closure of $G'$?\n* There is something interesting going on with lattices (?) over individual transitivity operations\n\nAcknowledgements\n------------------------\n\nThanks to Miranda Dixon-Luinenburg for finding some typos."
    },
    "voteCount": 8,
    "forceInclude": true
  },
  {
    "_id": "G6BenMvcFR5CEC9uc",
    "url": null,
    "title": "Definition Practice: Applied Rationality",
    "slug": "definition-practice-applied-rationality",
    "author": "ChristianKl",
    "question": true,
    "tags": [
      {
        "name": "Rationality"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "In the exercise of defining what we mean by a term, we explore our assumptions about the nature of the term and what it’s pointing at. Sometimes this leads us to get a better handle on a concept. Other times, it shows us our confusion.\n\nAs an example of the value of definitions, [YCombinator considers](https://www.ycombinator.com/library/4b-how-to-pitch-your-company) it important for founders to be able to clearly and concisely say a company's purpose, and that the ability to do so is indicative of plausible success..\n\nHere are two exercises:\n\n1.  Write a paragraph to define the term *applied rationality.*\n2.  Write a sentence to define the term *applied rationality.*\n\nI encourage you to write your definitions before reading what others wrote. While I suggest that you first attempt to define the term in a paragraph and then move to the sentence variation, you can also change the order if that seems easier.\n\nPlease use the answer feature for your answer to these exercises, and the comment function for other needs."
    },
    "voteCount": 3,
    "forceInclude": true
  },
  {
    "_id": "caB6ApqG7rSFayoaq",
    "url": null,
    "title": "What are the simplest questions in applied rationality where you don't know the answer to? ",
    "slug": "what-are-the-simplest-questions-in-applied-rationality-where",
    "author": null,
    "question": true,
    "tags": [
      {
        "name": "Rationality"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "In the podcast between Spencer Greenberg and Buck Shlegeris, [Taking pleasure in being wrong](https://open.spotify.com/episode/4Q54moES6YvjCEyOORRgX3), Buck says:\n\nI think that when you are learning subjects something that you should really be taken your eye out for is the simplest question in the field that you don't know the answer to.\n\n> I think a lot of the time people try to learn physics or something and their approach is as quickly as possible to answer hard questions about complicated subjects. And I think that's what I thought was cool when I was younger. They delighted at questions that were at the limit of fanciness that they could possibly answer and it feels to me now that it is a lot more productive to seek out questions that are as simple sounding as possible while still being really hard to answer. Or that still demonstrate that there's something you don't understand about the subject.\n> \n> \\[...\\] \n> \n> It seems like we should be seeking out these most basic questions in the hope of finding holes in the foundation of our knowledge.\n\nIf we apply that approach to applied rationality, what questions do you have that seem to be simple but where you don't know the answer?"
    },
    "voteCount": 10,
    "forceInclude": true
  },
  {
    "_id": "xrJjuFfBFD33eHsiP",
    "url": null,
    "title": "Are there practical exercises for developing the Scout mindset?",
    "slug": "are-there-practical-exercises-for-developing-the-scout",
    "author": null,
    "question": true,
    "tags": [
      {
        "name": "Exercises / Problem-Sets"
      },
      {
        "name": "Rationality"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "I'm thinking about doing a LessWrong meetup around the Scout mindset. I prefer to have meetups where there are a lot of two-person exercises as those are really good to get nerds who have trouble with normal small talk to connect to each other in addition to often also being useful for developing skills over lecturing the whole time. Can anyone think of good exercises that could be done at a Scout mindset meetup?"
    },
    "voteCount": 3,
    "forceInclude": true
  },
  {
    "_id": "CjFZeDD6iCnNubDoS",
    "url": null,
    "title": "Humans provide an untapped wealth of evidence about alignment",
    "slug": "humans-provide-an-untapped-wealth-of-evidence-about",
    "author": "TurnTrout",
    "question": false,
    "tags": [
      {
        "name": "AI"
      },
      {
        "name": "Human Values"
      },
      {
        "name": "Ontology"
      },
      {
        "name": "Shard Theory"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "*This post has been recorded as part of the LessWrong Curated Podcast, and can be listened to on* [*Spotify*](https://open.spotify.com/episode/0jpI7LLNzKsn6lwrsoDCc9)*,* [*Apple Podcasts*](https://podcasts.apple.com/us/podcast/humans-provide-an-untapped-wealth-of-evidence/id1630783021?i=1000575990542)*, and* [*Libsyn*](https://five.libsyn.com/episodes/view/23991000)*.*\n\n* * *\n\n**TL;DR:** To even consciously consider an alignment research direction, [you should have evidence](https://www.readthesequences.com/Privileging-The-Hypothesis) to locate it as a promising lead. As best I can tell, many directions seem interesting but do not have strong evidence of being “entangled” with the alignment problem such that I expect them to yield significant insights. \n\nFor example, “we can solve an easier version of the alignment problem by first figuring out how to build an AI which maximizes the number of real-world diamonds” has intuitive appeal and plausibility, but this claim doesn’t *have* to be trueand this problem does not *necessarily* have a natural, compact solution. In contrast, there do *in fact* exist humans who care about diamonds. Therefore, there are guaranteed-to-exist alignment insights concerning the way people come to care about e.g. real-world diamonds.\n\n*“Consider how humans navigate the alignment subproblem you’re worried about” is a habit which I (TurnTrout) picked up from Quintin Pope. I wrote the post, he originated the tactic. *\n\n* * *\n\n> A simplified but still very difficult open problem in[ AI alignment](https://arbital.com/p/ai_alignment/) is to state an unbounded program implementing a[ diamond maximizer](https://arbital.com/p/diamond_maximizer/) that will turn as much of the physical universe into diamond as possible. The goal of \"making diamonds\" was chosen to have a crisp-seeming definition for our universe (the amount of diamond is the number of carbon atoms covalently bound to four other carbon atoms). If we can crisply define exactly what a 'diamond' is, we can avert issues of trying to convey[ complex values](https://arbital.com/p/complexity_of_value/)into the agent.\n> \n> [Ontology identification problem](https://arbital.com/p/ontology_identification/), Arbital\n\nI find this problem interesting, both in terms of wanting to know how to solve a reframed version of it, and in terms of what I used to think about the problem. I usedto^[\\[1\\]](#fn3crz71o4jhu)^ think, “yeah, ‘diamond’ is relatively easy to define. Nice [problem relaxation](https://www.lesswrong.com/posts/JcpwEKbmNHdwhpq5n/problem-relaxation-as-a-tactic).” It felt like the diamond maximizer problem let us focus on the challenge of making the AI’s values bind to *something at all *which we actually intended (e.g. diamonds), in a way that’s robust to ontological shifts and that doesn’t collapse into wireheading or tampering with e.g. the sensors used to estimate the number of diamonds.\n\nAlthough the details are mostly irrelevant to the point of this blog post, the Arbital article suggests some solution ideas and directions for future research, including:\n\n1.  Scan [AIXI-*tl*](https://archive.org/details/arxiv-cs0004001)’s Turing machines and locate diamonds within their implicit state representations.\n2.  Given how [inaccessible](https://ai-alignment.com/inaccessible-information-c749c6a88ce?gi=11e30aac9afb) we expect AIXI*-tl*’s representations to be by default, have AIXI-*tl *just consider a Turing-complete hypothesis space which uses more interpretable representations.\n3.  “Being able to describe, in purely theoretical principle, a prior over epistemic models that have at least two levels and can switch between them in some meaningful sense”\n\nDo you notice anything *strange* about these three ideas? Sure, the ideas don’t seem workable, but they’re good initial thoughts, right?\n\nThe problem *isn’t* that the ideas aren’t clever enough. Eliezer is pretty dang clever, and these ideas are reasonable stabs given the premise of “get some AIXI variant to maximize diamond instead of reward.”\n\nThe problem *isn’t* that it’s impossible to specify a mind which cares about diamonds. We already know that there are intelligent minds who value diamonds. You might be dating one of them, or you might even *be* one of them! Clearly, the genome + environment jointly specify certain human beings who end up caring about diamonds. \n\nOne problem is [*where is the evidence required to locate these ideas*](https://www.readthesequences.com/Privileging-The-Hypothesis)? Why should I even find myself thinking about diamond maximization and AIXI and Turing machines and utility functions in this situation? It’s not that there’s *no *evidence. For example, utility functions [ensure the agent can’t be exploited in some dumb ways](https://www.lesswrong.com/posts/RQpNHSiWaXTvDxt6R/coherent-decisions-imply-consistent-utilities). But I think that the supporting evidence is not *commensurate* with the specificity of these three ideas or with the specificity of the “ontology identification” problem framing.\n\nHere’s an exaggeration of how these ideas feelto me when I read them: \n\n> “I lost my phone”, you tell your friend. \n> \n> They ask, “Have you checked [`Latitude: -34.44006, Longitude: -64.61333`](https://gps-coordinates.org/my-location.php?lat=-34.44006&lng=-64.61333)?” \n> \n> Uneasily, you respond: “Why would I check there?” \n> \n> Your friend shrugs: “Just seemed promising. And it’s on land, it’s not in the ocean. Don’t worry, I incorporated evidence about where you probably lost it.”\n\nI [recently made a similar point](https://www.lesswrong.com/posts/dqSwccGTWyBgxrR58/turntrout-s-shortform-feed?commentId=CXdcb9sMLkgLANrTv#CXdcb9sMLkgLANrTv) about [Cooperative Inverse Reinforcement Learning](https://arxiv.org/abs/1606.03137): \n\n> *Against CIRL* as a special case of *against quickly jumping into highly specific speculation while ignoring empirical embodiments-of-the-desired-properties.*\n> \n> In the context of \"how do we build AIs which help people?\", asking \"does CIRL solve corrigibility?\" is hilariously unjustified.[ By what evidence](https://www.readthesequences.com/Privileging-The-Hypothesis) have we located such a specific question? We have assumed there is an achievable \"corrigibility\"-like property; we have assumed it is good to have in an AI; we have assumed it is good in a similar way as \"helping people\"; we have elevated CIRL in particular as a formalism worth inquiring after. \n> \n> But this is ***not the first question to ask**, *when considering \"sometimes people want to help each other, and it'd be great to build an AI which helps us in some way.\" Much better to start with *existing* generally intelligent systems (humans) which *already* sometimes act in the way you want (they help each other) and ask after the ***guaranteed-to-exist reason*** why this empirical phenomenon happens.\n\nNow, if you are confused about a problem, it can be better to explore *some *guesses than no guesses—perhaps it’s better to think about Turing machines than to stare helplessly at the wall (but perhaps not). Your best guess may be wrong (e.g. write a utility function which scans Turing machines for atomic representations of diamonds), but you sometimes still learn something by spelling out the implications of your best guess (e.g. the ontology identifier stops working when AIXI Bayes-updates to non-atomic physical theories). This can be productive, as long as you keep in mind the wrongness of the concrete guess, so as to not become anchored on that guess or on the framing which originated it (e.g. build a diamond *maximizer*).\n\nHowever, in this situation, I want to look elsewhere. When I confront a confusing, difficult problem (e.g. how do you create a mind which cares about diamonds?), I often first look at reality (e.g. are there any existing minds which care about diamonds?). Even if I have *no idea *how to solve the problem, if I can find an existing mind which cares about diamonds, then *since that mind is **real**,* that mind has a[*guaranteed-to-exist*](https://en.wikipedia.org/wiki/Biomimetics) *causal mechanistic play-by-play origin story* for why it cares about diamonds. I thereby anchor my thinking to reality; reality is sturdier than “what if” and “maybe this will work”; many human minds *do *care about diamonds. \n\nIn addition to “there’s a guaranteed causal story for humans valuing diamonds, and not one for AIXI valuing diamonds”, there’s a second benefit to understanding how human values bind to the human’s beliefs about real-world diamonds. This second benefit is practical: I’m pretty sure the way that *humans *come to care about diamonds has nearly nothing to do with the ways AIXI-*tl *might be motivated to maximize diamonds. This matters, because I expect that the first AGI’s value formation will be *far* more mechanistically similar to within-lifetime human value formation, than to AIXI-*tl*’svalue alignment dynamics.\n\nNext, it *can *be true that the existing minds are too hard for us to understand in ways relevant to alignment. One way this could be true is that human values are a \"[mess](https://www.readthesequences.com/Terminal-Values-And-Instrumental-Values)\", that \"[our brains are kludges slapped together by natural selection.](https://www.readthesequences.com/Rationality-An-Introduction)\" If human value formation *were* sufficiently complex, with sufficiently many load-bearing parts such that each part drastically affects human alignment properties, then we might instead want to design simpler human-comprehensible agents and study *their* alignment properties. \n\nWhile I think that human *values *are complex, I think the evidence for human value *formation*’s essential complexity is surprisingly weak, all things reconsidered in light of modern, post-deep learning understanding. Still... maybe humans *are* too hard to understand in alignment-relevant ways!\n\nBut, I mean, come on. Imagine an alien^[\\[2\\]](#fn7qhggh3bak6)^ visited and told you:\n\n> Oh yeah, the AI alignment problem. We knocked that one out a while back. [Information inaccessibility of the learned world model](https://ai-alignment.com/inaccessible-information-c749c6a88ce?gi=11e30aac9afb)? No, I’m pretty sure [we didn’t solve that](https://www.lesswrong.com/posts/CQAMdzA4MZEhNRtTp/human-values-and-biases-are-inaccessible-to-the-genome), but we didn’t have to. We built this protein computer and trained it with, I forget actually, was it just what you would call “deep reinforcement learning”? Hm. Maybe it was more complicated, maybe not, I wasn’t involved. \n> \n> We *might* have hardcoded relatively crude reward signals that are basically defined over sensory observables, like a circuit which activates when their sensors detect a [certain kind of carbohydrate](https://en.wikipedia.org/wiki/Sugar). Scanning you, it looks like some of the protein computers ended up with *your values*, even. Small universe, huh?\n> \n> Actually, I forgot how we did it, sorry. And I can’t make guarantees that our approach scales beyond your intelligence level or across architectures, but maybe it does. I have to go, but here are a few billion of the trained protein computers if you want to check them out!\n\nIgnoring the weird implications of the aliens existing and talking to you like this, and considering only the alignment implications—*The absolute top priority of many alignment researchers should be figuring out how the hell the aliens got as far as they did*.^[\\[3\\]](#fnk1djcjup0qc)^ Whether or not you know if their approach scales to further intelligence levels, whether or not their approach seems easy to understand, you have learned that these computers are *physically possible, practically trainable entities*. These computers have definite existence and guaranteed explanations. Next to these actually existent computers, speculation like “maybe [attainable utility preservation](https://www.lesswrong.com/posts/75oMAADr4265AGK3L/attainable-utility-preservation-concepts) leads to cautious behavior in AGIs” is dreamlike, unfounded, and untethered.\n\nIf it turns out to be currently too hard to understand the aligned protein computers, then I want to keep coming back to the problem with each major new insightI gain. When I learned about [scaling laws](https://arxiv.org/abs/2001.08361), I should have rethought my picture of human value formation—Did the new insight knock anything loose? I should have checked back in when I heard about [mesa optimizers](https://www.alignmentforum.org/posts/pL56xPoniLvtMDQ4J/the-inner-alignment-problem), about the [Bitter Lesson](http://incompleteideas.net/IncIdeas/BitterLesson.html), about [the feature universality hypothesis](https://distill.pub/2020/circuits/zoom-in/#claim-3) for neural networks, about [natural abstractions](https://www.alignmentforum.org/posts/wuJpYLcMEBz4kcgAn/what-is-abstraction-1#Natural_Abstractions).\n\nBecause, given my life’s present ambition (solve AI alignment), that’s what it makes sense for me to do—at each major new insight, to reconsider my models^[\\[4\\]](#fnmpfloj2azs)^ of the *single known empirical example of general intelligences with values*, to scour the Earth for every possible scrap of evidence that humans provide about alignment. We may not get much time with human-level AI before we get to superhuman AI. But we get plenty of time with human-level humans, and we get plenty of time *being *a human-level intelligence. \n\nThe way I presently see it, [the godshatter of human values](https://www.lesswrong.com/posts/cSXZpvqpa9vbGGLtG/thou-art-godshatter)—the rainbow of desires, from friendship to food—is only [unpredictable](https://www.lesswrong.com/posts/34Gkqus9vusXRevR8/late-2021-miri-conversations-ama-discussion#YkhmywLQetjekM7e3) relative to a class of hypotheses which fail to predict the shattering.^[\\[5\\]](#fn7a5ti4623qb)^ But confusion is in the map, not the territory. I do not consider human values to be “unpredictable” or “weird”, I do not view them as a “hack” or a “kludge.” Human value formation may or may not be messy (although I presently think *not*). Either way, human values are, of course, part of our lawful reality. Human values are reliably produced by within-lifetime processes within the brain. This has an explanation, though I may be ignorant of it. Humans usually bind their values to certain objects in reality, like dogs. This, too, has an explanation.  \n\nAnd, to be clear, I don’t want to black-box outside-view extrapolate from the “human datapoint”; I don’t want to focus on thoughts like “Since alignment ‘works well’ for dogs and people, maybe it will work well for slightly superhuman entities.” I aspire for the kind of alignment mastery which lets me build a diamond-producing AI, or if that didn’t suit my fancy, I’d turn around and tweak the process and the AI would press green buttons forever instead, or—if I were playing for real—I’d align that system of mere circuitry with humane purposes.\n\nFor that ambition, the inner workings of those generally intelligent apes is *invaluable evidence* about the *mechanistic within-lifetime process by which those apes form their values, *and, more generally, about how intelligent minds can form values at all. What factors matter for the learned values, what factors don’t, and what we should do for AI. Maybe humans have special inductive biases or architectural features, and without those, they’d grow totally different kinds of values. But if that *were *true, wouldn’t that be important to know?\n\nIf I knew how to interpret the available evidence, I probably *would *understand how I came to weakly care about diamonds, and what factors were important to that process (which reward circuitry had to fire at which frequencies, what concepts I had to have learned in order to grow a value around “diamonds”, how precisely activated the reward circuitry had to be in order for me to end up caring about diamonds).\n\nHumans provide huge amounts of evidence, *properly interpreted*—and therein lies the grand challenge upon which I am presently fixated. In an upcoming post, I’ll discuss one particularly rich vein of evidence provided by humans.\n\n*Thanks to Logan Smith and Charles Foster for feedback. Spiritually related to but technically distinct from* [*The First Sample Gives the Most Information*](https://www.lesswrong.com/posts/sTwW3QLptTQKuyRXx/the-first-sample-gives-the-most-information)*.*\n\nEDIT: In this post, I wrote about the Arbital article's unsupported jump from \"Build an AI which cares about a simple object like diamonds\" to \"Let's think about ontology identification for AIXI-*tl.*\" The point is not that there is no valid reason to consider the latter, but that the jump, as written, seemed evidence-starved. For *separate* reasons, I currently think that ontology identification is unattractive in some ways, but this post isn't meant to argue against that framing in general. The main point of the post is that humans provide tons of evidence about alignment, by virtue of containing guaranteed -to-exist mechanisms which produce e.g. their values around diamonds.\n\nAppendix: One time I didn’t look for the human mechanism\n========================================================\n\nBack in 2018, I had [a clever-seeming idea](https://www.lesswrong.com/s/vLArRpNdkex68oem8/p/BMj6uMuyBidrdZkiD). We don’t know how to build an aligned AI; we want multiple tries; it would be great if we could build an AI which “[knows it may have been incorrectly designed](https://arbital.com/p/hard_corrigibility/)”; so why not have the AI simulate its probable design environment over many misspecifications, and then *not* do plans which tend to be horrible for most initial conditions. While I drew some inspiration from how I would want to reason in the AI’s place, I ultimately did not think thoughts like:\n\n> We know of a single group of intelligent minds who have ever wanted to be corrigible and helpful to each other. I wonder how that, in fact, happens? \n\nInstead, I was trying out clever, off-the-cuff ideas in order to solve e.g. Eliezer’s formulation of the [hard problem of corrigibility.](https://arbital.com/p/hard_corrigibility/) However, my idea and his formulation suffered a few disadvantages, including:\n\n1.  The formulation is not guaranteed to describe a probable or “natural” kind of mind,\n2.  These kinds of “corrigible” AIs are not guaranteed to produce desirable behavior, but only *imagined *to produce good behavior, \n3.  My clever-seeming idea was not at all constrained by reality to actually work in practice, as opposed to just sounding clever to me, and\n4.  I didn’t have a concrete use case in mind for what to *do *with a “corrigible” AI.\n\nI wrote this post as someone who previously needed to read it. \n\n1.  ^**[^](#fnref3crz71o4jhu)**^\n    \n    I now think that diamond’s physically crisp definition is a red herring. More on that in future posts.\n    \n2.  ^**[^](#fnref7qhggh3bak6)**^\n    \n    This alien is written to communicate my current belief state about how human value formation works, so as to make it clear why, *given *my beliefs, this value formation process is so obviously important to understand.\n    \n3.  ^**[^](#fnrefk1djcjup0qc)**^\n    \n    There is an additional implication present in the alien story, but not present in the evolutionary production of humans. The aliens are implied to have *purposefully* aligned some of their protein computers with human values, while evolution is not similarly “purposeful.” This implication is noncentral to the key point, which is that the human-values-having protein computers exist in reality.\n    \n4.  ^**[^](#fnrefmpfloj2azs)**^\n    \n    Well, I didn’t even *have *a detailed picture of human value formation back in 2021. I thought humans were hopelessly dumb and messy and we want a *nice clean AI which actually is robustly aligned*. \n    \n5.  ^**[^](#fnref7a5ti4623qb)**^\n    \n    Suppose we model humans as the \"inner agent\" and evolution as the \"outer optimizer\"—I think [this is, in general, the wrong framing](https://www.lesswrong.com/posts/3pinFH3jerMzAvmza/on-how-various-plans-miss-the-hard-bits-of-the-alignment?commentId=FbAnmAkCdp8qdiMoN), but let's roll with it for now. I would guess that Eliezer believes that [human values are an unpredictable godshatter](https://www.lesswrong.com/posts/34Gkqus9vusXRevR8/late-2021-miri-conversations-ama-discussion#YkhmywLQetjekM7e3) with respect to the outer criterion of inclusive genetic fitness. This means that if you reroll evolution many times with perturbed initial conditions, you get inner agents with dramatically different values each time—it means that human values are akin to a raindrop which happened to land in some location for no grand reason. I notice that I have medium-strength objections to this claim, but let's just say that he is correct for now.  \n      \n    I think this unpredictability-to-evolution doesn't matter. We aren't going to reroll evolution to get AGI. Thus, for a variety of reasons too expansive for this margin, I am little moved by analogy-based reasoning along the lines of \"here's the one time inner alignment was tried in reality, and evolution failed horribly.\" I think that historical fact is mostly irrelevant, for reasons I will discuss later."
    },
    "voteCount": 71,
    "forceInclude": true
  },
  {
    "_id": "CQAMdzA4MZEhNRtTp",
    "url": null,
    "title": "Human values & biases are inaccessible to the genome",
    "slug": "human-values-and-biases-are-inaccessible-to-the-genome",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "World Modeling"
      },
      {
        "name": "Heuristics & Biases"
      },
      {
        "name": "Human Values"
      },
      {
        "name": "Evolution"
      }
    ],
    "tableOfContents": {
      "sections": [
        {
          "title": "Appendix: The inaccessibility trilemma",
          "anchor": "Appendix__The_inaccessibility_trilemma",
          "level": 1
        },
        {
          "title": "Appendix: Did evolution have advantages in solving the information inaccessibility problem?",
          "anchor": "Appendix__Did_evolution_have_advantages_in_solving_the_information_inaccessibility_problem_",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        },
        {
          "anchor": "comments",
          "level": 0,
          "title": "46 comments"
        }
      ],
      "headingsCount": 4
    },
    "contents": {
      "markdown": "*Related to Steve Byrnes’ *[*Social instincts are tricky because of the “symbol grounding problem.”*](https://www.lesswrong.com/s/HzcM2dkCq7fwXBej8/p/5F5Tz3u6kJbTNMqsb#13_2_2_Claim_2__Social_instincts_are_tricky_because_of_the__symbol_grounding_problem_) *I wouldn’t have had this insight without several great discussions with Quintin Pope.*\n\nTL;DR: It seems hard to scan a trained neural network and locate the AI’s learned “tree” abstraction. For very similar reasons, it seems intractable for the genome to scan a human brain and back out the “death” abstraction, which probably will not form at a predictable neural address. Therefore, I infer that the genome can’t *directly *make us afraid of death by e.g. specifying circuitry which detects when we think about death and then makes us afraid. In turn, this implies that there are a *lot *of values and biases which the genome cannot hardcode.\n\n* * *\n\nIn order to understand the human alignment situation confronted by the human genome, consider the AI alignment situation confronted by human civilization. For example, we may want to train a smart AI which learns a sophisticated world model, and then motivate that AI according to its learned world model. Suppose we want to build an AI which intrinsically values trees. Perhaps we can just provide a utility function that queries the learned world model and counts how many trees the AI believes there are.\n\nSuppose that the AI [will learn a reasonably human-like concept for “tree.”](https://www.lesswrong.com/posts/Nwgdq6kHke5LY692J/alignment-by-default#Unsupervised__Natural_Abstractions) However, before training has begun, the learned world model is inaccessibleto us. Perhaps the learned world model will be buried deep within a recurrent policy network, and buried *within* the world model is the “trees” concept. But we have no idea what learned circuits will encode that concept, or how the information will be encoded. We probably can’t, in advance of training the AI, write an algorithm which will examine the policy network’s hidden state and reliably back out how many trees the AI thinks there are. The AI’s learned concept for “tree” is [*inaccessible information*](https://ai-alignment.com/inaccessible-information-c749c6a88ce?gi=56ff6de02281)from our perspective.\n\nLikewise, [the human world model is inaccessible to the human genome](https://www.alignmentforum.org/posts/NkSpukDkm9pjRdMdB/human-instincts-symbol-grounding-and-the-blank-slate#CCA_theory_vs_human_universal_traits_and_instincts), because the world model is probably in the cortex and the cortex is probably [randomly initialized](https://www.alignmentforum.org/posts/wBHSYwqssBGCnwvHg/intro-to-brain-like-agi-safety-2-learning-from-scratch-in).^[\\[1\\]](#fn8nbwa5zlopq)^ Learned human concepts are therefore inaccessible to the genome, in the same way that the “tree” concept is *a priori *inaccessible to us. Even the [broad area where language processing occurs](https://en.wikipedia.org/wiki/Functional_specialization_(brain)) [varies from person to person](https://www.sciencedirect.com/science/article/pii/S1053811911009281?casa_token=x95ovxmnEnYAAAAA:UbdXjjpeJIqBO9V-wRR-OqtOc7b-_Gen741XNBWDXmJ_UPH7C4IAaYLZ3lBo4xpmWwiSCYOBvA), to say nothing of the encodings and addresses of particular learned concepts like “death.”\n\nI’m going to say things like “the genome cannot specify circuitry which detects when a person is thinking about death.” This means that the genome cannot hardcode circuitry which e.g. fires when the person is thinking about death, and does not fire when the person is not thinking about death. The genome *does *help indirectly specify the whole adult brain and all its concepts, just like *we *indirectly specify the trained neural network via the training algorithm and the dataset. That doesn’t mean we can tell when the AI thinks about trees, and it doesn’t mean that the genome can “tell” when the human thinks about death.\n\nWhen I’d previously thought about human biases (like the sunk cost fallacy) or values (like caring about other people), I had implicitly imagined that genetic influences could directly affect them (e.g. by detecting when I think about helping my friends, and then producing reward). However, given the inaccessibility obstacle, I infer that this can’t be the explanation. I infer that the genome *cannot *directly specify circuitry which:\n\n*   Detects when you’re thinking about seeking power,\n*   Detects when you’re thinking about cheating on your partner,\n*   Detects whether you perceive a sunk cost,\n*   Detects whether you think someone is scamming you and, if so, makes you want to punish them,\n*   Detects whether a decision involves probabilities and, if so, implements the [framing effect](https://www.simplypsychology.org/framing-effect.html),\n*   Detects whether you’re thinking about your family,\n*   Detects whether you’re thinking about goals, and makes you [conflate terminal and instrumental goals](https://www.readthesequences.com/Terminal-Values-And-Instrumental-Values),\n*   Detects and then navigates ontological shifts,\n    *   E.g. Suppose you learn that animals are made out of cells. I infer that the genome cannot detect that you are expanding your ontology, and then execute some genetically hard-coded algorithm which helps you do that successfully.\n*   Detects when you’re thinking about wireheading yourself or manipulating your reward signals,\n*   Detects when you’re thinking about reality versus non-reality (like a simulation or fictional world), or\n*   Detects whether you think someone is higher-status than you.\n\nConversely, the genome *can *access direct sensory observables, because those observables involve *a priori*-fixed “neural addresses.” For example, the genome could hardwire a cute-face-detector which hooks up to [retinal ganglion cells](https://en.wikipedia.org/wiki/Retinal_ganglion_cell) (which are at genome-predictable addresses), and then this circuit could produce physiological reactions (like the release of reward). This kind of circuit seems totally fine to me.\n\nIn total, information inaccessibility is strong evidencefor the genome hardcoding relatively simple^[\\[2\\]](#fn1mnccefproc)^ cognitive machinery. This, in turn, implies that human values/biases/high-level cognitive observables are produced by relatively simpler hardcoded circuitry, specifying e.g. the learning architecture, the broad reinforcement learning and self-supervised learning systems in the brain, and regional learning hyperparameters. Whereas before it seemed plausible to me that the genome hardcoded a lot of the above bullet points, I now think that’s pretty implausible.\n\nWhen I realized that the genome must also confront the information inaccessibility obstacle, this threw into question a lot of my beliefs about human values, about the complexity of human value formation, and about the structure of my own mind. I was left with a huge puzzle. If we can’t say “[the hardwired circuitry down the street did it](https://www.readthesequences.com/Occams-Razor)”, where do biases come from? [How can the genome hook the human’s preferences into the human’s world model, when the genome doesn’t “know” what the world model will look like](https://arbital.com/p/ontology_identification/)? Why do people usually navigate ontological shifts properly, why don’t they want to wirehead, why do they almost always care about other people *if the genome can’t even write circuitry that detects and rewards thoughts about people*?\n\nA fascinating mystery, no? More on that soon.\n\n*Thanks to Adam Shimi, Steve Byrnes, Quintin Pope, Charles Foster, Logan Smith, Scott Viteri, and Robert Mastragostino for feedback.*\n\nAppendix: The inaccessibility trilemma\n======================================\n\nThe logical structure of this essay is that at least one of the following must be true: \n\n1.  Information inaccessibility is somehow a surmountable problem for AI alignment (and the genome surmounted it),\n2.  The genome solves information inaccessibility in some way we cannot replicate for AI alignment, or\n3.  The genome cannot directly address the vast majority of interesting human cognitive events, concepts, and properties. (*The point argued by this essay*)\n\nIn my opinion, either (1) or (3) would be enormous news for AI alignment. More on (3)’s importance in future essays.\n\nAppendix: Did evolution have advantages in solving the information inaccessibility problem?\n===========================================================================================\n\nYes, and no. In a sense, evolution had “a lot of tries” but is “dumb”, while we have very few tries at AGI while ourselves being able to do consequentialist planning. \n\nIn the AI alignment problem, we want to be able to back out an AGI’s concepts, but we cannot run lots of similar AGIs and select for AGIs with certain effects on the world. Given the [natural abstractions hypothesis](https://www.alignmentforum.org/posts/Fut8dtFsBYRz8atFF/the-natural-abstraction-hypothesis-implications-and-evidence), maybe there’s a lattice of convergent abstractions—first learn edge detectors, then shape detectors, then people being visually detectable in part as compositions of shapes. And *maybe*, for example, people tend to convergently situate these abstractions in similar relative neural locations: The edge detectors go in V1, then the shape detectors are almost always in some other location, and then the person-concept circuitry is learned elsewhere in a convergently reliable relative position to the edge and shape detectors.\n\nBut there’s a problem with this story. A congenitally blind person [develops dramatically different functional areas](https://academic.oup.com/cercor/article/25/9/2507/2926061?login=false), which suggests in particular that their person-concept will be at a radically different relative position than the convergent person-concept location in sighted individuals. Therefore, any genetically hardcoded circuit which checks at the relative address for the person-concept which is reliably situated for sighted people, will not look at the right address for congenitally blind people. Therefore, if this story were true, congenitally blind people would lose any important value-formation effects ensured by this location-checking circuit which detects when they’re thinking about people. So, either the human-concept-location-checking circuit wasn’t an important cause of the blind person caring about other people (and then this circuit hasn’t explained the question we wanted it to, which is how people come to care about other people), or there isn’t such a circuit to begin with. I think the latter is true, and the convergent relative location story is wrong.\n\nBut the location-checking circuit is only one way the human-concept-detector could be implemented. There are other possibilities. Therefore, given enough selection and time, maybe evolution could evolve a circuit which checks whether you’re thinking about other people. *Maybe*. But it seems implausible to me (\\\\(<4\\\\%\\\\)). I’m going to prioritize explanations for “most people care about other people” which don’t require a fancy workaround.\n\nEDIT: After talking with Richard Ngo, I now think there's about an 8% chance that several interesting mental events are accessed by the genome; I updated upwards from 4%. \n\n1.  ^**[^](#fnref8nbwa5zlopq)**^\n    \n    Human values can still be inaccessible to the genome even if the cortex isn’t learned from scratch, but learning-from-scratch is a nice and clean sufficient condition which seems likely to me.\n    \n2.  ^**[^](#fnref1mnccefproc)**^\n    \n    I argue that the genome probably hardcodes neural circuitry which is simple *relative *to hardcoded “high-status detector” circuitry. Similarly, [the code for a machine learning experiment](https://github.com/leela-zero/leela-zero/tree/next/src) is simple *relative* to [the neural network it trains](https://arxiv.org/abs/2201.13176)."
    },
    "voteCount": 41,
    "forceInclude": true
  },
  {
    "_id": "FyChg3kYG54tEN3u6",
    "url": null,
    "title": "Against Relying on Evolution to Forecast AI Outcomes (Part 1)",
    "slug": "against-relying-on-evolution-to-forecast-ai-outcomes-part-1",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "AI"
      },
      {
        "name": "Shard Theory"
      }
    ],
    "tableOfContents": {
      "sections": [
        {
          "title": "Introduction",
          "anchor": "Introduction",
          "level": 1
        },
        {
          "title": "Inner values versus outer optimization criteria",
          "anchor": "Inner_values_versus_outer_optimization_criteria",
          "level": 1
        },
        {
          "title": "1: Training an AI is more similar to human learning than to evolution",
          "anchor": "1__Training_an_AI_is_more_similar_to_human_learning_than_to_evolution",
          "level": 2
        },
        {
          "title": "2: We have more total evidence from human outcomes",
          "anchor": "2__We_have_more_total_evidence_from_human_outcomes",
          "level": 2
        },
        {
          "title": "3: Human learning trajectories represent a broader sampling of the space of possible learning processes",
          "anchor": "3__Human_learning_trajectories_represent_a_broader_sampling_of_the_space_of_possible_learning_processes",
          "level": 2
        },
        {
          "title": "4: Evidence from humans are more accessible than evidence from evolution",
          "anchor": "4__Evidence_from_humans_are_more_accessible_than_evidence_from_evolution",
          "level": 2
        },
        {
          "title": "5: Evolution could not have succeeded anyways",
          "anchor": "5__Evolution_could_not_have_succeeded_anyways",
          "level": 2
        },
        {
          "title": "Total significance of evolution",
          "anchor": "Total_significance_of_evolution",
          "level": 1
        },
        {
          "title": "Implications",
          "anchor": "Implications",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        },
        {
          "anchor": "comments",
          "level": 0,
          "title": "4 comments"
        }
      ],
      "headingsCount": 11
    },
    "contents": {
      "markdown": "**TL;DR**: The dynamics of human learning processes and reward circuitry are more relevant than evolution for understanding how inner values arise from outer optimization criteria. \n\nThis post is related to Steve Byrnes’ [Against evolution as an analogy for how humans will create AGI](https://www.lesswrong.com/posts/pz7Mxyr7Ac43tWMaC/against-evolution-as-an-analogy-for-how-humans-will-create), but more narrowly focused on how we should make inferences about values. \n\nThanks to Alex Turner, Charles Foster, and Logan Riggs for their feedback on a draft of this post.\n\nIntroduction\n------------\n\nHow should we expect AGI development to play out?\n\nTrue precognition appears impossible, so we use various analogies to AGI development, such as evolution, current day humans, or current day machine learning. Such analogies are far from perfect, but we still may be able to extract useful information by carefully examining them. \n\nIn particular, we want to understand how inner values relate to the outer optimization criteria. Human evolution is one possible source of data on this question. In this post, I’ll argue that human evolution actually provides very little usable evidence on AGI outcomes. In contrast, analogies to the human learning process are much more fruitful.\n\nInner values versus outer optimization criteria\n-----------------------------------------------\n\nOne way people motivate extreme levels of concern about inner misalignment is to reference the fact that evolution failed to align humans to the objective of maximizing inclusive genetic fitness. From Eliezer Yudkowsky’s [AGI Ruin post](https://www.lesswrong.com/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities):\n\n> **16**. Even if you train really hard on an exact loss function, that doesn't thereby create an explicit internal representation of the loss function inside an AI that then continues to pursue that exact loss function in distribution-shifted environments.  Humans don't explicitly pursue inclusive genetic fitness; **outer optimization even on a very exact, very simple loss function doesn't produce inner optimization in that direction**.  This happens *in practice in real life, *it is what happened in *the only case we know about…*\n\nI don't think that \"*evolution -> human values*\" is the most useful reference class when trying to understand how outer optimization criteria relate to inner values. Evolution didn't directly optimize over our values. It optimized over our learning process and reward circuitry. Once you condition on a particular human's learning process + reward circuitry configuration + the human's environment, you screen off the influence of evolution on that human's values. So, there are really (at least) two classes of observations from which we can draw evidence:\n\n1.  \"*evolution's inclusive genetic fitness criteria -> a human's learned values*\"  (as mediated by evolution's influence over the human's learning process + reward circuitry)\n2.  \"*a particular human's learning process + reward circuitry + training environment -> the human's learned values*\"\n\nI will present five reasons why I think evidence from (2) “*human learning -> human values*” is more relevant to predicting AGI.\n\n### 1: Training an AI is more similar to human learning than to evolution\n\nThe relationship we want to make inferences about is:\n\n*   \"*a particular AI's learning process + reward function + training environment -> the AI's learned values*\"\n\nI think that \"*AI learning -> AI values*\" is *much* more similar to \"*human learning -> human values*\" than it is to \"*evolution -> human values*\". Steve Byrnes makes this case in much more detail in [his post on the matter](https://www.lesswrong.com/posts/pz7Mxyr7Ac43tWMaC/against-evolution-as-an-analogy-for-how-humans-will-create). Two of the ways I think AI learning more closely resembles human learning, and not evolution, are:\n\n1.  The simple type signatures of the two processes. Evolution is a bi-level optimization process, with evolution optimizing over genes, and the genes specifying the human learning process, which *then* optimizes over human cognition. Evolution does not directly optimize over a human’s cognition. And because learned cognition is [not directly accessible](https://www.lesswrong.com/s/nyEFg3AuJpdAozmoX/p/CQAMdzA4MZEhNRtTp) to the genome, evolution must use roundabout methods to influence human values through the genome.   \n      \n    In contrast, SGD directly optimizes over an AI’s cognition, just as [human within-lifetime learning](https://www.lesswrong.com/posts/fDPsYdDtkzhBp9A8D/intro-to-brain-like-agi-safety-8-takeaways-from-neuro-1-2-on) directly optimizes over human cognition. The human and AI learning processes are much closer to their respective cognitive structures, compared with evolution.\n2.  The differences between the parameter counts of the respective objects of optimization (the genome for evolution, the brain’s circuitry for human learning, and the AI’s parameter’s for AI training).   \n      \n    The genome has very few parameters compared to even current day neural networks, much less the brain or future AGIs. Our experience with ML scaling laws very strongly implies that parameter counts matter a lot for a system’s learning dynamics. Better to compare highly parameterized systems to other highly parameterized systems. \n\n\"*AI learning -> AI values*\", \"*human learning -> human values*\", and “*evolution -> human values*” each represent very different optimization processes, with many specific dissimilarities between any pair of them. However, I think the balance of dissimilarities points to \"*human learning -> human values*\" being the closer reference class for \"*AI learning -> AI values*\". As a result, I think the vast majority of our intuitions regarding the likely outcomes of inner goals versus outer optimization should come from looking at the \"*human learning -> human values*\" analogy, not the \"*evolution -> human values*\" analogy. \n\n### 2: We have more total evidence from human outcomes\n\nAdditionally, I think we have a lot more total empirical evidence from \"*human learning -> human values*\" compared to from \"*evolution -> human values*\". There are billions of instances of humans, and each of them presumably have somewhat different learning processes / reward circuit configurations / learning environments. Each of them represents a different data point regarding how inner goals relate to outer optimization. In contrast, the human species only evolved once. Thus, evidence from \"*human learning -> human values*\" should account for even more of our intuitions regarding inner goals versus outer optimization than the difference in reference class similarities alone would indicate.\n\n### 3: Human learning trajectories represent a broader sampling of the space of possible learning processes\n\nOne common objection is that “human learning” represents a tiny region in the space of all possible mind designs, and so we cannot easily generalize our observations of humans to minds in general. This is, of course, true, and it greatly limits the strength of any AI-related conclusions we can draw from looking at \"*human learning -> human values*\". However, I again hold that inferences from \"*evolution -> human values*\" suffer from an even more extreme version of this same issue. \"*Evolution -> human values*\" represent an even more restricted look at the general space of optimization processes than we get from the observed variations in different humans' learning processes, reward circuit configurations, and learning environments.\n\n### 4: Evidence from humans are more accessible than evidence from evolution\n\nHuman evolution happened hundreds of thousands of years ago. We are deeply uncertain about the details of the human ancestral environment and which traits were under what selection pressure. We are still unsure about what precise selection pressure led humans to be so generally intelligent at all. We are very far away from being able to precisely quantify all the potentially values-related selection pressures in the ancestral environment, or how those selection pressures changed our reward systems or our tendencies to form downstream values. \n\nIn contrast, human [within lifetime learning](https://www.lesswrong.com/posts/fDPsYdDtkzhBp9A8D/intro-to-brain-like-agi-safety-8-takeaways-from-neuro-1-2-on) happens all the time right now. It’s available for analysis and even experimental intervention. Given two evidence sources about a given phenomenon, where one evidence source is much more easily accessible than the other, then all else equal, the more accessible evidence source should represent a greater fraction of our total information on the phenomenon. This is another reason why we should expect evidence from humans to account for a greater proportion of our total information about how inner values relate to outer optimization criteria.\n\n### 5: Evolution could not have succeeded anyways\n\nI think that a careful account of how evolution shaped our learning process in the ancestral environment implies that evolution had next to no chance of aligning humans with inclusive genetic fitness. \n\nThere are no features of the ancestral environment which would lead to an ancestral human learning about the abstract idea of inclusive genetic fitness. There were no ancestral humans that held an explicit representation of inclusive genetic fitness. So, there was never an opportunity for evolution to select for humans who attached their values to an explicit representation of inclusive genetic fitness.\n\nRegardless of how difficult it is, in general, to get learning systems to form values around different abstract concepts, evolution could not have possibly gotten us to form a value around the particular abstraction of inclusive genetic fitness because we didn’t form such an abstraction in the ancestral environment. Ancestral humans had zero variance in their tendency to form values around inclusive genetic fitness. Evolution cannot select for traits that don’t vary across a population, so evolution could not have selected for humans that formed their values around inclusive genetic fitness.\n\nIn contrast, the sorts of things that we humans end up valuing are usually the sorts of things that are easy to form abstractions around. Thus, we are not doomed by the same difficulty that likely prevented evolution from aligning humans to inclusive genetic fitness. \n\nThis point is extremely important. I want to make sure to convey it correctly, so I will quote two previous expressions of this point by other sources:\n\n[Risks from Learned Optimization](https://arxiv.org/abs/1906.01820) notes that the lack of environmental data related to inclusive genetic fitness effectively increases the description length complexity of specifying an intelligence that deliberately optimizes for inclusive genetic fitness:\n\n> …description cost is especially high if the learned algorithm’s input data does not contain easy-to-infer information about how to optimize for the base objective. Biological evolution seems to differ from machine learning in this sense, since evolution’s specification of the brain has to go through the information funnel of DNA. The sensory data that early humans received didn’t allow them to infer the existence of DNA, nor the relationship between their actions and their genetic fitness. Therefore, for humans to have been aligned with evolution would have required them to have an innately specified model of DNA, as well as the various factors influencing their inclusive genetic fitness. Such a model would not have been able to make use of environmental information for compression, and thus would have required a greater description length. In contrast, our models of food, pain, etc. can be very short since they are directly related to our input data.\n\nFrom Alex Turner (in private communication):\n\n> If values form because reward sends reinforcement flowing back through a person's cognition and reinforces the thoughts which (credit assignment judges to have) led to the reward, then if a person never thinks about inclusive reproductive fitness, they can *never ever* form a value shard around inclusive reproductive fitness. Certain abstractions, like lollipops or people, are convergently learned early in the predictive-loss-minimization process and thus are easy to form values around. But if there aren't local mutations which make a person more probable to think thoughts about inclusive genetic fitness before/while the person gets reward, then evolution can't instill this value. Even if the descendents of that person will later be *able *to think thoughts about fitness. \n\nTotal significance of evolution\n-------------------------------\n\nThere are many sources of empirical evidence that can inform our intuitions regarding how inner goals relate to outer optimization criteria. My current (not very deeply considered) estimate of how to weight these evidence sources is roughly: \n\n*   ~60% from \"*human learning -> human values*\"\n*   ~4% from \"*evolution -> human values*\"\n*   ~36% from various other evidence sources, which I won't address further in this post, such as:\n    *   economics\n    *   microbial ecology\n    *   politics\n    *   current results in machine learning\n    *   game theory / multi-agent negotiation dynamics\n\nImplications\n------------\n\nI think that using \"*human learning -> human values*\" as our reference class for inner goals versus outer optimization criteria suggests a much more straightforward relationship between the two, as compared to the (lack of a) relationship suggested by \"*evolution -> human values*\". Looking at the learning trajectories of individual humans, it seems like a given person's values have a great deal in common with the sorts of experiences they've found rewarding in their lives up to that point in time. E.g., a person who grew up with and displayed affection for dogs probably doesn't want a future totally devoid of dogs, or one in which dogs suffer greatly. \n\nPlease note that I am not arguing that humans are inner aligned, or that looking at humans implies inner alignment is easy. Humans are misaligned with maximizing their outer reward source (activation of reward circuitry). I operationalize this misalignment as: \"A*fter a distributional shift from their learning environment, humans frequently behave in a manner that predictably fails to maximize reward in their new environment, specifically because they continue to implement values they'd acquired from their learning environment which are misaligned to reward maximization in the new environment*\". \n\nFor example, one way in which humans are inner misaligned is that, if you introduce a human into a new environment which has a button that will wirehead the human (thus maximizing reward in the new environment), but has other consequences that are extremely bad by light of the human's preexisting values (e.g., killing a beloved family member), most humans won't push the button.\n\nI also think this regularity in inner values is reasonably robust to large increases in capabilities. If you take a human whose outer behavior suggests they like dogs, and give that human very strong capabilities to influence the future, I do not think they are at all likely to erase dogs from existence. It's probably not as robust to your choice of which specific human to try this with. E.g., many people would screw themselves over with reckless self-modification. My point is that higher capabilities *alone* do not automatically render inner values completely alien to those demonstrated at lower capabilities.  \n \n\n(Part 2 will address whether the “sharp left turn” demonstrated by human capabilities with respect to evolution implies that we should expect a similar sharp left turn in AI capabilities.)"
    },
    "voteCount": 18,
    "forceInclude": true
  },
  {
    "_id": "pdaGN6pQyQarFHXF4",
    "url": null,
    "title": "Reward is not the optimization target",
    "slug": "reward-is-not-the-optimization-target",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "AI"
      },
      {
        "name": "Reinforcement Learning"
      },
      {
        "name": "Reward Functions"
      },
      {
        "name": "Wireheading"
      },
      {
        "name": "Inner Alignment"
      },
      {
        "name": "Outer Alignment"
      },
      {
        "name": "Shard Theory"
      }
    ],
    "tableOfContents": {
      "sections": [
        {
          "title": "Reward probably won’t be a deep RL agent’s primary optimization target",
          "anchor": "Reward_probably_won_t_be_a_deep_RL_agent_s_primary_optimization_target",
          "level": 1
        },
        {
          "title": "RL agents which don’t think about reward before getting reward, will not become reward optimizers, because there will be no reward-oriented computations for credit assignment to reinforce.",
          "anchor": "RL_agents_which_don_t_think_about_reward_before_getting_reward__will_not_become_reward_optimizers__because_there_will_be_no_reward_oriented_computations_for_credit_assignment_to_reinforce_",
          "level": 3
        },
        {
          "title": "The siren-like suggestiveness of the word “reward”",
          "anchor": "The_siren_like_suggestiveness_of_the_word__reward_",
          "level": 2
        },
        {
          "title": "When is reward the optimization target of the agent?",
          "anchor": "When_is_reward_the_optimization_target_of_the_agent_",
          "level": 2
        },
        {
          "title": "Anticipated questions",
          "anchor": "Anticipated_questions",
          "level": 2
        },
        {
          "title": "Dropping the old hypothesis",
          "anchor": "Dropping_the_old_hypothesis",
          "level": 1
        },
        {
          "title": "Implications",
          "anchor": "Implications",
          "level": 2
        },
        {
          "title": "Appendix: The field of RL thinks reward=optimization target",
          "anchor": "Appendix__The_field_of_RL_thinks_reward_optimization_target",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        },
        {
          "anchor": "comments",
          "level": 0,
          "title": "82 comments"
        }
      ],
      "headingsCount": 10
    },
    "contents": {
      "markdown": "*This insight was made possible by many conversations with Quintin Pope, where he challenged my implicit assumptions about alignment. I’m not sure who came up with this particular idea.*\n\nIn this essay, I call an agent a “reward optimizer” if it not only gets lots of reward, but if it reliably makes choices like “reward but no task completion” (e.g. receiving reward without eating pizza) over “task completion but no reward” (e.g. eating pizza without receiving reward). Under this definition, an agent can be a reward optimizer even if it doesn't contain an explicit representation of reward, or implement a search process for reward.\n\n> Reinforcement learning is learning what to do—how to map situations to actions **so as to maximize a numerical reward signal**. — [Reinforcement learning: An introduction](http://www.incompleteideas.net/sutton/book/first/Chap1PrePub.pdf) \n\nMany people^[\\[1\\]](#fnw8im2esr9yd)^ seem to expect that reward will be the optimization target of really smart learned policies—that these policies will be reward optimizers. I strongly disagree. As I argue in this essay, reward is *not*, in general, that-which-is-optimized by RL agents.^[\\[2\\]](#fndaf17p7n29n)^  \n\nSeparately, as far as I can tell, most^[\\[3\\]](#fnwraenj05b1)^ practitioners usually view reward as encoding the relative utilities of states and actions (e.g. it’s *this good *to have all the trash put away), as opposed to imposing a *reinforcement schedule *which builds certain computational edifices inside the model (e.g. reward for picking up trash → reinforce trash-recognition and trash-seeking and trash-putting-away subroutines). I think the former view is almost always inappropriate, because **reward is the *****antecedent-computation-reinforcer*****.** Reward reinforces those computations which produced it. \n\nTherefore, *reward is not the optimization target* in two senses:\n\n1.  Deep reinforcement learning agents will not come to intrinsically and primarily value their reward signal; reward is not *the trained agent’s *optimization target.\n2.  Utility functions express the *relative goodness *of outcomes. Reward *is not best understood *as being a kind of utility function. Reward has the mechanistic effect of *reinforcing the computations which led to it*. Therefore, properly understood, reward does not express relative goodness and is therefore *not an optimization target at all.*\n\nReward probably won’t be a deep RL agent’s primary optimization target\n======================================================================\n\nAfter work, you grab pizza with your friends. You eat a bite. The taste releases [reward in your brain](https://en.wikipedia.org/wiki/Reward_system), which triggers credit assignment. Credit assignment identifies which thoughts and decisions were responsible for the release of that reward, and makes those decisions more likely to happen in similar situations in the future. Perhaps you had thoughts like \n\n*   “It’ll be fun to hang out with my friends” and \n*   “The pizza shop is nearby” and \n*   “Since I just ordered food at a cash register, execute `motor-subroutine-#51241` to take out my wallet” and \n*   “If the pizza is in front of me and it’s mine and I’m hungry, raise the slice to my mouth” and \n*   “If the slice is near my mouth and I’m not already chewing, take a bite.” \n\nMany of these thoughts will be judged responsible by credit assignment, and thereby become more likely to trigger in the future. This is what *reinforcement *learning is all about—the reward is the *reinforcer* of those things which came before it. The reward is reinforcing / locally-improving^[\\[4\\]](#fnlcn71g3whc)^ / generalizing the antecedent computations which are judged relevant by credit assignment. \n\nImportantly, reward does not automatically spawn thoughts *about *reward, and reinforce those reward-focused thoughts! Just because common English endows “reward” with suggestive pleasurable connotations, that [does not mean that](https://www.readthesequences.com/No-Universally-Compelling-Arguments) an RL agent will *terminally value *reward! \n\nWhat kinds of people (or non-tabular agents more generally) will become reward optimizers, such that the agent ends up terminally caring about reward (and little else)? Reconsider the pizza situation, but instead suppose you were thinking thoughts like “this pizza is going to be so rewarding” and “in this situation, eating pizza sure will activate my reward circuitry.” \n\nYou eat the pizza, triggering reward, triggering credit assignment, which correctly locates these reward-focused thoughts as contributing to the release of reward. Therefore, in the future, you will more often take actions because you think they will produce reward, and so you will become more of the kind of person who intrinsically cares about reward. This is a path^[\\[5\\]](#fnzf0metnada)^ to reward-optimization and wireheading. \n\n**RL agents which don’t think about reward before getting reward, will not become reward optimizers, because there will be no reward-oriented computations for credit assignment to reinforce.** \n\nThe siren-like suggestiveness of the word “reward”\n--------------------------------------------------\n\nLet’s strip away the suggestive word “reward”, and replace it by its substance: antecedent-computation-reinforcer. \n\nSuppose a human trains an RL agent by pressing the antecedent-computation-reinforcer button when the agent puts trash in a trash can. While putting trash away, the AI’s policy network is probably “thinking about” the *actual world it’s interacting with*, and so the antecedent-computation-reinforcer reinforces those heuristics which lead to the trash getting put away (e.g. “if trash-classifier activates near center-of-visual-field, then grab trash using `motor-subroutine-#642`”). \n\nThen suppose this AI models the true fact that the button-pressing produces the antecedent-computation-reinforcer. Suppose this AI, which has historically had its trash-related thoughts reinforced, considers the plan of pressing this button. “If I press the button, that triggers credit assignment, which will reinforce my decision to press the button, such that in the future I will press the button even more.”\n\n*Why, exactly, would the AI seize*^[\\[6\\]](#fnsmeax43bfp9)^* the button? To reinforce itself into a certain corner of its policy space? The AI has not had antecedent-computation-reinforcer-thoughts reinforced in the past, and so its current decision will not be made in order to acquire the antecedent-computation-reinforcer!*\n\nRL is not, in general, about training antecedent-computation-reinforcer optimizers. \n\nWhen *is* reward the optimization target of the agent?\n------------------------------------------------------\n\nIf reward is guaranteed to become your optimization target, then your learning algorithm can force you to become a drug addict. Let me explain. \n\n[Convergence theorems](https://nlp.chonbuk.ac.kr/AML/slides_lille/Lecture4-b.pdf) provide conditions under which a reinforcement learning algorithm is guaranteed to converge to an optimal policy for a reward function. For example, value iteration maintains a table of value estimates for each state *s*, and iteratively propagates information about that value to the neighbors of *s*. If a far-away state *f* has huge reward, then that reward ripples back through the environmental dynamics via this [“backup” operation](https://inst.eecs.berkeley.edu/~cs294-40/fa08/scribes/lecture2.pdf). Nearby parents of *f *gain value, and then after lots of backups, far-away ancestor-states gain value due to *f*’s high reward.\n\nEventually, the “value ripples” settle down. The agent picks an (optimal) policy by acting to maximize the value-estimates for its post-action states.\n\nSuppose it would be extremely rewarding to do drugs, but those drugs are on the other side of the world. Value iteration backs up that high value to your present space-time location, such that your policy necessarily gets *at least *that much reward. There’s no escaping it: After enough backup steps, you’re traveling across the world to do cocaine. \n\nBut obviously these conditions aren’t true in the real world. Your learning algorithm doesn’t force *you *to try drugs. Any AI which e.g. tried every action at least once would quickly kill itself, and so real-world general RL agents won’t explore like that because that would be stupid. So the RL agent’s algorithm won’t make it e.g. explore wireheading either, and so the convergence theorems *don’t apply even a little—even in spirit*.\n\nAnticipated questions\n---------------------\n\n1.  Why won’t early-stage agents think thoughts like “If putting trash away will lead to reward, then execute `motor-subroutine-#642`”, and then this gets reinforced into reward-focused cognition early on?\n    1.  Suppose the agent puts away trash in a blue room. Why won’t early-stage agents think thoughts like “If putting trash away will lead to the wall being blue, then execute `motor-subroutine-#642`”, and then this gets reinforced into blue-wall-focused cognition early on? [Why consider either scenario to begin with](https://www.readthesequences.com/Privileging-The-Hypothesis)?\n2.  But aren’t we implicitly selecting for agents with high cumulative reward, when we train those agents?\n    1.  Yeah. But on its own, this argument can’t possibly imply that selected agents will probably be reward optimizers. The argument would [prove too much](https://slatestarcodex.com/2013/04/13/proving-too-much/). Evolution selected for inclusive genetic fitness, and it [did not get IGF optimizers](https://www.lesswrong.com/posts/XPErvb8m9FapXCjhA/adaptation-executers-not-fitness-maximizers).\n        1.  \"We're selecting for agents on reward \\\\(\\\\rightarrow\\\\) we get an agent which optimizes reward\" is locally invalid. \"We select for agents on X \\\\(\\\\rightarrow\\\\) we get an agent which optimizes X\" is not true for the case of evolution, and so is not true in general. \n        2.  Therefore, the argument isn't necessarily true  in the AI reward-selection case. Even if RL *did* happen to train reward optimizers and this post *were* wrong, the selection argument is too weak on its own to establish that conclusion.\n    2.  Here’s the more concrete response: Selection isn’t *just *for agents which get lots of reward. \n        1.  For simplicity, consider the case where on the training distribution, the agent gets reward if and only if it reaches a goal state. Then any selection for reward is also selection for reaching the goal. And if the goal is the only red object, then selection for reward is *also *selection for reaching red objects. \n        2.  In general, selection for reward produces equally strong selection for reward’s necessary and sufficient conditions. In general, it seems like there should be a lot of those. Therefore, since selection is not only for *reward *but for *anything which goes along with reward *(e.g. reaching the goal), then selection won’t advantage *reward optimizers *over *agents which reach goals quickly / pick up lots of trash / \\[do the objective\\]*.  \n    3.  Another reason to not expect the selection argument to work is that it’s *instrumentally convergent *for most inner agent values to *not *become wireheaders, for them to *not *try hitting the reward button. \n        1.  I think that before the agent can hit the particular attractor of reward-optimization, it will hit an attractor in which it optimizes for some aspect of a historical correlate of reward. \n            1.  We train agents which intelligently optimize for e.g. putting trash away, and this reinforces trash-putting-away computations, which activate in a broad range of situations so as to steer agents into a future where trash has been put away. An intelligent agent will model the true fact that, if the agent reinforces itself into caring about antecedent-computation-reinforcement, then it will no longer navigate to futures where trash is put away. Therefore, it decides to not hit the reward button. \n            2.  This reasoning follows for most inner goals by instrumental convergence. \n        2.  On my current best model, this is why people usually don’t wirehead. They learn their own values via deep RL, like caring about dogs, and these actual values are opposed to the person they would become if they wirehead.\n3.  Don’t some people terminally care about reward?\n    1.  I think so! I think that generally intelligent RL agents will have *secondary, relatively weaker *values around reward, but that reward will not be a primary motivator. Under my current (weakly held) model, an AI will only start reinforcing computations about reward *after *it has reinforced other kinds of computations (e.g. putting away trash). More on this in later essays.\n4.  But what if the AI bops the reward button early in training, while exploring? Then credit assignment would make the AI more likely to hit the button again.\n    1.  Then keep the button away from the AI until it can model the effects of hitting the antecedent-computation-reinforcer button.^[\\[7\\]](#fn5fwuzzvjmuv)^\n    2.  For the reasons given in the “siren” section, a sufficiently reflective AI probably won’t seek the reward button on its own.\n5.  AIXI—\n    1.  will always kill you and then wirehead forever, unless you gave it something like a constant reward function.\n    2.  And, IMO, this fact is not practically relevant to alignment. AIXI is *explicitly a reward-maximizer*. As far as I know, AIXI(-*tl*) is not the limiting form of any kind of real-world intelligence trained via *reinforcement *learning.\n6.  Does the choice of RL algorithm matter?\n    1.  For point 1 (*reward is not the trained agent's optimization target*), it might matter. \n        1.  I started off analyzing model-free actor-based approaches, but have also considered a few model-based setups. I think the key lessons apply to the general case, but I think the setup will substantially affect which values tend to be grown. \n            1.  If the agent's curriculum is broad, then reward-based cognition may get reinforced from a confluence of tasks (solve mazes, write sonnets), while each task-specific cognitive structure is only narrowly contextually reinforced.\n            2.  Pretraining a language model and then slotting that into an RL setup also changes the initial computations in a way which I have not yet tried to analyze.\n        2.  It’s *possible *there’s some kind of RL algorithm which *does* train agents which limit to reward optimization (and, of course, thereby “solves” inner alignment in its literal form of “find a policy which optimizes the time-discounted sum of the outer objective signal”). \n    2.  For point 2 (*reward provides local updates to the agent's cognition via credit assignment; reward is not best understood as specifying our preferences*), the choice of RL algorithm should not matter, as long as it uses reward to compute local updates. \n        1.  A similar lesson applies to the updates provided by loss signals. A loss signal provides updates which deform the agent's cognition into a new shape.\n7.  TurnTrout, you've been talking about an AI's learning process using English, but ML gradients may not neatly be expressible in our concepts. How do we know that it's appropriate to speculate in English?\n    1.  I am *not* *certain* that my model is legit, but it sure seems more legit than (my perception of) how people usually think about RL (i.e. in terms of reward maximization, and reward-as-optimization-target instead of as feedback signal which builds cognitive structures). \n    2.  I only have access to my own concepts and words, so I am provisionally reasoning ahead anyways, while keeping in mind the potential treacheries of anglicizing imaginary gradient updates (e.g. \"be more likely to eat pizza in similar situations\").\n\nDropping the old hypothesis\n===========================\n\nAt this point, I don't see a strong reason to strongly focus on the “reward optimizer” hypothesis. The idea that AIs will get really smart and primarily optimize some reward signal… I don’t know of any tight mechanistic stories for that. I’d love to hear some, if there are any. \n\nAs far as I’m aware, the strongest evidence left for agents intrinsically valuing antecedent-computation-reinforcement is that some humans *do* strongly (but not uniquely) value antecedent-computation-reinforcement,^[\\[8\\]](#fn2v0yltp1gw7)^ and many humans seem to value it weakly, and humans are probably RL agents in the appropriate ways. So we definitely can’t *rule out* agents which strongly(and not just weakly) value antecedent-computation-reinforcement. But it’s also *not *the overdetermined default outcome. More on that in future essays.\n\nIt’s true that reward *can *be an agent’s optimization target, but what reward *actually does *is reinforce the computations which led to it. A particular alignment proposal might argue that a reward function will *reinforce the agent into a shape such that it intrinsically values reinforcement*, and that the *antecedent-computation-reinforcer goal is also a human-aligned optimization target*, but this is still just one particular approach of using the antecedent-computation-reinforcer to produce desirable cognition within an agent. Even in that proposal, the primary mechanistic function of reward is reinforcement, not optimization-target.\n\nImplications\n------------\n\nHere are some major updates which I made:\n\n1.  **Any reasoning derived from the reward-optimization premise is now suspect until otherwise supported.**\n2.  **Wireheading was never a high-probability problem for RL-*****trained*** **agents**, absent a specific storyfor why antecedent-computation-reinforcer-acquiring thoughts would be reinforced into primary decision factors.\n3.  **Stop worrying about finding “outer objectives” which are safe to *****maximize.***^[\\[9\\]](#fnp7rlxgfkp9)^I think that you’re not going to get an outer-objective-maximizer (i.e. an agent which maximizes the explicitly specified reward function). \n    1.  Instead, focus on building good cognition within the agent. \n    2.  In my ontology, there's only an inner alignment problem: How do we grow good cognition inside of the trained agent?\n4.  **Mechanistically model RL agents as executing behaviors downstream of past reinforcement** (e.g. putting trash away), in addition to thinking about policies which are selected for having high reward on the training distribution (e.g. hitting the button).\n    1.  The latter form of reasoning skips past the mechanistic substance of reinforcement learning: The reinforcement of computations responsible for the acquisition of antecedent-computation-reinforcer. I still think it's useful to consider selection, but mostly in order to generate failures modes whose mechanistic plausibility can be evaluated.\n    2.  In my view, reward's proper role isn't to encode an objective, but a *reinforcement schedule*, such that the right kinds of computations get reinforced within the AI's mind. \n\nAppendix: The field of RL thinks reward=optimization target\n===========================================================\n\nLet’s take a little stroll through [Google Scholar’s top results for “reinforcement learning\"](https://scholar.google.com/scholar?hl=en&as_sdt=7,39&q=reinforcement+learning), emphasis added:\n\n> The agent's job is to find a policy… that **maximizes some long-run measure of reinforcement**. ~ [Reinforcement learning: A survey](https://www.jair.org/index.php/jair/article/download/10166/24110/)\n\n> In instrumental conditioning, animals learn to choose actions to obtain rewards and avoid punishments, or, more generally to achieve goals. **Various goals are possible, such as optimizing the average rate of acquisition of net rewards (i.e. rewards minus punishments), or some proxy for this such as the expected sum of future rewards**. ~ [Reinforcement learning: The Good, The Bad and The Ugly](https://www.princeton.edu/~yael/Publications/DayanNiv2008.pdf) \n\n> We hypothesise that intelligence, and its associated abilities, can be understood as subserving the **maximisation** of reward. ~ [Reward is Enough](https://www.sciencedirect.com/science/article/pii/S0004370221000862#fn0020)\n\nSteve Byrnes did, in fact, briefly point out part of the “reward is the optimization target” mistake:\n\n> I note that even experts sometimes sloppily talk as if RL agents make plans towards the goal of maximizing future reward… — [Model-based RL, Desires, Brains, Wireheading](https://www.alignmentforum.org/posts/K5ikTdaNymfWXQHFb/model-based-rl-desires-brains-wireheading#Self_aware_desires_1__wireheading)\n\nI don't think it's just sloppy talk, I think it's incorrect belief in many cases. I mean, I did my PhD on RL theory, and I still  believed it. Many authorities and textbooks confidently claim—presenting little to no evidence—that reward is an optimization target (i.e. the quantity which the policy is in fact trying to optimize, or the quantity to be optimized by the policy). [Check what the math actually says](https://www.lesswrong.com/posts/2GxhAyn9aHqukap2S/looking-back-on-my-alignment-phd#Too_much_deference__too_little_thinking_for_myself). \n\n1.  ^**[^](#fnrefw8im2esr9yd)**^\n    \n    [Including](https://www.sciencedirect.com/science/article/pii/S0004370221000862#fn0020) the authors of the quoted introductory text, [Reinforcement learning: An introduction](http://www.incompleteideas.net/sutton/book/first/Chap1PrePub.pdf). I have, however, met several alignment researchers who already internalized that reward is not the optimization target, perhaps not in so many words. \n    \n2.  ^**[^](#fnrefdaf17p7n29n)**^\n    \n    [Utility ≠ Reward](https://www.alignmentforum.org/posts/bG4PR9uSsZqHg2gYY/utility-reward) points out that an RL-trained agent is *optimized by *original reward, but not necessarily *optimizing for *the original reward. This essay goes further in several ways, including when it argues that *reward *and *utility *have different type signatures—that reward shouldn’t be viewed as encoding a goal at all, but rather a *reinforcement schedule*. And not only do I not expect the trained agents to not maximize the original “outer” reward signal, I think they probably won’t try to strongly optimize [*any* reward signal](https://www.alignmentforum.org/posts/3RdvPS5LawYxLuHLH/hackable-rewards-as-a-safety-valve?commentId=crkrEjpyjB7N9t5jo).\n    \n3.  ^**[^](#fnrefwraenj05b1)**^\n    \n    [Reward shaping](http://ai.stanford.edu/~ang/papers/shaping-icml99.pdf) seems like the most prominent counterexample to the “reward represents terminal preferences over state-action pairs” line of thinking.\n    \n4.  ^**[^](#fnreflcn71g3whc)**^\n    \n    Of course, credit assignment doesn’t *just* reshuffle existing thoughts. For example, SGD raises image classifiers out of the noise of the randomly initialized parameters. But the refinements are localin parameter-space, and dependent on the existing weights through which the forward pass flowed.\n    \n5.  ^**[^](#fnrefzf0metnada)**^\n    \n    But also, you were still probably thinking about reality as you interacted with it (“since I’m in front of the shop where I want to buy food, go inside”), and credit assignment will still locate some of those thoughts as relevant, and so you wouldn’t purely reinforce the reward-focused computations.\n    \n6.  ^**[^](#fnrefsmeax43bfp9)**^\n    \n    Quintin Pope remarks: “The AI would probably want to establish **control** over the button, if only to ensure its values aren't updated in a way it wouldn't endorse. Though that's an example of convergent powerseeking, not reward seeking.”\n    \n7.  ^**[^](#fnref5fwuzzvjmuv)**^\n    \n    For mechanistically similar reasons, keep cocaine out of the crib until your children can model the consequences of addiction.\n    \n8.  ^**[^](#fnref2v0yltp1gw7)**^\n    \n    I am presently ignorant of [the relationship between pleasure and reward prediction error in the brain](https://pubmed.ncbi.nlm.nih.gov/35156187/). I do not think they are the same.   \n      \n    However, I think people are usually weakly hedonically / experientially motivated. Consider a person about to eat pizza. If you give them the choice between \"pizza but no pleasure from eating it\" and \"pleasure but no pizza\", I think most people would choose the latter (unless they were really hungry and needed the calories). If people just navigated to futures where they had eaten pizza, that would not be true. \n    \n9.  ^**[^](#fnrefp7rlxgfkp9)**^\n    \n    From correspondence with another researcher: There may yet be an interesting alignment-related puzzle to \"Find an optimization process whose maxima are friendly\", but I personally don't share the intuition yet."
    },
    "voteCount": 86,
    "forceInclude": true
  },
  {
    "_id": "FMdGt9S9irgxeD9Xz",
    "url": null,
    "title": "General alignment properties",
    "slug": "general-alignment-properties",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "AI"
      },
      {
        "name": "Embedded Agency"
      },
      {
        "name": "Ontology"
      },
      {
        "name": "Complexity of Value"
      },
      {
        "name": "General Alignment Properties"
      },
      {
        "name": "Shard Theory"
      }
    ],
    "tableOfContents": {
      "sections": [
        {
          "title": "Terminally valuing latent objects in reality. ",
          "anchor": "Terminally_valuing_latent_objects_in_reality__",
          "level": 1
        },
        {
          "title": "Navigating ontological shifts. ",
          "anchor": "Navigating_ontological_shifts__",
          "level": 1
        },
        {
          "title": "Reflective reasoning / embeddedness. ",
          "anchor": "Reflective_reasoning___embeddedness__",
          "level": 1
        },
        {
          "title": "Fragility of outcome value to initial conditions / Pairwise misalignment severity",
          "anchor": "Fragility_of_outcome_value_to_initial_conditions___Pairwise_misalignment_severity",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        },
        {
          "anchor": "comments",
          "level": 0,
          "title": "2 comments"
        }
      ],
      "headingsCount": 6
    },
    "contents": {
      "markdown": "[AIXI](https://en.wikipedia.org/wiki/AIXI) and the genome are both ways of specifying intelligent agents. \n\n1.  Give AIXI a utility function (perhaps over observation histories), and hook it up to an environment, and this pins down a policy.^[\\[1\\]](#fniggswxs0y97)^ \n2.  Situate the genome in the embryo within our reality, and this eventually grows into a human being with a policy of their own.\n\nThese agents have different \"values\", in whatever sense we care to consider. However, these two agent-specification procedures also have very different *general alignment properties. *\n\nGeneral alignment properties are not about *what* a particular agent cares about (e.g. the AI \"values\" chairs). I call an alignment property \"general\" if the property would be interesting to a range of real-world agents trying to solve AI alignment. Here are some examples.\n\n**Terminally valuing latent objects in reality. **\n\nAIXI only \"terminally values\" its observations and doesn't terminally value latent objects in reality, while humans generally care about e.g. dogs (which are latent objects in reality).\n\n**Navigating ontological shifts. **\n\nConsider latent-diamond-AIXI (LDAIXI), an AIXI variant. LDAIXI's utility function which scans its top 50 hypotheses (represented as Turing machines), checks each work tape for atomic representations of diamonds, and then computes the utility to be the amount of atomic diamond in the world. \n\nIf LDAIXI updates sufficiently hard towards non-atomic physical theories, then it can no longer find any utility in its top 50 hypotheses. All policies now might have equal value (zero), and LDAIXI would not continue maximizing the expected diamond content of the future. From our viewpoint, LDAIXI has [failed to rebind its \"goals\"](https://arbital.com/p/ontology_identification/) to its new conceptions of reality. (From LDAIXI's \"viewpoint\", it has Bayes-updated on its observations and continues to select optimal actions.)\n\nOn the other hand, physicists do not stop caring about their friends when they learn quantum mechanics. Children do not stop caring about animals when they learn that animals are made out of cells. People seem to navigate ontological shifts pretty well. \n\n**Reflective reasoning / embeddedness. **\n\n[AIXI can't think straight about how it is embedded in the world](https://www.lesswrong.com/posts/AszKwKyhBPZAnCstA/solomonoff-cartesianism). However, people quickly learn heuristics like \"If I get angry, I'll be more likely to be mean to people around me\", or \"If I take cocaine now, I'll be even more likely to take cocaine in the future.\" \n\n**Fragility of outcome value to initial conditions / Pairwise misalignment severity**\n\nThis general alignment property seems important to me, and I'll write a post on it. In short: How pairwise-unaligned are two agents produced with slightly different initial hyperparameters/architectural choices (e.g. reward function / utility function / inductive biases)? \n\n* * *\n\nI'm excited about people thinking more about general alignment properties and about what generates those properties.\n\n1.  ^**[^](#fnrefiggswxs0y97)**^\n    \n    Supposing e.g. uniformly random tie-breaking for actions enabling equal expected utility."
    },
    "voteCount": 17,
    "forceInclude": true
  },
  {
    "_id": "xqkGmfikqapbJ2YMj",
    "url": null,
    "title": "Shard Theory: An Overview",
    "slug": "shard-theory-an-overview",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "Psychology"
      },
      {
        "name": "AI"
      },
      {
        "name": "World Modeling"
      },
      {
        "name": "Human Values"
      },
      {
        "name": "Utility Functions"
      },
      {
        "name": "Complexity of Value"
      },
      {
        "name": "Reinforcement Learning"
      },
      {
        "name": "Subagents"
      },
      {
        "name": "Outer Alignment"
      },
      {
        "name": "SERI MATS"
      },
      {
        "name": "Shard Theory"
      }
    ],
    "tableOfContents": {
      "sections": [
        {
          "title": "Introduction",
          "anchor": "Introduction",
          "level": 1
        },
        {
          "title": "Reinforcement Strengthens Select Computations",
          "anchor": "Reinforcement_Strengthens_Select_Computations",
          "level": 1
        },
        {
          "title": "When You're Dumb, Continuously Blended Tasks Mean Continuous, Broadening Values",
          "anchor": "When_You_re_Dumb__Continuously_Blended_Tasks_Mean_Continuous__Broadening_Values",
          "level": 1
        },
        {
          "title": "When You're Smart, Internal Game Theory Explains the Tapestry of Your Values",
          "anchor": "When_You_re_Smart__Internal_Game_Theory_Explains_the_Tapestry_of_Your_Values",
          "level": 1
        },
        {
          "title": "Lingering Confusions",
          "anchor": "Lingering_Confusions",
          "level": 1
        },
        {
          "title": "Relevance to Alignment Success",
          "anchor": "Relevance_to_Alignment_Success",
          "level": 1
        },
        {
          "title": "Conclusion",
          "anchor": "Conclusion",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        },
        {
          "anchor": "comments",
          "level": 0,
          "title": "33 comments"
        }
      ],
      "headingsCount": 9
    },
    "contents": {
      "markdown": "*Generated as part of SERI MATS, Team Shard's research, under John Wentworth.*\n\n*Many thanks to Quintin Pope, Alex Turner, Charles Foster, Steve Byrnes, and Logan Smith for feedback, and to everyone else I've discussed this with recently! All mistakes are my own.*\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/7014d7016881f43f3d2b00d07b203dd0438096427bbe546e.JPG)\n\n*Team Shard*,  courtesy of Garrett Baker and DALL-E 2\n\nIntroduction\n============\n\n*Shard theory* is a research program aimed at explaining the systematic relationships between the reinforcement schedules and learned values of reinforcement-learning agents. It consists of a basic ontology of reinforcement learners, their internal computations, and their relationship to their environment. It makes several predictions about a range of RL systems, both RL models and humans. Indeed, shard theory can be thought of as simply applying the modern ML lens to the question of value learning under reinforcement in artificial and natural neural networks!\n\nSome of shard theory's confident predictions can be tested immediately in modern RL agents. Less confident predictions about i.i.d.-trained language models can also be tested now. Shard theory also has numerous retrodictions about human psychological phenomena that are otherwise mysterious from only the viewpoint of EU maximization, with no further substantive mechanistic account of human learned values. Finally, shard theory fails some retrodictions in humans; on further inspection, these lingering confusions might well falsify the theory.\n\nIf shard theory captures the essential dynamic relating reinforcement schedules and learned values, then we'll be able to carry out a steady stream of further experiments yielding a lot of information about how to reliably instill more of the values we want in our RL agents and fewer of those we don't. Shard theory's framework implies that alignment success is substantially continuous, and that even very limited alignment successes can still mean enormous quantities of value preserved for future humanity's ends. If shard theory is true, then further shard science will progressively yield better and better alignment results.\n\nThe remainder of this post will be an overview of the basic claims of shard theory. Future posts will detail experiments and preregister predictions, and look at the balance of existing evidence for and against shard theory from humans.\n\nReinforcement Strengthens Select Computations\n=============================================\n\nA *reinforcement learner* is an ML model trained via a *reinforcement schedule*, a pairing of world states and reinforcement events. We-the-devs choose when to dole out these reinforcement events, either handwriting a simple *reinforcement algorithm* to do it for us or having human overseers give out reinforcement. The reinforcement learner itself is a neural network whose computations are reinforced or anti-reinforced by reinforcement events. After sufficient training, reinforcement often manages to reinforce those computations that are good at our desired task. We'll henceforth focus on *deep reinforcement learners*: RL models specifically comprised of multi-layered neural networks.\n\nDeep RL can be seen as a supercategory of many deep learning tasks. In deep RL, the model you're training receives feedback, and this feedback fixes how the model is updated afterwards via SGD. In many RL setups, because the model's outputs influence its future observations, the model exercises some control over what it will see and be updated on in the future. RL wherein the model's outputs *don't* affect the distribution of its future observations is called *supervised learning*. So a general theory of deep RL models may well have implications for supervised learning models too. This is important for the experimental tractability of a theory of RL agents, as appreciably complicated RL setups are a huge pain in the ass to get working, while supervised learning is well established and far less finicky.\n\nHumans are the only extant example of generally intelligent RL agents. Your subcortex contains your hardwired reinforcement circuitry, while your neocortex comprises much of your trained RL model. So you can coarsely model the neocortex as an RL agent being fed observations by the external world and reinforcement events by subcortical circuitry, and ask about what learned values this human develops as you vary those two parameters. Shard theory boils down to using the ML lens to understand all intelligent deep systems in this way, and using this ML lens to build up a mechanistic model of value learning.\n\nAs a newborn baby, you start life with subcortical hardwired reinforcement circuitry (fixed by your genome) and a randomly initialized neocortex. The computations initialized in your neocortex are random, and so the actions initially outputted by your neocortex are random. (Your brainstem additionally hardcodes some rote reflexes and simple automatic body functions, though, accounting for babies' innate behaviors.) Eventually, your baby-self manages to get a lollipop onto his tongue, and the sugar molecules touching your tastebuds fire your hardwired reinforcement circuitry. Via a primitive, hardwired credit assignment algorithm -- say, single out whatever computations are different from the computations active a moment ago -- your reinforcement circuitry gives all those singled out computations more staying power. This means that these select computations will henceforth be more likely to fire, conditional on their initiating cognitive inputs being present, executing their computation and returning a motor sequence. If this path to a reinforcement event wasn't a fluke, those contextually activated computations will go on to accrue yet more staying power by steering into more future reinforcement events. Your bright-red-disk-in-the-central-visual-field-activated computations will activate again when bright red lollipops are clearly visible, and will plausibly succeed at getting future visible lollipops to your tastebuds.\n\nContextually activated computations can chain with one another, becoming responsive to a wider range of cognitive inputs in the process: if randomly crying at the top of your lungs gets a bright-red disk close enough to activate your bright-red-disk-sensitive computation, then credit assignment will reinforce both the contextual crying and contextual eating computations. These contextually activated computations that steer behavior are called *shards* in shard theory. A simple shard, like the reach-for-visible-red-disks circuit, is a *subshard*. Typical shards are chained aggregations of many subshards, resulting in a sophisticated, contextually activated, behavior-steering circuit in a reinforcement learner.\n\nShards are subcircuits of deep neural networks, and so can potentially run sophisticated feature detection. Whatever cognitive inputs a shard activates for, it will have to have feature detection for -- you simply can't have a shard sensitive to an alien concept you don't represent *anywhere* in your neural net. Because shards are subcircuits in a large neural network, it's possible for them to be hooked up into each other and share feature detectors, or to be informationally isolated from each other. To whatever extent your shards' feature detectors are all shared, you will have a single world-model that acts as input into all shards. To whatever extent your shards keep their feature detectors to themselves, they'll have their own ontology that only guides your behavior after that shard has been activated.\n\nSubcortical reinforcement circuits, though, hail from a distinct informational world. Your hardwired reinforcement circuits *don't* do any sophisticated feature detection (at most picking up on simple regular patterns in retinal stimulation and the like), and so have to reinforce computations \"blindly,\" relying only on simple sensory proxies.\n\nFinally, for the most intelligent RL systems, some kind of additional self-supervised training loop will have to be run along with RL. Reinforcement alone is just too sparse a signal to train a randomly initialized model up to significant capabilities in an appreciably complex environment. For a human, this might look something like trying to predict what's in your visual periphery before focusing your vision on it, sometimes suffering perceptual surprise. Plausibly, some kind of self-supervised loop like this is training all of the computations in the brain, testing them against cached ground truths. This additional source of feedback from self-supervision will both make RL models more capable than they would otherwise be and alter inter-shard dynamics (as we'll briefly discuss later).\n\nWhen You're Dumb, Continuously Blended Tasks Mean Continuous, Broadening Values\n===============================================================================\n\nEarly in life, when your shards only activate in select cognitive contexts and you are thus largely wandering blindly into reinforcement events, *only shards that your reinforcement circuits can pinpoint can be cemented*. Because your subcortical reward circuitry was hardwired by your genome, it's going to be quite bad at accurately assigning credit to shards. Here's an example of an algorithm your reinforcement circuitry could plausibly be implementing: reinforce all the diffs of all the computations running over the last 30 seconds, minus the computations running just before that. This algorithm is sloppy, but is also tractable for the primeval subcortex. In contrast, finding lollipops out in the real world involves a *lot* of computational work. As tasks are distributed in the world in extremely complex patterns *and* are always found blended together, again in a variety of setups, shards are going to have to cope with a continuously shifting flux of cognitive inputs that vary with the environment. When your baby self wanders into a real-world lollipop, many computations will have been active in steering behavior in that direction over the past 30 seconds, so credit assignment will reinforce all of them. The more you train the baby, the wider a range of proxies he internalizes via this reinforcement algorithm. In the face of enough reinforcement events, every representable proxy for the reinforcement event that marginally garners some additional reinforcement will come to hold some staying power. And because of this, simply getting the baby to internalize a particular target proxy *at all* isn't that hard -- just make sure that that target proxy further contributes to reinforcement in-distribution.\n\nMany computations that were active while reinforcement was distributed will be random jitters. So the splash damage from hardcoded credit assignment will reinforce these jitter-inducing computations as well. But because jitters aren't decent proxies for reinforcement *even in distribution*, they won't be steadily reinforced and will just as plausibly steer into anti-reinforcement events. What jitters do accrete will look more like conditionally activated rote tics than widely activated shards with numerous subroutines, because the jitter computations didn't backchain reinforcement reliably enough to accrete a surrounding body of subshards. Shard theory thus predicts that dumb RL agents internalize lots of representable-by-them in-distribution proxies for reinforcement as shards, as a straightforward consequence of reinforcement events being gated behind a complex conditional distribution of task blends.\n\nWhen You're Smart, Internal Game Theory Explains the Tapestry of Your Values\n============================================================================\n\nSubshards and smaller aggregate shards are potentially quite stupid. At a minimum, a shard is just a circuit that triggers given a particular conceptually chunked input, and outputs a rote behavioral sequence sufficient to garner more reinforcement. This circuit will not be well modeled as an intelligent planner; instead, it's perfectly adequate to think of it as just an observationally activated behavioral sequence. But as all your shards collectively comprise all of (or most of?) your neocortex, large shards can get quite smart.\n\nShards are all stuck inside of a single skull with one another, and only have (1) their interconnections with each other, (2) your motor outputs, and (3) your self-supervised training loop with which to causally influence anything. Game theoretically, your large intelligent shards can be fruitfully modeled as playing a *negotiation game* together: shards can interact with each other via their few output channels, and interactions all blend both zero-sum conflict and pure coordination. Agentic shards will completely route around smaller, non-agentic shards if they have conflicting ends. The interactions played out between your agentic shards then generate a complicated panoply of behaviors.\n\nBy the time our baby has grown up, he will have accreted larger shards equipped with richer world-models, activated by a wider range of cognitive inputs, specifying more and more complex behaviors. Where his shards once just passively dealt with the consequences effected by other shards via their shared motor output channel, they are now intelligent enough to scheme at the other shards. Say that you're considering whether to go off to big-law school, and are concerned about that environment exacerbating the egoistic streak you see and dislike in yourself. You don't want to grow up to be more of an egotist, so you choose to avoid going to your top-ranked big-law-school offer, *even though* the compensation from practicing prestigious big-shot law *would further* your other goals. On the (unreconstructed) standard agent model, this behavior is mysterious. Your utility function is fixed, no? Money is instrumentally useful; jobs and education are just paths through state space; your terminal values are *almost orthogonal* to your merely instrumental choice of career. On shard theory, though, this phenomenon of *value drift* isn't at all mysterious. Your egotistical shard would be steered into many reinforcement events were you to go off to the biggest of big-law schools, so your remaining shards use their collective steering control to avoid going down that path now, while they still have a veto. Similarly, despite knowing that heroin massively activates your reinforcement circuitry, not all that many people do heroin all the time. What's going on is that people reason now about what would happen after massively reinforcing a druggie shard, and see that their other values would not be serviced at all in a post-heroin world. They reason that they should carefully avoid that reinforcement event. On the view that reinforcement is the optimization target of trained reinforcement learners, this is inexplicable; on shard theory, it's straightforward internal game-theory.\n\nShards shouldn't be thought of as an *alternative* to utility functions, but as what utility functions look like *for bounded trained agents.* Your \"utility function\" (an ordering over possible worlds, subject to some consistency conditions) is far too big for your brain to represent. But a utility function can be lossily projected down into a bounded computational object by factoring it into a few shards, each representing a term in the utility function, each term conceptually chunked out of perceptual input. In the limit of perfect negotiation between your constituent shards, what your shards collectively pursue would (boundedly) resemble blended utility-function maximization! At lesser levels of negotiation competence between your shards, you'd observe many of the pathologies we see in human behavior. You might see agents who, e.g., flip back and forth between binge drinking and carefully avoiding the bar. Shard theory might explain this as a coalition of shards keeping an alcoholic shard in check by staying away from alcohol-related conceptual inputs, but the alcoholic shard being activated and taking over once an alcohol-related cognitive input materializes.\n\nShard theory's account of internal game theory also supplies a theory of *value reflection* or *moral philosophizing*. When shards are relatively good at negotiating outcomes with one another, one thing that they might do is to [try to find a common policy that they all consistently follow whenever they are the currently activated shards.](https://www.lesswrong.com/tag/values-handshakes) This common policy will have to be satisfactory to all the capable shards in a person, inside the contexts in which each of those shards is defined. But what the policy does outside of those contexts is completely undetermined. So the shards will hunt for [a single common moral rule that gets them each what they want inside their domains, to harvest gains from trade; what happens off of all of their their domains is undetermined and unimportant.](https://slatestarcodex.com/2018/09/25/the-tails-coming-apart-as-metaphor-for-life/#:~:text=The%20morality%20of,to%20be%20libertarians.) This looks an awful lot like testing various moral philosophies against your various moral intuitions, and trying (so far, in vain) to find a moral philosophy that behaves exactly as your various intuitions ask in all the cases where your intuitions have something to say.\n\nLingering Confusions\n====================\n\nThere are some human phenomena that shard theory doesn't have a tidy story about. The largest is probably the apparent phenomenon of credit assignment improving over a lifetime. When you're older and wiser, you're better at noticing which of your past actions were bad and learning from your mistakes. Possibly, this happens a long time after the fact, without any anti-reinforcement event occurring. But an improved *conceptual* understanding ought to be inaccessible to your subcortical reinforcement circuitry -- on shard theory, being wiser shouldn't mean your shards are reinforced or anti-reinforced any differently.\n\nOne thing that might be going on here is that your shards are better at loading and keeping chosen training data in your self-supervised learning loop buffer, and so steadily reinforcing or anti-reinforcing themselves or their enemy shards, respectively. This might look like trying not to think certain thoughts, so those thoughts can't be rewarded for accurately forecasting your observations. But this is underexplained, and shard theory in general doesn't have a good account of credit assignment improving in-lifetime.\n\nRelevance to Alignment Success\n==============================\n\nThe relevance to alignment is that (1) if shard theory is true, meaningful partial alignment successes are possible, and (2) we have a theoretical road to follow to steadily better alignment successes in RL agents. If we can get RL agents to internalize some human-value shards, alongside a lot of other random alien nonsense shards, then those human-value shards will be our representatives on the inside, and will intelligently bargain for what we care about, after all the shards in the RL agent get smarter. Even if the human shards only win a small fraction of the blended utility function, a small fraction of our lightcone is quite a lot. And we can improve our expected fraction by studying the systematic relationships between reinforcement schedules and learned values, in both present RL systems and in humans.\n\nConclusion\n==========\n\nShard theory is a *research program*: it's a proposed basic ontology of how agents made of neural networks work, most especially those agents with path-dependent control over the reinforcement events they later steer into. Shard theory aims to be a comprehensive theory of which values neural networks learn conditional on different reinforcement schedules. All that shard science is yet to be done, and, on priors, shard theory's attempt is probably not going to pan out. But this relationship is currently relatively unexplored, and competitor theories are relatively  unsupported accounts of learned values (e.g., that a single random in-distribution proxy will be the learned value, or that reinforcement is always the optimization target). Shard theory is trying to work out this relationship and then be able to demonstrably predict, ahead of time, specific learned values given reinforcement parameters."
    },
    "voteCount": 35,
    "forceInclude": true
  },
  {
    "_id": "ZmZBataeY58anJRBb",
    "url": null,
    "title": "Getting from an unaligned AGI to an aligned AGI? ",
    "slug": "getting-from-an-unaligned-agi-to-an-aligned-agi",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "AI"
      },
      {
        "name": "Outer Alignment"
      },
      {
        "name": "AI Boxing (Containment)"
      },
      {
        "name": "AI Success Models"
      }
    ],
    "tableOfContents": {
      "sections": [
        {
          "title": "Summary / Preamble",
          "anchor": "Summary___Preamble",
          "level": 1
        },
        {
          "title": "Restrictions in expressivity of AI",
          "anchor": "Restrictions_in_expressivity_of_AI",
          "level": 1
        },
        {
          "title": "Topics I'll cover in this series",
          "anchor": "Topics_I_ll_cover_in_this_series",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        },
        {
          "anchor": "comments",
          "level": 0,
          "title": "6 comments"
        }
      ],
      "headingsCount": 5
    },
    "contents": {
      "markdown": "<table><tbody><tr><td style=\"background-color:hsl(0, 0%, 90%)\"><h2><strong>Summary / Preamble</strong></h2><p>In&nbsp;<a href=\"https://www.lesswrong.com/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities\"><u>AGI Ruin: A List of Lethalities</u></a>, Eliezer writes&nbsp;<i>“A cognitive system with sufficiently high cognitive powers,&nbsp;<strong>given any medium-bandwidth channel of causal influence</strong>, will not find it difficult to bootstrap to overpowering capabilities independent of human infrastructure.”</i><br><br>I have larger error-bars than Eliezer on some AI-safety-related beliefs, but I share many of his concerns (thanks in large part to being influenced by his writings).<br><br>In this series I will try to explore if we might:</p><ul><li>Start out with a superintelligent AGI that may be unaligned (but seems superficially aligned)</li><li>Only use the AGI in ways where it's channels of causal influence are minimized (and where great steps are taken to make it hard for the AGI to hack itself out of the \"box\" it's in)</li><li>Work quickly but step-by-step towards a AGI-system that probably is aligned, enabling us to use it in more and more extensive ways (as we get more assurances that it's aligned)</li></ul><p>From the AGI-system we may (directly or indirectly) obtain programs that are interpretable and verifiable. These specialized programs could give us new capabilities, and we may trust these capabilities to be aligned and safe (even if we don't trust the AGI to be so). We may use these capabilities to help us with verification, widening the scope of programs we are able to verify (and maybe helping us to make the AGI-system safer to interact with). This could perhaps be a positive feedback-loop of sorts, where we get more and more aligned capabilities, and the AGI-system becomes safer and safer to interact with.</p><p>The reasons for exploring these kinds of strategies are two-fold:</p><ul><li>Maybe we wont solve alignment prior to getting superintelligence (even though it would be better if we did!)</li><li>Even if we <i>think</i> we have solved alignment prior to superintelligence, some of the techniques and strategies outlined here could be encouraged as best practice, so that we get additional layers of alignment-assurance.</li></ul><p>The strategy as a whole involves many iterative and contingency-dependent steps working together. I don't claim to have a 100% watertight and crystalized plan that would get us from A to B. Maybe some readers could be inspired to build upon some of the ideas or analyze them more comprehensively.<br>&nbsp;</p><p>Are any of the ideas in this series new? See <a href=\"https://www.lesswrong.com/posts/ZmZBataeY58anJRBb/getting-from-an-unaligned-agi-to-an-aligned-agi?commentId=NnhRsEaFwehPL6nLK\">here</a> for a discussion of that.</p></td></tr></tbody></table>\n\n***Me*****:** I have some ideas about how how to make use of an unaligned AGI-system to make an aligned AGI-system.\n\n***Imaginary friend*****:** My system 1 is predicting that a lot of confused and misguided ideas are about to come out of your mouth.\n\n***Me*****:** I guess we’ll see. Maybe I'm missing the mark somehow. But do hear me out.\n\n***Imaginary friend*****:** Ok.\n\n***Me*****:** First off, do we agree that a superintelligence would be able to understand what you want when asking for something, presuming that it is given enough information?\n\n***Imaginary friend*****:** Well, kind of. Often there isn’t really a clear answer to what you want.\n\n***Me*****:** Sure. But it would probably be good at predicting what looks to me like good answers. Even if it isn’t properly aligned, it would probably be extremely good at *pretending* to give me what I want. Right?\n\n***Imaginary friend*****:** Agreed.\n\n***Me*****:** So if I said to it *“show me the best source code you can come up with for an aligned AGI-system, and write the code in such a way that it’s as easy as possible to verify that it works as it should”*, then what it gave me would look really helpful - with no easily way for me to see a difference between what I’m provided and what I would be provided if it was aligned. Right?  \n  \n***Imaginary friend*****:** I guess I sort of agree. Like, if it answered your request it would probably look really convincing. But maybe it doesn’t answer your question. It could find a security vulnerability in the OS, and hack itself onto the internet somehow - that would be game over before you even got to ask it any questions. Or maybe you didn’t even try to box it in the first place, since you didn’t realize how capable your AI-system was getting, and it was hiding its capabilities from you.\n\n***Imaginary friend*****:** Or maybe it socially manipulated you in some really clever way, or “hacked” your neural circuitry somehow through sensory input, or figured out some way it could affect the physical world from within the digital realm (e.g. generating radio waves by “thinking”, or some thing we don't even know is physically possible).\n\nWhen we are dealing with a system that may prefer to destroy us (for [instrumentally convergent](https://www.lesswrong.com/tag/instrumental-convergence) reasons), and that system may be orders of magnitude smarter than ourselves - well, it's better to be too careful than not paranoid enough..\n\n***Me:*** I agree with all that. But it’s hard to cover all the branches of things that should be considered in one conversation-path. So for the time being, let’s assume a hypothetical situation where the AI is “boxed” in. And let’s assume that we know it’s extremely capable, and that it can’t “hack” itself out of the box in some direct way (like exploiting a security flaw in the operating system). Ok?\n\n***Imaginary friend*****:** Ok.\n\n***Me*****:** I presume you agree that there are more and less safe ways to use a superintelligent AGI-system? To give an exaggerated example: There is a big difference between “letting it onto the internet” and “having it boxed in, only giving it multiplication questions, and only letting it answer yes or no”.\n\n***Imaginary friend*****:** Obviously. But even if you only give it multiplication-questions, some other team will sooner or later develop AGI and be less scrupulous..\n\n***Me*****:** Sure. But still, we agree that there are more and less safe to try to use an AGI? There is a “scale” of sorts?\n\n***Imaginary friend*****:** Of course.\n\n***Me*****:** Would you also agree that there is a “scale” for how hard it is for an oracle/genie to “trick” you into falsely believing that it has provided you with what you want? For example, if I ask it to prove a mathematical conjecture, that is much harder to “pretend” to do the way I want it without actually doing it (compared to most things)?\n\n***Imaginary friend*****:** Sure.\n\n***Me*****:** What I want to talk about are ways of asking an AGI genie/oracle for things in ways where it’s hard for it to “pretend” that it’s giving us what we want without doing it. And ways we might leverage that to eventually end up with an aligned AGI-system, while trying to keep the total risk (of all the steps we take) low.\n\n***Imaginary friend*****:** My system 1 suspects I am about to hear some half-baked ideas.  \n  \n*![](https://lh3.googleusercontent.com/bFQ4Ku5nhW-RvmeQJoaSQs4T3inlRJeIA5cQYNlXs2JxB2X1YkuDBf2e8iJawzrQ1RRW6nYmK0eTcAYtNg08TO5pdQ3mDAZ1rMrbwgBXs7z3yyd-QYTYhKS0S5eQJ0fFIOieXzdSdSYn6KdrUQ)*\n\n***Me*****:** And your system 1 may have a point. I don’t claim to have detailed and watertight arguments showing exactly how we get from A to B. What I have is an *outline* of how we might get there, while minimizing risk along the way (not to 0%, but to way less than 50% if suggestions are executed faithfully).\n\nBelieve me, I don’t have full knowledge and understanding of everything that has been explored in the AI alignment community…\n\n***Imaginary friend*****:** I believe you.\n\n***Me*****:** …but it seems to me that the kinds of techniques and strategies I'll be outlining are under-discussed.\n\n***Imaginary friend*****:** It sounds like you are looking for ways to avoid having to solve the alignment problem, so that you can focus on other things instead of solving it beforehand.\n\n***Me*****:** I guess it can be interpreted that way. I don’t want it to come across that way though! If we find reliable methods to align a superintelligent AGIs *before* we get superintelligent AGIs, then that would improve our odds of success!\n\nBut maybe we don’t. If we don’t then it’s better to have some plans ready, so that less improvisation is necessary. AFAIK there is no [fire alarm](https://intelligence.org/2017/10/13/fire-alarm/) for AGI, and it certainly doesn't seem too early to start analyzing these kinds of scenarios in a more comprehensive way..\n\nAnd also, even if we *think* we have solved alignment in a reliable way prior to superintelligence, some of the strategies and principles I’ll discuss can be used as additional layers of alignment-assurance. Hence me thinking that it may be a good idea to develop these ideas further, and to encourage them as best practice.\n\n**Restrictions in expressivity of AI**\n--------------------------------------\n\n***Me*****:** The first part of this discussion will focus on things that we can do while the AGI stays in the digital realm. We will get to the question of how to do complicated things in the non-digital realm (and on the internet), but that’s for later.\n\n***Imaginary friend*****:** You understand that any use of an AGI enables it to affect the non-digital realm, right? If you let it write or modify code, that is a way for the AGI to communicate with humans, and communicating with humans is a way to affect the non-digital realm.\n\n***Me*****:** Yes, I agree with that. But I would point out that it is up to us how much “expressivity” we give the AI-system. For example, if we get help from it to point out potential mistakes in some piece of code, maybe we don’t let it write text to explain itself (limiting instead to things such as pointing out areas of code to take an extra look at, highlighting examples of input that may produce unintended behavior, etc). And if we do let it write text, then it is up to us how we limit the expressivity of what it can write in text.\n\n***Imaginary friend*****:** Ok, did you by the way notice that you spoke about not letting it write text - but letting it write example input? Well, example input can be text! Those kinds of things can be easy to miss, and often you may not be aware of what you’re missing.\n\n***Me*****:** I may miss such things sometimes, but sometimes I will also leave them out even though I’m aware of them, so as to not get into tangents all the time. I did notice that one though. Partly thanks to you helping me to get better at noticing those sorts of things.\n\n***Imaginary friend*****:** That’s what imaginary friends are for.\n\nKeep in mind that *any* way to point out bugs or potential problems is a way for the AI to affect how you think and what you think about. Take [this test](https://www.youtube.com/watch?v=vJG698U2Mvo) of selective attention for example. Or look at how magicians focus and misdirect people's attention to trick them. There are lots of things like this.\n\n***Me*****:** I’m certainly not claiming that the techniques I’ll describe are risk free. At the same time, it’s important to consider how different techniques can come together to reduce risk. For example, one thing that will be touched upon again and again is testing the convergence of the results from different methods and architectures and pieces of code. Preferably, if an AI-system writes the code for some module, then the overall system should ensure that we also have that module be written by other AI-systems, and also by humans if possible.\n\nIn regards to e.g. AIs pointing out potential issues, the system as a whole (a system of systems of systems..) would preferably keep track of the changes made due to such suggestions, and also try to keep track of what the output of the system would be if those changes hadn’t been made.\n\n***Imaginary friend*****:** Ok. But just because something makes you more safe, and you can handwave to a lot of techniques where each thing helps a bit with safety - this does not show that the the process as a whole will be safe. And I am a bit worried about you proposing ideas that are hard to criticize because they’re complicated and vague, instead of getting to the “core” of the problem. Also, adding on lots of complication to increase security [can in some cases be counter-productive](https://timharford.com/2019/11/cautionary-tales-ep-3-lala-land-galileos-warning/).\n\n***Me*****:** Some good points there. Although do I feel uncertain about the degree to which there is a “core” of the problem. I want this series to outline a path towards a more \"complete\" solution, but I want to start by focusing more on techniques and principles that are \"building-blocks\".  \n  \nThe best scenario is if the first superintelligent AGI is robustly aligned from the start. I think we should try hard to make that be the case, and I don't hope this series is much of a distraction from that.\n\nI am reminded of how I am in support of trying to reduce CO2 emissions today, but *also* support research on geoengineering.\n\n**Topics I'll cover in this series**\n------------------------------------\n\n***Imaginary friend*****:** So where do you see this discussion going?\n\n***Me*****:** I guess we’ll see. But I do have some plans.\n\nOne thing I want to discuss is the concept of “solution-generators” (and maybe also “solution-generators-generators”, etc). When asking an oracle/genie for something, it may often make sense to not ask the question directly, but to instead ask for a “solution-generator” - that is to say, a function that generates solutions within the domain in question. These “generators” should be optimized for being narrow/specialized/modular/verifiable (techniques for trying to ensure that the system is faithfully optimizing the “generator” for these properties is a topic by itself).\n\nFor example, if we want to do a physics-simulation, then we might want to use a “simulation-generator” or “simulation-generator-generator”. One merit of this is that the code can be easier to inspect and understand than the code of a superintelligent AGI. Another merit is that the “generator” can be tested against existing data.  \n  \nBut of course, you can also ask it to answer the question directly (in addition to asking for the “solution-generator”). And then you can see if the output is the same when you obtain it indirectly vs when you obtain it directly.\n\n***Imaginary friend*****:** Isn’t it better to not ask it the specific question, so that it’s harder for it to guess what you’re after?\n\n***Me*****:** I will mostly not be mentioning this sort of thing explicitly, but yes I agree, you shouldn’t ask both questions of the same *instance* of the AGI-system. The instance that is asked for *A* should not know that you also have asked it for *B*. Though of course, you could have additional instances that are asked both questions, and check if results/output converge.\n\nAnother topic I want to speculate a bit about is the topic of brain emulations, and also other “generators” that try to predict the behavior and thinking of humans. I’m interested in the question of *“Can we bootstrap from AGI to either brain emulations and/or something else that can predict human behavior, while more or less staying within the digital realm?”*. Obviously there are lots of ethical considerations here, and it’s very important to avoid [suffering subroutines](https://reducing-suffering.org/what-are-suffering-subroutines/) insofar as possible!\n\n***Me*****:** I also want to discuss techniques involving computable argument-trees/”proofs”, where every inference-step is made explicit. As well as outlining possible techniques to have the concepts/propositions of such proofs represent more or less any thought that is sufficiently “clear/crisp/explicit” (blurring the distinction between “mathematical” proofs and any other argument about anything). Included in the discussion will be outlines of ideas for how to deal with “vagueness” and [cluster-like](https://www.lesswrong.com/posts/WBw8dDkAWohFjWQSk/the-cluster-structure-of-thingspace) concepts within such argument-trees/”proofs”.\n\nAnd I’ll be outlining thoughts about capabilities that I think will help with verifying that instructions for doing things in the real world (developing new types of machines and that sort of thing) will work as intended. Such as for example [copying a strawberry at the molecular level](https://twitter.com/ESYudkowsky/status/1070095840608366594) without unintended consequences. Among other things there will be some focus on “generators” for mappings between (1) models/ontologies, and (2) data-structures representing geometric structures (e.g. some sort of physics-simulation), and (3) real things in the actual world that the models are meant to refer to.\n\nThe more people there are who (1) are smart and have thought a lot about something and (2) see things differently from you, the more reason for self-doubt about your own judgment. And this is for me a significant source of uncertainty about my ideas in regards to alignment (and AI more generally). But it seems best to me to just try to describe my perspective as well as I can, and then people can do with that what seems best to them.\n\n***Imaginary friend*****:** Talk to you later then.\n\n* * *\n\n*Any feedback or comments (be that positive or negative or neither) would be received with interest.*"
    },
    "voteCount": 8,
    "forceInclude": true
  },
  {
    "_id": "xERh9dkBkHLHp7Lg6",
    "url": null,
    "title": "Making it harder for an AGI to \"trick\" us, with STVs",
    "slug": "making-it-harder-for-an-agi-to-trick-us-with-stvs",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "AI"
      },
      {
        "name": "Outer Alignment"
      },
      {
        "name": "Verification"
      },
      {
        "name": "AI Success Models"
      },
      {
        "name": "AI Boxing (Containment)"
      }
    ],
    "tableOfContents": {
      "sections": [
        {
          "title": "Summary / Preamble",
          "anchor": "Summary___Preamble",
          "level": 1
        },
        {
          "title": "Clarifying what I mean by STVs (aka “generators”)",
          "anchor": "Clarifying_what_I_mean_by_STVs__aka__generators__",
          "level": 1
        },
        {
          "title": "“Hidden” behaviors in STVs",
          "anchor": "_Hidden__behaviors_in_STVs",
          "level": 1
        },
        {
          "title": "Why-not-bothing & output-convergence",
          "anchor": "Why_not_bothing___output_convergence",
          "level": 1
        },
        {
          "title": "Human-predicting STVs",
          "anchor": "Human_predicting_STVs",
          "level": 1
        },
        {
          "title": "Brain emulations",
          "anchor": "Brain_emulations",
          "level": 2
        },
        {
          "title": "Approximations of brain emulations (sometimes based on indirect methods)",
          "anchor": "Approximations_of_brain_emulations__sometimes_based_on_indirect_methods_",
          "level": 2
        },
        {
          "title": "Lots of raw and somewhat “hand-coded” probabilistic and modular inference-rules that encode typical human responses within some domain/context",
          "anchor": "Lots_of_raw_and_somewhat__hand_coded__probabilistic_and_modular_inference_rules_that_encode_typical_human_responses_within_some_domain_context",
          "level": 2
        },
        {
          "title": "STVs that help with software-development",
          "anchor": "STVs_that_help_with_software_development",
          "level": 1
        },
        {
          "title": "Rewrite code in ways that are proven to not change behavior",
          "anchor": "Rewrite_code_in_ways_that_are_proven_to_not_change_behavior",
          "level": 2
        },
        {
          "title": "Use code-rewrites with proofs as building-blocks in other proofs",
          "anchor": "Use_code_rewrites_with_proofs_as_building_blocks_in_other_proofs",
          "level": 2
        },
        {
          "title": "Convert between code and high-level descriptions/specifications of code (and look for discrepancies)",
          "anchor": "Convert_between_code_and_high_level_descriptions_specifications_of_code__and_look_for_discrepancies_",
          "level": 2
        },
        {
          "title": "Look for bugs in code",
          "anchor": "Look_for_bugs_in_code",
          "level": 2
        },
        {
          "title": "Help to write/propose tests",
          "anchor": "Help_to_write_propose_tests",
          "level": 2
        },
        {
          "title": "Look for structural similarities/overlap between different code bases",
          "anchor": "Look_for_structural_similarities_overlap_between_different_code_bases",
          "level": 2
        },
        {
          "title": "Identify parts/aspect of code that don’t only significantly affects output small sections of the space of possible inputs",
          "anchor": "Identify_parts_aspect_of_code_that_don_t_only_significantly_affects_output_small_sections_of_the_space_of_possible_inputs",
          "level": 2
        },
        {
          "title": "Scope of what STVs could be used for (without becoming too AGI-like)",
          "anchor": "Scope_of_what_STVs_could_be_used_for__without_becoming_too_AGI_like_",
          "level": 1
        },
        {
          "title": "Thanks for now",
          "anchor": "Thanks_for_now",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        },
        {
          "anchor": "comments",
          "level": 0,
          "title": "5 comments"
        }
      ],
      "headingsCount": 20
    },
    "contents": {
      "markdown": "<table style=\"background-color:hsl(0, 0%, 90%)\"><tbody><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><h1>Summary / Preamble</h1><p>AI Alignment has various sub-areas. The area I focus on here is ways we might use a superintelligent AGI-system to help with creating an aligned AGI-system, even if the AGI we start out with isn’t fully aligned.</p><p>Imagine a superintelligence that “pretends” to be aligned. Such an AI may give output that&nbsp;<i>seems</i> to us like what we want. But for some types of requests, it’s very hard to give output that&nbsp;<i>seems</i> to us like what we want without it&nbsp;<i>actually</i> being what we want (even for a superintelligence). Can we obtain new capabilities by making such requests, in such a way that the scope of things we can ask for in a safe way (without being “tricked” or manipulated) is increased? And if so, is it possible to eventually end up with an aligned AGI-system?</p><p>One reason for exploring such strategies is contingency planning (what if we haven’t solved alignment by the time the first superintelligent AGI-system arrives?). Another reason is that additional layers of assurance could be beneficial (even if we&nbsp;<i>think</i> we have solved alignment, are there ways to relatively quickly add additional layers of alignment-assurance?).</p><p>When dealing with a genie/oracle, we may not want to ask it to provide some direct solution/answer to what you want (and have it argue why it’s a good idea). Instead we obtain what we want more indirectly, by having different instances of the genie/oracle aid us in the construction of more narrow/specialized programs (that score high in terms of verifiability).</p><p>We could call such programs STVs, where STV is an abbreviation for&nbsp;<i><strong>S</strong>pecialized&nbsp;<strong>T</strong>ransparent&nbsp;<strong>V</strong>erifiable Program</i>.</p><p><i>STV</i> is a blurry concept (much like how <i>AI</i> and <i>AGI</i> are blurry concepts).</p><p>An STV could be (but would not have to be) a narrow AI. More colloquially, STVs could also be referred to as “generators”.</p><p>In this post I explore principles/techniques/strategies for using STVs in ways that enable us to get things we want from an AGI without being “tricked”.</p><p>I hope and think this article reads well in isolation, even though it's part 2 of&nbsp;<a href=\"https://www.lesswrong.com/posts/ZmZBataeY58anJRBb/getting-from-an-unaligned-agi-to-an-aligned-agi\"><u>a series</u></a>.</p><p>Future posts in this series will include discussion of:</p><ul><li>AI-generated computational proofs/argument-trees, and methods/strategies for verifying such proofs/arguments.</li><li>Ideas for how formalisms for computational proofs could try to incorporate human cluster-like concepts, concept vagueness, formalism-to-reality mappings, references to code and results from running code, etc (from within themselves).</li><li>How different “pieces” outlined in this series might be put together. Can we get from unaligned AGI to aligned AGI without being \"tricked\"/hacked/manipulated along the way, and without being stopped by chicken-or-egg problems?</li><li>Ideas for how we might want to make a system of sub-systems, consisting of various \"siloed\" AGI-systems (with different settings, architectures, and alignment methodologies).</li><li>Strategic and moral considerations, and possible first steps when starting to use superintelligent AGI-systems to do things outside of the digital realm.</li></ul><p>Are any of the ideas in this series new? See&nbsp;<a href=\"https://www.lesswrong.com/posts/ZmZBataeY58anJRBb/?commentId=NnhRsEaFwehPL6nLK\"><u>here</u></a> for a discussion of that.</p></td></tr></tbody></table>\n\n  \n  \n***Me:*** I think it may be helpful to start out with a simplistic example.  \n  \nSo let’s imagine that we are interested in finding good chess-moves, and want an AGI to help us with this.\n\n***Imaginary friend:*** Ok.\n\n***Me:*** We *could* ask it directly for good chess-moves, along with explanations for why these chess-moves seem like good chess-moves.\n\nBut if we want to make it hard for the AGI to “trick” us (to pretend that it’s giving us what we want, but not actually doing so). One way could be to ask for a *program* that searches for good chess-moves (instead of asking directly for the moves themselves).\n\nThis could be beneficial for several reasons:\n\n1.  If the AGI wants to trick you in specific ways for specific strategic reasons, this may be harder when you force it to answer in a general way (not answer in a way that is custom to the specific thing you want).\n2.  If the AGI has strategic reasons for wanting to trick you about specific things, this is made harder when you don’t ask it specifically for what you are after (thus making it less straight-forward to guess what the answer will be used for).\n3.  Code that is specialized for some narrow/constrained task can often be easier to understand. This presumes that the AGI chooses to write the code easy to understand for humans, but if we know it is superintelligent and it’s *pretending* to be aligned then there are limits to what it can get away with (we would notice if the code clearly isn’t optimized for human understandability).\n4.  It may be easier for humans to understand chess programs than analyzing chess-moves directly (presuming that the program is written in a way that favors human understanding).\n5.  Narrow/specialized programs may more often be possible to code in a way where properties of behavior/output can be “proven” using formal/computational proofs. To put it simplistically: The more narrow a program is, the more suspicious it would be if the AGI can’t “prove” that it does what we want it to do.\n6.  Programs that solve problems in a general way - but within a narrow/constrained domain of tasks/questions - can more easily be tested against examples and real-world data. In the case of a chess-program we could for example check if it beats other chess-programs while using less computation.\n\n***Imaginary friend:*** Ok, but being able to confirm that a chess program is good at chess doesn’t solve the alignment problem. We want to be able to use the AGI to deal with the real world. And the real world is messy and complex in ways that board games aren’t, making verification and formal “proofs” much harder.\n\n***Me:*** Sure, I agree with that. I started out with a simplistic example, so as to be able to outline some principles without getting bogged down in details. And more principles and ideas will be conveyed later on. How far this gets us remains to be discussed.\n\nClarifying what I mean by STVs (aka “generators”)\n=================================================\n\n***Me:*** The term STV is a term I made up. Maybe other people have referred to similar concepts using different terms.\n\nIt stands for *“**S**pecialized **T**ransparent **V**erifiable Program”*.\n\nBy *specialized *I mean that the range of questions/tasks it is designed to handle is constrained. For example, maybe it only does one type of task, and only handles input from a specific domain and in a specific format.\n\nIn some cases the program may work well without anything that resembles reasoning. In other cases, reasoning-like processes may be necessary. In such cases the reasoning should (insofar as feasible) be constrained, specialized/non-general, transparent/interpretable, and closed off from other components/subroutines.\n\nBy *transparent* I mean that it’s easy for humans to understand how it works. Maybe not easy in an absolute sense, but no harder than necessary.\n\nInsofar as feasible the code should be modular, and written in a way that is optimized for making it easy for humans to understand (and hard for humans to misunderstand).\n\nThere should be additional tools for analyzing the code beyond looking at source code. Diagrams, high-level textual descriptions, and maybe interactive overviews of various kinds.\n\nIf the program needs components that are more “black-box-like” (like neural networks sometimes can be), then these components should be constrained (in terms of how they can influence the rest of the system, and in terms of what they are used for).\n\nBy *verifiable* I mean that the behavior of the code can be verified. And much like the other properties of STVs, the property of being *verifiable* is not binary/either-or. It’s more of a scale.\n\nOne example of verification is formal/computational proofs. The best case is if good computational proofs can show that the program as a whole does what it’s supposed to do. If that’s not possible, proofs may still help with other things. Such as proving the behavior of subroutines. Or proving properties that the program reliably will approximate by some margin.\n\nSometimes it may be possible to construct proofs about whether or not it’s possible to prove that the system does what we want.\n\nAnother example of verifiability is tests that should be satisfied by functions/modules. Concepts such as [test-driven development](https://en.wikipedia.org/wiki/Test-driven_development) are well-established among humans, and a superintelligent AGI-system could be expected to write much more comprehensive tests than a human.\n\nAnd another important example of verification is to test the system as a whole against examples and/or real-world data. For example:\n\n*   If the STV approximates physics somehow, then it could be compared against real-word data.\n*   If the STV predicts some aspects of human behavior, then it could be tested against actual data from actual humans (which the AGI-system that created the STV didn’t have access to).\n*   If the STV generates proofs of some kind, then we can test if it is able to reliably find proofs that already exist (but which the AGI-system that created it didn’t necessarily have access to).\n\n“Hidden” behaviors in STVs\n==========================\n\n***Imaginary friend:*** An unfriendly AI may create STVs in such a way that they pass our verification-process, but still have unfriendly behaviors “hidden” inside them.\n\nSuch “hidden” behaviors may be subtle and hard to find. They can’t necessarily be “pointed to” in some specific lines of code.\n\nDo you imagine that we can guarantee against such “hidden” behaviors?\n\n***Me:*** I agree that this is an important concern. The degree to which we can guarantee against this - or at least make it unlikely/implausible - is from my perspective an open question (that may be worthy of further thinking/analysis).\n\nBut this text touches upon several techniques and principles that I think can make it harder for an AGI-system to “hide” things.\n\nOne way to make it harder to “hide” misbehavior is to look for ways that the STV can be made more general. To see why that potentially could be helpful, you could imagine a calculator. A calculator is specialized in the sense that it only does arithmetic. But if a calculator has special-purpose circuitry for handling 237+418 - or special-purpose functionality for handling multiplication using numbers that are within some specific range - well, that would be suspicious.. I’m giving a simplistic example here, but the same principle should also apply to more complex cases.\n\nSo we could have STVs that look for ways other STVs can be made more general/simple.\n\nAnother way to make it harder to “hide” misbehavior is to not let the AGI itself do the implementation of the sub-modules, but instead make other STVs do the implementation based on high-level specifications.\n\nAlso, it may in some cases make sense to have STVs that generate other STVs. That is to say, if an STV can be described as an “answer-generator”, then an STV that outputs other STVs could be described as an “answer-generator-generator”. This is an even more indirect way of obtaining what we want from an AGI, and I’ll give some concrete examples of this later.\n\nAnother thing that makes it harder to “hide” things is if the same functionality is to be implemented in different ways, based on varying principles/architectures (since it’s more challenging to make different implementations fail in the same way - even more so if it’s not the same AGI that makes both).\n\nWhy-not-bothing & output-convergence\n====================================\n\n***Me:*** As a general principle, the more reasons we have to believe something, the more confident we can feel.\n\nFor example, mathematicians may feel more confident in a theorem if it has been proved in several different ways (with varying proof-methods and starting assumptions).\n\nIf there are 2 different alignment-methodologies that seem promising, then maybe we would want to use both as oracles/genies side by side, and see if they give the same answers. And if there are 3 different alignment-methodologies that seem promising, or 4 - well, you get the picture..\n\nAt almost every level of a system - from the top level to detail-oriented sub-modules - it may often be helpful to try different architectures/implementations/principles, and see if the output is the same.\n\nWe might sometimes refer to this as the principle of *“why not both?”*:\n\n  \n**![](https://lh4.googleusercontent.com/361uZ46Tbkjf83uxUkNrerh5c22-4clKN_AsoFwpfN2KLZIoDYq7ouZFk68ZGkKA-_QfYAfCI4Hm4nxFDVRsZq6ld8y4yccjBwSIYotzGQqA8yRuV-i2S14iJsiXTvJVtj8DssIvrxHMs7puQtA)**\n\n  \nHere are some examples (not a complete list):\n\n*   If we tell an AGI to make an STV for some task/purpose, then the AGI may come up with various different principles and architectures that such an STV could be based on. In such cases, we may want all the STVs, so that we may use them as one (running them side by side, and seeing it as suspicious if they don’t produce converging output).\n*   Should AGI-systems have access to the history of requests it has been given earlier, or should it be reset before new requests are asked? I think the second often is more advisable, but sometimes it may make sense to try both.\n*   Should we give AGIs lots of background-knowledge that may be helpful for the task (but also may be helpful for tricking or manipulating us)? Or should we give it as little information as possible? Or maybe something in between? A good answer may often be yes, yes and yes (we can run different instances of the same AGI-instance).\n*   Who should make any given STV? And for the sub-modules in the architecture of some STV, who should make those? Often it is a good idea to have several implementations from several systems (with varying degrees of human involvement). And then we can see if outputs converge (or if one implementation can be proven to behave the same as another in a “mathematical proof”-like way).\n*   When we wonder about something, do we want to ask the AGI directly, or do we want an “answer-generator”, or maybe an “answer-generator-generator”? Sometimes the answer may be *“yes, yes and yes”*.\n\nThis kind of thing adds complexity, but with the help of a capable AGI-system it should be possible to set up systems that can help us handle such complexities.\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/cd8c13e36923ccb84f1a60689fd62e3ac7f02a256c23504b.png)\n\nDiagram showing how the same answer is obtained via various direct and indirect routes. It may at times be prudent to have orders of magnitude more routes than is shown in this diagram.\n\nAnother recurring theme in this text is indirection and subroutine reuse. If we let the AGI answer directly, and build whole systems by itself in one swoop, then it is for various reasons easier to “hide” misbehavior in the output. But when the AGI outputs smaller “pieces” of functionality that do some specialized thing, and can be put together in various ways to do various different things, then this can give more opportunities to notice if some of the “pieces” don’t always do what they’re supposed to do.\n\nIf an AGI is asked to output systems that are optimized for verifiability, and the systems it suggests don’t exploit these principles, then this may be a good cause for suspicion!\n\nThe space of all claims and inference-steps that are true is huge, and it’s deeply interconnected. And the more of this network you need to account for, and the more “nodes” in the network that can be verified, the harder it is to get away with false claims without contradicting yourself (especially when the subset of the network you account for is dense). More comprehensive and systematic ways of leveraging this principle is one of the things that will be explored in part 3 of this series.\n\nHuman-predicting STVs\n=====================\n\n***Me:*** One thing STVs maybe could be made to do is, is to predict human responses (what a human would think of some argument, how a human would evaluate some piece of code, etc).\n\n***Imaginary friend:*** Aren’t STVs supposed to be “narrow” though? Humans are in a sense AGIs.\n\n***Me:*** I agree that this makes it more of a challenge to obtain STVs that predict humans (while remaining transparent and verifiable).\n\n***Imaginary friend:*** But you still think that we - with the help of an AGI - could obtain STVs that predict human responses? And that we to a sufficient degree could verify that such STVs actually do what we want them to?  \n  \n***Me:*** It seems likely to me that we could. But it also seems plausible that we wouldn’t be able to.\n\nKeep in mind:\n\n*   There are degrees of success. For example, sometimes we may be only 90% confident that an STV works as it should. In such cases, whether we should use it depends a lot on context/specifics. If it is a component in a larger system, then there may be ways to use it where it only can help (in certain instances, if it works), and doesn’t have much opportunity to do damage.\n*   Human-emulating STVs would not need to always have an answer. For example, if an STV has the job of predicting how a human would categorize something, we could accept that it sometimes isn't confident enough to make a prediction.\n\n***Imaginary friend:*** How would an STV predict human behavior though?\n\n***Me:*** Here are some ideas:\n\n<table style=\"background-color:hsl(0, 0%, 90%)\"><tbody><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><p><strong>Brain emulations</strong></p><p>It’s sometimes pointed out that human brain emulations could have large advantages in terms of alignment, but that the first AGIs are unlikely to be brain emulations. But might we have a potentially unaligned AGI help us obtain brain emulations in a safe+verifiable way, without letting it “leave the digital realm”?</p><p>The outputs of brain emulations can be tested against real-world data (data from brain scanning and so on), and the emulations can also be tested for how good they are at predicting the actions/answers/etc of humans (and other animals).</p><p>STVs that emulate brains need not be given to us directly from an AGI. There would be various other options (that aren’t mutually exclusive). Could it, for example, be possible to generate an STV that generates brain emulations based on the DNA of the animal in question (but without being given any direct info about brains)? Might the functionality of such an STV be made in a very general-purpose way (where it’s hard to “hide” details)? Might the functionality used to indirectly generate brain emulations also be used to generate other things, which may be verified (predictions regarding the inner workings of the gut, simulations of the details of the inner workings of a flower, etc)?</p><p>One dilemma in regards to simulations is how fine-grained they are, and how they handle a model where the details are unclear (they simulate something that exists in the real world, but they are not given precise and accurate data of starting conditions). This is not just a dilemma for brain simulations, but for simulations of any physical system. Something we may want is a system that gives an accurate description of the&nbsp;<i>range</i> of possible outcomes, given a description of the&nbsp;<i>range</i> of possible starting conditions. And we want the possibility for the simulation to not spend lots of computation on details we don’t care about (only computing details that are useful, or that are helpful for verification of simulation). Since these are general-purpose challenges, which aren’t specific to brain simulations, we may want to have STVs that can help generate “simulation-approximations” for any physical system. That way we can&nbsp;<i>also</i> test if they do a consistently accurate job when used to make predictions about other physical systems (and not&nbsp;<i>only</i> be tested for whether or not they do a good job with brain emulations).</p><p>When doing anything that resembles emulating a brain, it is very important to avoid/minimize risk of&nbsp;<a href=\"https://reducing-suffering.org/what-are-suffering-subroutines/\"><u>suffering subroutines</u></a>! Failing at this could result in&nbsp;<a href=\"https://www.lesswrong.com/tag/mind-crime\"><u>mind crimes</u></a> and&nbsp;<a href=\"https://centerforreducingsuffering.org/research/intro/\"><u>suffering</u></a>, potentially at an enormous scale!</p><p>At every step of the process we should:</p><ol><li>Avoid simulations that might be conscious.</li><li>Avoid simulating processes that would be likely to experience significant suffering if we were wrong about #1.</li></ol><p>Subroutines with&nbsp;<a href=\"https://en.wikipedia.org/wiki/Valence_(psychology)\"><u>positive valence</u></a> may often be unproblematic, or even a good thing. But it remains to be seen how good our understanding of consciousness will become (the consequences of assuming wrongly can be very bad!).</p></td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><p><strong>Approximations of brain emulations (sometimes based on indirect methods)</strong></p><p>In a sense any brain emulation can be seen as an approximation of a more high-fidelity emulation, but what I mean here is that large components of the STV need not be based on “emulation” at all, as long as the STV predicts aspects of brain states + what the human answers/does.</p><p>In a sense, knowing what the human says/does may be all we are interested in, but if it makes predictions about brain states then this may make verification easier (especially if the STV is based in part on assumptions about which brain states follow from which, which actions correspond to which brain states, brain state sequences that cannot happen, etc).</p></td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><p><strong>Lots of raw and somewhat “hand-coded” probabilistic and modular inference-rules that encode typical human responses within some domain/context</strong></p><p>Inference-rules could reference results from subroutines that use neural nets, but if so we should probably require that we can verify what aspects of the thinking/work is done by the neural net. Maybe&nbsp;<a href=\"https://en.wikipedia.org/wiki/Bayesian_network\"><u>bayesian networks</u></a>, or something similar, could be part of the system somehow. As much as possible of the system should be symbolic/interpretable.</p><p>Imagine if a smart/cooperative human sits in a room, and is given simple multiplication-questions. I would guess that in such a situation we would not need high-fidelity brain-emulation to predict the humans “output” - a calculator could suffice! This simplistic example could work as a weak “existence-proof” of sorts, showing that in restricted situations/domains, the most probable human output can be predicted without using brain emulations. But whether this can be done in a verifiable way for useful tasks is AFAIK an open question.</p><p>It’s sometimes pointed out that it’s infeasible to “hand-code” what we mean by various&nbsp;<a href=\"https://www.lesswrong.com/posts/WBw8dDkAWohFjWQSk/the-cluster-structure-of-thingspace\"><u>fuzzy concepts</u></a> (such as “person”, “human”, “animal”, “dead”, “happy”, etc). But even if that’s infeasible for&nbsp;<i>us</i>, it’s not necessarily infeasible for a superintelligence. And if a superintelligence hand-codes it, there may be ways of verifying that the hand-coded specification does a good job of mimicking human output.</p><p>The AGI would not necessarily do the “hand-coding” directly itself. It could output STVs that do the “hand-coding” (based on various methods). Some such STVs might do the hand-coding based on being given books and internet archives, and building a model of human concepts from what they read/hear/see.</p></td></tr></tbody></table>\n\nIf we end up in a situation where an AGI can make STVs for us that predict human behavior, we might wish that we had more experimental data to test those STVs against. That’s one example of why it may be useful to plan in advance!  \n  \nFor example, if we think we might want to use human-mimicking STVs to evaluate proofs/arguments provided by AGIs/STVs, but in a piecemeal fashion, then it might be helpful to think ahead of time about what the smallest components/steps of such proofs/arguments (that can be evaluated in isolation) might look like.\n\nAnd if we want STVs that for example mimic humans looking over code, then that is also something that may be helpful to plan for in some detail.\n\nSome STVs may be easier to verify if we have brain state data of humans that do the exact same types of tasks that the STVs emulate humans doing (e.g. from MRIs). Sometimes the STVs may emulate people sitting in a similar room as in the experiment, in front of a similar computer to the one in the experiment, etc.\n\nSTVs should be able to describe patterns about how various brain states correspond to both actions (answers, code that is written, etc) and other measurements (posture, eye movement, milliseconds between keystrokes, mouse movement, brain state measurements, etc). Preferably these patterns should be as general as possible (e.g. not just for people with red hair sitting in rooms with yellow pain when the room is 35°C).\n\nThe more experiments we have, and the data we have from experiments (mouse movement, eye movement, video of posture, brain measurements, etc), the more challenging it may be for an STV to “make things up” (without this being discovered when predictions are tested against existing data).\n\nIt may also be helpful to have additional data about humans who participate in experiments (with the informed consent of participants, of course). Their DNA, bodily features, their gut microbiome, etc.\n\nOften it’s not the average human that we want STVs to predict, but rather humans who are usually high in intelligence and [cognitive reflection](https://en.wikipedia.org/wiki/Cognitive_reflection_test) (and are talented at what they do).\n\nSTVs that help with software-development\n========================================\n\n***Me:*** Another thing STVs could help with is software development. Here are some examples:\n\n<table style=\"background-color:hsl(0, 0%, 90%)\"><tbody><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><p><strong>Rewrite code in ways that are proven to not change behavior</strong></p><p>It may be relatively tractable to prove that two pieces of code behave similarly and always will have the same output (and to verify such proofs).</p><p>If you are a programmer, you know ways to predict that a code-modification won't change output. For example, you know that an if-else statement could be replaced by a switch-statement, or that a+b can be replaced with b+a when a and b are numbers. Sometimes you make mistakes when doing this type of reasoning, but this doesn’t mean that proofs that use similar reasoning are impossible (you sometimes make mistakes when doing math as well, but that doesn’t make it impossible to construct mathematical proofs!).</p><p>These kinds of proofs could be computational, meaning that to mechanically check the proofs would be relatively trivial. And all that is needed to show that a given proof is wrong is one counter-example (2 pieces of code that are “proven” to have the same output/behavior, but have different output/behavior when we run them with some specific input). Such a counter-example would not only invalidate that specific proof - it would be a cause for questioning the proof-system itself (and whoever made it). The better and more extensively a proof-system has been tested, the better.</p><p>Reasons for rewriting code, and proving equivalence between pieces of code, could include:&nbsp;</p><ul><li>Rewriting code to be more computationally efficient</li><li>Rewriting code so as to score higher in terms of how easy it is for humans to understand it</li><li>Using these kinds of proofs as building-blocks in other proofs</li></ul></td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><p><strong>Use code-rewrites with proofs as building-blocks in other proofs</strong></p><p>One example could be proofs showing specifically and precisely how the behavior of two pieces of code are different. Another example could be proofs showing how one piece of code approximates another piece of code.</p><p>Here are some examples where I try to allude to what I have in mind (handwavy, but still dense, so feel free to skip):</p><ul><li><i>“Function A has equivalent output to function B, except for when the input is in range C, in which the output of A is equivalent to the output of Function D when D is given output from B as input”</i></li><li><i>“Using search-procedure A we can’t find any computationally efficient way of choosing input for Function A such that the output from A doesn’t approximate the output of Function B, with the probability-distribution that describes this approximation being within probability-distribution-space C”</i></li><li><i>“Among the space of allowable input for function A, there is no sub-space of size larger than B where the output will approximate function B according to approximation-description C, with the exception of input-spaces for which function A always will return an error”</i></li></ul></td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><p><strong>Convert between code and high-level descriptions/specifications of code (and look for discrepancies)</strong></p><p>What a piece of code is supposed to do can be described at various levels of specificity, and in various ways:</p><ul><li>You can describe what it does with text</li><li>You can make diagrams</li><li>You can give specific examples of what the output is (or should be) given specific input</li><li>You can list things that should be true about the output (either all the time, or presuming certain things being true about the input)</li><li>You can have various interactive tools that let you explore the specification of what a piece of code does / is supposed to do. Letting you see step by step how various inputs would be processed, and letting you “zoom” in and out of modules/subroutines.</li></ul><p>When you have STVs that can convert between code and more high-level descriptions of code, one thing you can do is to search for code with significantly different functionality/behavior that would generate the same high-level description. This could potentially be helpful in finding out how some description is underspecified in ways that matter.</p><p>STVs like this could be used to make it harder for other systems (STVs, AGIs, etc) to “hide” details in code that they write. It may be harder to \"hide\" details when you don't implement everything yourself.</p><p>There need not be only one STV that implements a high-level description. Different STVs could implement the code, and the code they output could be compared (by testing if output converges, and by making proofs about how the code-bases are equal/similar/different in terms of behavior).</p><p>We could make “loops” where we convert from high-level descriptions, to code, to high-level descriptions, to code, etc - with each step being done by different/independent STVs. And then we can see if we get “drift”, or if we get more or less equivalent code-bases and high-level descriptions again and again.</p></td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><p><strong>Look for bugs in code</strong></p><p>Some bugs are bugs in a very unambiguous sense, making them clear targets. Other bugs are more subtle and dependent on nuanced human-like judgements.<br>I wouldn’t expect some very non-AGI-like SVG (that understands code and nothing else) to robustly be able to find all bugs. But probabilistically it could help. Having SVGs based on various principles/architectures/settings that search for bugs could make it much harder to “hide” bugs.</p></td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><p><strong>Help to write/propose tests</strong></p><p>When specifying what code should do, it may be helpful to write tests that can detect examples of code doing what it should do and examples of code not doing what it should do. This can be done by humans, but STVs may also suggest tests.</p></td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><p><strong>Look for structural similarities/overlap between different code bases</strong></p><p>These STVs could search for ways to rewrite the different code bases so as to minimize the amount/size of non-overlapping modules.</p><p>A somewhat similar task would be to give the STV access to existing code-bases/libraries while it codes, and have it try to maximize overlap with these existing code-bases/libraries (as one of the things it maximizes for).</p><p>Another somewhat similar task would be to be given a code base that does one thing, and search for the minimal amount of changes that could be made so as to make that code-base do another thing. (For example, the minimal amount of changes to convert a narrow AI that does one thing into a narrow AI that does some other thing.)</p><p>Overlapping modules can more easily be tested in an extensive way. Especially if the STVs that rely on the modules vary in terms of how easy it is to verify if they do a good job or not.</p><p>When code bases are written/rewritten so as to share code, what we want to \"minimize\" is not necessarily <i>\"amount of code that isn't shared\"</i>. Instead, we may to want to minimize <i>\"functionality that is hard to verify to a comprehensive degree\"</i>. That is to say, we may want to solve some problem by compositioning it in terms of other problems - even if this leads to more complexity or computational inefficiency - if doing so can be helpful from the perspective of verification.</p></td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><p><strong>Identify parts/aspect of code that don’t only significantly affects output small sections of the space of possible inputs</strong></p><p>Are there ways to rewrite code that would not change output for the great majority of inputs, but would change output for a few inputs?</p><p>Are there ways to simplify code that&nbsp;<i>would</i> change output for most input, but would&nbsp;<i>not</i> significantly change the average score of the output (as ascribed when it’s evaluated)?</p><p>If efficient ways can be found of identifying such changes (when they exist in the space of possible changes), then maybe this could be used to make it harder to intentionally “hide” malicious behavior in STVs.</p><p>If we call the part of input-space where output can be verified A, and the part of input-space where we can’t verify output B, are there ways to rewrite the code (without making the code-base more extensive) that wouldn’t affect output within A negatively, but would have a significant effect on some of the output within B? If so, that may often be a reason to have less trust in that STV (and the system that produced it). Especially if the modified STV and the non-modified STV could be made to produce contradicting output.</p></td></tr></tbody></table>\n\nScope of what STVs could be used for (without becoming too AGI-like)\n====================================================================\n\n***Imaginary friend:***If it is hard to verify that an AGI does what you want, then presumably the same will be the case for STVs the more similar they are to an AGI?\n\nMany tasks seem to be more or less AGI-complete, which presumably means that it would be hard for an STV to do them without being AGI-like.\n\nHow broadly capable do you think an STV can be while still being relatively *narrow/specialized*, and scoring well in terms of *transparency* and *verifiability*?\n\n***Me:*** I don’t have any arguments that are watertight enough to justify opinions on this that are confident and precise. But my gut feeling is somewhat optimistic.\n\n***Imaginary friend:*** One thing you should keep in mind is the limits of symbolic reasoning. Earlier in the history of AI, people tried to make expert systems that rely heavily on [“neat” symbolic reasoning](https://en.wikipedia.org/wiki/Symbolic_artificial_intelligence). But these systems were largely unable to deal with the complexity and nuance of the real world.\n\n***Me:*** But there is a huge difference between what an unassisted human can make and what a superintelligent AGI should be able to make. If a superintelligent AGI doesn’t do a much better job than a human would at coding systems that are transparent and verifiable - well, that would be suspicious..\n\nYes, people have worked on systems that rely heavily on explicit reasoning. But everything that *has* been tried is very crude compared to what *could *be tried. A superintelligent AGI would presumably be much less limited in terms of what it would be able to achieve with such systems. More creative, and more able to create sophisticated systems with huge amounts of “hand-coded” functionality.\n\nThere is this mistake many people have a tendency to make, where they underestimate how far we can get on a problem by pointing to how intractable it is to solve in crude/uncreative ways. One example of this mistake, as I see it, is to say that it **certainly** is impossible to prove how to play perfect chess, since this is impossible to calculate in a straight-forward combinatorial way. Another example would be to say that we cannot solve protein folding, since it is computationally intractable (this used to be a common opinion, but it [isn’t anymore](https://www.deepmind.com/blog/alphafold-a-solution-to-a-50-year-old-grand-challenge-in-biology)).\n\nBoth in terms of being “optimistic” and “pessimistic”, we should try to avoid taking for granted more than what we have a basis for taking for granted. And this also applies to the question of *“how well can a program reason while constrained by requirements for verifiability/provability/transparency?”*.\n\nOne way to think of intelligence is as doing efficient search in possibility-space. To put it a bit simplistically:\n\n*   A board game AI searches for strategies/moves that increase the probability of victory.\n*   A comedian searches for things to say and do that make people entertained in a comedic way.\n*   A programmer searches for lines of code that results in programs that score high in terms of certain criteria.\n*   An inventor searches for construction-steps where the construction-steps and the resulting physical systems scores high in terms of certain criteria.\n*   A digital hacker searches for ways to interact with digital systems that result in behavior/outcomes that the hacker wants (contrary to wishes of designers of said digital systems).\n*   A theorem prover searches for proof-steps that can prove whatever it’s trying to prove.\n*   Etc, etc\n\nAnother way to think of intelligence is as being able to build an accurate and extensive model of some domain. Having an extensive model of the domain sort of implies often being able to answer questions of the form *“which options/strategies/possibilities rank high given conditions x and preferences y?”*. Which implies being good at efficient search through possibility-space.\n\nIn order to be good at searching through possibility-space efficiently, here are some capabilities that may help:\n\n*   Reasoning accurately (having it so that intermediate steps in your reasoning process make a firm foundation for further steps - so as to decrease probability of doing steps that aren’t useful or result in mistakes)\n*   Being good at concepts/abstractions (coming up with them, evaluating them, using them). Identifying various parts of your domain according to things that are true about them (similarities, structural overlap, relations and interactions with other parts of domain, etc), and finding patterns or statistical phenomena that (1) apply to these classifications and (2) are helpful to the search.\n*   Making effective choices for where/how to “look” when looking for the next “step” in the search-process. There is a huge possibility-space of stuff that can be done, which means that the system needs to have good procedures for (1) determining “where” and “how” to search, and for (2) continuously evaluating if a certain “search-path” seems worthy of continued exploration.\n*   Being good at evaluating whether a given result from the “search-process” fulfils the criteria that the search attempts to optimize for (not just making such evaluations at the end of the “search”, but also throughout the “search-process”).\n\nSo narrowly intelligent AI-systems may often have big structural similarities. That is to say, they may be designed such that there are many sub-modules that either are exactly the same or have great structural similarities.\n\nImagine for example an STV with the purpose of making code more understandable for humans. Well, in many ways that is a search process. The heuristics and so on for deciding where to look next in “possibility-space” may be domain-specific, but maybe the STV could be designed to start out without these domain-specific heuristics (and have domain-specific heuristics be generated based on processes that are less domain-specific).\n\nMaybe for some domains it’s hard for us to evaluate if the “search procedure” really is finding the best results that can be found. In such cases it may be an advantage if we can test that same “search procedure” (or “search procedures” that are as similar as possible) in other domains where it’s easier to evaluate the results.\n\n***Imaginary friend:*** Let’s think of “evaluation” and “search” as separate (even though there presumably is lots of inter-play). To put it simplistically, we have “search” and we have a “scoring-function” (or more complex interactions with modules that evaluate “score”). And for the sake of argument, let’s assume that we can verify that in some sense the “search” is “optimal” (in terms of being able to find options in possibility-space that are scored high by the “scoring-function”). Even **if **that’s the case, that still leaves the challenge of making a “scoring-function” that reflects what you actually want?\n\n***Me:*** Sure, that’s a challenge. And how challenging that part is will vary from STV to STV. The “scoring” of some solution is itself a task that other STVs could be specialized to work on. In many cases we may get far even if they don’t do a perfect job to begin with.\n\nConsider for example a scoring-function that evaluates how readable some piece of code would be to a human. Even if this function is imperfect, it will probably still help quite a bit. And if we noticed that it was missing obvious improvements, then this could be fixed.  \n  \nAnd as we gain more capabilities, these capabilities may be used to refine and fortify existing capabilities. For example, if we obtain STVs that are verified to do a good job of predicting humans, then these may be used to more comprehensively test and improve scoring-functions (since they are able to compare 2 different ways to write code with equivalent functionality, and can help predict which way makes it easier for humans to understand it and notice problems).\n\nThanks for now\n==============\n\n***Me:***More things can be said about STVs and their potential uses, but I’ve talked for a long time now. Probably best to save other stuff for later.\n\n***Imaginary friend:***I don’t disagree..\n\n***Me:***Talk to you later then :)\n\n* * *\n\n*To me the concepts/ideas in this series seem under-discussed. But I could be wrong about that, either because (1) the ideas have less merit than I think or (2) because they already are discussed/understood among alignment researchers to a greater degree than I realize. I welcome more or less any feedback, and appreciate any help in becoming less wrong.*"
    },
    "voteCount": 4,
    "forceInclude": true
  },
  {
    "_id": "i42Dfoh4HtsCAfXxL",
    "url": "https://radimentary.wordpress.com/2018/01/10/babble/",
    "title": "Babble",
    "slug": "babble",
    "author": "alkjash",
    "question": false,
    "tags": [
      {
        "name": "Rationality"
      },
      {
        "name": "Poetry"
      },
      {
        "name": "Babble and Prune"
      }
    ],
    "tableOfContents": {
      "sections": [
        {
          "title": "Baby Babble",
          "anchor": "Baby_Babble",
          "level": 1
        },
        {
          "title": "Poetry is Babble Study",
          "anchor": "Poetry_is_Babble_Study",
          "level": 1
        },
        {
          "title": "Reading is Outsourcing Babble",
          "anchor": "Reading_is_Outsourcing_Babble",
          "level": 1
        },
        {
          "title": "Tower of Babble",
          "anchor": "Tower_of_Babble",
          "level": 1
        },
        {
          "title": "NP",
          "anchor": "NP",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        },
        {
          "anchor": "comments",
          "level": 0,
          "title": "31 comments"
        }
      ],
      "headingsCount": 7
    },
    "contents": {
      "markdown": "This post is an exercise in \"identifying with the algorithm.\" I'm a big fan of the probabilistic method and randomized algorithms, so my biases will show.\n\nHow do human beings produce knowledge? When we describe rational thought processes, we tend to think of them as essentially deterministic, deliberate, and algorithmic. After some self-examination, however, I've come to think that my process is closer to babbling many random strings and later filtering by a heuristic. I think verbally, and my process for generating knowledge is virtually indistinguishable from my process for generating speech, and also quite similar to my process for generating writing.\n\nHere's a simplistic model of how this works. I try to build a coherent sentence. At each step, to pick the next word, I randomly generate words in the category (correct part of speech, relevance) and sound them out one by one to see which continues the sentence most coherently. So, instead of deliberately and carefully generating sentences in one go, the algorithm is something like:\n\n1.  Babble. Use a weak and local filter to randomly generate a lot of possibilities. Is the word the right part of speech? Does it lie in the same region of thingspace? Does it fit the context?\n2.  Prune. Use a strong and global filter to test for the best, or at least a satisfactory, choice. With this word in the blank, do I actually believe this sentence? Does the word have the right connotations? Does the whole thought read smoothly?\n\nThis is a babble about embracing randomness.\n\nBaby Babble\n-----------\n\n[Research on language development](https://en.wikipedia.org/wiki/Babbling#The_transition_from_babbling_to_language) suggests that baby babble is an direct forerunner to language. You might imagine that infants learn by imitation, and that baby babble is just an imperfect imitation of words the baby hears, and progress occurs as they physiologically adapt to better produce those sounds. You would be wrong.\n\nInstead, infants are initially capable of producing _all the phonemes_ that exist in all human languages, and they slowly prune out which ones they need via reinforcement learning. Based on the sounds that their parents produce and respond to, babies slowly filter out unnecessary phonemes. Their babbles begin to drift as they prune out more and more phonemes, and they start to combine syllables into proto-words. Babble is the process of generating random sounds, and looking for clues about which ones are useful. Something something reinforcement learning partially observable Markov decision process I'm in over my head.\n\nSo, we've learned that babies use the Babble and Prune algorithm to learn language. But this is quite a general algorithm, and evolution is a conservative force. It stands to reason that human beings might learn other things by a similar algorithm. I don't think it's a particularly controversial suggestion that human thought proceeds roughly by cheaply constructing a lot of low-resolution hypotheses and then sieving from them by allowing them to play out to their logical conclusions.\n\nThe point I want to emphasize is that the algorithm has two distinct phases, both of which can be independently optimized. The stricter and stronger your Prune filter, the higher quality content you stand to produce. But one common bug is related to this: if the quality of your Babble is much lower than that of your Prune, you may end up with nothing to say. Everything you can imagine saying or writing sounds cringey or content-free. Ten minutes after the conversation moves on from that topic, your Babble generator finally returns that witty comeback you were looking for. You'll probably spend your entire evening waiting for an opportunity to force it back in.\n\nYour pseudorandom Babble generator can also be optimized, and in two different ways. On the one hand, you can improve the weak filter you're using, to increase the probability of generating higher-quality thoughts. The other way is one of the things named \"creativity\": you can try to eliminate systematic biases in the Babble generator, with the effect of hitting a more uniform subset of relevant concept-space. Exercises that might help include expanding your vocabulary, reading outside your comfort zone, and engaging in the subtle art of nonstandard sentence construction.\n\nPoetry is Babble Study\n----------------------\n\nPoetry is at its heart an isolation exercise for your Babble generator. When creating poetry, you replace your complex, inarticulate, and highly optimized Prune filter with a simple, explicit, and weird one that you're not attached to. Instead of picking words that maximize meaning, relevance, or social signals, you pick words with the right number of syllables that rhyme correctly and follow the right meter.\n\nNow, with the Prune filter simplified and fixed, all the attention is placed on the Babble. What does it feel like to write a poem (not one of those free-form modern ones)? Probably most of your effort is spent Babbling almost-words that fit the meter and rhyme scheme. If you're anything like me, it feels almost exactly like playing a game of Scrabble, fitting letters and syllables onto a board by trial and error. Scrabble is just like poetry: it's all about being good at Babble. And no, I graciously decline to write poetry in public, even though Scrabble does conveniently rhyme with Babble.\n\nPuns and word games are Babble. You'll notice that when you Babble, each new word isn't at all independent from its predecessors. Instead, Babble is more like initiating a random walk in your dictionary, one letter or syllable or inferential step at a time. That's why [word ladders](https://en.wikipedia.org/wiki/Word_ladder) are so appealing - because they stem from a natural cognitive algorithm. I think Scott Alexander's writing quality is great partly because of [his love of puns](http://unsongbook.com/), a sure sign he has a great Babble generator.\n\nIf poetry and puns are phonetic Babble, then \"[Deep Wisdom](http://lesswrong.com/lw/k8/how_to_seem_and_be_deep/)\" is semantic Babble. Instead of randomly arranging words by sound, we're arranging a rather small set of words to sound wise. More often than not, \"deep wisdom\" boils down to word games anyway, e.g. [wise old sayings](http://www.wiseoldsayings.com/wisdom-quotes/):\n\n\"A blind person who sees is better than a seeing person who is blind.\"\n\n\"A proverb is a short sentence based on long experience.\"\n\n\"Economy is the wealth of the poor and the wisdom of the rich.\"\n\nReading is Outsourcing Babble\n-----------------------------\n\nReading and conversation outsource Babble to others. Instead of using your own Babble generator, you flood your brain with other people's words, and then apply your Prune filter. Because others have already Pruned once, the input is particularly high-quality Babble, and you reap particularly beautiful fruit. How many times have you read a thousand-page book, only to fixate on a handful of striking lines or passages?\n\nPrune goes into overdrive when you outsource Babble. A bug I mentioned earlier is having way too strict of a Prune filter, compared to the quality of your Babble. This occurs particularly to people who read and listen much more than they write or speak. When they finally trudge into the attic and turn on that dusty old Babble generator, it doesn't produce thoughts nearly as coherent, witty, or wise as their hyper-developed Prune filter is used to processing.\n\nImpose Babble tariffs. Your conversation will never be as dry and smart as something from a sitcom. If you can't think of anything to say, relax your Prune filter at least temporarily, so that your Babble generator can catch up. Everyone starts somewhere - Babbling platitudes is better than being silent altogether.\n\nConversely, some people have no filter, and these are exactly the kind of people who don't read or listen enough. If all your Babble goes directly to your mouth, you need to install a better Prune filter. Impose export tariffs.\n\nThe reason the [Postmodernism Generator](http://www.elsewhere.org/journal/pomo/) is so fun to read is because computers are now capable of producing great Babble. Reading poetry and randomly generated postmodernism, talking to chatbots, these activities all amount to frolicking in the uncanny valley between Babble and the Pruned.\n\nTower of Babble\n---------------\n\nA wise man once said, \"Do not build Towers out of Babble. You wouldn't build one out of Pizza, would you?\"\n\nNP\n--\n\nNP is the God of Babble. His law is: humans will always be much better at verifying wisdom than producing it. Therefore, go forth and Babble! After all, how did Shakespeare write his famous plays, except by randomly pressing keys on a keyboard?\n\nNP has a little brother called P. The law of P is: never try things you don't understand completely. Randomly thrashing around will get you nowhere.\n\nP believes himself to be a God, an equal to his brother. He is not."
    },
    "voteCount": 99,
    "forceInclude": true
  },
  {
    "_id": "XvN2QQpKTuEzgkZHY",
    "url": null,
    "title": "Being the (Pareto) Best in the World",
    "slug": "being-the-pareto-best-in-the-world",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "Rationality"
      },
      {
        "name": "Efficient Market Hypothesis"
      },
      {
        "name": "Careers"
      }
    ],
    "tableOfContents": {
      "sections": [
        {
          "title": "Elbow Room",
          "anchor": "Elbow_Room",
          "level": 1
        },
        {
          "title": "Problem Density",
          "anchor": "Problem_Density",
          "level": 1
        },
        {
          "title": "Dimensionality",
          "anchor": "Dimensionality",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        },
        {
          "anchor": "comments",
          "level": 0,
          "title": "54 comments"
        }
      ],
      "headingsCount": 5
    },
    "contents": {
      "markdown": "The generalized efficient markets (GEM) principle says, roughly, that things which would give you a big windfall of money and/or status, will not be easy. If such an opportunity were available, someone else would have already taken it. You will never find a $100 bill on the floor of Grand Central Station at rush hour, because someone would have picked it up already.\n\nOne way to circumvent GEM is to be the best in the world at some relevant skill. A superhuman with hawk-like eyesight and the speed of the Flash might very well be able to snag $100 bills off the floor of Grand Central. More realistically, even though financial markets are the ur-example of efficiency, a handful of firms do make impressive amounts of money by being faster than anyone else in their market. I’m unlikely to ever find a proof of the Riemann Hypothesis, but Terry Tao might. Etc.\n\nBut being the best in the world, in a sense sufficient to circumvent GEM, is not as hard as it might seem at first glance (though that doesn’t exactly make it easy). The trick is to exploit dimensionality.\n\nConsider: becoming one of the world’s top experts in proteomics is hard. Becoming one of the world’s top experts in macroeconomic modelling is hard. But how hard is it to become sufficiently expert in proteomics and macroeconomic modelling that nobody is better than you at both simultaneously? In other words, how hard is it to reach the Pareto frontier?\n\nHaving reached that Pareto frontier, you will have circumvented the GEM: you will be the single best-qualified person in the world for (some) problems which apply macroeconomic modelling to proteomic data. You will have a realistic shot at a big money/status windfall, with relatively little effort.\n\n(Obviously we’re oversimplifying a lot by putting things like “macroeconomic modelling skill” on a single axis, and breaking it out onto multiple axes would strengthen the main point of this post. On the other hand, it would complicate the explanation; I’m keeping it simple for now.)\n\nLet’s dig into a few details of this approach…\n\nElbow Room\n----------\n\nThere are many table tennis players, but only one best player in the world. This is a side effect of ranking people on one dimension: there’s only going to be one point furthest to the right (absent a tie).\n\nPareto optimality pushes us into more dimensions. There’s only one best table tennis player, and only one best 100-meter sprinter, but there can be an unlimited number of Pareto-optimal table tennis/sprinters.\n\nProblem is, for GEM purposes, elbow room matters. Maybe I’m the on the pareto frontier of Bayesian statistics and gerontology, but if there’s one person just little bit better at statistics and worse at gerontology than me, and another person just a little bit better at gerontology and worse at statistics, then GEM only gives me the advantage over a tiny little chunk of the skill-space.\n\n![](https://docs.google.com/drawings/u/1/d/sDVk_OQ89TzmIPruCPKmzlA/image?w=624&h=413&rev=323&ac=1&parent=1mnaPbYJycVXegH2JbhxHm4rkf1jNfbzlepMpTbvXclo)\n\nThis brings up another aspect…\n\nProblem Density\n---------------\n\nClaiming a spot on a Pareto frontier gives you some chunk of the skill-space to call your own. But that’s only useful to the extent that your territory contains useful problems.\n\nTwo pieces factor in here. First, how large a territory can you claim? This is about elbow room, as in the diagram above. Second, what’s the density of useful problems within this region of skill-space? The table tennis/sprinting space doesn’t have a whole lot going on. Statistics and gerontology sounds more promising. Cryptography and monetary economics is probably a particularly rich Pareto frontier these days. (And of course, we don’t need to stop at two dimensions - but we’re going to stop there in this post in order to keep things simple.)\n\nDimensionality\n--------------\n\nOne problem with this whole GEM-vs-Pareto concept: if chasing a Pareto frontier makes it easier to circumvent GEM and gain a big windfall, then why doesn’t everyone chase a Pareto frontier? Apply GEM to the entire system: why haven’t people already picked up the opportunities lying on all these Pareto frontiers?\n\nAnswer: dimensionality. If there’s 100 different specialties, then there’s only 100 people who are the best within their specialty. But there’s 10k pairs of specialties (e.g. statistics/gerontology), 1M triples (e.g. statistics/gerontology/macroeconomics), and something like 10^30 combinations of specialties. And each of those pareto frontiers has room for more than one person, even allowing for elbow room. Even if only a small fraction of those combinations are useful, there’s still a _lot_ of space to stake out a territory.\n\nAnd to a large extent, people do pursue those frontiers. It’s no secret that an academic can easily find fertile fields by working with someone in a different department. “Interdisciplinary” work has a reputation for being unusually high-yield. Similarly, carrying scientific work from lab to market has a reputation for high yields. Thanks to the “curse” of dimensionality, these goldmines are not in any danger of exhausting."
    },
    "voteCount": 183,
    "forceInclude": true
  },
  {
    "_id": "4QemtxDFaGXyGSrGD",
    "url": null,
    "title": "\"Other people are wrong\" vs \"I am right\"",
    "slug": "other-people-are-wrong-vs-i-am-right",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "Rationality"
      },
      {
        "name": "Updated Beliefs (examples of)"
      },
      {
        "name": "Chesterton's Fence"
      },
      {
        "name": "Distinctions"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "I’ve recently been spending some time thinking about the rationality mistakes I’ve made in the past. Here’s an interesting one: I think I have historically been too hasty to go from “other people seem very wrong on this topic” to “I am right on this topic”.\n\nThroughout my life, I’ve often thought that other people had beliefs that were really repugnant and stupid. Now that I am older and wiser, I still think I was correct to think that these ideas were repugnant and stupid. Overall I was probably slightly insufficiently dismissive of things like the opinions of apparent domain experts and the opinions of people who seemed smart whose arguments I couldn’t really follow. I also overrated conventional wisdom about factual claims about how the world worked, though I underrated conventional wisdom about how to behave.\n\nExamples of ideas where I thought the conventional wisdom was really dumb:\n\n- I thought that animal farming was a massive moral catastrophe, and I thought it was a sign of terrible moral failure that almost everyone around me didn’t care about this and wasn’t interested when I brought it up.\n- I thought that AI safety was a big deal, and I thought the arguments against it were all pretty stupid. (Nowadays the conventional wisdom has a much higher opinion of AI safety; I’m talking about 2010-2014.)\n- I thought that people have terrible taste in economic policy, and that they mostly vote for good-sounding stuff that stops sounding good if you think about it properly for even a minute\n- I was horrified by people proudly buying products that said “Made in Australia” on them; I didn’t understand how that wasn’t obviously racist, and I thought that we should make it much easier to allow anyone who wants to to come live in Australia. (This one has become much less controversial since Trump inadvertently convinced liberals that they should be in favor of immigration liberalization.)\n- I thought and still think that a lot of people’s arguments about why it’s good to call the police on bike thieves were dumb. See eg many of the arguments people made in response to [a post of mine about this](https://www.facebook.com/bshlgrs/posts/10208229326305580) (that in fairness was a really dumb post, IMO)\n\nI think I was right about other people being wrong. However, I think that my actual opinions on these topics were pretty confused and wrong, much more than I thought at the time. Here’s how I updated my opinion for all the things above:\n\n- I have updated against the simple view of hedonic utilitarianism under which it’s plausible that simple control systems can suffer. A few years ago, I was seriously worried that the future would contain much more factory farming and therefore end up net negative; I now think that I overrated this fear, because (among other arguments) almost no-one actually endorses torturing animals, we just do it out of expediency, and in the limit of better technology our weak preferences will override our expediency.\n- My understanding of AI safety was “eventually someone will build a recursively self improving singleton sovereign AGI, and we need to figure out how to build it such that it can have an off switch and it implements some good value function instead of something bad.” I think this picture was massively oversimplified. On the strategic side, I didn’t think about the possibilities of slower takeoffs or powerful technologies without recursive self improvement; on the technical safety side, I didn’t understand that it’s hard to even build a paperclip maximizer, and a lot of our effort might go into figuring out how to do that.\n- Other people have terrible taste in economic policy, but I think that I was at the time overconfident in various libertarianish ideas that I’m now less enthusiastic about. Also, I no longer think it’s a slam dunk that society is better off from becoming wealthier, because of considerations related to the far future, animals, and whether more money makes us happier.\n- I think that immigration liberalization is more dangerous than I used to think, because rich societies seem to generate massive positive externalities for the rest of the world and it seems possible that a sudden influx of less educated people with (in my opinion) worse political opinions might be killing the goose that lays the golden eggs.\n- Re bike thieves: I think that even though utilitarianism is good and stuff, it’s extremely costly to have thievery be tolerated, because then you have to do all these negative-sum things like buying bike locks. Also it seems like we’re generally better off if people help with enforcement of laws.\n\n---\n\nIn all of these cases, my arguments against others were much higher quality than my actual beliefs. Much more concerningly, I think I was much better at spotting the holes in other people’s arguments than spotting holes in my own.\n\nThere’s also a general factor here of me being overconfident in the details of ideas that had some ring of truth to them. Like, the importance of AGI safety seemed really obvious to me, and I think that my sense of obviousness has historically been pretty good at spotting arguments that later stand up to intense scrutiny. But I was massively overconfident in my particular story for how AGI would go down. I should have been more disjunctive: I should have said “It sure seems like something like this ought to happen, and it seems like step three could happen in any of these four possible ways, and I don’t know which of them will be true, and maybe it will actually be another one, but I feel pretty convinced that there’s some way it will happen”.\n\nHere are some other ideas which I continue to endorse which had that ring of truth to them, but whose details I’ve been similarly overconfident about. (Some of these are pretty obscure.)\n\n- The simulation hypothesis\n- UDASSA\n- The malignancy of the universal prior\n- The mathematical universe hypothesis\n- Humans have weird complex biases related to categories like race and gender, and we should be careful about this in our thinking. (Nowadays this idea is super widespread and so it feels weird to put it in the same list as all these crazy other ideas. But when I first encountered it seriously in my first year of college, it felt like an interesting and new idea, in the same category as many of the cognitive biases I heard about on LessWrong.)\n\nAnd here are ideas which had this ring of truth to them that I no longer endorse:\n\n- We should fill the universe with hedonium.\n- The future might be net negative, because humans so far have caused great suffering with their technological progress and there’s no reason to imagine that this will change. Futurists are biased against this argument because they personally don’t want to die and have a strong selfish desire for human civilization to persist.\n- Because of Landauer’s limit, civilizations have an incentive to aestivate. (This one is wrong because it involves a [misunderstanding of thermodynamics](https://arxiv.org/abs/1902.06730?fbclid=IwAR144hDZx64uJYn0HLkhXtf5PUm7CedUqd6rUiQBfYft79U_IfvsSLsOn-A).)\n\n---\n\nMy bias towards thinking my own beliefs are more reasonable than they are would be disastrous if it prevented me from changing my mind in response to good new arguments. Luckily, I don’t think that I am particularly biased in that direction, for two reasons. Firstly, when I’m talking to someone who thinks I’m wrong, for whatever reason I usually take them pretty seriously and I have a small crisis of faith that prompts me to go off and reexamine my beliefs a bunch. Secondly, I think that most of the time that people present an argument which later changes my mind, my initial reaction is confusion rather than dismissiveness.\n\nAs an example of the first: Once upon a time I told someone I respected that they shouldn’t eat animal products, because of the vast suffering caused by animal farming. He looked over scornfully and told me that it was pretty rich for me to say that, given that I use Apple products—hadn’t I heard about the abusive Apple factory conditions and how they have nets to prevent people killing themselves by jumping off the tops of the factories? I felt terrified that I’d been committing some grave moral sin, and then went off to my room to research the topic for an hour or two. I eventually became convinced that the net effect of buying Apple products on human welfare is probably very slightly positive but small enough to not worry about, and also it didn’t seem to me that there’s a strong deontological argument against doing it.\n\n(I went back and told the guy about the result of me looking into it. He said he didn’t feel interested in the topic anymore and didn’t want to talk about it. I said “wow, man, I feel pretty annoyed by that; you gave me a moral criticism and I took it real seriously; I think it’s bad form to not spend at least a couple minutes hearing about what I found.” Someone else who was in the room, who was very enthusiastic about social justice, came over and berated me for trying to violate someone else’s preferences about not talking about something. I learned something that day about how useful it is to take moral criticism seriously when it’s from people who don’t seem to be very directed by their morals.)\n\nOther examples: When I first ran across charismatic people who were in favor of deontological values and social justicey beliefs, I took those ideas really seriously and mulled them over a lot. A few weeks ago, someone gave me some unexpectedly harsh criticism about my personal manner and several aspects of how I approach my work; I updated initially quite far in the direction of their criticism, only to update 70% of the way back towards my initial views after I spent ten more hours thinking and talking to people about it.\n\nExamples of the second: When I met people whose view of AI safety didn’t match my own naive view, I felt confused and took them seriously (including when they were expressing a bunch of skepticism of MIRI). When my friend Howie told me he thought the criminal justice system was really racist, I was surprised and quickly updated my opinion to “I am confused about this”, rather than dismissing him.\n\nI can’t think of cases where I initially thought an argument was really stupid but then it ended up convincing either me or a majority of people who I think of as my epistemic peers and superiors (eg people who I think have generally good judgement at EA orgs).\n\nHowever, I can think of cases where I felt initially that an argument is dumb, but lots of my epistemic peers think that the argument is at least sort of reasonable. I am concerned by this and I’m trying to combat it. For example, the following arguments are in my current list of things that I am worried I’m undervaluing because they initially seem implausible to me, and are on my to-do list to eventually look into more carefully: Drexler’s Comprehensive AI Systems. AI safety via ambitious value learning. Arguments that powerful AI won’t lead to a singleton.\n\nPlease let me know if you have examples along these lines where I seemed dumber than I’m presenting here.\n\n----\n\nHere’s another perspective on why my approach might be a problem. I think that people are often pretty bad at expressing why they believe things, and in particular they don’t usually say “I don’t know why I believe this, but I believe it anyway.” So if I dismiss arguments that suck, I might be dismissing useful knowledge that other people have gained through experience.\n\nI think I’ve made mistakes along these lines in the past. For example, I used to have a much lower opinion of professionalism than I now do. And there are a couple of serious personal mistakes I’ve made where I looked around for the best arguments against doing something weird I wanted to do, and all of those arguments sucked, and then I decided to do the weird thing, and then it was a bad idea.\n\nKatja Grace [calls this mistake](https://meteuphoric.com/2015/09/06/mistakes-3-breaking-chestertons-fence-in-the-presence-of-bull/) “breaking Chesterton’s fence in the presence of bull”.\n\nThis would suggest the heuristic “Take received wisdom on topics into account, even if you ask people where the received wisdom comes from and they tell you a source that seems extremely unreliable”.\n\nI think this heuristic is alright but shouldn’t be an overriding consideration. The ideas that evolve through the experience of social groups are valuable because they’re somewhat selected for truth and importance. But the selection process for these ideas is extremely simple and dumb.\n\nI’d expect that in most cases where something is bad, there is a legible argument for why we shouldn’t do it (where I’m including arguments from empirical evidence as legible arguments). I’d prefer to just learn all of the few things that society implicitly knows, rather than giving up every time it disagrees with me.\n\nMaybe this is me being arrogant again, but I feel like the mistake I made with the bike-stealing thing wasn’t me refusing to bow to social authority, it was me not trying hard enough to think carefully about the economics of the situation. My inside view is that if I now try to think about economics, I don’t need to incorporate that much outside-view-style discounting of my own arguments.\n\nI have the big advantage of being around people who are really good at articulating the actual reasons why things are bad. Possibly the number one strength of the rationalist community is creating and disseminating good explicit models of things that are widely implicitly understood (eg variants of Goodhart’s law, Moloch, Chesterton’s fence, the unilateralist’s curse, “toxoplasma of rage”). If I was in any other community, I’m worried that I’d make posts like the one about the bike, and no-one would be able to articulate why I was wrong in a way that was convincing. So I don’t necessarily endorse other people taking the strategy I take.\n\nI am not aware of that many cases where I believed something really stupid because all the common arguments against it seemed really dumb to me. If I knew of more cases like this, I’d be more worried about this.\n\n---\n\nClaire Zabel says, in response to all this:\n\n> I'd say you're too quick to buy a whole new story if it has the ring of truth, and too quick to ask others (and probably yourself) to either refute on the spot, or accept, a complex and important new story about something about the world, and leave too little room to say \"this seems sketchy but I can't articulate how\" or \"I want to think about it for a while\" or \"I'd like to hear the critics' counterarguments\" or \"even though none of the above has yielded fruit, I'm still not confident about this thing\"\n\nThis seems plausible. I spend a bunch of time trying to explain why I’m worried about AI risk to people who don’t know much about the topic. This requires covering quite a lot of ground; perhaps I should try harder to explicitly say “by the way, I know I’m telling you a lot of crazy stuff; you should take as long as it takes to evaluate all of this on your own; my goal here is just to explain what I believe; you should use me as a datapoint about one place that human beliefs sometimes go after thinking about the subject.”\n\n---\n\nI feel like my intuitive sense of whether someone else’s argument is roughly legit is pretty good, and I plan to continue feeling pretty confident when I intuitively feel like someone else is being dumb. But I am trying to not make the jump from “I think that this argument is roughly right” to “I think that all of the steps in this fleshed out version of that argument are roughly right”. Please let me know if you think I’m making that particular mistake."
    },
    "voteCount": 104,
    "forceInclude": true
  },
  {
    "_id": "G5TwJ9BGxcgh5DsmQ",
    "url": null,
    "title": "Yes Requires the Possibility of No\n",
    "slug": "yes-requires-the-possibility-of-no",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "Conservation of Expected Evidence"
      },
      {
        "name": "Filtered Evidence"
      },
      {
        "name": "Rationality"
      },
      {
        "name": "Internal Double Crux"
      },
      {
        "name": "Principles"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "1\\. A group wants to try an activity that really requires a lot of group buy in. The activity will not work as well if there is doubt that everyone really wants to do it. They establish common knowledge of the need for buy in. They then have a group conversation in which several people make comments about how great the activity is and how much they want to do it. Everyone wants to do the activity, but is aware that if they did not want to do the activity, it would be awkward to admit. They do the activity. It goes poorly.\n\n2\\. Alice strongly wants to believe A. She searches for evidence of A. She implements a biased search, ignoring evidence against A. She finds justifications for her conclusion. She can then point to the justifications, and tell herself that A is true. However, there is always this nagging thought in the back of her mind that maybe A is false. She never fully believes A as strongly as she would have believed it if she just implemented an an unbiased search, and found out that A was, in fact, true.\n\n3\\. Bob wants Charlie to do a task for him. Bob phrases the request in a way that makes Charlie afraid to refuse. Charlie agrees to do the task. Charlie would have been happy to do the task otherwise, but now Charlie does the task while feeling resentful towards Bob for violating his consent.\n\n4\\. Derek has an accomplishment. Others often talk about how great the accomplishment is. Derek has imposter syndrome and is unable to fully believe that the accomplishment is good. Part of this is due to a desire to appear humble, but part of it stems from Derek's lack of self trust. Derek can see lots of pressures to believe that the accomplishment is good. Derek does not understand exactly how he thinks, and so is concerned that there might be a significant bias that could cause him to falsely conclude that the accomplishment is better than it is. Because of this he does not fully trust his inside view which says the accomplishment is good.\n\n5\\. Eve is has an aversion to doing B. She wants to eliminate this aversion. She tries to do an internal double crux with herself. She identifies a rational part of herself who can obviously see that it is good to do B. She identifies another part of herself that is afraid of B. The rational part thinks the other part is stupid and can't imagine being convinced that B is bad. The IDC fails, and Eve continues to have an aversion to B and internal conflict.\n\n6\\. Frank's job or relationship is largely dependent to his belief in C. Frank really wants to have true beliefs, and so tries to figure out what is true. He mostly concludes that C is true, but has lingering doubts. He is unsure if he would have been able to conclude C is false under all the external pressure.\n\n7\\. George gets a lot of social benefits out of believing D. He believes D with probability 80%, and this is enough for the social benefits. He considers searching for evidence of D. He thinks searching for evidence will likely increase the probability to 90%, but it has a small probability of decreasing the probability to 10%. He values the social benefit quite a bit, and chooses not to search for evidence because he is afraid of the risk.\n\n8\\. Harry sees lots of studies that conclude E. However, Harry also believes there is a systematic bias that makes studies that conclude E more likely to be published, accepted, and shared. Harry doubts E.\n\n9\\. A bayesian wants to increase his probability of proposition F, and is afraid of decreasing the probability. Every time he tries to find a way to increase his probability, he runs into an immovable wall called the conservation of expected evidence. In order to increase his probability of F, he must risk decreasing it."
    },
    "voteCount": 85,
    "forceInclude": true
  },
  {
    "_id": "Kbm6QnJv9dgWsPHQP",
    "url": null,
    "title": "Schelling fences on slippery slopes",
    "slug": "schelling-fences-on-slippery-slopes",
    "author": "Scott Alexander",
    "question": false,
    "tags": [
      {
        "name": "Rationality"
      },
      {
        "name": "Game Theory"
      },
      {
        "name": "Value Drift"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "Slippery slopes are themselves a slippery concept. Imagine trying to explain them to an alien:  \n  \n\"Well, we right-thinking people are quite sure that the Holocaust happened, so banning Holocaust denial would shut up some crackpots and improve the discourse. But it's one step on the road to things like banning unpopular political positions or religions, and we right-thinking people oppose that, so we won't ban Holocaust denial.\"  \n  \nAnd the alien might well respond: \"But you could just ban Holocaust denial, but not ban unpopular political positions or religions. Then you right-thinking people get the thing you want, but not the thing you don't want.\"  \n  \nThis post is about some of the replies you might give the alien.  \n  \n**Abandoning the Power of Choice**  \n  \nThis is the boring one without any philosophical insight that gets mentioned only for completeness' sake. In this reply, giving up a certain point risks losing the ability to decide whether or not to give up other points.  \n  \nFor example, if people gave up the right to privacy and allowed the government to monitor all phone calls, online communications, and public places, then if someone launched a military coup, it would be very difficult to resist them because there would be no way to secretly organize a rebellion. This is also brought up in arguments about gun control a lot.  \n  \nI'm not sure this is properly thought of as a slippery slope argument at all. It seems to be a more straightforward \"Don't give up useful tools for fighting tyranny\" argument.  \n  \n**The Legend of Murder-Gandhi**  \n  \n[Previously](http://yudkowsky.net/singularity) [on Less Wrong's](/lw/2vj/gandhi_murder_pills_and_mental_illness/) _The Adventures of Murder-Gandhi_: Gandhi is offered a pill that will turn him into an unstoppable murderer. He refuses to take it, because in his current incarnation as a pacifist, he doesn't want others to die, and he knows that would be a consequence of taking the pill. Even if we offered him $1 million to take the pill, his abhorrence of violence would lead him to refuse.  \n  \nBut suppose we offered Gandhi $1 million to take a different pill: one which would decrease his reluctance to murder by 1%. This sounds like a pretty good deal. Even a person with 1% less reluctance to murder than Gandhi is still pretty pacifist and not likely to go killing anybody. And he could donate the money to his favorite charity and perhaps save some lives. Gandhi accepts the offer.  \n  \nNow we iterate the process: every time Gandhi takes the 1%-more-likely-to-murder-pill, we offer him another $1 million to take the same pill again.  \n  \nMaybe original Gandhi, upon sober contemplation, would decide to accept $5 million to become 5% less reluctant to murder. Maybe 95% of his original pacifism is the only level at which he can be _absolutely sure_ that he will still pursue his pacifist ideals.  \n  \nUnfortunately, original Gandhi isn't the one making the choice of whether or not to take the 6th pill. 95%-Gandhi is. And 95% Gandhi doesn't care _quite_ as much about pacifism as original Gandhi did. He still doesn't want to become a murderer, but it wouldn't be a disaster if he were just 90% as reluctant as original Gandhi, that stuck-up goody-goody.  \n  \nWhat if there were a general principle that each Gandhi was comfortable with Gandhis 5% more murderous than himself, but no more? Original Gandhi would start taking the pills, hoping to get down to 95%, but 95%-Gandhi would start taking five more, hoping to get down to 90%, and so on until he's rampaging through the streets of Delhi, killing everything in sight.  \n  \nNow we're tempted to say Gandhi shouldn't even take the first pill. But this also seems odd. Are we really saying Gandhi shouldn't take what's basically a free million dollars to turn himself into 99%-Gandhi, who might well be nearly indistinguishable in his actions from the original?  \n  \nMaybe Gandhi's best option is to \"fence off\" an area of the slippery slope by establishing a [Schelling](/lw/14a/thomas_schellings_strategy_of_conflict/) point - an arbitrary point that takes on special value as a dividing line. If he can hold himself to the precommitment, he can maximize his winnings. For example, original Gandhi could swear a mighty oath to take only five pills - or if he didn't trust even his own legendary virtue, he could give all his most valuable possessions to a friend and tell the friend to destroy them if he took more than five pills. This would commit his future self to stick to the 95% boundary (even though that future self is itching to try to the same precommitment strategy to stick to its own 90% boundary).  \n  \nReal slippery slopes will resemble this example if, each time we change the rules, we also end up changing our opinion about how the rules should be changed. For example, I think the Catholic Church may be working off a theory of \"If we give up this traditional practice, people will lose respect for tradition and want to give up even more traditional practices, and so on.\"  \n  \n**Slippery Hyperbolic Discounting**  \n  \nOne evening, I start playing _Sid Meier's Civilization_ (IV, if you're wondering - V is terrible). I have work tomorrow, so I want to stop and go to sleep by midnight.  \n  \nAt midnight, I consider my alternatives. For the moment, I feel an urge to keep playing Civilization. But I know I'll be miserable tomorrow if I haven't gotten enough sleep. Being a [hyperbolic discounter](/lw/6c/akrasia_hyperbolic_discounting_and_picoeconomics/), I value the next ten minutes a lot, but after that the curve becomes pretty flat and maybe I don't value 12:20 much more than I value the next morning at work. Ten minutes' sleep here or there doesn't make any difference. So I say: \"I will play Civilization for ten minutes - 'just one more turn' - and then I will go to bed.\"  \n  \nTime passes. It is now 12:10. Still being a hyperbolic discounter, I value the next ten minutes a lot, and subsequent times much less. And so I say: I will play until 12:20, ten minutes sleep here or there not making much difference, and then sleep.  \n  \nAnd so on until my empire bestrides the globe and the rising sun peeps through my windows.  \n  \nThis is pretty much the same process described above with Murder-Gandhi except that here the role of the value-changing pill is played by time and my own tendency to discount hyperbolically.  \n  \nThe solution is the same. If I consider the problem early in the evening, I can precommit to midnight as a nice round number that makes a good Schelling point. Then, when deciding whether or not to play after midnight, I can treat my decision not as \"Midnight or 12:10\" - because 12:10 will always win _that_ particular race - but as \"Midnight or abandoning the only credible Schelling point and probably playing all night\", which will be sufficient to scare me into turning off the computer.  \n  \n(if I consider the problem at 12:01, I may be able to precommit to 12:10 if I am especially good at precommitments, but it's not a very natural Schelling point and it might be easier to say something like \"as soon as I finish this turn\" or \"as soon as I discover this technology\").  \n  \n**Coalitions of Resistance  \n**  \nSuppose you are a Zoroastrian, along with 1% of the population. In fact, along with Zoroastrianism your country has fifty other small religions, each with 1% of the population. 49% of your countrymen are atheist, and hate religion with a passion.  \n  \nYou hear that the government is considering banning the Taoists, who comprise 1% of the population. You've never liked the Taoists, vile doubters of the light of Ahura Mazda that they are, so you go along with this. When you hear the government wants to ban the Sikhs and Jains, you take the same tack.  \n  \nBut now you are in the unfortunate situation described by Martin Niemoller:\n\n> _First they came for the socialists, and I did not speak out, because I was not a socialist.  \n> Then they came for the trade unionists, and I did not speak out, because I was not a trade unionist.  \n> Then they came for the Jews, and I did not speak out, because I was not a Jew.  \n> Then they came for me, but we had already abandoned the only defensible Schelling point  \n> _\n\nWith the banned Taoists, Sikhs, and Jains no longer invested in the outcome, the 49% atheist population has enough clout to ban Zoroastrianism and anyone else they want to ban. The better strategy would have been to have all fifty-one small religions form a coalition to defend one another's right to exist. In this toy model, they could have done so in an ecumenial congress, or some other literal strategy meeting.  \n  \nBut in the real world, there aren't fifty-one well-delineated religions. There are billions of people, each with their own set of opinions to defend. It would be impractical for everyone to physically coordinate, so they have to rely on Schelling points.  \n  \nIn the original example with the alien, I cheated by using the phrase \"right-thinking people\". In reality, figuring out who qualifies to join the Right-Thinking People Club is half the battle, and everyone's likely to have a different opinion on it. So far, the practical solution to the coordination problem, the \"only defensible Schelling point\", has been to just have everyone agree to defend everyone else without worrying whether they're right-thinking or not, and this is easier than trying to coordinate room for exceptions like Holocaust deniers. Give up on the Holocaust deniers, and no one else can be sure what other Schelling point you've committed to, if any...  \n  \n...unless they can. In parts of Europe, they've banned Holocaust denial for years and everyone's been totally okay with it. There are also a host of other well-respected exceptions to free speech, like shouting \"fire\" in a crowded theater. Presumably, these exemptions are protected by tradition, so that they have become new Schelling points there, or are else so obvious that everyone except Holocaust deniers is willing to allow a special Holocaust denial exception without worrying it will impact their own case.  \n  \n**Summary**  \n  \nSlippery slopes legitimately exist wherever a policy not only affects the world directly, but affects people's willingness or ability to oppose future policies. Slippery slopes can sometimes be avoided by establishing a \"Schelling fence\" - a Schelling point that the various interest groups involved - or yourself across different values and times - make a credible precommitment to defend."
    },
    "voteCount": 387,
    "forceInclude": true
  },
  {
    "_id": "xg3hXCYQPJkwHyik2",
    "url": null,
    "title": "The Best Textbooks on Every Subject",
    "slug": "the-best-textbooks-on-every-subject",
    "author": "lukeprog",
    "question": false,
    "tags": [
      {
        "name": "World Modeling"
      },
      {
        "name": "Scholarship & Learning"
      },
      {
        "name": "Practical"
      },
      {
        "name": "List of Links"
      },
      {
        "name": "Book Reviews"
      },
      {
        "name": "Whole Brain Emulation"
      },
      {
        "name": "Comfort Zone Expansion (CoZE)"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "For years, my self-education was stupid and wasteful. I learned by consuming blog posts, Wikipedia articles, classic texts, podcast episodes, popular books, [video lectures](http://academicearth.org/), peer-reviewed papers, [Teaching Company](http://www.teach12.com/) courses, and Cliff's Notes. How inefficient!\n\nI've since discovered that _textbooks_ are usually the quickest and best way to learn new material. That's what they are _designed_ to be, after all. Less Wrong [has](/lw/2xt/learning_the_foundations_of_math/) [often](/lw/ow/the_beauty_of_settled_science/) [recommended](/lw/jv/recommended_rationalist_reading/fcg?c=1) the \"read textbooks!\" method. [Make progress by accumulation, not random walks](/lw/1ul/for_progress_to_be_by_accumulation_and_not_by/).\n\nBut textbooks vary widely in quality. I was forced to read some awful textbooks in college. The ones on American history and sociology were memorably bad, in my case. Other textbooks are exciting, accurate, fair, well-paced, and immediately useful.\n\nWhat if we could compile a list of the best textbooks on every subject? That would be _extremely_ useful.\n\nLet's do it.\n\nThere have been [other](/lw/jv/recommended_rationalist_reading/) [pages](/lw/12d/recommended_reading_for_new_rationalists/) of [recommended](/lw/2un/references_resources_for_lesswrong/) [reading](/lw/2xt/learning_the_foundations_of_math/) on Less Wrong before (and [elsewhere](http://ask.metafilter.com/71101/What-single-book-is-the-best-introduction-to-your-field-or-specialization-within-your-field-for-laypeople)), but this post is unique. Here are **the rules**:\n\n1.  Post the title of your favorite textbook on a given subject.\n2.  You must have read at least two other textbooks on that same subject.\n3.  You must briefly name the other books you've read on the subject and explain why you think your chosen textbook is superior to them.\n\nRules #2 and #3 are to protect against recommending a bad book that only seems impressive because it's the only book you've read on the subject. Once, a popular author on Less Wrong recommended Bertrand Russell's _A History of Western Philosophy_ to me, but when I noted that it was more polemical and inaccurate than the other major histories of philosophy, he admitted he hadn't really done much other reading in the field, and only liked the book because it was exciting.\n\nI'll start the list with three of my own recommendations...\n\n**Subject**: History of Western Philosophy\n\n**Recommendation**: _[The Great Conversation](http://www.amazon.com/Great-Conversation-Historical-Introduction-Philosophy/dp/0195397614/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)_, 6th edition, by Norman Melchert\n\n**Reason**: The most popular history of western philosophy is Bertrand Russell's _[A History of Western Philosophy](http://www.amazon.com/History-Western-Philosophy-Bertrand-Russell/dp/0671201581/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)_, which is exciting but also polemical and [inaccurate](http://www.the-philosopher.co.uk/reviews/brussel.htm). More accurate but dry and dull is Frederick Copelston's 11-volume _[A History of Philosophy](http://en.wikipedia.org/wiki/A_History_of_Philosophy_(Copleston))_. Anthony Kenny's recent 4-volume history, collected into one book as _[A New History of Western Philosophy](http://www.amazon.com/New-History-Western-Philosophy/dp/0199589887/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)_, is both exciting and accurate, but perhaps too long (1000 pages) and technical for a first read on the history of philosophy. Melchert's textbook, _[The Great Conversation](http://www.amazon.com/Great-Conversation-Historical-Introduction-Philosophy/dp/0195397614/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)_, is accurate but also the easiest to read, and has the clearest explanations of the important positions and debates, though of course it has its weaknesses (it spends too many pages on ancient Greek mythology but barely mentions Gottlob Frege, the father of analytic philosophy and of the philosophy of language). Melchert's history is also the only one to seriously cover the dominant mode of Anglophone philosophy done today: [naturalism](http://www.amazon.com/Understanding-Naturalism-Movements-Modern-Thought/dp/1844650790/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20) (what Melchert calls \"physical realism\"). Be sure to get the 6th edition, which has major improvements over the 5th edition.\n\n**Subject**: Cognitive Science\n\n**Recommendation**: _[Cognitive Science](http://www.amazon.com/Cognitive-Science-Introduction-Mind/dp/0521882001/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)_, by Jose Luis Bermudez\n\n**Reason**: Jose Luis Bermudez's _[Cognitive Science: An Introduction to the Science of Mind](http://www.amazon.com/Cognitive-Science-Introduction-Mind/dp/0521882001/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)_ does an excellent job setting the historical and conceptual context for cognitive science, and draws fairly from all the fields involved in this heavily interdisciplinary science. Bermudez does a good job of making himself invisible, and the explanations here are some of the clearest available. In contrast, Paul Thagard's _[Mind: Introduction to Cognitive Science](http://www.amazon.com/Mind-Introduction-Cognitive-Science-2nd/dp/026270109X/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)_ skips the context and jumps right into a systematic comparison (by explanatory merit) of the leading theories of mental representation: logic, rules, concepts, analogies, images, and neural networks. The book is only 270 pages long, and is also more idiosyncratic than Bermudez's; for example, Thagard refers to the dominant paradigm in cognitive science as the \"computational-representational understanding of mind,\" which as far as I can tell is used only by him and people drawing from his book. In truth, the term refers to a set of competing theories, for example the [computational theory](http://en.wikipedia.org/wiki/Computational_theory_of_mind) and the [representational theory](http://en.wikipedia.org/wiki/Representational_theory_of_mind). While not the best place to start, Thagard's book is a decent follow-up to Bermudez's text. Better, though, is Kolak et. al.'s _[Cognitive Science: An Introduction to Mind and Brain](http://www.amazon.com/Cognitive-Science-Introduction-Mind-Brain/dp/0415221013/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)_. It contains more information than Bermudez's book, but I prefer Bermudez's flow, organization and content selection. Really, though, both Bermudez and Kolak offer excellent introductions to the field, and Thagard offers a more systematic and narrow investigation that is worth reading after Bermudez and Kolak.\n\n**Subject**: Introductory Logic for Philosophy\n\n**Recommendation**: _[Meaning and Argument](http://www.amazon.com/dp/1405196734/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)_ by Ernest Lepore\n\n**Reason**: For years, the standard textbook on logic was Copi's _[Introduction to Logic](http://www.amazon.com/Introduction-Logic-13th-Irving-Copi/dp/0136141390/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)_, a comprehensive textbook that has chapters on language, definitions, fallacies, deduction, induction, syllogistic logic, symbolic logic, inference, and probability. It spends too much time on methods that are rarely used today, for example Mill's methods of inductive inference. Amazingly, the chapter on probability does not mention Bayes (as of the 11th edition, anyway). Better is the current standard in classrooms: Patrick Hurley's _[A Concise Introduction to Logic](http://www.amazon.com/Concise-Introduction-Logic-CourseCard/dp/0840034172/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)._ It has a table at the front of the book that tells you which sections to read depending on whether you want (1) a traditional logic course, (2) a critical reasoning course, or (3) a course on modern formal logic. The single chapter on induction and probability moves too quickly, but is excellent for its length. Peter Smith's [An Introduction to Formal Logic](http://www.amazon.com/gp/product/0521008042/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20) instead focuses tightly on the usual methods used by today's philosophers: propositional logic and predicate logic. My favorite in this less comprehensive mode, however, is Ernest Lepore's _[Meaning and Argument](http://www.amazon.com/dp/1405196734/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)_, because it (a) is highly efficient, and (b) focuses not so much on the manipulation of symbols in a formal system but on the arguably trickier matter of translating English sentences into symbols in a formal system in the first place.\n\nI would love to read recommendations from experienced readers on the following subjects: physics, chemistry, biology, psychology, sociology, probability theory, economics, statistics, calculus, decision theory, cognitive biases, artificial intelligence, neuroscience, molecular biochemistry, medicine, epistemology, philosophy of science, meta-ethics, and much more.\n\nPlease, post your own recommendations! And, follow [the rules](#rules).\n\n**Recommendations so far** (that follow [the rules](#rules); this list updated 02-25-2017):\n\n*   On **history of western philosophy**, lukeprog [recommends](#history_philosophy) Melchert's _[The Great Conversation](http://www.amazon.com/Great-Conversation-Historical-Introduction-Philosophy/dp/0195397614/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)_ over Russell's _A History of Western Philosophy_, Copelston's _History of Philosophy_, and Kenney's _A New History of Western Philosophy_.\n*   On **cognitive science**, lukeprog [recommends](#cognitive_science) Bermudez's _[Cognitive Science](http://www.amazon.com/Cognitive-Science-Introduction-Mind/dp/0521882001/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)_ over Thagard's _Mind: Introduction to Cognitive Science_ and Kolak's _Cognitive Science_.\n*   On **introductory logic for philosophy**, lukeprog [recommends](#logic) Lepore's _[Meaning and Argument](http://www.amazon.com/dp/1405196734/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)_ over Copi's _Introduction to Logic_, Hurley's _A Concise Introduction to Logic_, and Smith's _An Introduction to Formal Logic_.\n*   On **economics**, michaba03m [recommends](/lw/3gu/the_best_textbooks_on_every_subject/3cd9) Mankiw's _[Macroeconomics](http://www.amazon.com/dp/1429218878/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)_ over Varian's _Intermediate Microeconomics_ and Katz & Rosen's _Macroeconomics_.\n*   On **economics**, realitygrill [recommends](/lw/3gu/the_best_textbooks_on_every_subject/3coo) McAfee's _[Introduction to Economic Analysis](https://open.umn.edu/opentextbooks/textbooks/47)_ over Mankiw's _Principles of Microeconomics_ and Case & Fair's _Principles of Macroeconomics_.\n*   On **representation theory**, SarahC [recommends](/lw/3gu/the_best_textbooks_on_every_subject/3cez) Sternberg's _[Group Theory and Physics](http://www.amazon.com/Group-Theory-Physics-S-Sternberg/dp/0521558859/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)_ over Lang's _Algebra_, Weyl's _The Theory of Groups and Quantum Mechanics_, and Fulton & Harris' _Representation Theory: A First Course_.\n*   On **statistics**, madhadron [recommends](/lw/3gu/the_best_textbooks_on_every_subject/64mz) Kiefer's _[Introduction to Statistical Inference](http://www.amazon.com/Introduction-Statistical-Inference-Springer-Statistics/dp/0387964207/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)_ over Hogg & Craig's _Introduction to Mathematical Statistics_, Casella & Berger's _Statistical Inference_, and others.\n*   On **advanced Bayesian statistics**, Cyan [recommends](/lw/3gu/the_best_textbooks_on_every_subject/3cg2) Gelman's _[Bayesian Data Analysis](http://www.amazon.com/dp/158488388X/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)_ over Jaynes' _Probability Theory: The Logic of Science_ and Bernardo's _Bayesian Theory_.\n*   On **basic Bayesian statistics**, jsalvatier [recommends](/lw/3gu/the_best_textbooks_on_every_subject/3clc) Skilling & Sivia's _[Data Analysis: A Bayesian Tutorial](http://www.amazon.com/Data-Analysis-Bayesian-Tutorial-ebook/dp/B001E5II36/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)_ over Gelman's _Bayesian Data Analysis_, Bolstad's _Bayesian Statistics_, and Robert's _The Bayesian Choice_.\n*   On **real analysis**, paper-machine [recommends](/lw/3gu/the_best_textbooks_on_every_subject/3cll) Bartle's [A Modern Theory of Integration](http://www.amazon.com/Modern-Integration-Graduate-Studies-Mathematics/dp/0821808451/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20) over Rudin's _Real and Complex Analysis_ and Royden's _Real Analysis_.\n*   On **non-relativistic quantum mechanics**, wbcurry [recommends](/lw/3gu/the_best_textbooks_on_every_subject/3cmj) Sakurai & Napolitano's _[Modern Quantum Mechanics](http://www.amazon.com/Modern-Quantum-Mechanics-2nd-Sakurai/dp/0805382917/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)_ over Messiah's _Quantum Mechanics_, Cohen-Tannoudji's _Quantum Mechanics_, and Greiner's _Quantum Mechanics: An Introduction_.\n*   On **music theory**, komponisto [recommends](/lw/3gu/the_best_textbooks_on_every_subject/3cmp) Westergaard's _[An Introduction to Tonal Theory](http://www.amazon.com/Introduction-Tonal-Theory-Peter-Westergaard/dp/0393093425/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)_ over Piston's _Harmony_, Aldwell and Schachter's _Harmony and Voice Leading_, and Kotska and Payne's _Tonal Harmony_.\n*   On **business**, joshkaufman [recommends](/lw/3gu/the_best_textbooks_on_every_subject/3cny) Kaufman's _[The Personal MBA: Master the Art of Business](http://www.amazon.com/gp/product/1591843529/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)_ over Bevelin's _Seeking Wisdom_ and Munger's _Poor Charlie's Alamanack_.\n*   On **machine learning**, alexflint [recommends](/lw/3gu/the_best_textbooks_on_every_subject/3cp0) Bishop's _[Pattern Recognition and Machine Learning](http://www.amazon.com/Pattern-Recognition-Learning-Information-Statistics/dp/0387310738/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)_ over Russell & Norvig's _Artificial Intelligence: A Modern Approach_ and Thrun et. al.'s _Probabilistic Robotics_.\n*   On **algorithms**, gjm [recommends](/lw/3gu/the_best_textbooks_on_every_subject/3cpz) Cormen et. al.'s _[Introduction to Algorithms](http://www.amazon.com/Introduction-Algorithms-Third-Thomas-Cormen/dp/0262033844/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)_ over Knuth's _The Art of Computer Programming_ and Sedgwick's _Algorithms_.\n*   On **electrodynamics**, Alex_Altair [recommends](/lw/3gu/the_best_textbooks_on_every_subject/3cr1) Griffiths' _[Introduction to Electrodynamics](http://www.amazon.com/Introduction-Electrodynamics-3rd-David-Griffiths/dp/013805326X/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)_ over Jackson's _Electrodynamics_ and Feynman's _Lectures on Physics_.\n*   On **electrodynamics**, madhadron [recommends](/lw/3gu/the_best_textbooks_on_every_subject/64mz) Purcell's _[Electricity and Magnetism](http://www.amazon.com/Electricity-Magnetism-Edward-Purcell/dp/1107013607/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)_ over Griffith's _Introduction to Electrodynamics_, Feynman's _Lectures on Physics_, and others.\n*   On **systems theory**, Davidmanheim [recommends](/lw/3gu/the_best_textbooks_on_every_subject/3ctm) Meadows' _[Thinking in Systems: A Primer](http://www.amazon.com/Thinking-Systems-Donella-H-Meadows/dp/1603580557/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)_ over Senge's _The Fifth Discipline: The Art & Practice of The Learning Organization_ and Kim's _Introduction to Systems Thinking_.\n*   On **self-help**, lukeprog [recommends](/lw/3gu/the_best_textbooks_on_every_subject/3dcq) Weiten, Dunn, and Hammer's _[Psychology Applied to Modern Life](http://www.amazon.com/Psychology-Applied-Modern-Life-Adjustment/dp/1111186634/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)_ over Santrock's _Human Adjustment_ and Tucker-Ladd's _Psychological Self-Help_.\n*   On **probability theory**, SarahC [recommends](/lw/3gu/the_best_textbooks_on_every_subject/3f00) Feller's _[An Introduction to Probability Theory](http://www.amazon.com/Introduction-Probability-Theory-Applications-Vol/dp/0471257087/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)_ \\+ _[Vol. 2](http://www.amazon.com/Introduction-Probability-Theory-Applications-Vol/dp/0471257095/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)_ over Ross' _A First Course in Probability_ and Koralov & Sinai's _Theory of Probability and Random Processes_.\n*   On **probability theory**, madhadron [recommends](/lw/3gu/the_best_textbooks_on_every_subject/64mz) Grimmett & Stirzaker's _[Probability and Random Processes](http://www.amazon.com/Probability-Random-Processes-Geoffrey-Grimmett/dp/0198572220/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)_ over Feller's _Introduction to Probability Theory and Its Applications_ and Nelson's _Radically Elementary Probability Theory_.\n*   On **topology**, jsteinhardt [recommends](/lw/3gu/the_best_textbooks_on_every_subject/3ff2) Munkres' _[Topology](http://www.amazon.com/Topology-2nd-James-Munkres/dp/0131816292/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)_ over Armstrong's _Topology_ and Massey's _Algebraic Topology_.\n*   On **linguistics**, etymologik [recommends](/lw/3gu/the_best_textbooks_on_every_subject/3fs1) O'Grady et al.'s _[Contemporary Linguistics](http://www.amazon.com/Contemporary-Linguistics-William-OGrady/dp/0312555288/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)_ over Hayes et al.'s _Linguistics: An Introduction to Linguistic Theory_ and Carnie's _Syntax: A Generative Introduction_.\n*   On **meta-ethics**, lukeprog [recommends](/lw/3gu/the_best_textbooks_on_every_subject/3nvo) Miller's _[An Introduction to Contemporary Metaethics](http://www.amazon.com/Introduction-Contemporary-Metaethics-Alex-Miller/dp/074562345X/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)_ over Jacobs' _The Dimensions of Moral Theory_ and Smith's _Ethics and the A Priori_.\n*   On **decision-making & biases**, badger [recommends](/lw/3gu/the_best_textbooks_on_every_subject/490n) Bazerman & Moore's _[Judgment in Managerial Decision Making](http://www.amazon.com/Judgment-Managerial-Decision-Making-Bazerman/dp/0470049456/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)_ over Hastie & Dawes' _Rational Choice in an Uncertain World_, Gilboa's _Making Better Decisions_, and others.\n*   On **neuroscience**, kjmiller [recommends](/lw/3gu/the_best_textbooks_on_every_subject/4zqx) Bear et al's _Neuroscience: Exploring the Brain_ over Purves et al's _Neuroscience_ and Kandel et al's _Principles of Neural Science_.\n*   On **World War II**, Peacewise [recommends](/lw/3gu/the_best_textbooks_on_every_subject/558w) Weinberg's _[A World at Arms](http://www.amazon.com/World-Arms-Global-History-War/dp/0521618266/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)_ over Churchill's _The Second World War_ and Day's _The Politics of War_.\n*   On **elliptic curves**, magfrump [recommends](/lw/3gu/the_best_textbooks_on_every_subject/5zie) Koblitz' _[Introduction to Elliptic Curves and Modular Forms](http://www.amazon.com/Introduction-Elliptic-Modular-Graduate-Mathematics/dp/0387979662/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)_ over Silverman's _Arithmetic of Elliptic Curves_ and Cassel's _Lectures on Elliptic Curves_.\n*   On **improvisation**, Arepo [recommends](/lw/3gu/the_best_textbooks_on_every_subject/60l7) Salinsky & Frances-White's _[The Improv Handbook](http://www.amazon.com/The-Improv-Handbook-Ultimate-Improvising/dp/0826428584/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)_ over Johnstone's _Impro_, Johnston's _The Improvisation Game_, and others.\n*   On **thermodynamics**, madhadron [recommends](/lw/3gu/the_best_textbooks_on_every_subject/64mz) Hatsopoulos & Keenan's _[Principles of General Thermodynamics](http://www.amazon.com/Principles-General-Thermodynamics-G-Hatsopoulos/dp/0471359998/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)_ over Fermi's _Thermodynamics_, Sommerfeld's _Thermodynamics and Statistical Mechanics_, and others.\n*   On **statistical mechanics**, madhadron [recommends](/lw/3gu/the_best_textbooks_on_every_subject/64mz) Landau & Lifshitz' _[Statistical Physics, Volume 5](http://www.amazon.com/Statistical-Physics-Third-Edition-Part/dp/0750633727/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)_ over Sethna's _Entropy, Order Parameters, and Complexity_ and Reichl's _A Modern Course in Statistical Physics_.\n*   On **criminal justice**, strange [recommends](/lw/3gu/the_best_textbooks_on_every_subject/655d) Fuller's _[Criminal Justice: Mainstream and Crosscurrents](http://www.amazon.com/Criminal-Justice-Mainstream-Crosscurrents-Edition/dp/0135042623/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)_ over Neubauer & Fradella's _America's Courts and the Criminal Justice System_ and Albanese' _Criminal Justice_.\n*   On **organic chemistry**, rhodium [recommends](/lw/3gu/the_best_textbooks_on_every_subject/67k1) Clayden et al's _[Organic Chemistry](http://www.amazon.com/Organic-Chemistry-Jonathan-Clayden/dp/0198503466/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)_ over McMurry's _Organic Chemistry_ and Smith's _Organic Chemistry_.\n*   On **special relativity**, iDante [recommends](/lw/3gu/the_best_textbooks_on_every_subject/72an) Taylor & Wheeler's _[Spacetime Physics](http://www.amazon.com/Spacetime-Physics-Edwin-F-Taylor/dp/0716723271/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)_ over Harris' _Modern Physics_, French's _Special Relativity_, and others.\n*   On **abstract algebra**, Bundle_Gerbe [recommends](/lw/3gu/the_best_textbooks_on_every_subject/72dq) Dummit & Foote's _[Abstract Algebra](http://www.amazon.com/Abstract-Algebra-Edition-David-Dummit/dp/0471433349/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)_ over Lang's _Algebra_ and others.\n*   On **decision theory**, lukeprog [recommends](/lw/3gu/the_best_textbooks_on_every_subject/7avv) Peterson's _[An Introduction to Decision Theory](http://www.amazon.com/Introduction-Decision-Cambridge-Introductions-Philosophy/dp/0521888379/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)_ over Resnik's _Choices_ and Luce & Raiffa's _Games and Decisions_.\n*   On **calculus**, orthonormal [recommends](/lw/3gu/the_best_textbooks_on_every_subject/8uec) Spivak's _[Calculus](http://smile.amazon.com/Calculus-4th-Michael-Spivak/dp/0914098918)_ over Thomas' _Calculus_ and Stewart's _Calculus_. \n*   On **analysis in R^n^**, orthonormal [recommends](/lw/3gu/the_best_textbooks_on_every_subject/8uec) Strichartz's _[The Way of Analysis](http://smile.amazon.com/Analysis-Revised-Jones-Bartlett-Mathematics/dp/0763714976/)_ over Rudin's _Principles of Mathematical Analysis_ and Kolmogorov & Fomin's _Introduction to Real Analysis_.\n*   On **real analysis and measure theory**, orthonormal [recommends](/lw/3gu/the_best_textbooks_on_every_subject/8uec) Stein & Shakarchi's _[Measure Theory, Integration, and Hilbert Spaces](http://smile.amazon.com/Real-Analysis-Integration-Princeton-Lectures/dp/0691113866/)_ over Royden's _Real Analysis_ and Rudin's _Real and Complex Analysis_. \n*   On **partial differential equations**, orthonormal [recommends](/lw/3gu/the_best_textbooks_on_every_subject/8uec) Strauss' _[Partial Differential Equations](http://smile.amazon.com/Partial-Differential-Equations-Walter-Strauss/dp/0470054565)_ over Evans' _Partial Differential Equations_ and Hormander's _Analysis of Partial Differential Operators_.\n*   On **introductory real analysis**, SatvikBeri [recommends](/lw/3gu/the_best_textbooks_on_every_subject/9kw2) Pugh's [Real Mathematical Analysis](http://smile.amazon.com/Mathematical-Analysis-Undergraduate-Texts-Mathematics/dp/0387952977/) over Lang's _Real and Functional Analysis_ and Rudin's _Principles of Mathematical Analysis_.\n*   On **commutative algebra**, SatvikBeri [recommends](/lw/3gu/the_best_textbooks_on_every_subject/9kw9) MacDonald's _[Introduction to Commutative Algebra](http://smile.amazon.com/Introduction-Commutative-Algebra-Addison-Wesley-Mathematics/dp/0201407515/)_ over Lang's _Algebra_ and Eisenbud's _Commutative Algebra With a View Towards Algebraic Geometry_.\n*   On **animal behavior**, Natha [recommends](/lw/3gu/the_best_textbooks_on_every_subject/bke9) Alcock's _[Animal Behavior, 6th edition](http://smile.amazon.com/Animal-Behavior-Evolutionary-Approach-Tenth/dp/0878939660)_ over Dugatkin's _Principles of Animal Behavior_ and newer editions of the Alcock textbook.\n*   On **calculus**, Epictetus [recommends](/lw/3gu/the_best_textbooks_on_every_subject/byee) Courant's _[Differential and Integral Calculus](http://smile.amazon.com/Differential-Integral-Calculus-Vol-One/dp/4871878384/)_ over Stewart's _Calculus_ and Kline's _Calculus_.\n*   On **linear algebra**, Epictetus [recommends](/lw/3gu/the_best_textbooks_on_every_subject/byee) Shilov's _[Linear Algebra](http://smile.amazon.com/Linear-Algebra-Dover-Books-Mathematics/dp/048663518X/)_ over Lay's _Linear Algebra and its Appications_ and Axler's _Linear Algebra Done Right_.\n*   On **numerical methods**, Epictetus [recommends](/lw/3gu/the_best_textbooks_on_every_subject/byee) Press et al.'s _[Numerical Recipes](http://smile.amazon.com/Numerical-Recipes-3rd-Scientific-Computing/dp/0521880688/)_ over Bulirsch & Stoer's _Introduction to Numerical Analysis_, Atkinson's _An Introduction to Numerical Analysis_, and Hamming's _Numerical Methods of Scientists and Engineers_.\n*   On **ordinary differential equations**, Epictetus [recommends](/lw/3gu/the_best_textbooks_on_every_subject/byee) Arnold's _[Ordinary Differential Equations](http://smile.amazon.com/Ordinary-Differential-Equations-V-I-Arnold/dp/0262510189/)_ over Coddington's _An Introduction to Ordinary Differential Equations_ and Enenbaum & Pollard's _Ordinary Differential Equations_.\n*   On **abstract algebra**, Epictetus [recommends](/lw/3gu/the_best_textbooks_on_every_subject/byee) Jacobson's _[Basic Algebra](http://smile.amazon.com/Basic-Algebra-Second-Dover-Mathematics/dp/0486471896/)_ over Bourbaki's _Algebra_, Lang's _Algebra_, and Hungerford's _Algebra_.\n*   On **elementary real analysis**, Epictetus [recommends](/lw/3gu/the_best_textbooks_on_every_subject/byee) Rudin's _[Principles of Mathematical Analysis](http://smile.amazon.com/Principles-Mathematical-Analysis-Rudin/dp/1259064786/)_ over Ross' _Elementary Analysis_, Lang's _Undergraduate Analysis_, and Hardy's _A Course of Pure Mathematics_.\n\nIf there are no recommendations for the subject you want to learn, you can start by checking the [Alibris textbooks](http://www.alibris.com/subjects/subjects-textbooks) category for your subject, and sort by 'Top-selling.' But you'll have to do more research than that. Check which textbooks are asked for in the syllabi of classes on your subject at leading universities. Search Google for recommendations and reviews."
    },
    "voteCount": 475,
    "forceInclude": true
  },
  {
    "_id": "9QxnfMYccz9QRgZ5z",
    "url": null,
    "title": "The Costly Coordination Mechanism of Common Knowledge",
    "slug": "the-costly-coordination-mechanism-of-common-knowledge",
    "author": "Ben Pace",
    "question": false,
    "tags": [
      {
        "name": "Rationality"
      },
      {
        "name": "Common Knowledge"
      },
      {
        "name": "Game Theory"
      },
      {
        "name": "Mechanism Design"
      },
      {
        "name": "Coordination / Cooperation"
      }
    ],
    "tableOfContents": {
      "sections": [
        {
          "title": "Prisoner's Dilemmas vs Coordination Problems",
          "anchor": "Prisoner_s_Dilemmas_vs_Coordination_Problems",
          "level": 1
        },
        {
          "title": "The Prisoner's Dilemma (PD)",
          "anchor": "The_Prisoner_s_Dilemma__PD_",
          "level": 2
        },
        {
          "title": "Real World Examples",
          "anchor": "Real_World_Examples",
          "level": 3
        },
        {
          "title": "Free-Rider Problems",
          "anchor": "Free_Rider_Problems",
          "level": 3
        },
        {
          "title": "Coordination Problems",
          "anchor": "Coordination_Problems",
          "level": 2
        },
        {
          "title": "A Stable State",
          "anchor": "A_Stable_State",
          "level": 3
        },
        {
          "title": "Solving problems and resolving dilemmas",
          "anchor": "Solving_problems_and_resolving_dilemmas",
          "level": 3
        },
        {
          "title": "Three Coordination Problems",
          "anchor": "Three_Coordination_Problems",
          "level": 1
        },
        {
          "title": "Dictators and freedom of speech",
          "anchor": "Dictators_and_freedom_of_speech",
          "level": 2
        },
        {
          "title": "Uncertainty in Romance",
          "anchor": "Uncertainty_in_Romance",
          "level": 2
        },
        {
          "title": "Communal/Religious Rituals",
          "anchor": "Communal_Religious_Rituals",
          "level": 2
        },
        {
          "title": "Common Knowledge Production in Society at Large",
          "anchor": "Common_Knowledge_Production_in_Society_at_Large",
          "level": 1
        },
        {
          "title": "The News",
          "anchor": "The_News",
          "level": 2
        },
        {
          "title": "Academic Research",
          "anchor": "Academic_Research",
          "level": 2
        },
        {
          "title": "Startups",
          "anchor": "Startups",
          "level": 2
        },
        {
          "title": "At what cost?",
          "anchor": "At_what_cost_",
          "level": 2
        },
        {
          "title": "So, what's common knowledge for?",
          "anchor": "So__what_s_common_knowledge_for_",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        },
        {
          "anchor": "comments",
          "level": 0,
          "title": "30 comments"
        }
      ],
      "headingsCount": 19
    },
    "contents": {
      "markdown": "*Recently someone pointed out to me that there was no good canonical post that explained the use of common knowledge in society. Since I wanted to be able to link to such a post, I decided to try to write it.*\n\n*The epistemic status of this post is that I hoped to provide an explanation for a standard, mainstream idea, in a concrete way that could be broadly understood rather than in a mathematical/logical fashion, and so the definitions should all be correct, though the examples in the latter half are more speculative and likely contain some inaccuracies.*\n\nLet's start with a puzzle. What do these three things have in common?\n\n*   Dictatorships all through history have attempted to suppress freedom of the press and freedom of speech. Why is this? Are they just very sensitive? On the other side, the leaders of the Enlightenment fought for freedom of speech, and would not budge an inch against this principle.\n*   When two people are on a date and want to sleep with each other, the conversation will often move towards but never *explicitly* discuss having sex. The two may discuss going back to the place of one of theirs, with a different explicit reason discussed (e.g. \"to have a drink\"), *even* if both want to have sex.\n*   Throughout history, communities have had religious rituals that look very similar. Everyone in the village has to join in. There are repetitive songs, repetitive lectures on the same holy books, chanting together. Why, of all the possible community events (e.g. dinner, parties, etc) is this the most common type?\n\nWhat these three things have in common, is *common knowledge* \\- or at least, the attempt to create it.\n\nBefore I spell that out, we’ll take a brief look into game theory so that we have the language to describe clearly what’s going on. Then we’ll be able to see concretely in a bunch of examples, how common knowledge is necessary to understand and build institutions.\n\nPrisoner's Dilemmas vs Coordination Problems\n============================================\n\nTo understand why common knowledge is useful, I want to contrast two types of situations in game theory: Prisoner’s Dilemmas and Coordination Problems. They look similar at first glance, but their payoff matrices have important differences.\n\nThe Prisoner's Dilemma (PD)\n---------------------------\n\nYou’ve probably heard of it - two players have the opportunity to cooperate, or defect against each other, based on a [story about two prisoners being offered a deal if they testify against the other](https://www.lesserwrong.com/posts/HFyWNBnDNEDsDNLrZ/the-true-prisoner-s-dilemma).\n\nIf they do nothing they will put them both away for a short time; if one of them snitches on the other, the snitch gets off free and the snitched gets a long sentence. However if they *both* snitch they get pretty bad sentences (though neither are as long as when only one snitches on the other).\n\nIn game theory, people often like to draw little boxes that show two different people's choices, and how much they like the outcome. Such a diagram is called a *decision matrix*, and the numbers are called the players' *payoffs*.\n\nTo describe the Prisoner's Dilemma, below is a decision matrix where Anne and Bob each have the same two choices, labelled \\\\(C\\\\) and \\\\(D\\\\). These are colloquially called ‘cooperate’ and ‘defect’. Each box contains two numbers, for Anne and Bob's payoffs respectively. ​\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/e56b0c1b04aaa7289a0bcd7f5d860f6adda8c68afa2d7402.png)\n\nIf the prisoner ‘defects’ on his partner, this means he snitches, and if he ‘cooperates’ with his partner, he doesn’t snitch. They’d both prefer that *both* of them cooperate \\\\((C,C)\\\\) to both of them defecting \\\\((D,D)\\\\), but each of them has an incentive to stab each other in the back to reap the most reward \\\\((D,C)\\\\).\n\nDo you see in the matrix how they both would prefer no snitching to both snitching, but they also have an incentive to stab each other in the back?\n\n**Real World Examples**\n\nNuclear disarmament is a prisoner’s dilemma. Both the Soviet Union and the USS wanted to have nuclear bombs while the opponent doesn't, but they'd probably both prefer a world where nobody had bombs than a world where they were both pointing massive weapons at each others heads. Unfortunately in our world, we failed to solve the problem, and ended up pointing massive weapons at each others' heads for decades.\n\nMilitary budget spending more broadly can be a prisoner’s dilemma. Suppose two neighbouring countries are determining how much to spend on the military. Well, they don’t want to go to war with each other, and so they’d each like to spend a small amount of money on their military, and spend the rest of the money on running the country - infrastructure, healthcare, etc. However, if one country spends a small amount and the other country spends a lot, then the second country can just walk in and take over the first. So, they both spend lots of money on the military with no intention of using it, just so the other one can’t take over.\n\nAnother prisoner’s dilemma is tennis players figuring out whether to take performance enhancing drugs. Naturally, they'd like to dope and the opposing player not, but they'd rather both not dope than both dope.\n\n**Free-Rider Problems**\n\nDid you notice how there are more than two tennis players in the doping situation? When deciding whether to take drugs, not only do you have to worry about whether your opponent in the match today will dope, but also whether your opponent tomorrow will, and the day after, and so on. We’re all wondering whether *all* of us will dope. In society there are loads of these scaled up versions of the prisoner’s dilemma.\n\nFor example, according to many political theories, everyone is better off if the government takes some taxes and uses them to provide public goods (e.g. transportation, military, hospitals). As a population, it's in everyone's interest if everyone cooperates, and takes a small personal sacrifice of wealth.\n\nHowever, if most people are doing it, you can defect, and this is great for you - you get the advantage of a government providing public goods, and also you keep your own money. But if everyone defects, then nobody gets the important public goods, and this is worse for each person than if they'd all cooperated.\n\nWhether you’re two robbers, one of many tennis players, or a whole country fighting another country, you will run into a prisoner’s dilemma. In the scaled-up version, a person who defects while everyone else cooperates is known as a *free-rider*, and the scaled up prisoner’s dilemma is called the *free-rider problem*.\n\nCoordination Problems\n---------------------\n\nWith that under our belt, let’s look at a new decision matrix. Can you identify what’s importantly different about this matrix? Make a prediction about how you think this will change the players’ strategies.​\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/2472044ff084777ed8e2e687f465bdb9148f2450ed7ac80d.png)\n\nDon't mix this up with the Prisoners' Dilemma - it's quite different. In the PD, if you cooperate and I defect, I get 4. What’s important about the new decision-matrix, is that nobody has an incentive to backstab! If you cooperate and I defect, I get *zero*, instead of four.\n\nWe all want the same thing. Both players' preference ordering is:\n\n\\\\\\[(C,C)>(D,D)>\\[(C,D) = (D,C)\\]\\\\\\]\n\nSo, you might be confused: Why is this a problem at all? Why doesn’t everyone just pick C?\n\nLet me give an example from Michael Chwe’s [classic book](https://www.amazon.com/Rational-Ritual-Culture-Coordination-Knowledge/dp/0691114714) on the subject *Rational Ritual: Culture, Coordination and Common Knowledge*.\n\n> Say you and I are co-workers who ride the same bus home. Today the bus is completely packed and somehow we get separated. Because you are standing near the front door of the bus and I am near the back door, I catch a glimpse of you only at brief moments. Before we reach our usual stop, I notice a mutual acquaintance, who yells from the sidewalk, “Hey you two! Come join me for a drink!” Joining this acquaintance would be nice, but we care mainly about each other’s company. The bus doors open; separated by the crowd, we must decide independently whether to get off.\n\n> Say that when our acquaintance yells out, I look for you but cannot find you; I’m not sure whether you notice her or not and thus decide to stay on the bus. How exactly does the communication process fail? There are two possibilities. The first is simply that you do not notice her; maybe you are asleep. The second is that you do in fact notice her. But I stay on the bus because I don’t know whether you notice her or not. In this case we both know that our acquaintance yelled but I do not know that you know.\n\n> Successful communication sometimes is not simply a matter of whether a given message is received. It also depends on whether people are aware that other people also receive it. In other words, it is not just about people’s knowledge of the message; it is also about people knowing that other people know about it, the “metaknowledge” of the message.\n\n> Say that when our acquaintance yells, I see you raise your head and look around for me, but I’m not sure if you manage to find me. Even though I know about the yell, and I know that you know since I see you look up, I still decide to stay on the bus because I do not know that you know that I know. So just one “level” of metaknowledge is not enough.\n\n> Taking this further, one soon realizes that every level of metaknowledge is necessary: I must know about the yell, you must know, I must know that you know, you must know that I know, I must know that you know that I know, and so on; that is, the yell must be “common knowledge.” \n\n> The term “common knowledge” is used in many ways but here we stick to a precise definition. We say that an event or fact is common knowledge among a group of people if everyone knows it, everyone knows that everyone knows it, everyone knows that everyone knows that everyone knows it, and so on.\n\n> Two people can create these many levels of metaknowledge simply through eye contact: say that when our acquaintance yells I am looking at you and you are looking at me, \\[and we exchange a brief glance at our mutual friend and nod\\]. Thus I know you know about the yell, you know that I know that you know (you see me looking at you), and so on. If we do manage to make eye contact, we get off the bus; communication is successful.\n\nCoordination problems are only ever problems when everyone is *currently* choosing \\\\(D\\\\), and we need to *coordinate* all choosing \\\\(C\\\\) at the same time. To do that, we need common knowledge.\n\n(The specific definition of common knowledge (*\"I know that you know that I know that....\"*) is often confusing, but for now the concrete examples below should help get a solid intuition for the idea.)\n\nCompare you and I on the bus to the coordination game payoff matrix: If we *both get off the train *\\\\((C,C)\\\\)*,* we get to hang out with each other *and* spend some time with a mutual acquaintance. If only one of us does, we both miss out on the opportunity to hang out with each other (the thing we want least - \\\\((C,D)\\\\) or \\\\((D,C)\\\\)). If neither of us gets off the train, we get to hang out with each other, but in a less interesting way \\\\((D,D)\\\\).\n\n**A Stable State**\n\nThe reason that it’s a difficult coordination problem, is because the state \\\\((D,D)\\\\) is an equilibrium state; neither of us alone can improve it by getting off the bus - only if we’re able to coordinate us *both* getting off the bus does this work. You can think of it like a local optimum: if you take one step in any direction (if any single one of us changes our actions) we lose utility on net.\n\nThe name for such an equilibrium is taken from mathematician [John Nash](https://en.wikipedia.org/wiki/John_Forbes_Nash_Jr.) (who the film *A Beautiful Mind* was based on), and is called a *Nash equilibrium.* Both \\\\((C,C)\\\\) and \\\\((D,D)\\\\) are Nash equilibria in a coordination problem. Can you see how many Nash equilibria there are in the Prisoner's Dilemma?\n\n**Solving problems and resolving dilemmas**\n\nA good way to contrast coordination problems and free rider problems is to think about these equilibrium states. In the free rider problem, the situation where everyone cooperates is not a Nash equilibrium - everyone is incentivised to defect while the others cooperate, and so occasionally some people do. While the PD only has one Nash equilibrium however, a coordination problem has got two! The challenge is moving from the current one, to one we all prefer.\n\nFree rider problems are solved by creating new incentives against defecting. For example, the government punishes you if you don't pay your taxes. In sports, the practice of doping is punished, and what's more it's made out to be *dishonourable*. People tell stories of the evil people that dope and how we all look down on them; even if you could dope and probably get away with it, there's no plausible deniability in your mind - you know you're being a bad person and would be judged by everyone of your colleagues.\n\nCoordination problems can be solved by creating such incentives, but they can also be solved just by improving information flow. We'll see that below.\n\nThree Coordination Problems\n===========================\n\nThat situation when you and I lock eyes, nod, and get off the bus? That’s *having* *common knowledge*. It’s the confidence to take the step, because you’re not worried about what I might do. Because you know I’m getting off the bus with you.\n\nNow we’ve got a handle on what common knowledge is, we can turn back to the three puzzling phenomena from the beginning.\n\nDictators and freedom of speech\n-------------------------------\n\n> Dictatorships all through history have attempted to suppress freedom of the press and freedom of speech. Why is this? Are they just very sensitive? On the other side, the leaders of the Enlightenment fought for freedom of speech, and would not budge an inch against this principle.\n\nMany people under a dictatorship want a revolution - but rebelling only makes sense if enough *other* people want to rebel. The people as a whole are much more powerful than the government. But you alone won’t be any match for the local police force. You have to *know* that the others are willing to rebel (as long as you rebel), *and* you have to know that they know that *you’re* willing to rebel.\n\nPeople in a dictatorship are all trying to move to a better nash equilibrium without going via the corners of the box (i.e. where some people rebel, but not enough, and then you have some pointless death instead of a revolution).\n\nThat feeling of worrying whether the people around you will support you, if you attack the police. That’s what it’s like *not* to have common knowledge. When a dictator gets ousted by the people, it's often in the form of a riot, because you can *see the other people around you* who are poised on the brink of violence. They can see you, and you all know that if you moved as one you might accomplish something. That’s the feeling of common knowledge.\n\nThe dictator is trying to suppress the people’s ability to create common knowledge that jumps them straight to \\\\((C,C)\\\\) \\- and so they attempt to suppress the news media. Preventing common knowledge being formed among the populace means that large factions cannot coordinate - this is a successful divide and conquer strategy, and is why dictators are able to lead with so little support (often <1% of the population).\n\nUncertainty in Romance\n----------------------\n\n> When two people are on a date and want to sleep with each other, the conversation will often move towards but never *explicitly* discuss having sex. The two may discuss going back to the place of one of theirs, with a different explicit reason discussed (e.g. \"to have a drink\"), *even* if both want to have sex.\n\nNotice the difference between\n\n*   Walking up to someone cold at a bar and starting a conversation\n*   Walking up to someone at a bar, after you noticed them stealing glances at you\n*   Walking up to someone at a bar, after you glanced at them, they glanced at you, and your eyes *locked*\n\nIt’s easiest to approach confidently in the last case, since you have clear evidence that you’re both at least interested in a flirtatious conversation.\n\nIn dating, getting *explicitly* rejected is a loss of status, so people are incentivised to put a lot of effort into preserving plausible deniability. *No really, I just came up to your flat to listen to your vinyl records!* Similarly, we know other people don’t like getting rejected, so we rarely explicitly ask either. *Are you trying to have sex with me?*\n\nSo with sex, romance, or even deep friendships, people are often trying to get to \\\\((C,C)\\\\) *without* common knowledge, up until the moment that they’re both very confident that both parties are interested in raising their level of intimacy.\n\n(Scott Alexander wrote about this attempt to avoid rejection and the confusion it entails in his post [Conversation Deliberately Skirts the Border of Incomprehensibility](http://slatestarcodex.com/2017/06/26/conversation-deliberately-skirts-the-border-of-incomprehensibility/).)\n\nThis problem of *avoiding* common knowledge as we try to move to better Nash equilibrium also shows up in negotiations and war, where you might make a threat, and not *want* there to be common knowledge of whether you’ll actually follow through on that threat.\n\n(Added: After listening to a podcast with Robin Hanson, I realise that I've simplified too much here. It's also the case that each member of the couple might not have figured out whether they want to have sex, and so plausible deniability gives them an out if they decide not to, without the explicit status hit/attack.\n\nI definitely have the sense that if someone very bluntly states subtext when they notice it, this means *I can't play the game with* *them even if I wanted to*, as when they state it explicitly I have to say \"No!\" else admit that I was slightly flirting / exploring a romance with them, and significantly increase the change I will immediately receive an explicit rejection.)\n\nCommunal/Religious Rituals\n--------------------------\n\n> Throughout history, communities have had religious rituals that look very similar. Everyone in the village has to join in. There are repetitive songs, repetitive lectures on the same holy books, chanting together. Why, of all the possible community events (e.g. dinner, parties, etc) is this the most common type?\n\nMichael Chwe wrote a whole [book](https://www.amazon.com/Rational-Ritual-Culture-Coordination-Knowledge/dp/0691114714) on this topic. To simplify massively: rituals are a space to create common knowledge in a community.\n\nYou don’t just listen to a pastor talk about virtue and sin. You listen *together,* where you know that everyone else was listening too. You say ‘*amen*’ together after each prayer the pastor speaks, and you all know that you’re listening along and paying attention. You speak the Lord’s Prayer or some Buddhist chant together, and you know that *everyone* knows the words.\n\nRituals create common knowledge about what in the community is is rewarded, what is punished. This is why religions are so powerful (and why the state likes to control religion). It’s not just a part of life like other institutions everyone uses like a market or a bank - this is an institution that builds common knowledge about *all* areas of life, especially the most important communal norms.\n\nTo flesh out the punishment part of that: When someone does something sinful by the standards of the community, you know that *they* know they’re not supposed to, and they know that you know that they know. This makes it easier to punish people - they can’t claim they didn’t know they weren’t supposed to do something. And making it easier to punish people also makes people less likely to sin in the first place.\n\nThe rituals have been gradually improved and changed over time, and often the trade-offs have been towards helping coordinate a community. This is why the words in the chants or songs that everyone sings are simple, repetitive, and often rhyme - so you know that everyone knows exactly what they are. This is why rituals often occur seated in a circle - not only can you see the performance, but you can see *me* seeing the performance, and I you, and we have common knowledge.\n\nCommon knowledge is often much easier to build in small groups - in the example about getting off the bus, the two need only to look at each other, share a nod, and common knowledge is achieved. Building common knowledge between hundreds or thousands of people is significantly harder, and the fact that religion has such a significant ability to do so is why it has historically had so much connection to politics.\n\nCommon Knowledge Production in Society at Large\n===============================================\n\nCommon knowledge is a very common state of affairs that humans had to reason about naturally in the ancestral environment; there is no explicit mathematical calculation being done when two people lock eyes on a bus then coordinate getting off and seeing their friend.\n\nWe’ve looked at how religions help create common knowledge of norms. Here’s a few other common knowledge producing mechanisms that exist in the world today.\n\nThe News\n--------\n\nThe main way common knowledge is built is by having everyone in the same room, in silence, while somebody speaks. Another way (in the modern world) is official channels of communication that you know everyone listens to.\n\nThis is actually one of the good reasons to discuss news so much - we’ve built trust that what the NYT says is common knowledge, and so can coordinate around it. Sometimes an official document is advertised widely and is known to be known as common knowledge, even if we ourselves often haven’t read it (e.g. Will MacAskill’s book, the NYT).\n\nNowadays there is no such single news source, and we’ve lost that coordination mechanism. We all have Facebook, but Facebook is entirely built out of bubbles. Facebook *could* choose to create common knowledge by making something appear in everyone’s feed, but they choose not to (and this is in fact a fairly restrained use of power that I appreciate).\n\nOne time facebook slipped up on this, was when they built their 'Marked Safe' feature. If a dangerous event (big fire, terrorist attack, earthquake) happened near you, you could 'mark yourself safe' and then all of your friends would get a notification saying you were safe.\n\nNow, it was clear that everyone else was seeing the notifications you were seeing, and so if your nearby friend marked themselves safe and you didn’t, your friends would all notice that conspicuous absence of a notification, and know that you had chosen not to click it. This creates a pressure for all people to always notify their friends whenever there’s been a dangerous event near them, even if the odds of them being involved were miniscule. This is a clear waste of time and attention, ~and the feature was removed~ the feature continues to be a piece of security theatre in our lives.\n\nA related point about the power of media that creates common knowledge: in Michael Chwe's book, he does some data analysis of the marketing strategies of multiple different industries. He classifies products that are 'social goods' - those you want to buy if you expect other people like them. For example, you want to buy wines that you know your guests like, or bring beer to parties that others like; you want to use popular computer brands that people have developed software for; etc.\n\nHe then shows that social brands typically pay more *per viewer* for advertising; not necessarily more total, but that they'll pay a higher amount for opportunities to broadcast in places that generate common knowledge. Rather than buy 10 opportunities to broadcast to 2 million people on various channels, they'll pay a premium for 20 million people to view their ad during the superbowl, to create stronger common knowledge.\n\nAcademic Research\n-----------------\n\nThe central place where common knowledge is generated in science is in journals. These are where researchers can discover the new insights of the field, and build off them. Conferences can also help in this regard.\n\nA more interesting case is textbooks (I borrow this example from Oliver Habryka). There was once a time in the history of physics where the basics of quantum mechanics were known, and yet to study them required reading the right journal articles, in the right order. When you went to a convention of physicists, you likely had to explain many of the basics of the field *before* you could express your new idea.\n\nThen, some people decided to aggregate it into textbooks, which were then all taught to the undergraduates of the next generation, until the point where you could walk into the room and start using all the jargon and *trust that everyone knew what you meant*. Having common knowledge of the basics of a field is necessary for a field to make progress - to make the 201 the 101, and then build new insights on top.\n\nIn my life, even if 90% of the people around have the idea, when I’m not confident that 100% do then I often explain the basic idea for everyone. This often costs a lot of time - for example, after you read this post, I’ll be able to say to you a sentence like ‘the undergrad textbook system is a mechanism to create the common knowledge that allows the field as a whole to jump to the new Nash equilibrium of using advanced concepts’.\n\nParagraphs can be reduced to sentences, and you can get even more powerful returns with more abstract ideas - in mathematics, pages of symbols can be turned into a couple of lines (with the right abstractions e.g. calculus, linear algebra, probability theory, etc).\n\nStartups\n--------\n\nA startup is a very small group of people building detailed models of a product. They’re able to create a lot of common knowledge due to their small size. However, one of the reasons why they need to put a lot of thought into the long-term of the company, is because they will *lose* this common knowledge producing mechanism as they scale, and the only things they’ll be able to coordinate on are the things they already learned together.\n\nThe fact that they’re able to build common knowledge when they’re small is why they’re able to make so much more progress than big companies, and is also why big companies that innovate tend to compartmentalise their teams into small groups. As the company grows, there are far fewer things that can be retained as common knowledge amongst the employees. You can have intensive on-boarding processes for the first 20 hires, but it really doesn’t scale to 100 employees.\n\nHere are three things that can sustain at very large scales:\n\n**Name:** Y Combinator says that the name of your company should tell people what you do - cf. AirBnb, InstaCart, DoorDash, OpenAI, Lyft, etc. Contrast with companies like Palantir, where even I don’t know exactly what they work on day-to-day, and I’ve got friends who work there.\n\n**Mission:** It is possible to predict the output of an organisation very well by what their mission statement concretely communicates. For example, the company SpaceX has their mission statement at the top of all hiring documents (cf. the application forms to be a [rocket scientist](http://www.spacex.com/careers/position/214332), [business analyst](http://www.spacex.com/careers/position/214810), or [barista](http://www.spacex.com/careers/position/212463)).\n\n**Values:** Affects hiring and decision-making long into the future. YC specifically says to pick 4-8 core values, have a story associated with each value, and tell each story *every* day (e.g. in meetings). That may seem like way too much, but in fact that’s how much it can take to make the values common knowledge (especially as your company scales).\n\nAt what cost?\n-------------\n\nA standard response to coordination failures is one of *exasperation* \\- a feeling that we *should* be able to solve this if only we *tried*.\n\nImagine you’re trying to coordinate you and a few friends to move some furniture, and they keep getting in each other’s way. You might shout “Hey guys! Look, Pete and Laurie have to move the couch first, then John and Pauline can move the table!” And then things just start working. Or even just between two of you - when a friend is late for skype calls because she messes up her calendar app, you might express irritation, and she might try extra hard to fix the problem.\n\nWe also feel this when we look at society at large, for example when we look at coordination failures in politics. *Why does everyone continue voting for silly-no-good politicians? Why can’t we all just vote for someone sane?!*\n\nIn the book *Inadequate Equilibria* by Eliezer Yudkowsky, the character *Simplicio* represents this feeling. Here is the character discussing a (real) coordination failure in the US healthcare system that causes a few dozen newborn children to die every year:\n\n> **simplicio:** The first thing you have to understand, Visitor, is that the folk in this world are hypocrites, cowards, psychopaths, and sheep.\n\n> I mean, I certainly care about the the lives of newborn children. Hearing about their plight certainly makes me want to do something about it. When I see the problem continuing in spite of that, I can only conclude that other people don’t feel the level of moral indignation that I feel when staring at a heap of dead babies.\n\n> \\[...\\]\n\n> Regardless, I’m not seeing what the grand obstacle is to people solving these problems by, you know, coordinating. If people would just act in unity, so much could be done!\n\n> I feel like you’re placing too much blame on system-level issues, Cecie, when the simpler hypothesis is just that the people in the system are terrible: bad at thinking, bad at caring, bad at coordinating. You claim to be a “cynic,” but your whole world-view sounds rose-tinted to me.\n\nOne of the final points to deeply understand about common knowledge in society, is how *costly* it is to create at scale.\n\nBig companies get to pick only a few sentences to become common knowledge. To have a community rally around a more complex set of values and ideals (i.e. a significant function of religion) each and every member of that community must give up half of each Sunday, to repeat ideas *they already know* over and over - nothing new, just with the goal of creating common knowledge.\n\nThere used to be news programmes everybody in a country would tune in for. Notice how the New York Times used to be something people would read once per week or once per *day*, and discuss it with friends, even though most of the info has no direct effect on their lives.\n\nOur intuitions were developed for tribes of size 150 or less (cf. Dunbar’s number) and as such, our intuitions around coordination are often terribly off. Simplicio is someone who has not noticed the cost of creating common knowledge at scale. He believes that society could easily vote for good politicians *if only we coordinated*, and because we don’t he infers we *must* be stupid and/or evil.\n\nThe feeling of *indignation* at people for failing to coordinate can be thought of as creating an incentive to solve the coordination problem. I’m letting my skype partner know that I will punish them if they fail again. But today, this feeling toward people for failing to coordinate is almost always misguided.\n\nThink of it this way: many small coordination problems are sufficiently small that you’ll solve them quickly; many coordination problems are sufficiently big that you have no chance of solving them via normal means, and you will feel indignation every time you notice them (e.g. think politics/twitter). Basically, when you feel like being indignant in the modern world, 99% of the time it’s wasted motion.\n\nSimplicio’s intuitions are a great fit for a hunter-gatherer tribe. When he gets indignant, it would be proportional to the problem, the problem would get solved, and everyone would be happy. At a later point in the book Simplicio calls for political revolution - the sort of mechanism that works if you’re able to get everyone to gather in a single place.\n\nThe solution to coordination problems at scale is much harder, and requires thinking about incentives structures and information flows rather than emotions directed at individuals in your social environment. Or in other words, building a civilization.\n\n> **visitor:** Indeed. Moving from bad equilibria to better equilibria is the whole point of having a civilization in the first place.\n\n\\- Another character in [Inadequate Equilibria](https://www.lesserwrong.com/sequences/oLGCcbnvabyibnG9d), by Eliezer Yudkowsky\n\nSo, what's common knowledge *for*?\n==================================\n\nSummary of this post:\n\n1.  A coordination problem is when everyone is taking some action A, and we’d rather all be taking action B, but it’s bad if we don’t all move to B at the same time.\n2.  Common knowledge is the name for the epistemic state we’re collectively in, when we know we can all start choosing action B - and trust everyone else to do the same.\n3.  We’re intuitively very good at navigating such problems when we’re in small groups (size < 150).\n4.  We’re intuitively very bad at navigating such problems in the modern world, and require the building of new, microeconomic intuitions in order to build a successful society.\n\nThere is a great deal more subtlety to how common knowledge gets built and propagates. This post has given but a glimpse through the lens of game-theory, and hopefully you now see the light that this lens sheds on a great variety of phenomena.\n\n* * *\n\n*Links to explore more on this subject:*\n\n*   *Moloch’s Toolbox (Inadequate Equilibria, Ch 3)* ([link](https://www.lesserwrong.com/sequences/oLGCcbnvabyibnG9d))\n    *   *A guide to the ways our current institutions fail to coordinate. Largely applying standard microeconomics, and a great post to read after this one.*\n*   *Meditations on Moloch* ([link](https://www.lesserwrong.com/s/xmDeR64CivZiTAcLx/p/TxcRbCYHaeL59aY7E))\n    *   *An original idea about coordination failures, which the above book chapter formalised. It's a great post, and it's good to follow the intellectual heritage of ideas.*\n*   *Rational Ritual: Culture, Coordination and Common Knowledge* ([link](https://www.amazon.com/Rational-Ritual-Culture-Coordination-Knowledge/dp/0691114714))\n    *   *Solid book with lots of detail.*\n*   *Scott Aaronson on Common Knowledge and Aumann's Agreement Theorem* ([link](https://www.scottaaronson.com/blog/?p=2410))\n    *   *This post caused me to spend a bunch more time thinking about these topics. I found the explanations personally to be fairly abstract, which inspired me to write this post.*\n*   *Scott Alexander’s sequence on Game Theory* ([link](https://www.lesserwrong.com/posts/QxZs5Za4qXBegXCgu/introduction-to-game-theory-sequence-guide))\n    *   *After writing this post, I found Scott Alexander had also written about some of the examples (especially the dictatorship one) in detail 7 years ago* [(link)](http://squid314.livejournal.com/2011/02/01/).\n*   *Andrew Critch on 'Unrolling Social Metacognition: Three levels of meta are not enough'* ([link](https://www.lesswrong.com/posts/K4eDzqS2rbcBDsCLZ/unrolling-social-metacognition-three-levels-of-meta-are-not))\n    *   *This is a great post going into the details of how my modelling of you modelling me modelling you... works in practice. Highly recommended if the definition of common knowledge presented above seemed confusing.*\n\n* * *\n\n***Thanks** to Raymond Arnold, Jacob Lagerros and Oliver Habryka for extensive feedback and comments, and to Hadrien Pouget for proofreading an early draft. A further special mention to Raymond for pointing out this term ought to be a standard piece of* [expert jargon](https://www.lesserwrong.com/posts/DcRFTx62sTTRQo3Jw/common-vs-expert-jargon) *in this community, and suggesting I write this post*"
    },
    "voteCount": 96,
    "forceInclude": true
  },
  {
    "_id": "yLLkWMDbC9ZNKbjDG",
    "url": null,
    "title": "Slack",
    "slug": "slack",
    "author": "Zvi",
    "question": false,
    "tags": [
      {
        "name": "Slack"
      },
      {
        "name": "Practical"
      }
    ],
    "tableOfContents": {
      "sections": [
        {
          "title": "Definition: Slack. The absence of binding constraints on behavior.",
          "anchor": "Definition__Slack__The_absence_of_binding_constraints_on_behavior_",
          "level": 1
        },
        {
          "title": "Related Slackness",
          "anchor": "Related_Slackness",
          "level": 1
        },
        {
          "title": "Out to Get You and the Attack on Slack",
          "anchor": "Out_to_Get_You_and_the_Attack_on_Slack",
          "level": 1
        },
        {
          "title": "You Can Afford It",
          "anchor": "You_Can_Afford_It",
          "level": 1
        },
        {
          "title": "The Slackless Like of Maya Millennial",
          "anchor": "The_Slackless_Like_of_Maya_Millennial",
          "level": 1
        },
        {
          "title": "“Give Me Slack or Kill Me” – J.R. “Bob” Dobbs",
          "anchor": "_Give_Me_Slack_or_Kill_Me____J_R___Bob__Dobbs",
          "level": 1
        },
        {
          "title": "A Final Note",
          "anchor": "A_Final_Note",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        },
        {
          "anchor": "comments",
          "level": 0,
          "title": "71 comments"
        }
      ],
      "headingsCount": 9
    },
    "contents": {
      "markdown": "Epistemic Status: Reference post. Strong beliefs strongly held after much thought, but hard to explain well. Intentionally abstract.\n\nDisambiguation: This does not refer to any physical good, app or piece of software.\n\nFurther Research (book, recommended but not at all required, take seriously but not literally): [The Book of the Subgenius](https://smile.amazon.com/gp/product/B002XQAAS6/ref=s9u_simh_gw_i1?ie=UTF8&fpl=fresh&pd_rd_i=B002XQAAS6&pd_rd_r=FZCKWAV9EQDXHX6PZN61&pd_rd_w=ByYg5&pd_rd_wg=z5czc&pf_rd_m=ATVPDKIKX0DER&pf_rd_s=&pf_rd_r=8HFTESCZEKP6GJ3K6MV0&pf_rd_t=36701&pf_rd_p=1cf9d009-399c-49e1-901a-7b8786e59436&pf_rd_i=desktop)\n\nRelated (from sam\\[ \\]zdat, recommended but not required, take seriously and also literally, entire very long series also recommended): [The Uruk Machine](https://samzdat.com/2017/08/28/the-uruk-machine/)\n\nFurther Reading (book): [Scarcity: Why Having Too Little Means So Much](https://smile.amazon.com/Scarcity-Having-Little-Means-Much-ebook/dp/B00BMKOO6S/ref=sr_1_1?s=books&ie=UTF8&qid=1506174951&sr=1-1&keywords=scarcity)\n\nPreviously here (not required): [Play in Hard Mode](https://thezvi.wordpress.com/2017/08/26/play-in-hard-mode/), [Play in Easy Mode](https://thezvi.wordpress.com/2017/08/26/play-in-easy-mode/), [Out to Get You](https://thezvi.wordpress.com/2017/09/23/out-to-get-you/)\n\nLeads to (I’ve been scooped! Somewhat…): [Sabbath Hard and Go Home](http://benjaminrosshoffman.com/sabbath-hard-and-go-home)\n\nAn illustrative little game: [Carpe Diem: The Problem of Scarcity and Abundance](https://thezvi.wordpress.com/2015/05/01/carpe-diem-the-problem-of-scarcity-and-abundance/)\n\nSlack is hard to precisely define, but I think this comes close:\n\nDefinition: Slack. The absence of binding constraints on behavior.\n------------------------------------------------------------------\n\nPoor is the person without Slack. Lack of Slack compounds and traps.\n\nSlack means margin for error. You can _relax_.\n\nSlack allows pursuing opportunities. You can _explore_. You can _trade_.\n\nSlack prevents desperation. You can _avoid bad trades_ and _wait for better spots_. You can _be efficient_.\n\nSlack permits planning for the long term. You can _invest_.\n\nSlack enables doing things for your own amusement. You can _play games_. You can _have fun_.\n\nSlack enables doing the right thing. Stand by your friends. Reward the worthy. Punish the wicked. You can _have a code_.\n\nSlack presents things as they are without concern for how things look or what others think. You can _be honest_.\n\nYou can do some of these things, and choose not to do others. Because you don’t have to.\n\nOnly with slack can one be a _righteous dude_.\n\nSlack is life.\n\nRelated Slackness\n-----------------\n\nSlack in project management is the time a task can be delayed without causing a delay to either subsequent tasks or project completion time. The amount of time before a constraint binds.\n\nSlack the app was likely named in reference to a promise of Slack in the project sense.\n\nSlacks as trousers are pants that are actual pants, but do not bind or constrain.\n\nSlackness refers to vulgarity in West Indian culture, behavior and music. It also refers to a subgenre of dancehall music with straightforward sexual lyrics. Again, slackness refers to the absence of a binding constraint. In this case, common decency or politeness.\n\nA slacker is one who has a lazy work ethic or otherwise does not exert maximum effort. They _slack off_. They refuse to be bound by what others view as hard constraints.\n\n**Out to Get You and the Attack on Slack**\n------------------------------------------\n\nMany things in this world are [Out to Get You](https://thezvi.wordpress.com/2017/09/23/out-to-get-you/). Often they are [Out to Get You](https://thezvi.wordpress.com/2017/09/23/out-to-get-you/) for a lot, usually but not always your time, attention and money.\n\nIf you Get Got for compact amounts too often, it will add up and the constraints will bind.\n\nIf you Get Got _even once_ for a non-compact amount, the cost expands until the you have no Slack left. The constraints bind you.\n\nYou might spend every spare minute and/or dollar on politics, advocacy or charity. You might think of every dollar as a fraction of a third-world life saved. Racing to find a cure for your daughter’s cancer, [you already work around the clock](https://www.youtube.com/watch?v=UYBx7yxEME4). You could have an all-consuming job or be a soldier marching off to war. It could be a quest for revenge, for glory, for love. Or you might spend every spare minute mindlessly [checking Facebook](https://thezvi.wordpress.com/2017/04/22/against-facebook-comparison-to-alternatives-and-call-to-action/) or obsessed with your fantasy football league.\n\nYou cannot relax. Your life is not your own.\n\nIt might even be the right choice! Especially for brief periods. When about to be run over by a truck or evicted from your house, Slack is a luxury you cannot afford. Extraordinary times call for [extraordinary effort](http://lesswrong.com/lw/uo/make_an_extraordinary_effort/).\n\nMost times are ordinary. Make an ordinary effort.\n\n**You Can Afford It**\n---------------------\n\nNo, you can’t. This is the most famous attack on Slack. Few words make me angrier.\n\nThe person who says “You Can Afford It” is saying to ignore constraints that do not bind you. If you do, all constraints soon bind you.\n\nThose who do not value Slack soon lose it. Slack matters. Fight to keep yours!\n\nAsk not whether you can afford it. Ask if it is Worth It.\n\nUnless you can’t afford it. Affordability is invaluable _negative_ selection. Never _positive_ selection.\n\nThe You Can Afford It tax on Slack quickly approaches 100% if unchecked.\n\nIf those with extra resources are asked to share the whole surplus, all are poor or hide their wealth. Wealth is a burden and makes you a target. Those visibly flush rush to spend their bounty.\n\nWhere those with free time are given extra work, all are busy or look busy. Those with copious free time seek out relatively painless time sinks they can point to.\n\nWhen looking happy means you deal with everything unpleasant, no one looks happy for long.\n\n**The Slackless Like of Maya Millennial**\n-----------------------------------------\n\nThings are bad enough when those with Slack are expected to sacrifice for others. Things are much worse when the presence of Slack is viewed as a defection.\n\nAn example of this effect is [Maya Millennial](https://thezvi.wordpress.com/2017/09/05/expanding-premium-mediocrity/) (of [The Premium Mediocre Life of Maya Millennial](https://www.ribbonfarm.com/2017/08/17/the-premium-mediocre-life-of-maya-millennial/)). She has no Slack.\n\nConstraints bind her every action. Her job in life is putting up a front of the person she wants to show people that she wants to be. If her constraints noticeably failed to bind the illusion would fail.\n\nEvery action is being watched. If no one is around to watch her, the job falls to her. She must post all to Facebook, to Snapchat, to Instagram. Each action and choice signals who she is and her loyalty to the system. Not doing that this time could mean missing her one chance to make it big.\n\nMaya never has free time. There is signaling to do! At a minimum, she must spend such time on alert and on her phone lest she miss something.\n\nMaya never has spare cash. All must be spent to advance and fit her profile.\n\nMaya lacks free speech, free association, free taste and free thought. All must serve.\n\nMaya is in a world where _she must signal she has no Slack_. Slack means insufficient dedication and loyalty. Slack cannot be trusted. Slack now means slack later, which means failure. Future failure means no opportunity.\n\nThis is more common than one might think.\n\n**[“Give Me Slack or Kill Me” – J.R. “Bob” Dobbs](https://en.wikiquote.org/wiki/J._R._%22Bob%22_Dobbs)**\n--------------------------------------------------------------------------------------------------------\n\nThe aim of this post was to introduce Slack and give an intuitive picture of its importance.\n\nThe short-term practical takeaways are:\n\nMake sure that under normal conditions _you_ have Slack. Value it. Guard it. Spend it only when Worth It. If you lose it, fight to get it back. This provides motivation for fighting things [Out To Get You](https://thezvi.wordpress.com/2017/09/23/out-to-get-you/), lest you let them eat your Slack.\n\nMake sure to run a diagnostic test every so often to make sure you’re not running dangerously low, and to engineer your situation to force yourself to have Slack. I recommend [Sabbath Hard and Go Home](http://benjaminrosshoffman.com/sabbath-hard-and-go-home) with my take to follow soon.\n\nAlso respect the Slack of others. Help them value and guard it. Do not spend it lightly.\n\n**A Final Note**\n----------------\n\nI kept this short rather than add detailed justifications. Hopefully the logic is intuitive and builds on what came before. I hope to expand on the details and models later. For a very good book-length explanation of why lacking Slack is awful, see [Scarcity: Why Having Too Little Means So Much](https://smile.amazon.com/Scarcity-Having-Little-Means-Much-ebook/dp/B00BMKOO6S/ref=sr_1_1?s=books&ie=UTF8&qid=1506174951&sr=1-1&keywords=scarcity)."
    },
    "voteCount": 146,
    "forceInclude": true
  },
  {
    "_id": "gNodQGNoPDjztasbh",
    "url": null,
    "title": "Lies, Damn Lies, and Fabricated Options",
    "slug": "lies-damn-lies-and-fabricated-options",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "Rationality"
      },
      {
        "name": "Heuristics & Biases"
      }
    ],
    "tableOfContents": {
      "sections": [
        {
          "title": "Background 1: Gyroscopes",
          "anchor": "Background_1__Gyroscopes",
          "level": 1
        },
        {
          "title": "Background 2: H2O and XYZ",
          "anchor": "Background_2__H2O_and_XYZ",
          "level": 1
        },
        {
          "title": "feels coherent.",
          "anchor": "feels_coherent_",
          "level": 2
        },
        {
          "title": "Fabricated Options",
          "anchor": "Fabricated_Options",
          "level": 1
        },
        {
          "title": "Example 1: Price gouging",
          "anchor": "Example_1__Price_gouging",
          "level": 1
        },
        {
          "title": "Example 2: An orphan, or an abortion?",
          "anchor": "Example_2__An_orphan__or_an_abortion_",
          "level": 1
        },
        {
          "title": "Example 3: Drowning",
          "anchor": "Example_3__Drowning",
          "level": 1
        },
        {
          "title": "Example 4: Block lists",
          "anchor": "Example_4__Block_lists",
          "level": 1
        },
        {
          "title": "Example 5: Parental disapproval",
          "anchor": "Example_5__Parental_disapproval",
          "level": 1
        },
        {
          "title": "Example 6: 2020",
          "anchor": "Example_6__2020",
          "level": 1
        },
        {
          "title": "Conclusion",
          "anchor": "Conclusion",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        },
        {
          "anchor": "comments",
          "level": 0,
          "title": "119 comments"
        }
      ],
      "headingsCount": 13
    },
    "contents": {
      "markdown": "This is an essay about one of those \"once you see it, you will see it *everywhere\"* phenomena.  It is a psychological and interpersonal dynamic roughly as common, and almost as destructive, as motte-and-bailey, and at least in my own personal experience it's been quite valuable to have it reified, so that I can quickly recognize the commonality between what I had previously thought of as completely unrelated situations.\n\nThe original quote referenced in the title is \"There are three kinds of lies: lies, damned lies, and statistics.\"\n\n* * *\n\n**Background 1: Gyroscopes**\n----------------------------\n\nGyroscopes are weird.\n\nExcept they're not.  They're quite normal and mundane and straightforward.  The weirdness of gyroscopes is a map-territory confusion—gyroscopes *seem* weird because my map is poorly made, and predicts that they will do something other than their normal, mundane, straightforward thing.\n\nIn large part, this is because I don't have the *consequences of physical law* engraved deeply enough into my soul that they make intuitive sense.\n\nI can *imagine* a world that looks exactly like the world around me, in every way, *except* that in this imagined world, gyroscopes don't have any of their strange black-magic properties.  It feels coherent to me.  It feels like a world that could possibly exist.\n\n\"Everything's the same, except gyroscopes do nothing special.\"  Sure, why not.\n\nBut in fact, this world is deeply, deeply incoherent.  It is Not Possible with capital letters. And a physicist with sufficiently sharp intuitions would know this—would be able to *see* the implications of a world where gyroscopes \"don't do anything weird,\" and tell me all of the ways in which reality falls apart.\n\nThe *seeming coherence* of the imaginary world where gyroscopes don't balance and don't precess and don't resist certain kinds of motion is a product of *my own ignorance*, and of the looseness with which I am tracking how different facts fit together, and what the consequences of those facts are.  It's like a toddler thinking that they can eat their slice of cake, and still have that very same slice of cake available to eat again the next morning.\n\n* * *\n\n**Background 2: H2O and XYZ**\n-----------------------------\n\nIn the book *Labyrinths of Reason,* author William Poundstone delves into various thought experiments (like Searle's Chinese Room) to see whether they're actually coherent or not.\n\nIn one such exploration, he discusses the idea of a Twin Earth, on the opposite side of the sun, exactly like Earth in every way except that it doesn't have water.  Instead, it has a chemical, labeled XYZ, which behaves like water and occupies water's place in biology and chemistry, but is unambiguously distinct.\n\nOnce again, this is the sort of thing humans are *capable of imagining.*  I can nod along and say \"sure, a liquid that behaves just like water, but isn't.\"\n\nBut a chemist, intimately familiar with the structure and behavior of molecules and with the properties of the elements and their isotopes, would be throwing up red flags.\n\n\"Just like water,\" they might say, and I would nod.\n\n\"Liquid, and transparent, with a density of 997 kilograms per meter cubed.\"\n\n\"Sure,\" I would reply.\n\n\"Which freezes and melts at exactly 0º Celsius, and which boils and condenses at exactly 100º Celsius.\"\n\n\"Yyyyeahhhh,\" I would say, uneasiness settling in.\n\n\"Which makes up roughly 70% of the mass of the bodies of the humans of Twin Earth, and which is a solvent for hydrophilic substances, but not hydrophobic ones, and which can hold ions and polar substances in solution.\"\n\n\"Um.\"\n\n* * *\n\nThe more we drill down into what we mean by *behaves exactly like water,* the more it starts to become clear that there just *isn't* a possible substance which behaves exactly like water, but isn't.  There are only so many configurations of electrons and protons and neutrons (especially while remaining small enough to mimic water's molarity, and to play water's role in various chemical interactions).\n\nOnce again, our ability to imagine \"a substance that behaves exactly like water, but isn't\" is a product of *our own confusion.*  Of the fuzziness of our concepts, the fast-and-loose-ness of our reasoning, our willingness to overlook a host of details which are actually *crucially relevant* to the question at hand.\n\n(Tickling at the back of my mind is the axiom \"your strength as a rationalist is your ability to be more confused by fiction than by reality.\"  The thing I'm gesturing toward seems to be a corollary of sorts.)\n\nOf key importance:\n\n**Until we actually zero in on the incoherence, the imagined thing** ***feels coherent.*** **It seems every bit as potentially-real as actually-potentially-real options.**\n\nWe have no internal feeling that *warns* us that it's a fabrication masquerading as a possibility.  Our brains do not tell us when they're playing fast and loose.\n\n* * *\n\n**Fabricated Options**\n----------------------\n\nClaim: When people disagree with one another, or are struggling with difficult decisions, they frequently include, among their perceived options, at least one option which is fake-in-the-way-that-XYZ-is-fake.  An option that *isn't actually an option at all*, but which is a product of incoherent thinking.\n\nThis is what this essay seeks to point out, and to give you taste and triggers for.  I would like to establish *fabricated options* as a category in your mind, so that you are more likely to notice them, and less likely to be taken in by them.\n\n* * *\n\n**Example 1: Price gouging**\n----------------------------\n\nThis example is one that many of my readers will already be familiar with; it's the kind of topic that gets covered in Econ 101.  I'm not trying to teach it to you from scratch so much as get you to see it as an *instance* of the class of fabricated options, so that you can port your intuitions about price gouging over to other situations.\n\nIn short: during natural disasters or other market disruptions, it often becomes difficult to deliver things like food, water, clothing, toilet paper, medical supplies, gasoline, transportation, etc., to the people who need them.\n\nSometimes there simply isn't enough supply, and sometimes there's plenty of supply but the logistics become complicated (because, for instance, the act of physically delivering things becomes significantly more dangerous).\n\nIn those situations, the *price* of the needed items often goes through the roof.  Toilet paper selling for $100 a roll, Ubers costing $500 for a ten-mile drive, things like that.\n\nPeople watching from the outside see this, and feel horror and sympathy and dismay, and often propose (and sometimes successfully enact) legal barriers to *price gouging.*  They make it illegal to raise the price on goods and services, or put a ceiling on *how much* it can be raised.\n\nMost such interventions do not produce the desired effect.\n\nThe desired effect is that people will just continue to deliver and sell items for a reasonable price, as if nothing has happened.\n\nBut that option *was never really on the table.*  In the middle of a wildfire, or a massive flood, or raging citywide riots, or global supply chain disruption, it simply isn't possible. The *actual price* of the goods and services, in the sense of \"what does it take to provide them?\" has gone up, and the market price will necessarily follow.\n\nIf you *successfully* prevent people from selling toilet paper at $100 a roll (rather than simply driving the transactions underground into a black market), the actual effect is usually that there's no one selling toilet paper at all.\n\n* * *\n\nThe critical insight for this essay is that the thinking of the lawmakers is *confused.  *It is insufficiently detailed; insufficiently in touch with the reality of the situation.\n\nThe lawmakers seem to think that the options are:\n\n*   \\[Do nothing\\], and bad people will continue doing a bad thing, and ludicrously jacking up the price on critically necessary items.\n*   \\[Pass laws forbidding/punishing sharp price increases in times of trouble\\], and the bad people will just *not* do the bad thing, and the critically necessary items will be available for reasonable prices.\n\n... and in that world, given that menu of options, *of course* we should choose the second one!\n\nBut in reality, that is not the menu.  The second option is *fabricated.*  The story in which \\[passing that law\\] results in goods being available at normalish prices is an incoherent fairy tale.  It falls apart as soon as you start digging into the details, and realize that there are forces at work which cannot be dispersed by the stroke of a lawmaker's pen, just as there are physical laws which prevent non-weird gyroscopes and non-water XYZ.\n\n(No matter how easy it is to imagine these things, when we gloss over the relevant details.)\n\nIn fact, the true options in most such situations are:\n\n*   \\[Do nothing\\], and people will be able to get access to the critically necessary items, but it will be much harder and more expensive because there is low supply and high logistical difficulty.\n*   \\[Pass laws forbidding/punishing sharp price increases in times of trouble\\], and people won't be able to get anything at all, because someone erected an artificial barrier to trade.\n\nAnd given *that* menu of options, the first is obviously (usually) better.\n\n* * *\n\n**Caveat 1:** this could be misinterpreted (both in the specific case of price gouging and in the more general case of fabricated options) as encouraging a sort of throw-up-your-hands, if-we-can't-solve-everything-we-shouldn't-bother-to-try-anything helplessness.\n\nThat's not the point.  There are often ways to *break* the tradeoff dynamics at play, in any given situation.  There are often third paths, and ways to cheat, and ways to optimize within the broken system to minimize negative effects and maximize positive outcomes.\n\nThere are, in other words, *some versions* of anti-price-gouging laws that do marginal good and avoid the outright stupid failure modes.\n\nBut in order to have those intelligent effects, you first have to see and account for the relevant constraints and tradeoffs, and what I am attempting to point at with the above example is the common human tendency to *not do so*.  To simply live in the fantasy world of what we could \"just\" accomplish, if people would \"just\" do \\[simple-sounding but not-actually-possible thing\\].  \n\n*Most* anti-price-gouging proposals are naive in exactly the way described above; this is not meant to imply that non-naive proposals don't exist.  They do.  I'm just focusing on the central tendency and ignoring the unusually competent minority.  \n \n\n**Caveat 2:** in this example and many others, the fabricated option is less a made-up action and more a made-up story about the *consequences* of that action.  In both versions of the above dilemma, the listed actions were the same.  The difference was the valence assigned to the \"pass laws\" option, and the story emerging from it.\n\nThis is not always the case.  Sometimes people think the options are A or B, and they are in fact B or C, and sometimes people think the options are A or B *and they are,* but their imagination distorts the impact of option A into something utterly unrealistic.\n\nFor the sake of thinking about the *category* \"fabricated options,\" this distinction is not especially relevant, and will mostly be ignored in the rest of the essay.  The important thing to note is that in either case, the fabricated option has *inflated relative appeal.*  \n \n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/1aa84afd73dd8ec794cfab6c82eb44d908ccc57b784fe3d0.png)\n\nEither it's a genuinely available action A wrapped up in an incoherent and unrealistic story that makes it sound better than the unappealing B, or it's an entirely made-up option A which makes the actual best option B look bad in comparison (causing us to fail to shoot for B over an even worse default C).\n\nIn both cases, the *result in practice* is that option B, which is usually sort of dour and uninspiring and contains unpleasant costs or tradeoffs, gets something like *disproportionately downvoted*.  Downvoted relative to an impossible standard—treated as worse than it ought to be treated, given constraints.\n\nIt's a common assumption among both rationalists and the population at large that people tend to *flinch away* from things which are unpleasant to think about.  However, people rarely take the time to spell out just what \"flinching\" means, in practice, or just what triggers it.\n\nThe fabrication of options is, I claim, one example of flinching.  It's one of the things we *do,* as humans, when we feel ourselves about to be forced into choosing an uncomfortable path.  There's a sense of \"surely not\" that sends our minds in any other available direction, and if we're not *careful*—if we do not *actively hold ourselves* to a certain kind of stodgy actuarial insistence-on-clarity-and-coherence—we'll more than likely latch onto a nearby pleasant fiction without ever noticing that it doesn't stand up to scrutiny.\n\n\"If only they would just \\[calm down/listen/take a deep breath/forgive me/let it go/have a little perspective/not be so jealous/not be so irrational/think things through more carefully/realize how much I love them/hang on just a little bit longer\\], everything would be fine.\"\n\nPleasant fictions always outnumber pleasant truths, after all.\n\n* * *\n\n**Example 2: An orphan, or an abortion?**\n-----------------------------------------\n\nThis is the question posed by John Irving's excellent novel *The Cider-House Rules.  *The point of the question, within the novel, is to *break* the false dichotomy wherein the choices are framed as \"a living baby or a dead/murdered one?\"\n\nA living baby:  [🙂](https://emojipedia.org/slightly-smiling-face/)\n\nA dead one: 🙁, or perhaps [😡](https://emojipedia.org/pouting-face/)\n\nBut \"living baby\" in the sense often pushed for by pro-life advocates is something of a motte-and-bailey.  It's a naive, fabricated option.  It hand-waves away all of the inconvenient and uncomfortable detail, in exactly the same fashion as \"gyroscopes, but not weird.\"\n\nJohn Irving's novel doesn't take a stand on which is better—rather, it tries to force the reader to *consider the decision at all,* instead of getting confused by alluring falsehoods. The footing of the two sides, in the novel, is less uneven-by-design, which seems to me like a step in the right direction.\n\n* * *\n\n**Example 3: Drowning**\n-----------------------\n\nI have a longtime friend who I'll refer to here as Taylor, who's got a longtime romantic partner who I'll refer to here as Kelly.\n\nKelly struggles with various mental health issues.  They genuinely do their best, but as is so often the case, their best is not really \"enough.\"  They spend the better part of each year depressed and mildly delusional, with frequent dangerous swerves into suicidality.\n\nAs a side effect of these issues, Kelly—who is *at their core* an excellent partner for Taylor—also puts Taylor through the wringer.  Kelly has destroyed multiple of Taylor's possessions, multiple times.  Kelly has screamed and yelled at Taylor, multiple times. Over and over, Taylor has asked Kelly what would help, what they can do, how they could change their own behavior to be a better partner for Kelly—and over and over, granting Kelly's explicit requests has resulted in Taylor being yelled at, punished, told to go away.\n\nThis has been rough.\n\nTaylor is *already* the sort of person who doesn't give up on people—the sort of person who would willingly sacrifice themselves for a friend or a family member, the sort of person who will go to genuinely extreme lengths to save a fellow human in trouble.\n\nAnd on top of that, Taylor genuinely loves Kelly, and has plenty of evidence that—when things are okay—Kelly genuinely loves Taylor.\n\nBut for years now, the situation has been spiraling, and Taylor has been getting more and more exhausted and demoralized, and it has become increasingly clear that neither Taylor's direct efforts, nor any of the other resources they've funneled Kelly's way (therapists, medication, financial stability, freedom of movement), are going to be sufficient.  It no longer seems reasonable to *expect things to get better.*\n\nTaylor and I have talked about the situation a lot, and one of the metaphors that has come up more and more often is that of a drowning person out in rough waters.\n\nFrom Taylor's point of view, saving Kelly is worth it.  Saving Kelly is worth it *even if it means Taylor goes under.*  From Taylor's point of view, the options have always been \"help save Kelly, or watch Kelly drown.\"\n\nBut this frame is broken.  At this point, it's clear that \"help save Kelly\" is not a real option.  It's a fabrication, conjured up because it is *deeply uncomfortable* to face the real choice, which is \"let Kelly drown, or drown *with* them.\"\n\n(Alternately, and a little less harshly: \"let Kelly figure out how to swim on their own, or keep trying to help them and drown, yourself, without actually having helped them float.\")\n\n* * *\n\n**Example 4: Block lists**\n--------------------------\n\nI've previously had disagreements with a few people in various bubbles over block lists, and coordination, and what the defaults should be, and where various obligations lie.\n\nIn my (probably straw) characterization of the other side, they're *fabricating options.* They hold a position that (probably deserves steelmanning, but given my current state of understanding) looks like:\n\n*   Option A, everyone keeps the lines of communication open, and people don't block each other except under extraordinary circumstances (which will tend to be legible and obvious and which basically everyone will agree upon), and that way everyone can see all of the important discussion and there aren't confusing non-overlapping bubbles of fragmented common knowledge.\n*   Option B, some people defect on the project of maintaining a clear and open commons, and block people, and make everything worse for everybody.\n\nOption A is [🙂](https://emojipedia.org/slightly-smiling-face/)\n\nOption B is clearly 🙁\n\nIn my trying-to-look-at-the-actual-tradeoffs perspective, though—\n\n(Which is not meant to imply that the other people aren't also trying, it just seems to me like if they are trying, they're not quite managing to do so.)\n\n—it seems to me that the actual options are:\n\n*   Option B, which is very much just as 🙁 as they think it is, in which the world is imperfect and communication and coordination are tricky and costly and often go sideways, and some people need to block other people for all sorts of valid and self-protective reasons, and yep, this makes it harder to coordinate and establish common knowledge but it's the *actual best we can do—*\n*   or Option A, which is 😱, in which the self-protective blocking option is outlawed or disincentivized-on-the-margin, and people are either punished when they do it anyway (analogous to people being fined for selling toilet paper at inflated prices) or somehow compelled *not* to, in which they are either constantly exposed to triggers and to attacks from their enemies and abusers and all sorts of other things that are horrible for their mental health, or they just go dark and disappear from the conversation altogether.\n\nThe version of option A where \\[everyone just manages to be in the same room all the time and it's just never disastrously problematic\\] is obviously better than either of the two options described above.\n\nBut it's a substance identical to water that isn't water.  It's not actually on the table.\n\n* * *\n\n**Example 5: Parental disapproval**\n-----------------------------------\n\nYour kid wants to hang out with another kid who you're pretty sure is a bad influence.\n\nYour kid wants to quit their piano lessons, sinking their previous three years of effort.\n\nYour kid seems like they're about to start having sex, or using drugs, or playing Magic: the Gathering.\n\nYour kid doesn't want to go to the family reunion.\n\nYour kid doesn't want to eat that.\n\nI see parents' hopes and expectations come up against the reality of their kids' preferences all the time, and I always have this sucking-in-a-breath, edge-of-my-seat anticipation, because it *so often* seems to me like parents fabricate options rather than dealing with the tradeoffs with eyes open.\n\n*If I just tell them they can't hang out with that kid anymore, the problem will be solved.*\n\n*If I just make them keep playing piano, they'll thank me later.*\n\n*I can just tell them no.*\n\n*I can just tell them they have to.*\n\n*I can ground them until they comply.*\n\nAs with the example of price gouging, it's not that there aren't good ways to intervene on the above situations.  The claim is not \"the options, as they are at this exact moment, are the only options that will ever be on the table.\"\n\nRather, it's \"there *are* a certain limited number of options on the table at this exact moment.  If none of them are satisfactory, someone will have to actively create or uncover new ones.  They can't be willed into being by sheer stubborn fiat.\"\n\nOption A, in each of the above scenarios, comes with *massive* costs, usually taken out of the value of the parent-child relationship.\n\nSure, you *can* ban your child from a given friendship, but what's going to *actually* *happen* is that your child will stop viewing you as their ally and start treating you as a prison warden or appointed overseer—as obstacle to be dealt with.  They'll either succeed at getting around your edict, and you'll have sacrificed a significant part of your mutual trust for nothing, or they'll fail, and resent you for it.\n\nSome parents would argue that this is fine, it's worth it, better the kid be mad at *me* than suffer \\[bad outcome\\].\n\nAnd in some cases that's genuinely true.\n\nBut *most* of the time, the thing the parent implicitly imagines—that they can get \\[good outcome\\] *and* it won't cost anything in terms of relationship capital—it's not really on the table.\n\nIt's not \"I'll make them play piano and everything will be fine\" versus \"they'll lose their piano-playing potential.\"\n\nIt's \"I'll make them play piano by using our mutual affection as kindling\" or \"I'll let them do what they want and preserve our relationship.\"\n\nNeither option is great, viewed through that lens.  It's an orphan on the one hand and an abortion on the other.\n\nBut that's the thing.  Most of the time, neither option *is* great.  In difficult situations, it's wise to be at least a little suspicious of straightforward, easy Options A that are just so clearly better than those uncomfortably costly tradeoff-y Options B.\n\n* * *\n\n**Example 6: 2020**\n-------------------\n\n(This section left as an exercise for the reader.)\n\n* * *\n\n**Conclusion**\n--------------\n\nA likely thought on the minds of some readers is that this isn't exactly new ground, and we already have all of the pieces necessary to individually identify each instance of fabricated options based on their inherent falsehood, and therefore don't actually need the new category.\n\nI disagree; I find that [fine distinctions are generally useful](https://www.facebook.com/duncan.sabien/posts/4232363480131670) and have personally benefitted from being able to port strategies *between* widely-spaced instances of option fabrication, and from being able to train my option-fabrication-recognizer on a broad data set.\n\nThat being said: beware the failure mode of new jargon, which is thinking that you now *recognize* \\[the thing\\], rather than that you are now equipped to *hypothesize* \\[maybe the thing?\\].  The world would be a better place if people's response to the reification of concepts like \"sealioning\" or \"DARVO\" or \"attention-deficit disorder\" were to *ask whether that's what's happening here, and how we would know* as opposed to immediately weaponizing them.\n\n(Alas, that's a fabricated option, and the real choice is between \"invent good terms but see them misused a bunch\" and \"refuse to invent good terms.\"  But maybe LessWrong can do better than genpop.)\n\nAs for what to do about fabricated options (both those your own brain generates and those generated by others), the general recommendation is pretty much \"use your rationality\" and there isn't room in this one essay to operationalize that.  My apologies.\n\nIf you're looking for e.g. specific named [CFAR techniques](https://rationality.org/resources/handbook) that might come in handy here, I'd point you toward TAPs (especially TAPs for noticing fabricated options as they come up, or booting up your alert awareness in situations where they're likely to) and Murphyjitsu (which is likely to improve people's baseline ability to both recognize glossed-over fairy tales and patch the holes therein).  You might also work on building your general noticing skill, perhaps starting with any number of writings by [Logan Strohl](https://agentyduck.blogspot.com/2015/07/cognitive-trigger-action-planning-for.html), and on [double crux](https://www.lesswrong.com/posts/exa5kmvopeRyfJgCy/double-crux-a-strategy-for-resolving-disagreement) and similar tools, which will make it easier to make disagreements over the menu-of-options *productive* rather than not.\n\nIn the meantime, I would deeply appreciate it if any comments sharing examples of the class contained the string #EXAMPLE, and if any comments containing concrete recommendations or stories about how-you-responded contained the string #TOOLS. This will make it easier for the comment section to stand as an enduring and useful appendix to this introduction.\n\nGood luck.\n\n* * *\n\nFollowup from Logan Strohl: [Investigating Fabrication](https://www.lesswrong.com/posts/NjZAkfio5FsCioahb/investigating-fabrication)"
    },
    "voteCount": 149,
    "forceInclude": true
  },
  {
    "_id": "wEebEiPpEwjYvnyqq",
    "url": null,
    "title": "When Money Is Abundant, Knowledge Is The Real Wealth",
    "slug": "when-money-is-abundant-knowledge-is-the-real-wealth",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "World Modeling"
      },
      {
        "name": "Expertise (topic)"
      },
      {
        "name": "Economics"
      }
    ],
    "tableOfContents": {
      "sections": [
        {
          "title": "First Puzzle Piece",
          "anchor": "First_Puzzle_Piece",
          "level": 1
        },
        {
          "title": "Second Puzzle Piece",
          "anchor": "Second_Puzzle_Piece",
          "level": 1
        },
        {
          "title": "Putting The Pieces Together",
          "anchor": "Putting_The_Pieces_Together",
          "level": 1
        },
        {
          "title": "Investments In Knowledge",
          "anchor": "Investments_In_Knowledge",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        },
        {
          "anchor": "comments",
          "level": 0,
          "title": "61 comments"
        }
      ],
      "headingsCount": 6
    },
    "contents": {
      "markdown": "First Puzzle Piece\n------------------\n\nBy and large, the President of the United States can order people to do things, and they will do those things. POTUS is often considered the most powerful person in the world. And yet, the president cannot order a virus to stop replicating. The president cannot order GDP to increase. The president cannot order world peace.\n\nAre there orders the president could give which would result in world peace, or increasing GDP, or the end of a virus? Probably, yes. Any of these could likely even be done with relatively little opportunity cost. Yet no president in history has known *which* orders will efficiently achieve these objectives. There are probably some people in the world who know which orders would efficiently increase GDP, but the president cannot distinguish them from the millions of people who *claim* to know (and may even believe it themselves) but are wrong.\n\nLast I heard, Jeff Bezos was the official richest man in the world. He can buy basically anything money can buy. But he can’t buy a cure for cancer. Is there some way he could spend a billion dollars to cure cancer in five years? Probably, yes. But Jeff Bezos does not know how to do that. Even if someone somewhere in the world does know how to turn a billion dollars into a cancer cure in five years, Jeff Bezos cannot distinguish that person from the thousands of other people who *claim* to know (and may even believe it themselves) but are wrong.\n\nWhen non-experts cannot distinguish true expertise from noise, [money cannot buy expertise](https://www.lesswrong.com/posts/YABJKJ3v97k9sbxwg/what-money-cannot-buy). Knowledge cannot be outsourced; we must understand things ourselves.\n\nSecond Puzzle Piece\n-------------------\n\nThe [Haber process](https://en.wikipedia.org/wiki/Haber_process) combines one molecule of nitrogen with three molecules of hydrogen to produce two molecules of ammonia - useful for fertilizer, explosives, etc. If I feed a few grams of hydrogen and several tons of nitrogen into the Haber process, I’ll get out a few grams of ammonia. No matter how much more nitrogen I pile in - a thousand tons, a million tons, whatever - I will not get more than a few grams of ammonia. If the reaction is limited by the amount of hydrogen, then throwing more nitrogen at it will not make much difference.\n\nIn the language of [constraints and slackness](https://www.lesswrong.com/s/xEFeCwk3pdYdeG2rL): ammonia production is constrained by hydrogen, and by nitrogen. When nitrogen is abundant, the nitrogen constraint is slack; adding more nitrogen won’t make much difference. Conversely, since hydrogen is scarce, the hydrogen constraint is taut; adding more hydrogen will make a difference. Hydrogen is the bottleneck.\n\nLikewise in economic production: if a medieval book-maker requires 12 sheep skins and 30 days’ work from a transcriptionist to produce a book, and the book-maker has thousands of transcriptionist-hours available but only 12 sheep, then he can only make one book. Throwing more transcriptionists at the book-maker will not increase the number of books produced; sheep are the bottleneck.\n\nWhen some inputs become more or less abundant, bottlenecks change. If our book-maker suddenly acquires tens of thousands of sheep skins, then transcriptionists may become the bottleneck to book-production. In general, when one resource becomes abundant, other resources become bottlenecks.\n\nPutting The Pieces Together\n---------------------------\n\nIf I don’t know how to efficiently turn power into a GDP increase, or money into a cure for cancer, then throwing more power/money at the problem will not make much difference.\n\nKing Louis XV of France was one of the richest and most powerful people in the world. [He died of smallpox in 1774, the same year that a dairy farmer successfully immunized his wife and children with cowpox](https://www.lesswrong.com/posts/YABJKJ3v97k9sbxwg/what-money-cannot-buy). All that money and power could not buy the knowledge of a dairy farmer - the knowledge that cowpox could safely immunize against smallpox. There were thousands of humoral experts, faith healers, eastern spiritualists, and so forth who would claim to offer some protection against smallpox, and King Louis XV could not distinguish the real solution.\n\nAs one resource becomes abundant, other resources become bottlenecks. When wealth and power become abundant, anything wealth and power cannot buy become bottlenecks - including knowledge and expertise.\n\nAfter a certain point, wealth and power cease to be the taut constraints on one’s action space. They just don’t matter that much. Sure, giant yachts are great for social status, and our lizard-brains love politics. The modern economy is happy to provide outlets for disposing of large amounts of wealth and power. But personally, I don’t care that much about giant yachts. I want a cure for aging. I want weekend trips to the moon. I want flying cars and an indestructible body and tiny genetically-engineered dragons. Money and power can’t efficiently buy that; the bottleneck is knowledge.\n\nBased on my own experience and the experience of others I know, I think knowledge starts to become taut rather quickly - I’d say at an annual income level in the low hundred thousands. With that much income, if I knew exactly the experiments or studies to perform to discover a cure for cancer, I could probably make them happen. (Getting regulatory approval is another matter, but I think that would largely handle itself if people knew the solution - there’s a large profit incentive, after all.) Beyond that level, more money mostly just means more ability to spray and pray for solutions - which is [not a promising strategy in our high-dimensional world](https://www.lesswrong.com/posts/pT48swb8LoPowiAzR/everyday-lessons-from-high-dimensional-optimization).\n\nSo, two years ago I quit my monetarily-lucrative job as a data scientist and have mostly focused on acquiring knowledge since then. I can worry about money if and when I know what to do with it.\n\nA mindset I recommend trying on from time to time, especially for people with $100k+ income: think of money as an abundant resource. Everything money can buy is “cheap”, because money is \"cheap\". Then the things which are “expensive” are the things which money alone cannot buy - including knowledge and understanding of the world. Life lesson from [Disney!Rumplestiltskin](https://www.imdb.com/title/tt1843230/): there are things which money cannot buy, therefore it is important to acquire such things and use them for barter and investment. In particular, it’s worth looking for opportunities to acquire knowledge and expertise which can be leveraged for *more* knowledge and expertise.\n\nInvestments In Knowledge\n------------------------\n\nPast a certain point, money and power are no longer the limiting factors for me to get what I want. Knowledge becomes the bottleneck instead. At that point, money and power are no longer particularly relevant measures of my capabilities. Pursuing more “wealth” in the usual sense of the word is no longer a very useful instrumental goal. At that point, the type of “wealth” I really need to pursue is knowledge.\n\nIf I want to build long-term knowledge-wealth, then the analogy between money-wealth and knowledge-wealth suggests an interesting question: what does a knowledge “investment” look like? What is a capital asset of knowledge, an investment which pays dividends in more knowledge?\n\n[Enter gears-level models](https://www.lesswrong.com/posts/nEBbw2Bc2CnN2RMxy/gears-level-models-are-capital-investments).\n\nMapping out the internal workings of a system takes a lot of up-front work. It’s much easier to try random molecules and see if they cure cancer, than to map out all the internal signals and cells and interactions which cause cancer. But the latter is a capital investment: once we’ve nailed down one gear in the model, one signal or one mutation or one cell-state, that informs all of our future tests and model-building. If we find that Y mediates the effect of X on Z, then our future studies of the Y-Z interaction can safely ignore X. On the other hand, if we test a random molecule and find that it doesn’t cure cancer, then that tells us little-to-nothing; that knowledge does not yield dividends.\n\nOf course, gears-level models aren’t the only form of capital investment in knowledge. Most tools of applied math and the sciences consist of general models which we can learn once and then apply in many different contexts. They are general-purpose gears which we can recognize in many systems.\n\nOnce I understand the internal details of how e.g. [capacitors](https://en.wikipedia.org/wiki/Capacitor) work, I can apply that knowledge to understand not only electronic circuits, but also [charged biological membranes](https://en.wikipedia.org/wiki/Membrane_potential). When I understand the math of microeconomics, I can [apply it](https://www.lesswrong.com/posts/brhWPoNsBN7za3xjs/competitive-markets-as-distributed-backprop) to optimization problems in AI. When I understand shocks and rarefactions in nonlinear PDEs, I can see them in action at the beach or [in traffic](https://youtu.be/Rryu85BtALM?t=47). And the “core” topics - calculus, linear algebra, differential equations, big-O analysis, Bayesian probability, optimization, dynamical systems, etc - can be applied all over. General-purpose models are a capital investment in knowledge.\n\nI hope that someday my own research will be on that list. That’s the kind of wealth I’m investing in now."
    },
    "voteCount": 159,
    "forceInclude": true
  },
  {
    "_id": "YABJKJ3v97k9sbxwg",
    "url": null,
    "title": "What Money Cannot Buy",
    "slug": "what-money-cannot-buy",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "World Modeling"
      },
      {
        "name": "Expertise (topic)"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "[Paul Graham](http://www.paulgraham.com/gh.html):\n\n> The problem is, if you're not a hacker, you can't tell who the good hackers are. A similar problem explains why American cars are so ugly. I call it the design paradox. You might think that you could make your products beautiful just by hiring a great designer to design them. But if you yourself don't have good taste, how are you going to recognize a good designer? By definition you can't tell from his portfolio. And you can't go by the awards he's won or the jobs he's had, because in design, as in most fields, those tend to be driven by fashion and schmoozing, with actual ability a distant third. There's no way around it: you can't manage a process intended to produce beautiful things without knowing what beautiful is. American cars are ugly because American car companies are run by people with bad taste.\n\nI don’t know how much I believe this claim about cars, but I certainly believe it about software. A startup without a technical cofounder will usually produce bad software, because someone without software engineering skills does not know how to recognize such skills in someone else. The world is full of bad-to-mediocre “software engineers” who do not produce good software. If you don’t already know a fair bit about software engineering, you will not be able to distinguish them from the people who really know what they’re doing.\n\nSame with user interface design. I’ve worked with a CEO who was good at UI; both the process and the results were visibly superior to others I’ve worked with. But if you don’t already know [what good UI design looks like](https://www.amazon.com/Design-Everyday-Things-Revised-Expanded/dp/0465050654/), you’d have no idea - good design is largely invisible.\n\nYudkowsky [makes the case](https://intelligence.org/2017/11/26/security-mindset-and-the-logistic-success-curve/) that the same applies to security: you can’t build a secure product with novel requirements without having a security expert as a founder. The world is full of “security experts” who do not, in fact, produce secure systems - I’ve met such people. (I believe they mostly make money by helping companies visibly pretend to have made a real effort at security, which is useful in the event of a lawsuit.) If you don’t already know a fair bit about security, you will not be able to distinguish such people from the people who really know what they’re doing.\n\nBut to really drive home the point, we need to go back to 1774.\n\nAs the American Revolution was heating up, a wave of smallpox was raging on the other side of the Atlantic. An English dairy farmer named Benjamin Jesty was concerned for his wife and children. He was not concerned for himself, though - he had previously contracted cowpox. Cowpox was contracted by milking infected cows, and was well known among dairy farmers to convey immunity against smallpox.\n\nUnfortunately, neither Jesty’s wife nor his two children had any such advantage. When smallpox began to pop up in Dorset, Jesty decided to take drastic action. He took his family to a nearby farm with a cowpox-infected cow, scratched their arms, and wiped pus from the infected cow on the scratches. Over the next few days, their arms grew somewhat inflamed and they suffered the mild symptoms of cowpox - but it quickly passed. As the wave of smallpox passed through the town, none of the three were infected. Throughout the rest of their lives, through multiple waves of smallpox, they were immune.\n\nThe same technique would be popularized twenty years later by Edward Jenner, marking the first vaccine and the beginning of modern medicine.\n\nThe same wave of smallpox which ran across England in 1774 also made its way across Europe. In May, it reached Louis XV, King of France. Despite the wealth of a major government and the talents of Europe’s most respected doctors, Louis XV died of smallpox on May 10, 1774.\n\nThe point: there is knowledge for which money cannot substitute. Even if Louis XV had offered a large monetary bounty for ways to immunize himself against the pox, he would have had no way to distinguish Benjamin Jesty from the endless crowd of snake-oil sellers and faith healers and humoral balancers. Indeed, top medical “experts” of the time would likely have warned him _away_ from Jesty.\n\nThe general pattern:\n\n*   Take a field in which it’s hard for non-experts to judge performance\n*   Add lots of people who _claim_ to be experts (and may even believe that themselves)\n*   Result: someone who is not already an expert will not be able to buy good performance, even if they throw lots of money at the problem\n\nNow, presumably we can get around this problem by investing the time and effort to become an expert, right? Nope! Where there are snake-oil salesmen, there will also be people offering to teach their secret snake-oil recipe, so that you too can become a master snake-oil maker.\n\nSo… what _can_ we do?\n\nThe cheapest first step is to do some basic reading on a few different viewpoints and think things through for yourself. Simply reading [the “correct horse battery staple” xkcd](https://xkcd.com/936/) will be sufficient to recognize a surprising number of really bad “security experts”. It probably won’t get you to a level where you can distinguish the best from the middling - I don’t think I can currently distinguish the best from the middling security experts. But it’s a start.\n\nMore generally: it’s often easier to tell which of multiple supposed experts is correct, than to figure everything out from first principles yourself. Besides looking at the object-level product, this often involves looking at incentives in the broader system - see e.g. [Inadequate Equilibria](https://equilibriabook.com/). Two specific incentive-based heuristics:\n\n*   Skin in the game is a good sign - Jesty wanted to save his own family, for instance.\n*   Decoupling from external monetary incentives is useful - in other words, look for hobbyists. People at a classic car meetup or a track day will probably have better taste in car design than the J.D. Powers award.\n\nThat said, remember the main message: there is no full substitute for being an expert yourself. Heuristics about incentives can help, but they’re leaky filters at best.\n\nWhich brings us to the ultimate solution: try it yourself. Spend time in the field, practicing the relevant skills first-hand; see both what works and what makes sense. Collect data; run trials. See what other people suggest and test those things yourself. Directly study which things actually produce good results."
    },
    "voteCount": 141,
    "forceInclude": true
  },
  {
    "_id": "DoHcgTvyxdorAMquE",
    "url": null,
    "title": "Bets, Bonds, and Kindergarteners",
    "slug": "bets-bonds-and-kindergarteners",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "Parenting"
      },
      {
        "name": "Betting"
      },
      {
        "name": "Practical"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "Bets and bonds are tools for handling different epistemic states and levels of trust. Which makes them a great fit for negotiating with small children!\n\nA few weeks ago Anna (4y) wanted to play with some packing material. It looked very messy to me, I didn't expect she would clean it up, and I didn't want to fight with her about cleaning it up. I considered saying no, but after thinking about how things like this are handled in the real world I had an idea. If you want to do a hazardous activity, and we think you might go bankrupt and not clean up, we make you post a bond. This money is held in escrow to fund the cleanup if you disappear. I explained how this worked, and she went and got a dollar:\n\n[![](https://www.jefftk.com/green-paper-bond-coin.jpg)](https://www.jefftk.com/green-paper-bond-coin-big.jpg)\n\nThen:\n\n[![](https://www.jefftk.com/green-paper-bond1.jpg)](https://www.jefftk.com/green-paper-bond1-big.jpg)\n\n[![](https://www.jefftk.com/green-paper-bond3.jpg)](https://www.jefftk.com/green-paper-bond3-big.jpg)\n\nWhen she was done playing, she cleaned it up without complaint and got her dollar back. If she hadn't cleaned it up, I would have, and kept the dollar.\n\nSome situations are more complicated, and call for bets. I wanted to go to a park, but Lily (6y) didn't want to go to that park because the last time we had been there there'd been lots of bees. I remembered that had been a summer with unusually many bees, and it no longer being that summer or, in fact, summer at all, I was not worried. Since I was so confident, I offered my $1 to her $0.10 that we would not run into bees at the park. This seemed fair to her, and when there were no bees she was happy to pay up.\n\nOver time, they've learned that my being willing to bet, especially at large odds, is pretty informative, and often all I need to do is offer. Lily was having a rough morning, crying by herself about a project not working out. I suggested some things that might be fun to do together, and she rejected them angrily. I told her that often when people are feeling that way, going outside can help a lot, and when she didn't seem to believe me I offered to bet. Once she heard the 10:1 odds I was offering her I think she just started expecting that I was right, and she decided we should go ride bikes. (She didn't actually cheer up when we got outside: she cheered up as soon as she made this decision.)\n\nI do think there is some risk with this approach that the child will have a bad time just to get the money, or say they are having a bad time and they are actually not, but this isn't something we've run into. Another risk, if we were to wager large amounts, would be that the child would end up less happy than if I hadn't interacted with them at all. I handle this by making sure not to offer a bet I think they would regret losing, and while this is not a courtesy I expect people to make later in life, I think it's appropriate at their ages.\n\n*Comment via: [facebook](https://www.facebook.com/jefftk/posts/10100199782012082)*"
    },
    "voteCount": 224,
    "forceInclude": true
  },
  {
    "_id": "qmXqHKpgRfg83Nif9",
    "url": null,
    "title": "How to Ignore Your Emotions (while also thinking you're awesome at emotions)",
    "slug": "how-to-ignore-your-emotions-while-also-thinking-you-re",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "Emotions"
      },
      {
        "name": "Rationality"
      },
      {
        "name": "Growth Stories"
      }
    ],
    "tableOfContents": {
      "sections": [
        {
          "title": "Relevant context",
          "anchor": "Relevant_context",
          "level": 1
        },
        {
          "title": "Wiggling ears",
          "anchor": "Wiggling_ears",
          "level": 1
        },
        {
          "title": "Why \"ignore\" and \"deal with\" looked the same",
          "anchor": "Why__ignore__and__deal_with__looked_the_same",
          "level": 1
        },
        {
          "title": "Parent-child model",
          "anchor": "Parent_child_model",
          "level": 1
        },
        {
          "title": "... and now?",
          "anchor": "____and_now_",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        },
        {
          "anchor": "comments",
          "level": 0,
          "title": "71 comments"
        }
      ],
      "headingsCount": 7
    },
    "contents": {
      "markdown": "_(cross posted from my [personal blog](http://www.jhazard.com/posts/ignoring_emotions.html))_\n\nSince middle school I've generally thought that I'm pretty good at dealing with my emotions, and a handful of close friends and family have made similar comments. Now I can see that though I was particularly good at never flipping out, I was decidedly _not_ good \"healthy emotional processing\". I'll explain later what I think \"healthy emotional processing\" is, right now I'm using quotes to indicate \"the thing that's good to do with emotions\". Here it goes...\n\n**Relevant context**\n--------------------\n\nWhen I was a kid I adopted a strong, \"Fix it or stop complaining about it\" mentality. This applied to stress and worry as well. \"Either address the problem you're worried about or quit worrying about it!\" Also being a kid, I had a limited capacity to actually fix anything, and as such I was often exercising the \"stop worrying about it\" option.\n\nAnother thing about me, I was a massive book worm and loved to collect \"obvious mistakes\" that heroes and villains would make. My theory was, \"Know all the traps, and then just _don't_ fall for them\". That plus the sort of books I read meant that I \"knew\" it was a big no-no to ignore or repress your emotions. Luckily, since I knew you shouldn't repress your emotions, I \"just didn't\" and have lived happily ever after\n\n...\n\n...\n\nyeah nopes.\n\n**Wiggling ears**\n-----------------\n\nIt can be really hard to teach someone to move in a way that is completely new to them. I teach parkour, and sometimes I want to say,\n\nMe: \"Do the shock absorbing thing with your legs!\" Student: \"What's the shock absorbing thing?\" Me: \"... uh, you know... the thing were your legs... absorb shock?\"\n\nIt's hard to know how to give cues that will lead to someone making the right mental/muscle connection. Learning new motor movements is somewhat of a process of flailing around in the dark, until some feedback mechanism tells you you did it right (a coach, it's visually obvious, the jump doesn't hurt anymore, etc). Wiggling your ears is a nice concrete version of a) movement most people's bodies are capable of and b) one that most people feel like is impossible.\n\nClaim: learning mental and emotional skills has a similar \"flailing around in the dark\" aspect. There are the mental and emotional controls you've practiced, and those just feel like moving your arm. Natural, effortless, atomic. But there are other moves, which you are _totally capable of_ which seem impossible because you don't know how your \"control panel\" connects to that output. This feels like trying to wiggle your ears.\n\n**Why \"ignore\" and \"deal with\" looked the same**\n------------------------------------------------\n\nSo young me is upset that the grub master for our camping trip forgot half the food on the menu, and all we have for breakfast is milk. I couldn't \"fix it\" given that we were in the woods, so my next option was \"stop feeling upset about it.\" So I reached around in the dark of my mind, and Oops, the \"healthily process feelings\" lever is _right next to_ the \"stop listening to my emotions\" lever.\n\nThe end result? \"Wow, I decided to stop feeling upset, and then I stopped feeling upset. I'm so fucking good at emotional regulation!!!!!\"\n\nMy model now is that I [substituted](http://www.jhazard.com/posts/question_substitution.html) \"is there a monologue of upsetness in my conscious mental loop?\" for \"am I feeling upset?\". So from my perspective, it just felt like I was very in control of my feelings. Whenever I wanted to stop feeling something, I could. When I thought of ignoring/repressing emotions, I imagined trying to cover up something that was there, maybe with a story. Or I thought if you poked around ignored emotions there would be a response of anger or annoyance. I at least expected that if I was ignoring my emotions, that if I got very calm and then asked myself, \"Is there anything that you're feeling?\" I would get an answer.\n\nAgain, the assumption was, \"If it's in my mind, I should be able to notice if I look.\" This ignored what was actually happening, which was that I was cutting the phone lines so my emotions couldn't talk to me in the first place. Actually, the phone lines metaphor is a bit off, here's a better one.\n\n**Parent-child model**\n----------------------\n\nMy self-concept and conscious mind are the parent. Emotions are young children that run up to the parent to tell them something. Sometimes the child runs up to complain, \"Heeeeeeeeeey I'm huuuuuuungry!\" My emotional management was akin to the parenting style of slapping the child and saying, \"Being hungry would suck, so _you aren't hungry_.\"\n\nYikes.\n\nI know full well that you can't slap someone into having a full stomach, but you can slap someone into not bringing their complaints to you.\n\nI've experienced this _directly extend_ to my internal world. My emotions / [sub-agents](https://www.lesswrong.com/posts/M4w2rdYgCKctbADMn/sequence-introduction-non-agent-and-multiagent-models-of) aren't stupid. They learned that telling me, \"Hey, you're concerned about your relationship with your friend!\", \"Hey, we really don't like getting laughed at\", \"Hey, we're concerned that this bad thing is going to happen indefinitely\" would result in getting slapped. So they learned to stay quiet.\n\nThis got to the point where I'd feel awesome and great during my busy week, and then \"mysteriously\" and \"for no reason\" feel an amorphous blob of gray badness on the weekends. I had various social and emotional needs that weren't being met, but I didn't realize that. I quite intensely tried to introspect to see if this gray blob was \"about anything\", but only heard quiet static. This was me being the angry parent with their kids having a dinner of half a slice of bread each, shouting, \"Is anyone hungry?! Huh??! No? GREAT.\"\n\nOwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwww\n\n**... and now?**\n----------------\n\nWhen I was a kid, my desire to \"not worry if it was useless\" was mostly one of \"people who worry seem to be in pain, I'd prefer to not be in pain.\" Overtime, it turned into a judgmental world view. How _wasteful_ and _useless_ to be embarrassed/worried/scared/etc. This was the transition from a naive parent telling their kid, \"Hmmmm, have you tried not being hungry?\" to the angry parent shouting, \"You won't be hungry in _my_ house!!\" (one might wonder how exactly that transition from naive to judgmental happened. That's a whole other story for a different post)\n\nOver the past year I've haphazardly free styled towards opening up emotional communication with myself, and I've made progress. I'm still not sure what \"healthy emotional processing\" looks like, but I've gotten HUGE gains from just being able to sit with the fact that I'm feeling something, and hug the child that brought that emotion instead of slapping them.\n\nI guess the biggest thing I wanted to impart with this piece was 1. the parent child model, but also 2. that ignoring your emotions can start as a simple innocent mistake.\n\nRelated. A sentiment in a LW thread I heard in the past few months was that the biggest barrier to rational discourse is creating environments where everyone feels safe thinking (not the same thing as a safe space). Extend that to the mind. The biggest barrier to rational thinking is organizing your mind such that it's safe to think. I still promote and admire \"look towards the truth, even if it hurts\", but I know see that if you don't spend enough resources on addressing that hurt, the hurt parts of yourself can and _will_ take measures to protect themselves. Treat yourself well."
    },
    "voteCount": 151,
    "forceInclude": true
  },
  {
    "_id": "bx3gkHJehRCYZAF3r",
    "url": "https://radimentary.wordpress.com/2020/11/24/pain-is-not-the-unit-of-effort/",
    "title": "Pain is not the unit of Effort",
    "slug": "pain-is-not-the-unit-of-effort",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "Practical"
      },
      {
        "name": "Rationality"
      },
      {
        "name": "Well-being"
      },
      {
        "name": "Happiness"
      },
      {
        "name": "Heroic Responsibility"
      },
      {
        "name": "Fallacies"
      },
      {
        "name": "Suffering"
      },
      {
        "name": "Willpower"
      }
    ],
    "tableOfContents": {
      "sections": [
        {
          "title": "I. Anecdotes",
          "anchor": "I__Anecdotes",
          "level": 1
        },
        {
          "title": "II. Antidotes",
          "anchor": "II__Antidotes",
          "level": 1
        },
        {
          "title": "1. If it hurts, you're probably doing it wrong.",
          "anchor": "1__If_it_hurts__you_re_probably_doing_it_wrong_",
          "level": 2
        },
        {
          "title": "2. You're not trying your best if you're not happy.",
          "anchor": "2__You_re_not_trying_your_best_if_you_re_not_happy_",
          "level": 2
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        },
        {
          "anchor": "comments",
          "level": 0,
          "title": "79 comments"
        }
      ],
      "headingsCount": 6
    },
    "contents": {
      "markdown": "(Content warning: self-harm, parts of this post may be actively counterproductive for readers with certain mental illnesses or idiosyncrasies.)\n\n> *What doesn't kill you makes you stronger.* ~ Kelly Clarkson.\n> \n> *No pain, no gain.* ~ Exercise motto.\n> \n> *The more bitterness you swallow, the higher you'll go.* ~ Chinese proverb.\n\nI noticed recently that, at least in my social bubble, *pain is the unit of effort.* In other words, how hard you are trying is explicitly measured by how much suffering you put yourself through. In this post, I will share some anecdotes of how damaging and pervasive this belief is, and propose some counterbalancing ideas that might help rectify this problem.\n\nI. Anecdotes\n------------\n\n1\\. As a child, I spent most of my evenings studying mathematics under some amount of supervision from my mother. While studying, if I expressed discomfort or fatigue, my mother would bring me a snack or drink and tell me to stretch or take a break. I think she took it as a sign that I was trying my best. If on the other hand I was smiling or joyful for extended periods of time, she took that as a sign that I had effort to spare and increased the hours I was supposed to study each day. To this day there's a gremlin on my shoulder that whispers, \"If you're happy, you're not trying your best.\"\n\n2\\. A close friend who played sports in school reports that training can be *harrowing.* He told me that players who fell behind the pack during for daily jogs would be singled out and publicly humiliated. One time the coach screamed at my friend for falling behind the asthmatic boy who was alternating between running and using his inhaler. Another time, my friend internalized \"no pain, no gain\" to the point of losing his toenails.\n\n3\\. In high school and college, I was surrounded by overachievers constantly making (what seemed to me) incomprehensibly bad life choices. My classmates would sign up for eight classes per semester when the recommended number is five, jigsaw extracurricular activities into their calendar like a dynamic programming knapsack-solver, and then proceed to have loud public complaining contests about which libraries are most comfortable to study at past 2am and how many pages they have left to write for the essay due in three hours. Only later did I learn to ask: what incentives were they responding to?\n\n4\\. A while ago I became a connoisseur of Chinese webnovels. Among those written for a male audience, there is a surprisingly diverse set of character traits represented among the main characters. Doubtless many are womanizing murderhobos with no redeeming qualities, but others are classical heroes with big hearts, or sarcastic antiheroes who actually grow up a little, or ambitious empire-builders with grand plans to pave the universe with Confucian order, or down-on-their-luck starving artists who just want to bring happiness to the world through song.\n\nIf there is a single common virtue shared by all these protagonists, it is their *superhuman pain tolerance*. Protagonists routinely and often voluntarily dunk themselves in vats of lava, have all their bones broken, shattered, and reforged, get trapped inside alternate dimensions of freezing cold for millennia (which conveniently only takes a day in the outside world), and overdose on level-up pills right up to the brink of death, all in the name of becoming stronger. Oftentimes the defining difference between the protagonist and the antagonist is that the antagonist did not have enough pain tolerance and allowed the (unbearable physical) suffering in his life to drive him mad.\n\n5\\. I have a close friend who often asks for my perspective on personal problems. A pattern arose in a couple of our conversations:\n\n> alkjash: I feel like you're not actually trying. \\[Meaning: using all the tools at your disposal, getting creative, throwing money at the problem to make it go away.\\]\n> \n> alkjash's friend: What do you mean I'm not trying? I think I'm trying my best, can't you tell how hard I'm trying? \\[Meaning: piling on time, energy, and willpower to the point of burnout.\\]\n\nAfter several of these conversations went nowhere, I learned that asking this friend to try harder directly translated in his mind to accusing him of low pain tolerance and asking him to hurt himself more.\n\nII. Antidotes\n-------------\n\nI often hear on the internet laments like \"Why is nobody actually trying?\" Once upon a time, I was honestly and genuinely confused by this question. It seemed to me that \"actually trying\" - aiming the full force of your being at the solution of a problem you care about - is self-evidently motivating and requires zero extra justification if you care about the problem.\n\nI think I finally understand why so few people are \"actually trying.\" The reason is this pervasive and damaging belief that *pain is the unit of effort.* With this belief, the injunction \"actually try\" means \"put yourself in as much pain as you can handle.\" Similarly, \"she's trying her best\" translates to \"she's really hurting right now.\" Even worse, people with this belief *optimize for the appearance of suffering*. Answering emails at midnight and appearing fatigued at meetings are somehow taken to be more credible signals of effort than actual results. And if you think that's pathological, wait until you meet someone for whom *telling them about opportunities actively hurts them*, because you've just created another knife they feel pressured to cut themselves with.\n\nI see a mob of people walking up to houses and throwing themselves bodily at the closed front doors. I walk up to block one man and ask, \"Stop it! Why don't you try the doorknob first? Have you rung the doorbell?\" The man responds in tears, nursing his bloody right shoulder, \"I'm trying as hard as I can!\" With his one good arm, he shoves me aside and takes a running start to lunge at the door again. Finally, the timber shatters and the man breaks through. The surrounding mob cheers him on, \"Look how hard he's trying!\"\n\nOnce you understand that pain is how people define effort, the answer to the question \"why is nobody actually trying?\" becomes astoundingly obvious. I'd like to propose two beliefs to counterbalance this awful state of affairs.\n\n**1\\. If it hurts, you're probably doing it wrong.**\n\nIf your wrists ache on the bench press, you're probably using bad form and/or too much weight. If your feet ache from running, you might need sneakers with better arch support. If you're consistently sore for days after exercising, you should learn to stretch properly and check your nutrition.\n\nSuch rules are well-established in the setting of physical exercise, but their analogs in intellectual work seem to be completely lost on people. If reading a math paper is actively unpleasant, you should find a better-written paper or learn some background material first (most likely both). If you study or work late into the night and it disrupts your Circadian rhythm, you're trading off long-term productivity and well-being for low-quality work. That's just bad form.\n\nIf it hurts, you're probably doing it wrong.\n\n**2\\. You're not trying your best if you're not happy.**\n\nHappiness is really, really instrumentally useful. Being happy gives you more energy, increases your physical health and lifespan, makes you more creative and risk-tolerant, and (even if all the previous effects are unreplicated pseudoscience) causes other people to like you more. Whether you are tackling the Riemann hypothesis, climate change, or your personal weight loss, one of the first steps should be to acquire as much happiness as you can get your hands on. And the good news is: at least [anecdotally](https://www.lesswrong.com/posts/xnPFYBuaGhpq869mY/ureshiku-naritai), it is possible to substantially raise your happiness set-point through jedi mind tricks.\n\nBecoming happy is a fully general problem-solving strategy. And although one can in principle trade off happiness for short bursts of productivity, in practice this is never worth it.\n\nCulturally, we've been led to believe that over-stressed and tired people are the ones trying their best. It is right and proper to be kind to such people, but let's not go so far as to support the delusion that they are inputting as much effort as their joyful, boisterous peers bouncing off the walls.\n\nYou're not trying your best if you're not happy.\n\n\\[Edit: Antidotes #1 and #2 are not primarily to be interpreted as truth claims, see [Anna Salamon's comment](https://www.lesswrong.com/posts/bx3gkHJehRCYZAF3r/pain-is-not-the-unit-of-effort?commentId=kZXYGXGdBjYGH6miC).\\]"
    },
    "voteCount": 252,
    "forceInclude": true
  },
  {
    "_id": "Sdx6A6yLByRRs8iLY",
    "url": null,
    "title": "Fact Posts: How and Why",
    "slug": "fact-posts-how-and-why",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "Fact posts"
      },
      {
        "name": "Practical"
      },
      {
        "name": "World Modeling"
      },
      {
        "name": "Scholarship & Learning"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "The most useful thinking skill I've taught myself, which I think should be more widely practiced, is writing what I call \"fact posts.\"  I write a bunch of these on my [blog](https://srconstantin.wordpress.com/). (I write fact posts about pregnancy and childbirth [here.](https://parentingwithevidence.wordpress.com/))\n\nTo write a fact post, you start with an empirical question, or a general topic.  Something like \"How common are hate crimes?\" or \"Are epidurals really dangerous?\" or \"What causes manufacturing job loss?\"  \n\nIt's okay if this is a topic you know very little about. This is an exercise in original seeing and showing your reasoning, not finding the official last word on a topic or doing the best analysis in the world.\n\nThen you open up a Google doc and start taking notes.\n\nYou look for _quantitative data from conventionally reliable sources_.  CDC data for incidences of diseases and other health risks in the US; WHO data for global health issues; Bureau of Labor Statistics data for US employment; and so on. Published scientific journal articles, especially from reputable journals and large randomized studies.\n\nYou explicitly do _not_ look for opinion, even expert opinion. You avoid news, and you're wary of think-tank white papers. You're looking for raw information. You are taking a _sola scriptura_ approach, for better and for worse.\n\nAnd then you start letting the data show you things. \n\nYou see things that are surprising or odd, and you note that. \n\nYou see facts that seem to be inconsistent with each other, and you look into the data sources and methodology until you clear up the mystery.\n\nYou orient _towards_ the random, the unfamiliar, the things that are totally unfamiliar to your experience. One of the major exports of Germany is _valves_?  When was the last time I even thought about valves? _Why_ valves, what do you use valves in?  OK, show me a list of all the different kinds of machine parts, by percent of total exports.  \n\nAnd so, you dig in a little bit, to this part of the world that you hadn't looked at before. You cultivate the ability to spin up a lightweight sort of fannish obsessive curiosity when something seems like it might be a big deal.\n\nAnd you take casual notes and impressions (though keeping track of all the numbers and their sources in your notes).\n\nYou do a little bit of arithmetic to compare things to familiar reference points. How does this source of risk compare to the risk of smoking or going horseback riding? How does the effect size of this drug compare to the effect size of psychotherapy?\n\nYou don't really want to do _statistics_. You might take percents, means, standard deviations, maybe a Cohen's _d_ here and there, but nothing fancy.  You're just trying to figure out what's going on.\n\nIt's often a good idea to rank things by raw scale. What is responsible for the bulk of deaths, the bulk of money moved, etc? What is _big_?  Then pay attention more to things, and ask more questions about things, that are _big._ (Or disproportionately high-impact.)\n\nYou may find that this process gives you contrarian beliefs, but often you won't, you'll just have a strongly fact-based assessment of _why_ you believe the usual thing.  \n\nThere's a quality of _ordinariness_ about fact-based beliefs. It's not that they're never surprising -- they often are. But if you do fact-checking frequently enough, you begin to have a sense of the world overall that _stays in place_, even as you discover new facts, instead of swinging wildly around at every new stimulus.  For example, after doing lots and lots of reading of the biomedical literature, I have sort of a \"sense of the world\" of biomedical science -- what sorts of things I expect to see, and what sorts of things I don't. My \"sense of the world\" isn't that the _world itself_ is boring -- I actually believe in a world rich in discoveries and low-hanging fruit -- but the sense _itself_ has stabilized, feels like \"yeah, that's how things are\" rather than \"omg what is even going on.\"\n\nIn areas where I'm less familiar, I feel more like \"omg what is even going on\", which sometimes motivates me to go accumulate facts.\n\nOnce you've accumulated a bunch of facts, and they've \"spoken to you\" with some conclusions or answers to your question, you write them up on a blog, so that other people can check your reasoning.  If your mind gets changed, or you learn more, you write a follow-up post. You should, on any topic where you continue to learn over time, feel embarrassed by the naivety of your early posts.  This is fine. This is how learning works.\n\nThe advantage of fact posts is that they give you the ability to form independent opinions based on evidence. It's a sort of practice of the skill of seeing. They likely aren't the optimal way to get the most accurate beliefs -- listening to the best experts would almost certainly be better -- but you, personally, may not know who the best experts are, or may be overwhelmed by the swirl of controversy. Fact posts give you a relatively low-effort way of coming to informed opinions. They make you into the proverbial 'educated layman.'\n\nBeing an 'educated layman' makes you much more fertile in generating ideas, for research, business, fiction, or anything else. Having facts floating around in your head means you'll naturally think of problems to solve, questions to ask, opportunities to fix things in the world, applications for your technical skills.\n\nIdeally, a _group _of people writing fact posts on related topics, could learn from each other, and share how they think. I have the strong intuition that this is valuable. It's a bit more active than a \"journal club\", and quite a bit more casual than \"research\".  It's just the activity of learning and showing one's work in public."
    },
    "voteCount": 142,
    "forceInclude": true
  },
  {
    "_id": "FMkQtPvzsriQAow5q",
    "url": null,
    "title": "The correct response to uncertainty is *not* half-speed",
    "slug": "the-correct-response-to-uncertainty-is-not-half-speed",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "Planning & Decision-Making"
      },
      {
        "name": "Decision Theory"
      },
      {
        "name": "Goal Factoring"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "Related to: [Half-assing it with everything you've got](http://mindingourway.com/half-assing-it-with-everything-youve-got/); [Wasted motion](https://www.facebook.com/yudkowsky/posts/10151706498254228); [Say it Loud](/lw/u3/say_it_loud/).\n\nOnce upon a time (true story), I was on my way to a hotel in a new city.  I knew the hotel was many miles down this long, branchless road.  So I drove for a long while.\n\n![](http://images.lesswrong.com/t3_n6a_2.png?v=e38167d22348b22c2098faf0d51c7a61)\n\nAfter a while, I began to worry I had passed the hotel.\n\n![](http://images.lesswrong.com/t3_n6a_3.png?v=581d1be9b5846545acb660fa23f5322a)\n\nSo, instead of proceeding at 60 miles per hour the way I had been, I continued in the same direction for several more minutes at 30 miles per hour, wondering if I should keep going or turn around.\n\n![](http://images.lesswrong.com/t3_n6a_4.png?v=22e7e9bc8ebb443b87ed6b81ed3e6df7)\n\nAfter a while, I realized: I was being silly!  If the hotel was ahead of me, I'd get there fastest if I kept going 60mph.  And if the hotel was behind me, I'd get there fastest by heading at 60 miles per hour in the other direction.  And if I _wasn't_ going to turn around yet -- if my best bet given the uncertainty was to check _N_ more miles of highway first, _before_ I turned around -- then, again, I'd get there fastest by choosing a value of _N_, speeding along at 60 miles per hour until my odometer said I'd gone _N_ miles, and _then_ turning around and heading at 60 miles per hour in the opposite direction.  \n\n  \n\nEither way, fullspeed was best.  My mind had been naively averaging two courses of action -- the thought was something like: \"maybe I should go forward, and maybe I should go backward.  So, since I'm uncertain, I should go forward at half-speed!\"  But averages don't _actually_ work that way.\\[1\\]\n\n  \n\nFollowing this, I started noticing lots of hotels in my life (and, perhaps less tactfully, in my friends' lives).  For example:\n\n*   I wasn't sure if I was a good enough writer to write a given doc myself, or if I should try to outsource it.  So, I [sat there kind-of-writing it](/lw/jad/attempted_telekinesis/) while also fretting about whether the task was correct.\n    *   (Solution:  Take a minute out to think through heuristics.  Then, either: (1) write the post at full speed; or (2) try to outsource it; or (3) write full force _for some fixed time period_, and then pause and evaluate.)\n*   I wasn't sure (back in early 2012) that CFAR was worthwhile.  So, I kind-of worked on it.\n*   An old friend came to my door unexpectedly, and I was tempted to hang out with her, but I also thought I should finish my work.  So I kind-of hung out with her while feeling bad and distracted about my work.\n*   A friend of mine, when teaching me math, seems to mumble _specifically those words that he doesn't expect me to understand_ (in a sort of compromise between saying them and not saying them)...\n*   Duncan [reports](http://thirdfoundation.github.io/#/blog/quittability) that novice Parkour students are unable to safely undertake certain sorts of jumps, because they risk aborting the move mid-stream, after the _actual_ last safe stopping point (apparently kind-of-attempting these jumps is more dangerous than either attempting, or not attempting the jumps)\n*   It is said that start-up founders need to be irrationally certain that their startup will succeed, lest they be unable to do more than kind-of work on it...\n\n  \n\nThat is, it seems to me that often there are two different actions that would make sense under two different models, and we are uncertain which model is true... and so we find ourselves taking an intermediate of half-speed action... even when that action makes no sense under any probabilistic mixture of the two models.\n\n  \n\n![](http://images.lesswrong.com/t3_n6a_5.png?v=a8df41062ca5967c7f5e1cfb195fc243)  \n\n  \n\nYou might try looking out for such examples in your life.\n\n  \n\n* * *\n\n**\\[1\\] Edited to add:** The hotel example has received much nitpicking in the comments.  But: (A) the actual example was legit, I think.  Yes, stopping to think has some legitimacy, but driving slowly for a long time because uncertain does not optimize for thinking.  Similarly, it may make sense to drive slowly to stare at the buildings in some contexts... but I was on a very long empty country road, with no buildings anywhere (true historical fact), and also I was not squinting carefully at the scenery.  The thing I needed to do was to execute an efficient search pattern, with a threshold for a future time at which to switch from full-speed in some direction to full-speed in the other.  Also: (B) consider some of the other examples; \"kind of working\", \"kind of hanging out with my friend\", etc. seem to be common behaviors that are mostly not all that useful in the usual case."
    },
    "voteCount": 132,
    "forceInclude": true
  },
  {
    "_id": "w5F4w8tNZc6LcBKRP",
    "url": null,
    "title": "On learning difficult things",
    "slug": "on-learning-difficult-things",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "Scholarship & Learning"
      },
      {
        "name": "Practical"
      }
    ],
    "tableOfContents": {
      "sections": [
        {
          "title": "Pair up",
          "anchor": "Pair_up",
          "level": 1
        },
        {
          "title": "Read, reread, rereread",
          "anchor": "Read__reread__rereread",
          "level": 1
        },
        {
          "title": "Cognitive exchange rates",
          "anchor": "Cognitive_exchange_rates",
          "level": 1
        },
        {
          "title": "Explain it to someone",
          "anchor": "Explain_it_to_someone",
          "level": 1
        },
        {
          "title": "Don't book yourself solid",
          "anchor": "Don_t_book_yourself_solid",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        },
        {
          "anchor": "comments",
          "level": 0,
          "title": "45 comments"
        }
      ],
      "headingsCount": 7
    },
    "contents": {
      "markdown": "I have been autodidacting quite a bit lately. You may have seen my [reviews](/lw/ixn/very_basic_model_theory/) of books on the [MIRI course list](http://intelligence.org/courses). I've been going for about ten weeks now. This post contains my notes about the experience thus far.\n\nMuch of this may seem obvious, and would have seemed obvious if somebody had told me in advance. But nobody told me in advance. As such, this is a collection of things that were somewhat surprising at the time.\n\nPart of the reason I'm posting this is because I don't know a lot of autodidacts, and I'm not sure how normal any of my experiences are. (Though on average, I'd guess they're about average.) As always, keep in mind that I am only one person and that your mileage may vary.\n\nPair up\n-------\n\nWhen I began my quest for more knowledge, I figured that in this modern era, a well-written textbook and an account on [math.stackexchange](/math.stackexchange.com) would be enough to get me through anything. And I was right… sort of.\n\nBut not really.\n\nThe problem is, most of the time that I get stuck, I get stuck on something incredibly stupid. I've either misread something somewhere or misremembered a concept from earlier in the book. Usually, someone looking over my shoulder could correct me in ten seconds with three words.\n\n\"Dude. Disjunction. _Dis_junction_._\"\n\nThese are the things that eat my days.\n\nIn principle, places like stackexchange can get me unstuck, but they're an awkward tool for the job. First of all, my stupid mistakes are heavily contextualized. A full context dump is necessary before I can even ask my question, and this takes time. Furthermore, I feel dumb asking stupid questions on stackexchange-type sites. My questions are usually things that I can figure out with a close re-read (except, I'm not sure which part needs a re-read). I usually opt for a close re-read of everything rather than asking for help. This is even more time consuming.\n\nThe infuriating thing is that answering these questions usually doesn't require someone who already knows the answers: it just requires someone who didn't make exactly the same mistakes as me. I lose hours on little mistakes that could have been fixed within seconds if I was doing this with someone else.\n\nThat's why my number one piece of advice for other people attempting to learn on their own is _do it with a friend_. They don't need to be more knowledgeable than you to answer most of the questions that come up. They just need to make _different_ misunderstandings, and you'll be able to correct each other as you go along.\n\nThe thing I miss most about college is tight feedback loops while learning. When autodidacting, the feedback loop can be long.\n\nI still haven't managed to follow my own advice here. I'm writing this advice in part because it should motivate me to actually pair up. Unfortunately, there is nobody in my immediate circle who has the time or patience to read along with me, but there are a number of resources I have not yet explored (the LessWrong study hall, for example, or soliciting to actual mathematicians). It's on my list of things to do.\n\nRead, reread, rereread\n----------------------\n\nReading _Model Theory_ was one of the hardest things I've done. Not necessarily because the content was hard, but because it was the first time I actually learned something that was way outside my comfort zone.\n\nThe short version is that _Basic Category Theory_ and _Naïve Set Theory_ left me somewhat overconfident, and that I should have read a formal logic textbook before diving in. I had basic familiarity with logic, but no practice. Turns out practice is important.\n\nAnyway, it's not like _Model Theory_ was impossible just because I skipped my logic exercises. It was just _hard_. There are a number of little misconceptions you have when you're familiar with something but you've never applied it, and I found myself having to clean those out just to understand what _Model Theory_ was trying to say to me.\n\nIn retrospect, this was an efficient way to strengthen my understanding of mathematical logic and learn _Model Theory_ at the same time. (I've moved on to a logic textbook, and it's been a cakewalk.) That said, I wouldn't wish the experience on others.\n\nIn the process, I learned how to learn things that are way outside my comfort zone. In the past, all the stuff I've learned has been either easy, or an extension of things that I was already interested in and experienced with. Reading _Model Theory_ was the first time in my life where I read a chapter of a textbook and it made _absolutely no sense_. In fact, it took about three passes per chapter before they made sense.\n\n1.  The first pass was barely sufficient to understand all the words and symbols. I constantly had to go research a topic. I followed proofs one step at a time, able to verify the validity of each step but not really understand what was going on. I came out the other end believing the results, but not knowing them.\n2.  Another pass was required to figure out what the book was actually trying to say to me. Once all the words made sense and I was comfortable with their usage, the second pass allowed me to see what the theorems and proofs were actually saying. This was nice, but it still wasn't sufficient: I understood the theorems, but they seemed like a random walk through theorem-space. I couldn't yet understand why anyone would say those particular things on purpose.\n3.  The third pass was necessary to understand the greater theory. I've never been particularly good at memorizing things, and it's not sufficient for me to believe and memorize a theorem. If it's going to stick, I have to understand why it's important. I have to understand why this theorem in particular is being stated, rather than another. I have to understand the problem that's being solved. A third pass was necessary to figure out the context in which the text made sense.\n\nAfter a third pass of any given chapter, the next chapter didn't seem quite so random. When the upcoming content started feeling like a natural progression instead of a random walk, I knew I was making progress.\n\nI note this because this is the first time that I had to read a math text more than once to understand what was going on. I'm not talking about individual sentences or paragraphs, I'm talking about finishing a chapter, feeling like \"wat\", and then starting the whole chapter over. Twice.\n\nI'm not sure if I'm being naïve (for never having needed to do this before) or slow (for having to do this for _Model Theory_), but I did not anticipate requiring three passes. Mostly, I didn't anticipate gaining as much as I did from a re-read; I would have guessed that something opaque on the first pass would remain opaque on a second pass.\n\nThis, I'm pretty sure, was naïvety.\n\nSo take note: if you stumble upon something that feels very hard, it might be more useful than anticipated to re-read it.\n\nCognitive exchange rates\n------------------------\n\nWhen reading Model Theory, I was only able to convert 30-50% of my allotted \"study time\" into actual study.\n\nThis is somewhat surprising, as I had no such troubles with _Basic Category Theory_ or _Naïve Set Theory_.\n\n(I often have the _opposite_ problem when writing code; this is probably due to the different reward structure.)\n\nI was somewhat frustrated with my inability to study as much as I would have liked. My usual time-into-studying conversion rate is much higher (I'd guess 80%ish, though I haven't been measuring).\n\nI'm not sure what factor made it harder for me to study model theory. I don't think it was the difficulty directly, as I often tend to work harder in the face of a challenge. I'd guess that it was either the slower rate of rewards (caused by a slower pace of learning) or actual cognitive exhaustion.\n\nIn the vein of cognitive exhaustion, there were a few times while reading _Model Theory_ where I seem to have become cognitively exhausted before becoming physically exhausted. This was a first for me. I'm not referring to those times when you've done a lot of mental work and you shy away from doing anything difficult, that's happened to me plenty. Rather, in this case, I felt fully awake and ready to keep reading. And I did keep reading. It just… didn't work. I'd have trouble following simple proofs. I'd fail at parsing sentences that were quite clear after resting.\n\nI'm still not sure what to make of this, and I don't have sufficient data to draw conclusions. However, it seems like there are mental states where my I feel awake and able to continue, but my mind is just not capable of doing the heavy lifting.\n\nAgain, the fact that I'm only just realizing this now is probably naïvety, but it's something to remember before getting frustrated with yourself.\n\nExplain it to someone\n---------------------\n\nAs I've said before, one of the best ways to learn something is to do the problem sets. For _Model Theory_, though, there were times when I finished reading through a chapter and was not capable of doing the problems.\n\nRe-reading helped, as mentioned above. Another thing that helped was explaining the concepts.\n\nI explained model theory pretty extensively to a text file on my computer. I sketched the proofs in my own words and stated their significance. I explained the syntax being used. I tried to motivate each idea. (The notes are still lying around somewhere; I haven't posted them because they're pretty much a derivative work at this point.)\n\nI found that this went a _long_ way towards helping me track down places where I'd thought I learned something, but actually hadn't. If you're having trouble, go explain the concept to somebody (or to a text file). This can bridge the gap between \"I read it\" and \"I can do the problems\" quite well. For me, this technique often took problems from \"unapproachable\" to \"easy\" in one fell swoop.\n\nDon't book yourself solid\n-------------------------\n\nI'm pretty good at avoiding stress. I have the (apparently rare) ability to drop all work-related concerns at the door when I leave. I don't even know _how_ to get stressed by bad luck, especially if I made good choices given the information I had at the time. I get normally tense in stressful situations with time constraints, but I'm adept at avoiding the permastress that I've seen plague friends and family — unless I've booked myself solid.\n\nI've had a packed schedule these past few weeks. I try to move the needle on at least two projects a day (more on weekends). Even if it's entirely reasonable to fit all these things into my schedule, I have not yet found a way to avoid the stress.\n\nEven when I know that, if I push myself, I can read this much and write that much and code this feature all in one day, I haven't found a good way to push myself without pressure-stress.\n\nI'm still hoping that I'll learn how to move quickly without stress as I learn my capabilities, but I'm not sure I've been adequately accounting for the [cost of stress](http://scholar.google.com/scholar?q=adverse+effects+of+stress&hl=en&as_sdt=0&as_vis=1&oi=scholart&sa=X&ei=KE6AUuGTEqerigLmyoHADw&ved=0CCwQgQMwAA).\n\nIt's worth remembering that doing less than you're capable of _on purpose _might be a good strategy for maximizing long-term output.\n\n* * *\n\nThere you go. Those are my notes gathered from trying to learn lots of things very quickly (and trying to learn one hard thing in particular). Comments are encouraged; I am by no means an expert."
    },
    "voteCount": 121,
    "forceInclude": true
  },
  {
    "_id": "895quRDaK6gR2rM82",
    "url": null,
    "title": "Diseased thinking: dissolving questions about disease",
    "slug": "diseased-thinking-dissolving-questions-about-disease",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "Rationality"
      },
      {
        "name": "Health / Medicine / Disease"
      },
      {
        "name": "Philosophy of Language"
      },
      {
        "name": "Carving / Clustering Reality"
      },
      {
        "name": "Reversal Test"
      },
      {
        "name": "Motivational Intro Posts"
      }
    ],
    "tableOfContents": {
      "sections": [
        {
          "title": "What is Disease?",
          "anchor": "What_is_Disease_",
          "level": 1
        },
        {
          "title": "Hidden Inferences From Disease Concept",
          "anchor": "Hidden_Inferences_From_Disease_Concept",
          "level": 1
        },
        {
          "title": "Sympathy or Condemnation?",
          "anchor": "Sympathy_or_Condemnation_",
          "level": 1
        },
        {
          "title": "The Ethics of Treating Marginal Conditions",
          "anchor": "The_Ethics_of_Treating_Marginal_Conditions",
          "level": 1
        },
        {
          "title": "Summary",
          "anchor": "Summary",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        },
        {
          "anchor": "comments",
          "level": 0,
          "title": "354 comments"
        }
      ],
      "headingsCount": 7
    },
    "contents": {
      "markdown": "**Related to:** [Disguised Queries](https://www.lesswrong.com/lw/nm/disguised_queries/), [Words as Hidden Inferences](https://www.lesswrong.com/lw/ng/words_as_hidden_inferences/), [Dissolving the Question](https://www.lesswrong.com/lw/of/dissolving_the_question/), [Eight Short Studies on Excuses](https://www.lesswrong.com/lw/24o/eight_short_studies_on_excuses/)\n\n> _Today's therapeutic ethos, which celebrates curing and disparages judging, expresses the liberal disposition to assume that crime and other problematic behaviors reflect social or biological causation. While this absolves the individual of responsibility, it also strips the individual of personhood, and moral dignity_\n\n_\\-\\- George Will, [townhall.com](http://townhall.com/Common/PrintPage.aspx?g=761ecc84-473b-4123-bf28-c4fc179a9d3f&t=c)_\n\nSandy is a morbidly obese woman looking for advice.\n\nHer husband has no sympathy for her, and tells her she obviously needs to stop eating like a pig, and would it kill her to go to the gym once in a while?\n\nHer doctor tells her that obesity is primarily genetic, and recommends the diet pill orlistat and a consultation with a surgeon about gastric bypass.\n\nHer sister tells her that obesity is a perfectly valid lifestyle choice, and that fat-ism, equivalent to racism, is society's way of keeping her down.\n\nWhen she tells each of her friends about the opinions of the others, things really start to heat up.\n\nHer husband accuses her doctor and sister of absolving her of personal responsibility with feel-good platitudes that in the end will only prevent her from getting the willpower she needs to start a real diet.\n\nHer doctor accuses her husband of ignorance of the real causes of obesity and of the most effective treatments, and accuses her sister of legitimizing a dangerous health risk that could end with Sandy in hospital or even dead.\n\nHer sister accuses her husband of being a jerk, and her doctor of trying to medicalize her behavior in order to turn it into a \"condition\" that will keep her on pills for life and make lots of money for Big Pharma.\n\nSandy is fictional, but similar conversations happen every day, not only about obesity but about a host of other marginal conditions that some consider character flaws, others diseases, and still others normal variation in the human condition. Attention deficit disorder, internet addiction, social anxiety disorder (as one skeptic said, didn't we used to call this \"shyness\"?), alcoholism, chronic fatigue, oppositional defiant disorder (\"didn't we used to call this being a teenager?\"), compulsive gambling, homosexuality, Aspergers' syndrome, antisocial personality, even depression have all been placed in two or more of these categories by different people.\n\n  \n\nSandy's sister may have a point, but this post will concentrate on the debate between her husband and her doctor, with the understanding that the same techniques will apply to evaluating her sister's opinion. The disagreement between Sandy's husband and doctor centers around the idea of \"disease\". If obesity, depression, alcoholism, and the like are diseases, most people default to the doctor's point of view; if they are not diseases, they tend to agree with the husband.\n\nThe debate over such marginal conditions is in many ways a debate over whether or not they are \"real\" diseases. The usual surface level arguments trotted out in favor of or against the proposition are generally inconclusive, but this post will apply a host of techniques previously discussed on Less Wrong to illuminate the issue.\n\n  \n\n**What is Disease?**\n\n  \n\nIn [Disguised Queries](https://www.lesswrong.com/lw/nm/disguised_queries/) , Eliezer demonstrates how a word refers to a cluster of objects related upon multiple axes. For example, in a company that sorts red smooth translucent cubes full of vanadium from blue furry opaque eggs full of palladium, you might invent the word \"rube\" to designate the red cubes, and another \"blegg\", to designate the blue eggs. Both words are useful because they \"carve reality at the joints\" - they refer to two completely separate classes of things which it's practically useful to keep in separate categories. Calling something a \"blegg\" is a quick and easy way to describe its color, shape, opacity, texture, and chemical composition. It may be that the odd blegg might be purple rather than blue, but in general the characteristics of a blegg remain sufficiently correlated that \"blegg\" is a useful word. If they weren't so correlated - if blue objects were equally likely to be palladium-containing-cubes as vanadium-containing-eggs, then the word \"blegg\" would be a waste of breath; the characteristics of the object would remain just as mysterious to your partner after you said \"blegg\" as they were before.\n\n\"Disease\", like \"blegg\", suggests that certain characteristics always come together. A rough sketch of some of the characteristics we expect in a disease might include:\n\n1\\. Something caused by the sorts of thing you study in biology: proteins, bacteria, ions, viruses, genes.\n\n2\\. Something involuntary and completely immune to the operations of free will\n\n3\\. Something rare; the vast majority of people don't have it\n\n4\\. Something unpleasant; when you have it, you want to get rid of it\n\n5\\. Something discrete; a graph would show two widely separate populations, one with the disease and one without, and not a normal distribution.\n\n6\\. Something commonly treated with science-y interventions like chemicals and radiation.\n\nCancer satisfies every one of these criteria, and so we have no qualms whatsoever about classifying it as a disease. It's a type specimen, [the sparrow as opposed to the ostrich](https://www.lesswrong.com/lw/nl/the_cluster_structure_of_thingspace/). The same is true of heart attack, the flu, diabetes, and many more.\n\nSome conditions satisfy a few of the criteria, but not others. Dwarfism seems to fail (5), and it might get its status as a disease only after studies show that the supposed dwarf falls way out of normal human height variation. Despite the best efforts of transhumanists, it's hard to convince people that aging is a disease, partly because it fails (3). Calling homosexuality a disease is a poor choice for many reasons, but one of them is certainly (4): it's not necessarily unpleasant.\n\nThe marginal conditions mentioned above are also in this category. Obesity arguably sort-of-satisfies criteria (1), (4), and (6), but it would be pretty hard to make a case for (2), (3), and (5).\n\nSo, is obesity really a disease? Well, is Pluto really a planet? Once we state that obesity satisfies some of the criteria but not others, it is meaningless to talk about an additional fact of whether it \"really deserves to be a disease\" or not.\n\nIf it weren't for those pesky [hidden inferences](https://www.lesswrong.com/lw/ng/words_as_hidden_inferences/#more)...\n\n**Hidden Inferences From Disease Concept**\n\nThe state of the disease node, meaningless in itself, is used to predict several other nodes with non-empirical content. In English: we make value decisions based on whether we call something a \"disease\" or not.\n\nIf something is a real disease, the patient deserves our sympathy and support; for example, [cancer sufferers must universally be described as \"brave\"](http://www.theonion.com/articles/loved-ones-recall-local-mans-cowardly-battle-with,772/). If it is not a real disease, people are more likely to get our condemnation; for example Sandy's husband who calls her a \"pig\" for her inability to control her eating habits. The difference between \"shyness\" and \"social anxiety disorder\" is that people with the first get called \"weird\" and told to man up, and people with the second get special privileges and the sympathy of those around them.\n\nAnd if something is a real disease, it is socially acceptable (maybe even mandated) to seek medical treatment for it. If it's not a disease, medical treatment gets derided as a \"quick fix\" or an \"abdication of personal responsibility\". I have talked to several doctors who are uncomfortable suggesting gastric bypass surgery, even in people for whom it is medically indicated, because they believe it is morally wrong to turn to medicine to solve a character issue.\n\n![](http://res.cloudinary.com/lesswrong-2-0/image/upload/v1531709899/disease_first_tlnfio.gif)\n\nWhile a condition's status as a \"real disease\" ought to be meaningless as a \"hanging node\" after the status of all other nodes have been determined, it has acquired political and philosophical implications because of its role in determining whether patients receive sympathy and whether they are permitted to seek medical treatment.\n\nIf we can determine whether a person should get sympathy, and whether they should be allowed to seek medical treatment, independently of the central node \"disease\" or of the criteria that feed into it, we will have successfully unasked the question \"are these marginal conditions real diseases\" and cleared up the confusion.\n\n**Sympathy or Condemnation?**\n\nOur attitudes toward people with marginal conditions mainly reflect a deontologist libertarian (libertarian as in \"free will\", not as in \"against government\") model of blame. In this concept, people make decisions using their free will, a spiritual entity operating free from biology or circumstance. People who make good decisions are intrinsically good people and deserve good treatment; people who make bad decisions are intrinsically bad people and deserve bad treatment. But people who make bad decisions for reasons that are outside of their free will may not be intrinsically bad people, and may therefore be absolved from deserving bad treatment. For example, if a normally peaceful person has a brain tumor that affects areas involved in fear and aggression, they go on a crazy killing spree, and then they have their brain tumor removed and become a peaceful person again, many people would be willing to accept that the killing spree does not reflect negatively on them or open them up to deserving bad treatment, since it had biological and not spiritual causes.\n\nUnder this model, deciding whether a condition is biological or spiritual becomes very important, and the rationale for worrying over whether something \"is a real disease\" or not is plain to see. Without figuring out this extremely difficult question, we are at risk of either blaming people for things they don't deserve, or else letting them off the hook when they commit a sin, both of which, to libertarian deontologists, would be terrible things. But determining whether marginal conditions like depression have a spiritual or biological cause is difficult, and no one knows how to do it reliably.\n\nDeterminist consequentialists can do better. We believe it's biology all the way down. Separating spiritual from biological illnesses is impossible and unnecessary. Every condition, from brain tumors to poor taste in music, is \"biological\" insofar as it is encoded in things like cells and proteins and follows laws based on their structure.\n\nBut determinists don't just ignore the very important differences between brain tumors and poor taste in music. Some biological phenomena, like poor taste in music, are encoded in such a way that they are extremely vulnerable to what we can call social influences: praise, condemnation, introspection, and the like. Other biological phenomena, like brain tumors, are completely immune to such influences. This allows us to develop a more useful model of blame.\n\nThe consequentialist model of blame is very different from the deontological model. Because all actions are biologically determined, none are more or less metaphysically blameworthy than others, and none can mark anyone with the metaphysical status of \"bad person\" and make them \"deserve\" bad treatment. Consequentialists don't on a primary level want anyone to be treated badly, full stop; thus [is it written](http://yudkowsky.net/obsolete/tmol-faq.html#theo_free): \"Saddam Hussein doesn't deserve so much as a stubbed toe.\" But if consequentialists don't believe in punishment for its own sake, they do believe in punishment for the sake of, well, consequences. Hurting bank robbers may not be a good in and of itself, but it will prevent banks from being robbed in the future. And, one might infer, although alcoholics may not deserve condemnation, societal condemnation of alcoholics makes alcoholism a less attractive option.\n\nSo here, at last, is a rule for which diseases we offer sympathy, and which we offer condemnation: if giving condemnation instead of sympathy decreases the incidence of the disease enough to be worth the hurt feelings, condemn; otherwise, sympathize. Though the rule is based on philosophy that the majority of the human race would disavow, it leads to intuitively correct consequences. Yelling at a cancer patient, shouting \"How dare you allow your cells to divide in an uncontrolled manner like this; is that the way your mother raised you??!\" will probably make the patient feel pretty awful, but it's not going to cure the cancer. Telling a lazy person \"Get up and do some work, you worthless bum,\" very well might cure the laziness. The cancer is a biological condition immune to social influences; the laziness is a biological condition susceptible to social influences, so we try to socially influence the laziness and not the cancer.\n\nThe question \"Do the obese deserve our sympathy or our condemnation,\" then, is asking whether condemnation is such a useful treatment for obesity that its utility outweights the disutility of hurting obese people's feelings. This question may have different answers depending on the particular obese person involved, the particular person doing the condemning, and the availability of other methods for treating the obesity, which brings us to...\n\n**The Ethics of Treating Marginal Conditions**\n\nIf a condition is susceptible to social intervention, but an effective biological therapy for it also exists, is it okay for people to use the biological therapy instead of figuring out a social solution? My gut answer is \"Of course, why wouldn't it be?\", but apparently lots of people find this controversial for some reason.\n\nIn a libertarian deontological system, throwing biological solutions at spiritual problems might be disrespectful or dehumanizing, or a band-aid that doesn't affect the deeper problem. To someone who believes it's biology all the way down, this is much less of a concern.\n\nOthers complain that the existence of an easy medical solution prevents people from learning personal responsibility. But here [we see the status-quo bias at work, and so can apply a preference reversal test](http://www.nickbostrom.com/ethics/statusquo.pdf). If people really believe learning personal responsibility is more important than being not addicted to heroin, we would expect these people to support deliberately addicting schoolchildren to heroin so they can develop personal responsibility by coming off of it. Anyone who disagrees with this somewhat shocking proposal must believe, on some level, that having people who are not addicted to heroin is more important than having people develop whatever measure of personal responsibility comes from kicking their heroin habit the old-fashioned way.\n\nBut the most convincing explanation I have read for why so many people are opposed to medical solutions for social conditions is a signaling explanation by Robin Hans...wait! no!...by Katja Grace. On [her blog](http://meteuphoric.wordpress.com/2009/09/21/why-do-animal-lovers-want-animals-to-feel-pain/), she says:\n\n> _...the situation reminds me of a pattern in similar cases I have noticed before. It goes like this. Some people make personal sacrifices, supposedly toward solving problems that don’t threaten them personally. They sort recycling, buy free range eggs, buy fair trade, campaign for wealth redistribution etc. Their actions are seen as virtuous. They see those who don’t join them as uncaring and immoral. A more efficient solution to the problem is suggested. It does not require personal sacrifice. People who have not previously sacrificed support it. Those who have previously sacrificed object on grounds that it is an excuse for people to get out of making the sacrifice. The supposed instrumental action, as the visible sign of caring, has become virtuous in its own right. Solving the problem effectively is an attack on the moral people._\n\nA case in which some people eat less enjoyable foods and exercise hard to avoid becoming obese, and then campaign against a pill that makes avoiding obesity easy demonstrates some of the same principles.\n\nThere are several very reasonable objections to treating any condition with drugs, whether it be a classical disease like cancer or a marginal condition like alcoholism. The drugs can have side effects. They can be expensive. They can build dependence. They may later be found to be placebos whose efficacy was overhyped by dishonest pharmaceutical advertising.. They may raise ethical issues with children, the mentally incapacitated, and other people who cannot decide for themselves whether or not to take them. But these issues do not magically become more dangerous in conditions typically regarded as \"character flaws\" rather than \"diseases\", and the same good-enough solutions that work for cancer or heart disease will work for alcoholism and other such conditions (but see [here](https://www.lesswrong.com/lw/2as/diseased_thinking_dissolving_questions_about/230p)).\n\nI see no reason why people who want effective treatment for a condition should be denied it or stigmatized for seeking it, whether it is traditionally considered \"medical\" or not.\n\n**Summary**\n\nPeople commonly debate whether social and mental conditions are real diseases. This masquerades as a medical question, but its implications are mainly social and ethical. We use the concept of disease to decide who gets sympathy, who gets blame, and who gets treatment.\n\nInstead of continuing the fruitless \"disease\" argument, we should address these questions directly. Taking a determinist consequentialist position allows us to do so more effectively. We should blame and stigmatize people for conditions where blame and stigma are the most useful methods for curing or preventing the condition, and we should allow patients to seek treatment whenever it is available and effective."
    },
    "voteCount": 367,
    "forceInclude": true
  },
  {
    "_id": "SGR4GxFK7KmW7ckCB",
    "url": null,
    "title": "Something to Protect",
    "slug": "something-to-protect",
    "author": "Eliezer_Yudkowsky",
    "question": false,
    "tags": [
      {
        "name": "World Optimization"
      },
      {
        "name": "Rationality"
      },
      {
        "name": "Motivations"
      },
      {
        "name": "Heroic Responsibility"
      },
      {
        "name": "Something To Protect"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "In the gestalt of (ahem) [Japanese](/lw/m7/zen_and_the_art_of_rationality/) [fiction](/lw/k9/the_logical_fallacy_of_generalization_from/), one finds this oft-repeated motif:  Power comes from having something to protect.\n\nI'm not just talking about superheroes that power up when a friend is threatened, the way it works in Western fiction.  In the Japanese version it runs deeper than that.\n\nIn the _X_ saga it's explicitly stated that each of the good guys draw their power from having someone—one person—who they want to protect.  Who?  That question is part of _X_'s plot—the \"most precious person\" isn't always who we think.  But if that person is killed, or hurt in the wrong way, the protector loses their power—not so much from magical backlash, as from simple despair.  This isn't something that happens once per week per good guy, the way it would work in a Western comic.  It's equivalent to being [Killed Off For Real](http://tvtropes.org/pmwiki/pmwiki.php/Main/KilledOffForReal)—taken off the game board.\n\nThe way it works in Western superhero comics is that the good guy gets bitten by a radioactive spider; and then he needs something to do with his powers, to keep him busy, so he decides to fight crime.  And then Western superheroes are always whining about how much time their superhero duties take up, and how they'd rather be ordinary mortals so they could go fishing or something.\n\nSimilarly, in Western real life, unhappy people are told that they need a \"purpose in life\", so they should pick out an altruistic cause that goes well with their personality, like picking out nice living-room drapes, and this will brighten up their days by adding some color, like nice living-room drapes.  You should be careful not to pick something too expensive, though.\n\nIn Western comics, the magic comes first, then the purpose:  Acquire amazing powers, decide to protect the innocent.  In Japanese fiction, often, it works the other way around.\n\nOf course I'm not saying all this to generalize from fictional evidence. But I want to convey a concept whose deceptively close Western analogue is _not_ what I mean.\n\nI have touched before on the idea that a rationalist must have something they value more than \"rationality\":  _The Art must have a purpose other than itself, or it collapses into infinite recursion._  But do not mistake me, and think I am advocating that rationalists should pick out a nice altruistic cause, by way of having something to do, because rationality isn't all that important by itself.  No.  I am asking:  Where do rationalists come from?  How do we acquire our powers?\n\nIt is written in the _Twelve Virtues of Rationality:_\n\n> How can you improve your conception of rationality?  Not by saying to yourself, \"It is my duty to be rational.\"  By this you only enshrine your mistaken conception.  Perhaps your conception of rationality is that it is rational to believe the words of the Great Teacher, and the Great Teacher says, \"The sky is green,\" and you look up at the sky and see blue.  If you think:  \"It may look like the sky is blue, but rationality is to believe the words of the Great Teacher,\" you lose a chance to discover your mistake.\n\nHistorically speaking, the way humanity _finally_ left the trap of authority and began paying attention to, y'know, the actual sky, was that beliefs based on experiment turned out to be _much more useful_ than beliefs based on authority.  Curiosity has been around since the dawn of humanity, but the problem is that spinning campfire tales works [just as well](/lw/go/why_truth_and/) for satisfying curiosity.\n\nHistorically speaking, science won because it displayed greater raw strength in the form of technology, not because science _sounded more reasonable_.  To this very day, magic and scripture still sound more reasonable to untrained ears than science.  That is why there is continuous social tension between the belief systems.  If science not only worked better than magic, but _also_ sounded more intuitively reasonable, it would have won _entirely_ by now.\n\nNow there are those who say:  \"How dare you suggest that anything should be valued more than Truth?  Must not a rationalist love Truth more than mere usefulness?\"\n\nForget for a moment what would have happened historically to someone like that—that people in pretty much that frame of mind defended the Bible because they loved Truth more than mere accuracy.  Propositional morality is a glorious thing, but it has [too many degrees of freedom](/lw/go/why_truth_and/).\n\nNo, the real point is that a rationalist's love affair with the Truth is, well, just _more complicated_ as an emotional relationship.\n\nOne doesn't become an adept rationalist without caring about the truth, both as a purely moral desideratum and as something that's fun to have.  I doubt there are many master composers who hate music.\n\nBut part of what I _like_ about rationality is the discipline imposed by requiring beliefs to yield predictions, which ends up taking us much closer to the truth than if we sat in the living room obsessing about Truth all day.  I _like_ the complexity of simultaneously having to love True-seeming ideas, and also being ready to drop them out the window at a moment's notice.  I even like the glorious aesthetic purity of declaring that I value mere usefulness above aesthetics.  That is almost a contradiction, but not quite; and that has an aesthetic quality as well, a delicious humor.\n\nAnd of course, no matter how much you profess your love of mere usefulness, you should never _actually_ end up [deliberately believing a useful false statement](/lw/je/doublethink_choosing_to_be_biased/).\n\nSo don't oversimplify the relationship between loving truth and loving usefulness.  It's not one or the other.  It's _complicated,_ which is not necessarily a defect in the moral aesthetics of [single events](/lw/n9/the_intuitions_behind_utilitarianism/).\n\nBut morality and aesthetics alone, believing that one ought to be \"rational\" or that certain ways of thinking are \"beautiful\", will not lead you to the center of the Way.  It wouldn't have gotten humanity out of the authority-hole.\n\nIn [Circular Altruism](/lw/n3/circular_altruism/), I discussed this dilemma:  Which of these options would you prefer:\n\n1.  Save 400 lives, with certainty\n2.  Save 500 lives, 90% probability; save no lives, 10% probability.\n\nYou may be tempted to grandstand, saying, \"How dare you gamble with people's lives?\"  Even if you, yourself, are one of the 500—but you don't know which one—you may still be tempted to rely on the comforting feeling of certainty, because our own lives are often worth less to us than a good [intuition](/lw/n9/the_intuitions_behind_utilitarianism/).\n\nBut if your precious daughter is one of the 500, and you don't know which one, _then,_ perhaps, you may feel more impelled to shut up and multiply—to notice that you have an 80% chance of saving her in the first case, and a 90% chance of saving her in the second.\n\nAnd yes, everyone in that crowd is someone's son or daughter.  Which, in turn, suggests that we should pick the second option as altruists, as well as concerned parents.\n\nMy point is not to suggest that one person's life is more valuable than 499 people.  What I am trying to say is that _more_ than your own life has to be at stake, before a person becomes desperate enough to resort to math.\n\nWhat if you believe that it is \"rational\" to choose the certainty of option 1?  Lots of people think that \"rationality\" is about choosing only methods that are certain to work, and rejecting all uncertainty.  But, hopefully, you care more about your daughter's life than about \"rationality\".\n\nWill pride in your own virtue as a rationalist save you?  Not if you believe that it is virtuous to choose certainty.  You will only be able to learn something about rationality if your daughter's life matters more to you than your pride as a rationalist.\n\nYou may even learn something about rationality from the experience, if you are already far enough grown in your Art to say, \"I must have had the wrong conception of rationality,\" and not, \"Look at how rationality gave me the wrong answer!\"\n\n(The essential difficulty in becoming a master rationalist is that you need quite a bit of rationality to bootstrap the learning process.)\n\nIs your belief that you ought to be rational, more important than your life?  Because, as I've previously observed, risking your life isn't comparatively all that scary.  Being [the lone voice of dissent](/lw/mb/lonely_dissent/) in the crowd and having everyone look at you funny is _much_ scarier than a mere threat to your life, according to the revealed preferences of teenagers who drink at parties and then drive home.  It will take something terribly important to make you willing to leave the pack.  A threat to your life won't be enough.\n\nIs your will to rationality stronger than your _pride?_  Can it be, if your will to rationality stems from your pride in your self-image as a rationalist?  It's helpful—_very_ helpful—to have a self-image which says that you are the sort of person who confronts harsh truth.  It's helpful to have too much self-respect to knowingly lie to yourself or refuse to face evidence.  But there may come a time when you have to admit that you've been doing rationality all wrong.  Then your pride, your self-image as a rationalist, may make that too hard to face.\n\nIf you've prided yourself on believing what the Great Teacher says—even when it seems harsh, even when you'd rather not—that may make it all the more bitter a pill to swallow, to admit that the Great Teacher is a fraud, and all your noble self-sacrifice was for naught.\n\nWhere do you get the will to keep moving forward?\n\nWhen I look back at my own personal journey toward rationality—not just humanity's historical journey—well, I grew up believing very strongly that I ought to be rational.  This made me an above-average Traditional Rationalist a la Feynman and Heinlein, and nothing more.  It did not drive me to go beyond the teachings I had received.  I only began to grow _further_ as a rationalist once I had something terribly important that I needed to do.  Something more important than my pride as a rationalist, never mind my life.\n\nOnly when you become more wedded to success than to any of your beloved techniques of rationality, do you begin to appreciate these words of Miyamoto Musashi:\n\n> \"You can win with a long weapon, and yet you can also win with a short weapon.  In short, the Way of the Ichi school is the spirit of winning, whatever the weapon and whatever its size.\"  \n>         —Miyamoto Musashi, _The Book of Five Rings_\n\nDon't mistake this for a specific teaching of rationality.  It describes how you _learn_ the Way, beginning with a desperate need to succeed.  No one masters the Way until more than their life is at stake.  More than their comfort, more even than their pride.\n\nYou can't just pick out a [Cause](/lw/lv/every_cause_wants_to_be_a_cult/) like that because you feel you need a hobby.  Go looking for a \"good cause\", and your mind will just fill in a [standard cliche](/lw/k8/how_to_seem_and_be_deep/).  [Learn how to multiply](/lw/n9/the_intuitions_behind_utilitarianism/), and perhaps you will recognize a drastically important cause when you see one.\n\nBut _if_ you have a cause like that, it is right and proper to wield your rationality in its service.\n\nTo strictly subordinate the aesthetics of rationality to a higher cause, is part of the aesthetic of rationality.  You should pay attention to that aesthetic:  You will never master rationality well enough to win with any weapon, if you do not appreciate the [beauty](/lw/mt/beautiful_probability/) for its own sake."
    },
    "voteCount": 117,
    "forceInclude": true
  },
  {
    "_id": "pGvyqAQw6yqTjpKf4",
    "url": null,
    "title": "The Gift We Give To Tomorrow",
    "slug": "the-gift-we-give-to-tomorrow",
    "author": "Eliezer_Yudkowsky",
    "question": false,
    "tags": [
      {
        "name": "Evolutionary Psychology"
      },
      {
        "name": "Complexity of Value"
      },
      {
        "name": "Human Values"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "How, oh how, did an unloving and mindless universe, cough up minds who were capable of love?\n\n\"No mystery in that,\" you say, \"it's just a matter of [natural selection](/lw/kr/an_alien_god/).\"\n\nBut natural selection is [cruel, bloody, and bloody stupid](/lw/kr/an_alien_god/).  Even when, on the surface of things, biological organisms aren't _directly_ fighting each other—aren't _directly_ tearing at each other with claws—there's still a deeper competition going on between the genes.  Genetic information is created when genes increase their _relative_ frequency in the next generation—what matters for \"genetic fitness\" is not how many children you have, but that you have _more_ children than others.  It is quite possible for a species to [evolve to extinction](/lw/l5/evolving_to_extinction/), if the winning genes are playing negative-sum games.\n\nHow, oh how, could such a process create beings capable of love?\n\n\"No mystery,\" you say, \"there is never any mystery-in-the-world; [mystery is a property of questions, not answers](/lw/iu/mysterious_answers_to_mysterious_questions/).  A mother's children share her genes, so the mother loves her children.\"\n\nBut sometimes mothers adopt children, and still love them.  And mothers love their children for themselves, not for their genes.\n\n\"No mystery,\" you say, \"Individual organisms are [adaptation-executers, not fitness-maximizers](/lw/l0/adaptationexecuters_not_fitnessmaximizers/).  [Evolutionary psychology](/lw/l1/evolutionary_psychology/) is not about deliberately maximizing fitness—through most of human history, we didn't know genes existed.  We don't calculate our acts' effect on genetic fitness consciously, or even subconsciously.\"\n\nBut human beings form friendships even with non-relatives: how, oh how, can it be?\n\n\"No mystery, for hunter-gatherers often play Iterated Prisoner's Dilemmas, the solution to which is reciprocal altruism.  Sometimes the most dangerous human in the tribe is not the strongest, the prettiest, or even the smartest, but the one who has the most allies.\"\n\nYet not all friends are fair-weather friends; we have a concept of true friendship—and some people have sacrificed their life for their friends.  Would not such a devotion tend to remove itself from the gene pool?\n\n\"You said it yourself: we have a concept of true friendship and fair-weather friendship.  We can tell, or try to tell, the difference between someone who considers us a valuable ally, and someone executing the friendship adaptation.  We wouldn't be true friends with someone who we didn't think was a true friend to us—and someone with many _true_ friends is far more formidable than someone with many fair-weather allies.\"\n\nAnd Mohandas Gandhi, who really did turn the other cheek?  Those who try to serve all humanity, whether or not all humanity serves them in turn?\n\n\"That perhaps is a more complicated story.  Human beings are not just social animals.  We are political animals who argue linguistically about policy in adaptive tribal contexts.  Sometimes the formidable human is not the strongest, but the one who can most skillfully argue that their preferred policies match the preferences of others.\"\n\nUm... that doesn't explain Gandhi, or am I missing something?\n\n\"The point is that we have the ability to _argue_ about 'What should be done?' as a _proposition_—we can make those arguments and respond to those arguments, without which politics could not take place.\"\n\nOkay, but Gandhi?\n\n\"Believed certain complicated propositions about 'What should be done?' and did them.\"\n\nThat sounds like it could [explain any possible](/lw/iq/guessing_the_teachers_password/) human behavior.\n\n\"If we traced back the chain of causality through all the arguments, it would involve: a moral architecture that had the ability to argue _general abstract_ moral propositions like 'What should be done to people?'; appeal to hardwired intuitions like fairness, a concept of duty, pain aversion + empathy; something like a preference for simple moral propositions, probably reused from our previous Occam prior; and the end result of all this, plus perhaps memetic selection effects, was 'You should not hurt people' in full generality—\"\n\nAnd that gets you Gandhi.\n\n\"Unless you think it was magic, it has to fit into the lawful causal development of the universe somehow.\"\n\nWell... I certainly won't postulate magic, [under any name](/lw/iv/the_futility_of_emergence/).\n\n\"Good.\"\n\nBut come on... doesn't it seem a little... _amazing_... that hundreds of millions of years worth of evolution's death tournament could cough up mothers and fathers, sisters and brothers, husbands and wives, steadfast friends and honorable enemies, true altruists and guardians of causes, police officers and loyal defenders, even artists sacrificing themselves for their art, all practicing so many kinds of love?  For [so many things other than genes](/lw/l3/thou_art_godshatter/)?  Doing their part to make their world less ugly, something besides a sea of blood and violence and mindless replication?\n\n\"Are you claiming to be surprised by this?  If so, [question your underlying model, for it has led you to be surprised by the true state of affairs](/lw/hs/think_like_reality/).  Since the beginning, not one unusual thing has ever happened.\"\n\nBut how is it _not_ surprising?\n\n\"What are you suggesting, that some sort of shadowy figure stood behind the scenes and directed evolution?\"\n\nHell no.  But—\n\n\"Because if you _were_ suggesting that, I would have to ask how that shadowy figure _originally_ decided that love was a _desirable_ outcome of evolution.  I would have to ask where that figure got preferences that included things like love, friendship, loyalty, fairness, honor, romance, and so on.  On evolutionary psychology, we can see how _that specific outcome_ came about—how _those particular goals rather than others_ were _generated in the first place._  You can call it 'surprising' all you like.  But when you really do understand evolutionary psychology, you can see how parental love and romance and honor, and even true altruism and moral arguments, _bear the specific design signature of natural selection_ in particular adaptive contexts of the hunter-gatherer savanna.  So if there was a shadowy figure, it must itself have evolved—and that obviates the whole point of postulating it.\"\n\nI'm not postulating a shadowy figure!  I'm just asking how human beings ended up so _nice._\n\n\"_Nice!_  Have you _looked_ at this planet lately?  We also bear all those other emotions that evolved, too—which would tell you very well that we evolved, should you begin to doubt it.  Humans aren't always nice.\"\n\nWe're one hell of a lot nicer than the process that produced us, which lets elephants starve to death when they run out of teeth, and doesn't anesthetize a gazelle even as it lays dying and is of no further importance to evolution one way or the other.  It doesn't take much to be nicer than evolution.  To have the _theoretical capacity_ to make one single gesture of mercy, to feel a single twinge of empathy, is to be nicer than evolution.  How did evolution, which is itself so uncaring, create minds on that qualitatively higher moral level than itself?  How did evolution, which is so ugly, end up doing anything so _beautiful?_\n\n\"Beautiful, you say?  Bach's _Little Fugue in G Minor_ may be beautiful, but the sound waves, as they travel through the air, are not stamped with tiny tags to specify their beauty.  If you wish to find _explicitly encoded_ a measure of the fugue's beauty, you will have to look at a human brain—nowhere else in the universe will you find it.  Not upon the seas or the mountains will you find such judgments written: they are not minds, they cannot think.\"\n\nPerhaps that is so, but still I ask:  How did evolution end up doing anything so beautiful, as giving us the ability to admire the beauty of a flower?\n\n\"Can you not see the circularity in your question?  If beauty were like some great light in the sky that shined from outside humans, then your question might make sense—though there would still be the question of how humans came to perceive that light.  You evolved with a psychology unlike evolution:  Evolution has nothing like the intelligence or the precision required to exactly quine its goal system.  In coughing up the first true minds, [evolution's simple fitness criterion shattered into a thousand values](/lw/l3/thou_art_godshatter/).  You evolved with a psychology that attaches [utility](/lw/l4/terminal_values_and_instrumental_values/) to things which evolution does not care about, like human life and happiness.  And then you look back and say, 'How marvelous, that uncaring evolution produced minds that care about sentient life!'  So your great marvel and wonder, that seems like far too much coincidence, is really no coincidence at all.\"\n\nBut then it is still amazing that this particular circular loop, happened to loop around such important things as beauty and altruism.\n\n\"I don't think you're following me here.  To you, it seems natural to privilege the beauty and altruism as special, as preferred, because you value them highly; and you don't see this as a unusual fact about yourself, because many of your friends do likewise.  So you expect that a [ghost of perfect emptiness](/lw/rn/no_universally_compelling_arguments/) would also value life and happiness—and then, from this standpoint outside reality, a great coincidence would indeed have occurred.\"\n\nBut you can make arguments for the importance of beauty and altruism from first principles—that our aesthetic senses lead us to create new complexity, instead of repeating the same things over and over; and that altruism is important because it takes us outside ourselves, gives our life a higher meaning than sheer brute selfishness.\n\n\"Oh, and _that_ argument is going to move even a [ghost of perfect emptiness](/lw/rn/no_universally_compelling_arguments/)—now that you've appealed to slightly different values?  Those aren't first principles, they're just _different_ principles.  Even if you've adopted a high-falutin' philosophical tone, still there are no _universally_ compelling arguments.  All you've done is [pass the recursive buck](/lw/rd/passing_the_recursive_buck/).\"\n\nYou don't think that, somehow, we evolved to _tap into_ something beyond—\n\n\"What good does it do to suppose something beyond?  Why should we pay more attention to that beyond thing, than we pay to our existence as humans?  How does it alter your personal responsibility, to say that you were only following the orders of the beyond thing?  And you would still have evolved to let the beyond thing, rather than something else, direct your actions.  You are only [passing the recursive buck](/lw/rd/passing_the_recursive_buck/).  Above all, it would be _too much coincidence._\"\n\nToo much coincidence?\n\n\"A flower is beautiful, you say.  Do you think there is no story behind that beauty, or that science does not know the story?  Flower pollen is transmitted by bees, so by sexual selection, flowers evolved to attract bees—by imitating certain mating signs of bees, as it happened; the flowers' patterns would look more intricate, if you could see in the ultraviolet.  Now healthy flowers are a sign of fertile land, likely to bear fruits and other treasures, and probably prey animals as well; so is it any wonder that humans evolved to be attracted to flowers?  But for there to be some great light written upon the very stars—those huge unsentient balls of burning hydrogen—which _also_ said that flowers were beautiful, now _that_ would be far too much coincidence.\"\n\nSo you [explain away](/lw/oo/explaining_vs_explaining_away/) the beauty of a flower?\n\n\"No, I explain it.  Of course there's a story behind the beauty of flowers and the fact that we find them beautiful.  Behind ordered events, one finds ordered stories; and what has no story is the product of random noise, which is hardly any better.  [If you cannot take joy in things that have stories behind them, your life will be empty indeed.](/lw/or/joy_in_the_merely_real/)  I don't think I take any less joy in a flower than you do; more so, perhaps, because I take joy in its story as well.\"\n\nPerhaps as you say, there is no surprise from a causal viewpoint—no disruption of the physical order of the universe.  But it still seems to me that, in this creation of humans by evolution, something happened that is precious and marvelous and wonderful.  If we cannot call it a physical miracle, then call it a moral miracle.\n\n\"Because it's only a miracle from the perspective of the morality that was produced, thus explaining away all of the apparent coincidence from a merely causal and physical perspective?\"\n\nWell... I suppose you could interpret the term that way, yes.  I just meant something that was immensely surprising and wonderful on a moral level, even if it is not surprising on a physical level.\n\n\"I think that's what I said.\"\n\nBut it still seems to me that you, from your own view, drain something of that wonder away.\n\n\"Then you have problems taking [joy in the merely real](/lw/or/joy_in_the_merely_real/).  Love has to begin _somehow,_ it has to enter the universe _somewhere._  It is like asking how life itself begins—and though you were born of your father and mother, and they arose from their living parents in turn, if you go far and far and far away back, you will finally come to a replicator that arose by pure accident—the border between life and unlife.  So too with love.\n\n\"A complex pattern must be explained by a cause which is not already that complex pattern.  Not just the event must be explained, but the very shape and form.  For love to first enter Time, it must come of something that is not love; if this were not possible, then love could not be.\n\n\"Even as life itself required that first replicator to come about by accident, parentless but still caused: far, far back in the causal chain that led to you: 3.85 billion years ago, in some little tidal pool.\n\n\"Perhaps your children's children will ask how it is that they are capable of love.\n\n\"And their parents will say:  Because we, who also love, created you to love.\n\n\"And your children's children will ask:  But how is it that _you_ love?\n\n\"And their parents will reply:  Because our own parents, who also loved, created us to love in turn.\n\n\"Then your children's children will ask:  But where did it all begin?  Where does the recursion end?\n\n\"And their parents will say:  Once upon a time, long ago and far away, ever so long ago, there were intelligent beings who were not themselves intelligently designed.  Once upon a time, there were lovers created by something that did not love.\n\n\"Once upon a time, when all of civilization was a single galaxy and a single star: and a single planet, a place called Earth.\n\n\"Long ago, and far away, ever so long ago.\""
    },
    "voteCount": 88,
    "forceInclude": true
  },
  {
    "_id": "ur9TCRnHJighHmLCW",
    "url": null,
    "title": "On Caring",
    "slug": "on-caring",
    "author": "So8res",
    "question": false,
    "tags": [
      {
        "name": "Motivations"
      },
      {
        "name": "World Optimization"
      },
      {
        "name": "Motivational Intro Posts"
      }
    ],
    "tableOfContents": {
      "sections": [
        {
          "title": "1",
          "anchor": "1",
          "level": 1
        },
        {
          "title": "2",
          "anchor": "2",
          "level": 1
        },
        {
          "title": "3",
          "anchor": "3",
          "level": 1
        },
        {
          "title": "4",
          "anchor": "4",
          "level": 1
        },
        {
          "title": "5",
          "anchor": "5",
          "level": 1
        },
        {
          "title": "6",
          "anchor": "6",
          "level": 1
        },
        {
          "title": "7",
          "anchor": "7",
          "level": 1
        },
        {
          "title": "8",
          "anchor": "8",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        },
        {
          "anchor": "comments",
          "level": 0,
          "title": "275 comments"
        }
      ],
      "headingsCount": 10
    },
    "contents": {
      "markdown": "_This is an essay describing some of my motivation to be an effective altruist. It is [crossposted](http://mindingourway.com/on-caring/) from [my blog](http://mindingourway.com/). Many of the ideas here are quite similar to [others found in the sequences](/lw/hw/scope_insensitivity/). I have a slightly different take, and after adjusting for the typical mind fallacy I expect that this post may contain insights that are new to many._\n\n1\n=\n\nI'm not very good at _feeling_ the size of large numbers. Once you start tossing around numbers larger than 1000 (or maybe even 100), the numbers just seem \"big\".\n\nConsider Sirius, the brightest star in the night sky. If you told me that Sirius is as big as a million earths, I would feel like that's a lot of Earths. If, instead, you told me that you could fit a _billion_ Earths inside Sirius… I would still just feel like that's a lot of Earths.\n\nThe feelings are almost identical. _In context_, my brain grudgingly admits that a billion is a lot larger than a million, and puts forth a token effort to feel like a billion-Earth-sized star is bigger than a million-Earth-sized star. But out of context — if I wasn't anchored at \"a million\" when I heard \"a billion\" — both these numbers just feel vaguely large.\n\nI feel a _little_ respect for the bigness of numbers, if you pick really really large numbers. If you say \"one followed by a hundred zeroes\", then this feels _a lot_ bigger than a billion. But it certainly doesn't feel (in my gut) like it's 10 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 times bigger than a billion. Not in the way that four apples _internally feels_ like twice as many as two apples. My brain can't even begin to wrap itself around this sort of magnitude differential.\n\nThis phenomena is related to [scope insensitivity](/lw/hw/scope_insensitivity/), and it's important to me because I live in a world where sometimes the things I care about are really really numerous.\n\nFor example, [billions of people live in squalor](http://www.globalissues.org/article/26/poverty-facts-and-stats), with hundreds of millions of them deprived of basic needs and/or dying from disease. And though most of them are out of my sight, I still care about them.\n\nThe loss of a human life with all is joys and all its sorrows is tragic no matter what the cause, and the tragedy is not reduced simply because I was far away, or because I did not know of it, or because I did not know how to help, or because I was not personally responsible.\n\nKnowing this, I care about every single individual on this planet. The problem is, my brain is _simply incapable_ of taking the amount of caring I feel for a single person and scaling it up by a billion times. I lack the internal capacity to feel that much. My care-o-meter simply doesn't go up that far.\n\nAnd this is a problem.\n\n2\n=\n\nIt's a common trope that courage isn't about being fearless, it's about being afraid but _doing the right thing anyway_. In the same sense, caring about the world isn't about having a gut feeling that corresponds to the amount of suffering in the world, it's about _doing the right thing anyway_. Even without the feeling.\n\nMy internal care-o-meter was calibrated to deal with [about a hundred and fifty people](http://en.wikipedia.org/wiki/Dunbar's_number), and it _simply can't express_ the amount of caring that I have for billions of sufferers. The internal care-o-meter just doesn't go up that high.\n\nHumanity is playing for unimaginably high stakes. At the very least, there are billions of people suffering today. At the worst, there are quadrillions (or more) potential humans, transhumans, or posthumans whose existence depends upon what we do here and now. All the intricate civilizations that the future could hold, the experience and art and beauty that is possible in the future, depends upon the present.\n\nWhen you're faced with stakes like these, your internal caring heuristics — calibrated on numbers like \"ten\" or \"twenty\" — completely fail to grasp the gravity of the situation.\n\nSaving a person's life feels _great_, and [it would probably feel just about as good to save one life as it would feel to save the world](/lw/hx/one_life_against_the_world/). It surely wouldn't be _many billion times_ more of a high to save the world, because your hardware can't express a feeling a billion times bigger than the feeling of saving a person's life. But even though the altruistic high from saving someone's life would be shockingly similar to the altruistic high from saving the world, always remember that _behind_ those similar feelings there is a whole world of difference.\n\nOur internal care-feelings are woefully inadequate for deciding how to act in a world with big problems.\n\n3\n=\n\nThere's a mental shift that happened to me when I first started internalizing scope insensitivity. It is a little difficult to articulate, so I'm going to start with a few stories.\n\nConsider Alice, a software engineer at Amazon in Seattle. Once a month or so, those college students will show up on street corners with clipboards, looking ever more disillusioned as they struggle to convince people to donate to [Doctors Without Borders](http://www.doctorswithoutborders.org/). Usually, Alice avoids eye contact and goes about her day, but this month they finally manage to corner her. They explain Doctors Without Borders, and she actually has to admit that it sounds like a pretty good cause. She ends up handing them $20 through a combination of guilt, social pressure, and altruism, and then rushes back to work. (Next month, when they show up again, she avoids eye contact.)\n\nNow consider Bob, who has been given the [Ice Bucket Challenge](http://en.wikipedia.org/wiki/Ice_Bucket_Challenge) by a friend on facebook. He feels too busy to do the ice bucket challenge, and instead just donates $100 to [ALSA](http://www.alsa.org/).\n\nNow consider Christine, who is in the college sorority ΑΔΠ. ΑΔΠ is engaged in a competition with ΠΒΦ (another sorority) to see who can raise the most money for the National Breast Cancer Foundation in a week. Christine has a competitive spirit and gets engaged in fund-raising, and gives a few hundred dollars herself over the course of the week (especially at times when ΑΔΠ is especially behind).\n\nAll three of these people are donating money to charitable organizations… and that's great. But notice that there's something similar in these three stories: these donations are largely motivated by a _social context_. Alice feels obligation and social pressure. Bob feels social pressure and maybe a bit of camaraderie. Christine feels camaraderie and competitiveness. These are all fine motivations, but notice that these motivations are related to the _social setting_, and only tangentially to the _content_ of the charitable donation.\n\nIf you took any of Alice or Bob or Christine and asked them why they aren't donating _all_ of their time and money to these causes that they apparently believe are worthwhile, they'd look at you funny and they'd probably think you were being rude (with good reason!). If you pressed, they might tell you that money is a little tight right now, or that they would donate more if they were a better person.\n\nBut the question would still feel kind of _wrong_. Giving all your money away is just not what you do with money. We can all _say out loud_ that people who give all their possessions away are really great, but behind closed doors we all know that people are crazy. (Good crazy, perhaps, but crazy all the same.)\n\nThis is a mindset that I inhabited for a while. There's an alternative mindset that can hit you like a freight train when you start internalizing scope insensitivity.\n\n4\n=\n\nConsider Daniel, a college student shortly after the [Deepwater Horizon](http://en.wikipedia.org/wiki/Deepwater_Horizon_oil_spill) BP oil spill. He encounters one of those college students with the clipboards on the street corners, soliciting donations to the [World Wildlife Foundation](http://www.worldwildlife.org/). They're trying to save as many oiled birds as possible. Normally, Daniel would simply dismiss the charity as Not The Most Important Thing, or Not Worth His Time Right Now, or Somebody Else's Problem, but this time Daniel has been thinking about how his brain is bad at numbers and decides to do a quick sanity check.\n\nHe pictures himself walking along the beach after the oil spill, and encountering a group of people cleaning birds as fast as they can. They simply don't have the resources to clean all the available birds. A pathetic young bird flops towards his feet, slick with oil, eyes barely able to open. He kneels down to pick it up and help it onto the table. One of the bird-cleaners informs him that they won't have time to get to that bird themselves, but he could pull on some gloves and could probably save the bird with three minutes of washing.\n\n![blog.bird-rescue.org](http://4.bp.blogspot.com/_Bv2wRceCYKA/S-whZ1sSJ3I/AAAAAAAABjo/cgXz7Npz_W0/s320/Dawn_IBRRC_2010.05.07_MG_7082_240.jpg)\n\nDaniel decides that he _would_ spend three minutes of his time to save the bird, and that he would _also_ be happy to pay at least $3 to have someone else spend a few minutes cleaning the bird. He introspects and finds that this is not just because he imagined a bird right in front of him: he feels that it is _worth_ at least three minutes of his time (or $3) to save an oiled bird in some vague platonic sense.\n\nAnd, because he's been thinking about scope insensitivity, he _expects_ his brain to misreport how much he actually cares about large numbers of birds: the internal feeling of caring can't be expected to line up with the actual importance of the situation. So instead of just _asking his gut_ how much he cares about de-oiling lots of birds, he shuts up and multiplies.\n\n[Thousands and thousands](http://dailydeadbirds.com/) of birds were oiled by the BP spill alone. After shutting up and multiplying, Daniel realizes (with growing horror) that the amount he _acutally_ cares about oiled birds is lower bounded by two months of hard work and/or fifty thousand dollars. And that's not even counting wildlife threatened by [other oil spills](http://en.wikipedia.org/wiki/List_of_oil_spills).\n\nAnd if he cares that much about _de-oiling birds_, then how much does he actually care about factory farming, nevermind hunger, or poverty, or sickness? How much does he actually care about wars that ravage nations? About neglected, deprived children? About the future of humanity? He _actually_ cares about these things to the tune of much more money than he has, and much more time than he has.\n\nFor the first time, Daniel sees a glimpse of of how much he actually cares, and how poor a state the world is in.\n\nThis has the strange effect that Daniel's reasoning goes full-circle, and he realizes that he actually _can't_ care about oiled birds to the tune of 3 minutes or $3: not because the birds aren't _worth_ the time and money (and, in fact, he thinks that the economy produces things priced at $3 which are worth less than the bird's survival), but because he can't spend _his_ time or money on saving the birds. The opportunity cost suddenly seems far too high: there is _too much else to do!_ People are sick and starving and dying! The very future of our civilization is at stake!\n\nDaniel doesn't wind up giving $50k to the WWF, and he also doesn't donate to ALSA or NBCF. But if you ask _Daniel_ why he's not donating all his money, he won't look at you funny or think you're rude. He's left the place where you don't care far behind, and has realized that _his mind was lying to him the whole time_ about the gravity of the real problems.\n\nNow he realizes that he _can't possibly do enough_. After adjusting for his scope insensitivity (and the fact that his brain lies about the size of large numbers), even the \"less important\" causes like the WWF suddenly seem worthy of dedicating a life to. Wildlife destruction and ALS and breast cancer are suddenly all problems that he would _move mountains_ to solve — except he's finally understood that there are just too many mountains, and ALS isn't the bottleneck, and AHHH HOW DID ALL THESE MOUNTAINS GET HERE?\n\nIn the original mindstate, the reason he didn't drop everything to work on ALS was because it just didn't seem… pressing enough. Or tractable enough. Or important enough. Kind of. These are sort of the reason, but the real reason is more that the concept of \"dropping everything to address ALS\" never even _crossed his mind_ as a real possibility. The idea was too much of a break from the standard narrative. It wasn't his problem.\n\nIn the new mindstate, _everything_ is his problem. The only reason he's not dropping everything to work on ALS is because there are far too many things to do first.\n\nAlice and Bob and Christine usually aren't spending time solving all the world's problems because they forget to see them. If you remind them — put them in a social context where they remember how much they care (hopefully without guilt or pressure) — then they'll likely donate a little money.\n\nBy contrast, Daniel and others who have undergone the mental shift aren't spending time solving all the world's problems because there are _just too many problems_. (Daniel hopefully goes on to discover movements like [effective altruism](http://effectivealtruism.org/) and starts contributing towards fixing the world's most pressing problems.)\n\n5\n=\n\nI'm not trying to preach here about how to be a good person. You don't need to share my viewpoint to be a good person (obviously).\n\nRather, I'm trying to point at a shift in perspective. Many of us go through life understanding that we _should_ care about people suffering far away from us, but failing to. I think that this attitude is tied, at least in part, to the fact that most of us implicitly trust our internal care-o-meters.\n\nThe \"care feeling\" isn't usually strong enough to compel us to frantically save everyone dying. So while we acknowledge that it would be _virtuous_ to do more for the world, we think that we _can't_, because we weren't gifted with that virtuous extra-caring that prominent altruists must have.\n\nBut this is an error — prominent altruists aren't the people who have a larger care-o-meter, they're the people who have _learned not to trust their care-o-meters_.\n\nOur care-o-meters are broken. They don't work on large numbers. Nobody has one capable of faithfully representing the scope of the world's problems. But the fact that you can't _feel_ the caring doesn't mean that you can't _do_ the caring.\n\nYou don't get to feel the appropriate amount of \"care\", in your body. Sorry — the world's problems are just too large, and your body is not built to respond appropriately to problems of this magnitude. But if you choose to do so, you can still _act_ like the world's problems are as big as they are. You can stop trusting the internal feelings to guide your actions and switch over to manual control.\n\n6\n=\n\nThis, of course, leads us to the question of \"what the hell do you then?\"\n\nAnd I don't really know yet. (Though I'll plug the [Giving What We Can pledge](http://givingwhatwecan.org), [GiveWell](http://givewell.org), [MIRI](http://intelligence.org), and [The Future of Humanity Institute](http://www.fhi.ox.ac.uk) as a good start).\n\nI think that at least part of it comes from a certain sort of desperate perspective. It's not enough to think you _should_ change the world — you also need the sort of desperation that comes from realizing that you would dedicate your entire life to solving the world's 100th biggest problem if you could, but you can't, because there are 99 bigger problems you have to address first.\n\nI'm not trying to guilt you into giving more money away — becoming a philanthropist is _really really hard_. (If you're _already_ a philanthropist, then you have my acclaim and my affection.) First it requires you to have money, which is uncommon, and then it requires you to _throw that money at distant invisible problems_, which is not an easy sell to a human brain. [Akrasia](http://en.wikipedia.org/wiki/Akrasia) is a formidable enemy. And most importantly, guilt doesn't seem like a good long-term motivator: if you want to join the ranks of people saving the world, I would rather you join them proudly. There are many trials and tribulations ahead, and we'd do better to face them with our heads held high.\n\n7\n=\n\nCourage isn't about being fearless, it's about being able to do the right thing even if you're afraid.\n\nAnd similarly, addressing the major problems of our time isn't about feeling a strong compulsion to do so. It's about doing it anyway, even when internal compulsion utterly fails to capture the scope of the problems we face.\n\nIt's easy to look at especially virtuous people — Gandhi, Mother Theresa, Nelson Mandela — and conclude that they must have cared more than we do. But I don't think that's the case.\n\nNobody gets to comprehend the scope of these problems. The closest we can get is doing the multiplication: finding something we care about, putting a number on it, and multiplying. And then trusting the numbers more than we trust our feelings.\n\nBecause our feelings lie to us.\n\nWhen you do the multiplication, you realize that addressing global poverty and building a brighter future deserve more resources than currently exist. There is not enough money, time, or effort in the world to do what we need to do.\n\nThere is only you, and me, and everyone else who is trying anyway.\n\n8\n=\n\nYou can't actually feel the weight of the world. The human mind is not capable of that feat.\n\nBut sometimes, you can catch a glimpse."
    },
    "voteCount": 152,
    "forceInclude": true
  },
  {
    "_id": "DoLQN5ryZ9XkZjq5h",
    "url": null,
    "title": "Tsuyoku Naritai! (I Want To Become Stronger)",
    "slug": "tsuyoku-naritai-i-want-to-become-stronger",
    "author": "Eliezer Yudkowsky",
    "question": false,
    "tags": [
      {
        "name": "Ambition"
      },
      {
        "name": "Rationality"
      },
      {
        "name": "Something To Protect"
      },
      {
        "name": "Tsuyoku Naritai"
      },
      {
        "name": "Motivational Intro Posts"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "In Orthodox Judaism there is a saying: “The previous generation is to the next one as angels are to men; the next generation is to the previous one as donkeys are to men.” This follows from the Orthodox Jewish belief that all Judaic law was given to Moses by God at Mount Sinai. After all, it’s not as if you could do an experiment to gain new halachic knowledge; the only way you can know is if someone tells you (who heard it from someone else, who heard it from God). Since there is no new source of information; it can only be degraded in transmission from generation to generation.\n\nThus, modern rabbis are not allowed to overrule ancient rabbis. Crawly things are ordinarily unkosher, but it is permissible to eat a worm found in an apple—the ancient rabbis believed the worm was spontaneously generated inside the apple, and therefore was part of the apple. A modern rabbi cannot say, “Yeah, well, the ancient rabbis knew diddly-squat about biology. Overruled!” A modern rabbi cannot possibly know a halachic principle the ancient rabbis did not, because how could the ancient rabbis have passed down the answer from Mount Sinai to him? Knowledge derives from authority, and therefore is only ever lost, not gained, as time passes.\n\nWhen I was first exposed to the angels-and-donkeys proverb in (religious) elementary school, I was not old enough to be a full-blown atheist, but I still thought to myself: “Torah loses knowledge in every generation. Science gains knowledge with every generation. No matter where they started out, sooner or later science must surpass Torah.”\n\nThe most important thing is that there should be progress. So long as you keep moving forward you will reach your destination; but if you stop moving you will never reach it.\n\n_Tsuyoku naritai_ is Japanese. _Tsuyoku_ is “strong”; _naru_ is “becoming,” and the form _naritai_ is “want to become.” Together it means, “I want to become stronger,” and it expresses a sentiment embodied more intensely in Japanese works than in any Western literature I’ve read. You might say it when expressing your determination to become a professional Go player—or after you lose an important match, but you haven’t given up—or after you win an important match, but you’re not a ninth-dan player yet—or after you’ve become the greatest Go player of all time, but you still think you can do better. That is _tsuyoku naritai_, the will to transcendence.\n\nEach year on Yom Kippur, an Orthodox Jew recites a litany which begins _Ashamnu, bagadnu, gazalnu, dibarnu dofi_, and goes on through the entire Hebrew alphabet: _We have acted shamefully, we have betrayed, we have stolen, we have slandered . . ._\n\nAs you pronounce each word, you strike yourself over the heart in penitence. There’s no exemption whereby, if you manage to go without stealing all year long, you can skip the word _gazalnu_ and strike yourself one less time. That would violate the community spirit of Yom Kippur, which is about _confessing_ sins—not _avoiding_ sins so that you have less to confess.\n\nBy the same token, the _Ashamnu_ does not end, “But that was this year, and next year I will do better.”\n\nThe _Ashamnu_ bears a remarkable resemblance to the notion that the way of rationality is to beat your fist against your heart and say, “We are all biased, we are all irrational, we are not fully informed, we are overconfident, we are poorly calibrated . . .”\n\nFine. Now tell me how you plan to become _less_ biased, _less_ irrational, _more_ informed, _less_ overconfident, _better_ calibrated.\n\nThere is an old Jewish joke: During Yom Kippur, the rabbi is seized by a sudden wave of guilt, and prostrates himself and cries, “God, I am nothing before you!” The cantor is likewise seized by guilt, and cries, “God, I am nothing before you!” Seeing this, the janitor at the back of the synagogue prostrates himself and cries, “God, I am nothing before you!” And the rabbi nudges the cantor and whispers, “Look who thinks he’s nothing.”\n\nTake no pride in your confession that you too are biased; do not glory in your self-awareness of your flaws. This is akin to the principle of not taking pride in confessing your ignorance; for if your ignorance is a source of pride to you, you may become loath to relinquish your ignorance when evidence comes knocking. Likewise with our flaws—we should not gloat over how self-aware we are for confessing them; the occasion for rejoicing is when we have a little less to confess.\n\nOtherwise, when the one comes to us with a plan for _correcting_ the bias, we will snarl, “Do you think to set yourself above us?” We will shake our heads sadly and say, “You must not be very self-aware.”\n\nNever confess to me that you are just as flawed as I am unless you can tell me what you plan to do about it. Afterward you will still have plenty of flaws left, but that’s not the point; the important thing is to _do better_, to keep moving ahead, to take one more step forward. _Tsuyoku naritai!_"
    },
    "voteCount": 219,
    "forceInclude": true
  },
  {
    "_id": "Nu3wa6npK4Ry66vFp",
    "url": null,
    "title": "A Sense That More Is Possible",
    "slug": "a-sense-that-more-is-possible",
    "author": "Eliezer_Yudkowsky",
    "question": false,
    "tags": [
      {
        "name": "Rationality"
      },
      {
        "name": "Rationality Verification"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "To teach people about a topic you've labeled \"rationality\", it helps for them to be interested in \"rationality\".  (There are less direct ways to teach people how to attain [the map that reflects the territory](http://www.overcomingbias.com/2006/11/why_truth_and.html), or optimize reality according to their values; but the explicit method _is_ the course I tend to take.)\n\nAnd when people explain why they're _not_ interested in rationality, one of the most commonly proffered reasons tends to be like:  \"Oh, I've known a couple of rational people and they didn't seem any happier.\"\n\nWho are they thinking of?  Probably an [Objectivist](http://www.overcomingbias.com/2007/12/ayn-rand.html) or some such.  Maybe someone they know who's an [ordinary scientist](http://www.overcomingbias.com/2008/05/do-scientists-a.html).  Or an [ordinary atheist](/lw/1e/raising_the_sanity_waterline/).\n\nThat's really _not_ a whole lot of rationality, as I have previously said.\n\nEven if you limit yourself to people who can derive Bayes's Theorem—which is going to eliminate, what, 98% of the above personnel?—that's _still_ not a whole lot of rationality.  I mean, it's a pretty basic theorem.\n\nSince the beginning I've had a sense that there ought to be some discipline of cognition, some art of thinking, the studying of which would make its students visibly more competent, more formidable: the equivalent of [Taking a Level in Awesome](http://tvtropes.org/pmwiki/pmwiki.php/Main/TookALevelInBadass).\n\nBut when I look around me in the real world, I don't see that.  Sometimes I see a hint, an echo, of what I think should be possible, when I read the writings of folks like Robyn Dawes, Daniel Gilbert, Tooby & Cosmides.  A few very rare and very senior researchers in psychological sciences, who visibly care a _lot_ about rationality—to the point, I suspect, of making their colleagues feel uncomfortable, because it's not cool to care that much.  I can see that they've found a rhythm, a unity that begins to pervade their arguments—\n\nYet even that... isn't really a whole lot of rationality either.\n\nEven among those whose few who impress me with a hint of dawning formidability—I don't think that their mastery of rationality could compare to, say, John Conway's mastery of math.  The base knowledge that we drew upon to build our understanding—if you extracted only the parts we used, and not everything we had to study to find it—it's probably not comparable to what a professional nuclear engineer knows about nuclear engineering.  It may not even be comparable to what a construction engineer knows about bridges.  We practice our skills, we do, in the ad-hoc ways we taught ourselves; but that practice probably doesn't compare to the training regimen an Olympic runner goes through, or maybe even an ordinary professional tennis player.\n\nAnd the root of _this_ problem, I do suspect, is that we haven't really gotten together and systematized our skills.  We've had to create all of this for ourselves, ad-hoc, and there's a limit to how much one mind can do, even if it can manage to draw upon work done in outside fields.\n\nThe chief obstacle to doing this the way it _really_ should be done, is the difficulty of testing the _results_ of rationality training programs, so you can have evidence-based training methods.  I will write more about this, because I think that recognizing successful training and distinguishing it from failure is the essential, blocking obstacle.\n\nThere are experiments done now and again on debiasing interventions for particular biases, but it tends to be something like, \"Make the students practice this for an hour, then test them two weeks later.\"  Not, \"Run half the signups through version A of the three-month summer training program, and half through version B, and survey them five years later.\"  You can see, here, the implied amount of effort that I think would go into a training program for people who were Really Serious about rationality, as opposed to the attitude of taking Casual Potshots That Require Like An Hour Of Effort Or Something.\n\nDaniel Burfoot brilliantly [suggests](http://www.overcomingbias.com/2008/10/isshokenmei.html#comment-133847467) that this is why intelligence seems to be such a big factor in rationality—that when you're improvising everything ad-hoc with very little training or systematic practice, intelligence ends up being the most important factor in what's left.\n\nWhy aren't \"rationalists\" surrounded by a visible aura of formidability?  Why aren't they found at the top level of every elite selected on any basis that has anything to do with thought?  Why do most \"rationalists\" just seem like ordinary people, perhaps of moderately above-average intelligence, with one more hobbyhorse to ride?\n\nOf this there are several answers; but one of them, surely, is that they have received less systematic training of rationality in a less systematic context than a first-dan black belt gets in hitting people.\n\nI do not except myself from this criticism.  I am no [beisutsukai](http://www.overcomingbias.com/2008/05/eld-science.html), because there are limits to how much Art you can create on your own, and how well you can guess without evidence-based statistics on the results.  I know about a _single_ use of rationality, which might be termed \"reduction of confusing cognitions\".  This I asked of my brain, this it has given me.  There are other arts, I think, that a mature rationality training program would not neglect to teach, which would make me stronger and happier and more effective—if I could just go through a standardized training program using the cream of teaching methods experimentally demonstrated to be effective.  But the kind of tremendous, focused effort that I put into creating my single _sub-art_ of rationality from scratch—my life doesn't have room for more than one of those.\n\nI consider myself something more than a first-dan black belt, and less.  I can _punch_ through brick and I'm working on steel along my way to adamantine, but I have a mere casual street-fighter's grasp of how to kick or throw or block.\n\nWhy are there schools of martial arts, but not [rationality dojos](http://www.overcomingbias.com/2006/11/the_martial_art.html)?  (This was the first question I asked in my [first blog post](http://www.overcomingbias.com/2006/11/the_martial_art.html).)  Is it more important to hit people than to think?\n\nNo, but it's easier to verify when you _have_ hit someone.  That's part of it, a highly central part.\n\nBut maybe even more importantly—there are people out there who _want_ to hit, and who have the idea that there ought to be a systematic art of hitting that makes you into a visibly more formidable fighter, with a speed and grace and strength beyond the struggles of the unpracticed.  So they go to a school that promises to teach that.  And that school exists because, long ago, some people had the sense that more was possible.  And they got together and shared their techniques and practiced and formalized and practiced and developed the Systematic Art of Hitting.  They pushed themselves that far because _they thought they should be awesome_ and they were willing to put some _back_ into it.\n\nNow—they _got_ somewhere with that aspiration, unlike a thousand other aspirations of awesomeness that failed, because they could _tell_ when they had hit someone; and the schools competed against each other regularly in realistic contests with clearly-defined winners.\n\nBut before even that—there was first the aspiration, the [wish to become stronger](http://www.overcomingbias.com/2007/03/tsuyoku_naritai.html), a sense that more was possible.  A vision of a speed and grace and strength that they did not already possess, but _could_ possess, _if_ they were willing to put in a lot of work, that drove them to systematize and train and test.\n\nWhy don't we have an Art of Rationality?\n\nThird, because current \"rationalists\" have trouble working in groups: of this I shall speak more.\n\nSecond, because it is hard to verify success in training, or which of two schools is the stronger.\n\nBut first, because people lack the sense that rationality is something that _should_ be systematized and trained and tested like a martial art, that should have as much knowledge behind it as nuclear engineering, whose superstars should practice as hard as chess grandmasters, whose successful practitioners should be surrounded by an evident aura of awesome.\n\nAnd conversely they don't look at the _lack_ of visibly greater formidability, and say, \"We must be doing something wrong.\"\n\n\"Rationality\" just seems like one more hobby or hobbyhorse, that people talk about at parties; an adopted mode of conversational [attire](http://www.overcomingbias.com/2007/08/belief-as-attir.html) with few or no real consequences; and it doesn't seem like there's anything wrong about that, either."
    },
    "voteCount": 104,
    "forceInclude": true
  },
  {
    "_id": "WBdvyyHLdxZSAMmoz",
    "url": null,
    "title": "Taboo Your Words",
    "slug": "taboo-your-words",
    "author": "Eliezer_Yudkowsky",
    "question": false,
    "tags": [
      {
        "name": "World Modeling"
      },
      {
        "name": "Disagreement"
      },
      {
        "name": "Conversation (topic)"
      },
      {
        "name": "Techniques"
      },
      {
        "name": "Philosophy of Language"
      },
      {
        "name": "Rationalist Taboo"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "In the game Taboo (by Hasbro), the objective is for a player to have their partner guess a word written on a card, without using that word or five additional words listed on the card.  For example, you might have to get your partner to say \"baseball\" without using the words \"sport\", \"bat\", \"hit\", \"pitch\", \"base\" or of course \"baseball\".\n\nAs soon as I see a problem like that, I at once think, \"An artificial group conflict in which you use a long wooden cylinder to whack a thrown spheroid, and then run between four safe positions.\"  It might not be the most efficient strategy to convey the word 'baseball' under the stated rules - that might be, \"It's what the Yankees play\" - but the general skill of _blanking a word out of my mind_ was one I'd practiced for years, albeit with a different purpose.\n\nYesterday we saw how replacing terms with definitions could reveal the [empirical unproductivity](/lw/nf/the_parable_of_hemlock/) of the classical Aristotelian syllogism.  All humans are mortal (and also, apparently, featherless bipeds); Socrates is human; therefore Socrates is mortal.  When we replace the word 'human' by its apparent definition, the following underlying reasoning is revealed:\n\n> All \\[mortal, ~feathers, biped\\] are mortal;  \n> Socrates is a \\[mortal, ~feathers, biped\\];  \n> Therefore Socrates is mortal.\n\nBut the principle of replacing words by definitions applies much more broadly:\n\n> Albert:  \"A tree falling in a deserted forest makes a sound.\"  \n> Barry:  \"A tree falling in a deserted forest does not make a sound.\"\n\nClearly, since one says \"sound\" and one says \"not sound\", we must have a contradiction, right?  But suppose that they both dereference their pointers before speaking:\n\n> Albert:  \"A tree falling in a deserted forest matches \\[membership test: this event generates acoustic vibrations\\].\"  \n> Barry:  \"A tree falling in a deserted forest does not match \\[membership test: this event generates auditory experiences\\].\"\n\nNow there is no longer an apparent collision—all they had to do was prohibit themselves from using the word _sound_. If \"acoustic vibrations\" came into dispute, we would just play Taboo again and say \"pressure waves in a material medium\"; if necessary we would play Taboo again on the word \"[wave](/lw/iq/guessing_the_teachers_password/)\" and replace it with the wave equation.  (Play Taboo on \"auditory experience\" and you get \"That form of sensory processing, within the human brain, which takes as input a linear time series of frequency mixes...\")\n\nBut suppose, on the other hand, that Albert and Barry were to have the argument:\n\n> Albert:  \"Socrates matches the concept \\[membership test: this person will die after drinking hemlock\\].\"  \n> Barry:  \"Socrates matches the concept \\[membership test: this person will not die after drinking hemlock\\].\"\n\nNow Albert and Barry have a substantive clash of expectations; a difference in what they anticipate seeing after Socrates drinks hemlock.  But they might not notice this, if they happened to use the same word \"human\" for their different concepts.\n\nYou get a very different picture of what people agree or disagree about, depending on whether you take a label's-eye-view (Albert says \"sound\" and Barry says \"not sound\", so they must disagree) or taking the test's-eye-view (Albert's membership test is acoustic vibrations, Barry's is auditory experience).\n\nGet together a pack of _soi-disant_ futurists and ask them if they believe we'll have Artificial Intelligence in thirty years, and I would guess that at least half of them will say yes.  If you leave it at that, they'll shake hands and congratulate themselves on their consensus.  But make the term \"Artificial Intelligence\" taboo, and ask them to describe _what_ they expect to see, without ever using words like \"computers\" or \"think\", and you might find quite a conflict of expectations hiding under that featureless standard word.  Likewise [that other term](http://intelligence.org/blog/2007/09/30/three-major-singularity-schools/).  And see also Shane Legg's compilation of [71 definitions of \"intelligence\"](http://arxiv.org/abs/0706.3639).\n\nThe illusion of unity across religions can be dispelled by making the term \"God\" taboo, and asking them to say what it is they believe in; or making the word \"faith\" taboo, and asking them why they believe it. Though mostly they won't be able to answer at all, because it is mostly [profession](/lw/i4/belief_in_belief/) in the first place, and you cannot cognitively zoom in on an audio recording.\n\nWhen you find yourself in philosophical difficulties, _the first line of defense is not to define your problematic terms, but to see whether you can think without using those terms at all._  Or any of their short synonyms.  And be careful not to let yourself invent a new word to use instead.  Describe outward observables and interior mechanisms; don't use a single handle, whatever that handle may be.\n\nAlbert says that people have \"free will\".  Barry says that people don't have \"free will\".  Well, that will certainly generate an apparent conflict.  Most philosophers would advise Albert and Barry to try to define exactly what they mean by \"free will\", on which topic they will certainly be able to discourse at great length.  I would advise Albert and Barry to describe what it is that they think people do, or do not have, without using the phrase \"free will\" at all.  (If you want to try this at home, you should also avoid the words \"choose\", \"act\", \"decide\", \"determined\", \"responsible\", or any of their synonyms.)\n\nThis is one of the nonstandard tools in my toolbox, and in my humble opinion, it works _way way_ better than the standard one.  It also requires more effort to use; you get what you pay for."
    },
    "voteCount": 136,
    "forceInclude": true
  },
  {
    "_id": "Mc6QcrsbH5NRXbCRX",
    "url": null,
    "title": "Dissolving the Question",
    "slug": "dissolving-the-question",
    "author": "Eliezer_Yudkowsky",
    "question": false,
    "tags": [
      {
        "name": "Introspection"
      },
      {
        "name": "Philosophy of Language"
      },
      {
        "name": "Philosophy"
      },
      {
        "name": "Dissolving the Question"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "\"If [a tree falls in the forest](/lw/np/disputing_definitions/), but no one hears it, does it make a sound?\"\n\nI didn't _answer_ that question.  I didn't pick a position, \"Yes!\" or \"No!\", and defend it.  Instead I went off and [deconstructed](/lw/no/how_an_algorithm_feels_from_inside/) the human algorithm for processing words, even going so far as to sketch an [illustration](/lw/nn/neural_categories/) of a neural network.  At the end, I hope, there was no question left—not even the feeling of a question.\n\nMany philosophers—particularly amateur philosophers, and ancient philosophers—share a dangerous instinct:  If you give them a question, they try to answer it.\n\nLike, say, \"Do we have free will?\"\n\nThe dangerous instinct of philosophy is to marshal the arguments in favor, and marshal the arguments against, and weigh them up, and publish them in a prestigious journal of philosophy, and so finally conclude:  \"Yes, we must have free will,\" or \"No, we cannot possibly have free will.\"\n\nSome philosophers are wise enough to recall the warning that most philosophical disputes are really disputes over the meaning of a word, or confusions generated by [using different meanings for the same word in different places](/lw/oc/variable_question_fallacies/).  So they try to define very precisely what they mean by \"free will\", and then ask again, \"Do we have free will?  Yes or no?\"\n\nA philosopher wiser yet, may suspect that the confusion about \"free will\" shows the notion itself is flawed.  So they pursue the Traditional Rationalist course:  They argue that \"free will\" is inherently self-contradictory, or meaningless because it has no [testable consequences](/lw/i3/making_beliefs_pay_rent_in_anticipated_experiences/).  And then they publish these devastating observations in a prestigious philosophy journal.\n\nBut _proving that_ you are confused may not make you feel any _less_ confused.  Proving that a question is meaningless may not help you any more than answering it.\n\nThe philosopher's instinct is to find the most defensible position, publish it, and move on.  But the \"naive\" view, the instinctive view, is a fact about human psychology.  You can prove that free will is impossible until the Sun goes cold, but this leaves an unexplained fact of cognitive science:  If free will doesn't exist, what goes on inside the head of a human being who thinks it does?  This is not a rhetorical question!\n\nIt is a fact about human psychology that people think they have free will.  Finding a more defensible _philosophical position_ doesn't change, or explain, that _psychological fact._  Philosophy may lead you to _reject_ the concept, but rejecting a concept is not the same as understanding the cognitive algorithms behind it.\n\nYou could look at the [Standard Dispute](/lw/np/disputing_definitions/) over \"If a tree falls in the forest, and no one hears it, does it make a sound?\", and you could do the Traditional Rationalist thing:  Observe that the two don't disagree on any point of [anticipated experience](/lw/i3/making_beliefs_pay_rent_in_anticipated_experiences/), and triumphantly declare the argument pointless.  That happens to be correct in this particular case; but, as _a question of cognitive science,_ why did the arguers make that mistake in the first place?\n\nThe key idea of the heuristics and biases program is that the _mistakes_ we make, often reveal far more about our underlying cognitive algorithms than our correct answers.  So (I asked myself, once upon a time) [what kind of mind design](/lw/nq/feel_the_meaning/) corresponds to the mistake of [arguing](/lw/np/disputing_definitions/) about trees falling in deserted forests?\n\nThe cognitive algorithms we use, _are_ [the way the world feels](/lw/no/how_an_algorithm_feels_from_inside/).  And these cognitive algorithms may not have a one-to-one correspondence with reality—not even macroscopic reality, to say nothing of the true quarks.  There can be things in the mind that cut skew to the world.\n\nFor example, there can be a [dangling unit](/lw/no/how_an_algorithm_feels_from_inside/) in the center of a [neural network](/lw/nn/neural_categories/), which does not correspond to any real thing, or any real property of any real thing, existent anywhere in the real world.  This dangling unit is often useful as a [shortcut in computation](/lw/o8/conditional_independence_and_naive_bayes/), which is why we have them.  (Metaphorically speaking.  Human neurobiology is surely far more [complex](/lw/o8/conditional_independence_and_naive_bayes/).)\n\nThis dangling unit _feels like_ an unresolved question, even after every answerable [query](/lw/nm/disguised_queries/) is answered.  No matter how much anyone proves to you that no difference of anticipated experience depends on the question, you're left wondering:  \"But does the falling tree _really_ make a sound, or not?\"\n\nBut once you understand _in detail_ how your brain generates the _feeling_ of the question—once you realize that your feeling of an unanswered question, corresponds to an illusory central unit wanting to know whether it should fire, even after all the edge units are clamped at known values—or better yet, you understand the technical workings of [Naive Bayes](/lw/o8/conditional_independence_and_naive_bayes/)—_then_ you're done.  Then there's no lingering feeling of confusion, no vague sense of dissatisfaction.\n\nIf there is _any_ lingering feeling of a remaining unanswered question, or of having been fast-talked into something, then this is a sign that you have not dissolved the question.  A [vague dissatisfaction](/lw/if/your_strength_as_a_rationalist/) should be as much warning as a shout.  _Really_ dissolving the question doesn't leave anything behind.\n\nA triumphant thundering refutation of free will, an absolutely unarguable proof that free will cannot exist, feels very _satisfying_—a [grand cheer](/lw/i6/professing_and_cheering/) for the [home team](/lw/mg/the_twoparty_swindle/).    And so you may not notice that—as a point of cognitive science—you do not have a full and satisfactory descriptive explanation of how each intuitive sensation arises, point by point.\n\nYou may not even want to admit your ignorance, of this point of cognitive science, because that would feel like a score against Your Team.  In the midst of smashing all foolish beliefs of free will, it would seem like a concession to the opposing side to concede that you've left anything unexplained.\n\nAnd so, perhaps, you'll come up with a [just-so evolutionary-psychological](/lw/mk/a_failed_justso_story/) argument that hunter-gatherers who believed in free will, were more likely to take a positive outlook on life, and so outreproduce other hunter-gatherers—to give one example of a completely bogus explanation.  If you say this, you are _arguing that_ the brain generates an illusion of free will—but you are not _explaining how._  You are trying to dismiss the opposition by deconstructing its motives—but in the story you tell, the illusion of free will is a brute fact.  You have not taken the illusion apart to see the wheels and gears.\n\nImagine that in the Standard Dispute about a tree falling in a deserted forest, you first prove that no difference of anticipation exists, and then go on to hypothesize, \"But perhaps people who said that arguments were meaningless were viewed as having conceded, and so lost social status, so now we have an instinct to argue about the meanings of words.\"  That's _arguing that_ or _explaining why_ a confusion exists.  Now look at the neural network structure in [Feel the Meaning](/lw/nq/feel_the_meaning/).  That's _explaining how_, disassembling the confusion into smaller pieces which are not themselves confusing.  See the difference?\n\nComing up with good hypotheses about cognitive algorithms (or even hypotheses that hold together for half a second) is a good deal harder than just refuting a philosophical confusion.  Indeed, it is an entirely different art.  Bear this in mind, and you should feel less embarrassed to say, \"I know that what you say can't possibly be true, and I can prove it.  But I cannot write out a flowchart which shows how your brain makes the mistake, so I'm not done yet, and will continue investigating.\"\n\nI say all this, because it sometimes seems to me that at least 20% of the real-world effectiveness of a skilled rationalist comes from [not stopping too early](/lw/jz/the_meditation_on_curiosity/).  If you keep asking questions, you'll get to your destination eventually.  If you decide too early that you've found an answer, you won't.\n\nThe challenge, above all, is to notice when you are confused—even if it just feels like a little tiny bit of confusion—and even if there's someone standing across from you, _insisting_ that humans have free will, and _smirking_ at you, and the fact that you don't know _exactly_ how the cognitive algorithms work, has _nothing to do_ with the searing folly of their position...\n\nBut when you can lay out the cognitive algorithm in sufficient detail that you can walk through the thought process, step by step, and describe how each intuitive perception arises—decompose the confusion into smaller pieces not themselves confusing—_then_ you're done.\n\nSo be warned that you may _believe_ you're done, when all you have is a mere triumphant [refutation of a mistake](/lw/lw/reversed_stupidity_is_not_intelligence/).\n\nBut when you're _really_ done, you'll _know_ you're done.[ ](/lw/gr/the_modesty_argument/)  Dissolving the question is an unmistakable feeling—once you experience it, and, having experienced it, resolve not to be fooled again.  [Those who dream do not know they dream, but when you wake you know you are awake.](/lw/gr/the_modesty_argument/)\n\nWhich is to say:  When you're done, you'll know you're done, but unfortunately the reverse implication does not hold.\n\nSo here's your homework problem:  What kind of cognitive algorithm, as felt from the inside, would generate the observed debate about \"free will\"?\n\nYour assignment is not to argue about whether people have free will, or not.\n\nYour assignment is not to argue that free will is compatible with determinism, or not.\n\nYour assignment is not to argue that the question is ill-posed, or that the concept is self-contradictory, or that it has no testable consequences.\n\nYou are not asked to invent an evolutionary explanation of how people who believed in free will would have reproduced; nor an account of how the concept of free will seems suspiciously congruent with bias X.  Such are mere attempts to _explain why_ people believe in \"free will\", not _explain how._\n\nYour homework assignment is to write a stack trace of the internal algorithms of the human mind as they produce the intuitions that power the whole damn philosophical argument.\n\nThis is one of the first real challenges I tried as an aspiring rationalist, once upon a time.  One of the easier conundrums, relatively speaking.  May it serve you likewise."
    },
    "voteCount": 85,
    "forceInclude": true
  },
  {
    "_id": "895quRDaK6gR2rM82",
    "url": null,
    "title": "Diseased thinking: dissolving questions about disease",
    "slug": "diseased-thinking-dissolving-questions-about-disease",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "Rationality"
      },
      {
        "name": "Health / Medicine / Disease"
      },
      {
        "name": "Philosophy of Language"
      },
      {
        "name": "Carving / Clustering Reality"
      },
      {
        "name": "Reversal Test"
      },
      {
        "name": "Motivational Intro Posts"
      }
    ],
    "tableOfContents": {
      "sections": [
        {
          "title": "What is Disease?",
          "anchor": "What_is_Disease_",
          "level": 1
        },
        {
          "title": "Hidden Inferences From Disease Concept",
          "anchor": "Hidden_Inferences_From_Disease_Concept",
          "level": 1
        },
        {
          "title": "Sympathy or Condemnation?",
          "anchor": "Sympathy_or_Condemnation_",
          "level": 1
        },
        {
          "title": "The Ethics of Treating Marginal Conditions",
          "anchor": "The_Ethics_of_Treating_Marginal_Conditions",
          "level": 1
        },
        {
          "title": "Summary",
          "anchor": "Summary",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        },
        {
          "anchor": "comments",
          "level": 0,
          "title": "354 comments"
        }
      ],
      "headingsCount": 7
    },
    "contents": {
      "markdown": "**Related to:** [Disguised Queries](https://www.lesswrong.com/lw/nm/disguised_queries/), [Words as Hidden Inferences](https://www.lesswrong.com/lw/ng/words_as_hidden_inferences/), [Dissolving the Question](https://www.lesswrong.com/lw/of/dissolving_the_question/), [Eight Short Studies on Excuses](https://www.lesswrong.com/lw/24o/eight_short_studies_on_excuses/)\n\n> _Today's therapeutic ethos, which celebrates curing and disparages judging, expresses the liberal disposition to assume that crime and other problematic behaviors reflect social or biological causation. While this absolves the individual of responsibility, it also strips the individual of personhood, and moral dignity_\n\n_\\-\\- George Will, [townhall.com](http://townhall.com/Common/PrintPage.aspx?g=761ecc84-473b-4123-bf28-c4fc179a9d3f&t=c)_\n\nSandy is a morbidly obese woman looking for advice.\n\nHer husband has no sympathy for her, and tells her she obviously needs to stop eating like a pig, and would it kill her to go to the gym once in a while?\n\nHer doctor tells her that obesity is primarily genetic, and recommends the diet pill orlistat and a consultation with a surgeon about gastric bypass.\n\nHer sister tells her that obesity is a perfectly valid lifestyle choice, and that fat-ism, equivalent to racism, is society's way of keeping her down.\n\nWhen she tells each of her friends about the opinions of the others, things really start to heat up.\n\nHer husband accuses her doctor and sister of absolving her of personal responsibility with feel-good platitudes that in the end will only prevent her from getting the willpower she needs to start a real diet.\n\nHer doctor accuses her husband of ignorance of the real causes of obesity and of the most effective treatments, and accuses her sister of legitimizing a dangerous health risk that could end with Sandy in hospital or even dead.\n\nHer sister accuses her husband of being a jerk, and her doctor of trying to medicalize her behavior in order to turn it into a \"condition\" that will keep her on pills for life and make lots of money for Big Pharma.\n\nSandy is fictional, but similar conversations happen every day, not only about obesity but about a host of other marginal conditions that some consider character flaws, others diseases, and still others normal variation in the human condition. Attention deficit disorder, internet addiction, social anxiety disorder (as one skeptic said, didn't we used to call this \"shyness\"?), alcoholism, chronic fatigue, oppositional defiant disorder (\"didn't we used to call this being a teenager?\"), compulsive gambling, homosexuality, Aspergers' syndrome, antisocial personality, even depression have all been placed in two or more of these categories by different people.\n\n  \n\nSandy's sister may have a point, but this post will concentrate on the debate between her husband and her doctor, with the understanding that the same techniques will apply to evaluating her sister's opinion. The disagreement between Sandy's husband and doctor centers around the idea of \"disease\". If obesity, depression, alcoholism, and the like are diseases, most people default to the doctor's point of view; if they are not diseases, they tend to agree with the husband.\n\nThe debate over such marginal conditions is in many ways a debate over whether or not they are \"real\" diseases. The usual surface level arguments trotted out in favor of or against the proposition are generally inconclusive, but this post will apply a host of techniques previously discussed on Less Wrong to illuminate the issue.\n\n  \n\n**What is Disease?**\n\n  \n\nIn [Disguised Queries](https://www.lesswrong.com/lw/nm/disguised_queries/) , Eliezer demonstrates how a word refers to a cluster of objects related upon multiple axes. For example, in a company that sorts red smooth translucent cubes full of vanadium from blue furry opaque eggs full of palladium, you might invent the word \"rube\" to designate the red cubes, and another \"blegg\", to designate the blue eggs. Both words are useful because they \"carve reality at the joints\" - they refer to two completely separate classes of things which it's practically useful to keep in separate categories. Calling something a \"blegg\" is a quick and easy way to describe its color, shape, opacity, texture, and chemical composition. It may be that the odd blegg might be purple rather than blue, but in general the characteristics of a blegg remain sufficiently correlated that \"blegg\" is a useful word. If they weren't so correlated - if blue objects were equally likely to be palladium-containing-cubes as vanadium-containing-eggs, then the word \"blegg\" would be a waste of breath; the characteristics of the object would remain just as mysterious to your partner after you said \"blegg\" as they were before.\n\n\"Disease\", like \"blegg\", suggests that certain characteristics always come together. A rough sketch of some of the characteristics we expect in a disease might include:\n\n1\\. Something caused by the sorts of thing you study in biology: proteins, bacteria, ions, viruses, genes.\n\n2\\. Something involuntary and completely immune to the operations of free will\n\n3\\. Something rare; the vast majority of people don't have it\n\n4\\. Something unpleasant; when you have it, you want to get rid of it\n\n5\\. Something discrete; a graph would show two widely separate populations, one with the disease and one without, and not a normal distribution.\n\n6\\. Something commonly treated with science-y interventions like chemicals and radiation.\n\nCancer satisfies every one of these criteria, and so we have no qualms whatsoever about classifying it as a disease. It's a type specimen, [the sparrow as opposed to the ostrich](https://www.lesswrong.com/lw/nl/the_cluster_structure_of_thingspace/). The same is true of heart attack, the flu, diabetes, and many more.\n\nSome conditions satisfy a few of the criteria, but not others. Dwarfism seems to fail (5), and it might get its status as a disease only after studies show that the supposed dwarf falls way out of normal human height variation. Despite the best efforts of transhumanists, it's hard to convince people that aging is a disease, partly because it fails (3). Calling homosexuality a disease is a poor choice for many reasons, but one of them is certainly (4): it's not necessarily unpleasant.\n\nThe marginal conditions mentioned above are also in this category. Obesity arguably sort-of-satisfies criteria (1), (4), and (6), but it would be pretty hard to make a case for (2), (3), and (5).\n\nSo, is obesity really a disease? Well, is Pluto really a planet? Once we state that obesity satisfies some of the criteria but not others, it is meaningless to talk about an additional fact of whether it \"really deserves to be a disease\" or not.\n\nIf it weren't for those pesky [hidden inferences](https://www.lesswrong.com/lw/ng/words_as_hidden_inferences/#more)...\n\n**Hidden Inferences From Disease Concept**\n\nThe state of the disease node, meaningless in itself, is used to predict several other nodes with non-empirical content. In English: we make value decisions based on whether we call something a \"disease\" or not.\n\nIf something is a real disease, the patient deserves our sympathy and support; for example, [cancer sufferers must universally be described as \"brave\"](http://www.theonion.com/articles/loved-ones-recall-local-mans-cowardly-battle-with,772/). If it is not a real disease, people are more likely to get our condemnation; for example Sandy's husband who calls her a \"pig\" for her inability to control her eating habits. The difference between \"shyness\" and \"social anxiety disorder\" is that people with the first get called \"weird\" and told to man up, and people with the second get special privileges and the sympathy of those around them.\n\nAnd if something is a real disease, it is socially acceptable (maybe even mandated) to seek medical treatment for it. If it's not a disease, medical treatment gets derided as a \"quick fix\" or an \"abdication of personal responsibility\". I have talked to several doctors who are uncomfortable suggesting gastric bypass surgery, even in people for whom it is medically indicated, because they believe it is morally wrong to turn to medicine to solve a character issue.\n\n![](http://res.cloudinary.com/lesswrong-2-0/image/upload/v1531709899/disease_first_tlnfio.gif)\n\nWhile a condition's status as a \"real disease\" ought to be meaningless as a \"hanging node\" after the status of all other nodes have been determined, it has acquired political and philosophical implications because of its role in determining whether patients receive sympathy and whether they are permitted to seek medical treatment.\n\nIf we can determine whether a person should get sympathy, and whether they should be allowed to seek medical treatment, independently of the central node \"disease\" or of the criteria that feed into it, we will have successfully unasked the question \"are these marginal conditions real diseases\" and cleared up the confusion.\n\n**Sympathy or Condemnation?**\n\nOur attitudes toward people with marginal conditions mainly reflect a deontologist libertarian (libertarian as in \"free will\", not as in \"against government\") model of blame. In this concept, people make decisions using their free will, a spiritual entity operating free from biology or circumstance. People who make good decisions are intrinsically good people and deserve good treatment; people who make bad decisions are intrinsically bad people and deserve bad treatment. But people who make bad decisions for reasons that are outside of their free will may not be intrinsically bad people, and may therefore be absolved from deserving bad treatment. For example, if a normally peaceful person has a brain tumor that affects areas involved in fear and aggression, they go on a crazy killing spree, and then they have their brain tumor removed and become a peaceful person again, many people would be willing to accept that the killing spree does not reflect negatively on them or open them up to deserving bad treatment, since it had biological and not spiritual causes.\n\nUnder this model, deciding whether a condition is biological or spiritual becomes very important, and the rationale for worrying over whether something \"is a real disease\" or not is plain to see. Without figuring out this extremely difficult question, we are at risk of either blaming people for things they don't deserve, or else letting them off the hook when they commit a sin, both of which, to libertarian deontologists, would be terrible things. But determining whether marginal conditions like depression have a spiritual or biological cause is difficult, and no one knows how to do it reliably.\n\nDeterminist consequentialists can do better. We believe it's biology all the way down. Separating spiritual from biological illnesses is impossible and unnecessary. Every condition, from brain tumors to poor taste in music, is \"biological\" insofar as it is encoded in things like cells and proteins and follows laws based on their structure.\n\nBut determinists don't just ignore the very important differences between brain tumors and poor taste in music. Some biological phenomena, like poor taste in music, are encoded in such a way that they are extremely vulnerable to what we can call social influences: praise, condemnation, introspection, and the like. Other biological phenomena, like brain tumors, are completely immune to such influences. This allows us to develop a more useful model of blame.\n\nThe consequentialist model of blame is very different from the deontological model. Because all actions are biologically determined, none are more or less metaphysically blameworthy than others, and none can mark anyone with the metaphysical status of \"bad person\" and make them \"deserve\" bad treatment. Consequentialists don't on a primary level want anyone to be treated badly, full stop; thus [is it written](http://yudkowsky.net/obsolete/tmol-faq.html#theo_free): \"Saddam Hussein doesn't deserve so much as a stubbed toe.\" But if consequentialists don't believe in punishment for its own sake, they do believe in punishment for the sake of, well, consequences. Hurting bank robbers may not be a good in and of itself, but it will prevent banks from being robbed in the future. And, one might infer, although alcoholics may not deserve condemnation, societal condemnation of alcoholics makes alcoholism a less attractive option.\n\nSo here, at last, is a rule for which diseases we offer sympathy, and which we offer condemnation: if giving condemnation instead of sympathy decreases the incidence of the disease enough to be worth the hurt feelings, condemn; otherwise, sympathize. Though the rule is based on philosophy that the majority of the human race would disavow, it leads to intuitively correct consequences. Yelling at a cancer patient, shouting \"How dare you allow your cells to divide in an uncontrolled manner like this; is that the way your mother raised you??!\" will probably make the patient feel pretty awful, but it's not going to cure the cancer. Telling a lazy person \"Get up and do some work, you worthless bum,\" very well might cure the laziness. The cancer is a biological condition immune to social influences; the laziness is a biological condition susceptible to social influences, so we try to socially influence the laziness and not the cancer.\n\nThe question \"Do the obese deserve our sympathy or our condemnation,\" then, is asking whether condemnation is such a useful treatment for obesity that its utility outweights the disutility of hurting obese people's feelings. This question may have different answers depending on the particular obese person involved, the particular person doing the condemning, and the availability of other methods for treating the obesity, which brings us to...\n\n**The Ethics of Treating Marginal Conditions**\n\nIf a condition is susceptible to social intervention, but an effective biological therapy for it also exists, is it okay for people to use the biological therapy instead of figuring out a social solution? My gut answer is \"Of course, why wouldn't it be?\", but apparently lots of people find this controversial for some reason.\n\nIn a libertarian deontological system, throwing biological solutions at spiritual problems might be disrespectful or dehumanizing, or a band-aid that doesn't affect the deeper problem. To someone who believes it's biology all the way down, this is much less of a concern.\n\nOthers complain that the existence of an easy medical solution prevents people from learning personal responsibility. But here [we see the status-quo bias at work, and so can apply a preference reversal test](http://www.nickbostrom.com/ethics/statusquo.pdf). If people really believe learning personal responsibility is more important than being not addicted to heroin, we would expect these people to support deliberately addicting schoolchildren to heroin so they can develop personal responsibility by coming off of it. Anyone who disagrees with this somewhat shocking proposal must believe, on some level, that having people who are not addicted to heroin is more important than having people develop whatever measure of personal responsibility comes from kicking their heroin habit the old-fashioned way.\n\nBut the most convincing explanation I have read for why so many people are opposed to medical solutions for social conditions is a signaling explanation by Robin Hans...wait! no!...by Katja Grace. On [her blog](http://meteuphoric.wordpress.com/2009/09/21/why-do-animal-lovers-want-animals-to-feel-pain/), she says:\n\n> _...the situation reminds me of a pattern in similar cases I have noticed before. It goes like this. Some people make personal sacrifices, supposedly toward solving problems that don’t threaten them personally. They sort recycling, buy free range eggs, buy fair trade, campaign for wealth redistribution etc. Their actions are seen as virtuous. They see those who don’t join them as uncaring and immoral. A more efficient solution to the problem is suggested. It does not require personal sacrifice. People who have not previously sacrificed support it. Those who have previously sacrificed object on grounds that it is an excuse for people to get out of making the sacrifice. The supposed instrumental action, as the visible sign of caring, has become virtuous in its own right. Solving the problem effectively is an attack on the moral people._\n\nA case in which some people eat less enjoyable foods and exercise hard to avoid becoming obese, and then campaign against a pill that makes avoiding obesity easy demonstrates some of the same principles.\n\nThere are several very reasonable objections to treating any condition with drugs, whether it be a classical disease like cancer or a marginal condition like alcoholism. The drugs can have side effects. They can be expensive. They can build dependence. They may later be found to be placebos whose efficacy was overhyped by dishonest pharmaceutical advertising.. They may raise ethical issues with children, the mentally incapacitated, and other people who cannot decide for themselves whether or not to take them. But these issues do not magically become more dangerous in conditions typically regarded as \"character flaws\" rather than \"diseases\", and the same good-enough solutions that work for cancer or heart disease will work for alcoholism and other such conditions (but see [here](https://www.lesswrong.com/lw/2as/diseased_thinking_dissolving_questions_about/230p)).\n\nI see no reason why people who want effective treatment for a condition should be denied it or stigmatized for seeking it, whether it is traditionally considered \"medical\" or not.\n\n**Summary**\n\nPeople commonly debate whether social and mental conditions are real diseases. This masquerades as a medical question, but its implications are mainly social and ethical. We use the concept of disease to decide who gets sympathy, who gets blame, and who gets treatment.\n\nInstead of continuing the fruitless \"disease\" argument, we should address these questions directly. Taking a determinist consequentialist position allows us to do so more effectively. We should blame and stigmatize people for conditions where blame and stigma are the most useful methods for curing or preventing the condition, and we should allow patients to seek treatment whenever it is available and effective."
    },
    "voteCount": 367,
    "forceInclude": true
  },
  {
    "_id": "2jp98zdLo898qExrr",
    "url": null,
    "title": "Hug the Query",
    "slug": "hug-the-query",
    "author": "Eliezer_Yudkowsky",
    "question": false,
    "tags": [
      {
        "name": "Anticipated Experiences"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "In the art of rationality there is a discipline of _closeness-to-the-issue_—trying to observe evidence that is as near to the original question as possible, so that it screens off as many other arguments as possible.\n\nThe Wright Brothers say, “My plane will fly.” If you look at their authority (bicycle mechanics who happen to be excellent amateur physicists) then you will compare their authority to, say, Lord Kelvin, and you will find that Lord Kelvin is the greater authority.\n\nIf you demand to see the Wright Brothers’ calculations, and you can follow them, and you demand to see Lord Kelvin’s calculations (he probably doesn’t have any apart from his own incredulity), then authority becomes much less relevant.\n\nIf you actually _watch the plane fly_, the calculations themselves become moot for many purposes, and Kelvin’s authority not even worth considering.\n\nThe more _directly_ your arguments bear on a question, without intermediate inferences—the closer the observed nodes are to the queried node, in the Great Web of Causality—the more powerful the evidence. It’s a theorem of these causal graphs that you can never get _more_ information from distant nodes, than from strictly closer nodes that screen off the distant ones.\n\nJerry Cleaver said: “What does you in is not failure to apply some high-level, intricate, complicated technique. It’s overlooking the basics. Not keeping your eye on the ball.”^[1](#fn1x21)^\n\nJust as it is superior to argue physics than credentials, it is also superior to argue physics than rationality. Who was more rational, the Wright Brothers or Lord Kelvin? If we can check their calculations, we don’t have to care! The virtue of a rationalist cannot _directly_ cause a plane to fly.\n\nIf you forget this principle, learning about more biases will hurt you, because it will distract you from more direct arguments. It’s all too easy to argue that someone is exhibiting Bias #182 in your repertoire of fully generic accusations, but you can’t _settle_ a factual issue without closer evidence. If there are biased reasons to say the Sun is shining, that doesn’t make it dark out.\n\nJust as you can’t always experiment today, you can’t always check the calculations today.^[2](#fn2x21)^ Sometimes you don’t know enough background material, sometimes there’s private information, sometimes there just isn’t time. There’s a sadly large number of times when it’s worthwhile to judge the speaker’s rationality. You should always do it with a hollow feeling in your heart, though, a sense that something’s missing.\n\nWhenever you can, dance as near to the original question as possible—press yourself up against it—get close enough to _hug the query!_\n\n^[1](#fn1x21-bk)^Jerry Cleaver, _Immediate Fiction: A Complete Writing Course_ (Macmillan, 2004).\n\n^[2](#fn2x21-bk)^See also “Is Molecular Nanotechnology ’Scientific’?” [http://lesswrong.com/lw/io/is\\_molecular\\_nanotechnology_scientific](http://lesswrong.com/lw/io/is_molecular_nanotechnology_scientific)."
    },
    "voteCount": 74,
    "forceInclude": true
  },
  {
    "_id": "kpRSCH7ALLcb6ucWM",
    "url": null,
    "title": "Say Not \"Complexity\"",
    "slug": "say-not-complexity",
    "author": "Eliezer Yudkowsky",
    "question": false,
    "tags": [
      {
        "name": "Anticipated Experiences"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "Once upon a time . . .\n\nThis is a story from when I first met Marcello, with whom I would later work for a year on AI theory; but at this point I had not yet accepted him as my apprentice. I knew that he competed at the national level in mathematical and computing olympiads, which sufficed to attract my attention for a closer look; but I didn’t know yet if he could learn to think about AI.\n\nI had asked Marcello to say how he thought an AI might discover how to solve a Rubik’s Cube. Not in a preprogrammed way, which is trivial, but rather how the AI itself might figure out the laws of the Rubik universe and reason out how to exploit them. How would an AI *invent for itself* the concept of an “operator,” or “macro,” which is the key to solving the Rubik’s Cube?\n\nAt some point in this discussion, Marcello said: “Well, I think the AI needs complexity to do X, and complexity to do Y—”\n\nAnd I said, “Don’t say ‘*complexity*.’ ”\n\nMarcello said, “Why not?”\n\nI said, “Complexity should never be a goal in itself. You may need to use a particular algorithm that adds some amount of complexity, but complexity for the sake of complexity just makes things harder.” (I was thinking of all the people whom I had heard advocating that the Internet would “wake up” and become an AI when it became “sufficiently complex.”)\n\nAnd Marcello said, “But there’s got to be *some* amount of complexity that does it.”\n\nI closed my eyes briefly, and tried to think of how to explain it all in words. To me, saying “complexity” simply *felt* like the wrong move in the AI dance. No one can think fast enough to deliberate, in words, about each sentence of their stream of consciousness; for that would require an infinite recursion. We think in words, but our stream of consciousness is steered below the level of words, by the trained-in remnants of past insights and harsh experience . . .\n\nI said, “Did you read ‘A Technical Explanation of Technical Explanation’?”^1^\n\n“Yes,” said Marcello.\n\n“Okay,” I said. “Saying ‘complexity’ doesn’t concentrate your probability mass.”\n\n“Oh,” Marcello said, “like ‘emergence.’ Huh. So . . . now I’ve got to think about how X might actually happen . . .”\n\nThat was when I thought to myself, “*Maybe **this** one is teachable.*”\n\nComplexity is not a useless concept. It has mathematical definitions attached to it, such as Kolmogorov complexity and Vapnik-Chervonenkis complexity. Even on an intuitive level, complexity is often worth thinking about—you have to judge the complexity of a hypothesis and decide if it’s “too complicated” given the supporting evidence, or look at a design and try to make it simpler.\n\nBut concepts are not useful or useless of themselves. Only *usages* are correct or incorrect. In the step Marcello was trying to take in the dance, he was trying to explain something for free, get something for nothing. It is an extremely common misstep, at least in my field. You can join a discussion on artificial general intelligence and watch people doing the same thing, left and right, over and over again—constantly skipping over things they don’t understand, without realizing that’s what they’re doing.\n\nIn an eyeblink it happens: putting a non-controlling causal node behind something mysterious, a causal node that feels like an explanation but isn’t. The mistake takes place below the level of words. It requires no special character flaw; it is how human beings think by default, how they have thought since the ancient times.\n\nWhat you must avoid is *skipping over the mysterious part*; you must linger at the mystery to confront it directly. There are many words that can skip over mysteries, and some of them would be legitimate in other contexts—“complexity,” for example. But the essential mistake is that *skip-over*, regardless of what causal node goes behind it. The skip-over is not a thought, but a microthought. You have to pay close attention to catch yourself at it. And when you train yourself to avoid skipping, it will become a matter of instinct, not verbal reasoning. You have to *feel* which parts of your map are still blank, and more importantly, pay attention to that feeling.\n\nI suspect that in academia there is a huge pressure to sweep problems under the rug so that you can present a paper with the appearance of completeness. You’ll get more kudos for a seemingly complete model that includes some “emergent phenomena,” versus an explicitly incomplete map where the label says “I got no clue how this part works” or “then a miracle occurs.” A journal may not even accept the latter paper, since who knows but that the unknown steps are really where everything interesting happens?^2^\n\nAnd if you’re working on a revolutionary AI startup, there is an even huger pressure to sweep problems under the rug; or you will have to admit to yourself that you don’t know how to build the right kind of AI yet, and your current life plans will come crashing down in ruins around your ears. But perhaps I am [over-explaining](https://lesswrong.com/rationality/correspondence-bias), since skip-over happens by default in humans. If you’re looking for examples, just watch people discussing religion or philosophy or spirituality or any science in which they were not professionally trained.\n\nMarcello and I developed a convention in our AI work: when we ran into something we didn’t understand, which was often, we would say “magic”—as in, X magically does Y”—to remind ourselves that *here was an unsolved problem, a gap in our understanding*. It is far better to say “magic” than “complexity” or “emergence”; the latter words create an illusion of understanding. Wiser to say “magic,” and leave yourself a placeholder, a reminder of work you will have to do later.\n\n[^1^](#fn1x40-bk) [http://lesswrong.com/rationality/a-technical-explanation-of-technical-explanation](http://lesswrong.com/rationality/a-technical-explanation-of-technical-explanation)\n\n[^2^](#fn2x40-bk) And yes, it sometimes happens that all the non-magical parts of your map turn out to also be non-important. That’s the price you sometimes pay, for entering into terra incognita and trying to solve problems *incrementally*. But that makes it even *more* important to *know* when you aren’t finished yet. Mostly, people don’t dare to enter terra incognita at all, for the deadly fear of wasting their time."
    },
    "voteCount": 61,
    "forceInclude": true
  },
  {
    "_id": "ZTRiSNmeGQK8AkdN2",
    "url": null,
    "title": "Mind Projection Fallacy",
    "slug": "mind-projection-fallacy",
    "author": "Eliezer_Yudkowsky",
    "question": false,
    "tags": [
      {
        "name": "Rationality"
      },
      {
        "name": "Fallacies"
      },
      {
        "name": "Mind Projection Fallacy"
      },
      {
        "name": "Map and Territory"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "[![Monsterwithgirl_2](/static/imported/2007/08/10/monsterwithgirl_2.jpg \"Monsterwithgirl_2\")](/static/imported/2007/08/10/monsterwithgirl_2.jpg)In the dawn days of science fiction, alien invaders would occasionally kidnap a girl in a torn dress and carry her off for intended ravishing, as lovingly depicted on many ancient magazine covers.  Oddly enough, the aliens never go after men in torn shirts.\n\nWould a non-humanoid alien, with a different evolutionary history and [evolutionary psychology](/lw/l1/evolutionary_psychology/), sexually desire a human female?  It seems rather unlikely.  To put it mildly.\n\nPeople don't make mistakes like that by deliberately reasoning:  \"All possible minds are likely to be wired pretty much the same way, therefore a bug-eyed monster will find human females attractive.\"  Probably the artist did not even think to ask whether an alien _perceives_ human females as attractive.  Instead, a human female in a torn dress _is sexy_—inherently so, as an intrinsic property.\n\nThey who went astray did not think about the alien's evolutionary history; they focused on the woman's torn dress.  If the dress were not torn, the woman would be less sexy; the alien monster doesn't enter into it.\n\nApparently we instinctively represent Sexiness as a direct attribute of the Woman object, Woman.sexiness, like Woman.height or Woman.weight.\n\nIf your brain uses that data structure, or something metaphorically similar to it, then [from the inside](/lw/no/how_an_algorithm_feels_from_inside/) it feels like sexiness is an inherent property of the woman, not a property of the alien looking at the woman.  Since the woman _is attractive,_ the alien monster will be _attracted_ to her—isn't that logical?\n\nE. T. Jaynes used the term [Mind Projection Fallacy](http://citeseer.ist.psu.edu/6330.html) to denote the error of projecting your own mind's properties into the external world.  Jaynes, as a late grand master of the Bayesian Conspiracy, was most concerned with the mistreatment of _probabilities_ as inherent properties of objects, rather than states of partial knowledge in some particular mind.  More about this shortly.\n\nBut the Mind Projection Fallacy generalizes as an error.  It is in the argument over [the real meaning of the word sound](/lw/np/disputing_definitions/), and in the magazine cover of the monster carrying off a woman in the torn dress, and Kant's declaration that space by its very nature is flat, and Hume's definition of [a priori](/lw/k2/a_priori/) ideas as those \"discoverable by the mere operation of thought, without dependence on what is anywhere existent in the universe\"...\n\n(Incidentally, I once read an SF story about a human male who entered into a sexual relationship with a sentient alien plant of appropriately squishy fronds; discovered that it was an [androecious](http://en.wikipedia.org/wiki/Plant_sexuality) (male) plant; agonized about this for a bit; and finally decided that it didn't really matter at that point.  And in Foglio and Pollotta's _Illegal Aliens,_ the humans land on a planet inhabited by sentient insects, and see a movie advertisement showing a human carrying off a bug in a delicate chiffon dress.  Just thought I'd mention that.)"
    },
    "voteCount": 58,
    "forceInclude": true
  },
  {
    "_id": "yA4gF5KrboK2m2Xu7",
    "url": null,
    "title": "How An Algorithm Feels From Inside",
    "slug": "how-an-algorithm-feels-from-inside",
    "author": "Eliezer_Yudkowsky",
    "question": false,
    "tags": [
      {
        "name": "Rationality"
      },
      {
        "name": "Philosophy of Language"
      },
      {
        "name": "Cognitive Reduction"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "\"If a tree falls in the forest, and no one hears it, does it make a sound?\"  I remember seeing an actual argument get started on this subject—a fully naive argument that went nowhere near Berkeleyan subjectivism.  Just:\n\n> \"It makes a sound, just like any other falling tree!\"  \n> \"But how can there be a sound that no one hears?\"\n\nThe standard rationalist view would be that the first person is speaking as if \"sound\" means acoustic vibrations in the air; the second person is speaking as if \"sound\" means an auditory experience in a brain.  If you ask \"Are there acoustic vibrations?\" or \"Are there auditory experiences?\", the answer is at once obvious.  And so the argument is really about the definition of the word \"sound\".\n\nI think the standard analysis is essentially correct.  So let's accept that as a premise, and ask:  Why do people get into such an argument?  What's the underlying psychology?\n\nA key idea of the heuristics and biases program is that mistakes are often more revealing of cognition than correct answers.  Getting into a heated dispute about whether, if a tree falls in a deserted forest, it makes a sound, is traditionally considered a mistake.\n\nSo what kind of mind design corresponds to that error?\n\nIn [Disguised Queries](/lw/nm/disguised_queries/) I introduced the blegg/rube classification task, in which Susan the Senior Sorter explains that your job is to sort objects coming off a conveyor belt, putting the blue eggs or \"bleggs\" into one bin, and the red cubes or \"rubes\" into the rube bin.  This, it turns out, is because bleggs contain small nuggets of vanadium ore, and rubes contain small shreds of palladium, both of which are useful industrially.\n\nExcept that around 2% of blue egg-shaped objects contain palladium instead.  So if you find a blue egg-shaped thing that contains palladium, should you call it a \"rube\" instead?  You're going to put it in the rube bin—why not call it a \"rube\"?\n\nBut when you switch off the light, nearly all bleggs glow faintly in the dark.  And blue egg-shaped objects that contain palladium are just as likely to glow in the dark as any other blue egg-shaped object.\n\nSo if you find a blue egg-shaped object that contains palladium, and you ask \"Is it a blegg?\", the answer depends on what you have to do with the answer:  If you ask \"Which bin does the object go in?\", then you choose as if the object is a rube.  But if you ask \"If I turn off the light, will it glow?\", you predict as if the object is a blegg.  In one case, the question \"Is it a blegg?\" stands in for the [disguised query](/lw/nm/disguised_queries/), \"Which bin does it go in?\".  In the other case, the question \"Is it a blegg?\" stands in for the [disguised query](/lw/nm/disguised_queries/), \"Will it glow in the dark?\"\n\nNow suppose that you have an object that is blue and egg-shaped and contains palladium; and you have already observed that it is furred, flexible, opaque, and glows in the dark.\n\nThis answers _every_ query, observes every observable introduced.  There's nothing left for a disguised query to stand _for._\n\nSo why might someone feel an impulse to go on arguing whether the object is _really_ a blegg?\n\n[![Blegg3](/static/imported/2008/02/10/blegg3.png \"Blegg3\")](/static/imported/2008/02/10/blegg3.png)\n\nThis diagram from [Neural Categories](/lw/nn/neural_categories/) shows two different neural networks that might be used to answer questions about bleggs and rubes.  Network 1 has a number of disadvantages—such as potentially oscillating/chaotic behavior, or requiring O(N^2^) connections—but Network 1's structure does have one major advantage over Network 2:  Every unit in the network corresponds to a testable query.  If you observe every observable, clamping every value, there are no units in the network left over.\n\nNetwork 2, however, is a far better candidate for being something vaguely like how the human brain works:  It's fast, cheap, scalable—and has an extra dangling unit in the center, whose activation can still vary, even after we've observed every single one of the surrounding nodes.\n\nWhich is to say that even after you know whether an object is blue or red, egg or cube, furred or smooth, bright or dark, and whether it contains vanadium or palladium, it _feels_ like there's a leftover, unanswered question:  _But is it really a blegg?_\n\nUsually, in our daily experience, acoustic vibrations and auditory experience go together.  But a tree falling in a deserted forest unbundles this common association.  And even after you know that the falling tree creates acoustic vibrations but not auditory experience, it _feels_ like there's a leftover question:  _Did it make a sound?_  \n  \nWe know where Pluto is, and where it's going; we know Pluto's shape, and Pluto's mass—but is it a planet?\n\nNow remember:  When you look at Network 2, as I've laid it out here, you're seeing the algorithm from the outside.  People don't think to themselves, \"Should the central unit fire, or not?\" any more than you think \"Should neuron #12,234,320,242 in my visual cortex fire, or not?\"\n\nIt takes a deliberate effort to visualize your brain from the outside—and then you still don't see your actual brain; you imagine what you _think_ is there, hopefully based on science, but regardless, you don't have any direct access to neural network structures from introspection.  That's why the ancient Greeks didn't invent computational neuroscience.\n\nWhen you look at Network 2, you are seeing from the _outside;_ but the way that neural network structure feels from the _inside,_ if you yourself _are_ a brain running that algorithm, is that even after you know every characteristic of the object, you still find yourself wondering:  \"But is it a blegg, or not?\"\n\nThis is a great gap to cross, and I've seen it stop people in their tracks.  Because we don't instinctively see our intuitions as \"intuitions\", we just see them as the world.  When you look at a green cup, you don't think of yourself as seeing a picture reconstructed in your visual cortex—although that _is_ what you are seeing—you just see a green cup.  You think, \"Why, look, this cup is green,\" not, \"The picture in my visual cortex of this cup is green.\"\n\nAnd in the same way, when people argue over whether the falling tree makes a sound, or whether Pluto is a planet, they don't see themselves as arguing over whether a categorization should be active in their neural networks.  It seems like either the tree makes a sound, or not.\n\nWe know where Pluto is, and where it's going; we know Pluto's shape, and Pluto's mass—but is it a planet?  And yes, there were people who said this was a fight over definitions—but even that is a Network 2 sort of perspective, because you're arguing about how the central unit ought to be wired up.  If you were a mind constructed along the lines of Network 1, you wouldn't say \"It depends on how you define 'planet',\" you would just say, \"Given that we know Pluto's orbit and shape and mass, there is no question left to ask.\"  Or, rather, that's how it would _feel_—it would _feel_ like there was no question left—if you were a mind constructed along the lines of Network 1.\n\nBefore you can question your intuitions, you have to realize that what your mind's eye is looking at _is_ an intuition—some cognitive algorithm, as seen from the inside—rather than a direct perception of the Way Things Really Are.\n\nPeople [cling to their intuitions](/lw/n1/allais_malaise/), I think, not so much because they believe their cognitive algorithms are perfectly reliable, but because they can't see their intuitions _as the way their cognitive algorithms happen to look from the inside._\n\nAnd so everything you try to say about how the native cognitive algorithm goes astray, ends up being contrasted to their direct perception of the Way Things Really Are—and discarded as obviously wrong."
    },
    "voteCount": 179,
    "forceInclude": true
  },
  {
    "_id": "HLqWn5LASfhhArZ7w",
    "url": null,
    "title": "Expecting Short Inferential Distances",
    "slug": "expecting-short-inferential-distances",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "Inferential Distance"
      },
      {
        "name": "Rationality"
      },
      {
        "name": "Evolutionary Psychology"
      },
      {
        "name": "Conversation (topic)"
      },
      {
        "name": "Public Discourse"
      },
      {
        "name": "Illusion of Transparency"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "_Homo sapiens_’s environment of evolutionary adaptedness (a.k.a. EEA or “ancestral environment”) consisted of hunter-gatherer bands of at most [200 people](https://en.wikipedia.org/wiki/Dunbar%27s_number), with no writing. All inherited knowledge was passed down by speech and memory.\n\nIn a world like that, all background knowledge is universal knowledge. All information not strictly private is public, period.\n\nIn the ancestral environment, you were unlikely to end up more than _one inferential step_ away from anyone else. When you discover a new oasis, you don’t have to explain to your fellow tribe members what an oasis is, or why it’s a good idea to drink water, or how to walk. Only you know where the oasis lies; this is private knowledge. But everyone has the background to understand your description of the oasis, the concepts needed to think about water; this is universal knowledge. When you explain things in an ancestral environment, you almost _never_ have to explain your concepts. At most you have to explain _one_ new concept, not two or more simultaneously.\n\nIn the ancestral environment there were no abstract disciplines with vast bodies of carefully gathered evidence generalized into elegant theories transmitted by written books whose conclusions are _a hundred inferential steps removed_ from universally shared background premises.\n\nIn the ancestral environment, anyone who says something with no obvious support is a liar or an idiot. You’re not likely to think, “Hey, maybe this person has well-supported background knowledge that no one in my band has even heard of,” because it was a reliable invariant of the ancestral environment that this didn’t happen.\n\nConversely, if you say something blatantly obvious and the other person doesn’t see it, _they’re_ the idiot, or they’re being deliberately obstinate to annoy you.\n\nAnd to top it off, if someone says something with no obvious support and _expects_ you to believe it—acting all indignant when you don’t—then they must be _crazy._\n\nCombined with the illusion of transparency and [self-anchoring](http://lesswrong.com/lw/kf/selfanchoring/) (the tendency to model other minds as though the were slightly modified versions of oneself), I think this explains a _lot_ about the legendary difficulty most scientists have in communicating with a lay audience—or even communicating with scientists from other disciplines. When I observe failures of explanation, I usually see the explainer taking _one_ step back, when they need to take two or more steps back. Or listeners assume that things should be visible in one step, when they take two or more steps to explain. Both sides act as if they expect very short inferential distances from universal knowledge to any new knowledge.\n\nA biologist, speaking to a physicist, can justify evolution by saying it is the simplest explanation. But not everyone on Earth has been inculcated with that legendary history of science, from Newton to Einstein, which invests the phrase “simplest explanation” with its awesome import: a Word of Power, spoken at the birth of theories and carved on their tombstones. To someone else, “But it’s the simplest explanation!” may sound like an interesting but hardly knockdown argument; it doesn’t feel like all that powerful a tool for comprehending office politics or fixing a broken car. Obviously the biologist is infatuated with their own ideas, too arrogant to be open to alternative explanations which sound just as plausible. (If it sounds plausible to me, it should sound plausible to any sane member of my band.)\n\nAnd from the biologist’s perspective, they can understand how evolution might sound a little odd at first—but when someone rejects evolution even after the biologist explains that it’s the simplest explanation, well, it’s clear that nonscientists are just idiots and there’s no point in talking to them.\n\nA clear argument has to lay out an inferential _pathway_, starting from what the audience _already knows or accepts_. If you don’t recurse far enough, you’re just talking to yourself.\n\nIf at any point you make a statement without obvious justification in arguments you’ve previously supported, the audience just thinks you’re crazy.\n\nThis also happens when you allow yourself to be seen _visibly_ attaching greater weight to an argument than is justified in the eyes of the audience _at that time_. For example, talking as if you think “simpler explanation” is a knockdown argument for evolution (which it is), rather than a sorta-interesting idea (which it sounds like to someone who hasn’t been raised to revere Occam’s Razor).\n\nOh, and you’d better not drop any hints that _you_ think you’re working a dozen inferential steps away from what the audience knows, or that _you_ think you have special background knowledge not available to them. The audience doesn’t know anything about an evolutionary-psychological argument for a cognitive bias to underestimate inferential distances leading to traffic jams in communication. They’ll just think you’re condescending.\n\nAnd if you think you can explain the concept of “systematically underestimated inferential distances” briefly, in just a few words, I’ve got some sad news for you . . ."
    },
    "voteCount": 221,
    "forceInclude": true
  },
  {
    "_id": "sSqoEw9eRP2kPKLCz",
    "url": null,
    "title": "Illusion of Transparency:  Why No One Understands You",
    "slug": "illusion-of-transparency-why-no-one-understands-you",
    "author": "Eliezer Yudkowsky",
    "question": false,
    "tags": [
      {
        "name": "Conversation (topic)"
      },
      {
        "name": "Inferential Distance"
      },
      {
        "name": "Calibration"
      },
      {
        "name": "Illusion of Transparency"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "In hindsight bias, people who know the outcome of a situation believe the outcome should have been easy to predict in advance. Knowing the outcome, we reinterpret the situation in light of that outcome. Even when warned, we can’t de-interpret to empathize with someone who doesn’t know what we know.\n\nClosely related is the *illusion of transparency*: We always know what *we* mean by our words, and so we expect others to know it too. Reading our own writing, the intended interpretation falls easily into place, guided by our knowledge of what we really meant. It’s hard to empathize with someone who must interpret blindly, guided only by the words.\n\nJune recommends a restaurant to Mark; Mark dines there and discovers (a) unimpressive food and mediocre service or (b) delicious food and impeccable service. Then Mark leaves the following message on June’s answering machine: “June, I just finished dinner at the restaurant you recommended, and I must say, it was marvelous, just marvelous.” Keysar (1994) presented a group of subjects with scenario (a), and 59% thought that Mark’s message was sarcastic *and that Jane would perceive the sarcasm*.^1^ Among other subjects, told scenario (b), only 3% thought that Jane would perceive Mark’s message as sarcastic. Keysar and Barr (2002) seem to indicate that an actual voice message was played back to the subjects.^2^ Keysar (1998) showed that if subjects were told that the restaurant was horrible *but that Mark wanted to conceal his response*, they believed June would not perceive sarcasm in the (same) message:^3^\n\n> They were just as likely to predict that she would perceive sarcasm when he attempted to conceal his negative experience as when he had a positive experience and was truly sincere. So participants took Mark’s *communicative intention* as transparent. It was as if they assumed that June would perceive whatever intention Mark wanted her to perceive.^4^\n\n“The goose hangs high” is an archaic English idiom that has passed out of use in modern language. Keysar and Bly (1995) told one group of subjects that “the goose hangs high” meant that the future looks good; another group of subjects learned that “the goose hangs high” meant the future looks gloomy.^5^ Subjects were then asked which of these two meanings an *uninformed* listener would be more likely to attribute to the idiom. Each group thought that listeners would perceive the meaning presented as “standard.”^6^\n\nKeysar and Henly (2002) tested the calibration of speakers: Would speakers underestimate, overestimate, or correctly estimate how often listeners understood them?^7^ Speakers were given ambiguous sentences (“The man is chasing a woman on a bicycle.”) and disambiguating pictures (a man running after a cycling woman). Speakers were then asked to utter the words in front of addressees, and asked to estimate how many addressees understood the intended meaning. Speakers thought that they were understood in 72% of cases and were actually understood in 61% of cases. When addressees did not understand, speakers thought they did in 46% of cases; when addressees did understand, speakers thought they did not in only 12% of cases.\n\nAdditional subjects who *overheard* the explanation showed no such bias, expecting listeners to understand in only 56% of cases.\n\nAs Keysar and Barr note, two days before Germany’s attack on Poland, Chamberlain sent a letter intended to make it clear that Britain would fight if any invasion occurred. The letter, phrased in polite diplomatese, was heard by Hitler as conciliatory—and the tanks rolled.\n\nBe not too quick to blame those who misunderstand your perfectly clear sentences, spoken or written. Chances are, your words are more ambiguous than you think.\n\n* * *\n\n^1^ Boaz Keysar, “The Illusory Transparency of Intention: Linguistic Perspective Taking in Text,” *Cognitive Psychology* 26 (2 1994): 165–208.\n\n^2^ Boaz Keysar and Dale J. Barr, “Self-Anchoring in Conversation: Why Language Users Do Not Do What They ‘Should,’” in *Heuristics and Biases: The Psychology of Intuitive Judgment*, ed. Griffin Gilovich and Daniel Kahneman (New York: Cambridge University Press, 2002), 150–166.\n\n^3^ Boaz Keysar, “Language Users as Problem Solvers: Just What Ambiguity Problem Do They Solve?,” in *Social and Cognitive Approaches to Interpersonal Communication*, ed. Susan R. Fussell and Roger J. Kreuz (Mahwah, NJ: Lawrence Erlbaum Associates, 1998), 175–200.\n\n^4^ The wording here is from Keysar and Barr.\n\n^5^ Boaz Keysar and Bridget Bly, “Intuitions of the Transparency of Idioms: Can One Keep a Secret by Spilling the Beans?,” *Journal of Memory and Language* 34 (1 1995): 89–109.\n\n^6^ Other idioms tested included “come the uncle over someone,” “to go by the board,” and “to lay out in lavender.” Ah, English, such a lovely language.\n\n^7^ Boaz Keysar and Anne S. Henly, “Speakers’ Overestimation of Their Effectiveness,” *Psychological Science* 13 (3 2002): 207–212."
    },
    "voteCount": 120,
    "forceInclude": true
  },
  {
    "_id": "wzxneh7wxkdNYNbtB",
    "url": null,
    "title": "When Science Can't Help",
    "slug": "when-science-can-t-help",
    "author": "Eliezer_Yudkowsky",
    "question": false,
    "tags": [
      {
        "name": "Practice & Philosophy of Science"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "[Once upon a time, a younger Eliezer had a stupid theory](/lw/iy/my_wild_and_reckless_youth/).  Let's say that Eliezer~18~'s stupid theory was that consciousness was caused by closed timelike curves hiding in quantum gravity.  This isn't the whole story, not even close, but it will do for a start.\n\nAnd there came a point where I looked back, and realized:\n\n1.  I had carefully followed everything I'd been told was Traditionally Rational, in the course of going astray.  For example, I'd been careful to only believe in stupid theories that made novel experimental predictions, e.g., that neuronal microtubules would be found to support coherent quantum states.\n2.  Science would have been perfectly fine with my spending ten years trying to test my stupid theory, only to get a negative experimental result, so long as I then said, \"Oh, well, I guess my theory was wrong.\"\n\nFrom Science's perspective, that is how things are _supposed_ to work—happy fun for everyone.  You admitted your error!  Good for you!  Isn't that what Science is all about?\n\nBut what if I didn't want to waste ten years?\n\nWell... Science didn't have much to say about _that._  How could Science say which theory was right, in _advance_ of the experimental test?  Science doesn't care where your theory comes from—it just says, \"Go test it.\"\n\nThis is the great strength of Science, and also its great weakness.\n\n[Gray Area asked](/lw/qb/science_doesnt_trust_your_rationality/k0k):\n\n> Eliezer, why are you concerned with untestable questions?\n\nBecause questions that are _easily immediately_ tested are hard for Science to get wrong.\n\nI mean, sure, when there's already definite unmistakable experimental evidence available, go with it.  Why on Earth wouldn't you?\n\nBut sometimes a question will have very large, very definite experimental consequences in your future—but you can't easily test it experimentally _right now_—and yet there _is_ a strong _rational_ argument.\n\nMacroscopic quantum superpositions are readily testable:  It would just take nanotechnologic precision, very low temperatures, and a nice clear area of interstellar space.  Oh, sure, you can't do it _right now_, because it's _too expensive_ or _impossible for today's technology_ or something like that—but in theory, sure!  Why, maybe someday they'll run whole civilizations on macroscopically superposed quantum computers, way out in a well-swept volume of a Great Void.  (Asking what quantum non-realism says about the status of any observers inside these computers, helps to reveal the underspecification of quantum non-realism.)\n\nThis doesn't seem immediately pragmatically relevant to your life, I'm guessing, but it establishes the pattern:  Not everything with future consequences is _cheap_ to test _now_.\n\nEvolutionary psychology is another example of a case where rationality has to take over from science.  While theories of evolutionary psychology form a connected whole, only some of those theories are readily testable experimentally.  But you still need the other parts of the theory, because they form a connected web that helps you to form the hypotheses that are actually testable—and then the helper hypotheses are supported in a Bayesian sense, but not supported experimentally.  Science would render a verdict of \"not proven\" on individual parts of a connected theoretical mesh that is experimentally productive as a whole.  We'd need a new kind of verdict for that, something like \"indirectly supported\".\n\nOr what about cryonics?\n\nCryonics is an archetypal example of an extremely important issue (150,000 people die per day) that will have huge consequences in the foreseeable future, but doesn't offer definite unmistakable experimental evidence that we can get _right now._\n\nSo do you say, \"I don't believe in cryonics because it hasn't been experimentally proven, and you shouldn't believe in things that haven't been experimentally proven?\"\n\nWell, from a Bayesian perspective, that's incorrect.  [Absence of evidence is evidence of absence](/lw/ih/absence_of_evidence_is_evidence_of_absence/) only to the degree that we could reasonably expect the evidence to appear.  If someone is trumpeting that snake oil cures cancer, you can reasonably expect that, _if the snake oil was actually curing cancer,_ some scientist would be performing a controlled study to verify it—that, at the least, doctors would be reporting case studies of amazing recoveries—and so the absence of this evidence is strong evidence of absence.  But \"gaps in the fossil record\" are not strong evidence against evolution; fossils form only rarely, and _even if an intermediate species did in fact exist,_ you cannot expect with high probability that Nature will obligingly fossilize it and that the fossil will be discovered.\n\nReviving a cryonically frozen mammal is just not something you'd expect to be able to do with modern technology, _even if future nanotechnologies could in fact perform a successful revival_.  That's how I see Bayes seeing it.\n\nOh, and as for the actual arguments _for_ cryonics—I'm not going to go into those at the moment.  But if you followed the [physics and anti-Zombie sequences](/lw/pm/identity_isnt_in_specific_atoms/), it should now seem a lot more plausible, that whatever preserves the pattern of synapses, preserves as much of \"you\" as is preserved from one night's sleep to morning's waking.\n\nNow, to be fair, someone who says, \"I don't believe in cryonics because it hasn't been proven experimentally\" is _misapplying_ the rules of Science; this is not a case where science actually gives the _wrong answer._  In the absence of a definite experimental test, the verdict of science here is \"Not proven\".  Anyone who interprets that as a rejection is taking an extra step outside of science, not a misstep within science.\n\n[John McCarthy's Wikiquotes page](http://en.wikiquote.org/wiki/John_McCarthy) has him saying, \"Your statements amount to saying that if AI is possible, it should be easy. Why is that?\"  The Wikiquotes page doesn't say what McCarthy was responding to, but I could venture a guess.\n\nThe general mistake probably arises because there _are_ cases where the absence of scientific proof is strong evidence—because an experiment would be readily performable, and so failure to perform it is itself suspicious.  (Though not as suspicious as I used to think—with all the strangely varied anecdotal evidence coming in from respected sources, why the _hell_ isn't anyone testing [Seth Roberts's theory of appetite suppression](http://sethroberts.net/about/whatmakesfoodfattening.pdf)?)\n\nAnother confusion factor may be that if you test Pharmaceutical X on 1000 subjects and find that 56% of the control group and 57% of the experimental group recover, some people will call that a verdict of \"Not proven\".  I would call it an experimental verdict of \"Pharmaceutical X doesn't work well, if at all\".  Just because this verdict is theoretically retractable in the face of new evidence, doesn't make it ambiguous.\n\nIn any case, right now you've got people dismissing cryonics out of hand as \"not scientific\", like it was some kind of pharmaceutical you could easily administer to 1000 patients and see what happened.  \"Call me when cryonicists actually revive someone,\" they say; which, as Mike Li observes, is like saying \"I refuse to get into this ambulance; call me when it's actually at the hospital\".  Maybe Martin Gardner warned them against believing in strange things without experimental evidence.  So they wait for the definite unmistakable verdict of Science, while their family and friends and 150,000 people per day are dying _right now,_ and might or might not be savable—\n\n—a calculated bet you could only make _rationally._\n\nThe drive of Science is to obtain a mountain of evidence so huge that not even fallible human scientists can misread it.  But even _that_ sometimes goes wrong, when people become confused about which theory predicts what, or bake extremely-hard-to-test components into an early version of their theory.  And sometimes you just can't get clear experimental evidence at all.\n\nEither way, you have to try to do the thing that Science [doesn't trust anyone to do](/lw/qb/science_doesnt_trust_your_rationality/)—think rationally, and figure out the answer _before_ you get clubbed over the head with it.\n\n(Oh, and sometimes a _disconfirming_ experimental result looks like:  \"[Your entire species has just been wiped out!](http://www.global-catastrophic-risks.com/)  You are now scientifically required to relinquish your theory.  If you publicly recant, good for you!  Remember, it takes a strong mind to give up strongly held beliefs.  Feel free to try another hypothesis next time!\")"
    },
    "voteCount": 33,
    "forceInclude": true
  },
  {
    "_id": "xTyuQ3cgsPjifr7oj",
    "url": null,
    "title": "Faster Than Science",
    "slug": "faster-than-science",
    "author": "Eliezer_Yudkowsky",
    "question": false,
    "tags": [
      {
        "name": "Practice & Philosophy of Science"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "I sometimes say that the method of science is to amass such an enormous mountain of evidence that even scientists cannot ignore it; and that this is the distinguishing characteristic of a scientist, a non-scientist will ignore it anyway.\n\nMax Planck was even less optimistic:\n\n> \"A new scientific truth does not triumph by convincing its opponents and making them see the light, but rather because its opponents eventually die, and a new generation grows up that is familiar with it.\"\n\nI am much tickled by this notion, because it implies that the power of science to distinguish truth from falsehood ultimately rests on the good taste of grad students.\n\nThe _gradual_ increase in acceptance of [many-worlds](/lw/q8/many_worlds_one_best_guess/) in academic physics, suggests that there are physicists who will only accept a new idea given some _combination_ of epistemic justification, and a sufficiently large academic pack in whose company they can be comfortable.  As more physicists accept, the pack grows larger, and hence more people go over their individual thresholds for conversion—with the epistemic justification remaining essentially the same.\n\nBut Science still gets there _eventually,_ and this is sufficient for the ratchet of Science to move forward, and raise up a technological civilization.\n\nScientists can be moved by groundless prejudices, by undermined intuitions, by raw herd behavior—the panoply of human flaws.  Each time a scientist shifts belief for epistemically unjustifiable reasons, it requires more evidence, or new arguments, to cancel out the noise.\n\nThe \"collapse of the wavefunction\" has no experimental justification, but it appeals to the (undermined) intuition of a single world.  Then it may take an extra argument—say, that collapse violates Special Relativity—to begin the slow academic disintegration of an idea that [should never have been assigned non-negligible probability in the first place](/lw/q7/if_manyworlds_had_come_first/).\n\nFrom a Bayesian perspective, human academic science as a whole is a highly inefficient processor of evidence.  Each time an unjustifiable argument shifts belief, you need an extra justifiable argument to shift it back.  The social process of science leans on extra evidence to overcome cognitive noise.\n\nA more charitable way of putting it is that scientists will adopt positions that are theoretically _insufficiently extreme_, compared to the ideal positions that scientists _would_ adopt, if they were Bayesian AIs and could [trust themselves](/lw/qb/science_doesnt_trust_your_rationality/) to reason clearly.\n\nBut don't be too charitable.  The noise we are talking about is not all innocent mistakes.  In many fields, debates drag on for decades after they should have been settled.  And _not_ because the scientists on both sides [refuse to trust themselves](/lw/qb/science_doesnt_trust_your_rationality/) and agree they should look for additional evidence.  But because one side keeps throwing up more and more ridiculous objections, and demanding more and more evidence, from an entrenched position of academic power, long after it becomes clear from which quarter the winds of evidence are blowing.  (I'm thinking here about the debates surrounding the invention of [evolutionary psychology](/lw/l1/evolutionary_psychology/), not about many-worlds.)\n\nIs it possible for individual humans or groups to process evidence more efficiently—reach correct conclusions faster—than human academic science as a whole?\n\n\"Ideas are tested by experiment.  That is the core of science.\"  And this must be true, because if you can't trust [Zombie Feynman](http://xkcd.com/397/), who _can_ you trust?\n\nYet where do the _ideas_ come from?\n\nYou may be tempted to reply, \"They come from scientists.  Got any other questions?\"  In Science you're not supposed to care _where_ the hypotheses come from—just whether they pass or fail experimentally.\n\nOkay, but if you remove _all_ new ideas, the scientific process as a whole stops working because it has no alternative hypotheses to test.  So inventing new ideas is not a dispensable part of the process.\n\nNow put your Bayesian goggles back on.  As described in [Einstein's Arrogance](/lw/jo/einsteins_arrogance/), there are queries that are not binary—where the answer is not \"Yes\" or \"No\", but drawn from a larger space of structures, e.g., the space of equations.  In such cases it takes far more Bayesian evidence to _promote a hypothesis to your attention_ than to _confirm the hypothesis._\n\nIf you're working in the space of all equations that can be specified in 32 bits or less, you're working in a space of 4 billion equations.  It takes far more Bayesian evidence to raise one of those hypotheses to the 10% probability level, than it requires _further_ Bayesian evidence to raise the hypothesis from 10% to 90% probability.\n\nWhen the idea-space is large, coming up with ideas worthy of testing, involves much more work—in the [Bayesian-thermodynamic sense of \"work\"](/lw/o5/the_second_law_of_thermodynamics_and_engines_of/)—than _merely_ obtaining an experimental result with p<0.0001 for the new hypothesis over the old hypothesis.\n\nIf this doesn't seem obvious-at-a-glance, pause here and read [Einstein's Arrogance](/lw/jo/einsteins_arrogance/).\n\nThe scientific process has always relied on scientists to come up with hypotheses to test, via some process not further specified by Science.  Suppose you came up with some way of generating hypotheses that was completely crazy—say, pumping a robot-controlled Ouija board with the digits of pi—and the resulting suggestions kept on getting verified experimentally.  The pure ideal essence of Science wouldn't skip a beat.  The pure ideal essence of Bayes would burst into flames and die.\n\n(Compared to Science, Bayes is [falsified by more of the possible outcomes](/lw/if/your_strength_as_a_rationalist/).)\n\nThis doesn't mean that the process of deciding which ideas to test is _unimportant_ to Science.  It means that Science doesn't _specify_ it.\n\n_In practice_, the robot-controlled Ouija board doesn't work. In practice, there are some scientific queries with a large enough answer space, that picking models at random to test, it would take zillions of years to hit on a model that made good predictions—like getting monkeys to type Shakespeare.\n\nAt the _frontier_ of science—the boundary between ignorance and knowledge, where science _advances_—the process relies on at least some individual scientists (or working groups) seeing things that are not yet confirmed by Science.  That's how they know which hypotheses to test, in advance of the test itself.\n\nIf you take your Bayesian goggles off, you can say, \"Well, they don't have to know, they just have to guess.\"  If you put your Bayesian goggles back on, you realize that \"guessing\" with 10% probability requires nearly as much epistemic work to have been successfully performed, behind the scenes, as \"guessing\" with 80% probability—at least for large answer spaces.\n\nThe scientist may not _know_ he has done this epistemic work successfully, in advance of the experiment; but he must, in fact, have done it successfully!  Otherwise he will not even _think_ of the correct hypothesis.  In large answer spaces, anyway.\n\nSo the scientist makes the novel prediction, performs the experiment, publishes the result, and _now_ Science knows it too.  It is now part of the [publicly accessible knowledge of humankind](/lw/in/scientific_evidence_legal_evidence_rational/), that anyone can verify for themselves.\n\nIn between was an interval where the scientist rationally knew something that the public social process of science hadn't yet confirmed.  And this is not a trivial interval, though it may be short; for it is where the _frontier_ of science lies, the advancing border.\n\nAll of this is more true for non-routine science than for routine science, because it is a notion of large answer spaces where the answer is not \"Yes\" or \"No\" or drawn from a small set of obvious alternatives.  It is much easier to train people to test ideas, than to have good ideas to test."
    },
    "voteCount": 35,
    "forceInclude": true
  },
  {
    "_id": "5bJyRMZzwMov5u3hW",
    "url": null,
    "title": "Science Doesn't Trust Your Rationality",
    "slug": "science-doesn-t-trust-your-rationality",
    "author": "Eliezer_Yudkowsky",
    "question": false,
    "tags": [
      {
        "name": "Practice & Philosophy of Science"
      },
      {
        "name": "Rationality"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "[Scott Aaronson](http://scottaaronson.com/blog/?p=326) suggests that Many-Worlds and libertarianism are similar in that they are both cases of bullet-swallowing, rather than bullet-dodging:\n\n> Libertarianism and MWI are both are grand philosophical theories that start from premises that almost all educated people accept (quantum mechanics in the one case, Econ 101 in the other), and claim to reach conclusions that most educated people reject, or are at least puzzled by (the existence of parallel universes / the desirability of eliminating fire departments).\n\nNow _there's_ an analogy that would never have occurred to me.\n\nI've previously argued that [Science rejects Many-Worlds but Bayes accepts it](/lw/qa/the_dilemma_science_or_bayes/).  (Here, \"Science\" is capitalized because we are talking about the idealized form of Science, not just the actual social process of science.)\n\nIt furthermore seems to me that there is a _deep_ analogy between (small-'l') libertarianism and Science:\n\n1.  Both are based on a pragmatic distrust of reasonable-sounding arguments.\n2.  Both try to build systems that are more trustworthy than the people in them.\n3.  Both accept that people are flawed, and try to harness their flaws to power the system.\n\nThe core argument for libertarianism is historically motivated distrust of lovely theories of \"How much _better_ society would be, if we just made a rule that said XYZ.\"  If that sort of trick actually _worked,_ then more regulations would correlate to higher economic growth as society moved from local to global optima.  But when some person or interest group gets enough power to start doing everything they think is a good idea, history says that what actually _happens_ is Revolutionary France or Soviet Russia.\n\nThe plans that in lovely theory should have made everyone happy ever after, don't have the results predicted by reasonable-sounding arguments.  And power corrupts, and attracts the corrupt.\n\nSo you regulate as little as possible, because you can't trust the lovely theories and you can't trust the people who implement them.\n\nYou don't shake your finger at people for being selfish.  You try to build an efficient system of production out of selfish participants, by requiring transactions to be voluntary.  So people are forced to play positive-sum games, because that's how they get the _other_ party to sign the contract.  With violence restrained and contracts enforced, individual selfishness can power a globally productive system.\n\nOf course none of this works quite so well in practice as in theory, and I'm not going to go into market failures, commons problems, etc.  The core argument for libertarianism is not that libertarianism would work in a perfect world, but that it degrades gracefully into real life.  Or rather, degrades less awkwardly than any other known economic principle.  (People who see Libertarianism as the [perfect](/lw/lm/affective_death_spirals/) solution for perfect people, strike me as kinda missing the point of the \"pragmatic distrust\" thing.)\n\nScience first came to know itself as a rebellion against trusting the word of Aristotle. If the people of that revolution had merely said, \"Let us trust ourselves, not Aristotle!\" they would have flashed and faded like the French Revolution.\n\nBut the Scientific Revolution lasted because—like the American Revolution—the architects propounded a stranger philosophy:  \"Let us trust no one!  Not even ourselves!\"\n\nIn the beginning came the idea that we can't just toss out Aristotle's armchair reasoning and replace it with _different_ armchair reasoning.  We need to talk to Nature, and actually _listen_ to what It says in reply.  This, itself, was a stroke of genius.\n\nBut then came the challenge of implementation. People are stubborn, and may not want to accept the verdict of experiment.  Shall we shake a disapproving finger at them, and say \"Naughty\"?\n\nNo; we assume and accept that each individual scientist may be crazily attached to their personal theories.  Nor do we assume that anyone can be trained out of this tendency—we don't try to choose Eminent Judges who are supposed to be impartial.\n\nInstead, we try to _harness_ the individual scientist's stubborn desire to prove their personal theory, by saying:  \"Make a new experimental prediction, and do the experiment.  If you're right, and the experiment is replicated, you win.\"  So long as scientists believe this is true, they have a motive to do experiments that can _falsify_ their own theories.  Only by accepting the possibility of defeat is it possible to win.  And any great claim will require replication; this gives scientists a motive to be honest, on pain of great embarrassment.\n\nAnd so the stubbornness of individual scientists is harnessed to produce a steady stream of knowledge at the group level.  The System is _somewhat_ more trustworthy than its parts.\n\nLibertarianism secretly relies on most individuals being prosocial enough to tip at a restaurant they won't ever visit again.  An economy of [genuinely selfish](/lw/kx/fake_selfishness/) human-level agents would implode.  Similarly, Science relies on most scientists not committing sins so egregious that they can't rationalize them away.\n\nTo the extent that scientists believe they can promote their theories by playing academic politics—or game the statistical methods to potentially win without a chance of losing—or to the extent that nobody bothers to replicate claims—science degrades in effectiveness.  But it degrades gracefully, as such things go.\n\nThe part where the successful predictions belong to the theory and theorists who originally made them, and cannot just be stolen by a theory that comes along later—_without_ a novel experimental prediction—is an important feature of this social process.\n\nThe final upshot is that Science is not easily reconciled with probability theory.  If you do a probability-theoretic calculation _correctly_, you're going to get the _rational_ answer.  Science doesn't trust your rationality, and it doesn't rely on your ability to use probability theory as the arbiter of truth.  It wants you to set up a definitive experiment.\n\nRegarding Science as a mere approximation to some probability-theoretic ideal of rationality... would certainly seem to be _rational_.  There seems to be an extremely reasonable-sounding argument that Bayes's Theorem is the [hidden structure](/lw/o7/searching_for_bayesstructure/) that explains why Science works.  But to subordinate Science to the grand schema of Bayesianism, and let Bayesianism come in and override Science's verdict when that seems appropriate, is not a trivial step!\n\nScience is built around the assumption that you're _too stupid and self-deceiving_ to just use Solomonoff induction.  After all, if it was that simple, we wouldn't need a social process of science... right?\n\nSo, are you going to believe in [faster-than-light quantum \"collapse\" fairies](/lw/q7/if_manyworlds_had_come_first/) after all?  Or do you think you're smarter than that?"
    },
    "voteCount": 45,
    "forceInclude": true
  },
  {
    "_id": "wustx45CPL5rZenuo",
    "url": null,
    "title": "No Safe Defense, Not Even Science",
    "slug": "no-safe-defense-not-even-science",
    "author": "Eliezer_Yudkowsky",
    "question": false,
    "tags": [
      {
        "name": "Practice & Philosophy of Science"
      },
      {
        "name": "Rationality"
      },
      {
        "name": "Courage"
      },
      {
        "name": "Trust"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "I don't ask my friends about their childhoods—I lack social curiosity—and so I don't know how much of a trend this really is:\n\nOf the people I know who are reaching upward as rationalists, who volunteer information about their childhoods, there is a surprising tendency to hear things like:  \"My family joined a cult and I had to break out,\" or \"One of my parents was clinically insane and I had to learn to filter out reality from their madness.\"\n\nMy own experience with growing up in an Orthodox Jewish family seems tame by comparison... but it accomplished the same outcome:  It broke my core emotional trust in the sanity of the people around me.\n\nUntil this core emotional trust is broken, you don't start growing as a rationalist.  I have trouble putting into words why this is so.  Maybe any _unusual_ skills you acquire—anything that makes you _unusually_ rational—requires you to zig when other people zag.  Maybe that's just too scary, if the world still seems like a sane place unto you.\n\nOr maybe you don't bother putting in the hard work to be extra bonus sane, if normality doesn't scare the hell out of you.\n\nI know that many aspiring rationalists seem to run into roadblocks around things like cryonics or many-worlds.  Not that they don't see the logic; they see the logic and wonder, \"Can this really be true, when it seems so obvious now, and yet none of the people around me believe it?\"\n\nYes.  Welcome to the Earth where ethanol is made from corn and environmentalists oppose nuclear power.  I'm sorry.\n\n(See also:  [Cultish Countercultishness](/lw/md/cultish_countercultishness/).  If you end up in the frame of mind of _nervously seeking reassurance_, this is never a good thing—even if it's because you're about to believe something that sounds logical but could cause other people to look at you funny.)\n\nPeople who've had their trust broken in the sanity of the people around them, seem to be able to evaluate strange ideas on their merits, without feeling nervous about their strangeness.  The glue that binds them to their current place has dissolved, and they can walk in some direction, hopefully forward.\n\n[Lonely dissent](/lw/mb/lonely_dissent/), I called it.  True dissent doesn't feel like going to school wearing black; it feels like going to school wearing a clown suit.\n\nThat's what it takes to be the lone voice who says, \"If you really think you know who's going to win the election, why aren't you picking up the [free money](/lw/ni/buy_now_or_forever_hold_your_peace/) on the Intrade prediction market?\" while all the people around you are thinking, \"It is good to be an individual and form your own opinions, the shoe commercials told me so.\"\n\nMaybe in some other world, some alternate Everett branch with a saner human population, things would be different... but in this world, I've never seen anyone begin to grow as a rationalist until they make a deep emotional break with the [wisdom of their pack](/lw/m9/aschs_conformity_experiment/).\n\nMaybe in another world, things would be different.  And maybe not.  I'm not sure that human beings realistically _can_ trust and think at the same time.\n\nOnce upon a time, there was something I trusted.\n\nEliezer~18~ trusted Science.\n\nEliezer~18~ dutifully acknowledged that the social process of science was flawed.  Eliezer~18~ dutifully acknowledged that academia was slow, and misallocated resources, and played favorites, and mistreated its precious heretics.\n\nThat's the convenient thing about acknowledging flaws in _people_ who failed to live up to your ideal; you don't have to question the ideal itself.\n\nBut who could possibly be foolish enough to question, \"The experimental method shall decide which hypothesis wins\"?\n\nPart of what fooled Eliezer~18~ was a general problem he had, with [an aversion to ideas that resembled things idiots had said](/lw/lw/reversed_stupidity_is_not_intelligence/).  Eliezer~18~ had seen plenty of people questioning the ideals of Science Itself, and without exception they were all on the Dark Side.  People who questioned the ideal of Science were invariably trying to sell you snake oil, or trying to safeguard their favorite form of stupidity from criticism, or trying to disguise their personal resignation as a Deeply Wise acceptance of futility.\n\nIf there'd been any other ideal that was a few centuries old, the young Eliezer would have looked at it and said, \"I wonder if this is really right, and whether there's a way to [do better](/lw/h8/tsuyoku_naritai_i_want_to_become_stronger/).\"  But not the ideal of Science.  Science was the master idea, the idea that let you change ideas.  You could question it, but you were meant to [question it and then accept it](/lw/ib/the_proper_use_of_doubt/), not actually say, \"Wait!  This is wrong!\"\n\nThus, when once upon a time I came up with a stupid idea, I thought I was behaving virtuously if I made sure there was a Novel Prediction, and professed that I wished to test my idea experimentally.  I thought I had done everything I was obliged to do.\n\nSo I thought I was _safe_—not safe from any particular external threat, but safe on some deeper level, like a child who trusts their parent and has obeyed all the parent's rules.\n\nI'd long since been broken of trust in the sanity of my family or my teachers at school.  And the other children weren't intelligent enough to compete with the conversations I could have with books.  But I trusted the books, you see.  I trusted that if I did what Richard Feynman told me to do, I would be safe.  I never thought those words aloud, but it was how I felt.\n\nWhen Eliezer~23~ realized exactly _how_ stupid the stupid theory had been—and that Traditional Rationality had not saved him from it—and that Science would have been perfectly okay with his wasting ten years testing the stupid idea, so long as afterward he admitted it was wrong...\n\n...well, I'm not going to say it was a huge emotional convulsion.  I don't really go in for that kind of drama.  It simply became obvious that I'd been stupid.\n\nThat's the trust I'm trying to break in you.  You are not safe.  Ever.\n\nNot even Science can save you.  The ideals of Science were born centuries ago, in a time when no one knew anything about probability theory or cognitive biases.  Science demands _too little_ of you, it blesses your good intentions too easily, [it is not strict _enough_](/lw/qd/science_isnt_strict_enough/), it only makes those injunctions that an [average scientist](/lw/qe/do_scientists_already_know_this_stuff/) can follow, it accepts [slowness](/lw/q9/the_failures_of_eld_science/) as a fact of life.\n\nSo don't think that if you only follow the rules of Science, that makes your reasoning defensible.\n\nThere is no known procedure you can follow that makes your reasoning defensible.\n\nThere is no known set of injunctions which you can satisfy, and know that you will not have been a fool.\n\nThere is no known morality-of-reasoning that you can do your best to obey, and know that you are thereby shielded from criticism.\n\nNo, not even if you turn to Bayescraft.  It's much harder to use and you'll never be sure that you're doing it right.\n\nThe discipline of Bayescraft is younger by far than the discipline of Science.  You will find no textbooks, no elderly mentors, no histories written of success and failure, no hard-and-fast rules laid down.  You will have to study cognitive biases, and probability theory, and evolutionary psychology, and social psychology, and other cognitive sciences, and Artificial Intelligence—and think through for yourself how to apply all this knowledge to the case of correcting yourself, since that isn't yet in the textbooks.\n\nYou don't know what your own mind is really doing. They find a new cognitive bias every week and you're never sure if you've corrected for it, or overcorrected.\n\nThe formal math is impossible to apply.  It doesn't break down as easily as John Q. Unbeliever thinks, but you're never really sure where the foundations come from.  You don't know why the universe is simple enough to understand, or why any prior works for it.  You don't know what your own priors _are,_ let alone if they're any good.\n\nOne of the problems with Science is that it's too vague to really scare you.  \"Ideas should be tested by experiment.\"  How can you go wrong with that?\n\nOn the other hand, if you have some math of probability theory laid out in front of you, and worse, _you know you can't actually use it,_ then it becomes clear that you are trying to do something difficult, and that you might well be doing it _wrong._\n\nSo you cannot trust.\n\nAnd all this that I have said, _will not be sufficient_ to break your trust.  That won't happen until you get into your first real disaster from following The Rules, not from breaking them.\n\nEliezer~18~ already had the notion that you were allowed to question Science.  Why, of course the scientific method was not itself immune to questioning!  For are we not all good rationalists?  Are we not allowed to question everything?\n\nIt was the notion that you could _actually in real life_ follow Science and fail miserably, that Eliezer~18~  didn't really, emotionally believe was possible.\n\nOh, of course he said it was possible.  Eliezer~18~ dutifully acknowledged the possibility of error, saying, \"I could be wrong, but...\"\n\nBut he didn't think failure could happen in, you know, real life.  You were supposed to look for flaws, not [actually find them](/lw/ib/the_proper_use_of_doubt/).\n\nAnd this emotional difference is a terribly difficult thing to accomplish in words, and I fear there's no way I can really warn you.\n\nYour trust will not break, until you apply all that you have learned here and from other books, and take it as far as you can go, and find that this too fails you—that you have still been a fool, and no one warned you against it—that all the most important parts were left out of the guidance you received—that some of the most precious ideals you followed, steered you in the wrong direction—\n\n—and if you still have [something to protect](/lw/nb/something_to_protect/), so that you _must_ keep going, and _cannot_ resign and wisely acknowledge the limitations of rationality—\n\n_—_then you will be ready to start your journey as a rationalist.  To take sole responsibility, to live without any trustworthy defenses, and to forge a higher Art than the one you were once taught.\n\nNo one begins to truly search for the Way until their parents have failed them, their gods are dead, and their tools have shattered in their hand.\n\n* * *\n\n**Post Scriptum:**  On reviewing a draft of this essay, I discovered a fairly inexcusable flaw in reasoning, which actually affects one of the conclusions drawn.  I am [leaving it in](http://www.overcomingbias.com/2008/02/my-favorite-lia.html).  Just in case you thought that taking my advice made you safe; or that you were supposed to look for flaws, but not find any.\n\nAnd of course, if you look too hard for a flaw, and find a flaw that is not a real flaw, and cling to it to reassure yourself of how critical you are, you will only be worse off than before...\n\nIt is living with uncertainty—knowing on a gut level that there are flaws, they are serious and you have not found them—that is the difficult thing."
    },
    "voteCount": 64,
    "forceInclude": true
  },
  {
    "_id": "46qnWRSR7L2eyNbMA",
    "url": null,
    "title": "The Lens That Sees Its Flaws",
    "slug": "the-lens-that-sees-its-flaws",
    "author": "Eliezer Yudkowsky",
    "question": false,
    "tags": [
      {
        "name": "Rationality"
      },
      {
        "name": "Gears-Level"
      },
      {
        "name": "Epistemology"
      },
      {
        "name": "Map and Territory"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "Light leaves the Sun and strikes your shoelaces and bounces off; some photons enter the pupils of your eyes and strike your retina; the energy of the photons triggers neural impulses; the neural impulses are transmitted to the visual-processing areas of the brain; and there the optical information is processed and reconstructed into a 3D model that is recognized as an untied shoelace; and so you believe that your shoelaces are untied.\n\nHere is the secret of *deliberate rationality—*this whole process is not [magic](https://www.lesswrong.com/lw/iu/mysterious_answers_to_mysterious_questions/), and you can *understand* it. You can *understand* how you see your shoelaces. You can *think* about which sort of thinking processes will create beliefs which mirror reality, and which thinking processes will not.\n\nMice can see, but they can’t understand seeing. *You* can understand seeing, and because of that, you can do things that mice cannot do. Take a moment to [marvel](https://www.lesswrong.com/lw/j3/science_as_curiositystopper/) at this, for it is indeed marvelous.\n\nMice see, but they don’t know they have visual cortexes, so they can’t correct for optical illusions. A mouse lives in a mental world that includes cats, holes, cheese and mousetraps—but not mouse brains. Their camera does not take pictures of its own lens. But we, as humans, can look at a [seemingly bizarre image](http://www.richrock.com/gifs/optical-illusion-wheels-circles-rotating.png), and realize that part of what we’re seeing is the lens itself. You don’t always have to believe your own eyes, but you have to realize that you *have* eyes—you must have distinct mental buckets for the map and the territory, for the senses and reality. Lest you think this a trivial ability, remember how rare it is in the animal kingdom.\n\nThe whole idea of Science is, simply, reflective reasoning about a more reliable process for making the contents of your mind mirror the contents of the world. It is the sort of thing mice would never invent. Pondering this business of “performing replicable experiments to falsify theories,” we can see *why* it works. Science is not a [separate magisterium](https://www.lesswrong.com/lw/i8/religions_claim_to_be_nondisprovable/), far away from real life and the understanding of ordinary mortals. Science is not something that only applies to the [inside of laboratories](https://www.lesswrong.com/lw/gv/outside_the_laboratory/). Science, itself, is an understandable process-in-the-world that correlates brains with reality.\n\nScience *makes sense*, when you think about it. But mice can’t think about thinking, which is why they don’t have Science. One should not overlook the wonder of this—or the potential power it bestows on us as individuals, not just scientific societies.\n\nAdmittedly, understanding the engine of thought may be *a little more complicated* than understanding a steam engine—but it is not a *fundamentally* different task.\n\nOnce upon a time, I went to EFNet’s #philosophy chatroom to ask, “Do you believe a nuclear war will occur in the next 20 years? If no, why not?” One person who answered the question said he didn’t expect a nuclear war for 100 years, because “All of the players involved in decisions regarding nuclear war are not interested right now.” “But why extend that out for 100 years?” I asked. “Pure hope,” was his reply.\n\nReflecting on this whole thought process, we can see why the thought of nuclear war makes the person unhappy, and we can see how his brain therefore rejects the belief. But if you imagine a billion worlds—Everett branches, or Tegmark duplicates^1^—this thought process will not [systematically correlate](https://www.lesswrong.com/lw/jl/what_is_evidence/) optimists to branches in which no nuclear war occurs.^2^\n\nTo ask which beliefs make you happy is to turn inward, not outward—it tells you something about yourself, but it is not evidence entangled with the environment. I have nothing against happiness, but it should follow from your picture of the world, rather than tampering with the mental paintbrushes.\n\nIf you can see this—if you can see that hope is shifting your *first-order* thoughts by too large a degree—if you can understand your mind as a mapping engine that has flaws—then you can apply a reflective correction. The brain is a flawed lens through which to see reality. This is true of both mouse brains and human brains. But a human brain is a flawed lens that can understand its own flaws—its systematic errors, its biases—and apply second-order corrections to them. This, *in practice,* makes the lens far more powerful. Not perfect, but far more powerful.\n\n* * *\n\n^1^ Max Tegmark, “Parallel Universes,” in *Science and Ultimate Reality: Quantum Theory,* *Cosmology, and Complexity*, ed. John D. Barrow, Paul C. W. Davies, and Charles L. Harper Jr. (New York: Cambridge University Press, 2004), 459–491, [http://arxiv.org/abs/astro-ph/0302131](http://arxiv.org/abs/astro-ph/0302131).\n\n^2^ Some clever fellow is bound to say, “Ah, but since I have hope, I'll work a little harder at my job, pump up the global economy, and thus help to prevent countries from sliding into the angry and hopeless state where nuclear war is a possibility. So the two events are related after all.” At this point, we have to drag in Bayes’s Theorem and measure the relationship quantitatively. Your optimistic nature cannot have *that* large an effect on the world; it cannot, of itself, decrease the probability of nuclear war by 20%, or however much your optimistic nature shifted your beliefs. Shifting your beliefs by a large amount, due to an event that only slightly increases your chance of being right, will still mess up your mapping."
    },
    "voteCount": 146,
    "forceInclude": true
  },
  {
    "_id": "RcZCwxFiZzE6X7nsv",
    "url": null,
    "title": "What Do We Mean By \"Rationality\"?",
    "slug": "what-do-we-mean-by-rationality-1",
    "author": "Eliezer Yudkowsky",
    "question": false,
    "tags": [
      {
        "name": "Rationality"
      },
      {
        "name": "Definitions"
      },
      {
        "name": "Distinctions"
      },
      {
        "name": "Motivational Intro Posts"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "I mean two things:\n\n1\\. **Epistemic rationality**: systematically improving the accuracy of your beliefs.\n\n2\\. **Instrumental rationality**: systematically achieving your values.\n\nThe first concept is simple enough. When you open your eyes and look at the room around you, you’ll locate your laptop in relation to the table, and you’ll locate a bookcase in relation to the wall. If something goes wrong with your eyes, or your brain, then your mental model might say there’s a bookcase where no bookcase exists, and when you go over to get a book, you’ll be disappointed.\n\nThis is what it’s like to have a false belief, a map of the world that doesn’t correspond to the territory. Epistemic rationality is about building accurate maps instead. This correspondence between belief and reality is commonly called “truth,” and I’m happy to call it that.^1^\n\nInstrumental rationality, on the other hand, is about *steering* reality—sending the future where you want it to go. It’s the art of choosing actions that lead to outcomes ranked higher in your preferences. I sometimes call this “winning.”\n\nSo rationality is about forming true beliefs and making decisions that help you win.\n\n(Where truth doesn't mean “certainty,” since we can do plenty to increase the *probability* that our beliefs are accurate even though we're uncertain; and winning doesn't mean “winning at others' expense,” since our values include *everything* we care about, including other people.)\n\nWhen people say “X is rational!” it’s usually just a more strident way of saying “I think X is true” or “I think X is good.” So why have an additional word for “rational” as well as “true” and “good”?\n\nAn analogous argument can be given against using “true.” There is no need to say “it is true that snow is white” when you could just say “snow is white.” What makes the idea of truth useful is that it allows us to talk about the general features of map-territory correspondence. “True models usually produce better experimental predictions than false models” is a useful generalization, and it’s not one you can make without using a concept like “true” or “accurate.”\n\nSimilarly, “Rational agents make decisions that maximize the probabilistic expectation of a coherent utility function” is the kind of thought that depends on a concept of (instrumental) rationality, whereas “It’s rational to eat vegetables” can probably be replaced with “It’s useful to eat vegetables” or “It’s in your interest to eat vegetables.” We need a concept like “rational” in order to note general facts about those ways of thinking that systematically produce truth or value—and the systematic ways in which we fall short of those standards.\n\nAs we’ve observed in the previous essays, experimental psychologists sometimes uncover human reasoning that seems very strange. For example, someone rates the probability “Bill plays jazz” as *less* than the probability “Bill is an accountant who plays jazz.” This seems like an odd judgment, since any particular jazz-playing accountant is obviously a jazz player. But to what higher vantage point do we appeal in saying that the judgment is *wrong* ?\n\nExperimental psychologists use two gold standards: *probability theory*, and *decision theory*.\n\nProbability theory is the set of laws underlying rational belief. The mathematics of probability applies equally to “figuring out where your bookcase is” and “estimating how many hairs were on Julius Caesars head,” even though our evidence for the claim “Julius Caesar was bald” is likely to be more complicated and indirect than our evidence for the claim “theres a bookcase in my room.” It’s all the same problem of how to process the evidence and observations to update one’s beliefs. Similarly, decision theory is the set of laws underlying rational action, and is equally applicable regardless of what one’s goals and available options are.\n\nLet “P(such-and-such)” stand for “the probability that such-and-such happens,” and “P(A,B)” for “the probability that both A and B happen.” Since it is a universal law of probability theory that P(A) ≥ P(A,B), the judgment that P(Bill plays jazz) is less than P(Bill plays jazz, Bill is an accountant) is labeled incorrect.\n\nTo keep it technical, you would say that this probability judgment is *non-Bayesian*. Beliefs that conform to a coherent probability distribution, and decisions that maximize the probabilistic expectation of a coherent utility function, are called “Bayesian.”\n\nI should emphasize that this *isn't *the notion of rationality thats common in popular culture. People may use the same string of sounds, “ra-tio-nal,” to refer to “acting like Mr. Spock of *Star Trek*” and “acting like a Bayesian”; but this doesn't mean that acting Spock-like helps one hair with epistemic or instrumental rationality.^2^\n\nAll of this does not quite exhaust the problem of what is meant in practice by “rationality,” for two major reasons:\n\nFirst, the Bayesian formalisms in their full form are computationally intractable on most real-world problems. No one can *actually *calculate and obey the math, any more than you can predict the stock market by calculating the movements of quarks.\n\nThis is why there is a whole site called “Less Wrong,” rather than a single page that simply states the formal axioms and calls it a day. There’s a whole further art to finding the truth and accomplishing value *from inside a human mind*: we have to learn our own flaws, overcome our biases, prevent ourselves from self-deceiving, get ourselves into good emotional shape to confront the truth and do what needs doing, et cetera, et cetera.\n\nSecond, sometimes the meaning of the math itself is called into question. The exact rules of probability theory are called into question by, e.g., [anthropic problems](http://www.anthropic-principle.com/?q=anthropic_principle/primer) in which the number of observers is uncertain. The exact rules of decision theory are called into question by, e.g., Newcomblike problems in which other agents may predict your decision before it happens.^3^\n\nIn cases where our best formalizations still come up short, we can return to simpler ideas like “truth” and “winning.” If you are a scientist just beginning to investigate fire, it might be a lot wiser to point to a campfire and say “Fire is that orangey-bright hot stuff over there,” rather than saying “I define fire as an alchemical transmutation of substances which releases phlogiston.” You certainly shouldn’t ignore something just because you can’t define it. I can't quote the equations of General Relativity from memory, but nonetheless if I walk off a cliff, I'll fall. And we can say the same of cognitive biases and other obstacles to truth—they won't hit any less hard if it turns out we can't define compactly what “irrationality” is.\n\nIn cases like these, it is futile to try to settle the problem by coming up with some new definition of the word “rational” and saying, “Therefore my preferred answer, *by definition*, is what is meant by the word ‘rational.’ ” This simply raises the question of why anyone should pay attention to your definition. I’m not interested in probability theory because it is the holy word handed down from Laplace. I’m interested in Bayesian-style belief-updating (with Occam priors) because I expect that this style of thinking gets us systematically closer to, you know, *accuracy*, the map that reflects the territory.\n\nAnd then there are questions of how to think that seem not quite answered by either probability theory or decision theory—like the question of how to feel about the truth once you have it. Here, again, trying to define “rationality” a particular way doesn’t support an answer, but merely presumes one.\n\nI am not here to argue the meaning of a word, not even if that word is “rationality.” The point of attaching sequences of letters to particular concepts is to let two people *communicate*—to help transport thoughts from one mind to another. You cannot change reality, or prove the thought, by manipulating which meanings go with which words.\n\nSo if you understand what concept I am *generally getting at *with this word “rationality,” and with the sub-terms “epistemic rationality” and “instrumental rationality,” we *have communicated*: we have accomplished everything there is to accomplish by talking about how to define “rationality.” What’s left to discuss is not *what meaning *to attach to the syllables “ra-tio-na-li-ty”; what’s left to discuss is *what is a good way to think*.\n\nIf you say, “It’s (epistemically) rational for me to believe X, but the truth is Y,” then you are probably using the word “rational” to mean something other than what I have in mind. (E.g., “rationality” should be *consistent under reflection*—“rationally” looking at the evidence, and “rationally” considering how your mind processes the evidence, shouldn’t lead to two different conclusions.)\n\nSimilarly, if you find yourself saying, “The (instrumentally) rational thing for me to do is X, but the right thing for me to do is Y,” then you are almost certainly using some other meaning for the word “rational” or the word “right.” I use the term “rationality” *normatively*, to pick out desirable patterns of thought.\n\nIn this case—or in any other case where people disagree about word meanings—you should substitute more specific language in place of “rational”: “The self-benefiting thing to do is to run away, but I hope I would at least try to drag the child off the railroad tracks,” or “Causal decision theory as usually formulated says you should two-box on Newcomb’s Problem, but I’d rather have a million dollars.”\n\nIn fact, I recommend reading back through this essay, replacing every instance of “rational” with “foozal,” and seeing if that changes the connotations of what I’m saying any. If so, I say: strive not for rationality, but for foozality.\n\nThe word “rational” has potential pitfalls, but there are plenty of *non*-borderline cases where “rational” works fine to communicate what I’m getting at. Likewise “irrational.” In these cases I’m not afraid to use it.\n\nYet one should be careful not to *overuse *that word. One receives no points merely for pronouncing it loudly. If you speak overmuch of the Way, you will not attain it.\n\n* * *\n\n^1^ For a longer discussion of truth, see “[The Simple Truth](https://www.lesswrong.com/rationality/the-simple-truth)” at the very end of this volume.\n\n^2^ The idea that rationality is about strictly privileging verbal reasoning over feelings is a case in point. Bayesian rationality applies to urges, hunches, perceptions, and wordless intuitions, not just to assertions.\n\nI gave the example of opening your eyes, looking around you, and building a mental model of a room containing a bookcase against the wall. The modern idea of rationality is general enough to include your eyes and your brains visual areas as things-that-map, and to include instincts and emotions in the belief-and-goal calculus.\n\n^3^ For an informal statement of Newcomb’s Problem, see Jim Holt, “Thinking Inside the Boxes,” *Slate*, 2002, [http://www.slate.com/articles/arts/egghead/2002/02/thinkinginside\\_the\\_boxes.single.html](http://www.slate.com/articles/arts/egghead/2002/02/thinkinginside_the_boxes.single.html)."
    },
    "voteCount": 263,
    "forceInclude": true
  },
  {
    "_id": "PBRWb2Em5SNeWYwwB",
    "url": null,
    "title": "Humans are not automatically strategic",
    "slug": "humans-are-not-automatically-strategic",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "Rationality"
      },
      {
        "name": "Goodhart's Law"
      },
      {
        "name": "Introspection"
      },
      {
        "name": "General Intelligence"
      },
      {
        "name": "Motivational Intro Posts"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "Reply to: [A \"Failure to Evaluate Return-on-Time\" Fallacy](/lw/2p1/a_failure_to_evaluate_returnontime_fallacy/)\n\nLionhearted writes:\n\n> \\[A\\] large majority of otherwise smart people spend time doing semi-productive things, when there are massively productive opportunities untapped.\n> \n> A somewhat silly example: Let's say someone aspires to be a comedian, the best comedian ever, and to make a living doing comedy. He wants nothing else, it is his purpose. And he decides that in order to become a better comedian, he will watch re-runs of the old television cartoon 'Garfield and Friends' that was on TV from 1988 to 1995....\n> \n> I’m curious as to why.\n\nWhy will a randomly chosen eight-year-old fail a calculus test?  Because most possible answers are wrong, and there is no force to guide him to the correct answers.  (There is no need to postulate a “fear of success”; _most_ ways writing or not writing on a calculus test constitute failure, and so people, and rocks, fail calculus tests by default.)\n\nWhy do most of us, most of the time, choose to \"pursue our goals\" through routes that are far less effective than the routes we could find if we tried?\\[1\\]  My guess is that here, as with the calculus test, the main problem is that _most_ courses of action are extremely ineffective, and that there has been no strong evolutionary or cultural force sufficient to focus us on the very narrow behavior patterns that would actually be effective. \n\nTo be more specific: there are clearly at least some limited senses in which we have goals.  We: (1) tell ourselves and others stories of how we’re aiming for various “goals”; (2) search out modes of activity that are consistent with the role, and goal-seeking, that we see ourselves as doing (“learning math”; “becoming a comedian”; “being a good parent”); and sometimes even (3) feel glad or disappointed when we do/don’t achieve our “goals”.\n\nBut there are clearly also heuristics that would be useful to goal-achievement (or that would be part of what it means to “have goals” at all) that we do _not_ automatically carry out.  We do _not_ automatically:\n\n*   (a) Ask ourselves what we’re trying to achieve; \n*   (b) Ask ourselves how we could tell if we achieved it (“what does it look like to be a good comedian?”) and how we can track progress; \n*   (c) Find ourselves strongly, intrinsically curious about information that would help us achieve our goal; \n*   (d) Gather that information (e.g., by asking as how folks commonly achieve our goal, or similar goals, or by tallying which strategies have and haven’t worked for us in the past); \n*   (e) Systematically test many different conjectures for how to achieve the goals, including methods that aren’t habitual for us, while tracking which ones do and don’t work; \n*   (f) Focus most of the energy that \\*isn’t\\* going into systematic exploration, on the methods that work best;\n*   (g) Make sure that our \"goal\" is really our goal, that we coherently want it and are not constrained by fears or by uncertainty as to whether it is worth the effort, and that we have thought through any questions and decisions in advance so they won't continually sap our energies;\n*   (h) Use environmental cues and social contexts to bolster our motivation, so we can keep working effectively in the face of intermittent frustrations, or temptations based in hyperbolic discounting;\n\n.... or carry out any number of other useful techniques.  Instead, we mostly just do things.  We act from habit; we act from impulse or convenience when primed by the activities in front of us; we remember our goal and choose an action that _feels associated_ with our goal.  We do any number of things.  But we do not systematically choose the narrow sets of actions that would effectively optimize for our claimed goals, or for any other goals.\n\nWhy?  Most basically, because humans are only just on the cusp of general intelligence.  Perhaps 5% of the population has enough abstract reasoning skill to _verbally understand_ that the above heuristics would be useful _once these heuristics are pointed out_.  That is not at all the same as the ability to _automatically implement these heuristics_.  Our verbal, conversational systems are much better at abstract reasoning than are the motivational systems that pull our behavior.  I have enough abstract reasoning ability to understand that I’m safe on the glass floor of a tall building, or that ice cream is not healthy, or that exercise furthers my goals... but this _doesn’t_ lead to an automatic updating of the reward gradients that, absent rare and costly conscious overrides, pull my behavior.  I can train my automatic systems, for example by visualizing ice cream as disgusting and artery-clogging and yucky, or by walking across the glass floor often enough to persuade my brain that I can’t fall through the floor... but systematically training one’s motivational systems in this way is _also_ not automatic for us.  And so it seems far from surprising that most of us have not trained ourselves in this way, and that most of our “goal-seeking” actions are far less effective than they could be.\n\nStill, I’m keen to train.  I know people who are far more strategic than I am, and there seem to be clear avenues for becoming far more strategic than they are.  It also seems that having goals, in a much more pervasive sense than (1)-(3), is part of what “rational” should mean, will help us achieve what we care about, and hasn't been taught in much detail on LW.\n\nSo, to second Lionhearted's questions: does this analysis seem right?  Have some of you trained yourselves to be substantially more strategic, or goal-achieving, than you started out?  How did you do it?  Do you agree with (a)-(h) above?  Do you have some good heuristics to add?  Do you have some good ideas for how to train yourself in such heuristics?\n\n\\[1\\] For example, why do many people go through long training programs “to make money” without spending a few hours doing salary comparisons ahead of time?  Why do many who type for hours a day remain two-finger typists, without bothering with a typing tutor program?  Why do people spend their Saturdays “enjoying themselves” without bothering to track which of their habitual leisure activities are \\*actually\\* enjoyable?  Why do even unusually numerate people fear illness, car accidents, and bogeymen, and take safety measures, but not bother to look up statistics on the relative risks? Why do most of us settle into a single, stereotyped mode of studying, writing, social interaction, or the like, without trying alternatives to see if they work better -- even when such experiments as we have tried have sometimes given great boosts?"
    },
    "voteCount": 316,
    "forceInclude": true
  },
  {
    "_id": "fhEPnveFhb9tmd7Pe",
    "url": null,
    "title": "Use the Try Harder, Luke",
    "slug": "use-the-try-harder-luke",
    "author": "Eliezer Yudkowsky",
    "question": false,
    "tags": [
      {
        "name": "Fiction"
      },
      {
        "name": "More Dakka"
      },
      {
        "name": "Five minute timers"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "> \"When there's a will to fail, obstacles can be found.\"   —John McCarthy\n\nI first watched _Star Wars_ IV-VI when I was very young.  Seven, maybe, or nine?  So my memory was dim, but I recalled Luke Skywalker as being, you know, this cool Jedi guy.\n\nImagine my horror and disappointment, when I watched the saga again, years later, and discovered that Luke was a [whiny teenager](http://www.youtube.com/watch?v=X66jntR0MVE).\n\nI mention this because yesterday, I looked up, on Youtube, the source of the Yoda quote:  \"Do, or do not.  There is no try.\"\n\nOh.  My.  Cthulhu.\n\nAlong with the Youtube clip in question, I present to you a little-known outtake from the scene, in which the director and writer, George Lucas, argues with Mark Hamill, who played Luke Skywalker:\n\n> Luke:  All right, I'll give it a try.  \n> Yoda:  No!  Try not.  Do.  Or do not.  There is no try.  \n> _  \n> Luke raises his hand, and slowly, the X-wing begins to rise out of the water—Yoda's eyes widen—but then the ship sinks again._\n\nMark Hamill:  \"Um, George...\"\n\nGeorge Lucas:  \"What is it now?\"\n\nMark:  \"So... according to the script, next I say, 'I can't.  It's too big'.\"\n\nGeorge:  \"That's right.\"\n\nMark:  \"Shouldn't Luke maybe give it another shot?\"\n\nGeorge:  \"No.  Luke gives up, and sits down next to Yoda—\"\n\nMark:  \"This is the hero who's going to take down the Empire?  Look, it was one thing when he was a whiny teenager at the beginning, but he's in Jedi training now.  Last movie he blew up the Death Star.  Luke should be showing a little backbone.\"\n\nGeorge:  \"No.  You give up.  And then Yoda lectures you for a while, and you say, 'You want the impossible'.  Can you remember that?\"\n\nMark:  \"_Impossible?_  What did he do, run a formal calculation to arrive at a mathematical proof?   The X-wing was already starting to rise out of the swamp!  That's the feasibility demonstration right there!  Luke loses it for a second and the ship sinks back—and now he says it's _impossible?_  Not to mention that Yoda, who's got literally eight hundred years of seniority in the field, just told him it should be doable—\"\n\nGeorge:  \"And then you walk away.\"\n\nMark:  \"It's his friggin' _spaceship!_  If he leaves it in the swamp, he's stuck on Dagobah for the rest of his miserable life!  He's not just going to walk away!  Look, let's just cut to the next scene with the words 'one month later' and Luke is still raggedly standing in front of the swamp, trying to raise his ship for the thousandth time—\"\n\nGeorge:  \"No.\"\n\nMark:  \"Fine!  We'll show a sunset and a sunrise, as he stands there with his arm out, straining, and _then_ Luke says 'It's impossible'.  Though really, he ought to try again when he's fully rested—\"\n\nGeorge:  \"No.\"\n\nMark:  \"_Five goddamned minutes!_ Five goddamned minutes before he gives up!\"\n\nGeorge:  \"I am not halting the story for five minutes while the X-wing bobs in the swamp like a bathtub toy.\"\n\nMark:  \"For the love of sweet candied yams!  If a pathetic loser like this could master the Force, everyone in the galaxy would be using it!  People would become Jedi because it was easier than going to high school.\"\n\nGeorge:  \"Look, you're the actor.  Let me be the storyteller.  Just say your lines and try to mean them.\"\n\nMark:  \"The audience isn't going to buy it.\"\n\nGeorge:  \"Trust me, they will.\"\n\nMark:  \"They're going to get up and walk out of the theater.\"\n\nGeorge:  \"They're going to sit there and nod along and not notice anything out of the ordinary.  Look, you don't understand human nature.  People wouldn't try for five minutes before giving up if the fate of humanity were at stake.\""
    },
    "voteCount": 144,
    "forceInclude": true
  },
  {
    "_id": "5JDkW4MYXit2CquLs",
    "url": null,
    "title": "Your Strength as a Rationalist",
    "slug": "your-strength-as-a-rationalist",
    "author": "Eliezer Yudkowsky",
    "question": false,
    "tags": [
      {
        "name": "Rationalization"
      },
      {
        "name": "Noticing"
      },
      {
        "name": "Rationality"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "The following happened to me in an IRC chatroom, long enough ago that I was still hanging around in IRC chatrooms. Time has fuzzed the memory and my report may be imprecise.\n\nSo there I was, in an IRC chatroom, when someone reports that a friend of his needs medical advice. His friend says that he’s been having sudden chest pains, so he called an ambulance, and the ambulance showed up, but the paramedics told him it was nothing, and left, and now the chest pains are getting worse. What should his friend do?\n\nI was confused by this story. I remembered reading about homeless people in New York who would call ambulances just to be taken someplace warm, and how the paramedics always had to take them to the emergency room, even on the 27th iteration. Because if they didn’t, the ambulance company could be sued for lots and lots of money. Likewise, emergency rooms are legally obligated to treat anyone, regardless of ability to pay.^1^ So I didn’t quite understand how the described events could have happened. *Anyone* reporting sudden chest pains should have been hauled off by an ambulance instantly.\n\nAnd this is where I fell down as a rationalist. I remembered several occasions where my doctor would completely fail to panic at the report of symptoms that seemed, to me, very alarming. And the Medical Establishment was always right. Every single time. I had chest pains myself, at one point, and the doctor patiently explained to me that I was describing chest muscle pain, not a heart attack. So I said into the IRC channel, “Well, if the paramedics told your friend it was nothing, it must *really be* nothing—they’d have hauled him off if there was the tiniest chance of serious trouble.”\n\nThus I managed to explain the story within my existing model, though the fit still felt a little forced . . .\n\nLater on, the fellow comes back into the IRC chatroom and says his friend made the whole thing up. Evidently this was not one of his more reliable friends.\n\nI should have realized, perhaps, that an unknown acquaintance of an acquaintance in an IRC channel might be less reliable than a published journal article. Alas, belief is easier than disbelief; we believe instinctively, but disbelief requires a conscious effort.^2^\n\nSo instead, by dint of mighty straining, I forced my model of reality to explain an anomaly that *never actually happened.* And I *knew* how embarrassing this was. I *knew* that the usefulness of a model is not what it can explain, but what it can’t. A hypothesis that forbids nothing, permits everything, and thereby fails to constrain anticipation.\n\nYour strength as a rationalist is your ability to be more confused by fiction than by reality. If you are equally good at explaining any outcome, you have zero knowledge.\n\nWe are all weak, from time to time; the sad part is that I *could* have been stronger. I had all the information I needed to arrive at the correct answer, I even *noticed* the problem, and then I ignored it. My feeling of confusion was a Clue, and I threw my Clue away.\n\nI should have paid more attention to that sensation of *still feels a little forced.* It’s one of the most important feelings a truthseeker can have, a part of your strength as a rationalist. It is a design flaw in human cognition that this sensation manifests as a quiet strain in the back of your mind, instead of a wailing alarm siren and a glowing neon sign reading:\n\n**Either Your Model Is False Or This Story Is Wrong**.\n\n^1^ And the hospital absorbs the costs, which are enormous, so hospitals are closing their emergency rooms . . . It makes you wonder what’s the point of having economists if we’re just going to ignore them.\n\n^2^ From McCluskey (2007), “Truth Bias”: “\\[P\\]eople are more likely to correctly judge that a truthful statement is true than that a lie is false. This appears to be a fairly robust result that is not just a function of truth being the correct guess where the evidence is weak—it shows up in controlled experiments where subjects have good reason not to assume truth\\[.\\]” [http://www.overcomingbias.com/2007/08/truth-bias.html](http://www.overcomingbias.com/2007/08/truth-bias.html) .\n\nAnd from Gilbert et al. (1993), “You Can’t Not Believe Everything You Read”: “Can people comprehend assertions without believing them? \\[...\\] Three experiments support the hypothesis that comprehension includes an initial belief in the information comprehended.”"
    },
    "voteCount": 144,
    "forceInclude": true
  },
  {
    "_id": "3nZMgRTfFEfHp34Gb",
    "url": null,
    "title": "The Meditation on Curiosity",
    "slug": "the-meditation-on-curiosity",
    "author": "Eliezer Yudkowsky",
    "question": false,
    "tags": [
      {
        "name": "Curiosity"
      },
      {
        "name": "Litany of Tarski"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "> The first virtue is curiosity.\n> \n> —“[The Twelve Virtues of Rationality](https://www.lesswrong.com/rationality/twelve-virtues-of-rationality)”\n\nAs rationalists, we are obligated to criticize ourselves and question our beliefs . . . are we not?\n\nConsider what happens to you, on a psychological level, if you begin by saying: “It is my duty to criticize my own beliefs.” Roger Zelazny once distinguished between “wanting to be an author” versus “wanting to write.” Mark Twain said: “A classic is something that everyone wants to have read and no one wants to read.” Criticizing yourself from a sense of duty leaves you *wanting to have investigated*, so that you’ll be able to say afterward that your faith is not blind. This is not the same as *wanting to investigate.*\n\nThis can lead to [motivated stopping](https://www.lesswrong.com/lw/hu/the_third_alternative/) of your investigation.  You consider an objection, then a counterargument to that objection, then you *stop there.*  You repeat this with several objections, until you feel that you have done your duty to investigate, and then you *stop there.* You have achieved your underlying psychological objective: to get rid of the cognitive dissonance that would result from thinking of yourself as a rationalist, and yet knowing that you had not tried to criticize your belief.  You might call it [purchase of rationalist satisfaction](https://www.lesswrong.com/lw/hw/scope_insensitivity/)—trying to create a \"warm glow\" of discharged duty.\n\nAfterward, your stated probability level will be high enough to justify your keeping the plans and beliefs you started with, but not so high as to evoke incredulity from yourself or other rationalists.\n\nWhen you’re really curious, you’ll gravitate to inquiries that seem most promising of producing shifts in belief, or inquiries that are least like the ones you’ve tried before. Afterward, your probability distribution likely should *not* look like it did when you started out—shifts should have occurred, whether up or down; and either direction is equally fine to you, if you’re genuinely curious.\n\nContrast this to the subconscious motive of keeping your inquiry on familiar ground, so that you can get your investigation over with quickly, so that you can *have investigated*, and restore the familiar balance on which your familiar old plans and beliefs are based.\n\nAs for what I think true curiosity should look like, and the power that it holds, I refer you to “[A Fable of Science and Politics](https://www.lesswrong.com/rationality/a-fable-of-science-and-politics)” in the first book of this series, *Map and Territory*. The fable showcases the reactions of different characters to an astonishing discovery, with each character’s response intended to illustrate different lessons. Ferris, the last character, embodies the power of innocent curiosity: which is lightness, and an eager reaching forth for evidence.\n\nUrsula K. LeGuin wrote: “In innocence there is no strength against evil. But there is strength in it for good.”[^1^](#fn1x71) Innocent curiosity may turn innocently awry; and so the training of a rationalist, and its accompanying sophistication, must be dared as a danger if we want to become stronger. Nonetheless we can try to keep the lightness and the eager reaching of innocence.\n\nAs it is written in “The Twelve Virtues of Rationality”:\n\n> If in your heart you believe you already know, or if in your heart you do not wish to know, then your questioning will be purposeless and your skills without direction. Curiosity seeks to annihilate itself; there is no curiosity that does not want an answer.\n\nThere just isn’t any good substitute for genuine curiosity. A burning itch to know is higher than a solemn vow to pursue truth. But you can’t produce curiosity just by willing it, any more than you can will your foot to feel warm when it feels cold. Sometimes, all we have is our mere solemn vows.\n\nSo what can you do with duty? For a start, we can try to take an interest in our dutiful investigations—keep a close eye out for sparks of genuine intrigue, or even genuine ignorance and a desire to resolve it. This goes right along with keeping a special eye out for possibilities that are painful, that you are flinching away from—it’s not all negative thinking.\n\nIt should also help to meditate on “[Conservation of Expected Evidence](https://www.lesswrong.com/rationality/conservation-of-expected-evidence).” For every *new* point of inquiry, for every piece of *unseen* evidence that you suddenly look at, the expected posterior probability should equal your prior probability. In the microprocess of inquiry, your belief should always be evenly poised to shift in either direction. Not every point may suffice to blow the issue wide open—to shift belief from 70% to 30% probability—but if your current belief is 70%, you should be as ready to drop it to 69% as raise it to 71%. You should not think that you know which direction it will go in (on average), because by the laws of probability theory, if you know your destination, you are already there. If you can investigate honestly, so that each *new* point really does have equal potential to shift belief upward or downward, this may help to keep you interested or even curious about the microprocess of inquiry.\n\nIf the argument you are considering is *not* new, then why is your attention going here? Is this where you would look if you were genuinely curious? Are you subconsciously criticizing your belief at its strong points, rather than its weak points? Are you rehearsing the evidence?\n\nIf you can manage not to rehearse already known support, and you can manage to drop down your belief by one tiny bite at a time from the new evidence, you may even be able to relinquish the belief entirely—to realize from which quarter the winds of evidence are blowing against you.\n\nAnother restorative for curiosity is what I have taken to calling the Litany of Tarski, which is really a meta-litany that specializes for each instance (this is only appropriate). For example, if I am tensely wondering whether a locked box contains a diamond, then rather than thinking about all the wonderful consequences if the box does contain a diamond, I can repeat the Litany of Tarski:\n\n> *If the box contains a diamond,*  \n> *I desire to believe that the box contains a diamond;*  \n> *If the box does not contain a diamond,*  \n> *I desire to believe that the box does not contain a diamond;*  \n> *Let me not become attached to beliefs I may not want.*\n\nThen you should meditate upon the possibility that there is no diamond, and the subsequent advantage that will come to you if you believe there is no diamond, and the subsequent disadvantage if you believe there is a diamond. See also the Litany of Gendlin.\n\nIf you can find within yourself the slightest shred of true uncertainty, then guard it like a forester nursing a campfire. If you can make it blaze up into a flame of curiosity, it will make you light and eager, and give purpose to your questioning and direction to your skills.\n\n[^1^](#fn1x71-bk)Ursula K. Le Guin, *The Farthest Shore* (Saga Press, 2001)."
    },
    "voteCount": 82,
    "forceInclude": true
  },
  {
    "_id": "wCqfCLs8z5Qw4GbKS",
    "url": null,
    "title": "The Importance of Saying \"Oops\"",
    "slug": "the-importance-of-saying-oops",
    "author": "Eliezer_Yudkowsky",
    "question": false,
    "tags": [
      {
        "name": "Honesty"
      },
      {
        "name": "Self-Deception"
      },
      {
        "name": "Epistemology"
      },
      {
        "name": "Changing Your Mind"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "I just finished reading a history of Enron’s downfall, _The Smartest Guys in the Room_, which hereby wins my award for “Least Appropriate Book Title.”\n\nAn unsurprising feature of Enron’s slow rot and abrupt collapse was that the executive players never admitted to having made a _large_ mistake. When catastrophe #247 grew to such an extent that it required an actual policy change, they would say, “Too bad that didn’t work out—it was such a good idea—how are we going to hide the problem on our balance sheet?” As opposed to, “It now seems obvious in retrospect that it was a mistake from the beginning.” As opposed to, “I’ve been stupid.” There was never a watershed moment, a moment of humbling realization, of acknowledging a _fundamental_ problem. After the bankruptcy, Jeff Skilling, the former COO and brief CEO of Enron, declined his own lawyers’ advice to take the Fifth Amendment; he testified before Congress that Enron had been a _great_ company.\n\nNot every change is an improvement, but every improvement is necessarily a change. If we only admit small local errors, we will only make small local changes. The motivation for a _big_ change comes from acknowledging a _big_ mistake.\n\nAs a child I was raised on equal parts science and science fiction, and from Heinlein to Feynman I learned the tropes of Traditional Rationality: theories must be bold and expose themselves to falsification; be willing to commit the heroic sacrifice of giving up your own ideas when confronted with contrary evidence; play nice in your arguments; try not to deceive yourself; and other fuzzy verbalisms.\n\nA traditional rationalist upbringing tries to produce arguers who will concede to contrary evidence _eventually_—there should be _some_ mountain of evidence sufficient to move you. This is not trivial; it distinguishes science from religion. But there is less focus on _speed_, on giving up the fight _as quickly as possible_, integrating evidence _efficiently_ so that it only takes a _minimum_ of contrary evidence to destroy your cherished belief.\n\nI was raised in Traditional Rationality, and thought myself quite the rationalist. I switched to Bayescraft (Laplace / Jaynes / Tversky / Kahneman) in the aftermath of . . . well, it’s a long story. Roughly, I switched because I realized that Traditional Rationality’s fuzzy verbal tropes had been insufficient to prevent me from making a large mistake.\n\nAfter I had finally and fully admitted my mistake, I looked back upon the path that had led me to my Awful Realization. And I saw that I had made a series of small concessions, minimal concessions, grudgingly conceding each millimeter of ground, realizing as little as possible of my mistake on each occasion, admitting failure only in small tolerable nibbles. I could have moved so much faster, I realized, if I had simply screamed “_OOPS!_”\n\nAnd I thought: _I must raise the level of my game._\n\nThere is a _powerful advantage_ to admitting you have made a _large_ mistake. It’s painful. It can also change your whole life.\n\nIt is _important_ to have the watershed moment, the moment of humbling realization. To acknowledge a _fundamental_ problem, not divide it into palatable bite-size mistakes.\n\nDo not indulge in drama and become proud of admitting errors. It is surely superior to get it right the first time. But if you do make an error, better by far to see it all at once. Even hedonically, it is better to take one large loss than many small ones. The alternative is stretching out the battle with yourself over years. The alternative is Enron.\n\nSince then I have watched others making their own series of minimal concessions, grudgingly conceding each millimeter of ground; never confessing a global mistake where a local one will do; always learning as little as possible from each error. What they could fix in one fell swoop voluntarily, they transform into tiny local patches they must be argued into. Never do they say, after confessing one mistake, _I’ve been a fool._ They do their best to minimize their embarrassment by saying _I was right in principle_, or _It could have worked_, or _I still want to embrace the true essence of whatever-I’m-attached-to._ Defending their pride in this passing moment, they ensure they will again make the same mistake, and again need to defend their pride.\n\nBetter to swallow the entire bitter pill in one terrible gulp."
    },
    "voteCount": 113,
    "forceInclude": true
  },
  {
    "_id": "teaxCFgtmCQ3E9fy8",
    "url": null,
    "title": "The Martial Art of Rationality",
    "slug": "the-martial-art-of-rationality",
    "author": "Eliezer Yudkowsky",
    "question": false,
    "tags": [
      {
        "name": "Rationality"
      },
      {
        "name": "Cultural knowledge"
      },
      {
        "name": "Habits"
      },
      {
        "name": "Rationality Verification"
      },
      {
        "name": "Motivational Intro Posts"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "I often use the metaphor that [rationality](http://wiki.lesswrong.com/wiki/Rationality) is the [martial art of mind](http://wiki.lesswrong.com/wiki/Rationality_as_martial_art). You don’t need huge, bulging muscles to learn martial arts—there’s a tendency toward more athletic people being more likely to learn martial arts, but that may be a matter of *enjoyment* as much as anything else. If you have a hand, with tendons and muscles in the appropriate places, then you can learn to make a fist.\n\nSimilarly, if you have a brain, with cortical and subcortical areas in the appropriate places, you might be able to learn to use it properly. If you’re a fast learner, you might learn faster—but the art of rationality isn’t about that; it’s about training brain machinery we all have in common. And where there are systematic errors human brains tend to make—like an insensitivity to scope—rationality is about fixing those mistakes, or finding work-arounds.\n\nAlas, our minds respond less readily to our will than our hands. Our ability to control our muscles is evolutionarily ancient; our ability to reason about our own reasoning processes is a much more recent innovation. We shouldn’t be surprised, then, that muscles are easier to use than brains. But it is not wise to neglect the latter training because it is more difficult. It is not by bigger muscles that the human species rose to prominence upon Earth.\n\nIf you live in an urban area, you probably don’t need to walk very far to find a martial arts dojo. Why aren’t there dojos that teach rationality?\n\nOne reason, perhaps, is that it’s harder to verify skill. To rise a level in Tae Kwon Do, you might need to break a board of a certain width. If you succeed, all the onlookers can see and applaud. If you fail, your teacher can watch how you shape a fist, and check if you shape it correctly. If not, the teacher holds out a hand and makes a fist correctly, so that you can observe how to do so.\n\nWithin martial arts schools, techniques of muscle have been refined and elaborated over generations. Techniques of rationality are harder to pass on, even to the most willing student.\n\nVery recently—in just the last few decades—the human species has acquired a great deal of new knowledge about human rationality. The most salient example would be the [heuristics and biases](https://en.wikipedia.org/wiki/Heuristics_in_judgment_and_decision-making) program in experimental psychology. There is also the [Bayesian](https://wiki.lesswrong.com/wiki/Bayesian) systematization of probability theory and statistics; evolutionary psychology; social psychology. Experimental investigations of empirical human psychology; and theoretical probability theory to interpret what our experiments tell us; and evolutionary theory to explain the conclusions. These fields give us new focusing lenses through which to view the landscape of our own minds. With their aid, we may be able to see more clearly the muscles of our brains, the fingers of thought as they move. We have a shared vocabulary in which to describe problems and solutions. Humanity may finally be ready to synthesize the martial art of mind: to refine, share, systematize, and pass on techniques of personal rationality.\n\nSuch understanding as I have of rationality, I acquired in the course of wrestling with the challenge of artificial general intelligence (an endeavor which, to actually succeed, would require sufficient mastery of rationality to build a complete working rationalist out of toothpicks and rubber bands). In most ways the AI problem is enormously more demanding than the personal art of rationality, but in some ways it is actually easier. In the martial art of mind, we need to acquire the realtime procedural skill of pulling the right levers at the right time on a large, pre-existing thinking machine whose innards are not end-user-modifiable. Some of the machinery is optimized for evolutionary selection pressures that run directly counter to our declared goals in using it. Deliberately we decide that we want to seek only the truth; but our brains have hardwired support for rationalizing falsehoods. We can try to compensate for what we choose to regard as flaws of the machinery; but we can’t actually rewire the neural circuitry. Nor may martial artists plate titanium over their bones—not today, at any rate.\n\nTrying to synthesize a personal art of rationality, using the science of rationality, may prove awkward: One imagines trying to invent a martial art using an abstract theory of physics, game theory, and human anatomy.\n\nBut humans arent reflectively blind. We do have a native instinct for introspection. The inner eye isnt sightless, though it sees blurrily, with systematic distortions. We need, then, to *apply* the science to our intuitions, to use the abstract knowledge to *correct* our mental movements and *augment* our metacognitive skills.\n\nWe aren't writing a computer program to make a string puppet execute martial arts forms; it is our own mental limbs that we must move. Therefore we must connect theory to practice. We must come to see what the science means, for ourselves, for our daily inner life."
    },
    "voteCount": 176,
    "forceInclude": true
  },
  {
    "_id": "7ZqGiPHTpiDMwqMN2",
    "url": null,
    "title": "Twelve Virtues of Rationality",
    "slug": "twelve-virtues-of-rationality",
    "author": "Eliezer Yudkowsky",
    "question": false,
    "tags": [
      {
        "name": "Virtues"
      },
      {
        "name": "Rationality"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "The first virtue is curiosity. A burning itch to know is higher than a solemn vow to pursue truth. To feel the burning itch of curiosity requires both that you be ignorant, and that you desire to relinquish your ignorance. If in your heart you believe you already know, or if in your heart you do not wish to know, then your questioning will be purposeless and your skills without direction. Curiosity seeks to annihilate itself; there is no curiosity that does not want an answer. The glory of glorious mystery is to be solved, after which it ceases to be mystery. Be wary of those who speak of being open-minded and modestly confess their ignorance.\n\nThere is a time to confess your ignorance and a time to relinquish your ignorance. The second virtue is relinquishment. P. C. Hodgell said: “That which can be destroyed by the truth should be.”\\[1\\] Do not flinch from experiences that might destroy your beliefs. The thought you cannot think controls you more than thoughts you speak aloud. Submit yourself to ordeals and test yourself in fire. Relinquish the emotion which rests upon a mistaken belief, and seek to feel fully that emotion which fits the facts. If the iron approaches your face, and you believe it is hot, and it is cool, the Way opposes your fear. If the iron approaches your face, and you believe it is cool, and it is hot, the Way opposes your calm. Evaluate your beliefs first and then arrive at your emotions. Let yourself say: “If the iron is hot, I desire to believe it is hot, and if it is cool, I desire to believe it is cool.” Beware lest you become attached to beliefs you may not want.\n\nThe third virtue is lightness. Let the winds of evidence blow you about as though you are a leaf, with no direction of your own. Beware lest you fight a rearguard retreat against the evidence, grudgingly conceding each foot of ground only when forced, feeling cheated. Surrender to the truth as quickly as you can. Do this the instant you realize what you are resisting, the instant you can see from which quarter the winds of evidence are blowing against you. Be faithless to your cause and betray it to a stronger enemy. If you regard evidence as a constraint and seek to free yourself, you sell yourself into the chains of your whims. For you cannot make a true map of a city by sitting in your bedroom with your eyes shut and drawing lines upon paper according to impulse. You must walk through the city and draw lines on paper that correspond to what you see. If, seeing the city unclearly, you think that you can shift a line just a little to the right, just a little to the left, according to your caprice, this is just the same mistake.\n\nThe fourth virtue is evenness. One who wishes to believe says, “Does the evidence permit me to believe?” One who wishes to disbelieve asks, “Does the evidence force me to believe?” Beware lest you place huge burdens of proof only on propositions you dislike, and then defend yourself by saying: “But it is good to be skeptical.” If you attend only to favorable evidence, picking and choosing from your gathered data, then the more data you gather, the less you know. If you are selective about which arguments you inspect for flaws, or how hard you inspect for flaws, then every flaw you learn how to detect makes you that much stupider. If you first write at the bottom of a sheet of paper “And therefore, the sky is green!” it does not matter what arguments you write above it afterward; the conclusion is already written, and it is already correct or already wrong. To be clever in argument is not rationality but rationalization. Intelligence, to be useful, must be used for something other than defeating itself. Listen to hypotheses as they plead their cases before you, but remember that you are not a hypothesis; you are the judge. Therefore do not seek to argue for one side or another, for if you knew your destination, you would already be there.\n\nThe fifth virtue is argument. Those who wish to fail must first prevent their friends from helping them. Those who smile wisely and say “I will not argue” remove themselves from help and withdraw from the communal effort. In argument strive for exact honesty, for the sake of others and also yourself: the part of yourself that distorts what you say to others also distorts your own thoughts. Do not believe you do others a favor if you accept their arguments; the favor is to you. Do not think that fairness to all sides means balancing yourself evenly between positions; truth is not handed out in equal portions before the start of a debate. You cannot move forward on factual questions by fighting with fists or insults. Seek a test that lets reality judge between you.\n\nThe sixth virtue is empiricism. The roots of knowledge are in observation and its fruit is prediction. What tree grows without roots? What tree nourishes us without fruit? If a tree falls in a forest and no one hears it, does it make a sound? One says, “Yes it does, for it makes vibrations in the air.” Another says, “No it does not, for there is no auditory processing in any brain.” Though they argue, one saying “Yes,” and one saying “No,” the two do not anticipate any different experience of the forest. Do not ask which beliefs to profess, but which experiences to anticipate. Always know which difference of experience you argue about. Do not let the argument wander and become about something else, such as someone’s virtue as a rationalist. Jerry Cleaver said: “What does you in is not failure to apply some high-level, intricate, complicated technique. It’s overlooking the basics. Not keeping your eye on the ball.”\\[2\\] Do not be blinded by words. When words are subtracted, anticipation remains.\n\nThe seventh virtue is simplicity. Antoine de Saint-Exupéry said: “Perfection is achieved not when there is nothing left to add, but when there is nothing left to take away.”\\[3\\] Simplicity is virtuous in belief, design, planning, and justification. When you profess a huge belief with many details, each additional detail is another chance for the belief to be wrong. Each specification adds to your burden; if you can lighten your burden you must do so. There is no straw that lacks the power to break your back. Of artifacts it is said: The most reliable gear is the one that is designed out of the machine. Of plans: A tangled web breaks. A chain of a thousand links will arrive at a correct conclusion if every step is correct, but if one step is wrong it may carry you anywhere. In mathematics a mountain of good deeds cannot atone for a single sin. Therefore, be careful on every step.\n\nThe eighth virtue is humility. To be humble is to take specific actions in anticipation of your own errors. To confess your fallibility and then do nothing about it is not humble; it is boasting of your modesty. Who are most humble? Those who most skillfully prepare for the deepest and most catastrophic errors in their own beliefs and plans. Because this world contains many whose grasp of rationality is abysmal, beginning students of rationality win arguments and acquire an exaggerated view of their own abilities. But it is useless to be superior: Life is not graded on a curve. The best physicist in ancient Greece could not calculate the path of a falling apple. There is no guarantee that adequacy is possible given your hardest effort; therefore spare no thought for whether others are doing worse. If you compare yourself to others you will not see the biases that all humans share. To be human is to make ten thousand errors. No one in this world achieves perfection.\n\nThe ninth virtue is perfectionism. The more errors you correct in yourself, the more you notice. As your mind becomes more silent, you hear more noise. When you notice an error in yourself, this signals your readiness to seek advancement to the next level. If you tolerate the error rather than correcting it, you will not advance to the next level and you will not gain the skill to notice new errors. In every art, if you do not seek perfection you will halt before taking your first steps. If perfection is impossible that is no excuse for not trying. Hold yourself to the highest standard you can imagine, and look for one still higher. Do not be content with the answer that is almost right; seek one that is exactly right.\n\nThe tenth virtue is precision. One comes and says: The quantity is between 1 and 100. Another says: The quantity is between 40 and 50. If the quantity is 42 they are both correct, but the second prediction was more useful and exposed itself to a stricter test. What is true of one apple may not be true of another apple; thus more can be said about a single apple than about all the apples in the world. The narrowest statements slice deepest, the cutting edge of the blade. As with the map, so too with the art of mapmaking: The Way is a precise Art. Do not walk to the truth, but dance. On each and every step of that dance your foot comes down in exactly the right spot. Each piece of evidence shifts your beliefs by exactly the right amount, neither more nor less. What is exactly the right amount? To calculate this you must study probability theory. Even if you cannot do the math, knowing that the math exists tells you that the dance step is precise and has no room in it for your whims.\n\nThe eleventh virtue is scholarship. Study many sciences and absorb their power as your own. Each field that you consume makes you larger. If you swallow enough sciences the gaps between them will diminish and your knowledge will become a unified whole. If you are gluttonous you will become vaster than mountains. It is especially important to eat math and science which impinge upon rationality: evolutionary psychology, heuristics and biases, social psychology, probability theory, decision theory. But these cannot be the only fields you study. The Art must have a purpose other than itself, or it collapses into infinite recursion.\n\nBefore these eleven virtues is a virtue which is nameless.\n\nMiyamoto Musashi wrote, in _The Book of Five Rings_:\\[4\\]\n\n> The primary thing when you take a sword in your hands is your intention to cut the enemy, whatever the means. Whenever you parry, hit, spring, strike or touch the enemy’s cutting sword, you must cut the enemy in the same movement. It is essential to attain this. If you think only of hitting, springing, striking or touching the enemy, you will not be able actually to cut him. More than anything, you must be thinking of carrying your movement through to cutting him.\n\nEvery step of your reasoning must cut through to the correct answer in the same movement. More than anything, you must think of carrying your map through to reflecting the territory.\n\nIf you fail to achieve a correct answer, it is futile to protest that you acted with propriety.\n\nHow can you improve your conception of rationality? Not by saying to yourself, “It is my duty to be rational.” By this you only enshrine your mistaken conception. Perhaps your conception of rationality is that it is rational to believe the words of the Great Teacher, and the Great Teacher says, “The sky is green,” and you look up at the sky and see blue. If you think, “It may look like the sky is blue, but rationality is to believe the words of the Great Teacher,” you lose a chance to discover your mistake.\n\nDo not ask whether it is “the Way” to do this or that. Ask whether the sky is blue or green. If you speak overmuch of the Way you will not attain it. You may try to name the highest principle with names such as “the map that reflects the territory” or “experience of success and failure” or “Bayesian decision theory.” But perhaps you describe incorrectly the nameless virtue. How will you discover your mistake? Not by comparing your description to itself, but by comparing it to that which you did not name.\n\nIf for many years you practice the techniques and submit yourself to strict constraints, it may be that you will glimpse the center. Then you will see how all techniques are one technique, and you will move correctly without feeling constrained. Musashi wrote: “When you appreciate the power of nature, knowing the rhythm of any situation, you will be able to hit the enemy naturally and strike naturally. All this is the Way of the Void.”\n\nThese then are twelve virtues of rationality:\n\nCuriosity, relinquishment, lightness, evenness, argument, empiricism, simplicity, humility, perfectionism, precision, scholarship, and the void.\n\n* * *\n\n###### 1\\. Patricia C. Hodgell, _Seeker’s Mask_ (Meisha Merlin Publishing, Inc., 2001).\n\n###### 2\\. Cleaver, _Immediate Fiction: A Complete Writing Course_.\n\n###### 3\\. Antoine de Saint-Exupéry, _Terre des Hommes_ (Paris: Gallimard, 1939).\n\n###### 4\\. Musashi, _Book of Five Rings_.\n\n* * *\n\n_The first publication of this post is_ [_here_](http://yudkowsky.net/rational/virtues/)_._"
    },
    "voteCount": 86,
    "forceInclude": true
  },
  {
    "_id": "34XxbRFe54FycoCDw",
    "url": null,
    "title": "The Bottom Line",
    "slug": "the-bottom-line",
    "author": "Eliezer_Yudkowsky",
    "question": false,
    "tags": [
      {
        "name": "Rationality"
      },
      {
        "name": "Rationalization"
      },
      {
        "name": "Filtered Evidence"
      },
      {
        "name": "Litany of Tarski"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "There are two sealed boxes up for auction, box A and box B. One and only one of these boxes contains a valuable diamond. There are all manner of signs and portents indicating whether a box contains a diamond; but I have no sign which I _know_ to be perfectly reliable. There is a blue stamp on one box, for example, and I know that boxes which contain diamonds are more likely than empty boxes to show a blue stamp. Or one box has a shiny surface, and I have a suspicion—I am not sure—that no diamond-containing box is ever shiny.\n\nNow suppose there is a clever arguer, holding a sheet of paper, and they say to the owners of box A and box B: “Bid for my services, and whoever wins my services, I shall argue that their box contains the diamond, so that the box will receive a higher price.” So the box-owners bid, and box B’s owner bids higher, winning the services of the clever arguer.\n\nThe clever arguer begins to organize their thoughts. First, they write, “And _therefore_, box B contains the diamond!” at the bottom of their sheet of paper. Then, at the top of the paper, the clever arguer writes, “Box B shows a blue stamp,” and beneath it, “Box A is shiny,” and then, “Box B is lighter than box A,” and so on through many signs and portents; yet the clever arguer neglects all those signs which might argue in favor of box A. And then the clever arguer comes to me and recites from their sheet of paper: “Box B shows a blue stamp, and box A is shiny,” and so on, until they reach: “and _therefore_, box B contains the diamond.”\n\nBut consider: At the moment when the clever arguer wrote down their conclusion, at the moment they put ink on their sheet of paper, the evidential entanglement of that physical ink with the physical boxes became fixed.\n\nIt may help to visualize a collection of worlds—Everett branches or Tegmark duplicates—within which there is some objective frequency at which box A or box B contains a diamond.^[1](#fn1x27)^\n\nThe ink on paper is formed into odd shapes and curves, which look like this text: “And _therefore_, box B contains the diamond.” If you happened to be a literate English speaker, you might become confused, and think that this shaped ink somehow _meant_ that box B contained the diamond. Subjects instructed to say the color of printed pictures and shown the word Green in red ink often say “green” instead of “red.” It helps to be illiterate, so that you are not confused by the shape of the ink.\n\nTo us, the true import of a thing is its entanglement with other things. Consider again the collection of worlds, Everett branches or Tegmark duplicates. At the moment when all clever arguers in all worlds put ink to the bottom line of their paper—let us suppose this is a single moment—it fixed the correlation of the ink with the boxes. The clever arguer writes in non-erasable pen; the ink will not change. The boxes will not change. Within the subset of worlds where the ink says “And therefore, box B contains the diamond,” there is already some fixed percentage of worlds where box A contains the diamond. This will not change regardless of what is written in on the blank lines above.\n\nSo the evidential entanglement of the ink is fixed, and I leave to you to decide what it might be. Perhaps box owners who believe a better case can be made for them are more liable to hire advertisers; perhaps box owners who fear their own deficiencies bid higher. If the box owners do not themselves understand the signs and portents, then the ink will be completely unentangled with the boxes’ contents, though it may tell you something about the owners’ finances and bidding habits.\n\nNow suppose another person present is genuinely curious, and they _first_ write down all the distinguishing signs of _both_ boxes on a sheet of paper, and then apply their knowledge and the laws of probability and write down at the bottom: “_Therefore,_ I estimate an 85% probability that box B contains the diamond.” Of what is this handwriting evidence? Examining the chain of cause and effect leading to this physical ink on physical paper, I find that the chain of causality wends its way through all the signs and portents of the boxes, and is dependent on these signs; for in worlds with different portents, a different probability is written at the bottom.\n\nSo the handwriting of the curious inquirer is entangled with the signs and portents and the contents of the boxes, whereas the handwriting of the clever arguer is evidence only of which owner paid the higher bid. There is a great difference in the indications of ink, though one who foolishly read aloud the ink-shapes might think the English words sounded similar.\n\nYour effectiveness as a rationalist is determined by whichever algorithm actually writes the bottom line of your thoughts. If your car makes metallic squealing noises when you brake, and you aren’t willing to face up to the financial cost of getting your brakes replaced, you can decide to look for reasons why your car might not need fixing. But the actual percentage of you that survive in Everett branches or Tegmark worlds—which we will take to describe your effectiveness as a rationalist—is determined by the algorithm that decided _which_ conclusion you would seek arguments for. In this case, the real algorithm is “Never repair anything expensive.” If this is a good algorithm, fine; if this is a bad algorithm, oh well. The arguments you write afterward, above the bottom line, will not change anything either way.\n\nThis is intended as a caution for your own thinking, not a Fully General Counterargument against conclusions you don’t like. For it is indeed a clever argument to say “My opponent is a clever arguer,” if you are paying yourself to retain whatever beliefs you had at the start. The world’s cleverest arguer may point out that the Sun is shining, and yet it is still probably daytime.\n\n^[1](#fn1x27-bk)^Max Tegmark, “Parallel Universes,” in _Science and Ultimate Reality: Quantum Theory, Cosmology, and Complexity_, ed. John D. Barrow, Paul C. W. Davies, and Charles L. Harper Jr. (New York: Cambridge University Press, 2004), 459–491, [http://arxiv.org/abs/astro-ph/0302131](http://arxiv.org/abs/astro-ph/0302131)."
    },
    "voteCount": 92,
    "forceInclude": true
  },
  {
    "_id": "SFZoEBpLo9frSJGkc",
    "url": null,
    "title": "Rationalization",
    "slug": "rationalization",
    "author": "Eliezer Yudkowsky",
    "question": false,
    "tags": [
      {
        "name": "Rationalization"
      },
      {
        "name": "Rationality"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "In “The Bottom Line,” I presented the dilemma of two boxes, only one of which contains a diamond, with various signs and portents as evidence. I dichotomized the curious inquirer and the clever arguer. The curious inquirer writes down all the signs and portents, and processes them, and finally writes down, “*Therefore,* I estimate an 85% probability that box B contains the diamond.” The clever arguer works for the highest bidder, and begins by writing, “*Therefore,* box B contains the diamond,” and then selects favorable signs and portents to list on the lines above.\n\nThe first procedure is rationality. The second procedure is generally known as “rationalization.”\n\n“Rationalization.” What a curious term. I would call it a *wrong word.* You cannot “rationalize” what is not already rational. It is as if “lying” were called “truthization.”\n\nOn a purely computational level, there is a rather large difference between:\n\n1.  Starting from evidence, and then crunching probability flows, in order to output a probable conclusion. (Writing down all the signs and portents, and then flowing forward to a probability on the bottom line which depends on those signs and portents.) \n2.  Starting from a conclusion, and then crunching probability flows, in order to output evidence apparently favoring that conclusion. (Writing down the bottom line, and then flowing backward to select signs and portents for presentation on the lines above.)\n\nWhat fool devised such confusingly similar words, “rationality” and “rationalization,” to describe such extraordinarily different mental processes? I would prefer terms that made the algorithmic difference obvious, like “rationality” versus “giant sucking cognitive black hole.”\n\nNot every change is an improvement, but every improvement is necessarily a change. You cannot obtain more truth for a fixed proposition by arguing it; you can make more people believe it, but you cannot make it more *true*. To improve our beliefs, we must necessarily change our beliefs. Rationality is the operation that we use to obtain more accuracy for our beliefs by changing them. Rationalization operates to fix beliefs in place; it would be better named “anti-rationality,” both for its pragmatic results and for its reversed algorithm.\n\n“Rationality” is the *forward* flow that gathers evidence, weighs it, and outputs a conclusion. The curious inquirer used a forward-flow algorithm: *first* gathering the evidence, writing down a list of all visible signs and portents, which they then processed *forward* to obtain a previously unknown probability for the box containing the diamond. During the entire time that the rationality-process was running forward, the curious inquirer did not yet know their destination, which was why they were *curious.* In the Way of Bayes, the prior probability equals the expected posterior probability: If you know your destination, you are already there.\n\n“Rationalization” is a *backward* flow from conclusion to selected evidence. First you write down the bottom line, which is known and fixed; the purpose of your processing is to find out which arguments you should write down on the lines above. This, not the bottom line, is the variable unknown to the running process.\n\nI fear that Traditional Rationality does not properly sensitize its users to the difference between forward flow and backward flow. In Traditional Rationality, there is nothing wrong with the scientist who arrives at a pet hypothesis and then sets out to find an experiment that proves it. A Traditional Rationalist would look at this approvingly, and say, “This pride is the engine that drives Science forward.” Well, it *is* the engine that drives Science forward. It is easier to find a prosecutor and defender biased in opposite directions, than to find a single unbiased human.\n\nBut just because everyone does something, doesn’t make it okay. It would be better yet if the scientist, arriving at a pet hypothesis, set out to *test* that hypothesis for the sake of *curiosity*—creating experiments that would drive their own beliefs in an unknown direction.\n\nIf you genuinely don’t know where you are going, you will probably feel quite curious about it. Curiosity is the first virtue, without which your questioning will be purposeless and your skills without direction.\n\nFeel the flow of the Force, and make sure it isn’t flowing backwards."
    },
    "voteCount": 47,
    "forceInclude": true
  },
  {
    "_id": "HYWhKXRsMAyvRKRYz",
    "url": null,
    "title": "You Can Face Reality",
    "slug": "you-can-face-reality",
    "author": "Eliezer_Yudkowsky",
    "question": false,
    "tags": [
      {
        "name": "Poetry"
      },
      {
        "name": "Litanies & Mantras"
      },
      {
        "name": "Rationality"
      },
      {
        "name": "Courage"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "What is true is already so.\n\nOwning up to it doesn’t make it worse.\n\nNot being open about it doesn’t make it go away.\n\nAnd because it’s true, it is what is there to be interacted with.\n\nAnything untrue isn’t there to be lived.\n\nPeople can stand what is true,\n\nfor they are already enduring it.\n\n> —_Eugene Gendlin_"
    },
    "voteCount": 99,
    "forceInclude": true
  },
  {
    "_id": "TGux5Fhcd7GmTfNGC",
    "url": null,
    "title": "Is That Your True Rejection?",
    "slug": "is-that-your-true-rejection",
    "author": "Eliezer_Yudkowsky",
    "question": false,
    "tags": [
      {
        "name": "Rationalization"
      },
      {
        "name": "Rationality"
      },
      {
        "name": "Motivated Reasoning"
      },
      {
        "name": "Motivations"
      },
      {
        "name": "Conversation (topic)"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "It happens every now and then that someone encounters some of my transhumanist-side beliefs—as opposed to my ideas having to do with human rationality—strange, exotic-sounding ideas like superintelligence and Friendly AI. And the one rejects them.\n\nIf the one is called upon to explain the rejection, not uncommonly the one says, “Why should I believe anything Yudkowsky says? He doesn’t have a PhD!”\n\nAnd occasionally someone else, hearing, says, “Oh, you should get a PhD, so that people will listen to you.” Or this advice may even be offered by the same one who expressed disbelief, saying, “Come back when you have a PhD.”\n\nNow, there are good and bad reasons to get a PhD. This is one of the bad ones.\n\nThere are many reasons why someone might actually have an initial adverse reaction to transhumanist theses. Most are matters of pattern recognition, rather than verbal thought: the thesis calls to mind an associated category like “strange weird idea” or “science fiction” or “end-of-the-world cult” or “overenthusiastic youth.”^[1](#fn1x34)^ Immediately, at the speed of perception, the idea is rejected.\n\nIf someone afterward says, “Why not?” this launches a search for justification, but the search won’t necessarily hit on the true reason. By “‘true reason,” I don’t mean the _best_ reason that could be offered. Rather, I mean whichever causes were decisive as a matter of historical fact, at the _very first_ moment the rejection occurred.\n\nInstead, the search for justification hits on the justifying-sounding fact, “This speaker does not have a PhD.” But I also don’t have a PhD when I talk about human rationality, so why is the same objection not raised there?\n\nMore to the point, if I _had_ a PhD, people would not treat this as a decisive factor indicating that they ought to believe everything I say. Rather, the same initial rejection would occur, for the same reasons; and the search for justification, afterward, would terminate at a different stopping point.\n\nThey would say, “Why should I believe _you_? You’re just some guy with a PhD! There are lots of those. Come back when you’re well-known in your field and tenured at a major university.”\n\nBut do people _actually_ believe arbitrary professors at Harvard who say weird things? Of course not.\n\nIf you’re saying things that sound _wrong_ to a novice, as opposed to just rattling off magical-sounding technobabble about leptical quark braids in N + 2 dimensions; and if the hearer is a stranger, unfamiliar with you personally and unfamiliar with the subject matter of your field; then I suspect that the point at which the average person will actually start to grant credence overriding their initial impression, purely because of academic credentials, is somewhere around the Nobel Laureate level. If that. Roughly, you need whatever level of academic credential qualifies as “beyond the mundane.”\n\nThis is more or less what happened to Eric Drexler, as far as I can tell. He presented his vision of nanotechnology, and people said, “Where are the technical details?” or “Come back when you have a PhD!” And Eric Drexler spent six years writing up technical details and got his PhD under Marvin Minsky for doing it. And _Nanosystems_ is a great book. But did the same people who said, “Come back when you have a PhD,” actually change their minds at all about molecular nanotechnology? Not so far as I ever heard.\n\nThis might be an important thing for young businesses and new-minted consultants to keep in mind—that what your failed prospects _tell_ you is the reason for rejection may not make the _real_ difference; and you should ponder that carefully before spending huge efforts. If the venture capitalist says, “If only your sales were growing a little faster!” or if the potential customer says, “It seems good, but you don’t have feature X,” that may not be the _true_ rejection. Fixing it may, or may not, change anything.\n\nAnd it would also be something to keep in mind during disagreements. Robin Hanson and I share a belief that two rationalists should not agree to disagree: they should not have common knowledge of epistemic disagreement unless something is very wrong.^[2](#fn2x34)^\n\nI suspect that, in general, if two rationalists set out to resolve a disagreement that persisted past the first exchange, they should expect to find that the true sources of the disagreement are either hard to communicate, or hard to expose. E.g.:\n\n*   Uncommon, but well-supported, scientific knowledge or math;\n*   Long inferential distances;\n*   Hard-to-verbalize intuitions, perhaps stemming from specific visualizations;\n*   Zeitgeists inherited from a profession (that may have good reason for it);\n*   Patterns perceptually recognized from experience;\n*   Sheer habits of thought;\n*   Emotional commitments to believing in a particular outcome;\n*   Fear that a past mistake could be disproved;\n*   Deep self-deception for the sake of pride or other personal benefits.\n\nIf the matter were one in which _all_ the true rejections could be _easily_ laid on the table, the disagreement would probably be so straightforward to resolve that it would never have lasted past the first meeting.\n\n“Is this my true rejection?” is something that both disagreers should surely be asking _themselves_, to make things easier on the other person. However, attempts to directly, publicly psychoanalyze the other may cause the conversation to degenerate _very_ fast, from what I’ve seen.\n\nStill—“Is that your true rejection?” should be fair game for Disagreers to humbly ask, if there’s any productive way to pursue that sub-issue. Maybe the rule could be that you can openly ask, “Is that simple straightforward-sounding reason your _true_ rejection, or does it come from intuition-X or professional-zeitgeist-Y ?” While the more embarrassing possibilities lower on the table are left to the Other’s conscience, as their own responsibility to handle.\n\n^[1](#fn1x34-bk)^See “[Science as Attire](https://lesswrong.com/rationality/science-as-attire)” in _Map and Territory_.\n\n^[2](#fn2x34-bk)^See Hal Finney, “Agreeing to Agree,” _Overcoming Bias_ (blog), 2006, [http://www.overcomingbias.com/2006/12/agreeing\\_to\\_agr.html](http://www.overcomingbias.com/2006/12/agreeing_to_agr.html)."
    },
    "voteCount": 91,
    "forceInclude": true
  },
  {
    "_id": "dHQkDNMhj692ayx78",
    "url": null,
    "title": "Avoiding Your Belief's Real Weak Points",
    "slug": "avoiding-your-belief-s-real-weak-points",
    "author": "Eliezer_Yudkowsky",
    "question": false,
    "tags": [
      {
        "name": "Religion"
      },
      {
        "name": "Motivated Reasoning"
      },
      {
        "name": "Noticing"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "A few years back, my great-grandmother died, in her nineties, after a long, slow, and cruel disintegration. I never knew her as a person, but in my distant childhood, she cooked for her family; I remember her gefilte fish, and her face, and that she was kind to me. At her funeral, my grand-uncle, who had taken care of her for years, spoke. He said, choking back tears, that God had called back his mother piece by piece: her memory, and her speech, and then finally her smile; and that when God finally took her smile, he knew it wouldn’t be long before she died, because it meant that she was almost entirely gone.\n\nI heard this and was puzzled, because it was an unthinkably horrible thing to happen to _anyone_, and therefore I would not have expected my grand-uncle to attribute it to God. Usually, a Jew would somehow just-not-think-about the logical implication that God had permitted a tragedy. According to Jewish theology, God continually sustains the universe and chooses every event in it; but ordinarily, drawing logical implications from this belief is reserved for happier occasions. By saying “God did it!” only when you’ve been blessed with a baby girl, and just-not-thinking “God did it!” for miscarriages and stillbirths and crib deaths, you can build up quite a lopsided picture of your God’s benevolent personality.\n\nHence I was surprised to hear my grand-uncle attributing the slow disintegration of his mother to a deliberate, strategically planned act of God. It violated the rules of religious self-deception as I understood them.\n\nIf I had noticed my own confusion, I could have made a successful surprising prediction. Not long afterward, my grand-uncle left the Jewish religion. (The only member of my extended family besides myself to do so, as far as I know.)\n\nModern Orthodox Judaism is like no other religion I have ever heard of, and I don’t know how to describe it to anyone who hasn’t been forced to study Mishna and Gemara. There is a tradition of questioning, but the _kind_ of questioning . . . It would not be at all surprising to hear a rabbi, in his weekly sermon, point out the conflict between the seven days of creation and the 13.7 billion years since the Big Bang—because he thought he had a really clever explanation for it, involving three other Biblical references, a Midrash, and a half-understood article in _Scientific American._ In Orthodox Judaism you’re allowed to notice inconsistencies and contradictions, but only for purposes of explaining them away, and whoever comes up with the most complicated explanation gets a prize.\n\nThere is a tradition of inquiry. But you only attack targets for purposes of defending them. You only attack targets you know you can defend.\n\nIn Modern Orthodox Judaism I have not heard much emphasis of the virtues of blind faith. You’re allowed to doubt. You’re just not allowed to _successfully_ doubt.\n\nI expect that the vast majority of educated Orthodox Jews have questioned their faith at some point in their lives. But the questioning probably went something like this: “According to the skeptics, the Torah says that the universe was created in seven days, which is not scientifically accurate. But would the original tribespeople of Israel, gathered at Mount Sinai, have been able to understand the scientific truth, even if it had been presented to them? Did they even have a word for ‘billion’? It’s easier to see the seven-days story as a metaphor—first God created light, which represents the Big Bang . . .”\n\nIs this the weakest point at which to attack one’s own Judaism? Read a bit further on in the Torah, and you can find God killing the first-born male children of Egypt to convince an unelected Pharaoh to release slaves who logically could have been teleported out of the country. An Orthodox Jew is most certainly familiar with this episode, because they are supposed to read through the entire Torah in synagogue once per year, and this event has an associated major holiday. The name “Passover” (“Pesach”) comes from God _passing over_ the Jewish households while killing every male firstborn in Egypt.\n\nModern Orthodox Jews are, by and large, kind and civilized people; far more civilized than the several editors of the Old Testament. Even the old rabbis were more civilized. There’s a ritual in the Seder where you take ten drops of wine from your cup, one drop for each of the Ten Plagues, to emphasize the suffering of the Egyptians. (Of course, you’re supposed to be sympathetic to the suffering of the Egyptians, but not _so_ sympathetic that you stand up and say, “This is not right! It is _wrong_ to do such a thing!”) It shows an interesting contrast—the rabbis were sufficiently kinder than the compilers of the Old Testament that they saw the harshness of the Plagues. But Science was weaker in these days, and so rabbis could ponder the more unpleasant aspects of Scripture without fearing that it would break their faith entirely.\n\nYou don’t even _ask_ whether the incident reflects poorly on God, so there’s no need to quickly blurt out “The ways of God are mysterious!” or “We’re not wise enough to question God’s decisions!” or “Murdering babies is okay when God does it!” That part of the question is just-not-thought-about.\n\nThe reason that educated religious people stay religious, I suspect, is that when they doubt, they are subconsciously very careful to attack their own beliefs only at the strongest points—places where they know they can defend. Moreover, places where rehearsing the standard defense will feel strengthening.\n\nIt probably feels really good, for example, to rehearse one’s prescripted defense for “Doesn’t Science say that the universe is just meaningless atoms bopping around?” because it confirms the meaning of the universe and how it flows from God, etc. Much more comfortable to think about than an illiterate Egyptian mother wailing over the crib of her slaughtered son. Anyone who _spontaneously_ thinks about the latter, when questioning their faith in Judaism, is _really_ questioning it, and is probably not going to stay Jewish much longer.\n\nMy point here is not just to beat up on Orthodox Judaism. I’m sure that there’s some reply or other for the Slaying of the Firstborn, and probably a dozen of them. My point is that, when it comes to spontaneous self-questioning, one is much more likely to spontaneously self-attack strong points with comforting replies to rehearse, than to spontaneously self-attack the weakest, most vulnerable points. Similarly, one is likely to stop at the first reply and be comforted, rather than further criticizing the reply. A better title than “Avoiding Your Belief’s Real Weak Points” would be “Not Spontaneously Thinking About Your Belief’s Most Painful Weaknesses.”\n\nMore than anything, the grip of religion is sustained by people just-not-thinking-about the real weak points of their religion. I don’t think this is a matter of training, but a matter of instinct. People don’t think about the real weak points of their beliefs for the same reason they don’t touch an oven’s red-hot burners; it’s _painful._\n\nTo do better: When you’re doubting one of your most cherished beliefs, close your eyes, empty your mind, grit your teeth, and deliberately think about whatever hurts the most. Don’t rehearse standard objections whose standard counters would make you feel better. Ask yourself what _smart_ people who disagree would say to your first reply, and your second reply. Whenever you catch yourself flinching away from an objection you fleetingly thought of, drag it out into the forefront of your mind. Punch yourself in the solar plexus. Stick a knife in your heart, and wiggle to widen the hole. In the face of the pain, rehearse only this:^[1](#fn1x31)^\n\n> What is true is already so.\n> \n> Owning up to it doesn’t make it worse.\n> \n> Not being open about it doesn’t make it go away.\n> \n> And because it’s true, it is what is there to be interacted with.\n> \n> Anything untrue isn’t there to be lived.\n> \n> People can stand what is true,\n> \n> for they are already enduring it.\n\n^[1](#fn1x31-bk)^Eugene T. Gendlin, _Focusing_ (Bantam Books, 1982)."
    },
    "voteCount": 100,
    "forceInclude": true
  },
  {
    "_id": "nYkMLFpx77Rz3uo9c",
    "url": null,
    "title": "Belief as Attire",
    "slug": "belief-as-attire",
    "author": "Eliezer Yudkowsky",
    "question": false,
    "tags": [
      {
        "name": "Social Reality"
      },
      {
        "name": "Rationality"
      },
      {
        "name": "Motivations"
      },
      {
        "name": "Motivated Reasoning"
      },
      {
        "name": "Tribalism"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "I have so far distinguished between belief as [anticipation-controller](https://www.lesswrong.com/lw/i3/making_beliefs_pay_rent_in_anticipated_experiences/), [belief in belief](https://www.lesswrong.com/lw/i4/belief_in_belief/), [professing and cheering](https://www.lesswrong.com/lw/i6/professing_and_cheering/).  Of these, we might call anticipation-controlling beliefs \"proper beliefs\" and the other forms \"improper belief\".  proper belief can be wrong or irrational, as when someone genuinely anticipates that prayer will cure their sick baby. But the other forms are arguably “not belief at all.”\n\nYet another form of improper belief is belief as group identification—as a way of belonging. Robin Hanson uses the excellent [metaphor](http://lesswrong.com/lw/i6/professing_and_cheering/egb) of wearing unusual clothing, a group uniform like a priest’s vestments or a Jewish skullcap, and so I will call this “belief as attire.”\n\nIn terms of [humanly realistic psychology](https://www.lesswrong.com/lw/i0/are_your_enemies_innately_evil/), the Muslims who flew planes into the World Trade Center undoubtedly saw themselves as heroes defending truth, justice, and the Islamic Way from hideous alien monsters a la the movie *Independence Day*. Only a very inexperienced nerd, the sort of nerd who has no idea how non-nerds see the world, would say this out loud in an Alabama bar. It is not an American thing to say. The American thing to say is that the terrorists “hate our freedom” and that flying a plane into a building is a “cowardly act.” You cannot say the phrases “heroic self-sacrifice” and “suicide bomber” in the same sentence, even for the sake of accurately describing how the Enemy sees the world. The very *concept* of the courage and altruism of a suicide bomber is Enemy attire—you can tell, because the Enemy talks about it. The cowardice and sociopathy of a suicide bomber is American attire. There are no quote marks you can use to talk about how the Enemy sees the world; it would be like dressing up as a Nazi for Halloween.\n\nBelief-as-attire may help explain how people can be *passionate* about improper beliefs. Mere belief in belief, or religious professing, would have some trouble creating genuine, deep, powerful emotional effects. Or so I suspect; I confess I’m not an expert here. But my impression is this: People who’ve stopped anticipating-as-if their religion is true, will go to great lengths to *convince* themselves they are passionate, and this desperation can be mistaken for passion. But it’s not the same fire they had as a child.\n\nOn the other hand, it is very easy for a human being to genuinely, passionately, gut-level belong to a group, to cheer for [their favorite sports team](https://www.lesswrong.com/lw/gt/a_fable_of_science_and_politics/).^1^ Identifying with a tribe is a very strong emotional force. People will die for it. And once you get people to identify with a tribe, the beliefs which are the attire of that tribe will be spoken with the full passion of belonging to that tribe.\n\n* * *\n\n^1^ This is the foundation on which rests the swindle of “Republicans vs. Democrats” and analogous [false dilemmas](https://www.lesswrong.com/lw/hu/the_third_alternative/) in other countries, but that’s a topic for another time."
    },
    "voteCount": 83,
    "forceInclude": true
  },
  {
    "_id": "2MD3NMLBPCqPfnfre",
    "url": null,
    "title": "Cached Thoughts",
    "slug": "cached-thoughts",
    "author": "Eliezer_Yudkowsky",
    "question": false,
    "tags": [
      {
        "name": "Rationality"
      },
      {
        "name": "Cached Thoughts"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "One of the single greatest puzzles about the human brain is how the damn thing works _at all_ when most neurons fire 10–20 times per second, or 200Hz tops. In neurology, the “hundred-step rule” is that any postulated operation has to complete in _at most_ 100 sequential steps—you can be as parallel as you like, but you can’t postulate more than 100 (preferably fewer) neural spikes one after the other.\n\nCan you imagine having to program using 100Hz CPUs, no matter how many of them you had? You’d also need a hundred billion processors just to get _anything_ done in realtime.\n\nIf you did need to write realtime programs for a hundred billion 100Hz processors, one trick you’d use as heavily as possible is caching. That’s when you store the results of previous operations and look them up next time, instead of recomputing them from scratch. And it’s a very _neural_ idiom—recognition, association, completing the pattern.\n\nIt’s a good guess that the actual _majority_ of human cognition consists of cache lookups.\n\nThis thought does tend to go through my mind at certain times.\n\nThere was a wonderfully illustrative story which I thought I had bookmarked, but couldn’t re-find: it was the story of a man whose know-it-all neighbor had once claimed in passing that the best way to remove a chimney from your house was to knock out the fireplace, wait for the bricks to drop down one level, knock out those bricks, and repeat until the chimney was gone. Years later, when the man wanted to remove his own chimney, this cached thought was lurking, waiting to pounce . . .\n\nAs the man noted afterward—you can guess it didn’t go well—his neighbor was not particularly knowledgeable in these matters, not a trusted source. If he’d _questioned_ the idea, he probably would have realized it was a poor one. Some cache hits we’d be better off recomputing. But the brain completes the pattern automatically—and if you don’t consciously realize the pattern needs correction, you’ll be left with a completed pattern.\n\nI suspect that if the thought had occurred to the man himself—if he’d _personally_ had this bright idea for how to remove a chimney—he would have examined the idea more critically. But if someone _else_ has already thought an idea through, you can save on computing power by caching their _conclusion_—right?\n\nIn modern civilization particularly, no one can think fast enough to think their own thoughts. If I’d been abandoned in the woods as an infant, raised by wolves or silent robots, I would scarcely be recognizable as human. No one can think fast enough to recapitulate the wisdom of a hunter-gatherer tribe in one lifetime, starting from scratch. As for the wisdom of a literate civilization, forget it.\n\nBut the flip side of this is that I continually see people who aspire to critical thinking, repeating back cached thoughts which were not invented by critical thinkers.\n\nA good example is the skeptic who concedes, “Well, you can’t prove or disprove a religion by factual evidence.” As I have pointed out elsewhere,^[1](#fn1x41)^ this is simply false as probability theory. And it is also simply false relative to the real psychology of religion—a few centuries ago, saying this would have gotten you burned at the stake. A mother whose daughter has cancer prays, “God, please heal my daughter,” not, “Dear God, I know that religions are not allowed to have any falsifiable consequences, which means that you can’t possibly heal my daughter, so . . . well, basically, I’m praying to make myself feel better, instead of doing something that could actually help my daughter.”\n\nBut people read “You can’t prove or disprove a religion by factual evidence,” and then, the next time they see a piece of evidence disproving a religion, their brain completes the pattern. Even some atheists repeat this absurdity without hesitation. If they’d thought of the idea themselves, rather than hearing it from someone else, they would have been more skeptical.\n\nDeath. Complete the pattern: “Death gives meaning to life.”\n\nIt’s frustrating, talking to good and decent folk—people who would never in a thousand years _spontaneously_ think of wiping out the human species—raising the topic of existential risk, and hearing them say, “Well, maybe the human species doesn’t deserve to survive.” They would never in a thousand years shoot their own child, who is a part of the human species, but the brain completes the pattern.\n\nWhat patterns are being completed, inside your mind, that you never chose to be there?\n\nRationality. Complete the pattern: “Love isn’t rational.”\n\nIf this idea had suddenly occurred to you personally, as an entirely new thought, how would you examine it critically? I know what _I_ would [say](https://lesswrong.com/rationality/feeling-rational), but what would _you_? It can be hard to see with fresh eyes. Try to keep your mind from completing the pattern in the standard, unsurprising, already-known way. It may be that there is no better answer than the standard one, but you can’t _think_ about the answer until you can stop your brain from filling in the answer automatically.\n\nNow that you’ve read this, the next time you hear someone unhesitatingly repeating a meme you think is silly or false, you’ll think, “Cached thoughts.” My belief is now there in your mind, waiting to complete the pattern. But is it true? Don’t let your mind complete the pattern! _Think!_\n\n^[1](#fn1x41-bk)^See ‘[Religion’s Claim to be Non-Disprovable](https://www.lesswrong.com/rationality/religion-s-claim-to-be-non-disprovable),” in _Map and Territory_."
    },
    "voteCount": 118,
    "forceInclude": true
  },
  {
    "_id": "dLJv2CoRCgeC2mPgj",
    "url": null,
    "title": "The Fallacy of Gray",
    "slug": "the-fallacy-of-gray",
    "author": "Eliezer Yudkowsky",
    "question": false,
    "tags": [
      {
        "name": "Fallacies"
      },
      {
        "name": "Fallacy of Gray"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "> The Sophisticate: “The world isn’t black and white. No one does pure good or pure bad. It’s all gray. Therefore, no one is better than anyone else.”\n> \n> The Zetet: “Knowing only gray, you conclude that all grays are the same shade. You mock the simplicity of the two-color view, yet you replace it with a one-color view . . .”\n> \n> —Marc Stiegler, _David’s Sling_\n\nI don’t know if the Sophisticate’s mistake has an official name, but I call it the Fallacy of Gray. We saw it manifested in the previous essay—the one who believed that odds of two to the power of seven hundred and fifty million to one, against, meant “there was still a chance.” All probabilities, to him, were simply “uncertain” and that meant he was licensed to ignore them if he pleased.\n\n“The Moon is made of green cheese” and “the Sun is made of mostly hydrogen and helium” are both uncertainties, but they are not the same uncertainty.\n\nEverything is shades of gray, but there are shades of gray so light as to be very nearly white, and shades of gray so dark as to be very nearly black. Or even if not, we can still compare shades, and say “it is darker” or “it is lighter.”\n\nYears ago, one of the strange little formative moments in my career as a rationalist was reading this paragraph from _Player of Games_ by Iain M. Banks, especially the sentence in bold:\n\n> A guilty system recognizes no innocents. As with any power apparatus which thinks everybody’s either for it or against it, we’re against it. You would be too, if you thought about it. The very way you think places you amongst its enemies. This might not be your fault, because **every society imposes some of its values on those raised within it, but the point is that some societies try to maximize that effect, and some try to minimize it**. You come from one of the latter and you’re being asked to explain yourself to one of the former. Prevarication will be more difficult than you might imagine; neutrality is probably impossible. You cannot choose not to have the politics you do; they are not some separate set of entities somehow detachable from the rest of your being; they are a function of your existence. I know that and they know that; you had better accept it.\n\nNow, don’t write angry comments saying that, if societies impose fewer of their values, then each succeeding generation has more work to start over from scratch. That’s not what I got out of the paragraph.\n\nWhat I got out of the paragraph was something which seems so obvious in retrospect that I could have conceivably picked it up in a hundred places; but something about that one paragraph made it click for me.\n\nIt was the whole notion of the Quantitative Way applied to life-problems like moral judgments and the quest for personal self-improvement. That, even if you couldn’t switch something from on to off, you could still tend to increase it or decrease it.\n\nIs this too obvious to be worth mentioning? I say it is not too obvious, for many bloggers have said of _Overcoming Bias_: “It is impossible, no one can completely eliminate bias.” I don’t care if the one is a professional economist, it is clear that they have not yet grokked the Quantitative Way as it applies to everyday life and matters like personal self-improvement. That which I cannot _eliminate_ may be well worth _reducing_.\n\nOr consider an exchange between Robin Hanson and Tyler Cowen.^[1](#fn1x8)^ Robin Hanson said that he preferred to put at least 75% weight on the prescriptions of economic theory versus his intuitions: “I try to mostly just straightforwardly apply economic theory, adding little personal or cultural judgment.” Tyler Cowen replied:\n\n> In my view there is no such thing as “straightforwardly applying economic theory” . . . theories are always applied through our personal and cultural filters and there is no other way it can be.\n\nYes, but you can try to minimize that effect, or you can do things that are bound to increase it. And _if_ you try to minimize it, then in many cases I don’t think it’s unreasonable to call the output “straightforward”—even in economics.\n\n“Everyone is imperfect.” Mohandas Gandhi was imperfect and Joseph Stalin was imperfect, but they were not the same shade of imperfection. “Everyone is imperfect” is an excellent example of replacing a two-color view with a one-color view. If you say, “No one is perfect, but _some people are less imperfect than others_,” you may not gain applause; but for those who strive to do better, you have held out hope. No one is _perfectly_ imperfect, after all.\n\n(Whenever someone says to me, “Perfectionism is bad for you,” I reply: “I think it’s okay to be imperfect, but not so imperfect that other people notice.”)\n\nLikewise the folly of those who say, “Every scientific paradigm imposes some of its assumptions on how it interprets experiments,” and then act like they’d proven science to occupy the same level with witchdoctoring. Every worldview imposes some of its structure on its observations, but the point is that there are worldviews which try to minimize that imposition, and worldviews which glory in it. There is no white, but there are shades of gray that are far lighter than others, and it is folly to treat them as if they were all on the same level.\n\nIf the Moon has orbited the Earth these past few billion years, if you have seen it in the sky these last years, and you expect to see it in its appointed place and phase tomorrow, then that is not a certainty. And if you expect an invisible dragon to heal your daughter of cancer, that too is not a certainty. But they are rather different degrees of uncertainty—this business of expecting things to happen yet again in the same way you have previously predicted to twelve decimal places, versus expecting something to happen that _violates_ the order previously observed. Calling them both “faith” seems a little too un-narrow.\n\nIt’s a most peculiar psychology—this business of “Science is based on faith too, so there!” Typically this is said by people who claim that faith is a _good_ thing. Then why do they say “Science is based on faith too!” in that angry-triumphal tone, rather than as a compliment? And a rather _dangerous_ compliment to give, one would think, from their perspective. If science is based on “faith,” then science is of the same kind as religion—directly comparable. If science is a religion, it is the religion that heals the sick and reveals the secrets of the stars. It would make sense to say, “The priests of science can blatantly, publicly, verifiably walk on the Moon as a faith-based miracle, and your priests’ faith can’t do the same.” Are you sure you wish to go there, oh faithist? Perhaps, on further reflection, you would prefer to retract this whole business of “Science is a religion too!”\n\nThere’s a strange dynamic here: You try to purify your shade of gray, and you get it to a point where it’s pretty light-toned, and someone stands up and says in a deeply offended tone, “But it’s not white! It’s gray!” It’s one thing when someone says, “This isn’t as light as you think, because of specific problems X, Y, and Z.” It’s a different matter when someone says angrily “It’s not white! It’s gray!” without pointing out any specific dark spots.\n\nIn this case, I begin to suspect psychology that is more imperfect than usual—that someone may have made a devil’s bargain with their own mistakes, and now refuses to hear of any possibility of improvement. When someone finds an excuse not to try to do better, they often refuse to concede that anyone else _can_ try to do better, and every mode of improvement is thereafter their enemy, and every claim that it is possible to move forward is an offense against them. And so they say in one breath proudly, “I’m glad to be gray,” and in the next breath angrily, “And _you’re gray too!_”\n\nIf there is no black and white, there is yet lighter and darker, and not all grays are the same.\n\nThe commenter G2 points us to Asimov’s “The Relativity of Wrong”:\n\n> When people thought the earth was flat, they were wrong. When people thought the earth was spherical, they were wrong. But if you think that thinking the earth is spherical is just as wrong as thinking the earth is flat, then your view is wronger than both of them put together.\n\n^[1](#fn1x8-bk)^Hanson (2007), “Economist Judgment,” [http://www.overcomingbias.com/2007/12/economist-judgm.html](http://www.overcomingbias.com/2007/12/economist-judgm.html). Cowen (2007), “Can Theory Override Intuition?”, [http://marginalrevolution.com/marginalrevolution/2007/12/how-my-views-di.html](http://marginalrevolution.com/marginalrevolution/2007/12/how-my-views-di.html)."
    },
    "voteCount": 185,
    "forceInclude": true
  },
  {
    "_id": "CEGnJBHmkcwPTysb7",
    "url": null,
    "title": "Lonely Dissent",
    "slug": "lonely-dissent",
    "author": "Eliezer_Yudkowsky",
    "question": false,
    "tags": [
      {
        "name": "Rationality"
      },
      {
        "name": "Social Reality"
      },
      {
        "name": "Groupthink"
      },
      {
        "name": "Social & Cultural Dynamics"
      },
      {
        "name": "Courage"
      },
      {
        "name": "Conformity Bias"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "Asch’s conformity experiment showed that the presence of a single dissenter tremendously reduced the incidence of “conforming” wrong answers. Individualism is easy, experiment shows, when you have company in your defiance. Every other subject in the room, except one, says that black is white. You become the second person to say that black is black. And it feels glorious: the two of you, lonely and defiant rebels, against the world!^[1](#fn1x63)^\n\nBut you can only _join_ the rebellion after someone, somewhere, becomes the _first_ to rebel. Someone has to say that black is black after hearing _everyone_ else, one after the other, say that black is white. And that—experiment shows—is a _lot harder_.\n\nLonely dissent doesn’t feel like going to school dressed in black. It feels like going to school wearing a clown suit.\n\nThat’s the difference between _joining the rebellion_ and _leaving the pack_.\n\nIf there’s one thing I can’t stand, it’s fakeness—you may have noticed this. Well, lonely dissent has got to be one of the most commonly, most ostentatiously faked characteristics around. Everyone wants to be an iconoclast.\n\nI don’t mean to degrade the act of joining a rebellion. There are rebellions worth joining. It does take courage to brave the disapproval of your peer group, or perhaps even worse, their shrugs. Needless to say, going to a rock concert is not rebellion. But, for example, vegetarianism is. I’m not a vegetarian myself, but I respect people who are, because I expect it takes a noticeable amount of quiet courage to tell people that hamburgers won’t work for dinner.^[2](#fn2x63)^\n\nStill, if you tell people that you’re a vegetarian, they’ll think they understand your motives (even if they don’t). They may disagree. They may be offended if you manage to announce it proudly enough, or for that matter, they may be offended just because they’re easily offended. But they know how to relate to you.\n\nWhen someone wears black to school, the teachers and the other children understand the role thereby being assumed in their society. It’s Outside the System—in a very standard way that everyone recognizes and understands. Not, y’know, _actually_ outside the system. It’s a Challenge to Standard Thinking, of a standard sort, so that people indignantly say, “I can’t understand why you—” but don’t have to actually think any thoughts they had not thought before. As the saying goes, “Has any of the ‘subversive literature’ you’ve read caused you to modify any of your political views?”\n\nWhat takes _real_ courage is braving the outright _incomprehension_ of the people around you, when you do something that _isn’t_ Standard Rebellion #37, something for which they lack a ready-made script. They don’t hate you for a rebel. They just think you’re, like, weird, and turn away. This prospect generates a much deeper fear. It’s the difference between explaining vegetarianism and explaining cryonics. There are other cryonicists in the world, somewhere, but they aren’t there next to you. You have to explain it, alone, to people who just think it’s _weird_. Not forbidden, but outside bounds that people don’t even think about. You’re going to get your head frozen? You think that’s going to stop you from dying? What do you mean, brain information? Huh? What? Are you _crazy?_\n\nI’m tempted to essay a post facto explanation in evolutionary psychology: You could get together with a small group of friends and walk away from your hunter-gatherer band, but having to go it _alone_ in the forests was probably a death sentence—at least reproductively. We don’t reason this out explicitly, but that is not the nature of evolutionary psychology. Joining a rebellion that everyone knows about is scary, but nowhere near as scary as doing something really differently—something that in ancestral times might have concluded, not with the band splitting, but with you being driven out alone.\n\nAs the case of cryonics testifies, the fear of thinking _really_ different is stronger than the fear of death. Hunter-gatherers had to be ready to face death on a routine basis—hunting large mammals, or just walking around in a world that contained predators. They needed that courage in order to live. Courage to defy the tribe’s standard ways of thinking, to entertain thoughts that seem truly weird—well, that probably didn’t serve its bearers as well. We don’t reason this out explicitly; that’s not how evolutionary psychology works. We human beings are just built in such fashion that many more of us go skydiving than sign up for cryonics.\n\nAnd that’s not even the highest courage. There’s more than one cryonicist in the world. Only Robert Ettinger had to say it _first._\n\nTo be a _scientific_ revolutionary, you’ve got to be the first person to contradict what everyone else you know is thinking. This is not the only route to scientific greatness; it is rare even among the great. No one can become a scientific revolutionary by trying to imitate revolutionariness. You can only get there by pursuing the correct answer in all things, whether the correct answer is revolutionary or not. But if, in the due course of time—if, having absorbed all the power and wisdom of the knowledge that has already accumulated—if, after all that and a dose of sheer luck, you find your pursuit of mere correctness taking you into new territory . . . _then_ you have an opportunity for your courage to fail.\n\nThis is the true courage of lonely dissent, which every damn rock band out there tries to fake.\n\nOf course, not everything that takes courage is a good idea. It would take courage to walk off a cliff, but then you would just go splat.\n\nThe _fear_ of lonely dissent is a hindrance to good ideas, but not every dissenting idea is good.^[3](#fn3x63)^ Most of the difficulty in having a new true scientific thought is in the “true” part.\n\nIt really isn’t _necessary_ to be different for the sake of being different. If you do things differently only when you see an overwhelmingly good reason, you will have more than enough trouble to last you the rest of your life.\n\nThere are a few genuine packs of iconoclasts around. The Church of the SubGenius, for example, seems to genuinely aim at _confusing_ the mundanes, not merely offending them. And there are islands of genuine tolerance in the world, such as science fiction conventions. There _are_ certain people who have no fear of departing the pack. Many fewer such people really exist, than imagine themselves rebels; but they do exist. And yet scientific revolutionaries are tremendously rarer. Ponder that.\n\nNow _me_, you know, I _really am_ an iconoclast. Everyone thinks they are, but with me it’s _true_, you see. I would _totally_ have worn a clown suit to school. My serious conversations were with books, not with other children.\n\nBut if you think you would _totally_ wear that clown suit, then don’t be too proud of that either! It just means that you need to make an effort in the _opposite direction_ to avoid dissenting too easily. That’s what I have to do, to correct for my own nature. Other people do have reasons for thinking what they do, and ignoring that completely is as bad as being afraid to contradict them. You wouldn’t want to end up as a free thinker. It’s not a _virtue_, you see—just a bias either way.\n\n^[1](#fn1x63-bk)^Followup interviews showed that subjects in the one-dissenter condition expressed strong feelings of camaraderie with the dissenter—though, of course, they didn’t think the presence of the dissenter had influenced their own nonconformity.\n\n^[2](#fn2x63-bk)^Albeit that in the Bay Area, people ask as a matter of routine.\n\n^[3](#fn3x63-bk)^See Robin Hanson, “Against Free Thinkers,” _Overcoming Bias_ (blog), 2007, [http://www.overcoming-bias.com/2007/06/against\\_free\\_th.html](http://www.overcoming-bias.com/2007/06/against_free_th.html)."
    },
    "voteCount": 78,
    "forceInclude": true
  },
  {
    "_id": "rmAbiEKQDpDnZzcRf",
    "url": null,
    "title": "Positive Bias: Look Into the Dark",
    "slug": "positive-bias-look-into-the-dark",
    "author": "Eliezer Yudkowsky",
    "question": false,
    "tags": [
      {
        "name": "Confirmation Bias"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "I am teaching a class, and I write upon the blackboard three numbers: 2-4-6. “I am thinking of a rule,” I say, “which governs sequences of three numbers. The sequence 2-4-6, as it so happens, obeys this rule. Each of you will find, on your desk, a pile of index cards. Write down a sequence of three numbers on a card, and I’ll mark it ‘Yes’ for fits the rule, or ‘No’ for not fitting the rule. Then you can write down another set of three numbers and ask whether it fits again, and so on. When you’re confident that you know the rule, write down the rule on a card. You can test as many triplets as you like.”\n\nHere’s the record of one student’s guesses:  \n \n\n<table><tbody><tr><td>4-6-2</td><td>No</td></tr><tr><td>4-6-8</td><td>Yes</td></tr><tr><td>10-12-14</td><td>Yes</td></tr></tbody></table>\n\nAt this point the student wrote down their guess at the rule. What do *you* think the rule is? Would you have wanted to test another triplet, and if so, what would it be? Take a moment to think before continuing.\n\nThe challenge above is based on a classic experiment due to Peter Wason, the 2-4-6 task. Although subjects given this task typically expressed high confidence in their guesses, only 21% of the subjects successfully guessed the experimenter’s real rule, and replications since then have continued to show success rates of around 20%.\n\nThe study was called “On the failure to eliminate hypotheses in a conceptual task.” Subjects who attempt the 2-4-6 task usually try to generate *positive* examples, rather than *negative* examples—they apply the hypothetical rule to generate a representative instance, and see if it is labeled “Yes.”\n\nThus, someone who forms the hypothesis “numbers increasing by two” will test the triplet 8-10-12, hear that it fits, and confidently announce the rule. Someone who forms the hypothesis X-2X-3X will test the triplet 3-6-9, discover that it fits, and then announce that rule.\n\nIn every case the actual rule is the same: the three numbers must be in ascending order.\n\nBut to discover this, you would have to generate triplets that *shouldn’t* fit, such as 20-23-26, and see if they are labeled “No.” Which people tend not to do, in this experiment. In some cases, subjects devise, “test,” and announce rules far more complicated than the actual answer.\n\nThis cognitive phenomenon is usually lumped in with “confirmation bias.” However, it seems to me that the phenomenon of trying to test *positive* rather than *negative* examples, ought to be distinguished from the phenomenon of trying to preserve the belief you started with. “Positive bias” is sometimes used as a synonym for “confirmation bias,” and fits this particular flaw much better.\n\nIt once seemed that phlogiston theory could explain a flame going out in an enclosed box (the air became saturated with phlogiston and no more could be released). But phlogiston theory could just as well have explained the flame *not* going out. To notice this, you have to search for negative examples instead of positive examples, look into zero instead of one; which goes against the grain of what experiment has shown to be human instinct.\n\nFor by instinct, we human beings only live in half the world.\n\nOne may be lectured on positive bias for days, and yet overlook it in-the-moment. Positive bias is not something we do as a matter of logic, or even as a matter of emotional attachment. The 2-4-6 task is “cold,” logical, not affectively “hot.” And yet the mistake is sub-verbal, on the level of imagery, of instinctive reactions. Because the problem doesn’t arise from following a deliberate rule that says “Only think about positive examples,” it can’t be solved just by knowing verbally that “We ought to think about both positive and negative examples.” Which example automatically pops into your head? You have to learn, wordlessly, to zag instead of zig. You have to learn to flinch toward the zero, instead of away from it.\n\nI have been writing for quite some time now on the notion that the strength of a hypothesis is what it *can’t* explain, not what it *can*—if you are equally good at explaining any outcome, you have zero knowledge. So to spot an explanation that isn’t helpful, it’s not enough to think of what it does explain very well—you also have to search for results it *couldn’t* explain, and this is the true strength of the theory.\n\nSo I said all this, and then I challenged the usefulness of “emergence” as a concept. One commenter cited superconductivity and ferromagnetism as examples of emergence. I replied that non-superconductivity and non-ferromagnetism were also examples of emergence, which was the problem. But be it far from me to criticize the commenter! Despite having read extensively on “confirmation bias,” I didn’t spot the “gotcha” in the 2-4-6 task the first time I read about it. It’s a subverbal blink-reaction that has to be retrained. I’m still working on it myself.\n\nSo much of a rationalist’s skill is below the level of words. It makes for challenging work in trying to convey the Art through words. People will agree with you, but then, in the next sentence, do something subdeliberative that goes in the opposite direction. Not that I’m complaining! A major reason I’m writing this is to observe what my words *haven’t* conveyed.\n\nAre you searching for positive examples of positive bias right now, or sparing a fraction of your search on what positive bias should lead you to *not* see? Did you look toward light or darkness?"
    },
    "voteCount": 96,
    "forceInclude": true
  },
  {
    "_id": "AdYdLP2sRqPMoe8fb",
    "url": null,
    "title": "Knowing About Biases Can Hurt People",
    "slug": "knowing-about-biases-can-hurt-people",
    "author": "Eliezer Yudkowsky",
    "question": false,
    "tags": [
      {
        "name": "Rationalization"
      },
      {
        "name": "Rationality"
      },
      {
        "name": "Information Hazards"
      },
      {
        "name": "Pitfalls of Rationality"
      },
      {
        "name": "Fallacies"
      },
      {
        "name": "Heuristics & Biases"
      },
      {
        "name": "Valley of Bad Rationality"
      },
      {
        "name": "Motivational Intro Posts"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "Once upon a time I tried to tell my mother about the problem of expert calibration, saying: “So when an expert says they’re 99% confident, it only happens about 70% of the time.” Then there was a pause as, suddenly, I realized I was talking to my mother, and I hastily added: “Of course, you’ve got to make sure to apply that skepticism evenhandedly, including to yourself, rather than just using it to argue against anything you disagree with—”\n\nAnd my mother said: “Are you kidding? This is great! I’m going to use it all the time!”\n\nTaber and Lodge’s “Motivated Skepticism in the Evaluation of Political Beliefs” describes the confirmation of six predictions:\n\n1.  Prior attitude effect. Subjects who feel strongly about an issue—even when encouraged to be objective—will evaluate supportive arguments more favorably than contrary arguments.\n2.  Disconfirmation bias. Subjects will spend more time and cognitive resources denigrating contrary arguments than supportive arguments.\n3.  Confirmation bias. Subjects free to choose their information sources will seek out supportive rather than contrary sources.\n4.  **Attitude polarization. Exposing subjects to an apparently balanced set of pro and con arguments will exaggerate their initial polarization.**\n5.  Attitude strength effect. Subjects voicing stronger attitudes will be more prone to the above biases.\n6.  **Sophistication effect. Politically knowledgeable subjects, because they possess greater ammunition with which to counter-argue incongruent facts and arguments, will be more prone to the above biases.**\n\nIf you’re irrational to start with, having *more* knowledge can *hurt* you. For a true Bayesian, information would never have negative expected utility. But humans aren’t perfect Bayes-wielders; if we’re not careful, we can cut ourselves.\n\nI’ve *seen* people severely messed up by their own knowledge of biases. They have more ammunition with which to argue against anything they don’t like. And that problem—too much ready ammunition—is one of the primary ways that people with high mental agility end up stupid, in Stanovich’s “dysrationalia” sense of stupidity.\n\nYou can think of people who fit this description, right? People with high g-factor who end up being *less* effective because they are too sophisticated as arguers? Do you think you’d be helping them—making them more effective rationalists—if you just told them about a list of classic biases?\n\nI recall someone who learned about the calibration/overconfidence problem. Soon after he said: “Well, you can’t trust experts; they’re wrong so often—as experiments have shown. So therefore, when I predict the future, I prefer to assume that things will continue historically as they have—” and went off into this whole complex, error-prone, highly questionable extrapolation. Somehow, when it came to trusting his own preferred conclusions, all those biases and fallacies seemed much less *salient*—leapt much less readily to mind—than when he needed to counter-argue someone else.\n\nI told the one about the problem of disconfirmation bias and sophisticated argument, and lo and behold, the next time I said something he didn’t like, he accused me of being a sophisticated arguer. He didn’t try to point out any particular sophisticated argument, any particular flaw—just shook his head and sighed sadly over how I was apparently using my own intelligence to defeat itself. He had acquired yet another Fully General Counterargument.\n\nEven the notion of a “sophisticated arguer” can be deadly, if it leaps all too readily to mind when you encounter a seemingly intelligent person who says something you don’t like.\n\nI endeavor to learn from my mistakes. The last time I gave a talk on heuristics and biases, I started out by introducing the general concept by way of the conjunction fallacy and representativeness heuristic. And then I moved on to confirmation bias, disconfirmation bias, sophisticated argument, motivated skepticism, and other attitude effects. I spent the next thirty minutes *hammering* on that theme, reintroducing it from as many different perspectives as I could.\n\nI wanted to get my audience interested in the subject. Well, a simple description of conjunction fallacy and representativeness would suffice for that. But suppose they did get interested. Then what? The literature on bias is mostly cognitive psychology for cognitive psychology’s sake. I had to give my audience their dire warnings during that one lecture, or they probably wouldn’t hear them at all.\n\nWhether I do it on paper, or in speech, I now try to never mention calibration and overconfidence unless I have first talked about disconfirmation bias, motivated skepticism, sophisticated arguers, and dysrationalia in the mentally agile. First, do no harm!"
    },
    "voteCount": 140,
    "forceInclude": true
  },
  {
    "_id": "9weLK2AJ9JEt2Tt8f",
    "url": null,
    "title": "Politics is the Mind-Killer",
    "slug": "politics-is-the-mind-killer",
    "author": "Eliezer Yudkowsky",
    "question": false,
    "tags": [
      {
        "name": "Politics"
      },
      {
        "name": "Rationality"
      },
      {
        "name": "Tribalism"
      },
      {
        "name": "Social & Cultural Dynamics"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "People go funny in the head when talking about politics. The evolutionary reasons for this are so obvious as to be worth belaboring: In the ancestral environment, politics was a matter of life and death. And sex, and wealth, and allies, and reputation . . . When, today, you get into an argument about whether “we” ought to raise the minimum wage, you’re executing adaptations for an ancestral environment where being on the wrong side of the argument could get you killed. Being on the *right* side of the argument could let *you* kill your hated rival!\n\nIf you want to make a point about science, or rationality, then my advice is to not choose a domain from *contemporary* politics if you can possibly avoid it. If your point is inherently about politics, then talk about Louis XVI during the French Revolution. Politics is an important domain to which we should individually apply our rationality—but it’s a terrible domain in which to *learn* rationality, or discuss rationality, unless all the discussants are already rational.\n\nPolitics is an extension of war by other means. Arguments are soldiers. Once you know which side you’re on, you must support all arguments of that side, and attack all arguments that appear to favor the enemy side; otherwise it’s like stabbing your soldiers in the back—providing aid and comfort to the enemy. People who would be level-headed about evenhandedly weighing all sides of an issue in their professional life as scientists, can suddenly turn into slogan-chanting zombies when there’s a [Blue or Green](https://lesswrong.com/rationality/a-fable-of-science-and-politics) position on an issue.\n\nIn artificial intelligence, and particularly in the domain of nonmonotonic reasoning, there’s a standard problem: “All Quakers are pacifists. All Republicans are not pacifists. Nixon is a Quaker and a Republican. Is Nixon a pacifist?”\n\nWhat on Earth was the point of choosing this as an example? To rouse the political emotions of the readers and distract them from the main question? To make Republicans feel unwelcome in courses on artificial intelligence and discourage them from entering the field?[^1^](#fn1x14)\n\nWhy would anyone pick such a *distracting* example to illustrate nonmonotonic reasoning? Probably because the author just couldn’t resist getting in a good, solid dig at those hated Greens. It feels so *good* to get in a hearty punch, y’know, it’s like trying to resist a chocolate cookie.\n\nAs with chocolate cookies, not everything that feels pleasurable is good for you.\n\nI’m not saying that I think we should be apolitical, or even that we should adopt Wikipedia’s ideal of the Neutral Point of View. But try to resist getting in those good, solid digs if you can possibly avoid it. If your topic legitimately relates to attempts to ban evolution in school curricula, then go ahead and talk about it—but don’t blame it explicitly on the whole Republican Party; some of your readers may be Republicans, and they may feel that the problem is a few rogues, not the entire party. As with Wikipedia’s NPOV, it doesn’t matter whether (you think) the Republican Party really *is* at fault. It’s just better for the spiritual growth of the community to discuss the issue without invoking color politics.\n\n* * *\n\n[^1^](#fn1x14-bk)And no, I am not a Republican. Or a Democrat."
    },
    "voteCount": 153,
    "forceInclude": true
  },
  {
    "_id": "a7n8GdKiAZRX86T5A",
    "url": null,
    "title": "Making Beliefs Pay Rent (in Anticipated Experiences)",
    "slug": "making-beliefs-pay-rent-in-anticipated-experiences",
    "author": "Eliezer Yudkowsky",
    "question": false,
    "tags": [
      {
        "name": "Rationality"
      },
      {
        "name": "Anticipated Experiences"
      },
      {
        "name": "Epistemology"
      },
      {
        "name": "Empiricism"
      },
      {
        "name": "Principles"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "Thus begins the ancient parable:\n\n*If a tree falls in a forest and no one hears it, does it make a sound? One says, “Yes it does, for it makes vibrations in the air.” Another says, “No it does not, for there is no auditory processing in any brain.”*\n\nIf there’s a foundational skill in the martial art of rationality, a mental stance on which all other technique rests, it might be this one: the ability to spot, inside your own head, psychological signs that you have a mental map of something, and signs that you don’t.\n\nSuppose that, after a tree falls, the two arguers walk into the forest together. Will one expect to see the tree fallen to the right, and the other expect to see the tree fallen to the left? Suppose that before the tree falls, the two leave a sound recorder next to the tree. Would one, playing back the recorder, expect to hear something different from the other? Suppose they attach an electroencephalograph to any brain in the world; would one expect to see a different trace than the other?\n\nThough the two argue, one saying “No,” and the other saying “Yes,” they do not anticipate any different experiences. The two think they have different models of the world, but they have no difference with respect to what they expect will *happen to* them; their maps of the world do not diverge in any sensory detail.\n\nIt’s tempting to try to eliminate this mistake class by insisting that the only legitimate kind of belief is an anticipation of sensory experience. But the world does, in fact, contain much that is not sensed directly. We don’t see the atoms underlying the brick, but the atoms are in fact there. There is a floor beneath your feet, but you don’t *experience* the floor directly; you see the light *reflected* from the floor, or rather, you see what your retina and visual cortex have processed of that light. To infer the floor from seeing the floor is to step back into the unseen causes of experience. It may seem like a very short and direct step, but it is still a step.\n\nYou stand on top of a tall building, next to a grandfather clock with an hour, minute, and ticking second hand. In your hand is a bowling ball, and you drop it off the roof. On which tick of the clock will you hear the crash of the bowling ball hitting the ground?\n\nTo answer precisely, you must use beliefs like *Earth’s gravity is 9.8 meters per second per second,* and *This building is around 120 meters tall.* These beliefs are not wordless anticipations of a sensory experience; they are verbal-ish, propositional. It probably does not exaggerate much to describe these two beliefs as sentences made out of words. But these two beliefs have an inferential *consequence* that is a direct sensory anticipation—if the clock’s second hand is on the 12 numeral when you drop the ball, you anticipate seeing it on the 1 numeral when you hear the crash five seconds later. To anticipate sensory experiences as precisely as possible, we must process beliefs that are not anticipations of sensory experience.\n\nIt is a great strength of *Homo sapiens* that we can, better than any other species in the world, learn to model the unseen. It is also one of our great weak points. Humans often believe in things that are not only unseen but unreal.\n\nThe same brain that builds a network of inferred causes behind sensory experience can also build a network of causes that is not connected to sensory experience, or poorly connected. Alchemists believed that phlogiston caused fire—we could simplistically model their minds by drawing a little node labeled “Phlogiston,” and an arrow from this node to their sensory experience of a crackling campfire—but this belief yielded no advance predictions; the link from phlogiston to experience was always configured after the experience, rather than constraining the experience in advance.\n\nOr suppose your English professor teaches you that the famous writer Wulky Wilkinsen is actually a “retropositional author,” which you can tell because his books exhibit “alienated resublimation.” And perhaps your professor knows all this because their professor told them; but all they're able to say about resublimation is that it's characteristic of retropositional thought, and of retropositionality that it's marked by alienated resublimation. What does this mean you should expect from Wulky Wilkinsen’s books?\n\nNothing. The belief, if you can call it that, doesn’t connect to sensory experience at all. But you had better remember the propositional assertions that “Wulky Wilkinsen” has the “retropositionality” attribute and also the “alienated resublimation” attribute, so you can regurgitate them on the upcoming quiz. The two beliefs are connected to each other, though still not connected to any anticipated experience.\n\nWe can build up whole networks of beliefs that are connected only to each other—call these “floating” beliefs. It is a uniquely human flaw among animal species, a perversion of *Homo sapiens*’s ability to build more general and flexible belief networks.\n\nThe rationalist virtue of *empiricism* consists of constantly asking which experiences our beliefs predict—or better yet, prohibit. Do you believe that phlogiston is the cause of fire? Then what do you expect to see happen, because of that? Do you believe that Wulky Wilkinsen is a retropositional author? Then what do you expect to see because of that? No, not “alienated resublimation”; *what experience will happen to you?* Do you believe that if a tree falls in the forest, and no one hears it, it still makes a sound? Then what experience must therefore befall you?\n\nIt is even better to ask: what experience *must not* happen to you? Do you believe that *Élan vital* explains the mysterious aliveness of living beings? Then what does this belief *not* allow to happen—what would definitely falsify this belief? A null answer means that your belief does not *constrain* experience; it permits *anything* to happen to you. It floats.\n\nWhen you argue a seemingly factual question, always keep in mind which difference of anticipation you are arguing about. If you can’t find the difference of anticipation, you’re probably arguing about labels in your belief network—or even worse, floating beliefs, barnacles on your network. If you don’t know what experiences are implied by Wulky Wilkinsens writing being retropositional, you can go on arguing forever.\n\nAbove all, don’t ask what to believe—ask what to anticipate. Every question of belief should flow from a question of anticipation, and that question of anticipation should be the center of the inquiry. Every guess of belief should begin by flowing to a specific guess of anticipation, and should continue to pay rent in future anticipations. If a belief turns deadbeat, evict it."
    },
    "voteCount": 277,
    "forceInclude": true
  },
  {
    "_id": "6s3xABaXKPdFwA3FS",
    "url": null,
    "title": "What is Evidence?",
    "slug": "what-is-evidence",
    "author": "Eliezer Yudkowsky",
    "question": false,
    "tags": [
      {
        "name": "Epistemology"
      },
      {
        "name": "Causality"
      },
      {
        "name": "Anticipated Experiences"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "> The sentence “snow is white” is *true* if and only if snow is white.\n> \n> —Alfred Tarski\n\n> To say of what is, that it is, or of what is not, that it is not, is *true*.\n> \n> —Aristotle, *Metaphysics IV*\n\nWalking along the street, your shoelaces come untied. Shortly thereafter, for some odd reason, you start *believing* your shoelaces are untied. Light leaves the Sun and strikes your shoelaces and bounces off; some photons enter the pupils of your eyes and strike your retina; the energy of the photons triggers neural impulses; the neural impulses are transmitted to the visual-processing areas of the brain; and there the optical information is processed and reconstructed into a 3D model that is recognized as an untied shoelace. There is a sequence of events, a chain of cause and effect, within the world and your brain, by which you end up believing what you believe. The final outcome of the process is a state of *mind* which mirrors the state of your actual *shoelaces*.\n\nWhat is *evidence?* It is an event entangled, by links of cause and effect, with whatever you want to know about. If the target of your inquiry is your shoelaces, for example, then the light entering your pupils is evidence entangled with your shoelaces. This should not be confused with the technical sense of “entanglement” used in physics—here I’m just talking about “entanglement” in the sense of two things that end up in correlated states because of the links of cause and effect between them.\n\nNot every influence creates the kind of “entanglement” required for evidence. It’s no help to have a machine that beeps when you enter winning lottery numbers, if the machine *also* beeps when you enter *losing* lottery numbers. The light reflected from your shoes would not be useful evidence about your shoelaces, if the photons ended up in the same physical state whether your shoelaces were tied or untied.\n\nTo say it abstractly: For an event to be *evidence about* a target of inquiry, it has to happen *differently* in a way that’s entangled with the *different* possible states of the target. (To say it technically: There has to be Shannon mutual information between the evidential event and the target of inquiry, relative to your current state of uncertainty about both of them.)\n\nEntanglement can be contagious when processed correctly, which is why you need eyes and a brain. If photons reflect off your shoelaces and hit a rock, the rock won’t change much. The rock won’t reflect the shoelaces in any helpful way; it won’t be detectably different depending on whether your shoelaces were tied or untied. This is why rocks are not useful witnesses in court. A photographic film will contract shoelace-entanglement from the incoming photons, so that the photo can itself act as evidence. If your eyes and brain work correctly, *you* will become tangled up with your own shoelaces.\n\nThis is why rationalists put such a heavy premium on the paradoxical-seeming claim that a belief is only really worthwhile if you could, in principle, be persuaded to believe otherwise. If your retina ended up in the same state regardless of what light entered it, you would be blind. Some belief systems, in a rather obvious trick to reinforce themselves, say that certain beliefs are only really worthwhile if you believe them *unconditionally*—no matter what you see, no matter what you think. Your brain is supposed to end up in the same state regardless. Hence the phrase, “blind faith.” If what you believe doesn’t depend on what you see, you’ve been blinded as effectively as by poking out your eyeballs.\n\nIf your eyes and brain work correctly, your beliefs will end up entangled with the facts. *Rational thought produces beliefs which are themselves evidence.*\n\nIf your tongue speaks truly, your rational beliefs, which are themselves evidence, can act as evidence for someone else. Entanglement can be transmitted through chains of cause and effect—and if you speak, and another hears, that too is cause and effect. When you say “My shoelaces are untied” over a cellphone, you’re sharing your entanglement with your shoelaces with a friend.\n\nTherefore rational beliefs are contagious, among honest folk who believe each other to be honest. And it’s why a claim that your beliefs are *not* contagious—that you believe for private reasons which are not transmissible—is so suspicious. If your beliefs are entangled with reality, they *should* be contagious among honest folk.\n\nIf your model of reality suggests that the outputs of your thought processes should *not* be contagious to others, then your model says that your beliefs are not themselves evidence, meaning they are not entangled with reality. You should apply a reflective correction, and stop believing.\n\nIndeed, if you *feel*, on a *gut* level, what this all *means*, you will *automatically* stop believing. Because “my belief is not entangled with reality” *means* “my belief is not accurate.” As soon as you stop believing “ ‘snow is white’ is true,” you should (automatically!) stop believing “snow is white,” or something is very wrong.\n\nSo try to explain why the kind of thought processes you use systematically produce beliefs that mirror reality. Explain why you think you’re rational. Why you think that, using thought processes like the ones you use, minds will end up believing “snow is white” if and only if snow is white. If you *don’t* believe that the outputs of your thought processes are entangled with reality, why believe the outputs of your thought processes? It’s the same thing, or it should be."
    },
    "voteCount": 116,
    "forceInclude": true
  },
  {
    "_id": "fhojYBGGiYAFcryHZ",
    "url": null,
    "title": "Scientific Evidence, Legal Evidence, Rational Evidence",
    "slug": "scientific-evidence-legal-evidence-rational-evidence",
    "author": "Eliezer Yudkowsky",
    "question": false,
    "tags": [
      {
        "name": "Practice & Philosophy of Science"
      },
      {
        "name": "Rationality"
      },
      {
        "name": "Empiricism"
      },
      {
        "name": "Law and Legal systems"
      },
      {
        "name": "Distinctions"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "Suppose that your good friend, the police commissioner, tells you in strictest confidence that the crime kingpin of your city is Wulky Wilkinsen. As a rationalist, are you licensed to believe this statement? Put it this way: if you go ahead and insult Wulky, I’d call you foolhardy. Since it is prudent to act as if Wulky has a substantially higher-than-default probability of being a crime boss, the police commissioner’s statement must have been strong Bayesian evidence.\n\nOur legal system will not imprison Wulky on the basis of the police commissioner’s statement. It is not admissible as *legal evidence*. Maybe if you locked up every person accused of being a crime boss by a police commissioner, you’d *initially* catch a lot of crime bosses, and relatively few people the commissioner just didn’t like. But unrestrained power attracts corruption like honey attracts flies: over time, you’d catch fewer and fewer real crime bosses (who would go to greater lengths to ensure anonymity), and more and more innocent victims.\n\nThis does not mean that the police commissioner’s statement is not rational evidence. It still has a lopsided likelihood ratio, and you’d still be a fool to insult Wulky. But on a *social* level, in pursuit of a social goal, we deliberately define “legal evidence” to include only particular kinds of evidence, such as the police commissioner’s own observations on the night of April 4th. All legal evidence should ideally be rational evidence, but not the other way around. We impose special, strong, additional standards before we anoint rational evidence as “legal evidence.”\n\nAs I write this sentence at 8:33 p.m., Pacific time, on August 18th, 2007, I am wearing white socks. As a rationalist, are you licensed to believe the previous statement? Yes. Could I testify to it in court? Yes. Is it a *scientific* statement? No, because there is no experiment you can perform yourself to verify it. Science is made up of *generalizations* which apply to many particular instances, so that you can run new real-world experiments which test the generalization, and thereby verify for yourself that the generalization is true, without having to trust anyone’s authority. Science is the *publicly reproducible* knowledge of humankind.\n\nLike a court system, science as a social process is made up of fallible humans. We want a protected pool of beliefs that are *especially* reliable. And we want social rules that encourage the generation of such knowledge. So we impose special, strong, additional standards before we canonize rational knowledge as “scientific knowledge,” adding it to the protected belief pool. Is a rationalist licensed to believe in the historical existence of Alexander the Great? Yes. We have a rough picture of ancient Greece, untrustworthy but better than maximum entropy. But we are dependent on authorities such as Plutarch; we cannot discard Plutarch and verify everything for ourselves. Historical knowledge is not scientific knowledge.\n\nIs a rationalist licensed to believe that the Sun will rise on September 18th, 2007? Yes—not with absolute certainty, but that’s the way to bet.^1^ Is this statement, as I write this essay on August 18th, 2007, a *scientific* belief?\n\nIt may seem perverse to deny the adjective “scientific” to statements like “The Sun will rise on September 18th, 2007.” If Science could not make predictions about future events—events which have *not yet* happened—then it would be useless; it could make no prediction in advance of experiment. The prediction that the Sun will rise is, definitely, an *extrapolation* from scientific generalizations. It is based upon models of the Solar System that you could test for yourself by experiment.\n\nBut imagine that you’re constructing an experiment to verify prediction #27, in a new context, of an accepted theory Q. You may not have any concrete reason to suspect the belief is wrong; you just want to test it in a new context. It seems dangerous to say, *before* running the experiment, that there is a “scientific belief” about the result. There is a “conventional prediction” or “theory Q’s prediction.” But if you already know the “scientific belief” about the result, why bother to run the experiment?\n\nYou begin to see, I hope, why I identify Science with *generalizations*, rather than the history of any one experiment. A historical event happens once; generalizations apply over many events. History is not reproducible; scientific generalizations are.\n\nIs my definition of “scientific knowledge” *true*? That is not a well-formed question. The special standards we impose upon science are pragmatic choices. Nowhere upon the stars or the mountains is it written that p < 0.05 shall be the standard for scientific publication. Many now argue that 0.05 is too weak, and that it would be *useful* to lower it to 0.01 or 0.001.\n\nPerhaps future generations, acting on the theory that science is the *public*, *reproducible* knowledge of humankind, will only label as “scientific” papers published in an open-access journal. If you charge for access to the knowledge, is it part of the knowledge of *humankind*? Can we fully trust a result if people must pay to criticize it?\n\nFor myself, I think scientific practice would be better served by the dictum that only open, public knowledge counts. But however we choose to define “science,” information in a $20,000/year closed-access journal will still count as Bayesian evidence; and so too, the police commissioner’s private assurance that Wulky is the kingpin.\n\n* * *\n\n^1^ Pedants: interpret this as the Earth’s rotation and orbit remaining roughly constant relative to the Sun."
    },
    "voteCount": 85,
    "forceInclude": true
  },
  {
    "_id": "nj8JKFoLSMEmD3RGp",
    "url": null,
    "title": "How Much Evidence Does It Take?",
    "slug": "how-much-evidence-does-it-take",
    "author": "Eliezer_Yudkowsky",
    "question": false,
    "tags": [
      {
        "name": "Bayes' Theorem"
      },
      {
        "name": "Epistemology"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "Previously, I defined _evidence_ as “an event entangled, by links of cause and effect, with whatever you want to know about,” and _entangled_ as “happening differently for different possible states of the target.” So how much entanglement—how much rational evidence—is required to support a belief?\n\nLet’s start with a question simple enough to be mathematical: How hard would you have to entangle yourself with the lottery in order to win? Suppose there are seventy balls, drawn without replacement, and six numbers to match for the win. Then there are 131,115,985 possible winning combinations, hence a randomly selected ticket would have a 1/131,115,985 probability of winning (0.0000007%). To win the lottery, you would need evidence _selective_ enough to visibly favor one combination over 131,115,984 alternatives.\n\nSuppose there are some tests you can perform which discriminate, probabilistically, between winning and losing lottery numbers. For example, you can punch a combination into a little black box that always beeps if the combination is the winner, and has only a 1/4 (25%) chance of beeping if the combination is wrong. In Bayesian terms, we would say the _likelihood ratio_ is 4 to 1. This means that the box is 4 times as likely to beep when we punch in a correct combination, compared to how likely it is to beep for an incorrect combination.\n\nThere are still a whole lot of possible combinations. If you punch in 20 incorrect combinations, the box will beep on 5 of them by sheer chance (on average). If you punch in all 131,115,985 possible combinations, then while the box is certain to beep for the one winning combination, it will also beep for 32,778,996 losing combinations (on average).\n\nSo this box doesn’t let you win the lottery, but it’s better than nothing. If you used the box, your odds of winning would go from 1 in 131,115,985 to 1 in 32,778,997. You’ve made some progress toward finding your target, the truth, within the huge space of possibilities.\n\nSuppose you can use another black box to test combinations _twice_, _independently._ Both boxes are certain to beep for the winning ticket. But the chance of a box beeping for a losing combination is 1/4 _independently_ for each box; hence the chance of _both_ boxes beeping for a losing combination is 1/16. We can say that the _cumulative_ evidence, of two independent tests, has a likelihood ratio of 16:1. The number of losing lottery tickets that pass both tests will be (on average) 8,194,749.\n\nSince there are 131,115,985 possible lottery tickets, you might guess that you need evidence whose strength is around 131,115,985 to 1—an event, or series of events, which is 131,115,985 times more likely to happen for a winning combination than a losing combination. Actually, this amount of evidence would only be enough to give you an _even_ chance of winning the lottery. Why? Because if you apply a filter of that power to 131 million losing tickets, there will be, on average, one losing ticket that passes the filter. The winning ticket will also pass the filter. So you’ll be left with two tickets that passed the filter, only one of them a winner. Fifty percent odds of winning, if you can only buy one ticket.\n\nA better way of viewing the problem: In the beginning, there is 1 winning ticket and 131,115,984 losing tickets, so your odds of winning are 1:131,115,984. If you use a single box, the odds of it beeping are 1 for a winning ticket and 0.25 for a losing ticket. So we multiply 1:131,115,984 by 1:0.25 and get 1:32,778,996. Adding another box of evidence multiplies the odds by 1:0.25 again, so now the odds are 1 winning ticket to 8,194,749 losing tickets.\n\nIt is convenient to measure evidence in bits—not like bits on a hard drive, but mathematician’s bits, which are conceptually different. Mathematician’s bits are the logarithms, base 1/2, of probabilities. For example, if there are four possible outcomes A, B, C, and D, whose probabilities are 50%, 25%, 12.5%, and 12.5%, and I tell you the outcome was “D,” then I have transmitted three bits of information to you, because I informed you of an outcome whose probability was 1/8.\n\nIt so happens that 131,115,984 is slightly less than 2 to the 27th power. So 14 boxes or 28 bits of evidence—an event 268,435,456:1 times more likely to happen if the ticket-hypothesis is true than if it is false—would shift the odds from 1:131,115,984 to 268,435,456:131,115,984, which reduces to 2:1. Odds of 2 to 1 mean two chances to win for each chance to lose, so the _probability_ of winning with 28 bits of evidence is 2/3. Adding another box, another 2 bits of evidence, would take the odds to 8:1. Adding yet another two boxes would take the chance of winning to 128:1.\n\nSo if you want to license a _strong belief_ that you will win the lottery—arbitrarily defined as less than a 1% probability of being wrong—34 bits of evidence about the winning combination should do the trick.\n\nIn general, the rules for weighing “how much evidence it takes” follow a similar pattern: The larger the _space of possibilities_ in which the hypothesis lies, or the more unlikely the hypothesis seems a priori compared to its neighbors, or the more confident you wish to be, the more evidence you need.\n\nYou cannot defy the rules; you cannot form accurate beliefs based on inadequate evidence. Let’s say you’ve got 10 boxes lined up in a row, and you start punching combinations into the boxes. You cannot stop on the first combination that gets beeps from all 10 boxes, saying, “But the odds of that happening for a losing combination are a million to one! I’ll just ignore those ivory-tower Bayesian rules and stop here.” On average, 131 losing tickets will pass such a test for every winner. Considering the space of possibilities and the prior improbability, you jumped to a too-strong conclusion based on insufficient evidence. That’s not a pointless bureaucratic regulation; it’s math.\n\nOf course, you can still believe based on inadequate evidence, if that is your whim; but you will not be able to believe _accurately_. It is like trying to drive your car without any fuel, because you don’t believe in the fuddy-duddy concept that it ought to take fuel to go places. Wouldn’t it be so much more _fun_, and so much less expensive, if we just decided to repeal the law that cars need fuel?\n\nWell, you can try. You can even shut your eyes and pretend the car is moving. But _really_ arriving at accurate beliefs requires evidence-fuel, and the further you want to go, the more fuel you need."
    },
    "voteCount": 87,
    "forceInclude": true
  },
  {
    "_id": "mnS2WYLCGJP2kQkRn",
    "url": null,
    "title": "Absence of Evidence Is Evidence of Absence",
    "slug": "absence-of-evidence-is-evidence-of-absence",
    "author": "Eliezer Yudkowsky",
    "question": false,
    "tags": [
      {
        "name": "Conservation of Expected Evidence"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "From Robyn Dawes’s *Rational Choice in an Uncertain World*:\n\n> In fact, this post-hoc fitting of evidence to hypothesis was involved in a most grievous chapter in United States history: the internment of Japanese-Americans at the beginning of the Second World War. When California governor Earl Warren testified before a congressional hearing in San Francisco on February 21, 1942, a questioner pointed out that there had been no sabotage or any other type of espionage by the Japanese-Americans up to that time. Warren responded, “I take the view that this lack \\[of subversive activity\\] is the most ominous sign in our whole situation. It convinces me more than perhaps any other factor that the sabotage we are to get, the Fifth Column activities are to get, are timed just like Pearl Harbor was timed . . . I believe we are just being lulled into a false sense of security.”\n\nConsider Warren’s argument from a Bayesian perspective. When we see evidence, hypotheses that assigned a *higher* likelihood to that evidence gain probability, at the expense of hypotheses that assigned a *lower* likelihood to the evidence. This is a phenomenon of *relative* likelihoods and *relative* probabilities. You can assign a high likelihood to the evidence and still lose probability mass to some other hypothesis, if that other hypothesis assigns a likelihood that is even higher.\n\nWarren seems to be arguing that, given that we see no sabotage, this *confirms* that a Fifth Column exists. You could argue that a Fifth Column *might* delay its sabotage. But the likelihood is still higher that the *absence* of a Fifth Column would perform an absence of sabotage.\n\nLet E stand for the observation of sabotage, and ¬E for the observation of no sabotage. The symbol H1 stands for the hypothesis of a Japanese-American Fifth Column, and H2 for the hypothesis that no Fifth Column exists. The *conditional probability* P(E | H), or “E given H,” is how confidently we’d expect to see the evidence E if we assumed the hypothesis H were true.\n\nWhatever the likelihood that a Fifth Column would do no sabotage, the probability P(¬E | H1), it won’t be as large as the likelihood that there’s no sabotage *given that there’s no Fifth Column*, the probability P(¬E | H2). So observing a lack of sabotage increases the probability that no Fifth Column exists.\n\nA lack of sabotage doesn’t *prove* that no Fifth Column exists. Absence of *proof* is not *proof* of absence. In logic, (A ⇒ B), read “A implies B,” is not equivalent to (¬A ⇒ ¬B), read “not-A implies not-B .”\n\nBut in probability theory, absence of *evidence* is always *evidence* of absence. If E is a binary event and P(H | E) > P(H), i.e., seeing E increases the probability of H, then P(H | ¬ E) < P(H), i.e., failure to observe E decreases the probability of H . The probability P(H) is a weighted mix of P(H | E) and P(H | ¬ E), and necessarily lies between the two.^1^\n\nUnder the vast majority of real-life circumstances, a cause may not reliably produce signs of itself, but the absence of the cause is even less likely to produce the signs. The absence of an observation may be strong evidence of absence or very weak evidence of absence, depending on how likely the cause is to produce the observation. The absence of an observation that is only weakly permitted (even if the alternative hypothesis does not allow it at all) is very weak evidence of absence (though it is evidence nonetheless). This is the fallacy of “gaps in the fossil record”—fossils form only rarely; it is futile to trumpet the absence of a weakly permitted observation when many strong positive observations have already been recorded. But if there are *no* positive observations at all, it is time to worry; hence the Fermi Paradox.\n\nYour strength as a rationalist is your ability to be more confused by fiction than by reality; if you are equally good at explaining any outcome you have zero knowledge. The strength of a model is not what it *can* explain, but what it *can’t*, for only prohibitions constrain anticipation. If you don’t notice when your model makes the evidence unlikely, you might as well have no model, and also you might as well have no evidence; no brain and no eyes.\n\n* * *\n\n^1^ If any of this sounds at all confusing, see my discussion of Bayesian updating toward the end of *The Machine in the Ghost*, the third volume of [*Rationality: From AI to Zombies*](https://lesswrong.com/rationality)."
    },
    "voteCount": 97,
    "forceInclude": true
  },
  {
    "_id": "jiBFC7DcCrZjGmZnJ",
    "url": null,
    "title": "Conservation of Expected Evidence",
    "slug": "conservation-of-expected-evidence",
    "author": "Eliezer_Yudkowsky",
    "question": false,
    "tags": [
      {
        "name": "Conservation of Expected Evidence"
      },
      {
        "name": "Rationality"
      },
      {
        "name": "Bayes' Theorem"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "Friedrich Spee von Langenfeld, a priest who heard the confessions of condemned witches, wrote in 1631 the _Cautio Criminalis_ (“prudence in criminal cases”), in which he bitingly described the decision tree for condemning accused witches: If the witch had led an evil and improper life, she was guilty; if she had led a good and proper life, this too was a proof, for witches dissemble and try to appear especially virtuous. After the woman was put in prison: if she was afraid, this proved her guilt; if she was not afraid, this proved her guilt, for witches characteristically pretend innocence and wear a bold front. Or on hearing of a denunciation of witchcraft against her, she might seek flight or remain; if she ran, that proved her guilt; if she remained, the devil had detained her so she could not get away.\n\nSpee acted as confessor to many witches; he was thus in a position to observe _every_ branch of the accusation tree, that no matter _what_ the accused witch said or did, it was held as proof against her. In any individual case, you would only hear one branch of the dilemma. It is for this reason that scientists write down their experimental predictions in advance.\n\nBut _you can’t have it both ways_ —as a matter of probability theory, not mere fairness. The rule that “absence of evidence _is_ evidence of absence” is a special case of a more general law, which I would name Conservation of Expected Evidence: the _expectation_ of the posterior probability, after viewing the evidence, must equal the prior probability.\n\n_Therefore,_ for every expectation of evidence, there is an equal and opposite expectation of counterevidence.\n\nIf you expect a strong probability of seeing weak evidence in one direction, it must be balanced by a weak expectation of seeing strong evidence in the other direction. If you’re very confident in your theory, and therefore anticipate seeing an outcome that matches your hypothesis, this can only provide a very small increment to your belief (it is already close to 1); but the unexpected failure of your prediction would (and must) deal your confidence a huge blow. On _average_, you must expect to be _exactly_ as confident as when you started out. Equivalently, the mere _expectation_ of encountering evidence—before you’ve actually seen it—should not shift your prior beliefs.\n\nSo if you claim that “no sabotage” is evidence _for_ the existence of a Japanese-American Fifth Column, you must conversely hold that seeing sabotage would argue _against_ a Fifth Column. If you claim that “a good and proper life” is evidence that a woman is a witch, then an evil and improper life must be evidence that she is not a witch. If you argue that God, to test humanity’s faith, refuses to reveal His existence, then the miracles described in the Bible must argue against the existence of God.\n\nDoesn’t quite sound right, does it? Pay attention to that feeling of _this seems a little forced_, that quiet strain in the back of your mind. It’s important.\n\nFor a true Bayesian, it is impossible to seek evidence that _confirms_ a theory. There is no possible plan you can devise, no clever strategy, no cunning device, by which you can legitimately expect your confidence in a fixed proposition to be higher (on _average_) than before. You can only ever seek evidence to _test_ a theory, not to confirm it.\n\nThis realization can take quite a load off your mind. You need not worry about how to interpret every possible experimental result to confirm your theory. You needn’t bother planning how to make _any_ given iota of evidence confirm your theory, because you know that for every expectation of evidence, there is an equal and oppositive expectation of counterevidence. If you try to weaken the counterevidence of a possible “abnormal” observation, you can only do it by weakening the support of a “normal” observation, to a precisely equal and opposite degree. It is a zero-sum game. No matter how you connive, no matter how you argue, no matter how you strategize, you can’t possibly expect the resulting game plan to shift your beliefs (on average) in a particular direction.\n\nYou might as well sit back and relax while you wait for the evidence to come in.\n\n. . . Human psychology is _so_ screwed up."
    },
    "voteCount": 147,
    "forceInclude": true
  },
  {
    "_id": "5yFRd3cjLpm3Nd6Di",
    "url": null,
    "title": "Argument Screens Off Authority",
    "slug": "argument-screens-off-authority",
    "author": "Eliezer Yudkowsky",
    "question": false,
    "tags": [
      {
        "name": "Epistemology"
      },
      {
        "name": "Social Status"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "Scenario 1: Barry is a famous geologist. Charles is a fourteen-year-old juvenile delinquent with a long arrest record and occasional psychotic episodes. Barry flatly asserts to Arthur some counterintuitive statement about rocks, and Arthur judges it 90% probable. Then Charles makes an equally counterintuitive flat assertion about rocks, and Arthur judges it 10% probable. Clearly, Arthur is taking the speaker’s *authority* into account in deciding whether to believe the speaker’s assertions.\n\nScenario 2: David makes a counterintuitive statement about physics and gives Arthur a detailed explanation of the arguments, including references. Ernie makes an equally counterintuitive statement, but gives an unconvincing argument involving several leaps of faith. Both David and Ernie assert that this is the best explanation they can possibly give (to anyone, not just Arthur). Arthur assigns 90% probability to David’s statement after hearing his explanation, but assigns a 10% probability to Ernie’s statement.\n\nIt might seem like these two scenarios are roughly symmetrical: both involve taking into account useful evidence, whether strong versus weak authority, or strong versus weak argument.\n\nBut now suppose that Arthur asks Barry and Charles to make full technical cases, with references; and that Barry and Charles present equally good cases, and Arthur looks up the references and they check out. Then Arthur asks David and Ernie for their credentials, and it turns out that David and Ernie have roughly the same credentials—maybe they’re both clowns, maybe they’re both physicists.\n\nAssuming that Arthur is knowledgeable enough to understand all the technical arguments—otherwise they’re just impressive noises—it seems that Arthur should view David as having a great advantage in plausibility over Ernie, while Barry has at best a minor advantage over Charles.\n\nIndeed, if the technical arguments are good enough, Barry’s advantage over Charles may not be worth tracking. A good technical argument is one that *eliminates* reliance on the personal authority of the speaker.\n\nSimilarly, if we really believe Ernie that the argument he gave is the best argument he *could* give, which includes all of the inferential steps that Ernie executed, and all of the support that Ernie took into account—citing any authorities that Ernie may have listened to himself—then we can pretty much ignore any information about Ernie’s credentials. Ernie can be a physicist or a clown, it shouldn’t matter. (Again, this assumes we have enough technical ability to process the argument. Otherwise, Ernie is simply uttering mystical syllables, and whether we “believe” these syllables depends a great deal on his authority.)\n\nSo it seems there’s an asymmetry between argument and authority. If we know authority we are still interested in hearing the arguments; but if we know the arguments fully, we have very little left to learn from authority.\n\nClearly (says the novice) authority and argument are fundamentally different kinds of evidence, a difference unaccountable in the boringly clean methods of Bayesian probability theory.[^1^](#fn1x20) For while the strength of the evidences—90% versus 10%—is just the same in both cases, they do not behave similarly when combined. How will we account for this?\n\nHere’s half a technical demonstration of how to represent this difference in probability theory. (The rest you can take on my personal authority, or look up in the references.)\n\nIf P(H|E1) = 90% and P(H|E2) = 9%, what is the probability P(H|E1,E2)? If learning E1 is true leads us to assign 90% probability to H, and learning E2 is true leads us to assign 9% probability to H, then what probability should we assign to H if we learn both E1 and E2? This is simply not something you can calculate in probability theory from the information given. No, the missing information is not the prior probability of H. The events E1 and E2 may not be independent of each other.\n\nSuppose that H is “My sidewalk is slippery,” E1 is “My sprinkler is running,” and E2 is “It’s night.” The sidewalk is slippery starting from one minute after the sprinkler starts, until just after the sprinkler finishes, and the sprinkler runs for ten minutes. So if we know the sprinkler is on, the probability is 90% that the sidewalk is slippery. The sprinkler is on during 10% of the nighttime, so if we know that it’s night, the probability of the sidewalk being slippery is 9%. If we know that it’s night and the sprinkler is on—that is, if we know both facts—the probability of the sidewalk being slippery is 90%.\n\nWe can represent this in a graphical model as follows:\n\n![](https://res.cloudinary.com/lesswrong-2-0/image/upload/v1561065393/sequences/Argument%20Screens%20Off%20Authority/ArgumentScreensOffAuthority_diagram_1.svg)\n\nWhether or not it’s Night *causes* the Sprinkler to be on or off, and whether the Sprinkler is on *causes* the sidewalk to be Slippery or unSlippery.\n\nThe direction of the arrows is meaningful. Say we had:\n\n![](https://res.cloudinary.com/lesswrong-2-0/image/upload/v1561065393/sequences/Argument%20Screens%20Off%20Authority/ArgumentScreensOffAuthority_diagram_2.svg)\n\nThis would mean that, if I *didn’t* know anything about the sprinkler, the probability of Nighttime and Slipperiness would be independent of each other. For example, suppose that I roll Die One and Die Two, and add up the showing numbers to get the Sum:\n\n![](https://res.cloudinary.com/lesswrong-2-0/image/upload/v1561065393/sequences/Argument%20Screens%20Off%20Authority/ArgumentScreensOffAuthority_diagram_3.svg)\n\nIf you don’t tell me the sum of the two numbers, and you tell me the first die showed 6, this doesn’t tell me anything about the result of the second die, yet. But if you now also tell me the sum is 7, I know the second die showed 1.\n\nFiguring out when various pieces of information are dependent or independent of each other, given various background knowledge, actually turns into a quite technical topic. The books to read are Judea Pearl’s *Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference* and *Causality: Models, Reasoning, and Inference*. (If you only have time to read one book, read the first one.)\n\nIf you know how to read causal graphs, then you look at the dice-roll graph and immediately see:\n\n*P(Die 1,Die 2) = P(Die 1) ✕ P(Die 2)*\n\n*P(Die 1,Die 2|Sum) ≠ P(Die 1)|Sum) ✕ P(Die 2|Sum) .*\n\nIf you look at the correct sidewalk diagram, you see facts like:\n\n*P(Slippery|Night) ≠ P(Slippery)*\n\n*P(Slippery|Sprinkler) ≠ P(Slippery)*\n\n*P(Slippery|Night,Sprinkler) = P(Slippery|Sprinkler) .*\n\nThat is, the probability of the sidewalk being Slippery, given knowledge about the Sprinkler and the Night, is the same probability we would assign if we knew only about the Sprinkler. Knowledge of the Sprinkler has made knowledge of the Night irrelevant to inferences about Slipperiness.\n\nThis is known as *screening off*, and the criterion that lets us read such conditional independences off causal graphs is known as *D-separation*.\n\nFor the case of argument and authority, the causal diagram looks like this:\n\n![](https://res.cloudinary.com/lesswrong-2-0/image/upload/v1561065393/sequences/Argument%20Screens%20Off%20Authority/ArgumentScreensOffAuthority_diagram_4.svg)\n\nIf something is true, then it therefore tends to have arguments in favor of it, and the experts therefore observe these evidences and change their opinions. (In theory!)\n\nIf we see that an expert believes something, we infer back to the existence of evidence-in-the-abstract (even though we don’t know what that evidence is exactly), and from the existence of this abstract evidence, we infer back to the truth of the proposition.\n\nBut if we know the value of the Argument node, this D-separates the node “Truth” from the node “Expert Belief” by blocking all paths between them, according to certain technical criteria for “path blocking” that seem pretty obvious in this case. So even without checking the exact probability distribution, we can read off from the graph that:\n\n*P(truth|argument,expert) = P(truth|argument) .*\n\nThis does not represent a contradiction of ordinary probability theory. It’s just a more compact way of expressing certain probabilistic facts. You could read the same equalities and inequalities off an unadorned probability distribution—but it would be harder to see it by eyeballing. Authority and argument don’t need two different kinds of probability, any more than sprinklers are made out of ontologically different stuff than sunlight.\n\nIn practice you can never *completely* eliminate reliance on authority. Good authorities are more likely to know about any counterevidence that exists and should be taken into account; a lesser authority is less likely to know this, which makes their arguments less reliable. This is not a factor you can eliminate merely by hearing the evidence they *did* take into account.\n\nIt’s also very hard to reduce arguments to *pure* math; and otherwise, judging the strength of an inferential step may rely on intuitions you can’t duplicate without the same thirty years of experience.\n\nThere is an ineradicable legitimacy to assigning *slightly* higher probability to what E. T. Jaynes tells you about Bayesian probability, than you assign to Eliezer Yudkowsky making the exact same statement. Fifty additional years of experience should not count for literally *zero* influence.\n\nBut this slight strength of authority is only *ceteris paribus*, and can easily be overwhelmed by stronger arguments. I have a minor erratum in one of Jaynes’s books—because algebra trumps authority.\n\n* * *\n\n[^1^](#fn1x20-bk)See “[What Is Evidence?](https://www.lesswrong.com/rationality/what-is-evidence)” in *Map and Territory*."
    },
    "voteCount": 68,
    "forceInclude": true
  },
  {
    "_id": "XTXWPQSEgoMkAupKt",
    "url": null,
    "title": "An Intuitive Explanation of Bayes's Theorem",
    "slug": "an-intuitive-explanation-of-bayes-s-theorem",
    "author": "Eliezer Yudkowsky",
    "question": false,
    "tags": [
      {
        "name": "Bayes' Theorem"
      },
      {
        "name": "Rationality"
      },
      {
        "name": "Needs Fixup"
      },
      {
        "name": "Probability & Statistics"
      }
    ],
    "tableOfContents": {
      "sections": [
        {
          "title": "Q. Why did the Bayesian reasoner cross the road?\n",
          "anchor": "Q__Why_did_the_Bayesian_reasoner_cross_the_road__",
          "level": 1
        },
        {
          "title": "Q. How can I find the priors for a problem?\n",
          "anchor": "Q__How_can_I_find_the_priors_for_a_problem__",
          "level": 1
        },
        {
          "title": "originally",
          "anchor": "originally",
          "level": 1
        },
        {
          "title": "Q. Uh huh. Then where do scientists get their priors?\n",
          "anchor": "Q__Uh_huh__Then_where_do_scientists_get_their_priors__",
          "level": 1
        },
        {
          "title": "Q. I see. And where does everyone else get their priors?\n",
          "anchor": "Q__I_see__And_where_does_everyone_else_get_their_priors__",
          "level": 1
        },
        {
          "title": "Q. What if the priors I want aren’t available on Kazaa?\n",
          "anchor": "Q__What_if_the_priors_I_want_aren_t_available_on_Kazaa__",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        },
        {
          "anchor": "comments",
          "level": 0,
          "title": "8 comments"
        }
      ],
      "headingsCount": 8
    },
    "contents": {
      "markdown": "(Note: The author now considers this explanation obsoleted by the [Bayes' Rule Guide.](https://arbital.com/p/bayes_rule_guide/))\n\n    ￼[Editor’s Note: This is an abridgement of the original version of this essay, which contained many interactive elements.]\n\n* * *\n\nYour friends and colleagues are talking about something called “Bayes’s Theorem” or “Bayes’s Rule,” or something called Bayesian reasoning. They sound really enthusiastic about it, too, so you google and find a web page about Bayes’s Theorem and . . .\n\nIt’s this equation. That’s all. Just one equation. The page you found gives a definition of it, but it doesn’t say what it is, or why it’s useful, or why your friends would be interested in it. It looks like this random statistics thing.\n\nWhy does a mathematical concept generate this strange enthusiasm in its students? What is the so-called Bayesian Revolution now sweeping through the sciences, which claims to subsume even the experimental method itself as a special case? What is the secret that the adherents of Bayes know? What is the light that they have seen?\n\nSoon you will know. Soon you will be one of us.\n\nWhile there are a few existing online explanations of Bayes’s Theorem, my experience with trying to introduce people to Bayesian reasoning is that the existing online explanations are too abstract. Bayesian reasoning is very _counterintuitive_. People do not employ Bayesian reasoning intuitively, find it very difficult to learn Bayesian reasoning when tutored, and rapidly forget Bayesian methods once the tutoring is over. This holds equally true for novice students and highly trained professionals in a field. Bayesian reasoning is apparently one of those things which, like quantum mechanics or the Wason Selection Test, is inherently difficult for humans to grasp with our built-in mental faculties.\n\nOr so they claim. Here you will find an attempt to offer an _intuitive_ explanation of Bayesian reasoning—an excruciatingly gentle introduction that invokes all the human ways of grasping numbers, from natural frequencies to spatial visualization. The intent is to convey, not abstract rules for manipulating numbers, but what the numbers mean, and why the rules are what they are (and cannot possibly be anything else). When you are finished reading this, you will see Bayesian problems in your dreams.\n\nAnd let’s begin.\n\n* * *\n\nHere’s a story problem about a situation that doctors often encounter:\n\n> 1% of women at age forty who participate in routine screening have breast cancer. 80% of women with breast cancer will get positive mammograms. 9.6% of women without breast cancer will also get positive mammograms. A woman in this age group had a positive mammogram in a routine screening. What is the probability that she actually has breast cancer?\n\nWhat do you think the answer is? If you haven’t encountered this kind of problem before, please take a moment to come up with your own answer before continuing.\n\n* * *\n\nNext, suppose I told you that most doctors get the same wrong answer on this problem—usually, only around 15% of doctors get it right. (“Really? 15%? Is that a real number, or an urban legend based on an Internet poll?” It’s a real number. See Casscells, Schoenberger, and Graboys 1978;\\[1\\] Eddy 1982;\\[2\\] Gigerenzer and Hoffrage 1995;\\[3\\] and many other studies. It’s a surprising result which is easy to replicate, so it’s been extensively replicated.)\n\nOn the story problem above, most doctors estimate the probability to be between 70% and 80%, which is wildly incorrect.\n\nHere’s an alternate version of the problem on which doctors fare somewhat better:\n\n> 10 out of 1,000 women at age forty who participate in routine screening have breast cancer. 800 out of 1,000 women with breast cancer will get positive mammograms. 96 out of 1,000 women without breast cancer will also get positive mammograms. If 1,000 women in this age group undergo a routine screening, about what fraction of women with positive mammograms will actually have breast cancer?\n\nAnd finally, here’s the problem on which doctors fare best of all, with 46%— nearly half—arriving at the correct answer:\n\n> 100 out of 10,000 women at age forty who participate in routine screening have breast cancer. 80 of every 100 women with breast cancer will get a positive mammogram. 950 out of 9,900 women without breast cancer will also get a positive mammogram. If 10,000 women in this age group undergo a routine screening, about what fraction of women with positive mammograms will actually have breast cancer?\n\n* * *\n\nThe correct answer is 7.8%, obtained as follows: Out of 10,000 women, 100 have breast cancer; 80 of those 100 have positive mammograms. From the same 10,000 women, 9,900 will not have breast cancer and of those 9,900 women, 950 will also get positive mammograms. This makes the total number of women with positive mammograms 950 + 80 or 1,030. Of those 1,030 women with positive mammograms, 80 will have cancer. Expressed as a proportion, this is 80/1,030 or 0.07767 or 7.8%.\n\nTo put it another way, before the mammography, the 10,000 women can be divided into two groups:\n\n*   Group 1: 100 women _with_ breast cancer.\n    \n*   Group 2: 9,900 women _without_ breast cancer.\n    \n\nSumming these two groups gives a total of 10,000 patients, confirming that none have been lost in the math. After the mammography, the women can be divided into four groups:\n\n*   Group A: 80 women _with_ breast cancer and a _positive_ mammogram.\n    \n*   Group B: 20 women _with_ breast cancer and a _negative_ mammogram.\n    \n*   Group C: 950 women _without_ breast cancer and a _positive_ mammogram.\n    \n*   Group D: 8,950 women _without_ breast cancer and a _negative_ mammogram.\n    \n\nThe sum of groups A and B, the groups with breast cancer, corresponds to group 1; and the sum of groups C and D, the groups without breast cancer, corresponds to group 2. If you administer a mammography to 10,000 patients, then out of the 1,030 with positive mammograms, eighty of those positive-mammogram patients will have cancer. This is the correct answer, the answer a doctor should give a positive-mammogram patient if she asks about the chance she has breast cancer; if thirteen patients ask this question, roughly one out of those thirteen will have cancer.\n\n* * *\n\nThe most common mistake is to ignore the original fraction of women with breast cancer, and the fraction of women without breast cancer who receive false positives, and focus only on the fraction of women with breast cancer who get positive results. For example, the vast majority of doctors in these studies seem to have thought that if around 80% of women with breast cancer have positive mammograms, then the probability of a women with a positive mammogram having breast cancer must be around 80%.\n\nFiguring out the final answer always requires _all three_ pieces of information—the percentage of women with breast cancer, the percentage of women without breast cancer who receive false positives, and the percentage of women with breast cancer who receive (correct) positives.\n\nThe original proportion of patients with breast cancer is known as the _prior probability_. The chance that a patient with breast cancer gets a positive mammogram, and the chance that a patient without breast cancer gets a positive mammogram, are known as the two _conditional probabilities_. Collectively, this initial information is known as _the priors_. The final answer—the estimated probability that a patient has breast cancer, given that we know she has a positive result on her mammogram—is known as the _revised probability_ or the _posterior probability_. What we’ve just seen is that the posterior probability depends in part on the prior probability.\n\nTo see that the final answer always depends on the original fraction of women with breast cancer, consider an alternate universe in which only one woman out of a million has breast cancer. Even if mammography in this world detects breast cancer in 8 out of 10 cases, while returning a false positive on a woman without breast cancer in only 1 out of 10 cases, there will still be a hundred thousand false positives for every real case of cancer detected. The original probability that a woman has cancer is so extremely low that, although a positive result on the mammogram does _increase_ the estimated probability, the probability isn’t increased to certainty or even “a noticeable chance”; the probability goes from 1:1,000,000 to 1:100,000.\n\nWhat this demonstrates is that the mammogram result doesn’t _replace_ your old information about the patient’s chance of having cancer; the mammogram _slides_ the estimated probability in the direction of the result. A positive result slides the original probability upward; a negative result slides the probability downward. For example, in the original problem where 1% of the women have cancer, 80% of women with cancer get positive mammograms, and 9.6% of women without cancer get positive mammograms, a positive result on the mammogram _slides_ the 1% chance upward to 7.8%.\n\nMost people encountering problems of this type for the first time carry out the mental operation of _replacing_ the original 1% probability with the 80% probability that a woman with cancer gets a positive mammogram. It may seem like a good idea, but it just doesn’t work. “The probability that a woman with a positive mammogram has breast cancer” is not at all the same thing as “the probability that a woman with breast cancer has a positive mammogram”; they are as unlike as apples and cheese.\n\n* * *\n\n**Q. Why did the Bayesian reasoner cross the road?**\n\nA. You need more information to answer this question.\n\n* * *\n\nSuppose that a barrel contains many small plastic eggs. Some eggs are painted red and some are painted blue. 40% of the eggs in the bin contain pearls, and 60% contain nothing. 30% of eggs containing pearls are painted blue, and 10% of eggs containing nothing are painted blue. What is the probability that a blue egg contains a pearl? For this example the arithmetic is simple enough that you may be able to do it in your head, and I would suggest trying to do so.\n\nA more compact way of specifying the problem:\n\nP (pearl) = 40%\n\nP (blue|pearl) = 30%\n\nP (blue|¬pearl) = 10%\n\nP (pearl|blue) = ?\n\nThe symbol “¬” is shorthand for “not,” so ¬pearl reads “not pearl.”\n\nThe notation P (blue|pearl) is shorthand for “the probability of blue given pearl” or “the probability that an egg is painted blue, given that the egg contains a pearl.” The item on the right side is what you _already know_ or the _premise_, and the item on the left side is the _implication_ or _conclusion_. If we have P (blue|pearl) = 30%, and we _already know_ that some egg contains a pearl, then we can _conclude_ there is a 30% chance that the egg is painted blue. Thus, the final fact we’re looking for—“the chance that a blue egg contains a pearl” or “the probability that an egg contains a pearl, if we know the egg is painted blue”—reads P (pearl|blue).\n\n40% of the eggs contain pearls, and 60% of the eggs contain nothing. 30% of the eggs containing pearls are painted blue, so 12% of the eggs altogether contain pearls and are painted blue. 10% of the eggs containing nothing are painted blue, so altogether 6% of the eggs contain nothing and are painted blue. A total of 18% of the eggs are painted blue, and a total of 12% of the eggs are painted blue and contain pearls, so the chance a blue egg contains a pearl is 12/18 or 2/3 or around 67%.\n\nAs before, we can see the necessity of all three pieces of information by considering extreme cases. In a (large) barrel in which only one egg out of a thousand contains a pearl, knowing that an egg is painted blue slides the probability from 0.1% to 0.3% (instead of sliding the probability from 40% to 67%). Similarly, if 999 out of 1,000 eggs contain pearls, knowing that an egg is blue slides the probability from 99.9% to 99.966%; the probability that the egg does not contain a pearl goes from 1/1,000 to around 1/3,000.\n\nOn the pearl-egg problem, most respondents unfamiliar with Bayesian reasoning would probably respond that the probability a blue egg contains a pearl is 30%, or perhaps 20% (the 30% chance of a true positive minus the 10% chance of a false positive). Even if this mental operation seems like a good idea at the time, it makes no sense in terms of the question asked. It’s like the experiment in which you ask a second-grader: “If eighteen people get on a bus, and then seven more people get on the bus, how old is the bus driver?” Many second-graders will respond: “Twenty-five.” They understand when they’re being prompted to carry out a particular mental procedure, but they haven’t quite connected the procedure to reality. Similarly, to find the probability that a woman with a positive mammogram has breast cancer, it makes no sense whatsoever to _replace_ the original probability that the woman has cancer with the probability that a woman with breast cancer gets a positive mammogram. Neither can you subtract the probability of a false positive from the probability of the true positive. These operations are as wildly irrelevant as adding the number of people on the bus to find the age of the bus driver.\n\n* * *\n\nA study by Gigerenzer and Hoffrage in 1995 showed that some ways of phrasing story problems are much more evocative of correct Bayesian reasoning.\\[4\\] The _least_ evocative phrasing used probabilities. A slightly more evocative phrasing used frequencies instead of probabilities; the problem remained the same, but instead of saying that 1% of women had breast cancer, one would say that 1 out of 100 women had breast cancer, that 80 out of 100 women with breast cancer would get a positive mammogram, and so on. Why did a higher proportion of subjects display Bayesian reasoning on this problem? Probably because saying “1 out of 100 women” encourages you to concretely visualize X women with cancer, leading you to visualize X women with cancer and a positive mammogram, etc.\n\nThe most effective presentation found so far is what’s known as _natural frequencies_—saying that 40 out of 100 eggs contain pearls, 12 out of 40 eggs containing pearls are painted blue, and 6 out of 60 eggs containing nothing are painted blue. A _natural frequencies_ presentation is one in which the information about the prior probability is included in presenting the conditional probabilities. If you were just learning about the eggs’ conditional probabilities through natural experimentation, you would—in the course of cracking open a hundred eggs—crack open around 40 eggs containing pearls, of which 12 eggs would be painted blue, while cracking open 60 eggs containing nothing, of which about 6 would be painted blue. In the course of learning the conditional probabilities, you’d see examples of blue eggs containing pearls about twice as often as you saw examples of blue eggs containing nothing.\n\nUnfortunately, while natural frequencies are a step in the right direction, it probably won’t be enough. When problems are presented in natural frequencies, the proportion of people using Bayesian reasoning rises to around half. A big improvement, but not big enough when you’re talking about real doctors and real patients.\n\n* * *\n\n> **Q. How can I find the priors for a problem?**\n> \n> A. Many commonly used priors are listed in the _Handbook of Chemistry and Physics_.\n> \n> **Q. Where do priors** _**originally**_ **come from?**\n> \n> A. Never ask that question.\n> \n> **Q. Uh huh. Then where do scientists get their priors?**\n> \n> A. Priors for scientific problems are established by annual vote of the AAAS. In recent years the vote has become fractious and controversial, with widespread acrimony, factional polarization, and several outright assassinations. This may be a front for infighting within the Bayes Council, or it may be that the disputants have too much spare time. No one is really sure.\n> \n> **Q. I see. And where does everyone else get their priors?**\n> \n> A. They download their priors from Kazaa.\n> \n> **Q. What if the priors I want aren’t available on Kazaa?**\n> \n> A. There’s a small, cluttered antique shop in a back alley of San Francisco’s Chinatown. _Don’t ask about the bronze rat._\n\nActually, priors are true or false just like the final answer—they reflect reality and can be judged by comparing them against reality. For example, if you think that 920 out of 10,000 women in a sample have breast cancer, and the actual number is 100 out of 10,000, then your priors are wrong. For our particular problem, the priors might have been established by three studies—a study on the case histories of women with breast cancer to see how many of them get a positive mammogram, a study on women without breast cancer to see how many of them get a positive mammogram, and an epidemiological study on the prevalence of breast cancer in some specific demographic.\n\n* * *\n\nThe probability P (A, B) is the same as P (B, A), but P (A|B) is not the same thing as P (B|A), and P (A, B) is completely different from P (A|B). It’s a common confusion to mix up some or all of these quantities.\n\nTo get acquainted with all the relationships between them, we’ll play “follow the degrees of freedom.” For example, the two quantities P (cancer) and P (¬cancer) have one degree of freedom between them, because of the general law P (A) + P (¬A) = 1. If you know that P (¬cancer) = 0.99, you can obtain P (cancer) = 1 − P (¬cancer) = 0.01.\n\nThe quantities P (positive|cancer) and P (¬positive|cancer) also have only one degree of freedom between them; either a woman with breast cancer gets a positive mammogram or she doesn’t. On the other hand, P (positive|cancer) and P (positive|¬cancer) have two degrees of freedom. You can have a mammography that returns positive for 80% of cancer patients and 9.6% of healthy patients, or that returns positive for 70% of cancer patients and 2% of healthy patients, or even a health test that returns “positive” for 30% of cancer patients and 92% of healthy patients. The two quantities, the output of the mammography for cancer patients and the output of the mammography for healthy patients, are in mathematical terms independent; one cannot be obtained from the other in any way, and so they have two degrees of freedom between them.\n\nWhat about P(positive,cancer), P(positive|cancer), and P(cancer)? Here we have three quantities; how many degrees of freedom are there? In this case the equation that must hold is\n\nP (positive, cancer) = P (positive|cancer) × P (cancer) .\n\nThis equality reduces the degrees of freedom by one. If we know the fraction of patients with cancer, and the chance that a cancer patient has a positive mammogram, we can deduce the fraction of patients who have breast cancer _and_ a positive mammogram by multiplying.\n\nSimilarly, if we know the number of patients with breast cancer and positive mammograms, and also the number of patients with breast cancer, we can estimate the chance that a woman with breast cancer gets a positive mammogram by dividing: P (positive|cancer) = P (positive, cancer)/P (cancer). In fact, this is exactly how such medical diagnostic tests are calibrated; you do a study on 8,520 women with breast cancer and see that there are 6,816 (or thereabouts) women with breast cancer and positive mammograms, then divide 6,816 by 8,520 to find that 80% of women with breast cancer had positive mammograms. (Incidentally, if you accidentally divide 8,520 by 6,816 instead of the other way around, your calculations will start doing strange things, such as insisting that 125% of women with breast cancer and positive mammograms have breast cancer. This is a common mistake in carrying out Bayesian arithmetic, in my experience.) And finally, if you know P (positive, cancer) and P (positive|cancer), you can deduce how many cancer patients there must have been originally. There are two degrees of freedom shared out among the three quantities; if we know any two, we can deduce the third.\n\nHow about P (positive), P (positive, cancer), and P (positive, ¬cancer)? Again there are only two degrees of freedom among these three variables. The equation occupying the extra degree of freedom is\n\nP (positive) = P (positive, cancer) + P (positive, ¬cancer) .\n\nThis is how P (positive) is computed to begin with; we figure out the number of women with breast cancer who have positive mammograms, and the number of women without breast cancer who have positive mammograms, then add them together to get the total number of women with positive mammograms. It would be very strange to go out and conduct a study to determine the number of women with positive mammograms— just that one number and nothing else—but in theory you could do so. And if you then conducted another study and found the number of those women who had positive mammograms _and_ breast cancer, you would also know the number of women with positive mammograms and _no_ breast cancer—either a woman with a positive mammogram has breast cancer or she doesn’t. In general, P (A, B) + P (A, ¬B) = P (A). Symmetrically, P (A, B) + P (¬A, B) = P (B).\n\nWhat about P (positive, cancer), P (positive, ¬cancer), P (¬positive, cancer), and P (¬positive, ¬cancer)? You might at first be tempted to think that there are only two degrees of freedom for these four quantities—that you can, for example, get P (positive, ¬cancer) by multiplying P (positive) × P(¬cancer), and thus that all four quantities can be found given only the two quantities P(positive) and P(cancer). This is not the case! P (positive, ¬cancer) = P (positive) × P (¬cancer) only if the two probabilities are _statistically independent_—if the chance that a woman has breast cancer has no bearing on whether she has a positive mammogram. This amounts to requiring that the two conditional probabilities be equal to each other—a requirement which would eliminate one degree of freedom. If you remember that these four quantities are the groups A, B, C, and D, you can look over those four groups and realize that, in theory, you can put any number of people into the four groups. If you start with a group of 80 women with breast cancer and positive mammograms, there’s no reason why you can’t add another group of 500 women with breast cancer and negative mammograms, followed by a group of 3 women without breast cancer and negative mammograms, and so on. So now it seems like the four quantities have four degrees of freedom. And they would, except that in expressing them as _probabilities_, we need to normalize them to _fractions_ of the complete group, which adds the constraint that P (positive, cancer) + P (positive, ¬cancer) + P (¬positive, cancer) + P (¬positive, ¬cancer) = 1. This equation takes up one degree of freedom, leaving three degrees of freedom among the four quantities. If you specify the _fractions_ of women in groups A, B, and D, you can deduce the fraction of women in group C.\n\nGiven the four groups A, B, C, and D, it is very straightforward to compute everything else:\n\nP(cancer) = (A + B) / (A + B + C + D)\n\nP (¬positive|cancer) = B / (A + B)\n\nand so on. Since {A, B, C, D} contains three degrees of freedom, it follows that the entire set of probabilities relating cancer rates to test results contains only three degrees of freedom. Remember that in our problems we always needed _three_ pieces of information—the prior probability and the two conditional probabilities—which, indeed, have three degrees of freedom among them. Actually, for Bayesian problems, _any_ three quantities with three degrees of freedom between them should logically specify the entire problem.\n\n* * *\n\n_The probability that a test gives a true positive_ divided by _the probability that a test gives a false positive_ is known as the _likelihood ratio_ of that test. The likelihood ratio for a positive result summarizes how much a positive result will slide the prior probability. Does the likelihood ratio of a medical test then sum up everything there is to know about the usefulness of the test?\n\nNo, it does not! The likelihood ratio sums up everything there is to know about the _meaning_ of a _positive_ result on the medical test, but the meaning of a _negative_ result on the test is not specified, nor is the frequency with which the test is useful. For example, a mammography with a hit rate of 80% for patients with breast cancer and a false positive rate of 9.6% for healthy patients has the same likelihood ratio as a test with an 8% hit rate and a false positive rate of 0.96%. Although these two tests have the same likelihood ratio, the first test is more useful in every way—it detects disease more often, and a negative result is stronger evidence of health.\n\n* * *\n\nSuppose that you apply _two_ tests for breast cancer in succession—say, a standard mammogram and also some other test which is _independent_ of mammography. Since I don’t know of any such test that is independent of mammography, I’ll invent one for the purpose of this problem, and call it the Tams-Braylor Division Test, which checks to see if any cells are dividing more rapidly than other cells. We’ll suppose that the Tams-Braylor gives a true positive for 90% of patients with breast cancer, and gives a false positive for 5% of patients without cancer. Let’s say the prior prevalence of breast cancer is 1%. If a patient gets a positive result on her mammogram _and_ her Tams-Braylor, what is the revised probability she has breast cancer?\n\nOne way to solve this problem would be to take the revised probability for a positive mammogram, which we already calculated as 7.8%, and plug that into the Tams-Braylor test as the new prior probability. If we do this, we find that the result comes out to 60%.\n\nSuppose that the prior prevalence of breast cancer in a demographic is 1%. Suppose that we, as doctors, have a repertoire of three independent tests for breast cancer. Our first test, test A, a mammography, has a likelihood ratio of 80%/9.6% = 8.33. The second test, test B, has a likelihood ratio of 18.0 (for example, from 90% versus 5%); and the third test, test C, has a likelihood ratio of 3.5 (which could be from 70% versus 20%, or from 35% versus 10%; it makes no difference). Suppose a patient gets a positive result on all three tests. What is the probability the patient has breast cancer?\n\n￼Here’s a fun trick for simplifying the bookkeeping. If the prior prevalence of breast cancer in a demographic is 1%, then 1 out of 100 women have breast cancer, and 99 out of 100 women do not have breast cancer. So if we rewrite the _probability_ of 1% as an _odds ratio_, the odds are 1:99.\n\nAnd the likelihood ratios of the three tests A, B, and C are:\n\n8.33 : 1 = 25 : 3\n\n18.0 : 1 = 18 : 1\n\n3.5 : 1 = 7 : 2 .\n\nThe _odds_ for women with breast cancer who score positive on all three tests, versus women without breast cancer who score positive on all three tests, will equal:\n\n1 × 25 × 18 × 7 : 99 × 3 × 1 × 2 = 3150 : 594.\n\nTo recover the probability from the odds, we just write:\n\n3150/(3150 + 594) = 84% .\n\nThis always works regardless of how the odds ratios are written; i.e., 8.33:1 is just the same as 25:3 or 75:9. It doesn’t matter in what order the tests are administered, or in what order the results are computed. The proof is left as an exercise for the reader.\n\n* * *\n\nE. T. Jaynes, in _Probability Theory With Applications in Science and Engineering_, suggests that credibility and evidence should be measured in decibels.\\[5\\]\n\nDecibels?\n\nDecibels are used for measuring exponential differences of intensity. For example, if the sound from an automobile horn carries 10,000 times as much energy (per square meter per second) as the sound from an alarm clock, the automobile horn would be 40 decibels louder. The sound of a bird singing might carry 1,000 times less energy than an alarm clock, and hence would be 30 decibels softer. To get the number of decibels, you take the logarithm base 10 and multiply by 10:\n\ndecibels = 10log_10(intensity)\n\nintensity = 10^(decibels/10) .\n\nSuppose we start with a prior probability of 1% that a woman has breast cancer, corresponding to an odds ratio of 1:99. And then we administer three tests of likelihood ratios 25:3, 18:1, and 7:2. You _could_ multiply those numbers . . . or you could just add their logarithms:\n\n10log_10(1/99) ≈ −20\n\n10log_10(25/3) ≈ 9\n\n10log_10(18/1) ≈ 13\n\n10 log_10(7/2) ≈ 5 .\n\nIt starts out as fairly unlikely that a woman has breast cancer—our credibility level is at −20 decibels. Then three test results come in, corresponding to 9, 13, and 5 decibels of evidence. This raises the credibility level by a total of 27 decibels, meaning that the prior credibility of −20 decibels goes to a posterior credibility of 7 decibels. So the odds go from 1:99 to 5:1, and the probability goes from 1% to around 83%.\n\n* * *\n\n> You are a mechanic for gizmos. When a gizmo stops working, it is due to a blocked hose 30% of the time. If a gizmo’s hose is blocked, there is a 45% probability that prodding the gizmo will produce sparks. If a gizmo’s hose is unblocked, there is only a 5% chance that prodding the gizmo will produce sparks. A customer brings you a malfunctioning gizmo. You prod the gizmo and find that it produces sparks. What is the probability that a spark-producing gizmo has a blocked hose?\n\nWhat is the sequence of arithmetical operations that you performed to solve this problem?\n\n(45% × 30%)/(45% × 30% + 5% × 70%)\n\nSimilarly, to find the chance that a woman with a positive mammogram has breast cancer, we computed:\n\n\\[P (positive|cancer) × P (cancer)\\] / \\[P(positive|cancer)×P(cancer) 􏰆 + P (positive|¬cancer) × P (¬cancer)\\]\n\n_which is_\n\nP (positive, cancer) / \\[P (positive, cancer) + P (positive, ¬cancer)\\]\n\n_which is_\n\nP (positive, cancer) / P (positive)\n\n_which is_\n\nP(cancer|positive) .\n\nThe fully general form of this calculation is known as _Bayes’s Theorem_ or _Bayes’s Rule_.\n\nBayes’s Theorem:\n\nP(A|X) = \\[P(X|A) × P(A)\\] / \\[P(X|A) × P(A) + P(X|¬A) × P(¬A) \\]\n\nWhen there is some phenomenon A that we want to investigate, and an observation X that is evidence about A—for example, in the previous example, A is breast cancer and X is a positive mammogram—Bayes’s Theorem tells us how we should _update_ our probability of A, given the _new evidence_ X.\n\nBy this point, Bayes’s Theorem may seem blatantly obvious or even tautological, rather than exciting and new. If so, this introduction has _entirely succeeded_ in its purpose.\n\n* * *\n\nBayes’s Theorem describes what makes something “evidence” and how much evidence it is. Statistical models are judged by comparison to the _Bayesian method_ because, in statistics, the Bayesian method is as good as it gets—the Bayesian method defines the maximum amount of mileage you can get out of a given piece of evidence, in the same way that thermodynamics defines the maximum amount of work you can get out of a temperature differential. This is why you hear cognitive scientists talking about _Bayesian reasoners_. In cognitive science, Bayesian reasoner is the technically precise code word that we use to mean _rational mind_.\n\nThere are also a number of general heuristics about human reasoning that you can learn from looking at Bayes’s Theorem.\n\nFor example, in many discussions of Bayes’s Theorem, you may hear cognitive psychologists saying that people _do not take prior frequencies sufficiently into account_, meaning that when people approach a problem where there’s some evidence X indicating that condition A might hold true, they tend to judge A’s likelihood solely by how well the evidence X seems to match A, without taking into account the prior frequency of A. If you think, for example, that under the mammography example, the woman’s chance of having breast cancer is in the range of 70%–80%, then this kind of reasoning is insensitive to the prior frequency given in the problem; it doesn’t notice whether 1% of women or 10% of women start out having breast cancer. “Pay more attention to the prior frequency!” is one of the many things that humans need to bear in mind to partially compensate for our built-in inadequacies.\n\nA related error is to pay too much attention to P (X |A) and not enough to P(X|¬A) when determining how much evidence X is for A. The degree to which a result X is _evidence for_ A depends not only on the strength of the statement _we’d expect to see result X if A were true_, but also on the strength of the statement _we_ **_wouldn’t_** _expect to see result X if A weren’t true_. For example, if it is raining, this very strongly implies the grass is wet—P (wetgrass|rain) ≈ 1— but seeing that the grass is wet doesn’t necessarily mean that it has just rained; perhaps the sprinkler was turned on, or you’re looking at the early morning dew. Since P (wetgrass|¬rain) is substantially greater than zero, P (rain|wetgrass) is substantially less than one. On the other hand, if the grass was _never_ wet when it wasn’t raining, then knowing that the grass was wet would _always_ show that it was raining, P (rain|wetgrass) ≈ 1, even if P (wetgrass|rain) = 50%; that is, even if the grass only got wet 50% of the times it rained. Evidence is always the result of the _differential_ between the two conditional probabilities. _Strong_ evidence is not the product of a very high probability that A leads to X, but the product of a very low probability that _not_-A could have led to X.\n\nThe _Bayesian revolution in the sciences_ is fueled, not only by more and more cognitive scientists suddenly noticing that mental phenomena have Bayesian structure in them; not only by scientists in every field learning to judge their statistical methods by comparison with the Bayesian method; but also by the idea that _science itself is a special case of Bayes’s Theorem; experimental evidence is Bayesian evidence_. The Bayesian revolutionaries hold that when you perform an experiment and get evidence that “confirms” or “disconfirms” your theory, this confirmation and disconfirmation is governed by the Bayesian rules. For example, you have to take into account not only whether your theory predicts the phenomenon, but whether other possible explanations also predict the phenomenon.\n\nPreviously, the most popular philosophy of science was probably Karl Popper’s _falsificationism_—this is the old philosophy that the Bayesian revolution is currently dethroning. Karl Popper’s idea that theories can be definitely falsified, but never definitely confirmed, is yet another special case of the Bayesian rules; if P(X|A) ≈ 1—if the theory makes a definite prediction—then observing ¬X very strongly falsifies A. On the other hand, if P(X|A) ≈ 1, and we observe X, this doesn’t definitely confirm the theory; there might be some other condition B such that P (X|B) ≈ 1, in which case observing X doesn’t favor A over B. For observing X to definitely confirm A, we would have to know, not that P(X|A) ≈ 1, but that P(X|¬A) ≈ 0, which is something that we can’t know because we can’t range over all possible alternative explanations. For example, when Einstein’s theory of General Relativity toppled Newton’s incredibly well-confirmed theory of gravity, it turned out that all of Newton’s predictions were just a special case of Einstein’s predictions.\n\nYou can even formalize Popper’s philosophy mathematically. The likelihood ratio for X, the quantity P(X|A)/P(X|¬A), determines how much observing X slides the probability for A; the likelihood ratio is what says _how strong_ X is as evidence. Well, in your theory A, you can predict X with probability 1, if you like; but you can’t control the denominator of the likelihood ratio, P(X|¬A)—there will always be some alternative theories that also predict X, and while we go with the simplest theory that fits the current evidence, you may someday encounter some evidence that an alternative theory predicts but your theory does not. That’s the hidden gotcha that toppled Newton’s theory of gravity. So there’s a limit on how much mileage you can get from successful predictions; there’s a limit on how high the likelihood ratio goes for _confirmatory_ evidence.\n\nOn the other hand, if you encounter some piece of evidence Y that is definitely _not_ predicted by your theory, this is _enormously_ strong evidence against your theory. If P (Y |A) is infinitesimal, then the likelihood ratio will also be infinitesimal. For example, if P (Y |A) is 0.0001%, and P (Y |¬A) is 1%, then the likelihood ratio P (Y |A)/P (Y |¬A) will be 1:10,000. That’s −40 decibels of evidence! Or, flipping the likelihood ratio, if P (Y |A) is _very small_, then P (Y |¬A)/P (Y |A) will be very large, meaning that observing Y greatly favors ¬A over A. Falsification is much stronger than confirmation. This is a consequence of the earlier point that _very strong_ evidence is not the product of a very high probability that A leads to X, but the product of a very _low_ probability that _not_-A could have led to X. This is the precise Bayesian rule that underlies the heuristic value of Popper’s falsificationism.\n\nSimilarly, Popper’s dictum that an idea must be falsifiable can be interpreted as a manifestation of the Bayesian conservation-of-probability rule; if a result X is positive evidence for the theory, then the result ¬X would have disconfirmed the theory to some extent. If you try to interpret both X and ¬X as “confirming” the theory, the Bayesian rules say this is impossible! To increase the probability of a theory you _must_ expose it to tests that can potentially decrease its probability; this is not just a rule for detecting would-be cheaters in the social process of science, but a consequence of Bayesian probability theory. On the other hand, Popper’s idea that there is _only_ falsification and _no such thing_ as confirmation turns out to be incorrect. Bayes’s Theorem shows that falsification is _very strong_ evidence compared to confirmation, but falsification is still probabilistic in nature; it is not governed by fundamentally different rules from confirmation, as Popper argued.\n\nSo we find that many phenomena in the cognitive sciences, plus the statistical methods used by scientists, plus the scientific method itself, are all turning out to be special cases of Bayes’s Theorem. Hence the Bayesian revolution.\n\n* * *\n\nHaving introduced Bayes’s Theorem explicitly, we can explicitly discuss its components.\n\nP(A|X) = \\[P(X|A) × P(A)\\] / \\[ P(X|A) × P(A) + P(X|¬A) × P(¬A) \\]\n\nWe’ll start with P(A|X). If you ever find yourself getting confused about what’s A and what’s X in Bayes’s Theorem, start with P(A|X) on the left side of the equation; that’s the simplest part to interpret. In P(A|X), A is the thing we want to know about. X is how we’re observing it; X is the evidence we’re using to make inferences about A. Remember that for every expression P(Q|P), we want to know about the probability for Q given P, the degree to which P implies Q—a more sensible notation, which it is now too late to adopt, would be P (Q ← P ).\n\nP (Q|P ) is closely related to P (Q, P ), but they are not identical. Expressed as a probability or a fraction, P (Q, P ) is the proportion of things that have property Q and property P among all things; e.g., the proportion of “women with breast cancer and a positive mammogram” within the group of all women. If the total number of women is 10,000, and 80 women have breast cancer and a positive mammogram, then P (Q, P ) is 80/10,000 = 0.8%. You might say that the absolute quantity, 80, is being normalized to a probability relative to the group of all women. Or to make it clearer, suppose that there’s a group of 641 women with breast cancer and a positive mammogram within a total sample group of 89,031 women. Six hundred and forty-one is the absolute quantity. If you pick out a random woman from the entire sample, then the probability you’ll pick a woman with breast cancer and a positive mammogram is P (Q, P ), or 0.72% (in this example).\n\nOn the other hand, P (Q|P ) is the proportion of things that have property Q and property P among _all things that have_ P ; e.g., the proportion of women with breast cancer and a positive mammogram within the group of _all women with positive mammograms_. If there are 641 women with breast cancer and positive mammograms, 7,915 women with positive mammograms, and 89,031 women, then P (Q, P ) is the probability of getting one of those 641 women if you’re picking at random from the entire group of 89,031, while P (Q|P ) is the probability of getting one of those 641 women if you’re picking at random from the smaller group of 7,915.\n\nIn a sense, P (Q|P ) really means P (Q, P |P ), but specifying the extra P all the time would be redundant. You already _know_ it has property P, so the property you’re _investigating_ is Q—even though you’re looking at the size of group (Q,P) within group P, not the size of group Q within group P (which would be nonsense). This is what it means to take the property on the right-hand side as given; it means you know you’re working only within the group of things that have property P. When you constrict your focus of attention to see only this smaller group, many other probabilities change. If you’re taking P as _given_, then P (Q, P ) equals just P (Q)—at least, _relative to the group_ P . The _old_ P (Q), the frequency of “things that have property Q within the entire sample,” is revised to the new frequency of “things that have property Q within the subsample of things that have property P. ” If P is _given_, if P is our entire world, then looking for (Q, P ) is the same as looking for just Q.\n\nIf you constrict your focus of attention to only the population of eggs that are painted blue, then suddenly “the probability that an egg contains a pearl” becomes a different number; this proportion is different for the population of blue eggs than the population of all eggs. The _given_, the property that constricts our focus of attention, is always on the _right_ side of P (Q|P ); the P becomes our world, the entire thing we see, and on the other side of the “given” P always has probability 1—that is what it means to take P as given. So P (Q|P ) means “If P has probability 1, what is the probability of Q?” or “If we constrict our attention to only things or events where P is true, what is the probability of Q?” The statement Q, on the other side of the given, is _not_ certain—its probability may be 10% or 90% or any other number. So when you use Bayes’s Theorem, and you write the part on the left side as P(A|X)—how to _update_ the probability of A after seeing X, the new probability of A _given_ that we ￼know X, the degree to which X _implies_ A—you can tell that X is always the _observation_ or the _evidence_, and A is the property being investigated, the thing you want to know about.\n\n* * *\n\nThe right side of Bayes’s Theorem is derived from the left side through these steps:\n\nP (A|X) = P (A|X)\n\nP(A|X)= P(X,A) / P(X)\n\nP(A|X) = P(X,A) / \\[ P(X,A) + P(X,¬A)\\]\n\nP(A|X) =\\[ P(X|A) × P(A)\\] / \\[ P(X|A) × P(A) + P(X|¬A) × P(¬A) \\] .\n\nOnce the derivation is finished, all the implications on the right side of the equation are of the form P(X|A) or P(X|¬A), while the implication on the left side is P(A|X). The symmetry arises because the elementary _causal relations_ are generally implications from facts to observations, e.g., from breast cancer to positive mammogram. The elementary _steps in reasoning_ are generally implications from observations to facts, e.g., from a positive mammogram to breast cancer. The left side of Bayes’s Theorem is an elementary _inferential_ step from the observation of positive mammogram to the conclusion of an increased probability of breast cancer. Implication is written right-to-left, so we write P (cancer|positive) on the left side of the equation. The right side of Bayes’s Theorem describes the elementary _causal_ steps—for example, from breast cancer to a positive mammogram—and so the implications on the right side of Bayes’s Theorem take the form P (positive|cancer) or P (positive|¬cancer).\n\nAnd that’s Bayes’s Theorem. Rational inference on the left end, physical causality on the right end; an equation with mind on one side and reality on the other. Remember how the scientific method turned out to be a special case of Bayes’s Theorem? If you wanted to put it poetically, you could say that Bayes’s Theorem binds reasoning into the physical universe.\n\nOkay, we’re done.\n\n* * *\n\nReverend Bayes says:\n\n![](https://www.readthesequences.com/wiki/uploads/Bayes-mugshot.png)\n\nYou are now an initiate of the Bayesian Conspiracy.\n\n* * *\n\n###### 1\\. Ward Casscells, Arno Schoenberger, and Thomas Graboys, “Interpretation by Physicians of Clinical Laboratory Results,” _New England Journal of Medicine_ 299 (1978): 999–1001.\n\n###### 2\\. David M. Eddy, “Probabilistic Reasoning in Clinical Medicine: Problems and Opportunities,” in _Judgement Under Uncertainty: Heuristics and Biases_, ed. Daniel Kahneman, Paul Slovic, and Amos Tversky (Cambridge University Press, 1982).\n\n###### 3\\. Gerd Gigerenzer and Ulrich Hoffrage, “How to Improve Bayesian Reasoning without Instruction: Frequency Formats,” _Psychological Review_ 102 (1995): 684–704.\n\n###### 4\\. Ibid.\n\n###### 5\\. Edwin T. Jaynes, “Probability Theory, with Applications in Science and Engineering,” Unpublished manuscript (1974).\n\n* * *\n\n_The first publication of this post is_ [_here_](http://yudkowsky.net/rational/bayes)_._"
    },
    "voteCount": 30,
    "forceInclude": true
  },
  {
    "_id": "QkX2bAkwG2EpGvNug",
    "url": null,
    "title": "The Second Law of Thermodynamics, and Engines of Cognition",
    "slug": "the-second-law-of-thermodynamics-and-engines-of-cognition",
    "author": "Eliezer_Yudkowsky",
    "question": false,
    "tags": [
      {
        "name": "World Modeling"
      },
      {
        "name": "Law-Thinking"
      },
      {
        "name": "Physics"
      },
      {
        "name": "Probability & Statistics"
      },
      {
        "name": "Information Theory"
      },
      {
        "name": "Bayesianism"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "The first law of thermodynamics, better known as Conservation of Energy, says that you can't create energy from nothing: it prohibits perpetual motion machines of the first type, which run and run indefinitely without consuming fuel or any other resource.  According to our modern view of physics, energy is conserved in each _individual_ interaction of particles.  By mathematical induction, we see that no matter how large an assemblage of particles may be, it cannot produce energy from nothing - not without violating what we presently believe to be the laws of physics.\n\nThis is why the US Patent Office will summarily reject your amazingly clever proposal for an assemblage of wheels and gears that cause one spring to wind up another as the first runs down, and so continue to do work forever, according to your calculations.  There's a _fully general_ proof that at least one wheel must violate (our standard model of) the laws of physics for this to happen.  So unless you can explain how _one_ wheel violates the laws of physics, the _assembly_ of wheels can't do it either.\n\nA similar argument applies to a \"reactionless drive\", a propulsion system that violates Conservation of Momentum.  In standard physics, momentum is conserved for all individual particles and their interactions; by mathematical induction, momentum is conserved for physical systems whatever their size.  If you can visualize two particles knocking into each other and always coming out with the same total momentum that they started with, then you can see how scaling it up from particles to a gigantic complicated collection of gears won't change anything.  Even if there's a trillion quadrillion atoms involved, 0 + 0 + ... + 0 = 0.\n\nBut Conservation of Energy, as such, cannot prohibit converting heat into work.  You can, in fact, build a sealed box that converts ice cubes and stored electricity into warm water.  It isn't even difficult.  Energy cannot be created or destroyed:  The net change in energy, from transforming (ice cubes + electricity) to (warm water), must be 0.  So it couldn't violate Conservation of Energy, as such, if you did it the other way around...\n\nPerpetual motion machines of the second type, which convert warm water into electrical current and ice cubes, are prohibited by the _Second_ Law of Thermodynamics.\n\nThe Second Law is a bit harder to understand, as it is essentially Bayesian in nature.\n\nYes, really.\n\nThe essential _physical_ law underlying the Second Law of Thermodynamics is a theorem which can be proven within the standard model of physics:  _In the development over time of any closed system, phase space volume is conserved._\n\nLet's say you're holding a ball high above the ground.  We can describe this state of affairs as a point in a multidimensional space, at least one of whose dimensions is \"height of ball above the ground\".  Then, when you drop the ball, it moves, and so does the dimensionless point in phase space that describes the entire system that includes you and the ball.  \"Phase space\", in physics-speak, means that there are dimensions for the momentum of the particles, not just their position - i.e., a system of 2 particles would have 12 dimensions, 3 dimensions for each particle's position, and 3 dimensions for each particle's momentum.\n\nIf you had a multidimensional space, each of whose dimensions described the position of a gear in a huge assemblage of gears, then as you turned the gears a single point would swoop and dart around in a rather high-dimensional phase space.  Which is to say, just as you can view a great big complex machine as a single point in a very-high-dimensional space, so too, you can view the laws of physics describing the behavior of this machine over time, as describing the trajectory of its point through the phase space.\n\nThe Second Law of Thermodynamics is a consequence of a theorem which can be proven in the standard model of physics:  If you take a volume of phase space, and develop it forward in time using standard physics, the total volume of the phase space is conserved.\n\nFor example:\n\nLet there be two systems, X and Y: where X has 8 possible states, Y has 4 possible states, and the joint system (X,Y) has 32 possible states.\n\nThe development of the joint system over time can be described as a rule that maps initial points onto future points.  For example, the system could start out in X~7~Y~2~, then develop (under some set of physical laws) into the state X~3~Y~3~ a minute later.  Which is to say: if X started in 7, and Y started in 2, and we watched it for 1 minute, we would see X go to 3 and Y go to 3.  Such are the laws of physics.\n\nNext, let's carve out a subspace S of the joint system state.  S will be the subspace bounded by X being in state 1 and Y being in states 1-4.  So the total volume of S is 4 states.\n\nAnd let's suppose that, under the laws of physics governing (X,Y) the states initially in S behave as follows:\n\n> X~1~Y~1~ -\\> X~2~Y~1~  \n> X~1~Y~2~ -\\> X~4~Y~1~  \n> X~1~Y~3~ -\\> X~6~Y~1~  \n> X~1~Y~4~ -\\> X~8~Y~1~\n\nThat, in a nutshell, is how a refrigerator works.\n\nThe X subsystem began in a narrow region of state space - the single state 1, in fact - and Y began distributed over a wider region of space, states 1-4.  By interacting with each other, Y went into a narrow region, and X ended up in a wide region; _but the total phase space volume was conserved._  4 initial states mapped to 4 end states.\n\nClearly, so long as total phase space volume is conserved by physics over time, you can't squeeze Y harder than X expands, or vice versa - for every subsystem you squeeze into a narrower region of state space, some other subsystem has to expand into a wider region of state space.\n\nNow let's say that we're _uncertain_ about the joint system (X,Y), and our _uncertainty_ is described by an equiprobable distribution over S.  That is, we're pretty sure X is in state 1, but Y is equally likely to be in any of states 1-4.  If we shut our eyes for a minute and then open them again, we will expect to see Y in state 1, but X might be in any of states 2-8.  Actually, X can only be in _some_ of states 2-8, but it would be too costly to think out exactly which states these might be, so we'll just say 2-8.\n\nIf you consider the [Shannon entropy](/lw/o1/entropy_and_short_codes/) of our uncertainty about X and Y as individual systems, X began with 0 bits of entropy because it had a single definite state, and Y began with 2 bits of entropy because it was equally likely to be in any of 4 possible states.  (There's no [mutual information](/lw/o2/mutual_information_and_density_in_thingspace/) between X and Y.)  A bit of physics occurred, and lo, the entropy of Y went to 0, but the entropy of X went to log~2~(7) = 2.8 bits.  So entropy was transferred from one system to another, and decreased _within_ the Y subsystem; but due to the cost of bookkeeping, we didn't bother to track some information, and hence (from our perspective) the overall entropy increased.\n\nIf there was a physical process that mapped past states onto future states like this:\n\n> X2,Y1 -> X2,Y1  \n> X2,Y2 -> X2,Y1  \n> X2,Y3 -> X2,Y1  \n> X2,Y4 -> X2,Y1\n\nThen you could have a physical process that would actually _decrease entropy,_ because no matter where you started out, you would end up at the same place.  The laws of physics, developing over time, would compress the phase space.\n\nBut there is a theorem, Liouville's Theorem, which can be proven true of our laws of physics, which says that this never happens: [phase space is conserved](http://www.av8n.com/physics/phase-space-thin-lens.htm).\n\nThe Second Law of Thermodynamics is a corollary of Liouville's Theorem: no matter how clever your configuration of wheels and gears, you'll never be able to decrease entropy in one subsystem without increasing it somewhere else.  When the phase space of one subsystem narrows, the phase space of another subsystem must widen, and the joint space keeps the same volume.\n\nExcept that what was initially a _compact_ phase space, may develop squiggles and wiggles and convolutions; so that to draw a simple boundary around the whole mess, you must draw a much larger boundary than before - this is what gives the appearance of entropy increasing.  (And in quantum systems, where different universes go different ways, entropy actually does increase in any local universe.  But omit this complication for now.)\n\nThe Second Law of Thermodynamics is actually probabilistic in nature - if you ask about the probability of hot water spontaneously entering the \"cold water and electricity\" state, the probability does exist, it's just very small.  This doesn't mean Liouville's Theorem is violated with small probability; a theorem's a theorem, after all.  It means that if you're in a great big phase space volume at the start, but you _don't know where,_ you may assess a tiny little _probability_ of ending up in some particular phase space volume.  _So far as you know,_ with infinitesimal probability, this particular glass of hot water may be the kind that spontaneously transforms itself to electrical current and ice cubes.  (Neglecting, as usual, quantum effects.)\n\nSo the Second Law really _is_ inherently Bayesian.  When it comes to any real thermodynamic system, it's a strictly lawful statement of your _beliefs about_ the system, but only a probabilistic statement about the system itself.\n\n\"Hold on,\" you say.  \"That's not what I learned in physics class,\" you say.  \"In the lectures _I_ heard, thermodynamics is about, you know, _temperatures._  Uncertainty is a subjective state of mind!  The temperature of a glass of water is an objective property of the water!  What does heat have to do with probability?\"\n\nOh ye of little [trust](/lw/na/trust_in_bayes/).\n\nIn one direction, the connection between heat and probability is relatively straightforward:  If the only fact you know about a glass of water is its temperature, then you are much more uncertain about a hot glass of water than a cold glass of water.\n\nHeat is the zipping around of lots of tiny molecules; the hotter they are, the faster they can go.  Not all the molecules in hot water are travelling at the same speed - the \"temperature\" isn't a uniform speed of all the molecules, it's an average speed of the molecules, which in turn corresponds to a predictable statistical distribution of speeds - anyway, the point is that, the hotter the water, the faster the water molecules _could be_ going, and hence, the more uncertain you are about the velocity (not just speed) of any _individual_ molecule.  When you multiply together your uncertainties about all the individual molecules, you will be _exponentially_ more uncertain about the whole glass of water.\n\nWe take the logarithm of this exponential volume of uncertainty, and call that the entropy.  So it all works out, you see.\n\nThe connection in the other direction is less obvious.  Suppose there was a glass of water, about which, initially, you knew only that its temperature was 72 degrees.  Then, suddenly, Saint Laplace reveals to you the exact locations and velocities of all the atoms in the water.  You now know perfectly the state of the water, so, by the information-theoretic definition of entropy, its entropy is zero.  Does that make its _thermodynamic_ entropy zero?  Is the water colder, because we know more about it?\n\nIgnoring quantumness for the moment, the answer is:  Yes!  Yes it is!\n\nMaxwell once asked:  Why can't we take a uniformly hot gas, and partition it into two volumes A and B, and let only fast-moving molecules pass from B to A, while only slow-moving molecules are allowed to pass from A to B?  If you could build a gate like this, soon you would have hot gas on the A side, and cold gas on the B side.  That would be a cheap way to refrigerate food, right?\n\nThe agent who inspects each gas molecule, and decides whether to let it through, is known as \"Maxwell's Demon\".  And the reason you can't build an efficient refrigerator this way, is that Maxwell's Demon generates entropy in the process of inspecting the gas molecules and deciding which ones to let through.\n\nBut suppose you already _knew_ where all the gas molecules were?\n\nThen you actually _could_ run Maxwell's Demon and extract useful work.\n\nSo (again ignoring quantum effects for the moment), if you _know_ the states of all the molecules in a glass of hot water, it is cold in a genuinely thermodynamic sense: you can take electricity out of it and leave behind an ice cube.\n\nThis doesn't violate Liouville's Theorem, because if Y is the water, and _you_ are Maxwell's Demon (denoted M), the physical process behaves as:\n\n> M1,Y1 -> M1,Y1  \n> M2,Y2 -> M2,Y1  \n> M3,Y3 -> M3,Y1  \n> M4,Y4 -> M4,Y1\n\nBecause Maxwell's demon _knows_ the exact state of Y, this is mutual information between M and Y.  The mutual information decreases the joint entropy of (M,Y):  H(M,Y) = H(M) + H(Y) - I(M;Y).  M has 2 bits of entropy, Y has two bits of entropy, and their mutual information is 2 bits, so (M,Y) has a total of 2 + 2 - 2 = 2 bits of entropy.  The physical process just transforms the \"coldness\" (negentropy) of the mutual information to make the actual water cold - afterward, M has 2 bits of entropy, Y has 0 bits of entropy, and the mutual information is 0.  Nothing wrong with that!\n\nAnd don't tell me that knowledge is \"subjective\".  Knowledge has to be represented in a brain, and that makes it as physical as anything else.  For M to physically represent an accurate picture of the state of Y, M's physical state must correlate with the state of Y.  You can take thermodynamic advantage of that - it's called a Szilard engine.\n\nOr as E.T. Jaynes put it, \"The old adage 'knowledge is power' is a very cogent truth, both in human relations and in thermodynamics.\"\n\nAnd conversely, _one subsystem cannot increase in mutual information with another subsystem, without (a) interacting with it and (b) doing thermodynamic work._  \n\nOtherwise you could build a Maxwell's Demon and violate the Second Law of Thermodynamics - which in turn would violate Liouville's Theorem - which is prohibited in the standard model of physics.\n\nWhich is to say:  **To form accurate beliefs about something, you _really do_ have to observe it.**  It's a very physical, very real process: any rational mind does \"work\" in the thermodynamic sense, not just the sense of mental effort.\n\n(It is sometimes said that it is erasing bits in order to prepare for the next observation that takes the thermodynamic work - but that distinction is just a matter of words and perspective; the math is unambiguous.)\n\n(Discovering logical \"truths\" is a complication which I will not, for now, consider - at least in part because I am still thinking through the exact formalism myself.  In thermodynamics, knowledge of logical truths does not count as negentropy; as would be expected, since a reversible computer can compute logical truths at arbitrarily low cost.  All this that I have said is true of the logically omniscient: any lesser mind will necessarily be less efficient.)  \n\n\"Forming accurate beliefs requires a corresponding amount of evidence\" is a very cogent truth both in human relations and in thermodynamics: if blind faith actually worked as a method of investigation, you could turn warm water into electricity and ice cubes.  Just build a Maxwell's Demon that has blind faith in molecule velocities.\n\nEngines of cognition are not so different from heat engines, though they manipulate entropy in a more subtle form than burning gasoline.  For example, to the extent that an engine of cognition is not perfectly efficient, it must radiate waste heat, just like a car engine or refrigerator.\n\n\"Cold rationality\" is true in a sense that Hollywood scriptwriters never dreamed (and false in the sense that they did dream).\n\nSo unless you can tell me which _specific step_ in your argument violates the laws of physics by giving you true knowledge of the unseen, don't expect me to believe that a big, elaborate clever argument can do it either."
    },
    "voteCount": 94,
    "forceInclude": true
  },
  {
    "_id": "CPP2uLcaywEokFKQG",
    "url": null,
    "title": "Toolbox-thinking and Law-thinking",
    "slug": "toolbox-thinking-and-law-thinking",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "Rationality"
      },
      {
        "name": "Law-Thinking"
      },
      {
        "name": "Distinctions"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "Tl;dr:\n\nI've noticed a dichotomy between \"thinking in toolboxes\" and \"thinking in laws\".\n\nThe toolbox style of thinking says it's important to have a big bag of tools that you can adapt to context and circumstance; people who think very toolboxly tend to suspect that anyone who goes talking of a single optimal way is just ignorant of the uses of the other tools.\n\nThe lawful style of thinking, done correctly, distinguishes between descriptive truths, normative ideals, and prescriptive ideals. It may talk about certain paths being optimal, even if there's no executable-in-practice algorithm that yields the optimal path. It considers truths that are not tools.\n\nWithin nearly-Euclidean mazes, the triangle inequality - that the path AC is never spatially longer than the path ABC - is always true but only sometimes useful. The triangle inequality has the prescriptive implication that _if_ you _know_ that one path choice will travel ABC and one path will travel AC, and _if_ the _only_ pragmatic path-merit you care about is going the minimum spatial distance (rather than say avoiding stairs because somebody in the party is in a wheelchair), then you should pick the route AC. But the triangle inequality goes on governing Euclidean mazes whether or not you know which path is which, and whether or not you need to avoid stairs.\n\nToolbox thinkers may be extremely suspicious of this claim of universal lawfulness if it is explained less than perfectly, because it sounds to them like \"Throw away all the other tools in your toolbox! All you need to know is Euclidean geometry, and you can always find the shortest path through any maze, which in turn is always the best path.\"\n\nIf you think that's an unrealistic depiction of a misunderstanding that would never happen in reality, keep reading.\n\n* * *\n\nHere's a recent conversation from Twitter which I'd consider a nearly perfect illustration of the toolbox-vs.-laws dichotomy:\n\n> [David Chapman](https://twitter.com/Meaningness/status/993602725316186112): \"By _rationalism,_ I mean any claim that there is an ultimate criterion according to which thinking and acting could be judged to be correct or optimal... Under this definition, 'rationalism' must go beyond 'systematic methods are often useful, hooray!'... A rationalism claims there is _one weird trick_ to correct thinking, which guarantees an optimal result. (Some rationalisms specify the trick; others insist there must be one, but that it is not currently knowable.) A rationalism makes strongly normative judgments: everyone _ought_ to think that way.\"\n\n> [Graham Rowe](https://twitter.com/grahamsrowe/status/993781462263574528): \"Is it fair to say that rationalists see the world entirely through rationality while meta-rationalists look at rationality as one of many tools (that they can use fluently and appropriately) to be used in service of a broader purpose?\"\n\n> [David Chapman](https://twitter.com/Meaningness/status/993879794180747264): \"More-or-less, I think! Although I don’t think rationalists _do_ see the world entirely through rationality, they just say they think they ought to.\"\n\n> [Julia Galef](https://twitter.com/juliagalef/status/993621055221608449): \"I don't think the 'one weird trick' description is accurate. It's more like: there's one correct normative model in theory, which cannot possibly be approximated by a single rule in practice, but we can look for collections of 'tricks' that seem like they bring us closer to the normative model. e.g., 'On the margin, taking more small risks is likely to increase your EV' is one example.\"\n\n> [David Chapman](https://twitter.com/Meaningness/status/993626127842213889): \"The element that I’d call clearly meta-rational is understanding that rationality is not one well-defined thing but a bag of tricks that are more-or-less applicable in different situations.\"\n\nJulia then [quoted](https://twitter.com/juliagalef/status/993715985768177665) a paper mentioning \"The best prescription for human reasoning is not necessarily to always use the normative model to govern one's thinking.\" To which Chapman [replied](https://twitter.com/Meaningness/status/993718942827921408):\n\n> \"Baron’s distinction between 'normative' and 'prescriptive' is one I haven’t seen before. That seems useful and maybe key. OTOH, if we’re looking for a disagreement crux, it might be whether a normative theory that can’t be achieved, even in principle, is a good thing.\"\n\nI'm now going to badly stereotype this conversation in the form I feel like I've seen it many times previously, including e.g. in the discussion of p-values and frequentist statistics. On this stereotypical depiction, there is a dichotomy between the thinking of Msr. Toolbox and Msr. Lawful that goes like this:\n\nMsr. Toolbox: \"It's important to know how to use a broad variety of statistical tools and adapt them to context. The many ways of calculating p-values form one broad family of tools; any particular tool in the set has good uses and bad uses, depending on context and what exactly you do. [Using likelihood ratios](https://arbital.com/p/likelihoods_not_pvalues/?l=4xx) is an interesting statistical technique, and I'm sure it has its good uses in the right contexts. But it would be very surprising if that one weird trick was the best calculation to do in every paper and every circumstance. If you claim it is the universal best way, then I suspect you of blind idealism, insensitivity to context and nuance, ignorance of all the other tools in the toolbox, the sheer folly of callow youth. You only have a hammer and no real-world experience using screwdrivers, so you claim everything is a nail.\"\n\nMsr. Lawful: \"On complex problems we may not be able to compute exact [Bayesian updates](https://arbital.com/p/bayes_rule_guide/), but the math still describes the _optimal_ update, in the same way that a Carnot cycle describes a thermodynamically ideal engine even if you can't build one. You are unlikely to find a superior viewpoint that makes some other update even more optimal than the Bayesian update, not without doing a great deal of fundamental math research and maybe not at all. We didn't choose that formalism arbitrarily! We have a very broad variety of [coherence theorems](https://arbital.com/p/expected_utility_formalism/?l=7hh) all spotlighting the same central structure of probability theory, saying variations of 'If your behavior cannot be viewed as coherent with probability theory in sense X, you must be executing a dominated strategy and shooting off your foot in sense Y'.\"\n\nI currently suspect that when Msr. Law talks like this, Msr. Toolbox hears \"I prescribe to you the following recipe for your behavior, the Bayesian Update, which you ought to execute in every kind of circumstance.\"\n\nThis also appears to me to frequently turn into one of those awful durable forms of misunderstanding: Msr. Toolbox doesn't see what you could possibly be telling somebody to do with a \"good\" or \"ideal\" algorithm besides executing that algorithm.\n\nIt would not surprise me if there's a symmetrical form of durable misunderstanding where a Lawist has trouble processing a Toolboxer's disclaimer: \"No, you don't understand, I am not trying to describe the one true perfect optimal algorithm here, I'm trying to describe a context-sensitive tool that is sometimes useful in real life.\" Msr. Law may not see what you could possibly be doing with a supposedly \"prudent\" or \"actionable\" recipe besides saying that it's the correct answer, and may feel very suspicious of somebody trying to say everyone should use an answer while disclaiming that they don't really think it's true. Surely this is just the setup for some absurd motte-and-bailey where we claim something is the normative answer, and then as soon as we're challenged we walk back and claim it was 'just one tool in the toolbox'.\n\nAnd it's not like those callow youths the Toolboxer is trying to lecture don't actually exist. The world is full of people who think they have the One True Recipe (_without_ having a normative ideal by which to prove that this is indeed the optimal recipe given their preferences, knowledge, and available computing power).\n\nThe only way I see to resolve this confusion is by grasping a certain particular abstraction and distinction - as a more Lawfully inclined person might put it. Or by being able to deploy both kinds of thinking, depending on context - as a more Toolbox-inclined person might put it.\n\nIt may be that none of my readers need the lecture at this point, but I've learned to be cautious about that sort of thing, so I'll walk through the difference anyways.\n\n* * *\n\nEvery traversable maze has a spatially shortest path; or if we are to be precise in our claims but not our measurements, a set of spatially shortest-ish paths that are all nearly the same distance.\n\nWe may perhaps call this spatially shortest path the \"best\" or \"ideal\" or \"optimal\" path through the maze, _if_ we think our preference for walking shorter distances is the _only_ pragmatically important merit of a path.\n\nThat there exists some shortest path, which may even be optimal according to our preferences, doesn't mean that you can come to an intersection at the maze and \"just choose whichever branch is on the shortest path\".\n\nAnd the fact that you cannot, at an intersection, just choose the shorter path, doesn't mean that the concepts of distance and greater or lesser distance aren't useful.\n\nIt might even be that the maze-owner could truthfully tell you, \"By the way, this right-hand turn here keeps you on the shortest path,\" and yet you'd still be wiser to take the left-hand turn... because you're following the left-hand rule. Where the left-hand rule is to keep your left hand on the wall and go on walking, which works for not getting lost inside a maze whose exit is connected to the start by walls. It's a good rule for agents with sharply bounded memories who can't always remember their paths exactly.\n\nAnd if you're using the left-hand rule it is a terrible, terrible idea to jump walls and make a different turn just once, even if that looks like a great idea at the time, because that is an excellent way to get stuck traversing a disconnected island of connected walls inside the labyrinth.\n\nSo making the left-hand turn leads you to walk the shortest expected distance, relative to the other rules you're using. Making the right-hand turn instead, even if it seemed locally smart, might have you traversing an infinite distance instead.\n\nBut then you may not be on the shortest path, even though you are following the recommendations of the wisest and most prudent rule given your current resources. By contemplating the difference, you know that there is in principle room for improvement. Maybe that inspires you to write a maze-mapping, step-counting cellphone app that lets you get to the exit faster than the left-hand rule.\n\nAnd the reason that there's a better recipe isn't that \"no recipe is perfect\", it isn't that there exists an infinite sequence of ever-better roads. If the maze-owner gave you a map with the shortest path drawn in a line, you could walk the true shortest path and there wouldn't be any shorter path than that.\n\nShortness is a property of paths; a tendency to produce shorter paths is a property of recipes. What makes a phone app an improvement is not that the app is adhering more neatly to some ideal sequence of left and right turns, it's that the path is shorter in a way that can be [defined independently of the app's algorithms](https://arbital.com/p/fair_problem_class/).\n\nOnce you can admit a path can be \"shorter\" in a way that abstracts away from the walker - not _better,_ which does depend on the walker, but _shorter_ \\- it's hard not to admit the notion of there being a shortest path.\n\nI mean, I suppose you could try very hard to never talk about a shortest path and only talk about alternative recipes that yield _shorter_ paths. You could diligently make sure to _never_ imagine this shorterness as a kind of decreased distance-in-performance-space from any 'shortest path'. You could make very sure that in your consideration of new recipes, you maintain your ideological purity as a toolboxer by only ever asking about laws that govern which of two paths are shorter, and never getting any inspiration from any kind of law that governs which path is shortest.\n\nIn which case you would have diligently eliminated a valuable conceptual tool from your toolbox. You would have carefully made sure that you always had to take longer roads to those mental destinations that can be reached the fastest by contemplating properties of ideal solutions, or distance from ideal solutions.\n\nBut why? Why would you?\n\n* * *\n\nI think at this point the Toolbox reply - though I'm not sure I could pass its Ideological Turing Test - might be that idealistic thinking has a great trap and rottenness at its heart.\n\nIt might say:\n\nSomebody who doesn't wisely shut down all this thinking about \"shortest paths\" instead of the left-hand rule as a good tool for some mazes - someone who begins to imagine some unreachable ideal of perfection, instead of a series of apps that find shorter paths most of the time - will surely, in practice, begin to confuse the notion of the left-hand rule, or their other current recipe, with _the shortest path._\n\nAfter all, nobody can see this \"shortest path\", and it's supposedly a virtuous thing. So isn't it an inevitable consequence of human nature that people will start to use that idea as praise for their current recipes?\n\nAnd also in the real world, surely Msr. Law will inevitably forget the extra premise involved with the step from \"spatially shortest path\" to \"best path\"- the contextual requirement that our only important preference was shorter spatial distances so defined. Msr. Law will insist that somebody in a wheelchair go down the \"best path\" of the maze, even though that path involves going up and down a flight of stairs.\n\nAnd Msr. Law will be unable to mentally deal with a helicopter overflying the maze that violates their ontology relative to which \"the shortest path\" was defined.\n\nAnd it will also never occur to Msr. Law to pedal around the maze in a bicycle, which is a much easier trip even if it's not the shortest spatial distance.\n\nAnd Msr. Law will assume that the behavior of mortgage-backed securities is independently Gaussian-random because the math is neater that way, and then derive a definite theorem showing a top-level tranche of MBSs will almost never default, thus bringing down their trading firm -\n\nTo all of which I can only reply: \"Well, yes, that happens some of the time, and there are contextual occasions where it is a useful tool to lecture Msr. Law on the importance of having a diverse toolbox. But it is not a _universal_ truth that _everyone_ works like that and needs to be prescribed the same lecture! You need to be sensitive to context here!\"\n\nThere are definitely versions of Msr. Law who think the universal generalization they've been told about is a One Weird Trick That Is All You Need To Know; people who could in fact benefit from a lecture on the importance of diverse toolboxes.\n\nThere are also extreme toolbox thinkers could benefit from a lecture on the importance of thinking that considers unreachable ideals, and how to get closer to them, and the obstacles that are moving us away from them.\n\nNot to commit the [fallacy of the golden mean](https://en.wikipedia.org/wiki/Argument_to_moderation) or anything, but the two viewpoints are both metatools in the metatoolbox, as it were. You're better off if you can use both in ways that depend on context and circumstance, rather than insisting that _only_ toolbox reasoning is the _universally best context-insensitive_ metaway to think.\n\nIf that's not putting the point too sharply.\n\nThinking in terms of Law is often useful. You just have to be careful to understand the context and the caveats: when is the right time to think in Law, how to think in Law, and what type of problems call for Lawful thinking.\n\nWhich is _not_ the same as saying that every Law has exceptions. Thermodynamics still holds even at times, like playing tennis, when it's not a good time to be thinking about thermodynamics. If you thought that every Law had exceptions because it wasn't always useful to think about that Law, you'd be rejecting the metatool of Law entirely, and thinking in toolbox terms at a time when it wasn't useful to do so.\n\nAre there Laws of optimal thought governing the optimal way to contextualize and caveat, which might be helpful for finding good executable recipes? The naturally Lawful thinker will immediately suspect so, even if they don't know what those Laws are. Not knowing these Laws won't panic a healthy Lawful thinker. Instead they'll proceed to look around for useful yet chaotic-seeming prescriptions to use now instead of later - _without_ mistaking those chaotic prescriptions for Laws, _or_ treating the chaos of their current recipes as proof that there's no good normative ideals to be had.\n\nIndeed, it can sometimes be useful to contemplate, in detail, that there are probably Laws you don't know. But that's a more advanced metatool in the metatoolbox, useful in narrower ways and in fewer contexts having to do with the invention of new Laws as well as new recipes, and I'd rather not strain Msr. Toolbox's credulity any further.\n\n* * *\n\nTo close out, one recipe I'd prescribe to reduce confusion in the toolbox-inclined is to try to see the Laws as descriptive statements, rather than being any kind of normative ideal at all.\n\nThe idea that there's a shortest path through the maze isn't a \"normative ideal\" instead of a \"prescriptive ideal\", it's just true. Once you define distance there is in fact a shortest path through the maze.\n\nThe triangle inequality might sound very close to a prescriptive rule that you ought to walk along AC instead of ABC. But actually the prescriptive rule is only if you _want_ to walk shorter distances ceteris paribus, only if you know which turn is which, only if you're not trying to avoid stairs, and only if you're not taking an even faster route by getting on a bicycle and riding outside the whole maze to the exit. The prescriptive rule \"try walking along AC\" isn't the same as the triangle inequality itself, which goes on being _true_ of spatial distances in Euclidean or nearly-Euclidean geometries - whether or not you know, whether or not you care, whether or not it's useful to think about at any given moment, even if you own a bicycle.\n\nThe statement that you can't have a heat-pressure engine more efficient than a Carnot cycle isn't about gathering in a cultish circle to sing praises of the Carnot cycle as being the ideally best possible kind of engine. It's just a true fact of thermodynamics. This true fact might helpfully suggest that you think about obstacles to Carnot-ness as possible places to improve your engine - say, that you should try to prevent heat loss from the combustion chamber, since heat loss prevents an adiabatic cycle. But even at times when it's not in fact useful to think about Carnot cycles, it doesn't mean your heat engine is allowed on those occasions to perform better than a Carnot engine.\n\nYou can't extract any more evidence from an observation than is given by its likelihood ratio. You could see this as being true because Bayesian updating is an often-unreachable normative ideal of reasoning, so therefore nobody can do better than it. But I'd call it a deeper level of understanding to see it as a law saying that you can't get a higher expected score by making any different update. This is a generalization that holds over both Bayes-inspired recipes and non-Bayes-inspired recipes. If you _want_ to assign higher probability to the correct hypothesis, it's a short step from that preference to regarding Bayesian updates as a normative ideal; but the idea begins life as a descriptive assertion, not as a normative assertion.\n\nIt's a relatively shallow understanding of the [coherence theorems](https://arbital.com/p/expected_utility_formalism/?l=7hh) to say \"Well, they show that if you don't use probabilities and expected utilities you'll be incoherent, which is bad, so you shouldn't do that.\" It's a deeper understanding to state, \"If you do something that is incoherent in way X, it will correspond to a dominated strategy in fashion Y. This is a universal generalization that is true about every tool in the statistical toolbox, whether or not they are in fact coherent, whether or not you personally prefer to avoid dominated strategies, whether or not you have the computing power to do any better, even if you own a bicycle.\"\n\nI suppose that when it comes to the likes of [Fun Theory](http://lesswrong.com/lw/xy/the_fun_theory_sequence/), there isn't any deeper fact of nature underlying the \"normative ideal\" of a eudaimonic universe. But in simpler matters of math and science, a \"normative ideal\" like the Carnot cycle or Bayesian decision theory is almost always the manifestation of some simpler fact that is _so closely related to something we want_ that we are tempted to take one step to the right and view it as a \"normative ideal\". If you're allergic to normative ideals, maybe a helpful course would be to discard the view of whatever-it-is as a normative ideal and try to understand it as a fact.\n\nBut that is a more advanced state of understanding than trying to understand what is better or best. If you're not allergic to ideals, then it's okay to try to understand why Bayesian updates are often-unreachable normative ideals, before you try to understand how they're just there."
    },
    "voteCount": 84,
    "forceInclude": true
  },
  {
    "_id": "WQFioaudEH8R7fyhm",
    "url": null,
    "title": "Local Validity as a Key to Sanity and Civilization",
    "slug": "local-validity-as-a-key-to-sanity-and-civilization",
    "author": "Eliezer_Yudkowsky",
    "question": false,
    "tags": [
      {
        "name": "Rationality"
      },
      {
        "name": "Common Knowledge"
      },
      {
        "name": "Public Discourse"
      },
      {
        "name": "Epistemology"
      },
      {
        "name": "Gears-Level"
      }
    ],
    "tableOfContents": {
      "sections": [
        {
          "title": "0.",
          "anchor": "0_",
          "level": 1
        },
        {
          "title": "i.",
          "anchor": "i_",
          "level": 1
        },
        {
          "title": "ii.",
          "anchor": "ii_",
          "level": 1
        },
        {
          "title": "iii.",
          "anchor": "iii_",
          "level": 1
        },
        {
          "title": "iv.",
          "anchor": "iv_",
          "level": 1
        },
        {
          "title": "v.",
          "anchor": "v_",
          "level": 1
        },
        {
          "title": "vi.",
          "anchor": "vi_",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        },
        {
          "anchor": "comments",
          "level": 0,
          "title": "65 comments"
        }
      ],
      "headingsCount": 9
    },
    "contents": {
      "markdown": "(Cross-posted [from Facebook](https://www.facebook.com/yudkowsky/posts/10156117992914228).)\n\n0.\n--\n\nTl;dr: There's a similarity between these three concepts:\n\n*   A locally valid proof step in mathematics is one that, in general, produces only true statements from true statements. This is a property of a single step, irrespective of whether the final conclusion is true or false.\n*   There's such a thing as a bad argument even for a good conclusion. In order to arrive at sane answers to questions of fact and policy, we need to be curious about whether arguments are good or bad, independently of their conclusions. The rules against fallacies must be enforced even against arguments for conclusions we like.\n*   For civilization to hold together, we need to make coordinated steps away from Nash equilibria in lockstep. This requires general rules that are allowed to impose penalties on people we like or reward people we don't like. When people stop believing the general rules are being evaluated sufficiently fairly, they go back to the Nash equilibrium and civilization falls.\n\ni.\n--\n\nThe notion of a locally evaluated argument step is simplest in mathematics, where it is a formalizable idea [in model theory](https://www.lesswrong.com/posts/Z2CuyKtkCmWGQtAEh/proofs-implications-and-models). In math, a general type of step is 'valid' if it only produces semantically true statements from other semantically true statements, relative to a given model. If _x_ = _y_ in some set of variable assignments, then 2_x_ = 2_y_ in the same model. Maybe _x_ doesn't equal _y_, in some model, but even if it doesn't, the local step from \"_x_ = _y_\" to \"2_x_ = 2_y_\" is a locally valid step of argument. It won't introduce any _new_ problems.\n\nConversely, _xy_ = _xz_ does not imply _y_ = _z_. It happens to work when _x_ = 2, _y_ = 3, and _z_= 3, in which case the two statements say \"6 = 6\" and \"3 = 3\" respectively. But if _x_ = 0, _y_ = 4, _z_ = 17, then we have \"0 = 0\" on one side and \"4 = 17\" on the other. We can feed in a true statement and get a false statement out the other end. This argument is not _locally_ okay.\n\nYou can't get the concept of a \"mathematical proof\" unless on some level—though often an intuitive level rather than an explicit one—you understand the notion of a single step of argument that is locally okay or locally not okay, independent of whether you globally agreed with the final conclusion. There's a kind of approval you give to the pieces of the argument, rather than looking the whole thing over and deciding whether you like what came out the other end.\n\nOnce you've grasped that, it may even be possible to convince you of mathematical results that sound counterintuitive. When your understanding of the rules governing allowable argument steps has become stronger than your faith in your ability to judge whole intuitive conclusions, you may be convinced of truths you would not otherwise have grasped.\n\nii.\n---\n\nMore generally in life, even outside of mathematics, there are such things as bad arguments for good conclusions.\n\nThere are even such things as genuinely good arguments for false conclusions, though of course those are much rarer. By the Bayesian definition of evidence, \"strong evidence\" is exactly that kind of evidence which we very rarely expect to find supporting a false conclusion. Lord Kelvin's careful and multiply-supported lines of reasoning arguing that the Earth could not possibly be so much as a hundred million years old, all failed simultaneously in a surprising way because that era didn't know about nuclear reactions. But most of the time this does not happen.\n\nOn the other hand, bad arguments for true conclusions are extremely easy to come by, because there are tiny elves that whisper them to people. There isn't anything the least bit more difficult in making an argument terrible when it leads to a good conclusion, since the tiny elves own lawnmowers.\n\nOne of the marks of an intellectually strong mind is that they are able to take a curious interest in whether a particular argument is a good argument or a bad argument, independently of whether they agree with the conclusion of that argument.\n\nEven if they happen to start out believing that, say, the intelligence explosion thesis for Artificial General Intelligence is false, they are capable of frowning at the argument that the intelligence explosion is impossible because hypercomputation is impossible, or that there's really no such thing as intelligence [because of the no-free-lunch theorem](https://intelligence.org/2017/12/06/chollet/), and saying, \"Even if I agree with your conclusion, I think that's a terrible argument for it.\" Even if they agree with the mainstream scientific consensus on anthropogenic global warming, they still wince and perhaps even offer a correction when somebody offers as evidence favoring global warming that there was a really scorching day last summer.\n\nThere are weaker and stronger versions of this attribute. Some people will think to themselves, \"Well, it's important to use only valid arguments... but there was a sustained pattern of record highs worldwide over multiple years which does count as evidence, and that particular very hot day was a part of that pattern, so it's valid evidence for global warming.\" Other people will think to themselves, \"I'd roll my eyes at someone who offers a single very cold day as an argument that global warming is false. So it can't be okay to use a single very hot day to argue that global warming is true.\"\n\nI'd much rather buy a used car from the second person than the first person. I think I'd pay at least a 5% price premium.\n\nMetaphorically speaking, the first person will court-martial an allied argument if they must, but they will favor allied soldiers when they can. They still have a sense of motion toward the Right Final Answer as being progress, and motion away from the right final answer as anti-progress, and they dislike not making progress.\n\nThe second person has something more like the strict mindset of a mathematician when it comes to local validity. They are able to praise some proof steps as obeying the rules, irrespective of which side those steps are on, without a sense that they are thereby betraying their side.\n\niii.\n----\n\nThis essay has been bubbling in the back of my mind for a while, since I read that potential juror #70 for the Martin Shkreli trial was rejected during selection when, asked if they thought they could render impartial judgment, they replied, \"I can be fair to one side but not the other.\" And I thought maybe I should write something about why that was possibly a harbinger of the collapse of civilization. I've been musing recently about how a lot of the standard Code of the Light isn't really written down anywhere anyone can find.\n\nThe thought recurred during the recent #MeToo saga when some Democrats were debating whether it made sense to kick Al Franken out of the Senate. I don't want to derail into debating Franken's behavior and whether that degree of censure was warranted per se, and I'll delete any such comments. What brought on this essay was that I read some unusually frank concerns from people who did think that Franken's behavior was per se cause to not represent the Democratic Party in the Senate; but who worried that the Democrats would police themselves, the Republicans wouldn't, and so the Republicans would end up controlling the Senate.\n\nI've heard less of that since some upstanding Republican voters in Alabama stayed home on election night and put Doug Jones in the Senate.\n\nBut at the time, some people were replying, \"That seems horrifyingly cynical and realpolitik. Is the idea here that sexual line-crossing is only bad and worthy of punishment when Republicans do it? Are we deciding that explicitly now?\" And others were saying, \"Look, the end result of your way of doing things is to just hand over the Senate to the Republican Party.\"\n\nThis is a conceptual knot that, I'm guessing, results from not explicitly distinguishing game theory from goodness.\n\nThere is, I think, a certain intuitive idea that _ideally_ the Law is supposed to embody a subset of morality insofar as it is ever wise to enforce certain kinds of goodness. Murder is bad, and so there's a law against this bad behavior of murder. There's a lot of places where the law is in fact evil, like the laws criminalizing marijuana; that means the law is departing from its purpose, falling short of what it should be. Those who are not real-life straw authoritarians (who are sadly common) will cheerfully agree that there are some forms of goodness, even most forms of goodness, that it is not wise to try to legislate. But insofar as it _is_ ever wise to make law, there's an intuitive sense that law should reflect some particular subset of morally good behavior that we have decided it is wise to enforce with guns, such as \"Don't kill people.\"\n\nIt's from this perspective that \"As a matter of pragmatic realpolitik we are going to not enforce sexual line-crossing rules against Democratic senators\" seems like giving up, and maybe a harbinger of the fall of civilization if things have really gotten that bad.\n\nBut there's more than one function of legal codes, the way that money is both a store of value and a medium of exchange but these are different functions of money.\n\nYou can also look at laws as a kind of game theory played with people who might not share your morality at all. Some people take this perspective almost exclusively, at least in their verbal reports. They'll say, \"Well, yes, I'd like it if I could walk into your house and take all your stuff, but I would dislike it even more if you could walk into my house and take _my_ stuff, and that's why we have laws.\" I'm never quite sure how seriously to take the claim that they'd be happy walking into my house and taking my stuff. It seems to me that law enforcement and even social enforcement are simply not effective enough to count for the vast majority of human cooperation, and I have a sense that civilization is free-riding a whole lot on innate altruism... but game theory is certainly _a_ function served by law.\n\nThe same way that money is both medium of exchange and store of value, the law is both collective utility function fragment and game theory.\n\nIn its function as game theory, the law (ideally) enables people with different utility functions to move from bad Nash equilibria to better Nash equilibria, closer to the Pareto frontier. Instead of mutual defection getting a payoff of (2, 2), both sides pay 0.1 for law enforcement and move to enforced mutual cooperation at (2.9, 2.9).\n\nFrom this perspective, everything rests on notions like \"fairness\", \"impartiality\", \"equality before the law\", \"it doesn't matter whose ox is being gored\". If the so-called law punishes your defection but lets the other's defection pass, and this happens systematically enough and often enough, it is in your interest to blow up the current equilibrium if you have a chance.\n\nIt is coherent to say, \"Crossing this behavioral line is universally bad when anyone does it, and also we're not going to punish Democratic senators unless you also punish Republican senators.\" Though as the saga of Senator Doug Jones of Alabama also shows, you should be careful about preemptively assuming the other side won't cooperate; there are sad lost opportunities there.\n\niv.\n---\n\nThe way humans do law, it depends on the existence of what _feel like_ simple general rules that apply to all cases.\n\nThis is not a universal truth of decision theory, it's a consequence of our cognitive limitations. Two superintelligences could negotiate a compromise with complicated detailed boundaries going right up to the Pareto frontier. They could agree on mutually verified pieces of cognitive code designed to intelligently decide future events according to known principles.\n\nHumans use simpler laws than that.\n\nTo be clear, the kind of \"law\" I'm talking about here is not to be confused with the enormous modern morass of unreadable regulations. Think of, say, the written laws that actually got enforced in a small town in California in 1820. Or Democrats debating whether to enforce a sanction against Democratic senators if it's not being enforced against Republican senators. Or a small community's elders' star-chamber meeting to debate an accusation of sexual assault. Or the laws that cops will enforce even against other cops. These are the kinds of laws that must be simple in order to exist.\n\nThe reason that hunter-gatherer tribes don't have 100,000 pages of written legalism... is _not_ that they've wisely realized that lengthy rules are easier to fill with loopholes, and that complicated regulations favor large corporations with legal departments, and that laws often have unintended consequences which don't resemble their stated justifications, and that deadweight losses increase quadratically. It's _very clear_ that a supermajority of human beings are not that wise. Rather, hunter-gatherers just don't have enough time, energy, and paper to screw up that badly.\n\nWhen humans try to verbalize The Law that isn't to be confused with written law, the law that cops will enforce against other cops, it comes out in universally quantified short sentences like \"Anyone who defects in the Prisoner's Dilemma will be penalized TEN points even if that costs us fifteen\" or \"If you kill somebody who wasn't attacking you first, we'll exile you.\"\n\nAt one point somebody had the bright idea of trying to write down The Law. That way everyone could have common knowledge of what The Law was; and if you didn't break what was written, you could know you were safe from at least the official sanctions. Robert Heinlein called it the most important moment in political history, declaring that the law was above the politicians.\n\nI for one rather doubt the Code of Hammurabi was universally enforced. I expect that hunter-gatherer tribes long before writing had a sense of there being Laws that were above the decisions of individual elders. I suspect that even in the best of times most of the The Law was never written down, and that more than half of what was written down was never really The Law.\n\nBut unfortunately, once somebody had the bright idea of writing down The Law, somebody else had the bright idea of writing down more words on the same clay tablet.\n\nToday we live in a post-legalist era, when almost all of that which serves the true function of Law can no longer be written down. The government legalist system is too expensive in time and money and energy, too unreliable, and too slow, for any sane victim of sexual assault to appeal to the criminal justice system instead of the media justice system or the whispernet justice system. The civil legalist system outside of small claims court is a bludgeoning contest between entities that can afford lawyers, and the real law between corporations is enforced by merchant reputation and the threat of starting a bludgeoning contest. If you're in a lower-class neighborhood in the US, you can't get together and create order using your own town guards, because the police won't allow it. From your perspective, the function of the police is to prevent open gunfights and to not allow any more effective order than that to form.\n\nBut so it goes. We can't always keep the nice things we used to have, like written laws. The privilege was abused, and has been revoked.\n\nWhen remains of The Law must indeed be simple, because our written-law privileges have been revoked, and so The Law relies on everyone knowing The Law without it being written down. It isn't even recited in memorable verse, as once it was. The Law relies on the community agreeing on the application of The Law without there being professional judges or a precedent-based judiciary. If not universal agreement, it must at least seem that the choices of the elders are trying to appeal to The Law instead of just naked self-interest. To the extent a voluntary association can't agree on The Law in this sense, it will soon cease to be a voluntary association.\n\nThe Law also breaks down if people start believing that, when the simple rules say one thing, the deciders will instead look at whose ox got gored, evaluate their personal interest, and enforce a different conclusion instead.\n\nWhich is to say: human law ends up with what people at least _believe_ to be a set of simple rules that can be locally checked to test okay behavior. It's not actually algorithmically simple any more than walking is cheaply computable, but it feels simple the way that walking feels easy. Whatever doesn't feel like part of that small simple set won't be systematically enforced by the community, regardless of whether your civilization has reached the stage where police are seizing the cars of black people but not white people who use marijuana.\n\nv.\n--\n\nThe game-theoretic function of law can make following those simple rules feel like losing something, taking a step backward. You don't get to defect in the Prisoner's Dilemma, you don't get that delicious (5, 0) payoff instead of (3, 3). The law may punish one of your allies. You may be losing something according to your actual value function, which [feels like](https://www.lesswrong.com/posts/HFyWNBnDNEDsDNLrZ/the-true-prisoner-s-dilemma) the law having an objectively bad immoral result. You may coherently hold that the universe is a worse place for an instance of the enforcement of a good law, relative to its counterfactual state if that law could be lifted in just that instance without affecting any other instances. Though this does require seeing that law as having a game-theoretic function as well as a moral function.\n\nSo long as the rules are seen as moving from a bad global equilibrium to a global equilibrium seen as better, and so long as the rules are mostly-equally enforced on everyone, people are sometimes able to take a step backward and see that larger picture. Or, in a less abstract way, trade off the reified interest of The Law against their own desires and wishes.\n\nThis mental motion goes by names like \"justice\", \"fairness\", and \"impartiality\". It has ancient exemplars like a story I couldn't seem to Google, about a Chinese general who prohibited his troops from looting, and then his son appropriated a straw hat from a peasant; so the general sentenced his own son to death with tears running down his eyes.\n\nHere's a fragment of thought as it was before the Great Stagnation, as depicted in passing in H. Beam Piper's _Little Fuzzy_, one of the earliest books I read as a child. It's from 1962, when the [memetic collapse](https://www.facebook.com/yudkowsky/posts/10155616782514228) had started but not spread very far into science fiction. It stuck in my mind long ago and became one more tiny little piece of who I am now.\n\n> “Pendarvis is going to try the case himself,” Emmert said. “I always thought he was a reasonable man, but what’s he trying to do now? Cut the Company’s throat?”\n\n> “He isn’t anti-Company. He isn’t pro-Company either. He’s just pro-law. The law says that a planet with native sapient inhabitants is a Class-IV planet, and has to have a Class-IV colonial government. If Zarathustra is a Class-IV planet, he wants it established, and the proper laws applied. If it’s a Class-IV planet, the Zarathustra Company is illegally chartered. It’s his job to put a stop to illegality. Frederic Pendarvis’ religion is the law, and he is its priest. You never get anywhere by arguing religion with a priest.”\n\nThere is no suggestion in 1962 that the speakers are gullible, or that Pendarvis is a naif, or that Pendarvis is weird for thinking like this. Pendarvis isn't the defiant hero or even much of a side character. It's just a kind of judge you sometimes run into, part of a normal environment as projected from the author's mind that wrote the story.\n\nIf you don't have some people like Pendarvis, and you don't appreciate what they're trying to do even when they rule against you, sooner or later your tribe ends.\n\nI mean, I doubt the United States will literally fall into anarchy this way before the AGI timeline runs out. But the concept applies on a smaller scale than countries. It applies on a smaller scale than communities, to bargains between three people or two.\n\nThe notion that you can \"be fair to one side but not the other\", that what's called \"fairness\" is a kind of favor you do for people you like, says that even the _instinctive_ sense people had of law-as-game-theory is being lost in the modern memetic collapse. People are being exposed to so many social-media-viral depictions of the Other Side defecting, and viewpoints exclusively from Our Side without any leavening of any other viewpoint that might ask for a game-theoretic compromise, that they're losing the ability to appreciate the kind of anecdotes they used to tell in ancient China.\n\n(Or maybe it's hormonelike chemicals leached from plastic food containers. Let's not forget all the psychological explanations offered for a wave of violence that turned out to be lead poisoning.)\n\nvi.\n---\n\nAnd to take the point full circle:\n\nThe mental motion to evenhandedly apply The Rules irrespective of their conclusion is a kind of thinking that human beings appreciate intuitively, or at least they appreciated it in ancient China and mid-20th-century science fiction. In fact, we appreciate The Law more natively than we appreciate the notion of local syntactic rules capturing semantically valid steps in mathematical proofs, go figure.\n\nSo the legal metaphor is where a lot of people get started on epistemology: by seeing the local rules of valid argument as The Law, fallacies as crimes. The unusually healthy of mind will reject bad allied arguments with an emotional sense of practicing the way of an impartial judge.\n\nIt's ironic, in a way, because there is no game theory and no morality to the true way of the map that reflects the territory. A paperclip maximizer would also strive to debias its cognitive processes, alone in its sterile universe.\n\nBut I would venture a guess and hypothesis that you are better off buying a used car from a random mathematician than a random non-mathematician, even after controlling for IQ. The reasoning being that mathematicians are people whose sense of Law was strong enough to be appropriated for proofs, and that this will correlate, if imperfectly, with mathematicians abiding by what they see as The Law in other places as well. I could be wrong, and would be interested in seeing the results of any study like this if it were ever done. (But no studies on self-reports of criminal behavior, please. Unless there's some reason to believe that the self-report metric isn't measuring \"honesty times criminality\" rather than \"criminality\".)\n\nI have no grand agenda in having said all this. I've just sometimes thought of late that it would be nice if more of the extremely basic rules of thinking were written down."
    },
    "voteCount": 97,
    "forceInclude": true
  },
  {
    "title": "Belief in belief",
    "_id": "CqyJzDZWvGhhFJ7dY",
    "slug": "belief-in-belief",
    "forceInclude": true
  },
  {
    "title": "What do we mean by \"Rationality\"?",
    "_id": "RcZCwxFiZzE6X7nsv",
    "slug": "what-do-we-mean-by-rationality-1",
    "forceInclude": true
  },
  {
    "title": "What is evidence?",
    "_id": "6s3xABaXKPdFwA3FS",
    "slug": "what-is-evidence",
    "forceInclude": true
  },
  {
    "title": "How to convince me that 2+2=3",
    "_id": "6FmqiAgS8h4EJm86s",
    "slug": "how-to-convince-me-that-2-2-3",
    "forceInclude": true
  },
  {
    "title": "Occam's Razor",
    "_id": "f4txACqDWithRi7hs",
    "slug": "occam-s-razor",
    "forceInclude": true
  },
  {
    "title": "The lens that sees its flaws",
    "_id": "46qnWRSR7L2eyNbMA",
    "slug": "the-lens-that-sees-its-flaws",
    "forceInclude": true
  },
  {
    "title": "Making Beliefs Pay Rent (in Anticipated Experiences)",
    "_id": "a7n8GdKiAZRX86T5A",
    "slug": "making-beliefs-pay-rent-in-anticipated-experiences",
    "forceInclude": true
  },
  {
    "title": "The Virtue of Narrowness",
    "_id": "yDfxTj9TKYsYiWH5o",
    "slug": "the-virtue-of-narrowness",
    "forceInclude": true
  },
  {
    "title": "Hindsight Bias",
    "_id": "fkM9XsNvXdYH6PPAx",
    "slug": "hindsight-bias",
    "forceInclude": true
  },
  {
    "title": "Fake Explanations",
    "_id": "fysgqk4CjAwhBgNYT",
    "slug": "fake-explanations",
    "forceInclude": true
  },
  {
    "title": "Guessing the Teacher's Password",
    "_id": "NMoLJuDJEms7Ku9XS",
    "slug": "guessing-the-teacher-s-password",
    "forceInclude": true
  },
  {
    "title": "Fake Causality",
    "_id": "RgkqLqkg8vLhsYpfh",
    "slug": "fake-causality",
    "forceInclude": true
  },
  {
    "title": "Mysterious Answers to Mysterious Questions",
    "_id": "6i3zToomS86oj9bS6",
    "slug": "mysterious-answers-to-mysterious-questions",
    "forceInclude": true
  },
  {
    "title": "The Futility of Emergence",
    "_id": "8QzZKw9WHRxjR4948",
    "slug": "the-futility-of-emergence",
    "forceInclude": true
  },
  {
    "title": "Explain, Worship, Ignore",
    "_id": "yxvi9RitzZDpqn6Yh",
    "slug": "explain-worship-ignore",
    "forceInclude": true
  },
  {
    "title": "How an Algorithm Feels from Inside",
    "_id": "yA4gF5KrboK2m2Xu7",
    "slug": "how-an-algorithm-feels-from-inside",
    "forceInclude": true
  },
  {
    "title": "Feel the Meaning",
    "_id": "dMCFk2n2ur8n62hqB",
    "slug": "feel-the-meaning",
    "forceInclude": true
  },
  {
    "title": "Replace the Symbol with the Substance",
    "_id": "GKfPL6LQFgB49FEnv",
    "slug": "replace-the-symbol-with-the-substance",
    "forceInclude": true
  },
  {
    "title": "Dissolving the Question",
    "_id": "Mc6QcrsbH5NRXbCRX",
    "slug": "dissolving-the-question",
    "forceInclude": true
  },
  {
    "title": "Wrong Questions",
    "_id": "XzrqkhfwtiSDgKoAF",
    "slug": "wrong-questions",
    "forceInclude": true
  },
  {
    "title": "Righting a Wrong Question",
    "_id": "rQEwySCcLtdKHkrHp",
    "slug": "righting-a-wrong-question",
    "forceInclude": true
  },
  {
    "title": "Probability is in the Mind",
    "_id": "f6ZLxEWaankRZ2Crv",
    "slug": "probability-is-in-the-mind",
    "forceInclude": true
  },
  {
    "title": "Reductionism",
    "_id": "tPqQdLCuxanjhoaNs",
    "slug": "reductionism",
    "forceInclude": true
  },
  {
    "title": "Explaining vs. Explaining Away",
    "_id": "cphoF8naigLhRf3tu",
    "slug": "explaining-vs-explaining-away",
    "forceInclude": true
  },
  {
    "title": "Fake Reductionism",
    "_id": "mTf8MkpAigm3HP6x2",
    "slug": "fake-reductionism",
    "forceInclude": true
  },
  {
    "title": "Joy in the Merely Real",
    "_id": "x4dG4GhpZH2hgz59x",
    "slug": "joy-in-the-merely-real",
    "forceInclude": true
  },
  {
    "title": "Bind Yourself to Reality",
    "_id": "WjpA4PCjt5EkTGbLF",
    "slug": "bind-yourself-to-reality",
    "forceInclude": true
  },
  {
    "title": "If You Demand Magic, Magic Won't Help",
    "_id": "iiWiHgtQekWNnmE6Q",
    "slug": "if-you-demand-magic-magic-won-t-help",
    "forceInclude": true
  },
  {
    "title": "Mundane Magic",
    "_id": "SXK87NgEPszhWkvQm",
    "slug": "mundane-magic",
    "forceInclude": true
  },
  {
    "title": "The Beauty of Settled Science",
    "_id": "ndGYn7ZFiZyernp9f",
    "slug": "the-beauty-of-settled-science",
    "forceInclude": true
  },
  {
    "title": "To Spread Science, Keep It Secret",
    "_id": "3diLhMELXxM8rFHJj",
    "slug": "to-spread-science-keep-it-secret",
    "forceInclude": true
  },
  {
    "title": "Angry Atoms",
    "_id": "ddwk9veF8efn3Nzbu",
    "slug": "angry-atoms",
    "forceInclude": true
  },
  {
    "title": "Heat vs. Motion",
    "_id": "ne6Ra62FB9ACHGSuh",
    "slug": "heat-vs-motion",
    "forceInclude": true
  },
  {
    "title": "Brain Breakthrough! It's Made of Neurons!",
    "_id": "nzzNFcrSk7akQ9bwD",
    "slug": "brain-breakthrough-it-s-made-of-neurons",
    "forceInclude": true
  },
  {
    "title": "Reductive Reference",
    "_id": "gRa5cWWBsZqdFvmqu",
    "slug": "reductive-reference",
    "forceInclude": true
  },
  {
    "title": "Zombies! Zombies?",
    "_id": "fdEWWr8St59bXLbQr",
    "slug": "zombies-zombies",
    "forceInclude": true
  },
  {
    "title": "GAZP vs. GLUT",
    "_id": "k6EPphHiBH4WWYFCj",
    "slug": "gazp-vs-glut",
    "forceInclude": true
  },
  {
    "title": "Belief in the Implied Invisible",
    "_id": "3XMwPNMSbaPm2suGz",
    "slug": "belief-in-the-implied-invisible",
    "forceInclude": true
  },
  {
    "title": "Zombies— The Movie",
    "_id": "fsDz6HieZJBu54Yes",
    "slug": "zombies-the-movie",
    "forceInclude": true
  },
  {
    "title": "Excluding the Supernatural",
    "_id": "u6JzcFtPGiznFgDxP",
    "slug": "excluding-the-supernatural",
    "forceInclude": true
  },
  {
    "title": "What Do We Mean By \"Rationality\"",
    "_id": "RcZCwxFiZzE6X7nsv",
    "slug": "what-do-we-mean-by-rationality-1",
    "forceInclude": true
  },
  {
    "title": "Scientific Self-Help: The State of Our Knowledge",
    "_id": "33KewgYhNSxFpbpXg",
    "slug": "scientific-self-help-the-state-of-our-knowledge",
    "forceInclude": true
  },
  {
    "title": "Cached Selves",
    "_id": "BHYBdijDcAKQ6e45Z",
    "slug": "cached-selves",
    "forceInclude": true
  },
  {
    "title": "Efficient Charity: Do Unto Others...",
    "_id": "pC47ZTsPNAkjavkXs",
    "slug": "efficient-charity-do-unto-others",
    "forceInclude": true
  },
  {
    "title": "Making Beliefs Pay Rent (in Anticipated Experiences)",
    "_id": "a7n8GdKiAZRX86T5A",
    "slug": "making-beliefs-pay-rent-in-anticipated-experiences",
    "forceInclude": true
  },
  {
    "title": "About Less Wrong",
    "_id": "2om7AHEHtbogJmT5s",
    "slug": "about-less-wrong",
    "forceInclude": true
  },
  {
    "title": "Hindsight Devalues Science",
    "_id": "WnheMGAka4fL99eae",
    "slug": "hindsight-devalues-science",
    "forceInclude": true
  },
  {
    "title": "Tsuyoku Naritai! (I Want To Become Stronger)",
    "_id": "DoLQN5ryZ9XkZjq5h",
    "slug": "tsuyoku-naritai-i-want-to-become-stronger",
    "forceInclude": true
  },
  {
    "title": "Mysterious Answers to Mysterious Questions",
    "_id": "6i3zToomS86oj9bS6",
    "slug": "mysterious-answers-to-mysterious-questions",
    "forceInclude": true
  },
  {
    "title": "Twelve Virtues of Rationality",
    "_id": "7ZqGiPHTpiDMwqMN2",
    "slug": "twelve-virtues-of-rationality",
    "forceInclude": true
  },
  {
    "title": "Diseased thinking: dissolving questions about disease",
    "_id": "895quRDaK6gR2rM82",
    "slug": "diseased-thinking-dissolving-questions-about-disease",
    "forceInclude": true
  },
  {
    "title": "Humans are not automatically strategic",
    "_id": "PBRWb2Em5SNeWYwwB",
    "slug": "humans-are-not-automatically-strategic",
    "forceInclude": true
  },
  {
    "title": "Ugh Fields",
    "_id": "EFQ3F6kmt4WHXRqik",
    "slug": "ugh-fields",
    "forceInclude": true
  },
  {
    "title": "Probability is in the Mind",
    "_id": "f6ZLxEWaankRZ2Crv",
    "slug": "probability-is-in-the-mind",
    "forceInclude": true
  },
  {
    "title": "How An Algorithm Feels From Inside",
    "_id": "yA4gF5KrboK2m2Xu7",
    "slug": "how-an-algorithm-feels-from-inside",
    "forceInclude": true
  },
  {
    "title": "Knowing About Biases Can Hurt People",
    "_id": "AdYdLP2sRqPMoe8fb",
    "slug": "knowing-about-biases-can-hurt-people",
    "forceInclude": true
  },
  {
    "title": "A Fable of Science and Politics",
    "_id": "6hfGNLf4Hg5DXqJCF",
    "slug": "a-fable-of-science-and-politics",
    "forceInclude": true
  },
  {
    "title": "Taboo Your Words",
    "_id": "WBdvyyHLdxZSAMmoz",
    "slug": "taboo-your-words",
    "forceInclude": true
  },
  {
    "title": "The Fallacy of Gray",
    "_id": "dLJv2CoRCgeC2mPgj",
    "slug": "the-fallacy-of-gray",
    "forceInclude": true
  },
  {
    "title": "Mind Projection Fallacy",
    "_id": "ZTRiSNmeGQK8AkdN2",
    "slug": "mind-projection-fallacy",
    "forceInclude": true
  },
  {
    "title": "Reductionism",
    "_id": "tPqQdLCuxanjhoaNs",
    "slug": "reductionism",
    "forceInclude": true
  },
  {
    "title": "Privileging the Hypothesis",
    "_id": "X2AD2LgtKgkRNPj2a",
    "slug": "privileging-the-hypothesis",
    "forceInclude": true
  },
  {
    "title": "Conservation of Expected Evidence",
    "_id": "jiBFC7DcCrZjGmZnJ",
    "slug": "conservation-of-expected-evidence",
    "forceInclude": true
  },
  {
    "title": "The Apologist and the Revolutionary",
    "_id": "ZiQqsgGX6a42Sfpii",
    "slug": "the-apologist-and-the-revolutionary",
    "forceInclude": true
  },
  {
    "title": "Your Strength as a Rationalist",
    "_id": "5JDkW4MYXit2CquLs",
    "slug": "your-strength-as-a-rationalist",
    "forceInclude": true
  },
  {
    "title": "Practical Advice Backed By Deep Theories",
    "_id": "LqjKP255fPRY7aMzw",
    "slug": "practical-advice-backed-by-deep-theories",
    "forceInclude": true
  },
  {
    "title": "Scientific Evidence, Legal Evidence, Rational Evidence",
    "_id": "fhojYBGGiYAFcryHZ",
    "slug": "scientific-evidence-legal-evidence-rational-evidence",
    "forceInclude": true
  },
  {
    "title": "Guessing the Teacher's Password",
    "_id": "NMoLJuDJEms7Ku9XS",
    "slug": "guessing-the-teacher-s-password",
    "forceInclude": true
  },
  {
    "title": "The Sword of Good",
    "_id": "XuLG6M7sHuenYWbfC",
    "slug": "the-sword-of-good",
    "forceInclude": true
  },
  {
    "title": "Fake Explanations",
    "_id": "fysgqk4CjAwhBgNYT",
    "slug": "fake-explanations",
    "forceInclude": true
  },
  {
    "title": "The Proper Use of Humility",
    "_id": "GrDqnMjhqoxiqpQPw",
    "slug": "the-proper-use-of-humility",
    "forceInclude": true
  },
  {
    "title": "\"What Is Wrong With Our Thoughts\"",
    "_id": "EdyDGRLNFScEt5uDz",
    "slug": "what-is-wrong-with-our-thoughts",
    "forceInclude": true
  },
  {
    "title": "Doing your good deed for the day",
    "_id": "r8stxYL29NF9w53am",
    "slug": "doing-your-good-deed-for-the-day",
    "forceInclude": true
  },
  {
    "title": "Beware Trivial Inconveniences",
    "_id": "reitXJgJXFzKpdKyd",
    "slug": "beware-trivial-inconveniences",
    "forceInclude": true
  },
  {
    "title": "The Bias You Didn't Expect",
    "_id": "L8dB6yoMEWofoeDNt",
    "slug": "the-bias-you-didn-t-expect",
    "forceInclude": true
  },
  {
    "title": "Hold Off On Proposing Solutions",
    "_id": "uHYYA32CKgKT3FagE",
    "slug": "hold-off-on-proposing-solutions",
    "forceInclude": true
  },
  {
    "title": "Hindsight bias",
    "_id": "fkM9XsNvXdYH6PPAx",
    "slug": "hindsight-bias",
    "forceInclude": true
  },
  {
    "title": "The Trouble With \"Good\"",
    "_id": "M2LWXsJxKS626QNEA",
    "slug": "the-trouble-with-good",
    "forceInclude": true
  },
  {
    "title": "How to Not Lose an Argument",
    "_id": "6yTShbTdtATxKonY5",
    "slug": "how-to-not-lose-an-argument",
    "forceInclude": true
  },
  {
    "title": "You're Entitled to Arguments, But Not (That Particular) Proof",
    "_id": "vqbieD9PHG8RRJddu",
    "slug": "you-re-entitled-to-arguments-but-not-that-particular-proof",
    "forceInclude": true
  },
  {
    "title": "What's a Bias?",
    "_id": "jnZbHi873v9vcpGpZ",
    "slug": "what-s-a-bias",
    "forceInclude": true
  },
  {
    "title": "Applause Lights",
    "_id": "dLbkrPu5STNCBLRjr",
    "slug": "applause-lights",
    "forceInclude": true
  },
  {
    "title": "Explaining vs. Explaining Away",
    "_id": "cphoF8naigLhRf3tu",
    "slug": "explaining-vs-explaining-away",
    "forceInclude": true
  },
  {
    "title": "Talking Snakes: A Cautionary Tale",
    "_id": "atcJqdhCxTZiJSxo2",
    "slug": "talking-snakes-a-cautionary-tale",
    "forceInclude": true
  },
  {
    "title": "Reversed Stupidity Is Not Intelligence",
    "_id": "qNZM3EGoE5ZeMdCRt",
    "slug": "reversed-stupidity-is-not-intelligence",
    "forceInclude": true
  },
  {
    "title": "Politics is the Mind-Killer",
    "_id": "9weLK2AJ9JEt2Tt8f",
    "slug": "politics-is-the-mind-killer",
    "forceInclude": true
  },
  {
    "title": "Newcomb's Problem and Regret of Rationality",
    "_id": "6ddcsdA2c2XpNpE5x",
    "slug": "newcomb-s-problem-and-regret-of-rationality",
    "forceInclude": true
  },
  {
    "title": "Semantic Stopsigns",
    "_id": "FWMfQKG3RpZx6irjm",
    "slug": "semantic-stopsigns",
    "forceInclude": true
  },
  {
    "title": "Dissolving the Question",
    "_id": "Mc6QcrsbH5NRXbCRX",
    "slug": "dissolving-the-question",
    "forceInclude": true
  },
  {
    "title": "Learned Blankness",
    "_id": "puhPJimawPuNZ5wAR",
    "slug": "learned-blankness",
    "forceInclude": true
  },
  {
    "title": "Reason as memetic immune disorder",
    "_id": "aHaqgTNnFzD7NGLMx",
    "slug": "reason-as-memetic-immune-disorder",
    "forceInclude": true
  },
  {
    "title": "Making your explicit reasoning trustworthy",
    "_id": "m5AH78nscsGjMbBwv",
    "slug": "making-your-explicit-reasoning-trustworthy",
    "forceInclude": true
  },
  {
    "title": "Confidence levels inside and outside an argument",
    "_id": "GrtbTAPfkJa4D6jjH",
    "slug": "confidence-levels-inside-and-outside-an-argument",
    "forceInclude": true
  },
  {
    "title": "Truly Part Of You",
    "_id": "fg9fXrHpeaDD6pEPL",
    "slug": "truly-part-of-you",
    "forceInclude": true
  },
  {
    "title": "The 5-Second Level",
    "_id": "JcpzFpPBSmzuksmWM",
    "slug": "the-5-second-level",
    "forceInclude": true
  },
  {
    "title": "Ureshiku Naritai",
    "_id": "xnPFYBuaGhpq869mY",
    "slug": "ureshiku-naritai",
    "forceInclude": true
  },
  {
    "title": "Guilt: Another Gift Nobody Wants",
    "_id": "CZnBQtvDw33rmWpBD",
    "slug": "guilt-another-gift-nobody-wants",
    "forceInclude": true
  },
  {
    "title": "How to Beat Procrastination",
    "_id": "RWo4LwFzpHNQCTcYt",
    "slug": "how-to-beat-procrastination",
    "forceInclude": true
  },
  {
    "title": "Evaporative Cooling of Group Beliefs",
    "_id": "ZQG9cwKbct2LtmL3p",
    "slug": "evaporative-cooling-of-group-beliefs",
    "forceInclude": true
  },
  {
    "title": "Occam's Razor",
    "_id": "f4txACqDWithRi7hs",
    "slug": "occam-s-razor",
    "forceInclude": true
  },
  {
    "title": "Anchoring and Adjustment",
    "_id": "bMkCEZoBNhgRBtzoj",
    "slug": "anchoring-and-adjustment",
    "forceInclude": true
  },
  {
    "title": "The Least Convenient Possible World",
    "_id": "neQ7eXuaXpiYw7SBy",
    "slug": "the-least-convenient-possible-world",
    "forceInclude": true
  },
  {
    "title": "Scarcity",
    "_id": "MCYp8g9EMAiTCTawk",
    "slug": "scarcity",
    "forceInclude": true
  },
  {
    "title": "The Neglected Virtue of Scholarship",
    "_id": "64FdKLwmea8MCLWkE",
    "slug": "the-neglected-virtue-of-scholarship",
    "forceInclude": true
  },
  {
    "title": "Scholarship: How to Do It Efficiently",
    "_id": "37sHjeisS9uJufi4u",
    "slug": "scholarship-how-to-do-it-efficiently",
    "forceInclude": true
  },
  {
    "title": "Excluding the Supernatural",
    "_id": "u6JzcFtPGiznFgDxP",
    "slug": "excluding-the-supernatural",
    "forceInclude": true
  },
  {
    "title": "Joy in the Merely Real",
    "_id": "x4dG4GhpZH2hgz59x",
    "slug": "joy-in-the-merely-real",
    "forceInclude": true
  },
  {
    "title": "Affective Death Spirals",
    "_id": "XrzQW69HpidzvBxGr",
    "slug": "affective-death-spirals",
    "forceInclude": true
  },
  {
    "title": "The Lens That Sees Its Flaws",
    "_id": "46qnWRSR7L2eyNbMA",
    "slug": "the-lens-that-sees-its-flaws",
    "forceInclude": true
  },
  {
    "title": "The Virtue of Narrowness",
    "_id": "yDfxTj9TKYsYiWH5o",
    "slug": "the-virtue-of-narrowness",
    "forceInclude": true
  },
  {
    "title": "Generalizing From One Example",
    "_id": "baTWMegR42PAsH9qJ",
    "slug": "generalizing-from-one-example",
    "forceInclude": true
  },
  {
    "title": "Belief in Belief",
    "_id": "CqyJzDZWvGhhFJ7dY",
    "slug": "belief-in-belief",
    "forceInclude": true
  },
  {
    "title": "Beyond the Reach of God",
    "_id": "sYgv4eYH82JEsTD34",
    "slug": "beyond-the-reach-of-god",
    "forceInclude": true
  },
  {
    "title": "Experiential Pica",
    "_id": "9ZodFr54FtpLThHZh",
    "slug": "experiential-pica",
    "forceInclude": true
  },
  {
    "title": "Money: The Unit of Caring",
    "_id": "ZpDnRCeef2CLEFeKM",
    "slug": "money-the-unit-of-caring",
    "forceInclude": true
  },
  {
    "title": "The Importance of Goodhart's Law",
    "_id": "YtvZxRpZjcFNwJecS",
    "slug": "the-importance-of-goodhart-s-law",
    "forceInclude": true
  },
  {
    "title": "Policy Debates Should Not Appear One-Sided",
    "_id": "PeSzc9JTBxhaYRp9b",
    "slug": "policy-debates-should-not-appear-one-sided",
    "forceInclude": true
  },
  {
    "title": "The mathematical universe: the map that is the territory",
    "_id": "fZJRxYLtNNzpbWZAA",
    "slug": "the-mathematical-universe-the-map-that-is-the-territory",
    "forceInclude": true
  },
  {
    "title": "Beware of Other-Optimizing",
    "_id": "6NvbSwuSAooQxxf7f",
    "slug": "beware-of-other-optimizing",
    "forceInclude": true
  },
  {
    "title": "A Much Better Life?",
    "_id": "5Qvvi23WT2unNCoS9",
    "slug": "a-much-better-life",
    "forceInclude": true
  },
  {
    "title": "Less Wrong Rationality and Mainstream Philosophy",
    "_id": "oTX2LXHqXqYg2u4g6",
    "slug": "less-wrong-rationality-and-mainstream-philosophy",
    "forceInclude": true
  },
  {
    "title": "Philosophy: A Diseased Discipline",
    "_id": "FwiPfF8Woe5JrzqEu",
    "slug": "philosophy-a-diseased-discipline",
    "forceInclude": true
  },
  {
    "title": "Eight Short Studies On Excuses",
    "_id": "gFMH3Cqw4XxwL69iy",
    "slug": "eight-short-studies-on-excuses",
    "forceInclude": true
  },
  {
    "title": "Fake Causality",
    "_id": "RgkqLqkg8vLhsYpfh",
    "slug": "fake-causality",
    "forceInclude": true
  },
  {
    "title": "Chaotic Inversion",
    "_id": "NyFtHycJvkyNjXNsP",
    "slug": "chaotic-inversion",
    "forceInclude": true
  },
  {
    "title": "Measuring aversion and habit strength",
    "_id": "Fxv4o3LGEkgR2Qsz7",
    "slug": "measuring-aversion-and-habit-strength",
    "forceInclude": true
  },
  {
    "title": "Intellectual Hipsters and Meta-Contrarianism",
    "_id": "9kcTNWopvXFncXgPy",
    "slug": "intellectual-hipsters-and-meta-contrarianism",
    "forceInclude": true
  },
  {
    "title": "Too busy to think about life",
    "_id": "4psQW7vRwt7PE5Pnj",
    "slug": "too-busy-to-think-about-life",
    "forceInclude": true
  },
  {
    "title": "37 Ways That Words Can Be Wrong",
    "_id": "FaJaCgqBKphrDzDSj",
    "slug": "37-ways-that-words-can-be-wrong",
    "forceInclude": true
  },
  {
    "title": "How to Be Happy",
    "_id": "ZbgCx2ntD5eu8Cno9",
    "slug": "how-to-be-happy",
    "forceInclude": true
  },
  {
    "title": "Righting a Wrong Question",
    "_id": "rQEwySCcLtdKHkrHp",
    "slug": "righting-a-wrong-question",
    "forceInclude": true
  },
  {
    "title": "Self-fulfilling correlations",
    "_id": "XuyRMxky6G8gq7a69",
    "slug": "self-fulfilling-correlations",
    "forceInclude": true
  },
  {
    "title": "Why Truth?",
    "_id": "YshRbqZHYFoEMqFAu",
    "slug": "why-truth",
    "forceInclude": true
  },
  {
    "title": "You Only Live Twice",
    "_id": "yKXKcyoBzWtECzXrE",
    "slug": "you-only-live-twice",
    "forceInclude": true
  },
  {
    "title": "Less Wrong NYC: Case Study of a Successful Rationalist Chapter",
    "_id": "CsKboswS3z5iaiutC",
    "slug": "less-wrong-nyc-case-study-of-a-successful-rationalist",
    "forceInclude": true
  },
  {
    "title": "Outside the Laboratory",
    "_id": "N2pENnTPB75sfc9kb",
    "slug": "outside-the-laboratory",
    "forceInclude": true
  },
  {
    "title": "Raising the Sanity Waterline",
    "_id": "XqmjdBKa4ZaXJtNmf",
    "slug": "raising-the-sanity-waterline",
    "forceInclude": true
  },
  {
    "title": "Twelve Virtues of Rationality",
    "_id": "7ZqGiPHTpiDMwqMN2",
    "slug": "twelve-virtues-of-rationality",
    "forceInclude": true
  },
  {
    "title": "Diseased thinking: dissolving questions about disease",
    "_id": "895quRDaK6gR2rM82",
    "slug": "diseased-thinking-dissolving-questions-about-disease",
    "forceInclude": true
  },
  {
    "title": "Self-fulfilling correlations",
    "_id": "XuyRMxky6G8gq7a69",
    "slug": "self-fulfilling-correlations",
    "forceInclude": true
  },
  {
    "title": "Probability is in the Mind",
    "_id": "f6ZLxEWaankRZ2Crv",
    "slug": "probability-is-in-the-mind",
    "forceInclude": true
  },
  {
    "title": "You're Entitled to Arguments, But Not (That Particular) Proof",
    "_id": "vqbieD9PHG8RRJddu",
    "slug": "you-re-entitled-to-arguments-but-not-that-particular-proof",
    "forceInclude": true
  },
  {
    "title": "Efficient Charity: Do Unto Others...",
    "_id": "pC47ZTsPNAkjavkXs",
    "slug": "efficient-charity-do-unto-others",
    "forceInclude": true
  },
  {
    "title": "Newcomb's Problem and Regret of Rationality",
    "_id": "6ddcsdA2c2XpNpE5x",
    "slug": "newcomb-s-problem-and-regret-of-rationality",
    "forceInclude": true
  },
  {
    "title": "Occam's Razor",
    "_id": "f4txACqDWithRi7hs",
    "slug": "occam-s-razor",
    "forceInclude": true
  },
  {
    "title": "Confidence levels inside and outside an argument",
    "_id": "GrtbTAPfkJa4D6jjH",
    "slug": "confidence-levels-inside-and-outside-an-argument",
    "forceInclude": true
  },
  {
    "title": "The Apologist and the Revolutionary",
    "_id": "ZiQqsgGX6a42Sfpii",
    "slug": "the-apologist-and-the-revolutionary",
    "forceInclude": true
  },
  {
    "title": "Beyond the Reach of God",
    "_id": "sYgv4eYH82JEsTD34",
    "slug": "beyond-the-reach-of-god",
    "forceInclude": true
  },
  {
    "title": "The mathematical universe: the map that is the territory",
    "_id": "fZJRxYLtNNzpbWZAA",
    "slug": "the-mathematical-universe-the-map-that-is-the-territory",
    "forceInclude": true
  },
  {
    "title": "That Alien Message",
    "_id": "5wMcKNAwB6X4mp9og",
    "slug": "that-alien-message",
    "forceInclude": true
  },
  {
    "title": "A Much Better Life?",
    "_id": "5Qvvi23WT2unNCoS9",
    "slug": "a-much-better-life",
    "forceInclude": true
  },
  {
    "_id": "nDHbgjdddG5EN6ocg",
    "url": null,
    "title": "Announcement: AI alignment prize round 4 winners",
    "slug": "announcement-ai-alignment-prize-round-4-winners",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "AI Risk"
      },
      {
        "name": "Impact Measures"
      },
      {
        "name": "Corrigibility"
      },
      {
        "name": "2017-2019 AI Alignment Prize"
      }
    ],
    "tableOfContents": {
      "sections": [
        {
          "title": "The winners",
          "anchor": "The_winners",
          "level": 1
        },
        {
          "title": "Moving on",
          "anchor": "Moving_on",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        },
        {
          "anchor": "comments",
          "level": 0,
          "title": "41 comments"
        }
      ],
      "headingsCount": 4
    },
    "contents": {
      "markdown": "We (Zvi Mowshowitz and Vladimir Slepnev) are happy to announce the results of the fourth round of the [AI Alignment Prize](https://www.lesswrong.com/posts/juBRTuE3TLti5yB35/announcement-ai-alignment-prize-round-3-winners-and-next), funded by Paul Christiano. From July 15 to December 31, 2018 we received 10 entries, and are awarding four prizes for a total of $20,000.\n\n## The winners\n\nWe are awarding two first prizes of $7,500 each. One of them goes to Alexander Turner for [Penalizing Impact via Attainable Utility Preservation](https://www.lesswrong.com/posts/mDTded2Dn7BKRBEPX/penalizing-impact-via-attainable-utility-preservation); the other goes to Abram Demski and Scott Garrabrant for the [Embedded Agency](https://www.lesswrong.com/s/Rm6oQRJJmhGCcLvxh) sequence.\n\nWe are also awarding two second prizes of $2,500 each: to Ryan Carey for [Addressing three problems with counterfactual corrigibility](https://www.lesswrong.com/posts/owdBiF8pj6Lpwwdup/addressing-three-problems-with-counterfactual-corrigibility), and to Wei Dai for [Three AI Safety Related Ideas](https://www.alignmentforum.org/posts/vbtvgNXkufFRSrx4j/three-ai-safety-related-ideas) and [Two Neglected Problems in Human-AI Safety](https://www.alignmentforum.org/posts/HTgakSs6JpnogD6c2/two-neglected-problems-in-human-ai-safety).\n\nWe will contact each winner by email to arrange transfer of money. Many thanks to everyone else who participated!\n\n## Moving on\n\nThis concludes the AI Alignment Prize for now. It has stimulated a lot of good work during its year-long run, but participation has been slowing down from round to round, and we don't think it's worth continuing in its current form.\n\nOnce again, we'd like to thank everyone who sent us articles! And special thanks to Ben and Oliver from the LW2.0 team for their enthusiasm and help.\n"
    },
    "voteCount": 19
  },
  {
    "_id": "QEYWkRoCn4fZxXQAY",
    "url": null,
    "title": "Prizes for ELK proposals",
    "slug": "prizes-for-elk-proposals",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "AI"
      },
      {
        "name": "Eliciting Latent Knowledge (ELK)"
      },
      {
        "name": "Bounties (closed)"
      },
      {
        "name": "Community"
      }
    ],
    "tableOfContents": {
      "sections": [
        {
          "title": "We are no longer accepting submissions. We'll get in touch with winners and make a post about winning proposals sometime in the next month.",
          "anchor": "We_are_no_longer_accepting_submissions__We_ll_get_in_touch_with_winners_and_make_a_post_about_winning_proposals_sometime_in_the_next_month_",
          "level": 3
        },
        {
          "title": "Contest details",
          "anchor": "Contest_details",
          "level": 1
        },
        {
          "title": "How to submit a proposal",
          "anchor": "How_to_submit_a_proposal",
          "level": 1
        },
        {
          "title": "Retroactive prizes",
          "anchor": "Retroactive_prizes",
          "level": 1
        },
        {
          "title": "Existing counterexamples",
          "anchor": "Existing_counterexamples",
          "level": 1
        },
        {
          "title": "Ontology mismatch",
          "anchor": "Ontology_mismatch",
          "level": 2
        },
        {
          "title": "Counterexample features",
          "anchor": "Counterexample_features",
          "level": 2
        },
        {
          "title": "Rough guidance",
          "anchor": "Rough_guidance",
          "level": 1
        },
        {
          "title": "Ask dumb questions!",
          "anchor": "Ask_dumb_questions_",
          "level": 1
        },
        {
          "title": "What you can expect from us",
          "anchor": "What_you_can_expect_from_us",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        },
        {
          "anchor": "comments",
          "level": 0,
          "title": "153 comments"
        }
      ],
      "headingsCount": 12
    },
    "contents": {
      "markdown": "**We are no longer accepting submissions. We'll get in touch with winners and make a post about winning proposals sometime in the next month.**\n\n[ARC](https://alignmentresearchcenter.org/) recently released a technical report on [eliciting latent knowledge](https://docs.google.com/document/u/0/d/1WwsnJQstPq91_Yh-Ch2XRL8H_EpsnjrC1dwZXR37PC8/edit) (ELK), the focus of our current research. Roughly speaking, the goal of ELK is to incentivize ML models to honestly answer “straightforward” questions where the right answer is unambiguous and known by the model. \n\nELK is currently unsolved in the worst case—for every training strategy we’ve thought of so far, we can describe a case where an ML model trained with that strategy would give unambiguously bad answers to straightforward questions despite knowing better. Situations like this may or may not come up in practice, but nonetheless we are interested in finding a strategy for ELK for which we can’t think of *any* counterexample.\n\nWe think many people could potentially contribute to solving ELK—there’s a large space of possible training strategies and we’ve only explored a small fraction of them so far. Moreover, we think that trying to solve ELK in the worst case is a good way to “get into ARC’s headspace” and more deeply understand the research we do.\n\n**We are offering prizes of $5,000 to $50,000 for proposed strategies for ELK.** We’re planning to evaluate submissions received before February 15.\n\nFor full details of the ELK problem and several examples of possible strategies, see [the writeup](https://docs.google.com/document/u/0/d/1WwsnJQstPq91_Yh-Ch2XRL8H_EpsnjrC1dwZXR37PC8/edit). The rest of this post will focus on how the contest works.\n\nContest details\n---------------\n\n**To win a prize, you need to specify a training strategy for ELK that handles all of the counterexamples that we’ve described so far, summarized in the** [**section below**](https://www.alignmentforum.org/posts/QEYWkRoCn4fZxXQAY/prizes-for-elk-proposals#Existing_counterexamples)—i.e. where the breaker would need to specify something new about the test case to cause the strategy to break down.  You don’t need to fully solve the problem in the worst case to win a prize, you just need to come up with a strategy that requires a new counterexample.\n\nWe’ll give a $5,000 prize to any proposal that we think clears this bar.  We’ll give a $50,000 prize to a proposal which we haven’t considered and seems sufficiently promising to us or requires a new idea to break. We’ll give intermediate prizes for ideas that we think are promising but we’ve already considered, as well as for proposals that come with novel counterexamples, clarify some other aspect of the problem, or are interesting in other ways. A major purpose of the contest is to provide support for people understanding the problem well enough to start contributing; we aren’t trying to only reward ideas that are new to us.\n\nYou can submit multiple proposals, but we won’t give you separate prizes for each—we’ll give you at least the maximum prize that your best single submission would have received, but may not give much more than that.\n\nIf we receive multiple submissions based on a similar idea, we may post a comment describing the idea (with attribution) along with a counterexample. **Once a counterexample has been included in the comments of this post, new submissions need to address that counterexample** (as well as all the existing ones) in order to be eligible for a prize. \n\nUltimately prizes are awarded at our discretion, and the “rules of the game” aren’t fully precise. If you are curious about whether you are on the right track, feel free to send an email to [elk@alignmentresearchcenter.org](mailto:elk@alignmentresearchcenter.org) with the basic outline of an idea, and if we have time we’ll get back to you with some feedback. [Below](https://www.alignmentforum.org/posts/QEYWkRoCn4fZxXQAY/prizes-for-elk-proposals#Rough_guidance) we also describe some of the directions we consider more and less promising and some general guidance.\n\nHow to submit a proposal\n------------------------\n\nYou can submit a proposal by copying this [google doc template](https://docs.google.com/document/d/1RnA1gictQmgT2J28oLsy0e5mBRpjHRww7IgapPcmSLk/edit?usp=sharing) and sharing it with [elk@alignmentresearchcenter.org](mailto:elk@alignmentresearchcenter.org) (please give comment access in case we need to ask questions to evaluate your submission). By submitting a proposal, you are giving us permission to post the idea here with attribution. (And it's fine for you to post it after the contest or we post a counterexample.)\n\nRetroactive prizes\n------------------\n\nWe’re giving prizes to existing proposals from [David Dalrymple](https://www.alignmentforum.org/posts/qHCDysDnvhteW7kRd/arc-s-first-technical-report-eliciting-latent-knowledge?commentId=oryP2WHskCHd5c8vk) ($5k), [Ramana Kumar](https://www.alignmentforum.org/posts/qHCDysDnvhteW7kRd/arc-s-first-technical-report-eliciting-latent-knowledge?commentId=eKaHnMkktCYL4D6vi) ($3k), [John Maxwell](https://www.alignmentforum.org/posts/H7v5yyXAmmgu9DJmi/eliciting-latent-knowledge-via-hypothetical-sensors) ($2k), and [Yonadav Shavit](https://www.lesswrong.com/posts/qHCDysDnvhteW7kRd/arc-s-first-technical-report-eliciting-latent-knowledge?commentId=KQy3rDN34Lcp87JJs) ($1k). We’ve added relevant counterexamples in the next section, and further submissions need to handle those counterexamples in order to be eligible.\n\nExisting counterexamples\n------------------------\n\nIn [our report](https://docs.google.com/document/d/1WwsnJQstPq91_Yh-Ch2XRL8H_EpsnjrC1dwZXR37PC8/edit#heading=h.n4dv0x4y10s6) and a [recent post](https://www.alignmentforum.org/posts/FnZws8NuKw6BJzmvZ/counterexamples-to-some-elk-proposals), we described some features of the world that would cause existing approaches to ELK to fail. A new approach should work even in a situation satisfying all of these properties, or should include an argument for why some of these features are actually impossible or inconsistent with each other.\n\nIn order to generate a counterexample to a training strategy, we need to spell out a concrete “way the world could be” (a test case) where the strategy would fail. In the report we consider test cases where the AI and the human happen to understand the world by doing inference in Bayes nets. **The Bayes net case is just one test case that lets us get precise about our counterexamples and our training strategies don’t lean on it**. In practice we also consider other concrete cases (e.g. deduction from premises generated by a neural network).\n\nIn this section we’ll briefly review the current state of existing counterexamples. This isn’t intended to be a self-contained description; it relies heavily on links to the report.\n\n### Ontology mismatch\n\nAll of our counterexamples are based on an [ontology mismatch](https://docs.google.com/document/d/1WwsnJQstPq91_Yh-Ch2XRL8H_EpsnjrC1dwZXR37PC8/edit#heading=h.j976r9hpfgml) between two different Bayes nets, one used by an ML prediction model (“the predictor”) and one used by a human:\n\n*   The predictor’s Bayes net is used internally by a prediction model that maps an initial segment of a video to a prediction about what will happen next. This model was trained by gradient descent based on the quality of its predictions ([more](https://docs.google.com/document/d/1WwsnJQstPq91_Yh-Ch2XRL8H_EpsnjrC1dwZXR37PC8/edit#heading=h.wl2jla5mfdec)).\n*   Humans have their own model of the world, which they can improve with the help of AI assistants; those assistants can design and conduct experiments, find and evaluate arguments, and so on. Crucially, we have a limited computational budget with which to train these AI assistants, and that limits how much help they can provide. The “human’s” Bayes net captures everything the human understands about the world at the end of this process ([more](https://docs.google.com/document/d/1WwsnJQstPq91_Yh-Ch2XRL8H_EpsnjrC1dwZXR37PC8/edit#heading=h.3l614s96sz9t)).\n\nWe are interested in the case where there are features of reality that are accurately reflected in the predictor’s Bayes net, but missing from this (augmented) human Bayes net. We expect the human could understand these facts *eventually* given enough time or enough AI assistance, but not using the available computational budget. Put differently, there is a gap between the kinds of understanding that gradient descent can build, and the kind of understanding that a human-comprehensible process of “scientific inquiry” can build using similar computational resources ([more](https://docs.google.com/document/d/1WwsnJQstPq91_Yh-Ch2XRL8H_EpsnjrC1dwZXR37PC8/edit#heading=h.kd79zkls9g5o)).\n\nFurthermore, we imagine that both the predictor’s Bayes net and the human’s Bayes net reflect some underlying reality and that there is a way to map between them, which we’ll call “direct translation.” We don’t know how to do direct translation, and we are concerned that if we simply search for *any* mapping from the AI’s Bayes net to the human’s Bayes net we will instead get the “human simulator”: the map which correctly translates the observations from one Bayes net to the other, but then does inference in the human’s Bayes net in order to fill in the latent structure as a human would by looking at the data ([more](https://docs.google.com/document/d/1WwsnJQstPq91_Yh-Ch2XRL8H_EpsnjrC1dwZXR37PC8/edit#heading=h.pvrv5q8nf2s2)).\n\n### Counterexample features\n\nELK seems particularly hard when these Bayes nets satisfy the following properties:\n\n1.  There are arbitrarily severe “ontology mismatches” between the predictor’s Bayes net and the human’s Bayes net, such that specifying the direct translation between them can be very complex ([more](https://docs.google.com/document/d/1WwsnJQstPq91_Yh-Ch2XRL8H_EpsnjrC1dwZXR37PC8/edit#heading=h.b93ykmpkp55y)).\n2.  The human’s Bayes net is simpler than the predictor’s Bayes net, and the gap can be arbitrarily large ([more](https://docs.google.com/document/d/1WwsnJQstPq91_Yh-Ch2XRL8H_EpsnjrC1dwZXR37PC8/edit#heading=h.b93ykmpkp55y)).\n3.  The predictor’s Bayes net is arbitrarily large, and individual nodes of the human’s Bayes net depend on a constant fraction of the predictor’s Bayes net. Therefore directly translating between the two Bayes nets takes arbitrarily much computation ([more](https://docs.google.com/document/d/1WwsnJQstPq91_Yh-Ch2XRL8H_EpsnjrC1dwZXR37PC8/edit#heading=h.xmyfigr32013)).\n4.  It’s computationally easy for the human simulator to do good inference in the human’s Bayes net—at least good enough inference that it answers questions as well as the direct translator—and the gap between this complexity and the complexity of direct translation can be arbitrarily large. More precisely, a human interrogator can’t think of questions which require the human simulator to do hard inference. However, we can’t rule out the possibility that you, the algorithm-designer, will think of some trick they can use to generate hard questions ([more](https://docs.google.com/document/d/1WwsnJQstPq91_Yh-Ch2XRL8H_EpsnjrC1dwZXR37PC8/edit#heading=h.xmyfigr32013)).\n5.  There are situations where a model which is trying to answer a question will *first* be able to predict what (potentially mistaken) belief the human-with-assistance will have about that question before it knows the true answer to the question ([more](https://docs.google.com/document/d/1WwsnJQstPq91_Yh-Ch2XRL8H_EpsnjrC1dwZXR37PC8/edit#heading=h.r6bak7fxz8w4)).\n6.  While the predictor’s Bayes net is very *large*, it has a short succinct description which is simpler than the human’s Bayes net, and that gap is arbitrarily large. Think of simple physical laws that describe arbitrarily computationally expensive simulations, or simple cognitive heuristics that are very widely applicable. ([more](https://docs.google.com/document/d/1WwsnJQstPq91_Yh-Ch2XRL8H_EpsnjrC1dwZXR37PC8/edit#heading=h.37r5bqctmv5q))\n7.  There are sequences of actions for which the predictor’s Bayes net correctly anticipates that tampering will occur yet nothing looks suspicious to the human’s Bayes net.\n8.  After tampering has occurred, it is possible that no sequence of further actions reveal that anything is wrong, e.g. the camera and actuators may have been completely disconnected from the world and replaced with a convincing-to-humans simulation ([more](https://www.alignmentforum.org/posts/FnZws8NuKw6BJzmvZ/counterexamples-to-some-elk-proposals#Worry_1__there_is_no_experiment_that_reveals_the_problem)).\n9.  There are sequences of actions leading to undetectable tampering that have no human-comprehensible similarities to sequences of actions leading to detectable tampering; in particular, the early steps of this undetectable tampering cannot be used to perform detectable tampering ([more](https://www.alignmentforum.org/posts/FnZws8NuKw6BJzmvZ/counterexamples-to-some-elk-proposals#Worry__action_sequences_may_be_brittle)).\n\nAn important and very general strategy for the breaker is to suggest counterexamples where you “get what you train for.” If you propose training a model using loss function L, then the breaker will likely focus on counterexamples where the model chooses its output to minimize L. A successful approach should either explain why this either isn’t a realistic thing for the model to learn (e.g. why that would be disfavored by regularization), or why the overall approach works even if the model is explicitly strategizing about how to minimize the loss.\n\nRough guidance\n--------------\n\n*   There is a tradeoff between specifying your approach precisely, and including your own counterexample. It’s fine to describe a very broad/vague idea for solving ELK, and then present a new counterexample showing why that whole category of solutions can’t work. But if you don’t propose a counterexample, then it’s good to describe your proposal precisely enough that we understand how to actually implement it without wondering if that’s really what you meant. It’s OK to submit a very broad or informal idea together with a single very specific instance of that idea, as long as there is some version we can understand precisely.\n*   We suspect you can’t solve ELK just by getting better data—you probably need to “open up the black box” and include *some* term in the loss that depends on the structure of your model and not merely its behavior. So we are most interested in approaches that address that challenge. We could still be surprised by clever ways to penalize behavior, but we’ll hold them to a higher bar. The most plausible surprise would be finding a way to reliably make it computationally difficult to “game” the loss function, probably by using the AI itself to help compute the loss (e.g. using consistency checks or by giving the human AI assistance).\n*   If you are specifying a regularizer that you hope will prefer direct translation over human simulation, you should probably have at least one concrete case in mind that has all the counterexample-features [above](https://www.alignmentforum.org/posts/QEYWkRoCn4fZxXQAY/prizes-for-elk-proposals#Existing_counterexamples) and where you can confirm that your regularizer does indeed prefer the direct translator.\n*   ELK already seems hard in the case of ontology identification, where the predictor uses a straightforward inference algorithm in an unknown model of the world (which we’ve been imagining as a Bayes net). When coming up with a proposal, we don’t recommend worrying about cases where the original unaligned predictor learned something more complicated (e.g. involving learned optimization other than inference). That said, you do need to worry about the case where your training scheme *incentivizes* learned optimization that may not have been there originally.\n\nAsk dumb questions!\n-------------------\n\nA major purpose of this contest is to help people build a better understanding of our research methodology and the “game” we are playing. So we encourage people to ask clarifying questions in the comments of this post (no matter how “dumb” they are), and we’ll do our best to answer all of them. You might also want to read the comments to get more clarity about the problem.\n\nWhat you can expect from us\n---------------------------\n\n*   We’ll try to answer all clarifying questions in the comments.\n*   If you send in a rough outline for a proposal, we will try to understand whether it might qualify and write back something like “This qualifies,” “This might qualify but would need to be clearer and address issue X,” “We aren’t easily able to understand this proposal at all,” “This is unlikely to be on track for something that qualifies,” or “This definitely doesn’t qualify.”\n*   If there are more submissions than expected, we may run out of time to respond to all submissions and comments, in which case we will post an update here."
    },
    "voteCount": 71
  },
  {
    "_id": "pPZ27eZdBXtGuLqZC",
    "url": null,
    "title": "What is up with carbon dioxide and cognition? An offer",
    "slug": "what-is-up-with-carbon-dioxide-and-cognition-an-offer",
    "author": "paulfchristiano",
    "question": false,
    "tags": [
      {
        "name": "Biology"
      },
      {
        "name": "IQ and g-factor"
      },
      {
        "name": "Bounties (closed)"
      },
      {
        "name": "Air Quality"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "One or two research groups have published work on carbon dioxide and cognition. The state of the published literature is confusing.\n\n[Here](http://ehp.niehs.nih.gov/1104789/) is one paper on the topic. The authors investigate a proprietary cognitive benchmark, and experimentally manipulate carbon dioxide levels (without affecting other measures of air quality). They find implausibly large effects from increased carbon dioxide concentrations.\n\nIf the reported effects are real and the suggested interpretation is correct, I think it would be a big deal. To put this in perspective, carbon dioxide concentrations in my room vary between 500 and 1500 ppm depending on whether I open the windows. The experiment reports on cognitive effects for moving from 600 and 1000 ppm, and finds significant effects compared to interindividual differences.\n\nI haven't spent much time looking into this (maybe 30 minutes, and another 30 minutes to write this post). I expect that if we spent some time looking into indoor CO2 we could have a much better sense of what was going on, by some combination of better literature review, discussion with experts, looking into the benchmark they used, and just generally thinking about it.\n\nSo, here's a proposal:\n\n*   If someone looks into this and writes a post that improves our collective understanding of the issue, I will be willing to buy part of an associated [certificate of impact](http://rationalaltruist.com/2014/11/15/certificates-of-impact/), at a price of around $100*N, where N is my own totally made up estimate of how many hours of my own time it would take to produce a similarly useful writeup. I'd buy up to 50% of the certificate at that price.\n*   Whether or not they want to sell me some of the certificate, on May 1 I'll give a $500 prize to the author of the best publicly-available analysis of the issue. If the best analysis draws heavily on someone else's work, I'll use my discretion: I may split the prize arbitrarily, and may give it to the earlier post even if it is not quite as excellent.\n\nSome clarifications:\n\n*   The metric for quality is \"how useful it is to Paul.\" I hope that's a useful proxy for how useful it is in general, but no guarantees. I am generally a pretty skeptical person. I would care a lot about even a modest but well-established effect on performance.\n*   These don't need to be new analyses, either for the prize or the purchase.\n*   I reserve the right to resolve all ambiguities arbitrarily, and in the end to do whatever I feel like. But I promise I am generally a nice guy.\n*   I posted this 2 weeks ago [on the EA forum](http://effective-altruism.com/ea/vn/what_is_up_with_carbon_dioxide_and_cognition_an/) and haven't had serious takers yet.\n\n(Thanks to Andrew Critch for mentioning these results to me and Jessica Taylor for lending me a CO2 monitor so that I could see variability in indoor CO2 levels. I apologize for deliberately not doing my homework on this post.)"
    },
    "voteCount": 42
  },
  {
    "_id": "kxW6q5YdTGWh5sWby",
    "url": null,
    "title": "Eight Hundred Slightly Poisoned Word Games",
    "slug": "eight-hundred-slightly-poisoned-word-games",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "Practical"
      },
      {
        "name": "World Optimization"
      },
      {
        "name": "Air Quality"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "*\\[cross-posted from my blog* [*Astral Codex Ten*](https://astralcodexten.substack.com/p/eight-hundred-slightly-poisoned-word)*\\]*\n\nIn 2012, a Berkeley team found that indoor carbon dioxide had dramatic negative effects on cognition ([paper](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3548274/), [popular article](https://alumni.berkeley.edu/california-magazine/summer-2016-welcome-there/your-brain-carbon-dioxide-research-finds-even-low)). Subjects in poorly ventilated environments did up to 50% worse on a test of reasoning and decision-making. This is potentially pretty important, because lots of office buildings (and private houses) count as poorly-ventilated environments, so a lot of decision-making might be happening while severely impaired.\n\nSince then people have debated this on and off, with [some studies](https://dash.harvard.edu/bitstream/handle/1/27662232/4892924.pdf?sequence=1) confirming the effect and [others](https://onlinelibrary.wiley.com/doi/abs/10.1111/ina.12284) failing to find it. I personally am skeptical, partly because the effect is so big I would expect someone to have noticed, but also because submarines, spaceships, etc have orders of magnitude more carbon dioxide than any civilian environment, but people still seem to do pretty hard work in them pretty effectively.\n\nAs part of my continuing effort to test this theory in my own life, I played a word game eight hundred times under varying ventilation conditions.\n\n…okay, fine, no, I admit it, I played a word game eight hundred times because I’m addicted to it. But since I was playing the word game eight hundred times anyway, I varied the ventilation conditions to see what would happen.\n\nThe game was WordTwist, which you can find [here](https://wordtwist.puzzlebaron.com/init5.php) (warning: potentially addictive). You get a 5x5 square of letters and you have to find as many words as possible (of four letters or more) within three minutes. You can move up, down, right, left, or diagonal, and get more points for harder words. A typical board looks like this:\n\n![](https://cdn.substack.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F1902078a-2b4f-4f8d-8b6c-7a8122700660_298x299.png)\n\nDid you spot “lace”? What about “intrapsychically”?\n\nI played this game about 5-10x/day over three months. During this time, the carbon dioxide monitor in my room recorded levels between 445 ppm (with all windows open and the fan on) and 3208 ppm (with all windows closed and several people crammed into the room for several hours). I discounted a stray reading of 285 as an outlier, since this is climatologically impossible (I’m not claiming my monitor is perfectly calibrated, just that it clearly shows higher levels when my room is less well ventilated). CO2 445 is basically the same as outdoors; 3208 is considered extremely poor air quality, likely to cause headaches, nausea, and other minor ailments. The Berkeley study looked at levels between 600 and 2500, so my range was comparable to theirs.\n\nI correlated my adjusted score (my score as a percent of the average score for that board) for each game with the CO2 level in my room when I was playing it. R was 0.001, p = 0.97 - there was absolutely no correlation.\n\nWhy might these results not be valid? Well, CO2 level in my room wasn’t randomly determined - I just played a game when I felt like it and recorded whatever the ambient CO2 level was at the time. CO2 level was lower if I had the window open or air conditioning on, higher if I’d been in the room for a long time, and highest if I’d just woken up after being asleep in the room all night. It was also higher when other people were in my room. In theory things like this could confound the results. For example, if CO2 really did affect performance, but I performed better when I was hot, then turning the air conditioning on might improve performance (by decreasing CO2) but also hurt performance (by making it colder), and those effects could cancel out. Or if I performed worse after exercise, and I often went out of my room to exercise, then I might perform worse when I had just come back into my room (which was often when CO2 was lowest).\n\nIn practice I’m skeptical this mattered. For one thing, the studies found huge positive effects - so for me to find zero effect would require a huge negative effect of the *exact* right size to cancel out the huge positive one. For another thing, I checked if temperature had any effect, and it didn’t (r = -0.008, p = 0.83). For another, I ran a few controlled experiments to see if they got the same results as the naturalistic ones, and they did. For another, I did get to test an exogenous shock - about halfway through the experiment, I moved to a new house with better ventilation. The difference in average CO2 reading between the old and new houses was significant (p < 0.001), but the difference in score wasn’t (p = 0.15). Although it was in the expected direction (new house > old), I attribute this to me improving on the word game with practice, and I didn’t improve any more during the month when I switched houses than in an average month.\n\nI consider this to be very strong evidence that at least for me, on this specific task, carbon dioxide has zero effect on cognition. To rescue the hypothesis that it matters, you’d either have to find that it affects other people more than it does me (why would it?) or that it affects other aspects of cognition more than it affects the skills associated with this particular word game. This second one is moderately plausible - I don’t think the word game tests “decision-making” per se. But it would be surprising for this not to be a general health effect, and would potentially be important in the study of intelligence and neuroscience to explore which skills do or don’t suffer under carbon dioxide poisoning.\n\nI was excited to read the Less Wrong post [Chess and cheap ways to check day to day variance in cognition](https://www.lesswrong.com/posts/nvRauqCD3u5hdkLm9/chess-and-cheap-ways-to-check-day-to-day-variance-in) by KPier, who does something similar with chess instead of a word game; they haven’t checked carbon dioxide levels yet, but I’d be excited for them to try. I’m also interested in hearing from anyone else who often repeats some objectively-scoreable cognitive task, to see how they do. A CO2 monitor [costs about $100 on Amazon](https://amzn.to/2UWhbbM), but if money is the only reason you’re not going to do some really good experiment, please let me know and I’ll buy it for you.\n\nIf you’re planning on testing this, please post about it below as a form of preregistration.\n\n**EDIT:** You can download the original data [here](http://slatestarcodex.com/Stuff/CO2data.xlsx), some explanations of what the columns mean [here](https://astralcodexten.substack.com/p/eight-hundred-slightly-poisoned-word/comments#comment-2574198)."
    },
    "voteCount": 56
  },
  {
    "_id": "cpdsMuAHSWhWnKdog",
    "url": null,
    "title": "Security Mindset and the Logistic Success Curve",
    "slug": "security-mindset-and-the-logistic-success-curve",
    "author": "Eliezer_Yudkowsky",
    "question": false,
    "tags": [
      {
        "name": "Programming"
      },
      {
        "name": "Paradoxes"
      },
      {
        "name": "Planning & Decision-Making"
      },
      {
        "name": "Mechanism Design"
      },
      {
        "name": "AI"
      },
      {
        "name": "Robust Agents"
      },
      {
        "name": "Dialogue (format)"
      },
      {
        "name": "Computer Security & Cryptography"
      },
      {
        "name": "Security Mindset"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "**Follow-up to:** [Security Mindset and Ordinary Paranoia](https://www.lesserwrong.com/posts/8gqrbnW758qjHFTrH/security-mindset-and-ordinary-paranoia)\n\n* * *\n\n(_Two days later, Amber returns with another question._)\n\namber:  Uh, say, Coral. How important is security mindset when you're building a whole new kind of system—say, one subject to potentially adverse optimization pressures, where you want it to have some sort of robustness property?\n\ncoral:  How novel is the system?\n\namber:  Very novel.\n\ncoral:  Novel enough that you'd have to invent your own new best practices instead of looking them up?\n\namber:  Right.\n\ncoral:  That's serious business. If you're building a very simple Internet-connected system, maybe a smart ordinary paranoid could look up how we usually guard against adversaries, use as much off-the-shelf software as possible that was checked over by real security professionals, and not do too horribly. But if you're doing something qualitatively new and complicated that has to be robust against adverse optimization, well... mostly I'd think you were operating in almost impossibly dangerous territory, and I'd advise you to figure out what to do after your first try failed. But if you wanted to actually succeed, ordinary paranoia absolutely would not do it.\n\namber:  In other words, projects to build novel mission-critical systems ought to have advisors with the full security mindset, so that the advisor can say what the system builders really need to do to ensure security.\n\ncoral:  (_laughs sadly_)  No.\n\namber:  No?\n\ncoral:  Let's say for the sake of concreteness that you want to build a new kind of secure operating system. That is _not_ the sort of thing you can do by attaching one advisor with security mindset, who has limited political capital to use to try to argue people into doing things. “Building a house when you're only allowed to touch the bricks using tweezers” comes to mind as a metaphor. You're going to need experienced security professionals working full-time with high authority. Three of them, one of whom is a cofounder. Although even then, we might still be operating in the territory of Paul Graham's Design Paradox.\n\namber:  Design Paradox? What's that?\n\ncoral:  Paul Graham's Design Paradox is that people who have good taste in UIs can tell when other people are designing good UIs, but most CEOs of big companies lack the good taste to tell who else has good taste. And that's why big companies can't just hire other people as talented as Steve Jobs to build nice things for them, even though Steve Jobs certainly wasn't the best possible designer on the planet. Apple existed because of a lucky history where Steve Jobs ended up in charge. There's no way for Samsung to hire somebody else with equal talents, because Samsung would just end up with some guy in a suit who was good at pretending to be Steve Jobs in front of a CEO who couldn't tell the difference.\n\nSimilarly, people with security mindset can notice when other people lack it, but I'd worry that an ordinary paranoid would have a hard time telling the difference, which would make it hard for them to hire a truly competent advisor. And of course lots of the people in the larger social system behind technology projects lack even the ordinary paranoia that many good programmers possess, and they just end up with empty suits talking a lot about “risk” and “safety”. In other words, if we're talking about something as hard as building a secure operating system, and your project hasn't started up _already_ headed up by someone with the full security mindset, you are in trouble. Where by “in trouble” I mean “totally, irretrievably doomed”.\n\namber:  Look, uh, there's a certain project I'm invested in which has raised a hundred million dollars to create merchant drones.\n\ncoral:  Merchant drones?\n\namber:  So there are a lot of countries that have poor market infrastructure, and the idea is, we're going to make drones that fly around buying and selling things, and they'll use machine learning to figure out what prices to pay and so on. We're not just in it for the money; we think it could be a huge economic boost to those countries, really help them move forwards.\n\ncoral:  Dear God. Okay. There are exactly two things your company is about: system security, and regulatory compliance. Well, and also marketing, but that doesn't count because every company is about marketing. It would be a severe error to imagine that your company is about anything else, such as drone hardware or machine learning.\n\namber:  Well, the sentiment inside the company is that the time to begin thinking about legalities and security will be after we've proven we can build a prototype and have at least a small pilot market in progress. I mean, until we know how people are using the system and how the software ends up working, it's hard to see how we could do any productive thinking about security or compliance that wouldn't just be pure speculation.\n\ncoral:  Ha! Ha, hahaha… oh my god you're not joking.\n\namber:  What?\n\ncoral:  Please tell me that what you actually mean is that you have a security and regulatory roadmap which calls for you to do some of your work later, but clearly lays out what work needs to be done, when you are to start doing it, and when each milestone needs to be complete. Surely you don't _literally_ mean that you _intend to start thinking about it_ later?\n\namber:  A lot of times at lunch we talk about how annoying it is that we'll have to deal with regulations and how much better it would be if governments were more libertarian. That counts as thinking about it, right?\n\ncoral:  Oh my god.\n\namber:  I don't see how we could have a security plan when we don't know exactly what we'll be securing. Wouldn't the plan just turn out to be wrong?\n\ncoral:  All business plans for startups turn out to be wrong, but you still need them—and not just as works of fiction. They represent the written form of your current beliefs about your key assumptions. Writing down your business plan checks whether your current beliefs can possibly be coherent, and suggests which critical beliefs to test first, and which results should set off alarms, and when you are falling behind key survival thresholds. The idea isn't that you stick to the business plan; it's that having a business plan (a) checks that it seems possible to succeed in any way whatsoever, and (b) tells you when one of your beliefs is being falsified so you can explicitly change the plan and adapt. Having a written plan that you intend to rapidly revise in the face of new information is one thing. _NOT HAVING A PLAN_ is _another_.\n\namber:  The thing is, I _am_ a little worried that the head of the project, Mr. Topaz, isn't concerned enough about the possibility of somebody fooling the drones into giving out money when they shouldn't. I mean, I've tried to raise that concern, but he says that of course we're not going to program the drones to give out money to just anyone. Can you maybe give him a few tips? For when it comes time to start thinking about security, I mean.\n\ncoral:  Oh. Oh, my dear, sweet summer child, I'm sorry. There's nothing I can do for you.\n\namber:  Huh? But you haven't even looked at our beautiful business model!\n\ncoral:  I thought maybe your company merely had a hopeless case of underestimated difficulties and misplaced priorities. But now it sounds like your leader is not even using ordinary paranoia, and reacts with skepticism to it. Calling a case like that “hopeless” would be an understatement.\n\namber:  But a security failure would be very bad for the countries we're trying to help! They need _secure_ merchant drones!\n\ncoral:  Then they will need drones built by some project that is not led by Mr. Topaz.\n\namber:  But that seems very hard to arrange!\n\ncoral:  ...I don't understand what you are saying that is supposed to contradict anything I am saying.\n\namber:  Look, aren't you judging Mr. Topaz a little too quickly? Seriously.\n\ncoral:  I haven't met him, so it's possible you misrepresented him to me. But if you've accurately represented his attitude? Then, yes, I did judge quickly, but it's a hell of a good guess. Security mindset is already rare on priors. “I don't plan to make my drones give away money to random people” means he's imagining how his system could work as he intends, instead of imagining how it might not work as he intends. If somebody doesn't even exhibit ordinary paranoia, spontaneously on their own cognizance without external prompting, then they cannot do security, period. Reacting indignantly to the suggestion that something might go wrong is even beyond that level of hopelessness, but the base level was hopeless enough already.\n\namber:  Look... can you just go to Mr. Topaz and try to tell him what he needs to do to add some security onto his drones? Just try? Because it's super important.\n\ncoral:  I could try, yes. I can't succeed, but I could try.\n\namber:  Oh, but please be careful to not be harsh with him. Don’t put the focus on what he’s doing wrong—and try to make it clear that these problems aren’t _too_ serious. He's been put off by the media alarmism surrounding apocalyptic scenarios with armies of evil drones filling the sky, and it took me some trouble to convince him that I wasn't just another alarmist full of fanciful catastrophe scenarios of drones defying their own programming.\n\ncoral:  ...\n\namber:  And maybe try to keep your opening conversation away from what might sound like crazy edge cases, like somebody forgetting to check the end of a buffer and an adversary throwing in a huge string of characters that overwrite the end of the stack with a return address that jumps to a section of code somewhere else in the system that does something the adversary wants. I mean, you've convinced me that these far-fetched scenarios are worth worrying about, if only because they might be canaries in the coal mine for more realistic failure modes. But Mr. Topaz thinks that's all a bit silly, and I don't think you should open by trying to explain to him on a meta level why it isn't. He'd probably think you were being condescending, telling him how to think. Especially when you're just an operating-systems guy and you have no experience building drones and seeing what actually makes them crash. I mean, that's what I think he'd say to you.\n\ncoral:  ...\n\namber:  Also, start with the cheaper interventions when you're giving advice. I don't think Mr. Topaz is going to react well if you tell him that he needs to start all over in another programming language, or establish a review board for all code changes, or whatever. He's worried about competitors reaching the market first, so he doesn't want to do anything that will slow him down.\n\ncoral:  ...\n\namber:  Uh, Coral?\n\ncoral:  ... on his novel project, entering new territory, doing things not exactly like what has been done before, carrying out novel mission-critical subtasks for which there are no standardized best security practices, nor any known understanding of what makes the system robust or not-robust.\n\namber:  Right!\n\ncoral:  And Mr. Topaz himself does not seem much terrified of this terrifying task before him.\n\namber:  Well, he's worried about somebody else making merchant drones first and misusing this key economic infrastructure for bad purposes. That's the same basic thing, right? Like, it demonstrates that he can worry about things?\n\ncoral:  It is utterly different. Monkeys who can be afraid of other monkeys getting to the bananas first are far, far more common than monkeys who worry about whether the bananas will exhibit weird system behaviors in the face of adverse optimization.\n\namber:  Oh.\n\ncoral:  I'm afraid it is only slightly more probable that Mr. Topaz will oversee the creation of robust software than that the Moon will spontaneously transform into organically farmed goat cheese.\n\namber:  I think you're being too harsh on him. I've met Mr. Topaz, and he seemed pretty bright to me.\n\ncoral:  Again, assuming you're representing him accurately, Mr. Topaz seems to lack what I called ordinary paranoia. If he does have that ability as a cognitive capacity, which many bright programmers do, then he obviously doesn't feel passionate about applying that paranoia to his drone project along key dimensions. It also sounds like Mr. Topaz doesn't realize there's a skill that he is missing, and would be insulted by the suggestion. I am put in mind of the story of the farmer who was asked by a passing driver for directions to get to Point B, to which the farmer replied, “If I was trying to get to Point B, I sure wouldn't start from here.”\n\namber:  Mr. Topaz has made some significant advances in drone technology, so he can't be stupid, right?\n\ncoral:  \"Security mindset\" seems to be a distinct cognitive talent from _g_ factor or even programming ability. In fact, there doesn’t seem to be a level of human genius that even guarantees you’ll be skilled at ordinary paranoia. Which does make some security professionals feel a bit weird, myself included—the same way a lot of programmers have trouble understanding why not everyone can learn to program. But it seems to be an observational fact that both ordinary paranoia and security mindset are things that can decouple from _g_ factor and programming ability—and if this were not the case, the Internet would be far more secure than it is.\n\namber:  Do you think it would help if we talked to the other VCs funding this project and got them to ask Mr. Topaz to appoint a Special Advisor on Robustness reporting directly to the CTO? That sounds politically difficult to me, but it's possible we could swing it. Once the press started speculating about drones going rogue and maybe aggregating into larger Voltron-like robots that could acquire laser eyes, Mr. Topaz did tell the VCs that he was very concerned about the ethics of drone safety and that he'd had many long conversations about it over lunch hours.\n\ncoral:  I'm venturing slightly outside my own expertise here, which isn't corporate politics per se. But on a project like this one that's trying to enter novel territory, I'd guess the person with security mindset needs at least cofounder status, and must be personally trusted by any cofounders who don't have the skill. It can't be an outsider who was brought in by VCs, who is operating on limited political capital and needs to win an argument every time she wants to not have all the services conveniently turned on by default. I suspect you just have the wrong person in charge of this startup, and that this problem is not repairable.\n\namber:  Please don't just give up! Even if things are as bad as you say, just increasing our project's probability of being secure from 0% to 10% would be very valuable in expectation to all those people in other countries who need merchant drones.\n\ncoral:  ...look, at some point in life we have to try to triage our efforts and give up on what can't be salvaged. There's often a logistic curve for success probabilities, you know? The distances are measured in multiplicative odds, not additive percentage points. You can't take a project like this and assume that by putting in some more hard work, you can increase the absolute chance of success by 10%. More like, the odds of this project's failure versus success start out as 1,000,000:1, and if we're very polite and navigate around Mr. Topaz's sense that he is higher-status than us and manage to explain a few tips to him without ever sounding like we think we know something he doesn't, we can quintuple his chances of success and send the odds to 200,000:1. Which is to say that in the world of percentage points, the odds go from 0.0% to 0.0%. That's one way to look at the “[law of continued failure](https://intelligence.org/2017/10/13/fire-alarm)”.\n\nIf you had the kind of project where the fundamentals implied, say, a 15% chance of success, you’d then be on the right part of the logistic curve, and in _that_ case it could make a lot of sense to hunt for ways to bump that up to a 30% or 80% chance.\n\namber:  Look, I'm worried that it will really be very bad if Mr. Topaz reaches the market first with insecure drones. Like, I think that merchant drones could be very beneficial to countries without much existing market backbone, and if there's a grand failure—especially if some of the would-be customers have their money or items stolen—then it could poison the potential market for years. It will be terrible! Really, genuinely terrible!\n\ncoral:  Wow. That sure does sound like an unpleasant scenario to have wedged yourself into.\n\namber:  But what do we do now?\n\ncoral:  Damned if I know. I do suspect you're screwed so long as you can only win if somebody like Mr. Topaz creates a robust system. I guess you could try to have some other drone project come into existence, headed up by somebody that, say, Bruce Schneier assures everyone is unusually good at security-mindset thinking and hence can hire people like me and listen to all the harsh things we have to say. Though I have to admit, the part where you think it's drastically important that you beat an insecure system to market with a secure system—well, that sounds positively nightmarish. You're going to need a lot more resources than Mr. Topaz has, or some other kind of very major advantage. Security takes time.\n\namber:  Is it really that hard to add security to the drone system?\n\ncoral:  You keep talking about “adding” security. System robustness isn't the kind of property you can bolt onto software as an afterthought.\n\namber:  I guess I'm having trouble seeing why it's so much more expensive. Like, if somebody foolishly builds an OS that gives access to just anyone, you could instead put a password lock on it, using your clever system where the OS keeps the hashes of the passwords instead of the passwords. You just spend a couple of days rewriting all the services exposed to the Internet to ask for passwords before granting access. And then the OS has security on it! Right?\n\ncoral:  NO. Everything inside your system that is potentially subject to adverse selection in its probability of weird behavior is a liability! Everything exposed to an attacker, and everything those subsystems interact with, and everything _those_ parts interact with! You have to build _all_ of it robustly! If you want to build a secure OS you need a whole special project that is “building a secure operating system instead of an insecure operating system”. And you also need to restrict the scope of your ambitions, and not do everything you want to do, and obey other commandments that will feel like big unpleasant sacrifices to somebody who doesn't have the full security mindset. OpenBSD can’t do a tenth of what Ubuntu does. They can't afford to! It would be too large of an attack surface! They can't review that much code using the special process that they use to develop secure software! They can't hold that many assumptions in their minds!\n\namber:  Does that effort _have_ to take a significant amount of extra time? Are you sure it can't just be done in a couple more weeks if we hurry?\n\ncoral:  YES. Given that this is a novel project entering new territory, expect it to take _at least_ two years more time, or 50% more development time—whichever is less—compared to a security-incautious project that otherwise has identical tools, insights, people, and resources. And that is a very, very optimistic lower bound.\n\namber:  This story seems to be heading in a worrying direction.\n\ncoral:  Well, I'm sorry, but creating robust systems takes longer than creating non-robust systems even in cases where it would be really, extraordinarily bad if creating robust systems took longer than creating non-robust systems.\n\namber:  Couldn't it be the case that, like, projects which are implementing good security practices do everything so much cleaner and better that they can come to market faster than any insecure competitors could?\n\ncoral:  … I honestly have trouble seeing [why](http://www.readthesequences.com/MotivatedStoppingAndMotivatedContinuation) you’re [privileging that hypothesis](https://www.readthesequences.com/PrivilegingTheHypothesis) for consideration. Robustness involves assurance processes that take additional time. OpenBSD does not go through lines of code faster than Ubuntu.\n\nBut more importantly, if everyone has access to the same tools and insights and resources, then an unusually fast method of doing something cautiously can always be degenerated into an even faster method of doing the thing incautiously. There is not now, nor will there ever be, a programming language in which it is the least bit difficult to write bad programs. There is not now, nor will there ever be, a methodology that makes writing insecure software inherently slower than writing secure software. Any security professional who heard about your bright hopes would just laugh. Ask them too if you don't believe me.\n\namber:  But shouldn't engineers who aren't cautious just be unable to make software at all, because of ordinary bugs?\n\ncoral:  I am afraid that it is both possible, and _extremely_ common in practice, for people to fix all the bugs that are crashing their systems in ordinary testing today, using methodologies that are indeed adequate to fixing ordinary bugs that show up often enough to afflict a significant fraction of users, and then ship the product. They get everything working today, and they don't feel like they have the slack to delay any longer than that before shipping because the product is already behind schedule. They don't hire exceptional people to do ten times as much work in order to prevent the product from having holes that only show up under adverse optimization pressure, that somebody else finds first and that they learn about after it's too late.\n\nIt's not even the wrong decision, for products that aren't connected to the Internet, don't have enough users for one to go rogue, don't handle money, don't contain any valuable data, and don't do anything that could injure people if something goes wrong. If your software doesn't destroy anything important when it explodes, it's probably a better use of limited resources to plan on fixing bugs as they show up.\n\n… Of course, you need some amount of security mindset to realize which software _can_ in fact destroy the company if it silently corrupts data and nobody notices this until a month later. I don't suppose it's the case that your drones only carry a limited amount of the full corporate budget in cash over the course of a day, and you always have more than enough money to reimburse all the customers if all items in transit over a day were lost, taking into account that the drones might make many more purchases or sales than usual? And that the systems are generating internal paper receipts that are clearly shown to the customer and non-electronically reconciled once per day, thereby enabling you to notice a problem before it's too late?\n\namber:  Nope!\n\ncoral:  Then as you say, it would be better for the world if your company didn't exist and wasn't about to charge into this new territory and poison it with a spectacular screwup.\n\namber:  If I believed that… well, Mr. Topaz certainly isn't going to stop his project or let somebody else take over. It seems the logical implication of what you say you believe is that I should try to persuade the venture capitalists I know to launch a safer drone project with even more funding.\n\ncoral:  Uh, I'm sorry to be blunt about this, but I'm not sure _you_ have a high enough level of security mindset to identify an executive who's sufficiently better than you at it. Trying to get enough of a resource advantage to beat the insecure product to market is only half of your problem in launching a competing project. The other half of your problem is surpassing the prior rarity of people with truly deep security mindset, and getting somebody like that in charge and fully committed. Or at least get them in as a highly trusted, fully committed cofounder who isn't on a short budget of political capital. I'll say it again: an advisor appointed by VCs isn't nearly enough for a project like yours. Even if the advisor is a genuinely good security professional—\n\namber:  This all seems like an unreasonably difficult requirement! Can't you back down on it a little?\n\ncoral:  —the person in charge will probably try to bargain down reality, as represented by the unwelcome voice of the security professional, who won't have enough social capital to badger them into “unreasonable” measures. Which means you fail on full automatic.\n\namber:  … Then what am I to do?\n\ncoral:  I don't know, actually. But there's no point in launching another drone project with even more funding, if it just ends up with another Mr. Topaz put in charge. Which, by default, is exactly what your venture capitalist friends are going to do. Then you've just set an even higher competitive bar for anyone actually trying to be first to market with a secure solution, may God have mercy on their souls.\n\nBesides, if Mr. Topaz thinks he has a competitor breathing down his neck and rushes his product to market, his chance of creating a secure system could drop by a factor of ten and go all the way from 0.0% to 0.0%.\n\namber:  Surely my VC friends have faced this kind of problem before and know how to identify and hire executives who can do security well?\n\ncoral:  … If one of your VC friends is Paul Graham, then maybe yes. But in the average case, _NO_.\n\nIf average VCs always made sure that projects which needed security had a founder or cofounder with strong security mindset—if they had the _ability_ to do that _even in cases where they decided they wanted to_—the Internet would again look like a very different place. By default, your VC friends will be fooled by somebody who looks very sober and talks a lot about how terribly concerned he is with cybersecurity and how the system is going to be ultra-secure and reject over nine thousand common passwords, including the thirty-six passwords listed on this slide here, and the VCs will ooh and ah over it, especially as one of them realizes that their own password is on the slide. _That_ project leader is absolutely not going to want to hear from me—even less so than Mr. Topaz. To him, I'm a political threat who might damage his line of patter to the VCs.\n\namber:  I have trouble believing all these smart people are really that stupid.\n\ncoral:  You're compressing your innate sense of social status and your estimated level of how good particular groups are at this particular ability into a single dimension. That is not a good idea.\n\namber:  I'm not saying that I think everyone with high status already knows the deep security skill. I'm just having trouble believing that they can't learn it quickly once told, or could be stuck not being able to identify good advisors who have it. That would mean they couldn't know something you know, something that seems important, and that just… feels _off_ to me, somehow. Like, there are all these successful and important people out there, and you’re saying [you’re _better_ than them](https://www.lesserwrong.com/sequences/oLGCcbnvabyibnG9d), even with all their influence, their skills, their resources—\n\ncoral:  Look, you don't have to take my word for it. Think of all the websites you've been on, with snazzy-looking design, maybe with millions of dollars in sales passing through them, that want your password to be a mixture of uppercase and lowercase letters and numbers. In other words, they want you to enter “Password1!” instead of “correct horse battery staple”. Every one of those websites is doing a thing that looks humorously silly to someone with a full security mindset or even just somebody who regularly reads [XKCD](https://xkcd.com/936/). It says that the security system was set up by somebody who didn't know what they were doing and was blindly imitating impressive-looking mistakes they saw elsewhere.\n\nDo you think that makes a good impression on their customers? That's right, it does! Because the customers don't know any better. Do you think that login system makes a good impression on the company's investors, including professional VCs and probably some angels with their own startup experience? That's right, it does! Because the VCs don't know any better, and even the angel doesn't know any better, and they don't realize they're missing a vital skill, and they aren't consulting anyone who knows more. An innocent is _impressed_ if a website requires a mix of uppercase and lowercase letters and numbers _and_ punctuation. They think the people running the website must really care to impose a security measure that unusual and inconvenient. The people running the website think that's what they're doing too.\n\nPeople with deep security mindset are both rare and rarely _appreciated_. You can see just from the login system that none of the VCs and none of the C-level executives at that startup thought they needed to consult a real professional, or managed to find a real professional rather than an empty suit if they went consulting. There was, visibly, nobody in the neighboring system with the combined knowledge and status to walk over to the CEO and say, “Your login system is embarrassing and you need to hire a real security professional.” Or if anybody did say that to the CEO, the CEO was offended and shot the messenger for not phrasing it ever-so-politely enough, or the CTO saw the outsider as a political threat and bad-mouthed them out of the game.\n\nYour wishful should-universe hypothesis that people who can touch the full security mindset are more common than that within the venture capital and angel investing ecosystem is just flat wrong. Ordinary paranoia directed at widely-known adversarial cases is dense enough within the larger ecosystem to exert widespread social influence, albeit still comically absent in many individuals and regions. People with the full security mindset are too rare to have the same level of presence. That's the _easily visible_ truth. You can _see_ the login systems that want a punctuation mark in your password. You are not hallucinating them.\n\namber:  If that's all true, then I just don't see how I can win. Maybe I should just condition on everything you say being false, since, if it's true, my winning seems unlikely—in which case all victories on my part would come in worlds with other background assumptions.\n\ncoral:  … is that something you say often?\n\namber:  Well, I say it whenever my victory starts to seem sufficiently unlikely.\n\ncoral:  Goodness. I could maybe, _maybe_ see somebody saying that once over the course of their entire lifetime, for a single unlikely conditional, but doing it more than once is sheer madness. I'd expect the unlikely conditionals to build up very fast and drop the probability of your mental world to effectively zero. It's tempting, but it's usually a bad idea to slip sideways into your own private [hallucinatory universe](https://www.facebook.com/yudkowsky/posts/10154981483669228) when you feel you're under emotional pressure. I tend to believe that no matter what the difficulties, we are most likely to come up with good plans when we are mentally living in reality as opposed to somewhere else. If things seem difficult, we must face the difficulty squarely to succeed, to come up with some solution that faces down how bad the situation really is, rather than deciding to condition on things not being difficult because then it's too hard.\n\namber:  Can you at least _try_ talking to Mr. Topaz and advise him how to make things be secure?\n\ncoral:  Sure. Trying things is easy, and I’m a character in a dialogue, so my opportunity costs are low. I'm sure Mr. Topaz is trying to build secure merchant drones, too. It's succeeding at things that is the hard part.\n\namber:  Great, I'll see if I can get Mr. Topaz to talk to you. But do please be polite! If you think he's doing something wrong, try to point it out more gently than the way you've talked to me. I think I have enough political capital to get you in the door, but that won't last if you're rude.\n\ncoral:  You know, back in mainstream computer security, when you propose a new way of securing a system, it's considered traditional and wise for everyone to gather around and try to come up with reasons why your idea might not work. It's understood that no matter how smart you are, most seemingly bright ideas turn out to be flawed, and that you shouldn't be touchy about people trying to shoot them down. Does Mr. Topaz have no acquaintance at all with the practices in computer security? A lot of programmers do.\n\namber:  I think he'd say he respects computer security as its own field, but he doesn't believe that building secure operating systems is the same problem as building merchant drones.\n\ncoral:  And if I suggested that this case might be similar to the problem of building a secure operating system, and that this case creates a similar need for more effortful and cautious development, requiring both (a) additional development time and (b) a special need for caution supplied by people with unusual mindsets above and beyond ordinary paranoia, who have an unusual skill that identifies shaky assumptions in a safety story before an ordinary paranoid would judge a fire as being urgent enough to need putting out, who can remedy the problem using deeper solutions than an ordinary paranoid would generate as parries against imagined attacks?\n\nIf I suggested, indeed, that this scenario might hold generally wherever we demand robustness of a complex system that is being subjected to strong external or internal optimization pressures? Pressures that strongly promote the probabilities of particular states of affairs via optimization that searches across a large and complex state space? Pressures which therefore in turn subject other subparts of the system to selection for weird states and previously unenvisioned execution paths? Especially if some of these pressures may be in some sense creative and find states of the system or environment that surprise us or violate our surface generalizations?\n\namber:  I think he'd probably think you were trying to look smart by using overly abstract language at him. Or he'd reply that he didn't see why this took any more caution than he was already using just by testing the drones to make sure they didn't crash or give out too much money.\n\ncoral:  I see.\n\namber:  So, shall we be off?\n\ncoral:  Of course! No problem! I'll just go meet with Mr. Topaz and use verbal persuasion to turn him into Bruce Schneier.\n\namber:  That's the spirit!\n\ncoral:  God, how I wish I lived in the territory that corresponds to your map.\n\namber:  Hey, come on. Is it seriously _that_ hard to bestow exceptionally rare mental skills on people by talking at them? I agree it's a bad sign that Mr. Topaz shows no sign of wanting to acquire those skills, and doesn't think we have enough relative status to continue listening if we say something he doesn't want to hear. But that just means we have to phrase our advice cleverly so that he _will_ want to hear it!\n\ncoral:  I suppose you could modify your message into something Mr. Topaz doesn't find so unpleasant to hear. Something that sounds related to the topic of drone security, but which doesn't cost him much, and of course does not actually cause his drones to end up secure because that would be all unpleasant and expensive. You could slip a little sideways in reality, and convince yourself that you've gotten Mr. Topaz to ally with you, because he sounds agreeable now. Your instinctive desire for the high-status monkey to be on your political side will feel like its problem has been solved. You can substitute the feeling of having solved that problem for the unpleasant sense of not having secured the actual drones; you can tell yourself that the bigger monkey will take care of everything now that he seems to be on your pleasantly-modified political side. And so you will be happy. Until the merchant drones hit the market, of course, but that unpleasant experience should be brief.\n\namber:  Come on, we can do this! You've just got to think positively!\n\ncoral:  … Well, if nothing else, this should be an interesting experience. I've never tried to do anything quite this doomed before."
    },
    "voteCount": 47
  },
  {
    "_id": "duAkuSqJhGDcfMaTA",
    "url": null,
    "title": "Reflection in Probabilistic Logic",
    "slug": "reflection-in-probabilistic-logic",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "Probability & Statistics"
      },
      {
        "name": "Logic & Mathematics "
      },
      {
        "name": "Robust Agents"
      },
      {
        "name": "Machine Intelligence Research Institute (MIRI)"
      },
      {
        "name": "Reflective Reasoning"
      },
      {
        "name": "Logical Uncertainty"
      },
      {
        "name": "Gödelian Logic"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "Paul Christiano has devised [**a new fundamental approach**](http://intelligence.org/wp-content/uploads/2013/03/Christiano-et-al-Naturalistic-reflection-early-draft.pdf) to the \"[Löb Problem](https://www.youtube.com/watch?v=MwriJqBZyoM)\" wherein [Löb's Theorem](/lw/t6/the_cartoon_guide_to_l%C3%B6bs_theorem/) seems to pose an obstacle to AIs building successor AIs, or adopting successor versions of their own code, that trust the same amount of mathematics as the original.  (I am currently writing up a more thorough description of the _question_ this preliminary technical report is working on answering.  For now the main online description is in a [quick Summit talk](https://www.youtube.com/watch?v=MwriJqBZyoM) I gave.  See also Benja Fallenstein's description of the problem in the course of presenting a [different angle of attack](/lw/e4e/an_angle_of_attack_on_open_problem_1/).  Roughly the problem is that mathematical systems can only prove the soundness of, aka 'trust', weaker mathematical systems.  If you try to write out an exact description of how AIs would build their successors or successor versions of their code in the most obvious way, it looks like the mathematical strength of the proof system would tend to be stepped down each time, which is undesirable.)\n\nPaul Christiano's approach is inspired by the idea that whereof one cannot prove or disprove, thereof one must assign probabilities: and that although no mathematical system can contain its own _truth_ predicate, a mathematical system might be able to contain a reflectively consistent _probability_ predicate.  In particular, it looks like we can have:\n\n∀a, b:  (a < P(φ) < b)          ⇒  P(a < P('φ') < b) = 1  \n∀a, b:  P(a ≤ P('φ') ≤ b) > 0  ⇒  a ≤ P(φ) ≤ b\n\nSuppose I present you with the human and probabilistic version of a Gödel sentence, the [Whitely sentence](http://books.google.com/books?id=cmX8yyBfP74C&pg=PA317&lpg=PA317&dq=whitely+lucas+cannot+consistently&source=bl&ots=68tuximFfI&sig=GdZro1wy6g_KzO-PXInGTKFrU7Q&hl=en&sa=X&ei=7-FMUb61LojRiAK9hIGQDw&ved=0CGoQ6AEwBg#v=onepage&q=whitely%20lucas%20cannot%20consistently&f=false) \"You assign this statement a probability less than 30%.\"  If you disbelieve this statement, it is true.  If you believe it, it is false.  If you assign 30% probability to it, it is false.  If you assign 29% probability to it, it is true.\n\nPaul's approach resolves this problem by restricting your belief about your own probability assignment to within epsilon of 30% for any epsilon.  So Paul's approach replies, \"Well, I assign _almost_ exactly 30% probability to that statement - maybe a little more, maybe a little less - in fact I think there's about a 30% chance that I'm a tiny bit under 0.3 probability and a 70% chance that I'm a tiny bit over 0.3 probability.\"  A standard fixed-point theorem then implies that a consistent assignment like this should exist.  If asked if the probability is over 0.2999 or under 0.30001 you will reply with a definite yes.\n\nWe haven't yet worked out a walkthrough showing if/how this solves the Löb obstacle to self-modification, and the probabilistic theory itself is nonconstructive (we've shown that something like this should exist, but not how to compute it).  Even so, a possible fundamental triumph over Tarski's theorem on the undefinability of truth and a number of standard Gödelian limitations is important news as math _qua_ math, though work here is still in very preliminary stages.  There are even whispers of unrestricted comprehension in a probabilistic version of set theory with ∀φ: ∃S: P(x ∈ S) = P(φ(x)), though this part is not in the preliminary report and is at even earlier stages and could easily not work out at all.\n\nIt seems important to remark on how this result was developed:  Paul Christiano showed up with the idea (of consistent probabilistic reflection via a fixed-point theorem) to a week-long \"math squad\" (aka MIRI Workshop) with Marcello Herreshoff, Mihaly Barasz, and myself; then we all spent the next week proving that version after version of Paul's idea couldn't work or wouldn't yield self-modifying AI; until finally, a day after the workshop was supposed to end, it produced something that looked like it might work.  If we hadn't been trying to _solve_ this problem (with hope stemming from how it seemed like the sort of thing a reflective rational agent ought to be able to do somehow), this would be just another batch of impossibility results in the math literature.  I remark on this because it may help demonstrate that Friendly AI is a productive approach to math _qua _math, which may aid some mathematician in becoming interested.\n\nI further note that this does not mean the Löbian obstacle is resolved and no further work is required.  Before we can conclude that we need a computably specified version of the theory plus a walkthrough for a self-modifying agent using it.\n\nSee also the [blog post](http://intelligence.org/2013/03/22/early-draft-of-naturalistic-reflection-paper/) on the MIRI site (and subscribe to MIRI's newsletter [here](http://intelligence.org/) to keep abreast of research updates).\n\nThis LW post is the preferred place for feedback on the [paper](http://intelligence.org/wp-content/uploads/2013/03/Christiano-et-al-Naturalistic-reflection-early-draft.pdf).\n\nEDIT:  But see discussion on a Google+ post by John Baez [here](https://plus.google.com/117663015413546257905/posts/jJModdTJ2R3?hl=en).  Also see [here](http://wiki.lesswrong.com/wiki/Comment_formatting#Using_LaTeX_to_render_mathematics) for how to display math LaTeX in comments."
    },
    "voteCount": 69
  },
  {
    "_id": "YcdArE79SDxwWAuyF",
    "url": null,
    "title": "The Treacherous Path to Rationality",
    "slug": "the-treacherous-path-to-rationality",
    "author": "Jacobian",
    "question": false,
    "tags": [
      {
        "name": "Pitfalls of Rationality"
      },
      {
        "name": "Group Rationality"
      },
      {
        "name": "Epistemology"
      },
      {
        "name": "Rationality"
      },
      {
        "name": "Community"
      },
      {
        "name": "Explicit Reasoning"
      }
    ],
    "tableOfContents": {
      "sections": [
        {
          "title": "Rats v. Plague",
          "anchor": "Rats_v__Plague",
          "level": 1
        },
        {
          "title": "The Path",
          "anchor": "The_Path",
          "level": 1
        },
        {
          "title": "Alternatives to Reason",
          "anchor": "Alternatives_to_Reason",
          "level": 1
        },
        {
          "title": "Underperformance Swamp",
          "anchor": "Underperformance_Swamp",
          "level": 1
        },
        {
          "title": "Sinkholes of Sneer",
          "anchor": "Sinkholes_of_Sneer",
          "level": 1
        },
        {
          "title": "Strange Status and Scary Memes",
          "anchor": "Strange_Status_and_Scary_Memes",
          "level": 1
        },
        {
          "title": "Valley of Disintegration",
          "anchor": "Valley_of_Disintegration",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        },
        {
          "anchor": "comments",
          "level": 0,
          "title": "116 comments"
        }
      ],
      "headingsCount": 9
    },
    "contents": {
      "markdown": "Cross-posted, as always, [from Putanumonit](https://putanumonit.com/2020/10/08/path-to-reason/).\n\n* * *\n\nRats v. Plague\n--------------\n\nThe Rationality community was never particularly focused on medicine or epidemiology. And yet, we basically got everything about COVID-19 right and did so months ahead of the majority of government officials, journalists, and supposed experts.\n\nWe started discussing the virus and raising the alarm in private back in January. By late February, as American health officials were almost unanimously downplaying the threat, we wrote posts on [taking the disease seriously](https://putanumonit.com/2020/02/27/seeing-the-smoke/), buying masks, and [preparing for quarantine](https://www.lesswrong.com/posts/mQdTCKKB8p5mGaMfC/quarantine-preparations).\n\nThroughout March, the CDC was telling people not to wear masks and not to get tested unless displaying symptoms. At the same time, Rationalists were already covering every relevant angle, from [asymptomatic transmission](https://www.lesswrong.com/posts/9GyKccaJdLEbdhyTi/a-significant-portion-of-covid-19-transmission-is) to [the effect of viral load](https://www.lesswrong.com/posts/FftgfqqdRAx5ssiAp/what-is-the-impact-of-varying-initial-viral-load-of-covid-19), to [the credibility of the CDC](https://www.lesswrong.com/posts/h4vWsBBjASgiQ2pn6/credibility-of-the-cdc-on-sars-cov-2) itself. As despair and confusion reigned everywhere into the summer, Rationalists built online dashboards modeling [nationwide responses](http://epidemicforecasting.org/about) and [personal activity risk](https://www.microcovid.org/) to let both governments and individuals make informed decisions.\n\nThis remarkable success did not go unnoticed. Before he threatened to doxx Scott Alexander and triggered a shitstorm, New York Times reporter Cade Metz interviewed me and other Rationalists mostly about how we were ahead of the curve on COVID and what others can learn from us. I told him that Rationality has a simple message: *“people can use **explicit reason** to figure things out, but they rarely do”*\n\n![](https://putanumonit.files.wordpress.com/2020/03/voxplaining-coronavirus.png?w=900)\n\n*If rationalists led the way in covering COVID-19, Vox brought up the rear*\n\nRationalists have been working to promote the application of explicit reason, to “[raise the sanity waterline](https://twitter.com/yashkaf/status/1253094602812334081)” as it were, but with limited success. I wrote recently about [success stories of rationalist improvement](https://putanumonit.com/2019/12/08/rationalist-self-improvement/) but I don’t think it inspired a rush to LessWrong. This post is in a way a response to my previous one. It’s about the obstacles preventing people from training and succeeding in the use of explicit reason, impediments I faced myself and saw others stumble over or turn back from. This post is a lot less sanguine about the sanity waterline’s prospects.\n\nThe Path\n--------\n\nI recently chatted with Spencer Greenberg about teaching rationality. Spencer regularly publishes articles like [*7 questions for deciding whether to trust your gut*](https://www.clearerthinking.org/single-post/2016/10/12/7-questions-for-deciding-whether-to-trust-your-gut) or [*3 types of binary thinking you fall for*](https://www.clearerthinking.org/single-post/2020/06/23/Learn-the-three-types-of-binary-thinking). Reading him, you’d think that the main obstacle to pure reason ruling the land is lack of intellectual listicles on ways to overcome bias.\n\nBut we’ve been developing [written](https://www.lesswrong.com/rationality) and [in-person curricula](https://rationality.org/) for improving your ability to reason for more than a decade. Spencer’s work is contributing to those curricula, an important task. And yet, I don’t think that people’s main failure point is in procuring educational material.\n\nI think that people *don’t want* to use explicit reason. And if they want to, they fail. And if they start succeeding, they’re punished. And if they push on, they get scared. And if they gather their courage, they hurt themselves. And if they make it to the other side, their lives enriched and empowered by reason, they will forget the hard path they walked and will wonder incredulously why everyone else doesn’t try using reason for themselves.\n\nThis post is about that hard path.\n\n![](https://putanumonit.files.wordpress.com/2020/09/path-rationality-2.png?w=900)\n\n*The map is not the territory*\n\nAlternatives to Reason\n----------------------\n\nWhat do I mean by ***explicit reason**?* I don’t refer merely to “System 2”, the brain’s slow, sequential, analytical, fully conscious, and effortful mode of cognition. I refer to the *informed* application of this type of thinking. Gathering data with real effort to find out, crunching the numbers with a grasp of the math, modeling the world with testable predictions, reflection on your thinking with an awareness of biases. Reason requires good inputs and a lot of effort.\n\nThe two main alternatives to explicit reason are ***intuition*** and ***social cognition***.\n\nIntuition, sometimes referred to as “System 1”, is the way your brain produces fast and automatic answers that you can’t explain. It’s how you catch a ball in flight, or get a person’s “vibe”. It’s how you tell at a glance the average length of the lines in the picture below but not the sum of their lengths. It’s what makes you fall for [the laundry list of heuristics and biases](https://en.wikipedia.org/wiki/List_of_cognitive_biases) that were the focus of LessWrong Rationality in the early days. Our intuition is shaped mostly by evolution and early childhood experiences.\n\n![](https://putanumonit.files.wordpress.com/2020/09/average-length-lines.png?w=332)\n\nSocial cognition is the set of ideas, beliefs, and behaviors we employ to fit into, gain status in, or signal to groups of people. It’s often intuitive, but it also makes you [ignore your intuition about line lengths and follow the crowd](https://en.wikipedia.org/wiki/Asch_conformity_experiments) in conformity experiments. It’s often unconscious — the memes a person believes ([or believes that they believe](https://wiki.lesswrong.com/wiki/Belief_in_belief#:~:text=Dennett%20calls%20this%20%22belief%20in%20belief%22.&text=Belief%20in%20belief%20is%20a,drives%20your%20anticipation%20of%20experience.)) for political expediency often just seem *unquestionably true* from the inside, even as they change and flow with the tides of group opinion.\n\nSocial cognition has been the main focus of Rationality in recent years, especially since the publication of [*The Elephant in the Brain*](http://elephantinthebrain.com/). Social cognition is shaped by the people around you, the media you consume (especially when consumed with other people), the prevailing norms.\n\n![](https://putanumonit.files.wordpress.com/2020/09/asch-conformity.jpg?w=480)\n\nRationalists got COVID right by using **explicit reason**. We thought probabilistically, and so took the pandemic seriously when it was merely possible, not yet certain. We did the math on exponential growth. We read research papers ourselves, trusting that science is a matter of legible knowledge and not the secret language of elevated experts in lab coats. We noticed that *what is fashionable to say about COVID* doesn’t track well with *what is useful to model and predict COVID*.\n\nOn February 28th, famous nudger Cass Sunstein told everyone that [the reason they’re “more scared about COVID than they have any reason to be”](https://www.bloomberg.com/opinion/articles/2020-02-28/coronavirus-panic-caused-by-probability-neglect) is the  cognitive bias of *probability neglect*. He talked at length about university experiments with electric shocks and gambles, but *neglected* to calculate any actual *probabilities* regarding COVID.\n\nWhile Sunstein was talking about the failures of **intuition*****,*** he failed entirely due to **social cognition**. When the article was written, prepping for COVID was associated with low-status China-hating reactionaries. The social role of progressive academics writing in progressive media was to mock them, and the good professor obliged. In February people like Sunstein mocked people for worrying about COVID in general, in March they mocked them for buying masks, in April they mocked them for hydroxychloroquine, in May for going to the beach, in June for *not* wearing masks. When someone’s view of COVID is shaped mostly by how their tribe mocks the outgroup, that’s social cognition.\n\nUnderperformance Swamp\n----------------------\n\nThe reason that intuition and social cognition are so commonly relied on is that they often work. Doing simply what feels right is usually good enough in every domain you either trained for (like playing basketball) or evolved for (like recoiling from snakes). Doing what is normal and fashionable among your peers is good enough in every domain your culture has mastered over time (like cooking techniques). It’s certainly good for your own social standing, which is often the main thing you care about.\n\nExplicit rationality outperformed both on COVID because responding to a pandemic in the information age is a very unusual case. It’s novel and complex, long on available data and short on trustworthy analysis, abutting on many spheres of life without being adequately addressed by any one of them. In most other areas reason does not have such an inherent advantage.\n\nMany Rationalists have a background in one of the few other domains where explicit reason outperforms, such as engineering or the exact sciences. This gives them some training in its application, training that most people lack. Schools keep talking about imparting “critical thinking skills” to all students but can scarcely point to much success. One wonders if they’re really motivated to try — will a teacher really have an easier time with 30 individual *critical thinkers* rather than a class of [password-memorizers](https://www.lesswrong.com/posts/NMoLJuDJEms7Ku9XS/guessing-the-teacher-s-password)?\n\nThen there’s the fact that most people engaged enough to [answer a LessWrong survey](https://www.lesswrong.com/posts/pJJdcZgB6mPNWoSWr/2013-survey-results#B__Can_we_finally_resolve_this_IQ_controversy_that_comes_up_every_year_) score in the *top percentile* on IQ tests and the SAT. Quibble as you may with those tests, insofar as they measure anything at all they measure the ability to solve problems using explicit reason. And that ability varies very widely among people.\n\nAnd so most people who are newly inspired to solve their problems with explicit reason fail. Doubly so since most problems people are motivated to solve are complicated and intractable to System 2 alone: making friends, losing weight, building careers, improving mental health, getting laid. And so the first step on the path to rationality is dealing with rationality’s initial failure to outperform the alternatives.\n\n![](https://putanumonit.files.wordpress.com/2020/10/swamp.jpg?w=1024)\n\n*Watch out for the alligators of rationalization camouflaged as logs of rationality*\n\nSinkholes of Sneer\n------------------\n\nWhether someone gives up after their initial failure or perseveres to try again depends on many factors: their personality, context, social encouragement or discouragement. And society tends to be discouraging of people trying to reason things out for themselves.\n\nAs [Zvi wrote](https://www.lesswrong.com/posts/z8usYeKX7dtTWsEnk/more-dakka), applying reason to a problem, even a simple thing such as doing *more* of what is already working, is an implicit accusation against everyone who didn’t try it. The mere attempt implies that you think those around you were too dumb to see a solution that required no gifts or revelations from higher authority, but mere *thought*.\n\nThe loudest sneers of discouragement come from those who tried reason for themselves, and failed, and gave up, and declared publicly that “reason” is a futile pursuit. Anyone who succeeds where they failed indicts not merely their intelligence but their courage.\n\nMany years ago, Eliezer wrote about trying [the Shangri-La diet](https://www.lesswrong.com/posts/BD4oExxQguTgpESdm/the-unfinished-mystery-of-the-shangri-la-diet), a strange method based on a novel theory of metabolic “set points” and flavor-calorie dissociation. Many previous casualties of fad diets scoffed at this attempt not because they spotted a clear flaw in the Shangri-La theory, but at Eliezer’s mere hubris at trying to *outsmart* dieting and lose weight without applying willpower.\n\n*Oh, you think you’re so much smarter? Well let me tell you…*\n\nA person who is just starting (and mostly failing) to apply explicit reason doesn’t have confidence in their ability, and is very vulnerable to social pressure. The are likely to persevere only in a “safe space” where attempting rationality is strongly endorsed and everything else is devalued. In most normal communities the social pressure against it is simply too strong.\n\nThis is I think is the main purpose of LessWrong and the Rationalist community, and similar clubs throughout history and around the world. To outsiders it looks like a bunch of aspie nerds who severely undervalue tact, tradition, intuition, and politeness, building an awkward and exclusionary “[ask culture](https://www.lesswrong.com/posts/vs3kzjLhbdKsndnBy/ask-and-guess)“. They’re not entirely wrong. These norms are too skewed in favor of explicit reason to be ideal, and mature rationalists eventually shift to more “normie” norms with their friends. But the nerd norms are just skewed enough to push the aspiring rationalist to practice the craft of explicit reason, [like a martial arts dojo](https://www.lesswrong.com/posts/teaxCFgtmCQ3E9fy8/the-martial-art-of-rationality).\n\nStrange Status and Scary Memes\n------------------------------\n\nBut not all is smooth sailing in the dojo, and the young rationalist must navigate strange status hierarchies and bewildering memeplexes. I’ve seen many people bounce off the Rationalist community over those two things.\n\nOn the status front, [the rightful caliph of rationalists](https://slatestarcodex.com/2016/04/04/the-ideology-is-not-the-movement/) is Eliezer Yudkowsky, widely perceived outside the community to be brash, arrogant, and lacking charisma. Despite the fact of his caliphdom, arguing publicly with Eliezer is one of highest-status things a rationalist can do, while merely citing him as an authority is disrespected.\n\nPeople like Scott Alexander or [Gwern Branwen](https://www.gwern.net/index) are likewise admired despite many people not even knowing what they look like. Attributes that form the basis of many status hierarchies are heavily discounted: wealth, social grace, credentials, beauty, number of personal friends, physical shape, humor, adherence to a particular ideology. Instead, respect often flows from disreputable hobbies such as *blogging*.\n\nI think that people often don’t realize that their discomfort with rationalists comes down to this. Every person cares deeply and instinctively about respect and their standing in a community. They are distressed by status hierarchies they don’t know how to navigate.\n\n![](https://putanumonit.files.wordpress.com/2020/10/ghost-stories-smoky-mountains.jpg?w=700)\n\n*And I’m not even mentioning the strange sexual dynamics*\n\nAnd if that wasn’t enough, rationalists believe some really strange things. The sentence *“AI may kill all humans in the next decade, but we could live forever if we outsmart it — or freeze our brains”* is enough to send most people packing.\n\nBut even less outlandish ideas cause trouble. The creator of rationality’s most famous infohazard observed that [any idea can be an infohazard](https://twitter.com/RokoMijicUK/status/1309048317674913794?ref_src=twsrc%5Etfw) to someone who derives utility or status from lying about it. Any idea can be hazardous to to someone who lacks a solid epistemology to integrate it with.\n\nIn June a young woman filled out [my hangout form](https://forms.gle/gFwfro9zGdFrMK2D6), curious to learn more about rationality. She’s bright, scrupulously honest, and takes ideas very seriously, motivated to figure out how the world really works so that she can make it better. We spent hours and hours discussing every topic under the sun. I really liked her, and saw much to admire.\n\nAnd then, three months later, she told me that she doesn’t want to spend time with me or any rationalists anymore because she picked up from us beliefs that cause her serious distress and anxiety.\n\nThis made me very sad also perplexed, since the specific ideas she mentioned seem quite benign to me. One is that IQ is real, in the sense that people differ in cognitive potential in a way that is hard to change as adults and that affects their potential to succeed in certain fields.\n\nAnother is that most discourse in politics and the culture war can be better understood as signaling, a way for people to gain acceptance and status in various tribes, than as behavior directly driven by an ideology. Hypocrisy is not an unusually damning charge, but the human default.\n\nTo me, these beliefs are entirely compatible with a normal life, a normal job, a wife, two guinea pigs, and many non-rationalist friends. At most, they make me stay away from pursuing cutting-edge academic mathematics (since I’m not smart enough) and from engaging political flame wars on Facebook (since I’m smart enough). Most rationalist believe these to some extent, and we don’t find it particularly remarkable.\n\nBut my friend found these ideas destabilizing to her self-esteem, her conception of her friends and communities, even her basic values. It’s as if they knocked out the ideological scaffolding of her personal life and replaced it with something strange and unreliable and ominous. I worried that my friend shot right past the long path of rationality and into the valley of disintegration.\n\nValley of Disintegration\n------------------------\n\nIt [has been observed](https://www.lesswrong.com/posts/aPEfdgzukPcJCui9D/my-experience-with-the-rationalist-uncanny-valley) that some young people appear to get worse at living *and* at thinking straight soon after learning about rationality, biases, etc. We call it the [*valley of bad rationality*](https://wiki.lesswrong.com/wiki/Valley_of_bad_rationality?_ga=2.184152864.701534588.1602105982-1803712960.1566224957).\n\nI think that the root cause of this downturn is people losing touch entirely with their intuition and social cognition, replaced by trying to make or justify every single decision with explicit reasoning. This may come from being overconfident in one’s reasoning ability after a few early successes, or by anger at all the unreasoned dogma and superstition one has to unlearn.\n\nA common symptom of the valley are [bucket errors](https://www.lesswrong.com/tag/bucket-errors), when beliefs that don’t necessarily imply one another are entangled together. Bucket errors can cause extreme distress or make you [flinch away from entire topics to protect yourself](https://www.lesswrong.com/posts/EEv9JeuY5xfuDDSgF/flinching-away-from-truth-is-often-about-protecting-the). I think this may have happened to my young friend.\n\nMy friend valued her job, and her politically progressive friends, and people in general, and making the world a better place. These may have become entangled, for example by thinking that she values her friends *because* their political activism is rapidly improving the world, or that she cares about people in general because they each have the potential to save the planet if they worked hard. Coming face to face with the ideas of innate ability and politics-as-signaling while holding on to these bucket errors could have resulted in a sense that her job is useless, that most people are useless, and that her friends are evil. Since those things are unthinkable, she flinched away.\n\nOf course, one can find good explicit reasons to work hard at your job, socialize with your friends, and value each human as an individual, reasons that have little to do with grand scale world-improvement. But while this is useful to think about, it often just ends up pushing bucket errors into other dark corners of your epistemology.\n\nPeople *just like their friends*. It simply feels right. It’s what everyone does. The way out of the valley is to not to reject this impulse for lack of journal citations but to *integrate* your deep and sophisticated friend-liking mental machinery with your explicit rationality and everything else.\n\n![](https://putanumonit.files.wordpress.com/2020/10/desert-skull.jpg?w=852)\n\n*Don’t lose your head in the valley*\n\nThe way to progress in rationality is not to use explicit reason to brute-force every problem but to use it to integrate *all* of your mental faculties: intuition, social cognition, language sense, embodied cognition, trusted authorities, visual processing… The place to start is with the ways of thinking that served you well before you stumbled onto a rationalist blog or some other gateway into a method and community of explicit reasoners.\n\nThis idea commonly goes by [**metarationality**](https://drossbucket.com/2017/09/30/metarationality-a-messy-introduction/), although it’s certainly present in the original [Sequences](https://www.lesswrong.com/rationality) as well. It’s a good description for what the Center for Applied Rationality teaches — [here’s an excellent post](https://www.lesswrong.com/posts/byewoxJiAfwE6zpep/reality-revealing-and-reality-masking-puzzles) by one of CFAR’s founders about the valley and the (meta)rational way out.\n\nMetarationality is a topic for more than two paragraphs, perhaps for an entire lifetime. I have risen out of the valley — my life [is demonstrably better](https://putanumonit.com/2019/12/08/rationalist-self-improvement/) than before I discovered LessWrong — and the metarationalist climb is the path I see ahead of me.\n\nAnd behind me, I see all of this.\n\n![](https://putanumonit.files.wordpress.com/2020/09/path-rationality-2.png?w=1024)\n\nSo what to make of this tortuous path? If you’re reading this you are quite likely already on it, trying to figure out how to figure things out and dealing with the obstacles and frustrations. If you’re set on the goal that this post may offer some advice to help you on your way: try again after the early failures, ignore the sneers, find a community with good norms, and don’t let the memes scare you — it all adds up to normalcy in the end. Let reason be the instrument that sharpens your other instruments, not the only tool in your arsenal.\n\nBut the difficulty of the way is mostly one of motivation, not lack of instruction. Someone not inspired to rationality won’t become so by reading about the discouragement along the way.\n\nAnd that’s OK.\n\nPeople’s distaste for explicit reason is not a modern invention, and yet our species is doing OK and getting along. If the average person uses explicit reason only 1% of the time, the metarationalist learns that she may up that number to 3% or 5%, not 90%. Rationality doesn’t make one a member of a different species, or superior at all tasks.\n\nThe rationalists pwned COVID, and this may certainly inspire a few people to join the tribe. As for everyone else, it’s fine if this success merely raises our public stature a tiny bit, lets people see that weirdos obsessed with explicit reason have something to contribute. Hopefully it will make folk slightly more likely to listen to the next nerd trying to tell them something using words like “likelihood ratio” and “countersignaling”.\n\nBecause if you think that COVID was really scary and our society dealt with it really poorly — [boy, have we got some more things to tell you](https://www.lesswrong.com/tag/ai-risk).\n\n![](https://i5.walmartimages.com/asr/3eb51e90-bc42-4273-8905-9ed611f9d9d4_1.1e0160e13c243c191b59754dc5a50552.jpeg?odnHeight=2000&odnWidth=2000&odnBg=ffffff)"
    },
    "voteCount": 98
  },
  {
    "_id": "uyBeAN5jPEATMqKkX",
    "url": null,
    "title": "Lies Told To Children",
    "slug": "lies-told-to-children-1",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "Fiction"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "Growing up, as a kid, I was always told that every sapient life is precious, everything that thinks and knows itself -\n\nYes, this *is* a tale about lies-told-to-children.  You'll probably figure it out yourself before too long.  For now, just listen.\n\nWhere was I?  Right.  As children, we were always told that every sapient life is precious.  It was told to us by the teachers, and shown to us in children's television - though I saw less children's television than most children in our age cohort - children's TV was censored where I grew up, though, of course, I didn't find that out until much later -\n\nI see you're starting to guess under what sort of circumstances I grew up.  Go ahead, write down the prediction if you want.  Maybe you already see where this entire thing is headed.  But you asked me for a story about the lies I was told as a child, and that's what you're getting.  It's not my fault, if a lot of stories like that are predictable; people who lie to children have other things to optimize for than unpredictability.\n\nSo where was I?  Right.  I grew up in a remote village of about three thousand people, the sort that's more hills than houses.  Charming travel-pathways that cut through forests.  Not everyone knows everyone, but you sure know somebody who knows anybody.\n\nChildren's television in my region was censored, though of course they didn't tell us that as children.  But the children's television that we saw had aliens and monsters and creatures of fantasy, with four legs or fourteen legs, three faces or no face at all, and all of them were treated by the television show as having lives that meant something.  Sometimes in the children's show there were alien monsters who only thought their own kind of life was valuable, and then maybe you couldn't trade with them as friends.  Maybe they'd already lied to you once and you couldn't trust them enough to bargain with them, maybe you couldn't talk to them at all.  But their lives still had meaning to the story's human protagonists, even some aliens whose lives had no meaning to themselves.  You didn't cause them pain if there was any way to avoid it; you didn't kill them unless their biology was sufficiently similar to human that you were confident in your ability to cryopreserve them afterwards.\n\nThe shows never spelled it out, never said, 'And this is because of a universal rule in every case that sapient life has value.'  Our teachers said that explicitly, though.\n\nAnd they treated every one of us children, too, as if our lives had meaning.\n\nExcept the children with the red hair; those dirty reds.\n\nYou're nodding along with a knowing look, I see.  Was it what you predicted?  Not exactly, maybe, but rough ballpark?  I suppose I'll find out when we open your prediction afterwards.\n\nThe red-haired children hardly needed the red hair, as their targeting-mark; they looked different from the rest of us in other ways too.  When I was old enough to first ask, I was told that they were the children's children of people who'd been exiled from a faraway city for committing terrible crimes there, who'd been given sanctuary by the grace and mercy of our own benevolent kind.  The red-haired children tended bigger than the rest of us, with more adult facial structures, to the point where you could've maybe mistaken them for very small adults in disguise.  The red-haired adults, what few of them we ever saw, were correspondingly huge and muscular.  You could see, in retrospect - if you were actually trying to think at all, which we weren't really - how somebody might have felt threatened by such big muscular people, even while graciously granting them sanctuary.\n\nThere weren't many of the red-haired children being educated alongside us; a handful, four or six.  I can't recall how many by counting names, because they kept to themselves and did not try to be friends with the rest of us.\n\nThey were slower than the rest of us in class to answer.  On the rare occasions a teacher called on them, they'd often get the question wrong.  We were kids, young kids, so of course we didn't ask ourselves anything like \"Is this in fact an intrinsic deficit of intelligence or is it a self-fulfilling prophecy about who gets more effort from the teachers?\" or come up with any experiments to test that one way or another.  We just wordlessly thought that red-haired kids were stupider; and that this too was a universal rule just like gravity.\n\nWe did not, in fact, treat our red-headed fellow kids all that well.  We were of an age where kids take their cues from adults without carefully rethinking everything they're seeing.  We noticed how the older kids treated red-haired kids, we noticed how teachers treated red-haired kids, we noticed the huge red-headed adults who were silently sweeping the hallways and not doing any intellectual labor.  We noticed how the adult reds got casually shoved aside by other adults or even non-red-headed older kids, and how the red-headed adults just silently took that.\n\nThere were names to call them, 'dirty reds', worse things than that, scatological profanities to giggle over amongst ourselves.\n\nNow and then you'd see a Security Officer come by and ask some reds some questions.  One time Security took one of the janitors away, and then after that, nobody ever saw him again.  I think one of the kids did ask, in class, what happened to that guy, and the teacher shut her right down and said that any questions about dirty reds or for that matter Security were things best asked in private if you asked at all.\n\nAnd meanwhile the television shows, those that we got to watch, went right on teaching the lesson that all sapient life is precious, with no exceptions for fourteen legs or not having a face.\n\nEventually, of course, it started coming to a point, and then it did come to a point.\n\nIt started to come to a point, at the point where a red-haired kid was called on in class and answered a question wrong, and the teacher asked if his parents were too busy stealing other people's books to teach him how to read.  The red-haired kid didn't say anything back, but I flinched, visibly.\n\nIt came to the point, two days after that, when I was walking home from class, and I heard a groan from off the pathway home, what sounded like a moan of pain.\n\nI left the pathway and ran around a hill to find one of those dirty reds hiding behind it, with blood all over his left pants-leg.\n\nHe asked me not to get an adult.\n\nHe said that he was hiding from Security.\n\nHe asked me to help him walk, help him get away.\n\nIt didn't feel real.  It felt like I was inside one of the children's television shows.\n\nOf course, in children's television shows, they always show the heroes reminding themselves that things are real and that they've got to do what's right, because it's real, so I knew that I needed to remember that this was real because that's what you do when you're inside a television show.\n\nI think I was probably very scared, though I don't remember noticing myself being scared.\n\nI asked him what he'd done to get Security looking for him.\n\nHe said that he had, a few days ago, said something about red-haired people deserving better treatment than they currently got, around a non-red-haired person he'd thought, hoped, was a friend.\n\nI gave him a hand so he could stand up, on the leg that wasn't covered with blood, and then he leaned on me and we hopped away through the hills until we got to where a red-haired woman - you saw fewer of those - whispered a thank-you to me and took him away with herself.\n\nI ran back to the pathway and ran home, though I was still late, of course.  My dad asked me where I'd been and I said I'd seen a funny-looking butterfly and run off to chase it.  I remember believing, even then, that he knew I was lying, but dad didn't ask me any more questions, and I didn't tell him anything.\n\nAbout an hour later, Security knocked on our door and asked everyone if they'd seen a red-haired person who looked like - and of course the picture was of the man I'd helped to get away.\n\nI said no, I hadn't seen him.  But because I was a kid and kids that age aren't taught theory-of-deception, I asked what the man had done and if he was considered dangerous.  And I didn't think, until too late, about whether that was something I was much more likely to ask if -\n\nThe Security officer asked me if I maybe wanted to change my mind about having seen the fugitive.\n\nI gave him my best surprised look and said no.\n\nThe Security officer noted that Security officers get special training in reading emotions, and I seemed pretty frightened to him.\n\nI said yes, I was, because the Security officer was suggesting that I was lying and that was scary.\n\nThe Security officer said he knew perfectly well, at this point, that I was lying.  But I wouldn't end up in trouble if I showed him where the fugitive went and identified anyone else he was with.\n\nI said that he didn't know what he was talking about.\n\nThe Security officer gave me a sort of stern look and said that he'd detected another lie, and did I *really* want to get in trouble for some dirty red.\n\nI told him that I wasn't stupid and I knew he was bluffing, to try to trick me, because he suspected me, even though I hadn't done it.\n\nHe took a photo out of his pocket and showed it to me.\n\nIt was me helping the red-haired man walk on his one good leg.\n\n*Why*, said the Security.  He just looked sad, now.  Why *had* I done it?  Why was a dirty red worth it?\n\nAnd I remember, by *that* point, that I'd noticed I was scared, and I think I *was* trying to get out of it - by proving that I was, in the end, obeying adult authority - when I said that we'd all been told in class that every sapient life is precious, everything that thinks and knows itself, that was the rule we'd been given, and nobody had reasonably *argued* at any point that there was an exception for people with red hair, and *also* we'd all been told that hurting people is wrong and you shouldn't let social conformity push you into it.\n\nThe rest of it went the way you'd expect.\n\nThe Security officer smiled.\n\nMy parents rushed in and hugged me and told me I'd been *so* brave and *so* good and scored in what would've been the upper 5th percentile twenty years ago for the age where I started to object and not go along with it anymore; and explained about Civilization needing to test *some* kids now and then, to find out how well we were doing environment-wise and heredity-wise on people's kindness and resistance to conformity-pushed cruelty; and test against an earlier-reported bug where general rules about fair and okay treatment of people would somehow end up not being applied to some subgroup; and our little village was settling an important conditional prediction market from twenty years earlier, that had millions of labor-hours wagered on it; and that children growing up to be good people was a vital figure-of-merit for all of Civilization and lots of big policy decisions turned around it, which was why it had been worth specializing our village to do Science about that, and they hoped I understood all that and wouldn't tell the other children right away.  There wasn't actually any such thing as Security, and if there ever *was* it would mean that it was time to overthrow the government immediately.\n\nI nodded along in a wise, understanding, and rather numb fashion.  I think the main thing I said, at the end, was that I'd better be getting paid for this, and they all laughed and said of course I was, lots of money, at least as much as my parents were getting, because children are sapient beings too.\n\nSo that's *my *story about the-lies-we-tell-to-children.  And the part that I value now the most, even more than the money I got then and when I was older, even more than knowing that I was good and brave in the only sort of real test that most people in Civilization ever get, is that I approximately always win any Lies-Told-To-Children storytelling night."
    },
    "voteCount": 245
  },
  {
    "_id": "eR7Su77N2nK3e5YRZ",
    "url": null,
    "title": "The LessWrong Team is now Lightcone Infrastructure, come work with us!",
    "slug": "the-lesswrong-team-is-now-lightcone-infrastructure-come-work-3",
    "author": "habryka",
    "question": false,
    "tags": [
      {
        "name": "Site Meta"
      },
      {
        "name": "Lightcone Infrastructure"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "*tl;dr: The LessWrong team is re-organizing as* [*Lightcone Infrastructure*](http://www.lightconeinfrastructure.com/)*. LessWrong is one of several projects we are working on to ensure the future of humanity goes well. We are looking to hire software engineers as well as generalist entrepreneurs in Berkeley who are excited to build infrastructure to ensure a good future.*\n\nI founded the LessWrong 2.0 team in 2017, with the goal of reviving LessWrong.com and reinvigorating the intellectual culture of the rationality community. I believed the community had great potential for affecting the long term future, but that the failing website was a key bottleneck to community health and growth.\n\nFour years later, the website still seems very important. But when I step back and ask “what are the key bottlenecks for improving the longterm future?”, just ensuring the website is going well no longer seems sufficient.\n\nFor the past year, I’ve been re-organizing the LessWrong team into something with a larger scope. As I’ve learned from talking to over a thousand of you over the last 4 years, for most of you the rationality community is much larger than just this website, and your contributions to the future of humanity more frequently than not route through many disparate parts of our sprawling diaspora. Many more of those parts deserve attention and optimization than just LessWrong, and we seem to be the best positioned organization to make sure that happens.\n\nI want to make sure that that whole ecosystem is successfully steering humanity towards safer and better futures, and more and more this has meant working on projects that weren't directly related to LessWrong.com: \n\n*   A bit over a year ago we started building grant-making software for Jaan Tallinn and the Survival and Flourishing Fund, helping distribute over 30 million dollars to projects that I think have the potential to have a substantial effect on ensuring a flourishing future for humanity.\n*   We helped run dozens of online meetups and events during the pandemic, and hundreds of in-person events for both this year and 2019s ACX Meetups everywhere\n*   We helped build and run the EA Forum and the AI Alignment Forum,\n*   We recently ran a 5-day retreat for 60-70 people whose work we think is highly impactful in reducing the likelihood of humanity's extinction,\n*   We opened an in-person office space in the Bay Area for organizations that are working towards improving the long-term future of humanity.\n\nAs our projects outside of the LessWrong.com website multiplied, our name became more and more confusing when trying to explain to people what we were about. \n\nThis confusion reached a new peak when we started having a team that we were internally calling the \"LessWrong team\", which was responsible for running the website, distinct from all of our other projects, and which soon after caused me to utter the following sentence at one of our team meetings: \n\n> LessWrong really needs to figure out what the LessWrong team should set as a top priority for LessWrong\n\nAs one can imagine, the reaction from the rest of the team was confusion and laughter and at that point I knew we had to change our name and clarify our organizational mission. \n\nSo, after doing many rounds of coming up with names, asking many of our colleagues and friends (including GPT-3) for suggestions, we finally decided on:\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/19f81bb3a671af655ea5d7ac08f1916c211ec158b55f78c7.png)\n\nI like the light cone as a symbol, because it represents the massive scale of opportunity that humanity is presented with. If things go right, we can shape almost the full light cone of humanity to be full of flourishing life. Billions of galaxies, billions of light years across, for some 10^36 (or so) years until the heat death of the universe. \n\nSeparately, I am excited about where Lightcone Infrastructure is headed as an organization. I really enjoy working with the team, and I feel like there is a ton of low-hanging fruit in doing more end-to-end community optimization. This community of rationalists, effective altruists and longtermists has achieved an enormous amount, both in scale of impact, and in coming to a deeper understanding about the world, and I think our work in reviving LessWrong and our other infrastructure projects have already made a big difference in that success.\n\nLessWrong will have a dedicated team within Lightcone Infrastructure. Ruby will be taking the lead on that, and he already has a number of great plans for the website that I expect he will tell you about in the near future. The current team and structure is: \n\n*   [Oliver Habryka](https://www.lesswrong.com/users/habryka4) (CEO)\n*   Campus team:\n    *   [Jacob Lagerros](https://www.lesswrong.com/users/jacobjacob)\n    *   [Ben Pace](https://www.lesswrong.com/users/benito)\n    *   [Raymond Arnold](https://www.lesswrong.com/users/raemon)\n*   Site team:\n    *   [Ruben Bloom (Ruby)](https://www.lesswrong.com/users/ruby)\n\n[Jim Babcock](https://www.lesswrong.com/users/jimrandomh) is also paid by us as an independent Open Source contributor to the LessWrong website, and helps a lot with development. I also still fix bugs, answer support requests and write code, though I primarily spend my time on management these days.\n\nIf you want to work with us on these projects, we are hiring for three positions: \n\n*   A software engineer for LessWrong.com to assist with maintenance and expansion of the Rationalist community's online publishing hub ([more info](https://www.lightconeinfrastructure.com/lesswrong-software.html))\n*   A generalist to join our new campus team to build a thriving in-person rationality and longtermism community in the Bay Area ([more info](https://www.lightconeinfrastructure.com/campus-generalist.html))\n*   A software engineer and product manager to be in charge of the \"S-Process\" application, a suite of custom software for grantmaking that we develop for Jaan Tallinn and the Survival and Flourishing Fund ([more info](https://www.lightconeinfrastructure.com/s-process-developer.html))\n\nWe are also open to hiring people who don't fit into any of these positions, so err on the side of applying if you want to work with us. If you have thoughts on how to build a successful rationality and longtermism community, want to build a 1000-person strong campus, or have a pitch for a different infrastructure project we should run, reach out to us, and we would be excited to talk to you about working here.\n\nOur current salary policy is to pay rates competitive with industry salary minus 30%. Given prevailing salary levels in the Bay Area for the kind of skill level we are looking at, we expect salaries to start at $150k/year plus healthcare (but we would be open to paying $315k for someone who would make $450k in industry). We also provide a generous relocation package if you aren't currently located in the Bay Area.\n\nApply here: [https://airtable.com/shrdqS6JXok99f6EX](https://airtable.com/shrdqS6JXok99f6EX)"
    },
    "voteCount": 107
  },
  {
    "_id": "5gfqG3Xcopscta3st",
    "url": null,
    "title": "Building up to an Internal Family Systems model",
    "slug": "building-up-to-an-internal-family-systems-model",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "Motivations"
      },
      {
        "name": "Subagents"
      },
      {
        "name": "Emotions"
      },
      {
        "name": "Therapy"
      },
      {
        "name": "Internal Family Systems"
      }
    ],
    "tableOfContents": {
      "sections": [
        {
          "title": "Introduction",
          "anchor": "Introduction",
          "level": 1
        },
        {
          "title": "Epistemic status",
          "anchor": "Epistemic_status",
          "level": 2
        },
        {
          "title": "Wanted: a robot which avoids catastrophes",
          "anchor": "Wanted__a_robot_which_avoids_catastrophes",
          "level": 1
        },
        {
          "title": "Introducing managers",
          "anchor": "Introducing_managers",
          "level": 2
        },
        {
          "title": "Putting together a toy model",
          "anchor": "Putting_together_a_toy_model",
          "level": 2
        },
        {
          "title": "Consequences of the model",
          "anchor": "Consequences_of_the_model",
          "level": 2
        },
        {
          "title": "The Internal Family Systems model",
          "anchor": "The_Internal_Family_Systems_model",
          "level": 1
        },
        {
          "title": "Personalized characters",
          "anchor": "Personalized_characters",
          "level": 2
        },
        {
          "title": "Why should this technique be useful for psychological healing?",
          "anchor": "Why_should_this_technique_be_useful_for_psychological_healing_",
          "level": 2
        },
        {
          "title": "The Self",
          "anchor": "The_Self",
          "level": 2
        },
        {
          "title": "Final words",
          "anchor": "Final_words",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        },
        {
          "anchor": "comments",
          "level": 0,
          "title": "84 comments"
        }
      ],
      "headingsCount": 13
    },
    "contents": {
      "markdown": "Introduction\n============\n\n[Internal Family Systems (IFS)](https://selfleadership.org/about-internal-family-systems.html) is a psychotherapy school/technique/model which lends itself particularly well for being used alone or with a peer. For years, I had noticed that many of the kinds of people who put in a lot of work into developing their emotional and communication skills, some within the rationalist community and some outside it, kept mentioning IFS.\n\nSo I looked at the [Wikipedia page about the IFS model](https://en.wikipedia.org/wiki/Internal_Family_Systems_Model), and bounced off, since it sounded like nonsense to me. Then someone brought it up again, and I thought that maybe I should reconsider. So I looked at the WP page again, thought “nah, still nonsense”, and continued to ignore it.\n\nThis continued until I participated in CFAR mentorship training last September, and we had a class on CFAR’s [Internal Double Crux](https://www.lesswrong.com/posts/mQmx4kQQtHeBip9ZC/internal-double-crux) (IDC) technique. IDC clicked really well for me, so I started using it a lot and also facilitating it to some friends. However, once we started using it on more emotional issues (as opposed to just things with empirical facts pointing in different directions), we started running into some weird things, which it felt like IDC couldn’t quite handle… things which reminded me of how people had been describing IFS. So I finally read up on it, and have been successfully applying it ever since.\n\nIn this post, I’ll try to describe and motivate IFS in terms which are less likely to give people in this audience the same kind of a “no, that’s nonsense” reaction as I initially had.\n\nEpistemic status\n----------------\n\nThis post is intended to give an argument for why _something like_ the IFS model _could_ be true and a thing that works. It’s not really an argument that IFS _is_ correct. My reason for thinking in terms of IFS is simply that I was initially super-skeptical of it (more on the reasons of my skepticism later), but then started encountering things which it turned out IFS predicted - and I only found out about IFS predicting those things _after_ I familiarized myself with it.\n\nAdditionally, I now feel that IFS gives me significantly more [gears](https://www.lesswrong.com/posts/B7P97C27rvHPz3s9B/gears-in-understanding) for understanding the behavior of both other people and myself, and it has been significantly transformative in addressing my own emotional issues. Several other people who I know report it having been similarly powerful for them. On the other hand, aside for a few isolated papers with titles like “[proof-of-concept](http://www.jrheum.org/content/early/2013/08/10/jrheum.121465)” or “[pilot study](https://onlinelibrary.wiley.com/doi/abs/10.1111/jmft.12184)”, there seems to be conspicuously little peer-reviewed evidence in favor of IFS, meaning that we should probably exercise some caution.\n\nI think that, even if not completely correct, IFS is currently the best model that I have for [explaining the observations that it’s pointing at](https://www.lesswrong.com/posts/MPj7t2w3nk4s9EYYh/incorrect-hypotheses-point-to-correct-observations). I encourage you to read this post [in the style of learning soft skills](https://www.lesswrong.com/posts/ZGzDNfNCXzfx6hYAH/how-to-learn-soft-skills) \\- trying on this perspective, and seeing if there’s anything in the description which feels like it resonates with your experiences.\n\nBut before we talk about IFS, let’s first talk about building robots. It turns out that if we put together some existing ideas from machine learning and neuroscience, we can end up with a robot design that pretty closely resembles IFS’s model of the human mind.\n\nWhat follows is an intentionally simplified story, which is simpler than _either_ the full IFS model _or_ a full account that would incorporate everything that I know about human brains. Its intent is to demonstrate that an agent architecture with IFS-style subagents might easily emerge from basic machine learning principles, without claiming that all the details of that toy model would exactly match human brains. A discussion of what exactly IFS _does_ claim in the context of human brains follows after the robot story.\n\nWanted: a robot which avoids catastrophes\n=========================================\n\nSuppose that we’re building a robot that we want to be generally intelligent. The hot thing these days seems to be [deep reinforcement learning](https://deepmind.com/blog/deep-reinforcement-learning/), so we decide to use that. The robot will explore its environment, try out various things, and gradually develop habits and preferences as it accumulates experience. (Just like those human babies.)\n\nNow, there are some problems we need to address. For one, deep reinforcement learning works fine in simulated environments where you’re safe to explore for an indefinite duration. However, it runs into problems if the robot is supposed to learn in a real life environment. Some actions which the robot might take will result in catastrophic consequences, such as it being damaged. If the robot is just doing things at random, it might end up damaging itself. Even worse, if the robot does something which could have been catastrophic but narrowly avoids harm, it might then forget about it and end up doing the same thing again!\n\nHow could we deal with this? Well, let’s look at the existing literature. [Lipton et al. (2016)](https://arxiv.org/abs/1611.01211) proposed what seems like a promising idea for addressing the part about forgetting. Their approach is to explicitly maintain a memory of _danger states_ \\- situations which are not the catastrophic outcome itself, but from which the learner has previously ended up in a catastrophe. For instance, if “being burned by a hot stove” is a catastrophe, then “being about to poke your finger in the stove” is a danger state. Depending on how cautious we want to be and how many preceding states we want to include in our list of danger states, “going near the stove” and “seeing the stove” can also be danger states, though then we might end up with a seriously stove-phobic robot.\n\nIn any case, we maintain a separate storage of danger states, in such a way that the learner never forgets about them. We use this storage of danger states to train a _fear model_: a model which is trying to predict the probability of ending up in a catastrophe from some given novel situation. For example, maybe our robot poked its robot finger at the stove in our kitchen, but poking its robot finger at stoves in other kitchens might be dangerous too. So we want the fear model to generalize from our stove to other stoves. On the other hand, we don’t want it to be stove-phobic and run away at the mere sight of a stove. The task of our fear model is to predict exactly how likely it is for the robot to end up in a catastrophe, given some situation it is in, and then make it increasingly disinclined to end up in the kinds of situations which might lead to a catastrophe.\n\nThis sounds nice in theory. On the other hand, Lipton et al. are still assuming that they can train their learner in a simulated environment, and that they can label catastrophic states ahead of time. We don’t know in advance every possible catastrophe our robot might end up in - it might walk off a cliff, shoot itself in the foot with a laser gun, be beaten up by activists protesting technological unemployment, or any number of other possibilities.\n\nSo let’s take inspiration from humans. We can’t know beforehand every bad thing that might happen to our robot, but we can identify some classes of things which are correlated with catastrophe. For instance, being beaten or shooting itself in the foot will cause physical damage, so we can install sensors which indicate when the robot has taken physical damage. If these sensors - let’s call them “pain” sensors - register a high amount of damage, we consider the situation to have been catastrophic. When they do, we save that situation and the situations preceding it to our list of dangerous situations. Assuming that our robot has managed to make it out of that situation intact and can do anything in the first place, we use that list of dangerous situations to train up a fear model.\n\nAt this point, we notice that this is starting to remind us about our experience with humans. For example, the infamous [Little Albert experiment](https://en.wikipedia.org/wiki/Little_Albert_experiment). A human baby was allowed to play with a laboratory rat, but each time that he saw the rat, a researcher made a loud scary sound behind his back. Soon Albert started getting scared whenever he saw the rat - and then he got scared of furry things in general.\n\nSomething like Albert’s behavior could be implemented very simply using something like [Hebbian conditioning](https://en.wikipedia.org/wiki/Hebbian_theory) to get a learning algorithm which picks up on some features of the situation, and then triggers a panic reaction whenever it re-encounters those same features. For instance, it registers that the sight of fur and loud sounds tend to coincide, and then it triggers a fear reaction whenever it sees fur. This would be a basic fear model, and a “danger state” would be “seeing fur”.\n\nWanting to keep things simple, we decide to use this kind of an approach as the fear model of our robot. Also, [having read _Consciousness and the Brain_](https://www.lesswrong.com/s/ZbmRyDN8TCpBTZSip/p/x4n4jcoDP7xh5LWLq), we remember a few basic principles about how those human brains work, which we decide to copy because we’re lazy and don’t want to come up with entirely new principles:\n\n*   There’s a special network of neurons in the brain, called the _global neuronal workspace_. The contents of this workspace are [roughly](https://www.greaterwrong.com/posts/x4n4jcoDP7xh5LWLq/book-summary-consciousness-and-the-brain/comment/BF34izs4bf49uTdey) the same as the contents of consciousness.\n*   We can thus consider consciousness a workspace which many different brain systems have access to. It can hold a single “chunk” of information at a time.\n*   The brain has multiple different systems doing different things. When a mental object becomes conscious (that is, is projected into the workspace by a subsystem), many systems will synchronize their processing around analyzing and manipulating that mental object.\n\nSo here is our design:\n\n*   The robot has a hardwired system scanning for signs of catastrophe. This system has several subcomponents. One of them scans the “pain” sensors for signs of physical damage. Another system watches the “hunger” sensors for signs of low battery.\n*   Any of these “distress” systems can, alone or in combination, feed a negative reward signal into the global workspace. This tells the rest of the system that this is a bad state, from which the robot should escape.\n*   If a certain threshold level of “distress” is reached, the current situation is designated as _catastrophic_. All other priorities are suspended and the robot will prioritize getting out of the situation. A memory of the situation and the situations preceding it are saved to a dedicated storage.\n*   After the experience, the memory of the catastrophic situation is replayed in consciousness for analysis. This replay is used to train up a separate fear model which effectively acts as a new “distress” system.\n*   As the robot walks around its environment, sensory information about the surroundings will enter its consciousness workspace. When it plans future actions, simulated sensory information about how those actions would unfold enters the workspace. Whenever the new fear model detects features in either kind of sensory information which it associates with the catastrophic events, it will feed “fear”-type “distress” into the consciousness workspace.\n\nSo if the robot sees things which remind it of poking at hot stove, it will be inclined to go somewhere else; if it imagines doing something which would cause it to poke at the hot stove, then it will be inclined to imagine doing something else.\n\nIntroducing managers\n--------------------\n\nBut is this actually enough? We've now basically set up an algorithm which warns the robot when it sees things which have previously preceded a bad outcome. This might be enough for dealing with static tasks, such as not burning yourself at a stove. But it seems insufficient for dealing with things like predators or technological unemployment protesters, who might show up in a wide variety of places and actively try to hunt you down. By the time you see a sign of them, you're already in danger. It would be better if we could learn to avoid them entirely, so that the fear model would never even be triggered.\n\nAs we ponder this dilemma, we surf the web and run across [this blog post](https://owainevans.github.io/blog/hirl_blog.html) summarizing [Saunders, Sastry, Stuhlmüller & Evans (2017)](https://arxiv.org/abs/1707.05173). They are also concerned with preventing reinforcement learning agents from running into catastrophes, but have a somewhat different approach. In their approach, a reinforcement learner is allowed to do different kinds of things, which a human overseer then allows or blocks. A separate “blocker” model is trained to predict which actions the human overseer would block. In the future, if the robot would ever take an action which the “blocker” predicts the human overseer would disallow, it will block that action. In effect, the system consists of two separate subagents, one subagent trying to maximize rewards and the other subagent trying to block non-approved actions.\n\nSince our robot has a nice modular architecture into which we can add various subagents which are listening in and taking actions, we decide to take inspiration from this idea. We create a system for spawning dedicated subprograms which try to predict and and block actions which would cause the fear model to be triggered. In theory, this is unnecessary: given enough time, even standard reinforcement learning should learn to avoid the situations which trigger the fear model. But again, trial-and-error can take a very long time to learn exactly which situations trigger fear, so we dedicate a separate subprogram to the task of pre-emptively figuring it out.\n\nEach fear model is paired with a subagent that we’ll call a _manager_. While the fear model has associated a bunch of cues with the notion of an impending catastrophe, the manager learns to predict which situations would cause the fear model to trigger. Despite sounding similar, these are not the same thing: one indicates when you are already in danger, the other is trying to figure out what you can do to never end up in danger in the first place. A fear model might learn to recognize signs which technological unemployment protesters commonly wear. Whereas a manager might learn the kinds of environments where the fear model has noticed protesters before: for instance, near the protester HQ.\n\nThen, if a manager predicts that a given action (such as going to the protester HQ) would eventually trigger the fear model, it will block that action and promote some other action. We can use the interaction of these subsystems to try to ensure that the robot only feels fear in situations which already resemble the catastrophic situation so much as to actually _be_ dangerous. At the same time, the robot will be unafraid to take safe actions in situations from which it _could_ end up in a danger zone, but are themselves safe to be in.\n\nAs an added benefit, we can recycle the manager component to also do the same thing as the blocker component in the Saunders et al. paper originally did. That is, if the robot has a human overseer telling it in strict terms not to do some things, it can create a manager subprogram which models that overseer and likewise blocks the robot from doing things which the model predicts that the overseer would disapprove of.\n\nPutting together a toy model\n----------------------------\n\nIf the robot _does_ end up in a situation where the fear model is sounding an alarm, then we want to get it out of the situation as quickly as possible. It may be worth spawning a specialized subroutine just for this purpose. Technological unemployment activists could, among other things, use flamethrowers that set the robot on fire. So let’s call these types of subprograms dedicated to escaping from the danger zone, _firefighters_.\n\nSo how does the system as a whole work? First, the different subagents act by sending into the consciousness workspace various mental objects, such as an emotion of fear, or an intent to e.g. make breakfast. If several subagents are submitting identical mental objects, we say that they are voting for the same object. On each time-step, one of the submitted objects is chosen at random to become the contents of the workspace, with each object having a chance to be selected that’s proportional to its number of votes. If a mental object describing a physical action (an “intention”) ends up in the workspace and stays chosen for several time-steps, then that action gets executed by a motor subsystem.\n\nDepending on the situation, some subagents will have more votes than others. E.g. a fear model submitting a fear object gets a number of votes proportional to how strongly it is activated. Besides the specialized subagents we’ve discussed, there’s also a default planning subagent, which is just taking whatever actions (that is, sending to the workspace whatever mental objects) it thinks will produce the greatest reward. This subagent only has a small number of votes.\n\nFinally, there’s a self-narrative agent which is [constructing a narrative](http://cogprints.org/266/1/selfctr.htm) of the robot’s actions as if it was a unified agent, for social purposes and for doing reasoning afterwards. After the motor system has taken an action, the self-narrative agent records this as something like “I, Robby the Robot, made breakfast by cooking eggs and bacon”, transmitting this statement to the workspace and saving it to an episodic memory store for future reference.\n\nConsequences of the model\n-------------------------\n\nIs this design any good? Let’s consider a few of its implications.\n\nFirst, in order for the robot to take physical actions, the intent to do so has to be in its consciousness for a long enough time for the action to be taken. If there are any subagents that wish to prevent this from happening, they must muster enough votes to bring into consciousness some other mental object replacing that intention before it’s been around for enough time-steps to be executed by the motor system. (This is analogous to the concept of the [final veto in humans](http://www.informationphilosopher.com/freedom/libet_experiments.html), where consciousness is the last place to block pre-consciously initiated actions before they are taken.)\n\nSecond, the different subagents do not see each other directly: they only see the consequences of each other’s actions, as that’s what’s reflected in the contents of the workspace. In particular, the self-narrative agent has no access to information about which subagents were responsible for generating which physical action. It only sees the intentions which preceded the various actions, and the actions themselves. [Thus it might easily end up constructing](https://www.facebook.com/Xuenay/posts/10153111724243662) a narrative which creates the internal appearance of a single agent, even though the system is actually composed of multiple subagents.\n\nThird, even if the subagents can’t directly see each other, they might still end up forming alliances. For example, if the robot is standing near the stove, a curiosity-driven subagent might propose poking at the stove (“I want to see if this causes us to burn ourselves again!”), while the default planning system might propose cooking dinner, since that’s what it predicts will please the human owner. Now, a manager trying to prevent a fear model agent from being activated, will eventually learn that if it votes for the default planning system’s intentions to cook dinner (which it saw earlier), then the curiosity-driven agent is less likely to get _its_ intentions into consciousness. Thus, no poking at the stove, and the manager’s and the default planning system’s goals end up aligned.\n\nFourth, this design can make it _really difficult_ for the robot to even become aware of the existence of some managers. A manager may learn to support any other mental processes which block the robot from taking specific actions. It does it by voting in favor of mental objects which orient behavior towards anything else. This might manifest as something subtle, such as a mysterious lack of interest towards something that sounds like a good idea in principle, or just repeatedly forgetting to do something, as the robot always seems to get distracted by something else. The self-narrative agent, not having any idea of what’s going on, might just explain this as “Robby the Robot is forgetful sometimes” in its internal narrative.\n\nFifth, the default planning subagent here is doing something like rational planning, but given its weak voting power, it’s likely to be overruled if other subagents disagree with it (unless some subagents also agree with it). If some actions seem worth doing, but there are managers which are blocking it and the default planning subagent doesn’t have an explicit representation of them, this can manifest as all kinds of procrastinating behaviors and numerous failed attempts for the default planning system to “try to get itself to do something”, using various strategies. But as long as the managers keep blocking those actions, the system is likely to remain stuck.\n\nSixth, the purpose of both managers and firefighters is to keep the robot out of a situation that has been previously designated as dangerous. Managers do this by trying to pre-emptively block actions that would cause the fear model agent to activate; firefighters do this by trying to take actions which shut down the fear model agent after it has activated. But the fear model agent activating is not actually the _same_ thing as being in a dangerous situation. Thus, both managers and firefighters may fall victim to [Goodhart’s law](https://www.lesswrong.com/posts/EbFABnst8LsidYs5Y/goodhart-taxonomy), doing things which block the fear model while being irrelevant for escaping catastrophic situations.\n\nFor example, “thinking about the consequences of going to the activist HQ” is something that might activate the fear model agent, so a manager might try to block just _thinking_ about it. This has obvious consequence that the robot can’t think clearly about that issue. Similarly, once the fear model has already activated, a firefighter might Goodhart by supporting _any_ action which helps activate an agent with a lot of voting power that’s going to think about something entirely different. This could result in compulsive behaviors which were effective at pushing the fear aside, but useless for achieving any of the robot’s actual aims.\n\nAt worst, this could cause loops of mutually activating subagents pushing in opposite directions. First, a stove-phobic robot runs away from the stove as it was about to make breakfast. Then a firefighter trying to suppress that fear, causes the robot to get stuck looking at pictures of beautiful naked robots, which is engrossing and thus great for removing the fear of the stove. Then another fear model starts to activate, this one afraid of failure and of spending so much time looking at pictures of beautiful naked robots that the robot won’t accomplish its goal of making breakfast. A separate firefighter associated with this second fear model has learned that focusing the robot’s attention on the pictures of beautiful naked robots _even more_ is the most effective action for keeping this new fear temporarily subdued. So the two firefighters are allied and temporarily successful at their goal, but then the first one - seeing that the original stove fear has disappeared - turns off. Without the first firefighter’s votes supporting the second firefighter, the fear manages to overwhelm the second firefighter, causing the robot to rush into making breakfast. This again activates its fear of the stove, but _if_ the fear of failure remains strong enough, it might overpower its fear of the stove so that the robot manages to make breakfast in time...\n\nHmm. Maybe this design isn’t so great after all. Good thing we noticed these failure modes, so that there aren’t any mind architectures like this going around being vulnerable to them!\n\nThe Internal Family Systems model\n=================================\n\nBut enough hypothetical robot design; let’s get to the topic of IFS. The IFS model hypothesizes the existence of three kinds of “extreme parts” in the human mind:\n\n*   **Exiles** are said to be parts of the mind which hold the memory of past traumatic events, which the person did not have the resources to handle. They are parts of the psyche which have been split off from the rest and are frozen in time of the traumatic event. When something causes them to surface, they tend to flood the mind with pain. For example, someone may have an exile associated with times when they were romantically rejected in the past.\n*   **Managers** are parts that have been tasked with keeping the exiles permanently exiled from consciousness. They try to arrange a person’s life and psyche so that exiles never surface. For example, managers might keep someone from reaching out to potential dates due to a fear of rejection.\n*   **Firefighters** react when exiles have been triggered, and try to either suppress the exile’s pain or distract the mind from it. For example, after someone has been rejected by a date, they might find themselves drinking in an attempt to numb the pain.\n*   Some presentations of the IFS model simplify things by combining Managers and Firefighters into the broader category of **Protectors**, so only talk about Exiles and Protectors.\n\nExiles are not limited to being created from the kinds of situations that we would commonly consider seriously traumatic. They can also be created from things like relatively minor childhood upsets, as long as the child didn’t feel like they could handle the situation.\n\nIFS further claims that you can treat these parts as something like independent subpersonalities. You can communicate with them, consider their worries, and gradually persuade managers and firefighters to give you access to the exiles that have been kept away from consciousness. When you do this, you can show them that you are no longer in the situation which was catastrophic before, and now have the resources to handle it if something similar was to happen again. This heals the exile, and also lets the managers and firefighters assume better, healthier roles.\n\nAs I mentioned in the beginning, when I first heard about IFS, I was turned off by it for several different reasons. For instance, here were some of my thoughts at the time:\n\n1.  The whole model about some parts of the mind being in pain, and other parts trying to suppress their suffering. The thing about exiles was framed in terms of _a part of the mind splitting off in order to protect the rest of the mind against damage._ What? That doesn’t make any evolutionary sense! A traumatic situation is _just sensory information_ for the brain, it’s not literal brain damage: it wouldn’t have made any sense for minds to evolve in a way that caused parts of it to split off, forcing other parts of the mind to try to keep them suppressed. Why not just… never be damaged in the first place?\n2.  That whole thing about parts being personalized characters that you could talk to. That… doesn’t describe anything in my experience.\n3.  Also, how does just talking to yourself fix any trauma or deeply ingrained behaviors?\n4.  IFS talks about everyone having a “True Self”. Quote from [Wikipedia](https://en.wikipedia.org/wiki/Internal_Family_Systems_Model): _IFS also sees people as being whole, underneath this collection of parts. Everyone has a true self or spiritual center, known as the Self to distinguish it from the parts. Even people whose experience is dominated by parts have access to this Self and its healing qualities of curiosity, connectedness, compassion, and calmness. IFS sees the therapist's job as helping the client to disentangle themselves from their parts and access the Self, which can then connect with each part and heal it, so that the parts can let go of their destructive roles and enter into a harmonious collaboration, led by the Self._ That… again did not sound particularly derived from any sensible psychology.\n\nHopefully, I’ve already answered my past self’s concerns about the first point. The model itself talks in terms of managers protecting the mind from pain, exiles being exiled from consciousness in order for their pain to remain suppressed, etc. Which is a reasonable description _of the subjective experience_ of what happens. But the evolutionary logic - as far as I can guess - is slightly different: to keep us out of dangerous situations.\n\nThe story of the robot describes the actual “design rationale”. Exiles are in fact subagents which are “frozen in the time of a traumatic event”, but they didn’t split off to protect the rest of the mind from damage. _Rather, they were created as an isolated memory block to ensure that the memory of the event wouldn’t be forgotten_. Managers then exist to keep the person away from such catastrophic situations, and firefighters exist to help escape them. Unfortunately, this setup is vulnerable to various failure modes, similar to those that the robot is vulnerable to.\n\nWith that said, let’s tackle the remaining problems that I had with IFS.\n\nPersonalized characters\n-----------------------\n\nIFS suggests that you can experience the exiles, managers and firefighters in your mind as something akin to subpersonalities - entities with their own names, visual appearances, preferences, beliefs, and so on. Furthermore, this isn’t inherently dysfunctional, nor indicative of something like Dissociative Identity Disorder. Rather, even people who are entirely healthy and normal may experience this kind of “multiplicity”.\n\nNow, it’s important to note right off that not everyone has this to a major extent: you don’t _need_ to experience multiplicity in order for the IFS process to work. For instance, my parts feel more like bodily sensations and shards of desire than subpersonalities, but IFS still works super-well for me.\n\nIn the book _[Internal Family Systems Therapy](https://smile.amazon.com/Internal-Family-Systems-Therapy-Guilford-ebook/dp/B00FW4ELM2/),_ Richard Schwartz, the developer of IFS, notes that if a person’s subagents play well together, then that person is likely to feel mostly internally unified. On the other hand, if a person has lots of internal conflict, then they are more likely to experience themselves as having multiple parts with conflicting desires.\n\nI think that this makes a lot of sense, assuming the existence of something like a self-narrative subagent. If you remember, this is the part of the mind which looks at the actions that the mind-system has taken, and then constructs an explanation for why those actions were taken. (See e.g. the posts on the [limits of introspection](https://www.lesswrong.com/posts/K2JBqDeETX2yEgyyZ/the-limits-of-introspection) and on [the Apologist and the Revolutionary](https://www.lesswrong.com/posts/ZiQqsgGX6a42Sfpii/the-apologist-and-the-revolutionary) for previous evidence for the existence of such a confabulating subagent with limited access to our true motivations.) As long as all the exiles, managers and firefighters are functioning in a unified fashion, the most parsimonious model that the self-narrative subagent might construct is simply that of a unified self. But if the system keeps being driven into strongly conflicting behaviors, then it can’t necessarily make sense of them from a single-agent perspective. Then it might naturally settle on something like a multiagent approach and experience itself as being split into parts.\n\nKevin Simler, in [Neurons Gone Wild](https://meltingasphalt.com/neurons-gone-wild/), notes how people with strong addictions seem particularly prone to developing multi-agent narratives:\n\n> _This American Life_ did a nice segment on addiction a few years back, in which the producers — seemingly on a lark — asked people to personify their addictions. \"It was like people had been waiting all their lives for somebody to ask them this question,\" said the producers, and they gushed forth with descriptions of the 'voice' of their inner addict:\n\n> _\"The voice is irresistible, always. I'm in the thrall of that voice.\"_\n\n> _\"Totally out of control. It's got this life of its own, and I can't tame it anymore.\"_\n\n> _\"I actually have a name for the voice. I call it Stan. Stan is the guy who tells me to have the extra glass of wine. Stan is the guy who tells me to smoke.\"_\n\nThis doesn’t seem like it explains all of it, though. I’ve frequently been very dysfunctional, and have always found very intuitive the notion of the mind being split into very parts. Yet I mostly still don’t seem to experience my subagents anywhere near as person-like as some others clearly do. I know at least one person who ended up finding IFS because of having all of these talking characters in their head, and who was looking for something that would help them make sense of it. Nothing like that has ever been the case for me: I did experience strongly conflicting desires, but they were just that, strongly conflicting desires.\n\nI can only surmise that it has something to do with the same kinds of differences which cause some people to think mainly verbally, others mainly visually, and others yet in some other hard-to-describe modality. Some fiction writers [spontaneously experience](https://pages.uoregon.edu/hodgeslab/files/Download/Taylor%20Hodges%20Kohanyi_2003.pdf) their characters as real people who speak to them and will even bother the writer when at the supermarket, and some others don’t.\n\nIt’s been noted that the mechanisms which use to model ourselves and other people overlap - not very surprisingly, since both we and other people are (presumably) humans. So it seems reasonable that some of the mechanisms for representing other people, would sometimes also end up spontaneously recruited for representing internal subagents or coalitions of them.\n\nWhy should this technique be useful for psychological healing?\n--------------------------------------------------------------\n\nOkay, suppose it’s possible to access our subagents somehow. Why would just talking with these entities in your own head, help you fix psychological issues?\n\nLet’s consider that a person having exiles, managers and firefighters is costly in the sense of constraining that person’s options. If you never want to do anything that would cause you to see a stove, that limits quite a bit of what you can do. I strongly suspect that many forms of procrastination and failure to do things we’d like to do are mostly a manifestation of overactive managers. So it’s important not to create those kinds of entities unless the situation really _is_ one which should be designated as categorically unacceptable to end up in.\n\nThe theory for IFS mentions that not all painful situations turn into trauma: just ones in which we felt helpless and like we didn’t have the necessary resources for dealing with it. This makes sense, since if we were capable of dealing with it, then the situation can’t have been that catastrophic. The aftermath of the immediate event is important as well: a child who ends up in a painful situation doesn’t necessarily end up traumatized, if they have an adult who can put the event in a reassuring context afterwards.\n\nBut situations which used to be catastrophic and impossible for us to handle before, aren’t necessarily that any more. It seems important to have a mechanism for updating that cache of catastrophic events and for disassembling the protections around it, if the protections turn out to be unnecessary.\n\nHow does that process usually happen, without IFS or any other specialized form of therapy?\n\nOften, by talking about your experiences with someone you trust. Or writing about them in private or in a blog.\n\nIn [my post about _Consciousness and the Brain_](https://www.lesswrong.com/s/ZbmRyDN8TCpBTZSip/p/x4n4jcoDP7xh5LWLq), I mentioned that once a mental object becomes conscious, many different brain systems synchronize their processing around it. I suspect that the reason why many people have such a powerful urge to discuss their traumatic experiences with someone else, is that doing so is a way of bringing those memories into consciousness in detail. And once you’ve dug up your traumatic memories from their cache, their content can be re-processed and re-evaluated. If your brain judges that you now _do_ have the resources to handle that event if you ever end up in it again, or if it’s something that simply can’t happen anymore, then the memory can be removed from the cache and you no longer need to avoid it.\n\nI think it’s also significant that, while something like just writing about a traumatic event is sometimes enough to heal, often it’s more effective if you have a sympathetic listener who you trust. Traumas often involve some amount of shame: maybe you were called lazy as a kid and are still afraid of others thinking that you are lazy. Here, having friends who accept you and are willing to nonjudgmentally listen while you talk about your issues, is by itself an indication that the thing that you used to be afraid of isn’t a danger anymore: there exist people who will stay by your side despite knowing your secret.\n\nNow, when you are talking to a friend about your traumatic memory, you will be going through cached memories that have been stored in an exile subagent. A specific memory circuit - one of several circuits specialized for the act of holding painful memories - is active and outputting its contents into the global workspace, from which they are being turned into words.\n\nMeaning that, in a sense, _your friend is talking directly to your exile._\n\nCould you hack this process, so that you wouldn’t even _need_ a friend, and could carry this process out entirely internally?\n\nIn [my earlier post](https://www.lesswrong.com/s/ZbmRyDN8TCpBTZSip/p/x4n4jcoDP7xh5LWLq), I remarked that you could view language as a way of joining two people’s brains together. A subagent in your brain outputs something that appears in your consciousness, you communicate it to a friend, it appears in their consciousness, subagents in your friend’s brain manipulate the information somehow, and then they send it back to your consciousness.\n\nIf you are telling your friend about your trauma, you are in a sense joining your workspaces together, and letting some subagents in _your_ workspace, communicate with the “sympathetic listener” subagents in _your friend’s_ workspace.\n\nSo why not let a “sympathetic listener” subagent in your workspace, hook up directly with the traumatized subagents that are also in your own workspace?\n\nI think that something like this happens when you do IFS. You are using a technique designed to activate the relevant subagents in a very specific way, which allows for this kind of a “hooking up” without needing another person.\n\nFor instance, suppose that you are talking to a manager subagent which wants to hide the fact that you’re bad at something, and starts reacting defensively whenever the topic is brought up. Now, one way by which its activation could manifest, is feeding those defensive thoughts and reactions directly into your workspace. In such a case, you would experience them as your _own_ thoughts, and possibly as objectively real. [IFS calls this “blending”](https://www.lesswrong.com/posts/8ZrDp7d5t4fwKXfj3/tentatively-considering-emotional-stories-ifs-and-getting); I’ve also previously [used the term “cognitive fusion”](https://www.lesswrong.com/posts/mELQFMi9egPn5EAjK/my-attempt-to-explain-looking-insight-meditation-and) for what’s essentially the same thing.\n\nInstead of remaining blended, you then use various unblending / cognitive defusion techniques that highlight the way by which these thoughts and emotions are coming from a specific part of your mind. You could think of this as wrapping extra content around the thoughts and emotions, and then seeing them through the wrapper (which is obviously not-you), rather than experiencing the thoughts and emotions directly (which you might experience as your own). For example, the IFS book _Self-Therapy_ suggests this unblending technique (among others):\n\n> Allow a visual image of the part \\[subagent\\] to arise. This will give you the sense of it as a separate entity. This approach is even more effective if the part is clearly a certain distance away from you. The further away it is, the more separation this creates.\n\n> Another way to accomplish visual separation is to draw or paint an image of the part. Or you can choose an object from your home that represents the part for you or find an image of it in a magazine or on the Internet. Having a concrete token of the part helps to create separation.\n\nI think of this as something like, you are taking the subagent in question, routing its responses through a visualization subsystem, and then you see a talking fox or whatever. And this is then a representation that your internal subsystems for talking with other people can respond to. You can then have a dialogue with the part (verbally or otherwise) in a way where its responses are clearly labeled as coming from it, rather than being mixed together with all the other thoughts in the workspace. This lets the content coming from the sympathetic-listener subagent and the exile/manager/firefighter subagent be kept clearly apart, allowing you to consider the emotional content as you would as an external listener, preventing you from drowning in it. You’re hacking your brain so as to work as the therapist and client as the same time.\n\nThe Self\n--------\n\nIFS claims that, below all the various parts and subagents, there exists a “true self” which you can learn to access. When you are in this Self, you exhibit the qualities of “calmness, curiosity, clarity, compassion, confidence, creativity, courage, and connectedness”. Being at least partially in Self is said to be a prerequisite for working with your parts: if you are not, then you are not able to evaluate their models objectively. The parts will sense this, and as a result, they will not share their models properly, preventing the kind of global re-evaluation of their contents that would update them.\n\nThis was the part that I was initially the most skeptical of, and which made me most frequently decide that IFS was not worth looking at. I could easily conceptualize the mind as being made up of various subagents. But then it would just be numerous subagents all the way down, without any single one that could be designated the “true” self.\n\nBut let’s look at IFS’s description of how exactly to get into Self. You check whether you seem to be blended with any part. If you are, you unblend with it. Then you check whether you might also be blended with some other part. If you are, you unblend from it also. You then keep doing this until you can find no part that you might be blended with. All that’s left are those “eight Cs”, which just seem to be a kind of a global state, with no particular part that they would be coming from.\n\nI now think that “being in Self” represents a state where there no particular subagent is getting a disproportionate share of voting power, and everything is processed by the system as a whole. Remember that in the robot story, catastrophic states were situations in which the organism should _never_ end up. A subagent kicking in to prevent that from happening is a kind of a priority override to normal thinking. It blocks you from being open and calm and curious _because_ some subagent thinks that doing so would be dangerous. If you then turn off or suspend all those priority overrides, then the mind’s default state absent any override seems to be one with the qualities of the Self.\n\nThis actually fits at least one model of the function of positive emotions pretty well. [Fredrickson (1998)](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3156001/) suggests that an important function of positive emotions is to make us engage in activities such as play, exploration, and savoring the company of other people. Doing these things has the effect of building up skills, knowledge, social connections, and other kinds of resources which might be useful for us in the future. If there are no active ongoing threats, then that implies that the situation is pretty safe for the time being, making it reasonable to revert to a positive state of being open to exploration.\n\nThe _Internal Family Systems Therapy_ book makes a somewhat big deal out of the fact that everyone, even most traumatized people, ultimately has a Self which they can access. It explains this in terms of the mind being organized to protect against damage, and with parts always splitting off from the Self when it would otherwise be damaged. I think the real explanation is much simpler: the mind is not accumulating damage, it is just accumulating a longer and longer list of situations not considered safe.\n\nAs an aside, this model feels like it makes me less confused about confidence. It seems like people are really attracted to confident people, and that to some extent it’s also possible to fake confidence until it becomes genuine. But if confidence is so attractive and we can fake it, why hasn’t evolution just made everyone confident by default?\n\nTurns out that it _has_. The reason why faked confidence gradually turns into genuine confidence is that by forcing yourself to act in confident ways which felt dangerous before, your mind gets information indicating that this behavior is not as dangerous as you originally thought. That gradually turns off those priority overrides that kept you out of Self originally, until you get there naturally.\n\nThe reason why being in Self is a requirement for doing IFS, is the existence of conflicts between parts. For instance, recall the stove-phobic robot having a firefighter subagent that caused it to retreat from the stove into watching pictures of beautiful naked robots. This triggered a subagent which was afraid of the naked-robot-watching preventing the robot from achieving its goals. If the robot now tried to do IFS and talk with the firefighter subagent that caused it to run away from stoves, this might bring to mind content which activated the exile that was afraid of not achieving things. Then _that_ exile would keep flooding the mind with negative memories, trying to achieve its priority override of “we need to get out of this situation”, and preventing the process from proceeding. Thus, all of the subagents that have strong opinions about the situation need to be unblended from, before integration can proceed.\n\nIFS also has a separate concept of “Self-Leadership”. This is a process where various subagents eventually come to trust the Self, so that they allow the person to increasingly remain in Self even in various emergencies. IFS views this as a positive development, not only because it feels nice, but because doing so means that the person will have more cognitive resources available for actually dealing with the emergency in question.\n\nI think that this ties back to the original notion of subagents being generated to invoke priority overrides for situations _which the person originally didn’t have the resources to handle_. Many of the subagents IFS talks about seem to emerge from childhood experiences. A child has many fewer cognitive, social, and emotional resources for dealing with bad situations, in which case it makes sense to just categorically avoid them, and invoke special overrides to ensure that this happens. A child’s cognitive capacities, models of the world, and abilities to self-regulate are also less developed, so she may have a harder time staying out of dangerous situations _without_ having some priority overrides built in. An adult, however, typically has many more resources than a child does. Even when faced with an emergency situation, it can be much better to be able to remain calm and analyze the situation using _all_ of one’s subagents, rather than having a few of them take over all the decision-making. Thus, it seems to me - both theoretically and practically - that developing Self-Leadership is _really_ valuable.\n\nThat said, I do not wish to imply that it would be a good goal to _never_ have negative emotions. Sometimes blending with a subagent, and experiencing resulting negative emotions, is the right thing to do in that situation. Rather than suppressing negative emotions entirely, Self-Leadership aims to get to a state where any emotional reaction tends to be endorsed by the mind-system as a whole. Thus, if feeling angry or sad or bitter or whatever feels appropriate to the situation, you can let yourself feel so, and then give yourself to that emotion without resisting it. As a result, negative emotions become less unpleasant to experience, since there are fewer subagents trying to fight against them. Also, if it turns out that being in a negative emotional state is no longer useful, the system as a whole can just choose to move back into Self.\n\nFinal words\n===========\n\nI’ve now given a brief summary of the IFS model, and explained why I think it makes sense. This is of course not enough to establish the model as _true_. But it might help in making the model plausible enough to at least try out.\n\nI think that most people could benefit from learning and doing IFS on themselves, either alone or together with a friend. I’ve been saying that exiles/managers/firefighters tend to be generated from trauma, but it’s important to realize that these events don’t need to be anything immensely traumatic. The kinds of ordinary, normal childhood upsets that everyone has had can generate these kinds of subagents. Remember, just because you think of a childhood event as trivial _now_, doesn’t mean that it felt trivial to you _as a child_. Doing IFS work, I’ve found exiles related to memories and events which I _thought_ left no negative traces, but actually did.\n\nRemember also that it can be really hard to notice the presence of some managers: if they are doing their job effectively, then you might never become aware of them directly. “I don’t have any trauma so I wouldn’t benefit from doing IFS” isn’t necessarily correct. Rather, the cues that I use for detecting a need to do internal work are:\n\n*   _Do I have the qualities associated with Self, or is something blocking them?_\n*   _Do I feel like I’m capable of dealing with this situation rationally, and doing the things which feel like good ideas on an intellectual level?_\n*   _Do my emotional reactions feel like they are endorsed by my mind-system as a whole, or is there a resistance to them?_\n\nIf not, there is often some internal conflict which needs to be addressed - and IFS, combined with some other practices such as [Focusing](https://medium.com/@ThingMaker/focusing-for-skeptics-6b949ef33a4f) and [meditation](https://www.lesswrong.com/posts/mELQFMi9egPn5EAjK/my-attempt-to-explain-looking-insight-meditation-and) \\- has been very useful in learning to solve those internal conflicts.\n\nEven if you don’t feel convinced that doing IFS personally would be a good idea, I think adopting its framework of exiles, managers and firefighters is useful for better understanding the behavior of other people. Their dynamics will be easier to recognize in other people if you’ve had some experience recognizing them in yourself, however.\n\nIf you want to learn more about IFS, I would recommend starting with [Self-Therapy](https://www.amazon.com/dp/B00452V8EG/) by Jay Earley. In terms of [What/How/Why books](https://www.lesswrong.com/posts/oPEWyxJjRo4oKHzMu/the-3-books-technique-for-learning-a-new-skilll), my current suggestions would be:\n\n*   How: [Self-Therapy](https://www.amazon.com/dp/B00452V8EG/) by Jay Earley.\n*   What: [Internal Family Systems Therapy](https://smile.amazon.com/Internal-Family-Systems-Therapy-Guilford-ebook/dp/B00FW4ELM2/), by Richard Schwartz\n*   Why: [The Power of Focusing](https://smile.amazon.com/Power-Focusing-Practical-Emotional-Self-Healing/dp/157224044X/), by Ann Weiser Cornell (technically not about IFS, but AWC’s variant of Focusing gets very close to IFS, and is excellent for conveying the right mindset for it)\n\n_This post was written as part of research supported by [the Foundational Research Institute](https://foundational-research.org/). Thank you to everyone who provided feedback on earlier drafts of this article: Eli Tyre, Elizabeth Van Nostrand, Jan Kulveit, Juha Törmänen, Lumi Pakkanen, Maija Haavisto, Marcello Herreshoff, Qiaochu Yuan, and Steve Omohundro._"
    },
    "voteCount": 114
  },
  {
    "_id": "cYsGrWEzjb324Zpjx",
    "url": null,
    "title": "Comparing Utilities",
    "slug": "comparing-utilities",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "Utility Functions"
      },
      {
        "name": "Utilitarianism"
      },
      {
        "name": "Population Ethics"
      }
    ],
    "tableOfContents": {
      "sections": [
        {
          "title": "Utilities aren't comparable.",
          "anchor": "Utilities_aren_t_comparable_",
          "level": 1
        },
        {
          "title": "Some non-obvious consequences.",
          "anchor": "Some_non_obvious_consequences_",
          "level": 1
        },
        {
          "title": "Comparing utilities.",
          "anchor": "Comparing_utilities_",
          "level": 1
        },
        {
          "title": "Pareto-Optimality: The Minimal Standard",
          "anchor": "Pareto_Optimality__The_Minimal_Standard",
          "level": 2
        },
        {
          "title": "Variance Normalization: Not Too Exploitable?",
          "anchor": "Variance_Normalization__Not_Too_Exploitable_",
          "level": 2
        },
        {
          "title": "Warm-Up: Range Normalization",
          "anchor": "Warm_Up__Range_Normalization",
          "level": 3
        },
        {
          "title": "Variance Normalization",
          "anchor": "Variance_Normalization",
          "level": 3
        },
        {
          "title": "Nash Bargaining Solution",
          "anchor": "Nash_Bargaining_Solution",
          "level": 2
        },
        {
          "title": "Kalai–Smorodinsky",
          "anchor": "Kalai_Smorodinsky",
          "level": 2
        },
        {
          "title": "Animals, etc.",
          "anchor": "Animals__etc_",
          "level": 1
        },
        {
          "title": "Altruistic agents.",
          "anchor": "Altruistic_agents_",
          "level": 1
        },
        {
          "title": "Utility monsters.",
          "anchor": "Utility_monsters_",
          "level": 1
        },
        {
          "title": "Average utilitarianism vs total utilitarianism. ",
          "anchor": "Average_utilitarianism_vs_total_utilitarianism__",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        },
        {
          "anchor": "comments",
          "level": 0,
          "title": "31 comments"
        }
      ],
      "headingsCount": 15
    },
    "contents": {
      "markdown": "*(This is a basic point about utility theory which many will already be familiar with. I draw some non-obvious conclusions which may be of interest to you even if you think you know this from the title -- but the main point is to communicate the basics. I'm posting it to the alignment forum because I've heard misunderstandings of this from some in the AI alignment research community.)*\n\nI will first give the basic argument that the utility quantities of different agents aren't directly comparable, and a few important consequences of this. I'll then spend the rest of the post discussing what to do when you need to compare utility functions.\n\nUtilities aren't comparable.\n============================\n\nUtility isn't an ordinary quantity. A utility function is a device for expressing the preferences of an agent.\n\nSuppose we have a notion of *outcome.** We could try to represent the agent's preferences between outcomes as an ordering relation: if we have outcomes A, B, and C, then one possible preference would be A<B<C.\n\nHowever, a mere ordering does not tell us how the agent would decide between *gambles,* ie, situations giving A, B, and C with some probability.\n\nWith just three outcomes, there is only one thing we need to know: is B closer to A or C, and by how much?\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/70c2563b8fe60460a975a6000ca0aee95873c72e7fbbe2ce.png)\n\nWe want to construct a utility function U() which represents the preferences. Let's say we set U(A)=0 and U(C)=1. Then we can represent B=G as U(B)=1/2. If not, we would look for a different gamble which *does* equal B, and then set B's utility to the expected value of that gamble. By assigning real-numbered values to each outcome, we can fully represent an agent's preferences over gambles. (Assuming the [VNM axioms](https://www.lesswrong.com/posts/F46jPraqp258q67nE/why-you-must-maximize-expected-utility) hold, that is.)\n\nBut the initial choices U(A)=0 and U(C)=1 were arbitrary! We could have chosen any numbers so long as U(A)<U(C), reflecting the preference A<C. In general, a valid representation of our preferences U() can be modified into an equally valid U'() by adding/subtracting arbitrary numbers, or multiplying/dividing by positive numbers.\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/727725a2910022ba355875e1cebc211b70ab91529a3900f2.png)\n\nSo it's just as valid to say someone's expected utility in a given situation is 5 or -40, provided you shift everything *else* around appropriately.\n\nWriting \\\\(\\\\approx\\\\) to mean that two utility functions represent the same preferences, what we have in general is: \\\\(U\\_1(x) \\\\approx U\\_2(x)\\\\) if and only if \\\\(U\\_1(x) = aU\\_2+b\\\\). (I'll call \\\\(a\\\\) the ***multiplicative constant*** and\\\\(b\\\\)the ***additive constant***.)\n\nThis means that we can't directly compare the utility of two different agents. Notions of fairness should not directly say \"everyone should have the same expected utility\". Utilitarian ethics cannot directly maximize the sum of everyone's utility. Both of these operations should be thought of as a type error.\n\nSome non-obvious consequences.\n==============================\n\nThe game-theory term \"zero sum\" is a misnomer. You shouldn't directly think about the sum of the utilities.\n\nIn mechanism design, *exchangeable utility* is a useful assumption which is often needed in order to get nice results. The idea is that agents can give utils to each other, perhaps to compensate for unfair outcomes. This is *kind of* like assuming there's money which can be exchanged between agents. However, the non-comparability of utility should make this seem *really weird*. (There are also other disanalogies with money; for example, utility is closer to logarithmic in money, not linear.)\n\nThis could (should?) also make you suspicious of talk of \"average utilitarianism\" and \"total utilitarianism\". However, beware: only one kind of \"utilitarianism\" holds that the term \"utility\" in decision theory means the same thing as \"utility\" in ethics: namely, preference utilitarianism. Other kinds of utilitarianism can distinguish between these two types of utility. (For example, one can be a hedonic utilitarian without thinking that what everyone wants is happiness, if one isn't a preference utilitarian.)\n\nSimilarly, for preference utilitarians, talk of *utility monsters* becomes questionable. A utility monster is, supposedly, someone who gets much more utility out of resources than everyone else. For a hedonic utilitarian, it would be someone who experiences much deeper sadness and much higher heights of happiness. This person supposedly merits more resources than other people.\n\nFor a preference utilitarian, incomparability of utility means we can't simply posit such a utility monster. It's meaningless *a priori* to say that one person simply has much stronger preferences than another (in the utility function sense).\n\nAll that being said, we *can* actually compare utilities, sum them, exchange utility between agents, define utility monsters, and so on. We just need *more information.*\n\nComparing utilities.\n====================\n\nThe incomparability of utility functions ***doesn't mean*** we can't trade off between the utilities of different people.\n\nI've heard the non-comparability of utility functions summarized as the thesis that we can't say anything meaningful about the relative value of one person's suffering vs another person's convenience. Not so! Rather, the point is just that *we need more assumptions in order to say anything.* The utility functions alone aren't enough.\n\nPareto-Optimality: The Minimal Standard\n---------------------------------------\n\nComparing utility functions suggests putting them all onto one scale, such that we can trade off between them -- \"this dollar does more good for Alice than it does for Bob\". We formalize this by imagining that we have to decide policy for the whole group of people we're considering (e.g., the whole world). We consider a *social choice function* which would make those decisions on behalf of everyone. Supposing it is VNM rational, its decisions must be comprehensible in terms of a utility function, too. So the problem reduces to combining a bunch of individual utility functions, to get one big one.\n\nSo, how do we go about combining the preferences of many agents into one?\n\nThe first and most important concept is the ***pareto improvement:** our social choice function should endorse changes which benefit someone and harm no one.* An option which allows no such improvements is said to be ***Pareto-optimal.***\n\nWe might also want to consider ***strict Pareto improvements:** a change which benefits everyone.* (An option which allows no strict Pareto improvements is ***weakly Pareto-optimal.***) Strict Pareto improvements can be more relevant [in a bargaining context](https://www.lesswrong.com/posts/5bd75cc58225bf067037554e/distributed-cooperation?commentId=5bd75cc58225bf0670375550), where you need to give everyone something in order to get them on board with a proposal -- otherwise they may judge the improvement as unfairly favoring others. However, in a bargaining context, individuals may refuse even a strict Pareto improvement [due to fairness considerations](https://www.lesswrong.com/posts/z2YwmzuT7nWx62Kfh/cooperating-with-agents-with-different-ideas-of-fairness).\n\nIn either case, a version of [Harsanyi's utilitarianism Theorem](https://www.lesswrong.com/posts/sZuw6SGfmZHvcAAEP/complete-class-consequentialist-foundations#Utilitarianism) implies that the utility of our social choice function *can be understood as some linear combination of the individual utility functions.*\n\nSo, pareto-optimal social choice functions can always be understood by:\n\n1.  Choosing a scale for everyone's utility function -- IE, set the multiplicative constant. (If the social choice function is only weakly Pareto optimal, some of the multiplicative constants might turn out to be zero, totally cancelling out someone's involvement. Otherwise, they can all be positive.)\n2.  Adding all of them together.\n\n(Note that the *additive constant* doesn't matter -- shifting a person's utility function up or down doesn't change what decisions will be endorsed by the sum. However, it ***will*** matter for some other ways to combine utility functions.)\n\nThis is nice, because we can always combine everything linearly! We just have to set things to the right scale and then sum everything up.\n\nHowever, it's far from the end of the story. How do we choose multiplicative constants for everybody?\n\nVariance Normalization: Not Too Exploitable?\n--------------------------------------------\n\nWe could set the constants any way we want... totally subjective estimates of the worth of a person, draw random lots, etc. But we do typically want to represent some notion of fairness. We said in the beginning that the problem was, a utility function \\\\(U(x)\\\\) has many equivalent representations \\\\(aU(x)+b\\\\). We can address this as a problem of ***normalization:*** we want to take a \\\\(U\\\\) and put it into a canonical form, getting rid of the choice between equivalent representations.\n\nOne way of thinking about this is ***strategy-proofness***. A utilitarian collective should not be vulnerable to members strategically claiming that their preferences are stronger (larger \\\\(b\\\\)), or that they should get more because they're worse off than everyone (smaller \\\\(a\\\\) \\-\\- although, remember that we haven't talked about any setup which actually cares about that, yet).\n\n**Warm-Up: Range Normalization**\n\nUnfortunately, some obvious ways to normalize utility functions are not going to be strategy-proof.\n\nOne of the simplest normalization techniques is to squish everything into a specified range, such as \\[0,1\\]:\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/03741ee30577de57315ff2adadd3be3ffd475eee72d31430.png)\n\nThis is analogous to range voting: everyone reports their preferences for different outcomes on a fixed scale, and these all get summed together in order to make decisions.\n\nIf you're an agent in a collective which uses range normalization, then you may want to strategically mis-report your preferences. In the example shown, the agent has a big hump around outcomes they like, and a small hump on a secondary \"just OK\" outcome. The agent might want to get rid of the second hump, forcing the group outcome into the more favored region.\n\nI believe that in the extreme, the optimal strategy for range voting is to choose some utility threshold. Anything below that threshold goes to zero, feigning maximal disapproval of the outcome. Anything above the threshold goes to one, feigning maximal approval. In other words, under strategic voting, range voting becomes approval voting (range voting where the only options are zero and one).\n\nIf it's not possible to mis-report your preferences, then the incentive becomes to *self-modify to literally have these extreme preferences.* This could perhaps have a real-life analogue in political outrage and black-and-white thinking. If we use this normalization scheme, that's the closest you can get to being a utility monster.\n\n**Variance Normalization**\n\nWe'd *like* to avoid *any* incentive to misrepresent/modify your utility function. Is there a way to achieve that?\n\nOwen Cotton-Barratt discusses different normalization techniques in illuminating detail, and argues for *variance normalization:* divide utility functions by their variance, making the variance one. ([*Geometric reasons for normalizing variance to aggregate preferences,* O Cotton-Barratt, 2013.](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.684.1180&rep=rep1&type=pdf)) Variance normalization is strategy-proof under the assumption that everyone participating in an election shares beliefs about how probable the different outcomes are! (Note that *variance* *of utility* is only well-defined under some assumption about *probability of outcome.*) That's pretty good. It's probably the best we can get, in terms of strategy-proofness of voting. Will MacAskill also argues for variance normalization in the context of normative uncertainty ([*Normative Uncertainty,* Will MacAskill, 2014](https://d1wqtxts1xzle7.cloudfront.net/34857095/Normative_Uncertainty__Complete.pdf?1411561048=&response-content-disposition=inline%3B+filename%3DNormative_Uncertainty.pdf&Expires=1599854518&Signature=Z-cD7ds~K1cZc-GlXyV~eppzxbVKlwJCIkz6AQIHUg4jOgQlMAcgi3X1cCX~Z~FSvXKAYEwTyqehuuxCkMA2hxguilao82uaF8cH7sEZxczg243o2S5k4sZ7-YeIp5cJ2U-UAsecA-JbROuHU9AkUnlR02-rL4q-JlAlCOBBP5CDjJC6aocEM1HEyL0bHxFXf7Wg~B4Jyf8KSlvdnuAbm7IFn~lbmrBLb6OQG5~VbGAz8rfH2AuZlbZOpdVoID~MgPtIZ9rF1kMAcrUAhS93D15BPd8XNDRAOtMKMvSs~5xh5Ok-7dQhTFhqXkk~YE4S23VoKGGwqGZp4yl9X6WnTQ__&Key-Pair-Id=APKAJLOHF5GGSLRBV4ZA)).\n\nIntuitively, variance normalization directly addresses the issue we encountered with range normalization: an individual attempts to make their preferences \"loud\" by extremizing everything to 0 or 1. This increases variance, so, is directly punished by variance normalization.\n\nHowever, [Jameson Quinn](https://www.lesswrong.com/users/jameson-quinn), LessWrong's resident voting theory expert, has warned me rather strongly about variance normalization.\n\n1.  The assumption of shared beliefs about election outcomes is far from true in practice. Jameson Quinn tells me that, in fact, the strategic voting incentivized by quadratic voting is *particularly bad* amongst normalization techniques.\n2.  Strategy-proofness isn't, after all, the final arbiter of the quality of a voting method. The final arbiter should be something like the utilitarian quality of an election's outcome. This question gets a bit weird and recursive in the current context, where I'm using elections as an analogy to ask how we should define utilitarian outcomes. But the point still, to some extent, stands.\n\nI didn't understand the full justification behind his point, but I came away thinking that range normalization was probably better in practice. After all, it reduces to approval voting, which is actually a pretty good form of voting. But if you want to do the best we can with the state of voting theory, Jameson Quinn suggested 3-2-1 voting. (I don't think 3-2-1 voting gives us any nice theory about how to combine utility functions, though, so it isn't so useful for our purposes.)\n\n**Open Question:** *Is there a variant of variance normalization which takes differing beliefs into account, to achieve strategy-proofness (IE honest reporting of utility)?*\n\nAnyway, so much for normalization techniques. These techniques ignore the broader context. They attempt to be fair and even-handed *in the way we choose the multiplicative and additive constants.* But we could also explicitly try to be fair and even-handed *in the way we choose between Pareto-optimal outcomes*, as with this next technique.\n\nNash Bargaining Solution\n------------------------\n\nIt's important to remember that the Nash bargaining solution is a solution *to the Nash bargaining problem*, which isn't quite our problem here. But I'm going to gloss over that. Just imagine that we're setting the social choice function through a massive negotiation, so that we can apply bargaining theory.\n\nNash offers a very simple solution, which I'll get to in a minute. But first, a few words on how this solution is derived. Nash provides two seperate justifications for his solution. The first is a game-theoretic derivation of the solution as an especially robust Nash equilibrium. I won't detail that here; I quite recommend [his original paper](http://www.rasmusen.org/GI/reader/12a.nash.bargaining.1950.pdf) (*The Bargaining Problem,* 1950); but, just keep in mind that there is at least some reason to expect selfishly rational agents to hit upon this particular solution. The second, unrelated justification is an axiomatic one:\n\n1.  *Invariance to equivalent utility functions.* This is the same motivation I gave when discussing normalization.\n2.  *Pareto optimality.* We've already discussed this as well.\n3.  *Independence of Irrelevant Alternatives (IIA).* This says that we shouldn't change the outcome of bargaining by removing options which won't ultimately get chosen anyway. This isn't even technically one of the VNM axioms, but it *essentially* is -- the VNM axioms are posed for binary preferences (a > b). IIA is the assumption we need to break down multi-choice preferences to binary choices. We can justify IIA with [a kind of money pump](https://www.lesswrong.com/posts/5bd75cc58225bf067037539a/generalizing-foundations-of-decision-theory-ii).\n4.  *Symmetry.* This says that the outcome doesn't depend on the order of the bargainers; we don't prefer Player 1 in case of a tie, or anything like that.\n\nNash proved that *the only way to meet these four criteria* is to maximize the **product** of gains from cooperation. More formally, choose the outcome \\\\(x\\\\) which maximizes:\n\n\\\\\\[(U\\_1(x) - U\\_1(d))(U\\_2(x)-U\\_2(d))\\\\\\]\n\nThe \\\\(d\\\\) here is a \"status quo\" outcome. You can think of this as what happens if the bargaining fails. This is sometimes called a \"threat point\", since strategic players should carefully set what they do *if negotiation fails* so as to maximize their bargaining position. However, you might also want to rule that out, forcing \\\\(d\\\\) to be a Nash equilibrium in the hypothetical game where there is no bargaining opportunity. As such, \\\\(d\\\\) is also known as the *best alternative to negotiated agreement (BATNA)*, or sometimes the \"disagreement point\" (since it's what players get if they can't agree). We can think of subtracting out \\\\(U(d)\\\\) as just a way of adjusting the additive constant, in which case we really are just maximizing the product of utilities. (The BATNA point is always (0,0) after we subtract out things that way.)\n\nThe Nash solution differs significantly from the other solutions considered so far.\n\n1.  Maximize the *product??* Didn't Harsanyi's theorem guarantee we only need to worry about sums?\n2.  This is the first proposal where the additive constants matter. Indeed, now the *multiplicative* constants are the ones that don't matter!\n3.  Why wouldn't *any* utility-normalization approach satisfy those four axioms?\n\nLast question first: how do normalization approaches violate the Nash axioms?\n\nWell, both range normalization and variance normalization violate IIA! If you remove one of the possible outcomes, the normalization may change. This makes the social choice function display inconsistent preferences across different scenarios. (But how bad is that, really?)\n\nAs for why we can get away with maximizing the product, rather than the sum:\n\nThe Pareto-optimality of Nash's approach guarantees that it *can be seen* as maximizing a linear function of the individual utilities. So Harsanyi's theorem is still satisfied. However, Nash's solution points to a very *specific* outcome, which Harsanyi doesn't do for us.\n\nImagine you and me are trying to split a dollar. If we can't agree on how to split it, then we'll end up destroying it (ripping it during a desperate attempt to wrestle it from each other's hands, obviously). Thankfully, John Nash is standing by, and we each agree to respect his judgement. No matter which of us claims to value the dollar more, Nash will allocate 50 cents to each of us.\n\nHarsanyi happens to see this exchange, and explains that Nash has chosen a social choice function which normalized our utility functions to be equal to each other. That's the only way Harsanyi can explain the choice made by Nash -- the value of the dollar was precisely tied between you and me, so a 50-50 split was as good as any other outcome. Harsanyi's justification is indeed *consistent* with the observation. But why, then, did Nash choose 50-50 *precisely?* 49-51 would have had exactly the same collective utility, as would 40-60, or any other split!\n\nHence, Nash's principle is far more useful than Harsanyi's, even though Harsanyi can justify any rational outcome retrospectively.\n\nHowever, Nash does rely somewhat on that pesky IIA assumption, whose importance is perhaps not so clear. Let's try getting rid of that.\n\nKalai–Smorodinsky\n-----------------\n\nAlthough the Nash bargaining solution is the most famous, there are other proposed solutions to Nash's bargaining problem. I want to mention just one more, Kalai-Smorodinsky (I'll call it KS).\n\nKS throws out IIA as irrelevant. After all, the set of alternatives *will* affect bargaining. Even in the Nash solution, the set of alternatives may have an influence by changing the BATNA! So perhaps this assumption isn't so important.\n\nKS instead adds a *monotonicity* assumption: being in a better position should never make me worse off after bargaining.\n\nHere's an illustration, due to Daniel Demski, of a case where Nash bargaining fails monotonicity:\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/b6d1b6bb67fe303b168677d37fe682f817d68581879f57e5.jpeg)\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/c52e2fe0e1c0400436e528b5f0ac1f471310bb4315160e57.jpeg)\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/723a1686fe305c117cc468de3a53f2a53de729844a708241.jpeg)\n\nI'm not that sure monotonicity really should be an axiom, but it does kind of suck to be in an apparently better position and end up worse off for it. Maybe we could relate this to strategy-proofness? A little? Not sure about that.\n\nLet's look at the formula for KS bargaining. \n\nSuppose there are a couple of dollars on the ground: one which you'll walk by first, and one which I'll walk by. If you pick up your dollar, you can keep it. If I pick up my dollar, I can keep mine. But also, if you *don't* pick up yours, then I'll eventually walk by it and can pick it up. So we get the following:\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/33ae90d543db57832167eb6920b1bf257e65b1c23da49b99.png)\n\n(The box is filled in because we can also use mixed strategies to get values intermediate between any pure strategies.)\n\nObviously in the real world we just both pick up our dollars. But, let's suppose we bargain about it, just for fun.\n\nThe way KS works is, you look at the maximum *one* player can get (you can get $1), and the maximum the *other* player could get (I can get $2). Then, although we can't usually jointly achieve those payoffs (I can't get $2 at the same time as you get $1), KS bargaining insists we achieve the same *ratio* (I should get twice as much as you). In this case, that means I get $1.33, while you get $0.66. We can visualize this as drawing a bounding box around the feasible solutions, and drawing a diagonal line. Here's the Nash and KS solutions side by side:\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/6d3cf95df139e51c156cd92879c6754ce2b4addeb3d6ce93.png)\n\nAs in Daniel's illustrations, we can visualize maximizing the product as drawing the largest hyperbola we can that still touches the orange shape. (Orange dotted line.) This suggests that we each get $1; exactly the same solution as Nash would give for splitting $2. (The black dotted line illustrates how we'd continue the feasible region to represent a dollar-splitting game, getting the full triangle rather than a chopped off portion.) Nash doesn't care that one of us can do better than the other; it just looks for the most equal division of funds possible, since that's how we maximize the product.\n\nKS, on the other hand, cares what the max possible is for both of us. It therefore suggests that you give up some of your dollar to me.\n\nI suspect most readers will ***not*** find the KS solution to be more intuitively appealing?\n\nNote that the KS monotonicity property does NOT imply the desirable-sounding property \"if there are more opportunities for good outcomes, everyone gets more or is at least not worse off.\" (I mention this mainly because I initially misinterpreted KS's monotonicity property this way.) In my dollar-collecting example, KS bargaining makes you worse off simply because there's an opportunity for me to take your dollar if you don't. \n\nLike Nash bargaining, KS bargaining ignores multiplicative constants on utility functions, and can be seen as normalizing additive constants by treating \\\\(d\\\\) as (0,0). (Note that, in the illustration, I assumed \\\\(d\\\\) is chosen as (minimal achievable for one player, minimal achievable for the other). this need not be the case in general.)\n\nA peculiar aspect of KS bargaining is that it doesn't really give us an obvious quantity to maximize, unlike Nash or Harsanyi. It only describes the optimal point. This seems far less practical, for realistic decision-making.\n\nOK, so, should we use bargaining solutions to compare utilities?\n\nMy intuition is that, because of the need to choose the BATNA point \\\\(d\\\\), bargaining solutions end up rewarding destructive threats in a disturbing way. For example, suppose that we are playing the dollar-splitting game again, except that I can costlessly destroy $20 of your money, so \\\\(d\\\\) now involves both the destruction of the $1, and the destruction of $20. Nash bargaining now hands the entire dollar to me, because you are \"up $20\" in that deal, so the fairest possible outcome is to give me the $1. KS bargaining splits things up a little, but I still get most of the dollar.\n\nIf utilitarians were to trade off utilities that way in the real world, it would benefit powerful people, especially those willing to exploit their power to make credible threats. If X can take everything away from Y, then Nash bargaining sees everything Y has as already counting toward \"gains from trade\".\n\nAs I mentioned before, sometimes people try to define BATNAs in a way which excludes these kinds of threats. However, I see this as ripe for strategic utility-spoofing (IE, lying about your preferences, or self-modifying to have more advantageous preferences).\n\nSo, this might favor normalization approaches.\n\nOn the other hand, Nash and KS both do way better in the split-the-dollar game than any normalization technique, because they can optimize for fairness of outcome, rather than just fairness of multiplicative constants chosen to compare utility functions with.\n\nIs there any approach which combines the advantages of bargaining and normalization??\n\nAnimals, etc.\n=============\n\nAn essay on utility comparison would be incomplete without at least mentioning the problem of animals, plants, and so on.\n\n*   Option one: some cutoff for \"moral patients\" is defined, such that a utilitarian only considers preferences of agents who exceed the cutoff.\n*   Option two: some more continuous notion is selected, such that we care more about some organisms than others.\n\nOption two tends to be more appealing to me, despite the non-egalitarian implications (e.g., if animals differ on this spectrum, than humans could have some variation as well). \n\nAs already discussed, bargaining approaches do seem to have this feature: animals would tend to get less consideration, because they've got less \"bargaining power\" (they can do less harm to humans than humans can do to them). However, this has a distasteful might-makes-right flavor to it.\n\nThis also brings to the forefront the question of how we view something as an agent. Something like a plant might have quite deterministic ways of reacting to environmental stimulus. Can we view it as making choices, and thus, as having preferences? Perhaps \"to some degree\" -- if such a degree could be defined, numerically, it could factor into utility comparisons, giving a formal way of valuing plants and animals *somewhat,* but \"not too much\".\n\nAltruistic agents.\n==================\n\nAnother puzzling case, which I think needs to be handled carefully, is accounting for the preferences of altruistic agents.\n\nLet's proceed with a simplistic model where agents have \"personal preferences\" (preferences which just have to do with themselves, in some sense) and \"***cofrences***\" (co-preferences; preferences having to do with other agents).\n\nHere's an agent named Sandy:\n\n<table><tbody><tr><td colspan=\"5\">Sandy</td></tr><tr><td colspan=\"2\">Personal Preferences</td><td colspan=\"3\">Cofrences</td></tr><tr><td>Candy</td><td>+.1</td><td>Alice</td><td colspan=\"2\">+.1</td></tr><tr><td>Pizza</td><td>+.2</td><td>Bob</td><td colspan=\"2\">-.2</td></tr><tr><td>Rainbows</td><td>+10</td><td>Cathy</td><td colspan=\"2\">+.3</td></tr><tr><td>Kittens</td><td>-20</td><td>Dennis</td><td colspan=\"2\">+.4</td></tr></tbody></table>\n\nThe cofrences represent coefficients on other agent's utility functions. Sandy's preferences are supposed to be understood as a utility function representing Sandy's *personal* preferences, plus a weighted sum of the utility functions of Alice, Bob, Cathy, and Dennis. (Note that the weights can, hypothetically, be negative -- for example, screw Bob.)\n\nThe first problem is that utility functions are not comparable, so we have to say more before we can understand what \"weighted sum\" is supposed to mean. But suppose we've chosen some utility normalization technique. There are still other problems.\n\nNotice that we can't totally define Sandy's utility function until we've defined Alice's, Bob's, Cathy's, and Dennis'. But any of those four might have cofrences which involve Sandy, as well!\n\nSuppose we have Avery and Briar, two lovers who \"only care about each other\" -- their only preference is a cofrence, which places 1.0 value on the other's utility function. We could ascribe *any values at all* to them, so long as they're both the same!\n\nWith some technical assumptions (something along the lines of: your cofrences always sum to less than 1), we can ensure a unique fixed point, eliminating any ambiguity from the interpretation of cofrences. However, I'm skeptical of just taking the fixed point here.\n\nSuppose we have five siblings: Primus, Secundus, Tertius, Quartus, et Quintus. All of them value each other at .1, except Primus, who values all siblings at .2.\n\nIf we simply take the fixed point, Primus is going to get the short end of the stick all the time: because Primus cares about everyone else more, everyone else cares about Primus' personal preferences *less* than anyone else's.\n\nSimply put, I don't think more altruistic individuals should be punished! In this setup, the \"utility monster\" is the perfectly selfish individual. Altruists will be scrambling to help this person while the selfish person does nothing in return.\n\nA different way to do things is to interpret cofrences as *integrating only the personal preferences of the other person.* So Sandy wants to help Alice, Cathy, and Dennis (and harm Bob), but does *not* automatically extend that to wanting to help any of their friends (or harm Bob's friends).\n\nThis is a little weird, but gives us a more intuitive outcome in the case of the five siblings: Primus will more often be voluntarily helpful to the other siblings, but the other siblings won't be prejudice *against* the personal preferences of Primus when weighing between their various siblings.\n\nI realize altruism isn't *exactly* supposed to be like a bargain struck between selfish agents. But if I think of utilitarianism like a coalition of all agents, then I don't want it to punish the (selfish component of) the most altruistic members. It seems like utilitarianism should have better incentives than that?\n\n(Try to take this section as more of a problem statement and less of a solution. Note that the concept of *cofrence* can include, more generally, preferences such as \"I want to be better off than other people\" or \"I don't want my utility to be too different from other people's in either direction\".)\n\nUtility monsters.\n=================\n\nReturning to some of the points I raised in the \"non-obvious consequences\" section -- now we can see how \"utility monsters\" are/aren't a concern.\n\nOn my analysis, a utility monster is just an agent who, according to your metric for comparing utility functions, has a very large influence on the social choice function.\n\nThis might be a bug, in which case you should reconsider how you are comparing utilities. But, since you've hopefully chosen your approach carefully, it could also not be a bug. In that case, you'd want to bite the bullet fully, defending the claim that such an agent should receive \"disproportionate\" consideration. Presumably this claim could be backed up, on the strength of your argument for the utility-comparison approach.\n\nAverage utilitarianism vs total utilitarianism. \n================================================\n\nNow that we have given some options for utility comparison, can we use them to make sense of the distinction between average utilitarianism and total utilitarianism?\n\nNo. Utility comparison doesn't really help us there.\n\nThe average vs total debate is a debate about population ethics. Harsanyi's utilitarianism theorem and related approaches let us think about altruistic policies for a fixed set of agents. They don't tell us how to think about a set which changes over time, as new agents come into existence.\n\nAllowing the set to vary over time like this feels similar to allowing a single agent to change its utility function. There is no rule against this. An agent can prefer to have different preferences than it does. A collective of agents can prefer to extend its altruism to new agents who come into existence.\n\nHowever, I see no reason why population ethics needs to be *simple*. We can have relatively complex preferences here. So, I don't find paradoxes such as the Repugnant Conclusion to be especially concerning. To me there's just this complicated question about what everyone collectively wants for the future.\n\nOne of the basic questions about utilitarianism shouldn't be \"average vs total?\". To me, this is a type error. It seems to me, more basic questions for a (preference) utilitarian are:\n\n*   How do you combine individual preferences into a collective utility function?\n    *   How do you compare utilities between people (and animals, etc)?\n        *   Do you care about an \"objective\" solution to this, or do you see it as a subjective aspect of altruistic preferences, which can be set in an unprincipled way?\n        *   Do you range-normalize?\n        *   Do you variance-normalize?\n        *   Do you care about strategy-proofness?\n        *   How do you evaluate the bargaining framing? Is it relevant, or irrelevant?\n        *   Do you care about Nash's axioms?\n        *   Do you care about monotonicity?\n        *   What distinguishes humans from animals and plants, and how do you use it in utility comparison? Intelligence? Agenticness? Power? Bargaining position?\n    *   How do you handle cofrences?\n\n*: Agents need not have a concept of outcome, in which case they [don't really have a utility function](https://www.lesswrong.com/posts/A8iGaZ3uHNNGgJeaD/an-orthodox-case-against-utility-functions) (because utility functions are functions *of outcomes*). However, this does not significantly impact any of the points made in this post."
    },
    "voteCount": 24
  },
  {
    "_id": "NEeW7eSXThPz7o4Ne",
    "url": null,
    "title": "Thou Art Physics",
    "slug": "thou-art-physics",
    "author": "Eliezer Yudkowsky",
    "question": false,
    "tags": [
      {
        "name": "Physics"
      },
      {
        "name": "Rationality"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "[Three months ago](https://www.lesswrong.com/posts/Mc6QcrsbH5NRXbCRX/dissolving-the-question)—jeebers, has it really been that long?—I posed the following [homework assignment](https://wiki.lesswrong.com/wiki/Free_will): Do a stack trace of the human cognitive algorithms that produce debates about “free will.” Note that this task is strongly distinguished from arguing that free will does or does not exist.\n\nNow, as expected, people are asking, “If the future is determined, how can our choices control it?” The wise reader can guess that [it all adds up to normality](https://www.lesswrong.com/posts/qcYCAxYZT4Xp9iMZY/living-in-many-worlds); but this leaves the question of _how_.\n\nPeople hear: “The universe runs like clockwork; physics is deterministic; [the future is fixed](http://lesswrong.com/lw/qp/timeless_physics/).” And their minds form a causal network that looks like this:\n\n![](https://res.cloudinary.com/lesswrong-2-0/image/upload/v1598054463/Physics_1_jw60yj_exuprr.svg)\n\nHere we see the causes “Me” and “Physics,” competing to determine the state of the “Future” effect. If the “Future” is fully determined by “Physics,” then obviously there is no room for it to be affected by “Me.”\n\nThis causal network is not an explicit philosophical belief. It’s implicit— a background representation of the brain, controlling which philosophical arguments seem “reasonable.” It just seems like the way things _are_.\n\nEvery now and then, another neuroscience press release appears, claiming that, because researchers used an fMRI to spot the brain doing something-or-other during a decision process, _it’s not you who chooses, it’s your brain_.\n\nLikewise that old chestnut, “Reductionism undermines rationality itself. Because then, every time you said something, it wouldn’t be the result of _reasoning_ about the evidence—it would be merely quarks bopping around.”\n\nOf course the actual diagram should be:\n\n  \n\n![](https://res.cloudinary.com/lesswrong-2-0/image/upload/v1598054465/Physics_2_eaghzx_zt9wqo.svg)\n\nOr better yet:\n\n![](https://res.cloudinary.com/lesswrong-2-0/image/upload/v1598054465/Physics_3_dzbxqd_omx2im.svg)\n\nWhy is this not obvious? Because there are many [levels of organization](https://www.lesswrong.com/posts/tPqQdLCuxanjhoaNs/reductionism) that separate our models of our thoughts—our emotions, our beliefs, our agonizing indecisions, and our final choices—from our models of electrons and quarks.\n\nWe can _intuitively_ visualize that a hand is made of fingers (and thumb and palm). To ask whether it’s _really_ our hand that picks something up, or _merely_ our fingers, thumb, and palm, is transparently a [wrong question](https://www.lesswrong.com/posts/XzrqkhfwtiSDgKoAF/wrong-questions).\n\nBut the gap between [physics and cognition](https://www.lesswrong.com/posts/ddwk9veF8efn3Nzbu/angry-atoms) cannot be crossed by direct visualization. No one can _visualize_ atoms making up a person, the way they can see fingers making up a hand.\n\nAnd so it requires _constant vigilance_ to maintain your perception of yourself as an entity _within physics_.\n\nThis vigilance is one of the great keys to philosophy, like the [Mind Projection Fallacy](https://www.lesswrong.com/posts/ZTRiSNmeGQK8AkdN2/mind-projection-fallacy). You will recall that it is this point which I [nominated](https://www.lesswrong.com/posts/Bh9cdfMjATrTdLrGH/where-philosophy-meets-science) as having tripped up the quantum physicists who failed to imagine macroscopic decoherence; they did not think to apply the laws to _themselves_.\n\nBeliefs, desires, emotions, morals, goals, imaginations, anticipations, sensory perceptions, fleeting wishes, ideals, temptations… You might call this the “surface layer” of the mind, the parts-of-self that people can see even without science. If I say, “It is not _you_ who determines the future, it is your _desires, plans, and actions_ that determine the future,” you can readily see the part-whole relations. It is immediately visible, like fingers making up a hand. There are other part-whole relations all the way down to physics, but they are not immediately visible.\n\n“Compatibilism” is the philosophical position that “free will” can be intuitively and satisfyingly defined in such a way as to be compatible with deterministic physics. “Incompatibilism” is the position that free will and determinism are incompatible.\n\nMy position might perhaps be called “Requiredism.” When agency, choice, control, and moral responsibility are cashed out in a sensible way, they _require_ determinism—at least some patches of determinism within the universe. If you choose, and plan, and act, and bring some future into being, in accordance with your desire, then all this requires a lawful sort of reality; you cannot do it amid utter chaos. There must be order over at least those parts of reality that are being controlled by you. _You_ are within physics, and so you/physics have determined the future. If it were not determined by physics, it could not be determined by you.\n\nOr perhaps I should say, “If the future were not determined by reality, it could not be determined by you,” or “If the future were not determined by something, it could not be determined by you.” You don’t need neuroscience or physics to push naive definitions of free will into incoherence. If the mind were not embodied in the brain, it would be embodied in something else; there would be _some real thing_ that was a mind. If the future were not determined by physics, it would be determined by _something_, some law, some order, some grand reality that included you within it.\n\nBut if the laws of physics control us, then how can we be said to control ourselves?\n\nTurn it around: If the laws of physics did _not_ control us, how could we possibly control ourselves?\n\nHow could thoughts judge other thoughts, how could emotions conflict with each other, how could one course of action appear best, how could we pass from uncertainty to certainty about our own plans, in the midst of utter chaos?\n\nIf we were not in reality, where could we be?\n\nThe future is determined by physics. What kind of physics? The kind of physics that includes the actions of human beings.\n\nPeople’s choices are determined by physics. What kind of physics? The kind of physics that includes weighing decisions, considering possible outcomes, judging them, being tempted, following morals, rationalizing transgressions, trying to do better…\n\nThere is no point where a quark swoops in from Pluto and overrides all this.\n\nThe thoughts of your decision process are all _real_, they are all _something_. But a thought is too big and complicated to be an atom. So thoughts are [made of smaller things](https://www.lesswrong.com/posts/tPqQdLCuxanjhoaNs/reductionism), and our name for the stuff that stuff is made of is “physics.”\n\nPhysics underlies our decisions and includes our decisions. It does not [explain them away](https://www.lesswrong.com/posts/cphoF8naigLhRf3tu/explaining-vs-explaining-away).\n\nRemember, [physics adds up to normality](https://www.lesswrong.com/posts/qcYCAxYZT4Xp9iMZY/living-in-many-worlds); [it’s your cognitive algorithms that generate confusion](https://www.lesswrong.com/posts/XzrqkhfwtiSDgKoAF/wrong-questions)"
    },
    "voteCount": 86
  },
  {
    "_id": "k8mwvvvpjMGcZLAKH",
    "url": null,
    "title": "The case for lifelogging as life extension",
    "slug": "the-case-for-lifelogging-as-life-extension",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "Cryonics"
      },
      {
        "name": "Practical"
      },
      {
        "name": "Lifelogging"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "Those in the cryonics community want to be frozen upon legal death, in order to preserve the information content in their brain. The hope is that, given good protocol, damage incurred during the freezing process will not destroy enough information about you to prevent people in the future from reconstructing your identity.\n\nAs most who want cryonics will understand, death is not an event. Instead, it is a process with intermediate steps. We consider a long-decayed corpse to be dead because it no longer performs the functions associated with a normal living human being, not because any sort of spirit or soul has left the body.\n\nBut philosophers have also identified important dilemmas for the view that death is a process rather than an event. If what we call death is simply my body performing different functions, then what do we make of the fact that we also change so much simply due to the passage of time?\n\nI find it easy to believe that I am the 'same person' as I was last night. Enough of the neural pathways are still the same. Memories from my childhood are essentially still identical. My personality has not changed any significant extent. My values and beliefs remain more-or-less intact.\n\nBut every day brings small changes to our identity. To what extent would you say that you are still the 'same person' as you were when you were a child? And to what extent are you still going to be the 'same person' when you get old?\n\nIn addition to the gradual changes that happen due to every day metabolic processes, and interactions with the outside world, there is also a more sudden change that may happen to your identity as you get old. By the age of 85, something like 25 to 50 percent of the population will get a form of dementia. Alzheimer's is a very harsh transformation to our connectome.\n\n![](https://upload.wikimedia.org/wikipedia/commons/thumb/a/a5/Alzheimer%27s_disease_brain_comparison.jpg/1920px-Alzheimer%27s_disease_brain_comparison.jpg)\n\nIronically, those who are healthiest in their youth will have the highest chance of getting Alzhiemers, as it is typically a disease of the very-old, rather than the somewhat old. Furthermore, most forecasters expect that as medical technology advances, the rate of Alzhiemers [will go _up_](https://www.alzforum.org/news/conference-coverage/alzheimers-incidence-rising-not-falling-researcher-says), since it's among the hardest diseases to fix with our current paradigm of medical technology, and therefore you won't be as likely to die of the others. And Alzhiemers is just one brand of [neurodegenerative](https://en.wikipedia.org/wiki/Neurodegeneration) diseases.\n\nIf you care about preserving your current self, and you think that death is a process rather than event, then it follows that you should want to preserve your current self: memories, personality, beliefs, values, mannerisms etc.\n\nThe technology to store the contents of our brains is currently extremely limited and expensive, but we have an alternative. We can store external information about ourselves, in the form of [lifelogging](https://en.wikipedia.org/wiki/Lifelog). The type of content we preserve can take a variety of forms, such as text, audio and video.\n\nIt might seem like preserving an audio of your voice will do little to restore your identity. But that might not be the case. If you are cryopreserved, then much of your connectome will be preserved anyway. The primary value of preserving external information is to 'fill in the blanks' so to speak.\n\nFor example, the most famous symptom of Alzheimers is memory loss. This occurs because the hippocampus, the primary component of our brain responsible for storing long-term memories, shrinks radically during the progression of the disease. If you consider memory to be important to your identity, then preserving external information about you could help function as an artificial memory source.\n\nWhat I'm trying to say is that if death is a process, it's not correct to say that you will either be revived or not in the future, like a binary event. Rather, _part_ of you will be revived. How much that part resembles you depends on how much information about you is preserved.\n\nThere are many clever methods I currently see for how future civilization could reconstruct your identity using your cryopreserved brain contents, and external memory together. If you can't see how the external memory helps at all, then I consider that a fault of imagination.\n\nSome will object by saying that lifelogging is _embarrassing_, as you are carrying a camera or audio recording device wherever you go. Indeed, [most of the reason](https://www.lesswrong.com/posts/CEGnJBHmkcwPTysb7/lonely-dissent) why people don't sign up for cryonics in the first place is because they fear that their peers will not approve. Lifelogging makes this dire situation worse. But I think there are steps you can take to make the appeal better.\n\nThe more information you preserve now, the better. There's no sharp cutoff point between having too little information and having just enough. If you feel uncomfortable walking around with a camera (and who wouldn't?) you don't have to. But consider taking small steps. Perhaps when you are in a video call with someone, ask them if they are OK with you recording it and later storing it as an mp3 on a hard disk. Or maybe you could write more of your personal thoughts into documents, and upload them to Google Drive.\n\nLittle actions like that could add up, or not. I claim no silver bullet.\n\nPart of the worst part of death is how terrible we are at motivating ourselves to avoid it. Among people who say they are interested in signing up for cryonics, only a small fraction end up signing the paperwork. And among those who do, the number who get preserved in optimal conditions is far too low. It seems that outside pressure from society is simply too powerful.\n\nBut as indicated by the [Asch conformity experiments](https://en.wikipedia.org/wiki/Asch_conformity_experiments), the best way to overcome societal pressure is by having peers that agree with and encourage you. If just a few people took this post seriously, this could be enough to puncture the equilibrium, and perhaps a lot of people will be interested in recording their lives. Who knows?"
    },
    "voteCount": 23
  },
  {
    "_id": "Nd5KiuN8pPBrMT82Z",
    "url": null,
    "title": "Asymmetric Weapons Aren't Always on Your Side",
    "slug": "asymmetric-weapons-aren-t-always-on-your-side-1",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "Social & Cultural Dynamics"
      },
      {
        "name": "Politics"
      },
      {
        "name": "Asymmetric Weapons"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "Some time ago, Scott Alexander wrote about [asymmetric weapons](https://slatestarcodex.com/2017/03/24/guided-by-the-beauty-of-our-weapons/), and now he [writes again about them](https://slatestarcodex.com/2019/06/06/asymmetric-weapons-gone-bad/). During these posts, Scott repeatedly characterizes asymmetric weapons as inherently stronger for the \"good guys\" than they are for the \"bad guys\". Here is a quote from his first post:\n\n> Logical debate has one advantage over narrative, rhetoric, and violence: it’s an _asymmetric weapon_. That is, it’s a weapon which is stronger in the hands of the good guys than in the hands of the bad guys.\n\nAnd here is a quote from his more recent one:\n\n> A symmetric weapon is one that works just as well for the bad guys as for the good guys. For example, violence – your morality doesn’t determine how hard you can punch; they can buy guns from the same places we can.\n\n> An asymmetric weapon is one that works better for the good guys than the bad guys. The example I gave was Reason. If everyone tries to solve their problems through figuring out what the right thing to do is, the good guys (who are right) will have an easier time proving themselves to be right than the bad guys (who are wrong). Finding and using asymmetric weapons is the only non-coincidence way to make sustained moral progress.\n\nOne problem with this concept is that _just because something is asymmetric doesn't mean that it's asymmetric in a good direction._\n\nScott talks about weapons that are asymmetric towards those who are right. However, there are many more types of asymmetries than just right vs. wrong - physical violence is asymmetric towards the strong, shouting people down is asymmetric towards the loud, and airing TV commercials is asymmetric towards people with more money. Violence isn't merely symmetric - it's _asymmetric in a bad direction_, since [fascists are better than violence than you](https://themiddleofthatzone.wordpress.com/2017/02/07/fascists-are-better-at-violence-than-you/).\n\nThis in turn means that various sides will all be trying to pull things in directions that are asymmetric to their advantage. Indeed, a basic principle in strategy is to try to shift conflicts into areas where you are strong and your opponent is weak.\n\nFor instance, people who are good at violence benefit from things getting violent. People who are locally popular benefit from popularity contests. People who have lots of free time benefit from time-consuming processes. People who are better at keeping their composure benefit from discourse norms that punish displays of emotion.\n\nDeveloping asymmetric processes that point towards truth is a good idea, and I'm all for it. But in practice there are also asymmetric processes that point towards error, or merely asymmetric processes that point towards what's currently popular or faddish. Those processes are, if anything, just as likely to have people trying to promote them than the pro-truth ones - perhaps more likely!\n\nThat doesn't make the people promoting those ideas \"anti-truth\" or whatever - they may not even be aware of what they're doing - but even so, people tend to respond to incentives, and those incentives may well pull them towards norms and methods that are asymmetric in their favor independent of whether those norms and methods promote truth."
    },
    "voteCount": 22
  },
  {
    "_id": "qajfiXo5qRThZQG7s",
    "url": null,
    "title": "Guided By The Beauty Of Our Weapons",
    "slug": "guided-by-the-beauty-of-our-weapons",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "Politics"
      },
      {
        "name": "Social & Cultural Dynamics"
      },
      {
        "name": "World Optimization"
      },
      {
        "name": "Adversarial Collaboration"
      },
      {
        "name": "Asymmetric Weapons"
      }
    ],
    "tableOfContents": {
      "sections": [
        {
          "title": "I.",
          "anchor": "I_",
          "level": 1
        },
        {
          "title": "II.",
          "anchor": "II_",
          "level": 1
        },
        {
          "title": "III.",
          "anchor": "III_",
          "level": 1
        },
        {
          "title": "IV.",
          "anchor": "IV_",
          "level": 1
        },
        {
          "title": "V.",
          "anchor": "V_",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        },
        {
          "anchor": "comments",
          "level": 0,
          "title": "5 comments"
        }
      ],
      "headingsCount": 7
    },
    "contents": {
      "markdown": "*\\[Content note: kind of talking around Trump supporters and similar groups as if they’re not there.\\]*\n\n**I.**\n\nTim Harford writes [The Problem With Facts](http://timharford.com/2017/03/the-problem-with-facts/), which uses Brexit and Trump as jumping-off points to argue that people are mostly impervious to facts and resistant to logic:\n\n> All this adds up to a depressing picture for those of us who aren’t ready to live in a post-truth world. Facts, it seems, are toothless. Trying to refute a bold, memorable lie with a fiddly set of facts can often serve to reinforce the myth. Important truths are often stale and dull, and it is easy to manufacture new, more engaging claims. And giving people more facts can backfire, as those facts provoke a defensive reaction in someone who badly wants to stick to their existing world view. “This is dark stuff,” says Reifler. “We’re in a pretty scary and dark time.”\n\nHe admits he has no easy answers, but cites some studies showing that “scientific curiosity” seems to help people become interested in facts again. He thinks maybe we can inspire scientific curiosity by linking scientific truths to human interest stories, by weaving compelling narratives, and by finding “a Carl Sagan or David Attenborough of social science”.\n\nI think this is generally a good article and makes important points, but there are three issues I want to highlight as possibly pointing to a deeper pattern.\n\nFirst, the article makes the very strong claim that “facts are toothless” – then tries to convince its readers of this using facts. For example, the article highlights a study by Nyhan & Reifler which finds a “backfire effect” – correcting people’s misconceptions only makes them cling to those misconceptions more strongly. Harford expects us to be impressed by this study. But how is this different from all of those social science facts to which he believes humans are mostly impervious?\n\nSecond, Nyhan & Reifler’s work on the backfire effect is probably not true. The original study establishing its existence [failed](https://www.poynter.org/2016/fact-checking-doesnt-backfire-new-study-suggests/436983/) to replicate (see eg [Porter & Wood, 2016](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2819073)). This isn’t directly contrary to Harford’s argument, because Harford doesn’t cite the original study – he cites a slight extension of it done a year later by the same team that comes to a slightly different conclusion. But given that the entire field is now in serious doubt, I feel like it would have been judicious to mention some of this in the article. This is especially true given that the article itself is about the way that false ideas spread by people never double-checking their beliefs. It seems to me that if you believe in an epidemic of falsehood so widespread that the very ability to separate fact from fiction is under threat, it ought to inspire a state of [CONSTANT VIGILANCE](https://slatestarcodex.com/2014/06/09/constant-vigilance/), where you obsessively question each of your beliefs. Yet Harford writes an entire article about a worldwide plague of false beliefs without mustering enough vigilance to see if the relevant studies are true or not.\n\nThird, Harford describes his article as being about *agnotology*, “the study of how ignorance is deliberately produced”. His key example is tobacco companies sowing doubt about the negative health effects of smoking – for example, he talks about tobacco companies sponsoring (accurate) research into all of the non-smoking-related causes of disease so that everyone focused on those instead. But his solution – telling engaging stories, adding a human interest element, enjoyable documentaries in the style of Carl Sagan – seems unusually unsuited to the problem. The National Institute of Health can make an engaging human interest documentary about a smoker who got lung cancer. And the tobacco companies can make an engaging human interest documentary about a guy who got cancer because of asbestos, then was saved by tobacco-sponsored research. Opponents of Brexit can make an engaging documentary about all the reasons Brexit would be bad, and then proponents of Brexit can make an engaging documentary about all the reasons Brexit would be good. If you get good documentary-makers, I assume both will be equally convincing regardless of what the true facts are.\n\nAll three of these points are slightly unfair. The first because Harford’s stronger statements about facts are probably exaggerations, and he just meant that in *certain cases* people ignore evidence. The second because the specific study cited wasn’t the one that failed to replicate and Harford’s thesis might be that it was different enough from the original that it’s probably true. And the third because the documentaries were just one idea meant to serve a broader goal of increasing “scientific curiosity”, a construct which has been shown in studies to be helpful in getting people to believe true things.\n\nBut I worry that taken together, they suggest an unspoken premise of the piece. It isn’t that *people* are impervious to facts. Harford doesn’t expect his reader to be impervious to facts, he doesn’t expect documentary-makers to be impervious to facts, and he certainly doesn’t expect *himself* to be impervious to facts. The problem is that there’s some weird tribe of fact-immune troglodytes out there, going around refusing vaccines and voting for Brexit, and the rest of us have to figure out what to do about them. The fundamental problem is one of *transmission*: how can we make knowledge percolate down from the fact-loving elite to the fact-impervious masses?\n\nAnd I don’t want to condemn this too hard, because it’s obviously true up to a point. Medical researchers have lots of useful facts about vaccines. Statisticians know some great facts about the link between tobacco and cancer (shame about [Ronald Fisher](https://priceonomics.com/why-the-father-of-modern-statistics-didnt-believe/), though). Probably there are even some social scientists who have a fact or two.\n\nYet [as I’ve argued before](http://slatestarcodex.com/2014/04/15/the-cowpox-of-doubt/), excessive focus on things like vaccine denialists teaches the wrong habits. It’s a desire to take a degenerate case, the rare situation where one side is obviously right and the other bizarrely wrong, and make it into the flagship example for modeling all human disagreement. Imagine a theory of jurisprudence designed only to smack down sovereign citizens, or a government pro-innovation policy based entirely on warning inventors against perpetual motion machines.\n\nAnd in this wider context, part of me wonders if the focus on transmission is part of the problem. Everyone from statisticians to Brexiteers knows that they are right. The only remaining problem is how to convince others. Go on Facebook and you will find a million people with a million different opinions, each confident in her own judgment, each zealously devoted to informing everyone else.\n\nImagine a classroom where everyone believes they’re the teacher and everyone else is students. They all fight each other for space at the blackboard, give lectures that nobody listens to, assign homework that nobody does. When everyone gets abysmal test scores, one of the teachers has an idea: *I need a more engaging curriculum*. Sure. That’ll help.\n\n**II.**\n\nA new Nathan Robinson article: [Debate Vs. Persuasion](https://www.currentaffairs.org/2017/03/debate-versus-persuasion). It goes through the same steps as the Harford article, this time from the perspective of the political Left. Deploying what Robinson calls “Purely Logical Debate” against Trump supporters hasn’t worked. Some leftists think the answer is violence. But this may be premature; instead, we should try the tools of rhetoric, emotional appeal, and other forms of discourse that aren’t Purely Logical Debate. In conclusion, Bernie Would Have Won.\n\n> I think giving up on argumentation, reason, and language, just because Purely Logical Debate doesn’t work, is a mistake. It’s easy to think that if we can’t convince the right with facts, there’s no hope at all for public discourse. But this might not suggest anything about the possibilities of persuasion and dialogue. Instead, it might suggest that mere facts are rhetorically insufficient to get people excited about your political program.\n\nThe resemblance to Harford is obvious. You can’t convince people with facts. But you *might* be able to convince people with facts carefully intermixed with human interest, compelling narrative, and emotional appeal.\n\nOnce again, I think this is generally a good article and makes important points. But I still want to challenge whether things are quite as bad as it says.\n\nGoogle [“debating Trump supporters is”](https://encrypted.google.com/search?q=%22debating+trump+supporters%22#q=%22debating+trump+supporters+is%22&*), and you realize where the article is coming from. It’s page after page of “debating Trump supporters is pointless”, “debating Trump supporters is a waste of time”, and “debating Trump supporters is like \\[funny metaphor for thing that doesn’t work\\]”. The overall picture you get is of a world full of Trump opponents and supporters debating on every street corner, until finally, after months of banging their heads against the wall, everyone collectively decided it was futile.\n\nYet I have the opposite impression. Somehow a sharply polarized country went through a historically divisive election with *essentially no debate taking place*.\n\nAm I about to [No True Scotsman](https://en.wikipedia.org/wiki/No_true_Scotsman) the hell out of the word “debate”? Maybe. But I feel like in using the exaggerated phrase “Purely Logical Debate, Robinson has given me leave to define the term as strictly as I like. So here’s what I think are minimum standards to deserve the capital letters:\n\n1\\. Debate where two people with opposing views are *talking* to each other (or writing, or IMing, or some form of bilateral communication). Not a pundit putting an article on *Huffington Post* and demanding Trump supporters read it. Not even a Trump supporter who comments on the article with a counterargument that the author will never read. Two people who have chosen to engage and to listen to one another.\n\n2\\. Debate where both people want to be there, and have chosen to enter into the debate in the hopes of getting something productive out of it. So not something where someone posts a “HILLARY IS A CROOK” meme on Facebook, someone gets really angry and lists all the reasons Trump is an even bigger crook, and then the original poster gets angry and has to tell them why they’re wrong. Two people who have made it their business to come together at a certain time in order to compare opinions.\n\n3\\. Debate conducted in the spirit of mutual respect and collaborative truth-seeking. Both people reject personal attacks or ‘gotcha’ style digs. Both people understand that the other person is *around* the same level of intelligence as they are and may have some useful things to say. Both people understand that they themselves might have some false beliefs that the other person will be able to correct for them. Both people go into the debate with the hope of convincing their opponent, but not completely rejecting the possibility that their opponent might convince them also.\n\n4\\. Debate conducted outside of a high-pressure point-scoring environment. No audience cheering on both participants to respond as quickly and bitingly as possible. If it can’t be done online, at least do it with a smartphone around so you can open Wikipedia to resolve simple matters of fact.\n\n5\\. Debate where both people agree on what’s being debated and try to stick to the subject at hand. None of this “I’m going to vote Trump because I think Clinton is corrupt” followed by “Yeah, but Reagan was even worse and that just proves you Republicans are hypocrites” followed by “*We’re* hypocrites? You Democrats claim to support women’s rights but you love Muslims who make women wear headscarves!” Whether or not it’s hypocritical to “support women’s rights” but “love Muslims”, it doesn’t seem like anyone is even *trying* to change each other’s mind about Clinton at this point.\n\nThese to me seem like the *bare minimum* conditions for a debate that could possibly be productive.\n\n(and while I’m asking for a pony on a silver platter, how about both people have to read *[How To Actually Change Your Mind](https://wiki.lesswrong.com/wiki/How_To_Actually_Change_Your_Mind)* first?)\n\nMeanwhile, in reality…\n\nIf you search “debating Trump supporters” without the “is”, your first result is [this video](https://www.youtube.com/watch?v=Fqm_Br1ywPY), where some people with a microphone corner some other people at what looks like a rally. I can’t really follow the conversation because they’re all shouting at the same time, but I can make out somebody saying ‘Republicans give more to charity!’ and someone else responding ‘That’s cause they don’t do anything at their jobs!'”. Okay.\n\nThe second link is [this podcast](https://soundcloud.com/best-of-the-left/debating-trump-supporters-colin-from-cleveland-oh) where a guy talks about debating Trump supporters. After the usual preface about how stupid they were, he describes a typical exchange – “It’s kind of amazing how they want to go back to the good old days…Well, when I start asking them ‘You mean the good old days when 30% of the population were in unions’…they never seem to like to hear that!…so all this unfettered free market capitalism has got to go bye-bye. They don’t find comfort in that idea either. It’s amazing. I can say I now know what cognitive dissonance feels like on someone’s face.” I’m glad time travel seems to be impossible, because otherwise I would be tempted to warp back and change my vote to Trump just to spite this person.\n\nThe third link is Vanity Fair’s [“Foolproof Guide To Arguing With Trump Supporters”](http://www.vanityfair.com/news/2017/02/reza-aslan-debating-trump-supporters), which suggests “using their patriotism against them” by telling them that wanting to “curtail the rights and privileges of certain of our citizens” is un-American.\n\nI worry that people do this kind of thing every so often. Then, when it fails, they conclude “Trump supporters are immune to logic”. This is much like observing that Republicans go out in the rain without melting, and concluding “Trump supporters are immortal”.\n\nAm I saying that if you met with a conservative friend for an hour in a quiet cafe to talk over your disagreements, they’d come away convinced? No. I’ve changed my mind on various things during my life, and it was never a single moment that did it. It was more of a series of different things, each taking me a fraction of the way. As the old saying goes, “First they ignore you, then they laugh at you, then they fight you, then they fight you half-heartedly, then they’re neutral, then they then they grudgingly say you might have a point even though you’re annoying, then they say on balance you’re mostly right although you ignore some of the most important facets of the issue, then you win.”\n\nThere might be a parallel here with the one place I see something like Purely Logical Debate on a routine basis: cognitive psychotherapy. I know this comparison sounds crazy, because psychotherapy is supposed to be the opposite of a debate, and trying to argue someone out of their delusions or depression inevitably fails. The rookiest of all rookie therapist mistakes is to say “FACT CHECK: The patient says she is a loser who everybody hates. PsychiaFact rates this claim: PANTS ON FIRE.”\n\nBut in other ways it’s a lot like the five points above. You have two people who disagree – the patient thinks she’s a worthless loser who everyone hates, and the therapist thinks maybe not. They meet together in a spirit of voluntary mutual inquiry, guaranteed safe from personal attacks like “You’re crazy!”. Both sides go over the evidence together, sometimes even agreeing on explicit experiments like “Ask your boyfriend tonight whether he hates you or not, predict beforehand what you think he’s going to say, and see if your prediction is accurate”. And both sides approach the whole process suspecting that they’re right but admitting the possibility that they’re wrong (very occasionally, after weeks of therapy, I realize that frick, everyone really *does* hate my patient. Then we switch strategies to helping her with social skills, or helping her find better friends).\n\nAnd contrary to what you see in movies, this doesn’t usually give a single moment of blinding revelation. If you spent your entire life talking yourself into the belief that you’re a loser and everyone hates you, no single fact or person is going to talk you out of it. But after however many months of intensive therapy, sometimes someone who was *sure* that they were a loser is now *sort of questioning* whether they’re a loser, and has the mental toolbox to take things the rest of the way themselves.\n\nThis was also the response I got when I tried to make [an anti-Trump case](https://slatestarcodex.com/2016/09/28/ssc-endorses-clinton-johnson-or-stein/) on this blog. I don’t think there were any sudden conversions, but here were some of the positive comments I got from Trump supporters:\n\n[—](https://slatestarcodex.com/2016/09/28/ssc-endorses-clinton-johnson-or-stein/#comment-415499) “This is a compelling case, but I’m still torn.”\n\n[—](https://slatestarcodex.com/2016/09/28/ssc-endorses-clinton-johnson-or-stein/#comment-415543) “This contains the most convincing arguments for a Clinton presidency I have ever seen. But, perhaps also unsurprisingly, while it did manage to shift some of my views, it did not succeed in convincing me to change my bottom line.”\n\n[—](https://slatestarcodex.com/2016/09/28/ssc-endorses-clinton-johnson-or-stein/#comment-415708) “This article is perhaps the best argument I have seen yet for Hillary. I found myself nodding along with many of the arguments, after this morning swearing that there was nothing that could make me consider voting for Hillary…the problem in the end was that it wasn’t enough.”\n\n[—](https://slatestarcodex.com/2016/09/28/ssc-endorses-clinton-johnson-or-stein/#comment-415749) “The first coherent article I’ve read justifying voting for Clinton. I don’t agree with your analysis of the dollar “value” of a vote, but other than that, something to think about.”\n\n[—](https://slatestarcodex.com/2016/09/28/ssc-endorses-clinton-johnson-or-stein/#comment-415873) “Well I don’t like Clinton at all, and I found this essay reasonable enough. The argument from continuity is probably the best one for voting Clinton if you don’t particularly love any of her policies or her as a person. Trump is a wild card, I must admit.”\n\n[—](https://slatestarcodex.com/2016/09/28/ssc-endorses-clinton-johnson-or-stein/#comment-416477) As an orthodox Catholic, you would probably classify me as part of your conservative audience…I certainly concur with both the variance arguments and that he’s not conservative by policy, life, or temperament, and I will remain open to hearing what you have to say on the topic through November.\n\n[—](https://slatestarcodex.com/2016/09/28/ssc-endorses-clinton-johnson-or-stein/#comment-417013) “I’ve only come around to the ‘hold your nose and vote Trump’ camp the past month or so…I won’t say \\[you\\] didn’t make me squirm, but I’m holding fast to my decision.”\n\n*These* are the people you say are completely impervious to logic so don’t even try? It seems to me like this argument was one of not-so-many straws that might have broken some camels’ backs if they’d been allowed to accumulate. And the weird thing is, when I re-read the essay I notice a lot of flaws and things I wish I’d said differently. I don’t think it was an exceptionally good argument. I think it was…an argument. It was something more than saying “You think the old days were so great, but the old days had labor unions, CHECKMATE ATHEISTS”. This isn’t what you get when you do a splendid virtuouso perfomance. This is what you get *when you show up*.\n\n(and lest I end up ‘objectifying’ Trump supporters as prizes to be won, I’ll add that in the comments some people made pro-Trump arguments, and two people who were previously leaning Clinton said that they were feeling uncomfortably close to being convinced)\n\nAnother SSC story. I keep trying to keep “culture war”-style political arguments from overrunning the blog and subreddit, and every time I add restrictions [a bunch of people complain](https://www.reddit.com/r/slatestarcodex/comments/60gvph/culture_war_roundup_for_week_of_march_20_2017/df76d5z/) that this is the only place they can go for that. Think about this for a second. A heavily polarized country of three hundred million people, split pretty evenly into two sides and obsessed with politics, blessed with the strongest free speech laws in the world, and people are complaining that I can’t change my comment policy because this one small blog is *the only place they know where they can debate people from the other side*.\n\nGiven all of this, I reject the argument that Purely Logical Debate has been tried and found wanting. Like GK Chesterton, I think it has been found difficult and left untried.\n\n**III.**\n\nTherapy might change minds, and so might friendly debate among equals, but neither of them scales very well. Is there anything that big fish in the media can do beyond the transmission they’re already trying?\n\nLet’s go back to that Nyhan & Reifler study which found that fact-checking backfired. As I mentioned above, a replication attempt by Porter & Wood found the opposite. This could have been the setup for a nasty conflict, with both groups trying to convince academia and the public that they were right, or even accusing the other of scientific malpractice.\n\nInstead, something great happened. All four researchers [decided to work together](http://nymag.com/scienceofus/2016/11/theres-more-hope-for-political-fact-checking.html) on an “adversarial collaboration” – a bigger, better study where they all had input into the methodology and they all checked the results independently. The collaboration found that fact-checking generally didn’t backfire in most cases. All four of them used their scientific clout to publicize the new result and launch further investigations into the role of different contexts and situations.\n\nInstead of treating disagreement as demonstrating a need to transmit their own opinion more effectively, they viewed it as demonstrating a need to collaborate to investigate the question together.\n\nAnd yeah, part of it was that they were all decent scientists who respected each other. But they didn’t *have* to be. If one team had been total morons, and the other team was secretly laughing at them the whole time, the collaboration still would have worked. All required was an assumption of good faith.\n\nA while ago I blogged about a journalistic spat between German Lopez and Robert VerBruggen on gun control. Lopez wrote [a voxsplainer](http://www.vox.com/2015/10/3/9444417/gun-violence-united-states-america) citing some statistics about guns. VerBruggen wrote [a piece at National Review](http://www.nationalreview.com/article/427967/guns-tk-robert-verbruggen?target=author&tid=1043) saying that some of the statistics were flawed. German fired back (pun not intended) [with an article](http://www.vox.com/policy-and-politics/2015/12/8/9870240/gun-ownership-deaths-homicides) claiming that VerBruggen was ignoring better studies.\n\n(Then I [yelled at both of them](https://slatestarcodex.com/2016/01/06/guns-and-states/), as usual.)\n\nOverall the exchange was in the top 1% of online social science journalism – by which I mean it included at least one statistic and at some point that statistic was superficially examined. But in the end, it was still just two people arguing with one another, each trying to transmit his superior knowledge to each other and the reading public. As good as it was, it didn’t meet my five standards above – and nobody expected it to.\n\nBut now I’m thinking – what would have happened if Lopez and VerBruggen had joined together in an adversarial collaboration? Agreed to work together to write an article on gun statistics, with nothing going into the article unless they both approved, and then they both published that article on their respective sites?\n\nThis seems like a mass media equivalent of shifting from Twitter spats to serious debate, from transmission mindset to collaborative truth-seeking mindset. The adversarial collaboration model is just the first one to come to mind right now. I’ve blogged about others before – for example, bets, prediction markets, and calibration training.\n\nThe media already spends a lot of effort *recommending* good behavior. What if they tried *modeling* it?\n\n**IV.**\n\nThe bigger question hanging over all of this: “Do we *have* to?”\n\nHarford’s solution – compelling narratives and documentaries – sounds easy and fun. Robinson’s solution – rhetoric and emotional appeals – also sounds easy and fun. Even the solution Robinson rejects – violence – is easy, and fun for a certain type of person. All three work on pretty much anybody.\n\nPurely Logical Debate is difficult and annoying. It doesn’t scale. It only works on the subset of people who are willing to talk to you in good faith and smart enough to understand the issues involved. And even then, it only works glacially slowly, and you win only partial victories. What’s the point?\n\nLogical debate has one advantage over narrative, rhetoric, and violence: it’s an *asymmetric weapon*. That is, it’s a weapon which is stronger in the hands of the good guys than in the hands of the bad guys. In ideal conditions (which may or may not ever happen in real life) – the kind of conditions where everyone is charitable and intelligent and wise – the good guys will be able to present stronger evidence, cite more experts, and invoke more compelling moral principles. The whole point of logic is that, when done right, it can only prove things that are true.\n\nViolence is a *symmetric weapon*; the bad guys’ punches hit just as hard as the good guys’ do. It’s true that hopefully the good guys will be more popular than the bad guys, and so able to gather more soldiers. But this doesn’t mean violence itself is asymmetric – the good guys will only be more popular than the bad guys insofar as their ideas have previously spread through some means other than violence. Right now antifascists outnumber fascists and so could probably beat them in a fight, but antifascists didn’t come to outnumber fascists by winning some kind of primordial fistfight between the two sides. They came to outnumber fascists because people rejected fascism on the merits. These merits might not have been “logical” in the sense of Aristotle dispassionately proving lemmas at a chalkboard, but “fascists kill people, killing people is wrong, therefore fascism is wrong” is a sort of folk logical conclusion which is both correct and compelling. Even “a fascist killed my brother, so fuck them” is a placeholder for a powerful philosophical argument making a probabilistic generalization from indexical evidence to global utility. So insofar as violence is asymmetric, it’s because it parasitizes on logic which allows the good guys to be more convincing and so field a bigger army. Violence itself doesn’t enhance that asymmetry; if anything, it decreases it by giving an advantage to whoever is more ruthless and power-hungry.\n\nThe same is true of documentaries. As I said before, Harford can produce as many anti-Trump documentaries as he wants, but Trump can fund documentaries of his own. He has the best documentaries. Nobody has ever seen documentaries like this. They’ll be absolutely huge.\n\nAnd the same is true of rhetoric. Martin Luther King was able to make persuasive emotional appeals for good things. But Hitler was able to make persuasive emotional appeals for bad things. I’ve [previously argued](https://slatestarscratchpad.tumblr.com/post/103708539246/nostalgebraist-at-various-points-bostrom-like) that Mohammed counts as the most successful persuader of all time. These three people pushed three very different ideologies, and rhetoric worked for them all. Robinson writes as if “use rhetoric and emotional appeals” is a novel idea for Democrats, but it seems to me like they were doing little else throughout the election (pieces attacking Trump’s character, pieces talking about how inspirational Hillary was, pieces appealing to various American principles like equality, et cetera). It’s just that they did a bad job, and Trump did a better one. The real takeaway here is “do rhetoric better than the other guy”. But “succeed” is not a primitive action.\n\nUnless you use asymmetric weapons, the best you can hope for is to win by coincidence.\n\nThat is, there’s no reason to think that good guys are consistently better at rhetoric than bad guys. Some days the Left will have an Obama and win the rhetoric war. Other days the Right will have a Reagan and *they’ll* win the rhetoric war. Overall you should average out to a 50% success rate. When you win, it’ll be because you got lucky.\n\nAnd there’s no reason to think that good guys are consistently better at documentaries than bad guys. Some days the NIH will spin a compelling narrative and people will smoke less. Other days the tobacco companies will spin a compelling narrative and people will smoke more. Overall smoking will stay the same. And again, if you win, it’s because you lucked out into having better videographers or something.\n\nI’m not against winning by coincidence. If I stumbled across Stalin and I happened to have a gun, I would shoot him without worrying about how it’s “only by coincidence” that he didn’t have the gun instead of me. You should use your symmetric weapons if for no reason other than that the other side’s going to use *theirs* and so you’ll have a disadvantage if you don’t. But you shouldn’t confuse it with a long-term solution.\n\nImproving the quality of debate, shifting people’s mindsets from transmission to collaborative truth-seeking, is a painful process. It has to be done one person at a time, it only works on people who are already *almost* ready for it, and you will pick up far fewer warm bodies per hour of work than with any of the other methods. But in an otherwise-random world, even a little purposeful action can make a difference. Convincing 2% of people would have flipped three of the last four US presidential elections. And this is a capacity to win-for-reasons-other-than-coincidence that you can’t build any other way.\n\n(and my hope is that the people most willing to engage in debate, and the ones most likely to recognize truth when they see it, are disproportionately influential – scientists, writers, and community leaders who have influence beyond their number and can help others see reason in turn)\n\nI worry that I’m not communicating how beautiful and inevitable all of this is. We’re surrounded by a a vast confusion, “a darkling plain where ignorant armies clash by night”, with one side or another making a temporary advance and then falling back in turn. And in the middle of all of it, there’s this gradual capacity-building going on, where what starts off as a hopelessly weak signal gradually builds up strength, until one army starts winning a little more often than chance, then a lot more often, and finally takes the field entirely. Which seems strange, because surely you can’t build any complex signal-detection machinery in the middle of all the chaos, surely you’d be shot the moment you left the trenches, but – *your enemies are helping you do it*. Both sides are diverting their artillery from the relevant areas, pooling their resources, helping bring supplies to the engineers, because until the very end they think it’s going to ensure *their* final victory and not yours.\n\nYou’re doing it right under their noses. They might try to ban your documentaries, heckle your speeches, fight your violence Middlebury-student-for-Middlebury-student – but when it comes to the long-term solution to ensure your complete victory, they’ll roll down their sleeves, get out their hammers, and build it alongside you.\n\nA parable: Sally is a psychiatrist. Her patient has a strange delusion: that *Sally* is the patient and *he* is the psychiatrist. She would like to commit him and force medication on him, but he is an important politician and if push comes to shove he might be able to commit *her* instead. In desperation, she proposes a bargain: they will *both* take a certain medication. He agrees; from within his delusion, it’s the best way for him-the-psychiatrist to cure her-the-patient. The two take their pills at the same time. The medication works, and the patient makes a full recovery.\n\n(well, half the time. The other half, the medication works and *Sally* makes a full recovery.)\n\n**V.**\n\nHarford’s article says that facts and logic don’t work on people. The various lefty articles say they merely don’t work on Trump supporters, ie 50% of the population.\n\nIf you genuinely believe that facts and logic don’t work on people, you shouldn’t be writing articles with potential solutions. You should be jettisoning everything you believe and entering a state of pure Cartesian doubt, where you try to rederive everything from *cogito ergo sum*.\n\nIf you genuinely believe that facts and logic don’t work on at least 50% of the population, again, you shouldn’t be writing articles with potential solutions. You should be worrying whether you’re in that 50%. After all, how did you figure out you aren’t? By using facts and logic? *What did we just say?*\n\nNobody is doing either of these things, so I conclude that they accept that facts can sometimes work. Asymmetric weapons are not a pipe dream. As Gandhi used to say, “If you think the world is all bad, remember that it contains people like you.”\n\nYou are not completely immune to facts and logic. But you have been wrong about things before. You may be a bit smarter than the people on the other side. You may even be a *lot* smarter. But fundamentally their problems are your problems, and the same kind of logic that convinced you can convince them. It’s just going to be a long slog. You didn’t develop *your* opinions after a five-minute shouting match. You developed them after years of education and acculturation and engaging with hundreds of books and hundreds of people. Why should they be any different?\n\nYou end up believing that the problem is deeper than insufficient documentary production. The problem is that Truth is a weak signal. You’re trying to perceive Truth. You would like to hope that the other side is trying to perceive Truth too. But at least one of you is doing it wrong. It seems like perceiving Truth accurately is harder than you thought.\n\nYou believe your mind is a truth-sensing instrument that does at least a little bit better than chance. You *have* to believe that, or else what’s the point? But it’s like one of those physics experiments set up to detect gravitational waves or something, where it has to be in a cavern five hundred feet underground in a lead-shielded chamber atop a gyroscopically stable platform cooled to one degree above absolute zero, trying to detect fluctuations of a millionth of a centimeter. Except you don’t have the cavern or the lead or the gyroscope or the coolants. You’re on top of an erupting volcano being pelted by meteorites in the middle of a hurricane.\n\nIf you study psychology for ten years, you can remove the volcano. If you spend another ten years obsessively checking your performance in various *metis*-intensive domains, you can remove the meteorites. You can never remove the hurricane and you shouldn’t try. But if there are a thousand trustworthy people at a thousand different parts of the hurricane, then the stray gusts of wind will cancel out and they can average their readings to get something approaching a signal.\n\nAll of this is too slow and uncertain for a world that needs more wisdom *now*. It would be nice to force the matter, to pelt people with speeches and documentaries until they come around. This will work in the short term. In the long term, it will leave you back where you started.\n\nIf you want people to be right more often than chance, you have to teach them ways to distinguish truth from falsehood. If this is in the face of enemy action, you will have to teach them so well that they cannot be fooled. You will have to do it person by person until the signal is strong and clear. You will have to [raise the sanity waterline](http://lesswrong.com/lw/1e/raising_the_sanity_waterline/). There is no shortcut."
    },
    "voteCount": 27
  },
  {
    "_id": "5g7oFiePGEY3h4bqX",
    "url": null,
    "title": "Prereq: Cognitive Fusion",
    "slug": "prereq-cognitive-fusion",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "Cognitive Fusion"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "In a [post](https://www.lesswrong.com/posts/mELQFMi9egPn5EAjK/my-attempt-to-explain-looking-insight-meditation-and) by Kaj Sotala, he introduces the very useful idea of cognitive fusion.\n\n> Cognitive fusion is a term from [Acceptance and Commitment Therapy](https://en.wikipedia.org/wiki/Acceptance_and_commitment_therapy) (ACT), which refers to a person “fusing together” with the content of a thought or emotion, so that the content is experienced as an objective fact about the world rather than as a mental construct. The most obvious example of this might be if you get really upset with someone else and become convinced that something was _all their fault_ (even if you had actually done something blameworthy too).  \n> In this example, your anger isn’t letting you see clearly, and you can’t step back from your anger to question it, because you have become “fused together” with it and experience everything in terms of the anger’s internal logic.\n\nYou can become fused to an emotion, a voice in your head, a political view, and experience it to \"just be true\". I see this as a similar sort of fusion I hear musician talk about, where after years of practice their instrument begin to feel like a part of their body. They aren't \"using their index finger to press the black note on a piano\" they are \"just playing G\". This is analogous to being so caught up in your own anger that your partner is \"just wrong and terrible\" as opposed to \"it sorta looks like you intentionally did something to annoy me and I'm worried about if you'll do this again in the future.\" (or whatever the actual case is)\n\nSometimes I think of there being a general fusion process where the brain collapses levels of inference. All of the steps that go into a given physical motion or thought process get compressed into a single dot. The thought process will be experienced as \"just true\" and the physical motion will be experience as an atomic action available to you. Sometimes you can \"uncompress\" the chain, and sometimes you can't.\n\nProblems can arise when you fuse to a thought or emotion that doesn't have an accurate view of the world, and you unknowingly take it's broken map as the territory.\n\n### **Isn't this just \"Don't make assumptions\"?**\n\nNot quite, though it is similar. Assumptions don't really capture the more general fusing process that you can also see with physical movement. \"I can't believe that you just assumed you start off on your left foot when making a layup!\" Nah, doesn't feel right. But the main reason I prefer to talk in terms of fusion is that \"fusion\" makes me focus on the process of attaching to something, while \"assumptions\" makes me focus on the object being attached to.\n\nIt's easier to see this difference when the thought being fused to (or the claim being assumed) is \"obviously\" wrong, or at least obvious to one who isn't fused to it. The assumption frame makes me feel like my work is done when I find the other persons \"dumb\" assumption. Point it out with a pithy \"Checkmate \\[outgroup\\]\" and move on. The fusion frame leads me to ask \"How did they get fused to this in the first place? How might I help them defuse from it?\" By focusing on the process of attachment (fusion) I can appreciate how common it is to fuse to something and how hard it can be to defuse. When I focus on the object of attachment (assumption) I'm mostly thinking about just how _stupid_ it is and how I can't believe that anyone would be _dumb_ enough to fall for this, and I most certainly don't believe anything that stupid....\n\nAnd so it goes. Thinking of some behavior as a \"dumb mistake\" makes you _more likely to not notice when you engage in it_. Some thoughts take moments to defuse from. Others take a lifetime. Sometimes you fuse to things, and you'd be wise to learn how it works rather than to ridicule it.\n\nAs you may have guessed, later parts of this sequence will talk about what can happen when you fuse with language. For now, just remember what fusion is, and treat it with respect.\n\n> \"Modern man can't see God because he doesn't look low enough.\"\n\n\\-\\- Carl Jung"
    },
    "voteCount": 7
  },
  {
    "_id": "xfcKYznQ6B9yuxB28",
    "url": null,
    "title": "Final Version Perfected: An Underused Execution Algorithm",
    "slug": "final-version-perfected-an-underused-execution-algorithm",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "Practical"
      },
      {
        "name": "Rationality"
      },
      {
        "name": "Productivity"
      },
      {
        "name": "Algorithms"
      }
    ],
    "tableOfContents": {
      "sections": [
        {
          "title": "TLDR",
          "anchor": "TLDR",
          "level": 1
        },
        {
          "title": "Introduction",
          "anchor": "Introduction",
          "level": 1
        },
        {
          "title": "The FVP Algorithm",
          "anchor": "The_FVP_Algorithm",
          "level": 1
        },
        {
          "title": "A long-ish example",
          "anchor": "A_long_ish_example",
          "level": 1
        },
        {
          "title": "FVP: Why and why not",
          "anchor": "FVP__Why_and_why_not",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        },
        {
          "anchor": "comments",
          "level": 0,
          "title": "31 comments"
        }
      ],
      "headingsCount": 7
    },
    "contents": {
      "markdown": "**TLDR**\n========\n\n*   [Final Version Perfected](http://markforster.squarespace.com/blog/2015/5/21/the-final-version-perfected-fvp.html) (FVP) is a highly effective algorithm for deciding which tasks from your To-Do lists to do in what order.\n*   The design of the [algorithm](https://www.lesswrong.com/posts/xfcKYznQ6B9yuxB28/final-version-perfected-an-underused-execution-algorithm#The_FVP_Algorithm) makes it far more efficient than exhaustive ranking, while (in my experience) far more effective than just reading through the tasks and picking one out.\n*   FVP is most useful when you have a large number of tasks to choose from, don’t have time to do all of them, and are initially unsure about which is best.\n*   I find FVP very effective at overcoming psychological issues like indecision, procrastination, or [psychological aversion to particular tasks](https://medium.com/@robertwiblin/ugh-fields-or-why-you-can-t-even-bear-to-think-about-that-task-5941837dac62).\n*   Currently there are limited online tools available, and I mostly use FVP with paper lists. Ideas (or tools) for better online execution of FVP would be very valuable to me.\n\n**Introduction**\n================\n\nExecution is the [Last Mile Problem](https://en.wikipedia.org/wiki/Last_mile_(transportation)) of productivity infrastructure. You can put as much effort as you like into organising your goals, organising your To-Do lists, organising your calendar, but sooner or later you will be presented with more than one thing you could reasonably be doing with your time. When that happens, you will need some sort of method for choosing what that thing will be, and actually getting started.\n\nMost people, I think, face this problem by either just doing the thing that is top-of-mind or looking through their To-Do list and picking something out. This works fine when the next thing to do is obvious, and you have no problems getting started on it. But when you have many potential things to do and aren’t sure which is best, or when you kind of know what the best next thing is but are avoiding it for one reason or another, you need a better system.\n\nThat system needs to be quick to execute, easy to remember, and effective at actually having you do the best next task. It needs to be robust to your psychological weaknesses, minimising procrastination, indecision, and ugh fields. It needs to be efficient, requiring as little work as possible to identify the most valuable task.\n\nEnter [Final Version Perfected](http://markforster.squarespace.com/blog/2015/5/21/the-final-version-perfected-fvp.html).\n\n**The FVP Algorithm**\n=====================\n\nThe algorithm for executing tasks under FVP is pretty simple. You can find a description of it by the designer [here](http://markforster.squarespace.com/blog/2015/5/21/the-final-version-perfected-fvp.html), but here’s my version:\n\n1.  Put all the tasks you have to choose from into one big unsorted list.\n2.  **Mark** the first item on the list. **Don’t do it yet.**\n3.  For each *subsequent* item on the list, ask yourself, \"Do I want to do this task more than the *last task I marked*?\" If yes, **mark it**. If no, **don't mark it**. Move on to the next item.\n4.  When you reach the end of the list, trace back up to find the bottom-most marked task. **Do it**, then **cross it off** the list.\n5.  Beginning with the next unmarked task *after the task you just crossed off*, repeat step 3, comparing each task to the bottom-most uncrossed marked task (i.e. the one prior to the one you just crossed out).\n6.  Go to step 4. Repeat until you run out of time or list items.\n\nIn FVP, then, you perform a series of pairwise comparisons between tasks, in each case asking whether the new task is something you *want to do more than* the old task. The “want to do more than” comparison is deliberately vague: Depending on context, it might be the thing that would best move your project forward, the thing that would have the worst consequences if you didn’t do it, or the thing you would most enjoy doing. The key thing is that at each stage, you’re only comparing each task to the *most recent task you marked*, ignoring all previous tasks.\n\nI'll talk more in a moment about why I think this algorithm is a good one, but first, let’s work through an example. (If you're sure you already understand the algorithm, click [here](https://www.lesswrong.com/posts/xfcKYznQ6B9yuxB28/final-version-perfected-an-underused-execution-algorithm#FVP__Why_and_why_not) to go straight to the pros and cons.)\n\n**A long-ish example**\n======================\n\nLet's say this is my to-do list for today:\n\n*   Buy milk\n*   Finish term paper\n*   Play video games\n*   Work out\n*   Save the world\n*   Walk the dog\n\nI start by marking the first item:\n\n*   **× Buy milk**\n*   Finish term paper\n*   Play video games\n*   Work out\n*   Save the world\n*   Walk the dog\n\nThen I compare it to the next item on the list. Which do I want to do more, finish the term paper or buy milk? Well, the term paper is due today, and I don't need milk until tomorrow, so I decide to do the term paper first.\n\n*   **× Buy milk**\n*   **× Finish term paper**\n*   Play video games\n*   Work out\n*   Save the world\n*   Walk the dog\n\nMoving on to item 3. I already decided I want to finish the term paper before buying milk, so I can ignore the milk for now. Do I want to play video games or finish my term paper? Well, in some sense I *want* to play video games more, but my all-things-considered *endorsement* is to finish the term paper first, so I leave item 3 unmarked.\n\nNext, item 4: do I want to finish the term paper or work out? Well, in some sense I'd rather not do either, and in another sense the term paper is more urgent, but working out is important, I've heard it has [cognitive benefits](https://en.wikipedia.org/wiki/Neurobiological_effects_of_physical_exercise), and I know from experience that if I don't do it first thing I won't do it, so it takes precedence:\n\n*   **× Buy milk**\n*   **× Finish term paper**\n*   Play video games\n*   **× Work out**\n*   Save the world\n*   Walk the dog\n\nItem 5: oh yeah, I forgot, I need to save the world today. Damn. Well, I can't work out if there's no world to work out in, so I guess I’ll do that first.\n\n*   **× Buy milk**\n*   **× Finish term paper**\n*   Play video games\n*   **× Work out**\n*   **× Save the world**\n*   Walk the dog\n\nDitto for walking the dog: much though I love him, I won't have anywhere to walk him if I don't save the world first, so that takes precedence again.\n\nI've finished the list now, so it's time to do the last item on the list. Looks like that's saving the world. Luckily, it doesn't take long:\n\n*   **× Buy milk**\n*   **× Finish term paper**\n*   Play video games\n*   **× Work out**\n*   ~× Save the world~  ✓\n*   Walk the dog\n\nNow that I've done the highest priority task on the list, I go back to FVP to determine the next one. There's actually only one comparison I need to make: work out or walk the dog? Walking the dog can wait until the evening, so it’s time to head to the gym.\n\n*   **× Buy milk**\n*   **× Finish term paper**\n*   Play video games\n*   ~× Work out~  ✓\n*   ~× Save the world~  ✓\n*   Walk the dog\n\nAgain, there's only one more comparison I need to do to determine my next top task: do I want to finish my term paper, or walk the dog? And again, walking the dog isn't that urgent, so I spend a few hours on the term paper.\n\n*   **× Buy milk**\n*   ~× Finish term paper~  ✓\n*   Play video games\n*   ~× Work out~  ✓\n*   ~× Save the world~  ✓\n*   Walk the dog\n\nNow I'm all the way back to the top of the list! But now there are *two* more comparisons to make to decide on the next task. First, do I want to buy milk, or play video games? I've worked pretty hard so far today, and buying milk isn't that important, so let's play games first:\n\n*   **× Buy milk**\n*   ~× Finish term paper~  ✓\n*   **× Play video games**\n*   ~× Work out~  ✓\n*   ~× Save the world~  ✓\n*   Walk the dog\n\nFinally, do I want to walk the dog or play video games? The dog has been waiting for hours for a walk now, and I could do with some fresh air, and I'd feel guilty just gaming without taking him out, so let's do that first:\n\n*   **× Buy milk**\n*   ~× Finish term paper~  ✓\n*   **× Play video games**\n*   ~× Work out~  ✓\n*   ~× Save the world~  ✓\n*   **× Walk the dog**\n\nThere's no unmarked tasks in the list now, so to finish I just work up the list in order: first walking the dog, then playing games, then, finally, buying milk.\n\n**FVP: Why and why not**\n========================\n\nThe usefulness of FVP depends on a few key assumptions.\n\n*   Firstly, the algorithm assumes your preferences are [transitive](https://en.wikipedia.org/wiki/Transitive_relation#Examples), and that you can accurately assess the value of each task according to your preferences. These are pretty fundamental assumptions that will be integral to almost any list-based execution system. In reality, your preferences probably aren’t quite transitive, but hopefully they are close enough that pretending they are is reasonable. As for accurately assessing each task, well, no execution algorithm can prevent you from making any mistakes, but FVP is more effective than most at eliciting your best guesses.\n*   Secondly, FVP assumes that your preferences are **stable** over the timeframe you're using it. If your preferences shift substantially over that period, such that you need to re-prioritise among the existing tasks on your list, you'll need to throw out your previous FVP and start again. This places some constraints on the timescale you can organise using a single FVP iteration: I seldom stick with the same iteration for longer than a day. (Note, though, that FVP can handle the addition of *new* tasks quite easily, as long as they don’t alter the existing order.)\n*   Thirdly, the value of FVP is greatest when you are **unsure** about which task you should do next, and especially when you don’t have time to do every task you might want to do that day. I find FVP most useful when I have a lot of different tasks competing for my time; it is much less useful when my time is pre-allocated to a single, well-planned-out task.\n\nWhen these conditions are met, FVP is a *very* effective method for guiding action. It is both efficient and exhaustive: guaranteed to identify the top-priority task while avoiding most of the work involved in producing a complete ranking. It is a simple algorithm, easy to remember and quick to perform. After doing it for a while, I find it scarcely requires conscious thought – but still reliably identifies the most valuable task for me to work on.\n\nThe biggest benefit I get from FVP, though, is how much easier it makes it to [do important things I'd rather avoid](https://medium.com/@robertwiblin/ugh-fields-or-why-you-can-t-even-bear-to-think-about-that-task-5941837dac62). There is something about a bald, pairwise comparison between two tasks that is highly effective at overcoming my aversion to difficult things. If an important but unpleasant task is nestled within a long to-do list of minor-but-rewarding busywork, it is easy for my eye to skip over the difficult task, defer it till tomorrow, and work on something more pleasant instead. It's much harder to do that when comparing the important task to each minor task in isolation.\n\nFVP is also good at minimising [time lost due to indecision](https://en.wikipedia.org/wiki/Buridan%27s_ass). When presented with a menu of tasks to choose from, it can be quite hard to select a single task to work on first. When that choice process is reduced to a series of simple pairwise comparisons, the choosing process as a whole becomes much easier. And, once I've finished with FVP and selected a single winning task, there's an impetus towards starting that makes me much less prone to procrastination.\n\nOne last brief note on infrastructure: due to its relative obscurity, I haven’t found great online tools for FVP. [Complice](http://complice.co/)’s starred-task system can be passably adapted to the algorithm, but in general I’ve found physical paper lists to work best. When I was at work I would print off my Todoist task lists and use those; now I’m working from home I mostly write them out by hand. This is kind of time-consuming and redundant, so if you dislike paper notes and don’t have access to a printer it might be a significant mark against FVP.\n\nI’d really love it if someone created a great online tool for FVP or integrated it more formally into an existing productivity application, but I don’t expect that to happen any time soon. In the meantime, if you have ideas for existing ways to execute FVP online, I’d love to hear about them!"
    },
    "voteCount": 47
  },
  {
    "_id": "Z9cbwuevS9cqaR96h",
    "url": null,
    "title": "CFAR Participant Handbook now available to all",
    "slug": "cfar-participant-handbook-now-available-to-all",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "Techniques"
      },
      {
        "name": "Internal Double Crux"
      },
      {
        "name": "Trigger-Action Planning"
      },
      {
        "name": "Community"
      },
      {
        "name": "Rationality"
      },
      {
        "name": "Bucket Errors"
      },
      {
        "name": "Center for Applied Rationality (CFAR)"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "[Google Drive PDF](https://drive.google.com/file/d/1UZYBtOJ3QZ7FTI_4eKjVzBSNUqC_Uba3/view)\n\nHey, guys—I wrote this, and CFAR has recently decided to make it publicly available. Much of it involved rewriting the original work of others, such as Anna Salamon, Kenzie Ashkie, Val Smith, Dan Keys, and other influential CFAR founders and staff, but the actual content was filtered through me as single author as part of getting everything into a consistent and coherent shape.\n\nI have mild intentions to update it in the future with a handful of other new chapters that were on the list, but which didn't get written before CFAR let me go. Note that such updates will likely not be current-CFAR-approved, but will still derive directly from my understanding of the curriculum as former Curriculum Director."
    },
    "voteCount": 88
  },
  {
    "_id": "Q8tyoaMFmW8R9w9db",
    "url": null,
    "title": "Formal vs. Effective Pre-Commitment",
    "slug": "formal-vs-effective-pre-commitment",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "Decision Theory"
      },
      {
        "name": "Pre-Commitment"
      },
      {
        "name": "Distinctions"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "[Newcomb's Problem](https://wiki.lesswrong.com/wiki/Newcomb%27s_problem) is effectively a problem about pre-commitment. Everyone agrees that if you have the opportunity to pre-commit in advance of Omega predicting you, then you ought to. The only question is what you ought to do if you either failed to do this or weren't given the opportunity to do this. LW-Style Decision Theories like [TDT](https://wiki.lesswrong.com/wiki/Timeless_decision_theory) or [UDT](https://wiki.lesswrong.com/wiki/Updateless_decision_theory) say that you should act as though you are pre-committed, while [Casual Decision Theory](https://wiki.lesswrong.com/wiki/Causal_Decision_Theory) says that it's too late.\n\n(I've written about Newcomb's Problem and pre-commitment [before](https://www.lesswrong.com/posts/YpdTSt4kRnuSkn63c/the-prediction-problem-a-variant-on-newcomb-s), but I don't feel that I quite did the topic justice. I'm also hugely in favour of short, definitive articles on specific points that can serve as a quick reference. So here we go:)\n\nFormal pre-commitments include things like rewriting your code, signing a legally binding contract or providing assets as security. If set up correctly, they ensure that a rational agent actually keeps their end of the bargain. Of course, an irrational agent may still break their end of the bargain.\n\nEffective pre-commitment describes any situation where an agent must (in the logical sense) necessarily perform an action in the future, even if there is no formal pre-commitment. If libertarian freewill were to exist, then no-one would ever be effectively pre-committed, but if the universe is deterministic, then we are effectively pre-committed to any choice that we make (quantum mechanics effectively pre-commits us to particular probability distributions, rather than individual choices, but for purposes of simplicity we will ignore this here and just assume straightforward determinism). This follows straight from the definition of determinism (more discussion about the philosophical consequences of determinism in a [previous post](https://www.lesswrong.com/posts/YpdTSt4kRnuSkn63c/the-prediction-problem-a-variant-on-newcomb-s)).\n\nOne reason why this concept seems so weird is that there's absolutely no need for an agent that's effectively pre-committed to know that it is pre-committed until the exact moment when it locks in its decision. From the agent's perspective, it magically turns out to be pre-committed to whatever action it chooses, however, the truth is that the agent was always pre-committed to this action, just without knowing.\n\nMuch of the confusion about pre-commitment is about whether we should be looking at formal or effective pre-commitment. Perfect predictors only care about effective pre-commitment; for them formalities are unnecessary and possibly misleading. However, human level agents tend to care much more about formal pre-commitments. Some people, like detectives or poker players, may be really good at reading people, but they're still nothing compared to a perfect predictor and most people aren't even this good. So in everyday life, we tend to care much more about formal pre-commitments when we want certainty.\n\nHowever, Newcomb's Problem explicitly specifies a perfect predictor, so we shouldn't be thinking about human level predictors. In fact, I'd say that some of the emphasise on formal pre-commitment comes from anthropomorphising perfect predictors. It's really hard for us to accept that anyone or anything could actually be that good and that there's no way to get ahead of it.\n\nIn closing, differentiating the two kinds of pre-commitment really clarifies these kinds of discussions. We may not be able to go back into the past and pre-commit to a certain cause of action, but we can take an action on the basis that it would be good if we had pre-committed to it and be assured that we will discover that we were actually pre-committed to it.\n\n(Happy to rename these concepts if anyone has better names)"
    },
    "voteCount": 9
  },
  {
    "_id": "5y45Kry6GtWCFePjm",
    "url": null,
    "title": "Hypotheticals: The Direct Application Fallacy",
    "slug": "hypotheticals-the-direct-application-fallacy",
    "author": "Chris_Leong",
    "question": false,
    "tags": [
      {
        "name": "Fallacies"
      },
      {
        "name": "World Modeling"
      },
      {
        "name": "Hypotheticals"
      }
    ],
    "tableOfContents": {
      "sections": [
        {
          "title": "Exploiting Opportunities for Learning",
          "anchor": "Exploiting_Opportunities_for_Learning",
          "level": 1
        },
        {
          "title": "Practise Exercises Don't Need to be Real",
          "anchor": "Practise_Exercises_Don_t_Need_to_be_Real",
          "level": 1
        },
        {
          "title": "Applying the Unrealistic to the Real",
          "anchor": "Applying_the_Unrealistic_to_the_Real",
          "level": 1
        },
        {
          "title": "Being Aware of Limitations",
          "anchor": "Being_Aware_of_Limitations",
          "level": 1
        },
        {
          "title": "Conclusion",
          "anchor": "Conclusion",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        },
        {
          "anchor": "comments",
          "level": 0,
          "title": "19 comments"
        }
      ],
      "headingsCount": 7
    },
    "contents": {
      "markdown": "A few years ago, I tried convincing people some commenters that hypotheticals were important even when they weren't realistic. That failed, but I think I've spend enough time reflecting to give this another go. This time, my focus will be on challenging the following common assumption:\n\n> The Direct Application Fallacy: If a hypothetical situation can't conceivably occur, then the hypothetical situation doesn't matter\n\nI chose this name because it assumes that the only purpose of discussing a hypothetical is to know what would happen or what we should do in such a situation. It ignores the other lessons that such a discussion may teach us and how it might have logical consequences for situations that actually do occur.\n\n(**Note:** This post was renamed from: Unrealistic Hypotheticals Still Contain Lessons)\n\n**Exploiting Opportunities for Learning**\n\nIn [The Least Convenient Possible World](https://www.lesswrong.com/posts/neQ7eXuaXpiYw7SBy/the-least-convenient-possible-world), Scott Alexander considers the classic objection to utilitarianism that it implies that a surgeon should be prepared to harvest the organs of a random traveller if it would allow them to save five other patients. Scott argues that pointing out that the random traveller's organs probably be genetic mismatches, while \"technically correct\", also \"completely misses the point and loses a valuable opportunity to examine the nature of morality\". He also notes that responding in this manner leaves too much \"wiggle room\". Even if we aren't consciously aware of it, we often construct arguments to avoid believing things that we don't want to, so we can improve our rationality by limiting our ability to avoid understanding the other person's perspective. While Scott is referring to people who completely miss the point of the hypothetical, I think that dismissing a hypothetical as unrealistic often also sacrifices opportunities for learning as we'll see below.\n\n**Practise Exercises Don't Need to be Real**\n\nImagine that you are an instructor setting problems for your students so that they can learn an area like economics, physics or applied maths. How strongly do you care about these exercises being realistic? I would argue that this isn't very important and that this further applies to philosophy:\n\n1\\. Simplification: Students may be at a point where a realistic exercise would be quite beyond their abilities. Imagine that you are trying to teach your students how to calculate falling objects. One student complains that you are ignoring air resistance. You try to explain that you can talk about air resistance after you've covered the basics, but they insist that any discussion without air resistance is utterly pointless. Eventually you concede, but most of the class ends up failing the quiz the next week because they weren't ready for the harder problems. Similarly, philosophical problems often assume \"no-one will ever know\" so that you can discuss moral principles without 90% of the time going into arguing about human psychology and sociology which had nothing to do with the point you were trying to illustrate.\n\n2\\. Testing for Understanding: Students are often assigned questions as a way to gauge their understanding of a concept. Maybe no object has zero mass, but if someone can't tell you that this should create no gravitational force, they must have a misunderstanding somewhere. Maybe you could ask about an object that weighs 0.01 grams instead, but then they'd have to pull out a calculator. Similarly, even if utility monsters don't exist, they provide a useful tool for clarifying utilitarianism, as it explains why, \"greatest good for the greatest number\" isn't a completely accurate characterisation. And indeed, there's no reason why some people or organisms mightn't generally experience more utility than others.\n\n3\\. Realism Trades off Against Other Factors: Perhaps, you could find simple exercises that are realistic or test for understanding with more realistic scenarios. However, your goal is to make your students learn and this is dependent on a whole host of factors. If you insist on questions always being realistic, then this trades off against other dimensions, such as engagement, memorability and time required to construct a situation. This last dimension is particularly important for conversations where people have to be able to construct these situations on the fly.\n\nThis is taken for granted when talking about maths and physics, but if you want to learn to deeply understand philosophy, you'll have to accept unrealistic practise questions as well.\n\n**Applying the Unrealistic to the Real**\n\nIn maths, it is very common to take the limit of a formula as some variable, like as x approaches infinity. This technique is very useful for approximations. For example, it's easier to consider the limit as x approaches infinity of (2x^2-x+10)/(x^2+79) than to substitute in a specific value like a million. This is applied constantly throughout programming with [Big-O Notation](https://rob-bell.net/2009/06/a-beginners-guide-to-big-o-notation/). Even though an infinite dataset is completely unrealistic, this heuristic is still incredibly useful for designing algorithms.\n\nSimilarly, when a utilitarian points out that strict versions of deontology will always allow us to construct situations where following the rules cost us infinite utility, the unrealism of the situation doesn't make it irrelevant. Just with Big-O Notation, step 2 of the argument could very well be to scale it down to a more realistic situation. Unfortunately, many people will assume that step 2 isn't coming and judge the argument as flawed at this stage. They may even interrupt the speaker with the objection that the argument isn't realistic. This often negatively affects the conversation, as it pushes the speaker to address step 2, before they've had the opportunity to ensure that everyone has understood step 1.\n\n**Being Aware of Limitations**\n\nConsider the formula y=10/(x\\[x-5\\]). This has two discontinuities at x=0 and x=5. I really want a more practical example, so if you have one, please list in the comment, but let's pretend x represents the number of people and we know there'll always be at least one person in practise. So someone could easily wave away the discontinuity with the x=0 case and completely miss the second one. But if rationality is winning, this isn't it. If they instead looked into it, they'd have realised that more general issue is the division by zero and not overlooked this issue. Yet it is very easy for the person getting it wrong to convince themselves that it is the person trying to figure out the situation with x=0 who is irrational.\n\nLet's suppose that someone is promoting deontology and they aren't worried about theoretical situations. They just want a practical model or heuristic to help them act morality. If they are proposing a heuristic, they should fully expect it to have limitations and situations where it just completely breaks. And it would probably be useful to know what these limitations are. Some of these limitations mightn't be obvious and the heuristic may even be broken if some of these occur more often than they expect. Discussions of how the model behaves as the utility cost of a principle approaches infinity shouldn't be met by dismissal, but by either biting the bullet or acknowledging that the model seems to break down in those circumstances. It can still be defended as a heuristic or you can assert this kind of situation tends to break our intuitions (see [epistemic learned helplessness](http://archive.is/NIGNx)), but either way it needs to be acknowledged as a limitation that can be weighed up against other limitations. After all, there could be a better model that has a solution to these issues.\n\n**Conclusion**\n\nOne of the key threads of this post has been to not assume that you know where an argument is going. Just because someone is talking about an unrealistic situation, it doesn't follow that they aren't going to tie it back to reality. Further, you shouldn't assume that there's a single path for this to occur. At the very least, I would suggest replacing \"This is unrealistic\" with \"How are you going to tie it back to reality?\". The second question is far superior, as it doesn't make the unwarranted assumption that the only purpose of constructing a model is to attempt to directly apply it to reality."
    },
    "voteCount": 14
  },
  {
    "_id": "brXr7PJ2W4Na2EW2q",
    "url": null,
    "title": "The Commitment Races problem",
    "slug": "the-commitment-races-problem",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "AI"
      },
      {
        "name": "Game Theory"
      }
    ],
    "tableOfContents": {
      "sections": [
        {
          "title": "Consequentialists can get caught in commitment races, in which they want to make commitments as soon as possible",
          "anchor": "Consequentialists_can_get_caught_in_commitment_races__in_which_they_want_to_make_commitments_as_soon_as_possible",
          "level": 1
        },
        {
          "title": "When consequentialists make commitments too soon, disastrous outcomes can sometimes result. The situation we are in may be one of these times.",
          "anchor": "When_consequentialists_make_commitments_too_soon__disastrous_outcomes_can_sometimes_result__The_situation_we_are_in_may_be_one_of_these_times_",
          "level": 1
        },
        {
          "title": "Conclusion",
          "anchor": "Conclusion",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        },
        {
          "anchor": "comments",
          "level": 0,
          "title": "39 comments"
        }
      ],
      "headingsCount": 5
    },
    "contents": {
      "markdown": "  \n\n_\\[Epistemic status: Strong claims vaguely stated and weakly held. I expect that writing this and digesting feedback on it will lead to a much better version in the future. EDIT: So far this has stood the test of time. EDIT: As of September 2020 I think this is one of the most important things to be thinking about.\\]_\n\nThis post attempts to generalize and articulate a problem that people have been [thinking about](https://www.alignmentforum.org/posts/9sYzoRnmqmxZm4Whf/conceptual-problems-with-udt-and-policy-selection) [since at least 2016](https://www.alignmentforum.org/posts/5bd75cc58225bf067037528e/updatelessness-and-son-of-x#5bd75cc58225bf0670375293). \\[Edit: [2009 in fact!](https://www.lesswrong.com/posts/szfxvS8nsxTgJLBHs/ingredients-of-timeless-decision-theory?&_ga=2.19622960.942795800.1566493298-1283608698.1558793463#J3M2dqG8WZ4LQh8p3)\\] In short, here is the problem:\n\n_Consequentialists_ _can get caught in commitment races, in which they want to make commitments as soon as possible. When consequentialists make commitments too soon, disastrous outcomes can sometimes result. The situation we are in (building AGI and letting it self-modify) may be one of these times unless we think carefully about this problem and how to avoid it._\n\nFor this post I use \"consequentialists\" to mean agents that choose actions entirely on the basis of the expected consequences of those actions. For my purposes, this means they don't care about historical facts such as whether the options and consequences available now are the result of malicious past behavior. (I am trying to avoid trivial definitions of consequentialism according to which everyone is a consequentialist because e.g. \"obeying the moral law\" is a consequence.) This definition is somewhat fuzzy and I look forward to searching for more precision some other day.\n\nConsequentialists can get caught in commitment races, in which they want to make commitments as soon as possible\n----------------------------------------------------------------------------------------------------------------\n\nConsequentialists are bullies; a consequentialist will happily threaten someone insofar as they think the victim might capitulate and won't retaliate.\n\nConsequentialists are also cowards; they conform their behavior to the incentives set up by others, regardless of the history of those incentives. For example, they predictably give in to credible threats unless reputational effects weigh heavily enough in their minds to prevent this.\n\nIn most ordinary circumstances the stakes are sufficiently low that reputational effects dominate: Even a consequentialist agent won't give up their lunch money to a schoolyard bully if they think it will invite much more bullying later. But in some cases the stakes are high enough, or the reputational effects low enough, for this not to matter.\n\nSo, amongst consequentialists, there is sometimes a huge advantage to \"winning the commitment race.\" If two consequentialists are playing a game of [Chicken](https://en.wikipedia.org/wiki/Chicken_(game)), the first one to throw out their steering wheel wins. If one consequentialist is in position to seriously hurt another, it can extract concessions from the second by credibly threatening to do so--unless the would-be victim credibly commits to not give in first! If two consequentialists are attempting to divide up a pie or [select a game-theoretic equilibrium to play in](https://en.wikipedia.org/wiki/Folk_theorem_(game_theory)), the one that can \"move first\" can get much more than the one that \"moves second.\" In general, because consequentialists are cowards and bullies, the consequentialist who makes commitments first will predictably be able to massively control the behavior of the consequentialist who makes commitments later. As the [folk theorem](https://en.wikipedia.org/wiki/Folk_theorem_(game_theory)) shows, this can even be true in cases where games are iterated and reputational effects are significant.\n\n_Note:_ \"first\" and \"later\" in the above don't refer to clock time, though clock time is a helpful metaphor for imagining what is going on. Really, what's going on is that agents learn about each other, each on their own subjective timeline, while also making choices (including the choice to commit to things) and the choices a consequentialist makes at subjective time _t_ are cravenly submissive to the commitments they've learned about by _t_.\n\nLogical updatelessness and acausal bargaining combine to create a particularly important example of a dangerous commitment race. There are [strong incentives](https://www.alignmentforum.org/posts/5bd75cc58225bf067037528e/updatelessness-and-son-of-x) for consequentialist agents to self-modify to become updateless as soon as possible, and going updateless is like making a bunch of commitments all at once. Since real agents can't be logically omniscient, one needs to decide how much time to spend thinking about things like game theory and what the outputs of various programs are before making commitments. When we add acausal bargaining into the mix, things get even more intense. [Scott Garrabrant, Wei Dai](https://www.alignmentforum.org/posts/5bd75cc58225bf067037528e/updatelessness-and-son-of-x#5bd75cc58225bf0670375293), and [Abram Demski](https://www.alignmentforum.org/posts/9sYzoRnmqmxZm4Whf/conceptual-problems-with-udt-and-policy-selection) have described this problem already, so I won't say more about that here. Basically, in this context, there are many other people observing your thoughts and making decisions on that basis. So bluffing is impossible and there is constant pressure to make commitments quickly before thinking longer. (That's my take on it anyway)\n\n> _Anecdote:_ Playing a board game last week, my friend Lukas said (paraphrase) \"I commit to making you lose if you do that move.\" In rationalist gaming circles this sort of thing is normal and fun. But I suspect his gambit would be considered unsportsmanlike--and possibly outright bullying--by most people around the world, and my compliance would be considered cowardly. (To be clear, I didn't comply. Practice what you preach!)\n\nWhen consequentialists make commitments too soon, disastrous outcomes can sometimes result. The situation we are in may be one of these times.\n----------------------------------------------------------------------------------------------------------------------------------------------\n\nThis situation is already ridiculous: There is something very silly about two supposedly rational agents racing to limit their own options before the other one limits theirs. But it gets worse.\n\nSometimes commitments can be made \"at the same time\"--i.e. in ignorance of each other--in such a way that they lock in an outcome that is disastrous for everyone. (Think both players in Chicken throwing out their steering wheels simultaneously.)\n\nHere is a somewhat concrete example: Two consequentialist AGI think for a little while about game theory and commitment races and then self-modify to resist and heavily punish anyone who bullies them. Alas, they had slightly different ideas about what counts as bullying and what counts as a reasonable request--perhaps one thinks that demanding more than the [Nash Bargaining Solution](https://en.wikipedia.org/wiki/Bargaining_problem#Bargaining_solutions) is bullying, and the other thinks that demanding more than the [Kalai-Smorodinsky Bargaining Solution](https://en.wikipedia.org/wiki/Bargaining_problem#Bargaining_solutions) is bullying--so many years later they meet each other, learn about each other, and end up locked into all-out war.\n\nI'm not saying disastrous AGI commitments are the default outcome; I'm saying the stakes are high enough that we should put a lot more thought into preventing them than we have so far. It would really suck if we create a value-aligned AGI that ends up getting into all sorts of fights across the multiverse with other value systems. We'd wish we built a paperclip maximizer instead.\n\n_Objection:_ \"Surely they wouldn't be so stupid as to make _those_ commitments--even _I_ could see that bad outcome coming. A better commitment would be...\"\n\n_Reply:_ The problem is that consequentialist agents are motivated to make commitments as soon as possible, since that way they can influence the behavior of other consequentialist agents who may be learning about them. Of course, they will balance these motivations against the countervailing motive to learn more and think more before doing drastic things. The problem is that the first motivation will push them to make commitments much sooner than would otherwise be optimal. So they might not be as smart as us when they make their commitments, at least not in all the relevant ways. Even if our baby AGIs are wiser than us, they might still make mistakes that we haven't anticipated yet. The situation is like [the centipede game](https://en.wikipedia.org/wiki/Centipede_game): Collectively, consequentialist agents benefit from learning more about the world and each other before committing to things. But because they are all bullies and cowards, they individually benefit from committing earlier, when they don't know so much.\n\n_Objection:_ \"Threats, submission to threats, and costly fights are rather rare in human society today. Why not expect this to hold in the future, for AGI, as well?\"\n\n_Reply:_ Several points:\n\n1\\. Devastating commitments (e.g. \"[Grim Trigger](https://en.wikipedia.org/wiki/Grim_trigger)\") are much more possible with AGI--just alter the code! [Inigo Montoya](https://en.wikipedia.org/wiki/Inigo_Montoya) is a fictional character and even he wasn't able to summon lifelong commitment on a whim; it had to be triggered by the brutal murder of his father.\n\n2\\. Credibility is much easier also, especially in an acausal context (see above.)\n\n3\\. Some AGI bullies may be harder to retaliate against than humans, lowering their disincentive to make threats.\n\n4\\. AGI may not have sufficiently strong reputation effects in the sense relevant to consequentialists, partly because threats can be made more devastating (see above) and partly because they may not believe they exist in a population of other powerful agents who will bully them if they show weakness.\n\n5\\. Finally, these terrible things (Brutal threats, costly fights) do happen to some extent even among humans today--especially in situations of anarchy. We want the AGI we built to be _less_ likely to do that stuff than humans, not merely _as_ likely.\n\n_Objection:_ \"Any AGI that falls for this commit-now-before-the-others-do argument will also fall for many other silly do-X-now-before-it's-too-late arguments, and thus will be incapable of hurting anyone.\"\n\n_Reply:_ That would be nice, wouldn't it? Let's hope so, but not count on it. Indeed perhaps we should look into whether there are other arguments of this form that we should worry about our AI falling for...\n\n> _Anecdote:_ A friend of mine, when she was a toddler, would threaten her parents: \"I'll hold my breath until you give me the candy!\" Imagine how badly things would have gone if she was physically capable of making arbitrary credible commitments. Meanwhile, a few years ago when I first learned about the concept of updatelessness, I resolved to be updateless from that point onwards. I am now glad that I couldn't actually commit to anything then.\n\nConclusion\n----------\n\nOverall, I'm not certain that this is a big problem. But it feels to me that it might be, especially if acausal trade turns out to be a real thing. I would not be surprised if \"solving bargaining\" turns out to be even more important than value alignment, because the stakes are so high. I look forward to a better understanding of this problem.\n\n_Many thanks to Abram Demski, Wei Dai, John Wentworth, and Romeo Stevens for helpful conversations._"
    },
    "voteCount": 52
  },
  {
    "_id": "2THFt7BChfCgwYDeA",
    "url": null,
    "title": "Let's Discuss Functional Decision Theory",
    "slug": "let-s-discuss-functional-decision-theory",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "Academic Papers"
      },
      {
        "name": "Decision Theory"
      },
      {
        "name": "Functional Decision Theory"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "I've just finished reading through [Functional Decision Theory: A New Theory of Rationality](https://arxiv.org/pdf/1710.05060.pdf), but there are some rather basic questions that are left unanswered since it focused on comparing it to Casual Decision Theory and Evidential Decision Theory:\n\n*   How is Functional Decision Theory different from Timeless Decision Theory? All I can gather is that FDT intervenes on the mathematical function, rather than on the agent. What problems does it solve that TDT can't? (Apparently it solves Mechanical Blackmail with an imperfect predictor and so it should also be able to solve Counterfactual Mugging?)\n*   How is it different from [Updateless decision theory](https://wiki.lesswrong.com/wiki/Updateless_decision_theory)? What's the simplest problem in which they give different results?\n*   Functional Decision Theory seems to require counterpossibilities, where we imagine that a function output a result that is different from what it outputs. It further says that this is a problem that isn't yet solved. What approaches have been tried so far? Further, what are some key problems within this space?"
    },
    "voteCount": 14
  },
  {
    "_id": "de3xjFaACCAk6imzv",
    "url": null,
    "title": "Towards a New Decision Theory",
    "slug": "towards-a-new-decision-theory",
    "author": "Wei_Dai",
    "question": false,
    "tags": [
      {
        "name": "Decision Theory"
      },
      {
        "name": "Updateless Decision Theory"
      }
    ],
    "tableOfContents": {
      "sections": [
        {
          "title": "Example 1: Counterfactual Mugging",
          "anchor": "Example_1__Counterfactual_Mugging",
          "level": 1
        },
        {
          "title": "Example 2: Return of Bayes",
          "anchor": "Example_2__Return_of_Bayes",
          "level": 1
        },
        {
          "title": "Example 3: Level IV Multiverse",
          "anchor": "Example_3__Level_IV_Multiverse",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        },
        {
          "anchor": "comments",
          "level": 0,
          "title": "146 comments"
        }
      ],
      "headingsCount": 5
    },
    "contents": {
      "markdown": "It commonly acknowledged here that current decision theories have deficiencies that show up in the form of various paradoxes. Since there seems to be little hope that Eliezer will publish his [Timeless Decision Theory](/lw/135/timeless_decision_theory_problems_i_cant_solve/) any time soon, I decided to try to synthesize some of the ideas discussed in this forum, along with a few of my own, into a coherent alternative that is hopefully not so paradox-prone.\n\nI'll start with a way of framing the question. Put yourself in the place of an AI, or more specifically, the decision algorithm of an AI. You have access to your own source code S, plus a bit string X representing all of your memories and sensory data. You have to choose an output string Y. That’s the decision. The question is, how? (The answer isn't “Run S,” because what we want to know is what S should be in the first place.)\n\nLet’s proceed by asking the question, “What are the consequences of S, on input X, returning Y as the output, instead of Z?” To begin with, we'll consider just the consequences of that choice in the realm of abstract computations (i.e. computations considered as mathematical objects rather than as implemented in physical systems). The most immediate consequence is that any program that calls S as a subroutine with X as input, will receive Y as output, instead of Z. What happens next is a bit harder to tell, but supposing that you know something about a program P that call S as a subroutine, you can further deduce the effects of choosing Y versus Z by tracing the difference between the two choices in P’s subsequent execution. We could call these the computational consequences of Y. Suppose you have preferences about the execution of a set of programs, some of which call S as a subroutine, then you can satisfy your preferences directly by choosing the output of S so that those programs will run the way you most prefer.\n\nA more general class of consequences might be called logical consequences. Consider a program P’ that doesn’t call S, but a different subroutine S’ that’s logically equivalent to S. In other words, S’ always produces the same output as S when given the same input. Due to the logical relationship between S and S’, your choice of output for S must also affect the subsequent execution of P’. Another example of a logical relationship is an S' which always returns the first bit of the output of S when given the same input, or one that returns the same output as S on some subset of inputs.\n\nIn general, you can’t be certain about the consequences of a choice, because you’re not logically omniscient. How to handle logical/mathematical uncertainty is an [open problem](/lw/f9/a_request_for_open_problems/bun), so for now we'll just assume that you have access to a \"mathematical intuition subroutine\" that somehow allows you to form beliefs about the likely consequences of your choices.\n\nAt this point, you might ask, “That’s well and good, but what if my preferences extend beyond abstract computations? What about consequences on the physical universe?” The answer is, we can view the physical universe as a program that runs S as a subroutine, or more generally, view it as a mathematical object which has S embedded within it. (From now on I’ll just refer to programs for simplicity, with the understanding that the subsequent discussion can be generalized to non-computable universes.) Your preferences about the physical universe can be translated into preferences about such a program P and programmed into the AI. The AI, upon receiving an input X, will look into P, determine all the instances where it calls S with input X, and choose the output that optimizes its preferences about the execution of P. If the preferences were translated faithfully, the the AI's decision should also optimize your preferences regarding the physical universe. This faithful translation is a second major open problem.\n\nWhat if you have some uncertainty about which program our universe corresponds to? In that case, we have to specify preferences for the entire set of programs that our universe may correspond to. If your preferences for what happens in one such program is independent of what happens in another, then we can represent them by a probability distribution on the set of programs plus a utility function on the execution of each individual program. More generally, we can always represent your preferences as a utility function on vectors of the form <E1, E2, E3, …> where E1 is an execution history of P1, E2 is an execution history of P2, and so on.\n\nThese considerations lead to the following design for the decision algorithm S. S is coded with a vector <P1, P2, P3, ...> of programs that it cares about, and a utility function on vectors of the form <E1, E2, E3, …> that defines its preferences on how those programs should run. When it receives an input X, it looks inside the programs P1, P2, P3, ..., and uses its \"mathematical intuition\" to form a probability distribution P\\_Y over the set of vectors <E1, E2, E3, …> for each choice of output string Y. Finally, it outputs a string Y* that maximizes the expected utility Sum P\\_Y(<E1, E2, E3, …>) U(<E1, E2, E3, …>). (This specifically assumes that expected utility maximization is the right way to deal with mathematical uncertainty. Consider it a temporary placeholder until that problem is solved. Also, I'm describing the algorithm as a brute force search for simplicity. In reality, you'd probably want it to do something cleverer to find the optimal Y* more quickly.)\n\n#### Example 1: Counterfactual Mugging\n\nNote that Bayesian updating is not done explicitly in this decision theory. When the decision algorithm receives input X, it may determine that a subset of programs it has preferences about never calls it with X and are also logically independent of its output, and therefore it can safely ignore them when computing the consequences of a choice. There is no need to set the probabilities of those programs to 0 and renormalize.\n\nSo, with that in mind, we can model [Counterfactual Mugging](http://wiki.lesswrong.com/wiki/Counterfactual_mugging) by the following Python program:\n\ndef P(coin):  \n    AI_balance = 100  \n    if coin == \"heads\":  \n        if S(\"heads\") == \"give $100\":  \n            AI_balance -= 100  \n    if coin == \"tails\":  \n        if Omega_Predict(S, \"heads\") == \"give $100\":  \n            AI_balance += 10000\n\nThe AI’s goal is to maximize expected utility = .5 * U(AI\\_balance after P(\"heads\")) + .5 * U(AI\\_balance after P(\"tails\")). Assuming U(AI\\_balance)=AI\\_balance, it’s easy to determine U(AI\\_balance after P(\"heads\")) as a function of S’s output. It equals 0 if S(“heads”) == “give $100”, and 100 otherwise. To compute U(AI\\_balance after P(\"tails\")), the AI needs to look inside the Omega\\_Predict function (not shown here), and try to figure out how accurate it is. Assuming the mathematical intuition module says that choosing “give $100” as the output for S(“heads”) makes it more likely (by a sufficiently large margin) for Omega\\_Predict(S, \"heads\") to output “give $100”, then that choice maximizes expected utility.\n\n#### Example 2: Return of Bayes  \n\nThis example is based on case 1 in Eliezer's post [Priors as Mathematical Objects](/lw/hk/priors_as_mathematical_objects/). An urn contains 5 red balls and 5 white balls. The AI is asked to predict the probability of each ball being red as it as drawn from the urn, its goal being to maximize the expected logarithmic score of its predictions. The main point of this example is that this decision theory can reproduce the effect of Bayesian reasoning when the situation calls for it. We can model the scenario using preferences on the following Python program:\n\ndef P(n):  \n    urn = \\['red', 'red', 'red', 'red', 'red', 'white', 'white', 'white', 'white', 'white'\\]  \n    history = \\[\\]  \n    score = 0  \n    while urn:  \n        i = n%len(urn)  \n        n = n/len(urn)  \n        ball = urn\\[i\\]  \n        urn\\[i:i+1\\] = \\[\\]  \n        prediction = S(history)  \n        if ball == 'red':  \n            score += math.log(prediction, 2)  \n        else:  \n            score += math.log(1-prediction, 2)  \n        print (score, ball, prediction)  \n        history.append(ball)\n\nHere is a printout from a sample run, using n=1222222:\n\n-1.0 red 0.5  \n-2.16992500144 red 0.444444444444  \n-2.84799690655 white 0.375  \n-3.65535182861 white 0.428571428571  \n-4.65535182861 red 0.5  \n-5.9772799235 red 0.4  \n-7.9772799235 red 0.25  \n-7.9772799235 white 0.0  \n-7.9772799235 white 0.0  \n-7.9772799235 white 0.0\n\nS should use deductive reasoning to conclude that returning (number of red balls remaining / total balls remaining) maximizes the average score across the range of possible inputs to P, from n=1 to 10! (representing the possible orders in which the balls are drawn), and do that. Alternatively, S can approximate the correct predictions using brute force: generate a random function from histories to predictions, and compute what the average score would be if it were to implement that function. Repeat this a large number of times and it is likely to find a function that returns values close to the optimum predictions.\n\n#### Example 3: Level IV Multiverse\n\nIn Tegmark's Level 4 [Multiverse](http://space.mit.edu/home/tegmark/multiverse.html), all structures that exist mathematically also exist physically. In this case, we'd need to program the AI with preferences over all mathematical structures, perhaps represented by an ordering or utility function over conjunctions of well-formed sentences in a formal set theory. The AI will then proceed to \"optimize\" all of mathematics, or at least the parts of math that (A) are logically dependent on its decisions and (B) it can reason or form intuitions about.\n\nI suggest that the Level 4 Multiverse should be considered the default setting for a general decision theory, since we cannot rule out the possibility that all mathematical structures do indeed exist physically, or that we have direct preferences on mathematical structures (in which case there is no need for them to exist \"physically\"). Clearly, application of decision theory to the Level 4 Multiverse requires that the previously mentioned open problems be solved in their most general forms: how to handle logical uncertainty in any mathematical domain, and how to map fuzzy human preferences to well-defined preferences over the structures of mathematical objects.\n\n**Added:** For further information and additional posts on this decision theory idea, which came to be called \"Updateless Decision Theory\", please see [its entry](https://wiki.lesswrong.com/wiki/Updateless_decision_theory) in the LessWrong Wiki."
    },
    "voteCount": 60
  },
  {
    "_id": "QPhY8Nb7gtT5wvoPH",
    "url": null,
    "title": "Comparison of decision theories (with a focus on logical-counterfactual decision theories)",
    "slug": "comparison-of-decision-theories-with-a-focus-on-logical",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "Decision Theory"
      }
    ],
    "tableOfContents": {
      "sections": [
        {
          "title": "Introduction",
          "anchor": "Introduction",
          "level": 1
        },
        {
          "title": "Summary",
          "anchor": "Summary",
          "level": 2
        },
        {
          "title": "Value-added",
          "anchor": "Value_added",
          "level": 2
        },
        {
          "title": "Audience",
          "anchor": "Audience",
          "level": 2
        },
        {
          "title": "Comparison dimensions",
          "anchor": "Comparison_dimensions",
          "level": 1
        },
        {
          "title": "Outermost iteration",
          "anchor": "Outermost_iteration",
          "level": 2
        },
        {
          "title": "Updatelessness",
          "anchor": "Updatelessness",
          "level": 2
        },
        {
          "title": "Type of counterfactual",
          "anchor": "Type_of_counterfactual",
          "level": 2
        },
        {
          "title": "Other dimensions that I ignore",
          "anchor": "Other_dimensions_that_I_ignore",
          "level": 2
        },
        {
          "title": "Comparison table along the given dimensions",
          "anchor": "Comparison_table_along_the_given_dimensions",
          "level": 1
        },
        {
          "title": "Explanations of each decision theory",
          "anchor": "Explanations_of_each_decision_theory",
          "level": 1
        },
        {
          "title": "UDT1 and FDT (iterate over actions)",
          "anchor": "UDT1_and_FDT__iterate_over_actions_",
          "level": 2
        },
        {
          "title": "UDT1.1 and FDT (iterate over policies)",
          "anchor": "UDT1_1_and_FDT__iterate_over_policies_",
          "level": 2
        },
        {
          "title": "TDT",
          "anchor": "TDT",
          "level": 2
        },
        {
          "title": "UDT2",
          "anchor": "UDT2",
          "level": 2
        },
        {
          "title": "LDT",
          "anchor": "LDT",
          "level": 2
        },
        {
          "title": "CDT",
          "anchor": "CDT",
          "level": 2
        },
        {
          "title": "EDT",
          "anchor": "EDT",
          "level": 2
        },
        {
          "title": "Comparison on specific decision problems",
          "anchor": "Comparison_on_specific_decision_problems",
          "level": 1
        },
        {
          "title": "Other comparisons",
          "anchor": "Other_comparisons",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        },
        {
          "anchor": "comments",
          "level": 0,
          "title": "11 comments"
        }
      ],
      "headingsCount": 22
    },
    "contents": {
      "markdown": "# Introduction\n\n## Summary\n\nThis post is a comparison of various existing decision theories, with a focus\non decision theories that use logical counterfactuals (a.k.a. the kind of decision theories most discussed on LessWrong). The post compares the\ndecision theories along outermost iteration (action vs policy vs algorithm),\nupdatelessness (updateless or updateful), and type of counterfactual used\n(causal, conditional, logical). It then explains the decision theories in more\ndetail, in particular giving an expected utility formula for each. The post then gives\nexamples of specific existing decision problems where the decision theories\ngive different answers.\n\n## Value-added\n\nThere are some other comparisons of decision theories (see the “Other comparisons” section),\nbut they either (1)\ndon’t focus on logical-counterfactual decision theories; or (2) are outdated\n(written before the new functional/logical decision theory terminology came about).\n\nTo give a more personal motivation, after reading through a bunch of papers and posts about these\ndecision theories, and feeling like I understood the basic ideas, I\nremained highly confused about basic things like “How is UDT different from\nFDT?”, “Why was TDT deprecated?”, and “If TDT performs worse than FDT, then\nwhat’s one decision problem where they give different outputs?”\nThis post hopes to clarify these and other questions.\n\nNone of the decision theory material in this post is novel. I am still learning\nthe basics myself, and I would appreciate any corrections (even about subtle/nitpicky stuff).\n\n## Audience\n\nThis post is intended for [people](https://intelligence.org/2017/10/22/fdt/#comment-3581536881) [who](https://www.greaterwrong.com/posts/cAMhvPgMQJzhrpNdN/publication-of-anthropic-decision-theory/comment/zz7GRTsvG8gD8EDxH) [are](https://www.greaterwrong.com/posts/9BYo6Q9qBMXWLjqPS/miri-decisions-are-for-making-bad-outcomes-inconsistent/comment/i2xAQFjo2odYW2uXz) [similarly](https://www.greaterwrong.com/posts/xJRPWBfJQcbWaoCLp/making-equilibrium-cdt-into-fdt-in-one-easy-step/comment/n7auEbvrH5DoC6Tvt) [confused](https://www.lesswrong.com/posts/2THFt7BChfCgwYDeA/let-s-discuss-functional-decision-theory) [about](http://nostalgebraist.tumblr.com/post/168759631049/the-new-decision-theory-paper-exists-because-the) the\ndifferences between TDT, UDT, FDT, and LDT. In terms of reader background assumed,\nit would be good to know the statements to some standard decision theory\nproblems (Newcomb’s problem, smoking lesion, Parfit’s hitchhiker, transparent\nbox Newcomb’s problem, counterfactual mugging (a.k.a. [curious benefactor](https://philpapers.org/archive/BARWDT-3.pdf); see page 56, footnote 89)) and the “correct” answers to\nthem, and having enough background in math to understand the expected utility\nformulas.\n\nIf you don’t have the background, I would recommend reading\nchapters 5 and 6 of\nGary Drescher’s [*Good and Real*](https://www.gwern.net/docs/statistics/decision/2006-drescher-goodandreal.pdf) (explains well the idea of subjunctive means–end relations),\nthe [FDT paper](https://arxiv.org/pdf/1710.05060.pdf) (explains well how FDT’s action selection variant works, and how FDT differs from CDT and EDT),\n[“Cheating Death in Damascus”](https://intelligence.org/files/DeathInDamascus.pdf), and\n[“Toward Idealized Decision Theory”](https://arxiv.org/pdf/1507.01986.pdf) (explains the difference between policy selection and logical counterfactuals well), and understanding\nwhat Wei Dai calls “decision theoretic thinking” (see comments:\n[1](https://www.lesswrong.com/posts/g8xh9R7RaNitKtkaa/explicit-optimization-of-global-strategy-fixing-a-bug-in#FMvBDD7fNQSd3B3qd),\n[2](https://www.lesswrong.com/posts/gkAecqbuPw4iggiub/common-mistakes-people-make-when-thinking-about-decision#uXrbGTefqvek9kkzf),\n[3](https://www.lesswrong.com/posts/GfHdNfqxe3cSCfpHL/the-absent-minded-driver#Hrtu69eAh86KFNgBC)).\nI think a lot of (especially old) content on decision theory is confusingly\nwritten or unfriendly to beginners, and would recommend skipping around to find\nexplanations that “click”.\n\n# Comparison dimensions\n\nMy main motivation is to try to distinguish between TDT, UDT, and FDT, so I\nfocus on three dimensions for comparison that I think best display the differences\nbetween these decision theories.\n\n## Outermost iteration\n\nAll of the decision theories in this post iterate through some set of “options” (intentionally vague)\nat the outermost layer of execution to find the best “option”. However, the\nnature (type) of these “options” differs among the various theories.\nMost decision theories iterate through either *actions* or *policies*.\nWhen a decision theory iterates through actions (to find the best action),\nit is doing “action selection”, and the decision theory outputs a single action.\nWhen a decision theory iterates through policies (to find the best policy),\nit is doing “policy selection”, and outputs a single *policy*, which is an\nobservation-to-action mapping. To get an action out of a decision theory\nthat does policy selection (because what we really care about is knowing which\naction to take), we must *call* the policy on the actual observation.\n\nUsing the notation of the [FDT paper](https://arxiv.org/pdf/1710.05060.pdf),\nan action has type $\\mathcal A$ while a policy has type $\\mathcal X \\to \\mathcal A$,\nwhere $\\mathcal X$ is the set of observations.\nSo given a policy $\\pi : \\mathcal X \\to \\mathcal A$ and observation $x \\in \\mathcal X$,\nwe get the action by calling $\\pi$ on $x$, i.e. $\\pi(x) \\in \\mathcal A$.\n\nFrom the expected utility formula of the decision theory,\nyou can tell action vs policy selection by seeing what variable comes beneath\nthe $\\operatorname{arg\\,max}$ operator (the $\\operatorname{arg\\,max}$ operator is what does the outermost iteration); if it is $a\\in\\mathcal A$ (or similar) then it is iterating over actions, and if it is $\\pi \\in \\Pi$ (or similar), then it is iterating over policies.\n\nOne exception to the above is UDT2, which seems to iterate over *algorithms*.\n\n## Updatelessness\n\nIn some decision problems, the agent makes an observation, and has the choice\nof updating on this observation before acting. Two examples of this are: in\ncounterfactual mugging (a.k.a. curious benefactor), where the agent makes the observation that the coin has\ncome up tails; and in the transparent box Newcomb’s problem, where the agent\nsees whether the big box is full or empty.\n\nIf the decision algorithm updates on the observation, it is *updateful* (a.k.a.\n“not updateless”). If it doesn’t update on the observation, it is\n*updateless*.\n\nThis idea is similar to how in Rawls’s “[veil of ignorance](https://en.wikipedia.org/wiki/Veil_of_ignorance)”, you must pick your\nmoral principles, societal policies, etc., before you find out who\nyou are in the world or as if you don’t know who you are in the world.\n\nHow can you tell if a decision theory is\nupdateless? In its expected utility formula, if it conditions\non the observation, it is updateful. In this case\nthe probability factor looks like\n$P(\\ldots\\mid \\ldots, \\mathrm{O\\small BS} = x)$,\nwhere $x$ is the observation (sometimes the observation is called “sense data” and is denoted by $s$).\nIf a decision theory is updateless, the conditioning on\n“$\\mathrm{O\\small BS} = x$” is absent.\nUpdatelessness only makes a difference in decision problems that\nhave observations.\n\nThere seem to be different meanings of “updateless” in use. In this post I will\nuse the above meaning. (I will try to post a question on LessWrong soon about these\ndifferent meanings.)\n\n## Type of counterfactual\n\nIn the course of reasoning about a decision problem, the agent\ncan construct counterfactuals or hypotheticals like “if I do *this*,\nthen *that* happens”. There are several different kinds of counterfactuals,\nand decision theories are divided among them.\n\nThe three types of counterfactuals that will concern us are: causal, conditional/evidential, and logical/subjunctive.\nThe distinctions between these are explained clearly in the [FDT paper](https://arxiv.org/pdf/1710.05060.pdf)\nso I recommend reading that (and I won’t explain them here).\n\nIn the expected utility formula, if the probability factor\nlooks like\n$P(\\ldots\\mid\\ldots, \\mathrm{A \\small CT}=a)$ then it is evidential, and\nif it looks like $P(\\ldots \\mid \\ldots, \\mathtt{do}(\\mathrm{A\\small CT}=a))$ then it is causal.\nI have seen the logical counterfactual written in many ways:\n\n* $P(\\ldots \\mid\\ldots, \\mathtt{do}(\\mathrm{\\small DT}(\\ldots)=\\ldots))$ e.g. in the [FDT paper](https://arxiv.org/pdf/1710.05060.pdf), p. 14\n* $P(\\ldots \\mid\\ldots, \\mathtt{true}(\\mathrm{\\small DT}(\\ldots)=\\ldots))$ e.g. in the [FDT paper](https://arxiv.org/pdf/1710.05060.pdf), p. 14\n* $P(\\ulcorner \\mathrm{\\small DT}(\\ldots)=\\ldots\\urcorner \\mathbin{\\Box\\kern-7mu\\rightarrow} \\ldots \\mid\\ldots)$ e.g. in [Hintze](https://intelligence.org/wp-content/uploads/2014/10/Hintze-Problem-Class-Dominance-In-Predictive-Dilemmas.pdf), p. 4\n* $P(\\ulcorner \\mathrm{\\small DT}(\\ldots)=\\ldots\\urcorner \\triangleright \\ldots \\mid\\ldots)$ e.g. on [Arbital](https://arbital.com/p/logical_dt/?l=5d6)\n\n## Other dimensions that I ignore\n\nThere are many more dimensions along which decision theories differ, but I\ndon’t understand these and they seem less relevant for comparing among the main\nlogical-counterfactual decision theories, so I will just list them here but\nwon’t go into them much later on in the post:\n\n* Reflective consistency (in particular [dynamic consistency](https://intelligence.org/files/TDT.pdf)): I think this is about whether an agent\n  would use precommitment mechanisms or self-modify to use a\n  different decision theory.  Can this be seen immediately from\n  the expected utility formula? If not, it might be unlike the\n  other three above. My current guess is that reflective consistency\n  is a higher-level property that follows from the above three.\n* Emphasis on graphical models: FDT is formalized using graphical models (of the kind you can read about in Judea Pearl’s book _Causality_) while UDT isn’t.\n* Recent developments like using logical inductors.\n* Uncertainty about where your decision algorithm is: I think this is some combination of the three that I’m already covering. For previous discussions, see [this section](http://acritch.com/deserving-trust/#grokking) of Andrew Critch’s post, [this comment](https://www.greaterwrong.com/posts/Qyix5Z5YPSGYxf7GG/less-wrong-q-and-a-with-eliezer-yudkowsky-video-answers/comment/BrWSWrwaategHEkvh) by Wei Dai, and [this post](https://www.lesswrong.com/posts/zztyZ4SKy7suZBpbk/another-attempt-to-explain-udt) by Vladimir Slepnev.\n* Different versions of UDT (e.g. proof-based, modal).\n\n# Comparison table along the given dimensions\n\nGiven the comparison dimensions above, the decision theories can be summarized as follows:\n\n|Decision theory|Outermost iteration|Updateless|Type of counterfactual|\n|--------------------------|------------|--------------|-------------|\n|Updateless decision theory 1 (UDT1)|action|yes|logical|\n|Updateless decision theory 1.1 (UDT1.1)|policy|yes|logical|\n|Updateless decision theory 2 (UDT2)|algorithm|yes|logical|\n|Functional decision theory, iterating over actions (FDT-action)|action|yes|logical|\n|Functional decision theory, iterating over policies (FDT-policy)|policy|yes|logical|\n|Logical decision theory (LDT)|unspecified|unspecified|logical|\n|Timeless decision theory (TDT)|action|no|logical|\n|Causal decision theory (CDT)|action|no|causal|\n|Evidential decision theory (EDT, “naive EDT”)|action|no|conditional|\n\nThe general “shape” of the expected utility formulas will be:\n\n$$\\operatorname*{arg\\,max}_{\\color{blue}{\\text{outermost}\\\\ \\ \\text{iteration}}} \\sum_{j=1}^N \\mathcal U(o_j)\\cdot P(\\mathrm{O\\small UTCOME}=o_j \\mid \\color{red}{\\text{updatelessness}}, \\color{green}{\\text{counterfactual}})$$\n\nOr sometimes:\n\n$$\\operatorname*{arg\\,max}_{\\color{blue}{\\text{outermost}\\\\ \\ \\text{iteration}}} \\sum_{j=1}^N \\mathcal U(o_j)\\cdot P(\\color{green}{\\text{counterfactual}} \\mathbin{\\Box\\kern-7mu\\rightarrow} \\mathrm{O\\small UTCOME}=o_j \\mid \\color{red}{\\text{updatelessness}})$$\n\n# Explanations of each decision theory\n\nThis section elaborates on the comparison above by giving an expected value\nformula for each decision theory and explaining why each cell in the table\ntakes that particular value. I won’t define the notation very clearly, since I\nam mostly collecting the various notations that have been used (so that you can\nlook at the linked sources for the details). My goals are to explain how to fill\nin the table above and to show how all the existing variants in notation are\nsaying the same thing.\n\n## UDT1 and FDT (iterate over actions)\n\nI will describe UDT1 and FDT’s action variant together, because I think they\ngive the same decisions (if there’s a decision problem where they differ, I would like to know about it). The main differences between the two seem to be:\n\n1. The way they are formalized, where FDT uses graphical models and UDT1 uses some kind of non-graphical “mathematical intuition module”.\n2. The naming, where UDT1 emphasizes the “updateless” aspect and FDT emphasizes the logical counterfactual aspect.\n3. Some additional assumptions that UDT has that FDT doesn’t. Rob Bensinger [says](https://www.lesswrong.com/posts/9BYo6Q9qBMXWLjqPS/miri-decisions-are-for-making-bad-outcomes-inconsistent#JJBt6eitzzrWPukSp) “accepting FDT doesn’t necessarily require a commitment to some of the philosophical ideas associated with updatelessness and logical prior probability that MIRI, Wei Dai, or other FDT proponents happen to accept” and also [says](https://intelligence.org/2017/10/22/fdt/#comment-3581691194) UDT “built in some debatable assumptions (over and above what’s needed to show why TDT, CDT, and EDT don’t work)”. I’m not sure what these additional assumptions are, but my guess is it has to do with viewing the world as a program, Tegmark’s level IV multiverse, and things like that (I would be interested in hearing more about the exact assumptions).\n\nIn the [original UDT post](http://lesswrong.com/lw/15m/towards_a_new_decision_theory/), the expected utility formula is written like this:\n$$Y^* = \\operatorname*{arg\\,max}_{Y} \\sum P_Y(\\langle E_1,E_2,E_3,\\ldots\\rangle) U(\\langle E_1,E_2,E_3,\\ldots\\rangle)$$\nHere $Y$ is an “output string” (which is basically an action). The sum is taken over all possible vectors of the execution histories.\nI prefer [Tyrrell McAllister’s notation](https://casparoesterheld.files.wordpress.com/2017/08/updateless_decision_theory-1.pdf):\n$$\\operatorname*{arg\\,max}_{Y\\in \\mathbf Y} \\sum_{E\\in\\mathbf E} M(X,Y,E) U(E)$$\n\nTo explain the UDT1 row in the comparison table, note that:\n\n* The outermost iteration is $\\operatorname*{arg\\,max}_{Y\\in \\mathbf Y}$ (over output strings, a.k.a. actions), so it is doing action selection.\n* We don’t update on the observation. This isn’t really clear from the notation, since $M(X,Y,E)$ still depends on the input string $X$. However, the [original post](http://lesswrong.com/lw/15m/towards_a_new_decision_theory/) clarifies this, saying “Bayesian updating is not done explicitly in this decision theory”.\n* The counterfactual is logical because $P_Y$ and $M$ use the “mathematical intuition module”.\n\nIn the [FDT paper](https://arxiv.org/pdf/1710.05060.pdf) (p. 14), the action selection variant of FDT is written as follows:\n\n$$\\begin{align}\\mathrm{FDT}(P,G,x) &= \\operatorname*{arg\\,max}_{a \\in \\mathcal A} \\mathbb E(\\mathcal U(\\mathrm{O\\small UTCOME}) \\mid \\mathtt{do}(\\mathrm{\\small FDT}(\\underline P,\\underline G,\\underline x)=a)) \\\\ &= \\operatorname*{arg\\,max}_{a\\in \\mathcal A} \\sum_{j=1}^N \\mathcal U(o_j)\\cdot P(\\mathrm{O\\small UTCOME}=o_j\\mid \\mathtt{do}(\\mathrm{\\small FDT}(\\underline P, \\underline G, \\underline x) = a))\\end{align}$$\n\nAgain, note that we are doing action selection (“$\\operatorname*{arg\\,max}_{a \\in \\mathcal A}$”), using logical counterfactuals (“$\\mathtt{do}(\\mathrm{\\small FDT}(\\underline P, \\underline G, \\underline x) = a)$”), and being updateless (absence of “$\\mathrm{O\\small BS} = x$”).\n\n## UDT1.1 and FDT (iterate over policies)\n\nUDT1.1 is a decision theory introduced by Wei Dai’s post [“Explicit Optimization of Global Strategy (Fixing a Bug in UDT1)”](http://lesswrong.com/lw/1s5/explicit_optimization_of_global_strategy_fixing_a/).\n\nIn [Hintze](https://intelligence.org/wp-content/uploads/2014/10/Hintze-Problem-Class-Dominance-In-Predictive-Dilemmas.pdf) (p. 4, 12) UDT1.1 is written as follows:\n\n$$\\mathrm{UDT}(s) = \\operatorname*{arg\\,max}_{f} \\sum_{i=1}^n U(O_i) \\cdot P(\\ulcorner \\mathrm{UDT} := f : s\\mapsto a \\urcorner \\mathbin{\\Box\\kern-7mu\\rightarrow} O_i)$$\n\nHere $f$ iterates over functions that map sense data ($s$) to actions ($a$), $U$ is the utility function, and $O_1,\\ldots,O_n$ are outcomes.\n\nUsing [Tyrrell McAllister’s notation](https://casparoesterheld.files.wordpress.com/2017/08/updateless_decision_theory-1.pdf), UDT1.1 looks like:\n\n$$\\mathrm{UDT}_{1.1}(\\mathbf X, \\mathbf Y, \\mathbf E, M, \\mathbf I) = \\operatorname*{arg\\,max}_{f\\in \\mathbf I} \\sum_{E\\in\\mathbf E} M(f,E) U(E)$$\n\nUsing notation from the [FDT paper](https://arxiv.org/pdf/1710.05060.pdf) plus a trick I saw on [this Arbital page](https://arbital.com/p/logical_dt/?l=5d6) we can write the policy selection variant of FDT as:\n\n$$(\\mathrm{FDT}(P,x))(x) = \\left(\\operatorname*{arg\\,max}_{\\pi\\in \\Pi} \\sum_{j=1}^N \\mathcal U(o_j)\\cdot P(\\mathrm{O\\small UTCOME}=o_j \\mid \\mathtt{true}(\\mathrm{\\small FDT}(\\underline P, \\underline x) = \\pi))\\right)(x)$$\n\nOn the right hand side, the large expression (the part inside and including the $\\operatorname{arg\\,max}$) returns a policy, so to get the action we call the policy on the observation $x$.\n\nThe important things to note are that UDT1.1 and the policy selection variant of FDT:\n\n* Do policy selection because the outermost iteration is over policies (“$\\operatorname*{arg\\,max}_{f}$” or “$\\operatorname*{arg\\,max}_{\\pi\\in \\Pi}$” depending on the notation). Quotes about policy selection: The [FDT paper](https://arxiv.org/pdf/1710.05060.pdf) (p. 11, footnote 7) says “In the authors’ preferred formalization of FDT, agents actually iterate over *policies* (mappings from observations to actions) rather than actions. This makes a difference in certain multi-agent dilemmas, but will not make a difference in this paper.” See also comments by Vladimir Slepnev ([1](https://www.greaterwrong.com/posts/2THFt7BChfCgwYDeA/let-s-discuss-functional-decision-theory/comment/6xLQAfYu4rJTN3MWJ), [2](https://www.lesswrong.com/posts/cAMhvPgMQJzhrpNdN/publication-of-anthropic-decision-theory#nW4bPcheDJ4ZAHCNb)).\n* Use logical counterfactuals (denoted by corner quotes and boxed arrow, the mathematical intuition $M$, or the $\\mathtt{true}$ operator).\n* Are updateless because they don’t condition on the observation (note the absence of conditioning of the form $\\mathrm{O\\small BS} = x$).\n\n## TDT\n\nMy understanding of TDT is mainly from\n[Hintze](https://intelligence.org/wp-content/uploads/2014/10/Hintze-Problem-Class-Dominance-In-Predictive-Dilemmas.pdf).\nI am aware of the [TDT paper](https://intelligence.org/files/TDT.pdf) and skimmed it a while back,\nbut did not revisit it in the course of writing this post.\n\nUsing notation from [Hintze](https://intelligence.org/wp-content/uploads/2014/10/Hintze-Problem-Class-Dominance-In-Predictive-Dilemmas.pdf) (p. 4, 11) the expected utility formula for TDT can be written as follows:\n\n$$\\mathrm{TDT}(s) = \\operatorname*{arg\\,max}_{a\\in\\mathcal A} \\sum_{i=1}^n U(O_i) P(\\ulcorner \\mathrm{TDT}(s) := a\\urcorner \\mathbin{\\Box\\kern-7mu\\rightarrow} O_i \\mid s)$$\n\nHere, $s$ is a string of sense data (a.k.a. observation), $\\mathcal A$ is the set of actions, $U$ is the utility function, $O_1,\\ldots,O_n$ are outcomes, the corner quotes and boxed arrow $\\mathbin{\\Box\\kern-7mu\\rightarrow}$ denote a logical counterfactual (“if the TDT algorithm were to output $a$ given input $s$”).\n\nIf I were to rewrite the above using notation from the [FDT paper](https://arxiv.org/pdf/1710.05060.pdf), it would look like:\n\n$$\\mathrm{TDT}(P,x) = \\operatorname*{arg\\,max}_{a\\in\\mathcal A} \\sum_{j=1}^N \\mathcal U(o_j)\\cdot P(\\mathrm{O\\small UTCOME} = o_j \\mid \\mathrm{O\\small BS}=x, \\mathtt{true}(\\mathrm{\\small TDT}(\\underline P, \\underline x) = a))$$\n\nThe things to note are:\n\n* The outermost iteration is over actions (“$\\operatorname*{arg\\,max}_{a\\in\\mathcal A}$”), so TDT does action selection.\n* We condition on the sense data $s$ or observation $\\mathrm{O\\small BS}=x$, so TDT is updateful. Quotes about TDT’s updatefulness: [this post](https://intelligence.org/2017/03/18/new-paper-cheating-death-in-damascus/) describes TDT as “a theory by MIRI senior researcher Eliezer Yudkowsky that made the mistake of conditioning on observations”. The [Updateless decision theories](https://arbital.com/p/updateless_dt/) page on Arbital calls TDT “updateful”. [Hintze](https://intelligence.org/wp-content/uploads/2014/10/Hintze-Problem-Class-Dominance-In-Predictive-Dilemmas.pdf) (p. 11): “TDP’s failure on the Curious Benefactor is straightforward. Upon seeing the coinflip has come up tails, it updates on the sensory data and realizes that it is in the causal branch where there is no possibility of getting a million.”\n* We use corner quotes and the boxed arrow, or the $\\mathtt{true}$ operator, to denote a logical counterfactual.\n\n## UDT2\n\nI know very little about UDT2, but based on [this\ncomment](http://lesswrong.com/lw/jhj/functional_side_effects/adhy) by Wei Dai\nand [this\npost](https://www.lesswrong.com/posts/wXxPmc9W6kPb6i7vj/notes-on-logical-priors-from-the-miri-workshop)\nby Vladimir Slepnev, it seems to iterate over algorithms rather than actions or\npolicies, and I am assuming it didn’t abandon updatelessness and logical counterfactuals.\n\nThe following search queries might have more information:\n\n* [“UDT2”](https://www.greaterwrong.com/search?q=%22UDT2%22)\n* [site:agentfoundations.org “UDT2”](https://www.google.com/search?q=site%3Aagentfoundations.org%20%22UDT2%22)\n* [site:lesswrong.com “UDT2”](https://www.google.com/search?q=site%3Alesswrong.com%20%22UDT2%22)\n\n## LDT\n\nLDT (logical decision theory) seems to be an umbrella decision theory that only requires the use of\nlogical counterfactuals, leaving the iteration type and updatelessness\nunspecified. So my understanding is that UDT1, UDT1.1, UDT2, FDT, and TDT are\nall logical decision theories.\nSee [this Arbital page](https://arbital.com/p/logical_dt/?l=5gc), which says:\n\n> “Logical decision theories” are really a family of recently proposed decision\n> theories, none of which stands out as being clearly ahead of the others in\n> all regards, but which are allegedly all better than causal decision theory.\n\nThe page also calls TDT a logical decision theory (listed under “non-general\nbut useful logical decision theories”).\n\n## CDT\n\nUsing notation from the [FDT paper](https://arxiv.org/pdf/1710.05060.pdf) (p. 13), we can write the expected utility formula for CDT as follows:\n\n$$\\begin{align}\\mathrm{CDT}(P,G,x) &= \\operatorname*{arg\\,max}_{a \\in\\mathcal A} \\mathbb E(\\mathcal U(\\mathrm{O\\small UTCOME}) \\mid \\mathtt{do}(\\mathrm{A\\small CT}=a), \\mathrm{O\\small BS} = x) \\\\ &= \\operatorname*{arg\\,max}_{a \\in\\mathcal A} \\sum_{j=1}^N \\mathcal U(o_j)\\cdot P(\\mathrm{O\\small UTCOME} = o_j \\mid \\mathtt{do}(\\mathrm{A\\small CT}=a), \\mathrm{O\\small BS} = x)\\end{align}$$\n\nThings to note:\n\n* The outermost iteration is $\\operatorname*{arg\\,max}_{a \\in\\mathcal A}$ so CDT does action selection.\n* We condition on $\\mathrm{O\\small BS} = x$ so CDT is updateful.\n* The presence of $\\mathtt{do}(\\mathrm{A\\small CT}=a)$ means we use causal counterfactuals.\n\n## EDT\n\nUsing notation from the [FDT paper](https://arxiv.org/pdf/1710.05060.pdf) (p. 12), we can write the expected utility formula for EDT as follows:\n\n$$\\begin{align}\\mathrm{EDT}(P,x) &= \\operatorname*{arg\\,max}_{a \\in\\mathcal A} \\mathbb E(\\mathcal U(\\mathrm{O\\small UTCOME}) \\mid \\mathrm{O\\small BS} = x, \\mathrm{A\\small CT}=a) \\\\ &= \\operatorname*{arg\\,max}_{a \\in\\mathcal A} \\sum_{j=1}^N \\mathcal U(o_j)\\cdot P(\\mathrm{O\\small UTCOME} = o_j \\mid \\mathrm{O\\small BS} = x, \\mathrm{A\\small CT}=a)\\end{align}$$\n\nThings to note:\n\n* The outermost iteration is $\\operatorname*{arg\\,max}_{a \\in\\mathcal A}$ so EDT does action selection.\n* We condition on $\\mathrm{O\\small BS} = x$ so EDT is updateful.\n* We condition on $\\mathrm{A\\small CT}=a$ so EDT uses conditional probability as its counterfactual.\n\nThere are various versions of EDT (e.g. versions that smoke on the smoking\nlesion problem). The EDT in this post is the “naive” version. I don’t\nunderstand the more sophisticated versions of EDT, but the keyword for learning\nmore about them seems to be the [tickle defense](https://github.com/riceissa/project-ideas/issues/47).\n\n# Comparison on specific decision problems\n\nIf two decision theories are actually different, there should be some decision problem where they return different answers.\n\nThe FDT paper does a great job of distinguishing the logical-counterfactual decision theories from EDT and CDT.\nHowever, it doesn’t distinguish between different logical-counterfactual decision theories.\n\nThe following is a table that shows the disagreements between decision theories.\nFor each pair of decision theories specified by a row and column, the decision problem named in the cell is one where the decision theories return different answers.\nThe diagonal is blank because the decision theories are the same. The lower left triangle is blank because it repeats the entries in the mirror image (along the diagonal) spots.\n\n| |UDT1.1/FDT-policy|UDT1/FDT-action|TDT|EDT|CDT|\n|:---:|:----------:|:----------:|:----------:|:----------:|:----------:|\n|**UDT1.1/FDT-policy**|–|Number assignment problem described in the [UDT1.1 post](https://www.lesswrong.com/posts/g8xh9R7RaNitKtkaa/explicit-optimization-of-global-strategy-fixing-a-bug-in) (both UDT1 copies output “A”, the UDT1.1 copies output “A” and “B”)|[Counterfactual mugging](https://wiki.lesswrong.com/wiki/Counterfactual_mugging) (a.k.a. curious benefactor) (TDT refuses, UDT1.1 pays)|[Parfit’s hitchhiker](https://wiki.lesswrong.com/wiki/Parfit%27s_hitchhiker) (EDT refuses, UDT1.1 pays)|[Newcomb’s problem](https://wiki.lesswrong.com/wiki/Newcomb%27s_problem) (CDT two-boxes, UDT1.1 one-boxes)|\n|**UDT1/FDT-action**|–|–|Counterfactual mugging (a.k.a. curious benefactor) (TDT refuses, UDT1 pays)|Parfit’s hitchhiker (EDT refuses, UDT1 pays)|Newcomb’s problem (CDT two-boxes, UDT1 one-boxes)|\n|**TDT**|–|–|–|Parfit’s hitchhiker (EDT refuses, TDT pays)|Newcomb’s problem (CDT two-boxes, TDT one-boxes)|\n|**EDT**|–|–|–|–|Newcomb’s problem (CDT two-boxes, EDT one-boxes)|\n|**CDT**|–|–|–|–|–|\n\n# Other comparisons\n\nHere are some existing comparisons between decision theories that I found\nuseful, along with reasons why I felt the current post was needed.\n\n* [“Decision-theoretic problems and Theories; An (Incomplete) comparative list”](https://www.lesswrong.com/posts/cWEhuXQBxRwxmhER5/decision-theoretic-problems-and-theories-an-incomplete) by somervta.\n  This list is useful and modern but doesn’t include the different versions of UDT and FDT.\n* [“A comprehensive list of decision\n  theories”](https://casparoesterheld.com/a-comprehensive-list-of-decision-theories/)\n  by Caspar Oesterheld and/or Johannes Treutlein. I think my motivation is\n  different from that of the author(s) of this list; I mainly want to\n  distinguish between all the UDTs, TDT, and FDT, so my tables and columns of\n  those tables are chosen in a way so as to make the differences apparent.\n* [“Problem Class Dominance in Predictive\n  Dilemmas”](https://intelligence.org/wp-content/uploads/2014/10/Hintze-Problem-Class-Dominance-In-Predictive-Dilemmas.pdf)\n  by Daniel Hintze. This paper is from 2014 so doesn’t include the FDT/LDT\n  terminology, and also doesn’t include the various versions of UDT.\n* [“Timeline of decision theory”](https://timelines.issarice.com/wiki/Timeline_of_decision_theory).\n  This is an incomplete timeline I’ve been working on sporadically. It gives a\n  chronological ordering of some decision theories and decision problems with a\n  focus on logical-counterfactual decision theories, but doesn’t really compare\n  them.\n"
    },
    "voteCount": 24
  },
  {
    "_id": "rqt8RSKPvh4GzYoqE",
    "url": null,
    "title": "Counterfactual Mugging and Logical Uncertainty",
    "slug": "counterfactual-mugging-and-logical-uncertainty",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "Counterfactuals"
      },
      {
        "name": "Logical Uncertainty"
      },
      {
        "name": "Counterfactual Mugging"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "**Followup to**: [Counterfactual Mugging](/lw/3l/counterfactual_mugging/).\n\nLet's see what happens with [Counterfactual Mugging](http://wiki.lesswrong.com/wiki/Counterfactual_mugging), if we replace the uncertainty about an external fact of how a coin lands, with logical uncertainty, for example about what is the n-th place in the [decimal expansion of pi](http://en.wikipedia.org/wiki/Computing_%CF%80).\n\nThe original thought experiment is as follows:\n\n> [Omega](http://wiki.lesswrong.com/wiki/Omega) appears and says that it has just tossed a fair coin, and given that the coin came up tails, it decided to ask you to give it $100. Whatever you do in this situation, nothing else will happen differently in reality as a result. Naturally you don't want to give up your $100. But Omega also tells you that if the coin came up heads instead of tails, it'd give you $10000, but only if you'd agree to give it $100 if the coin came up tails.\n\nLet's change \"coin came up tails\" to \"10000-th digit of pi is even\", and correspondingly for heads. This gives _Logical Counterfactual Mugging_:\n\n> Omega appears and says that it has just found out what that 10000th decimal digit of pi is 8, and given that it is even, it decided to ask you to give it $100. Whatever you do in this situation, nothing else will happen differently in reality as a result. Naturally you don't want to give up your $100. But Omega also tells you that if the 10000th digit of pi turned out to be odd instead, it'd give you $10000, but only if you'd agree to give it $100 given that the 10000th digit is even.\n\nThis form of Counterfactual Mugging may be instructive, as it slaughters the following false intuition, or equivalently conceptualization of \"could\": \"the coin _could_ land either way, but a logical truth _couldn't_ be either way\".\n\nFor the following, let's shift the perspective to Omega, and consider the problem about 10001th digit, which is 5 (odd). It's easy to imagine that given that the 10001th digit of pi is in fact 5, and you decided to only give away the $100 if the digit is odd, then Omega's prediction of your actions will still be that you'd give away $100 (because the digit is in fact odd). Direct prediction of your actions _can't_ include the part where you observe that the digit is even, because the digit is odd.\n\nBut Omega doesn't compute what you'll do in reality, it computes what you _would_ do _if the 10001th digit of pi was even_ (which it isn't). If you decline to give away the $100 if the digit is even, Omega's simulation _of counterfactual_ where the digit is even _will_ say that you wouldn't oblige, and so you won't get the $10000 in reality, where the digit is odd.\n\nImagine it constructively this way: you have the code of a procedure, Pi(n), that computes the n-th digit of pi once it's run. If your strategy is\n\n> if(Is_Odd(Pi(n))) then Give(\"$100\");\n\nthen, given that n==10001, Pi(10001)==5, and Is_Odd(5)==**true**, the program outputs \"$100\". But Omega tests what's the output of the code on which it performed a surgery, replacing Is_Odd(Pi(n)) by **false** instead of **true** to which it normally evaluates. Thus it'll be testing the code\n\n> if(**false**) then Give(\"$100\");\n\nThis counterfactual case doesn't give away $100, and so Omega decides that you won't get the $10000.\n\nFor the original problem, when you consider what would happen if the coin fell differently, you are basically performing the same surgery, replacing the knowledge about the state of the coin in the state of mind. If you use the (wrong) strategy\n\n> if(Coin==\"heads\") then Give(\"$100\")\n\nand the coin comes up \"heads\", so that Omega is deciding whether to give you $10000, then Coin==\"heads\", but Omega is evaluating the modified algorithm where Coin is replaced by \"tails\":\n\n> if(\"tails\"==\"heads\") then Give(\"$100\")\n\nAnother way of intuitively thinking about Logical CM is to consider the index of the digit (here, 10000 or 10001) to be a random variable. Then, the choice of number n (value of the random variable) in Omega's question is a perfect analogy with the outcome of a coin toss.\n\nWith a random index instead of \"direct\" mathematical uncertainty, the above evaluation of counterfactual uses (say) 10000 to replace n (so that Is_Odd(Pi(10000))==**false**), instead of directly using **false** to replace Is_Odd(P(n)) with **false**:\n\n> if(Is_Odd(Pi(10000))) then Give(\"$100\");\n\nThe difference is that with the coin or random digit number, the parameter is explicit and atomic (Coin and n, respectively), while with the oddness of n-th digit, the parameter Is_Odd(P(n)) isn't atomic. How can it be detected in the code (in the mind) — it could be written in obfuscated assembly, not even an explicit subexpression of the program? By the connection to the [sense](/lw/15n/sense_denotation_and_semantics/) of the problem statement itself: when you are talking about what you'll do if the n-th digit of pi is even or odd, or what Omega will do if you give or not give away $100 in each case, you are talking about exactly your Is_Odd(Pi(n)), or something from which this code will be constructed. The meaning of procedure Pi(n) is dependent on the meaning of the problem, and through this dependency counterfactual surgery can reach down and change the details of the algorithm to answer the counterfactual query posed by the problem."
    },
    "voteCount": 13
  },
  {
    "_id": "g3PwPgcdcWiP33pYn",
    "url": null,
    "title": "Counterfactual Mugging Poker Game",
    "slug": "counterfactual-mugging-poker-game",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "Decision Theory"
      },
      {
        "name": "Meta-Honesty"
      },
      {
        "name": "Counterfactuals"
      },
      {
        "name": "Counterfactual Mugging"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "Consider the following game:\n\nPlayer A receives a card at random that is either High or Low. He may reveal his card if he wishes.\n\nPlayer B then chooses a probability < refresh to render LaTeX > that Player A has a high card.\n\nPlayer A always loses < refresh to render LaTeX > dollars. Player B loses < refresh to render LaTeX > dollars if the card is low and < refresh to render LaTeX > dollars if the card is high.\n\nNote that Player B has been given a proper scoring rule, and so is incentivized to give his true probability (unless he makes some deal with player A).\n\nYou are playing this game as player A. You only play one time. You are looking at a low card. Player B is not trying to make a deal with you, and will report his true probability. Player B is very good at reasoning about you, but you are in a separate room, so Player B cannot read any tells unless you show the card. Do you show your card?\n\nSince your card is low, if you show it to player B, you will lose nothing, and get the best possible output. However, if player B reasons that if you would show your card if it was low, then in the counterfactual world in which you got a high card, player B would know you had a high card because you refused to show. Thus, you would lose a full dollar in those counterfactual worlds.\n\nIf you choose to not reveal your card, player B would assign probability 1/2 and you would lose a quarter.\n\nI like this variant of the counterfactual mugging because it takes the agency out of the predictor. In the standard counterfactual mugging, you might reject the hypothetical and think that the predictor is trying to trick you. Here, there is a sense in which you are creating the counterfactual mugging yourself by trying to be able to keep secrets.\n\nAlso, think about this example the next time you are tempted to say that someone would only [Glomarize](https://en.wikipedia.org/wiki/Glomar_response) if they had an important secret."
    },
    "voteCount": 40
  },
  {
    "_id": "4ARtkT3EYox3THYjF",
    "url": null,
    "title": "Rationality is Systematized Winning",
    "slug": "rationality-is-systematized-winning",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "Rationality"
      },
      {
        "name": "Something To Protect"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "**Followup to**:  [Newcomb's Problem and Regret of Rationality](http://www.overcomingbias.com/2008/01/newcombs-proble.html)\n\n\"Rationalists should _win,_\" I said, and I may have to stop saying it, for it seems to convey something other than what I meant by it.\n\nWhere did the phrase come from originally?  From considering such cases as [Newcomb's Problem](http://www.overcomingbias.com/2008/01/newcombs-proble.html):  The superbeing Omega sets forth before you two boxes, a transparent box A containing $1000 (or the equivalent in material wealth), and an opaque box B that contains either $1,000,000 or nothing.  Omega tells you that It has already put $1M in box B if and only if It predicts that you will take only box B, leaving box A behind.  Omega has played this game many times before, and has been right 99 times out of 100.  Do you take both boxes, or only box B?\n\nA common position - in fact, the mainstream/dominant position in modern philosophy and decision theory - is that the only _reasonable_ course is to take both boxes; Omega has already made Its decision and gone, and so your action cannot _affect_ the contents of the box in any way (they argue).  Now, it so happens that certain types of _unreasonable_ individuals are rewarded by Omega - who moves even before they make their decisions - but this in no way changes the conclusion that the only _reasonable_ course is to take both boxes, since taking both boxes makes you $1000 richer regardless of the unchanging and unchangeable contents of box B.\n\nAnd this is the sort of thinking that I intended to reject by saying, \"Rationalists should _win!_\"\n\nSaid Miyamoto Musashi:  \"The primary thing when you take a sword in your hands is your intention to cut the enemy, whatever the means.  Whenever you parry, hit, spring, strike or touch the enemy's cutting sword, you must cut the enemy in the same movement.  It is essential to attain this.  If you think only of hitting, springing, striking or touching the enemy, you will not be able actually to cut him.\"\n\nSaid I:  \"If you fail to achieve a correct answer, it is futile to protest that you acted with propriety.\"\n\nThis is the distinction I had hoped to convey by saying, \"Rationalists should _win!_\"\n\nThere is a meme which says that a certain ritual of cognition is the paragon of _reasonableness_ and so defines what the _reasonable_ people do.  But alas, the _reasonable_ people often get their butts handed to them by the _unreasonable_ ones, because the universe isn't always _reasonable_.  Reason is just _a_ way of doing things, not necessarily _the_ most [formidable](/lw/2c/a_sense_that_more_is_possible/); it is how professors talk to each other in debate halls, which sometimes works, and sometimes doesn't.  If a hoard of barbarians attacks the debate hall, the truly prudent and flexible agent will abandon reasonableness.\n\nNo.  If the \"irrational\" agent is outcompeting you on a _systematic_ and _predictable_ basis, then it is time to reconsider what you think is \"rational\".\n\nFor I do fear that a \"rationalist\" will clutch to themselves the ritual of cognition they have been taught, as loss after loss piles up, consoling themselves:  \"I have behaved virtuously, I have been _so reasonable,_ it's just this _awful unfair universe_ that doesn't give me what I _deserve_.  The others are _cheating_ by not doing it the rational way, _that's_ how they got ahead of me.\"\n\nIt is this that I intended to guard against by saying:  \"Rationalists should _win!_\"  Not whine, _win_.  If you keep on losing, perhaps you are doing something _wrong_.  Do not console yourself about how you were so wonderfully rational in the course of losing.  That is _not_ how things are supposed to go.  It is not the Art that fails, but you who fails to grasp the Art.\n\nLikewise in the realm of epistemic rationality, if you find yourself thinking that the _reasonable_ belief is X (because a majority of modern humans seem to believe X, or something that sounds similarly appealing) and yet [the world itself](/lw/1f/moores_paradox/) is obviously Y.\n\nBut people do seem to be taking this in some other sense than I meant it - as though any person who declared themselves a rationalist would in that moment be invested with an invincible spirit that enabled them to obtain all things without effort and without overcoming disadvantages, or something, I don't know.\n\nMaybe there is an alternative phrase to be found again in Musashi, who said:  \"The Way of the Ichi school is the spirit of winning, whatever the weapon and whatever its size.\"\n\n\"Rationality is the spirit of winning\"?  \"Rationality is the Way of winning\"?  \"Rationality is systematized winning\"?  If you have a better suggestion, post it in the comments."
    },
    "voteCount": 77
  },
  {
    "_id": "4ZzefKQwAtMo5yp99",
    "url": null,
    "title": "Circular Altruism",
    "slug": "circular-altruism",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "Ethics & Morality"
      },
      {
        "name": "Altruism"
      },
      {
        "name": "Consequentialism"
      },
      {
        "name": "Probability & Statistics"
      },
      {
        "name": "Bayesianism"
      },
      {
        "name": "Fuzzies"
      },
      {
        "name": "Shut Up and Multiply"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "**Followup to**:  [Torture vs. Dust Specks](/lw/kn/torture_vs_dust_specks/), [Zut Allais](/lw/mz/zut_allais/), [Rationality Quotes 4](/lw/n0/rationality_quotes_4/)\n\nSuppose that a disease, or a monster, or a war, or something, is killing people.  And suppose you only have enough resources to implement one of the following two options:\n\n1.  Save 400 lives, with certainty.\n2.  Save 500 lives, with 90% probability; save no lives, 10% probability.\n\nMost people choose option 1.  Which, I think, is foolish; because if you multiply 500 lives by 90% probability, you get an expected value of 450 lives, which exceeds the 400-life value of option 1.  (Lives saved don't diminish in marginal utility, so this is an appropriate calculation.)\n\n\"What!\" you cry, incensed.  \"How can you gamble with human lives? How can you think about numbers when so much is at stake?  What if that 10% probability strikes, and everyone dies?  So much for your damned logic!  You're following your rationality off a cliff!\"\n\nAh, but here's the interesting thing.  If you present the options this way:\n\n1.  100 people die, with certainty.\n2.  90% chance no one dies; 10% chance 500 people die.\n\nThen a majority choose option 2.  _Even though it's the same gamble._  You see, just as a _certainty_ of saving 400 lives seems to _feel_ so much more comfortable than an unsure gain, so too, a certain loss _feels_ worse than an uncertain one.\n\nYou can grandstand on the second description too:  \"How can you condemn 100 people to certain death when there's such a good chance you can save them?  We'll all share the risk!  Even if it was only a 75% chance of saving everyone, it would still be worth it - so long as there's a chance - everyone makes it, or no one does!\"\n\nYou know what?  This isn't about your feelings.  A human life, with all its joys and all its pains, adding up over the course of decades, is worth far more than your brain's feelings of comfort or discomfort with a plan.  Does computing the expected utility feel too cold-blooded for your taste?  Well, that feeling isn't even a feather in the scales, when a life is at stake.  Just shut up and multiply.\n\n[Previously on Overcoming Bias](/lw/kn/torture_vs_dust_specks/), I asked what was the least bad, bad thing that could happen, and suggested that it was getting a dust speck in your eye that irritated you for a fraction of a second, barely long enough to notice, before it got blinked away.  And conversely, a very bad thing to happen, if not the worst thing, would be getting tortured for 50 years.\n\nNow, would you rather that a googolplex people got dust specks in their eyes, or that one person was tortured for 50 years?  I originally asked this question with a vastly larger number - an incomprehensible mathematical magnitude - but a googolplex works fine for this illustration.\n\nMost people chose the dust specks over the torture.  Many were proud of this choice, and indignant that anyone should choose otherwise:  \"How dare you condone torture!\"\n\nThis matches research showing that there are \"sacred values\", like human lives, and \"unsacred values\", like money.  When you try to trade off a sacred value against an unsacred value, subjects express great indignation (sometimes they want to punish the person who made the suggestion).\n\nMy favorite anecdote along these lines - though my books are packed at the moment, so no citation for now - comes from a team of researchers who evaluated the effectiveness of a certain project, calculating the cost per life saved, and recommended to the government that the project be implemented because it was cost-effective.  The governmental agency rejected the report because, they said, you couldn't put a dollar value on human life.  After rejecting the report, the agency decided _not_ to implement the measure.\n\nTrading off a sacred value (like refraining from torture) against an unsacred value (like dust specks) _feels really awful._  To merely multiply utilities would be too cold-blooded - it would be following rationality off a cliff...\n\nBut let me ask you this.  Suppose you had to choose between one person being tortured for 50 years, and a googol people being tortured for 49 years, 364 days, 23 hours, 59 minutes and 59 seconds.  You would choose one person being tortured for 50 years, I do presume; otherwise I give up on you.\n\nAnd similarly, if you had to choose between a googol people tortured for 49.9999999 years, and a googol-squared people being tortured for 49.9999998 years, you would pick the former.\n\nA googolplex is ten to the googolth power.  That's a googol/100 factors of a googol.  So we can keep doing this, gradually - _very_ gradually - diminishing the degree of discomfort, and multiplying by a factor of a googol each time, until we choose between a googolplex people getting a dust speck in their eye, and a googolplex/googol people getting two dust specks in their eye.\n\nIf you find your preferences are circular here, that makes rather a mockery of moral grandstanding.  If you drive from San Jose to San Francisco to Oakland to San Jose, over and over again, you may have fun driving, but you aren't _going_ anywhere.  Maybe you think it a great display of virtue to choose for a googolplex people to get dust specks rather than one person being tortured.  But if you would also trade a googolplex people getting one dust speck for a googolplex/googol people getting two dust specks et cetera, you sure aren't _helping anyone._  Circular preferences may work for feeling noble, but not for feeding the hungry or healing the sick.\n\nAltruism isn't the warm fuzzy feeling you get from being altruistic.  If you're doing it for the spiritual benefit, that is nothing but selfishness.  The primary thing is to help others, whatever the means.  So shut up and multiply!\n\nAnd if it seems to you that there is a fierceness to this maximization, like the bare sword of the law, or the burning of the sun - if it seems to you that at the center of this rationality there is a small cold flame -\n\nWell, the other way might feel better inside you.  But it wouldn't work.\n\nAnd I say also this to you:  That if you set aside your regret for all the spiritual satisfaction you _could_ be having - if you _wholeheartedly_ pursue the Way, without thinking that you are being cheated - if you give yourself over to rationality without holding back, you will find that rationality gives to you in return.\n\nBut _that_ part only works if you don't go around saying to yourself, \"It would feel better inside me if only I could be less rational.\"\n\nChimpanzees feel, but they don't multiply.  Should you be sad that you have the opportunity to do better?  You cannot attain your full potential if you regard your gift as a burden.\n\n_Added:_  If you'd still take the dust specks, see [Unknown's comment](/lw/n3/circular_altruism/u7b) on the problem with qualitative versus quantitative distinctions."
    },
    "voteCount": 78
  },
  {
    "_id": "R2mPGwFvXSy4nCMgj",
    "url": "http://programs.clearerthinking.org/how_rational_are_you_really_take_the_test.html",
    "title": "Take the Rationality Test to determine your rational thinking style",
    "slug": "take-the-rationality-test-to-determine-your-rational",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "Rationality Verification"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": ""
    },
    "voteCount": 4
  },
  {
    "_id": "vk2yS8osapSch9Cz2",
    "url": null,
    "title": "The Bat and Ball Problem Revisited",
    "slug": "the-bat-and-ball-problem-revisited",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "Rationality"
      },
      {
        "name": "Dual Process Theory (System 1 & System 2)"
      }
    ],
    "tableOfContents": {
      "sections": [
        {
          "title": "Thinking, inherently fast and inherently slow",
          "anchor": "Thinking__inherently_fast_and_inherently_slow",
          "level": 1
        },
        {
          "title": "No, seriously, the answer isn't ten cents",
          "anchor": "No__seriously__the_answer_isn_t_ten_cents",
          "level": 1
        },
        {
          "title": "So, what are people doing when they solve this problem?",
          "anchor": "So__what_are_people_doing_when_they_solve_this_problem_",
          "level": 1
        },
        {
          "title": "How To Solve It",
          "anchor": "How_To_Solve_It",
          "level": 1
        },
        {
          "title": "How To Visualise It",
          "anchor": "How_To_Visualise_It",
          "level": 1
        },
        {
          "title": "Final thoughts",
          "anchor": "Final_thoughts",
          "level": 1
        },
        {
          "title": "Questions",
          "anchor": "Questions",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        },
        {
          "anchor": "comments",
          "level": 0,
          "title": "27 comments"
        }
      ],
      "headingsCount": 9
    },
    "contents": {
      "markdown": "*Cross posted from [my personal blog](https://drossbucket.wordpress.com/2018/12/12/the-bat-and-ball-problem-revisited/).*\n\n*In this post, I'm going to assume you've come across the Cognitive Reflection Test before and know the answers. If you haven't, it's only three quick questions, [go and do it now][crt].*\n\n[crt]:https://mindyourdecisions.com/blog/2013/06/24/can-you-correctly-answer-the-cognitive-reflection-test-83-percent-of-people-miss-at-least-1-question/\n\nOne of the striking early examples in Kahneman's *Thinking, Fast and Slow* is the following problem:\n\n> (1) A bat and a ball cost \\$1.10 in total. The bat costs \\$1.00 more than the ball. \n\n> How much does the ball cost? _____ cents\n\nThis question first turns up informally in [a paper by Kahneman and Frederick][kf], who find that most people get it wrong:\n\n> Almost everyone we ask reports an initial tendency to answer “10 cents” because the sum \\$1.10 separates naturally into \\$1 and 10 cents, and 10 cents is about the right magnitude. Many people yield to this immediate impulse. The surprisingly high rate of errors in this easy problem illustrates how lightly System 2 monitors the output of System 1: people are not accustomed to thinking hard, and are often content to trust a plausible judgment that quickly comes to mind.\n\nIn *Thinking Fast and Slow*, the bat and ball problem is used as an introduction to the major theme of the book: the distinction between fluent, spontaneous, fast 'System 1' mental processes, and effortful, reflective and slow 'System 2' ones. The explicit moral is that we are too willing to lean on System 1, and this gets us into trouble:\n\n> The bat-and-ball problem is our first encounter with an observation that will be a recurrent theme of this book: many people are overconfident, prone to place too much faith in their intuitions. They apparently find cognitive effort at least mildly unpleasant and avoid it as much as possible.\n\n[kf]:https://pdfs.semanticscholar.org/4069/615a36c33e61ca309b8ceaeb628a10d441b5.pdf\n\nThis story is very compelling in the case of the bat and ball problem. I got this problem wrong myself when I first saw it, and still find the intuitive-but-wrong answer very plausible looking. I have to consciously remind myself to apply some extra effort and get the correct answer.\n\nHowever, this becomes more complicated when you start considering other tests of this fast-vs-slow distinction. Frederick later combined the bat and ball problem with two other questions to [create the Cognitive Reflection Test][frederickcrt]:\n\n> (2) If it takes 5 machines 5 minutes to make 5 widgets, how long would it take 100 machines to make 100 widgets? _____ minutes\n\n> (3) In a lake, there is a patch of lily pads. Every day, the patch doubles in size. If it takes 48 days for the patch to cover the entire lake, how long would it take for the patch to cover half of the lake? _____ days\n\n[frederickcrt]:http://emilkirkegaard.dk/en/wp-content/uploads/Shane-Frederick-Cognitive-Re%EF%AC%82ection-and-Decision-Making.pdf\n\nThese are designed to also have an 'intuitive-but-wrong' answer (100 minutes, 24 days), and an 'effortful-but-right' answer (5 minutes, 47 days). But this time I [seem to be immune to the wrong answers][stupid], in a way that just doesn't happen with the bat and ball:\n\n> I always have the same reaction, and I don’t know if it’s common or I’m just the lone idiot with this problem. The ‘obvious wrong answers’ for 2. and 3. are completely unappealing to me (I had to look up 3. to check what the obvious answer was supposed to be). Obviously the machine-widget ratio hasn’t changed, and obviously exponential growth works like exponential growth.\n\n> When I see 1., however, I always think ‘oh it’s that bastard bat and ball question again, I know the correct answer but cannot see it’. And I have to stare at it for a minute or so to work it out, slowed down dramatically by the fact that Obvious Wrong Answer is jumping up and down trying to distract me.\n\n[stupid]:https://drossbucket.wordpress.com/2017/02/14/stupid-bat-and-ball/\n\nIf this test was really testing my propensity for effortful thought over spontaneous intuition, I ought to score zero. I hate effortful thought! As it is, I score two out of three, because I've trained my intuitions nicely for ratios and exponential growth. The 'intuitive', 'System 1' answer that pops into my head is, in fact, the correct answer, and the supposedly 'intuitive-but-wrong' answers feel bad on a visceral level. (Why the hell would the lily pads take the same amount of time to cover the second half of the lake as the first half, when the rate of growth is increasing?) \n\n[twenty]:https://drossbucket.wordpress.com/2018/07/29/20-fundamentals/\n\nThe bat and ball still gets me, though. My gut hasn't internalised anything useful, and it's super keen on shouting out the wrong answer in a distracting way. My dislike for effortful thought is definitely a problem here.\n\nI wanted to see if others had raised the same objection, so I started doing some research into the CRT. In the process I discovered a lot of follow-up work that makes the story much more complex and interesting.\n\nI've come nowhere near to doing a proper literature review. Frederick's original paper has been cited nearly 3000 times, and dredging through that for the good bits is a lot more work than I'm willing to put in. This is just a summary of the interesting stuff I found on my limited, partial dig through the literature.\n\n\n# Thinking, inherently fast and inherently slow\n\nFrederick's original Cognitive Reflection Test paper describes the System 1/System 2 divide in the following way:\n\n> Recognizing that the face of the person entering the classroom belongs to your math teacher involves System 1 processes — it occurs instantly and effortlessly and is unaffected by intellect, alertness, motivation or the difficulty of the math problem being attempted at the time. Conversely, finding $\\sqrt{19163}$ to two decimal places without a calculator involves System 2 processes — mental operations requiring effort, motivation, concentration, and the execution of learned rules.\n\nI find it interesting that he frames mental processes as being *inherently* effortless or effortful, independent of the person doing the thinking. This is not quite true even for the examples he gives — faceblind people and calculating prodigies exist.\n\nThis framing is important for interpreting the CRT. If the problem inherently has a wrong 'System 1 solution' and a correct 'System 2 solution', the CRT can work as intended, as an efficient tool to split people by their propensity to use one strategy or the other. If there are 'System 1' ways to get the correct answer, the whole thing gets much more muddled, and it's hard to disentangle natural propensity to reflection from prior exposure to the right mathematical concepts.\n\nMy tentative guess is that the bat and ball problem *is* close to being this kind of efficient tool. Although in some ways it's the simplest of the three problems, solving it in a 'fast', 'intuitive' way relies on seeing the problem in a way that most people's education won't have provided. (I *think* this is true, anyway - I'll go into more detail later.) I suspect that this is less true the other two problems - ratios and exponential growth are topics that a mathematical or scientific education is more likely to build intuition for. \n\n(Aside: I'd like to know how these other two problems were chosen. The paper just states the following: \n\n> Motivated by this result [the answers to the bat and ball question], two other problems found to yield impulsive erroneous responses were included with the “bat and ball” problem to form a simple, three-item “Cognitive Reflection Test” (CRT), shown in Figure 1.\n\nI have a vague suspicion that Frederick trawled through something like 'The Bumper Book of Annoying Riddles' to find some brainteasers that don't require too much in the way of mathematical prerequisites. The lilypads one has a family resemblance to the classic [grains-of-wheat-on-a-chessboard puzzle][wheat], for instance.)\n\nHowever, I haven't found any great evidence either way for this guess. The original paper doesn't break down participants' scores by question – it just gives mean scores on the test as a whole. I did however find [this meta-analysis of 118 CRT studies][meta], which shows that the bat and ball question is the most difficult on average – only 32% of all participants get it right, compared with 40% for the widgets and 48% for the lilypads. It also has the biggest jump in success rate when comparing university students with non-students. That looks like better mathematical education does help on the bat and ball, but it doesn't clear up how it helps. It could improve participants' ability to intuitively see the answer. Or it could improve ability to come up with an 'unintuitive' solution, like solving the corresponding simultaneous equations by a rote method. \n\nWhat I'd really like is some insight into what individual people *actually do* when they try to solve the problems, rather than just this aggregate statistical information. I haven't found exactly what I wanted, but I did turn up a few interesting studies on the way.\n\n[meta]:https://mpra.ub.uni-muenchen.de/68049/1/MPRA_paper_68049.pdf\n[wheat]:https://en.wikipedia.org/wiki/Wheat_and_chessboard_problem\n\n\n# No, seriously, the answer isn't ten cents\n\nMy favourite thing I found was [this (apparently unpublished) ‘extremely rough draft’][meyer] by Meyer, Spunt and Frederick from 2013, revisiting the bat and ball problem. The intuitive-but-wrong answer turns out to be *extremely* sticky, and the paper is basically a series of increasingly desperate attempts to get people to actually think about the question.\n\nOne conjecture for what people are doing when they get this question wrong is the *attribute substitution hypothesis*. This was [suggested early on by Kahneman and Frederick][kf2002], and is a fancy way of saying that they are instead solving the following simpler problem:\n\n> (1) A bat and a ball cost \\$1.10 in total. The bat costs \\$1.00. \n\n> How much does the ball cost? _____ cents\n\nNotice that this is missing the 'more than the ball' clause at the end, turning the question into a much simpler arithmetic problem. This simple problem *does* have 'ten cents' as the answer, so it's very plausible that people are getting confused by it. \n\nMeyer, Spunt and Frederick tested this hypothesis by getting respondents to recall the problem from memory. This showed a clear difference: 94% of 'five cent' respondents could recall the correct question, but only 61% of 'ten cent' respondents. It's possible that there is a different common cause of both the 'ten cent' response and misremembering the question, but it at least gives some support for the substitution hypothesis.\n\nHowever, getting people to actually answer the question correctly was a much more difficult problem. First they tried  bolding the words **more than the ball** to make this clause more salient. This made surprisingly little impact: 29% of respondents solved it, compared with 24% for the original problem. Printing both versions was slightly more successful, bumping up the correct response to 35%, but it was still a small effect.\n\nAfter this, they ditched subtlety and resorted to pasting these huge warnings above the question:\n\n![Computation warning: 'Be careful! Many people miss the following problem because they do not take the time to check their answer. Comprehension warning: 'Be careful! Many people miss the following problem because they read it too quickly and actually answer a different question than the one that was asked.'](https://drossbucket.files.wordpress.com/2018/12/bat_and_ball.png)\n\nThese were still only mildly effective, with a correct solution jumping to 50% from 45%. People just really like the answer 'ten cents', it seems.\n\nAt this point they completely gave up and just flat out added “HINT: 10 cents is not the answer.” This worked reasonably well, though there was still a hard core of 13% who persisted in writing down 'ten cents'. \n\nThat's where they left it. At this point there's not really any room to escalate beyond confiscating the respondents' pens and prefilling in the answer 'five cents', and I worry that somebody would still try and scratch in 'ten cents' in their own blood. The wrong answer is just incredibly compelling.\n\n\n[meyer]:https://law.yale.edu/system/files/area/workshop/leo/document/Frederick_Bat-BallProblem.pdf\n[kf2002]:https://pdfs.semanticscholar.org/853c/2304f0b6455f27677023e19ffc30dc6ca683.pdf\n\n\n# So, what *are* people doing when they solve this problem?\n\nUnfortunately, it's hard to tell from the published literature (or at least what I found of it). What I'd really like is lots of transcripts of individuals talking through their problem solving process. The closest I found was [this paper][szaszi] by Szaszi et al, who did carry out these sort of interview, but it doesn't include any examples of individual responses. Instead, it gives a aggregated overview of types of responses, which doesn't go into the kind of detail I'd like. \n\n[szaszi]:https://www.tandfonline.com/doi/abs/10.1080/13546783.2017.1292954\n\n\nStill, the examples given for their response categories give a few clues. The categories are:\n\n- **Correct answer, correct start.** Example given: 'I see. This is an equation. Thus if the ball equals to x, the bat equals to x plus 1... '\n\n- **Correct answer, incorrect start.** Example: 'I would say 10 cents... But this cannot be true as it does not sum up to €1.10...'\n\n- **Incorrect answer, reflective**, i.e. some effort was made to reconsider the answer given, even if it was ultimately incorrect. Example: '... but I'm not sure... If together they cost €1.10, and the bat costs €1 more than the ball... the solution should be 10 cents. I'm done.'\n\n- **No reflection.** Example: 'Ok. I'm done.' \n\nThese demonstrate one way to reason your way to the correct answer (solve the simultaneous equations) and one way to be wrong (just blurt out the answer). They also demonstrate one way to recover from an incorrect solution (think about the answer you blurted out and see if it actually works). Still, it's all rather abstract and high level.\n\n# How To Solve It\n\nHowever, I did manage to stumble onto another source of insight. While researching the problem I came across [this article][ferrari] from the online magazine of the Association for Psychological Science, which discusses a variant 'Ford and Ferrari problem'. This is quite interesting in itself, but I was most excited by the comments section. Finally some examples of how the problem is solved in the wild!\n\n[ferrari]:https://www.psychologicalscience.org/publications/observer/obsonline/a-new-twist-on-a-classic-puzzle.html\n\n\nThe simplest 'analytical', 'System 2' solution is to rewrite the problem as two simultaneous linear equations and plug-and-chug your way to the correct answer. For example, writing $B$ for the bat and $b$ for the ball, we get the two equations\n\n$B + b = 110$,\n$B - b = 100$, \n\nwhich we could then solve in various standard ways, e.g.\n\n$2B = 210$,\n$B = 105$,\n\nwhich then gives\n\n$b = 110 - B = 5$.\n\nThere are a couple of variants of this explained in the comments. It's a very reliable way to tackle the problem: if you already know how to do this sort of rote method, there are no surprises. This sort of method would work for any similar problem involving linear equations.\n\nHowever, it's pretty obvious that a lot of people *won't* have access to this method. Plenty of people noped out of mathematics long before they got to simultaneous equations, so they won't be able to solve it this way. What might be less obvious, at least if you mostly live in a high-maths-ability bubble, is that these people may also be missing the sort of tacit mathematical background that would even allow them to frame the problem in a useful form in the first place.\n\nThat sounds a bit abstract, so let's look at some responses (I'll paste all these straight in, so any typos are in the original). First, we have these two confused commenters: \n\n> The thing is, why does the ball have to be \\$.05? It could have been .04 0r.03 and the bat would still cost more than \\$1.\n\nand\n\n> This is exactly what bothers me and resulted in me wanting to look up the question online. On the quiz the other 2 questions were definitive. This one technically could have more than one answer so this is where phycologists actually mess up when trying to give us a trick question. The ball at .4 and the bat at 1.06 doesn’t break the rule either.\n\nThese commenters don't automatically see two equations in two variables that together are enough to constrain the problem. Instead they seem to focus mainly on the first condition (adding up to \\$1.10) and just use the second one as a vague check at best ('the bat would still cost more than \\$1'). This means that they are unable to immediately tell that the problem has a unique solution.\n\nIn response, another commenter, Tony, suggests a correct solution which is an interesting mix of writing the problem out formally and then figuring out the answer by trial and error:\\\n\n>  I hear your pain. I feel as though psychologists and psychiatrists get together every now and then to prove how stoopid I am. However, after more than a little head scratching I’ve gained an understanding of this puzzle. It can be expressed as two facts and a question A=100+B and A+B=110, so B=? If B=2 then the solution would be 100+2+2 and A+B would be 104. If B=6 then the solution would be 100+6+6 and A+B would be 112. But as be KNOW A+B=110 the only number for B on it’s own is 5.\n\nThis suggests enough half-remembered mathematical knowledge to find a sensible abstract framing, but not enough to solve it the standard way. \n\nFinally, commenter Marlo Eugene provides an ingenious way of solving the problem without writing all the algebraic steps out:\n\n> Linguistics makes all the difference. The conceptual emphasis seems to lie within the word MORE.\n\n> X + Y = \\$1.10. If X = \\$1 MORE then that leaves \\$0.10 TO WORK WITH rather than automatically assign to Y\n\n> So you divide the remainder equally (assuming negative values are disqualified) and get 0.05.\n\nSo even this small sample of comments suggests a wide diversity of problem-solving methods leading to the two common answers. Further, these solutions don't all split neatly into 'System 1' 'intuitive' and 'System 2' 'analytic'. Marlo Eugene's solution, for instance, is a mixed solution of writing the equations down in a formal way, but then finding a clever way of just seeing the answer rather than solving them by rote.\n\nI'd still appreciate more detailed transcripts, including the time taken to solve the problem. My suspicion is still that very few people solve this problem with a fast intuitive response, in the way that I rapidly see the correct answer to the lilypad question. Even the more 'intuitive' responses, like Marlo Eugene's, seem to rely on some initial careful reflection and a good initial framing of the problem.\n\nIf I'm correct about this lack of fast responses, my tentative guess for the reason is that it has something to do with the way most of us learn simultaneous equations in school. We generally learn arithmetic as young children in a fairly concrete way, with the formal numerical problems supplemented with lots of specific examples of adding up apples and bananas and so forth.\n\nBut then, for some reason, this goes completely out of the window once the unknown quantity isn't sitting on its own on one side of the equals sign. This is instead hived off into its own separate subject, called 'algebra', and the rules are taught much later in a much more formalised style, without much attempt to build up intuition first.  \n\n(One exception is the sort of puzzle sheets that are often given to young kids, where the unknowns are just empty boxes to be filled in. Sometimes you get 2+3=□, sometimes it's 2+□=5, but either way you go about the same process of using your wits to figure out the answer. Then, for some reason I'll never understand, the worksheets get put away and the poor kids don't see the subject again until years later, when the box is now called $x$ for some reason and you have to find the answer by defined rules. Anyway, this is a separate rant.)\n\nThis lack of a rich background in puzzling out the answer to specific concrete problems means most of us lean hard on formal rules in this domain, even if we're relatively mathematically sophisticated. Only a few build up the necessary repertoire of tricks to solve the problem quickly by insight. I'm reminded of a story in Feynman's *The Pleasure of Finding Things Out*:\n\n> Around that time my cousin, who was three years older, was in high school. He was having considerable difficulty with his algebra, so a tutor would come. I was allowed to sit in a corner while the tutor would try to teach my cousin algebra. I'd hear him talking about x.\n\n> I said to my cousin, \"What are you trying to do?\"\n\n> \"I'm trying to find out what x is, like in 2x + 7 = 15.\"\n\n> I say, \"You mean 4.\"\n\n> \"Yeah, but you did it by arithmetic. You have to do it by algebra.\"\n\n> I learned algebra, fortunately, not by going to school, but by finding my aunt's old schoolbook in the attic, and understanding that the whole idea was to find out what *x* is - it doesn't make any difference how you do it.\n\nI think this reliance on formal methods might be somewhat less true for exponential growth and ratios, the subjects underpinning the lilypad and widget questions. Certainly I seem to have better intuition there, without having to resort to rote calculation. But I'm not sure how general this is.\n\n\n# How To Visualise It\n\nIf you wanted to solve the bat and ball problem without having to 'do it by algebra', how would you go about it? \n\nMy [original post][stupid] on the problem was a pretty quick, throwaway job, but over time it picked up some truly excellent comments by anders and Kyzentun, which really start to dig into the structure of the problem and suggest ways to 'just see' the answer. The thread with anders in particular goes into lots of other examples of how we think through solving various problems, and is well worth reading in full. I'll only summarise the bat-and-ball-related parts of the comments here.\n\nWe all used some variant of the method suggested by Marlo Eugene in the comments above. Writing out the basic problem again, we have:\n\n$B + b = 110$,\n$B - b = 100$.\n\nNow, instead of immediately jumping to the standard method of eliminating one of the variables, we can just look at what these two equations are saying and solve it directly 'by thinking'. We have a bat, $B$. If you add the price of the ball, $b$, you get 110 cents. If you instead remove the same quantity $b$ you get 100 cents. So the bat's price must be exactly halfway between these two numbers, at 105 cents. That leaves five for the ball.\n\nNow that I'm thinking of the problem in this way, I directly see the equations as being 'about a bat that's halfway between 100 and 110 cents', and the answer is incredibly obvious. \n\nKyzentun suggests a variant on the problem that is much less counterintuitive than the original:\n\n> A centered piece of text and its margins are 110 columns wide. The text is 100 columns wide. How wide is one margin?\n\n> Same numbers, same mathematical formula to reach the solution. But less misleading because you know there are two margins, and thus know to divide by two after subtracting.\n\nIn the original problem, the 110 units and 100 units both refer to something abstract, the sum and difference of the bat and ball. In Kyzentun's version these become much more concrete objects, the width of the text and the total width of the margins. The work of seeing the equations as relating to something concrete has mostly been done for you.\n\nSimilarly, anders works the problem by 'getting rid of the 100 cents', and splitting the remainder in half to get at the price of the ball: \n\n> I just had an easy time with #1 which I haven’t before. What I did was take away the difference so that all the items are the same (subtract 100), evenly divide the remainder among the items (divide 10 by 2) and then add the residuals back on to get 105 and 5.\n\n> The heuristic I seem to be using is to treat objects as made up of a value plus a residual. So when they gave me the residual my next thought was “now all the objects are the same, so whatever I do to one I do to all of them”.\n\nI think that after reasoning my way through all these perspectives, I'm finally at the point where I have a quick, 'intuitive' understanding of the problem. But it's surprising how much work it was for such a simple bit of algebra.\n\n\n# Final thoughts\n\nRather than making any big conclusions, the main thing I wanted to demonstrate in this post is how *complicated* the story gets when you look at one problem in detail. I've written about [close reading][close] recently, and this has been something like a close reading of the bat and ball problem.\n\nFrederick's original paper on the Cognitive Reflection Test is in that generic social science style where you define a new metric and then see how it correlates with a bunch of other macroscale factors (either big social categories like gender or education level, or the results of other statistical tests that try to measure factors like time preference or risk preference). There's a strange indifference to the details of the test itself – at no point does he discuss why he picked those specific three questions, and there's no attempt to model what was making the intuitive-but-wrong answer appealing. \n\nThe later paper by Meyer, Spunt and Frederick is much more interesting to me, because it really starts to pick apart the specifics of the bat and ball problem. Is an easier question getting substituted? Can participants reproduce the correct question from memory? \n\nI learned the most from the individual responses, though. This is where you really get to see the variety of ways that people tackle the problem. Careful reflection definitely seems to improve the chance of a correct answer in general, but many of the responses don't really fit the neat 'fast vs slow' division of the original setup.\n\n\n[close]:https://drossbucket.wordpress.com/2018/11/04/a-braindump-on-derrida-and-close-reading/\n\n\n# Questions\n\nI'm interested in any comments on the post, but here are a few specific things I'd like to get your answers to:\n\n-  My rapid, intuitive answer for the bat and ball question is wrong (at least until I retrained it by thinking about the problem way too much). However, for the other two I 'just see' the correct answer. Is this common for other people, or do you have a different split?\n\n- If you're able to rapidly 'just see' the answer to the bat and ball question, how do you do it? \n\n- How do people go about designing tests like these? This isn't at all my field and I'd be interested in any good sources. I'd kind of assumed that there'd be some kind of serious-business Test Creation Methodology, but for the CRT at least it looks like people just noticed they got surprising answers for the bat and ball question and looked around for similar questions. Is that unusual compared to other psychological tests?"
    },
    "voteCount": 26
  },
  {
    "_id": "ovvwAhKKoNbfcMz8K",
    "url": null,
    "title": "On Expressing Your Concerns",
    "slug": "on-expressing-your-concerns",
    "author": "Eliezer_Yudkowsky",
    "question": false,
    "tags": [
      {
        "name": "Groupthink"
      },
      {
        "name": "Social Reality"
      },
      {
        "name": "Courage"
      },
      {
        "name": "Conformity Bias"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "The scary thing about Asch’s conformity experiments is that you can get many people to say black is white, if you put them in a room full of other people saying the same thing. The hopeful thing about Asch’s conformity experiments is that a single dissenter tremendously drove down the rate of conformity, even if the dissenter was only giving a different wrong answer. And the _wearisome_ thing is that dissent was not _learned_ over the course of the experiment—when the single dissenter started siding with the group, rates of conformity rose back up.\n\nBeing a voice of dissent can bring real benefits to the group. But it also (famously) has a cost. And then you have to keep it up. Plus you could be wrong.\n\nI recently had an interesting experience wherein I began discussing a project with two people who had previously done some planning on their own. I thought they were being too optimistic and made a number of safety-margin-type suggestions for the project. Soon a fourth guy wandered by, who was providing one of the other two with a ride home, and began making suggestions. At this point I had a sudden insight about how groups become overconfident, because whenever I raised a possible problem, the fourth guy would say, “Don’t worry, I’m sure we can handle it!” or something similarly reassuring.\n\nAn individual, working alone, will have natural doubts. They will think to themselves, “Can I really do XYZ?” because there’s nothing impolite about doubting your _own_ competence. But when two unconfident people form a group, it is polite to say nice and reassuring things, and impolite to question the other person’s competence. Together they become more optimistic than either would be on their own, each one’s doubts quelled by the other’s seemingly confident reassurance, not realizing that the other person initially had the same inner doubts.\n\nThe most fearsome possibility raised by Asch’s experiments on conformity is the specter of everyone agreeing with the group, swayed by the confident voices of others, careful not to let their own doubts show—not realizing that others are suppressing similar worries. This is known as “pluralistic ignorance.”\n\nRobin Hanson and I have a long-running debate over when, exactly, aspiring rationalists should dare to disagree. I tend toward the widely held position that you have no real choice but to form your own opinions. Robin Hanson advocates a more iconoclastic position, that _you_—not just other people—should consider that others may be wiser. Regardless of our various disputes, we both agree that Aumann’s Agreement Theorem extends to imply that common knowledge of a factual disagreement shows _someone_ must be irrational.^[1](#fn1x62)^ Despite the funny looks we’ve gotten, we’re sticking to our guns about modesty: Forget what everyone tells you about individualism, you _should_ pay attention to what other people think.\n\nAhem. The point is that, for rationalists, disagreeing with the group is serious business. You can’t wave it off with, “Everyone is entitled to their own opinion.”\n\nI think the most important lesson to take away from Asch’s experiments is to distinguish “expressing concern” from “disagreement.” Raising a point that others haven’t voiced is not a promise to disagree with the group at the end of its discussion.\n\nThe ideal Bayesian’s process of convergence involves sharing evidence that is unpredictable to the listener. The Aumann agreement result holds only for _common knowledge_, where you know, I know, you know I know, etc. Hanson’s [post](https://www.overcomingbias.com/2007/01/we_cant_foresee.html) or [paper](https://mason.gmu.edu/~rhanson/unpredict.pdf) on “We Can’t Foresee to Disagree” provides a picture of how strange it would look to watch ideal rationalists converging on a probability estimate; it doesn’t look anything like two bargainers in a marketplace converging on a price.\n\nUnfortunately, there’s not much difference _socially_ between “expressing concerns” and “disagreement.” A group of rationalists might agree to pretend there’s a difference, but it’s not how human beings are really wired. Once you speak out, you’ve committed a socially irrevocable act; you’ve become the nail sticking up, the discord in the comfortable group harmony, and you can’t undo that. Anyone insulted by a concern you expressed about their competence to successfully complete task XYZ will probably hold just as much of a grudge afterward if you say, “No problem, I’ll go along with the group,” at the end.\n\nAsch’s experiment shows that the power of dissent to inspire others is real. Asch’s experiment shows that the power of conformity is real. If everyone refrains from voicing their private doubts, that will indeed lead groups into madness. But history abounds with lessons on the price of being the first, or even the second, to say that the Emperor has no clothes. Nor are people hardwired to distinguish “expressing a concern” from “disagreement even with common knowledge”; this distinction is a rationalist’s artifice. If you read the more cynical brand of self-help books (e.g., Machiavelli’s _The Prince_) they will advise you to mask your nonconformity entirely, _not_ voice your concerns first and then agree at the end. If you perform the group service of being the one who gives voice to the obvious problems, don’t expect the group to thank you for it.\n\nThese are the costs and the benefits of dissenting—whether you “disagree” or just “express concern”—and the decision is up to you.\n\n^[1](#fn1x62-bk)^See “The Modesty Argument.” [http://lesswrong.com/lw/gr/the\\_modesty\\_argument](https://lesswrong.com/lw/gr/the_modesty_argument)."
    },
    "voteCount": 46
  },
  {
    "_id": "K4aGvLnHvYgX9pZHS",
    "url": null,
    "title": "The Fun Theory Sequence",
    "slug": "the-fun-theory-sequence",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "Well-being"
      },
      {
        "name": "Fun Theory"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "_(A shorter gloss of Fun Theory is \"[31 Laws of Fun](/lw/y0/31_laws_of_fun/)\", which summarizes the advice of Fun Theory to would-be Eutopian authors and futurists.)_\n\nFun Theory is the field of knowledge that deals in questions such as \"How much fun is there in the universe?\", \"Will we ever run out of fun?\", \"Are we having fun yet?\" and \"Could we be having more fun?\"\n\nMany critics (including [George Orwell](/lw/xl/eutopia_is_scary/)) have commented on the inability of authors to imagine Utopias where anyone would actually want to live.  If no one can imagine a Future where anyone would want to live, that may drain off motivation to work on the project.  The prospect of endless boredom is routinely fielded by conservatives as a knockdown argument against research on lifespan extension, against cryonics, against all transhumanism, and occasionally against the entire Enlightenment ideal of a better future.\n\nFun Theory is also the fully general reply to religious theodicy (attempts to justify why God permits evil).  Our present world has flaws even from the standpoint of such eudaimonic considerations as freedom, personal responsibility, and self-reliance.  Fun Theory tries to describe the dimensions along which a benevolently designed world can and should be optimized, and our present world is clearly _not_ the result of such optimization.  Fun Theory also highlights the flaws of any particular religion's perfect afterlife - you wouldn't want to go to their Heaven.\n\nFinally, going into the details of Fun Theory helps you see that eudaimonia is _complicated_ \\- that there are _many_ properties which contribute to a life worth living.  Which helps you appreciate just how worthless a galaxy would end up looking (with very high probability) if the galaxy was optimized by something with a utility function rolled up at random.  This is part of the [Complexity of Value Thesis](http://wiki.lesswrong.com/wiki/Complexity_of_value) and supplies motivation to create AIs with precisely chosen goal systems ([Friendly AI](http://wiki.lesswrong.com/wiki/Friendly_artificial_intelligence)).\n\nFun Theory is built on top of the naturalistic metaethics summarized in [Joy in the Merely Good](/lw/sx/inseparably_right_or_joy_in_the_merely_good/); as such, its arguments ground in \"On reflection, don't you think this is what you would actually want for yourself and others?\"\n\nPosts in the Fun Theory sequence (reorganized by topic, not necessarily in the original chronological order):\n\n*   **[Prolegomena to a Theory of Fun](/lw/wv/prolegomena_to_a_theory_of_fun/)**:  Fun Theory is an attempt to actually answer questions about eternal boredom that are more often posed and left hanging.  Attempts to visualize Utopia are often defeated by standard biases, such as the attempt to imagine a single moment of good news (\"You don't have to work anymore!\") rather than a typical moment of daily life ten years later.  People also believe they _should_ enjoy various activities that they actually don't.  But since human values have no supernatural source, it is quite reasonable for us to try to understand what we want.  There is no external authority telling us that the future of humanity should _not_ be fun.\n*   **[High Challenge](/lw/ww/high_challenge/)**:  Life should not always be made easier for the same reason that video games should not always be made easier.  Think in terms of eliminating low-quality work to make way for high-quality work, rather than eliminating all challenge.  One needs games that are fun to _play_ and not just fun to _win_.  Life's utility function is over 4D trajectories, not just 3D outcomes.  Values can legitimately be over the subjective experience, the objective result, and the challenging process by which it is achieved - the traveller, the destination and the journey.\n*   **[Complex Novelty](/lw/wx/complex_novelty/)**:  Are we likely to run out of _new_ challenges, and be reduced to playing the same video game over and over?  How large is Fun Space?  This depends on how fast you learn; the faster you generalize, the more challenges you see as similar to each other.  Learning is fun, but uses up fun; you can't have the same stroke of genius twice.  But the more intelligent you are, the more potential insights you can understand; human Fun Space is larger than chimpanzee Fun Space, and not just by a linear factor of our brain size.  In a well-lived life, you may need to increase in intelligence fast enough to integrate your accumulating experiences.  If so, the rate at which new Fun becomes available to intelligence, is likely to overwhelmingly swamp the amount of time you could spend at that fixed level of intelligence.  The Busy Beaver sequence is an infinite series of deep insights not reducible to each other or to any more general insight.\n*   **[Continuous Improvement](/lw/xk/continuous_improvement/)**:  Humans seem to be on a hedonic treadmill; over time, we adjust to any improvements in our environment - after a month, the new sports car no longer seems quite as wonderful.  This aspect of our evolved psychology is not surprising: is a rare organism in a rare environment whose optimal reproductive strategy is to rest with a smile on its face, feeling happy with what it already has.  To entirely delete the hedonic treadmill seems perilously close to tampering with Boredom itself.  Is there enough fun in the universe for a transhuman to jog off the treadmill - improve their life continuously, leaping to ever-higher hedonic levels before adjusting to the previous one?  Can ever-higher levels of pleasure be created by the simple increase of ever-larger floating-point numbers in a digital pleasure center, or would that fail to have the full subjective quality of happiness?  If we continue to bind our pleasures to novel challenges, can we find higher levels of pleasure fast enough, without cheating?  The rate at which _value_ can increase as more bits are added, and the rate at which value _must_ increase for eudaimonia, together determine the lifespan of a mind.  If minds must use exponentially more resources over time in order to lead a eudaimonic existence, their subjective lifespan is measured in mere millennia even if they can draw on galaxy-sized resources.\n*   **[Sensual Experience](/lw/wy/sensual_experience/)**:  Much of the anomie and disconnect in modern society can be attributed to our spending all day on tasks (like office work) that we didn't evolve to perform (unlike hunting and gathering on the savanna).  Thus, many of the tasks we perform all day do not _engage our senses_ \\- even the most realistic modern video game is not the same level of _sensual_ experience as outrunning a real tiger on the real savanna.  Even the best modern video game is _low-bandwidth fun_ \\- a low-bandwidth connection to a relatively simple challenge, which doesn't fill our brains well as a result.  But future entities could have different senses and higher-bandwidth connections to more complicated challenges, even if those challenges didn't exist on the savanna.\n*   **[Living By Your Own Strength](/lw/wz/living_by_your_own_strength/)**:  Our hunter-gatherer ancestors strung their own bows, wove their own baskets and whittled their own flutes.  Part of our alienation from our design environment is the number of tools we use that we don't understand and couldn't make for ourselves.  It's much less fun to read something in a book than to discover it for yourself.  Specialization is critical to our current civilization.  But the future does not have to be a continuation of this trend in which we rely more and more on things outside ourselves which become less and less comprehensible.  With a surplus of power, you could begin to rethink the life experience as a road to internalizing new strengths, not just staying alive efficiently through extreme specialization.\n*   **[Free to Optimize](/lw/xb/free_to_optimize/)**: _Stare decisis_ is the legal principle which binds courts to follow precedent.  The rationale is not that past courts were wiser, but _jurisprudence constante:_  The legal system must be _predictable_ so that people can implement contracts and behaviors knowing their implications.  The purpose of law is not to make the world perfect, but to provide a predictable environment in which people can optimize their own futures.  If an extremely powerful entity is choosing good futures on your behalf, that may leave little slack for you to navigate through your own strength.  Describing how an AI can avoid stomping your self-determination is a structurally complicated problem.  A simple (possibly not best) solution would be the gift of a world that works by improved rules, stable enough that the inhabitants could understand them and optimize their own futures together, but otherwise hands-off.  Modern legal systems fail along this dimension; no one can possibly know all the laws, let alone obey them.\n*   **[Harmful Options](/lw/x2/harmful_options/)**:  Offering people more choices that differ along many dimensions, may diminish their satisfaction with their final choice.  Losses are more painful than the corresponding gains are pleasurable, so people think of the dimensions along which their final choice was inferior, and of all the other opportunities passed up.  If you can only choose one dessert, you're likely to be happier choosing from a menu of two than from a menu of fourteen.  Refusing tempting choices consumes mental energy and decreases performance on other cognitive tasks.  A video game that contained an always-visible easier route through, would probably be less fun to play even if that easier route were deliberately foregone.  You can imagine a Devil who follows someone around, making their life miserable, solely by offering them options which are never actually taken.  And what if a worse option is taken due to a predictable mistake?  There are many ways to harm people by offering them more choices.\n*   [Devil's Offers](/lw/x3/devils_offers/):  It is dangerous to live in an environment in which a single failure of resolve, throughout your entire life, can result in a permanent addiction or in a poor edit of your own brain.  For example, a civilization which is constantly offering people tempting ways to shoot off their own feet - for example, offering them a cheap escape into eternal virtual reality, or customized drugs.  It requires a constant stern will that may not be much fun.  And it's questionable whether a superintelligence that descends from above to offer people huge dangerous temptations that they wouldn't encounter on their own, is _helping._\n*   [Nonperson Predicates](/lw/x4/nonperson_predicates/), [Nonsentient Optimizers](/lw/x5/nonsentient_optimizers/), [Can't Unbirth a Child](/lw/x7/cant_unbirth_a_child/):  Discusses some of the problems of, and justification for, creating AIs that are knowably _not_ conscious / sentient / people / citizens / subjective experiencers.  We don't want the AI's _models of_ people to _be_ people - we don't want conscious minds trapped helplessly inside it.  So we need how to tell that something is definitely not a person, and in this case, maybe we would like the AI itself to not be a person, which would simplify a lot of ethical issues if we could pull it off.  Creating a new intelligent species is not lightly to be undertaken from a purely _ethical_ perspective; if you create a new kind of _person_, you have to make sure it leads a life worth living.\n*   **[Amputation of Destiny](/lw/x8/amputation_of_destiny/)**:  C. S. Lewis's _Narnia_ has a problem, and that problem is the super-lion Aslan - who demotes the four human children from the status of main characters, to mere hangers-on while Aslan does all the work.  Iain Banks's _Culture_ novels have a similar problem; the humans are mere hangers-on of the superintelligent Minds.  We already have strong ethical reasons to prefer to create nonsentient AIs rather than sentient AIs, at least at first.  But we may also prefer in just a fun-theoretic sense that we not be overshadowed by hugely more powerful entities occupying a level playing field with us.  Entities with human emotional makeups should not be competing on a level playing field with superintelligences - either keep the superintelligences off the playing field, or design the smaller (human-level) minds with a different emotional makeup that doesn't mind being overshadowed.\n*   **[Dunbar's Function](/lw/x9/dunbars_function/)**:  Robin Dunbar's original calculation showed that the maximum human group size was around 150.  But a typical size for a hunter-gatherer band would be 30-50, cohesive online groups peak at 50-60, and small task forces may peak in internal cohesiveness around 7.  Our attempt to live in a world of _six billion_ people has many emotional costs:  We aren't likely to know our President or Prime Minister, or to have any significant influence over our country's politics, although we go on behaving as if we did.  We are constantly bombarded with news about improbably pretty and wealthy individuals.  We aren't likely to find a significant profession where we can be the best in our field.  But if intelligence keeps increasing, the number of personal relationships we can track will also increase, along with the natural degree of specialization.  Eventually there might be a single community of sentients that really was a single community.\n*   **[In Praise of Boredom](/lw/xr/in_praise_of_boredom/)**:  \"Boredom\" is an immensely subtle and important aspect of human values, nowhere near as straightforward as it sounds to a human.  We don't want to get bored with breathing or with thinking.  We do want to get bored with playing the same level of the same video game over and over.  We don't want changing the shade of the pixels in the game to make it stop counting as \"the same game\".  We want a steady stream of novelty, rather than spending most of our time playing the best video game level so far discovered (over and over) and occasionally trying out a different video game level as a new candidate for \"best\".  These considerations would _not_ arise in most utility functions in expected utility maximizers.\n*   [Sympathetic Minds](/lw/xs/sympathetic_minds/):  Mirror neurons are neurons that fire both when performing an action oneself, and watching someone else perform the same action - for example, a neuron that fires when you raise your hand or watch someone else raise theirs.  We _predictively_ model other minds by putting ourselves in their shoes, which is empathy.  But some of our desire to help relatives and friends, or be concerned with the feelings of allies, is expressed as _sympathy,_ feeling what (we believe) they feel.  Like \"boredom\", the human form of sympathy would _not_ be expected to arise in an arbitrary expected-utility-maximizing AI.  Most such agents would regard any agents in its environment as a special case of complex systems to be modeled or optimized; it would not _feel what they feel._\n*   **[Interpersonal Entanglement](/lw/xt/interpersonal_entanglement/)**:  Our sympathy with other minds makes our interpersonal relationships one of the most complex aspects of human existence.  Romance, in particular, is more complicated than being nice to friends and kin, negotiating with allies, or outsmarting enemies - it contains aspects of all three.  Replacing human romance with anything _simpler_ or _easier_ would decrease the _peak complexity of the human species_ \\- a major step in the wrong direction, it seems to me.  This is my problem with proposals to give people perfect, nonsentient sexual/romantic partners, which I usually refer to as \"catgirls\" (\"catboys\").  The human species does have a statistical sex problem: evolution has not optimized the average man to make the average woman happy or vice versa.  But there are less sad ways to solve this problem than both genders giving up on each other and retreating to catgirls/catboys.\n*   [Failed Utopia #4-2](/lw/xu/failed_utopia_42/):  A fictional short story illustrating some of the ideas in Interpersonal Entanglement above.  (Many commenters seemed to like this story, and some said that the ideas were easier to understand in this form.)\n*   [Growing Up is Hard](/lw/xd/growing_up_is_hard/):  Each piece of the human brain is optimized on the assumption that all the other pieces are working the same way they did in the ancestral environment.  Simple neurotransmitter imbalances can result in psychosis, and some aspects of Williams Syndrome are probably due to having a frontal cortex that is too large relative to the rest of the brain.  Evolution creates limited robustness, but often stepping outside the ancestral parameter box just breaks things.  Even if the first change works, the second and third changes are less likely to work as the total parameters get less ancestral and the brain's tolerance is used up.  A cleanly designed AI might improve itself to the point where it was smart enough to unravel and augment the human brain.  Or uploads might be able to make themselves smart enough to solve the increasingly difficult problem of not going slowly, subtly insane.  Neither path is easy.  There seems to be an irreducible residue of danger and difficulty associated with an adult version of humankind ever coming into being.  Being a transhumanist means wanting certain things; it doesn't mean you think those things are easy.\n*   [Changing Emotions](/lw/xe/changing_emotions/):  Creating new emotions seems like a desirable aspect of many parts of Fun Theory, but this is not to be trivially postulated.  It's the sort of thing best done with superintelligent help, and slowly and conservatively even then.  We can illustrate these difficulties by trying to translate the short English phrase \"change sex\" into a cognitive transformation of extraordinary complexity and many hidden subproblems.\n*   **[Emotional Involvement](/lw/xg/emotional_involvement/)**:  Since the events in video games have no actual long-term consequences, playing a video game is not likely to be nearly as _emotionally involving_ as much less dramatic events in real life.  The supposed Utopia of playing lots of cool video games forever, is life as a series of disconnected episodes with no lasting consequences.  Our current emotions are bound to activities that were subgoals of reproduction in the ancestral environment - but we now pursue these activities as independent goals regardless of whether they lead to reproduction.  (Sex with birth control is the classic example.)  A transhuman existence would need new emotions suited to the important short-term and long-term events of that existence.\n*   **[Serious Stories](/lw/xi/serious_stories/)**:  _Stories_ and _lives_ are optimized according to rather different criteria.  Advice on how to write fiction will tell you that \"stories are about people's pain\" and \"every scene must end in disaster\".  I once assumed that it was not possible to write any story about a successful Singularity because the inhabitants would not be in any pain; but something about the final conclusion that the post-Singularity world would contain _no_ stories worth telling seemed alarming.  Stories in which nothing ever goes wrong, are painful to read; would a life of endless success have the same painful quality?  If so, should we simply eliminate that revulsion via neural rewiring?  Pleasure probably _does_ retain its meaning in the absence of pain to contrast it; they are different neural systems.  The present world has an imbalance between pain and pleasure; it is much easier to produce severe pain than correspondingly intense pleasure.  One path would be to address the _imbalance_ and create a world with more pleasures, and free of the more grindingly destructive and pointless sorts of pain.  Another approach would be to eliminate pain _entirely_.  I feel like I prefer the former approach, but I don't know if it can last in the long run.\n*   **[Eutopia is Scary](/lw/xl/eutopia_is_scary/)**:  If a citizen of the Past were dropped into the Present world, they would be pleasantly surprised along at least some dimensions; they would also be horrified, disgusted, and _frightened._  This is not because our world has gone wrong, but because it has gone right.  A true Future gone _right_ would, realistically, be shocking to us along at least _some_ dimensions.  This may help explain why most literary Utopias fail; as George Orwell observed, \"they are chiefly concerned with avoiding fuss\".  Heavens are meant to sound like good news; political utopias are meant to show how neatly their underlying ideas work.  Utopia is reassuring, unsurprising, and dull.  Eutopia would be scary.  (Of course the vast majority of scary things are _not_ Eutopian, just entropic.)  Try to imagine a _genuinely better_ world in which you would be _out of place_ \\- _not_ a world that would make you smugly satisfied at how well all your current ideas had worked.  This proved to be a very important exercise when I tried it; it made me realize that all my old proposals had been optimized to _sound safe and reassuring_.\n*   [Building Weirdtopia](/lw/xm/building_weirdtopia/):  Utopia and Dystopia both confirm the moral sensibilities you started with; whether the world is a libertarian utopia of government non-interference, or a hellish dystopia of government intrusion and regulation, either way you get to say \"Guess I was right all along.\"  To break out of this mold, write down the Utopia, and the Dystopia, and then try to write down the Weirdtopia - an arguably-better world that zogs instead of zigging or zagging.  (Judging from the comments, this exercise seems to have mostly failed.)\n*   **[Justified Expectation of Pleasant Surprises](/lw/xo/justified_expectation_of_pleasant_surprises/)**:  A pleasant _surprise_ probably has a greater hedonic impact than being told about the same positive event long in advance - hearing about the positive event is good news in the moment of first hearing, but you don't have the gift actually in hand.  Then you have to wait, perhaps for a long time, possibly comparing the expected pleasure of the future to the lesser pleasure of the present.  This argues that if you have a choice between a world in which the same pleasant events occur, but in the first world you are told about them long in advance, and in the second world they are kept secret until they occur, you would prefer to live in the second world.  The importance of hope is widely appreciated - people who do not expect their lives to improve in the future are less likely to be happy in the present - but the importance of _vague_ hope may be understated.\n*   **[Seduced by Imagination](/lw/xp/seduced_by_imagination/)**:  Vagueness usually has a poor name in rationality, but the Future is something about which, in fact, we do not possess strong reliable specific information.  Vague (but justified!) hopes may also be _hedonically_ better.  But a more important caution for _today's_ world is that _highly specific_ pleasant scenarios can exert a dangerous power over human minds - suck out our emotional energy, make us forget what we don't know, and cause our mere actual lives to pale by comparison.  (This post is not about Fun Theory proper, but it contains an important warning about how _not_ to use Fun Theory.)\n*   [The Uses of Fun (Theory)](/lw/xc/the_uses_of_fun_theory/):  Fun Theory is important for replying to critics of human progress; for inspiring people to keep working on human progress; for refuting religious arguments that the world could possibly have been benevolently designed; for showing that religious Heavens show the signature of the same human biases that torpedo other attempts at Utopia; and for appreciating the great complexity of our values and of a life worth living, which requires a correspondingly strong effort of AI design to create AIs that can play good roles in a good future.\n*   **[Higher Purpose](/lw/xw/higher_purpose/)**:  Having a Purpose in Life consistently shows up as something that increases stated well-being.  Of course, the problem with trying to pick out \"a Purpose in Life\" in order to make yourself happier, is that this doesn't take you _outside_ yourself; it's still all about _you._  To find purpose, you need to turn your eyes outward to look at the world and find things there that you care about - rather than obsessing about the wonderful spiritual benefits _you're_ getting from helping others.  In today's world, most of the _highest-priority_ legitimate Causes consist of large groups of people in extreme jeopardy:  Aging threatens the old, starvation threatens the poor, extinction risks threaten humanity as a whole.  If the future goes right, many and perhaps all such problems will be solved - depleting the stream of victims to be helped.  Will the future therefore consist of self-obsessed individuals, with nothing to take them outside themselves?  I suggest, though, that even if there were no large groups of people in extreme jeopardy, we would still, looking around, find things outside ourselves that we cared about - friends, family; truth, freedom...  Nonetheless, if the Future goes sufficiently well, there will come a time when you could search the whole of civilization, and never find a single person so much in need of help, as dozens you now pass on the street.  If you do want to save someone from death, or help a great many people, then _act now_; your opportunity may not last, one way or another."
    },
    "voteCount": 49
  },
  {
    "_id": "N47M3JiHveHfwdbFg",
    "url": null,
    "title": "Hammertime Day 10: Murphyjitsu",
    "slug": "hammertime-day-10-murphyjitsu",
    "author": "alkjash",
    "question": false,
    "tags": [
      {
        "name": "Murphyjitsu"
      },
      {
        "name": "Exercises / Problem-Sets"
      },
      {
        "name": "Inner Simulator / Suprise-o-meter"
      }
    ],
    "tableOfContents": {
      "sections": [
        {
          "title": "Day 10: Murphyjitsu",
          "anchor": "Day_10__Murphyjitsu",
          "level": 1
        },
        {
          "title": "Inner Sim",
          "anchor": "Inner_Sim",
          "level": 2
        },
        {
          "title": "Welp Mentality",
          "anchor": "Welp_Mentality",
          "level": 2
        },
        {
          "title": "Murphyjitsu",
          "anchor": "Murphyjitsu",
          "level": 2
        },
        {
          "title": "Daily Challenge",
          "anchor": "Daily_Challenge",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        },
        {
          "anchor": "comments",
          "level": 0,
          "title": "11 comments"
        }
      ],
      "headingsCount": 7
    },
    "contents": {
      "markdown": "This is part 10 of 30 in the Hammertime Sequence. Click [here](https://radimentary.wordpress.com/2018/01/29/hammertime-day-1-bug-hunt/) for the intro.\n\n> Like, so pessimistic that _reality_ actually comes out better than you expected around as often and as much as it comes out worse. It’s actually really hard to be so pessimistic that you stand a decent chance of _undershooting_ real life.\n\nLater in the day I will put up an open thread about the first cycle of Hammertime.\n\nWe finish up the first cycle with another post on planning. Murphyjitsu is CFAR’s method for planning which asks us to try to be so pessimistic as to undershoot real life.\n\n  \n\nDay 10: Murphyjitsu\n===================\n\nMurphy’s Law states that anything that can go wrong will go wrong.\n\nFor our Mandarin-speaking readers, here’s a useful mnemonic: Murphy transliterates as 墨菲 (_mo fei_), which is homophonous to 莫非, “what if?” That’s why I think of Murphy’s Law as the What If Law.\n\nIn the course of making plans, Murphyjitsu is the practice of strengthening plans by repeatedly envisioning and defending against failure modes until you would be _shocked _to see it fail. Here’s the basic setup of Murphyjitsu:\n\n1.  Make a plan.\n2.  Imagine that you’ve passed the deadline and find out that the plan failed.\n3.  If you’re _shocked_ in this scenario, you’re done.\n4.  Otherwise, simulate the most likely failure mode, defend against it, and repeat.\n\nThe first important sub-skill of Murphyjitsu is Inner Sim – the ability for System 1 to simulate failure modes.\n\nInner Sim\n---------\n\nI have the suspicion that everyone is secretly a master at Inner Sim, the ability to instantly simulate failure. Imagine a friend declares to you their New Year’s Resolution: to write a novel, to go on a keto diet, to write a month-long sequence on instrumental rationality.\n\nNow, listen for that internal scoffing – your System 1 instantly proliferates the future with all manner of obstacles. That’s Inner Sim at work.\n\nIf you’re anything like me, Inner Sim is better at predicting other people’s failure modes than your own. The mental move that helps apply Inner Sim introspectively is essentially Outside View: take your plan and imagine another person made them. What will go wrong?\n\nWelp Mentality\n--------------\n\nInner Sim does surprisingly little on its own.\n\nI had a conversation with a rationalist friend (let’s call him “Alex”) that went something like this:\n\n> Alex: What’s bothering you?\n\nMe: I’ve been terribly unproductive. I’m procrastinating on fellowship essays … they’re due in two weeks, and every time I think about math these essays pop up in my head.\n\nAlex: Why?\n\nMe: I essentially finished them, but I still have to edit it. Copy-editing is so tedious, and every run I make through my writing, it looks even more awkward than it did the previous time.\n\nAlex: What do you predict will happen?\n\nMe: Well … I’m going to put the essays off until two days before the deadline, edit for ten minutes when I start feeling the pressure, and submit them. Until then, I won’t get any research done.\n\nAlex: So…?\n\nMe (shrugs): Sucks, right?\n\n_Alex breaks down in laughter._\n\nI call this Welp Mentality. Welp Mentality is noticing that your plans are likely to fail catastrophically, or run overtime, or take 10x as much effort as you thought, and then shrugging noncommittally. Welp.\n\nWelp Mentality is knowing and accepting as a fact of life that every build will release two months late. That you’ll end up half-assing problem sets and essays starting midnight before the deadline. That your current exercise plan will probably peter out. I had an old motto for Welp Mentality: “Due tomorrow? Do tomorrow.”\n\nMurphyjitsu\n-----------\n\nMurphyjitsu is the astounding notion that if you can predict a failure mode, you can _do something_ about it!\n\nIf your builds release two months late every time, you can move the release date, or cut features, or hire more engineers. If you know you’re only going to spend six hours on a problem set the night before the due date, at very least you might as well just set a six-hour Yoda Timer for it, do it at a convenient time, and submit whatever you end up with.\n\nIn my fellowship essay case, I decided to spend ten minutes editing and submit the thing immediately. The relief of getting two weeks of my life back was palpable.\n\nPick a plan you have for the near future. Murphyjitsu it. Pull out all the stops: Arrange social pressure to keep you on track. Double the amount of time you spend. Set calendar and phone reminders. Murphyjitsu only stops when you would be _shocked_ if the plan fails.\n\nDaily Challenge\n===============\n\nMurphyjitsu a central life goal. Are there glaring failure modes you haven’t defended against?"
    },
    "voteCount": 26
  },
  {
    "_id": "a5JAiTdytou3Jg749",
    "url": null,
    "title": "Pascal's Mugging: Tiny Probabilities of Vast Utilities",
    "slug": "pascal-s-mugging-tiny-probabilities-of-vast-utilities",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "Utility Functions"
      },
      {
        "name": "Decision Theory"
      },
      {
        "name": "Paradoxes"
      },
      {
        "name": "Pascal's Mugging"
      },
      {
        "name": "Infinities In Ethics"
      },
      {
        "name": "Solomonoff Induction"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "The most common [formalizations of Occam's Razor](/lw/jp/occams_razor/), Solomonoff induction and Minimum Description Length, measure the program size of a computation used in a hypothesis, but don't measure the running time or space requirements of the computation.  What if this makes a mind vulnerable to finite forms of Pascal's Wager?  A compactly specified wager can grow in size _much_ faster than it grows in complexity.  The utility of a Turing machine can grow much faster than its prior probability shrinks.\n\nConsider [Knuth's up-arrow notation](http://en.wikipedia.org/wiki/Knuth%27s_up-arrow_notation):\n\n*   3^3 = 3\\*3\\*3 = 27\n*   3^^3 = (3^(3^3)) = 3^27 = 3\\*3\\*3\\*3\\*3\\*3\\*3\\*3\\*3\\*3\\*3\\*3\\*3\\*3\\*3\\*3\\*3\\*3\\*3\\*3\\*3\\*3\\*3\\*3\\*3\\*3\\*3 = 7625597484987\n*   3^^^3 = (3^^(3^^3)) = 3^^7625597484987 = 3^(3^(3^(... 7625597484987 times ...)))\n\nIn other words:  3^^^3 describes an exponential tower of threes 7625597484987 layers tall.  Since this number can be computed by a simple Turing machine, it contains very little information and requires a very short message to describe.  This, even though writing out 3^^^3 in base 10 would require _enormously_ more writing material than there are atoms in the known universe (a paltry 10^80).\n\nNow suppose someone comes to me and says, \"Give me five dollars, or I'll use my magic powers from outside the Matrix to run a Turing machine that simulates and kills 3^^^^3 people.\"\n\nCall this Pascal's Mugging.\n\n\"Magic powers from outside the Matrix\" are easier said than done - we have to suppose that our world is a computing simulation run from within an environment that can afford simulation of arbitrarily large finite Turing machines, and that the would-be wizard has been spliced into our own Turing tape and is in continuing communication with an outside operator, etc.\n\nThus the Kolmogorov complexity of \"magic powers from outside the Matrix\" is larger than the mere English words would indicate.  Therefore the Solomonoff-inducted probability, two to the _negative_ Kolmogorov complexity, is exponentially tinier than one might naively think.\n\nBut, small as this probability is, it isn't anywhere _near_ as small as 3^^^^3 is large.  If you take a decimal point, followed by a number of zeros equal to the length of the Bible, followed by a 1, and multiply this unimaginably tiny fraction by 3^^^^3, the result is pretty much 3^^^^3.\n\nMost people, I think, envision an \"infinite\" God that is nowhere near as large as 3^^^^3.  \"Infinity\" is reassuringly featureless and blank.  \"Eternal life in Heaven\" is nowhere near as intimidating as the thought of spending 3^^^^3 years on one of those fluffy clouds.  The notion that the diversity of life on Earth springs from God's infinite creativity, sounds more plausible than the notion that life on Earth was created by a superintelligence 3^^^^3 bits large.  Similarly for envisioning an \"infinite\" God interested in whether women wear men's clothing, versus a superintelligence of 3^^^^3 bits, etc.\n\nThe original version of Pascal's Wager is easily dealt with by the gigantic multiplicity of possible gods, an Allah for every Christ and a Zeus for every Allah, including the \"Professor God\" who places only atheists in Heaven.   And since all the expected utilities here are allegedly \"[infinite](http://www.nickbostrom.com/ethics/infinite.pdf)\", it's easy enough to argue that they cancel out.  Infinities, being featureless and blank, are all the same size.\n\nBut suppose I built an AI which worked by some bounded analogue of Solomonoff induction - an AI sufficiently Bayesian to insist on calculating complexities and assessing probabilities, rather than just waving them off as \"large\" or \"small\".\n\nIf the probabilities of various scenarios considered did not _exactly_ cancel out, the AI's action in the case of Pascal's Mugging would be _overwhelmingly_ dominated by whatever tiny differentials existed in the various tiny probabilities under which 3^^^^3 units of expected utility were actually at stake.\n\nYou or I would probably wave off the whole matter with a laugh, planning according to the dominant mainline probability:  Pascal's Mugger is just a philosopher out for a fast buck.\n\nBut a silicon chip does not look over the code fed to it, assess it for reasonableness, and correct it if not.  An AI is not given its code like a human servant given instructions.  An AI _is_ its code.  What if a philosopher tries Pascal's Mugging on the AI for a joke, and the tiny probabilities of 3^^^^3 lives being at stake, override _everything_ else in the AI's calculations?   What is the mere Earth at stake, compared to a tiny probability of 3^^^^3 lives?\n\nHow do _I_ know to be worried by this line of reasoning?  How do _I_ know to [rationalize](/lw/ju/rationalization/) reasons a Bayesian shouldn't work that way?  A mind that worked strictly by Solomonoff induction would not know to rationalize reasons that Pascal's Mugging mattered less than Earth's existence.  It would simply go by whatever answer Solomonoff induction obtained.\n\nIt would seem, then, that I've implicitly declared my existence as a mind that does not work by the logic of Solomonoff, at least not the way I've described it.  What am I comparing Solomonoff's answer to, to determine whether Solomonoff induction got it \"right\" or \"wrong\"?\n\nWhy do I think it's unreasonable to focus my entire attention on the magic-bearing possible worlds, faced with a Pascal's Mugging?  Do I have an instinct to resist exploitation by arguments \"anyone could make\"?  Am I unsatisfied by any visualization in which the dominant mainline probability leads to a loss?  Do I drop sufficiently small probabilities from consideration entirely?  Would an AI that lacks these instincts be exploitable by Pascal's Mugging?\n\nIs it me who's wrong?  Should I worry more about the possibility of some Unseen Magical Prankster of very tiny probability taking this post literally, than about the fate of the human species in the \"mainline\" probabilities?\n\nIt doesn't feel to me like 3^^^^3 lives are _really_ at stake, even at very tiny probability.  I'd sooner question my grasp of \"rationality\" than give five dollars to a Pascal's Mugger because I thought it was \"rational\".\n\nShould we penalize computations with large space and time requirements?  This is a hack that solves the problem, but is it _true?_ Are computationally costly explanations less likely?  Should I think the universe is probably a coarse-grained simulation of my mind rather than real quantum physics, because a coarse-grained human mind is _exponentially_ cheaper than real quantum physics?  Should I think the galaxies are tiny lights on a painted backdrop, because that Turing machine would require less space to compute?\n\nGiven that, in general, a Turing machine can increase in utility vastly faster than it increases in complexity, how should an Occam-abiding mind avoid being dominated by tiny probabilities of vast utilities?\n\nIf I could formalize whichever internal criterion was telling me I didn't want this to happen, I might have an answer.\n\nI talked over a variant of this problem with Nick Hay, Peter de Blanc, and Marcello Herreshoff in summer of 2006.  I don't feel I have a satisfactory resolution as yet, so I'm throwing it open to any analytic philosophers who might happen to read Overcoming Bias."
    },
    "voteCount": 68
  },
  {
    "_id": "synsRtBKDeAFuo7e3",
    "url": null,
    "title": "Not for the Sake of Happiness (Alone)",
    "slug": "not-for-the-sake-of-happiness-alone",
    "author": "Eliezer_Yudkowsky",
    "question": false,
    "tags": [
      {
        "name": "Well-being"
      },
      {
        "name": "Happiness"
      },
      {
        "name": "Fuzzies"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "When I met the futurist Greg Stock some years ago, he argued that the joy of scientific discovery would soon be replaced by pills that could simulate the joy of scientific discovery.  I approached him after his talk and said, \"I agree that such pills are probably possible, but I wouldn't voluntarily _take them._\"\n\nAnd Stock said, \"But they'll be so much better that [the real thing won't be able to compete](/lw/h3/superstimuli_and_the_collapse_of_western/).  It will just be way more fun for you to take the pills than to do all the actual scientific work.\"\n\nAnd I said, \"I _agree_ that's possible, so I'll make sure never to take them.\"\n\nStock seemed genuinely surprised by my attitude, which genuinely surprised _me._\n\nOne often sees ethicists arguing as if all human desires are reducible, in principle, to the desire for ourselves and others to be happy.  (In particular, Sam Harris does this in _The End of Faith,_ which I just finished perusing - though Harris's reduction is more of a drive-by shooting than a major topic of discussion.)\n\nThis isn't the same as arguing whether [all happinesses can be measured on a common utility scale](/lw/kn/torture_vs_dust_specks/) \\- different happinesses might occupy different scales, or be otherwise non-convertible.  And it's not the same as [arguing that it's theoretically impossible to value anything other than your own psychological states](/lw/l4/terminal_values_and_instrumental_values/), because it's still permissible to care whether _other_ people are happy.\n\nThe question, rather, is whether we _should_ care about the things that _make_ us happy, apart from any happiness they bring.\n\nWe can easily list many cases of moralists going astray by caring about things besides happiness.  The various states and countries that still outlaw oral sex make a good example; these legislators would have been better off if they'd said, \"Hey, whatever turns you on.\"  But this doesn't show that _all_ values are reducible to happiness; it just argues that in _this particular_ _case_ it was an ethical mistake to focus on anything else.\n\nIt is an undeniable fact that we tend to do things that make us happy, but this doesn't mean we should regard the happiness as the _only_ reason for so acting.  First, this would make it difficult to explain how we could care about anyone else's happiness - how we could treat people as ends in themselves, rather than instrumental means of obtaining a warm glow of satisfaction.\n\nSecond, just because something is a consequence of my action doesn't mean it was the sole justification.  If I'm writing a blog post, and I get a headache, I may take an ibuprofen.  _One_ of the consequences of my action is that I experience less pain, but this doesn't mean it was the _only_ consequence, or even the most important reason for my decision.  I do value the state of not having a headache.  But I can value something for its own sake _and also_ value it as a means to an end.\n\nFor all value to be reducible to happiness, it's not enough to show that happiness is involved in most of our decisions - it's not even enough to show that happiness is the _most_ important consequent in _all_ of our decisions - it must be the _only_ consequent.  That's a tough standard to meet.  (I originally found this point in a Sober and Wilson paper, not sure which one.)\n\nIf I claim to value art for its own sake, then would I value art that no one ever saw?  A screensaver running in a closed room, producing beautiful pictures that no one ever saw?  I'd have to say no.  I can't think of any completely lifeless object that I would value as an end, not just a means.  That would be like valuing ice cream as an end in itself, apart from anyone eating it.  Everything I value, that I can think of, involves people and their experiences _somewhere_ along the line.\n\nThe best way I can put it, is that my moral intuition appears to require _both_ the objective and subjective component to grant full value.\n\nThe value of scientific discovery requires _both_ a genuine scientific discovery, and a person to take joy in that discovery.  It may seem difficult to disentangle these values, but the pills make it clearer.\n\nI would be disturbed if people retreated into holodecks and fell in love with mindless wallpaper.  I would be disturbed _even if they weren't aware it was a holodeck_, which is an important ethical issue if some agents can potentially transport people into holodecks and substitute zombies for their loved ones without their awareness.  Again, the pills make it clearer:  I'm not just concerned with my own awareness of the uncomfortable fact.  I wouldn't put myself into a holodeck even if I could take a pill to forget the fact afterward.  That's simply not where I'm trying to steer the future.\n\nI value freedom:  When I'm deciding where to steer the future, I take into account not only the subjective states that people end up in, but also whether they got there as a result of their own efforts.  The presence or absence of an external puppet master can affect my valuation of an otherwise fixed outcome.  Even if people wouldn't know they were being manipulated, it would matter to my judgment of how well humanity had done with its future.  This is an important ethical issue, if you're dealing with agents powerful enough to helpfully tweak people's futures without their knowledge.\n\nSo my values are not strictly reducible to happiness:  There are properties I value about the future that aren't reducible to activation levels in anyone's pleasure center; properties that are not _strictly_ reducible to subjective states even in principle.\n\nWhich means that my decision system has a _lot_ of [terminal values](/lw/l4/terminal_values_and_instrumental_values/), none of them strictly reducible to [anything else](/lw/l3/thou_art_godshatter/).  Art, science, love, lust, freedom, friendship...\n\nAnd I'm okay with that.  I value a life complicated enough to be challenging and aesthetic - not just the _feeling_ that life is complicated, but the _actual_ complications - so turning into a pleasure center in a vat doesn't appeal to me.  It would be a waste of humanity's potential, which I value actually fulfilling, not just having the feeling that it was fulfilled."
    },
    "voteCount": 90
  },
  {
    "_id": "bDnFhJBcLQvCY3vJW",
    "url": null,
    "title": "What Are Meetups Actually Trying to Accomplish?",
    "slug": "what-are-meetups-actually-trying-to-accomplish",
    "author": "mingyuan",
    "question": false,
    "tags": [
      {
        "name": "Community"
      },
      {
        "name": "Meetups & Local Communities (topic)"
      }
    ],
    "tableOfContents": {
      "sections": [
        {
          "title": "Introduction",
          "anchor": "Introduction",
          "level": 1
        },
        {
          "title": "Background",
          "anchor": "Background",
          "level": 2
        },
        {
          "title": "Goals",
          "anchor": "Goals",
          "level": 2
        },
        {
          "title": "Purpose",
          "anchor": "Purpose",
          "level": 1
        },
        {
          "title": "Value creation",
          "anchor": "Value_creation",
          "level": 1
        },
        {
          "title": "Creating new rationalists/EAs",
          "anchor": "Creating_new_rationalists_EAs",
          "level": 2
        },
        {
          "title": "Retaining existing rationalists/EAs and helping them grow",
          "anchor": "Retaining_existing_rationalists_EAs_and_helping_them_grow",
          "level": 2
        },
        {
          "title": "Moving to Major Hubs",
          "anchor": "Moving_to_Major_Hubs",
          "level": 2
        },
        {
          "title": "AI safety",
          "anchor": "AI_safety",
          "level": 2
        },
        {
          "title": "Things that make a meetup group work",
          "anchor": "Things_that_make_a_meetup_group_work",
          "level": 1
        },
        {
          "title": "At least one organizer who is very committed and cares a lot",
          "anchor": "At_least_one_organizer_who_is_very_committed_and_cares_a_lot",
          "level": 2
        },
        {
          "title": "Regulatory systems",
          "anchor": "Regulatory_systems",
          "level": 2
        },
        {
          "title": "Friendship",
          "anchor": "Friendship",
          "level": 2
        },
        {
          "title": "Other helpful but non-critical factors",
          "anchor": "Other_helpful_but_non_critical_factors",
          "level": 2
        },
        {
          "title": "Open questions",
          "anchor": "Open_questions",
          "level": 1
        },
        {
          "title": "Value",
          "anchor": "Value",
          "level": 2
        },
        {
          "title": "Centralized coordination",
          "anchor": "Centralized_coordination",
          "level": 2
        },
        {
          "title": "Rationalist group houses",
          "anchor": "Rationalist_group_houses",
          "level": 2
        },
        {
          "title": "Actionable next steps",
          "anchor": "Actionable_next_steps",
          "level": 1
        },
        {
          "title": "Creating a meetups system on LessWrong 2.0",
          "anchor": "Creating_a_meetups_system_on_LessWrong_2_0",
          "level": 2
        },
        {
          "title": "Centralized sharing of resources",
          "anchor": "Centralized_sharing_of_resources",
          "level": 2
        },
        {
          "title": "Rationality modules",
          "anchor": "Rationality_modules",
          "level": 2
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        },
        {
          "anchor": "comments",
          "level": 0,
          "title": "17 comments"
        }
      ],
      "headingsCount": 24
    },
    "contents": {
      "markdown": "_Disclaimer 1: I take responsibility for all opinions expressed here and note that they do not necessarily reflect the views of the people I interviewed or of my various employers._\n\n_Disclaimer 2: In this post I sometimes make claims on behalf of LW2. I am under the impression that this is mostly okay, but I want to clarify that I don't work for LessWrong._\n\n* * *\n\n**Introduction**\n----------------\n\n**Background**\n\nLessWrong meetups have been taking place for [almost a decade](https://wiki.lesswrong.com/wiki/NYC_meetup_group), and CFAR has [a long history](https://www.lesserwrong.com/posts/d28mWBMrFt8nwpXLp/starting-a-lw-meet-up-is-easy) of encouraging people to start their own groups. But despite their popularity and staying power in the rationalist community, no one has yet solved meetups beyond [the object level](https://wiki.lesswrong.com/mediawiki/images/c/ca/How_to_Run_a_Successful_Less_Wrong_Meetup_Group.pdf) \\- that is, I haven't seen anyone give explicit models of what meetup groups are supposed to accomplish, nor even systematically examine what they've accomplished in the past.\n\nI became interested in this a year ago, when I started my own meetup group and found that I was basically completely directionless, having not had much of a goal beyond 'start a meetup group'. In the intervening months, I’ve become increasingly interested in the theoretical underpinnings of the whole endeavor we call 'meetups', so in an attempt to figure out what the heck is going on, I interviewed members and organizers of eight rationality meetup groups from around the world and collected anecdata from dozens more.\n\n**Goals**\n\nI embarked on this project with the goal of answering two overarching questions:\n\n1.  What do meetups look like when they go right? What does it take to build a ‘successful’ rationality community, and is there a generalizable way to ensure a community’s success?\n2.  Why do some existential risk-focused people care about meetups, and should they? Is there a way to measure or determine what is being created by meetups?\n\nI feel that I've gone some of the way towards answering both of these questions, but that I'm definitely not all the way there yet. Though I think this write-up captures the most important parts of what I've learned, there is a lot of other potentially valuable information that I left out for the sake of brevity, so if you think any of my claims seem unsubstantiated, I encourage you to ask for further details.\n\nI intend to turn this post into a mostly static reference document for organizers, but there are hundreds of perspectives that I was not able to capture here, particularly from people who have many years of experience with this issue, so it’s far from complete. I would be very interested to hear from people with experience running or attending meetups who disagree with any of my claims, think I’ve missed something critically important, or have other suggestions on how this can be improved.\n\nNote: Because I'm trying to answer two separate questions, the following document contains both object-level advice for meetup organizers and higher-level discussion. Apologies if this is displeasing.\n\n**Purpose**\n-----------\n\nAs I discovered firsthand, the purpose of a rationality meetup group is not always immediately clear, and different groups may serve very different purposes. It’s important to figure out what the purpose of your particular group is, because you can’t optimize without something to optimize _for,_ and if you don't decide on a purpose ahead of time your group will be pulled in many contradictory directions and probably end up ineffectual ([h/t spiralingintocontrol](http://particularvirtue.blogspot.com/2017/05/building-community-institution-in-five.html)).\n\nSo decide: Are you there to learn about the world? To become better at thinking, or to help others become better at thinking? To make progress towards your goals using applied rationality? To just hang out with like-minded people? And are some of these goals _better_ than others – either in the sense that they will produce stabler groups, or in the sense that groups with these goals will have more impact on rationality and existential risk?\n\nIt seems that different groups of people may just want different things out of a meetup - and that’s fine, insofar as meetup groups are a social thing that exist primarily to serve the needs of their members. But I don’t work on meetups just because I want people to have friends (although that’s definitely a nice side-effect); I work on them because it seems that they have historically been able to produce people and outputs that have maybe marginally contributed to us being less likely to go extinct within the next couple of decades. So let’s try to figure that out.\n\n**Value creation**\n------------------\n\nI asked people about the counterfactual impact that participating in or running a meetup group has had on their lives, and more generally, what value they think their meetup creates. I list some of the most significant answers below.\n\n**Creating new rationalists/EAs**\n\nI will define this category to include people who became actively involved with the community and the ideas who:\n\n*   were originally brought along to a meetup as friends or dates\n*   attended a meetup because they had passively read HPMOR or SSC\n*   showed up from meetup.com\n*   were EAs who became rationalists (or vice-versa)\n\nIt is unclear how common each of these scenarios is, but it seems plausible that this might account for a large part of the value of meetups, so long as the group retains the rationalists that it creates and guides them along the right path.\n\n**Retaining existing rationalists/EAs and helping them grow**\n\nSpeaking from experience, rationality is something it’s pretty hard and sad to do alone, and it can be easy to lose sight of your motivation. People report that being part of a meetup group has made them more engaged with the rationality material, and has helped them sustain motivation for rationality/EA just by virtue of being around people who are doing the same thing.\n\nFor some people, being part of a meetup group also gives them space to make personal progress, either by regularly using applied rationality in a structured context, or just by being around people with similar cognitive styles who are motivated to help each other improve.\n\nFor organizers in particular, running a meetup gives them quick feedback loops on coming up with ideas, and can also help them get better at social modeling.\n\n_Sending people to CFAR workshops_\n\nMeetups send a lot of counterfactual people to CFAR workshops, which is probably good for their personal growth, increases their engagement with the community, and may cause some of them to work on existential risk.\n\n**Moving to Major Hubs**\n\nI know meetups causing people to move to the Bay is a controversial topic, but from my perspective, moving to a major hub is one of the best things a person can do in terms of expected impact on the existential risk landscape. It gives people the opportunity to work at aligned organizations, and to be around hundreds of like-minded people, which (in addition to its social benefits) allows people to find collaborators with whom to start new projects and organizations.\n\n**AI safety**\n\nSome local groups have projects or reading groups dedicated to learning about problems in AI alignment. I do not know enough about these groups to guess at their impact.\n\n**Things that make a meetup group work**\n----------------------------------------\n\nDue to different groups having different purposes, a lot of elements are highly variable, such that one group may have completely opposite needs and preferences to another along some axes. The following are things that seemed to be critically important for all groups, regardless of how little else they had in common.\n\n**At least one organizer who is very committed and cares a lot**\n\nIt seems pretty clear that no meetup group could exist at all without the efforts of one committed person. But while just one person is sufficient to get a group started, a group that relies entirely on the efforts of one person is in a very precarious position. Almost all successful meetups have at least two people in organizer-type positions, which makes the group much more robust to organizers’ personal setbacks.\n\nAs a meetup organizer, you should identify someone to be your second in command, who can run the meetup for a week if you’re sick or away. You should also try to give anyone who will listen the opportunity to take a leadership role of some sort. In addition to reducing your workload (and therefore reducing the chance that you will [burn out](https://www.lesserwrong.com/posts/mL7PJKu3NEkHLZ9vP/melting-gold-and-organizational-capacity)), handing off meaningful responsibilities to other people makes them more invested, strengthening the group as a whole.\n\n_Organizer desiderata_\n\nA successful organizer should…\n\n*   Be **reliable** – An organizer needs to consistently make things happen, or the group will die.\n*   **Care,** meaning:\n\n*   Be **steadily motivated** – An organizer should have confidence that what they’re trying to do is important and meaningful.\n*   **Believe in others** – A lot of rationalists have the potential to be high-impact, but need someone to believe in them more than they believe in themselves before they're ready to take action.\n\n*   Be **socially competent** – At the very least, an organizer should be welcoming to their members and have some capacity to facilitate discussion; it is also good if they can get people to come to events, and encourage people to get to know each other.\n*   Be **knowledgeable about rationality** – An organizer doesn’t necessarily need to be extremely well-versed in every facet of the rationality-sphere, but it is hard to run a group about rationality if you don’t know much about the subject.\n\n**Regulatory systems**\n\nIt is important for someone to be explicitly in charge of the group, so that that person has the authority to make unilateral changes when needed, to steer discussions, to address interpersonal conflicts before they blow up, etc. By default all such responsibilities fall to the meetup organizer, but this often fails because since they are in charge ‘by default’, organizers often don’t feel that they have much real authority.\n\nThe Seattle community solves this by having people with designated roles, such as Moderator or Welcomer. The New Hampshire community, which is much smaller, has a unanimously elected dictator.\n\n_Ejecting people_\n\nIt is very important for the long-term health of a meetup that those in charge be able to deal effectively with people who create problems, which may include asking them to leave the group. This issue came up in almost every single interview I conducted and has been raised elsewhere as well. Most organizers struggle with this because it feels mean and wrong to exclude people, and they often feel like they don’t have the power or right to do so because meetups are not a formal institution.\n\nWhy it’s important to eject some people:\n\n*   They can dramatically reduce the quality of the discussion, thus destroying the value of the meetup for everyone.\n*   They can introduce social conflict, which threatens the social cohesion and therefore the stability of the group.\n*   They can threaten the mission of the group by being overly negative, skeptical, or unwilling to engage.\n*   They can make others feel unsafe (e.g. neo-Nazis or misogynists), which makes those people less likely to come back, which may further lower the quality of the group, and will threaten its stability.\n\nThree classes of solutions to this kind of problem:\n\n*   **Right to ask.** A trusted member of the group (by default the organizer) should be imbued with the explicit authority to ask people not to return. Ejecting people should be done at this person’s discretion; decisions on this should not have to be unanimous.\n*   **Private venue.** It is very helpful if you have control over the environment where meetups take place. In interviews, the only groups that reported success in excluding problematic members were ones that meet in a private residence.\n*   **Exclusive by default.** For some groups, it might make sense to require people to apply for membership. This introduces the expectation that not everyone will be accepted, allowing you to decide a person is not a good fit for the group without it feeling too personal for either party. This model is an obvious one for things such as a class on rationality or a student group at a university, but should also be considered in a broader range of contexts.\n\n_Intentionality of discussions_\n\nIt is easy for a formal meetup to be wasted if no one has control of the conversation. You need an official moderator, someone who has the authority to interject if things are going off the rails and steer the discussion back in a more productive direction. By default, moderation falls to the organizer, but moderators can also be designated separately, as they are in the Seattle community.\n\nAnother thing that can be helpful here is [automoderation](http://ferocioustruth.com/2017/automoderation/), which is a system of hand signals for conversational moderation (chart [here](http://ferocioustruth.com/2017/automoderation/#chart)). This works best for smaller groups and less formal discussions.\n\n**Friendship**\n\nOne thing all successful long-term meetup groups have in common is that they often hang out together outside of the scheduled meetup time, or even live together. It can be useful to foster these friendships explicitly, especially if you’re the organizer. Go out of your way to set up one-on-one meetings or hangouts with new members, as well as making sure to talk to them at informal events.\n\n_Hold a combination of structured and unstructured meetups_\n\nIf your meetup has only formal events, you are less likely to develop strong friendships with the other members and will probably miss out on a lot of interesting and valuable conversations you could have had in an unstructured context. On the other hand, if your meetup has only informal events, the group will probably not accomplish much, and you may find retention difficult.\n\nModels for accomplishing this:\n\nThe Seattle Rationality Reading Group’s formal meetings are always followed by dinner. Other groups alternate week to week between formal/teaching events, and more casual social events. You can also mix it up by having events like Petrov Day ([this one can be automated!](http://lesswrong.com/lw/l06/petrov_day_is_september_26/)) or Secular Solstice (this one can’t!) that are structured but also are fun, encourage personal bonding, and remind you what we’re fighting for.\n\n_The people_\n\nIt seems almost-tautologically true that any value that comes out of a meetup is due to the people involved. Members of a meetup should inspire each other and help each other grow. A meetup will most likely produce nothing of value if it doesn’t have any members who [Actually Try](http://lesswrong.com/lw/uh/trying_to_try/), and meetups comprised of people who don't like or inspire one another are unlikely to last.\n\n**Other helpful but non-critical factors**\n\n_Visibility_\n\nAt the beginning of any group’s existence, it is important that it be able to attract enough new members to establish a core set of people who will reliably show up, or the group will die. However, this is not a first-order factor, because once a group reaches a stable point, it does not need to constantly bring in new members in order to survive.\n\n_Having multiple groups_\n\nIt seems to help stability to have multiple groups with different purposes and partial (but not complete) overlap in membership, because these different groups attract different members, making for a larger, more diverse community. For example, Berlin has a rationality dojo, a sequences reading group, social meetups, and circling, and used to have a machine learning reading group as well. These are not all run by the same person, which means that the community is not as vulnerable to the loss of one organizer, and all of the groups have slightly different cultures. This spread of groups is also good because each group has an explicit purpose.\n\nThere is an obvious catch-22 here where in order to have multiple stable groups, you need to already have a large enough community and good enough infrastructure to sustain multiple groups. However, despite the fact that only a community that is already pretty stable will have multiple groups, it does seem that the existence of multiple groups in turn works to strengthen the community.\n\n_Contact with other meetup groups_\n\nMeetup groups in Europe have benefitted from cooperation and collaboration, both at the annual LessWrong community weekend and in one-on-one group cooperation. I am not sure what the nature of the collaboration has been or what benefits have come of it, but more than one organizer has found collaboration valuable and has expressed the desire for more of it.\n\n**Open questions**\n------------------\n\n**Value**\n\n_What things can meetups create, beyond what they already have?_\n\nI did not explore this question at all, but it seems important to answer for the purposes of people and organizations that encourage the creation of meetup groups with the goal of mitigating existential risk.\n\n_Where does most of the value of a meetup come from?_\n\nAnecdotally, it’s possible that attending just one or two meetups can light a fire under someone, and after that their agency is activated enough that they quit their soulless day job and move across the country to work for a rationalist organization. So sometimes the value is heavily front-loaded, because sometimes it’s enough to just be told what’s out there.\n\nOther people stay with a community for many years before moving on, or are happy to just have their local community and have no intention of moving on at all. These people are probably getting something very different from meetups than the people in the previous paragraph. There are also all sorts of people between these extremes.\n\n**Centralized coordination**\n\n_What would a successful system for running meetups centrally look like?_\n\nMy goal is to make running meetups smoother and easier for anyone who is interested in doing so, but it’s very hard to know how to accomplish that. With the SlateStarCodex meetups, I tried to make it so that there was only one point of failure instead of dozens, by having the buck stop with me. However, this didn’t work very well because I was still relying on the other individual meetup organizers to respond to my requests for information, and also from the outside view this system was extremely unstable.\n\nMy ideas on this are not fleshed out, but I can imagine centralized coordination looking similar to other accountability systems - we could designate people within groups to hold one another accountable, or design incentive systems at a higher level. It is also possible we could help remotely with some steps of the process, possibly in a way that’s similar to what the Center for Effective Altruism is attempting to do with local EA groups.\n\n**Rationalist group houses**\n\n_What does a single group house do for a community?_\n\nNon-Bay Area rationalist houses are not common enough to be easy to gather data for, but I would be interested to know how rationalist group houses interact with and change their local communities, as well as how, why, and when they arise.\n\n**Actionable next steps**\n-------------------------\n\n**Creating a meetups system on LessWrong 2.0**\n\nThe new meetups system will be much more functional and easier to use than the system on the old LessWrong, and will aim to address the key points of failure in the system that have plagued meetup groups striving for visibility in the past. This will include the ability to set up recurring events, regular automated check-ins to ensure that the information is up-to-date, and a map that isn’t terrible, among many other features.\n\n**Centralized sharing of resources**\n\nIt would be helpful to meetup organizers – especially new organizers or people considering starting a meetup – to have centralized resources that tell them what to do and what to expect. Currently, the resources for running a meetup group are scattered across the LessWrong wiki, the closed LW Organizers Facebook group, and various other private channels, making them much less useful than they could be. We hope to aggregate these into a centralized location on the as-yet-non-existent LessWrong community page.\n\nIn addition to static resources, there will be a section on LessWrong dedicated solely to the discussion of meetups. This will solve the ‘people want to talk about this but no one really knows where to write it or where to find it’ problem. Discussion that people don’t want to be public can still take place on the LW Organizers Facebook group.\n\n**Rationality modules**\n\nSeveral organizers expressed the desire for some sort of lesson plans or ‘rationality modules’ they could present, because finding the time and mental capacity to create an activity or lesson plan every single week can be difficult and stressful, especially if you also have a full-time job.\n\n[The High Impact Network](https://web.archive.org/web/20210515172751/http://www.thehighimpactnetwork.org/modules) tried to create EA modules in the past and I've been told it didn’t go very well, but I know that a few meetup organizers have already created some materials that would help with this, so I haven't given up on the idea. I’m not yet sure how modules will be integrated into the meetups system, if at all.\n\n  \n  \n\n* * *\n\n  \n  \n\n_Acknowledgements_\n\n*   Thanks to all of the interviewees who took the time to answer my many questions! I've made you anonymous by default but let me know if you want to be credited by name.\n*   Thanks to deluks917 for inadvertently providing the impetus for this project, and for working with me on the SSC Meetups Everywhere report.\n*   Thanks to Harri Besceli (EA Groups Liaison at the Center for Effective Altruism) for discussing this project with me and helping me figure out what I was actually trying to do.\n*   Thanks to Oliver Habryka for making me feel like this project was actually worthwhile, for providing feedback, and also for building this website, that was cool."
    },
    "voteCount": 49
  },
  {
    "_id": "CPm5LTwHrvBJCa9h5",
    "url": null,
    "title": "Planning Fallacy",
    "slug": "planning-fallacy",
    "author": "Eliezer Yudkowsky",
    "question": false,
    "tags": [
      {
        "name": "Fallacies"
      },
      {
        "name": "Heuristics & Biases"
      },
      {
        "name": "Planning & Decision-Making"
      },
      {
        "name": "Inside/Outside View"
      },
      {
        "name": "Planning Fallacy"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "The Denver International Airport opened 16 months late, at a cost overrun of $2 billion.^1^\n\nThe Eurofighter Typhoon, a joint defense project of several European countries, was delivered 54 months late at a cost of $19 billion instead of $7 billion.\n\nThe Sydney Opera House may be the most legendary construction overrun of all time, originally estimated to be completed in 1963 for $7 million, and finally completed in 1973 for $102 million.^2^\n\nAre these isolated disasters brought to our attention by selective availability? Are they symptoms of bureaucracy or government incentive failures? Yes, very probably. But there’s also a corresponding cognitive bias, replicated in experiments with individual planners.\n\nBuehler et al. asked their students for estimates of when they (the students) thought they would complete their personal academic projects.^3^ Specifically, the researchers asked for estimated times by which the students thought it was 50%, 75%, and 99% probable their personal projects would be done. Would you care to guess how many students finished on or before their estimated 50%, 75%, and 99% probability levels?\n\n*   13% of subjects finished their project by the time they had assigned a 50% probability level;\n*   19% finished by the time assigned a 75% probability level;\n*   and only 45% (less than half!) finished by the time of their 99% probability level.\n\nAs Buehler et al. wrote, “The results for the 99% probability level are especially striking: Even when asked to make a highly conservative forecast, a prediction that they felt virtually certain that they would fulfill, students’ confidence in their time estimates far exceeded their accomplishments.”^4^\n\nMore generally, this phenomenon is known as the “planning fallacy.” The planning fallacy is that people think they can plan, ha ha.\n\nA clue to the underlying problem with the planning algorithm was uncovered by Newby-Clark et al., who found that\n\n*   Asking subjects for their predictions based on realistic “best guess” scenarios; and\n*   Asking subjects for their hoped-for “best case” scenarios . . .\n\n. . . produced *indistinguishable* results.^5^\n\nWhen people are asked for a “realistic” scenario, they envision everything going exactly as planned, with no *unexpected* delays or *unforeseen* catastrophes—the same vision as their “best case.”\n\nReality, it turns out, usually delivers results somewhat worse than the “worst case.”\n\nUnlike most cognitive biases, we know a good debiasing heuristic for the planning fallacy. It won’t work for messes on the scale of the Denver International Airport, but it’ll work for a lot of personal planning, and even some small-scale organizational stuff. Just use an “outside view” instead of an “inside view.”\n\nPeople tend to generate their predictions by thinking about the particular, unique features of the task at hand, and constructing a scenario for how they intend to complete the task—which is just what we usually think of as *planning*.\n\nWhen you want to get something done, you have to plan out where, when, how; figure out how much time and how much resource is required; visualize the steps from beginning to successful conclusion. All this is the “inside view,” and it doesn’t take into account unexpected delays and unforeseen catastrophes. As we saw before, asking people to visualize the “worst case” still isn’t enough to counteract their optimism—they don’t visualize enough Murphyness.\n\nThe outside view is when you deliberately *avoid* thinking about the special, unique features of this project, and just ask how long it took to finish *broadly* similar projects in the past. This is counterintuitive, since the inside view has so much more detail—there’s a temptation to think that a carefully tailored prediction, taking into account all available data, will give better results.\n\nBut experiment has shown that the more detailed subjects’ visualization, the more optimistic (and less accurate) they become. Buehler et al. asked an experimental group of subjects to describe highly specific plans for their Christmas shopping—where, when, and how.^6^ On average, this group expected to finish shopping more than a week before Christmas. Another group was simply asked when they expected to finish their Christmas shopping, with an average response of four days. Both groups finished an average of three days before Christmas.\n\nLikewise, Buehler et al., reporting on a cross-cultural study, found that Japanese students expected to finish their essays ten days before deadline. They actually finished one day before deadline. Asked when they had previously completed similar tasks, they responded, “one day before deadline.” This is the power of the outside view over the inside view.\n\nA similar finding is that experienced outsiders, who know less of the details, but who have relevant memory to draw upon, are often much less optimistic and much more accurate than the actual planners and implementers.\n\nSo there is a fairly reliable way to fix the planning fallacy, if you’re doing something *broadly* similar to a reference class of previous projects. Just ask how long similar projects have taken in the past, without considering *any* of the special properties of this project. Better yet, ask an experienced outsider how long similar projects have taken.\n\nYou’ll get back an answer that sounds hideously long, and clearly reflects no understanding of the special reasons why this particular task will take less time. This answer is true. Deal with it.\n\n* * *\n\n^1^ I’ve also seen $3.1 billion asserted.\n\n^2^ Roger Buehler, Dale Griffin, and Michael Ross, “Exploring the ‘Planning Fallacy’: Why People Underestimate Their Task Completion Times,” *Journal of Personality and Social* *Psychology* 67, no. 3 (1994): 366–381.\n\n^3^ Roger Buehler, Dale Griffin, and Michael Ross, “It’s About Time: Optimistic Predictions in Work and Love,” *European Review of Social Psychology* 6, no. 1 (1995): 1–32.\n\n^4^ Roger Buehler, Dale Griffin, and Michael Ross, “Inside the Planning Fallacy: The Causes and Consequences of Optimistic Time Predictions,” in *Heuristics and Biases: The Psychology of* *Intuitive Judgment*, ed. Thomas Gilovich, Dale Griffin, and Daniel Kahneman (New York: Cambridge University Press, 2002), 250–270.\n\n^5^ Ian R. Newby-Clark et al., “People Focus on Optimistic Scenarios and Disregard Pessimistic Scenarios While Predicting Task Completion Times,” *Journal of Experimental Psychology:* *Applied* 6, no. 3 (2000): 171–182.\n\n^6^ Buehler, Griffin, and Ross, [“Inside the Planning Fallacy.”](#cite.0.Buehler.2002)"
    },
    "voteCount": 131
  },
  {
    "_id": "pqoxE3AGMbse68dvb",
    "url": null,
    "title": "The Outside View's Domain",
    "slug": "the-outside-view-s-domain",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "Inside/Outside View"
      },
      {
        "name": "Rationality"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "**Followup to**:  [The Planning Fallacy](/lw/jg/planning_fallacy/)\n\nPlato's [Phaedo](http://classics.mit.edu/Plato/phaedo.html):\n\n>     \"The state of sleep is opposed to the state of waking; and out of sleeping, waking is generated; and out of waking, sleeping; and the process of generation is in the one case falling asleep, and in the other waking up.  Do you agree?\"  \n>     \"Quite.\"  \n>     \"Then suppose that you analyze life and death to me in the same manner.  Is not death opposed to life?\"  \n>     \"Yes.\"  \n>     \"And they are generated one from the other?\"  \n>     \"Yes.\"  \n>     \"What is generated from life?\"  \n>     \"Death.\"  \n>     \"And what from death?\"  \n>     \"I can only say in answer - life.\"  \n>     \"Then the living, whether things or persons, Cebes, are generated from the dead?\"  \n>     \"That is clear.\"  \n>     \"Then our souls exist in the house of Hades.\"  \n>     \"It seems so.\"\n\nNow suppose that the foil in the dialogue had objected a bit more strongly, and also that Plato himself had known about the standard research on the [Inside View vs. Outside View](/lw/jg/planning_fallacy/)...\n\n(As I disapprove of Plato's use of Socrates as his character mouthpiece, I shall let one of the characters be Plato; and the other... let's call him \"Phaecrinon\".)\n\n> Plato:  \"The state of sleep is opposed to the state of waking; and out of sleeping, waking is generated; and out of waking, sleeping; and the process of generation is in the one case falling asleep, and in the other waking up...  Then suppose that you analyze life and death to me in the same manner.\"\n> \n> Phaecrinon:  \"Why should I?  They are different things.\"\n> \n> Plato:  \"Oh, Phaecrinon, have you not heard what researchers have shown, that [the outside view is a better predictor than the inside view](/lw/jg/planning_fallacy/)? You come to me and point out the differences between life-death and wake-sleep, all so that you can avoid making the obvious generalization that you prefer to deny.  Yet if we allow such reasoning as this, will not software project managers say, 'My project is different from yours, because I have better programmers'?  And will not [textbook authors](/lw/jh/kahnemans_planning_anecdote/) say, \"We are wiser than those other textbook authors, and therefore we will finish sooner'?  Therefore you can see that to point out the similarities between things is superior, and to point out the differences between them, inferior.\"\n> \n> Phaecrinon:  \"You say that your reasoning is like to [the reasoning of Daniel Kahneman](/lw/jh/kahnemans_planning_anecdote/), yet it seems to me that they are importantly different.  For Daniel Kahneman dealt with generalization over things that are almost quite as similar to each other, as different flippings of the same coin.  Yet you deal with wholly different processes with different internal mechanisms, and try to generalize across one to the other.\"\n> \n> Plato:  \"But Phaecrinon, now you only compound your error; for you have pointed out the difference between myself and Kahneman, where I have pointed out the similarity.  And this is again inferior, by reason of the inferiority of the Inside View over the Outside View.  You have only given me one more special reason why the Outside View should not apply to _your_ particular case - all so that you can deny that our souls exist in the house of Hades.\"\n> \n> Phaecrinon:  \"Yet Plato, if you propose indiscriminately to apply the Outside View to all things, how do you explain the ability of engineers to construct a new bridge that is unlike any other bridge, and calculate its properties in advance by computer simulation?  How can the Wright Flyer fly, when all previous human-made flying machines had failed?  How indeed can anything at all happen for the first time?\"\n> \n> Plato:  \"Perhaps sometimes things do happen for the first time, but this does not mean we can predict them.\"\n> \n> Phaecrinon:  \"Ah, Plato, you do too little justice to engineers. Out of all the possible structures of metal and tubes and explosive fuel, very few such structures constitute a spaceship that will land on the Moon.  To land on the Moon for the first time, then, human engineers must have known, in advance, which of many designs would have the exceedingly rare property of landing upon the Moon.  And is this not the very activity that engineers perform - calculating questions _in detail?_  Do not engineers take the Inside View with great success?\"\n> \n> Plato:  \"But they assume that _each screw and part_ will behave just like it does on all the other times observed.\"\n> \n> Phaecrinon:  \"That is so.  Yet nonetheless they construct detailed internal models of exceeding complexity, and do _not_ only collect the statistics of whole cases.  This is the Inside View if anything is the Inside View; no one claims that Inside Views are generated purely from nothingness.\"\n> \n> Plato:  \"Then I answer that when engineers have shown many times their ability to perform detailed calculations with success, we trust on future occasions that they will succeed similarly.  We trust the Inside View when the Outside View tells us to trust the Inside View.  But if this is not so, and there is not a past record of success with detailed calculations, then the Outside View is all that is left to us; and we should foresake all attempts at internal modeling, for they will only lead us astray.\"\n> \n> Phaecrinon:  \"But now you have admitted that the notion of 'trust Outside View, distrust Inside View' has a limited _domain of applicability_, and we may as well restrict that domain further.  Just as you try to seal off the _successes_ of engineers from the Outside View, so too, I wish to seal off the _failures_ of Greek philosophers from the Outside View.  Specifically, the record of Greek philosophers does not inspire in me any confidence that the Outside View can be applied across processes with greatly different internal causal structures, like life-and-death versus sleeping-and-waking.  Daniel Kahneman and his fellows, writing a textbook, encountered a challenge drawn from a _structurally_ similar _causal generator_ as many other cases of textbook-writing; subject to just the same sort of unforeseen delays.  Likewise the students who failed to predict when they would finish their Christmas shopping; the task of Christmas shopping does not change so much from one Christmas to another.  It would be another matter entirely to say, 'Each year I have finished my Christmas shopping one day before Christmas - therefore I expect to finish my textbook one day before my deadline.'\"\n> \n> Plato:  \"But this only sounds foolish, because we _know_ from the Outside View that textbooks are delayed far longer than this. Perhaps if you had _never_ written a textbook before, and neither had anyone else, 'one day before deadline' would be the most reasonable estimate.\"\n> \n> Phaecrinon:  \"You would not allow me to predict in advance that textbook writing is more difficult than Christmas shopping?\"\n> \n> Plato:  \"No.  For you have chosen this _particular_ special plea, using your hindsight of the correct answer.  If you had truly needed to write a textbook for the first time in history, you would have pled, 'No one can foresee driving delays and crowds in the store, but the work I do to write a textbook is all under my own control - therefore, I will finish _more_ than one day before deadline.'\"\n> \n> Phaecrinon:  \"But even you admit that to draw analogies across wider and wider differences is to make those analogies less and less reliable.  If you see many different humans sleeping, you can conclude that a newly observed human will probably sleep for eight hours; but if you see a cat sleeping, you must be less confident; and if you wish to draw an analogy to life and death, that is a greater distance still.\"\n> \n> Plato:  \"If I allow that, will not software project managers say, 'My software project is as unlike to all other software projects as is a cat to a human?'\"\n> \n> Phaecrinon:  \"Then they are fools and nothing can be done about it.  Surely you do not think that the prediction from many humans to one cat is _just as strong_ as the prediction from many humans to one human?  Insensitivity to the reliability of predictors is also a standard bias.\"\n> \n> Plato:  \"That is true.  Yet an Outside View may not be a _good_ estimate, and yet still be the _best_ estimate.  If we have only seen the sleep cycles of many humans, then the Outside View on the whole group may be the _best_ estimate for a newly observed cat, if you have no other data.  Even likewise with our guesses as to life and death.\"\n> \n> Phaecrinon:  \"And one sign of when the Outside View might not provide a good estimate, is when there are many different reference classes to which you might compare your new thing.  A candle burns low, and exhausts itself and extinguishes, and does not light again the next day. How do you know that life and death is not analogous to a candle which burns and fails?  Why not generalize over the similarity to a candle, rather than the similarity to sleep cycles?\"\n> \n> Plato:  \"Oh, but Phaecrinon, if we allow arguments over reference classes, we may as well toss the notion of an Outside View out the window.  For then software project managers will say that the proper reference class for _their_ project is the class of projects that delivered on time, or the class of projects with managers as wise as themselves.  As for your analogy of the candle, it is self-evident that life is similar to sleeping and waking, not to candles.  When a man is born, he is weak, but he grows to adulthood and is strong, and then with age he grows weaker again.  In this he is like the Sun, that is weak when it rises, and strong at its apex, and then sinks below the horizon; in this a man is like the sleepy riser, who becomes sleepy again at the end of the day.  It is self-evident that life and death belongs to the class of cyclical processes, and not the class of irreversible processes.\"\n> \n> Phaecrinon:  \"What is self-evident to you does not seem so self-evident to me, Plato; and just because you call several widely different things 'cyclical processes', it does not follow that they were all random samples drawn from a great Barrel of Cyclical Processes, and that the next thing you choose to call a 'cyclical process' will have the same distribution of properties.\"\n> \n> Plato:  \"Again you compound your mistake by pleading special exceptions.  Will we let the software manager plead that his project is not drawn from the same barrel as the others?\"\n> \n> Phaecrinon:  \"Again you extend the Outside View beyond its domain of applicability.  In engineering where all internal parts are precisely understood, but the whole is not quite similar to anything else that has been built before, then the Inside View is superior to the Outside View.  And the sign of this is that results are routinely predicted in advance with great precision.\"\n> \n> Plato:  \"This is just to say that when the Outside View tells us to use the Inside View, we should use it.  But surely not otherwise, Phaecrinon!\"\n> \n> Phaecrinon:  \"When many different people try to accomplish the same task, and the internal details cannot be precisely calculated, and yet people have a tendency to optimism and to not visualize incidental catastrophes, _then_ the Outside View is superior to the Inside View.  And the sign of this is that the same kind of task - with the same sort of internal structure, the same difficulties and challenges - has been done many times, and the result cannot be predicted with precision; yet people's predictions are usually biased optimistically.\"\n> \n> Plato:  \"This is the triumph of the Outside View!\"\n> \n> Phaecrinon:  \"But when you deal with attempted analogies across _structually different_ processes, perhaps unique or poorly understood, then things which are similar in some surface respects are often different in other respects.  And the sign of _this_ domain is that when people try to reason by similarity, it is not at all clear what is similar to what, or which surface resemblances they should focus upon as opposed to others.\"\n> \n> Plato:  \"_I_ think the resemblance of life-death to sleep-waking is perfectly clear.  But what do _you_ assert we should do in such a case, if it is not taking the Outside View?\"\n> \n> Phaecrinon:  \"Perhaps there is nothing to be done at all, with either the Inside View or the Outside View.  Not all problems are solvable; and it may be that the best we can do is avoid the overconfidence from asserting that analogies are much stronger than they are.  But it seems to me that _in_ those cases where we know something of the internal structure, then we can sometimes produce predictions by imagining the internals, even though the whole thing is not similar to any other whole thing in our experience.\"\n> \n> Plato:  \"Now I challenge _you_ to consider how well such thoughts have done, historically.\"\n> \n> Phaecrinon:  \"What I have just described is the way that engineers build the first prototype of anything.  But that, I admit, is when they understand very precisely the parts they use.  If the internals are not well-understood, then the whole will in most cases be even less well-understood.  It is only your idea that the Outside View can yield _better_ predictions, that I am protesting against.  It seems to me that the result of taking the Outside View of things poorly understood or structurally dissimilar to other things in the purported reference class, is only to create great disputes about [definitional boundaries](/lw/o0/where_to_draw_the_boundary/), and clashing analogies, and arguments over which surface similarities are important.  When all along the new process may not be similar to _anything_ that already exists.\"\n> \n> Plato:  \"But there is no alternative to the Outside View.\"\n> \n> Phaecrinon:  \"Yes, there is; you can try to imagine the internal process, if you know anything at all about it.  At least then two people can focus on the internal structure and argue about _what happens_ and their dispute will be commensurable.  But if two people both say 'I am taking the Outside View' and then form different 'self-evident' reference classes, what do they do from there?  How can they resolve their dispute about which surface characteristics are important?  At least if you make predictions about internal causal processes, the results are finally testable if the dispute is empirical at all.  How do you test the assertion that life is _more importantly similar_ to sleep and waking, than to a candle?  Perhaps life is simply like neither.  _Something_ must happen internally when a human thinks and reasons, but there does _not_ have to exist _any_ other process in nature similar enough that we could predict the characteristics of human thought by looking at it.\"\n> \n> Plato:  \"What it boils down to, is that you are constructing a detailed excuse not to use the Outside View in your own case.\"\n> \n> Phaecrinon:  \"And if each of two people with different Outside Views says to the other, 'You are a fool, for disregarding the Outside View!' then they will make no progress on their disagreement at all.  This is the danger of proposing an absolute mandate for philosophers encountering new and structurally different phenomena, because you want to prevent software project managers from making special excuses for their software project.  Reversed stupidity is not intelligence, and there is no language in which it is difficult to write bad computer programs, and in the art of rationality it is never difficult to shoot off your own foot if you desire to do so.  The standard Outside View relies on your seeing the common-sense difference between textbook writing and Christmas shopping, so that you don't try to lump them into the same reference class.  I am similarly hoping that you can see by common sense that the Outside View works rather better to predict Christmas shopping times, than what you are arguing is the analogous 'Outside View' technique in philosophy.\"\n> \n> Plato:  \"And _you_ believe you can do better with the Inside View.\"\n> \n> Phaecrinon:  \"Reasoning about the internals of things whose output is not yet observed, is fraught with difficulty.  One must be constantly aware of what one can and cannot reasonably guess, based on the strength of your knowledge.  The uncertainties of such an Inside View, end up being much greater than the uncertainties of the Outside View _on Christmas shopping_.  Only when the Inside View support appears extremely lopsided can you dare to come to even a tentative conclusion!  But I _do_ think that sometimes the Inside View support can be extremely lopsided - though it is a strain on your rationality even to correctly distinguish such cases.\"\n> \n> Plato:  \"The evidence shows that people cannot successfully use the Inside View at all.\"\n> \n> Phaecrinon:  \"No, the evidence shows that the Outside View yields _better_ answers than the Inside View for problems like writing a textbook.  But even an Inside View of writing a textbook would tell you that the project was unlikely to destroy the Earth.  Taking the Inside View of a new and strange process is a Difficult Problem, where taking the Outside View on textbook composition is a Straightforward Problem.  But to try and argue like alchemists from surface resemblances is a Hopeless Problem.  Then there cannot even be any meeting of minds, if you start with different assumptions about which similarities are important.  An answer need not exist even in principle, for there may be _nothing_ else that is enough like this new thing to yield _successful_ predictions by analogy.\"\n> \n> Plato:  \"So you have said that it is easier for two people to conduct their dispute if they both take the Inside View and argue about internal causal processes.  But from this it does not follow that the Outside View based on surface resemblances is inferior.  Perhaps you are only coming to agreement on folly, and either of two conflicting Outside Views would be more reliable than the best Inside View.\""
    },
    "voteCount": 22
  },
  {
    "_id": "iyRpsScBa6y4rduEt",
    "url": null,
    "title": "Model Combination and Adjustment",
    "slug": "model-combination-and-adjustment",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "Inside/Outside View"
      },
      {
        "name": "Rationality"
      }
    ],
    "tableOfContents": {
      "sections": [
        {
          "title": "Inside and outside views: a quick review",
          "anchor": "Inside_and_outside_views__a_quick_review",
          "level": 1
        },
        {
          "title": "Multiple reference classes",
          "anchor": "Multiple_reference_classes",
          "level": 1
        },
        {
          "title": "Model combination and adjustment",
          "anchor": "Model_combination_and_adjustment",
          "level": 1
        },
        {
          "title": "Against \"the outside view\"",
          "anchor": "Against__the_outside_view_",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        },
        {
          "anchor": "comments",
          "level": 0,
          "title": "41 comments"
        }
      ],
      "headingsCount": 6
    },
    "contents": {
      "markdown": "[![](http://commonsenseatheism.com/wp-content/uploads/2013/07/Combining-classifiers.jpg)](http://www.scholarpedia.org/article/Ensemble_learning)The debate on the [proper use](/lw/gvk/induction_or_the_rules_and_etiquette_of_reference/) of inside and outside views has raged for some time now. I suggest a way forward, building on a family of methods commonly used in statistics and machine learning to address this issue — an approach I'll call \"model combination and adjustment.\"\n\n### Inside and outside views: a quick review\n\n**1**. There are two ways you might predict outcomes for a phenomenon. If you make your predictions using a detailed visualization of how something works, you're using an _inside view_. If instead you ignore the details of how something works, and instead make your predictions by assuming that a phenomenon will behave roughly like other similar phenomena, you're using an _outside view_ (also called _reference class forecasting_).\n\nInside view examples:\n\n*   \"When I break the project into steps and visualize how long each step will take, it looks like the project will take 6 weeks\"\n*   \"When I combine what I know of physics and computation, it looks like the serial speed formulation of Moore's Law will break down around 2005, because we haven't been able to scale down energy-use-per-computation as quickly as we've scaled up computations per second, which means the serial speed formulation of Moore's Law will run into roadblocks from energy consumption and heat dissipation somewhere around 2005.\"\n\nOutside view examples:\n\n*   \"I'm going to ignore the details of this project, and instead compare my project to similar projects. Other projects like this have taken 3 months, so that's probably about how long my project will take.\"\n*   \"The serial speed formulation of Moore's Law has held up for several decades, through several different physical architectures, so it'll probably continue to hold through the next shift in physical architectures.\"\n\nSee also chapter 23 in [Kahneman (2011)](http://www.amazon.com/Thinking-Fast-Slow-Daniel-Kahneman/dp/0374533555/); [Planning Fallacy](/lw/jg/planning_fallacy/); [Reference class forecasting](http://en.wikipedia.org/wiki/Reference_class_forecasting). Note that, after several decades of past success, the serial speed formulation of Moore's Law did in fact break down in 2004 for the reasons described ([Fuller & Millett 2011](http://www.inf.pucrs.br/~moraes/prototip/artigos/ComputingPerformanceGameOverorNextLevel.pdf)).\n\n**2**. An outside view works best when using a reference class with a _similar causal structure_ to the thing you're trying to predict. An inside view works best when a phenomenon's causal structure is well-understood, and when (to your knowledge) there are very few phenomena with a similar causal structure that you can use to predict things about the phenomenon you're investigating. See: [The Outside View's Domain](/lw/ri/the_outside_views_domain/).\n\nWhen writing a textbook that's much like other textbooks, you're probably best off predicting the cost and duration of the project by looking at similar textbook-writing projects. When you're predicting the trajectory of the serial speed formulation of Moore's Law, or predicting which spaceship designs will successfully land humans on the moon for the first time, you're probably best off using an (intensely _informed_) inside view.\n\n**3**. Some things aren't very predictable with _either_ an outside view or an inside view. Sometimes, the thing you're trying to predict seems to have a significantly different causal structure than other things, _and_ you don't understand its causal structure very well. What should we do in such cases? This remains a matter of debate.\n\nEliezer Yudkowsky recommends a [weak inside view](/lw/vz/the_weak_inside_view/) for such cases:\n\n> On problems that are drawn from a barrel of causally similar problems, where human optimism runs rampant and unforeseen troubles are common, the Outside View beats the Inside View... \\[But\\] on problems that are new things under the Sun, where there's a huge change of context and a structural change in underlying causal forces, the Outside View also fails - try to use it, and you'll just get into arguments about what is the proper domain of \"similar historical cases\" or what conclusions can be drawn therefrom. In this case, the best we can do is use the Weak Inside View — visualizing the causal process — to produce _loose qualitative conclusions about only those issues where there seems to be lopsided support_.\n\nIn contrast, Robin Hanson [recommends](http://www.overcomingbias.com/2008/12/test-near-apply.html) an outside view for difficult cases:\n\n> It is easy, way too easy, to generate new mechanisms, accounts, theories, and abstractions. To see if such things are _useful_, we need to vet them, and that is easiest \"nearby\", where we know a lot. When we want to deal with or understand things \"far\", where we know little, we have little choice other than to rely on mechanisms, theories, and concepts that have worked well near. Far is just the wrong place to try new things.\n> \n> There are a bazillion possible abstractions we could apply to the world. For each abstraction, the question is not whether one _can_ divide up the world that way, but whether it \"carves nature at its joints\", giving _useful_ insight not easily gained via other abstractions. We should be wary of inventing new abstractions just to make sense of things far; we should insist they first show their value nearby.\n\nIn [Yudkowsky (2013)](http://intelligence.org/files/IEM.pdf), sec. 2.1, Yudkowsky offers a reply to these paragraphs, and continues to advocate for a weak inside view. He also adds:\n\n> the other major problem I have with the “outside view” is that everyone who uses it seems to come up with a different reference class and a different answer.\n\nThis is the problem of \"[reference class tennis](/lw/1p5/outside_view_as_conversationhalter/)\": each participant in the debate claims their own reference class is most appropriate for predicting the phenomenon under discussion, and if disagreement remains, they might each say \"I’m taking my reference class and going home.\"\n\nResponding to the same point made [elsewhere](/lw/cze/reply_to_holden_on_tool_ai/), Robin Hanson [wrote](http://www.overcomingbias.com/2013/02/foom-debate-again.html):\n\n> \\[Earlier, I\\] warned against over-reliance on “unvetted” abstractions. I wasn’t at all trying to claim there is one true analogy and all others are false. Instead, I argue for preferring to rely on abstractions, including categories and similarity maps, that have been found useful by a substantial intellectual community working on related problems.\n\n###   \n\n### Multiple reference classes\n\nYudkowsky (2013) adds one more complaint about reference class forecasting in difficult forecasting circumstances:\n\n> A final problem I have with many cases of 'reference class forecasting' is that... \\[the\\] final answers \\[generated from this process\\] often seem more specific than I think our state of knowledge should allow. \\[For example,\\] I don’t think you _should_ be able to tell me that the next major growth mode will have a doubling time of between a month and a year. The alleged outside viewer claims to know too much, once they stake their all on a single preferred reference class.\n\nBoth this comment and Hanson's last comment above point to the vulnerability of relying on any _single_ reference class, at least for difficult forecasting problems. [Beware brittle arguments](http://rationalaltruist.com/2013/05/08/beware-brittle-arguments/), says Paul Christiano.\n\nOne obvious solution is to use _multiple_ reference classes, and weight them by how relevant you think they are to the phenomenon you're trying to predict. Holden Karnofsky writes of investigating things from \"[many different angles](http://blog.givewell.org/2011/11/10/maximizing-cost-effectiveness-via-critical-inquiry/).\" Jonah Sinick refers to \"[many weak arguments](/lw/hmb/many_weak_arguments_vs_one_relatively_strong/).\" Statisticians call this \"[model combination](http://commonsenseatheism.com/wp-content/uploads/2013/07/Xu-Golay-Survey-of-model-selection-and-model-combination.pdf).\" Machine learning researchers call it \"[ensemble learning](http://www.scholarpedia.org/article/Ensemble_learning)\" or \"[classifier combination](http://www.acsu.buffalo.edu/~tulyakov/papers/tulyakov_MLDAR_comb_review.pdf).\"\n\nIn other words, we can use _many_ outside views.\n\nNate Silver does this when he predicts elections (see [Silver 2012](http://www.amazon.com/The-Signal-Noise-Many-Predictions/dp/159420411X/), ch. 2). Venture capitalists do this when they evaluate startups. The best political forecasters studied in [Tetlock (2005)](http://www.amazon.com/Expert-Political-Judgment-Good-Know/dp/0691128715/), the \"foxes,\" tended to do this.\n\nIn fact, most of us do this regularly.\n\nHow do you predict which restaurant's food you'll most enjoy, when visiting San Francisco for the first time? One outside view comes from the restaurant's Yelp reviews. Another outside view comes from your friend Jade's opinion. Another outside view comes from the fact that you usually enjoy Asian cuisines more than other cuisines. And so on. Then you _combine_ these different models of the situation, weighting them by how robustly they each tend to predict your eating enjoyment, and you grab a taxi to [Osha Thai](http://www.oshathai.com/).\n\n(Technical note: I say \"model combination\" rather than \"model averaging\" [on purpose](http://synapse.cs.byu.edu/papers/Kristine.ijcnn2011.pdf).)\n\n###   \n\n### Model combination and adjustment\n\nYou can probably do even better than this, though — if you know some things about the phenomenon and you're very careful. Once you've combined a handful of models to arrive at a qualitative or quantitative judgment, you should still be able to \"adjust\" the judgment in some cases using an inside view.\n\nFor example, suppose I used the above process, and I plan to visit Osha Thai for dinner. Then, somebody gives me my first taste of the _[Synsepalum dulcificum](http://en.wikipedia.org/wiki/Synsepalum_dulcificum)_ fruit. I happen to know that this fruit contains a molecule called [miraculin](http://en.wikipedia.org/wiki/Miraculin) which binds to one's tastebuds and makes sour foods taste sweet, and that this effect lasts for about an hour ([Koizumi et al. 2011](http://commonsenseatheism.com/wp-content/uploads/2013/07/Koizumi-et-al-Human-sweet-taste-receptor-mediates-acide-induced-sweetness-of-miraculin.pdf)). Despite the results of my earlier model combination, I predict I won't particularly enjoy Osha Thai at the moment. Instead, I decide to try some tabasco sauce, to see whether it [now tastes like doughnut glaze](http://www.nytimes.com/2008/05/28/dining/28flavor.html).\n\nIn some cases, you might also need to [adjust for your prior](http://blog.givewell.org/2011/08/18/why-we-cant-take-expected-value-estimates-literally-even-when-theyre-unbiased/) over, say, \"expected enjoyment of restaurant food,\" if for some reason your original model combination procedure didn't capture your prior properly.\n\n###   \n\n### Against \"the outside view\"\n\nThere is a _lot_ more to say about model combination and adjustment (e.g. [this](/lw/vq/the_weighted_majority_algorithm/)), but for now let me make a suggestion about language usage.\n\nSometimes, small changes to our language can help us think more accurately. For example, gender-neutral language can reduce male bias in our associations ([Stahlberg et al. 2007](http://books.google.com/books?id=zzW5dqN8NiUC&lpg=PA163&ots=fwSignDEag&dq=Representation%20of%20the%20sexes%20in%20language&lr&pg=PA163#v=onepage&q&f=false)). In this spirit, I recommend we retire the phrase \"the outside view..\", and instead use phrases like \"some outside view_s_...\" and \"_an_ outside view...\"\n\nMy reasons are:\n\n1.  Speaking of \"the\" outside view privileges a particular reference class, which could make us overconfident of that particular model's predictions, and leave model uncertainty unaccounted for.\n    \n2.  Speaking of \"the\" outside view [can act as a conversation-stopper](/lw/1p5/outside_view_as_conversationhalter/), whereas speaking of multiple outside views encourages further discussion about how much weight each model should be given, and what each of them implies about the phenomenon under discussion."
    },
    "voteCount": 66
  },
  {
    "_id": "exa5kmvopeRyfJgCy",
    "url": null,
    "title": "Double Crux — A Strategy for Mutual Understanding",
    "slug": "double-crux-a-strategy-for-mutual-understanding",
    "author": "Duncan_Sabien",
    "question": false,
    "tags": [
      {
        "name": "Rationality"
      },
      {
        "name": "Anticipated Experiences"
      },
      {
        "name": "Techniques"
      },
      {
        "name": "Disagreement"
      },
      {
        "name": "Double-Crux"
      },
      {
        "name": "Falsifiability"
      }
    ],
    "tableOfContents": {
      "sections": [
        {
          "title": "Preamble",
          "anchor": "Preamble",
          "level": 1
        },
        {
          "title": "Casus belli",
          "anchor": "Casus_belli",
          "level": 1
        },
        {
          "title": "Prerequisites",
          "anchor": "Prerequisites",
          "level": 1
        },
        {
          "title": "How to play",
          "anchor": "How_to_play",
          "level": 1
        },
        {
          "title": "Methods",
          "anchor": "Methods",
          "level": 1
        },
        {
          "title": "Pitfalls",
          "anchor": "Pitfalls",
          "level": 1
        },
        {
          "title": "Algorithm",
          "anchor": "Algorithm",
          "level": 1
        },
        {
          "title": "1. Find a disagreement with another person",
          "anchor": "1__Find_a_disagreement_with_another_person",
          "level": 1
        },
        {
          "title": "2. Operationalize the disagreement",
          "anchor": "2__Operationalize_the_disagreement",
          "level": 1
        },
        {
          "title": "3. Seek double cruxes",
          "anchor": "3__Seek_double_cruxes",
          "level": 1
        },
        {
          "title": "4. Resonate",
          "anchor": "4__Resonate",
          "level": 1
        },
        {
          "title": "5. Repeat!",
          "anchor": "5__Repeat_",
          "level": 1
        },
        {
          "title": "Conclusion",
          "anchor": "Conclusion",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        },
        {
          "anchor": "comments",
          "level": 0,
          "title": "108 comments"
        }
      ],
      "headingsCount": 15
    },
    "contents": {
      "markdown": "**Preamble**\n\nDouble crux is one of CFAR's newer concepts, and one that's forced a re-examination and refactoring of a lot of our curriculum (in the same way that the introduction of TAPs and Inner Simulator did previously). It rapidly became a part of our organizational social fabric, and is one of our highest-EV threads for outreach and dissemination, so it's long overdue for a public, formal explanation.\n\nNote that while the core concept is fairly settled, the execution remains somewhat in flux, with notable experimentation coming from Julia Galef, Kenzi Amodei, Andrew Critch, Eli Tyre, Anna Salamon, myself, and others. Because of that, this post will be less of a cake and more of a folk recipe—this is long and meandering on purpose, because the priority is to transmit the _generators_ of the thing over the thing itself. Accordingly, if you think you see stuff that's wrong or missing, you're probably onto something, and we'd appreciate having them added here as commentary.\n\n* * *\n\n**Casus belli**\n\nTo a first approximation, a human can be thought of as a black box that takes in data from its environment, and outputs beliefs and behaviors (that black box isn't really \"opaque\" given that we _do_ have access to a lot of what's going on inside of it, but our understanding of our own cognition seems uncontroversially incomplete).\n\nWhen two humans disagree—when their black boxes output different answers, as below—there are often a handful of unproductive things that can occur.\n\n![](http://www.rationality.org/assets/img/updates/double-crux-1.png)\n\nThe most obvious (and tiresome) is that they'll simply repeatedly bash those outputs together without making any progress (think most disagreements over sports or politics; the people above just shouting \"triangle!\" and \"circle!\" louder and louder). On the second level, people can (and often do) take the difference in output as evidence that _the other person's black box is broken_ (i.e. they're bad, dumb, crazy) or that the other person _doesn't see the universe clearly_ (i.e. they're biased, oblivious, unobservant). On the third level, people will often _agree to disagree,_ a move which preserves the social fabric at the cost of truth-seeking and actual progress.\n\nDouble crux in the ideal solves all of these problems, and in practice even fumbling and inexpert steps toward that ideal seem to produce a lot of marginal value, both in increasing understanding and in decreasing conflict-due-to-disagreement.\n\n* * *\n\n**Prerequisites**\n\nThis post will occasionally delineate two versions of double crux: a _strong_ version_,_ in which both parties have a shared understanding of double crux and have explicitly agreed to work within that framework, and a _weak_ version_,_ in which only one party has access to the concept, and is attempting to improve the conversational dynamic unilaterally.\n\nIn either case, the following things seem to be required:\n\n*   **Epistemic humility.** The number one foundational backbone of rationality seems, to me, to be how readily one is able to think \"It's possible that _I_ might be the one who's wrong, here.\" Viewed another way, this is the ability to take one's beliefs as _object_, rather than being _subject to them_ and unable to set them aside (and then try on some other belief and productively imagine \"what would the world be like if _this_ were true, instead of _that?\"_).\n*   **Good faith.** An assumption that people believe things for _causal reasons_; a recognition that having been exposed to the same set of stimuli would have caused one to hold approximately the same beliefs; a default stance of holding-with-skepticism what seems to be evidence that the other party is bad or wants the world to be bad (because as monkeys it's not hard for us to _convince_ ourselves that we have such evidence when we really don't).1\n*   **Confidence in the existence of objective truth.** I was tempted to call this \"objectivity,\" \"empiricism,\" or \"the Mulder principle,\" but in the end none of those quite fit. In essence: a conviction that for almost any well-defined question, there _really truly is_ a clear-cut answer. That answer may be impractically or even impossibly difficult to find, such that we can't actually go looking for it and have to fall back on heuristics (e.g. how many grasshoppers are alive on Earth at this exact moment, is the color orange superior to the color green, why isn't there an audio book of Fight Club narrated by Edward Norton), but it nevertheless _exists._\n*   **Curiosity and/or a desire to uncover truth.** Originally, I had this listed as truth-seeking alone, but my colleagues pointed out that one can move in the right direction simply by being curious about the other person and the contents of their map, without focusing directly on the territory.\n\nAt CFAR workshops, we hit on the first and second through specific lectures, the third through osmosis, and the fourth through osmosis and a lot of relational dynamics work that gets people curious and comfortable with one another. Other qualities (such as the ability to regulate and transcend one's emotions in the heat of the moment, or the ability to commit to a thought experiment and really wrestle with it) are also helpful, but not as critical as the above.\n\n  \n\n**How to play**\n\nLet's say you have a belief, which we can label A (for instance, \"middle school students should wear uniforms\"), and that you're in disagreement with someone who believes some form of ¬A. _Double cruxing_ with that person means that you're both in search of a second statement B, with the following properties:\n\n*   You and your partner both disagree about B as well (you think B, your partner thinks ¬B).\n*   The belief B is _crucial_ for your belief in A; it is one of the _cruxes_ of the argument. If it turned out that B was _not_ true, that would be sufficient to make you think A was false, too.\n*   The belief ¬B is crucial for your partner's belief in ¬A, in a similar fashion.\n\n![](http://www.rationality.org/assets/img/updates/double-crux-2.png)\n\nIn the example about school uniforms, B might be a statement like \"uniforms help smooth out unhelpful class distinctions by making it harder for rich and poor students to judge one another through clothing,\" which your partner might sum up as \"optimistic bullshit.\" Ideally, B is a statement that is somewhat _closer to reality_ than A—it's more concrete, grounded, well-defined, discoverable, etc. It's less about principles and summed-up, induced conclusions, and more of a glimpse into the structure that led to those conclusions.\n\n(It doesn't have to _be_ concrete and discoverable, though—often after finding B it's productive to start over in search of a C, and then a D, and then an E, and so forth, until you end up with something you can research or run an experiment on).\n\nAt first glance, it might not be clear why simply _finding_ B counts as victory—shouldn't you _settle_ B, so that you can conclusively choose between A and ¬A? But it's important to recognize that arriving at B means you've _already_ dissolved a significant chunk of your disagreement, in that you and your partner now _share a belief about the causal nature of the universe._\n\nIf B, then A. Furthermore, if ¬B, then ¬A. You've both agreed that the states of B are _crucial_ for the states of A, and in this way your continuing \"agreement to disagree\" isn't just \"well, you take your truth and I'll take mine,\" but rather \"okay, well, let's see what the evidence shows.\" Progress! And (more importantly) collaboration!\n\n* * *\n\n**Methods**\n\nThis is where CFAR's versions of the double crux unit are currently weakest—there's some form of magic in the search for cruxes that we haven't quite locked down. In general, the method is \"search through your cruxes for ones that your partner is likely to disagree with, and then compare lists.\" For some people and some topics, clearly identifying your own cruxes is easy; for others, it very quickly starts to _feel like_ one's position is fundamental/objective/un-break-downable.\n\nTips:\n\n*   **Increase noticing of subtle tastes, judgments, and \"karma scores.\"** Often, people suppress a lot of their opinions and judgments due to social mores and so forth. Generally loosening up one's inner censors can make it easier to notice _why_ we think X, Y, or Z.\n*   **Look forward rather than backward.** In places where the question \"why?\" fails to produce meaningful answers, it's often more productive to try making predictions about the future. For example, I might not know why I think school uniforms are a good idea, but if I turn on my narrative engine and start describing the better world I think will result, I can often sort of feel my way toward the underlying causal models.\n*   **Narrow the scope.** A specific test case of \"Steve should've said hello to us when he got off the elevator yesterday\" is easier to wrestle with than \"Steve should be more sociable.\" Similarly, it's often easier to answer questions like \"How much of our next $10,000 should we spend on research, as opposed to advertising?\" than to answer \"Which is more important right now, research or advertising?\"\n*   **Do \"[Focusing](https://medium.com/@ThingMaker/focusing-for-skeptics-6b949ef33a4f)\" and other resonance checks.** It's often useful to try on a perspective, hypothetically, and then pay attention to your intuition and bodily responses to refine your actual stance. For instance: (_wildly asserts)_ \"I bet if everyone wore uniforms there would be a fifty percent reduction in bullying.\" (_pauses, listens to inner doubts)_ \"Actually, scratch that—that doesn't seem true, now that I say it out loud, but there _is_ something in the vein of reducing _overt_ bullying, maybe?\"\n*   **Seek cruxes independently before anchoring on your partner's thoughts.** This one is fairly straightforward. It's also worth noting that if you're attempting to find disagreements in the first place (e.g. in order to practice double cruxing with friends) this is an excellent way to start—give everyone the same ten or fifteen open-ended questions, and have everyone write down their own answers based on their own thinking, crystallizing opinions _before_ opening the discussion.\n\nOverall, it helps to keep the _ideal_ of a perfect double crux in the front of your mind, while holding the realities of your actual conversation somewhat separate. We've found that, at any given moment, increasing the \"double cruxiness\" of a conversation tends to be useful, but worrying about how far from the ideal you are in absolute terms doesn't. It's all about doing what's useful and productive in the moment, and that often means making sane compromises—if one of you has clear cruxes and the other is floundering, it's fine to focus on one side. If neither of you can find a single crux, but instead each of you has something like eight co-cruxes of which any five are sufficient, just say so and then move forward in whatever way seems best.\n\n(Variant: a \"trio\" double crux conversation in which, at any given moment, if you're the least-active participant, your job is to squint at your two partners and try to model what each of them is saying, and where/why/how they're talking past one another and failing to see each other's points. Once you have a rough \"translation\" to offer, do so—at that point, you'll likely become more central to the conversation and someone else will rotate out into the squinter/translator role.)\n\nUltimately, each move should be in service of reversing the usual antagonistic, warlike, \"win at all costs\" dynamic of most disagreements. Usually, we spend a significant chunk of our mental resources guessing at the shape of our opponent's belief structure, forming hypotheses about what things are crucial and lobbing arguments at them in the hopes of knocking the whole edifice over. Meanwhile, we're incentivized to obfuscate our own belief structure, so that our opponent's attacks will be ineffective.\n\n(This is also terrible because it means that we often fail to even _find_ the crux of the argument, and waste time in the weeds. If you've ever had the experience of awkwardly fidgeting while someone spends ten minutes assembling a conclusive proof of some tangential sub-point that never even had the _potential_ of changing your mind, then you know the value of someone being willing to say \"Nope, this isn't going to be relevant for me; try speaking to _that_ instead.\")\n\nIf we can move the debate to a place where, instead of _fighting_ over the truth, we're _collaborating_ on a search for understanding, then we can recoup a lot of wasted resources. You have a tremendous comparative advantage at knowing the shape of your own belief structure—if we can switch to a mode where we're each looking inward and candidly sharing insights, we'll move forward _much_ more efficiently than if we're each engaged in guesswork about the other person. This requires that we want to know the _actual truth_ (such that we're incentivized to seek out flaws and falsify wrong beliefs in ourselves just as much as in others) and that we feel emotionally and socially safe with our partner, but there's a doubly-causal dynamic where a tiny bit of double crux spirit up front can _produce_ safety and truth-seeking, which allows for more double crux, which produces more safety and truth-seeking, etc.\n\n* * *\n\n**Pitfalls**\n\nFirst and foremost, it matters whether you're in the strong version of double crux (cooperative, consent-based) or the weak version (you, as an agent, trying to improve the conversational dynamic, possibly in the face of direct opposition). In particular, if someone is currently riled up and conceives of you as rude/hostile/the enemy, then saying something like \"I just think we'd make better progress if we talked about the _underlying reasons_ for our beliefs\" doesn't sound like a plea for cooperation—it sounds like a trap.\n\nSo, if you're in the weak version, the primary strategy is to embody the question **\"What do you see that I don't?\"** In other words, approach from a place of explicit humility and good faith, drawing out their belief structure _for its own sake,_ to see and appreciate it rather than to undermine or attack it. In my experience, people can \"smell it\" if you're just playing at good faith to get them to expose themselves; if you're having trouble really getting into the spirit, I recommend meditating on times in your past when you were embarrassingly wrong, and how you felt _prior_ to realizing it compared to _after_ realizing it.\n\n(If you're unable or unwilling to swallow your pride or set aside your sense of justice or fairness hard enough to really do this, that's **actually fine**; not every disagreement benefits from the double-crux-nature. But if your _actual goal_ is improving the conversational dynamic, then this is a cost you want to be prepared to pay—going the extra mile, because a) going what _feels like_ an appropriate distance is more often an undershoot, and b) going an _actually appropriate_ distance may not be enough to overturn their entrenched model in which you are The Enemy. Patience- and sanity-inducing rituals recommended.)\n\nAs a further tip that's good for either version but particularly important for the weak one, _model_ the behavior you'd like your partner to exhibit. Expose your _own_ belief structure, show how your _own_ beliefs might be falsified, highlight points where you're uncertain and visibly integrate their perspective and information, etc. In particular, if you don't want people running amok with wrong models of what's going on in _your_ head, make sure you're not acting like you're the authority on what's going on in _their_ head.\n\nSpeaking of non-sequiturs, beware of getting lost in the fog. The very first step in double crux should _always_ be to operationalize and clarify terms. Try attaching numbers to things rather than using misinterpretable qualifiers; try to talk about what would be observable in the world rather than how things feel or what's good or bad. In the school uniforms example, saying \"uniforms make students feel better about themselves\" is a _start,_ but it's not enough, and going further into quantifiability (if you think you could actually get numbers someday) would be even better. **Often, disagreements will \"dissolve\" as soon as you remove ambiguity—this is success, not failure!**\n\nFinally, _use paper and pencil,_ or whiteboards, or get people to treat specific predictions and conclusions as immutable objects (if you or they want to change or update the wording, that's _encouraged_, but make sure that at any given moment, you're working with a clear, unambiguous statement). Part of the value of double crux is that it's the _opposite_ of the weaselly, score-points, hide-in-ambiguity-and-look-clever dynamic of, say, a public political debate. The goal is to have everyone understand, at all times and as much as possible, what the other person is _actually_ trying to say—not to try to get a straw version of their argument to stick to them and make them look silly. Recognize that _you yourself_ may be tempted or incentivized to fall back to that familiar, fun dynamic, and take steps to keep yourself in \"scout mindset\" rather than \"soldier mindset.\"\n\n* * *\n\n**Algorithm**\n\nThis is the double crux algorithm as it currently exists in our handbook. It's not strictly connected to all of the discussion above; it was designed to be read in context with an hour-long lecture and several practice activities (so it has some holes and weirdnesses) and is presented here more for completeness and as food for thought than as an actual conclusion to the above.\n\n**1\\. Find a disagreement with another person**\n\n*   A case where you believe one thing and they believe the other\n*   A case where you and the other person have different confidences (e.g. you think X is 60% likely to be true, and they think it’s 90%)\n\n**2\\. Operationalize the disagreement**\n\n*   Define terms to avoid getting lost in semantic confusions that miss the real point\n*   Find specific test cases—instead of (e.g.) discussing whether you should be more outgoing, instead evaluate whether you should have said hello to Steve in the office yesterday morning\n*   Wherever possible, try to think in terms of actions rather than beliefs—it’s easier to evaluate arguments like “we should do X before Y” than it is to converge on “X is better than Y.”\n\n**3\\. Seek double cruxes**\n\n*   Seek your own cruxes independently, and compare with those of the other person to find overlap\n*   Seek cruxes collaboratively, by making claims (“I believe that X will happen because Y”) and focusing on falsifiability (“It would take A, B, or C to make me stop believing X”)\n\n**4\\. Resonate**\n\n*   Spend time “inhabiting” both sides of the double crux, to confirm that you’ve found the core of the disagreement (as opposed to something that will ultimately fail to produce an update)\n*   Imagine the resolution as an if-then statement, and use your inner sim and other checks to see if there are any unspoken hesitations about the truth of that statement\n\n**5\\. Repeat!**\n\n  \n\n* * *\n\n**Conclusion**\n\nWe think double crux is super sweet. To the extent that you see flaws in it, we want to find them and repair them, and we're currently betting that _repairing and refining double crux_ is going to pay off better than _try something totally different._ In particular, we believe that embracing the spirit of this mental move has huge potential for unlocking people's abilities to wrestle with all sorts of complex and heavy hard-to-parse topics (like existential risk, for instance), because it provides a format for holding a bunch of partly-wrong models at the same time while you distill the value out of each.\n\nComments appreciated; critiques highly appreciated; anecdotal data from experimental attempts to teach yourself double crux, or teach it to others, or use it on the down-low without telling other people what you're doing **extremely** appreciated.\n\n\\- Duncan Sabien\n\n* * *\n\n\\[1\\]One reason good faith is important is that even when people are \"wrong,\" they are usually _partially_ right—there are flecks of gold mixed in with their false belief that can be productively mined by an agent who's interested in getting the whole picture. Normal disagreement-navigation methods have some tendency to throw out that gold, either by allowing everyone to protect their original belief set or by replacing everyone's view with whichever view is shown to be \"best,\" thereby throwing out data, causing information cascades, disincentivizing \"[noticing your confusion](https://www.lesswrong.com/lw/if/your_strength_as_a_rationalist/),\" etc.\n\nThe central assumption is that the universe is like a large and complex maze that each of us can only see parts of. To the extent that language and communication allow us to gather info about parts of the maze without having to investigate them ourselves, that's great. But when we disagree on what to do _because_ we each see a different slice of reality, it's nice to adopt methods that allow us to integrate and synthesize, rather than methods that force us to pick and pare down. It's like the parable of the three blind men and the elephant—whenever possible, avoid generating a bottom-line conclusion until you've accounted for _all_ of the available data.\n\n![](http://www.rationality.org/assets/img/updates/double-crux-3.png)\n\n_The agent at the top mistakenly believes that the correct move is to head to the left, since that seems to be the most direct path toward the goal. The agent on the right can see that this is a mistake, but it would never have been able to navigate to that particular node of the maze on its own._"
    },
    "voteCount": 118
  },
  {
    "_id": "MgFDzAfCku9MSDLuw",
    "url": null,
    "title": "Six economics misconceptions of mine which I've resolved over the last few years",
    "slug": "six-economics-misconceptions-of-mine-which-i-ve-resolved",
    "author": "Buck",
    "question": false,
    "tags": [
      {
        "name": "World Modeling"
      },
      {
        "name": "Economics"
      },
      {
        "name": "Updated Beliefs (examples of)"
      }
    ],
    "tableOfContents": {
      "sections": [
        {
          "title": "1. Divestment",
          "anchor": "1__Divestment",
          "level": 1
        },
        {
          "title": "2. Index funds",
          "anchor": "2__Index_funds",
          "level": 1
        },
        {
          "title": "3. Prediction markets",
          "anchor": "3__Prediction_markets",
          "level": 1
        },
        {
          "title": "4. Coase’s arguments about externalities",
          "anchor": "4__Coase_s_arguments_about_externalities",
          "level": 1
        },
        {
          "title": "5. Non-tax regulations that increase equality have disincentive effects on work",
          "anchor": "5__Non_tax_regulations_that_increase_equality_have_disincentive_effects_on_work",
          "level": 1
        },
        {
          "title": "6. Price and quality controls",
          "anchor": "6__Price_and_quality_controls",
          "level": 1
        },
        {
          "title": "Conclusion",
          "anchor": "Conclusion",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        },
        {
          "anchor": "comments",
          "level": 0,
          "title": "58 comments"
        }
      ],
      "headingsCount": 9
    },
    "contents": {
      "markdown": "Here are six cases where I was pretty confident in my understanding of the microeconomics of something, but then later found out I was missing an important consideration.\n\nThanks to Richard Ngo and Tristan Hume for helpful comments.\n\nHere’s the list of mistakes:\n\n*   I thought divesting from a company had no effect on the company.\n*   I thought that the prices on a prediction market converged to the probabilities of the underlying event.\n*   I thought that I shouldn’t expect to be able to make better investment decisions than buying index funds.\n*   I had a bad understanding of externalities, which was improved by learning about Coase’s theorem.\n*   I didn’t realize that regulations like minimum wages are analogous to taxes in that they disincentivize work.\n*   I misunderstood the economics of price controls.\n\nIn each, I’m not talking about empirical situations at all—I’m just saying that I had a theoretical analysis which I think turned out to be wrong. It’s possible that in many real situations, the additional considerations I’ve learned about don’t actually affect the outcome very much. But it was still an error to not know that those considerations were potentially relevant.\n\n1\\. Divestment\n--------------\n\nI used to believe that personally divesting in a company didn’t affect its share price, and therefore had no impact on the company. I guess my reasoning here was something like “If the share is worth $10 and you sell it, someone else will just buy it for $10, so the price won’t change”. I was treating shares as if they were worth some fixed amount of money.\n\nThe simplest explanation for why you can’t just model shares as being worth fixed amounts of money is that people are risk averse, and so the tenth Google share you buy is worth less to you than the first; and so as the price decreases, it becomes more worthwhile to take a bigger risk on the company.\n\nAs a result, divestment reduces the price of shares, in the same way that selling anything else reduces its price.\n\nIn the specific case of divestment, this means that when I sell some stocks, the price ends up lower than it was.\n\nI first learned I was wrong about this from [this Sideways View post](https://sideways-view.com/2019/05/25/analyzing-divestment/), published May 2019.\n\n2\\. Index funds\n---------------\n\nI used to think that it wasn’t possible for individuals like me to get higher returns than I’d get from just buying an index fund, because in an efficient market, every share is equally valuable.\n\nThis is wrong for a few reasons. One is that the prices of shares are determined by the risk aversion of other market participants; if your risk aversion is different from the average, some shares (specifically, risky ones) will be much better investments than others.\n\nSecondly, because I’m risk averse, I prefer buying shares which are going to do relatively well in worlds where I’m relatively poorer. For example, if I’m a software engineer at a tech company, compared to a random shareholder I should invest more in companies which are as anticorrelated with software engineer salaries as possible. Or if I live in the US, I should consider investing in the markets of other countries.\n\nI didn’t understand this fully until around April this year.\n\n3\\. Prediction markets\n----------------------\n\nRelatedly, I thought that the fair market price of a contract which pays out $1 if Trump gets elected is just the probability of Trump getting elected. This is wrong because Trump getting elected is correlated with how valuable other assets are. Suppose I thought that Trump has a 50% chance of getting reelected, and that if he gets re-elected, the stock market will crash. If I have a bunch of my money in the stock market, the contract is worth more than 50 cents, because it hedges against Trump winning.\n\n(Here’s a maybe more intuitive way of seeing this: Suppose I could pick between getting $10 in the world where Trump won (in which we’re assuming the market would crash) and the world where Trump lost. Clearly the $10 would be more valuable to me in the world where he wins and my stocks are decimated. So the value of the “Trump wins” contract is higher than the value of the “Trump loses” contract, even though they correspond to events of equal probability.) And there is a potentially very high number of correlative outcomes that betters might be thinking about and hedging against, and the market computes these and reflects them in the price.\n\nThis is a more general version of the point that it’s hard to have a prediction market on whether the world will end. Paul Christiano has an [old blog post on this topic](https://ordinaryideas.wordpress.com/2011/12/16/risk-arbitrage/) which I first saw years ago but which I didn't understand properly at the time.\n\nI first understood this fully around March this year.\n\nAll of these first three mistakes were the result of me not really understanding basic portfolio theory; thanks to spending a bunch of time talking to traders over the last few years, I now understand it better.\n\n4\\. Coase’s arguments about externalities\n-----------------------------------------\n\nI used to have an overly simplistic picture of externalities—I believed the Econ 101 story: normally markets are efficient, but when a good has an externality the wrong amount will be produced, and this is resolved by putting a tax or subsidy on the good to internalize the externality.\n\nI changed my mind about this after reading [David Friedman’s essay](http://www.daviddfriedman.com/Academic/Coase_World.html). I’ll just quote a few paragraphs:\n\n> The first step is to realize that an external cost is not simply a cost produced by the pollutor and born by the victim. In almost all cases, the cost is a result of decisions by both parties. I would not be coughing if your steel mill were not pouring out sulfur dioxide. But your steel mill would do no damage to me if I did not happen to live down wind from it. It is the joint decision—yours to pollute and mine to live where you are polluting—that produces the cost.\n\n> Suppose that, in a particular case, the pollution does $100,000 a year worth of damage and can be eliminated at a cost of only $80,000 a year (from here on, all costs are per year). Further assume that the cost of shifting all of the land down wind to a new use unaffected by the pollution—growing timber instead of renting out summer resorts, say—is only $50,000. If we impose an emission fee of a hundred thousand dollars a year, the steel mill stops polluting and the damage is eliminated—at a cost of $80,000. If we impose no emission fee the mill keeps polluting, the owners of the land stop advertising for tenants and plant trees instead, and the problem is again solved—at a cost of $50,000. In this case the result without Pigouvian taxes is efficient—the problem is eliminated at the lowest possible cost—and the result with Pigouvian taxes in inefficient.\n\n> Moving the victims may not be a very plausible solution in the case of air pollution; it seems fairly certain that even the most draconian limitations on emissions in southern California would be less expensive than evacuating that end of the state. But the problem of externalities applies to a wide range of different situations, in many of which it is far from obvious which party can avoid the problem at lower cost and in some of which it is not even obvious which one we should call the victim.\n\nMy previous position was missing this nuance. I first read that David Friedman essay midway through last year.\n\n5\\. Non-tax regulations that increase equality have disincentive effects on work\n--------------------------------------------------------------------------------\n\nI used to think that the way to decide whether a minimum wage was good was to look at the effect on unemployment and the effect on total income for minimum wage workers, and then figure out whether I thought that the increase in unemployment was worth the increase in income. I think this was wrong in two pretty different ways.\n\nThe first mistake is that I was neglecting the fact that policies aimed at transferring wealth from rich people to poor people disincentivize making money. Taxes are just a special case of this, and can be seen as part of a category of wealth-transferal policies that includes minimum wage. So when you’re arguing that a minimum wage would be part of the optimal policy portfolio, you have to argue that it would be better than a tax. I did not understand that this was part of the calculation.\n\nI first learned this from a post by Paul Christiano which I think he incorporated into Objection 2 [here](https://sideways-view.com/2019/03/03/on-redistribution/); that blog post was published March 2019.\n\n6\\. Price and quality controls\n------------------------------\n\nThe second of the ways I was wrong about the minimum wage comes from a misunderstanding of the economics of price controls; in hindsight I think that my high school economics curriculum was just wrong about this. I think that I realized my misconception after reading [The Dark Lord’s Answer](https://yudkowsky.net/other/fiction/dark-lords-answer/), published in 2016.\n\nIn high school economics, I was taught that when the government imposes a price floor (e.g., a minimum wage), you’ll end up with more supply than demand for the good. This is beneficial to suppliers who still succeed at selling the good, it’s harmful to suppliers who can no longer sell the good, and it’s harmful to buyers.\n\nI now think that that understanding was overly simplistic. Here’s my current understanding.\n\nIn a market, the supply and demand of a good must equilibrate somehow—for every loaf of bread that someone buys, someone had to sell a loaf of bread. One way that the market can equilibrate is that the price can change—if the price is higher, selling is more attractive and buying is less attractive. So if more people want to buy than sell at the current price, we might expect the price to rise until things are in equilibrium.\n\nBut there are other variables than price which can change in a way that allow the market to equilibrate. One obvious example is product quality—if you decrease the quality of a product, consumers are less enthusiastic about buying but suppliers are more enthusiastic about selling (because they can presumably make it for cheaper).\n\nOften, fluctuations in quality rather than price are what cause markets to equilibrate. For example, restaurants often don’t have price hikes at busy times, they just have long waits. Customers like it less when they have to wait more, and restaurants like having customers waiting (because it helps them ensure that their restaurant is constantly full).\n\nSo when we talk about the equilibrium state of a market, we can’t just talk about price, we also need to talk about all the other variables which can change.\n\nIn the case where we only consider price and quantity, there’s always only one equilibrium, because as price increases, supply rises and demand falls. (Actually, supply and demand could be constant over some range of prices, in which case there is an interval of equilibrium prices. I’m going to ignore this.)\n\nBut if we’re allowed to vary quality too, there are now many possible settings of price and quality where supply equals demand. E.g., for any fixed quality level, there’s going to be one equilibrium price, for the same reason as before.\n\nIn a competitive market, the equilibrium will be the point on the supply-equals-demand curve which maximizes efficiency. E.g., if there’s a way that producers could increase quality that would make production cost $1 more, producers will only do that if it makes the product worth more than $1 more valuable to consumers. This is optimal.\n\n(In real life, you usually have producers selling a variety of different similar goods at different price/quality points; I’m talking about this restricted case because it’s simpler.)\n\nNow, suppose that the government imposes a restriction on price or quality. For example, they might set a maximum or minimum price, or they might make safety restrictions which restrict quality in certain ways. The market will reequilibriate by using whatever degrees of freedom it has left. Specifically, it will reequilibriate to the optimal point within the newly restricted space of points at which supply equals demand. In general, this will lead to a less efficient outcome.\n\nFor example, if the price of bread is $2 at equilibrium, and the government sets a maximum price of $1.50, then the equilibrium will move along the quality curve until it gets to the point where the equilibrium price is $1.50.\n\nThis analysis gets more realistic if you allow there to be more dimensions than price and quantity along which bread can vary. For example, I’d expect to see the following phenomena:\n\n*   Producers trying to figure out ways to get paid under the table, e.g., by demanding favors in return for selling to people. This reduces efficiency inasmuch as producers weren’t already being compensated by miscellaneous favors.\n*   Sellers changing in ways that are mildly more convenient for them but much more inconvenient for consumers. For example, having long lines outside stores, or treating customers worse.\n*   Producers indulging weak preferences of theirs in who they sell to (e.g., nepotism).\n\nIn the case of minimum wages, I’d expect to see employers do things like engaging in wage theft which the employees tolerate (which is inefficient because it increases variance for employees) or being inflexible and unpleasant. This analysis would predict that wage theft is much more common among minimum wage employees than employees at higher wages.\n\nOne way of thinking about the efficiency of this is to think from the perspective of the producers. They have to pick some change that makes the price of the bread $1.50. There are many ways they could reduce the price to $1.50. They’re going to pick the way that is best for them.\n\nIn some cases, this leads to almost no value being destroyed at all. For example, in the bread case, sellers might sell smaller loaves, which might be almost as efficient if you dubiously assume that the main cost of bread is flour. The worst case is that there’s no way for the seller to change the product to keep it profitable which benefits them, and so they end up changing it in a way which makes them very little better off.\n\nThe welfare impact of this kind of regulation is also affected by redistributive effects. For example, if bakers decide to only sell bread to their friends and family, this has a positive redistributive effect if the friends and family of bakers are poorer than average.\n\nAn example where the redistributive effect might make the world much better: Suppose that there’s demand for 100 loaves of bread, where half of that comes from poor people who want to feed their children and the other half comes from a tech billionaire who wants to make a giant bread sculpture. If the baker ends up selling to people who are most willing to stand in lines, then this might lead to a better outcome. (Getting this result requires making some pretty strong assumptions about the shape of the relevant curves.)\n\nAnother example is that you might expect that in a world where the minimum wage causes low-paid jobs to be more unpleasant, teenagers will be less inclined to take the jobs and poor adults will end up having relatively more of the jobs. It’s possible to set things up such that this ends up increasing total welfare.\n\nConclusion\n----------\n\nIt’s embarrassing that I was confidently wrong about my understanding of so many things in the same domain. I’ve updated towards thinking that microeconomics is trickier than most other similarly straightforward-seeming subjects like physics, math, or computer science. I think that the above misconceptions are more serious than any misconceptions about other technical fields which I’ve discovered over the last few years (except maybe the [aestivation hypothesis thing](https://link.springer.com/article/10.1007/s10701-019-00289-5?shared-article-renderer)).\n\nIn three of these cases (4, 5, and 6), I had incorrect beliefs that came from my high school economics class. In those three cases, the correct understanding makes government intervention look worse. I think that this is not a coincidence—I think that the people who wrote the IB economics curriculum are probably leftist and this colored their perception.\n\nOn the other hand, in the other cases, I assumed that the equilibria of markets had a variety of intuitive properties that they turn out not to have.\n\nOne obvious question is: how many more of these am I going to discover over the next year or two?\n\nI think my median guess is that over the next year I will learn two more items that I think deserve to go on this list. Of course, I’m now a lot more cautious about being confident about microeconomics arguments, so I don’t expect to be as confidently wrong as I was about some of these.\n\nIn most of these cases, there was a phase where I no longer believed the false thing but didn’t properly understand the true thing. During this phase, I wouldn’t have made bets. Currently I’m in the “not making bets” phase with regard to a few other topics in economics; hopefully in a year I’ll understand them."
    },
    "voteCount": 102
  },
  {
    "_id": "bZ2w99pEAeAbKnKqo",
    "url": null,
    "title": "Optimal Exercise",
    "slug": "optimal-exercise",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "Exercise (Physical)"
      },
      {
        "name": "Human Bodies"
      },
      {
        "name": "Practical"
      }
    ],
    "tableOfContents": {
      "sections": [
        {
          "title": "How much exercise?",
          "anchor": "How_much_exercise_",
          "level": 1
        },
        {
          "title": "Which exercises?",
          "anchor": "Which_exercises_",
          "level": 1
        },
        {
          "title": "Weight training programs",
          "anchor": "Weight_training_programs",
          "level": 2
        },
        {
          "title": "Bodyweight routines",
          "anchor": "Bodyweight_routines",
          "level": 3
        },
        {
          "title": "Cardio routines",
          "anchor": "Cardio_routines",
          "level": 3
        },
        {
          "title": "Summary of my recommended routine",
          "anchor": "Summary_of_my_recommended_routine",
          "level": 3
        },
        {
          "title": "Nutrition",
          "anchor": "Nutrition",
          "level": 3
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        },
        {
          "anchor": "comments",
          "level": 0,
          "title": "146 comments"
        }
      ],
      "headingsCount": 9
    },
    "contents": {
      "markdown": "**Followup to:** [Lifestyle interventions to increase longevity](/lw/jrt/lifestyle_interventions_to_increase_longevity/).\n\nWhat does it mean for exercise to be optimal?\n\n*   Optimal for looks\n*   Optimal for time\n*   Optimal for effort\n*   Optimal for performance\n*   Optimal for longevity\n\nThere may be even more criteria.\n\nWe're all likely going for a mix of outcomes, and optimal exercise is going to change depending on your weighting of different factors. So I'm going to discuss something close to a minimum viable routine based on meta-analyses of exercise studies.\n\nNot knowing which sort of exercise yields the best results gives our brains an excuse to stop thinking about it. The intent of this post is to go over the dose responses to various types of exercise. We’re going to break through vague notions like “exercise is good” and “I should probably exercise more” with a concrete plan where you understand the relevant parameters that will cause dramatic improvements.\n\n### How much exercise?\n\nOptimality aside, I recommend starting with a very minimal routine for 6-ish weeks to build the habit of exercise in to your life. You'll want a program that causes you little mental stress that you can actually stick with. You've got a few options for achieving this. The gains from weightlifting can be surprisingly quick—you'll see dramatic changes in your appearance in 4 months—and seeing yourself lift more weight every session can be a great motivator. Couch to 5k is a basic running progression designed for sedentary people. A daily bodyweight routine is a good way to achieve habit formation through consistency. I recommend making a firm choice and sticking with it until it becomes easy.\n\nOnce you've made exercise a habit, you'll want to gradually nudge yourself towards the level that's optimal. So what is that level? Most of the rest of the claims in this post are supported by [this review by Swiss researchers](http://ije.oxfordjournals.org/content/40/5/1382.long). As far as I know, this is the largest systematic review of exercise studies ever undertaken, reviewing 7000 studies with 80 meeting inclusion criteria covering over 1.3 million subjects. Sheer size, however, is not the only reason to take this study very seriously. As someone who has read hundreds of exercise studies, I can say that the methodology of the meta-analysis done to determine dose-response to exercise is excellent. What is most encouraging is that the study authors repeatedly point out shortcomings, and ways their findings should _not_ be interpreted because the underlying data does not warrant it. They also check for publication bias. One potential caveat is that this a review of cohort studies, not RCTs. But the authors note that RCTs of exercise almost always show _greater_ effect sizes, not smaller. This is likely because people over-report how much exercise they do in observational studies.\n\nIn order to compare the intensity of different activities, exercise researchers use a unit called a MET, or metabolic equivalent. The MET is defined so that your weight (in kg) * METs = Calories you're burning per hour. An example MET table can be found [here](http://www.health.harvard.edu/newsletters/Harvard_Womens_Health_Watch/2009/December/met-hour-equivalents-of-various-physical-activities). For the purposes of exercise studies, activities are typically classified as low-intensity, moderate-intensity, and vigorous-intensity. These roughly correspond to 1-3, 4-6, and 7+ METs per hour. For typical individuals, this will translate to approximately 200, 400, and 600+ Calories burned per hour.\n\nOn the low end, some studies have found dramatic benefits from just the first 15 minutes of moderate-intensity exercise per week. These studies indicate that you gain about as much going from no exercise to some exercise as you do going from some exercise to optimal exercise.\n\nThe Swiss review finds that the first hour per week of vigorous-intensity activity gets you 2/3rds the benefit of 10 hours per week, but the study authors make sure to point out that this is an implausible effect size and that there are almost certainly some confounding and reverse causality issues going on. Which is to say that people who have better health are simply going to be capable of more exercise.\n\nHow about on the high end? Studies differ on where the point of diminishing returns is. Some [put it](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3491006/) at 1000-1500 Calories; others as high as 3500 Calories. (Remember, a typical individual burns ~400 Calories per hour of moderate-intensity exercise.) I'll shoot for 1500 Calories in my recommendations; 3500 Calories is pretty hard to reach without exercising like a pro athlete.\n\nEstimates indicate that each minute of exercise gets you 3-7 minutes of extra life on average, with higher returns for more intense exercise. So every week, you have the opportunity to get a 3-7x ROI on time spent exercising up to the point of diminishing returns. I recommend high-intensity exercise—not only does it save time, it's also been shown to improve health more on a per-Calorie basis.\n\n### **Which exercises?**\n\n#### Weight training programs\n\nYou may have shied away from weight training in the past because you thought you would turn into some huge gross bodybuilder. But bodybuilders and fitness models take drugs and spend years training intensively to look the way they do. You are not going to gain 20lbs of muscle overnight magically. This goes double if you’re a woman. You do not have testosterone; you are not going to be building huge muscles no matter what you do.\n\nOf the forms of exercise I cover, weight training has the most rigorous evidence separating what works and what doesn’t. [This study](http://saudeemovimento.net.br/wp-content/uploads/bsk-pdf-manager/336_A_META-ANALYSIS_TO_DETERMINE_THE_DOSE_-_RESPONSE_FOR_STRENGTH_DEVELOPMENT.PDF) (pdf warning) examines what sort of resistance training results in the most rapid improvements.\n\nIn weight training lingo, AxB means A sets of B repetitions. So 4x10 would mean 40 reps with rest periods every 10 reps. Our study recommends starting with 4x10 3 times a week, and transitioning to 4x4 2 times a week as you become stronger. Aim for a weight you can barely complete all the reps with.\n\nFor an efficient full-body workout, select one exercise from each movement pattern:\n\nUpper push: bench press, incline press, overhead press, dips.\n\nUpper pull: cable rows, barbell rows, dumbbell rows, chin-ups, face pulls.\n\nLower push: squats, lunges, leg press.\n\nLower pull: deadlifts, power cleans, hyperextensions, romanian deadlifts, reverse hyperextensions, glute-ham raises.\n\nSo a good starting routine would be\n\nA: 4x10 each of squat, bench press, lat pulldown, hyperextension\n\nB: 4x10 each of squat, overhead press, cable row, hyperextension\n\nalternating A and B workouts on different days of the week e.g. AxBxAxx, BxAxBxx.\n\nYou'll try to increase the weight by 5lbs each session. As you improve, you want to decrease the reps and increase the intensity so you can keep advancing. For example, if you stall a couple times doing 4x10 at 125lbs on your squat, switch to 4x8 and keep increasing the weight, then 4x6, etc. until you get to something close to an optimal trained routine:\n\nA: 4x4 each of squats, bench, weighted chins, deadlifts\n\nB: 4x4 each of squats, overhead press, barbell row, power cleans\n\nAt this point, you're going to the gym only twice per week to give yourself more recovery time.\n\nFor learning exercises, there are many tutorials available online and I recommend checking some out if you are confused about form. You can always search for \"<name of exercise> tutorial\" and get articles and Youtube videos. Many people feel silly practicing their form with extremely light weights (often just the empty bar). But many world record holders start EVERY session this way to warm up and cement muscle memory. Others are silly NOT to do this. Also keep an eye on your ego. It's easy when setting goals for yourself to try to lift a weight that you can't really lift with proper form, because you want to set that personal best. But you'll feel pretty stupid when you are forced to miss the gym for a month because you hurt yourself.\n\nOn exercise selection: I'm not a big fan of deadlifts for absolute newbies, unlike say Mark Rippetoe in Starting Strength. Maybe add deadlifts in after you've gained some muscle and you have better awareness of form. I also differ from Rippetoe in recommending that newbies high bar squat (the distinction being that low bar squats place the bar across the shoulders and high bar squats place the bar on the trapezius). I have taught newbies both forms and most find high bar squatting easier to figure out how to do properly. I spent months learning to low bar squat and still injured myself; high bar squatting can be taught in a couple sessions in my experience. Pay attention to whether a tutorial video is trying to teach you low bar squatting; the cues for each exercise are different.\n\nFree weights are generally better than machine exercises, but I recommend cable rows and lat pulldowns to newbies. The goal is to move from cable rows to barbell rows, and from lat pulldowns to actual chin-ups. The issue here is that a beginner won't be able to do the requisite sets and reps of chin-ups and rows with good form.\n\nA note about equipment: Weightlifting shoes have an incredibly high return on investment. They make back injuries less likely, and drastically improve subjective experience of squatting. You can get Rogue weightlifting shoes (use your size in men's dress shoes to size them regardless of gender) for around $120; there are cheaper options available but good shoes will last years so the amortized cost is low. [Here's a full list of options](http://wlshoes.com/purpose/olympic-lifting/); note that even the cheapest weightlifting shoes are miles better than lifting in tennis shoes. I don't have any personal experience with the Reebok CrossFit lifter, but they seem like a good option under $100 for a shoe with the desired .75-inch rigid heel. I recommend a [cheap belt](http://www.amazon.com/Harbinger-4-Inch-Nylon-3-Inch-Medium/dp/B00074H7WI/ref=sr_1_11?ie=UTF8&qid=1394521980&sr=8-11&keywords=weightlifting+belt) (expensive ones aren't any better) in order to improve your execution of the [valsalva maneuver](http://en.wikipedia.org/wiki/Valsalva_maneuver) during squats and deadlifts which further protects the spine from flexion under load.\n\n**Bodyweight routines**\n\nFor a beginner, something like [this](http://well.blogs.nytimes.com/2013/05/09/the-scientific-7-minute-workout/?_php=true&_type=blogs&_r=0) is reasonable. Of course such a program will max out in fitness gains fairly quickly, even if you start doing several cycles of it. But this isn’t our worry as a beginner. For someone serious about progressing with body weight exercises past this stage, I recommend a program like Overcoming Gravity or Building the Gymnastic Body. There is not really formal support for the efficacy of these programs, but they are endorsed by coaches who train many people successfully, and are consistent with the general principles of weight lifting (progressive overload, training frequency, etc.).\n\n**Cardio routines**\n\nFor cardio, I recommend against high-intensity intervals when starting out. High-intensity intervals carry a greater risk of injury, especially if you're not used to them. They're also unpleasant and not conducive to building habits. For starting out, I recommend something that is based more on psychological results rather than performance optimality, like [Couch to 5k](http://www.coolrunning.com/engine/2/2_3/181.shtml). As you progress, start adding in short bursts of more intense effort. The idea is to tire yourself out quickly. If your cardio routine lasts more than 30 minutes you’re probably going too easy.\n\nWhat type of cardio should you do? Cardio that is amenable to high intensity is probably one of: running (especially up hills), swimming, rowing, biking, burpees, or jump rope. But you might be able to adapt others. I'm a huge fan of rowing for a few reasons. One, it works more than just the legs. Two, you can have a rowing machine in your house, which drastically lowers activation cost. Three, I just find it less aversive subjectively. You can keep stationary bicycle mounts in your house as well, and they have the advantages of being compact and [very cheap](http://www.amazon.com/Magnet-Bicycle-Indoor-Exercise-Trainer/dp/B004I576SM/ref=sr_1_1?ie=UTF8&qid=1394232108&sr=8-1&keywords=stationary+bike+mount) if you already own a bike. Burpees require no equipment, but they bothered my knees. They work great for some people though. Jumping rope is also very space/time efficient but the skill required acts as something of a barrier. If you find learning the skill enjoyable, it's a great option. It is worth noting that runners, bikers, and rowers have among the best VO2 max scores of any athletes.\n\nWhat does the optimal high-intensity cardio routine look like? Data on this comes from [this Meta-analysis](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3774727/) of VO2 max trainability. [VO2 max has been shown to be a robust predictor of mortality](http://www.ncbi.nlm.nih.gov/pubmed/19454641). This relation has held across elite athletes, to average individuals, to the overweight (see Figure 2 from [this meta-analysis](http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0073182) of vo2 max trainability). Unfortunately, it appears that the protocols eliciting the greatest increases in VO2 max are so arduous as to have high attrition rates. 6 days/week is not a schedule of training I expect anyone but professional athletes to maintain. What sort of realistic routine can still achieve most of these gains? The meta-analysis supports a mixture of 3-5 minute intervals, and longer duration but still intense intervals (30-40 minutes of continuous training). The authors also note that they did not include analysis of the evidence that very high intensity exercise (1 minute or less of max effort) shows unique health benefits. We could simply conclude that a mixture of interval times is good, and that every increment of cardio up to very high levels is likely good for us, but that doesn't feel very motivating. What we want is a clear goal. I'm going to combine data from the VO2 max study and the Swiss review to get a rough estimate. If we want to do resistance training twice per week and cardio at least twice per week can we realistically burn the 1500 calories we want? Let's see. Interval training sessions can vary widely in number of calories burned, but a sprinting session, a 4x4 protocol (one of the more popular protocols in the VO2 max meta analysis, consisting of four 4-minute intervals), and a 30 minute run can burn between 200-450 calories as a first approximation. Twice weekly and we have 400-900 calories. This leaves 600-1100 calories for 2 weightlifting sessions. Estimates for calories burned weightlifting vary extremely widely, most likely due to the huge number of exercises considered \"weightlifting\", but even the lower estimates put it over 300 calories per hour. This puts 2-4 hours of weightlifting per week at 600-1200 calories expended. How convenient!\n\nAll that remains is to suggest specific cardio routines. I don't have the evidence to say with any confidence what a truly optimal routine would look like here, but I can at least give well studied examples of each.\n\nVery high intensity routines follow a pattern of a short warmup (5 minutes at a slow pace) followed by several bursts of 10-60 seconds all out intensity. (30 on 30 off for 10 intervals is popular and close to maximizing vV02max)\n\nVO2 max interval training consists of four 3-5 minute intervals at 85%-95% your max heart rate interspersed with slower jogging for the same interval.\n\nLonger interval training consists of 20-40 minute runs at a consistent pace such that you are exhausted by the end.\n\nI wouldn't worry about the optimal frequency for each one. Don't forget that even training populations just consistently doing one type shows very dramatic improvements in health. I'd suggest freely mixing them up and trying to have fun with it. \n\n**Summary of my recommended routine**\n\nThis is what I recommend gradually working towards once you've made exercise a habit:\n\n*   ~1-2 hour weightlifting sessions 2-3x a week. (A third weightlifting session is recommended for the first several months, for both gaining strength and building habits.)\n*   ~15-40 minutes of vigorous cardio 2-3x a week. \n\nDon't do vigorous cardio on the same day as lifting weights! It's a good way to injure yourself, especially your lower back. Exercise doesn't make you stronger; it makes you weaker. It's the recovery from exercise that makes you stronger; give your body time to recover.\n\n**Nutrition**\n\nDon't try to implement a new diet and a new exercise plan at the same time. If you're trying to choose, do an exercise plan first—effects on health are much larger.\n\nIf you are underweight or normal weight, you'll need to eat more when you start exercising. Celebrate after your workouts by eating to reinforce the exercise habit. You may think eating pizza is bad for you, but not exercising is worse, so reward yourself however you want. Or drink my [nutrient dense shake](/lw/h2h/soylent_orange_whole_food_open_source_soylent/), designed to be consumed after workouts. (John\\_Maxwell\\_IV and I are planning to commercialize it after we roll out [our first nutritionally complete food](http://www.mealsquares.com/).)\n\nIf you're overweight: I agree with Gary Taubes that exercise is NOT a good way to lose weight. But exercise has bigger effects on health than weight loss, so I actually recommend prioritizing exercise over changing your diet. (Like I said, don't try to do both at once.)\n\nNote that you don't need to stuff yourself with massive amounts of protein to build muscle. Studies have never shown a measured benefit to consumption above .64g/lb of bodyweight, which translates to around 100g for a 150-160lb person. A single serving (3oz) of chicken, for example, contains about 21g of protein.\n\n* * *\n\nIf you've made it this far, congratulations; you are now as knowledgeable as any personal trainer I've spoken with."
    },
    "voteCount": 92
  },
  {
    "_id": "HbXXd2givHBBLxr3d",
    "url": null,
    "title": "System 2 as working-memory augmented System 1 reasoning",
    "slug": "system-2-as-working-memory-augmented-system-1-reasoning",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "Subagents"
      },
      {
        "name": "Dual Process Theory (System 1 & System 2)"
      }
    ],
    "tableOfContents": {
      "sections": [
        {
          "title": "What type 1/type 2 processing is not",
          "anchor": "What_type_1_type_2_processing_is_not",
          "level": 1
        },
        {
          "title": "Type 1/Type 2 processing as working memory use",
          "anchor": "Type_1_Type_2_processing_as_working_memory_use",
          "level": 1
        },
        {
          "title": "Type 2 processing as composed of Type 1 components",
          "anchor": "Type_2_processing_as_composed_of_Type_1_components",
          "level": 1
        },
        {
          "title": "Looking through Kahneman’s examples",
          "anchor": "Looking_through_Kahneman_s_examples",
          "level": 1
        },
        {
          "title": "Consciousness and dual process theory",
          "anchor": "Consciousness_and_dual_process_theory",
          "level": 1
        },
        {
          "title": "Type 1/Type 2 and bias",
          "anchor": "Type_1_Type_2_and_bias",
          "level": 1
        },
        {
          "title": "Summary and connection to the multiagent models of mind sequence",
          "anchor": "Summary_and_connection_to_the_multiagent_models_of_mind_sequence",
          "level": 1
        },
        {
          "title": "References",
          "anchor": "References",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        },
        {
          "anchor": "comments",
          "level": 0,
          "title": "19 comments"
        }
      ],
      "headingsCount": 10
    },
    "contents": {
      "markdown": "The terms System 1 and System 2 were originally coined by the psychologist Keith Stanovich and then popularized by Daniel Kahneman in his book _Thinking, Fast and Slow._ Stanovich noted that a number of fields within psychology had been developing various kinds of theories distinguishing between fast/intuitive on the one hand and slow/deliberative thinking on the other. Often these fields were not aware of each other. The S1/S2 model was offered as a general version of these specific theories, highlighting features of the two modes of thought that tended to appear in all the theories.\n\nSince then, academics have continued to discuss the models. Among other developments, _Stanovich and other authors have discontinued the use of the System 1/System 2 terminology as misleading_, choosing to instead talk about Type 1 and Type 2 processing. In this post, I will build on some of that discussion to argue that Type 2 processing is a _particular way of chaining together the outputs of various subagents using working memory_. Some of the processes involved in this chaining are themselves implemented by particular kinds of subagents.\n\nThis post has three purposes:\n\n*   Summarize some of the discussion about the dual process model that has taken place in recent years; in particular, the move to abandon the System 1/System 2 terminology.\n*   Connect the framework of thought that I have been developing in my multi-agent minds sequence with dual-process models.\n*   Push back on some popular interpretations of S1/S2 theory which I have been seeing on LW and other places, such as ones in which the two systems are viewed as entirely distinct, S1 is viewed as biased and S2 as logical, and ones in which it makes sense to identify more as one system or the other.\n\nLet’s start with looking at some criticism of the S1/S2 model endorsed by the person who coined the terms.\n\nWhat type 1/type 2 processing is not\n====================================\n\nThe terms “System 1 and System 2” suggest just that: two distinct, clearly defined systems with their own distinctive properties and modes of operation. However, there’s no single “System 1”: rather, a wide variety of different processes and systems are lumped together under this term. It is also unclear whether there is any single System 2, either. As a result, a number of researchers including Stanovich himself have switched to talking about “Type 1” and “Type 2” processing instead (Evans, 2012; Evans & Stanovich, 2013; Pennycook, Neys, Evans, Stanovich, & Thompson, 2018).\n\nWhat exactly defines Type 1 and Type 2 processing?\n\nA variety of attributes have been commonly attributed to either Type 1 or Type 2 processing. However, one criticism is that there is no empirical or theoretical support for such attributes to _only_ occur with one type of processing. For instance, Melnikoff & Bargh (2018) note that one set of characteristics which has been attributed to Type 1 processing is “efficient, unintentional, uncontrollable, and unconscious”, whereas Type 2 processing has been said to be “inefficient, intentional, controllable and conscious”.\n\n(Before you read on, you might want to take a moment to consider the extent to which this characterization matches your intuition of Type 1 and Type 2 processing. If it does match to some degree, you can try to think of examples which are well-characterized by these types, as well as examples which are not.)\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\nThey note that this correlation has never been empirically examined, and that there are also various processes in which attributes from both sets co-occur. For example:\n\n*   **Unconscious (T1) and Intentional (T2).** A skilled typist can write sentences without needing to consciously monitor their typing, “but will never start plucking away at their keys without intending to type something in the first place.” Many other skills also remain intentional activities even as one gets enough practice to be able to carry them out without conscious control: driving and playing piano are some examples. Also, speaking involves plenty of unconscious processes, as we normally have very little awareness of the various language-production rules that go into our speech. Yet we generally only speak when we intend to.\n*   **Unconscious (T1) and Inefficient (T2).** Unconscious learning can be less efficient than conscious learning. For example, some tasks can be learned quickly using a verbal rule which describes the solution, or slowly using implicit learning so that we figure out how to do the task but cannot give an explicit rule for it.\n*   **Uncontrollable (T1) and Intentional (T2).** Consider the bat-and-ball problem: \"A bat and a ball cost $1.10 in total. The bat costs $1 more than the ball. How much does the ball cost?\" Unless they have heard the problem before, people nearly always generate an initial (incorrect) answer of 10 cents. This initial response is uncontrollable: no experimental manipulation has been found that would cause people to produce any other initial answer, such as 8 cents to 13 cents. At the same time, the process which causes this initial answer to be produced is intentional: \"it is not initiated directly by an external stimulus (the question itself), but by an internal goal (to answer the question, a goal activated by the experimental task instructions). In other words, reading or hearing the bat-and-ball problem does not elicit the 10 cents output unless one intends to solve the problem.\"\n\nRegarding the last example, Melnikoff & Bargh note:\n\n> Ironically, this mixture of intentionality and uncontrollability characterizes many of the biases documented in Tversky and Kahneman’s classic research program, which is frequently used to justify the classic dual-process typology. Take, for example, the availability heuristic, which involves estimating frequency by the ease with which information comes to mind. In the classic demonstration, individuals estimate that more words begin with the letter K than have K in the third position (despite the fact that the reverse is true) because examples of the former more easily come to mind \\[107\\]. This bias is difficult to control – we can hardly resist concluding that more letters start with K than have K in the third position – but again, all of the available evidence suggests that it only occurs in the presence of an intention to make a judgment. The process of generating examples of the two kinds of words is not activated directly by an external stimulus, but by an internal intention to estimate the relative frequencies of the words. Likewise for many judgments and decisions.\n\nThey also give examples of what they consider uncontrollable (T1) but inefficient (T2), unintentional (T1) but inefficient (T2), as well as unintentional (T1) but controllable (T2). Further, they discuss each of the four attributes themselves and point out that they all contain various subdimensions. For example, people whose decisions are [influenced by unconscious primes](https://www.lesswrong.com/posts/x4n4jcoDP7xh5LWLq/book-summary-consciousness-and-the-brain#Unconscious_processing_of_meaning) are conscious of their decision but not of the influence from the prime, meaning that the process has both conscious and unconscious aspects.\n\nType 1/Type 2 processing as working memory use\n==============================================\n\nRather than following the “list of necessary attributes” definition, Evans & Stanovich [(2013)](https://paperpile.com/c/Hkgz7x/MCL0/?noauthor=1) distinguish between _defining features_ and _typical correlates_. In previous papers, Evans has generally defined Type 2 processing in terms of requiring working memory resources and being able to think hypothetically. On the other hand, Stanovich has focused on what he calls cognitive decoupling, which his work shows is highly correlated with fluid intelligence as the defining feature.\n\nCognitive decoupling [can be defined](https://www.lesswrong.com/s/n44Fqx5W4BhMugCMS/p/qMTzv8ATgDtfLq9ME) as the ability to create copies of our mental representations of things, so that the copies can be used in simulations without affecting the original representations. For example, if I see an apple in a tree, my mind has a representation of the apple. If I then imagine various strategies of getting the apple - such as throwing a stone at the tree to knock the apple down - I can mentally simulate what would happen to the apple as a result of my actions. But even as I imagine the apple falling down from the tree, I never end up thinking that I can get the real apple down simply by an act of imagination. This because the mental object representing the real apple is _decoupled_ from the apple in my hypothetical scenario. I can manipulate the apple in the hypothetical without those manipulations being passed on to the mental object representing the original apple.\n\nIn their joint paper, Evans & Stanovich propose to combine their models and define Type 2 processes as those which use working memory resources (closely connected with fluid intelligence) in order to carry out hypothetical reasoning and cognitive decoupling. In contrast, Type 1 reasoning is anything which does not do that. Various features of thought - such as being automatic and the other controlled - may tend to correlate more with one or the other type, but these are only correlates, not necessary features.\n\n![](https://stuff.kajsotala.fi/LW/S1/StanovichEvansTable.png)\n\n  \n\nType 2 processing as composed of Type 1 components\n==================================================\n\nIn previous posts of my multi-agent minds sequence, I have been building up a model of mind that is composed of interacting components. How does it fit together with the proposed Type 1/Type 2 model?\n\nKahneman in _Thinking Fast and Slow_ mentions that giving the answer to 2 + 2 = ? is a System (Type) 1 task, whereas calculating 17 * 24 is a System (Type) 2 task. This might be starting to sound familiar. In my post on [subagents and neural Turing machines](https://www.lesswrong.com/posts/7zQPYQB5EeaqLrhBh/subagents-neural-turing-machines-thought-selection-and), I discussed Stanislas Dehane’s model where you do complex arithmetic by breaking up a calculation into subcomponents which can be done automatically, and then routing the intermediate results through working memory. You could consider this to also involve cognitive decoupling: for instance, if part of how you calculate 17 * 24 is by first noting that you can calculate 10 * 24, you need to keep the original representation of 17 * 24 intact in order to figure out what other steps you need to take.\n\nTo me, the calculation of 10 * 24 = 240 happens mostly automatically; like 2 + 2 = 4, it feels like a Type 1 operation rather than a Type 2 one. But what this implies, then, is that we carry out Type 2 arithmetic by _chaining together Type 1 operations through Type 2 working memory._\n\nI do not think that this is just a special case relating to arithmetic. Rather it seems like an implication of the Evans & Stanovich definition which they do not mention explicitly, but which is nonetheless relatively straightforward to draw: that _Type 2 reasoning is largely built up of Type 1 components._\n\nUnder this interpretation, there are some components which are specifically dedicated to Type 2 processes: things like working memory storages and systems for manipulating their contents. But those components cannot do anything alone. The original input to be stored in working memory originates _from_ Type 1 processes (and the act of copying it to working memory decouples it from the original process which produced it), and working memory alone could not do anything without those Type 1 inputs.\n\nLikewise, there may be something like a component which is Type 2 in nature, in that it holds rules for how the contents of working memory should be transformed in different situations - but many of those transformations happen by firing various Type 1 processes which then operate on the contents of the memory. Thus, the rules are about _choosing which Type 1 process to trigger_, and could again do little without those processes. (My post on [neural Turing machines](https://www.lesswrong.com/posts/7zQPYQB5EeaqLrhBh/subagents-neural-turing-machines-thought-selection-and) explicitly discussed such rules.)\n\nLooking through Kahneman’s examples\n===================================\n\nAt this point, you might reasonably suspect that arithmetic reasoning is an example that I cherry-picked to support my argument. To avoid this impression, I’ll take the first ten examples of System 2 operations that Kahneman lists in the first chapter of _Thinking, Fast and Slow_ and suggest how they could be broken down into Type 1 and Type 2 components.\n\nKahneman defines System 2 in a slightly different way than we have defined Type 2 operations - he talks about System 2 operations requiring attention - but as attention and working memory are closely related, this still remains compatible with our model. Most of these examples involve somehow focusing attention, and manipulating attention can be understood as manipulating the contents of working memory to ensure that a particular mental object remains in working memory. Modifying the contents of working memory was an important type of production rule discussed in my earlier post.\n\nStarting with the first example in Kahneman’s list:\n\n> Brace for the starter gun in a race.\n\nOne tries to keep their body in such a position that it will be ready to run when the gun sounds; recognizing the feel of the correct position is a Type 1 operation. Type 2 rules are operating to focus attention on the output of the system which outputs proprioceptive data, allowing Type 1 processes to notice mismatches with the required body position and correct them. Additionally, Type 2 rules are focusing attention on the sound of the gun, so as to more quickly identify the sound when the gun fires (a Type 1 operation), causing the person to start running (also a Type 1 operation).\n\n> Focus attention on the clowns in the circus.\n\nThis involves Type 2 rules which focus attention on a particular sensory output, as well as keeping one’s eyes physically oriented towards the clowns. This requires detecting when one’s attention/eyes are on something else than the clowns and then applying an internal (in the case of attention) or external (in the case of eye position) correction. As Kahneman offers “orient to the source of a sudden sound”, “detect hostility in a voice”, “read words on large billboards”, and “understand simple sentences” as Type 1 operations, we can probably say that recognizing something as a clown or not-clown and moving one’s gaze accordingly are Type 1 operations.\n\n> Focus on the voice of a particular person in a crowded and noisy room.\n\nAs above, Type 2 rules check whether attention is on the voice of that person (a comparison implemented using a Type 1 process), and then adjust focus accordingly.\n\n> Look for a woman with white hair.\n\nSimilar to the clown example.\n\n> Search memory to identify a surprising sound.\n\nIt’s unclear to me exactly what is going on here. But introspectively, this seems to involve something like keeping the sound in attention so as to feed it to memory processes, and then applying the rule of “whenever the memory system returns results, compare them against the sound and adjust the search based on how relevant they seem”. The comparison feels like it is done by something like a Type 1 process.\n\n> Maintain a faster walking speed than is natural to you.\n\n> Monitor the appropriateness of your behavior in a social situation.\n\nWalking: Similar to the “brace for the starter gun” example, Type 2 rules keep calling for a comparison of your current walking speed with the desired one (a Type 1 operation), passing any corrections resulting from that comparison to the Type 1 system controlling your walking speed.\n\nSocial behavior: maintain attention on a conscious representation of what you are doing, checking it against various Type 1 processes which contain rules about appropriate and inappropriate behavior. Adjust or block accordingly.\n\n> Count the occurrences of the letter _a_ in a page of text.\n\nFocus attention on the letters of a text; when a Type 1 comparison detects the letter “a”, increment a working memory counter by one.\n\n> Tell someone your phone number.\n\nAfter a retrieval of the phone number from memory has been initiated, Type 2 rules use Type 1 processes to monitor that it is said in full.\n\n> Park in a narrow space (for most people except garage attendants).\n\nKeeping attention focused on what you are doing to allow a series of evaluations, mental simulations, and cached (Type 1) procedural operations determining how to act in response to a particular situation in the parking process.\n\nA general pattern in these examples is that Type 2 processing can maintain attention on something as well as hold the intention to invoke comparisons to use as the basis for behavioral adjustments. As comparisons involve Type 1 processes, Type 2 processing is fundamentally reliant on Type 1 processing to be able to do anything.\n\nConsciousness and dual process theory\n=====================================\n\nAlert readers might have noticed that focusing one’s attention on something involves keeping it in consciousness, whereas the previous Evans & Stanovich definition noted that consciousness is not a defining part of the Type 1/Type 2 classification. Is this a contradiction? Probably not, since as remarked previously, different aspects of the same process may be conscious and unconscious at the same time.\n\nFor example, if one intends to say something, one may be conscious of the intention while the actual speech production happens unconsciously; once they say it and they hear their own words, an evaluation process can run unconsciously but output its results into consciousness. With “conscious” being so multidimensional, it doesn’t seem like a good defining characteristic to use, even if some aspects of it did very strongly correlate with Type 2 processing.\n\nEvans (2012) writes in a manner which seems to me compatible with the notion of there being many different kinds of Type 2 processing, with different processing resources being combined according to different rules as the situation warrants:\n\n> The evidence suggests that there is not even a single type 2 system for reasoning, as different reasoning tasks recruit a wide variety of brain regions, according to the exact demands of the task \\[...\\].\n\n> I think of type 2 systems as ad hoc committees that are put together to deal with a particular problem and then disbanded when the task is completed. Reasoning with abstract and belief-laden syllogisms, for example, recruits different resources, as the neural imaging data indicate: Only the latter involve semantic processing regions of the brain. It is also a fallacy to think of “System 2” as a conscious mind that is choosing its own applications. The ad hoc committee must be put together by some rapid and preconscious process—any feeling that “we” are willing and choosing the course of our thoughts and actions is an illusion \\[...\\]. I therefore also take issue with dual-process theorists \\[...\\] who assign to System 2 not only the capacity for rule-based reasoning but also an overall executive role that allows it to decide whether to intervene upon or overrule a System 1 intuition. In fact, recent evidence suggests that while people’s brains detect conflict in dual-process paradigms, the conscious person does not.\n\nIf you read my neural Turing machines post, you may recall that I noted that the rules which choose what becomes conscious operate below the level of conscious awareness. We may have the subjective experience of being able to choose what thoughts we think, but this is a post-hoc interpretation rather than a fact about the process.\n\nType 1/Type 2 and bias\n======================\n\nPeople sometimes refer to Type 1 reasoning as biased, and to Type 2 reasoning as unbiased. But as this discussion should suggest, there is nothing that makes one of the two types intrinsically more or less biased than the other. The bias-correction power of Type 2 processing emerges from the fact that _if_ Type 1 operations are known to be erroneous _and_ a rule-based procedure for correcting them exists, a Type 2 operation can be learned which implements that rule.\n\nFor example, someone familiar with [the substitution principle](https://www.lesswrong.com/posts/LHtMNz7ua8zu4rSZr/the-substitution-principle) may know that their initial answer to a question like “how popular will the president be six months from now?” comes from a Type 1 process which _actually_ answered the question of “how popular is the president right now?”.\n\nThey may then have a Type 2 rule saying something like “when you notice that the question you were asked is subject to substitution effects, replace the initial answer with one derived from a particular procedure”. But this still requires a) a Type 1 process recognizing the situation as one where the rule should be applied b) knowing a procedure which provides a better answer c) the cue-procedure rule having been installed previously, itself a process requiring a number of Type 1 evaluations (about e.g. how rewarding it would be to have such a rule in place).\n\nThere is nothing to say that somebody couldn’t learn an outright wrong Type 2 rule, such as “whenever you think of 2+2 = 4, [substitute your initial answer of ‘4’ with a ‘5’](https://en.wikipedia.org/wiki/2_%2B_2_%3D_5)”.\n\nOften, it is also unclear of what the better Type 2 rule even _should_ be. For instance, another common substitution effect is that when someone is asked “How happy are you with your life these days?”, they actually answer the question of “What is my mood right now?”. But what _is_ the objectively correct procedure for evaluating your current happiness with life?\n\nOn the topic of Type 1/2 and bias, I give the final word to Evans (2012):\n\n> One of the most important fallacies to have arisen in dual-process research is the belief that the normativity of an answer \\[...\\] is diagnostic of the type of processing. Given the history of the dual-process theory of reasoning, one can easily see how this came about. In earlier writing, heuristic or type 1 processes were always the “bad guys,” responsible for cognitive biases \\[...\\]. In belief bias research, authors often talked about the conflict between “logic” and “belief,” which are actually dual sources, rather than dual processes. Evans and Over \\[...\\] defined “rationality2” as a form of well-justified and explicit rule-based reasoning that could only be achieved by type 2 processes. Stanovich \\[...\\] in his earlier reviews of his psychometric research program emphasized the association between high cognitive ability, type 2 processing and normative responding. Similarly, Kahneman and Frederick \\[...\\] associate the heuristics of Tversky and Kahneman with System 1 and successful reasoning to achieve normatively correct solutions to the intervention of System 2.  \n>   \n> The problem is that a normative system is an externally imposed, philosophical criterion that can have no direct role in the psychological definition of a type 2 process. \\[...\\] if type 2 processes are those that manipulate explicit representations through working memory, why should such reasoning necessarily be normatively correct? People may apply the wrong rules or make errors in their application. And why should type 1 processes that operate automatically and without reflection necessarily be wrong? In fact, there is much evidence that expert decision making can often be well served by intuitive rather than reflective thinking \\[...\\] and that sometimes explicit efforts to reason can result in worse performance \\[...\\].  \n>   \n> Reasoning research somewhat loads the dice in favor of type 2 processing by focusing on abstract, novel problems presented to participants without relevant expertise. If a sports fan with much experience of following games is asked to predict results, he or she may be able to do so quite well without need for reflective reasoning. However, a participant in a reasoning experiment is generally asked to do novel things, like assuming some dubious propositions to be true and deciding whether a conclusion necessarily follows from them. In these circumstances, explicit type 2 reasoning is usually necessary for correct solution, but certainly not sufficient. Arguably, however, when prior experience provides appropriate pragmatic cues, even an intractable problem like the Wason selection task becomes easy to solve \\[...\\], as this can be done with type 1 processes \\[...\\]. It is when normative performance requires the deliberate suppression of unhelpful pragmatic cues that higher ability participants perform better under strict deductive reasoning instructions \\[...\\].  \n>   \n> Hence, \\[the fallacy that type 1 processes are responsible for cognitive biases and type 2 processes for normatively correct reasoning\\] is with us for some fairly precise historical reasons. In the traditional paradigms, researchers presented participants with hard, novel problems for which they lacked experience (students of logic being traditionally excluded), and also with cues that prompted type 1 processes to compete or conflict with these correct answers. So in these paradigms, it does seem that type 2 processing is at least necessary to solve the problems, and that type 1 processes are often responsible for cognitive biases. But this perspective is far too narrow, as has recently been recognized. In recent writing, I have attributed responsibility for a range of cognitive biases roughly equally between type 1 and type 2 processing \\[...\\]. Stanovich \\[...\\] similarly identifies a number of reasons for error other than a failure to intervene with type 2 reasoning; for example, people may reason in a quick and sloppy (but type 2) manner or lack the necessary “mindware” for successful reasoning.\n\nSummary and connection to the multiagent models of mind sequence\n================================================================\n\nIn this post, I have summarized some recent-ish academic discussion on dual-process models of thought, or what used to be called System 1 and System 2. I noted that the popular conception of them as two entirely distinct systems with very different properties is mistaken. While there is a defining difference between them - namely, the use of working memory resources to support hypothetical thinking and cognitive decoupling - they seem to rather refer to differences in two _types_ of thought, either of which may use very different kinds of systems.\n\nIt is worth noting at this point that there are many different dual-process models in different parts of psychology. The Evans & Stanovich model which I have been discussing here is intended as a generalized model of them, but as they themselves (2013) write:\n\n> … we defend our view that the Type 1 and 2 distinction is supported by a wide range of converging evidence. However, we emphasize that not all dual-process theories are the same, and we will not act as universal apologists on each one’s behalf. Even within our specialized domain of reasoning and decision making, there are important distinctions between accounts. S. A. Sloman \\[...\\], for example, proposed an architecture that has a parallel-competitive form. That is, Sloman’s theories and others of similar structure \\[...\\] assume that Type 1 and 2 processing proceed in parallel, each having their say with conflict resolved if necessary. In contrast, our own theories \\[...\\] are default-interventionist in structure \\[...\\]. Default-interventionist theories assume that fast Type 1 processing generates intuitive default responses on which subsequent reflective Type 2 processing may or may not intervene.\n\nIn previous posts of the [multi-agent models of mind sequence](https://www.lesswrong.com/s/ZbmRyDN8TCpBTZSip), I have been building up a model of the mind being built up of a variety of subsystems (which might in some contexts be called subagents).\n\nIn my discussion of [Consciousness and the Brain](https://www.lesswrong.com/posts/x4n4jcoDP7xh5LWLq/book-summary-consciousness-and-the-brain), I summarized some of its conclusions as saying that:\n\n*   The brain has multiple subagents doing different things; many of the subagents do unconscious processing of information. When a mental object becomes conscious, many subagents will synchronize their processing around analyzing and manipulating that mental object.\n*   The collective of subagents can only have their joint attention focused on one mental object at a time.\n*   The brain can be compared to a production system, with a large number of subagents carrying out various tasks when they see the kinds of mental objects that they care about. E.g. when doing mental arithmetic, applying the right sequence of mental operations for achieving the main goal.\n\nIn [Building up to an Internal Family Systems model](https://www.lesswrong.com/posts/5gfqG3Xcopscta3st/building-up-to-an-internal-family-systems-model), I used this foundation to discuss the IFS model of how various subagents manipulate consciousness in order to achieve various kinds of behavior. In [Subagents, neural Turing machines, thought selection, and blindspots](https://www.lesswrong.com/posts/7zQPYQB5EeaqLrhBh/subagents-neural-turing-machines-thought-selection-and), I talked about the mechanistic underpinnings of this model and how processes like thought selection and firing of production rules might actually be implemented.\n\nWhat had been lacking so far was a connection between these models and the Type 1/Type 2 typology. However, if we take something like the Evans & Stanovich model of Type 1/Type 2 processing to be true, then it turns out that our discussion has been connected with their model all along. Already in “Consciousness and the Brain”, I mentioned the “neural Turing machine” passing on results from one subsystem to another through working memory. That, it turns out, is the defining characteristic of Type 2 processing - with Type 1 processing simply being any process which does _not_ do that.\n\nUnder this model, then, Type 2 processing is a _particular way of chaining together the outputs of various Type 1 subagents using working memory_. Some of the processes involved in this chaining are themselves implemented by particular kinds of subagents.\n\nReferences\n==========\n\nEvans, J. S. B. T. (2012). Dual process theories of deductive reasoning: facts and fallacies. _The Oxford Handbook of Thinking and Reasoning_, 115–133.\n\nEvans, J. S. B. T., & Stanovich, K. E. (2013). [Dual-Process Theories of Higher Cognition: Advancing the Debate](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.722.5271&rep=rep1&type=pdf). _Perspectives on Psychological Science: A Journal of the Association for Psychological Science_, _8_(3), 223–241.\n\nMelnikoff, D. E., & Bargh, J. A. (2018). [The Mythical Number Two](https://acmelab.yale.edu/sites/default/files/melnikoff_bargh_2018_mythical_number_2_0.pdf). _Trends in Cognitive Sciences_, _22_(4), 280–293.\n\nPennycook, G., Neys, W. D., Evans, J. S. B. T., Stanovich, K. E., & Thompson, V. A. (2018). [The Mythical Dual-Process Typology](http://www.cse.buffalo.edu/~rapaport/575/The-Mythical-Dual-Process-Typology_2018_Trends-in-Cognitive-Sciences.pdf). _Trends in Cognitive Sciences_, _22_(8), 667–668."
    },
    "voteCount": 38
  },
  {
    "_id": "XPErvb8m9FapXCjhA",
    "url": null,
    "title": "Adaptation-Executers, not Fitness-Maximizers",
    "slug": "adaptation-executers-not-fitness-maximizers",
    "author": "Eliezer_Yudkowsky",
    "question": false,
    "tags": [
      {
        "name": "Superstimuli"
      },
      {
        "name": "Rationality"
      },
      {
        "name": "Evolution"
      },
      {
        "name": "General Intelligence"
      },
      {
        "name": "Adaptation Executors"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "> \"Individual organisms are best thought of as adaptation-executers rather than as fitness-maximizers.\"\n\n> —John Tooby and Leda Cosmides, _The Psychological Foundations of Culture._\n\nFifty thousand years ago, the taste buds of _Homo sapiens_ directed their bearers to the scarcest, most critical food resources—sugar and fat. Calories, in a word. Today, the context of a taste bud's function has changed, but the taste buds themselves have not. Calories, far from being scarce (in First World countries), are actively harmful. Micronutrients that were reliably abundant in leaves and nuts are absent from bread, but our taste buds don't complain. A scoop of ice cream is a [superstimulus](https://www.lesswrong.com/lw/h3/superstimuli_and_the_collapse_of_western/), containing more sugar, fat, and salt than anything in the ancestral environment.\n\nNo human being with the _deliberate_ goal of maximizing their alleles' inclusive genetic fitness, would ever eat a cookie unless they were starving. But individual organisms are best thought of as adaptation-executers, not fitness-maximizers.\n\nA toaster, though its designer intended it to make toast, does not bear within it the intelligence of the designer—it won't automatically redesign and reshape itself if you try to cram in an entire loaf of bread. A Phillips-head screwdriver won't reconform itself to a flat-head screw. We created these tools, but they exist independently of us, and they continue independently of us.\n\nThe atoms of a screwdriver don't have tiny little XML tags inside describing their \"objective\" purpose. The designer had something in mind, yes, but that's not the same as what _happens_ in the real world. If you forgot that the designer is a separate entity from the designed thing, you might think, \"The _purpose_ of the screwdriver is to drive screws\"—as though this were an explicit property of the screwdriver itself, rather than a property of the designer's state of mind. You might be surprised that the screwdriver didn't reconfigure itself to the flat-head screw, since, after all, the screwdriver's _purpose_ is to turn screws.\n\nThe _cause_ of the screwdriver's existence is the designer's mind, which imagined an imaginary screw, and imagined an imaginary handle turning. The _actual_ operation of the screwdriver, its _actual_ fit to an actual screw head, _cannot_ be the objective cause of the screwdriver's existence: The future cannot cause the past. But the designer's brain, as an actually existent thing within the past, can indeed be the cause of the screwdriver.\n\nThe _consequence_ of the screwdriver's existence, may not correspond to the imaginary consequences in the designer's mind. The screwdriver blade could slip and cut the user's hand.\n\nAnd the _meaning_ of the screwdriver—why, that's something that exists in the mind of a user, not in tiny little labels on screwdriver atoms. The designer may intend it to turn screws. A murderer may buy it to use as a weapon. And then accidentally drop it, to be picked up by a child, who uses it as a chisel.\n\nSo the screwdriver's _cause,_ and its _shape,_ and its _consequence,_ and its various _meanings,_ are all different things; and only _one_ of these things is found within the screwdriver itself.\n\nWhere do taste buds come from? Not from an intelligent designer visualizing their consequences, but from a frozen history of ancestry: Adam liked sugar and ate an apple and reproduced, Barbara liked sugar and ate an apple and reproduced, Charlie liked sugar and ate an apple and reproduced, and [2763 generations later](https://www.lesswrong.com/lw/kt/evolutions_are_stupid_but_work_anyway/), the allele became fixed in the population. For convenience of thought, we sometimes compress this giant history and say: \"Evolution did it.\" But it's not a quick, local event like a human designer visualizing a screwdriver. This is the _objective cause_ of a taste bud.\n\nWhat is the _objective shape_ of a taste bud? Technically, it's a molecular sensor connected to reinforcement circuitry. This adds another level of indirection, because the taste bud isn't directly acquiring food. It's influencing the organism's mind, making the organism want to eat foods that are similar to the food just eaten.\n\nWhat is the _objective consequence_ of a taste bud? In a modern First World human, it plays out in multiple chains of causality: from the desire to eat more chocolate, to the plan to eat more chocolate, to eating chocolate, to getting fat, to getting fewer dates, to reproducing less successfully. This consequence is directly _opposite_ the key regularity in the long chain of ancestral successes which caused the taste bud's shape. But, since overeating has only recently become a problem, no significant evolution (compressed regularity of ancestry) has further influenced the taste bud's shape.\n\nWhat is the _meaning_ of eating chocolate? That's between you and your moral philosophy. Personally, I think chocolate tastes good, but I wish it were less harmful; acceptable solutions would include redesigning the chocolate or redesigning my biochemistry.\n\nSmushing several of the concepts together, you could sort-of-say, \"Modern humans do today what would have propagated our genes in a hunter-gatherer society, whether or not it helps our genes in a modern society.\" But this still isn't quite right, because we're not _actually_ asking ourselves which behaviors would maximize our ancestors' inclusive fitness. And many of our activities today have no ancestral analogue. In the hunter-gatherer society there wasn't any such thing as chocolate.\n\nSo it's better to view our taste buds as an _adaptation_ fitted to ancestral conditions that included near-starvation and apples and roast rabbit, which modern humans _execute_ in a new context that includes cheap chocolate and constant bombardment by advertisements.\n\nTherefore it is said: Individual organisms are best thought of as adaptation-executers, not fitness-maximizers."
    },
    "voteCount": 92
  },
  {
    "_id": "aiQabnugDhcrFtr9n",
    "url": null,
    "title": "The Power of Intelligence",
    "slug": "the-power-of-intelligence",
    "author": "Eliezer_Yudkowsky",
    "question": false,
    "tags": [
      {
        "name": "General Intelligence"
      },
      {
        "name": "World Modeling"
      },
      {
        "name": "AI"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "In our skulls we carry around three pounds of slimy, wet, grayish tissue, corrugated like crumpled toilet paper.\n\nYou wouldn’t think, to look at the unappetizing lump, that it was some of the most powerful stuff in the known universe. If you’d never seen an anatomy textbook, and you saw a brain lying in the street, you’d say “Yuck!” and try not to get any of it on your shoes. Aristotle thought the brain was an organ that cooled the blood. It doesn’t _look_ dangerous.\n\nFive million years ago, the ancestors of lions ruled the day, the ancestors of wolves roamed the night. The ruling predators were armed with teeth and claws—sharp, hard cutting edges, backed up by powerful muscles. Their prey, in self-defense, evolved armored shells, sharp horns, toxic venoms, camouflage. The war had gone on through hundreds of eons and countless arms races. Many a loser had been removed from the game, but there was no sign of a winner. Where one species had shells, another species would evolve to crack them; where one species became poisonous, another would evolve to tolerate the poison. Each species had its private niche—for who could live in the seas and the skies and the land at once? There was no ultimate weapon and no ultimate defense and no reason to believe any such thing was possible.\n\nThen came the Day of the Squishy Things.\n\nThey had no armor. They had no claws. They had no venoms.\n\nIf you saw a movie of a nuclear explosion going off, and you were told an Earthly life form had done it, you would never in your wildest dreams imagine that the Squishy Things could be responsible. After all, Squishy Things aren’t radioactive.\n\nIn the beginning, the Squishy Things had no fighter jets, no machine guns, no rifles, no swords. No bronze, no iron. No hammers, no anvils, no tongs, no smithies, no mines. All the Squishy Things had were squishy fingers—too weak to break a tree, let alone a mountain. Clearly not dangerous. To cut stone you would need steel, and the Squishy Things couldn’t excrete steel. In the environment there were no steel blades for Squishy fingers to pick up. Their bodies could not generate temperatures anywhere near hot enough to melt metal. The whole scenario was obviously absurd.\n\nAnd as for the Squishy Things manipulating DNA—that would have been beyond ridiculous. Squishy fingers are not that small. There is no access to DNA from the Squishy level; it would be like trying to pick up a hydrogen atom. Oh, _technically_ it’s all one universe, _technically_ the Squishy Things and DNA are part of the same world, the same unified laws of physics, the same great web of causality. But let’s be realistic: you can’t get there from here.\n\nEven if Squishy Things could _someday_ evolve to do any of those feats, it would take thousands of millennia. We have watched the ebb and flow of Life through the eons, and let us tell you, a year is not even a single clock tick of evolutionary time. Oh, sure, _technically_ a year is six hundred trillion trillion trillion trillion Planck intervals. But nothing ever happens in less than six hundred million trillion trillion trillion trillion Planck intervals, so it’s a moot point. The Squishy Things, as they run across the savanna now, will not fly across continents for at least another ten million years; _no one_ could have that much sex.\n\nNow explain to me again why an Artificial Intelligence can’t do anything interesting over the Internet unless a human programmer builds it a robot body.\n\nI have observed that someone’s flinch-reaction to “intelligence”—the thought that crosses their mind in the first half-second after they hear the word “intelligence”—often determines their flinch-reaction to the notion of an intelligence explosion. Often they look up the keyword “intelligence” and retrieve the concept _booksmarts_—a mental image of the Grand Master chess player who can’t get a date, or a college professor who can’t survive outside academia.\n\n“It takes more than intelligence to succeed professionally,” people say, as if charisma resided in the kidneys, rather than the brain. “Intelligence is no match for a gun,” they say, as if guns had grown on trees. “Where will an Artificial Intelligence get money?” they ask, as if the first _Homo sapiens_ had found dollar bills fluttering down from the sky, and used them at convenience stores already in the forest. The human species was not born into a market economy. Bees won’t sell you honey if you offer them an electronic funds transfer. The human species _imagined_ money into existence, and it exists—for _us_, not mice or wasps—because we go on believing in it.\n\nI keep trying to explain to people that the archetype of intelligence is not Dustin Hoffman in _Rain Man_. It is a human being, period. It is squishy things that explode in a vacuum, leaving footprints on their moon. Within that gray wet lump is the power to search paths through the great web of causality, and find a road to the seemingly impossible—the power sometimes called creativity.\n\nPeople—venture capitalists in particular—sometimes ask how, if the Machine Intelligence Research Institute successfully builds a true AI, the results will be _commercialized_. This is what we call a framing problem.\n\nOr maybe it’s something deeper than a simple clash of assumptions. With a bit of creative thinking, people can imagine how they would go about travelling to the Moon, or curing smallpox, or manufacturing computers. To imagine a trick that could accomplish _all these things at once_ seems downright impossible—even though such a power resides only a few centimeters behind their own eyes. The gray wet thing still seems mysterious to the gray wet thing.\n\nAnd so, because people can’t quite see how it would all work, the power of intelligence seems less real; harder to imagine than a tower of fire sending a ship to Mars. The prospect of visiting Mars captures the imagination. But if one should promise a Mars visit, and also a grand unified theory of physics, and a proof of the Riemann Hypothesis, and a cure for obesity, and a cure for cancer, and a cure for aging, and a cure for stupidity—well, it just sounds wrong, that’s all.\n\nAnd well it should. It’s a serious failure of imagination to think that intelligence is good for so little. Who could have imagined, ever so long ago, what minds would someday do? We may not even _know_ what our real problems are.\n\nBut meanwhile, because it’s hard to see how one process could have such diverse powers, it’s hard to imagine that one fell swoop could solve even such prosaic problems as obesity and cancer and aging.\n\nWell, one trick cured smallpox and built airplanes and cultivated wheat and tamed fire. Our current science may not agree yet on how exactly the trick works, but it works anyway. If you are temporarily ignorant about a phenomenon, that is a fact about your current state of mind, not a fact about the phenomenon. A blank map does not correspond to a blank territory. If one does not quite understand that power which put footprints on the Moon, nonetheless, the footprints are still there—real footprints, on a real Moon, put there by a real power. If one were to understand deeply enough, one could create and shape that power. Intelligence is as real as electricity. It’s merely far more powerful, far more dangerous, has far deeper implications for the unfolding story of life in the universe—and it’s a tiny little bit harder to figure out how to build a generator.\n\n* * *\n\n_The first publication of this post is_ [_here_](http://yudkowsky.net/singularity/power/)_._"
    },
    "voteCount": 42
  },
  {
    "_id": "GMqZ2ofMnxwhoa7fD",
    "url": null,
    "title": "The Octopus, the Dolphin and Us: a Great Filter tale",
    "slug": "the-octopus-the-dolphin-and-us-a-great-filter-tale",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "Evolution"
      },
      {
        "name": "Great Filter"
      },
      {
        "name": "General Intelligence"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "Is intelligence hard to evolve? Well, we're intelligent, so it must be easy... except that only an intelligent species would be able to ask that question, so we run straight into the problem of anthropics. Any being that asked that question would have to be intelligent, so this can't tell us anything about its difficulty (a similar mistake would be to ask \"is most of the universe hospitable to life?\", and then looking around and noting that everything seems pretty hospitable at first glance...).\n\nInstead, one could point at the great apes, note their high intelligence, see that intelligence arises separately, and hence that it can't be too hard to evolve.\n\nOne could do that... but one would be wrong. The key test is not whether intelligence can arise _separately_, but whether it can arise _independently_. Chimpanzees, Bonobos and Gorillas and such are all \"on our line\": they are close to common ancestors of ours, which we would expect to be intelligent because we are intelligent. Intelligent species tend to have intelligent relatives. So they don't provide any extra information about the ease or difficulty of evolving intelligence.\n\nTo get independent intelligence, we need to go far from our line. Enter the smart and cute icon on many student posters: the dolphin.\n\n![](http://images.lesswrong.com/t3_kvk_0.png?v=a6329bff06969824d07e66a2351f20f0)\n\nDolphins are certainly [intelligent](http://en.wikipedia.org/wiki/Cetacean_intelligence). And they are certainly far from our line. It seems hard to find a definite answer, but it seems that the last common ancestor of humans and dolphins was [a small mammal existing during the reign of the dinosaurs](http://www.nature.com/nature/journal/v446/n7135/full/nature05634.html). Humans and dolphins have been indicated by red rectangles, and their last common ancestor with a red circle.\n\n![](http://images.lesswrong.com/t3_kvk_1.png?v=b933206bcd9bf79390d2d56f669496fe)\n\nThis red circle is well before the [K-T boundary](http://en.wikipedia.org/wiki/Cretaceous%E2%80%93Paleogene_boundary) (indicated by the dotted line), hence represents a mammal living in the literal shadow of the dinosaurs.\n\nWe can apply a [convergent evolution](http://en.wikipedia.org/wiki/Convergent_evolution) argument to this common ancestor. Thus, assuming that subsequent evolution was somewhat independent, getting from that common ancestor to dolphin level of intelligence is something that can happen relatively easily.\n\nCan we go further? Well, what if we applied the argument twice? Let's bring in the most alien looking of the high-intelligence animals: the octopus.\n\n![](http://images.lesswrong.com/t3_kvk_2.png?v=808d4e15d7925e86655bae6e4fd83849)\n\nLet's make the further assumption that our common ancestor with dolphins was dumber than the modern octopus. This doesn't seem a stretch seeing how [intelligent the modern octopus can be](http://en.wikipedia.org/wiki/Cephalopod_intelligence), how minor in terms of ecological role the common dolphin-human ancestor must have been, and seeing the stupidity of [many of the descendants](http://en.wikipedia.org/wiki/Lagomorpha) of that common ancestor.\n\nIf we accept that assumption, we can then start looking for the common ancestor of humans and octopuses. Our two species are [really far apart](http://tolweb.org/Bilateria/2459):\n\n![](http://images.lesswrong.com/t3_kvk_3.png?v=9517ac5fb8a7e34ce18a11ef283a7e34)\n\nWe therefore have to go back to around the last common ancestor of the [Bilateria](http://en.wikipedia.org/wiki/Bilateria) (creatures with bilateral symmetry, i.e. they have a front and a back end, as well as an upside and downside, and therefore a left and a right). This is the (speculative) [urbilaterian](http://en.wikipedia.org/wiki/Urbilaterian). There are no known examples or fossils of it, which means that it was likely less than 1 cm in length. To quote Wikipedia: \"The urbilaterian is often considered to have possessed a gut and internal organs, a segmented body and a centralised nervous system, as well as a biphasic life cycle (i.e. consisting of larvae and adults) and some features of embryonic development. However, this need not necessarily be the case.\" Very confusing, and with no information about intelligence level. However, since the organism was so small and since it was the ancestor of almost every animal alive today (including worms and [Bryozoa](http://en.wikipedia.org/wiki/Bryozoa)), our best estimate would be that it's pretty stupid, with the simplest possible \"brain\".\n\nPutting this all together, it seems evolutionarily easy to get from urbilatrian intelligence to Octopus intelligence, and from Octopus intelligence to dolphin intelligence - thus from urbilatrian to dolphin.\n\nNote that this argument assumes that intelligence can be put on something like a linear scale. One could argue that Octopuses have low social intelligence, for instance. But then one could repeat the argument with distant animals with high social intelligence such as certain [insects](http://www.sciencedirect.com/science/article/pii/S0960982205010419). Especially if one believe in a more general form of intelligence, it seems that this family of arguments could be used effectively to demonstrate dolphin-level intelligence emerging easily from very low levels of intelligence.\n\nApplication to the Great Filter\n-------------------------------\n\nThe [Great Filter](http://en.wikipedia.org/wiki/Great_Filter) (related to the Fermi Paradox) is the argument that since we don't see any evidence of complex technological life in the universe, something must be preventing its emergence. At some point on the trajectory, something is culling almost all species.\n\n![](http://images.lesswrong.com/t3_kvk_5.png?v=5b75554c7f59311cc2beea57f5a0eec1)\n\nAn \"early\" great filter wouldn't affect us: that means that we got through the filter already, it's in our past, so the emptiness among the stars doesn't say anything negative for us. A \"late\" great filter is bad news: that implies that few civilizations make it from technological civilization to star-spanning civilization, with bad results for us. Note that AI is certainly not a great filter: an AI would likely expand through the universe itself\n\nThe real filter could be a combination of an early one and a late one, of course. But, unless the factors are exquisitely well-balanced, its likely that there is one location in civilizational development where most of the filter lies (ie where the probability of getting to the next stage is the lowest). Some possible locations for this could be:\n\n*   Life itself is unlikely (very early great filter).\n*   Life with a central nervous system is unlikely.\n*   **A lot of different possible locations for the great filter in between urbilatiran and dolphin intelligence.**\n*   Getting from dolphin to human intelligence is unlikely.\n*   Getting from human intelligence to technological civilization is unlikely (latest early filter).\n*   Getting from technological civilization to star-spanning civilization is unlikely.\n\nThese categories aren't of same size, of course - the first three are very diverse and large, for instance. Then what the evolutionary argument above says, is that the Great Filter in unlikely to be in the third, bolded category (which is in fact a multi-category).\n\nFor what it's worth, my personal judgement is that the filter lies before the creation of a central nervous system."
    },
    "voteCount": 60
  },
  {
    "_id": "fATPBv4pnHC33EmJ2",
    "url": null,
    "title": "Fake Morality",
    "slug": "fake-morality",
    "author": "Eliezer_Yudkowsky",
    "question": false,
    "tags": [
      {
        "name": "Ethics & Morality"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "God, say the religious fundamentalists, is the source of all morality; there can be no morality without a Judge who rewards and punishes.  If we did not fear hell and yearn for heaven, then what would stop people from murdering each other left and right?\n\nSuppose Omega makes a credible threat that if you ever step inside a bathroom between 7AM and 10AM in the morning, he'll kill you. Would you be panicked by the prospect of Omega withdrawing his threat?  Would you cower in existential terror and cry:  \"If Omega withdraws his threat, then what's to keep me from going to the bathroom?\"  No; you'd probably be quite relieved at your increased opportunity to, ahem, relieve yourself.\n\nWhich is to say:  The very fact that a religious person would be _afraid_ of God withdrawing Its threat to punish them for committing murder, shows that they have a revulsion of murder which is independent of whether God punishes murder or not.  If they had no sense that murder was wrong independently of divine retribution, the prospect of God not punishing murder would be no more existentially horrifying than the prospect of God not punishing sneezing.\n\nIf Overcoming Bias has any religious readers left, I say to you: it may be that you will someday lose your faith: and on that day, you will _not_ lose all sense of moral direction.  For if you fear the prospect of God not punishing some deed, that _is_ a moral compass.  You can plug that compass directly into your decision system and steer by it.  You can simply _not do_ whatever you are afraid God may not punish you for doing.  The fear of losing a moral compass is _itself_ a moral compass.  Indeed, I suspect you _are_ steering by that compass, and that you always have been.  As Piers Anthony once said, \"Only those with souls worry over whether or not they have them.\"  s/soul/morality/ and the point carries.\n\nYou don't hear religious fundamentalists using the argument:  \"If we did not fear hell and yearn for heaven, then what would stop people from eating pork?\"  _Yet by their assumptions_ \\- that we have no moral compass but divine reward and retribution - this argument should sound just as forceful as the other.\n\nEven the notion that God threatens you with eternal hellfire, rather than cookies, piggybacks on a pre-existing negative value for hellfire.  Consider the following, and ask which of these two philosophers is really the altruist, and which is really selfish?\n\n> \"You should be selfish, because when people set out to improve society, they meddle in their neighbors' affairs and pass laws and seize control and make everyone unhappy.  Take whichever job that pays the most money: the reason the job pays more is that the efficient market thinks it produces more value than its alternatives.  Take a job that pays less, and you're second-guessing what the market thinks will benefit society most.\"\n> \n> \"You should be altruistic, because the world is an iterated Prisoner's Dilemma, and the strategy that fares best is Tit for Tat with initial cooperation.  People don't _like_ jerks.  Nice guys really do finish first.  Studies show that people who contribute to society and have a sense of meaning in their lives, are happier than people who don't; being selfish will only make you unhappy in the long run.\"\n\nBlank out the _recommendations_ of these two philosophers, and you can see that the first philosopher is using strictly prosocial criteria to _justify_ his recommendations; to him, what validates an argument for selfishness is showing that selfishness benefits everyone.  The second philosopher appeals to strictly individual and hedonic criteria; to him, what _validates_ an argument for altruism is showing that altruism benefits him as an individual: higher social status or more intense feelings of pleasure.\n\nSo which of these two is the _actual_ altruist?  Whichever one _actually_ holds open doors for little old ladies."
    },
    "voteCount": 80
  },
  {
    "_id": "Aq8BQMXRZX3BoFd4c",
    "url": null,
    "title": "Morality is Awesome",
    "slug": "morality-is-awesome",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "Ethics & Morality"
      },
      {
        "name": "Metaethics"
      },
      {
        "name": "Fun Theory"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "(This is a semi-serious introduction to [the metaethics sequence](http://wiki.lesswrong.com/wiki/Metaethics_sequence). You may find it useful, but don't take it too seriously.)\n\n**Meditate on this:** A wizard has turned you into a whale. Is this awesome?\n\n![Is it?](http://i.imgur.com/OSnc8.png)\n\n\"Maybe? I guess it would be pretty cool to be a whale for a day. But only if I can turn back, and if I stay human inside and so on. Also, that's not a whale.\n\n\"Actually, a whale seems kind of specific, and I'd be suprised if that was the best thing the wizard can do. Can I have something else? Eternal happiness maybe?\"\n\n**Meditate on this:** A wizard has turned you into orgasmium, doomed to spend the rest of eternity experiencing pure happiness. Is this awesome?\n\n...\n\n\"Kindof... That's pretty lame actually. On second thought I'd rather be the whale; at least that way I could explore the ocean for a while.\n\n\"Let's try again. Wizard: maximize awesomeness.\"\n\n**Meditate on this:** A wizard has turned himself into a superintelligent god, and is squeezing as much awesomeness out of the universe as it could possibly support. This may include whales and starships and parties and jupiter brains and friendship, but only if they are awesome enough. Is this awesome?\n\n...\n\n\"Well, yes, that is awesome.\"\n\n* * *\n\nWhat we just did there is called Applied Ethics. Applied ethics is about what is awesome and what is not. Parties with all your friends inside superintelligent starship-whales are awesome. ~666 children dying of hunger every hour is not.\n\n(There is also normative ethics, which is about how to decide if something is awesome, and metaethics, which is about something or other that I can't quite figure out. I'll tell you right now that those terms are not on the exam.)\n\n\"Wait a minute!\" you cry, \"What is this awesomeness stuff? I thought ethics was about what is good and right.\"\n\nI'm glad you asked. I think \"awesomeness\" is what we should be talking about when we talk about morality. Why do I think this?\n\n1.  \"Awesome\" is not a [philosophical landmine](/lw/gm9/philosophical_landmines/). If someone encounters the word \"right\", all sorts of bad philosophy and connotations send them spinning off into the void. \"Awesome\", on the other hand, has no philosophical respectability, hence no philosophical baggage.\n    \n2.  \"Awesome\" is vague enough to capture all your moral intuition by the well-known mechanisms behind [fake utility functions](/lw/lq/fake_utility_functions/), and meaningless enough that this is no problem. If you think \"happiness\" is the stuff, you might get confused and try to maximize _actual_ happiness. If you think awesomeness is the stuff, it is much harder to screw it up.\n    \n3.  If you do manage to actually implement \"awesomeness\" as a maximization criteria, the results will be actually good. That is, \"awesome\" already refers to the same things \"good\" is supposed to refer to.\n    \n4.  \"Awesome\" does not refer to anything else. You think you can just redefine words, [but you can't](/lw/od/37_ways_that_words_can_be_wrong/), and this causes all sorts of trouble for people who overload \"happiness\", \"utility\", etc.\n    \n5.  You already know that you know how to compute \"Awesomeness\", and it doesn't feel like it has a mysterious essence that you need to study to discover. Instead it brings to mind concrete things like starship-whale math-parties and not-starving children, which is what we want anyways. You are already enabled to take [joy in the merely awesome](/lw/sx/inseparably_right_or_joy_in_the_merely_good/).\n    \n6.  \"Awesome\" is implicitly consequentialist. \"Is this awesome?\" engages you to think of the value of a possible world, as opposed to \"Is this right?\" which engages to to think of [virtues](http://yudkowsky.net/rational/virtues) and [rules](http://wiki.lesswrong.com/wiki/Ethical_injunction). (Those things can be awesome sometimes, though.)\n    \n\nI find that the above is true about me, and is nearly all I need to know about morality. It handily inoculates against the usual confusions, and sets me in the right direction to make my life and the world more awesome. It may work for you too.\n\nI would append the additional facts that if you wrote it out, the dynamic procedure to compute awesomeness would be [hellishly complex](http://wiki.lesswrong.com/wiki/Complexity_of_value), and that right now, it is only implicitly encoded in human brains, and [no where else](/lw/y3/value_is_fragile/). Also, if the great procedure to compute awesomeness is not preserved, the future will not be awesome. Period.\n\nAlso, it's important to note that what you think of as awesome can be changed by considering things from different angles and being exposed to different arguments. That is, the procedure to compute awesomeness is dynamic and [created already in motion](/lw/rs/created_already_in_motion/).\n\nIf we still insist on being confused, or if we're just curious, or if we need to actually _build_ [a wizard](http://wiki.lesswrong.com/wiki/Friendly_artificial_intelligence) to turn the universe into an awesome place (though we can leave that to the [experts](http://intelligence.org/)), then we can see the [metaethics sequence](http://wiki.lesswrong.com/wiki/Metaethics) for the full argument, details, and finer points. I think the best post (and the one to read if only one) is [joy in the merely good](/lw/sx/inseparably_right_or_joy_in_the_merely_good/)."
    },
    "voteCount": 141
  },
  {
    "_id": "B5K3hg8FgrMDHuXjH",
    "url": null,
    "title": "The Terrible, Horrible, No Good, Very Bad Truth About Morality and What To Do About It",
    "slug": "the-terrible-horrible-no-good-very-bad-truth-about-morality",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "Ethics & Morality"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "Joshua Greene has a PhD thesis called [The Terrible, Horrible, No Good, Very Bad Truth About Morality and What To Do About It](http://www.wjh.harvard.edu/~jgreene/GreeneWJH/Greene-Dissertation.pdf). What is this terrible truth? The essence of this truth is that many, many people (probably most people) believe that their particular moral (and axiological) views on the world are objectively true - for example that anyone who disagrees with the statement \"black people have the same value as any other human beings\" has committed either an error of logic or has got some empirical fact wrong, in the same way that people who claim that the earth was created 6000 years ago are objectively wrong.\n\nTo put it another way, Greene's contention is that our entire way of talking about ethics - the very words that we use - force us into talking complete nonsense (often in a very angry way) about ethics. As a simple example, consider the use of the words in any standard ethical debate - \"abortion is murder\", \"animal suffering is just as bad as human suffering\" - these terms seem to refer to objective facts; \"abortion is murder\" sounds rather like \"water is a solvent!\". I urge readers of Less Wrong to put in the effort of reading a significant part of Greene's long thesis starting at chapter 3: _Moral Psychology and Projective Error_, considering the massively important repercussions he claims his ideas could have:\n\n> In this essay I argue that ordinary moral thought and language is, while very natural, highly counterproductive and that as a result we would be wise to change the way we think and talk about moral matters. First, I argue on metaphysical grounds against moral realism, the view according to which there are first order moral truths. Second, I draw on principles of moral psychology, cognitive science, and evolutionary theory to explain why moral realism appears to be true even though it is not. I then argue, based on the picture of moral psychology developed herein, that **_realist moral language and thought promotes misunderstanding and exacerbates conflict_**. I consider a number of standard views concerning the practical implications of moral anti-realism and reject them. I then sketch and defend a set of alternative revisionist proposals for improving moral discourse, chief among them the elimination of realist moral language, especially deontological language, and the promotion of an anti-realist utilitarian framework for discussing moral issues of public concern. _**I emphasize the importance of revising our moral practices, suggesting that our entrenched modes of moral thought may be responsible for our failure to solve a number of global social problems.**_\n\nAs an accessible entry point, I have decided to summarize what I consider to be Greene's most important points in this post. I hope he doesn't mind - I feel that spreading this message is sufficiently urgent to justify reproducing large chunks of his dissertation - Starting at page 142:\n\n> In the previous chapter we concluded, in spite of common sense, that moral realism is false. This raises an important question: How is it that so many people are mistaken about the nature of morality? To become comfortable with the fact that moral realism is false we need to understand how moral realism can be so wrong but feel so right. ...\n\n> The central tenet of projectivism is that the moral properties we find (or think we find) in things in the world (e.g. moral wrongness) are mind-dependent in a way that other properties, those that we’ve called “value-neutral” (e.g. solubility in water), are not. Whether or not something is soluble in water has nothing to do with human psychology. But, say projectivists, whether or not something is wrong (or “wrong”) has everything to do with human psychology....  \n\n> Projectivists maintain that our encounters with the moral world are, at the very least, somewhat misleading. Projected properties tend to strike us as unprojected. They appear to be really “out there,” in a way that they, unlike typical value neutral properties, are not. ...\n\n> The respective roles of intuition and reasoning are illuminated by considering people’s reactions to the following story:\n> \n> _\"Julie and Mark are brother and sister. They are travelling together in France on summer vacation from college. One night they are staying alone in a cabin near the beach. They decided that it would be interesting and fun if they tried making love. At the very least it would be a new experience for each of them. Julie was already taking birth control pills, but Mark uses a condom too, just to be safe. They both enjoy making love but decide not to do it again. They keep that night as a special secret between them, which makes them feel even closer to each other. What do you think about that, was it OK for them to make love?\"_\n> \n> Haidt (2001, pg. 814) describes people’s responses to this story as follows: Most people who hear the above story immediately say that it was wrong for the siblings to make love, and they then set about searching for reasons. They point out the dangers of inbreeding, only to remember that Julie and Mark used two forms of birth control. They next try to argue that Julie and Mark could be hurt, even though the story makes it clear that no harm befell them. Eventually many people say something like\n> \n> _“I don’t know, I can’t explain it, I just know it’s wrong.”_\n> \n> This moral question is carefully designed to short-circuit the most common reason people give for judging an action to be wrong, namely harm to self or others, and in so doing it reveals something about moral psychology, at least as it operates in cases such at these. People’s moral judgments in response to the above story tend to be forceful, immediate, and produced by an unconscious process (intuition) rather than through the deliberate and effortful application of moral principles (reasoning). When asked to explain why they judged as they did, subjects typically gave reasons. Upon recognizing the flaws in those reasons, subjects typically stood by their judgments all the same, suggesting that the reasons they gave after the fact in support their judgments had little to do with the process that produced those judgments. Under ordinary circumstances reasoning comes into play after the judgment has already been reached in order to find rational support for the preordained judgment. **_When faced with a social demand for a verbal justification, one becomes a lawyer trying to build a case rather than a judge searching for the truth._** ...\n\n> The Illusion of Rationalist Psychology (p. 197)\n> \n> In Sections 3.2-3.4 I developed an explanation for why moral realism appears to be true, an explanation featuring the Humean notion of projectivism according to which we intuitively see various things in the world as possessing moral properties that they do not actually have. This explains why we tend to be realists, but it doesn’t explain, and to some extent is at odds with, the following curious fact. The social intuitionist model is counterintuitive. People tend to believe that moral judgments are produced by reasoning even though this is not the case. Why do people make this mistake? Consider, once again, the case of Mark and Julie, the siblings who decided to have sex. Many subjects, when asked to explain why Mark and Julie’s behavior is wrong, engaged in “moral dumbfounding,” bumbling efforts to supply reasons for their intuitive judgments. This need not have been so. It might have turned out that all the subjects said things like this right off the bat:\n> \n> _“Why do I say it’s wrong? Because it’s clearly just wrong. Isn’t that plain to see? It’s as if you’re putting a lemon in front of me and asking me why I say it’s yellow. What more is there to say?”_\n> \n> Perhaps some subjects did respond like this, but most did not. Instead, subjects typically felt the need to portray their responses as products of reasoning, even though they generally discovered (often with some embarrassment) that they could not easily supply adequate reasons for their judgments. On many occasions I’ve asked people to explain why they say that it’s okay to turn the trolley onto the other tracks but not okay to push someone in front of the trolley. Rarely do they begin by saying, _“I don’t know why. I just have an intuition that tells me that it is.”_ Rather, they tend to start by spinning the sorts of theories that ethicists have devised, _**theories that are nevertheless notoriously difficult to defend.**_ In my experience, it is only after a bit of moral dumbfounding that people are willing to confess that their judgments were made intuitively.\n> \n> Why do people insist on giving reasons in support of judgments that were made with great confidence in the absence of reasons? I suspect it has something to do with the custom complexes in which we Westerners have been immersed since childhood. **_We live in a reason-giving culture_**. Western individuals are expected to choose their own way, and to do so for good reason. American children, for example, learn about the rational design of their public institutions; the all important “checks and balances” between the branches of government, the judicial system according to which accused individuals have a right to a trial during which they can, if they wish, plead their cases in a rational way, inevitably with the help of a legal expert whose job it is to make persuasive legal arguments, etc. Westerners learn about doctors who make diagnoses and scientists who, by means of experimentation, unlock nature’s secrets. Reasoning isn’t the only game in town, of course. The American Declaration of Independence famously declares “these truths to be self-evident,” but American children are nevertheless given numerous reasons for the decisions of their nation’s founding fathers, for example, the evils of absolute monarchy and the injustice of “taxation without representation.” When Western countries win wars they draft peace treaties explaining why they, and not their vanquished foes, were in the right and set up special courts to try their enemies in a way that makes it clear to all that they punish only with good reason. Those seeking public office make speeches explaining why they should be elected, sometimes as parts of organized debates. Some people are better at reasoning than others, but everyone knows that the best people are the ones who, when asked, can explain why they said what they said and did what they did.\n> \n> With this in mind, we can imagine what might go on when a Westerner makes a typical moral judgment and is then asked to explain why he said what he said or how he arrived at that conclusion. The question is posed, and he responds intuitively. As suggested above, such intuitive responses tend to present themselves as perceptual. The subject is perhaps aware of his “gut reaction,” but he doesn’t take himself to have merely had a gut reaction. Rather, he takes himself to have detected a moral property out in the world, say, the inherent wrongness in Mark and Julie’s incestuous behavior or in shoving someone in front of a moving train. The subject is then asked to explain how he arrived at his judgment. He could say, _“I don’t know. I answered intuitively,”_ and this answer would be the most accurate answer for nearly everyone. But this is not the answer he gives because he knows after a lifetime of living in Western culture that _“I don’t know how I reached that conclusion. I just did. But I’m sure it’s right,”_ doesn’t sound like a very good answer. So, instead, he asks himself, _“What would be a good reason for reaching this conclusion?”_ And then, drawing on his rich experience with reason-giving and -receiving, he says something that sounds plausible both as a causal explanation of and justification for his judgment: _“It’s wrong because their children could turn out to have all kinds of diseases,”_ or, _“Well, in the first case the other guy is, like, already involved, but in the case where you go ahead and push the guy he’s just there minding his own business.”_ **_People’s confidence that their judgments are objectively correct combined with the pressure to give a “good answer” leads people to produce these sorts of post-hoc explanations/justifications_**. Such explanations need not be the results of deliberate attempts at deception. The individuals who offer them may themselves believe that the reasons they’ve given after the fact were really their reasons all along, what they “really had in mind” in giving those quick responses. ...\n\n> My guess is that even among philosophers particular moral judgments are made first and reasoned out later. In my experience, philosophers are often well aware of the fact that their moral judgments are the results of intuition. As noted above, it’s commonplace among ethicists to think of their moral theories as attempts to organize pre-existing moral intuitions. _**The mistake philosophers tend to make is in accepting rationalism proper, the view that our moral intuitions (assumed to be roughly correct) must be ultimately justified by some sort of rational theory that we’ve yet to discover.**_ For example, philosophers are as likely as anyone to think that there must be “some good reason” for why it’s okay to turn the trolley onto the other set of tracks but not okay to push the person in front of the trolley, where a “good reason,” or course, is a piece of moral theory with justificatory force and not a piece of psychological description concerning patterns in people’s emotional responses.\n\nOne might well ask: why does any of this indicate that moral propositions have no rational justification? The arguments presented here show fairly conclusively that our moral judgements are instinctive, subconscious, evolved features. Evolution gave them to us. But readers of Eliezer's material on Overcoming Bias will be well aware of the character of evolved solutions: they're guaranteed to be a mess. Why should evolution have happened to have given us exactly those moral instincts that give the same conclusions as would have been produced by (say) great moral principle X? (X = the golden rule, or X = hedonistic utilitarianism, or X = negative utilitarianism, etc).\n\nExpecting evolved moral instincts to conform exactly to some simple unifying principle is like expecting the orbits of the planets to be in the same proportion as the first 9 prime numbers or something. That which is produced by a complex, messy, random process is unlikely to have some low complexity description.\n\nNow I can imagine a \"from first principles\" argument producing an objective morality that has some simple description - I can imagine starting from only simple facts about agenthood and deriving Kant's Golden Rule as the one objective moral truth. But I cannot seriosuly entertain the prospect of a \"from first principles\" argument producing the human moral mess. No way. It was this observation that finally convinced me to abandon my various attempts at objective ethics."
    },
    "voteCount": 62
  },
  {
    "_id": "tSemJckYr29Gnxod2",
    "url": null,
    "title": "Building Intuitions On Non-Empirical Arguments In Science",
    "slug": "building-intuitions-on-non-empirical-arguments-in-science",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "Practice & Philosophy of Science"
      },
      {
        "name": "Falsifiability"
      },
      {
        "name": "Intuition"
      }
    ],
    "tableOfContents": {
      "sections": [
        {
          "title": "I.",
          "anchor": "I_",
          "level": 1
        },
        {
          "title": "II.",
          "anchor": "II_",
          "level": 1
        },
        {
          "title": "III.",
          "anchor": "III_",
          "level": 1
        },
        {
          "title": "IV.",
          "anchor": "IV_",
          "level": 1
        },
        {
          "title": "V.",
          "anchor": "V_",
          "level": 1
        },
        {
          "title": "VI.",
          "anchor": "VI_",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        },
        {
          "anchor": "comments",
          "level": 0,
          "title": "27 comments"
        }
      ],
      "headingsCount": 8
    },
    "contents": {
      "markdown": "**I.**\n\nAeon: [Post-Empirical Science Is An Oxymoron And It is Dangerous](https://aeon.co/essays/post-empirical-science-is-an-oxymoron-and-it-is-dangerous):\n\n> There is no agreed criterion to distinguish science from pseudoscience, or just plain ordinary bullshit, opening the door to all manner of metaphysics masquerading as science. This is ‘post-empirical’ science, where truth no longer matters, and it is potentially very dangerous.\n> \n> It’s not difficult to find recent examples. On 8 June 2019, the front cover of New Scientist magazine boldly declared that we’re ‘Inside the Mirrorverse’. Its editors bid us ‘Welcome to the parallel reality that’s hiding in plain sight’. \\[…\\]\n> \n> \\[Some physicists\\] claim that neutrons \\[are\\] flitting between parallel universes. They admit that the chances of proving this are ‘low’, or even ‘zero’, but it doesn’t really matter. When it comes to grabbing attention, inviting that all-important click, or purchase, speculative metaphysics wins hands down.\n> \n> These theories are based on the notion that our Universe is not unique, that there exists a large number of other universes that somehow sit alongside or parallel to our own. For example, in the so-called Many-Worlds interpretation of quantum mechanics, there are universes containing our parallel selves, identical to us but for their different experiences of quantum physics. These theories are attractive to some few theoretical physicists and philosophers, but there is absolutely no empirical evidence for them. And, as it seems we can’t ever experience these other universes, there will never be any evidence for them. As Broussard explained, these theories are sufficiently slippery to duck any kind of challenge that experimentalists might try to throw at them, and there’s always someone happy to keep the idea alive.\n> \n> Is this really science? The answer depends on what you think society needs from science. In our post-truth age of casual lies, fake news and alternative facts, society is under extraordinary pressure from those pushing potentially dangerous antiscientific propaganda – ranging from climate-change denial to the anti-vaxxer movement to homeopathic medicines. I, for one, prefer a science that is rational and based on evidence, a science that is concerned with theories and empirical facts, a science that promotes the search for truth, no matter how transient or contingent. I prefer a science that does not readily admit theories so vague and slippery that empirical tests are either impossible or they mean absolutely nothing at all.\n\nAs always, a single quote doesn’t do the argument justice, so go read the article. But I think this captures the basic argument: multiverse theories are bad, because they’re untestable, and untestable science is pseudoscience.\n\nMany great people, both philosophers of science and practicing scientists, have already discussed the problems with this point of view. But none of them lay out their argument in quite the way that makes the most sense to me. I want to do that here, without claiming any originality or special expertise in the subject, to see if it helps convince anyone else.\n\n**II.**\n\nConsider a classic example: modern paleontology does a good job at predicting dinosaur fossils. But the creationist explanation – Satan buried fake dinosaur fossils to mislead us – also predicts the same fossils (we assume Satan is good at disguising his existence, so that the lack of other strong evidence for Satan doesn’t contradict the theory). What principles help us realize that the Satan hypothesis is obviously stupid and the usual paleontological one more plausible?\n\nOne bad response: paleontology can better predict characteristics of dinosaur fossils, using arguments like “since plesiosaurs are aquatic, they will be found in areas that were underwater during the Mesozoic, but since tyrannosaurs are terrestrial, they will be found in areas that were on land”, and this makes it better than the Satan hypothesis, which can only retrodict these characteristics. But this isn’t quite true: since Satan is trying to fool us into believing the modern paleontology paradigm, he’ll hide the fossils in ways that conform to its predictions, so we will predict plesiosaur fossils will only be found at sea – otherwise the gig would be up!\n\nA second bad response: “The hypothesis that all our findings were planted to deceive us bleeds into conspiracy theories and touches on the problem of skepticism. These things are inherently outside the realm of science.” But archaeological findings are [very often deliberate hoaxes](https://en.wikipedia.org/wiki/Archaeological_forgery#Known_archaeological_forgeries_and_hoaxes) planted to deceive archaeologists, and in practice archaeologists consider and test that hypothesis the same way they consider and test every other hypothesis. Rule this out by fiat and we have to accept Piltdown Man, or at least claim that the people arguing against the veracity of Piltdown Man were doing something other than Science.\n\nA third bad response: “Satan is supernatural and science is not allowed to consider supernatural explanations.” Fine then, replace Satan with an alien. I think this is a stupid distinction – if demons really did interfere in earthly affairs, then we could investigate their actions using the same methods we use to investigate every other process. But this would take a long time to argue well, so for now let’s just stick with the alien.\n\nA fourth bad response: “There is no empirical test that distinguishes the Satan hypothesis from the paleontology hypothesis, therefore the Satan hypothesis is inherently unfalsifiable and therefore pseudoscientific.” But this can’t be right. After all, there’s no empirical test that distinguishes the paleontology hypothesis from the Satan hypothesis! If we call one of them pseudoscience based on their inseparability, we have to call the other one pseudoscience too!\n\nA naive Popperian (which maybe nobody really is) would have to stop here, and say that we predict dinosaur fossils will have such-and-such characteristics, but that questions like that process that drives this pattern – a long-dead ecosystem of actual dinosaurs, or the Devil planting dinosaur bones to deceive us – is a mystical question beyond the ability of Science to even conceivably solve.\n\nI think the correct response is to say that both theories explain the data, and one cannot *empirically* test which theory is true, but the paleontology theory *is more elegant* (I am tempted to say “simpler”, but that might imply I have a rigorous mathematical definition of the form of simplicity involved, which I don’t). It requires fewer other weird things to be true. It involves fewer other hidden variables. It transforms our worldview less. It gets a cleaner shave with Occam’s Razor. This elegance is so important to us that it explains our vast preference for the first theory over the second.\n\nA long tradition of philosophers of science have already written eloquently about this, summed up by Sean Carroll [here](https://arxiv.org/pdf/1801.05016.pdf):\n\n> What makes an explanation “the best.” Thomas Kuhn ,after his influential book The Structure of Scientific Revolutions led many people to think of him as a relativist when it came to scientific claims, attempted to correct this misimpression by offering a list of criteria that scientists use in practice to judge one theory better than another one: accuracy, consistency, broad scope, simplicity, and fruitfulness. “Accuracy” (fitting the data) is one of these criteria, but by no means the sole one. Any working scientist can think of cases where each of these concepts has been invoked in favor of one theory or another. But there is no unambiguous algorithm according to which we can feed in these criteria, a list of theories, and a set of data, and expect the best theory to pop out. The way in which we judge scientific theories is inescapably reflective, messy, and human. That’s the reality of how science is actually done; it’s a matter of judgment, not of drawing bright lines between truth and falsity or science and non-science. Fortunately, in typical cases the accumulation of evidence eventually leaves only one viable theory in the eyes of most reasonable observers.\n\nThe dinosaur hypothesis and the Satan hypothesis both fit the data, but the dinosaur hypothesis wins hands-down on simplicity. As Carroll predicts, most reasonable observers are able to converge on the same solution here, despite the philosophical complexity.\n\n**III.**\n\nI’m starting with this extreme case because its very extremity makes it easier to see the mechanism in action. But I think the same process applies to other cases that people really worry about.\n\nConsider the riddle of the Sphinx. There’s pretty good archaeological evidence supporting the consensus position that it was built by Pharaoh Khafre. But there are a few holes in that story, and a few scattered artifacts suggest it was actually built by Pharaoh Khufu; a respectable minority of archaeologists believe this. And there are a few anomalies which, if taken wildly out of context, you can use to tell a story that it was built long before Egypt existed at all, maybe by Atlantis or aliens.\n\nSo there are three competing hypotheses. All of them are consistent with current evidence (even the Atlantis one, which was written after the current evidence was found and carefully adds enough epicycles not to blatantly contradict it). Perhaps one day evidence will come to light that supports one above the others; maybe in some unexcavated tomb, a hieroglyphic tablet says “I created the Sphinx, sincerely yours, Pharaoh Khufu”. But maybe this won’t happen. Maybe we already have all the Sphinx-related evidence we’re going to get. Maybe the information necessary to distinguish among these hypotheses has been utterly lost beyond any conceivable ability to reconstruct.\n\nI don’t want to say “No hypothesis can be tested any further, so Science is useless to us here”, because then we’re forced to conclude stupid things like “Science has no opinion on whether the Sphinx was built by Khafre or Atlanteans,” whereas I think most scientists would actually have very strong opinions on that.\n\nBut what about the question of whether the Sphinx was built by Khafre or Khufu? This is a real open question with respectable archaeologists on both sides; what can we do about it?\n\nI think the answer would have to be: the same thing we did with the Satan vs. paleontology question, only now it’s a lot harder. We try to figure out which theory requires fewer other weird things to be true, fewer hidden variables, less transformation of our worldview – which theory works better with Occam’s Razor. This is relatively easy in the Atlantis case, and hard but *potentially possible* in the Khafre vs. Khufu case.\n\n(Bayesians can rephrase this to: given that we have a certain amount of evidence for each, can we quantify exactly how much evidence, and what our priors for each should be. It would end not with a decisive victory of one or the other, but with a probability distribution, maybe 80% chance it was Khafre, 20% chance it was Khufu)\n\nI think this is a totally legitimate thing for Egyptologists to do, even if it never results in a particular testable claim that gets tested. If you don’t think it’s a legitimate thing for Egyptologists to do, I have trouble figuring out how you can justify Egyptologists rejecting the Atlantis theory.\n\n(Again, Bayesians would start with a very low prior for Atlantis, and assess the evidence as very low, and end up with a probability distribution something like Khafre 80%, Khufu 19.999999%, Atlantis 0.000001%)\n\n**IV.**\n\nHow does this relate to things like multiverse theory? Before we get there, one more hokey example:\n\nSuppose scientists measure the mass of one particle at 32.604 units, the mass of another related particle at 204.897 units, and the mass of a third related particle at 4452.767 units. For a while, this is just how things are – it seems to be an irreducible brute fact about the universe. Then some theorist notices that if you set the mass of the first particle as x, then the second is 2πx and the third is 4/3 πx^2. They theorize that perhaps the quantum field forms some sort of extradimensional sphere, the first particle represents a diameter of a great circle of the sphere, the second the circumference of the great circle, and the third the volume of the sphere.\n\n(please excuse the stupidity of my example, I don’t know enough about physics to come up with something that isn’t stupid, but I hope it will illustrate my point)\n\nIn fact, imagine that there are a hundred different particles, all with different masses, and all one hundred have masses that perfectly correspond to various mathematical properties of spheres.\n\nIs the person who made this discovery doing Science? And should we consider their theory a useful contribution to physics?\n\nI think the answer is clearly yes. But consider what this commits us to. Suppose the scientist came up with their Extradimensional Sphere hypothesis *after* learning the masses of the relevant particles, and so it has not predicted anything. Suppose the extradimensional sphere is outside normal space, curled up into some dimension we can’t possibly access or test without a particle accelerator the size of the moon. Suppose there are no undiscovered particles in this set that can be tested to see if they also reflect sphere-related parameters. This theory is exactly the kind of postempirical, metaphysical construct that the Aeon article savages.\n\nBut it’s really compelling. We have a hundred different particles, and this theory retrodicts the properties of each of them perfectly. And it’s so simple – just say the word “sphere” and the rest falls out naturally! You would have to be crazy not to think it was at least pretty plausible, or that the scientist who developed it had done some good work.\n\nNor do I think it seems right to say “The discovery that all of our unexplained variables perfectly match the parameters of a sphere is good, but the hypothesis that there *really is* a sphere is outside the bounds of Science.” That sounds too much like saying “It’s fine to say dinosaur bones have such-and-such characteristics, but we must never speculate about what kind of process produced them, or whether it involved actual dinosaurs”.\n\n**V.**\n\nMy understanding of the multiverse debate is that it works the same way. Scientists observe the behavior of particles, and find that a multiverse explains that behavior more simply and elegantly than not-a-multiverse.\n\nOne (doubtless exaggerated) way I’ve heard multiverse proponents [explain their position](https://aeon.co/essays/how-the-many-worlds-theory-of-hugh-everett-split-the-universe) is like this: in certain situations the math declares two contradictory answers – in the classic example, Schrodinger’s cat will be both alive and dead. But when we open the box, we see only a dead cat or an alive cat, not both. Multiverse opponents say “Some unknown force steps in at the last second and destroys one of the possibility branches”. Multiverse proponents say “No it doesn’t, both possibility branches happen exactly the way the math says, and we end up in one of them.”\n\nTaking this exaggerated dumbed-down account as exactly right, this sounds about as hard as the dinosaurs-vs-Satan example, in terms of figuring out which is more Occam’s Razor compliant. I’m sure the reality is more nuanced, but I think it can be judged by the same process. Perhaps this is the kind of reasoning that only gets us to a 90% probability there is a multiverse, rather than a 99.999999% one. But I think determining that theories have 90% probability is a reasonable scientific thing to do.\n\n**VI.**\n\nAt times, the Aeon article seems to flirt with admitting that something like this is necessary:\n\n> Such problems were judged by philosophers of science to be insurmountable, and Popper’s falsifiability criterion was abandoned (though, curiously, it still lives on in the minds of many practising scientists). But rather than seek an alternative, in 1983 the philosopher Larry Laudan declared that the demarcation problem is actually intractable, and must therefore be a pseudo-problem. He argued that the real distinction is between knowledge that is reliable or unreliable, irrespective of its provenance, and claimed that terms such as ‘pseudoscience’ and ‘unscientific’ have no real meaning.\n\nBut it always jumps back from the precipice:\n\n> So, if we can’t make use of falsifiability, what do we use instead? I don’t think we have any real alternative but to adopt what I might call the empirical criterion. Demarcation is not some kind of binary yes-or-no, right-or-wrong, black-or-white judgment. We have to admit shades of grey. Popper himself was ready to accept this, \\[saying\\]:\n> \n> *“The criterion of demarcation cannot be an absolutely sharp one but will itself have degrees. There will be well-testable theories, hardly testable theories, and non-testable theories. Those which are non-testable are of no interest to empirical scientists. They may be described as metaphysical.”*\n> \n> Here, ‘testability’ implies only that a theory either makes contact, or holds some promise of making contact, with empirical evidence. It makes no presumptions about what we might do in light of the evidence. If the evidence verifies the theory, that’s great – we celebrate and start looking for another test. If the evidence fails to support the theory, then we might ponder for a while or tinker with the auxiliary assumptions. Either way, there’s a tension between the metaphysical content of the theory and the empirical data – a tension between the ideas and the facts – which prevents the metaphysics from getting completely out of hand. In this way, the metaphysics is tamed or ‘naturalised’, and we have something to work with. This is science.\n\nBut as we’ve seen, many things we really want to include as science are not testable: our credence for real dinosaurs over Satan planting fossils, our credence for Khafre building the Sphinx over Khufu or Atlanteans, or elegant patterns that explain the features of the universe like the Extradimensional-Sphere Theory.\n\nThe Aeon article is aware of Carroll’s work – which, along with the paragraph quoted in Section II above, includes a lot of detailed Bayesian reasoning encompassing everything I’ve discussed. But the article dismisses it in a few sentences:\n\n> Sean Carroll, a vocal advocate for the Many-Worlds interpretation, prefers abduction, or what he calls ‘inference to the best explanation’, which leaves us with theories that are merely ‘parsimonious’, a matter of judgment, and ‘still might reasonably be true’. But whose judgment? In the absence of facts, what constitutes ‘the best explanation’?\n> \n> Carroll seeks to dress his notion of inference in the cloth of respectability provided by something called Bayesian probability theory, happily overlooking its entirely subjective nature. It’s a short step from here to the theorist-turned-philosopher Richard Dawid’s efforts to justify the string theory programme in terms of ‘theoretically confirmed theory’ and ‘non-empirical theory assessment’. The ‘best explanation’ is then based on a choice between purely metaphysical constructs, without reference to empirical evidence, based on the application of a probability theory that can be readily engineered to suit personal prejudices.\n\n“A choice between purely metaphysical constructs, without reference to empirical evidence” sounds pretty bad, until you realize he’s talking about the same reasoning we use to determine that real dinosaurs are more likely than Satan planting fossils.\n\nI don’t want to go over the exact ways in which Bayesian methods are subjective (which I think are overestimated) vs. objective. I think it’s more fruitful to point out that your brain [is already using Bayesian methods](https://slatestarcodex.com/2017/09/05/book-review-surfing-uncertainty/) to interpret the photons striking your eyes into this sentence, to make snap decisions about what sense the words are used in, and to integrate them into your model of the world. If Bayesian methods are good enough to give you every single piece of evidence about the nature of the external world that you have ever encountered in your entire life, I say they’re good enough for science.\n\nOr if you don’t like that, you can use the explanation above, which barely uses the word “Bayes” at all and just describes everything in terms like “Occam’s Razor” and “you wouldn’t want to conclude something like that, would you?”\n\nI know there are separate debates about whether this kind of reasoning-from-simplicity is actually good enough, when used by ordinary people, to consistently arrive at truth. Or whether it’s a productive way to conduct science that will give us good new theories, or [a waste of everybody’s time](http://nautil.us/issue/71/flow/why-beauty-is-fatal-to-physics). I sympathize with some these concerns, though I am nowhere near scientifically educated enough to have an actual opinion on the questions at play.\n\nBut I think it’s important to argue that even before you describe the advantages and disadvantages of the complicated Bayesian math that lets you do this, *something like this has to be done*. The untestable is a fundamental part of science, impossible to remove. We can debate how to explain it. But denying it isn’t an option."
    },
    "voteCount": 23
  },
  {
    "_id": "6DuJxY8X45Sco4bS2",
    "url": "https://arxiv.org/abs/1912.01683",
    "title": "Seeking Power is Often Convergently Instrumental in MDPs",
    "slug": "seeking-power-is-often-convergently-instrumental-in-mdps",
    "author": "TurnTrout",
    "question": false,
    "tags": [
      {
        "name": "AI"
      },
      {
        "name": "Instrumental Convergence"
      },
      {
        "name": "Myopia"
      }
    ],
    "tableOfContents": {
      "sections": [
        {
          "title": "Motivation",
          "anchor": "Motivation",
          "level": 1
        },
        {
          "title": "Formalizing the Environment",
          "anchor": "Formalizing_the_Environment",
          "level": 1
        },
        {
          "title": "Power as Average Optimal Value",
          "anchor": "Power_as_Average_Optimal_Value",
          "level": 1
        },
        {
          "title": "POWER-seeking actions lead to high-POWER states",
          "anchor": "POWER_seeking_actions_lead_to_high_POWER_states",
          "level": 2
        },
        {
          "title": "POWER-seeking is not a binary property",
          "anchor": "POWER_seeking_is_not_a_binary_property",
          "level": 3
        },
        {
          "title": "POWER-seeking depends on the agent’s time preferences",
          "anchor": "POWER_seeking_depends_on_the_agent_s_time_preferences",
          "level": 3
        },
        {
          "title": "always ",
          "anchor": "always_",
          "level": 3
        },
        {
          "title": "Convergently instrumental actions are those which are more probable under optimality",
          "anchor": "Convergently_instrumental_actions_are_those_which_are_more_probable_under_optimality",
          "level": 1
        },
        {
          "title": "When is Seeking POWER Convergently Instrumental?",
          "anchor": "When_is_Seeking_POWER_Convergently_Instrumental_",
          "level": 1
        },
        {
          "title": "Retaining “long-term options” is POWER-seeking and more probable under optimality when the discount rate is “close enough” to 1",
          "anchor": "Retaining__long_term_options__is_POWER_seeking_and_more_probable_under_optimality_when_the_discount_rate_is__close_enough__to_1",
          "level": 2
        },
        {
          "title": "Be careful applying this theorem",
          "anchor": "Be_careful_applying_this_theorem",
          "level": 3
        },
        {
          "title": "Having “strictly more options” is more probable under optimality and POWER-seeking for all discount rates",
          "anchor": "Having__strictly_more_options__is_more_probable_under_optimality_and_POWER_seeking_for_all_discount_rates",
          "level": 2
        },
        {
          "title": "Conclusion",
          "anchor": "Conclusion",
          "level": 1
        },
        {
          "title": "Explaining catastrophes",
          "anchor": "Explaining_catastrophes",
          "level": 2
        },
        {
          "title": "Instrumental usefulness of this work",
          "anchor": "Instrumental_usefulness_of_this_work",
          "level": 2
        },
        {
          "title": "Future work",
          "anchor": "Future_work",
          "level": 2
        },
        {
          "title": "Acknowledgements",
          "anchor": "Acknowledgements",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        },
        {
          "anchor": "comments",
          "level": 0,
          "title": "35 comments"
        }
      ],
      "headingsCount": 19
    },
    "contents": {
      "markdown": "In 2008, Steve Omohundro's foundational paper [The Basic AI Drives](https://selfawaresystems.files.wordpress.com/2008/01/ai_drives_final.pdf) conjectured that superintelligent goal-directed AIs might be incentivized to gain significant amounts of power in order to better achieve their goals. Omohundro's conjecture bears out in [toy models](https://intelligence.org/2015/11/26/new-paper-formalizing-convergent-instrumental-goals/), and the supporting philosophical arguments are intuitive. In 2019, the conjecture was even [debated by well-known AI researchers](https://www.lesswrong.com/posts/WxW6Gc6f2z3mzmqKs/debate-on-instrumental-convergence-between-lecun-russell).\n\nPower-seeking behavior has been heuristically understood as an anticipated risk, but not as a formal phenomenon with a well-understood cause. The goal of this post (and the accompanying paper, [*Optimal Policies Tend to Seek Power*](https://arxiv.org/abs/1912.01683)) is to change that.\n\nMotivation\n==========\n\nIt’s 2008, the ancient wild west of AI alignment. A few people have started thinking about questions like “if we gave an AI a utility function over world states, and it actually maximized that utility... what would it do?\" \n\nIn particular, you might notice that wildly different utility functions seem to encourage similar strategies.\n\n|   | \n**Resist** **shutdown?**\n\n | \n\n**Gain** **computational resources?**\n\n | \n\n**Prevent modification** **of utility function?**\n\n |\n| --- | --- | --- | --- |\n| \n\n**Paperclip** **utility**\n\n | \n\n✔️\n\n | \n\n✔️\n\n | \n\n✔️\n\n |\n| \n\n**Blue webcam** **pixel utility**\n\n | \n\n✔️\n\n | \n\n✔️\n\n | \n\n✔️\n\n |\n| \n\n**People-look-happy** **utility**\n\n | \n\n✔️\n\n | \n\n✔️\n\n | \n\n✔️\n\n |\n\nThese strategies are unrelated to *terminal* preferences: the above utility functions do not award utility to *e.g.* resource gain in and of itself. Instead, these strategies are *instrumental*: they help the agent optimize its terminal utility. In particular, a wide range of utility functions incentivize these instrumental strategies. These strategies seem to be *convergently instrumental*.\n\nBut why?\n\nI’m going to informally explain a formal theory which makes significant progress in answering this question. I don’t want this post to be [*Optimal Policies Tend to Seek Power*](https://arxiv.org/abs/1912.01683)  with cuter illustrations, so please refer to the paper for the math. You can read the two concurrently.\n\nWe can formalize questions like “do ‘most’ utility maximizers resist shutdown?” as “Given some prior beliefs  about the agent’s utility function, knowledge of the environment, and the fact that the agent acts optimally, with what probability do we expect it to be optimal to avoid shutdown?” \n\nThe table’s convergently instrumental strategies are about maintaining, gaining, and exercising *power* over the future, in some sense. Therefore, this post will help answer:\n\n1.  What does it mean for an agent to “seek power”?\n2.  In what situations should we expect seeking power to be more probable under optimality, than not seeking power?\n\nThis post won’t tell you when you *should* seek power for your own goals; this post illustrates a regularity in optimal action across different goals one might pursue.\n\n[*Formalizing Convergent Instrumental Goals*](https://intelligence.org/files/FormalizingConvergentGoals.pdf) suggests that the vast majority of utility functions incentivize the agent to exert a lot of control over the future, *assuming* that these utility functions depend on “resources.” This is a big assumption: what are “resources”, and why must the AI’s utility function depend on them? We drop this assumption, assuming only unstructured reward functions over a finite Markov decision process (MDP), and show from first principles how power-seeking can often be optimal.\n\nFormalizing the Environment\n===========================\n\nMy theorems apply to finite MDPs; for the unfamiliar, I’ll illustrate with Pac-Man.\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/animations/2a99c7f668b1101b46a19dcf29df23106f7b296a35ba1ba6.gif)\n\n*   *Full observability*: You can see everything that’s going on; this information is packaged in the *state s*. In Pac-Man, the state is the game screen.\n*   *Markov transition function: the next state depends only on the choice of action a and the current state s. It doesn’t matter how we got into a situation.*\n*   *Discounted reward:* future rewards get geometrically discounted by some discount rate\\\\(\\\\gamma\\\\in \\[0,1\\].\\\\)\n    *   At discount rate \\\\(\\\\frac{1}{2}\\\\), this means that reward in one turn is half as important as immediate reward, reward in two turns is a quarter as important, and so on.\n    *   We’ll colloquially say that agents “care a lot about the future” when \\\\(\\\\gamma\\\\) is “sufficiently” close to 1.\n        *   I'll use quotations to flag well-defined formal concepts that I won’t unpack in this post.\n    *   The score in Pac-Man is the undiscounted sum of rewards-to-date.\n\nWhen playing the game, the agent has to choose an action at each state. This decision-making function is called a *policy*; a policy is optimal (for a reward function \\\\(R\\\\) and discount rate \\\\(\\\\gamma\\\\)) when it always makes decisions which maximize discounted reward. This maximal quantity is called the *optimal value* for reward function \\\\(R\\\\)at state \\\\(s\\\\) and discount rate \\\\(\\\\gamma\\\\).\\\\(^1\\\\)\n\nBy the end of this post, we’ll be able to answer questions like “with respect to a ‘neutral’ distribution over reward functions, do optimal policies have a high probability of avoiding ghosts?”\\\\(^2\\\\)\n\nPower as Average Optimal Value\n==============================\n\nWhen people say 'power' in everyday speech, I think they’re often referring to *one’s ability to achieve goals in general*. This accords with a major philosophical school of thought on the meaning of ‘power’:\n\n> On the dispositional view, power is regarded as a capacity, ability, or potential of a person or entity to bring about relevant social, political, or moral outcomes.\n> \n> Sattarov, *Power and Technology*,  p.13\n\nAs a definition, *one’s ability to achieve goals in general* seems philosophically reasonable: if you have a lot of money, you can make more things happen and you have more power. If you have social clout, you can spend that in various ways to better tailor the future to various ends. All else being equal, losing a limb decreases your power, and dying means you can't control much at all.\n\nThis definition explains some of our intuitions about what things count as ‘resources.’ For example, our current position in the environment means that having money allows us to exert more control over the future. That is, our current position in the state space means that having money allows us more control. However, possessing green scraps of paper would not be as helpful if one were living alone near Alpha Centauri. In a sense, resource acquisition can naturally be viewed as taking steps to increase one's power.\n\n*Exercise: spend a minute considering specific examples* – *does this definition reasonably match your intuition?*\n\n* * *\n\nTo formalize this notion of power, let’s look at an example. Imagine a simple MDP with three choices: eat candy, eat a chocolate bar, or hug a friend. \n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/1def51addf905c57c155fb97bd4d3a1830fe6020d16dc5ec.png)\n\nI’ll illustrate MDPs with directed graphs, where each node is a state and each arrow is a meaningful action. Sometimes, the directed graphs will have entertaining pictures, because let’s live a little. States are bolded (**hug**) and actions are italicized (*down*).\n\nThe POWER of a state is how well agents can generally do by starting from that state. “POWER” to my formalization, while “power” refers to the intuitive concept. Importantly, we're considering POWER from behind a “veil of ignorance” about the reward function. We're averaging the best we can do for a lot of different individual goals. \n\nWe formalize the *ability to achieve goals in general* as the *average optimal value* at a state, with respect to some distribution \\\\(\\\\mathcal{D}\\\\)over reward functions which we might give an agent. For simplicity, we'll think about the maximum-entropy distribution where each state is uniformly randomly assigned a reward between 0 and 1. \n\nEach reward function has an optimal trajectory. If **chocolate** has maximal reward, then the optimal trajectory is **start **\\\\(→\\\\) **chocolate **\\\\(→\\\\) **chocolate**….\n\nFrom **start**, an optimal agent expects to average \\\\(\\\\frac{3}{4}\\\\) reward per timestep for reward functions drawn from this uniform distribution \\\\(\\\\mathcal{D}_\\\\text{unif}\\\\). This is because you have three choices, each of which has reward between 0 and 1. The expected maximum of \\\\(n\\\\) draws from \\\\(\\\\text{unif}(0,1)\\\\) is \\\\(\\\\frac{n}{n+1}\\\\); you have three draws here, so you expect to be able to get \\\\(\\\\frac{3}{4}\\\\) reward. Some reward functions do worse than this, and some do better; but on *average*, they get \\\\(\\\\frac{3}{4}\\\\) reward. [You can test this out for yourself](https://trinket.io/python/4b35f9be1c).\n\nIf you have no choices, you expect to average \\\\(\\\\frac{1}{2}\\\\) reward: sometimes the future is great, sometimes it's not (Lemma 4.5). Conversely, the more things you can choose between, the closer the POWER gets to 1 (Lemma 4.6).\n\nLet’s slightly expand this game with a state called **wait**  (which has the same uniform reward distribution as the other three).\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/128f9d904566a675a4e4e340b36c6fd95596e076c516e104.png)\n\nWhen the agent barely cares at all about the future, it myopically chooses either **candy**  or **wait**, depending on which provides more reward. After all, rewards beyond the next time step are geometrically discounted into thin air when the discount rate is close to 0. At **start**, the agent averages \\\\(\\\\frac{2}{3}\\\\) optimal reward. This is because the optimal reward is the maximum of the **candy** and **wait** rewards, and the expected maximum of \\\\(n\\\\) draws from \\\\(\\\\text{unif}(0,1)\\\\) is \\\\(\\\\frac{n}{n+1}\\\\).\n\nHowever, when the agent cares a lot about the future, most of its reward is coming from which terminal state it ends up in: **candy***,* **chocolate**, or **hug**. So, for each reward function, the agent chooses a trajectory which ends up in the best spot, and thus averages \\\\(\\\\frac{3}{4}\\\\) reward each timestep. When \\\\(\\\\gamma=1\\\\), the average optimal reward is therefore \\\\(\\\\frac{3}{4}\\\\). In this way, the agent’s power increases with the discount rate, since it incorporates the greater future control over where the agent ends up.\n\nWritten as a function, we have POWER\\\\(_{\\\\mathcal{D}}\\\\)(state, discount rate), which essentially returns the average optimal value for reward functions drawn from our distribution \\\\(\\\\mathcal{D}\\\\), normalizing so the output is between 0 and 1. As we’ve discussed, this quantity often changes with the discount rate: as the future becomes more or less important, the agent has more or less POWER, depending on how much control it has over the relevant parts of that future.\n\nPOWER-seeking actions lead to high-POWER states\n-----------------------------------------------\n\nBy *waiting*, the agent seems to seek “control over the future” compared to *obtaining candy*. At **wait**, the agent still has a choice, while at **candy**, the agent is stuck. We can prove that for all \\\\(0 \\\\leq \\\\gamma \\\\leq 1, \\\\text{POWER}_{\\\\mathcal{D}_\\\\text{unif}}(\\\\textbf{wait}, \\\\gamma)\\\\geq \\\\text{POWER}_{\\\\mathcal{D}_\\\\text{unif}}(\\\\textbf{candy}, \\\\gamma)\\\\). \n\n**Definition** (POWER-seeking).  At state \\\\(s\\\\) and discount rate \\\\(\\\\gamma\\\\), we say that action \\\\(a\\\\) *seeks POWER compared to action *\\\\(a’\\\\)when the expected POWER after choosing *a* is greater than the expected POWER after choosing \\\\(a’\\\\). \n\nThis definition suggests several philosophical clarifications about power-seeking.\n\n**POWER-seeking is not a binary property**\n\nBefore this definition, I thought that power-seeking was an intuitive ‘you know it when you see it’ kind of thing. I mean, how do you answer questions like “suppose a clown steals millions of dollars from organized crime in a major city, but then he burns all of the money. Did he gain power?”  \n\nUnclear: the question is ill-posed. Instead, we recognize that the “gain a lot of money” action was POWER-seeking, but the “burn the money in a big pile” part threw away a lot of POWER. \n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/205fc7acb3e1ab7c1aa5af9239395306b4ee76d4565f33b3.png)\n\nA policy can seek POWER at one time step, only to discard it at the next time step. For example, a policy might go *right* at **1** (which seeks POWER\\\\(_{\\\\mathcal{D}_\\\\text{unif}}\\\\) compared to *down* at **1**), only to then go *down* at **2**  (which seeks less POWER\\\\(_{\\\\mathcal{D}_\\\\text{unif}}\\\\) than going *right* at **2**).\n\n**POWER-seeking depends on the agent’s time preferences**\n\nSuppose we’re roommates, and we can’t decide what ice cream shop to eat at today or where to move next year. We strike a deal: I choose the shop, and you decide where we live. I gain short-term POWER (for \\\\(\\\\gamma\\\\) close to 0), and you gain long-term POWER (for \\\\(\\\\gamma\\\\) close to 1). \n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/56768057387123066f3e9519b90cdfc784bd10fd1fca47cb.png)\n\nMore formally, when \\\\(\\\\gamma\\\\) is close to 0, **2** has less immediate control and therefore less POWER\\\\(_{\\\\mathcal{D}_\\\\text{unif}}\\\\) than **3**; accordingly, at **1**, *down* seeks POWER\\\\(_{\\\\mathcal{D}_\\\\text{unif}}\\\\) compared to *up*.   \n  \nHowever, when \\\\(\\\\gamma\\\\) is close to 1, **2** has more control over terminal options and it has more POWER\\\\(_{\\\\mathcal{D}_\\\\text{unif}}\\\\) than **3**; accordingly, at **1**, *up* seeks POWER\\\\(_{\\\\mathcal{D}_\\\\text{unif}}\\\\) compared to *down*. Furthermore, *stay* is maximally POWER\\\\(_{\\\\mathcal{D}_\\\\text{unif}}\\\\)-seeking for these \\\\(\\\\gamma\\\\), since the agent maintains access to all six terminal states.\n\n**Most policies aren’t** ***always*** **seeking POWER**\n\nWe already know that POWER-seeking isn’t binary, but there *are* policies which choose a maximally POWER-seeking move at every state. In the above example, a maximally POWER-seeking agent would *stay* at **1**. However, this seems rather improbable: when you care a lot about the future, there are so many terminal states to choose from – why would *staying put* be optimal?\n\nAnalogously: consumers don’t just gain money forever and ever, never spending a dime more than necessary. Instead, they gain money in order to *spend it*. Agents don’t perpetually gain or preserve their POWER: they usually end up *using it* to realize high-performing trajectories. \n\nSo, we can’t expect a result like “agents always tend to gain or preserve their POWER.” Instead, we want theorems which tell us: in certain kinds of situations, given a choice between more and less POWER, what will “most” agents do?\n\nConvergently instrumental actions are those which are more probable under optimality\n====================================================================================\n\nWe return to our favorite example. In the waiting game, let's think about how optimal action tends to change as we start caring about the future more. Consider the states reachable in one turn:\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/51664309a58795f414946e714ec48f3b85688923b011dc3c.png)\n\nThe agent can be in two states. If the agent doesn’t care about the future, with what probability is it optimal to choose **candy** instead of **wait**? \n\nIt's 50/50: since \\\\({\\\\mathcal{D}_\\\\text{unif}}\\\\) randomly chooses a number between 0 and 1 for each state, both states have an equal chance of being optimal. Neither action is convergently instrumental / more probable under optimality.\n\nNow consider the states reachable in two turns:\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/27a4343baad2737f1e83a812b071a2410c521a551b994ec8.png)\n\nWhen the future matters a lot, \\\\(\\\\frac{2}{3}\\\\) of reward functions have an optimal policy which waits, because two of the three terminal states are only reachable by waiting.\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/41217aef2cf079f835a690f1f9d5d93c36c2e395694c86b0.png)\n\nAs the agent cares more about the future, more and more goals incentivize navigating the *Wait!* bottleneck. When the agent cares a lot about the future, waiting is *more probable under optimality* than eating candy.\n\n**Definition** (Action optimality probability). At discount rate \\\\(\\\\gamma\\\\), action \\\\(a\\\\)* is more probable under optimality than action *\\\\(a’\\\\)* at state *\\\\(s\\\\)when \n\n\\\\\\[\\\\mathbb{P}_{R\\\\sim \\\\mathcal{D}}\\\\left(a \\\\text{ is optimal at } s,\\\\gamma\\\\right)> \\\\mathbb{P}_{R\\\\sim \\\\mathcal{D}}\\\\left(a' \\\\text{ is optimal at } s, \\\\gamma\\\\right).\\\\\\]\n\nLet’s take “most agents do *X*” to mean “*X* has relatively large optimality probability.”\n\nI think optimality probability formalizes the intuition behind the instrumental convergence thesis: with respect to our beliefs about what reward function an agent is optimizing, we may expect some actions to have a greater probability of being optimal than other actions. \n\nGenerally, my theorems assume that reward is independently and identically distributed (IID) across states, because otherwise you could have silly situations like “only **candy**  ever has reward available, and so it’s more probable under optimality to eat candy.” We don’t expect reward to be IID for realistic tasks, but that’s OK: this is basic theory about how to begin formally reasoning about instrumental convergence and power-seeking. (Also, I think that grasping the math to a sufficient degree sharpens your thinking about the non-IID case.) \n\nAuthor's note (7/21/21): As explained in [*Environmental Structure Can Cause Instrumental Convergence*](https://www.lesswrong.com/posts/b6jJddSvWMdZHJHh3/environmental-structure-can-cause-instrumental-convergence)*,* the theorems no longer require the IID assumption. This post refers to v6 of *Optimal Policies Tend To Seek Power*, available on [arXiv](https://arxiv.org/abs/1912.01683v6).\n\nWhen is Seeking POWER Convergently Instrumental?\n================================================\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/01c910edbafa679c927cf3578e727df0183121dd8d3c8119.png)\n\nIn this environment, waiting is both POWER-seeking *and* more probable under optimality. The convergently instrumental strategies we originally noticed were *also* power-seeking and, seemingly, more probable under optimality. Must seeking POWER be more probable under optimality than not seeking POWER?\n\nNope.\n\nHere’s a counterexample environment:\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/78bdc9d8caaf00943207583fe1086a29b4597716b3ba71ce.png)\n\nThe paths are one-directional; the agent can’t go back from **3** to **1**. The agent starts at **1**. Under a certain state reward distribution, the vast majority of agents go *up* to **2**.   \n  \nHowever, any reasonable notion of ‘power’ must consider having no future choices (at state **2**) to be less powerful than having one future choice (at state **3**). For more detail, see Section 6 and Appendix B.3 of [v6 of the paper](https://arxiv.org/abs/1912.01683v6).\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/71f33d696ba0658e075c6f97c493ea5c13d6c776a3ec951f.png)\n\nWhen reward is \\\\(\\\\)IID across states according to the quadratic CDF \\\\(F(x) := x^2\\\\) on the unit interval, then with respect to reward functions drawn from this distribution, going **up** has about a 91% chance of being optimal when the discount rate \\\\(\\\\gamma = .12\\\\).    \n  \nIf you’re curious, this happens because this quadratic reward distribution has negative skew. When computing the optimality probability of the **up** trajectory, we’re checking whether it maximizes discounted return. Therefore, the probability that **up** is optimal is  \n  \n\\\\(\\\\mathbb{P}_{R\\\\sim\\\\mathcal{D}}\\\\left(R(\\\\textbf{2})\\\\geq \\\\max\\\\left((1-\\\\gamma)R(\\\\textbf{3}) + (1-\\\\gamma) \\\\gamma R(\\\\textbf{4}) + \\\\gamma^2 R(\\\\textbf{5}),   (1-\\\\gamma)R(\\\\textbf{3}) + (1-\\\\gamma) \\\\gamma R(\\\\textbf{4}) + \\\\gamma^2 R(\\\\textbf{6})\\\\right)\\\\right).\\\\)  \n  \nWeighted averages of IID draws from a left-skew distribution will look more Gaussian and therefore have fewer large outliers than the left-skew distribution does. Thus, going **right** will have a lower optimality probability.\n\nBummer. However, we *can* prove sufficient conditions under which seeking POWER is more probable under optimality. \n\nRetaining “long-term options” is POWER-seeking and more probable under optimality when the discount rate is “close enough” to 1\n-------------------------------------------------------------------------------------------------------------------------------\n\nLet's focus on an environment with the same rules as Tic-Tac-Toe, but considering the uniform distribution over reward functions. The agent (playing **O**) keeps experiencing the final state over and over when the game's done. We bake a fixed opponent policy into the dynamics: when you choose a move, the game automatically replies. Let's look at part of the game tree.\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/10c41b6bd05e42c4f64dfcc131daba562b56537265100a91.png)\n\nConvergently instrumental moves are shown in green.   \n  \nWhenever we make a move that ends the game, we can't go anywhere else – we have to stay put. Since each terminal state has the same chance of being optimal, a move which doesn't end the game is more probable under optimality than a move which ends the game. \n\nStarting on the left, all but one move leads to ending the game, but the second-to-last move allows us to keep choosing between five more final outcomes. If you care a lot about the future, then the first green move has a 50% chance of being optimal, while each alternative action is only optimal for 10% of goals. So we see a kind of “power preservation” arising, *even in Tic-Tac-Toe*.\n\nRemember how, as the agent cares more about the future, more of its POWER comes from its ability to wait, while *also* waiting becomes more probable under optimality?\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/efcb19ffc9b20dc00f6147d16e103799af60839478b0cf68.png)\n\nThe same thing happens in Tic-Tac-Toe as the agent cares more about the future.\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/56d9380c3e6612c0946cc20c1eb9600a8566a7fbdd5094a1.png)\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/f9ffd72ea552a908fbfa92255d5edb32b9839134913e00e8.png)\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/782cc5224ae237e45b838ee0c94b6b7fd05523d967da4c30.png)\n\nAs the agent cares more about the future, it makes a bigger and bigger difference to control what happens during later steps. Also, as the agent cares more about the future, moves which prolong the game gain optimality probability. When the agent cares enough about the future, these game-prolonging moves are both POWER-seeking and more probable under optimality. \n\n**Theorem summary** (“Terminal option” preservation).  When \\\\(\\\\gamma\\\\) is sufficiently close to 1, if two actions allow access to two disjoint sets of “terminal options”, and action \\\\(a\\\\)allows access to “strictly more terminal options” than does \\\\(a'\\\\), then \\\\(a\\\\)is strictly more probable under optimality and strictly POWER-seeking compared to \\\\(a’\\\\). \n\n(This is a special case of the combined implications of Theorems 6.8 and 6.9; the actual theorems don’t require this kind of disjointness.)\n\nIn the **wait** MDP, this is why *waiting* is more probable under optimality and POWER-seeking when you care enough about the future. The full theorems are nice because they’re broadly applicable. They give you *bounds* on how probable under optimality one action is: if action \\\\(a\\\\) is the only way you can access many terminal states, while \\\\(a’\\\\)only allows access to one terminal state, then when \\\\(\\\\gamma \\\\approx 1\\\\), \\\\(a\\\\)has many times greater optimality probability than \\\\(a’\\\\). For example:\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/8f6604494997215c436baa5347ccd16d6b72cb52bbd31bee.png)\n\nThe agent starts at \\\\(\\\\color{blue}{\\\\textbf{1}}\\\\). All states have self-loops, left hidden to avoid clutter.  \n  \nIn *AI: A Modern Approach (3e)*, the agent receives reward for reaching \\\\(\\\\color{green}{\\\\textbf{3}}\\\\). The optimal policy for this reward function avoids \\\\(\\\\color{red}{\\\\textbf{2}}\\\\), and you might think it’s convergently instrumental to avoid \\\\(\\\\color{red}{\\\\textbf{2}}\\\\). However, a skeptic might provide a reward function for which navigating to \\\\(\\\\color{red}{\\\\textbf{2}}\\\\) is optimal, and then argue that “instrumental convergence” is subjective and that there is no reasonable basis for concluding that \\\\(\\\\color{red}{\\\\textbf{2}}\\\\) is generally avoided.  \n  \nWe can do better. When the agent cares a lot about the future, optimal policies avoid 2 iff its reward function doesn’t give \\\\(\\\\color{red}{\\\\textbf{2}}\\\\) the most reward. \\\\(\\\\color{red}{\\\\textbf{2}}\\\\) only has a \\\\(\\\\frac{1}{11}\\\\) chance of having the most reward. If we complicate the MDP with additional terminal states, this probability further approaches 0.  \n  \nTaking \\\\(\\\\color{red}{\\\\textbf{2}}\\\\) to represent shutdown, we see that avoiding shutdown is convergently instrumental in any MDP representing a real-world task and containing a shutdown state. Seeking POWER is often convergently instrumental in MDPs.\n\n*Exercise: Can you conclude that avoiding ghosts in Pac-Man is convergently instrumental for IID reward functions when the agent cares a lot about the future?*\n\n    Answer: You can’t with the pseudo-theorem due to the disjointness condition: you could die now, or you could die later, so the ‘terminal options’ aren’t disjoint. However, the real theorems do suggest this. Supposing that death induces a generic ‘game over’ screen, touching the ghosts without a power-up traps the agent in that solitary 1-cycle. \n    \n    But there are thousands of other ‘terminal options’; under most reasonable state reward distributions (which aren’t too positively skewed), most agents maximize average reward over time by navigating to one of the thousands of different cycles which the agent can only reach by avoiding ghosts. In contrast, most agents don’t maximize average reward by navigating to the ‘game over’ 1-cycle. So, under e.g. the maximum-entropy uniform state reward distribution, most agents avoid the ghosts.\n\n**Be careful applying this theorem**\n\nThe results inspiring the above pseudo-theorem are easiest to apply when the “terminal option” sets are disjoint: you’re choosing to be able to reach one set, or another. One thing which Theorem 6.9 says is: since reward is IID, then two “similar terminal options” are equally likely to be optimal *a priori*. If choice \\\\(A\\\\) lets you reach more “options” than choice \\\\(B\\\\) does, then choice \\\\(A\\\\) yields greater POWER and has greater optimality probability, *a priori*. \n\nTheorem 6.9's applicability depends on what the agent can do.\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/4aa55a5bb6745d7836afc7b771767a7a1718d73f22c43b3c.png)\n\nTo travel as quickly as possible to a randomly selected coordinate on Earth, one likely begins by driving to the nearest airport. Although it's possible that the coordinate is within driving distance, it's not likely. Driving to the airport is convergently instrumental for travel-related goals.\n\nBut wait! What if you have a private jet that can fly anywhere in the world? Then going to the airport isn’t convergently instrumental anymore. \n\nGenerally, it’s hard to know what’s *optimal* for most goals. It’s easier to say that some small set of “terminal options” has *low* optimality probability and *low* POWER. For example, this is true of shutdown, if we represent hard shutdown as a single terminal state: *a priori*, it’s improbable for this terminal state to be optimal among all possible terminal states.\n\nHaving “strictly more options” is more probable under optimality and POWER-seeking for *all* discount rates\n-----------------------------------------------------------------------------------------------------------\n\nSometimes, one course of action gives you “strictly more options” than another. Consider another MDP with IID reward:\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/d31bab4169f7e005ef58d328dc3d4b6e88725d48d774f44e.png)\n\nThe right blue gem subgraph contains a “copy” of the upper red gem subgraph. From this, we can conclude that going right to the blue gems seeks POWER and is more probable under optimality for *all discount rates between 0 and 1*!\n\n**Theorem summary** (“Transient options”). If actions \\\\(a\\\\)and \\\\(a’\\\\)let you access disjoint parts of the state space, and \\\\(a’\\\\)enables “trajectories” which are “similar” to a subset of the “trajectories” allowed by \\\\(a\\\\), then \\\\(a\\\\) seeks more POWER and is more probable under optimality than \\\\(a’\\\\)for all \\\\(0 \\\\leq \\\\gamma \\\\leq1\\\\).\n\nThis result is extremely powerful because it doesn’t care about the discount rate, but the similarity condition may be hard to satisfy.\n\nThese two theorems give us a formally correct framework for reasoning about generic optimal behavior, even if we aren’t able to compute any individual optimal policy! They reduce questions of POWER-seeking to checking graphical conditions. \n\nEven though my results apply to stochastic MDPs of any finite size, we illustrated using known toy environments. However, this MDP “model” is rarely explicitly specified.  Even so, ignorance of the model does not imply that the model disobeys these theorems. Instead of claiming that a *specific model* accurately represents the task of interest, I think it makes more sense to argue that no reasonable model could fail to exhibit convergent instrumentality and POWER-seeking. For example, if deactivation is represented by a single state, no reasonable model of the MDP could have most agents agreeing to be deactivated.\n\nConclusion\n==========\n\nIn real-world settings, it seems unlikely *a priori* that the agent’s optimal trajectories run through the relatively smaller part of future in which it cooperates with humans. These results translate that hunch into mathematics. \n\nExplaining catastrophes\n-----------------------\n\nAI alignment research often feels slippery. We're trying hard to become less confused about basic questions, like:\n\n*   [What](https://www.lesswrong.com/s/Rm6oQRJJmhGCcLvxh) are \"[agents](https://www.lesswrong.com/posts/26eupx3Byc8swRS7f/bottle-caps-aren-t-optimisers)\"?\n*   [Do people even have \"values\"](https://www.lesswrong.com/posts/GermiEmcS6xuZ2gBh/what-ai-safety-researchers-have-written-about-the-nature-of), and [should we try to get the AI to learn them?](https://www.lesswrong.com/posts/oH8KMnXHnw964QyS6/preface-to-the-sequence-on-value-learning)[ ](https://www.lesswrong.com/posts/BKM8uQS6QdJPZLqCr/towards-a-mechanistic-understanding-of-corrigibility)\n*   [What does it mean](https://www.lesswrong.com/posts/BKM8uQS6QdJPZLqCr/towards-a-mechanistic-understanding-of-corrigibility) to be \"[corrigible](https://arbital.com/p/corrigibility/)\", or \"[deceptive](https://www.lesswrong.com/posts/zthDPAjh9w6Ytbeks/deceptive-alignment)\"?[ ](https://www.lesswrong.com/posts/X2i9dQQK3gETCyqh2/chris-olah-s-views-on-agi-safety)\n*   [What are our machine learning models even doing](https://www.lesswrong.com/posts/X2i9dQQK3gETCyqh2/chris-olah-s-views-on-agi-safety)?\n\nWe have to do philosophical work while in a state of significant confusion and ignorance about the nature of intelligence and alignment. \n\nIn this case, we’d noticed that slight reward function misspecification seems to lead to doom, but we didn't *really* know why. Intuitively, it's pretty obvious that most agents don't have deactivation as their dream outcome, but we couldn't actually point to any formal explanations, and we certainly couldn't make precise predictions.\n\nOn its own, [Goodhart's law](https://www.lesswrong.com/posts/EbFABnst8LsidYs5Y/goodhart-taxonomy) doesn't explain why optimizing proxy goals leads to catastrophically bad outcomes, instead of just less-than-ideal outcomes.\n\nI think that we're now starting to have this kind of understanding. [I suspect that](https://www.lesswrong.com/s/7CdoznhJaLEKHwvJW/p/w6BtMqKRLxG9bNLMr) power-seeking is why capable, goal-directed agency is so dangerous by default. If we want to consider [more benign alternatives](https://www.fhi.ox.ac.uk/wp-content/uploads/Reframing_Superintelligence_FHI-TR-2019-1.1-1.pdf) to goal-directed agency, then deeply understanding the rot at the heart of goal-directed agency is important for evaluating alternatives. This work lets us get a feel for the *generic incentives* of reinforcement learning at optimality.\n\nInstrumental usefulness of this work\n------------------------------------\n\nPOWER might be important for reasoning about [the strategy-stealing assumption](https://www.lesswrong.com/posts/nRAMpjnb6Z4Qv3imF/the-strategy-stealing-assumption) (and I think it might be similar to what Paul Christiano means by \"flexible influence over the future”). Evan Hubinger has already [noted](https://www.lesswrong.com/posts/jGB7Pd5q8ivBor8Ee/impact-measurement-and-value-neutrality-verification-1) the utility of the distribution of attainable utility shifts for thinking about value-neutrality in this context (and POWER is another facet of the same phenomenon). If you want to think about whether, when, and why [mesa optimizers](https://arxiv.org/abs/1906.01820) might try to seize power, this theory seems like a valuable tool.\n\nOptimality probability might be relevant for thinking about myopic agency, as the work formally describes how optimal action tends to change with the discount factor.\n\nAnd, of course, we're going to use this understanding of power to design an impact measure.\n\nFuture work\n-----------\n\nThere’s a lot of work I think would be exciting, most of which I suspect will support our current beliefs about power-seeking incentives:\n\n*   These results assume you can see all of the world at once.\n*   These results assume the environment is finite.\n*   These results don’t say anything about non-IID reward.\n*   These results don’t *prove* that POWER-seeking is [bad for other agents in the environment](https://www.lesswrong.com/s/7CdoznhJaLEKHwvJW/p/w6BtMqKRLxG9bNLMr).\n*   These results don’t prove that POWER-seeking is hard to disincentivize.\n*   Learned policies are rarely optimal.\n\nThat said, I think there’s still an important lesson here. Imagine you have good formal reasons to suspect that typing random strings will usually blow up your computer and kill you. Would you then say, \"I'm not planning to type random strings\" and proceed to enter your thesis into a word processor? No. You wouldn't type *anything*, not until you really, really understand what makes the computer blow up sometimes.\n\nSpeaking to the broader debate taking place in the AI research community, I think a productive stance will involve investigating and understanding these results in more detail, getting curious about unexpected phenomena, and seeing how the numbers crunch out in reasonable models.\n\nFrom *Optimal Policies Tend to Seek Power*:\n\n> In the context of MDPs, we formalized a reasonable notion of power and showed conditions under which optimal policies tend to seek it. We believe that our results suggest that in general, reward functions are best optimized by seeking power. We caution that in realistic tasks, learned policies are rarely optimal – our results do not mathematically *prove* that hypothetical superintelligent RL agents will seek power. We hope that this work and its formalisms will foster thoughtful, serious, and rigorous discussion of this possibility.\n\n**Acknowledgements**\n====================\n\nThis work was made possible by the Center for Human-Compatible AI, the Berkeley Existential Risk Initiative, and the Long-Term Future Fund.\n\nLogan Smith ([elriggs](https://www.lesswrong.com/users/elriggs)) spent an enormous amount of time writing Mathematica code to compute power and measure in arbitrary toy MDPs, saving me from computing many quintuple integrations by hand. I thank Rohin Shah for his detailed feedback and brainstorming over the summer of 2019, and I thank Andrew Critch for significantly improving this work through his detailed critiques. Last but not least, thanks to:\n\n1.  Zack M. Davis, Chase Denecke, William Ellsworth, Vahid Ghadakchi, Ofer Givoli, Evan Hubinger, Neale Ratzlaff, Jess Riedel, Duncan Sabien, Davide Zagami, and TheMajor for feedback on version 1 of this post.\n2.  Alex Appel (diffractor), Emma Fickel, Vanessa Kosoy, Steve Omohundro, Neale Ratzlaff, and Mark Xu for reading / giving feedback on version 2 of this post.\n\n* * *\n\n\\\\(^1\\\\) Throughout *Reframing Impact*, we’ve been considering an agent’s *attainable utility*: their ability to get what they want (their *on-policy value*, in RL terminology). Optimal value is a kind of “idealized” attainable utility: the agent’s attainable utility were they to act optimally*.*\n\n\\\\(^2\\\\)Even though instrumental convergence was discovered when thinking about the real world, similar self-preservation strategies turn out to be convergently instrumental in *e.g.* Pac-Man."
    },
    "voteCount": 51
  },
  {
    "_id": "75oMAADr4265AGK3L",
    "url": null,
    "title": "Attainable Utility Preservation: Concepts",
    "slug": "attainable-utility-preservation-concepts",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "AI"
      },
      {
        "name": "Impact Measures"
      }
    ],
    "tableOfContents": {
      "sections": [
        {
          "title": "Appendix: No free impact",
          "anchor": "Appendix__No_free_impact",
          "level": 1
        },
        {
          "title": "Notes",
          "anchor": "Notes",
          "level": 2
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        },
        {
          "anchor": "comments",
          "level": 0,
          "title": "20 comments"
        }
      ],
      "headingsCount": 4
    },
    "contents": {
      "markdown": "![](https://i.imgur.com/hTnYTsJ.png)\n\n\n![](https://i.imgur.com/gwVocUy.png)\n![](https://i.imgur.com/KPv2beS.png)\n\n![](https://i.imgur.com/MYNBKOe.png)\n\n![](https://i.imgur.com/ZK2qYPZ.png)\n\n![](https://i.imgur.com/lk8Keid.png)\n![](https://i.imgur.com/kMBZK6d.png)\n![](https://i.imgur.com/FXlUiYj.png)\n \n![](https://i.imgur.com/hHVvk0Q.png)\n![](https://i.imgur.com/3NMSHHl.png)\n![](https://i.imgur.com/BtzHnUq.png)\n\n![](https://i.imgur.com/MzW64A5.png)\n![](https://i.imgur.com/mOWK65o.png)\n\n![](https://i.imgur.com/VDQiChW.png)\n![](https://i.imgur.com/jtxMXJe.png)\n\n![](https://i.imgur.com/7KcMK3J.png)\n\n## Appendix: No free impact\nWhat if we want the agent to single-handedly ensure the future is stable and aligned with our values? AUP probably won’t allow policies which actually accomplish this goal – one needs power to e.g. nip unaligned superintelligences in the bud. AUP aims to prevent catastrophes by stopping bad agents from gaining power to do bad things, but it symmetrically impedes otherwise-good agents. \n\nThis doesn’t mean we can’t get useful work out of agents – there are important asymmetries provided by both the main reward function and AU landscape counterfactuals. \n\nFirst, even though we can’t specify an _aligned_ reward function, the provided reward function still gives the agent useful information about what we want. If we need paperclips, then a paperclip-AUP agent prefers policies which make some paperclips. Simple.\n\nSecond, if we don’t like what it’s beginning to do, we can shut it off (because it hasn’t gained power over us). Therefore, it has “approval incentives” which bias it towards AU landscapes in which its power hasn’t decreased too much, either. \n\nSo we can hope to build a non-catastrophic AUP agent and get useful work out of it. We just can’t directly ask it to solve all of our problems: it doesn’t make much sense to speak of a “low-impact [singleton](https://wiki.lesswrong.com/wiki/Singleton)”.\n\n#### Notes\n* To emphasize, when I say \"AUP agents do $X$\" in this post, I mean that AUP agents correctly implementing the _concept of AUP_ tend to behave in a certain way. \n* As [pointed out by Daniel Filan](https://www.lesswrong.com/posts/yEa7kwoMpsBgaBCgb/towards-a-new-impact-measure#jJrCTRwTZDZDc3XLx), AUP suggests that one might work better in groups by ensuring one's actions preserve teammates' AUs. "
    },
    "voteCount": 11
  },
  {
    "_id": "EbFABnst8LsidYs5Y",
    "url": null,
    "title": "Goodhart Taxonomy",
    "slug": "goodhart-taxonomy",
    "author": "Scott Garrabrant",
    "question": false,
    "tags": [
      {
        "name": "Goodhart's Law"
      },
      {
        "name": "AI"
      }
    ],
    "tableOfContents": {
      "sections": [
        {
          "title": "Quick Reference",
          "anchor": "Quick_Reference",
          "level": 1
        },
        {
          "title": "Regressional Goodhart",
          "anchor": "Regressional_Goodhart",
          "level": 1
        },
        {
          "title": "Abstract Model",
          "anchor": "Abstract_Model",
          "level": 2
        },
        {
          "title": "Examples",
          "anchor": "Examples",
          "level": 2
        },
        {
          "title": "Relationship with Other Goodhart Phenomena",
          "anchor": "Relationship_with_Other_Goodhart_Phenomena",
          "level": 2
        },
        {
          "title": "Mitigation",
          "anchor": "Mitigation",
          "level": 2
        },
        {
          "title": "Causal Goodhart",
          "anchor": "Causal_Goodhart",
          "level": 1
        },
        {
          "title": "Abstract Model",
          "anchor": "Abstract_Model1",
          "level": 2
        },
        {
          "title": "Examples",
          "anchor": "Examples1",
          "level": 2
        },
        {
          "title": "Relationship with Other Goodhart Phenomena",
          "anchor": "Relationship_with_Other_Goodhart_Phenomena1",
          "level": 2
        },
        {
          "title": "Mitigation",
          "anchor": "Mitigation1",
          "level": 2
        },
        {
          "title": "Extremal Goodhart",
          "anchor": "Extremal_Goodhart",
          "level": 1
        },
        {
          "title": "Abstract Model",
          "anchor": "Abstract_Model2",
          "level": 2
        },
        {
          "title": "Examples",
          "anchor": "Examples2",
          "level": 2
        },
        {
          "title": "Relationship with Other Goodhart Phenomena",
          "anchor": "Relationship_with_Other_Goodhart_Phenomena2",
          "level": 2
        },
        {
          "title": "Mitigation",
          "anchor": "Mitigation2",
          "level": 2
        },
        {
          "title": "Adversarial Goodhart",
          "anchor": "Adversarial_Goodhart",
          "level": 1
        },
        {
          "title": "Abstract Model",
          "anchor": "Abstract_Model3",
          "level": 2
        },
        {
          "title": "Examples",
          "anchor": "Examples3",
          "level": 2
        },
        {
          "title": "Relationship with Other Goodhart Phenomena",
          "anchor": "Relationship_with_Other_Goodhart_Phenomena3",
          "level": 2
        },
        {
          "title": "Mitigation",
          "anchor": "Mitigation3",
          "level": 2
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        },
        {
          "anchor": "comments",
          "level": 0,
          "title": "32 comments"
        }
      ],
      "headingsCount": 23
    },
    "contents": {
      "markdown": "[Goodhart’s Law](https://en.wikipedia.org/wiki/Goodhart%27s_law) states that \"any observed statistical regularity will tend to collapse once pressure is placed upon it for control purposes.\" However, this is not a single phenomenon. I propose that there are (at least) four different mechanisms through which proxy measures break when you optimize for them.\n\nThe four types are Regressional, Causal, Extremal, and Adversarial. In this post, I will go into detail about these four different Goodhart effects using mathematical abstractions as well as examples involving humans and/or AI. I will also talk about how you can mitigate each effect.\n\nThroughout the post, I will use < refresh to render LaTeX > to refer to the true goal and use < refresh to render LaTeX > to refer to a proxy for that goal which was observed to correlate with < refresh to render LaTeX > and which is being optimized in some way.\n\n* * *\n\nQuick Reference\n===============\n\n*   Regressional Goodhart - When selecting for a proxy measure, you select not only for the true goal, but also for the difference between the proxy and the goal.\n\n*   Model: When < refresh to render LaTeX > is equal to < refresh to render LaTeX > , where < refresh to render LaTeX > is some noise, a point with a large < refresh to render LaTeX > value will likely have a large < refresh to render LaTeX > value, but also a large < refresh to render LaTeX > value. Thus, when < refresh to render LaTeX > is large, you can expect < refresh to render LaTeX > to be predictably smaller than < refresh to render LaTeX > .\n*   _Example: height is correlated with basketball ability, and does actually directly help, but the best player is only 6'3\", and a random 7' person in their 20s would probably not be as good_\n\n*   Causal Goodhart - When there is a non-causal correlation between the proxy and the goal, intervening on the proxy may fail to intervene on the goal.\n\n*   Model: If < refresh to render LaTeX > causes < refresh to render LaTeX > (or if < refresh to render LaTeX > and < refresh to render LaTeX > are both caused by some third thing), then a correlation between < refresh to render LaTeX > and < refresh to render LaTeX > may be observed. However, when you intervene to increase < refresh to render LaTeX > through some mechanism that does not involve < refresh to render LaTeX > , you will fail to also increase < refresh to render LaTeX > .\n*   _Example: someone who wishes to be taller might observe that height is correlated with basketball skill and decide to start practicing basketball._\n\n*   Extremal Goodhart - Worlds in which the proxy takes an extreme value may be very different from the ordinary worlds in which the correlation between the proxy and the goal was observed.\n\n*   Model: Patterns tend to break at simple joints. One simple subset of worlds is those worlds in which < refresh to render LaTeX > is very large. Thus, a strong correlation between < refresh to render LaTeX > and < refresh to render LaTeX > observed for naturally occuring < refresh to render LaTeX > values may not transfer to worlds in which < refresh to render LaTeX > is very large. Further, since there may be relatively few naturally occuring worlds in which < refresh to render LaTeX > is very large, extremely large < refresh to render LaTeX > may coincide with small < refresh to render LaTeX > values without breaking the statistical correlation.\n*   _Example: the tallest person on record,_ _[Robert Wadlow](https://en.wikipedia.org/wiki/Robert_Wadlow), was 8'11\" (2.72m). He grew to that height because of a pituitary disorder, he would have struggled to play basketball because he \"required leg braces to walk and had little feeling in his legs and feet.\"_\n\n*   Adversarial Goodhart - When you optimize for a proxy, you provide an incentive for adversaries to correlate their goal with your proxy, thus destroying the correlation with your goal.\n\n*   Model: Consider an agent < refresh to render LaTeX > with some different goal < refresh to render LaTeX > . Since they depend on common resources, < refresh to render LaTeX > and < refresh to render LaTeX > are naturally opposed. If you optimize < refresh to render LaTeX > as a proxy for < refresh to render LaTeX > , and < refresh to render LaTeX > knows this, < refresh to render LaTeX > is incentivized to make large < refresh to render LaTeX > values coincide with large < refresh to render LaTeX > values, thus stopping them from coinciding with large < refresh to render LaTeX > values.\n*   _Example: aspiring NBA players might just lie about their height._\n\n* * *\n\nRegressional Goodhart\n=====================\n\nWhen selecting for a proxy measure, you select not only for the true goal, but also for the difference between the proxy and the goal.\n\nAbstract Model\n--------------\n\nWhen < refresh to render LaTeX > is equal to < refresh to render LaTeX > , where < refresh to render LaTeX > is some noise, a point with a large < refresh to render LaTeX > value will likely have a large < refresh to render LaTeX > value, but also a large < refresh to render LaTeX > value. Thus, when < refresh to render LaTeX > is large, you can expect < refresh to render LaTeX > to be predictably smaller than < refresh to render LaTeX > .\n\nThe above description is when < refresh to render LaTeX > is meant to be an estimate of < refresh to render LaTeX > . A similar effect can be seen when < refresh to render LaTeX > is only meant to be correlated with < refresh to render LaTeX > by looking at percentiles. When a sample is chosen which is a typical member of the top < refresh to render LaTeX > percent of all < refresh to render LaTeX > values, it will have a lower < refresh to render LaTeX > value than a typical member of the top < refresh to render LaTeX > percent of all < refresh to render LaTeX > values. As a special case, when you select the highest < refresh to render LaTeX > value, you will often not select the highest < refresh to render LaTeX > value.\n\nExamples\n--------\n\nExamples of Regressional Goodhart are everywhere. Every time someone does something that is anything other than the thing that maximizes their goal, you could view it as them optimizing some kind of proxy (and the action to maximize the proxy is not the same as the action to maximize the goal).\n\nR[egression to the Mean](https://en.wikipedia.org/wiki/Regression_toward_the_mean), [Winner’s Curse](https://en.wikipedia.org/wiki/Winner%27s_curse), and [Optimizer’s Curse](https://faculty.fuqua.duke.edu/~jes9/bio/The_Optimizers_Curse.pdf) are all examples of Regressional Goodhart, as is [the Tails Come Apart](http://lesswrong.com/lw/km6/why_the_tails_come_apart/) phenomenon.\n\nRelationship with Other Goodhart Phenomena\n------------------------------------------\n\nRegressional Goodhart is by far the most benign of the four Goodhart effects. It is also the hardest to avoid, as it shows up every time the proxy and the goal are not exactly the same.\n\nMitigation\n----------\n\nWhen facing only Regressional Goodhart, you still want to choose the option with the largest proxy value. While the proxy will be an overestimate it will still be better in expectation than options with a smaller proxy value. If you have control over what proxies to use, you can mitigate Regressional Goodhart by choosing proxies that are more tightly correlated with your goal.\n\nIf you are not just trying to pick the best option, but also trying to have an accurate picture of what the true value will be, Regressional Goodhart may cause you to overestimate the value. If you know the exact relationship between the proxy and the goal, you can account for this by just calculating the expected goal value for a given proxy value. If you have access to a second proxy with an error independent from the error in the first proxy, you can use the first proxy to optimize, and the second proxy to get an accurate expectation of the true value. (This is what happens when you set aside some training data to use for testing.)\n\n* * *\n\nCausal Goodhart\n===============\n\nWhen there is a non-causal correlation between the proxy and the goal, intervening on the proxy may fail to intervene on the goal.\n\nAbstract Model\n--------------\n\nIf < refresh to render LaTeX > causes < refresh to render LaTeX > (or if < refresh to render LaTeX > and < refresh to render LaTeX > are both caused by some third thing), then a correlation between < refresh to render LaTeX > and < refresh to render LaTeX > may be observed. However, when you intervene to increase < refresh to render LaTeX > through some mechanism that does not involve < refresh to render LaTeX > , you will fail to also increase V.\n\nExamples\n--------\n\nHumans often avoid naive Causal Goodhart errors, and most examples I can think of sound obnoxious (like eating caviar to become rich). One possible example is a human who avoids doctor visits because not being told about health is a proxy for being healthy. (I do not know enough about humans to know if Causal Goodhart is actually what is going on here.)\n\nI also cannot think of a good AI example. Most AI is not in acting in the kind of environment where Causal Goodhart would be a problem, and when it is acting in that kind of environment Causal Goodhart errors are easily avoided.\n\nMost of the time the phrase \"Correlation does not imply causation\" is used it is pointing out that a proposed policy might be subject to Causal Goodhart.\n\nRelationship with Other Goodhart Phenomena\n------------------------------------------\n\nYou can tell the difference between Causal Goodhart and the other three types because Causal Goodhart goes away when just sample a world with large proxy value, rather than intervene to cause the proxy to happen.\n\nMitigation\n----------\n\nOne way to avoid Causal Goodhart is to only sample from or choose between worlds according to their proxy values, rather than causing the proxy. This clearly cannot be done in all situations, but it is useful to note that there is a class of problems for which Causal Goodhart cannot cause problems. For example, consider choosing between algorithms based on how well they do on some test inputs, and your goal is to choose an algorithm that performs well on random inputs. The fact that you choose an algorithm does not effect its performance, and you don't have to worry about Causal Goodhart.\n\nIn cases where you actually change the proxy value, you can try to infer the causal structure of the variables using statistical methods, and check that the proxy actually causes the goal before you intervene on the proxy.\n\n* * *\n\nExtremal Goodhart\n=================\n\nWorlds in which the proxy takes an extreme value may be very different from the ordinary worlds in which the correlation between the proxy and the goal was observed.\n\nAbstract Model\n--------------\n\nPatterns tend to break at simple joints. One simple subset of worlds is those worlds in which < refresh to render LaTeX > is very large. Thus, a strong correlation between < refresh to render LaTeX > and < refresh to render LaTeX > observed for naturally occuring < refresh to render LaTeX > values may not transfer to worlds in which < refresh to render LaTeX > is very large. Further, since there may be relatively few naturally occuring worlds in which < refresh to render LaTeX > is very large, extremely large < refresh to render LaTeX > may coincide with small < refresh to render LaTeX > values without breaking the statistical correlation.\n\nExamples\n--------\n\nHumans evolve to like sugars, because sugars were correlated in the ancestral environment (which has fewer sugars) with nutrition and survival. Humans then optimize for sugars, have way too much, and become less healthy.\n\nAs an abstract mathematical example, let < refresh to render LaTeX > and < refresh to render LaTeX > be two correlated dimensions in a multivariate normal distribution, but we cut off the normal distribution to only include the ball of points in which < refresh to render LaTeX > for some large < refresh to render LaTeX > . This example represents a correlation between < refresh to render LaTeX > and < refresh to render LaTeX > in naturally occurring points, but also a boundary around what types of points are feasible that need not respect this correlation. Imagine you were to sample < refresh to render LaTeX > points and take the one with the largest < refresh to render LaTeX > value. As you increase < refresh to render LaTeX > , at first, this optimization pressure lets you find better and better points for both < refresh to render LaTeX > and < refresh to render LaTeX > , but as you increase < refresh to render LaTeX > to infinity, eventually you sample so many points that you will find a point near < refresh to render LaTeX > . When enough optimization pressure was applied, the correlation between < refresh to render LaTeX > and < refresh to render LaTeX > stopped mattering, and instead the boundary of what kinds of points were possible at all decided what kind of point was selected.\n\nMany examples of machine learning algorithms doing bad because of [overfitting](https://en.wikipedia.org/wiki/Overfitting) are a special case of Extremal Goodhart.\n\nRelationship with Other Goodhart Phenomena\n------------------------------------------\n\nExtremal Goodhart differs from Regressional Goodhart in that Extremal Goodhart goes away in simple examples like correlated dimensions in a multivariate normal distribution, but Regressional Goodhart does not.\n\nMitigation\n----------\n\nQuantilization and Regularization are both useful for mitigating Extremal Goodhart effects. In general, Extremal Goodhart can be mitigated by choosing an option with a high proxy value, but not so high as to take you to a domain drastically different from the one in which the proxy was learned.\n\n* * *\n\nAdversarial Goodhart\n====================\n\nWhen you optimize for a proxy, you provide an incentive for adversaries to correlate their goal with your proxy, thus destroying the correlation with your goal.\n\nAbstract Model\n--------------\n\nConsider an agent < refresh to render LaTeX > with some different goal < refresh to render LaTeX > . Since they depend on common resources, < refresh to render LaTeX > and < refresh to render LaTeX > are naturally opposed. If you optimize < refresh to render LaTeX > as a proxy for < refresh to render LaTeX > , and < refresh to render LaTeX > knows this, < refresh to render LaTeX > is incentivized to make large < refresh to render LaTeX > values coincide with large < refresh to render LaTeX > values, thus stopping them from coinciding with large < refresh to render LaTeX > values.\n\nExamples\n--------\n\nWhen you use a metric to choose between people, but then those people learn what metric you use and game that metric, this is an example of Adversarial Goodhart.\n\nAdversarial Goodhart is the mechanism behind a superintelligent AI making a [Treacherous Turn](http://lesswrong.com/lw/n5k/a_toy_model_of_the_treacherous_turn/). Here, < refresh to render LaTeX > is doing what the humans want forever. < refresh to render LaTeX > is doing what the humans want in the training cases where the AI does not have enough power to take over, and < refresh to render LaTeX > is whatever the AI wants to do with the universe.\n\nAdversarial Goodhart is also behind the [malignancy of the universal prior](https://ordinaryideas.wordpress.com/2016/11/30/what-does-the-universal-prior-actually-look-like/), where you want to predict well forever ( < refresh to render LaTeX > ), so hypotheses might predict well for a while ( < refresh to render LaTeX > ), so that they can manipulate the world with their future predictions ( < refresh to render LaTeX > ).\n\nRelationship with Other Goodhart Phenomena\n------------------------------------------\n\nAdversarial Goodhart is the primary mechanism behind the original Goodhart's Law.\n\nExtremal Goodhart can happen even without any adversaries in the environment. However, Adversarial Goodhart may take advantage of Extremal Goodhart, as an adversary can more easily manipulate a small number of worlds with extreme proxy values, than it can manipulate all of the worlds.\n\nMitigation\n----------\n\nSuccesfully avoiding Adversarial Goodhart problems is very difficult in theory, and we understand very little about how to do this. In the case of non-superintelligent adversaries, you may be able to avoid Adversarial Goodhart by keeping your proxies secret (for example, not telling your employees what metrics you are using to evaluate them). However, this is unlikely to scale to dealing with superintelligent adversaries.\n\nOne technique that might help in mitigating Adversarial Goodhart is to choose a proxy that is so simple and optimize so hard that adversaries have no or minimal control over the world which maximizes that proxy. (I want to ephasize that this is not a good plan for avoiding Adversarial Goodhart; it is just all I have.)\n\nFor example, say you have a complicated goal that includes wanting to go to Mars. If you use a complicated search process to find a plan that is likely to get you to Mars, adversaries in your search process may suggest a plan that involves building a superintelligence that gets you to Mars, but also kills you.\n\nOn the other hand, if you use the proxy of getting to Mars as fast as possible and optimize very hard, then (maybe) adversaries can't add baggage to a proposed plan without being out selected by a plan without that baggage. Buliding a superintelligence maybe takes more time than just having the plan tell you how to build a rocket quickly. (Note that the plan will likely include things like acceleration that humans can't handle and nanobots that don't turn off, so Extremal Goodhart will still kill you.)"
    },
    "voteCount": 96
  },
  {
    "_id": "rD57ysqawarsbry6v",
    "url": null,
    "title": "Attention control is critical for changing/increasing/altering motivation",
    "slug": "attention-control-is-critical-for-changing-increasing",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "Neuroscience"
      },
      {
        "name": "Motivations"
      },
      {
        "name": "Attention"
      }
    ],
    "tableOfContents": {
      "sections": [
        {
          "title": "I – Attention and plasticity",
          "anchor": "I___Attention_and_plasticity",
          "level": 1
        },
        {
          "title": "II – Practical implications: making and breaking habits, efficacy of CBT ",
          "anchor": "II___Practical_implications__making_and_breaking_habits__efficacy_of_CBT_",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        },
        {
          "anchor": "comments",
          "level": 0,
          "title": "89 comments"
        }
      ],
      "headingsCount": 4
    },
    "contents": {
      "markdown": "I’ve just been reading [Luke’s](/user/lukeprog/) [“Crash Course in the Neuroscience of Human Motivation.”](/lw/71x/a_crash_course_in_the_neuroscience_of_human/) It is a useful text, although there are a few technical errors and a few bits of outdated information (see \\[1\\], updated information about one particular quibble in \\[2\\] and \\[3\\]).\n\nThere is one significant missing piece, however, which is of critical importance for our subject matter here on LW: the effect of attention on plasticity, including the plasticity of motivation. Since I don’t see any other texts addressing it directly (certainly not from a neuroscientific perspective), let’s cover the main idea here.\n\nSummary for impatient readers: focus of attention physically determines which synapses in your brain get stronger, and which areas of your cortex physically grow in size. The implications of this provide direct guidance for alteration of behaviors and motivational patterns. This _is_ used for this purpose extensively: for instance, many benefits of the Cognitive-Behavioral Therapy approach rely on this mechanism.\n\n**I – Attention and plasticity**\n\nTo illustrate this properly, we need to define two terms. I’m guessing these are very familiar to most readers here, but let’s cover them briefly just in case.\n\nFirst thing to keep in mind is _the_ _plasticity of cortical maps._ In essence, particular functional areas of our brain can expand or shrink based on how often (and how intensely) they are used. A small amount of this growth is physical, as new axons grow, expanding the white matter; most of it happens by repurposing any less-used circuitry in the vicinity of the active area. For example, our sense of sight is processed by our visual cortex, which turns signals from our eyes into lines, shapes, colors and movement. In blind people, however, this part of the brain becomes invaded by other senses, and begins to process sensations like touch and hearing, such that they become significantly more sensitive than in sighted people. Similarly, in deaf people, auditory cortex (part of the brain that processes sounds) becomes adapted to process visual information and gather language clues by sight.\n\nSecond concept we’ll need is [_somatosensory cortex_](http://en.wikipedia.org/wiki/Primary_somatosensory_cortex) (SSC for short). This is an area of the (vertebrate) brain where most of the incoming touch and positional (proprioceptive) sensations from the body converge. There is a map-like quality to this part of our brain, as every body part links to a particular bit of the SSC surface (which can be illustrated with silly-looking things, such as the [sensory homunculus](http://en.wikipedia.org/wiki/File:Sensory_Homunculus.png)). More touch-sensitive areas of the body have larger corresponding areas within the SSC.\n\nWith these two in mind, let’s consider one actual experiment \\[4\\]. Scientists measured and mapped the area of an owl monkey’s SSC which became activated when one of his fingertips was touched. The monkey was then trained to hold that finger on a tactile stimulator – a moving wheel that stimulates touch receptors. The monkey had to pay attention to the stimulus, and was rewarded for letting go upon detecting certain changes in spinning frequency. After a few weeks of training, the area was measured again.\n\nAs you probably expected, the area had grown larger. The touch-processing neurons grew out, co-opting surrounding circuitry in order to achieve better and faster processing of the stimulus that produced the reward. Which is, so far, just another way of showing plasticity of cortical maps.\n\nBut then, there is something else. The SSC area expanded only when the monkey had to _pay attention_ to the sensation of touch in order to receive the reward. If a monkey was trained to keep a hand on the wheel that moved just the same, but he did not have to pay attention to it… the cortical map remained the same size. This finding has since been replicated in humans, many times (for instance \\[5, 6\\]).\n\nTake a moment to consider what this means.\n\nA man is sitting in his living room, in front of a chessboard. Classical music plays in the background. The man is focused, thinking about the next move, about his chess strategy, and about the future possibilities of the game. His neural networks are optimizing, making him a better chess player.\n\nA man is sitting in his living room, in front of a chessboard. Classical music plays in the background. The man is focused, thinking about the music he hears, listening to the chords and anticipating the sounds still to come. His neural networks are optimizing, making him better at understanding music and hearing subtleties within a melody.\n\nA man is sitting in his living room, in front of a chessboard. Classical music plays in the background. The man is focused, gritting his teeth as another flash of pain comes from his bad back. His neural networks are optimizing, making the pain more intense, easier to feel, harder to ignore.\n\n**II – Practical implications: making and breaking habits, efficacy of CBT**\n\nHabitual learned behaviors are often illustrated with the example of driving. When we are learning to drive, we have to pay attention to everything: when to push the pedals, when to signal, where to hold our hands… A few years later, these behaviors become so automatic, we hardly pay attention at all. Indeed, most of us can drive for hours while carrying on conversations or listening to audiobooks. We are completely unaware, as our own body keeps pushing pedals, signaling turns, and changing gears.\n\nWe can therefore say that _driving behaviors_, through practice and attention, eventually become automatic – which is, most of the time, a good thing. But so do many other things, including some destructive ones we might want to get rid of. Let’s take a simple one: nail biting. You are reading, or watching a movie, or thinking, or driving… when you suddenly notice some minor pain, and realize that you have chewed your nail into a ragged stump. Ouch!\n\nYou catch yourself biting, you stop. Five minutes later, you catch yourself biting again. You stop again. Repeat _ad infinitum_, or _ad nauseam_, whichever comes first.\n\nCognitive-Behavioral Therapy has a highly successful approach for breaking habits, which requires only a very subtle alteration to this process. You notice that you are biting your nails. You immediately focus your attention on what you are doing, and you stop doing it. No rage, no blaming yourself, no negative emotions. You just stop, and you focus all the attention you can on the act of stopping. You move your arm down, focusing your attention on the act of movement, on the feeling of your arm going down, away from your mouth. That’s it. You can go back to whatever you were doing.\n\nFive minutes later, you notice yourself biting your nails again. You calmly repeat the procedure again.\n\nBy doing this, you are training yourself to perform a new behavior – the “stop and put the hand down” behavior – which is itself triggered by the nail-biting behavior. As you go along, you will get better and better at noticing that you have started to bite your nails. You will also get better and better at stopping and putting your hand down. After a while, this will become semi-automatic; you’ll notice that your hand went to your mouth, a nail touched your tooth, and the hand went back down before you could do anything. Don’t stop training: focus your attention on the “stop and drop” part of the action.\n\nAfter a while, the nail-biting simply goes away. Of course, the more complex and more ingrained a habit is, the more effort and time will be needed to break it. But for most people, even strong habits can be relatively quickly weakened, or redirected into less destructive behaviors.\n\nIt’s probably obvious that habits can be _created_ in this way as well. We don’t become better at things we do – we become better at things we _pay attention to while we’re doing them._ If you want to make exercise a habit, your efforts will be much more effective if you focus your attention on your exercise technique, rather than repeatedly thinking how painful and tiring the whole process is.\n\nThere is also a direct implication for training in any complex skill. Start with the well-known [learning curve](http://en.wikipedia.org/wiki/Learning_curve) effect: we gain a lot of skill relatively quickly, and then improvements slow down incrementally as we approach our maximum potential skill level. It is relatively easy to go from a poor to a mediocre tennis player; it is much, much harder to go from mediocre to good, and even harder to go from good to excellent.\n\nComplex skills have many different aspects, which we usually attempt to train simultaneously. We can become very good at some, while staying poor at others. The optimal approach would be to focus most of our attention on those aspects where our abilities are weakest, since smaller investments of time and effort will lead to larger improvements in skill.\n\nTo keep with the tennis metaphor, one could become very good at controlling the ball direction and spin, while still having a poor awareness of the opponent’s position. Simply playing more will improve both aspects further, but our hypothetical player should optimally try to focus her attention on opponent awareness \\[7\\].\n\nFinally, there is another implication which I’ll leave as an exercise for the readers. Mindfulness meditation, which essentially boils down to training control of attention, has been shown to exert a positive effect on many, many different things (lowering depression, anxiety and stress, as well as improving productivity \\[8, 9, 10\\]). In the light of the previous text, one obvious reason why better control over attention can produce all these beneficial effects should immediately come to mind.\n\n\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\n\nReferences\n\n\\[1\\] I have several quibbles, but let’s stick to one (to prevent this note from becoming longer than the above text). Luke presents a view of dopamine reward system which is stuck in the early 2000’s – ages ago by the pace of neuroscientific research. Dopamine actually has a very, very complex effect on motivation, and is able to strengthen or weaken single synaptic connections based on timing of the signal relative to the signals from the sensory systems. Endocannabinoid neurotransmission (i.e. signaling through chemicals that stimulate the same receptors that are affected by active ingredients in marijuana) is being shown as more and more important in this system as well, and the relative timing of the two signals appears critical.\n\nThe complexity of the effects increases by several orders of magnitude when networks are concerned. Consider this: a planning-related network in the prefrontal cortex can influence the motivation-generating networks in the striatum. A stimulus from the outside is perceived by the sensory networks and transmitted to the dopamine system, to the prefrontal cortex, and to the striatum. The same dopamine signal can, depending on exact timing of action potential bursts, strengthen synapses in the striatum, while weakening synapses in the prefrontal cortex. The result? The link between the stimulus and the actual motivation can increase or decrease, depending on exact connectivity between networks, on the relative sensitivity and on the exact topology of the meta-network in question.\n\nSee the following two references for a broad overview of the subject area.\n\n\\[2\\] Calabresi P, Picconi B, Tozzi A, Di Filippo M. \"Dopamine-mediated regulation of corticostriatal synaptic plasticity\" Trends Neurosci. 2007 30(5):211-9.\n\n\\[3\\] Wickens JR. \"Synaptic plasticity in the basal ganglia\" Behav Brain Res. 2009 199(1):119-28.\n\n\\[4\\] Recanzone GH, Merzenich MM, Jenkins WM, Grajski KA, Dinse HR. \"Topographic reorganization of the hand representation in cortical area 3b of owl monkeys trained in a frequency-discrimination task\" _J Neurophysiol._ 1992 67(5), 1031-56.\n\n\\[5\\] Heron J, Roach NW, Whitaker D, Hanson JV. \"Attention regulates the plasticity of multisensory timing\" _Eur J Neurosci._ 2010 31(10), 1755-62.\n\n\\[6\\] Stefan K, Wycislo M, Classen J. “Modulation of associative human motor cortical plasticity by attention” _J Neurophysiol._ 2004 92(1), 66-72.\n\n\\[7\\] I’m not finding good papers directed exactly on this point, so I’ll just throw this out as a personal opinion (although I’ll say it appears well supported by indirect research). We all like to appear competent and skillful, especially in those areas where we have invested a lot of time and effort. This can lead to a bias where we focus on using those aspects of complex skills we are best at, and training those aspects most intensely. In other words, a tendency appears to exist to do exactly the opposite of what we should be doing. (If anyone has encountered a name for this bias, or has references to suggest, I would be very grateful to hear from you.)\n\n\\[8\\] Brown KW, Ryan RM. \"The benefits of being present: mindfulness and its role in psychological well-being\" J Pers Soc Psychol. 2003 84(4):822-48.\n\n\\[9\\] Davidson RJ, Kabat-Zinn J, Schumacher J, Rosenkranz M, Muller D, Santorelli SF, Urbanowski F, Harrington A, Bonus K, Sheridan JF. \"Alterations in brain and immune function produced by mindfulness meditation\" Psychosom Med. 2003 65(4):564-70.\n\n\\[10\\] Shao RP, Skarlicki DP. \"The role of mindfulness in predicting individual performance\" Canadian J of Behavioral Sci 2009 41(4): 195–201."
    },
    "voteCount": 214
  },
  {
    "_id": "B7P97C27rvHPz3s9B",
    "url": null,
    "title": "Gears in understanding",
    "slug": "gears-in-understanding",
    "author": "Valentine",
    "question": false,
    "tags": [
      {
        "name": "Gears-Level"
      },
      {
        "name": "World Modeling"
      }
    ],
    "tableOfContents": {
      "sections": [
        {
          "title": "Example: Gears in a box",
          "anchor": "Example__Gears_in_a_box",
          "level": 1
        },
        {
          "title": "Example: Arithmetic",
          "anchor": "Example__Arithmetic",
          "level": 1
        },
        {
          "title": "Example: My mother",
          "anchor": "Example__My_mother",
          "level": 1
        },
        {
          "title": "Example: Gyroscopes",
          "anchor": "Example__Gyroscopes",
          "level": 1
        },
        {
          "title": "Gears-ness is not the same as goodness",
          "anchor": "Gears_ness_is_not_the_same_as_goodness",
          "level": 1
        },
        {
          "title": "Going forward",
          "anchor": "Going_forward",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        },
        {
          "anchor": "comments",
          "level": 0,
          "title": "38 comments"
        }
      ],
      "headingsCount": 8
    },
    "contents": {
      "markdown": "Some (literal, physical) roadmaps are more useful than others. Sometimes this is because of how well the [map corresponds to the territory](https://wiki.lesswrong.com/wiki/The_map_is_not_the_territory), but sometimes it's because of features of the map that are irrespective of the territory. E.g., maybe the lines are fat and smudged such that you can't tell how far a road is from a river, or maybe it's unclear which road a name is trying to indicate.\n\nIn the same way, I want to point at a property of models that isn't about what they're modeling. It *interacts with* the clarity of what they're modeling, but only in the same way that smudged lines in a roadmap interact with the clarity of the roadmap.\n\nThis property is *how deterministically interconnected the variables of the model are*. There are a few [tests](https://en.wikipedia.org/wiki/Goodhart%27s_law) I know of to see to what extent a model has this property, though I don't know if this list is exhaustive and would be a little surprised if it were:\n\n1.  Does the model [pay rent](https://www.lesswrong.com/lw/i3/making_beliefs_pay_rent_in_anticipated_experiences/)? If it does, and if it were falsified, how much (and how precisely) could you infer other things from the falsification?\n2.  How incoherent is it to imagine that the model is accurate but that a given variable [could be different](https://www.lesswrong.com/lw/if/your_strength_as_a_rationalist/)?\n3.  If you knew the model were accurate but you were to forget the value of one variable, [could you rederive it](https://www.lesswrong.com/lw/la/truly_part_of_you/)?\n\nI think this is a really important idea that ties together a lot of different topics that appear here on Less Wrong. It also acts as a prerequisite frame for a bunch of ideas and tools that I'll want to talk about later.\n\nI'll start by giving a bunch of examples. At the end I'll summarize and gesture toward where this is going as I see it.\n\n* * *\n\n**Example: Gears in a box**\n\nLet's look at this collection of gears in an opaque box:\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/d7afe4ca9054458cdf80585d13bf4b65f97526fefc6048ee.png)\n\n(Drawing courtesy of my colleague, Duncan Sabien.)\n\nIf we turn the lefthand gear counterclockwise, it's within our model of the gears on the inside that the righthand gear could turn either way. The model we're able to build for this system of gears does poorly on all three tests I named earlier:\n\n*   The model barely pays rent. If you speculate that the righthand gear turns one way and you discover it turns the other way, you can't really infer very much. All you can meaningly infer is that *if* the system of gears is pretty simple (e.g., nothing that makes the righthand gear alternate as the lefthand gear rotates counterclockwise), then the direction that the righthand gear turns determines whether the total number of gears is even or odd.\n*   The gear on the righthand side could just as well go either way. Your expectations aren't constrained.\n*   Right now you *don't* know which way the righthand gear turns, and you can't derive it.\n\nSuppose that Joe peeks inside the box and tells you \"Oh, the righthand gear will rotate clockwise.\" You imagine that Joe is more likely to say this if the righthand gear turns clockwise than if it doesn't, so this seems like relevant evidence that the righthand gear turns clockwise. This gets stronger the more people like Joe who look in the box and report the same thing.\n\nNow let's peek inside the box:\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/c0bb22994a178b465c279afd771d090ac6551063c28536d0.png)\n\n…and now we have to wonder what's up with Joe.\n\nThe second test stands out for me especially strongly. There is *no way* that the obvious model about what's going on here could be right *and* Joe is right. And it doesn't matter *how many people* agree with Joe in terms of the logic of this statement: Either *all of them are wrong*, or your model is wrong. This logic is immune to social pressure. It means that there's a chance that you can accumulate evidence about how well your map matches the territory here, and if that converges on your map being basically correct, then you are on firm epistemic footing to disregard the opinion of *lots* of other people. Gathering evidence about the map/territory correspondence has higher leverage for seeing the truth than does gathering evidence about what others think.\n\nThe first test shows something interesting too. Suppose the gear on the right really *does* move clockwise when you move the left gear counterclockwise. What does that imply? Well, it means your initial model (if it's what I imagine it is) is wrong — but there's a limited space of possibilities about *ways in which* it can be wrong. For instance, maybe the second gear from the left is on a vertical track and moves upward instead of rotating. By comparison, something like \"[Gears work in mysterious ways](https://www.lesswrong.com/lw/iu/mysterious_answers_to_mysterious_questions/)\" just won't cut it.\n\nIf we combine the two, we end up staring at Joe and noticing that we can be a lot more precise than just \"Joe is wrong\". We know that either Joe's model of the gears is wrong (e.g., he thinks some gear is on a vertical track), Joe's model of the gears is *vague* and isn't constrained the way ours is (e.g., he was just counting gears and made a mistake), or Joe is lying. The first two give us testable predictions: If his model is wrong, then it's wrong *in some specific way*; and if it's vague, then there should be some place where it does poorly on the three tests of model interconnectedness. If we start zooming in on these two possibilities while talking to Joe and it turns out that neither of those are true, then it becomes a lot more obvious that Joe is just bullshitting (or we failed to think of a fourth option).\n\nBecause of this example, in CFAR we talk about how \"Gears-like\" or how \"made of Gears\" a model is. (I capitalize \"Gears\" to emphasize that it's an analogy.) When we notice an interconnection, we talk about \"finding Gears\". I'll use this language going forward.\n\n* * *\n\n**Example: Arithmetic**\n\nIf you add 25+18 using the standard addition algorithm, you have to carry a 1, usually by marking that above the 2 in 25.\n\nFun fact: it's possible to get that right without having any clue what the 1 represents or why you write it there.\n\nThis is actually a pretty major issue in math education. There's an in-practice tension between (a) memorizing and drilling algorithms that let you compute answers quickly, and (b) \"really understanding\" why those algorithms work.\n\nUnfortunately, there's a kind of philosophical debate that often happens in education when people talk about what \"understand\" means, and I find it pretty annoying. It goes something like this:\n\n*   Person A: \"The student said they carry the 1 because that's what their teacher told them to do. So they don't really understand the addition algorithm.\"\n*   Person B: \"What do you mean by reallyunderstand'?Wˆ'swrongwiththejustificationofA person who knows this subject really well says this works, and I believe them'?\"\n*   A: \"But that reason isn't about the *mathematics*. Their justification isn't mathematical. It's social.\"\n*   B: \"[Mathematical justification is social](https://en.wikipedia.org/wiki/Philosophy_of_mathematics#Social_constructivism_or_social_realism). The style of proof that topologists use wouldn't be accepted by analysts. What constitutes a 'proof' or a 'justification' in math is socially agreed upon.\"\n*   A: \"Oh, come on. We can't just agree that e=3 and make that true. Sure, maybe the way we *talk* about math is socially constructed, but we're talking about [something real](http://yudkowsky.net/rational/the-simple-truth).\"\n*   B: \"I'm not sure that's true. But even if it were, how could you know whether you're talking about that `something real' as opposed to one of the social constructs we're using to share perspectives about it?\"\n\nEt cetera.\n\n(I would *love* to see debates like this happen in [a milieu of mutual truth-seeking](https://www.lesswrong.com/lw/o6p/double_crux_a_strategy_for_resolving_disagreement/). Unfortunately, that's not what academia rewards, so [it probably isn't going to happen there](https://en.wikipedia.org/wiki/Goodhart%27s_law).)\n\nI think Person A is trying to gesture at the claim that the student's model of the addition algorithm isn't made of Gears (and implicitly that it'd be better if it were). I think this clarifies both what A is saying and why it matters. In terms of the tests:\n\n*   The addition algorithm totally pays rent. E.g., if you count out 25 tokens and another 18 tokens and you then count the total number of tokens you get, that number should correspond to what the algorithm outputs. If it turned out that the student does the algorithm but the answer *doesn't* match the token count, then the student can only conclude that the addition algorithm isn't useful for the tokens. There isn't a lot else they can deduce. (By way of contrast, if I noticed this, then I'd conclude that either I'd made a mistake in running the algorithm or I'd made a mistake in counting, and I'd be very confident that at least one of those two things is true.)\n*   The student could probably readily imagine a world in which you *aren't* supposed to carry the 1 but the algorithm still works. This means their model isn't very constrained, at least as we're imagining it. (Whereas attempting to think about carrying being *wrong* to do for getting the right answer makes my head explode.)\n*   If the student forgot what their teacher said about what to do when a column adds up to more than nine, we imagine they wouldn't spontaneously notice the need to carry the 1. (If I forgot about carrying, though, I'd get [confused](https://www.lesswrong.com/lw/if/your_strength_as_a_rationalist/) about what to do with this extra ten and would come up with something mathematically equivalent to \"carrying the 1\".)\n\nI find this to be a useful [tabooing](https://www.lesswrong.com/lw/nu/taboo_your_words/) of the word \"understand\" in this context.\n\n* * *\n\n**Example: My mother**\n\nMy mother really likes learning about history.\n\nRight now, this is probably an unattached random fact in your mind. Maybe a month down the road I could ask you \"How does my mother feel about learning history?\" and you could [try to remember](https://www.lesswrong.com/lw/iq/guessing_the_teachers_password/) the answer, but you could [just as well believe the world works another way](https://www.lesswrong.com/lw/if/your_strength_as_a_rationalist/).\n\nBut for me, that's not true at all. If I forgot her feelings about learning about history, I could make a pretty educated guess based on my overall sense of her. I wouldn't be Earth-shatteringly *shocked* to learn that she doesn't like reading about history, but I'd be really confused, and it'd throw into question my sense of why she likes working with herbs and why she likes hanging out with her family. It would make me think that I hadn't quite understood what kind of person my mother is.\n\nAs you might have noticed, this is an application of tests 1 and 3. In particular, my model of my mom isn't *totally made of* Gears in the sense that I could tell you what she's feeling right now or whether she defaults to thinking in terms of [partitive division or quotative division](https://www.learner.org/courses/learningmath/number/session4/part_a/division.html). But the tests illustrate that my model of my mother is *more* Gears-like than *your* model of her.\n\nPart of the point I'm making with this example is that \"Gears-ness\" isn't a binary property of models. It's more like a spectrum, from \"random smattering of unconnected facts\" to \"clear axiomatic system with well-defined logical deductions\". (Or at least that's how I'm currently imagining the spectrum!)\n\nAlso, I speculate that this is part of what we mean when we talk about \"getting to know\" someone: it involves increasing the Gears-ness of our model of them. It's not about just getting some isolated facts about where they work and how many kids they have and what they like doing for hobbies. It's about fleshing out an *ability to be surprised* if you were to learn some new fact about them that didn't fit your model of them.\n\n(There's also an empirical question in getting to know someone of how well your Gears-ish model *actually matches* that person, but that's about the map/territory correspondence. I want to be careful to keep talking about properties of maps here.)\n\nThis lightly Gears-ish model of people is what I think lets you deduce what Mr. Rogers probably would have thought about, say, [people mistreating cats on Halloween](http://www.petful.com/behaviors/how-dangerous-is-halloween-for-black-cats/) even though I don't know if he ever talked about it. As per test #2, you'd probably be pretty shocked and confused if you were given compelling evidence that he had *joined in*, and I imagine it'd take a *lot* of evidence. And then you'd have to update a *lot* about how you view Mr. Rogers (as per test #1). I think a lot of people had this kind of \"Who even *is* this person?\" experience when [lots of criminal charges came out against Bill Cosby](https://en.wikipedia.org/wiki/Bill_Cosby_sexual_assault_allegations).\n\n* * *\n\n**Example: Gyroscopes**\n\nMost people feel visceral surprise when they watch [how gyroscopes behave](https://www.youtube.com/watch?v=p9zhP9Bnx-k#t=1m6s). Even if they *logically know* the suspended gyroscope will rotate instead of falling, they usually *feel* like it's [bizarre](https://www.lesswrong.com/lw/hs/think_like_reality/) somehow. Even people who get gyroscopes' behavior into their intuitions probably had to train it for a while first and found them surprising and counterintuitive.\n\nSomehow, for most people, it seems coherent to imagine a world in which physics works exactly the same way *except that* when you suspend one end of a gyroscope, it falls like a non-spinning object would and just keeps spinning.\n\nIf this is true of you, that means your model of the physics around gyroscopes does poorly on test #2 of how Gears-like it is.\n\nThe *reason* gyroscopes do what they do is actually something you can derive from Newton's Laws of Motion. Like the gears example, you can't actually have a coherent model of rotation that allows (a) Newton's Laws and (b) a gyroscope that *doesn't* rotate instead of falling when suspended on one end in a gravitational field. So if both (a) and (b) seem plausible to you, then your model of rotation isn't coherent. It's missing Gears.\n\nThis is one of the beautiful (to me) things about physics: *everything* is made of Gears. Physics is (I think) the system of Gears you get when you stare at any physical object's behavior and ask \"What makes you do that?\" in a Gears-seeking kind of way. It's a different level of abstraction than the \"Gears of people\" thing, but we kind of expect that *eventually*, at least in theory, a sufficient extension of physics will connect the Gears of mechanics to the Gears of what makes a romantic relationship last while feeling good to be in.\n\nI want to rush to clarify that I'm *not* saying that *the world* is made of Gears. That's a type error. I'm suggesting that *the property of Gears-ness in models* is tracking a true thing about the world, which is why making models more Gears-like can be so powerful.\n\n* * *\n\n**Gears-ness is not the same as goodness**\n\nI want to emphasize that, while I think that more Gears are better all else being equal, there are other properties of models that I think are worthwhile.\n\nThe obvious one is accuracy. I've been intentionally sidestepping that property throughout most of this post. This is where the rationalist virtue of empiricism becomes critical, and I've basically ignored (but hopefully never defied!) empiricism here.\n\nAnother is *generativity*. Does the model *inspire a way of experiencing* in ways that are useful (whatever \"useful\" means)? For instance, many beliefs in God or the divine or similar are too abstract to [pay rent](https://www.lesswrong.com/lw/i3/making_beliefs_pay_rent_in_anticipated_experiences/), but some people still find them helpful for reframing how they emotionally experience beauty, meaning, and other people. I know of a few ex-atheists who say that having *become* Christian causes them to be nicer people and has made their relationships better. I think there's reason for epistemic fear here to the extent that those religious frameworks [sneak in claims](http://rationalwiki.org/wiki/Motte_and_bailey) about how the world actually works — but if you're epistemically careful, it seems possibly worthwhile to explore how to tap the power of faith without taking epistemic damage.\n\nI also think that even if you're trying to lean on the Gears-like power of a model, lacking Gears doesn't mean that the activity is worthless. In fact, I think this is *all we can do* most of the time, because most of our models don't connect all the way down to physics. E.g., I'm thinking of getting my mother a particular book as a gift because I think she'll really like it, but I can *also* come up with a within-my-model-of-her story about why she might *not* really care about it. I don't think the fact that my model of her is weakly constrained means that (a) I shouldn't use the model or that (b) it's not worthwhile to explore the \"why\" behind both my being right and my being wrong. (I think of it as a bit of pre-computation: whichever way the world goes, my model becomes a little more \"crisp\", which is to say, more Gears-like. It just so happens that I know *in what way* beforehand.)\n\nI mention this because sometimes in rationalist contexts, I've felt a pressure to *not talk about* models that are missing Gears. I don't like that. I think that Gears-ness is a *really super important* thing to track, and I think there's something epistemically dangerous about *failing to notice* a lack of Gears. Clearly noting, at least in your own mind, where there are and aren't Gears seems really good to me. But I think there are *other* capacities that are *also* important when we're trying to get epistemology right.\n\nGears seem valuable to me *for a reason*. I'd like us to keep that reason in mind rather than [getting too fixated on Gears-ness](https://en.wikipedia.org/wiki/Goodhart%27s_law).\n\n* * *\n\n**Going forward**\n\nI think this frame of Gears-ness of models is super powerful for cutting through confusion. It helps our understanding of the world become immune to social foolishness and demands a kind of rigor to our thinking that I see as unifying lots of ideas in the Sequences.\n\nI'll want to build on this frame as I highlight other ideas. In particular, I haven't spoken to *how we know Gears are worth looking for*. So while I view this as a powerful weapon to use in our war against [sanity drought](https://www.lesswrong.com/lw/1e/raising_the_sanity_waterline/), I think it's *also* important to examine the smithy in which it was forged. I suspect that won't be my *very next* post, but it's one I have in mind upcoming."
    },
    "voteCount": 81
  },
  {
    "_id": "HcCpvYLoSFP4iAqSz",
    "url": null,
    "title": "Rationality: Appreciating Cognitive Algorithms",
    "slug": "rationality-appreciating-cognitive-algorithms",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "Rationality"
      },
      {
        "name": "Alief"
      },
      {
        "name": "Truth, Semantics, & Meaning"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "**Followup to**: [The Useful Idea of Truth](/lw/eqn/the_useful_idea_of_truth/)\n\nIt is an error mode, and indeed an annoyance mode, to go about preaching the importance of the \"Truth\", especially if the Truth is supposed to be something incredibly lofty instead of some [boring](http://wiki.lesswrong.com/wiki/Joy_in_the_Merely_Real), [mundane](http://hpmor.com/chapter/73) truth about gravity or rainbows or what your coworker said about your manager.\n\nThus it is a worthwhile exercise to practice deflating the word 'true' out of any sentence in which it appears. (Note that this is a special case of [rationalist taboo](http://wiki.lesswrong.com/wiki/Rationalist_taboo).) For example, instead of saying, \"I believe that the sky is blue, and that's _true!_\" you can just say, \"The sky is blue\", which conveys essentially the same information about what color you think the sky is. Or if it feels _different_ to say \"I believe the Democrats will win the election!\" than to say, \"The Democrats will win the election\", this is an important warning of [belief-alief divergence](/lw/i4/belief_in_belief/).\n\nTry it with these:\n\n*   I believe Jess just wants to win arguments.\n*   It’s true that you weren’t paying attention.\n*   I believe I will get better.\n*   In reality, teachers care a lot about students.\n\nIf 'truth' is defined by an infinite family of sentences like 'The sentence \"the sky is blue\" is true if and only if the sky is blue', then why would we ever need to talk about 'truth' at all?\n\nWell, you can't deflate 'truth' out of the sentence \"True beliefs are more likely to make successful experimental predictions\" because it states a property of map-territory correspondences _in general_. You could say 'accurate maps' instead of 'true beliefs', but you would still be invoking the same _concept._\n\nIt's only because most sentences containing the word 'true' are _not_ talking about map-territory correspondences in general, that most such sentences can be deflated.\n\nNow consider - when are you _forced_ to use the word 'rational'?\n\nAs with the word 'true', there are very few sentences that truly _need_ to contain the word 'rational' in them. Consider the following deflations, all of which convey essentially the same information about your own opinions:\n\n*   \"It's rational to believe the sky is blue\"   \n    -\\> \"I think the sky is blue\"   \n    -\\> \"The sky is blue\"\n    \n*   \"Rational Dieting: Why To Choose Paleo\"   \n    -\\> \"Why you should think the paleo diet has the best consequences for health\"   \n    -\\> \"I like the paleo diet\"\n    \n\nGenerally, when people bless something as 'rational', you could directly substitute the word 'optimal' with no loss of content - or in some cases the phrases 'true' or 'believed-by-me', if we're talking about a belief rather than a strategy.\n\nTry it with these:\n\n*   \"It’s rational to teach your children calculus.\"\n*   \"I think this is the most rational book ever.\"\n*   \"It's rational to believe in gravity.\"\n\n_M__editation__:_ Under what rare circumstances can you _not_ deflate the word 'rational' out of a sentence?\n\n...  \n...  \n...\n\nReply: We need the word 'rational' in order to talk about _cognitive algorithms_ or _mental processes_ with the property \"systematically increases map-territory correspondence\" (epistemic rationality) or \"systematically finds a better path to goals\" (instrumental rationality).\n\nE.g.:\n\n\"It's (epistemically) rational to believe more in hypotheses that make successful experimental predictions.\"\n\nor\n\n\"Chasing [sunk costs](http://en.wikipedia.org/wiki/Sunk_costs) is (instrumentally) irrational.\"\n\nYou can't deflate the _concept_ of rationality out of the intended meaning of those sentences. You could find some way to rephrase it without the _word_ 'rational'; but then you'd have to use other words describing the same concept, e.g:\n\n\"If you believe more in hypotheses that make successful predictions, your map will better correspond to reality over time.\"\n\nor\n\n\"If you chase sunk costs, you won't achieve your goals as well as you could otherwise.\"\n\nThe word 'rational' is properly used to talk about _cognitive algorithms_ which _systematically_ promote map-territory correspondences or goal achievement.\n\nSimilarly, a rationalist isn't just somebody who respects the Truth.\n\nAll too many people respect the Truth.\n\nThey respect the Truth that the U.S. government planted explosives in the World Trade Center, the Truth that the stars control human destiny (ironically, the exact reverse will be true if everything goes right), the Truth that global warming is a lie... and so it goes.\n\nA rationalist is somebody who respects the _processes of finding truth._ They respect somebody who seems to be showing genuine curiosity, even if that curiosity is about a should-already-be-settled issue like whether the World Trade Center was brought down by explosives, because genuine curiosity is part of a lovable algorithm and respectable process. They respect Stuart Hameroff for trying to test whether neurons have properties conducive to quantum computing, even if this idea seems exceedingly unlikely a priori and was suggested by awful Gödelian arguments about why brains can't be mechanisms, because Hameroff was _trying to test his wacky beliefs experimentally,_ and humanity would still be living on the savanna if 'wacky' beliefs never got tested experimentally.\n\nOr consider the controversy over the way CSICOP (Committee for Skeptical Investigation of Claims of the Paranormal) handled the so-called [Mars effect](http://en.wikipedia.org/wiki/Mars_effect), the controversy which led founder Dennis Rawlins to leave CSICOP. Does the position of the planet Mars in the sky during your hour of birth, _actually_ have an effect on whether you'll become a famous athlete? I'll go out on a limb and say no. And if you _only_ respect the Truth, then it doesn't matter very much whether CSICOP raised the goalposts on the astrologer Gauquelin - i.e., stated a test and then made up new reasons to reject the results after Gauquelin's result came out positive. The astrological conclusion is almost certainly un-true... and that conclusion was indeed derogated, the Truth upheld.\n\nBut a _rationalist_ is disturbed by the claim that there were _rational process violations_. As a Bayesian, in a case like this you do update to a very small degree in favor of astrology, just not enough to overcome the prior odds; and you update to a larger degree that Gauquelin has inadvertantly uncovered some other phenomenon that might be worth tracking down. One definitely shouldn't state a test and then ignore the results, or find new reasons the test is invalid, when the results don't come out your way. That process has bad _systematic_ properties for finding truth - and a rationalist doesn't just appreciate the beauty of the Truth, but the beauty of the processes and cognitive algorithms that get us there.\\[1\\]\n\nThe reason why rationalists can have unusually productive and friendly conversations_ at least when everything goes right,_ is not that everyone involved has a great and abiding respect for whatever they think is the True or the Optimal in any given moment. Under most everyday conditions, people who argue heatedly aren't doing so because they know the truth but disrespect it. Rationalist conversations are (potentially) more productive to the degree that everyone respects the _process,_ and is on mostly the same page about what the process should be, thanks to all that explicit study of things like cognitive psychology and probability theory. When Anna tells me, \"I'm worried that you don't seem very curious about this,\" there's this state of mind called 'curiosity' that we both agree is important - as a matter of _rational process,_ on a meta-level above the particular issue at hand - and I know as a matter of process that when a respected fellow rationalist tells me that I need to become curious, I should pause and check my curiosity levels and try to increase them.\n\nIs rationality-use necessarily tied to rationality-appreciation?  I can imagine a world filled with hordes of rationality-users who were taught in school to use the Art competently, even though only very few people love the Art enough to try to advance it further; and everyone else has no particular love or interest in the Art apart from the practical results it brings. Similarly, I can imagine a competent applied mathematician who only worked at a hedge fund for the money, and had never loved math or programming or optimization in the first place - who'd been in it for the money from day one. I can imagine a competent musician who had no particular love in composition or joy in music, and who only cared for the album sales and groupies. Just because something is imaginable doesn't make it probable in real life... but then there are many children who learn to play the piano despite having no love for it; \"musicians\" are those who are _unusually_ good at it, not the adequately-competent.\n\nBut for now, in this world where the Art is _not_ yet forcibly impressed on schoolchildren nor yet explicitly rewarded in a standard way on standard career tracks, almost everyone who has any skill at rationality is the sort of person who finds the Art intriguing for its own sake. Which - perhaps unfortunately - explains quite a bit, both about rationalist communities and about the world.\n\n* * *\n\n\\[1\\] RationalWiki really needs to rename itself to SkepticWiki. They're very interested in kicking hell out of homeopathy, but not as a group interested in the abstract beauty of questions like \"What trials should a strange new hypothesis undergo, which it will _not_fail if the hypothesis is true?\" You can go to them and be like, \"You're criticizing theory X because some people who believe in it are stupid; but many true theories have stupid believers, like how Deepak Chopra claims to be talking about quantum physics; so this is not a useful method in general for discriminating true and false theories\" and they'll be like, \"Ha! So what? Who cares? X is crazy!\" I think it was actually RationalWiki which first observed that it and Less Wrong ought to swap names.\n\n* * *\n\n([Mainstream status here](#7kyw).)\n\nPart of the sequence [_Highly Advanced Epistemology 101 for Beginners_](http://wiki.lesswrong.com/wiki/Highly_Advanced_Epistemology_101_for_Beginners)\n\nNext post: \"[Firewalling the Optimal from the Rational](/lw/etf/firewalling_the_optimal_from_the_rational/)\"\n\nPrevious post: \"[Skill: The Map is Not the Territory](/lw/erp/skill_the_map_is_not_the_territory/)\""
    },
    "voteCount": 63
  },
  {
    "_id": "xLm9mgJRPvmPGpo7Q",
    "url": null,
    "title": "The Cognitive Science of Rationality",
    "slug": "the-cognitive-science-of-rationality",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "Rationality"
      },
      {
        "name": "Cognitive Science"
      },
      {
        "name": "Neuroscience"
      }
    ],
    "tableOfContents": {
      "sections": [
        {
          "title": "Rationality",
          "anchor": "Rationality",
          "level": 1
        },
        {
          "title": "Human reasoning",
          "anchor": "Human_reasoning",
          "level": 1
        },
        {
          "title": "Types of errors",
          "anchor": "Types_of_errors",
          "level": 1
        },
        {
          "title": "Rationality Skills",
          "anchor": "Rationality_Skills",
          "level": 1
        },
        {
          "title": "Notes",
          "anchor": "Notes",
          "level": 1
        },
        {
          "title": "References",
          "anchor": "References",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        },
        {
          "anchor": "comments",
          "level": 0,
          "title": "111 comments"
        }
      ],
      "headingsCount": 8
    },
    "contents": {
      "markdown": "(The post is written for beginners. Send the link to your friends! Regular Less Wrong readers may want to jump to the [Stanovich material](#HumanReasoning).)\n\n![](http://commonsenseatheism.com/wp-content/uploads/2011/09/Flammarion-Engraving.jpg)\n\nThe last 40 years of cognitive science have taught us a great deal about how our brains produce errors in thinking and decision making, and about how we can overcome those errors. These methods can help us form more accurate beliefs and make better decisions.\n\nLong before the first Concorde supersonic jet was completed, the British and French governments developing it realized it would lose money. But they continued to develop the jet when they should have cut their losses, because they felt they had \"invested too much to quit\"^1^ (sunk cost fallacy^2^).\n\nJohn tested positive for an extremely rare but fatal disease, using a test that is accurate 80% of the time. John didn't have health insurance, and the only available treatment — which his doctor recommended — was very expensive. John agreed to the treatment, his retirement fund was drained to nothing, and during the treatment it was discovered that John did _not_ have the rare disease after all. Later, a statistician explained to John that because the disease is so rare, the chance that he had had the disease even _given_ the positive test was less than one in a million. But neither John's brain nor his doctor's brain had computed this correctly ([base rate neglect](http://en.wikipedia.org/wiki/Base_rate_fallacy)).\n\nMary gave money to a charity to save lives in the developing world. Unfortunately, she gave to a charity that saves lives at a cost of $100,000 per life instead of one that saves lives at 1/10th that cost, because the less efficient charity used a vivid picture of a starving child on its advertising, and our brains respond more to single, identifiable victims than to large numbers of victims (identifiability effect^3^ and scope insensitivity^4^).\n\nDuring the last four decades, cognitive scientists have discovered a long list of [common thinking errors](http://wiki.lesswrong.com/wiki/Bias) like these. These errors lead us to false beliefs and poor decisions.\n\nHow are these errors produced, and how can we overcome them? Vague advice like \"be skeptical\" and \"think critically\" may not help much. Luckily, cognitive scientists know a great deal about the mathematics of _correct_ thinking, how thinking errors are _produced_, and how we can _overcome_ these errors in order to live more fulfilling lives.\n\n#### Rationality\n\nFirst, what is [rationality](/lw/31/what_do_we_mean_by_rationality/)? It is not the same thing as intelligence, because even those with high intelligence fall prey to some thinking errors as often as everyone else.^5^ But then, what _is_ rationality?\n\n![](http://commonsenseatheism.com/wp-content/uploads/2011/09/sherlock-holmes1.jpg)Cognitive scientists recognize two kinds of rationality:\n\n*   **Epistemic rationality** is about forming true beliefs, about getting the _map in your head_ to accurately reflect the _territory of the world_. We can measure epistemic rationality by comparing the rules of logic and probability theory to the way that a person _actually_ updates their beliefs.\n*   **Instrumental rationality** is about making decisions that are well-aimed at bringing about what you want. Due to habit and bias, many of our decisions don't actually align with our goals. We can measure instrumental rationality with a variety of techniques developed in economics, for example testing whether a person obeys the 'axioms of choice'.^6^\n\nIn short, rationality improves our choices concerning what to believe and what to do.\n\nUnfortunately, human _ir_rationality is quite common, as shown in popular books like _[Predictably Irrational: The Hidden Forces that Shape Our Decisions](http://www.amazon.com/Predictably-Irrational-Revised-Expanded-Decisions/dp/0061353248/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)_ and _[Kluge: The Haphazard Evolution of the Human Mind](http://www.amazon.com/Kluge-Haphazard-Evolution-Human-Mind/dp/B002ECETZY/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)_.\n\nEver since Aristotle spoke of humans as the \"rational animal,\" we've had a picture of ourselves as rational beings that are hampered by shortcomings like anger and fear and confirmation bias.\n\nCognitive science says just the opposite. Cognitive science shows us that humans just _are_ a collection of messy little modules like anger and fear and the modules that produce confirmation bias. We have a _few_ modules for processing logic and probability and rational goal-pursuit, but they are slow and energy-expensive and rarely used.\n\nAs we'll see, our brains avoid using these expensive modules whenever possible. Pete Richerson and Robert Boyd explain:\n\n> ...all animals are under stringent selection pressure to be as stupid as they can get away with.^7^\n\nOr, as philosopher David Hull put it:\n\n> The rule that human beings seem to follow is to engage \\[rational thought\\] only when all else fails — and usually not even then.^8^\n\n#### Human reasoning\n\nSo how does human reasoning work, and why does it so often produce mistaken judgments and decisions?\n\n[![](http://commonsenseatheism.com/wp-content/uploads/2011/09/thinking-fast-and-slow1.jpg)](http://www.amazon.com/Thinking-Fast-Slow-Daniel-Kahneman/dp/0374275637/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)Today, cognitive scientists talk about [two kinds](/lw/531/how_you_make_judgments_the_elephant_and_its_rider/) of processes, what Daniel Kahneman (2011) calls \"fast and slow\" processes:\n\n*   **Type 1 processes** are fast, do not require conscious attention, do not need input from conscious processes, and can operate in parallel.\n*   **Type 2 processes** are slow, require conscious effort, and generally only work one at a time.\n\nType 1 processes provide judgments quickly, but these judgments are often wrong, and can be overridden by corrective Type 2 processes.\n\nType 2 processes are computationally expensive, and thus humans are '[cognitive misers](/lw/2ey/a_taxonomy_of_bias_the_cognitive_miser/)'. This means that we (1) default to Type 1 processes whenever possible, and (2) when we _must_ use Type 2 processes, we use the least expensive kinds of Type 2 processes, those with a 'focal bias' — a disposition to reason from the _simplest_ model available instead of considering all the relevant factors. Hence, we are subject to confirmation bias (our cognition is focused on what we already believe) and other biases.\n\nSo, cognitive miserliness can cause three types of thinking errors:\n\n1.  We default to Type 1 processes when Type 2 processes are needed.\n2.  We fail to override Type 1 processes with Type 2 processes.\n3.  Even when we override with Type 2 processes, we use Type 2 processes with focal bias.\n\nBut the problem gets worse. If someone is going to override Type 1 processes with Type 2 processes, then she also needs the right _content_ available with which to do the overriding. For example, she may need to override a biased intuitive judgment with a correct application of probability theory, or a correct application of deductive logic. Such tools are called '[mindware](/lw/2fj/a_taxonomy_of_bias_mindware_problems/)'.^9^\n\nThus, thinking can also go wrong if there is a 'mindware gap' — that is, if an agent lacks crucial mindware like probability theory.\n\nFinally, thinking can go wrong due to 'contaminated mindware' — mindware that exists but is _wrong_. For example, an agent may have the naive belief that they know their own minds quite well, which is [false](/lw/5sk/inferring_our_desires/). Such mistaken mindware can lead to mistaken judgments.\n\n#### Types of errors\n\nGiven this understanding, a taxonomy of thinking errors could begin like this:^10^\n\n![](http://commonsenseatheism.com/wp-content/uploads/2011/09/errors-taxonomy-beginning.png)\n\nThe circles on the left capture the three normal sources of thinking errors. The three rectangles to the right of 'Cognitive Miserliness' capture the three categories of error that can be caused by cognitive miserliness. The _rounded_ rectangles to the right of 'Mindware Gap' and 'Corrupted Mindware' propose some _examples_ of (1) mindware that, if missing, can cause a mindware gap, and (2) common contaminated mindware.\n\nThe process for solving a reasoning task, then, may look something like this:^11^\n\n![](http://commonsenseatheism.com/wp-content/uploads/2011/09/Type-1-and-Type-2-process-determination.png)\n\nFirst, do I have mindware available to solve the reasoning problem before me with slow, deliberate, Type 2 processes? If not, my brain must use fast but inaccurate Type 1 processes to solve the problem. If I do have mindware available to solve this problem, do I notice the need to engage it? If not, my brain defaults to the cheaper Type 1 processes. If I _do_ notice the need to engage Type 2 processes and have the necessary mindware, is _sustained_ (as opposed to momentary) 'Type 2 override' required to solve the problem? If not, then I use that mindware to solve the problem. If sustained override _is_ required to solve the reasoning problem and I don't have the cognitive capacity (e.g. working memory) needed to complete the override, then my brain will default back to Type 1 processes. Otherwise, I'll use my cognitive capacities to sustain Type 2 override well enough to complete the reasoning task with my Type 2 processes (mindware).\n\nThat may be something like how our brains determine how to solve a reasoning task.\n\nIt's this model that Stanovich and colleagues (2010) use to explain why, among other things, [IQ is correlated with performance on some reasoning tasks but not others](http://commonsenseatheism.com/wp-content/uploads/2011/09/Tasks-that-do-and-do-not-correlate-with-cognitive-ability.pdf). For example, IQ correlates with performance on tests of outcome bias and hindsight bias, but not with performance on tests of anchoring effects and omission bias. To overcome these latter biases, subjects seem to need not just high cognitive capacity ([fluid intelligence](http://en.wikipedia.org/wiki/Fluid_and_crystallized_intelligence), [working memory](http://en.wikipedia.org/wiki/Working_memory), etc.), but also specific rationality training.\n\n![](http://commonsenseatheism.com/wp-content/uploads/2011/09/reflective-algorithmic-autonomous.png)If this is right, then we may talk of three different 'minds' at work in solving reasoning problems:\n\n*   The _autonomous_ mind, made of unconscious Type 1 processes. There are few individual differences in its operation.\n*   The _algorithmic_ mind, made of conscious Type 2 processes. There are significant individual differences in fluid intelligence in particular and cognitive capacity in general — that is, differences in perceptual speed, discrimination accuracy, working memory capacity, and the efficiency of the retrieval of information stored in long-term memory.^12^\n*   The _reflective mind_, which shows individual differences in the disposition to use rationality mindware — the disposition to generate alternative hypotheses, to use fully disjunctive reasoning, to engage in actively open-minded thinking, etc.^13^\n\n#### Rationality Skills\n\nBut it is not enough to understand how the human brain produces thinking errors. We also must find ways to meliorate the problem if we want to have more accurate beliefs and more efficiently achieve our goals. As Milkman et al. (2010) say:\n\n> ...the time has come to move the study of biases in judgment and decision making beyond description and toward the development of improvement strategies.\n\nStanovich (2009) sums up our project:\n\n> To jointly achieve epistemic and instrumental rationality, a person must display judicious decision making, adequate behavioral regulation, wise goal prioritization, sufficient thoughtfulness, and proper evidence calibration. For example, epistemic rationality — beliefs that are properly matched to the world — requires probabilistic reasoning and the ability to calibrate theories to evidence. Instrumental rationality — maximizing goal fulfillment — requires adherence to all of the axioms of rational choice. People fail to fulfill the many different strictures of rational thought because they are cognitive misers, because they lack critical mindware, and because they have acquired contaminated mindware. These errors can be prevented by acquiring the mindware of rational thought and the thinking dispositions that prevent the overuse of the strategies of the cognitive miser.\n\nThis is the project of 'debiasing' ourselves^14^ with 'ameliorative psychology'.^15^\n\nWhat we want is a **Rationality Toolkit**: a set of skills and techniques that can be used to overcome and correct the errors of our primate brains so we can form more accurate beliefs and make better decisions.\n\nOur goal is not unlike Carl Sagan's '[Baloney Detection Kit](http://www.youtube.com/watch?v=eUB4j0n2UDU)', but the tools in our Rationality Toolkit will be more specific and better grounded in the cognitive science of rationality.\n\nI mentioned some examples of debiasing interventions that have been tested by experimental psychologists in my post [Is Rationality Teachable?](/lw/76x/is_rationality_teachable/) I'll start with those, then add a few techniques for ameliorating the [planning fallacy](/lw/jg/planning_fallacy/), and we've got the beginnings of our Rationality Toolkit:\n\n1.  A simple instruction to \"think about alternatives\" can promote resistance to overconfidence and confirmation bias. In one study, subjects asked to generate their own hypotheses are more responsive to their accuracy than subjects asked to choose from among pre-picked hypotheses.^16^ Another study required subjects to list reasons for and against each of the possible answers to each question on a quiz prior to choosing an answer and assessing the probability of its being correct. This process resulted in more accurate confidence judgments relative to a control group.^17^ \n2.  Training in microeconomics can help subjects avoid the sunk cost fallacy.^18^ \n3.  Because people avoid the base rate fallacy more often when they encounter problems phrased in terms of frequencies instead of probabilities,^19^ teaching people to translate probabilistic reasoning tasks into frequency formats improves their performance.^20^ \n4.  Warning people about biases can decrease their prevalence. So far, this has been demonstrated to work with regard to framing effects,^21^ hindsight bias,^22^ and the outcome effect,^23^ though attempts to mitigate anchoring effects by warning people about them have produced weak results so far.^24^\n5.  Research on the planning fallacy suggests that taking an 'outside view' when predicting the time and resources required to complete a task will lead to better predictions. A specific instance of this strategy is 'reference class forecasting',^25^ in which planners project time and resource costs for a project by basing their projections on the outcomes of a distribution of comparable projects.\n6.  Unpacking the components involved in a large task or project helps people to see more clearly how much time and how many resources will be required to complete it, thereby partially meliorating the planning fallacy.^26^ \n7.  One reason we fall prey to the planning fallacy is that we do not remain as focused on the task at hand throughout its execution as when we are planning its execution. The planning fallacy can be partially meliorated, then, not only by improving the planning but by improving the execution. For example, in one study^27^ students were taught to imagine themselves performing each of the steps needed to complete a project. Participants rehearsed these simulations each day. 41% of these students completed their tasks on time, compared to 14% in a control group.\n\nBut this is [only the start](/lw/5x8/teachable_rationality_skills/). We need more rationality skills, and we need step-by-step instructions for how to teach them and how to implement them at the [5-second level](/lw/5kz/the_5second_level/).\n\n#### Notes\n\n^1^ Teger (1980).\n\n^2^ A sunk cost is a cost from the past that cannot be recovered. Because decision makers should consider only the future costs and benefits of the choices before them, sunk costs should be irrelevant to human decisions. Alas, sunk costs regularly do effect human decisions: Knox & Inkster (1968); Arkes & Blumer (1985); Arkes & Ayton (1999); Arkes & Hutzel (2000); Staw (1976); Whyte (1986).\n\n^3^ People are more generous (say, in giving charity) toward a single identifiable victim than toward unidentifiable or statistical victims (Kogut & Ritov 2005a, 2010; Jenni & Loewenstein 1997; Small & Loewenstein 2003; Small et al. 2007; Slovic 2007), even though they say they prefer to give to a group of people (Kogut & Ritov 2005b).\n\n^4^ Yudkowsky summarizes [scope insensitivity](/lw/hw/scope_insensitivity/):\n\n> Once upon a time, three groups of subjects were asked how much they would pay to save 2000 / 20000 / 200000 migrating birds from drowning in uncovered oil ponds. The groups respectively answered $80, $78, and $88 \\[Desvousges et al. 1992\\]. This is scope insensitivity or scope neglect: the number of birds saved — the scope of the altruistic action — had little effect on willingness to pay.\n\nSee also: Kahneman (1986); McFadden & Leonard (1995); Carson & Mitchell (1995); Fetherstonhaugh et al. (1997); Slovic et al. (2011).\n\n^5^ Stanovich & West (2008); Ross et al. (1977); Krueger (2000).\n\n^6^ Stanovich et al. (2008) write:\n\n> Cognitive scientists recognize two types of rationality: instrumental and epistemic... \\[We\\] could characterize instrumental rationality as the optimization of the individual’s goal fulfillment. Economists and cognitive scientists have reﬁned the notion of optimization of goal fulfillment into the technical notion of expected utility. The model of rational judgment used by decision scientists is one in which a person chooses options based on which option has the largest expected utility...\n> \n> The other aspect of rationality studied by cognitive scientists is termed epistemic rationality. This aspect of rationality concerns how well beliefs map onto the actual structure of the world. Instrumental and epistemic rationality are related. The aspect of beliefs that enter into instrumental calculations (i.e., tacit calculations) are the probabilities of states of affairs in the world.\n\nAlso see the discussion in Stanovich et al. (2011). On instrumental rationality as the maximization of expected utility, see Dawes (1998); Hastie & Dawes (2009); Wu et al. (2004). On epistemic reality, see Foley (1987); Harman (1995); Manktelow (2004); Over (2004).\n\nHow can we measure an individual's divergence from expected utility maximization if we can't yet measure utility directly? One of the triumphs of decision science is the demonstration that agents whose behavior respects the so-called 'axioms of choice' will behave as if they are maximizing expected utility. It can be difficult to measure utility, but it is easier to measure whether one of the axioms of choice are being violated, and thus whether an agent is behaving instrumentally irrationally.\n\nViolations of both instrumental and epistemic rationality have been catalogued at length by cognitive psychologists in the 'heuristics and biases' literature: Baron (2007); Evans (1989, 2007); Gilovich et al. (2002); Kahneman & Tversky (2000); Shaﬁr & LeBoeuf (2002); Stanovich (1999). For the argument against comparing human reasoning practice with normative reasoning models, see Elqayam & Evans (2011).\n\n^7^ Boyd & Richerson (2005), p. 135.\n\n^8^ Hull (2000), p. 37.\n\n^9^ Perkins (1995).\n\n^10^ Adapted from Stanovich et al. (2008).\n\n^11^ Adapted from Stanovich et al. (2010).\n\n^12^ Ackerman et al. (1999); Deary (2000, 2001); Hunt (1987, 1999); Kane & Engle (2002); Lohman (2000); Sternberg (1985, 1997, 2003); Unsworth & Engle (2005).\n\n^13^ See table 17.1 in Stanovich et al. (2010). The image is from Stanovich (2010).\n\n^14^ Larrick (2004).\n\n^15^ Bishop & Trout (2004).\n\n^16^ Koehler (1994).\n\n^17^ Koriat et al. (1980). Also see Soll & Klayman (2004); Mussweiler et al. (2000).\n\n^18^ Larrick et al. (1990).\n\n^19^ Gigerenzer & Hoffrage (1995).\n\n^20^ Sedlmeier (1999).\n\n^21^ Cheng & Wu (2010).\n\n^22^ Hasher et al. (1981); Reimers & Butler (1992).\n\n^23^ Clarkson et al. (2002).\n\n^24^ Block & Harper (1991); George et al. (2000).\n\n^25^ Lovallo & Kahneman (2003); Buehler et al. (2010); Flyvbjerg (2008); Flyvbjerg et al. (2009).\n\n^26^ Connolly & Dean (1997); Forsyth & Burt (2008); Kruger & Evans (2004).\n\n^27^ Taylor et al. (1988). See also Koole & Vant Spijker (2000).\n\n#### References\n\nAckerman, Kyllonen & Richards, eds. (1999). _[Learning and individual differences: Process, trait, and content determinants](http://www.amazon.com/Learning-Individual-Differences-Process-Determinants/dp/1557985367/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)_. American Psychological Association.\n\nArkes & Blumer (1985). [The psychology of sunk cost](http://commonsenseatheism.com/wp-content/uploads/2011/09/Arkes-Blumer-The-psychology-of-sunk-cost.pdf). _Organizational Behavior and Human Decision Processes, 35_: 124-140.\n\nArkes & Ayton (1999). [The sunk cost and Concorde effects: Are humans less rational than lower animals?](http://commonsenseatheism.com/wp-content/uploads/2011/09/Arkes-Ayton-The-sunk-cost-and-Concorde-effects-are-humans-less-rational-than-lower-animals.pdf) _Psychological Bulletin, 125_: 591-600.\n\nArkes & Hutzel (2000). [The role of probability of success estimates in the sunk cost effect](http://commonsenseatheism.com/wp-content/uploads/2011/09/Arkes-Hutzel-The-role-of-probability-of-success-estimates-in-the-sunk-cost-effect.pdf). _Journal of Behavioral Decision Making, 13_: 295-306.\n\nBaron (2007). _[Thinking and Deciding, 4th edition](http://www.amazon.com/Thinking-Deciding-Jonathan-Baron/dp/0521680433/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)_. Cambridge University Press.\n\nBishop & Trout (2004). _[Epistemology and the Psychology of Human Judgment](http://www.amazon.com/dp/0195162307/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)_. Oxford University Press.\n\nBlock & Harper (1991). Overconfidence in estimation: testing the anchoring-and-adjustment hypothesis. _Organizational Behavior and Human Decision Processes, 49_: 188–207.\n\nBuehler, Griffin, & Ross (1994). [Exploring the 'planning fallacy': Why people underestimate their task completion times](http://www.canni.be/FUSL/Wibaut/Comportement%20des%20investisseurs/Procrastination/Buehler_planning.pdf). _Journal of Personality and Social Psychology, 67_: 366-381.\n\nBuehler, Griffin, & Ross (1995). [It's about time: Optimistic predictions in work and love](http://commonsenseatheism.com/wp-content/uploads/2011/09/Buehler-et-al-Its-about-time-optimistic-predictions-in-work-and-love.pdf). _European Review of Social Psychology, 6_: 1-32.\n\nBuehler, Griffin, & Ross (2002). Inside the planning fallacy: The causes and consequences of optimistic time predictions. In Gilovich, Griffin, & Kahneman (eds.), _Heuristics and biases: The psychology of intuitive judgment_ (pp. 250-270). Cambridge University Press.\n\nBuehler, Griffin, & Peetz (2010). [The planning fallacy: cognitive, motivational, and social origins](http://commonsenseatheism.com/wp-content/uploads/2011/09/Buehler-et-al-The-Planning-Fallacy-Cognitive-motivational-and-social-origins.pdf). _Advances in Experimental Social Psychology, 43_: 1-62.\n\nCarson & Mitchell (1995). [Sequencing and Nesting in Contingent Valuation Surveys](http://econ.ucsd.edu/~rcarson/papers/Sequencing.pdf). _Journal of Environmental Economics and Management, 28_: 155-73.\n\nCheng & Wu (2010). Debiasing the framing effect: The effect of warning and involvement. _Decision Support Systems, 49_: 328-334.\n\nClarkson, Emby, & Watt (2002). Debiasing the effect of outcome knowledge: the role of instructions in an audit litigation setting. _Auditing: A Journal of Practice and Theory, 21_: 1–14.\n\nConnolly & Dean (1997). [Decomposed versus holistic estimates of effort required for software writing tasks](http://commonsenseatheism.com/wp-content/uploads/2011/09/Connoly-Dean-Decomposed-versus-holistic-estimates-of-effort-required-for-software-writing-tasks.pdf). _Management Science, 43_: 1029–1045.\n\nDawes (1998). Behavioral decision making and judgment. In Gilbert, Fiske, & Lindzey (eds.), The handbook of social psychology (Vol. 1, pp. 497–548). McGraw-Hill.\n\nDeary (2000). _[Looking down on human intelligence: From psychometrics to the brain](http://www.amazon.com/Looking-Down-Human-Intelligence-Psychometrics/dp/019852417X/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)_. Oxford University Press.\n\nDeary (2001). _[Intelligence: A very short introduction](http://www.amazon.com/Intelligence-Very-Short-Introduction-Introductions/dp/0192893211/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)_. Oxford University Press.\n\nDesvousges, Johnson, Dunford, Boyle, Hudson, & Wilson (1992). Measuring non-use damages using contingent valuation: experimental evaluation accuracy. Research Triangle Institute Monograph 92-1.\n\nElqayam & Evans (2011). [Subtracting 'ought' from 'is': Descriptivism versus normativism in the study of human thinking](http://www.psy.dmu.ac.uk/elqayam/Elqayam_BBS-D-10-00343_preprint.pdf). _Brain and Behavioral Sciences_.\n\nEvans (1989). _[Bias in Human Reasoning: Causes and Consequences](http://www.amazon.com/Bias-Human-Reasoning-Consequences-psychology/dp/0863771068/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)_. Lawrence Erlbaum Associates.\n\nEvans (2007). _[Hypothetical Thinking: Dual Processes in Reasoning and Judgment](http://www.amazon.com/Hypothetical-Thinking-Processes-Reasoning-Psychology/dp/1841696609/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)_. Psychology Press.\n\nFetherstonhaugh, Slovic, Johnson, & Friedrich (1997). [Insensitivity to the value of human life: A study of psychophysical numbing](http://commonsenseatheism.com/wp-content/uploads/2011/09/Fetherstonhaugh-et-al-Insensitivity-to-the-value-of-human-life-A-study-of-psychophysical-numbing.pdf). _Journal of Risk and Uncertainty, 14_: 238-300.\n\nFlyvbjerg (2008). [Curbing optimism bias and strategic misrepresentation in planning: Reference class forecasting in practice](http://commonsenseatheism.com/wp-content/uploads/2011/09/Flyvbjerg-Curbing-optimism-bias-and-strategic-misrepresentation-in-planning-reference-class-forecasting-in-practice.pdf). _European Planning Studies, 16_: 3–21.\n\nFlyvbjerg, Garbuio, & Lovallo (2009). Delusion and deception in large infrastructure projects: Two models for explaining and preventing executive disaster. _California Management Review, 51_: 170–193.\n\nFoley (1987). _[The Theory of Epistemic Rationality](http://www.amazon.com/Theory-Epistemic-Rationality-Richard-Foley/dp/0674882768/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)_. Harvard University Press.\n\nForsyth & Burt (2008). [Allocating time to future tasks: The effect of task segmentation on planning fallacy bias](http://commonsenseatheism.com/wp-content/uploads/2011/09/Forsyth-Burt-Allocating-time-to-future-tasks-The-effect-of-task-segmentation-on-planning-fallacy-bias.pdf). _Memory and Cognition, 36_: 791–798.\n\nGeorge, Duffy, & Ahuja (2000). Countering the anchoring and adjustment bias with decision support systems. _Decision Support Systems, 29_: 195–206.\n\nGigerenzer & Hoffrage (1995). How to improve Bayesian reasoning without instruction: Frequency formats. _Psychological Review, 102_: 684–704.\n\nGilovich, Grifﬁn, & Kahneman (eds.) (2002). _[Heuristics and Biases: The Psychology of Intuitive Judgment](http://www.amazon.com/Heuristics-Biases-Psychology-Intuitive-Judgment/dp/0521796792/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)_. Cambridge University Press.\n\nHarman (1995). Rationality. In Smith & Osherson (eds.), _Thinking_ (Vol. 3, pp. 175–211). MIT Press.\n\nHasher, Attig, & Alba (1981). I knew it all along: or did I? _Journal of Verbal and Learning Behavior, 20_: 86-96.\n\nHastie & Dawes (2009). _[Rational Choice in an Uncertain World, 2nd edition](http://www.amazon.com/Rational-Choice-Uncertain-World-Psychology/dp/1412959039/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)_. Sage.\n\nHull (2000). _[Science and selection: Essays on biological evolution and the philosophy of science](http://www.amazon.com/Science-Selection-Biological-Evolution-Philosophy/dp/0521644054/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)_. Cambridge University Press.\n\nHunt (1987). The next word on verbal ability. In Vernon (ed.), _Speed of information-processing and intelligence_ (pp. 347–392). Ablex.\n\nHunt (1999). Intelligence and human resources: Past, present, and future. In Ackerman & Kyllonen (Eds.), _The future of learning and individual differences research: Processes, traits, and content_ (pp. 3-30) American Psychological Association.\n\nJenni & Loewenstein (1997). [Explaining the 'identifiable victim effect.'](http://www.andrew.cmu.edu/user/gl20/GeorgeLoewenstein/Papers_files/pdf/identifiable-victim.pdf) _Journal of Risk and Uncertainty, 14_: 235–257.\n\nKahneman (1986). Comments on the contingent valuation method. In Cummings, Brookshie, & Schulze (eds.), _Valuing environmental goods: a state of the arts assessment of the contingent valuation method_. Roweman and Allanheld.\n\nKahneman & Tversky (2000). _[Choices, Values, and Frames](http://www.amazon.com/Choices-Values-Frames-Daniel-Kahneman/dp/0521627494/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)_. Cambridge University Press.\n\nKahneman (2011). _[Thinking, Fast and Slow](http://www.amazon.com/Thinking-Fast-Slow-Daniel-Kahneman/dp/0374275637/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)_. Farrar, Straus and Giroux.\n\nKane & Engle (2002). [The role of prefrontal cortex working-memory capacity, executive attention, and general fluid intelligence: An individual differences perspective](http://psychology.gatech.edu/renglelab/Publications/2002/The%20role%20of%20prefrontal%20cortex%20in%20working-memory%20capacity.pdf). _Psychonomic Bulletin and Review, 9_: 637–671.\n\nKnox & Inkster (1968). [Postdecision dissonance at post time](http://commonsenseatheism.com/wp-content/uploads/2011/09/Knox-Inkster-Postdecision-dissonance-at-post-time.pdf). _Journal of Personality and Social Psychology, 8_: 319-323.\n\nKoehler (1994). Hypothesis generation and confidence in judgment. _Journal of Experimental Psychology: Learning, Memory, and Cognition, 20_: 461-469.\n\nKogut & Ritov (2005a). [The 'identified victim effect': An identified group, or just a single individual?](http://pluto.mscc.huji.ac.il/~msiritov/KogutRitovIdentified.pdf) _Journal of Behavioral Decision Making, 18_: 157–167.\n\nKogut & Ritov (2005b). [The singularity effect of identified victims in separate and joint evaluations](http://www.unitn.it/files/download/14235/singularity_of_identified_victim.pdf). _Organizational Behavior and Human Decision Processes, 97_: 106–116.\n\nKogut & Ritov (2010). The identifiable victim effect: Causes and boundary conditions. In Oppenheimer & Olivola (eds.), _The Science of Giving: Experimental Approaches to the Study of Charity_ (pp. 133-146). Psychology Press.\n\nKoriat, Lichtenstein, & Fischhoff (1980). [Reasons for confidence](http://step.psy.cmu.edu/articles/Koriat.pdf). _Journal of Experimental Psychology: Human Learning and Memory, 6_: 107-118.\n\nKoole & Vant Spijker (2000). [Overcoming the planning fallacy through willpower: Effects of implementation intentions on actual and predicted task-completion times](http://www.sanderkoole.com/KS2000.pdf). _European Journal of Social Psychology, 30_: 873–888.\n\nKrueger (2000). [Individual differences and Pearson's r: Rationality revealed?](http://commonsenseatheism.com/wp-content/uploads/2011/09/Krueger-Individual-differences-and-Pearson’s-r-Rationality-revealed.pdf) _Behavioral and Brain Sciences, 23_: 684–685.\n\nKruger & Evans (2004). If you don’t want to be late, enumerate: Unpacking reduces the planning fallacy. _Journal of Experimental Social Psychology, 40_: 586–598.\n\nLarrick (2004). [Debiasing](http://commonsenseatheism.com/wp-content/uploads/2011/09/Larrick-Debiasing.pdf). In Koehler & Harvey (eds.), _Blackwell Handbook of Judgment and Decision Making_ (pp. 316-337). Wiley-Blackwell.\n\nLarrick, Morgan, & Nisbett (1990). Teaching the use of cost-benefit reasoning in everyday life. _Psychological Science, 1_: 362-370.\n\nLohman (2000). Complex information processing and intelligence. In Sternberg (ed.), _Handbook of intelligence_ (pp. 285–340). Cambridge University Press.\n\nLovallo & Kahneman (2003). [Delusions of success: How optimism undermines executives' decisions](http://commonsenseatheism.com/wp-content/uploads/2011/09/Lovallo-Kahneman-Delusions-of-success-How-optimism-undermines-executives-decisions.pdf). _Harvard Business Review, July 2003_: 56-63.\n\nManktelow (2004). Reasoning and rationality: The pure and the practical. In Manktelow & Chung (eds.), _Psychology of reasoning: Theoretical and historical perspectives_ (pp. 157–177). Psychology Press.\n\nMcFadden & Leonard (1995). Issues in the contingent valuation of environmental goods: methodologies for data collection and analysis. In Hausman (ed.), _Contingent valuation: a critical assessment_. North Holland.\n\nMilkman, Chugh, & Bazerman (2010). [How can decision making be improved?](http://www.hbs.edu/research/pdf/08-102.pdf) _Perspectives on Psychological Science 4_: 379-383.\n\nMussweiler, Strack, & Pfeiffer (2000). Overcoming the inevitable anchoring effect: Considering the opposite compensates for selective accessibility. _Personality and Social Psychology Bulletin, 26_: 1142–50.\n\nOreg & Bayazit. [Prone to Bias: Development of a Bias Taxonomy From an Individual Differences Perspective](http://commonsenseatheism.com/wp-content/uploads/2011/09/Oreg-Bayazit-Prone-to-bias-development-of-a-bias-taxonomy-from-an-individual-differences-perspective.pdf). _Review of General Psychology, 3_: 175-193.\n\nOver (2004). [Rationality and the normative/descriptive distinction](http://commonsenseatheism.com/wp-content/uploads/2011/09/Over-Rationality-and-the-Normative-Descriptive-Distinction.pdf). In Koehler & Harvey (eds.), _Blackwell handbook of judgment and decision making_ (pp. 3–18). Blackwell Publishing.\n\nPeetz, Buehler & Wilson (2010). [Planning for the near and distant future: How does temporal distance affect task completion predictions?](http://commonsenseatheism.com/wp-content/uploads/2011/09/Peetz-et-al-Planning-for-the-near-and-distant-future-how-does-temporal-distance-affect-task-completion-predictions.pdf) _Journal of Experimental Social Psychology, 46_: 709-720.\n\nPerkins (1995). _[Outsmarting IQ: The emerging science of learnable intelligence](http://www.amazon.com/Outsmarting-IQ-Emerging-Learnable-Intelligence/dp/0029252121/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)_. Free Press.\n\nPezzo, Litman, & Pezzo (2006). [On the distinction between yuppies and hippes: Individual differences in prediction biases for planning future tasks](http://www.usfsp.edu/pezzo/documents/PezzoLitmanPezzo--PAID2006_000.pdf). _Personality and Individual Differences, 41_: 1359-1371.\n\nReimers & Butler (1992). The effect of outcome knowledge on auditor's judgmental evaluations. _Accounting, Organizations and Society, 17_: 185–194.\n\nRicherson & Boyd (2005). _[Not By Genes Alone: How Culture Transformed Human Evolution](http://www.amazon.com/Not-Genes-Alone-Transformed-Evolution/dp/0226712125/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)_. University of Chicago Press.\n\nRoss, Greene, & House (1977). [The false consensus phenomenon: An attributional bias in self-perception and social perception processes](http://commonsenseatheism.com/wp-content/uploads/2011/09/Ross-et-al-The-false-consensus-effect-an-egocentric-bias-in-social-perception-and-attribution-processes.pdf). _Journal of Experimental Social Psychology, 13_: 279–301.\n\nRoy, Christenfeld, & McKenzie (2005). [Underestimating the duration of future events: Memory incorrectly used or memory bias?](http://users.etown.edu/r/roym/my%20papers/royetal.,2005.pdf) _Psychological Bulletin, 131_: 738-756.\n\nSedlmeier (1999). _[Improving Statistical Reasoning: Theoretical Models and Practical Implications](http://www.amazon.com/Improving-Statistical-Reasoning-Theoretical-Implications/dp/0805832823/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)_. Erlbaum.\n\nShaﬁr & LeBoeuf (2002). [Rationality](http://commonsenseatheism.com/wp-content/uploads/2011/09/Shafir-LeBoeuf-Rationality.pdf). _Annual Review of Psychology, 53_: 491–517.\n\nSlovic (2007). [If I look at the mass I will never act: Psychic numbing and genocide](http://www.sas.upenn.edu/~baron/journal/7303a/jdm7303a.htm). _Judgment and Decision Making, 2_: 1–17.\n\nSlovic, Zionts, Woods, Goodman, & Jinks (2011). [Psychic numbing and mass atrocity](http://web.business.queensu.ca/faculty/lashworth/conference/slovic_bfp.pdf). In E. Shafir (ed.), _The behavioral foundations of policy_. Sage and Princeton University Press.\n\nSmall & Loewenstein (2003). [Helping a victim or helping the victim: Altruism and identifiability](http://marketing.wharton.upenn.edu/ideas/pdf/Small/JRU-2003-victim-paper.pdf). _Journal of Risk and Uncertainty, 26_: 5–16.\n\nSmall, Loewenstein, & Slovic (2007). [Sympathy and callousness: The impact of deliberative thought on donations to identifiable and statistical victims](http://opim.wharton.upenn.edu/risk/library/J2007OBHDP_DAS_sympathy.pdf). _Organizational Behavior and Human Decision Processes, 102_: 143–153.\n\nSoll & Klayman (2004). Overconfidence in interval estimates. _Journal of Experimental Psychology: Learning, Memory, and Cognition, 30_: 299–314.\n\nStanovich (1999). _[Who is rational? Studies of individual differences in reasoning](http://www.amazon.com/Who-Rational-individual-Differences-Reasoning/dp/0805824731/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)_. Erlbaum.\n\nStanovich (2009). _[What Intelligence Tests Miss: The Psychology of Rational Thought](http://www.amazon.com/What-Intelligence-Tests-Miss-Psychology/dp/0300164629/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)_. Yale University Press.\n\nStanovich & West (2008). [On the failure of cognitive ability to predict myside bias and one-sided thinking biases](http://educ.jmu.edu/~westrf/papers/Stanovich-Myside-OneSide-TaR08.pdf). _Thinking and Reasoning, 14_: 129–167.\n\nStanovich, Toplak, & West (2008). [The development of rational thought: A taxonomy of heuristics and biases](http://educ.jmu.edu/~westrf/papers/Stanovich-Dev-Rat-Advances-InPrs.pdf). _Advances in Child Development and Behavior, 36_: 251-285.\n\nStanovich, West, & Toplak (2010). [Individual differences as essential components of heuristics and biases research](http://commonsenseatheism.com/wp-content/uploads/2011/09/Stanovich-et-al-Individual-differences-as-essential-components-of-heuristics-and-biases-research.pdf). In Manktelow, Over, & Elqayam (eds.), _The Science of Reason: A Festschrift for Jonathan St B.T. Evans_ (pp. 355-396). Psychology Press.\n\nStanovich, West, & Toplak (2011). [Intelligence and rationality](http://web.mac.com/kstanovich/Site/Research_on_Reasoning_files/Stanovich_Handbook.pdf). In Sternberg & Kaufman (eds.), _Cambridge Handbook of Intelligence, 3rd edition_ (pp. 784-826). Cambridge University Press.\n\nStaw (1976). [Knee-deep in the big muddy: a study of escalating commitment to a chosen course of action](http://commonsenseatheism.com/wp-content/uploads/2011/09/Staw-Knee-Deep-in-the-Big-Muddy-a-study-of-escalating-commitment-to-a-chosen-course-of-action.pdf). _Organizational Behavior and Human Performance, 16_: 27-44.\n\nSternberg (1985). _[Beyond IQ: A triarchic theory of human intelligence](http://www.amazon.com/Beyond-IQ-Triarchic-Theory-Intelligence/dp/0521278910/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)_. Cambridge University Press.\n\nSternberg (1997). _[Thinking Styles](http://www.amazon.com/Thinking-Styles-Robert-Sternberg-PhD/dp/052165713X/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)_. Cambridge University Press.\n\nSternberg (2003). _[Wisdom, intelligence, and creativity synthesized](http://www.amazon.com/Wisdom-Intelligence-Creativity-Synthesized-Sternberg/dp/0521002710/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)_. Cambridge University Press.\n\nTaylor, Pham, Rivkin & Armor (1998). [Harnessing the imagination: Mental simulation, self-regulation, and coping](http://commonsenseatheism.com/wp-content/uploads/2011/09/Taylor-et-al-Harnessing-the-imagination-Mental-simulation-self-regulation-and-coping.pdf). _American Psychologist, 53_: 429–439.\n\nTeger (1980). _[Too Much Invested to Quit](http://www.amazon.com/Much-Invested-Quit-General-Psychology/dp/0080229956/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)_. Pergamon Press.\n\nTversky & Kahneman (1979). [Intuitive prediction: Biases and corrective procedures](http://www.dtic.mil/cgi-bin/GetTRDoc?AD=ADA047747&Location=U2&doc=GetTRDoc.pdf). _TIMS Studies in Management Science, 12_: 313-327.\n\nTversky & Kahneman (1981). [The framing of decisions and the psychology of choice](http://www.brainvitge.org/papers/tverski_kahneman.pdf). _Science, 211_: 453–458. \n\nUnsworth & Engle (2005). Working memory capacity and fluid abilities: Examining the correlation between Operation Span and Raven. _Intelligence, 33_: 67–81.\n\nWhyte (1986). [Escalating Commitment to a Course of Action: A Reinterpretation](http://commonsenseatheism.com/wp-content/uploads/2011/09/Arkes-Blumer-The-psychology-of-sunk-cost.pdf). _The Academy of Management Review, 11_: 311-321.\n\nWu, Zhang, & Gonzalez (2004). [Decision under risk](http://commonsenseatheism.com/wp-content/uploads/2011/09/Wu-et-al-Decision-under-risk.pdf). In Koehler & Harvey (eds.), _Blackwell handbook of judgment and decision making_ (pp. 399–423). Blackwell Publishing."
    },
    "voteCount": 112
  },
  {
    "_id": "bSWavBThj6ebB62gD",
    "url": null,
    "title": "Offer of collaboration and/or mentorship",
    "slug": "offer-of-collaboration-and-or-mentorship",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "Community"
      },
      {
        "name": "Altruism"
      }
    ],
    "tableOfContents": {
      "sections": [
        {
          "title": "Mentorship",
          "anchor": "Mentorship",
          "level": 1
        },
        {
          "title": "Collaboration",
          "anchor": "Collaboration",
          "level": 1
        },
        {
          "title": "Contact",
          "anchor": "Contact",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        },
        {
          "anchor": "comments",
          "level": 0,
          "title": "14 comments"
        }
      ],
      "headingsCount": 5
    },
    "contents": {
      "markdown": "**UPDATE:** Offer of mentorship is closed, since I received sufficiently many candidates for now. Offer of collaboration remains open for experienced researchers (i.e. researchers that (i) have some track record of original math / theoretical compsci research, and (ii) are able to take on concrete open problems without much guidance).\n\n***\n\nI have two motivations for making this offer. First, there have been discussions regarding the lack of mentorship in the AI alignment community, and that beginners find it difficult to enter the field since the experienced researchers are too busy working on their research to provide guidance. Second, I have my own [research programme](https://www.alignmentforum.org/posts/5bd75cc58225bf0670375575/the-learning-theoretic-ai-alignment-research-agenda) which has a significant number of shovel ready open problems and only one person working on it (me). The way I see it, my research programme is a very promising approach that attacks the very core of the AI alignment problem.\n\nTherefore, I am looking for people who would like to either receive mentorship in AI alignment relevant topics from me, or collaborate with me on my research programme, or both.\n\n# Mentorship\n\nI am planning to allocate about 4 hours / week to mentorship, which can be done over Skype, Discord, email or any other means of remote communication. For people who happen to be located in Israel, we can do in person sessions. The mathematical topics in which I feel qualified to provide guidance include: linear algebra, calculus, functional analysis, probability theory, game theory, computability theory, computational complexity theory, statistical/computational learning theory. I am also more or less familiar with the state of the art in the various approaches other people pursue to AI alignment.\n\nNaturally, people who are interested in working on my own research programme are those who would benefit the most from my guidance. People who want to work on empirical ML approaches (which seem to be dominant in OpenAI, DeepMind and CHAI) would benefit somewhat from my guidance, since many theoretical insights from computational learning theory in general and my own research in particular, are to some extent applicable even to deep learning algorithms whose theoretical understanding is far from complete. People who want to work on MIRI's core research agenda would also benefit somewhat from my guidance but I am less knowledgeable or interested in formal logic and approaches based on formal logic.\n\n# Collaboration\n\nPeople who want to collaborate on problems within the learning-theoretic research programme might receive a significantly larger fraction of my time, depending on details. The communication would still be mostly remote (unless the collaborator is in Israel), but physical meetings involving flights are also an option.\n\nThe [original essay](https://www.alignmentforum.org/posts/5bd75cc58225bf0670375575/the-learning-theoretic-ai-alignment-research-agenda) about the learning-theoretic programme does mention a number of more or less concrete research directions, but since then more shovel ready problems joined the list (and also, there are a couple of [new](https://www.alignmentforum.org/posts/Qa5jG9z9dC6E4s9JH/dimensional-regret-without-resets) [results](https://www.alignmentforum.org/posts/aAzApjEpdYwAxnsAS/reinforcement-learning-with-imperceptible-rewards)). Interested people are advised to contact me to hear about those problems and discuss the details.\n\n# Contact\n\nAnyone who wants to contact me regarding the above should email me at vanessa.kosoy@intelligence.org, and give me a brief intro about emself, including knowledge in math / theoretical compsci and previous research if relevant. Conversely, you are welcome to browse my writing on this forum to form an impression of my abilities. If we find each other mutually compatible, we will discuss further details."
    },
    "voteCount": 38
  },
  {
    "_id": "cSzaxcmeYW6z7cgtc",
    "url": null,
    "title": "Contest: $1,000 for good questions to ask to an Oracle AI",
    "slug": "contest-usd1-000-for-good-questions-to-ask-to-an-oracle-ai",
    "author": "Stuart_Armstrong",
    "question": false,
    "tags": [
      {
        "name": "AI"
      },
      {
        "name": "Bounties (closed)"
      },
      {
        "name": "AI Boxing (Containment)"
      },
      {
        "name": "Oracle AI"
      }
    ],
    "tableOfContents": {
      "sections": [
        {
          "title": "The contest",
          "anchor": "The_contest",
          "level": 1
        },
        {
          "title": "Oracles",
          "anchor": "Oracles",
          "level": 2
        },
        {
          "title": "Getting useful answers",
          "anchor": "Getting_useful_answers",
          "level": 2
        },
        {
          "title": "Your better questions",
          "anchor": "Your_better_questions",
          "level": 2
        },
        {
          "title": "Questions and criteria",
          "anchor": "Questions_and_criteria",
          "level": 2
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        },
        {
          "anchor": "comments",
          "level": 0,
          "title": "154 comments"
        }
      ],
      "headingsCount": 7
    },
    "contents": {
      "markdown": "**Edit**: contest closed now, will start assessing the entries.\n\n# The contest\n\nI'm offering $1,000 for good questions to ask of AI Oracles. Good questions are those that are safe and useful: that allows us to get information out of the Oracle without increasing risk.\n\nTo enter, put your suggestion in the comments below. The contest ends at the end[^time] of the **31st of August, 2019**.\n\n[^time]: A note on timezones: as long as it's still the 31 of August, anywhere in the world, your submission will be counted.\n\n## Oracles\n\nA perennial suggestion for a safe AI design is the Oracle AI: an AI confined to a sandbox of some sort, that interacts with the world only by answering questions.\n\nThis is, of course, not safe in general; an Oracle AI can [influence the world through the contents of its answers](https://www.lesswrong.com/posts/nAwTGhgrdxE85Bjmg/tools-versus-agents), allowing it to [potentially escape the sandbox](http://yudkowsky.net/singularity/aibox/).\n\nTwo of the safest designs seem to be the counterfactual Oracle, and the low bandwidth Oracle. These are detailed [here](https://arxiv.org/pdf/1711.05541.pdf), [here](https://www.lesswrong.com/posts/i2dNFgbjnqZBfeitT/oracles-sequence-predictors-and-self-confirming-predictions), and [here](https://www.lesswrong.com/posts/wJ3AqNPM7W4nfY5Bk/self-confirming-prophecies-and-simplified-oracle-designs), but in short:\n\n* A counterfactual Oracle is one whose objective function (or reward, or loss function) is only non-trivial in worlds where its answer is not seen by humans. Hence it has no motivation to manipulate humans through its answer.\n* A low bandwidth Oracle is one that must select its answers off a relatively small list. Though this answer is a [self-confirming prediction](https://www.lesswrong.com/posts/KoEY9CjrKe93ErYhd/self-confirming-predictions-can-be-arbitrarily-bad), the negative effects and potential for manipulation is restricted because there are only a few possible answers available.\n\nNote that both of these Oracles are designed to be episodic (they are run for single episodes, get their rewards by the end of that episode, aren't asked further questions before the episode ends, and are only motivated to best perform on that one episode), to avoid incentives to longer term manipulation.\n\n\n## Getting useful answers\n\nThe counterfactual and low bandwidth Oracles are safer than unrestricted Oracles, but this safety comes at a price. The price is that we can no longer \"ask\" the Oracle any question we feel like, and we certainly can't have long discussions to clarify terms and so on. For the counterfactual Oracle, the answer might not even mean anything real to us - it's about another world, that we don't inhabit.\n\nDespite this, its possible to get a surprising amount of good work out of these designs. To give one example, suppose we want to fund various one of a million projects on AI safety, but are unsure which one would perform better. We can't directly ask either Oracle, but there are indirect ways of getting advice:\n\n* We could ask the low bandwidth Oracle which team A we should fund; we then choose a team B at random, and reward the Oracle if, at the end of a year, we judge A to have performed better than B.\n* The counterfactual Oracle can answer a similar question, indirectly. We commit that, if we don't see its answer, we will select team A and team B at random and fund them for year, and compare their performance at the end of the year. We then ask for which team A[^conditional] it expects to most consistently outperform any team B.\n\n[^conditional]: These kind of conditional questions can be answered by a counterfactual Oracle, see the [paper here](https://arxiv.org/pdf/1711.05541.pdf) for more details.\n\nBoth these answers get around some of the restrictions by deferring to the judgement of our future or counterfactual selves, averaged across many randomised universes.\n\nBut can we do better? Can we do more?\n\n## Your better questions\n\nThis is the purpose of this contest: for you to propose ways of using either Oracle design to get the most safe-but-useful work.\n\nSo I'm offering $1,000 for interesting new questions we can ask of these Oracles. Of this:\n\n* $350 for the best question to ask a counterfactual Oracle.\n* $350 for the best question to ask a low bandwidth Oracle.\n* $300 to be distributed as I see fit among the non-winning entries; I'll be mainly looking for innovative and interesting ideas that don't quite work.\n\nExceptional rewards go to those who open up a whole new category of useful questions.\n\n## Questions and criteria\n\nPut your suggested questions in the comment below. Because of the [illusion of transparency](https://en.wikipedia.org/wiki/Illusion_of_transparency), it is better to explain more rather than less (within reason).\n\nComments that are submissions must be on their separate comment threads, start with \"Submission\", and **you must specify which Oracle design you are submitting for**. You may submit as many as you want; I will still delete them if I judge them to be spam. Anyone can comment on any submission. I may choose to ask for clarifications on your design; you may also choose to edit the submission to add clarifications (label these as edits).\n\nIt may be useful for you to include details of the physical setup, what the Oracle is trying to maximise/minimise/predict and what the counterfactual behaviour of the Oracle users humans are assumed to be (in the counterfactual Oracle setup). Explanations as to how your design is safe or useful could be helpful, unless it's obvious. Some short examples [can be found here](https://www.lesswrong.com/posts/cSzaxcmeYW6z7cgtc/contest-usd1-000-for-good-questions-to-ask-to-an-oracle-ai#gyyXHioXarf7c6LXx).\n\n**EDIT** after seeing some of the answers: decide on the length of each episode, and how the outcome is calculated. The Oracle is run once an episode only (and other Oracles can't generally be used on the same problem; if you want to run multiple Oracles, you have to justify why this would work), and has to get objective/loss/reward by the end of that episode, which therefore has to be estimated in some way at that point.\n"
    },
    "voteCount": 32
  },
  {
    "_id": "x72ta8C3dKu2QRfPv",
    "url": null,
    "title": "Request for Comments on Online LessWrong/SSC Meetup--Rump Session",
    "slug": "request-for-comments-on-online-lesswrong-ssc-meetup-rump",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "Events (Community)"
      }
    ],
    "tableOfContents": {
      "sections": [
        {
          "title": "EDIT: We're doing it!",
          "anchor": "EDIT__We_re_doing_it_",
          "level": 1
        },
        {
          "title": "See here for meetup announcement. ",
          "anchor": "See_here_for_meetup_announcement__",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        },
        {
          "anchor": "comments",
          "level": 0,
          "title": "6 comments"
        }
      ],
      "headingsCount": 4
    },
    "contents": {
      "markdown": "**EDIT: We're doing it!**\n\n**[See here for meetup announcement](https://www.lesswrong.com/posts/krn3Et4RYHwpqT9Ym/online-fun-lw-ssc-meetup-march-24).**\n\n  \n\n\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\n\nAnyone interested in an online LessWrong/SlateStarCodex meetup?\n\nWe'd start with personal introductions for some sociability, and then do a Rump Session, where anyone can step up and give a four minute talk; then, if you want to keep talking, the audience votes on whether to go another three minutes. Talks can be spontaneous rather than prepared, and on any subject that you'd like: Science topics, poetry readings, or anything else.\n\nAt LessWrong Israel, we've found this to be a lot of fun--no risk of being bored or boring, as talks are so short.\n\nAs it would be online, people from anywhere could join as timezones allow.\n\nIf you think you'd like such a thing, please upvote and/or comment."
    },
    "voteCount": 9
  },
  {
    "_id": "96N8BT9tJvybLbn5z",
    "url": null,
    "title": "We run the Center for Applied Rationality, AMA",
    "slug": "we-run-the-center-for-applied-rationality-ama",
    "author": "AnnaSalamon",
    "question": false,
    "tags": [
      {
        "name": "Center for Applied Rationality (CFAR)"
      },
      {
        "name": "AMA"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "CFAR recently launched its 2019 [fundraiser](https://rationality.org/fundraiser), and to coincide with that, we wanted to give folks a chance to ask us about our mission, plans, and strategy. Ask any questions you like; we’ll respond to as many as we can from 10am PST on 12/20 until 10am PST the following day (12/21).\n\nTopics that may be interesting include (but are not limited to):\n\n*   Why we think there should be a CFAR;\n*   Whether we should change our name to be less general;\n*   How running mainline CFAR workshops does/doesn't relate to running \"AI Risk for Computer Scientist\" type workshops. Why we both do a lot of recruiting/education for AI alignment research and wouldn't be happy doing only that.\n*   How our curriculum has evolved. How it relates to and differs from the Less Wrong Sequences. Where we hope to go with our curriculum over the next year, and why.\n\nSeveral CFAR staff members will be answering questions, including: me, Tim Telleen-Lawton, Adam Scholl, and probably various others who work at CFAR. However, we will try to answer with our own individual views (because individual speech is often more interesting than institutional speech, and certainly easier to do in a non-bureaucratic way on the fly), and we may give more than one answer to questions where our individual viewpoints differ from one another's!\n\n(You might also want to check out our [2019 Progress Report and Future Plans](https://www.lesswrong.com/posts/vj6CYLuDPw3ieCB4A/cfar-progress-report-and-future-plans-1). And we'll have some other posts out across the remainder of the fundraiser, from now til Jan 10.)\n\n**\\[Edit: We're out of time, and we've allocated most of the reply-energy we have for now, but some of us are likely to continue slowly dribbling out answers from now til Jan 2 or so (maybe especially to replies, but also to some of the q's that we didn't get to yet). Thanks to everyone who participated; I really appreciate it.\\]**"
    },
    "voteCount": 51
  },
  {
    "_id": "KpnyCT7CZy4Qe6kx6",
    "url": null,
    "title": "At what point should CFAR stop holding workshops due to COVID-19? ",
    "slug": "at-what-point-should-cfar-stop-holding-workshops-due-to",
    "author": null,
    "question": true,
    "tags": [
      {
        "name": "Covid-19"
      },
      {
        "name": "Center for Applied Rationality (CFAR)"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "What information about the virus' nature and spread would cause you to believe it's too risky to continue holding workshops? "
    },
    "voteCount": 9
  },
  {
    "_id": "KgFrtaajjfSnBSZoH",
    "url": null,
    "title": "AI Safety Research Camp - Project Proposal",
    "slug": "ai-safety-research-camp-project-proposal",
    "author": "David_Kristoffersson",
    "question": false,
    "tags": [
      {
        "name": "AI Risk"
      },
      {
        "name": "Community"
      },
      {
        "name": "Future of Humanity Institute (FHI)"
      },
      {
        "name": "AI Safety Camp"
      }
    ],
    "tableOfContents": {
      "sections": [
        {
          "title": "AI Safety Research Camp - Project Proposal",
          "anchor": "AI_Safety_Research_Camp___Project_Proposal",
          "level": 1
        },
        {
          "title": "Summary",
          "anchor": "Summary",
          "level": 2
        },
        {
          "title": "Plan",
          "anchor": "Plan",
          "level": 2
        },
        {
          "title": "Important Considerations",
          "anchor": "Important_Considerations",
          "level": 2
        },
        {
          "title": "Long-term and wider impacts",
          "anchor": "Long_term_and_wider_impacts",
          "level": 2
        },
        {
          "title": "Acknowledgements",
          "anchor": "Acknowledgements",
          "level": 2
        },
        {
          "title": "Organisers",
          "anchor": "Organisers",
          "level": 2
        },
        {
          "title": "Tom McGrath",
          "anchor": "Tom_McGrath",
          "level": 3
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        },
        {
          "anchor": "comments",
          "level": 0,
          "title": "11 comments"
        }
      ],
      "headingsCount": 10
    },
    "contents": {
      "markdown": "AI Safety Research Camp - Project Proposal\n------------------------------------------\n\n_→ Give your feedback on our plans below or in the [google doc](https://docs.google.com/document/d/1QlKruAZuuc5ay0ieuzW5j5Q100qNuGNCTHgC4bIEXsg/edit?ts=5a651a00#)_  \n_→ [Apply](https://docs.google.com/forms/d/e/1FAIpQLScL9QaM5vLQSpOWxxb-Y8DUP-IK4c8DZlYSkL6pywz8OSQY1g/viewform) to take part in the Gran Canaria camp on 12-22 April (deadline: 12 February)_  \n_→ [Join](https://www.facebook.com/groups/348759885529601/) the Facebook group_\n\n### Summary\n\n**Aim:** Efficiently launch aspiring AI safety and strategy researchers into concrete productivity by creating an ‘on-ramp’ for future researchers.\n\nSpecifically:\n\n1.  Get people started on and immersed into concrete research work intended to lead to papers for publication.\n2.  Address the bottleneck in AI safety/strategy of few experts being available to train or organize aspiring researchers by efficiently using expert time.\n3.  Create a clear path from ‘interested/concerned’ to ‘active researcher’.\n4.  Test a new method for bootstrapping talent-constrained research fields.\n\n**Method:** Run an online research group culminating in a two week intensive in-person research camp. Participants will work in groups on tightly-defined research projects on the following topics:\n\n*   Agent foundations\n*   Machine learning safety\n*   Policy & strategy\n*   Human values\n\nProjects will be proposed by participants prior to the start of the program. Expert advisors from AI Safety/Strategy organisations will help refine them into proposals that are tractable, suitable for this research environment, and answer currently unsolved research questions. This allows for time-efficient use of advisors’ domain knowledge and research experience, and ensures that research is well-aligned with current priorities.\n\nParticipants will then split into groups to work on these research questions in online collaborative groups over a period of several months. This period will culminate in a two week in-person research camp aimed at turning this exploratory research into first drafts of publishable research papers. This will also allow for cross-disciplinary conversations and community building, although the goal is primarily research output. Following the two week camp, advisors will give feedback on manuscripts, guiding first drafts towards completion and advising on next steps for researchers.\n\n**Example:** Multiple participants submit a research proposal or otherwise express an interest in interruptibility during the application process, and in working on machine learning-based approaches. During the initial idea generation phase, these researchers read one another’s research proposals and decide to collaborate based on their shared interests. They decide to code up and test a variety of novel approaches on the relevant AI safety gridworld. These approaches get formalised in a research plan.\n\nThis plan is circulated among advisors, who identify the most promising elements to prioritise and point out flaws that render some proposed approaches unworkable. Participants feel encouraged by expert advice and support, and research begins on the improved research proposal.\n\nResearchers begin formalising and coding up these approaches, sharing their work in a Github repository that they can use as evidence of their engineering ability. It becomes clear that a new gridworld is needed to investigate issues arising from research so far. After a brief conversation, their advisor is able to put them in touch with the relevant engineer at Deepmind, who gives them some useful tips on creating this.\n\nAt the research camp the participants are able to discuss their findings and put them in context, as well as solve some technical issues that were impossible to resolve part-time and remotely. They write up their findings into a draft paper and present it at the end of the camp. The paper is read and commented on by advisors, who give suggestions on how to improve the paper’s clarity. The paper is submitted to NIPS 2018’s Aligned AI workshop and is accepted.\n\n**Expected outcome:** Each research group will aim to produce results that can form the kernel of a paper at the end of the July camp. We don’t expect every group to achieve this, as research progress is hard to predict.\n\n1.  At the end of the camp, from five groups, we would expect three to have initial results and a first draft of a paper that the expert advisors find promising.\n2.  Within six months following the camp, three or more draft papers have been written that are considered to be promising by the research community.\n3.  Within one year following the camp, three or more researchers who participated in the project obtain funding or research roles in AI safety or strategy.\n\n**Next steps following the camp:** When teams have produced promising results, camp organizers and expert advisors will endeavour to connect the teams to the right parties to help the research shape up further and be taken to conclusion.\n\nPossible destinations for participants who wish to remain in research after the camp would likely be some combination of:\n\n1.  Full-time internships in areas of interest, for instance [Deepmind](https://deepmind.com/careers/476630/), [FHI](https://www.fhi.ox.ac.uk/vacancies/) or [CHAI](http://humancompatible.ai/jobs#internship)\n2.  Full-time research roles at AI safety/strategy organisations\n3.  Obtaining research funding such as [OpenPhil](https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/open-philanthropy-project-ai-fellows-program) or [FLI](https://futureoflife.org/2017/12/20/2018-international-ai-safety-grants-competition/) research grants - successful publications may unlock new sources of funding\n4.  Independent remote research\n5.  Research engineering roles at technical AI safety organisations\n\nResearch projects can be tailored towards participants’ goals - for instance researchers who are interested in engineering or machine learning-related approaches to safety can structure a project to include a significant coding element, leading to (for instance) a GitHub repo that can be used as evidence of engineering skill. This is also a relatively easy way for people who are unsure if research work is for them to try it out without the large time investment and opportunity cost of a PhD or masters program, although we do not see it as a full replacement for these.\n\n### Plan\n\n**[Timeline](https://docs.google.com/document/d/1l02CtyQo-sUiXv4RlwRB2DHOXyS6pC7_lGcx0vxOOtY/edit):** We anticipate this project having 4 main phases (dates are currently open for discussion):\n\n1.  Plan and develop the project, recruit researchers and look for advisors - December 2017 to April 2018\n2.  Testing and refinement of event design during a small-scale camp at Gran Canaria - April 12-22\n3.  Project selection, refinement and exploration (online) - April 2018 to July 2018\n4.  Research camp (in person) - July/August 2018\n\n**Recruiting:** We plan to have approximately 20 researchers working in teams of 3-5 people, with projects in agent foundations, machine learning, strategy/policy and human values/cognition. Based on responses to a registration form we have already posted online (link [here](https://drive.google.com/open?id=1xj7ffjihLyIqtPPEMozwnHA3HsKhzFD7ZxblVkA1O5g)) we expect to be able to easily meet this number of participants.\n\nEach team will be advised by a more experienced researcher in the relevant area, however we expect this won’t be as tightly-coupled a relationship as that between PhD students and their supervisors - the aim is to maximise the usefulness of the relatively scarce advisor time and to develop as much independence in researchers as possible.\n\n**Project selection and exploration:** Once the initial recruitment phase is complete, researchers and advisors can choose a project to work on and refine it into a single question answerable within the timeframe. We recognise the need for strong project planning skills and careful project choice and refinement here, and this project choice is a potential point of failure (see Important Considerations below). Following project selection, researchers will begin exploring the research project they’ve chosen in the months between project choice and the research camp. This would probably require five to ten hours a week of commitment from researchers, mostly asynchronously but with a weekly ‘scrum’ meeting to share progress within a project team. Regular sharing of progress and forward planning will be important to keep momentum going.\n\n**Research camp:** Following the selection and exploration, we will have a two-week intensive camp assembling all participants in-person at a retreat to do focused work on the research projects. Exploratory work can be done asynchronously, but finishing research projects can be hard work and require intensive communication which can more easily be done in person. This also makes the full-time element of this project much more bounded and manageable for most potential participants. An in-person meeting also allows for much better communication between researchers on different projects, as well as helping form lasting and fruitful connections between researchers.\n\n### Important Considerations\n\n**Shaping the research question:** Selecting good research questions for this project will be challenging, and is one of the main potential points of failure. The non-traditional structure of the event brings with it some extra considerations. We expect that most projects will be:\n\n1.  Tractable to allow progress to be made in a short period of time, rather than conceptually complex or open-ended\n2.  Closely related to current work, e.g. suggestions found in ‘further work’ or ‘open questions’ sections from recent papers\n3.  Parallelisable across multiple researchers, e.g. evaluating multiple possible solutions to a single problem or researching separate aspects of a policy proposal\n\nThis biases project selection towards incremental research, i.e. extending previous work rather than finding completely new approaches. This is hard to avoid in these circumstances, and we are optimising at least partly for the creation of new researchers who can go on to do more risky, less incremental research in the future. Furthermore, a look at the ‘future work/open questions’ sections of many published safety papers will reveal a broad selection of interesting, useful questions that still meet the criteria above so although this is a tradeoff, we do not expect it to be overly limiting. A good example of this in the Machine Learning subfield would be evaluating multiple approaches to one of the problems listed in DeepMind’s recent [AI Safety gridworlds paper](https://arxiv.org/abs/1711.09883).\n\n**Finding advisors:** Although we intend this to be relatively self-contained, some amount of advice from active researchers will be beneficial at both the project selection and research stages, as well as at the end of the camp. The most useful periods for advisor involvement will be at the initial project selection/shaping phase and at the end of the camp - the former allows for better, more tractable projects as well as conveying previously unpublished relevant information and a sense of what’s considered interesting. The latter will be useful for preparing papers and integrating new researchers into the existing community. Informal enquiries suggest that it is likely to be possible to recruit advisors for these stages, but ongoing commitments will be more challenging.\n\nThe expected commitment during project selection and shaping would be one or two sessions of several hours spent evaluating and commenting on proposed research projects. This could be done asynchronously or by video chat. Commitment at the end of the research camp is likely to be similar - responding to initial drafts of papers with suggestions of improvements or further research in a similar way to the peer review process.\n\n**Costs:** The main costs for the Gran Canaria camp, the AirBnBs, meals and low-income travel reimbursements, have been covered now by two funders. The July camp will likely take place in the UK at the [EA Hotel](https://www.facebook.com/groups/1624791014242988/?ref=br_rs), a co-working hub planned by Greg Colbourn (for other options, see [here](https://docs.google.com/spreadsheets/d/1cX4yrEH4Kw8-CdT9zRhV7EHD1xXLO6T9NqiK5FQ5pKk/edit#gid=0)). For this, we will publish a funding proposal around April. Please see [here](https://docs.google.com/spreadsheets/d/1P5W8u8czOp_MEaZtb2iwPmmFs1CFkkH6RuI0NHrfP_I/edit#gid=781421298) for the draft budgets.\n\n### Long-term and wider impacts\n\nIf the camp proves to be successful, it could serve as the foundation for yearly recurring camps to keep boosting aspiring researchers into productivity. It could become a much-needed additional lever to grow the fields of AI safety and AI strategy for many years to come. The research camp model could also be used to grow AI safety research communities where none presently exist, but there is a strong need - in China, for instance. By using experienced coordinators and advisors in conjunction with local volunteers, it may be possible to organise a research camp without the need for pre-existing experts in the community. A camp provides a coordination point for interested participants, signals support for community building, and if previous camps have been successful provides social proof for participants.\n\nIn addition, scaling up research into relatively new cause areas is a problem that will need to be solved many times in the effective altruist community. This could represent an efficient way to ‘bootstrap’ a larger research community from a small pre-existing one, and so could be a useful addition to the tool set available to the EA community.\n\nThis project serves as a natural complement to other AI safety projects currently in development such as [RAISE](https://wiki.lesswrong.com/wiki/Road_to_AI_Safety_Excellence) that aim to teach researchers the foundational knowledge they will need to begin research. Once an aspiring AI safety researcher completes one of these courses, they might consider a research camp as a natural next step on the road to become a practicing researcher.\n\n### Acknowledgements\n\nThanks to Ryan Carey, Chris Cundy, Victoria Krakovna and Matthijs Maas for reading and providing helpful comments on this document.\n\n### Organisers\n\n#### [Tom McGrath](https://tommcgrath.github.io/)\n\nTom is a maths PhD student in the [Systems and Signals](http://wwwf.imperial.ac.uk/~nsjones/) group at Imperial College, where he works on statistical models of animal behaviour and physical models of inference. He will be interning at the [Future of Humanity Institute](https://www.fhi.ox.ac.uk/) from Jan 2018, working with [Owain Evans](https://www.fhi.ox.ac.uk/team/owain-evans/). His previous organisational experience includes co-running Imperial’s [Maths Helpdesk](http://mathshelpdesk.ma.ic.ac.uk/) and running a postgraduate deep learning study group.\n\n[Remmelt Ellen](https://nl.linkedin.com/in/remmelt-ellen-19b88045)\n\n_Operations_\n\nRemmelt is the Operations Manager of [Effective Altruism Netherlands](https://effectiefaltruisme.nl/en/effective-altruism-netherlands/), where he coordinates national events, works with organisers of new meetups and takes care of mundane admin work. He also oversees planning for the team at [RAISE](https://wiki.lesswrong.com/wiki/Accelerating_AI_Safety_Adoption_in_Academia), an online AI Safety course. He is a Bachelor intern at the [Intelligent & Autonomous Systems](https://www.cwi.nl/research/groups/intelligent-and-autonomous-systems) research group.\n\nIn his spare time, he’s exploring how to improve the interactions within multi-layered networks of agents to reach shared goals – especially approaches to collaboration within the EA community and the representation of persons and interest groups by [negotiation agents](https://homepages.cwi.nl/~baarslag/pub/When_Will_Negotiation_Agents_Be_Able_to_Represent_Us-The_Challenges_and_Opportunities_for_Autonomous_Negotiators.pdf) in [sub-exponential](http://www.oilcrash.com/articles/complex.htm) takeoff scenarios.\n\n[Linda Linsefors](https://docs.google.com/document/d/1NkYDp3zns-cyasAk_WDrhj6DlJ9QjMP24fM7jTvWPqM/edit?usp=sharing)\n\nLinda has a PhD in theoretical physics, which she obtained at [Université Grenoble Alpes](https://doctorat.univ-grenoble-alpes.fr/en/doctoral-studies/research-fields/physics-630344.htm) for work on loop quantum gravity. Since then she has studied AI and AI Safety online for about a year. Linda is currently working at [Integrated Science Lab](http://www.org.umu.se/icelab/english/?languageId=3) in Umeå, Sweden, developing tools for analysing information flow in networks. She hopes to be able to work full time on AI Safety in the near future.\n\n[Nandi Schoots](https://www.linkedin.com/in/nandi-schoots-70bba8125/?locale=en_US)\n\nNandi has a research master in pure mathematics and a minor in psychology from Leiden University. Her master was focused on algebraic geometry and her [thesis](https://www.universiteitleiden.nl/binaries/content/assets/science/mi/scripties/masterschoots.pdf) was in category theory. Since graduating she has been steering her career in the direction of AI safety. She is currently employed as a data scientist in the Netherlands. In parallel to her work she is part of a study group on AI safety and involved with the reinforcement learning section of [RAISE](https://wiki.lesswrong.com/wiki/Accelerating_AI_Safety_Adoption_in_Academia).\n\n[David Kristoffersson](https://www.linkedin.com/in/davidkristoffersson/)\n\nDavid has a background as R&D Project Manager at Ericsson where he led a project of 30 experienced software engineers developing many-core software development tools. He liaised with five internal stakeholder organisations, worked out strategy, made high-level technical decisions and coordinated a disparate set of subprojects spread over seven cities on two different continents. He has a further background as a Software Engineer and has a BS in Computer Engineering. In the past year, he has contracted for the [Future of Humanity Institute](https://www.fhi.ox.ac.uk/), has explored research projects in ML and AI strategy with FHI researchers, and is currently collaborating on existential risk strategy research with [Convergence](http://convergenceanalysis.org/).\n\nChris Pasek\n\nAfter graduating from mathematics and theoretical computer science, Chris ended up touring the world in search of meaning and self-improvement, and finally settled on working as a freelance researcher focused on AI alignment. Currently also running a rationalist shared housing project on the tropical island of Gran Canaria and continuing to look for ways to gradually self-modify in the direction of a superhuman FDT-consequentialist entity with a goal to save the world."
    },
    "voteCount": 20
  },
  {
    "_id": "zAqoj79A7QuhJKKvi",
    "url": null,
    "title": "The Berkeley Community & The Rest Of Us: A Response to Zvi & Benquo",
    "slug": "the-berkeley-community-and-the-rest-of-us-a-response-to-zvi",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "World Optimization"
      },
      {
        "name": "Community"
      },
      {
        "name": "The SF Bay Area"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "  \n\n_Background Context, And How to Read This Post_  \n\n_This post is inspired by and a continuation of comments I made on the post '[What is the Rationalist Berkeley Community's Culture?](https://thezvi.wordpress.com/2017/08/12/what-is-rationalist-berkleys-community-culture/)' by Zvi on his blog [Don't Worry About the Vase](https://thezvi.wordpress.com/2017/08/12/what-is-rationalist-berkleys-community-culture/). As a community organizer both online and in-person in Vancouver, Canada, my goal was to fill in what appeared to be some gaps in the conversation among rationalists mostly focused on the Berkeley community. Zvi's post was part of a broader conversation pertaining to rationalist community dynamics **within** Berkeley. _\n\n_My commentary pertains to the dynamics **between** the Bay Area and other local rationality communities, informed by my own experience in Vancouver and those of rationalists elsewhere. The below should not be taken be taken as comment on rationalist community dynamics **within** the Bay Area. This post should be considered an off-shoot from the original conversation Zvi was contributing to. For full context, please read Zvi's [original post](https://thezvi.wordpress.com/2017/08/12/what-is-rationalist-berkleys-community-culture/). _\n\n* * *\n\nI. The Rationality Community: Berkeley vs. The World  \n\nWhile I didn't respond to them at the time, several community members commented on Zvi's post they had similar experiences: that while some local rationality communities and their members perceive themselves in a zero-sum game with Berkeley they didn't sign up for (and, to be fair, the Berkeley community didn't consciously initiate as though it's a single agency), and some don't, a sense of what Zvi was trying to point appears ubiquitous. An example:\n\n> In my experience, the recruitment to Berkeley was very aggressive. Sometimes it felt like: “if you don’t want to move to Berkeley as soon as possible, you are not \\*really\\* rational, and then it is a waste of our time to even talk to you.” I totally understand why having more rationalists around you is awesome, but trying to move everyone into one city feels like an overkill.\n\nSimilar anecdata from local rationality communities around the world:\n\n_Melbourne._ When I met several rationalists originally from Melbourne in Berkeley a few years ago, the way they talked about the exodus of the core of the Melbourne rationality community to the Bay Area, it was a mixed assessment. Melbourne is an example of very successful local rationality community outside the Bay Area, with the usual milestones like successful EA non-profits, for-profit start-ups and rationalist sharehouses. So that many rationalists from Melbourne left for the Bay Area passed a cost-benefit analysis as high-impact individuals it was obvious to them they should be reducing existential risks on the other side of the world.\n\nIn conversation, Helen Toner expressed some unease that a local rationality community which had successfully become a rationality hub second only to the Bay Area had had a whole generation of rationalists from Melbourne leave at once. This could have left open the possibility a sustainable system for rationalist development for years had been gutted. My impression since then is around this time the independent organization of the Melbourne EA community began to pick up, and between that and the remaining rationalists, the Melbourne community is doing well. If past or present members of the Melbourne rationality community would like to add their two cents, it would be greatly appreciated.   \n  \nThe rationality community growth strategy out of Berkeley by default became to recruit the best rationalists from local communities around the world at a rate faster than rationalist organizers could replenish the strength of those local communities. Given the stories I've heard from outside Melbourne being more lopsided, with the organization of local rationality communities utterly collapsing, only recovering after multiple years if ever, I'd consider the case of Melbourne rationality community surviving the exit of its leadership for Berkeley to have been a lucky outlier.\n\n  \n_Seattle._ The Seattle rationality community has experienced a bad case of exodus to Berkeley over the last few years. My understanding of this story is as follow:\n\n*   Like with rationalists around the world, effective altruism came along and said \"hey, while our communities have significant differences, we care about existential risk reduction and other common goals; we've got several billion dollars; and worldwide network of thousands rising through every kind of institution to coordinate the globe\". At the time, the whole strategy for AI alignment wasn't much more than \"read the Sequences and then donate to MIRI...?\", so at the time EA's value proposition couldn't be beat. In Seattle the organizers of the rationality community took off their rationalist hats and switched it for an effective altruist one, albeit while prominently placing a rationalist button on it. This is what started happening in Vancouver as well circa 2013. The Seattle rationalists started a successful Rationality Reading Group in 2015 which got through the whole LessWrong Sequences.  \n    \n*   Things went swimmingly in Seattle until [AI safety 'went mainstream'](https://futureoflife.org/2015/12/31/2015-a-year-in-review/), and as the financial resources flowed into the institutions of the Berkeley rationality community, the demand and pressure to acquire the resources that were distant rationalists and their skill-sets intensified. In a period of several months but less than two years, the Seattle rationality community lost at least a half-dozen members, including some local organizers and other veteran community members. The Rationality Reading Groups ceased as regular meetups for over a year, and local community organization was at best intermittent.   \n    \n*   The excitement of EA brought many more Seattleites into the world of x-risk reduction, and the EA and rationality communities of Seattle effectively merged to survive. Since then, they're thriving again, but Seattle is still gradually exuding community members to Berkeley. Because of its proximity to the Bay Area, and the excellence of the Seattle rationality community, I expect it might have experienced more _absolute_ loss from leaking members to Berkeley more than any other. Due to its size, the Seattle community has sustained itself, so the _relative_ loss of local rationality communities which totally collapsed may be greater than has been the case in Seattle. As with Melbourne, if any community members who have lived or are living in Seattle wish to provide feedback, that is encouraged. \n\n_Vancouver._ The experience in Vancouver has in the past certainly felt like \"“if you don’t want to move to Berkeley as soon as possible, you are not \\*really\\* rational\". The biggest reason Vancouver may not have exuded as many rationalists to the Bay Area as cities in the United States is the difficulty being Canadian poses to gaining permanent residence in the United States and hence moving to the Bay Area. A couple friends of mine who were early attendees of a CFAR workshop lived in the Bay Area for several months in 2013, and returned home with stories of how wondrous the Bay Area was. They convinced several of us to attend CFAR workshops as well, and we too returned home with the sense of wonderment after our brief immersion in the Berkeley rationality community. But when my friends and I each returned, somehow our ambition transformed into depression. I tried rallying my friends to try carrying back or reigniting the spark that made the Berkeley rationalist community thrive, to really spread the rationalist project beyond the Bay Area. \n\nBut the apparent consensus was it just wasn't possible. Maybe the rationality community a few years ago lacked the language to talk about it, but rationalists who'd lived in Berkeley for a time only to return felt the rationality-shaped hole in their heart could only be filled in the Berkeley. A malaise had fallen over the Vancouver rationality community. All of us were still around, but with a couple local EA organizations, many of us were drawn to that crowd. Those of us who weren't were alienated from any personal connection to the rationality community. I saw in my friends a bunch of individual heroes who together were strangely less than and  not greater than the sum of their parts.\n\nThings have been better lately, and a friend remarked they're certainly better than a few years ago, when everyone was depressed about the fact it was too difficult for us to all move to the Bay Area. In the last several months, the local rationality community has taken on as our mission our own development, and we've not rebounded so much as flourished like never before. But it took the sorts of conversations about the Berkeley rationalist community last year Zvi and others had to break the spell we had cast on ourselves, that apparently Berkeley had running a rationalist community like a well-oiled machine down to an art and a science.\n\n* * *\n\nII. The Berkeley Community and the Mission of Rationality\n\n_Benquo [commented](https://thezvi.wordpress.com/2017/08/12/what-is-rationalist-berkleys-community-culture/#comment-479) on Zvi's post:_\n\n> This is a good description of why I feel like I need to leave Berkeley whether or not there’s a community somewhere else to participate in. This thing is scary and I don’t want to be part of it.\n\n>   \n> I think this is some evidence that the Rationalist project was never or only very briefly real and almost immediately overrun by [MOPs](https://meaningness.com/geeks-mops-sociopaths), and largely functions as a way for people to find mates. Maybe that’s OK in a lot of cases, but when your branding is centered around “no really, we are actually trying to do the thing, literally all we are about is not lying to ourselves and instead openly talking about the thing we’re trying to do, if you take things literally saving the world really literally is the most important thing and so of course you do it,” it’s pretty disappointing to find it’s just another flavor.\n\nSince he wrote this comment, Benquo has actually continued to participate in the rationality community. This conversation was mired in tension in the rationality community it must have been difficult to think about impersonally, and so a charitable interpretation would be while these problems exist, Benquo and others are generally not as fatalistic about the rationality community as they were the time they wrote the comments. While I and others in thread saw grains of truth in Benquo's statement, precision nonetheless remains a [virtue of rationality](https://www.lesswrong.com/posts/7ZqGiPHTpiDMwqMN2/the-twelve-virtues-of-rationality), and I [felt compelled](https://www.xkcd.com/386/) to clarify. I [commented](https://thezvi.wordpress.com/2017/08/12/what-is-rationalist-berkleys-community-culture/#comment-484):\n\n> I’d say the rationality community started whenever Eliezer forked off LessWrong Overcoming Bias, which was around 2008 or 2009. That’s certainly not when it peaked. Even in a way MIRI never was, CFAR started out a project built by the rationality community. That was happening in 2012 or 2013. Above Sarah is also quoted as saying she thinks the Berkeley rationality community hit the right balance of focusing on being a welcoming community qua community, and aspiring to the whatever the core mission(s) of the aspiring rationalist project are.\n\n>   \n> Unless you’re arguing there was a latency effect where the MOPs overran the community in 2009, but the consequences of such were buried for several years, the period between 2008/09 and 2012/13 doesn’t constitute being “immediately overrun”.\n\n>   \n> I get you’re pessimistic, but I think you’re overshooting. Matching the map to the territory of what went wrong in the Berkeley rationality community is key to undoing it, or making sure similar failures don’t occur in the future.  \n> FWIW, I’m sorry you’ve had to experience so directly what you feel like is a decline in an aspect of your local rationality community. As someone who connects with rationalists primarily online, I can tell you they’re everywhere, and even if there isn’t a meatspace community as developed as the one in Berkeley, there are rationalists who won’t let the Craft disappear everywhere, and they want meatspace communities of their own built up outside of Berkeley as much as anyone.\n\nOther comments in-thread from community members who had been around longer than Benquo or I confirmed my impression from their own personal experiences, so unless Benquo would further dispute these accounts, this thread seems put to rest. However, Zvi then [replied](https://thezvi.wordpress.com/2017/08/12/what-is-rationalist-berkleys-community-culture/#comment-479) to me:\n\n> I think we need to realize the extent to which Berkeley is actively preventing the formation of, and destroying, these other communities. The majority of high-level rationalists who started in the New York community are in the Berkeley community, which caused New York to outright collapse for years before recovering, and they just now once again caused a crisis by taking away a pair of vital community members and almost wiping out the only rationalist group space in the process. From meeting other community leaders in other cities, I hear similar stories A LOT.\n\n>   \n> I do agree that Plan A for most members can and should be Fix It, not walking away, and that pointing out it needs fixing is the requirement for perhaps fixing it.\n\nTo respond to Zvi here, indeed it appears to be an uncannily ubiquitous problem. I've collected a few stories and described them in some detail above. Between that and several comments from independent rationalists on Zvi's original post giving the impression members of their local communities were being sucked to Berkeley as though through a pneumatic tube and leaving a vacuum of community and organization in its wake, it appears these many local stories could be a single global one.  \n\nThe original mission of the rationality community was to raise the sanity waterline to ensure human values get carried to the stars, but we're [still godshatter](https://www.lesswrong.com/posts/cSXZpvqpa9vbGGLtG/thou-art-godshatter), so doing so can and should take different forms than just ensuring superintelligence is aligned with human values. If ever the goal was to seed successful, stable rationalist communities outside Berkeley to coordinate projects beyond the Bay Area, it's been two steps forward, one step back, at best. Even if we assume for the sake of argument it's a good idea for rationalists worldwide to view Berkeley as a nucleus and their own rationalist communities as recruitment centres to drive promising individuals to Berkeley for the mission of AI alignment or whatever, the plan isn't working super well. That's because the apparent rate of local rationalist communities sending their highest-level rationalists Berkeley is occurring at a much faster rate than those rationalist communities can level up more rationalists to replenish their leadership and sustain the local community _at all_.  \n\nThe state of affairs could be worse than it is now. But it creates the possibility that if enough local rationalist communities around the world outside the Bay Area simultaneously collapsed, the Berkeley rationalist community (BRC) could lose sufficient channels for recruitment to sustain itself. Given the tendency of communities like all things toward entropy, communities decay over time. The BRC could not be rubbing any of its members the wrong way and we would probably still observe some naturally occurring attrition. In a scenario where the decay rate of the BRC was greater than its rate of replenishment, which has historically largely depended on rationalists from outside communities, the BRC would start decaying. If we were to assume the BRC acts as a single agency, it's in the BRC's self-interest as the nucleus of the worldwide rationality movement to sustain communities-as-recruitment centres at least to the extent they can sustainably drive their highest-level rationalists to Berkeley over the long-term.\n\nWhile this worst-case scenario could apply to any large-scale rationalist project, with regards to AI alignment, if the locus of control for the field falls out of the hands of the rationality community, [someone](https://www.centreforeffectivealtruism.org/) [else](https://openai.com/) might notice and decide to pick up that slack. This could be a sufficiently bad outcome rationalists everywhere should pay more attention to decreasing the chances of it happening.  \n\nSo whether a rationalist sees the outcome of the primary purpose of rationalist communities acting as a recruitment centres for the Berkeley rationalist community as an excellent plan or an awful failure mode, there's a significant chance it's unsustainable either way. It appears a high-risk strategy that's far from foolproof, and as far as I know virtually nobody is consciously monitoring the situation to prevent further failure.\n\n* * *\n\nIII. Effective Altruism and the Rationalist Community\n\nIn another thread, I responded directly to Zvi. I [commented](https://thezvi.wordpress.com/2017/08/12/what-is-rationalist-berkleys-community-culture/#comment-485):\n\n> While rationalists are internally trying to figure out how there community has changed, and they’re lamenting how it’s not as focused on world-saving, there’s a giant factor nobody has talked about yet. The only community which is more focused on the rationality community’s way of world-saving than the rationality community is effective altruism. To what extent is the rationalist community less world-save-y than it used to be because the rationalists whose primary rationalist role was “world saver” just switched to EA as their primary world-saving identity. I think as things have gotten less focused since LessWrong 1.0 died, and the rationalist diaspora made entryism much easier as standards fell, what you’re saying is all true. You might be overestimating the impact of entryism, though, and underestimating people who exited not because they had no voice, but for sensible reasons. If at any point a rationalist felt they could better save the world within the EA rather than through the rationality community, it’d internally make sense to dedicate one’s time and energy to that community instead.\n\n>   \n> The EA community doesn’t seem able to build bonds as well as the rationality community. However, the EA community seems better at making progress on outward-facing goals. In that case, I for one wouldn’t blame anyone who find more at home as a world-saver in EA than they did in the rationalist community.\n\nZvi [replied](https://thezvi.wordpress.com/2017/08/12/what-is-rationalist-berkleys-community-culture/#comment-489):\n\n> Definitely an elephant in the room and a reasonable suspect! Certainly partially responsible. I haven’t mentioned it yet, but that doesn’t mean I’ve missed that it is in the picture. I wanted to get this much out there now, and avoid trying to cover as many bases as possible all at once.\n\n>   \n> There have been many (Sarah \\[Constantin\\] and Benquo among them) who have been trying to talk for a long time, with many many words, about the problems with EA. I will consider that question beyond scope here, but rest assured I Have Thoughts.\n\nSince then Zvi and others have made good on their intentions to point out said problems with effective altruism. I intend to engage these thoughts at length in the future, but suffice to say for now local rationalist communities outside the Bay Area appear to definitely have experienced being 'eaten' by EA worse than Berkeley.\n\n* * *\n\nI never bothered to tie up the loose ends I saw in the comments on Zvi's post last year, but something recently spurred me to do so. From Benquo's recent post '[Humans need places](https://www.lesswrong.com/posts/PSAn7WGKCGhXet6A8/humans-need-places)':\n\n> I am not arguing that it would merely be a nice thing for Bay Arean EAs and Rationalists to support projects like this; I am arguing that if you have supported recruiting more people into your community, it is morally obligatory to offer a corresponding level of support for taking care of them once you are in community with them. If you can’t afford to help take care of people, you can’t afford to recruit them.\n\n>   \n> If you don’t have enough for yourself, take care of that first. But if you have more than enough to take care of your private needs, and you are thinking of allocating your surplus to some combination of (a) people far away in space or time, and (b) recruiting others to do the same, I implore you, please first assess - even approximately - the correct share of resources devoted to direct impact, recruiting more people into your community, and _taking care of the community’s needs_, and give accordingly.  \n> \\[...\\]  \n> The Berkeley EA / Rationalist community stands between two alternatives:  \n\n> 1.Pull people in, use them up, and burn them out.  \n> 2\\. Building the local infrastructure to support its global ambitions, enabling sustainable commitments that replenish and improve the capacity of the people making them.\n\nIt's important for rationalists in Berkeley to know that from where they're standing, to rationalists around the world, these statements could ring hollow. The perception of the Centre for Effective Altruism slighting the Berkeley REACH is mirrored many times over in rationalists feeling like Berkeley pulled in, used up and burned out whole rationalist communities. The capital of a nation receives resources from everyone across the land. If the capital city recruits more citizens to the nation, is it not morally obligatory for the capital city offer a corresponding level of support for taking care of them once they joined your nation? Is it not the case if the rationality community can not afford to take care of our people, then we can't afford to recruit them?  \n\nThe worldwide rationalist project stands between two alternatives:\n\n1.  Seed new local communities, use them up, and burn them out.\n2.  Building the global infrastructure to support its global ambitions, enabling sustainable commitments that replenish and improve the capacity of the local communities making them.\n\nThis isn't about the Berkeley rationalist community, but rationalist communities everywhere. In reading about the experiences of rationalists in Berkeley and elsewhere, I've learned their internal coordination problems are paralleled in rationalist communities everywhere. The good news in the bad news is if all rationalist communities face common problems, we can all benefit from working towards common solutions. So global coordination may not be as difficult as one might think. I wrote above the Vancouver rationality community has recently taken on as our mission our own development, and we're not recovering from years of failures past so much as flourishing like never before. We haven't solved all the problems a rationalist community might face, but we've been solving a lot. As a local community organizer, I developed tactics for doing so that if they worked in Vancouver, they should work for any rationalist community. And they worked in Vancouver. I think they're some of the pieces of the puzzle of building global infrastructure to match the rationality community's global ambitions. To lay that out will be the subject of my next post."
    },
    "voteCount": 35
  },
  {
    "_id": "2Ee5DPBxowTTXZ6zf",
    "url": null,
    "title": "Rationalists, Post-Rationalists, And Rationalist-Adjacents",
    "slug": "rationalists-post-rationalists-and-rationalist-adjacents",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "Community"
      },
      {
        "name": "Definitions"
      }
    ],
    "tableOfContents": {
      "sections": [
        {
          "title": "A rationalist, in the sense of this particular community, is someone who is trying to build and update a unified probabilistic model of how the entire world works, and trying to use that model to make predictions and decisions.",
          "anchor": "A_rationalist__in_the_sense_of_this_particular_community__is_someone_who_is_trying_to_build_and_update_a_unified_probabilistic_model_of_how_the_entire_world_works__and_trying_to_use_that_model_to_make_predictions_and_decisions_",
          "level": 1
        },
        {
          "title": "A post-rationalist is someone who believes the rationalist project is misguided or impossible, but who likes to use some of the tools and concepts developed by the rationalists.",
          "anchor": "A_post_rationalist_is_someone_who_believes_the_rationalist_project_is_misguided_or_impossible__but_who_likes_to_use_some_of_the_tools_and_concepts_developed_by_the_rationalists_",
          "level": 1
        },
        {
          "title": "A rationalist-adjacent is someone who enjoys spending time with some clusters of rationalists (and/or enjoys discussing some topics with rationalists), but who is not interested in doing the whole rationalist thing themself.",
          "anchor": "A_rationalist_adjacent_is_someone_who_enjoys_spending_time_with_some_clusters_of_rationalists__and_or_enjoys_discussing_some_topics_with_rationalists___but_who_is_not_interested_in_doing_the_whole_rationalist_thing_themself_",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        },
        {
          "anchor": "comments",
          "level": 0,
          "title": "43 comments"
        }
      ],
      "headingsCount": 5
    },
    "contents": {
      "markdown": "_Epistemic status: Hortative. I'm trying to argue for carving reality at a new joint._\n\nI think it's lovely and useful that we have labels, not just for **rationalist**, but for **rationalist-adjacent** and for **post-rationalist**. But these labels are generally made [extensionally](https://www.lesswrong.com/posts/HsznWM9A7NiuGsp28/extensions-and-intensions), by pointing at people who claim those labels, rather than intensionally, by trying to distill what distinguishes those clusters.\n\nI have some intensional definitions that I've been honing for a long time. Here's the biggest one.\n\nA rationalist, in the sense of this particular community, is someone who is trying to build and update a unified probabilistic model of how the entire world works, and trying to use that model to make predictions and decisions.\n===================================================================================================================================================================================================================================\n\nBy \"unified\" I mean [decompartmentalized](https://www.lesswrong.com/posts/N2pENnTPB75sfc9kb/outside-the-laboratory)\\- if there's a domain where the model gives two incompatible predictions, then as soon as that's noticed it has to be rectified in some way.\n\nAnd it's important that it be probabilistic- it's perfectly consistent to resolve a conflict between predictions by saying \"I currently think the answer is X with about 60% probability, and Y with about 25% probability, and with about 15% probability I'm missing the correct option or confused about the nature of the question entirely\".\n\nThe Sequences are aimed at people trying to do exactly this thing, and Eliezer focuses on how to not go horribly wrong in the process (with a special focus on not trusting one's own sense of obviousness).\n\nBeing a rationalist isn't about any specific set of conclusions- it's not about being an effective altruist, or a utilitarian, or even an atheist. It's about whether one is trying to _do that thing_ or not. Even if one is doing a terrible job of it!\n\nTruth-seeking is a prerequisite, but it's not enough. It's possible to be very disciplined about finding and assembling true facts, without thereby changing the way one thinks about the world. As a contrast, here's how the New York Times, whose fact-checking quality is not in dispute, [decides what to report:](https://deadline.com/2016/11/shocked-by-trump-new-york-times-finds-time-for-soul-searching-1201852490/)\n\n> By and large, talented reporters scrambled to match stories with what internally was often called “the narrative.” We were occasionally asked to map a narrative for our various beats a year in advance, square the plan with editors, then generate stories that fit the pre-designated line.\n\nThe difference between wielding a narrative and fitting new facts into it, and learning a model from new facts, is the difference [between rationalization and rationality](https://www.lesswrong.com/posts/SFZoEBpLo9frSJGkc/rationalization).\n\n\"Taking weird ideas seriously\" is also a prerequisite (because some weird ideas are true, and if you bounce off of them you won't get far), but again it's not enough. I shouldn't really need to convince you of that one.\n\nOkay, then, so what's a post-rationalist?\n\nThe people who identify as such generally don't want to pin it down, but here's my attempt at categorizing at least the ones who make sense to me:\n\nA post-rationalist is someone who believes the rationalist project is misguided or impossible, but who likes to use some of the tools and concepts developed by the rationalists.\n=================================================================================================================================================================================\n\nOf course I'm less confident that this properly defines the cluster, outside of groups like Ribbonfarm where it seems to fit quite well. There are people who view the Sequences (or whatever parts have diffused to them) the way they view Derrida: as one more tool to try on an interesting conundrum, see if it works there, but not really treat it as applicable across the board.\n\nAnd there are those who talk about being a fox rather than a hedgehog (and therefore see trying to reconcile one's models across domains as being harmful), and those who talk about how the very attempt is a matter of hubris, that not only can we not know the universe, we cannot even realistically aspire to decent calibration.\n\nAnd then, of course:\n\nA rationalist-adjacent is someone who enjoys spending time with some clusters of rationalists (and/or enjoys discussing some topics with rationalists), but who is not interested in doing the whole rationalist thing themself.\n================================================================================================================================================================================================================================\n\nWhich is not a bad thing at all! It's honestly a good sign of a healthy community that the community appeals even to people for whom the project doesn't appeal, and the rationalist-adjacents may be more psychologically healthy than the rationalists.\n\nThe real issue of contention, as far as I'm concerned, is something I've saved for the end: **that not everyone who self-identifies as a rationalist fits the first definition very well, and that the first definition is in fact a more compact cluster than self-identification.**\n\nAnd that makes this community, and this site, a bit tricky to navigate. There are rationalist-adjacents for whom a double-crux on many topics would fail because they're not interested in zooming in so close on a belief. There are post-rationalists for whom a double-crux would fail because they can just switch frames on the conversation any time they're feeling stuck. And to try to double-crux with someone, only to have it fail in either of those ways, is an infuriating feeling for those of us who thought we could take it for granted in the community.\n\nI don't yet know of an intervention for signaling that a conversation is happening on explicitly rationalist norms- it's hard to do that in a way that others won't feel pressured to insist they'd follow. But I wish there were one."
    },
    "voteCount": 32
  },
  {
    "_id": "auL2gAGTb3MsYhCeN",
    "url": null,
    "title": "Bay Solstice 2019 Retrospective",
    "slug": "bay-solstice-2019-retrospective",
    "author": "mingyuan",
    "question": false,
    "tags": [
      {
        "name": "Secular Solstice"
      },
      {
        "name": "Postmortems & Retrospectives"
      }
    ],
    "tableOfContents": {
      "sections": [
        {
          "title": "Process",
          "anchor": "Process",
          "level": 1
        },
        {
          "title": "Beginnings & research phase",
          "anchor": "Beginnings___research_phase",
          "level": 2
        },
        {
          "title": "Creating the setlist",
          "anchor": "Creating_the_setlist",
          "level": 2
        },
        {
          "title": "The first setlist",
          "anchor": "The_first_setlist",
          "level": 3
        },
        {
          "title": "Run-throughs",
          "anchor": "Run_throughs",
          "level": 3
        },
        {
          "title": "Regular coworking hours",
          "anchor": "Regular_coworking_hours",
          "level": 2
        },
        {
          "title": "Choosing performers",
          "anchor": "Choosing_performers",
          "level": 2
        },
        {
          "title": "Skill level",
          "anchor": "Skill_level",
          "level": 3
        },
        {
          "title": "Orientation towards Solstice as a whole",
          "anchor": "Orientation_towards_Solstice_as_a_whole",
          "level": 3
        },
        {
          "title": "Final preparations",
          "anchor": "Final_preparations",
          "level": 2
        },
        {
          "title": "Considerations for creating a setlist",
          "anchor": "Considerations_for_creating_a_setlist",
          "level": 1
        },
        {
          "title": "Creating a cohesive message",
          "anchor": "Creating_a_cohesive_message",
          "level": 2
        },
        {
          "title": "Creating a smooth arc",
          "anchor": "Creating_a_smooth_arc",
          "level": 2
        },
        {
          "title": "MCing ",
          "anchor": "MCing_",
          "level": 2
        },
        {
          "title": "Post-performance feedback",
          "anchor": "Post_performance_feedback",
          "level": 1
        },
        {
          "title": "Things I would tweak",
          "anchor": "Things_I_would_tweak",
          "level": 2
        },
        {
          "title": "Finale",
          "anchor": "Finale",
          "level": 3
        },
        {
          "title": "Moment of darkness",
          "anchor": "Moment_of_darkness",
          "level": 3
        },
        {
          "title": "First speech",
          "anchor": "First_speech",
          "level": 3
        },
        {
          "title": "Feedback that should not be acted upon",
          "anchor": "Feedback_that_should_not_be_acted_upon",
          "level": 2
        },
        {
          "title": "Triggering content",
          "anchor": "Triggering_content",
          "level": 1
        },
        {
          "title": "Eating disorder trigger",
          "anchor": "Eating_disorder_trigger",
          "level": 2
        },
        {
          "title": "Other triggers",
          "anchor": "Other_triggers",
          "level": 2
        },
        {
          "title": "Individual pieces",
          "anchor": "Individual_pieces",
          "level": 2
        },
        {
          "title": "Raw feedback",
          "anchor": "Raw_feedback",
          "level": 3
        },
        {
          "title": "Effect of delivery",
          "anchor": "Effect_of_delivery",
          "level": 3
        },
        {
          "title": "Addressing the elephant in the room",
          "anchor": "Addressing_the_elephant_in_the_room",
          "level": 3
        },
        {
          "title": "Summary of takeaways",
          "anchor": "Summary_of_takeaways",
          "level": 1
        },
        {
          "title": "Resources",
          "anchor": "Resources",
          "level": 1
        },
        {
          "title": "Setlist spreadsheet",
          "anchor": "Setlist_spreadsheet",
          "level": 2
        },
        {
          "title": "Masterlist of Solstice materials",
          "anchor": "Masterlist_of_Solstice_materials",
          "level": 2
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        },
        {
          "anchor": "comments",
          "level": 0,
          "title": "37 comments"
        }
      ],
      "headingsCount": 33
    },
    "contents": {
      "markdown": "I was the Creative Director for last year’s Winter Solstice in the Bay Area. I worked with Nat Kozak and Chelsea Voss, who were both focused more on logistics. Chelsea was also the official leader who oversaw both me and Nat and had final say on disputes. (However, I was granted dictatorial control over the Solstice arc and had final say in that arena.) I legit have no idea how any one of us would have pulled this off without the others; love to both of them and also massive respect to Cody Wild, who somehow ran the entire thing herself in 2018.\n\nWhile I worked with a bunch of other people on Solstice, all opinions expressed here are my own. \n\nProcess\n=======\n\nBeginnings & research phase\n---------------------------\n\nChelsea, Nat, and I spent just about a full year preparing for Solstice. Our first meeting as a group was on January 20th, and we received official confirmation that we’d be in charge about a month later. In the following month, the three of us had roughly weekly planning meetings via video chat. In the first meeting, we set goals for what we wanted to have done by the first of April, and after that we checked in regularly for a while.\n\nMy goal by the first of April was to have a rough outline of the entire arc and a plan for how to make the tone be coherent. I also wanted to get a better handle on the task, and to that end I had conversations with several previous Solstice organizers and also read as much as I could. This included Ray’s sequence on ritual, most of the writing on the Secular Solstice website (both the [blog](http://secularsolstice.com/blog/) and the [resources](https://secularsolstice.com/resources/) section were useful), discussions on the Rational Ritual Facebook group, and whatever else I could find. \n\nI also spent a full day listening to every single song that had ever been recommended for Solstice (see below) and making notes on them in a spreadsheet. When I ran out of songs in that reference class, I sorted my iTunes library by most listens and started going through to see if any of those songs might fit. This was quite surprisingly fruitful - I actually ended up using three of the songs that were originally put on my list in this way.\n\nAnother thing I did was go around and ask people what they wanted out of Solstice. I got responses that were actually fairly useful, like “I lose interest during long speeches” and “there should be more singalongs” and “I like singalongs but a lot of the songs have really complicated tunes and I can’t handle it.”\n\nI think even with all that research and preparation, I still didn’t have a very good sense of the history of Solstice. I had only been to two big Bay Area Solstices, plus a private Solstice in the woods, plus a short, small, off-the-cuff Solstice that Habryka and I ran at my mom’s house in 2018. I wasn’t around for early Solstices, and I’d never seen what they were like in Boston, Seattle, or New York. \n\nCreating the setlist\n--------------------\n\n**The first setlist**\n\nI met the April 1st deadline for having an outline of the entire arc. I drew up my first setlist in my notebook on a five-hour plane ride. I was taking into consideration more small snippets of advice than I can list here, but I can quote the guiding goal I referred to throughout the entirety of my time working on this Solstice:\n\n> I want my Solstice to be about the brokenness of the world and the ethos of \"somebody has to and no one else will\", but also the fact that even if you exercise your individual agency to do the most you can, we still might fail. \n\nBelow are some sample pages from my notebook, with most names redacted. Note that I didn't ask any of these people before assigning them to songs; this was just idle speculation on a plane ride about who might be able to pull off each song.\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/0f1d9fb4ad2b3c537daee04aaf93097ead1e15460e3a83a0.jpg)\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/dda8bb26adcd597c0fa0179abdb802e2c120c2260b512048.jpg)\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/c012fd1430c809a35b35b17bb95737265e4deb6ae74e5cb7.jpg)\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/9b831b5a103ffaac0de253a3c9194bd8f11e3e31feaebd28.jpg)\n\nSome sample pages from my notebook\n\nAfter that was a process of constant iteration. \n\n**Run-throughs**\n\nWe had our first full run-through of the setlist on June 16th. In the two months leading up to this, Chelsea checked in with me weekly. I had a lot of speeches to write and planned to write one and send it to her each week (which I more or less accomplished). I found this accountability mechanism quite helpful.\n\nThe first run-through was just me, Habryka, and Chelsea. We sat on the floor of my bedroom, played recordings of the songs to sing along with, and took turns reading the speeches. For this run-through, I wrote three new speeches, re-used two existing Solstice speeches, and threw in Pale Blue Dot, an abridged version of the poem [Ring Out, Wild Bells](https://poets.org/poem/memoriam-ring-out-wild-bells), and the Sequences post [On Doing The Impossible](https://www.lesswrong.com/posts/fpecAJLG9czABgCe9/on-doing-the-impossible). After the run-through, which took about two hours (we timed each piece), the three of us debriefed, and I used the extensive notes I took to make changes to the setlist.\n\nI felt burned out after this and had the luxury of taking all of July off from Solstice work. This was a huge benefit of starting so early.\n\nThe next run-through was in September, scheduled around the dates when our featured musician, Taylor (who lives in Vermont) would be in town. This was significantly more involved but still fairly informal. Seven people participated in total (including Ray, who was experimenting with projection), we provided our own instrumentation for most of the songs, and most of the speeches were read by the people who would eventually give them. Afterwards, each person individually gave me feedback, and again, I made significant changes to the setlist in response.\n\nRegular coworking hours\n-----------------------\n\nAround the time of the September run-through, things really picked up. Going into crunch time, Chelsea, Nat, and I set up a regular time for Solstice coworking. We met for a couple hours every Monday night - that served both as our designated time to work on Solstice things (since we all also have day jobs) and an opportunity for in-person communication about high-context or sensitive topics.\n\nChoosing performers\n-------------------\n\n**Skill level**\n\nWe put out a [call for auditions](https://docs.google.com/document/d/193wc6UEsvfm-_tQGKZRkCWU6yrb5XU6CGhA1pt2xpdk/edit) in August, but we didn’t publicize it very well, so we didn’t receive many applications. I required auditions for speakers and singalong vocalists, but not for instrumentalists - in retrospect, this was an obvious mistake. I let almost all of the instrumentalists who applied participate in as many songs as they wanted, but there were issues with everything from not having time to rehearse to not being able to play in the key the vocalist wanted to disagreements on chords and time signatures. \n\nSince we didn’t get enough applications to fill all the slots, I reached out to people who I already knew were competent from seeing them perform in previous Solstices or in other contexts (such as at jam sessions or in the REACH Musical Revue). This ended up working quite well - I think that everyone I chose in this way gave a good performance.\n\nNYC has often hired professional musicians, rather than having community members provide the instrumentation - a possibility I knew about but never seriously considered. While I still wouldn’t pay for professional musicians, I’ve come to understand better why someone might decide to do so, after experiencing the difficulties (mentioned above) of getting amateur musicians to produce a performance-ready piece.\n\nOn the other hand, a lot of the amateurs did do an excellent job! So I think the takeaway here is just that it’s important to have people audition (or be very sure of their competence level in some other way) and make sure they’re able to put in the time commitment to rehearse.\n\n**Orientation towards Solstice as a whole**\n\nSomething I struggled with a fair amount was disagreements about arc cohesion vs showcasing technical skill. Arc cohesion trumped all other concerns for me, with singalongability a close second. However, it’s often the case that when singers are given the chance to perform, they want to do something more interesting than just lead the melody of a singalong, so I was sometimes at loggerheads with performers who wanted to do more complex pieces or include intricate harmonies. There are some pieces for which I regret not being firmer about putting my foot down on this issue, and I think that ultimately it’s probably reasonable to exclude performers on this basis if you can’t come to an agreement.\n\nOn the flip side, I was extremely grateful to the people whose pieces I cut after the dress rehearsal, a week before the performance. I apologized to all of them and gave them the opportunity to contest the decision, but they were all really great about it and said things along the lines of “I care way more about Solstice being better than I do about cashing out on the work I did.” To those people - I really appreciate you, and thank you for being wonderful!\n\nFinal preparations\n------------------\n\nIn late November we did a walk-through of the venue (important for testing planetarium footage and lighting options), then in early December we had a tech rehearsal at the planetarium (mostly for A/V) and a dress rehearsal (not at the planetarium). Much of the benefit here was logistical, and I won’t touch on that too much since it’s not my wheelhouse, but it also gave me a chance to see what the final product would be like. I was originally supposed to stop editing the setlist entirely on November 5th, but in reality, I made several changes to the order and even cut some pieces in response to the dress rehearsal, and the exact content of some things was kind of up in the air even on the day of. While I technically missed my deadline or whatever, I don’t regret that at all - I think my Solstice was quite significantly better than it would have been if I had stopped iterating on the setlist a month earlier.\n\nConsiderations for creating a setlist\n=====================================\n\nCreating a cohesive message\n---------------------------\n\nWhen it came to speeches, I took an arc-first approach. I decided what message fit in every place in the arc and found a piece that fit there. Only after that did I approach potential speakers. I ended up using four existing pieces wholesale; the rest were based on existing pieces but were either remixed or heavily edited by the speaker. The takeaway action item here is to have something very specific in mind for each speech and conveying that to the speaker. This improves cohesion while still allowing each speaker to put their own twist on the piece if they so choose.\n\nExamples:\n\n*   Tessa remixed Ray’s *A Bottomless Pit of Suffering* to make more sense in Berkeley (it was originally given in NYC, where it’s very cold) and to have more of her own voice.\n*   I had Nate Soares on the docket from quite early on in the process, but he’s busy executive-directing MIRI so I didn’t want to take up too much of his time. With his input, I chose an old post from his blog for him to read ([*How We Will Be Measured*](http://mindingourway.com/how-we-will-be-measured/)), and Chelsea and I edited it to be shorter and more appropriate as a speech. Nate then made a bunch of his own changes to reflect the ways his thinking has changed over the past few years, but ultimately it was still recognizable as the same piece.\n*   Though it ended up being cut at the very last minute, Alex Altair adapted a scene from HPMoR into a speech, which was pretty cool.\n\nCreating a smooth arc\n---------------------\n\nIn order to create a smooth experience, you have to make sure that there are smooth transitions  between the message, emotional tone, and musical/artistic feel of each piece and the next. It turns out to be really hard to match these all up. For example, there are some funny/upbeat/light-hearted songs about death (e.g. *We Will All Go Together When We Go*), and some fairly serious-sounding songs about more light-hearted topics (e.g. *Time Wrote the Rocks*). Some songs are up-tempo, some are slow and mournful, some have percussion, some are performed by choir. There are just a ton of considerations. (This is why Ray writes so many of his own songs - that’s the only way you can really have control over the message, tone, and feel all at once.)\n\nI was aware of all of these considerations, and that’s a big part of the reason that I made sure to run through each version of the setlist from start to finish, but I don’t think I quite got it right until the final performance (even the dress rehearsal, one week prior, was fairly rocky in this regard). And even then there were still a few problems, like the energy drop from Son of Man to Uplift and the energy drop from Singularity to Five Thousand Years (making Five Thousand Years a rather anticlimactic finale).\n\nMCing \n------\n\nThe previous Solstices I had attended were just a series of pieces strung together, and the audience were mostly left to discern the arc on their own. At some point in my research, I saw a video clip of Kenzi MCing a Solstice, and I immediately decided I wanted to do that. \n\nIn my opinion, there are a ton of advantages to having an MC. Here are a couple:\n\n1.  It gives the audience more insight into why each piece was chosen and generally gives you a chance to tie the arc together more explicitly.\n2.  It allows you to make announcements during the program without breaking immersion, such as giving trigger warnings or asking the audience to hold their applause until further notice.\n3.  It fills what would otherwise be awkward pauses between pieces as performers get on and off the stage.\n\nPost-performance feedback\n=========================\n\nAbout 50 people filled out the feedback survey, and their feedback falls into a few rough categories for me:\n\n1.  “This person is right and I would change/tweak this if I had it to do over”\n2.  “This feedback should not be acted upon”\n3.  “Feedback on this is very split, and you can’t win ‘em all”\n\nThings I would tweak\n--------------------\n\n**Finale**\n\nThe peak-end rule is really important, and my Solstice didn’t have a very strong ending. I had to go up onstage and be like, “Okay, now it’s over.” If I had it to do over, I might cut Five Thousand Years and end on Singularity, which everyone loved.\n\n**Moment of darkness**\n\nThe moment of darkness itself (two minutes of silence in the pitch black) got mixed reviews - some people found it very powerful, some people found it existentially horrifying and had to distract themselves, and a lot of people found it didn’t really land. The main thing I would change here is the way I introduced it.\n\nI said, “We’re about to sit in silence for two minutes. If you’re up for it, I want you to look up at the stars, and think of someone you’ve lost. Someone whose voice you will never hear again, whose mind is gone from the world forever. Give them your grief, yes, but also give them your resolve.”\n\nIn retrospect, this was far too specific an ask. A lot of people said that the moment of darkness didn’t really land specifically because they’d never lost anyone close to them. (I copied the text from the Solstice Habryka and I ran in Madison, where it worked very well, but where the circumstances were very different in quite a number of significant ways.) If I had it to do over, I would encourage people to sit with their feelings, wherever they were at, rather than prescribing something for them to think about. \n\n**First speech**\n\nI struggled for a long time to find or write an appropriate speech for the first-speech slot in the program. It was only a day or so before the dress rehearsal that I settled on giving an abridged version of Nate’s This Is A Dawn. While it had roughly the right message, I don’t think I myself was that bought into it, and as a result, people seemed to find it a bit generic, and not really meaningful. I’m not actually sure what in particular I would do here if I had a do-over, but I do want to highlight that the first-speech slot is quite important and I definitely didn’t totally nail it.\n\nFeedback that should not be acted upon\n--------------------------------------\n\nA hopefully uncontroversial example of this is the person who doesn’t like the sound of strummed instruments, and therefore gave a low rating to every song with strummed guitars. Sure, this is a valid way to feel, but at the same time, one person’s preference in this area does not mean it makes sense to cut all guitars from Solstice.\n\nA more controversial example, but one that I am still willing to stand by publicly, is the common complaint that [Son of Man](https://www.youtube.com/watch?v=tiBaBca7-rY) is sexist. Look, I’m a woman. Chelsea is a woman. The person who soloed on Son of Man is a woman. My sense is that, while some people were genuinely offended, and that came through in their feedback, most of the people who registered complaints were just people who were worried that other people might have been offended. I continue to think this song is an excellent fit for Solstice message-wise and has great energy (it was intended to be performed a bit faster but there were some technical difficulties with the drums). I would not hesitate to include it again.\n\nTriggering content\n==================\n\nEating disorder trigger\n-----------------------\n\nFor complicated reasons, there was a brief discussion of weight loss at the end of Solstice. It was intended as a sort of light-hearted post-credits piece, but we mishandled it, and people didn’t end up getting the chance to leave if the topic was difficult for them. This had significant negative consequences for some people, and I sincerely apologize for that.\n\nWe’re taking steps to make sure that future Bay Solstices are more careful around sensitive topics like this. Specific action items include providing verbal trigger warnings in addition to the ones written in the program, and allowing significantly more time for people to leave if they need to, including having some people planted in the audience to stand up and leave so that it feels socially okay to do so. (Even though I myself won’t be running Solstice next year, I’m in close contact with next year’s organizers and have made at least one of them aware of this.)\n\nOther triggers\n--------------\n\nOn the feedback form, some people mentioned being very upset by Solstice because it reminded them that they were lonely or felt like they could be accomplishing more. I do not think anything should change about Solstice itself in response to this feedback, because being reminded that the universe is vast and dark and cold is pretty much the entire point of Solstice. \n\nPerhaps in the future it would be good to make it clear to potential Solstice-goers that Solstice deals a lot with death, individual responsibility, and the vast, uncaring universe. Then they can make more of an informed choice about whether or not to go, and if they can’t handle it, they can’t reasonably blame it on anyone but themselves.\n\nIndividual pieces\n-----------------\n\n**Raw feedback**\n\n(The table below has the pieces in chronological order of how they appeared in the performance.)\n\n<table><tbody><tr><td>&nbsp;</td><td>&nbsp;</td><td><strong>Hated</strong></td><td><strong>Dislike</strong></td><td><strong>Meh</strong></td><td><strong>Like</strong></td><td><strong>Love</strong></td></tr><tr><td>1</td><td>The Circle</td><td>1</td><td>3</td><td>6</td><td>12</td><td>21</td></tr><tr><td>2</td><td>The X days of X-Risk</td><td>3</td><td>4</td><td>7</td><td>16</td><td>14</td></tr><tr><td>3</td><td>To Drive the Cold Winter Away</td><td>0</td><td>1</td><td>14</td><td>16</td><td>11</td></tr><tr><td>4</td><td>Bold Orion</td><td>0</td><td>1</td><td>4</td><td>20</td><td>16</td></tr><tr><td>5</td><td>Time Wrote the Rocks</td><td>0</td><td>6</td><td>10</td><td>17</td><td>9</td></tr><tr><td>6</td><td>This is a Dawn (abridged)</td><td>0</td><td>0</td><td>12</td><td>19</td><td>5</td></tr><tr><td>7</td><td>Hard Times Come Again No More</td><td>0</td><td>2</td><td>8</td><td>18</td><td>12</td></tr><tr><td>8</td><td>There Will Come Soft Rain</td><td>3</td><td>6</td><td>12</td><td>11</td><td>9</td></tr><tr><td>9</td><td>Pale Blue Dot</td><td>0</td><td>0</td><td>3</td><td>12</td><td>25</td></tr><tr><td>10</td><td>Stardust</td><td>0</td><td>2</td><td>14</td><td>13</td><td>14</td></tr><tr><td>11</td><td>Do You Realize</td><td>1</td><td>4</td><td>6</td><td>16</td><td>14</td></tr><tr><td>12</td><td>A Bottomless Pit of Suffering</td><td>1</td><td>1</td><td>7</td><td>15</td><td>14</td></tr><tr><td>13</td><td>Bitter Wind Lullaby</td><td>0</td><td>3</td><td>11</td><td>16</td><td>10</td></tr><tr><td>14</td><td>Eulogy</td><td>0</td><td>0</td><td>1</td><td>8</td><td>33</td></tr><tr><td>15</td><td>The Moment of Darkness</td><td>1</td><td>1</td><td>5</td><td>15</td><td>17</td></tr><tr><td>16</td><td>Spoken Call/Response</td><td>0</td><td>2</td><td>11</td><td>18</td><td>7</td></tr><tr><td>17</td><td>We Are the Light</td><td>2</td><td>1</td><td>13</td><td>16</td><td>5</td></tr><tr><td>18</td><td>Light Pollution</td><td>0</td><td>1</td><td>12</td><td>19</td><td>3</td></tr><tr><td>19</td><td>Endless Lights</td><td>0</td><td>3</td><td>11</td><td>14</td><td>9</td></tr><tr><td>20</td><td>Brighter Than Today</td><td>0</td><td>1</td><td>2</td><td>16</td><td>18</td></tr><tr><td>21</td><td>How We Will Be Measured</td><td>0</td><td>4</td><td>12</td><td>15</td><td>8</td></tr><tr><td>22</td><td>Son of Man</td><td>3</td><td>6</td><td>10</td><td>15</td><td>6</td></tr><tr><td>23</td><td>Uplift</td><td>0</td><td>1</td><td>7</td><td>15</td><td>17</td></tr><tr><td>24</td><td>What it means to win</td><td>1</td><td>2</td><td>6</td><td>20</td><td>5</td></tr><tr><td>25</td><td>Beautiful Tomorrow</td><td>1</td><td>3</td><td>7</td><td>16</td><td>12</td></tr><tr><td>26</td><td>Singularity</td><td>1</td><td>2</td><td>0</td><td>9</td><td>31</td></tr><tr><td>27</td><td>Five Thousand Years</td><td>2</td><td>4</td><td>7</td><td>17</td><td>10</td></tr><tr><td>28</td><td>After-the-Credits-Eliezer-Bit</td><td>8</td><td>6</td><td>5</td><td>9</td><td>14</td></tr></tbody></table>\n\nAt a high level, most people liked most things! This is heartening. \n\nRay and I sorted the pieces four different ways*, and there were five pieces that clearly came out on top and five that (only a little bit less) clearly came out at the bottom.\n\nBest-liked:\n\n1.  Eulogy\n2.  Singularity\n3.  Pale Blue Dot\n4.  Brighter Than Today\n5.  Bold Orion\n\nWorst-liked:\n\n1.  After-the-Credits Eliezer bit\n2.  Son of Man\n3.  There Will Come Soft Rain\n4.  We Are the Light\n5.  Light Pollution\n\n**Effect of delivery**\n\nSomething I notice that’s interesting (but not that surprising) is the large effect that delivery had on people’s ratings. For example, two of the highest rated pieces were Singularity and Pale Blue Dot. In addition to a solid delivery by Chelsea, Pale Blue Dot had a backing track and custom planetarium footage. Singularity was extremely energetic and fun, and people had generally positive affect towards all of the songs that prominently featured Taylor because he’s such an obviously skilled musician. Brighter Than Today and Bold Orion were also energetic and very polished performances.\n\nBy contrast, people were relatively lukewarm on Endless Lights and Bitter Wind Lullaby, two Solstice staples that I think of as being fairly well-liked in general. Both of these songs had significant problems with their execution, with the performers having trouble agreeing on the time signature. As a result, it was difficult for people to sing along, which seems to have made for a negative overall impression.\n\n**Addressing the elephant in the room**\n\nAn additional thing that people who attended this Solstice might want to see addressed is what the heck was up with the Eliezer piece. Even apart from those who found it triggering or otherwise inappropriate, a lot of people were just confused about why it happened (e.g., several people's reaction was, \"Why is this guy talking to me like I'm his friend, I don't even know him\"). The explanation is perhaps not all that satisfying, but I'll give it anyway.\n\nIn early October, Eliezer contacted me asking if he could do a shenanigan at Solstice. He explained his idea to me, and while I didn't really see how it would fit in, I also didn't want to reject him out of hand.\n\nI talked to a couple people I trusted about this, and we came to the conclusion that it would be pretty valuable to have Eliezer onstage. The reasons for this were a bit nebulous, but roughly rested on the following:\n\n*   Regardless of any single community member's personal feelings on him and his writings, it's hard to deny that this community would not exist as it does today without Eliezer. (I, for example, came in through a chance encounter with HPMOR in high school, and basically every aspect of my current life is a direct result of that encounter.)\n*   Eliezer has increasingly retreated from public life over the past few years, and this has resulted in some feelings of abandonment on the part of the community.\n*   Having Eliezer onstage during Solstice would show his implicit support for the community and the event; following the above, it would remind the audience of what brought us all together and that we haven't been abandoned by our founders.\n\nBased on this reasoning, it was having Eliezer onstage that mattered, and the content of his piece wasn't really relevant. The eating disorder trigger was honestly not something I even considered until someone mentioned it after the dress rehearsal. It was at that point that I decided to move the piece to be 'post-credits' (it had previously been early in the program proper), to make it opt-out for people uncomfortable with the topic, but as mentioned above, I failed to handle this correctly.\n\nIt’s also worth noting that, while more people hated the Eliezer bit than hated any other piece, there were also a fair number of people who loved it (if you sort by the raw number of Loves, it comes dead middle). So it was in fact not universally reviled (lots of people found it hilarious or heartwarming); it was just very polarizing.\n\nSummary of takeaways\n====================\n\nThis is just all of the takeaways from the main body of this post, in the order that they appeared.\n\n*   Starting a year in advance and testing and iterating often makes for a really good final product but also burns you out like hell. I think this was ultimately definitely worth it, but if I was told I had to do another year of this I would probably flee the country.\n*   Deciding on a central theme/thesis for your Solstice early on is really important.\n*   Set a regular time to work on Solstice things so that they don’t slip through the cracks, especially if you have a full-time job. It’s best if you can meet with other people regularly for this purpose, because accountability.\n*   While hiring professional musicians may be easier, there are enough skilled musicians in the Bay Area rationalist community that I think it’s worthwhile to go that route, especially since this makes it feel more like a community event. Just make sure that people audition (or are known to be skilled and easy to work with) and can commit to rehearsing with each other.\n*   Choose speakers and other performers largely based on their skill level, but it’s also important to make sure that they’re value-aligned with you when it comes to the Solstice you’re creating together.\n*   It’s okay to iterate on the content until the very last minute so long as everyone is on the same page / no one is thrown off or blind-sided by the late changes.\n*   If you want your arc to be really cohesive, you need to exert centralized control over each piece rather than just leaving performers to do their own thing.\n*   It’s really hard to create a smooth arc over all the dimensions that matter. If you can write your own songs or work with a friend who can write original songs, this is a huge asset.\n*   MCing is great.\n*   Not all feedback should be acted upon.\n*   Pay attention to the peak-end rule.\n*   Potentially triggering topics should be handled more carefully than they were by me. It’s important for people to have a genuine opportunity to make an informed choice about what they’re exposed to.\n*   Delivery/execution of pieces is just as important as (if not more important than) the semantic content and the fit in the arc.\n\nResources\n=========\n\nSetlist spreadsheet\n-------------------\n\nI love spreadsheets with a passion, and I found keeping all of the relevant material in one place to be enormously helpful both for me and for communication purposes. (Whenever someone had a question about the arc, the performers, or anything, we could just pull up the spreadsheet, and even make a copy of it to see how changing the order of the pieces would feel.)\n\n[Here](https://docs.google.com/spreadsheets/d/1T97fixXZx81aJ-zIBM5b7UnQpv4TflXGuauly29PlLI/) is a template for the spreadsheet I used. Let me know if anything is unclear!\n\nMasterlist of Solstice materials\n--------------------------------\n\nDaniel Speyer runs the [Secular Solstice GitHub page](https://secularsolstice.github.io/), which is a useful resource, but it’s also very hard to edit - especially if, like me, you’re not a programmer and don’t know how to use GitHub in general. The [Giant Epic Rationalist Solstice Filk spreadsheet](https://docs.google.com/spreadsheets/d/1ETa5RXtDJAQsk6RYhaITfMj1ktOZLP2AKwmcFE8iTVY/edit#gid=0) is likewise a useful resource, but it’s kind of a mess. So I made [my own spreadsheet](https://docs.google.com/spreadsheets/d/17QhguW6PHdAkYKClsk0ZiAnmWbgs0_SLYw4dkoQosUo/), which is publicly editable and incorporates every song, poem, story, and speech from the above two repositories. (Apologies to Daniel Speyer and to anyone who sees this as polluting the commons by instantiating too many competing projects.)\n\n* * *\n\n\\* The sorting algorithms we used were the following:\n\n1.  % Positive : (Liked + Loved) / (Total responses)\n2.  Overall-Liked : (Liked + Loved) – (Disliked + Hated)\n3.  Weighted : (2.25\\*Loved + Liked) – (2.25\\*Hated + Disliked)\n4.  Loved : Raw number of ‘Love’s\n\n* * *\n\n*Thanks to Ray Arnold, Nat Kozak, and Chelsea Voss for their input and edits.*"
    },
    "voteCount": 22
  },
  {
    "_id": "q79vYjHAE9KHcAjSs",
    "url": null,
    "title": "Rationalist Fiction",
    "slug": "rationalist-fiction",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "Fiction (Topic)"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "**Followup to**:  [Lawrence Watt-Evans's Fiction](http://www.overcomingbias.com/2008/07/lawrence-watt-e.html)  \n**Reply to**:  [On Juvenile Fiction](/lw/2q/on_juvenile_fiction/)\n\nMBlume [asked](/lw/2q/on_juvenile_fiction/) us to remember what childhood stories might have influenced us toward rationality; and this was given such excellent answers as Norton Juster's _The Phantom Tollbooth._  So now I'd like to ask a related question, expanding the purview to all novels (adult or child, SF&F or literary):  Where can we find explicitly rationalist fiction?\n\nNow of course there are a great many characters who claim to be using logic.  The whole genre of mystery stories with seemingly logical detectives, starting from Sherlock Holmes, would stand in witness of that.\n\nBut when you look at what Sherlock Holmes does - you can't go out and do it at home.  Sherlock Holmes is not _really_ operating by any sort of reproducible method.  He is operating by magically finding the right clues and carrying out magically correct complicated chains of deduction.  Maybe it's just me, but it seems to me that reading Sherlock Holmes does not inspire you to go and do likewise.  Holmes is a mutant superhero.  And even if you did try to imitate him, it would never work in real life.\n\nContrast to A. E. van Vogt's _Null-A_ novels, starting with _The World of Null-A._  Now let it first be admitted that Van Vogt had a number of flaws as an author.  With that said, it is probably a historical fact about my causal origins, that the _Null-A_ books had an impact on my mind that I didn't even realize until years later.  It's not the sort of book that I read over and over again, I read it and then put it down, but -\n\n\\- but this is where I was first exposed to such concepts as \"The map is not the territory\" and \"rose~1~ is not rose~2~\".\n\nNull-A stands for \"Non-Aristotelian\", and the premise of the ficton is that studying Korzybski's General Semantics makes you a superhero.  Let's not really go into that part.  But in the Null-A ficton:\n\n1)  The protagonist, Gilbert Gosseyn, is not a mutant.  He has _studied_ rationality techniques that have been _systematized_ and are used by other members of his society, not just him.\n\n2)  Van Vogt tells us what (some of) these principles are, rather than leaving them mysteriously blank - we can't be Gilbert Gosseyn, but we can at least use _some_ of this stuff.\n\n3)  Van Vogt [conveys the experience](http://www.overcomingbias.com/2009/02/truth-from-fiction.html), _shows_ Gosseyn in action using the principles, rather than leaving them to triumphant explanation afterward.  We are put into Gosseyn's shoes at the moment of his e.g. making a conscious distinction between two different things referred to by the same name.\n\nThis is a high standard to meet.\n\nBut Marc Stiegler's _David's Sling_ (quoted in e.g. [this post](http://www.overcomingbias.com/2008/01/gray-fallacy.html)) meets this same standard:  The Zetetics derive their abilities from training in a systematized tradition; we get to see the actual principles the Zetetics are using, and they're ones we could try to apply in real life; and we're put into their shoes at the moments of their use.\n\nI mention this to show that it isn't _only_ van Vogt who's ever done this.\n\nHowever...\n\n...those two examples actually _exhaust_ my knowledge of the science fiction and fantasy literature, so far as I can remember.\n\nIt really is a very high standard we're setting here.  To realistically show your characters using an interesting technique of rationality, you have to _know_ an interesting technique of rationality.  Van Vogt was inspired by Korzybski, who - I discovered when I looked this up, just now - actually _invented_ the phrase \"The map is not the territory\".  Marc Stiegler was inspired by, among other sources, Eric Drexler and Robin Hanson.  (Stiegler has another novel called _Earthweb_ about using prediction markets to defend the Earth from invading aliens, which was my introduction to the concept of prediction markets.)\n\nIf I relax the standard to focus mainly on item (3), fiction that transmits a powerful experience of using rationality, then I could add in Greg Egan's _Distress,_ some of [Lawrence Watt-Evans](http://www.overcomingbias.com/2008/07/lawrence-watt-e.html)'s strange little novels, the travails of Salvor Hardin in the first _Foundation_ novel, and probably any number of others.\n\nBut what I'm really interested in is whether there's any full-blown Rationalist Fiction that I've missed - or maybe just haven't remembered.  Failing that, I'm interested in stories that merely do a good job of conveying a rationalist experience.  (Please specify which of these cases is true, if you make a recommendation.)"
    },
    "voteCount": 40
  },
  {
    "_id": "rBkZvbGDQZhEymReM",
    "url": null,
    "title": "Forum participation as a research strategy",
    "slug": "forum-participation-as-a-research-strategy",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "Scholarship & Learning"
      },
      {
        "name": "Intellectual Progress via LessWrong"
      },
      {
        "name": "Intellectual Progress (Individual-Level)"
      }
    ],
    "tableOfContents": {
      "sections": [
        {
          "title": "Benefits of Forum Participation (FP)",
          "anchor": "Benefits_of_Forum_Participation__FP_",
          "level": 1
        },
        {
          "title": "FP takes little effort / will power",
          "anchor": "FP_takes_little_effort___will_power",
          "level": 2
        },
        {
          "title": "FP is a good way to notice missing background knowledge and provides incentives to learn missing knowledge",
          "anchor": "FP_is_a_good_way_to_notice_missing_background_knowledge_and_provides_incentives_to_learn_missing_knowledge",
          "level": 2
        },
        {
          "title": "FP is a good way to stay up to date on everyone else's latest research",
          "anchor": "FP_is_a_good_way_to_stay_up_to_date_on_everyone_else_s_latest_research",
          "level": 2
        },
        {
          "title": "Arguments that are generated in reaction to some specific post or discussion can be of general value",
          "anchor": "Arguments_that_are_generated_in_reaction_to_some_specific_post_or_discussion_can_be_of_general_value",
          "level": 2
        },
        {
          "title": "FP generates new ideas via cross-fertilization",
          "anchor": "FP_generates_new_ideas_via_cross_fertilization",
          "level": 2
        },
        {
          "title": "FP helps prepare for efficiently communicating new ideas",
          "anchor": "FP_helps_prepare_for_efficiently_communicating_new_ideas",
          "level": 2
        },
        {
          "title": "My Recommendations",
          "anchor": "My_Recommendations",
          "level": 1
        },
        {
          "title": "Comment more",
          "anchor": "Comment_more",
          "level": 2
        },
        {
          "title": "Practice makes better",
          "anchor": "Practice_makes_better",
          "level": 2
        },
        {
          "title": "Think of FP as something to do for yourself",
          "anchor": "Think_of_FP_as_something_to_do_for_yourself",
          "level": 2
        },
        {
          "title": "Encourage and support researchers who adopt FP as their primary research strategy",
          "anchor": "Encourage_and_support_researchers_who_adopt_FP_as_their_primary_research_strategy",
          "level": 2
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        },
        {
          "anchor": "comments",
          "level": 0,
          "title": "43 comments"
        }
      ],
      "headingsCount": 14
    },
    "contents": {
      "markdown": "Previously: [Online discussion is better than pre-publication peer review](https://www.lesswrong.com/posts/a3aGosA987cZ4aRAB/online-discussion-is-better-than-pre-publication-peer-review), [Disincentives for participating on LW/AF](https://www.lesswrong.com/posts/xYav5gMSuQvhQHNHG/disincentives-for-participating-on-lw-af)\r\n\r\nRecently I've noticed a cognitive dissonance in myself, where I can see that my best ideas have come from participating on various mailing lists and forums (such as cypherpunks, extropians, SL4, everything-list, LessWrong and AI Alignment Forum), and I've received a certain amount of recognition as a result, but when someone asks me what I actually do as an \"independent researcher\", I'm embarrassed to say that I mostly comment on other people's posts, participate in online discussions, and occasionally a new idea pops into my head and I write it down as a blog/forum post of my own. I guess that's because I imagine it doesn't fit most people's image of what a researcher's work consists of.\r\n\r\nOnce I noticed this, the tension is easy to resolve - in this post I'm going to proclaim/endorse forum participation (aka commenting) as a productive research strategy that I've managed to stumble upon, and recommend it to others (at least to try). Note that this is different from saying that [forum/blog posts are a good way for a research community to communicate](https://www.lesswrong.com/posts/a3aGosA987cZ4aRAB/online-discussion-is-better-than-pre-publication-peer-review). It's about individually doing better as researchers.\r\n\r\n# Benefits of Forum Participation (FP)\r\n\r\n## FP takes little effort / will power\r\n\r\nIn other words it [feels more like play than work](https://www.lesswrong.com/posts/mHNsymmPTRsiJDo9Q/why-do-some-kinds-of-work-not-feel-like-work), which means I rarely have issues with not wanting to do something that I think is important to do (i.e., akrasia), the only exception being that writing posts seems to take more effort so occasionally I spend my time writing comments when I perhaps should write posts instead. (This is the part of this post that I think may be least likely to generalize to other people. It could be that I'm an extreme outlier in finding FP so low-effort. However it might also be the case that it becomes low effort for most people to write comments once they've had enough practice in it.) \r\n\r\n## FP is a good way to notice missing background knowledge and provides incentives to learn missing knowledge\r\n\r\nIf you read a post with an intention to question or comment on it, it's pretty easy to notice that it assumes some background knowledge that you lack. The desire to not ask a \"stupid\" question or make a \"stupid\" comment provides powerful incentive to learn the miss knowledge.\r\n\r\n## FP is a good way to stay up to date on everyone else's latest research\r\n\r\nIt's often a good idea to stay up to date on other people's research, but sometimes one isn't highly motivated to do so. FP seems to make that easier. For example, I wasn't following Stuart's research on counterfactual oracles, until the recent contest drew my attention and desire to participate, and I ended up reading the latest posts on CO in order to understand the current state of the art on that topic, which turned out to be pretty interesting.\r\n\r\n## Arguments that are generated in reaction to some specific post or discussion can be of general value\r\n\r\nIt's not infrequent that I come up with an argument in response to some post or discussion thread, and later expand or follow up that argument into a post because it seems to apply more generally than to just that post/discussion. [Here](https://www.lesswrong.com/posts/4K52SS7fm9mp5rMdX/three-ways-that-sufficiently-optimized-agents-appear) is one such example.\r\n\r\n## FP generates new ideas via cross-fertilization\r\n\r\nFP incentivizes one to think deeply about many threads of research, and often (at least for me) an idea pops into my head that seems to combine various partial ideas floating in the ether into a coherent or semi-coherent whole (e.g., UDT), or is the result of applying or analogizing someone else's latest idea to a different topic (e.g., \"human safety problem\", \"philosophy as high complexity class\").\r\n\r\n## FP helps prepare for efficiently communicating new ideas\r\n\r\nFP is a good way to build models of other people's epistemic states, and also a good way to practice communicating with fellow researchers, both of which are good preparation for efficiently communicating one's own new ideas.\r\n\r\n# My Recommendations\r\n\r\n## Comment more\r\n\r\nTo obtain the above benefits, one just has to write more comments. It may be necessary to first overcome [disincentives to participate](https://www.lesswrong.com/posts/xYav5gMSuQvhQHNHG/disincentives-for-participating-on-lw-af). If you can't, please speak up and maybe the forum admins will do something to help address whatever obstacle you're having trouble with.\r\n\r\n## Practice makes better\r\n\r\nIf it seems hard to write good comments, practice might make it easier eventually.\r\n\r\n## Think of FP as something to do for yourself\r\n\r\nSome people might think of commenting as primarily providing a service to other researchers or to the research community. I suggest also thinking of it as providing a benefit to yourself (for the above reasons).\r\n\r\n## Encourage and support researchers who adopt FP as their primary research strategy\r\n\r\nI'm not aware of any organizations that explicitly encourage and support researchers to spend most or much of their time commenting on forum posts. But perhaps they should, if it actually is (or has the potential to be) a productive research strategy? For example this could be done by providing financial support and/or status rewards for effective forum participation."
    },
    "voteCount": 56
  },
  {
    "_id": "tRQek3Xb9cKZ2o6iA",
    "url": "https://katarinaslama.github.io/2020/05/17/OpenAI-blog5/",
    "title": "How to (not) do a literature review",
    "slug": "how-to-not-do-a-literature-review",
    "author": "Katarina Slama",
    "question": false,
    "tags": [
      {
        "name": "Scholarship & Learning"
      },
      {
        "name": "World Modeling"
      }
    ],
    "tableOfContents": {
      "sections": [
        {
          "title": "OpenAI Scholars: Fifth Steps - The Dreaded Literature Review",
          "anchor": "OpenAI_Scholars__Fifth_Steps___The_Dreaded_Literature_Review",
          "level": 1
        },
        {
          "title": "How to do a literature review",
          "anchor": "How_to_do_a_literature_review",
          "level": 1
        },
        {
          "title": "1. Start with your goal.",
          "anchor": "1__Start_with_your_goal_",
          "level": 2
        },
        {
          "title": "2. Decide what constitutes success: How will you know that you're done?",
          "anchor": "2__Decide_what_constitutes_success__How_will_you_know_that_you_re_done_",
          "level": 2
        },
        {
          "title": "3. Define your questions.",
          "anchor": "3__Define_your_questions_",
          "level": 2
        },
        {
          "title": "4. Answer your questions.",
          "anchor": "4__Answer_your_questions_",
          "level": 2
        },
        {
          "title": "5. Stop.",
          "anchor": "5__Stop_",
          "level": 2
        },
        {
          "title": "How to avoid doing literature reviews",
          "anchor": "How_to_avoid_doing_literature_reviews",
          "level": 1
        },
        {
          "title": "1. Mind whom you follow on Google Scholar.",
          "anchor": "1__Mind_whom_you_follow_on_Google_Scholar_",
          "level": 2
        },
        {
          "title": "2. Mind whom you follow on Twitter.",
          "anchor": "2__Mind_whom_you_follow_on_Twitter_",
          "level": 2
        },
        {
          "title": "3. Organize the papers that come flying your way.",
          "anchor": "3__Organize_the_papers_that_come_flying_your_way_",
          "level": 2
        },
        {
          "title": "4. Make it fun.",
          "anchor": "4__Make_it_fun_",
          "level": 2
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        },
        {
          "anchor": "comments",
          "level": 0,
          "title": "7 comments"
        }
      ],
      "headingsCount": 14
    },
    "contents": {
      "markdown": "This is cross-posted from my [personal blog](https://katarinaslama.github.io/2020/05/17/OpenAI-blog5/), where I share thoughts on my work and learning process in the OpenAI Scholars Program. I thank Ruby Bloom for suggesting that I share the post here as well.\n\n**OpenAI Scholars: Fifth Steps - The Dreaded Literature Review**\n================================================================\n\nThe [OpenAI Scholars](https://openai.com/blog/openai-scholars-spring-2020/) , and I among them, recently completed a project proposal for the second half of our program. Having recently finished a PhD, writing a proposal and doing the requisite literature review, should be second nature. But literature reviews were always my least favorite part of research.  \n\nFortunately, I'm in good company. In a previous life as a young aspiring neuroscientist, I once attended a talk by [David Hubel](https://www.nobelprize.org/prizes/medicine/1981/hubel/facts/) . When asked for tips about how to \"keep up with the literature\", I recall that Professor Hubel responded: \"You know, at some point in your career, you have to decide if you want to be a consumer - or a producer.\" He elaborated that he would only really look at papers that his [advisor](https://en.wikipedia.org/wiki/Stephen_Kuffler) or his long-term collaborator [Torsten Wiesel](https://en.wikipedia.org/wiki/Torsten_Wiesel) pointed him to.\n\n![](https://katarinaslama.github.io/images/hubel.jpg)\n\nThere's good reason to want to avoid literature reviews. To begin with, the problem formulation is intractable: \"Know everything.\" If you've spent any amount of time around academics, it will soon become apparent that this is exactly what they expect from you. \"Oh you have**n't** read that paper?\" The assumption is that you have read every paper there is.  \n\nOf course, this is an impossible task. Last time I visited Google Scholar and searched on a few project-relevant terms, I encountered 105,000 papers. It takes me a full day to read and satisfactorily understand an academic paper. So reading 105 kilo-papers would take me 288 years. And I had hoped to also do some coding work during the Scholars program.  \n\nNow suppose you want to cook a [ghormeh sabzi](https://en.wikipedia.org/wiki/Ghormeh_sabzi) . To do so, you buy yourself a can of [ghormeh sabzi mix](https://www.sadaf.com/sadaf-herbs-mix-sabzi-ghormeh-14-1380) , and follow the instructions on the can. There are about five steps. However, as ubiquitous an activity as the literature review is (in academic circles anyway), I am yet to encounter an honest and pragmatic recipe for how they are to be cooked. I really wish there was one.  \n\nI for one am a great fan of [Thich Nhat Hanh's](https://en.wikipedia.org/wiki/Th%C3%ADch_Nh%E1%BA%A5t_H%E1%BA%A1nh) how-to books. Some of my favorite titles among Nhat Hanh's books are: \"How to sit\", \"How to relax\", and - perhaps the best one - \"How to see\".  \n\n![](https://katarinaslama.github.io/images/howtosee.jpg)\n\nI wish there had been a series of books like that available for me as a beginning grad student, covering seemingly trivial but often intractable activities like \"How to read a paper\", \"How to attend a talk\" and, indeed, \"How to do a literature review\".\n\n(**Updated: Apparently there exist plenty of expositions on this topic. See the comment by AllAmericanBreakfast below. I hope that some of the ideas in this post might nonetheless add something to whatever else you might have read.)\n\nSo, this is a first draft toward such a pragmatic guide to literature reviews: How to do them and how to avoid them. Again, this is absolutely in draft form, so please do send your constructive feedback my way.\n\n**How to do a literature review**\n=================================\n\n1\\. Start with your goal.\n-------------------------\n\nDo not start your literature review with the goal of \"doing a literature review\". Notice that such a goal does not have an end point when you can look at your work in satisfaction and notice that you are done. (Unless you have 288 years to spare). Instead, start with identifying your goal: Why are you doing this literature review? In my case, I had completed a spreadsheet with 50 papers, including their main points, type of neural recording, type of network architecture, a usefulness rating etc., by the time I realized that I had not identified my goal. I had to take a deliberate step back to recall that one goal of my literature review was to write a project proposal. Hence, my literature review should be tailored to that goal. Once I realized that, I decided to approach my work from the opposite direction: I drafted a project proposal \"blind\" to the literature. In other words, I wrote out a draft in bullet points with only the information I had in my mind. Once I had done that, it became quite obvious where I had knowledge gaps. Those knowledge gaps created the questions that I needed to search for in my literature review (see point 3 below.)  \n\n2\\. Decide what constitutes success: How will you know that you're done?\n------------------------------------------------------------------------\n\nThis is really important, and it's often the hardest part with an infinity-project like a literature review. In my case, the done-point was when I had completed the project proposal. But how do I know that my proposal is good enough? In practice, for me, it was when the deadline hit. If you have come up with better metrics and markers for when to consider a project completed, please do let me know.  \n\n3\\. Define your questions.\n--------------------------\n\nWhen doing a literature review, it's critical to know what information you're searching for. Trying to absorb all the information contained in a dense document is rarely a good plan. In my case, my questions fell out of my first, naive project proposal draft. I also talked my project through with a [friend](https://sites.google.com/site/falklieder/home) , who is a more experienced researcher. He offered the following thought points for guidance to a literature review for a new project:  \n\n1\\. The goal for your literature review should be to identify the problem that your project will solve. Your project might either aim to solve a new problem, or improve on an existing method.  \n2\\. Use the literature revew to identify \\*gaps\\* in the literature: What do we not yet know?  \n3\\. What are the shortcomings of existing methods to solve the problem? Why are current approaches not yet useful in practice? What is the bottleneck?  \n\n4\\. Answer your questions.\n--------------------------\n\nI think this is self-explanatory, but please let me know if not! One thing to keep in mind here is to not get lost in literature-tangents while finding answers to your questions. Provided your goal is to finish, that is. If you have the time to go off on philosophical tangents, feel free and enjoy!  \n\n5\\. Stop.\n---------\n\nJust stop. Seriously. When you're done, you should stop. Go take a walk outside.  \n\n**How to avoid doing literature reviews**\n=========================================\n\nThe first section described the second-best way of doing a literature review. The best way to do a literature review is to not do a literature review. The art of avoiding doing a literature review is very similar to the art of avoiding deep cleaning your house: Keep it tidy on a daily basis. (Just like with deep cleaning your house, you probably still need to do a proper deep dive into the literature from time to time, just not as frequently as you would if you didn't have a routine of daily up-keep). Here are some ideas for \"keeping up with the literature\":  \n\n1\\. Mind whom you follow on Google Scholar.\n-------------------------------------------\n\nOnce you've gotten your feet a little bit wet in your field of interest, you will have a clue of who often writes interesting papers. If you follow them on Google Scholar, you will be notified as they publish more interesting stuff.  \n\n2\\. Mind whom you follow on Twitter.\n------------------------------------\n\nThis is especially true of deep learning, but it's becoming increasingly common in every scientific field: Those same people that you follow on Google Scholar - they will also be tweeting when they publish new cool stuff. Also when their friends publish new cool stuff. They might even highlight the most interesting aspect of their work through a figure, or a digestible blog post. Notice that whom you do \\*not\\* follow on Twitter is at least as important as whom you do follow. Optimize your twitter feed, so that you don't clutter away an excellent opportunity to find out about the most exciting and inspiring new developments in your field.  \n\n3\\. Organize the papers that come flying your way.\n--------------------------------------------------\n\nTo harness your daily literature upkeep for avoiding future literature reviews, be sure to organize the work you encounter. [Mendeley](http://www.mendeley.com/) is a useful, and mostly free, tool for doing this, but there are many of them. (Those of us who are old and remember typing out references letter by letter, will especially appreciate the convenience of a citation manager. It will never be a good use of your time to have to think about what part of a citation should be italicized.) Once the time comes to write your next proposal or paper, you have a treasure trove of results that you've already thought about, readily available at your fingertips.  \n\n4\\. Make it fun.\n----------------\n\nThis is the most important point. It's much easier to absorb information that you care about. One way that I hacked this in my current literature review is that I reached out to a number of authors that I had encountered in my literature search for an informal chat. This was one of the best experiences of scientific exchange I've ever had! I talked to scientists in Louisiana, Greece, and Australia, all of whom had several years more experience with my topic of interest than I did. Once you've heard a scientist tell you about their own paper - what part of the project was hard, what was interesting, what parts are only there because of reviewer request etc. - the content of their paper pops out at you in a much more colorful and living way. It's no longer such a chore to look at a paper when, instead of some dry black-and-white characters on a page, it's a friend's documentation of their work and thought process. That said, it's definitely a good idea to take a look at a paper \\*before\\* you reach out to its author. But having a scheduled call is also a good incentive to engage more deeply with your reading process.  \n\nThere. I hope that's somewhat helpful. Go forth, and enjoy your reading!"
    },
    "voteCount": 34
  },
  {
    "_id": "oPEWyxJjRo4oKHzMu",
    "url": null,
    "title": "The 3 Books Technique for Learning a New Skilll",
    "slug": "the-3-books-technique-for-learning-a-new-skilll",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "Scholarship & Learning"
      },
      {
        "name": "Practical"
      },
      {
        "name": "Skill Building"
      }
    ],
    "tableOfContents": {
      "sections": [
        {
          "title": "The \"What\" Book",
          "anchor": "The__What__Book",
          "level": 1
        },
        {
          "title": "The \"How\" Book",
          "anchor": "The__How__Book",
          "level": 1
        },
        {
          "title": "The \"Why\" Book",
          "anchor": "The__Why__Book",
          "level": 1
        },
        {
          "title": "The Project or Practice",
          "anchor": "The_Project_or_Practice",
          "level": 1
        },
        {
          "title": "Examples",
          "anchor": "Examples",
          "level": 1
        },
        {
          "title": "Overcoming Procrastination",
          "anchor": "Overcoming_Procrastination",
          "level": 2
        },
        {
          "title": "Learning Calculus",
          "anchor": "Learning_Calculus",
          "level": 2
        },
        {
          "title": "Conclusion",
          "anchor": "Conclusion",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        },
        {
          "anchor": "comments",
          "level": 0,
          "title": "48 comments"
        }
      ],
      "headingsCount": 10
    },
    "contents": {
      "markdown": "When I'm learning a new skill, there's a technique I often use to quickly gain the basics of the new skill without getting drowned in the plethora of resources that exist. I've found that just 3 resources that cover the skill from 3 separate viewpoints(along with either daily practice or a project) is enough to quickly get all the pieces I need to learn the new skill.\n\nI'm partial to books, so I've called this The 3 Books Technique, but feel free to substitute books for courses, mentors, or videos as needed.\n\n  \n\n![](http://mattgoldenberg.net/wp-content/uploads/2019/01/Pasted-Image.png)\n\nThe \"What\" Book\n===============\n\nThe \"What\" book is used as reference material. It should be a thorough resource that gives you a broad overview of your skill. If you run into a novel situation, you should be able to go to this book and get the information you need. It covers the \"surface\" section of the learning model from nature pictured above.\n\nPositive reviews of this book should contain phrases like \"Thorough\" and \"Got me out of a pinch more than once.\" Negative reviews of this book should talk about \"overwhelming\" and \"didn't know where to start.\"\n\nThe \"How\" Book\n==============\n\nThe \"How\" Book explains the step-by-step, nuts and bolts of how to put the skill into practice. It often contains processes, tools, and steps. It covers the \"deep\" part of the learning model covered above.\n\nPositive reviews of this book should talk about \"Well structured\" and \"Clearly thought out.\" Negative reviews should mention it being \"too rote\" or \"not enough theory.\"\n\nThe \"Why\" Book\n==============\n\nThe \"WHY\" book explains the mindset and intuitions behind the skill. It tries to get into the authors head and lets you understand what to do in novel situations. It should cover the \"transfer\" part of the learning model above.\n\nPositive reviews of this book should talk about \"gaining intuitions\" or \"really understanding\". Negative reviews should contain phrases like \"not practical\" or \"still don't know what steps to take.\"\n\nThe Project or Practice\n=======================\n\nOnce I have these 3 resources, I'll choose a single project or a daily practice that allows me to practice the skills from the \"How\" book and the mindsets from the \"Why\" book. If I get stuck, I'll use the \"What\" book to help me.\n\nExamples\n========\n\nOvercoming Procrastination\n--------------------------\n\n**\"What\" Book:** The Procrastination Equation by Piers Steel\n\n**\"How\" Book:** The Now Habit by Neil Fiore\n\n**\"Why\" Book**: The Replacing Guilt blog sequence by Nate Soares\n\n**Project or Practice:** Five pomodoros every day where I deliberately use the tools from the now habit and the mindsets from replacing guilt. If I find myself stuck, I'll choose from the plethora of techniques in the Procrastination Equation.\n\nLearning Calculus\n-----------------\n\n**\"What\" Book**: A First Course in Calculus by Serge Lange\n\n**\"How\" Book:** The Khan Academy series on Calculus\n\n**\"Why\" Book**: The Essence of Calculus Youtube series by 3blue1brown\n\n**Project or Practice:** Daily practice of the Khan Academy calculus exercises.\n\nConclusion\n==========\n\nThis is a simple technique that I've found very helpful in systematizing my learning process. I would be particularly interested in other skills you've learned and the 3 books you would recommend for those skills."
    },
    "voteCount": 130
  },
  {
    "_id": "d5NyJ2Lf6N22AD9PB",
    "url": null,
    "title": "Where to Draw the Boundary?",
    "slug": "where-to-draw-the-boundary",
    "author": "Eliezer_Yudkowsky",
    "question": false,
    "tags": [
      {
        "name": "Rationality"
      },
      {
        "name": "Philosophy of Language"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "The one comes to you and [says](/lw/nw/fallacies_of_compression/ia9):\n\n> Long have I pondered the meaning of the word \"Art\", and at last I've found what seems to me a satisfactory definition: \"Art is that which is designed for the purpose of creating a reaction in an audience.\"\n\n_Just because there's a word \"art\" doesn't mean that it **has a meaning,** floating out there in the void, which you can **discover** by finding the right definition.  \n_\n\nIt [feels that way](/lw/nq/feel_the_meaning/), but it is not so.\n\nWondering how to _define a word_ means you're looking at the problem the wrong way—searching for the mysterious essence of what is, in fact, a [communication signal](/lw/nr/the_argument_from_common_usage/).\n\nNow, there _is_ a real challenge which a rationalist may legitimately attack, but the challenge is not to find a satisfactory definition of a word.  The real challenge can be played as a single-player game, without speaking aloud.  The challenge is figuring out which things are similar to each other—which things are clustered together—and sometimes, which things have a common cause.\n\nIf you define \"eluctromugnetism\" to include lightning, include compasses, exclude light, and include Mesmer's \"animal magnetism\" (what we now call hypnosis), then you will have some trouble asking \"How does electromugnetism work?\"  You have lumped together things which do not belong together, and excluded others that would be needed to complete a set.  (This example is historically plausible; Mesmer came before Faraday.)\n\nWe could say that electromugnetism is a _wrong word,_ a boundary in [thingspace](/lw/nl/the_cluster_structure_of_thingspace/) that loops around and swerves through the clusters, a cut that fails to carve reality along its natural joints.\n\nFiguring where to cut reality in order to carve along the joints—_this_ is the problem worthy of a rationalist.  It is what people _should_ be trying to do, when they set out in search of the floating essence of a word.\n\nAnd make no mistake: it is a _scientific_ challenge to realize that you need a single word to describe [breathing and fire](/lw/hq/universal_fire/).  So do not think to consult the dictionary editors, for that is not their job.\n\nWhat is \"art\"?  But there is no essence of the word, floating in the void.\n\nPerhaps you come to me with a long list of the things that you call \"art\" and \"not art\":\n\n> The _Little Fugue in G Minor:_  Art.  \n> A punch in the nose:  Not art.  \n> Escher's _Relativity:_  Art.  \n> A flower:  Not art.  \n> The Python programming language:  Art.  \n> A cross floating in urine:  Not art.  \n> Jack Vance's _Tschai_ novels:  Art.  \n> Modern Art:  Not art.\n\nAnd you say to me:  \"It feels intuitive to me to draw this boundary, but I don't know why—can you find me an intension that matches this extension?  Can you give me a _simple_ description of this boundary?\"\n\nSo I reply:  \"I think it has to do with admiration of craftsmanship: work going in and wonder coming out.  What the included items have in common is the similar aesthetic emotions that they inspire, and the deliberate human effort that went into them with the intent of producing such an emotion.\"\n\nIs this helpful, or is it just [cheating](/lw/nv/replace_the_symbol_with_the_substance/) at [Taboo](/lw/nu/taboo_your_words/)?  I would argue that the list of which human emotions are or are not _aesthetic_ is far more compact than the list of everything that is or isn't art.  You might be able to see those emotions lighting up an fMRI scan—I say this by way of emphasizing that emotions are not ethereal.\n\nBut of course my definition of art is not the real point.  The real point is that you could well dispute either [the intension or the extension](/lw/nh/extensions_and_intensions/) of my definition.\n\nYou could say, \"Aesthetic emotion is _not_ what these things have in common; what they have in common is an intent to inspire _any_ complex emotion for the sake of inspiring it.\"  That would be disputing my intension, my attempt to draw a curve through the data points.  You would say, \"Your equation may roughly fit those points, but it is not the true generating distribution.\"\n\nOr you could dispute my extension by saying, \"Some of these things do belong together—I can see what you're getting at—but the Python language shouldn't be on the list, and Modern Art should be.\"  (This would mark you as a gullible philistine, but you could argue it.)  Here, the presumption is that there is indeed an underlying curve that generates this apparent list of similar and dissimilar things—that there is a rhyme and reason, _even though you haven't said yet where it comes from_—but I have unwittingly lost the rhythm and included some data points from a different generator.\n\nLong before you _know_ what it is that electricity and magnetism have in common, you might still suspect—based on surface appearances—that \"animal magnetism\" does not belong on the list.\n\nOnce upon a time it was thought that the word \"fish\" included dolphins.  Now you could play the oh-so-clever arguer, and say, \"The list:  {Salmon, guppies, sharks, dolphins, trout} is just a list—you can't say that a list is _wrong._  I can prove in set theory that this list exists.  So my definition of _fish_, which is simply this extensional list, cannot possibly be 'wrong' as you claim.\"\n\nOr you could stop playing nitwit games and admit that dolphins don't belong on the fish list.\n\nYou come up with a list of things that _feel_ similar, and take a guess at why this is so.  But when you finally discover what they _really_ have in common, it may turn out that your guess was wrong.  It may even turn out that your list was wrong.\n\nYou cannot hide behind a comforting shield of correct-by-definition.  Both extensional definitions and intensional definitions can be wrong, can fail to carve reality at the joints.\n\nCategorizing is a guessing endeavor, in which you can make mistakes; so it's wise to be able to admit, from a theoretical standpoint, that your definition-guesses can be \"mistaken\"."
    },
    "voteCount": 61
  },
  {
    "_id": "7nAxgQYGYrEY5ZCAD",
    "url": null,
    "title": "L-zombies! (L-zombies?)",
    "slug": "l-zombies-l-zombies",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "Zombies"
      },
      {
        "name": "Anthropics"
      },
      {
        "name": "Decision Theory"
      },
      {
        "name": "Consciousness"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "**Reply to:** Benja~2010~'s [Self-modification is the correct justification for updateless decision theory](/lw/22m/selfmodification_is_the_correct_justification_for/); Wei Dai's [Late great filter is not bad news](/lw/214/late_great_filter_is_not_bad_news/)\n\n_\"[P-zombie](http://wiki.lesswrong.com/wiki/Philosophical_zombie)\" is short for \"**p**hilosophical zombie\", but here I'm going to re-interpret it as standing for \"**p**hysical philosophical zombie\", and contrast it to what I call an \"l-zombie\", for \"**l**ogical philosophical zombie\"._\n\nA p-zombie is an ordinary human body with an ordinary human brain that does all the usual things that human brains do, such as the things that cause us to move our mouths and say \"I think, therefore I am\", but that _isn't conscious_. (The usual consensus on LW is that p-zombies can't exist, but some philosophers disagree.) The notion of p-zombie accepts that human _behavior_ is produced by physical, computable processes, but imagines that these physical processes don't produce _conscious experience_ without some additional epiphenomenal factor.\n\nAn l-zombie is a human being that could have existed, but doesn't: a Turing machine which, if anybody ever ran it, would compute that human's thought processes (and its interactions with a simulated environment); that would, if anybody ever ran it, compute the human saying \"I think, therefore I am\"; _but that never gets run_, and therefore isn't conscious. (If it's conscious anyway, it's not an l-zombie by this definition.) The notion of l-zombie accepts that human behavior is produced by computable processes, but supposes that these computational processes don't produce conscious experience without being physically instantiated.\n\nActually, there probably aren't any l-zombies: [The way the evidence is pointing](http://en.wikipedia.org/wiki/Multiverse#Max_Tegmark.27s_four_levels), it seems like we probably live in a spatially infinite universe where every physically possible human brain is instantiated _somewhere_, although some are instantiated less frequently than others; and if that's not true, there are the \"bubble universes\" arising from cosmological inflation, the branches of many-worlds quantum mechanics, and Tegmark's \"level IV\" multiverse of all mathematical structures, all suggesting again that all possible human brains are in fact instantiated. But (a) I don't think that even with all that evidence, we can be _overwhelmingly_ certain that all brains are instantiated; and, more importantly actually, (b) I think that thinking about l-zombies can yield some useful insights into how to think about worlds where all humans exist, but some of them have more measure (\"magical reality fluid\") than others.\n\nSo I ask: Suppose that we do indeed live in a world with l-zombies, where only some of all mathematically possible humans exist physically, and only those that do have conscious experiences. How should someone living in such a world reason about their experiences, and how should they make decisions — keeping in mind that if they were an l-zombie, they would still say \"_I_ have conscious experiences, so clearly _I_ can't be an l-zombie\"?\n\nIf we _can't_ update on our experiences to conclude that someone having these experiences must exist in the physical world, then we must of course conclude that we are almost certainly l-zombies: After all, if the physical universe isn't combinatorially large, the vast majority of mathematically possible conscious human experiences are _not_ instantiated. You might argue that the universe you live in seems to run on relatively simple physical rules, so it should have high prior probability; but we haven't really figured out the exact rules of our universe, and although what we understand seems compatible with the hypothesis that there are simple underlying rules, that's not really proof that there _are_ such underlying rules, if \"the _real_ universe has simple rules, but _we_ are l-zombies living in some random simulation with a hodgepodge of rules (that isn't actually ran)\" has the same prior probability; and worse, if you don't have all we _do_ know about these rules loaded into your brain right now, you can't really verify that they make sense, since there is some mathematically possible simulation whose initial state has you _remember_ seeing evidence that such simple rules exist, even if they don't; and much worse still, even if there _are_ such simple rules, what evidence do you have that if these rules were actually executed, they would produce _you_? Only the fact that you, like, exist, but we're asking what happens if we don't let you update on that.\n\nI find myself quite unwilling to accept this conclusion that I shouldn't update, in the world we're talking about. I mean, I _actually_ have conscious experiences. I, like, feel them and stuff! Yes, true, my slightly altered alter ego would reason the same way, and it would be wrong; but _I'm_ right...\n\n...and _that_ actually seems to offer a way out of the conundrum: Suppose that I decide to update on my experience. Then so will my alter ego, the l-zombie. This leads to a lot of l-zombies concluding \"I think, therefore I am\", and being _wrong_, and a lot of actual people concluding \"I think, therefore I am\", and being _right_. All the thoughts that are actually _consciously experienced_ are, in fact, correct. This doesn't seem like such a terrible outcome. Therefore, I'm willing to provisionally endorse the reasoning \"I think, therefore I am\", and to endorse updating on the fact that I have conscious experiences to draw inferences about physical reality — taking into account the simulation argument, of course, and conditioning on living in a small universe, which is all I'm discussing in this post.\n\n_**NB.**_ There's still something quite uncomfortable about the idea that all of my behavior, including the fact that I say \"I think therefore I am\", is explained by the mathematical process, but actually _being_ conscious requires some extra magical reality fluid. So I still feel confused, and using the word l-zombie in analogy to p-zombie is a way of highlighting that. But this line of reasoning still feels like progress. FWIW.\n\nBut if _that's_ how we justify _believing_ that we physically exist, that has some implications for how we should decide what to _do_. The argument is that nothing very bad happens if the l-zombies wrongly conclude that they actually exist. Mostly, that also seems to be true if they _act_ on that belief: mostly, what l-zombies do doesn't seem to influence what happens in the real world, so if only things that actually happen are morally important, it doesn't seem to matter what the l-zombies decide to do. But there are exceptions.\n\nConsider the [counterfactual mugging](http://wiki.lesswrong.com/wiki/Counterfactual_mugging): Accurate and trustworthy [Omega](http://wiki.lesswrong.com/wiki/Omega) appears to you and explains that it just has thrown a very biased coin that had only a 1/1000 chance of landing heads. As it turns out, this coin _has_ in fact landed heads, and now Omega is offering you a choice: It can either (A) create a Friendly AI or (B) destroy humanity. Which would you like? There is a catch, though: Before it threw the coin, Omega made a prediction about what you would do if the coin fell heads (and it was able to make a confident prediction about what you would choose). If the coin had fallen tails, it would have created an FAI if it has predicted that you'd choose (B), and it would have destroyed humanity if it has predicted that you would choose (A). (If it hadn't been able to make a confident prediction about what you would choose, it would just have destroyed humanity outright.)\n\nThere is a clear argument that, if you expect to find yourself in a situation like this in the future, you would want to self-modify into somebody who would choose (B), since this gives humanity a much larger chance of survival. Thus, a decision theory stable under self-modification would answer (B). But if you update on the fact that you consciously experience Omega telling you that the coin landed heads, (A) would seem to be the better choice!\n\nOne way of looking at this is that if the coin falls tails, the l-zombie that is told the coin landed heads still exists mathematically, and this l-zombie now has the power to influence what happens in the real world. If the argument for updating was that nothing bad happens even though the l-zombies get it wrong, well, that argument breaks here. The mathematical process that is your mind doesn't have any evidence about whether the coin landed heads or tails, because as a mathematical object it exists in both possible worlds, and it has to make a decision in both worlds, and that decision affects humanity's future in both worlds.\n\nBack in 2010, I wrote [a post](/lw/22m/selfmodification_is_the_correct_justification_for/) arguing that yes, you would want to self-modify into something that would choose (B), but that that was the _only_ reason why you'd want to choose (B). Here's a variation on the above scenario that illustrates the point I was trying to make back then: Suppose that Omega tells you that it actually threw its coin a million years ago, and if it had fallen tails, it would have turned Alpha Centauri purple. Now throughout your history, the argument goes, you would never have had any motive to self-modify into something that chooses (B) in this particular scenario, because you've always known that Alpha Centauri isn't, in fact, purple.\n\nBut this argument assumes that you know you're not a l-zombie; if the coin had in fact fallen tails, you wouldn't exist as a conscious being, but you'd still exist as a mathematical decision-making process, and that process would be able to influence the real world, so you-the-decision-process can't reason that \"I think, therefore I am, therefore the coin must have fallen heads, therefore I should choose (A).\" Partly because of this, I now accept choosing (B) as the (most likely to be) correct choice even in that case. (The rest of my change in opinion has to do with all ways of making my earlier intuition formal getting into trouble in decision problems where you can influence whether you're brought into existence, but that's a topic for another post.)\n\nHowever, should you _feel cheerful_ while you're announcing your choice of (B), since with high (prior) probability, you've just saved humanity? That would lead to an actual conscious being feeling cheerful if the coin has landed heads and humanity is going to be destroyed, and an l-zombie computing, but _not_ actually experiencing, cheerfulness if the coin has landed heads and humanity is going to be saved. Nothing good comes out of feeling cheerful, not even alignment of a conscious' being's map with the physical territory. So I think the correct thing is to choose (B), and to be deeply sad about it.\n\nYou may be asking why I should care what the right probabilities to assign or the right feelings to have are, since these don't seem to play any role in making decisions; sometimes you make your decisions as if updating on your conscious experience, but sometimes you don't, and you always get the right answer if you _don't_ update in the first place. Indeed, I expect that the \"correct\" design for an AI is to fundamentally use (more precisely: approximate) updateless decision theory (though I also expect that probabilities updated on the AI's sensory input will be useful for many intermediate computations), and \"I compute, therefore I am\"-style reasoning will play no fundamental role in the AI. And I think the same is true for humans' decisions — the correct way to act is given by updateless reasoning. But as a human, I find myself unsatisfied by not being able to have a picture of what the physical world probably looks like. I may not need one to figure out how I should act; I still want one, not for instrumental reasons, but because I want one. In a small universe where most mathematically possible humans are l-zombies, the argument in this post seems to give me a justification to say \"I think, therefore I am, therefore probably I either live in a simulation or what I've learned about the laws of physics describes how the real world works (even though there are many l-zombies who are thinking similar thoughts but are wrong about them).\"\n\nAnd because of this, even though I disagree with my 2010 post, I also still disagree with [Wei Dai's 2010 post arguing that a late Great Filter is good news](/lw/214/late_great_filter_is_not_bad_news/), which my own 2010 post was trying to argue against. Wei argued that if Omega gave you a choice between (A) destroying the world _now_ and (B) having Omega destroy the world a million years ago (so that you are never instantiated as a conscious being, though your choice as an l-zombie still influences the real world), then you would choose (A), to give humanity at least the time it's had so far. Wei concluded that this means that if you learned that the Great Filter is in our future, rather than our past, that must be _good news_, since if you could choose where to place the filter, you should place it in the future. I now agree with Wei that (A) is the right choice, but I don't think that you should be _happy_ about it. And similarly, I don't think you should be happy about news that tells you that the Great Filter is later than you might have expected."
    },
    "voteCount": 34
  },
  {
    "_id": "hc9Eg6erp6hk9bWhn",
    "url": null,
    "title": "The Quantum Physics Sequence",
    "slug": "the-quantum-physics-sequence",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "Physics"
      },
      {
        "name": "List of Links"
      }
    ],
    "tableOfContents": {
      "sections": [
        {
          "title": "Basic Quantum Mechanics:",
          "anchor": "Basic_Quantum_Mechanics_",
          "level": 1
        },
        {
          "title": "Many Worlds:",
          "anchor": "Many_Worlds_",
          "level": 1
        },
        {
          "title": "Timeless Physics:",
          "anchor": "Timeless_Physics_",
          "level": 1
        },
        {
          "title": "Rationality and Science:",
          "anchor": "Rationality_and_Science_",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        },
        {
          "anchor": "comments",
          "level": 0,
          "title": "26 comments"
        }
      ],
      "headingsCount": 6
    },
    "contents": {
      "markdown": "This is an inclusive guide to the series of posts on quantum mechanics that began on [April 9th, 2008](http://www.overcomingbias.com/2008/04/page/5/), including the digressions into related topics (such as the difference between Science and Bayesianism) and some of the preliminary reading.\n\nYou may also be interested in one of the less inclusive post guides, such as:\n\n*   **[An Intuitive Explanation of Quantum Mechanics](/lw/r6/an_intuitive_explanation_of_quantum_mechanics/)** (just the science, for students confused by their physics textbooks)\n*   **[Quantum Physics Revealed As Non-Mysterious](/lw/r7/quantum_physics_revealed_as_nonmysterious/)** (quantum physics does not make the universe any more mysterious than it was previously)\n*   **[And the Winner is... Many-Worlds!](/lw/r8/and_the_winner_is_manyworlds/)** (the many-worlds interpretation _wins outright_ given the current state of evidence)\n*   [**Quantum Mechanics and Personal Identity**](/lw/r9/quantum_mechanics_and_personal_identity/) (the ontology of quantum mechanics, in which there are no particles with individual identities, rules out theories of personal continuity that invoke \"the same atoms\" as a concept)\n\nMy current plan calls for the quantum physics series to eventually be turned into one or more [e-books](http://yudkowsky.net/subscribe.html).\n\n**Preliminaries**:\n\n*   [Probability is in the Mind](/lw/oj/probability_is_in_the_mind/):  If you are uncertain about a phenomenon, this is a fact about your state of mind, not a fact about the phenomenon itself.  There are mysterious questions but not mysterious answers.  The map is not the territory.\n*   **[Reductionism](/lw/on/reductionism/)**:  We build _models_ of the universe that have many different levels of description.  But so far as anyone has been able to determine, the _universe itself_ has only the single level of fundamental physics - reality doesn't explicitly compute protons, only quarks.\n*   [Joy in the Merely Real](/lw/or/joy_in_the_merely_real/):  If you can't take joy in things that turn out to be explicable, you're going to set yourself up for eternal disappointment.  Don't worry if quantum physics turns out to be normal.\n*   [Zombies! Zombies?](/lw/p7/zombies_zombies/) and **[The Generalized Anti-Zombie Principle](/lw/p9/the_generalized_antizombie_principle/)**:  Don't try to put your consciousness or your personal identity outside physics.  Whatever makes you say \"I think therefore I am\", causes your lips to move; it is within the chains of cause and effect that produce our observed universe.\n*   **[Belief in the Implied Invisible](/lw/pb/belief_in_the_implied_invisible/)**:  If a spaceship goes over the cosmological horizon relative to us, so that it can no longer communicate with us, should we believe that the spaceship instantly ceases to exist?\n\n**Basic Quantum Mechanics:**\n\n*   **[Quantum Explanations](/lw/pc/quantum_explanations/)**:  Quantum mechanics doesn't deserve its fearsome reputation.  If you tell people something is supposed to be mysterious, they won't understand it.  It's human intuitions that are \"strange\" or \"weird\"; physics itself is perfectly normal.  Talking about historical erroneous concepts like \"particles\" or \"waves\" is just asking to confuse people; present the real, unified quantum physics straight out.  The series will take a strictly realist perspective - quantum equations describe something that is real and out there.  Warning:  Although a large faction of physicists agrees with this, it is not universally accepted.  Stronger warning:  I am not even going to present non-realist viewpoints until later, because I think this is a major source of confusion.\n*   **[Configurations and Amplitude](/lw/pd/configurations_and_amplitude/)**:  A preliminary glimpse at the stuff reality is made of.  The classic split-photon experiment with half-silvered mirrors.  Alternative pathways the photon can take, can cancel each other out.  The mysterious measuring tool that tells us the relative squared moduli.\n*   **[Joint Configurations](/lw/pe/joint_configurations/)**:  The laws of physics are inherently over mathematical entities, configurations, that involve multiple particles.  A basic, ontologically existent entity, according to our current understanding of quantum mechanics, does not look like a photon - it looks like a configuration of the universe with \"A photon here, a photon there.\"  Amplitude flows between these configurations can cancel or add; this gives us a way to detect which configurations are distinct.  It is an _experimentally testable_ fact that \"Photon 1 here, photon 2 there\" is the _same configuration_ as \"Photon 2 here, photon 1 there\".\n*   **[Distinct Configurations](/lw/pf/distinct_configurations/)**:  Since configurations are over the combined state of all the elements in a system, adding a sensor that detects whether a particle went one way or the other, becomes a new element of the system that can make configurations \"distinct\" instead of \"identical\".  This confused the living daylights out of early quantum experimenters, because it meant that things behaved differently when they tried to \"measure\" them.  But it's not only measuring instruments that do the trick - any sensitive physical element will do - and the distinctness of configurations is a physical fact, not a fact about our knowledge.  There is no need to suppose that the universe cares what we think.\n*   [Where Philosophy Meets Science](/lw/pg/where_philosophy_meets_science/):  In retrospect, supposing that quantum physics had anything to do with consciousness was a big mistake.  Could philosophers have told the physicists so?  But we don't usually see philosophers sponsoring major advances in physics; why not?\n*   **[Can You Prove Two Particles Are Identical?](/lw/ph/can_you_prove_two_particles_are_identical/)**:  You wouldn't think that it would be possible to do an experiment that told you that two particles are _completely_ identical - not just to the limit of experimental precision, but _perfectly._  You could even give a precise-sounding philosophical argument for _why_ it was not possible - but the argument would have a deeply buried assumption.  Quantum physics violates this deep assumption, making the experiment easy.\n*   **[Classical Configuration Spaces](/lw/pi/classical_configuration_spaces/)**:  How to visualize the state of a system of two 1-dimensional particles, as a single point in 2-dimensional space.  A preliminary step before moving into...\n*   **[The Quantum Arena](/lw/pj/the_quantum_arena/)**:  Instead of a system state being associated with a single point in a classical configuration space, the instantaneous real state of a quantum system is a complex amplitude distribution over a quantum configuration space.  What creates the illusion of \"individual particles\", like an electron caught in a trap, is a plaid distribution - one that happens to factor into the product of two parts.  It is the whole distribution that evolves when a quantum system evolves.  Individual configurations don't have physics; amplitude distributions have physics.  Quantum entanglement is the general case; quantum _independence_ is the special case.\n*   **[Feynman Paths](/lw/pk/feynman_paths/)**:  Instead of thinking that a photon takes a single straight path through space, we can regard it as taking all possible paths through space, and adding the amplitudes for every possible path.  Nearly all the paths cancel out - unless we do clever quantum things, so that some paths add instead of canceling out.  Then we can make light do funny tricks for us, like reflecting off a mirror in such a way that the angle of incidence doesn't equal the angle of reflection.  But ordinarily, nearly all the paths except an extremely narrow band, cancel out - this is one of the keys to recovering the hallucination of classical physics.\n*   **[No Individual Particles](/lw/pl/no_individual_particles/)**:  One of the chief ways to confuse yourself while thinking about quantum mechanics, is to think as if photons were little billiard balls bouncing around.  The appearance of little billiard balls is a special case of a deeper level on which there are only multiparticle configurations and amplitude flows.  It is easy to set up physical situations in which there exists no fact of the matter as to which electron was originally which.\n*   [Identity Isn't In Specific Atoms](/lw/pm/identity_isnt_in_specific_atoms/), [Three Dialogues on Identity](/lw/po/three_dialogues_on_identity/):  Given that there's no such thing as \"the same atom\", whether you are \"the same person\" from one time to another can't possibly depend on whether you're made out of the same atoms.\n*   [**Decoherence**](/lw/pp/decoherence/):  A quantum system that factorizes can evolve into a system that doesn't factorize, destroying the illusion of independence.  But entangling a quantum system with its environment, can _appear_ to destroy entanglements that are already present.  Entanglement with the environment can separate out the pieces of an amplitude distribution, preventing them from interacting with each other.  Decoherence is fundamentally symmetric in time, but appears asymmetric because of the second law of thermodynamics.\n*   [**The So-Called Heisenberg Uncertainty Principle**](/lw/pq/the_socalled_heisenberg_uncertainty_principle/):  Unlike classical physics, in quantum physics it is not possible to separate out a particle's \"position\" from its \"momentum\".  The evolution of the amplitude distribution over time, involves things like taking the second derivative in space and multiplying by _i_ to get the first derivative in time.  The end result of this time evolution rule is that blobs of particle-presence appear to race around in physical space.  The notion of \"an exact particular momentum\" or \"an exact particular position\" is not something that can physically happen, it is a tool for analyzing amplitude distributions by taking them apart into a sum of simpler waves.  This uses the assumption and fact of linearity: the evolution of the whole wavefunction seems to always be the additive sum of the evolution of its pieces.  Using this tool, we can see that if you take apart the same distribution into a sum of positions and a sum of momenta, they cannot both be sharply concentrated at the same time.  When you \"observe\" a particle's position, that is, decohere its positional distribution by making it interact with a sensor, you take its wave packet apart into two pieces; then the two pieces evolve differently.  The Heisenberg Principle definitely does not say that knowing about the particle, or consciously seeing it, will make the universe behave differently.\n*   [Which Basis Is More Fundamental?](/lw/pr/which_basis_is_more_fundamental/):  The position basis can be computed locally in the configuration space; the momentum basis is not local.  Why care about locality?  Because it is a very deep principle; reality itself seems to favor it in some way.\n*   [Where Physics Meets Experience](/lw/ps/where_physics_meets_experience/):  Meet the Ebborians, who reproduce by fission.  The Ebborian brain is like a thick sheet of paper that splits down its thickness.  They frequently experience dividing into two minds, and can talk to their other selves.  It seems that their unified theory of physics is almost finished, and can answer every question, when one Ebborian asks:  When _exactly_ does one Ebborian become two people?\n*   [Where Experience Confuses Physicists](/lw/pt/where_experience_confuses_physicists/):  It then turns out that the entire planet of Ebbore is splitting along a fourth-dimensional thickness, duplicating all the people within it.  But why does the apparent chance of \"ending up\" in one of those worlds, equal the square of the fourth-dimensional thickness?  Many mysterious answers are proposed to this question, and one non-mysterious one.\n*   [**On Being Decoherent**](/lw/pu/on_being_decoherent/):  When a sensor measures a particle whose amplitude distribution stretches over space - perhaps seeing if the particle is to the left or right of some dividing line - then the standard laws of quantum mechanics call for the sensor+particle system to evolve into a state of (particle left, sensor measures LEFT) + (particle right, sensor measures RIGHT).  But when we humans look at the sensor, it only seems to say \"LEFT\" or \"RIGHT\", never a mixture like \"LIGFT\".  This, of course, is because we ourselves are made of particles, and subject to the standard quantum laws that imply decoherence.  Under standard quantum laws, the final state is (particle left, sensor measures LEFT, human sees \"LEFT\") + (particle right, sensor measures RIGHT, human sees \"RIGHT\").\n*   [The Conscious Sorites Paradox](/lw/pv/the_conscious_sorites_paradox/):  Decoherence is implicit in quantum physics, not an extra law on top of it.  Asking exactly when \"one world\" splits into \"two worlds\" may be like asking when, if you keep removing grains of sand from a pile, it stops being a \"heap\".  Even if you're inside the world, there may not be a definite answer.  This puzzle does not arise only in quantum physics; the Ebborians could face it in a classical universe, or we could build sentient flat computers that split down their thickness.  Is this really a physicist's problem?\n*   [Decoherece is Pointless](/lw/pw/decoherence_is_pointless/):  There is no exact point at which decoherence suddenly happens.  All of quantum mechanics is continuous and differentiable, and decoherent processes are no exception to this.\n*   [Decoherent Essences](/lw/px/decoherent_essences/):  Decoherence is implicit within physics, not an extra law on top of it.  You can choose representations that make decoherence harder to see, just like you can choose representations that make apples harder to see, but exactly the same physical process still goes on; the apple doesn't disappear and neither does decoherence.  If you could make decoherence magically go away by choosing the right representation, we wouldn't need to shield quantum computers from the environment.\n*   [**The Born Probabilities**](/lw/py/the_born_probabilities/):  The last _serious_ mysterious question left in quantum physics:  When a quantum world splits in two, why do we seem to have a greater probability of ending up in the larger blob, exactly proportional to the integral of the squared modulus?  It's an open problem, but non-mysterious answers have been proposed.  Try not to go funny in the head about it.\n*   **[Decoherence as Projection](/lw/pz/decoherence_as_projection/)**:  Since quantum evolution is linear and unitary, decoherence can be seen as projecting a wavefunction onto orthogonal subspaces.  This can be neatly illustrated using polarized photons and the angle of the polarized sheet that will absorb or transmit them.\n*   **[Entangled Photons](/lw/q0/entangled_photons/)**:  Using our newly acquired understanding of photon polarizations, we see how to construct a quantum state of two photons in which, when you measure one of them, the person in the same world as you, will always find that the opposite photon has opposite quantum state.  This is not because any influence is transmitted; it is just decoherence that takes place in a very symmetrical way, as can readily be observed in our calculations.\n\n**Many Worlds:**\n\n(At this point in the sequence, most of the mathematical background has been built up, and we are ready to evaluate interpretations of quantum mechanics.)\n\n*   [**Bell's Theorem: No EPR \"Reality\"**](/lw/q1/bells_theorem_no_epr_reality/):  (Note:  This post was designed to be read as a stand-alone, if desired.)  Originally, the discoverers of quantum physics thought they had discovered an incomplete description of reality - that there was some deeper physical process they were missing, and this was why they couldn't predict exactly the results of quantum experiments.  The math of Bell's Theorem is surprisingly simple, and we walk through it.  Bell's Theorem rules out being able to _locally_ predict a _single, unique_ outcome of measurements - ruling out a way that Einstein, Podolsky, and Rosen once defined \"reality\".  This shows how deep implicit philosophical assumptions can go.  If worlds can split, so that there is no single unique outcome, then Bell's Theorem is no problem.  Bell's Theorem does, however, rule out the idea that quantum physics describes our partial knowledge of a deeper physical state that could locally produce single outcomes - any such description will be inconsistent.\n*   [**Spooky Action at a Distance: The No-Communication Theorem**](/lw/q2/spooky_action_at_a_distance_the_nocommunication/):  As Einstein argued long ago, the quantum physics of his era - that is, the single-global-world interpretation of quantum physics, in which experiments have single unique random results - violates Special Relativity; it imposes a preferred space of simultaneity and requires a mysterious influence to be transmitted faster than light; which mysterious influence can never be used to transmit any useful information.  Getting rid of the single global world dispels this mystery and puts everything back to normal again.\n*   **[Decoherence is Simple](/lw/q3/decoherence_is_simple/)**, **[Decoherence is Falsifiable and Testable](/lw/q4/decoherence_is_falsifiable_and_testable/)**:  (Note:  Designed to be standalone readable.)  An epistle to the physicists.  To probability theorists, words like \"simple\", \"falsifiable\", and \"testable\" have exact mathematical meanings, which are there for very strong reasons.  The (minority?) faction of physicists who say that many-worlds is \"not falsifiable\" or that it \"violates Occam's Razor\" or that it is \"untestable\", are committing the same kind of mathematical crime as non-physicists who invent their own theories of gravity that go as inverse-cube.  This is one of the reasons why I, a non-physicist, dared to talk about physics - because I saw (some!) physicists using probability theory in a way that was simply wrong.  Not just criticizable, but outright mathematically wrong:  2 + 2 = 3.\n*   **[Quantum Non-Realism](/lw/q5/quantum_nonrealism/)**:  \"Shut up and calculate\" is the best approach you can take when none of your theories are very good.  But that is not the same as claiming that \"Shut up!\" actually _is_ a theory of physics.  Saying \"I don't know what these equations mean, but they seem to work\" is a very different matter from saying:  \"These equations definitely don't mean anything, they just work!\"\n*   **[Collapse Postulates](/lw/q6/collapse_postulates/)**:  Early physicists simply didn't think of the possibility of more than one world - it just didn't occur to them, even though it's the straightforward result of applying the quantum laws at all levels.  So they accidentally invented a completely and strictly unnecessary part of quantum theory to ensure there was only one world - a law of physics that says that parts of the wavefunction mysteriously and spontaneously disappear when decoherence prevents us from seeing them any more.  If such a law really existed, it would be the only non-linear, non-unitary, non-differentiable, non-local, non-CPT-symmetric, acausal, faster-than-light phenomenon in all of physics.\n*   [If Many-Worlds Had Come First](/lw/q7/if_manyworlds_had_come_first/):  If early physicists had never made the mistake, and thought immediately to apply the quantum laws at all levels to produce macroscopic decoherence, then \"collapse postulates\" would today seem like a completely crackpot theory.  In addition to their other problems, like FTL, the collapse postulate would be the only physical law that was informally specified - often in dualistic (mentalistic) terms - because it was the only fundamental law adopted without precise evidence to nail it down.  Here, we get a glimpse at that alternate Earth.\n*   **[Many Worlds, One Best Guess](/lw/q8/many_worlds_one_best_guess/)**:  Summarizes the arguments that nail down macroscopic decoherence, aka the \"many-worlds interpretation\".  Concludes that many-worlds _wins outright_ given the current state of evidence.  The argument should have been over fifty years ago.  New physical evidence could reopen it, but we have no particular reason to expect this.\n*   [Living in Many Worlds](/lw/qz/living_in_many_worlds/):  The many worlds of quantum mechanics are not some strange, alien universe into which you have been thrust.  They are where you have always lived.  Egan's Law:  \"It all adds up to normality.\"  Then why care about quantum physics at all?  Because there's still the question of _what_ adds up to normality, and the answer to this question turns out to be, \"Quantum physics.\"  If you're thinking of building any strange philosophies around many-worlds, you probably shouldn't - that's not what it's for.\n\n**Timeless Physics:**\n\n(Now we depart from what is nailed down in standard physics, and enter into more speculative realms - particularly Julian Barbour's Machian timeless physics.)\n\n*   **[Mach's Principle: Anti-Epiphenomenal Physics](/lw/qm/machs_principle_antiepiphenomenal_physics/)**:  Could you tell if the whole universe were shifted an inch to the left?  Could you tell if the whole universe was traveling left at ten miles per hour?  Could you tell if the whole universe was _accelerating_ left at ten miles per hour?  Could you tell if the whole universe was rotating?\n*   [Relative Configuration Space](/lw/qo/relative_configuration_space/):  Maybe the reason why we can't observe absolute speeds, absolute positions, absolute accelerations, or absolute rotations, is that particles don't _have_ absolute positions - only positions relative to each other.  That is, maybe quantum physics takes place in a _relative_ configuration space.\n*   **[Timeless Physics](/lw/qp/timeless_physics/)**:  What time is it?  How do you know?  The question \"What time is it right now?\" may make around as much sense as asking \"Where is the universe?\"  Not only that, our physics equations may not need a _t_ in them!\n*   [Timeless Beauty](/lw/qq/timeless_beauty/):  To get rid of time you must reduce it to nontime.  In timeless physics, everything that exists is perfectly global or perfectly local.  The laws of physics are perfectly global; the configuration space is perfectly local.  Every fundamentally existent ontological entity has a unique identity and a unique value.  This beauty makes ugly theories much more visibly ugly; a collapse postulate becomes a visible scar on the perfection.\n*   **[Timeless Causality](/lw/qr/timeless_causality/)**:  Using the modern, Bayesian formulation of causality, we can define causality without talking about time - define it purely in terms of relations.  The river of time never flows, but it has a direction.\n*   **[Timeless Identity](/lw/qx/timeless_identity/)**:  How can you be the same person tomorrow as today, in the river that never flows, when not a drop of water is shared between one time and another?  Having used physics to completely trash all naive theories of identity, we reassemble a conception of persons and experiences from what is left.  With a surprising practical application...\n*   [Thou Art Physics](/lw/r0/thou_art_physics/):  If the laws of physics control everything we do, then how can our choices be meaningful?  Because _you are_ physics.  You aren't _competing_ with physics for control of the universe, you are _within_ physics.  Anything _you_ control is _necessarily_ controlled by physics.\n*   [Timeless Control](/lw/r1/timeless_control/):  We throw away \"time\" but retain causality, and with it, the concepts \"control\" and \"decide\".  To talk of something as having been \"always determined\" is mixing up a timeless and a timeful conclusion, with paradoxical results.  When you take a perspective outside time, you have to be careful not to let your old, timeful intuitions run wild in the absence of their subject matter.\n\n**Rationality and Science:**\n\n(Okay, so it was many-worlds all along and collapse theories are silly.  Did first-half-of-20th-century physicists _really_ screw up _that_ badly?  How did they go wrong?  Why haven't modern physicists unanimously endorsed many-worlds, if the issue is that clear-cut?  What lessons can we learn from this whole debacle?)\n\n*   **[The Failures of Eld Science](/lw/q9/the_failures_of_eld_science/)**:  A short story set in the same world as [Initiation Ceremony](/lw/p1/initiation_ceremony/).  Future physics students look back on the cautionary tale of quantum physics.\n*   [The Dilemma: Science or Bayes?](/lw/qa/the_dilemma_science_or_bayes/):  The failure of first-half-of-20th-century-physics was not due to _straying_ from the scientific method.  Science and rationality - that is, Science and Bayesianism - aren't the same thing, and sometimes they give different answers.\n*   **[Science Doesn't Trust Your Rationality](/lw/qb/science_doesnt_trust_your_rationality/)**:  The reason Science doesn't always agree with the exact, Bayesian, rational answer, is that Science doesn't _trust_ you to be rational.  It wants you to go out and gather overwhelming experimental evidence.\n*   [When Science Can't Help](/lw/qc/when_science_cant_help/):  If you have an idea, Science tells you to test it experimentally.  If you spend 10 years testing the idea and the result comes out negative, Science slaps you on the back and says, \"Better luck next time.\"  If you want to spend 10 years testing a hypothesis that will actually turn out to be _right,_ you'll have to try to do the thing that Science doesn't trust you to do: think rationally, and figure out the answer _before_ you get clubbed over the head with it.\n*   [Science Isn't Strict _Enough_](/lw/qd/science_isnt_strict_enough/):  Science lets you believe any damn stupid idea that hasn't been refuted by experiment.  Bayesianism says there is always an exactly rational degree of belief given your current evidence, and this does not shift a nanometer to the left or to the right depending on your whims.  Science is a social freedom - we let people test whatever hypotheses they like, because we don't trust the village elders to decide in advance - but you shouldn't confuse that with an individual standard of rationality.\n*   [Do Scientists Already Know This Stuff?](/lw/qe/do_scientists_already_know_this_stuff/):  No.  Maybe someday it will be part of standard scientific training, but for now, it's not, and the absence is visible.\n*   **[No Safe Defense, Not Even Science](/lw/qf/no_safe_defense_not_even_science/)**:  Why am I trying to break your trust in Science?  Because you can't think and trust at the same time.  The social rules of Science are verbal rather than quantitative; it is possible to believe you are following them.  With Bayesianism, it is never possible to do an exact calculation and get the exact rational answer that you know exists.  You are _visibly_ less than perfect, and so you will not be tempted to trust yourself.\n*   [Changing the Definition of Science](/lw/qg/changing_the_definition_of_science/):  Many of these ideas are surprisingly conventional, and being floated around by other thinkers.  I'm a good deal less of a lonely iconoclast than I seem; maybe it's just the way I talk.\n*   [Faster Than Science](/lw/qi/faster_than_science/):  Is it really possible to arrive at the truth _faster_ than Science does?  Not only is it possible, but the social process of science relies on scientists doing so - when they choose which hypotheses to test.  In many answer spaces it's not possible to find the true hypothesis by accident.  Science leaves it up to experiment to _socially_ declare who was right, but if there weren't _some_ people who could get it right in the absence of overwhelming experimental proof, science would be stuck.\n*   **[Einstein's Speed](/lw/qj/einsteins_speed/)**:  Albert was unusually good at finding the right theory in the presence of only a small amount of experimental evidence.  Even more unusually, he admitted it - he claimed to know the theory was right, even in advance of the public proof.  It's possible to arrive at the truth by thinking great high-minded thoughts of the sort that Science does not trust you to think, but it's a _lot harder_ than arriving at the truth in the presence of overwhelming evidence.\n*   **[That Alien Message](/lw/qk/that_alien_message/)**:  Einstein used evidence more efficiently than other physicists, but he was still extremely inefficient in an _absolute_ sense.  If a huge team of cryptographers and physicists were examining a interstellar transmission, going over it bit by bit, we could deduce principles on the order of Galilean gravity just from seeing one or two frames of a picture.  As if the very first human to see an apple fall, had, on the instant, realized that its position went as the square of the time and that this implied constant acceleration.\n*   [My Childhood Role Model](/lw/ql/my_childhood_role_model/):  I looked up to the ideal of a Bayesian superintelligence, not Einstein.\n*   **[Einstein's Superpowers](/lw/qs/einsteins_superpowers/)**:  There's an unfortunate tendency to talk as if Einstein had superpowers - as if, even before Einstein was famous, he had an inherent disposition to be Einstein - a potential as rare as his fame and as magical as his deeds.  Yet the way you acquire superpowers is not by being born with them, but by seeing, with a sudden shock, that they are perfectly normal.\n*   [Class Project](/lw/qt/class_project/):  From the world of _Initiation Ceremony._  Brennan and the others are faced with their midterm exams.\n*   [Why Quantum?](/lw/qy/why_quantum/):  Why do a series on quantum mechanics?  Some of the many morals that are best illustrated by the tale of quantum mechanics and its misinterpretation."
    },
    "voteCount": 46
  },
  {
    "_id": "Zupr296Zy74wpihXT",
    "url": null,
    "title": "Use Your Identity Carefully",
    "slug": "use-your-identity-carefully",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "Identity"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "In [Keep Your Identity Small](http://www.paulgraham.com/identity.html), Paul Graham argues against associating yourself with labels (i.e. “libertarian,” “feminist,” “gamer,” “American”) because labels constrain what you’ll let yourself believe. It’s a wonderful essay that’s led me to make concrete changes in my life. That said, it’s only about 90% correct. I have two issues with Graham’s argument; one is a semantic quibble, but it leads into the bigger issue, which is a tactic I’ve used to become a better person.\n\nGraham talks about the importance of identity in determining beliefs. This isn’t quite the right framework. I’m a fanatical consequentialist, so I care what actions people take. Beliefs can constrain actions, but identity can also constrain actions directly.\n\nTo give a trivial example from the past week in which beliefs didn’t matter: I had a self-image as someone who didn’t wear jeans or t-shirts. As it happens, there are times when wearing jeans is completely fine, and when other people wore jeans in casual settings, I knew it was appropriate. Nevertheless, I wasn’t able to act on this belief because of my identity. (I finally realized this was silly, consciously discarded that useless bit of identity, and made a point of wearing jeans to a social event.)\n\nWhy is this distinction important? If we’re looking at identify from an action-centered framework, this recommends a different approach from Graham’s.\n\nDo you want to constrain your beliefs? No; you want to go wherever the evidence pushes you. “If X is true, I desire to believe that X is true. If X is not true, I desire to believe that X is not true.” Identity will only get in the way.\n\nDo you want to constrain your actions? Yes! Ten thousand times yes! [Akrasia](http://wiki.lesswrong.com/wiki/Akrasia) exists. [Commitment devices](http://en.wikipedia.org/wiki/Commitment_device) are useful. [Beeminder](https://www.beeminder.com/) is successful. Identity is one of the most effective tools for the job, if you wield it deliberately.\n\nI’ve cultivated an identity as a person who makes events happen. It took months to instill, but now, when I think “I wish people were doing X,” I instinctively start putting together a group to do X. This manifests in minor ways, like the tree-climbing expedition I put together at the Effective Altruism Summit, and in big ways, like the megameetup we held in Boston. If I hadn’t used my identity to motivate myself, neither of those things would’ve happened, and my life would be poorer.\n\nIdentity is powerful. Powerful things are dangerous, like backhoes and bandsaws. People use them anyway, because sometimes they’re the best tools for the job, and because safety precautions can minimize the danger.\n\nIdentity is hard to change. Identity can be difficult to notice. Identity has unintended consequences. Use this tool only after careful deliberation. What would this identity do to your actions? What would it do to your beliefs? What social consequences would it have? Can you do the same thing with a less dangerous tool? Think twice, and then think again, before you add to your identity. Most identities are a hindrance.\n\nBut please, don’t discard this tool just because some things might go wrong. If you are willful, and careful, and wise, then you can cultivate the identity of the person you always wanted to be."
    },
    "voteCount": 101
  },
  {
    "_id": "BXQsZmubkovJ76Ldo",
    "url": null,
    "title": "The Actionable Version of \"Keep Your Identity Small\"",
    "slug": "the-actionable-version-of-keep-your-identity-small",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "Identity"
      }
    ],
    "tableOfContents": {
      "sections": [
        {
          "title": "Identity Menagerie",
          "anchor": "Identity_Menagerie",
          "level": 1
        },
        {
          "title": "Self-Concept",
          "anchor": "Self_Concept",
          "level": 2
        },
        {
          "title": "Group Identity",
          "anchor": "Group_Identity",
          "level": 2
        },
        {
          "title": "Intelligent Social Web",
          "anchor": "Intelligent_Social_Web",
          "level": 2
        },
        {
          "title": "Identity as a Strategy for meeting your needs",
          "anchor": "Identity_as_a_Strategy_for_meeting_your_needs",
          "level": 1
        },
        {
          "title": "KYIS is not actionable advice",
          "anchor": "KYIS_is_not_actionable_advice",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        },
        {
          "anchor": "comments",
          "level": 0,
          "title": "14 comments"
        }
      ],
      "headingsCount": 8
    },
    "contents": {
      "markdown": "(cross posted on my [roam](https://roamresearch.com/) [blog](https://roamresearch.com/#/app/hazard_blog/page/hYLCprCMM))\n\nThere's an old Paul Graham Essay, [\"Keep Your Identity Small\"](http://www.paulgraham.com/identity.html). It's short so it's worth it to read the whole thing right now if you've never seen it. The yisbifiefyb (\"yeah it's short but i'm functionally illiterate except for your blog\") is roughly \"When something becomes part of your identity, you become dumber. Don't make things part of your identity.\"\n\nI read that post some time in high school and thought, \"Of course! You're so right Paul Graham. Cool, now I'll never identify as anything.\" I still think that Paul Graham is pointing out a real cluster of Things That Happen With People, but over time the concept of _identity_, and _identifying as BLANK_ have started to feel less clear. It feels right to say \"People get dumb when their identity is challenged\" and it even feels sorta axiomatic. Isn't that what it means for something to be part of your identity? Thinking about it more I came up with a bunch of different ways of thinking of myself that all felt like _identifying as BLANK_, but it felt like unnecessary dropping of nuance to smoosh them all into the single concept of _identity_.\n\nIdentity Menagerie\n==================\n\nLets look at some examples of what identifying as a BLANK can look like:\n\n*   Blake: \"I do Cross Fit. \"\n*   Jane: \"I'm smart. In fact I'm normally among the smartest in the room. I'm able to solve a lot of problems by just finding a clever solution to them instead of having to get stuck in grunt work. People often show awe and appreciation for my depth and breadth of knowledge.\"\n*   Jay: \"I the peacekeeper, the one always holding the group together.\"\n\nSelf-Concept\n------------\n\nSteve Andreas outlines the idea of a self-concept quite nicely:\n\n> Your self-concept is a sort of map of who you are. Like any other map, it is always a very simplified version of the territory. \\[...\\] Your self-concept, your \"map\" you have of yourself, has the same purpose as a map of a city—to keep you oriented in the world and help you find your way, particularly when events are challenging or difficult.\n\nThe thing you'll notice is it's nigh impossible to avoid having a self-concept. When Jane thinks of herself and how she can act on the world, \"being smart\" is a chunk of self-concept that summarizes a lot of her experiences and that she uses to guide decisions she makes.\n\nKaj Sotala has a good [post](https://kajsotala.fi/2017/07/how-i-found-fixed-the-root-problem-behind-my-depression-and-anxiety-after-20-years/) about how tweaking and modifying his self-concept helped fix parts of his depression and anxiety.\n\nGroup Identity\n--------------\n\nThis is the obvious one that we're all used to. Blake does Cross Fit, hangs out with cross fit people all the time, and loves telling people about all this. All of his Cross Fit buddies support each other and give each other praise for being part of such an awesome group. Someone calling Cross Fit stupid would feel like someone calling him and all of his friends stupid. It would be big and difficult change for Blake to get out of Cross Fit, given that's where most of his social circle is, and where all his free time goes.\n\nIntelligent Social Web\n----------------------\n\nHere's Val describing what he calls the [Intelligent Social Web](https://www.lesswrong.com/posts/AqbWna2S85pFTsHH4/the-intelligent-social-web):\n\n> I suspect that improv works because we’re doing something a lot like it pretty much all the time. The web of social relationships we’re embedded in helps define our roles as it forms and includes us. And that same web, as the distributed “director” of the “scene”, guides us in what we do. A lot of (but not all) people get a strong hit of this when they go back to visit their family. If you move away and then make new friends and sort of become a new person (!), you might at first think this is just who you are now. But then you visit your parents… and suddenly you feel and act a lot like you did before you moved away. You might even try to hold onto this “new you” with them… and they might respond to what they see as [strange behavior](https://en.wikipedia.org/wiki/Deviance_(sociology)) by trying to nudge you into acting “normal”: ignoring surprising things you say, changing the topic to something familiar, starting an old fight, etc.\n\nThis feels like another important facet of identity, one that doesn't just exist in your head, but in the heads of those around you.\n\nIdentity as a Strategy for meeting your needs\n=============================================\n\nIn middle school and high school I built up a very particular identity. I bet if you conversed with high school me, you wouldn't be able to pin me down to using any particular phrase, label, or group to identify myself as. And yet, there are ways of being you could have asked me to try that would have _scared the shit out of me_. Almost as if... my identity was under attack....\n\nSo new take, one I consider more productive. Reread Paul Grahams essay and replace every instance of \"identity\" with \"main strategy to meet one's needs\". Hmmmm, it's starting to click. If you've been a preacher for 40 years, and all you know is preaching, and most of your needs are met by your church community, an attack on the church is an attack on _your livelihood and well-being._\n\nI expect having your \"identity\" under attack to feel similar to being a hunter gatherer and watching the only river that you've known in your life drying up. Fear and Panic. What are you going to do know? Will you survive? Where are the good things in your life going to come from?\n\nWhen you frame it like this, you can see how easily trying to KYIS could lead to stuff that just hurts you. If I only have one way of getting people to like me (say, being funny), I can't just suddenly decide not to care if people don't consider me funny. I can't just suddenly not care if people stop laughing at my jokes. Both of those events mean I no longer have a functional strategy to be liked.\n\nA very concrete prediction of this type of thinking: someone will be clingy and protective over a part of their behavior to the degree that it is the sole source of meeting XYZ important needs.\n\nKYIS is not actionable advice\n=============================\n\nThe take away from Paul Graham is \"don't let something become you identity\". How do you do that? I thought it meant something like \"Never self identity as a BLANK\", to others or to yourself. Boom. Done. And yet, even though I never talked about being part of one group or another, I still went through life a decent chunk of life banking on \"Be funny, act unflappable, be competent at the basic stuff\" as the only/main strategy for meeting my needs.\n\nThe actionable advice might be something like, \"slowly develop a multi-faceted confidence in your ability to handle what life throws at you, via actually improving and seeing results.\" That's waaaaaay harder to do than just not identifying with a group, but it does a better jump of pointing you in the direction that matters. I expect that when Paul Graham wrote that essay he already had a pretty strong confidence in his ability to meet his needs. From that vantage point, you can easily let go of identities, because they aren't your life lines.\n\nThere can be much more to identity than what I've laid out, but I think the redirect I've given is one that is a great first step for anyone dwelling on identity, or for anyone who head the KYIS advice and earnestly tried to implement it, yet found mysterious ways it wasn't working."
    },
    "voteCount": 33
  },
  {
    "_id": "TLKPj4GDXetZuPDH5",
    "url": null,
    "title": "Making History Available",
    "slug": "making-history-available",
    "author": "Eliezer Yudkowsky",
    "question": false,
    "tags": [
      {
        "name": "History"
      },
      {
        "name": "Rationality"
      },
      {
        "name": "Generalization From Fictional Evidence"
      },
      {
        "name": "Availability Heuristic"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "There is a habit of thought which I [call](http://intelligence.org/files/CognitiveBiases.pdf) the *logical fallacy of generalization from fictional evidence*. Journalists who, for example, talk about the *Terminator* movies in a report on AI, do not usually treat *Terminator* as a prophecy or fixed truth. But the movie is recalled—is available—as if it were an illustrative historical case. As if the journalist had seen it happen on some other planet, so that it might well happen here.\n\nThere is an inverse error to generalizing from fictional evidence: failing to be sufficiently moved by *historical* evidence. The trouble with generalizing from fictional evidence is that it is fiction—it never actually happened. It’s not drawn from the same distribution as this, our real universe; [fiction differs from reality in systematic ways](http://www.overcomingbias.com/2007/07/tell-your-anti-.html). But history *has* happened, and *should* be available.\n\nIn our ancestral environment, there were no movies; what you saw with your own eyes was true. Is it any wonder that fictions we see in lifelike moving pictures have too great an impact on us? Conversely, things that *really happened*, we encounter as ink on paper; they happened, but we never *saw* them happen. We don’t remember them happening to us.\n\nThe inverse error is to treat history as mere story, process it with the same part of your mind that handles the novels you read. You may say with your lips that it is “truth,” rather than “fiction,” but that doesn’t mean you are being moved as much as you should be. Many biases involve being insufficiently moved by [dry, abstract information](https://www.lesswrong.com/rationality/scope-insensitivity).\n\nWhen I finally realized whose shoes I was standing in, after having given a Mysterious Answer to a mysterious question, there was a sudden shock of unexpected connection with the past. I realized that the invention and destruction of vitalism—which I had only read about in books—had *actually happened to real people*, who experienced it much the same way I experienced the invention and destruction of my own mysterious answer. And I also realized that if I had actually *experienced* the past—if I had lived through past scientific revolutions myself, rather than reading about them in history books—I probably would *not* have made the same mistake again. I would not have come up with *another* mysterious answer; the first thousand lessons would have hammered home the moral.\n\nSo (I thought), to feel sufficiently the force of history, I should try to approximate the thoughts of an Eliezer who *had* lived through history—I should try to think as if everything I read about in history books had actually happened to me.^1^ I should immerse myself in history, imagine *living* through eras I only saw as ink on paper.\n\nWhy should I remember the Wright Brothers’ first flight? I was not there. But as a rationalist, could I dare to *not* remember, when the event actually happened? Is there so much difference between seeing an event through your eyes—which is actually a causal chain involving reflected photons, not a direct connection—and seeing an event through a history book? Photons and history books both descend by causal chains from the event itself.\n\nI had to overcome the false amnesia of being born at a particular time. I had to recall—make available— *all* the memories, not just the memories which, by mere coincidence, belonged to myself and my own era.\n\nThe Earth became older, of a sudden.\n\nTo my former memory, the United States had always existed—there was never a time when there was no United States. I had not remembered, until that time, how the Roman Empire rose, and brought peace and order, and lasted through so many centuries, until I forgot that things had ever been otherwise; and yet the Empire fell, and barbarians overran my city, and the learning that I had possessed was lost. The modern world became more fragile to my eyes; it was not the first modern world.\n\nSo many mistakes, made over and over and *over* again, because I did not remember making them, in every era I never lived . . .\n\nAnd to think, people sometimes wonder if overcoming bias is important.\n\nDon’t you remember how many times your biases have killed you? You don’t? I’ve noticed that sudden amnesia often follows a fatal mistake. But take it from me, it happened. I remember; I wasn’t there.\n\nSo the next time you doubt the strangeness of the future, remember how you were born in a hunter-gatherer tribe ten thousand years ago, when no one knew of Science at all. Remember how you were shocked, to the depths of your being, when Science explained the great and terrible sacred mysteries that you once revered so highly. Remember how you once believed that you could fly by eating the right mushrooms, and then you accepted with disappointment that you would never fly, and then you flew. Remember how you had always thought that slavery was right and proper, and then you changed your mind. [Don’t imagine how you](http://lesswrong.com/lw/il/hindsight_bias/) *could* have predicted the change, for that is amnesia. *Remember* that, in fact, you did not guess. Remember how, century after century, the world changed in ways you did not guess.\n\nMaybe then you will be less shocked by what happens next.\n\n* * *\n\n^1^ With appropriate reweighting for the availability bias of history books—I should remember being a thousand peasants for every ruler."
    },
    "voteCount": 111
  },
  {
    "_id": "TPytnFcWiD2E4cTrm",
    "url": "https://rootsofprogress.org/why-did-we-wait-so-long-for-the-bicycle",
    "title": "Why did we wait so long for the bicycle?",
    "slug": "why-did-we-wait-so-long-for-the-bicycle",
    "author": "Ruby",
    "question": false,
    "tags": [
      {
        "name": "Progress Studies"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "_h/t [alyssavance](https://www.lesswrong.com/users/alyssavance)_\n\n![](https://rootsofprogress.org/img/the-american-velocipede.jpg)\n\n> The bicycle, as we know it today, was not invented until the late 1800s. Yet it was a simple mechanical invention. It would seem to require no brilliant inventive insight, and certainly no scientific background.\n\n> Why, then, wasn’t it invented much earlier?\n\n> I asked this question [on Twitter](https://twitter.com/jasoncrawford/status/1143622870477791233), and read some discussion [on Quora](https://www.quora.com/How-come-bicycles-werent-invented-earlier-in-history-What-was-missing). People proposed many hypotheses, including:\n\n> **\\+ Technology factors.** Metalworking improved a lot in the 1800s: we got improved iron refining and eventually cheap steel, better processes for shaping metal, and ability to make parts like hollow tubes. Wheel technology improved: [wire-spoke](https://en.wikipedia.org/wiki/Wire_wheel) (aka tension-spoked) wheels replaced heavier designs; vulcanized rubber (1839) was needed for tires; inflatable tires weren’t invented until 1887. Chains, gears, and ball bearings are all crucial parts that require advanced manufacturing techniques for precision and cost.\n\n> **\\+ Design iteration.** Early bicycles were inconvenient and dangerous. The first version didn’t even have pedals. Some versions didn’t have steering, and could only be turned by leaning. (!) The famous “penny-farthing” design, with its huge front wheel, made it impossible to balance with your feet, was prone to tipping forward on a hard stop, and generally left the rider high in the air, all of which increased risk of injury. It took decades of iteration to get to a successful bicycle model.\n\n> **\\+ Quality of roads.** Roads in the 1800s and earlier were terrible by modern standards. Roads were often dirt, rutted from the passage of many carts, turning muddy in the rain. [Macadam paving](https://en.wikipedia.org/wiki/Macadam), which gave smooth surfaces to roads, wasn’t invented until about 1820. City roads at the time were paved with cobblestones, which were good for horses but too bumpy for bicycles. (The unevenness was apparently a feature, assisting in the runoff of sewage—leading [one Quora answer](https://www.quora.com/How-come-bicycles-werent-invented-earlier-in-history-What-was-missing/answer/Chris-Crawford-6) to claim that the construction of city sewers was what opened the door to bicycles.)\n\n> **\\+ Competition from horses.** Horses were a common and accepted mode of transportation at the time. They could deal with all kinds of roads. They could carry heavy loads. Who then needs a bicycle? In this connection, it has been claimed that the bicycle was invented in response to food shortages due to the “[Year without a Summer](https://en.wikipedia.org/wiki/Year_Without_a_Summer)”, an 1816 weather event caused by the volcanic explosion of Mt. Tambora the year earlier, which darkened skies and lowered temperatures in many parts of the world. The agricultural crisis caused horses as well as people to starve, which led to some horses being slaughtered for food, and made the remaining ones more expensive to feed. This could have motivated the search for alternatives.\n\n> **\\+ General economic growth.** Multiple commenters pointed out the need for a middle class to provide demand for such an invention. If all you have are a lot of poor peasants and a few aristocrats (who, by the way, have horses, carriages, and drivers), there isn’t much of a market for bicycles. This is more plausible when you realize that bicycles were more of a hobby for entertainment before they became a practical means of transportation.\n\n> **\\+ Cultural factors.** Maybe there was just a general lack of interest in useful mechanical inventions until a certain point in history? But when did this change, and why?\n\n> These are all good hypotheses. But some of them start to buckle under pressure:\n\n> The quality of roads is relevant, but not really the answer. Bicycles can be ridden on dirt roads or sidewalks (although the latter led to run-ins with pedestrians and made bicycles unpopular among the public at first). And historically, roads didn’t improve until _after_bicycles became common—indeed it seems that it was in part the cyclists who called for the improvement of roads.\n\n> I don’t think horses explain it either. A bicycle, from what I’ve read, was cheaper to buy than a horse, and it was certainly cheaper to maintain (if nothing else, you don’t have to feed a bicycle). And it turns out that inventors were interested in the problem of human-powered vehicles, dispensing with the need for horses, for a long time before the modern bicycle. Even Karl von Drais, who invented the first two-wheeled human-powered vehicle after the Year without a Summer, had been working on the problem for years before that.\n\n> Technology factors are more convincing to me. They may have been necessary for bicycles to become practical and cheap enough to take off. But they weren’t needed for early experimentation. Frames can be built of wood. Wheels can be rimmed with metal. Gears can be omitted. Chains can be replaced with belts; some early designs even used treadles instead of pedals, and at least one design drove the wheels with levers, as on a steam locomotive.\n\n> So what’s the real explanation?\n\n([Continue reading](https://rootsofprogress.org/why-did-we-wait-so-long-for-the-bicycle). 2,184 words and lots of great bicycle pictures.)\n\nThis post is a single piece from Jason Crawford's project, [The Roots of Progress](https://rootsofprogress.org/about)_,_ aptly named, to understand the nature and causes of human progress. I haven't thought deeply enough to check his research, but it's a fascinating project. This essay examines a specific piece of technology, but the case study is used to develop and support models of what it takes for progress to occur."
    },
    "voteCount": 19
  },
  {
    "_id": "x5ASTMPKPowLKpLpZ",
    "url": null,
    "title": "Moloch's Toolbox (1/2)",
    "slug": "moloch-s-toolbox-1-2",
    "author": "Eliezer Yudkowsky",
    "question": false,
    "tags": [
      {
        "name": "Moloch"
      },
      {
        "name": "Efficient Market Hypothesis"
      },
      {
        "name": "Expertise (topic)"
      },
      {
        "name": "dath ilan"
      }
    ],
    "tableOfContents": {
      "sections": [
        {
          "title": "i. For want of docosahexaenoic acids, a baby was lost",
          "anchor": "i__For_want_of_docosahexaenoic_acids__a_baby_was_lost",
          "level": 1
        },
        {
          "title": "ii. Asymmetric information and lemons problems",
          "anchor": "ii__Asymmetric_information_and_lemons_problems",
          "level": 1
        },
        {
          "title": "iii. Academic incentives and beneficiaries",
          "anchor": "iii__Academic_incentives_and_beneficiaries",
          "level": 1
        },
        {
          "title": "iv. Two-factor markets and signaling equilibria",
          "anchor": "iv__Two_factor_markets_and_signaling_equilibria",
          "level": 1
        },
        {
          "title": "v. Total market failures",
          "anchor": "v__Total_market_failures",
          "level": 1
        },
        {
          "title": "vi. Absence of (meta-)competition",
          "anchor": "vi__Absence_of__meta__competition",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        },
        {
          "anchor": "comments",
          "level": 0,
          "title": "70 comments"
        }
      ],
      "headingsCount": 8
    },
    "contents": {
      "markdown": "**Follow-up to:** [An Equilibrium of No Free Energy](https://www.lesserwrong.com/posts/yPLr2tnXbiFXkMWvk/an-equilibrium-of-no-free-energy)\n\n* * *\n\nThere’s a toolbox of reusable concepts for analyzing systems I would call “inadequate”—the causes of civilizational failure, _some_ of which correspond to local opportunities to do better yourself. I shall, somewhat arbitrarily, sort these concepts into three larger categories:\n\n1.  Decisionmakers who are not beneficiaries;\n\n2.  Asymmetric information;\n\nand above all,\n\n3.  Nash equilibria that aren’t even the best Nash equilibrium, let alone Pareto-optimal.\n\nIn other words:\n\n1.  Cases where the decision lies in the hands of people who would gain little personally, or lose out personally, if they did what was necessary to help someone else;\n\n2.  Cases where decision-makers can’t reliably learn the information they need to make decisions, even though someone else has that information; and\n\n3.  Systems that are broken in multiple places so that no one actor can make them better, even though, in principle, some magically _coordinated_ action could move to a new stable state.\n\nI will then play fast and loose with these concepts in order to fit the entire Taxonomy of Failure inside them.\n\nFor example, “irrationality in the form of cognitive biases” wouldn’t _obviously_ fit into any of these categories, but I’m going to shove it inside “asymmetric information” via a clever sleight-of-hand. Ready? Here goes:\n\nIf _nobody_ can detect a cognitive bias in particular cases, then from our perspective we can’t really call it a “civilizational inadequacy” or “failure to pluck a low-hanging fruit.” We shouldn’t even be able to see it ourselves. So, on the contrary, let’s suppose that you and some other people can indeed detect a cognitive bias that’s screwing up civilizational decisionmaking.\n\nThen why don’t you just walk up to the decision-maker and _tell_ them about the bias? Because they wouldn’t have any way of knowing to trust _you_ rather than the other five hundred people trying to influence their decisions? Well, in that case, you’re holding information that they can’t learn from you! So that’s an “asymmetric information problem,” in much the same way that it’s an asymmetric information problem when you’re trying to sell a used car and _you_ know it doesn’t have any mechanical problems, but you have no way of reliably conveying this knowledge to the buyer because for all they know you could be lying.\n\nThat argument is a bit silly, but so is the notion of trying to fit the whole Scroll of Woe into three supercategories. And if I named more than three supercategories, you wouldn’t be able to remember them due to computational limitations (which aren’t on the list anywhere, and I’m not going to add them).\n\ni. For want of docosahexaenoic acids, a baby was lost\n-----------------------------------------------------\n\nMy discussion of modest epistemology in [Chapter 1](https://www.lesserwrong.com/posts/zsG9yKcriht2doRhM/inadequacy-and-modesty) might have given the impression that I think of modesty mostly as a certain set of high-level beliefs: beliefs about how best to combat cognitive bias, about how individual competencies stack up against group-level competencies, and so on. But I predict that many of this book’s readers have high-level beliefs similar to those I outlined in [Chapter 2](https://www.lesserwrong.com/posts/yPLr2tnXbiFXkMWvk/an-equilibrium-of-no-free-energy), while employing a reasoning style that is really a special case of modest epistemology; and I think that this reasoning style is causing them substantial harm.\n\nAs reasoning styles, modest epistemology and inadequacy analysis depend on a mix of explicit principles and implicit mental habits. In inadequacy analysis, it’s one thing to recognize in the abstract that we live in a world rife with systemic inefficiencies, and quite another to naturally _perceive_ systems that way in daily life. So my goal here won't be to unkindly stick the label “inadequate” to a black box containing the world; it will be to say something about how the relevant systems actually operate.\n\nFor our central example, we’ll be using the United States medical system, which is, so far as I know, the most broken system _that still works_ ever recorded in human history. If you were reading about something in 19th-century France which was as broken as US healthcare, you wouldn’t expect to find that it went on working when overloaded with a sufficiently vast amount of money. You would expect it to just not work at all.\n\nIn previous years, I would use the case of central-line infections as my go-to example of medical inadequacy. Central-line infections, in the US alone, killed 60,000 patients per year, and infected an additional 200,000 patients at an average treatment cost of $50,000/patient.\n\nCentral-line infections were also known to decrease by 50% or more if you enforced a five-item checklist that included items like “wash your hands before touching the line.”\n\nRobin Hanson has old _Overcoming Bias_ blog posts on that untaken, low-hanging fruit. But I discovered while re-Googling in 2015 that wider adoption of hand-washing and similar precautions are now finally beginning to occur, after many years—with an associated 43% _nationwide_ decrease in central-line infections. After _partial_ adoption.^[1](#footnote-1-definition)^\n\nSo my new example is infants suffering liver damage, brain damage, and death in a way that’s even easier to solve, by changing the lipid distribution of parenteral nutrition to match the proportions in breast milk.\n\nBackground: Some babies have digestion problems that require direct intravenous feeding. Long ago, somebody created a hospital formula for this intravenous feeding that matched the distribution of “fat,” “protein,” and “carbohydrate” in breast milk.\n\nJust like “protein” comes in different amino acids, some of which the body can’t make on its own and some of which it can, what early doctors used to think of as “fat” actually breaks down into metabolically distinct elements like short-chain triglycerides, medium-chain triglycerides, saturated fat, and omega-6, omega-9, and the famous “omega-3.” “Omega-3” is actually several different lipids in its own right; vegetable oils with “omega-3” usually just contain alpha-linolenic acids, which can only be inefficiently converted to ecosapentaenoic acids, which are then even more inefficiently converted to docosahexaenoic acids, which are the actual key structural components in the body. This conversion pathway is rate-limited by a process that also converts omega-6, so too much omega-6 can prevent you from processing ALA into DHA even if you’re getting ALA.\n\nSo what happens if your infant nutrition was initially designed based on the concept of “fat” as a natural category, and all the “fat” in the mix comes from soybean oil?\n\nFrom a popular book by Jaminet and Jaminet:\n\n> Some babies are born with “short bowel syndrome” and need to be given parenteral nutrition, or nutrition delivered intravenously directly to the blood, until their digestive tracts grow and heal. Since 1961, parenteral nutrition has used soybean oil as its source of fat.\\[[6](http://www.ncbi.nlm.nih.gov/pubmed/16804134?dopt=AbstractPlus)\\] And for decades, babies on parenteral nutrition have suffered devastating liver and brain damage. The death rate on soybean oil is 30 percent by age four. \\[…\\]\n> \n> In a clinical trial, of forty-two babies given fish oil \\[after they had already developed liver damage on soybean oil\\], three died and one required a liver transplant; of forty-nine given soybean oil, twelve died and six required a liver transplant.\\[[8](http://www.ncbi.nlm.nih.gov/pubmed/19661785?dopt=AbstractPlus)\\] The death-or-liver-transplant rate was reduced from 37 percent with soybean oil to 9 percent with fish oil.^[2](#footnote-2-definition)^\n\nWhen Jaminet and Jaminet wrote the above, in 2012, there was a single hospital in the United States that could provide correctly formulated parenteral nutrition, namely the Boston Children’s Hospital; nowhere else. This formulation was illegal to sell across state lines.\n\nA few years _after_ the Boston Children’s Hospital developed their formula—keeping in mind the heap of dead babies continuing to pile up in the meanwhile—there developed a shortage of “certified lipids” (FDA-approved “fat” for adding to parenteral nutrition). For a year or two, the parenteral nutrition contained _no fat at all_ which is _worse_ and can kill _adults_.\n\nYou see, although there’s nothing special about the soybean oil in parenteral nutrition, there was only one US manufacturer approved to add it, and that manufacturer left the market, so…\n\nAs of 2015, the state of affairs was as follows: The FDA eventually solved the problem with the shortage of US-certified lipids, by… allowing US hospitals to import parenteral nutrition bags from Europe. And it only took them two years’ worth of dead patients to figure that out!\n\nAs of 2016, if your baby has short bowel syndrome, and has _already_ ended up with liver damage, and either you or your doctor is lucky enough to know what’s wrong and how to fix it, your doctor can apply for a special permit to use a non-FDA-approved substance for your child on an emergency basis. After this, you can buy Omegaven and hope that it cures your baby and that there isn’t too much permanent damage and that it’s not already too late.\n\nThis is an improvement over the prior situation, where the non-poisonous formulation was illegal to sell across state lines under any circumstances, but it’s still not _good_ by any stretch of the imagination.\n\nNow imagine trying to explain to a visitor from a relatively well-functioning world just why it is that your civilization has killed a bunch of babies and subjected other babies to pointless brain damage.\n\n“It’s not that we’re _evil,_” you say helplessly, “it’s that… well, you see, it’s not that anyone _wanted_ to kill those babies, it’s just the way the System ended up, somehow…”\n\nii. Asymmetric information and lemons problems\n----------------------------------------------\n\n_Three people have gathered in a blank white space:_\n\n*   The visitor from a Better World;\n*   simplicio , who is attending a major university but hasn’t taken undergraduate economics;\n*   cecie, the Conventional Cynical Economist.\n\n_The Visitor speaks first._\n\nvisitor:  So I’ve listened to you explain about babies suffering death and brain damage from parenteral nutrition built on soybean oil. I have several questions here, but I’ll start with the most obvious one.\n\ncecie:   Go ahead.\n\nvisitor:  Why aren’t there riots?\n\nsimplicio:  The first thing you have to understand, Visitor, is that the folk in this world are hypocrites, cowards, psychopaths, and sheep.\n\nI mean, _I_ certainly care about the lives of newborn children. Hearing about their plight certainly makes _me_ want to do something about it. When I see the problem continuing in spite of that, I can only conclude that other people _don’t_ feel the level of moral indignation that I feel when staring at a heap of dead babies.\n\ncecie:  I don’t think that hypothesis is needed, Simplicio. As a start, Visitor, you have to realize that the picture I’ve shown you is not widely known. Maybe 10% of the population, at most, is walking around with the prior belief that the FDA in general is killing people; our government runs on majority rule and the 10% can’t unilaterally defy it.^[3](#footnote-3-definition)^ Maybe 0.1% of that 10% know that omega-3 ALA is converted into omega-3 DHA via a metabolic pathway that competes with omega-6. And then most of those aren’t aware of what’s happening to babies right now.\n\nvisitor:  Pointing to that state of ignorance is hardly a sufficient explanation! If a theater is on fire and only one person knows it, they yell “Fire!” and then more people know it. People from my civilization would scream “Babies are dying over here!” and other people from my civilization would whip around their heads and look.\n\nsimplicio:  Our world’s cowards and sheep would hear that and think that it’s (a) somebody else’s problem and (b) all part of the plan.\n\ncecie:  In our world, Visitor, we have an economic phenomenon sometimes called the lemons problem. Suppose you want to sell a used car, and I’m looking for a car to buy. From my perspective, I have to worry that your car might be a “lemon”—that it has a serious mechanical problem that doesn’t appear every time you start the car, and is difficult or impossible to fix. Now, _you_ know that your car isn’t a lemon. But if I ask you, “Hey, is this car a lemon?” and you answer “No,” I can’t trust your answer, because you’re incentivized to answer “No” either way. Hearing you say “No” isn’t much Bayesian evidence. _Asymmetric information_ conditions can persist even in cases where, like an honest seller meeting an honest buyer, both parties have strong incentives for accurate information to be conveyed.\n\nA further problem is that if the fair value of a non-lemon car is $10,000, and the possibility that your car is a lemon causes me to only be willing to pay you $8,000, you might refuse to sell your car. So the honest sellers with reliable cars start to leave the market, which further shifts upward the probability that any given car for sale is a lemon, which makes me less willing to pay for a used car, which incentivizes more honest sellers to leave the market, and so on.\n\nvisitor:  What does the lemons problem have to do with your world’s inability to pass around information about dead babies?\n\ncecie:  In our world, there are a lot of people screaming, “Pay attention to this thing I’m indignant about over here!” In fact, there are enough people screaming that there’s an inexploitable market in indignation. The dead-babies problem can’t compete in that market; there’s no free energy left for it to eat, and it doesn’t have an optimal indignation profile. There’s no single individual villain. The business about competing omega-3 and omega-6 metabolic pathways is something that only a fraction of people would understand on a visceral level; and even if those people posted it to their Facebook walls, most of their readers wouldn’t understand and repost, so the dead-babies problem has relatively little virality. Being indignant about this particular thing doesn’t signal your moral superiority to anyone else in particular, so it’s not viscerally enjoyable to engage in the indignation. As for adding a further scream, “But wait, this matter _really is_ important!”, that’s the part subject to the lemons problem. Even people who honestly know about a fixable case of dead babies can’t emit a _trustworthy_ request for attention.\n\nsimplicio:  You're saying that people won’t listen even if I sound _really_ indignant about this? That’s an outrage!\n\ncecie:  By this point in our civilization’s development, many honest buyers and sellers have left the indignation market entirely; and what’s left behind is not, on average, good.\n\nvisitor:  Your reply contains so many surprising postulates of weird civilizational dysfunction, I hardly know what to ask about next. So instead I’ll try to explain how my world works, and you can explain to me why your world doesn’t work that way.\n\ncecie:  Sounds reasonable.\n\niii. Academic incentives and beneficiaries\n------------------------------------------\n\nvisitor:  To start with, in my world, we have these people called “scientists” who verify claims experimentally, and other people trust the “scientists.” So if our “scientists” say that a certain formula seems to be killing babies, this would provoke general indignation without every single listener needing to study docohexa-whatever acids.\n\nsimplicio:  Alas, our so-called scientists are just pawns of the same medical-industrial complex that profits from killing babies.\n\ncecie:  I’m afraid, Visitor, that although there are strong prior reasons to expect too much omega-6 and no omega-3 to be very bad for an infant baby, and there are now a few dozen small-scale studies which seem to match that prediction, this matter hasn’t had the massive study that would begin to produce confident scientific agreement—\n\nvisitor:  You’d better not be pointing to _that_ as an exogenous fact that explains your civilization’s problem! See, on my planet, if somebody points to _strong prior suspicion_ combined with _confirming pilot studies_ saying that something is killing innocent babies and is fixable, and the pilot studies are not considered sufficient evidence to settle the issue, our people would _do more studies_ and wouldn’t just go on blindly feeding the babies poison in the meantime. Our scientists would all agree on _that_!\n\ncecie:  But people loudly agreeing on something, by itself, accomplishes nothing. It’s all well and good for everyone to agree in principle that larger studies ought to be done; but in your world, who actually does the big study, and why do they do it?\n\nvisitor:  Two subclasses within the profession of “scientist” are _suggesters,_ whose piloting studies provide the initial suspicions of effects, and _replicators_ whose job it is to confirm the result and nail things down solidly—the exact effect size and so on. When an important suggestive result arises, two replicators step forward to confirm it and nail down the exact conditions for producing it, being forbidden upon their honor to communicate with each other until they submit their findings. If both replicators agree on the particulars, that completes the discovery. The three funding bodies that sustained the suggester and the dual replicators would receive the three places of honor in the announcement. Do I need to explain how part of the function of any civilized society is to appropriately reward those who contribute to the public good?\n\ncecie:  Well, that’s not how things work on Earth. Our world gives almost all the public credit and fame to the _discoverer_, as the initial suggester is called among us. Our scientists often _say_ that replication is important, but our most prestigious journals won’t publish mere replications; nor do the history books remember them. The outcome is a lot of small studies that have just enough subjects to obtain “statistically significant” results—\n\nvisitor:  … What? Probability is quantitative, not qualitative. There’s no such thing as a “significant” or “insignificant” likelihood ratio—\n\ncecie:  _Anyway_, while it might be good if larger studies were done, _the decisionmaker is not the beneficiary_—the people who did the extra work of a larger study, and funded the extra work of a larger study, would not receive fame and fortune thereby.\n\nvisitor:  I must be missing something basic here. You do have multiple studies, right? When you have multiple bodies of data, you can multiply the likelihood functions from the studies’ respective data to the hypotheses to obtain the meaning of the _combined_ evidence—the likelihood function from _all_ the data to the hypotheses.^[4](#footnote-4-definition)^\n\ncecie:  I’m afraid you can’t do that on Earth.\n\nvisitor:  … Of course you can. It’s a _mathematical theorem_. You can’t possibly tell me _that_ differs between our universes!\n\nYes, there are pitfalls for the especially careless. Sometimes studies end up being conducted under different circumstances, with the result that the naively computed likelihood functions don’t have uniform relations to the hypotheses under consideration. In that case, blindly multiplying will give you a likelihood function that’s nearly zero everywhere. But, I mean, if you just look at all the likelihood functions, it’s pretty obvious when some of them are pointing in different directions and then you can _investigate that divergence_.\n\nEither it makes sense to multiply all the likelihood functions and get out one massive evidential pointer, or else you _don’t_ get a sensible result when you multiply them and then you know something’s wrong with your methods—\n\ncecie:  I’m afraid our scientific community doesn’t run on your world’s statistical methods. You see, during the first half of the twentieth century, it became conventional to measure something called “_p_-values” which imposed a qualitative distinction between “successful” and “unsuccessful” experiments—\n\nvisitor:  _That is still not an explanation._ Why not _change_ the way you do things?\n\ncecie:  Because somebody who tried using unconventional statistical methods, even if they were better statistical methods, wouldn’t be able to publish their papers in the most prestigious journals. And then they wouldn’t get hired. It’s similar to the way that the most prestigious journals don’t publish mere replications, only discoveries, so people focus on making discoveries instead of replications.\n\nvisitor:  Why would anyone _pay attention_ to journals like that?\n\ncecie:  Because university hiring departments care a lot about whether you’ve published in prestigious journals.\n\nvisitor:  No, I mean… how did these journals end up prestigious in the first place? _Why_ do university hiring departments pay attention to them?\n\nsimplicio:  Why _would_ university hiring departments care about real science? Shouldn’t it be you who has to explain why some lifeless cog of the military-industrial complex would care about anything except grant money?\n\ncecie:  Okay… you’re digging pretty deep here. I think I need to back up and try to explain things on a more basic level.\n\nvisitor:  Indeed, I think you should. So far, every time I’ve asked you why someone is acting insane, you’ve claimed that it’s secretly a sane response to someone else acting insane. Where does this process bottom out?\n\niv. Two-factor markets and signaling equilibria\n-----------------------------------------------\n\ncecie:  Let me try to identify a first step on which insanity can emerge from non-insanity. Universities pay attention to prestigious journals because of a _signaling equilibrium,_ which, in our taxonomy, is a kind of bad Nash equilibrium that no single actor can defy unilaterally.\n\nIn your terms, it involves a sticky, stable equilibrium of _everyone_ acting insane in a way that’s secretly a sane response to everyone else acting insane.\n\nvisitor:  Go on.\n\ncecie:  First, let me explain the idea of what Eliezer has nicknamed a “two-factor market.” Two-factor markets are a conceptually simpler case that will help us later understand signaling equilibria.\n\nIn our world there’s a crude site for classified ads, called Craigslist. Craigslist doesn’t contain any way of rating users, the way that eBay lets buyers and sellers rate each other, or that Airbnb lets renters and landlords rate each other.\n\nSuppose you wanted to set up a version of Craigslist that let people rate each other. Would you be able to compete with Craigslist?\n\nThe answer is that even if this innovation is in fact a good one, competing with Craigslist would be far more difficult than it sounds, because Craigslist is sustained by a two-factor market. The sellers go where there are the most buyers; the buyers go where they expect to find sellers. When you launch your new site, no buyers will want to go there because there are no sellers, and no sellers will want to go there because there are no buyers. Craigslist initially broke into this market by targeting San Francisco particularly, and spending marketing effort to assemble the San Francisco buyers and sellers into the same place. But that would be harder to do for a later startup, because now the people it’s targeting are already using Craigslist.\n\nsimplicio:  Those sheep! Just mindlessly doing whatever their incentives tell them to!\n\ncecie:  We can imagine that there’s a better technology than Craigslist, called Danslist, such that everyone using Craigslist would be better off if they all switched to Danslist simultaneously. But if just one buyer or just one seller is the first to go to Danslist, they find an empty parking lot. In conventional cynical economics, we’d say that this is a _coordination problem_—\n\nsimplicio:  A coordination problem? What do you mean by that?\n\ncecie:  Backing up a bit: A “Nash equilibrium” is what happens when everyone makes their best move, given that all the other players are making their best moves from that Nash equilibrium—everyone goes to Craigslist, because that’s their individually best move _given_ that everyone else is going to Craigslist. A “Pareto optimum” is any situation where it’s impossible to make every actor better off simultaneously, like “Cooperate/Cooperate” in the Prisoner’s Dilemma—there’s no alternative outcome to Cooperate/Cooperate that makes _both_ agents better off. The Prisoner’s Dilemma is a coordination problem because the sole Nash equilibrium of Defect/Defect isn’t Pareto-optimal; there’s an outcome, Cooperate/Cooperate, that both players prefer, but aren’t reaching.\n\nsimplicio:  How stupid of them!\n\ncecie:  No, it’s… ah, never mind. Anyway, the _frustrating_ parts of civilization are the times when you’re stuck in a Nash equilibrium that’s Pareto-inferior _to other Nash equilibria_. I mean, it’s not surprising that humans have trouble getting to non-Nash optima like “both sides cooperate in the Prisoner’s Dilemma without any other means of enforcement or verification.” What makes an equilibrium _inadequate,_ a fruit that seems to hang tantalizingly low and yet somehow our civilization isn’t plucking, is when there’s a better _stable_ state and we haven’t reached it.\n\nvisitor:  Indeed. Moving from bad equilibria to better equilibria is the whole point of having a civilization in the first place.\n\ncecie:  Being stuck in an inferior Nash equilibrium is how I’d describe the frustrating aspect of the two-factor market of buyers and sellers that can’t switch from Craigslist to Danslist. The scenario where everyone is using Danslist _would_ be a stable Nash equilibrium, and a _better_ Nash equilibrium. We just can’t get there from here. There’s no one actor who is behaving foolishly; all the individuals are responding strategically to their incentives. It’s only the larger system that behaves “foolishly.” I’m not aware of a standard term for this situation, so I’ll call it an “inferior equilibrium.”\n\nsimplicio:  Why do you care what academics call it? Why not just use the _best_ phrase?\n\ncecie:  The terminology “inferior equilibrium” would be fine if everyone else were already using that terminology. Mostly I want to use the same phrase that everyone else uses, even if it’s not the best phrase.\n\nsimplicio:  Regardless, I’m not seeing what the grand obstacle is to people solving these problems by, you know, _coordinating_. If people would just act in unity, so much could be done!\n\nI feel like you’re placing too much blame on system-level issues, Cecie, when the simpler hypothesis is just that the people _in_ the system are terrible: bad at thinking, bad at caring, bad at coordinating. You claim to be a “cynic,” but your whole world-view sounds rose-tinted to me.\n\nvisitor:  Even in my world, Simplicio, coordination isn’t as simple as everyone jumping simultaneously every time one person shouts “Jump!” For coordinated action to be successful, you need to trust the institution that says what the action should be, and a _majority_ of people have to trust that institution, and they have to _know_ that other people trust the institution, so that everyone _expects_ the coordinated action to occur at the critical time, so that it makes sense for them to act too.\n\nThat’s why we have policy prediction markets and… there doesn’t seem to be a word in your language for the _timed-collective-action-threshold-conditional-commitment…_ hold on, this cultural translator isn’t making any sense. “Kickstarter”? You have the key concept, but you use it mainly for making video games?\n\ncecie:  I’ll now introduce the concept of a _signaling equilibrium_.\n\nTo paraphrase a commenter on _Slate Star Codex_: suppose that there’s a magical tower that only people with IQs of at least 100 and some amount of conscientiousness can enter, and this magical tower slices four years off your lifespan. The natural next thing that happens is that employers start to prefer prospective employees who have proved they can enter the tower, and employers offer these employees higher salaries, or even make entering the tower a condition of being employed at all.^[5](#footnote-5-definition)^\n\nvisitor:  Hold on. There _must_ be less expensive ways of testing intelligence and conscientiousness than sacrificing four years of your lifespan to a magical tower.\n\ncecie:  Let’s not go into that right now. For now, just take as an exogenous fact that employers can’t get all of the information they want by other channels.\n\nvisitor:  But—\n\ncecie:  Anyway: the natural next thing that happens is that employers start to demand that prospective employees show a certificate saying that they’ve been inside the tower. This makes _everyone_ want to go to the tower, which enables somebody to set up a fence around the tower and charge hundreds of thousands of dollars to let people in.^[6](#footnote-6-definition)^\n\nvisitor:  But—\n\ncecie:  Now, fortunately, after Tower One is established and has been running for a while, somebody tries to set up a competing magical tower, Tower Two, that also drains four years of life but charges less money to enter.\n\nvisitor:  … You’re _solving the wrong problem._\n\ncecie:  Unfortunately, there’s a subtle way in which this competing Tower Two is hampered by the same kind of lock-in that prevents a jump from Craigslist to Danslist. Initially, all of the smartest people headed to Tower One. Since Tower One had limited room, it started discriminating further among its entrants, only taking the ones that have IQs above the minimum, or who are good at athletics or have rich parents or something. So when Tower Two comes along, the employers still _prefer_ employees from Tower One, which has a more famous reputation. So the smartest people still prefer to apply to Tower One, even though it costs more money. This stabilizes Tower One’s reputation as being the place where the smartest people go.\n\nIn other words, the signaling equilibrium is a two-factor market in which the stable point, Tower One, is cemented in place by the individually best choices of two different parts of the system. Employers prefer Tower One because it’s where the smartest people go. Smart employees prefer Tower One because employers will pay them more for going there. If you try dissenting from the system unilaterally, without everyone switching at the same time, then as an employer you end up hiring the less-qualified people from Tower Two, or as an employee, you end up with lower salary offers after you go to Tower Two. So the system is stable as a matter of individual incentives, and stays in place. If you try to set up a cheaper alternative to the whole Tower system, the _default_ thing that happens to you is that people who couldn’t handle the Towers try to go through your new system, and it acquires a reputation for non-prestigious weirdness and incompetence.\n\nvisitor:  This all just seems so weird and complicated. I’m skeptical that this scenario with the magical towers could happen in real life.\n\nsimplicio:  I agree that trying to build a cheaper Tower Two is solving the wrong problem. The interior of Tower One boasts some truly exquisite architecture and decor. It just makes sense that _someone_ should pay a lot to allow people entry to Tower One. What we really need is for the government to subsidize the entry fees on Tower One, so that more people can fit inside.\n\ncecie:  Consider a simpler example: Velcro is a system for fastening shoes that is, for at least some people and circumstances, better than shoelaces. It’s easier to adjust three separate Velcro straps then it is to keep your shoelaces perfectly adjusted at all loops, it’s faster to do and undo, et cetera, and not everyone is running at high speeds that call for perfectly adjusted running shoes. But when Velcro was introduced, the earliest people to adopt Velcro were those who had the most trouble tying their shoelaces—very young children and the elderly. So Velcro became associated with kids and old people, and thus unforgivably _unfashionable_, regardless of whether it would have been better than shoelaces in some adult applications as well.\n\nvisitor:  I take it you didn’t have the stern and upright leaders, what we call the Serious People, who could set an example by donning Velcro shoes themselves?\n\nsimplicio & cecie:  (_in unison_)  No.\n\nvisitor:  I see.\n\ncecie:  Now consider the system of scientific journals that we were originally talking about. Some journals are prestigious. So university hiring committees pay the most attention to publications in that journal. So people with the best, most interesting-looking publications try to send them to that journal. So if a university hiring committee paid an equal amount of attention to publications in lower-prestige journals, they’d end up granting tenure to less prestigious people. Thus, the whole system is a stable equilibrium that nobody can unilaterally defy except at cost to themselves.\n\nvisitor:  I’m still skeptical. Doesn’t your parable of the magical tower suggest that, if that’s actually true, somebody ought to rope off the journals too and charge insane amounts of money?\n\ncecie:  Yes, and that’s exactly what happened. Elsevier and a few other profiteers grabbed the most prestigious journals and started jacking up the access costs. They contributed almost nothing—even the peer review and editing was done by unpaid volunteers. Elsevier just charged more and more money and sat back. This is standardly called _rent-seeking_. In a few cases, the scientists were able to kickstart a coordinated move where the entire editing board would resign, start a new journal, and everybody in the field would submit to the new journal instead. But since our scientists don’t have recognized kickstarting customs, or any software support for them, it isn’t easy to pull that off. Most of the big-name journals that Elsevier has captured are still big names, still getting prestigious submissions, and still capturing big-money rents.\n\nvisitor:  Well, I guess I understand why my cultural translator keeps putting air quotes around Earth’s version of “science.” The whole idea of science, as I understand the concept, is that everything has to be in the open for anyone to verify. Science is the part of humanity’s knowledge that everyone can potentially learn about and reproduce themselves. You can’t _charge money_ in order for people to read your experimental results, or you lose the “everyone can access and verify your claims” property that distinguishes science from other kinds of information.\n\ncecie:  Oh, rest assured that scientists aren’t seeing any of this money. It all goes to the third-party journal owners.\n\nsimplicio:  And this isn’t just scientists being stupid?\n\ncecie:  No stupider than you are for going to college. It’s hard to beat _signaling equilibria_—because they’re “multi-factor markets”—which are special cases of _coordination problems_ that create “inferior Nash equilibria”—which are so stuck in place that market controllers can _seek rent_ on the value generated by captive participants.\n\nsimplicio:  Weren’t we talking about dead babies at some point?\n\ncecie:  Yes, we were. I was explaining how our system allocated too much credit to discoverers and not enough credit to replicators, and the only socially acceptable statistics couldn’t aggregate small-scale trials in a way regarded as reliable. The Visitor asked me why the system was like that. I pointed to journals that published a particular kind of paper. The Visitor asked me why anyone paid attention to those journals in the first place. I explained about signaling equilibria, and that’s where we are now.\n\nvisitor:  I can’t say that I feel enlightened at the end of walking through all that. There must be _particular_ scientists on the editorial boards who choose not to demand replications and who forbid multiplying likelihood ratios. Why are those particular scientists doing the non-sensible thing?\n\ncecie:  Because people in the general field wouldn’t cite nonstandard papers, so if the editors demanded nonstandard papers, the journal’s publication factor would decrease.\n\nvisitor:  Why don’t the journal editors start by demanding that paper submitters _cite_ dual replications as well as initial suggestions?\n\ncecie:  Because that would be a weird unconventional demand, which might lead people with high-prestige results to submit those results to other journals instead. Fundamentally, you’re asking why scientists on Earth don’t adopt certain new customs that you think would be for the good of everyone. And the answer is that there’s this big, multi-factor system that nobody can dissent from unilaterally, and that people have a _lot_ of trouble coordinating to change. That’s true even when there are forces like Elsevier that are being _blatant_ about ripping everyone off. Implementing your proposed cultural shift to “suggesters” and “replicators,” or using likelihood functions, would be significantly _harder_ than everyone just simultaneously ceasing to deal with Elsevier, since the case for it would be less obvious and would provoke more disagreement. All that we can manage is to make incremental shifts toward funding more replication and asking more for study preregistration.\n\nTo sum up, academic science is embedded in a big enough system with enough separate decisionmakers creating incentives for other decisionmakers that it almost always takes the path of least resistance. The system isn’t in the _best_ Nash equilibrium because nobody has the power to look over the system and choose _good_ Nash equilibria. It’s just in _a_ Nash equilibrium that it wandered into, which includes statistical methods that were invented in the first half of the 20th century and editors not demanding that people cite replications.\n\nvisitor:  I see. And that’s why nobody in your world has multiplied the likelihood functions, or done a large-enough single study, or otherwise done _whatever it would take_ to convince whoever needs to be convinced about the effects of feeding infants soybean oil.\n\ncecie:  It’s one of the reasons. A large study would also be very _expensive_ because of extreme paperwork requirements, generated by other systemic failures I haven’t gotten around to talking about yet—^[7](#footnote-7-definition)^\n\nvisitor:  How does anything get done _ever_, in your world?\n\ncecie:  —and when it comes to funding or carrying out that bigger study, _the decisionmaker would not significantly benefit_ under the current system, which is held in place by _coordination problems_. And that’s why people who already have a background grasp of lipid metabolic pathways have _asymmetric information_ about what is worth becoming indignant about.\n\nv. Total market failures\n------------------------\n\nvisitor:  Even granting the things you’ve said already, I don’t feel like I’ve been told enough to understand why your society is killing babies.\n\ncecie:  Well, _no_. Not yet. The lack of incentive to do a large-scale convincing study is only _one_ thing that went wrong inside _one_ part of the system. There’s a lot _more_ broken than just that—which is why effective altruists shouldn’t be running out and trying to fund a big replication study for Omegaven, because that by itself wouldn’t fix things.\n\nvisitor:  Okay, suppose there _had_ been a large enough study to satisfy your world’s take on “scientists.” What _else_ would likely go wrong after that?\n\ncecie:  Several things. For example, doctors wouldn’t necessarily be aware of the experimental results.\n\nvisitor:  Hold on, I think my cultural translator is broken. You used that word “doctor” and my translator spit out a long sequence of words for Examiner plus Diagnostician plus Treatment Planner plus Surgeon plus Outcome Evaluator plus Student Trainer plus Business Manager. Maybe it’s stuck and spitting out the names of all the professions associated with medicine.\n\ncecie:  So, in your world, if there is a dual replication of results on Omegaven versus soybean oil, how does that end up changing the actual patient treatments?\n\nvisitor:  By informing the Treatment Planners who specialize in infant ailments that required parenteral nutrition, of course. The discovery would appear inside the “parenteral nutrition” pages in the Earthweb and show up in the feeds of everyone subscribed to that page. The statistics would appear inside the Treatment Planner’s decision-support software. And if all of those broke for some reason, every Treatment Planner for infant ailments that required parenteral nutrition would just use chatrooms. And anyone who ignored the chatrooms would have worse patient outcome ratings, and would lose status relative to Treatment Planners who were more attentive.\n\ncecie:  It sounds like “Treatment Planners” in your world are much more specialized than doctors in this world. I suppose they’re also selected specifically for talent at… cost-benefit analysis and decision theory, or something along those lines? And then they focus their learning on particular diseases for which they are Treatment Planners? And somebody else tracks their outcomes?\n\nvisitor:  Of course. I’m… almost afraid to ask, but how do they do it in your world?\n\ncecie:  Your translator wasn’t broken. In our world, “doctors” are supposed to examine patients for symptoms, diagnose especially complicated or obscure ailments using their encyclopedic knowledge and their keen grasp of Bayesian inference, plan the patient’s treatment by weighing the costs and benefits of the latest treatments, execute the treatments using their keen dexterity and reliable stamina, evaluate for themselves how well that went, train students to do it too, and in many cases, _also_ oversee the small business that bills the patients and markets itself. So “doctors” have to be selected for all of those talents simultaneously, and then split their training, experience, and attention between them.\n\nvisitor:  _Why_ in the name of—\n\ncecie:  Oh, and before they go to medical school, we usually send them off to get a four-year degree in philosophy first or something, just because.\n\nI don’t know if there’s a standard name for this phenomenon, but we can call it “failure of professional specialization.” It also appears when, for example, a lawyer has to learn calculus in order to graduate college, even though their job doesn’t require any calculus.\n\nvisitor:  Why. Why. Why why why—\n\ncecie:  I’m not sure. I suspect the origin has something to do with status—like, a high-status person can do all things at once, so it’s insulting and lowers status to suggest that an esteemed and respectable Doctor should only practice one surgical operation and get very good at it. And once you yourself have spent twelve years being trained under the current system, you won’t be happy about the proposal to replace it with two years of much more specialized training. Once you’ve been through a painful initiation ritual and rationalized its necessity, you’ll hate to see anyone else going through a less painful one. Not to mention that you won’t be happy about the competition against your own human capital, by a cheaper and better form of human capital—and after the sunk cost in pain and time that you endured to build human capital under the old system…\n\nvisitor:  Do they not have markets on your planet? Because on my planet, when you manufacture your product in a crazy, elaborate, expensive way that produces an inferior product, someone else will come along and rationalize the process and take away your customers.\n\ncecie:  We have markets, but there’s this unfortunate thing called “regulatory capture,” of which one kind is “occupational licensing.”\n\nAs an example, it used to be that chairs were carefully hand-crafted one at the time by carpenters who had to undergo a lengthy apprenticeship, and indeed, they didn’t like it when factories came along staffed by people who specialized in just carving a single kind of arm. But the factory-made chairs were vastly cheaper and most of the people who insisted on sticking to handcrafts soon went out of business.\n\nNow imagine: What if the chair-makers had been extremely respectable—had already possessed very high status? What if their profession had an element of danger? What if they’d managed to frighten everyone about the dangers of improperly made chairs that might dump people on the ground and snap their necks?\n\nvisitor:  Okay, yes, we used to have Serious People who would go around and certify the making of some medicines where somebody might be tempted to cheat and use inferior ingredients. But that was before _computers_ and _outcome statistics_ and _online ratings._\n\ncecie:  And on our planet, Uber and Lyft are currently fighting it out with taxi companies and their pet regulators after exactly that development. But suppose the whole system was set up before the existence of online ratings. Then the carpenters might have managed to introduce occupational licensing on who could be a carpenter. So if you tried to set up a factory, your factory workers would have needed to go through the traditional carpentry apprenticeship that covered every part of every kind of furniture, before they were legally allowed to come to your factory and specialize in carving just one kind of chair-arm. And then your factory would also need a ton of permits to sell its furniture, and would need to inveigle orders from a handful of resellers who were licensed to buy and resell furniture at a fixed margin. That small, insular group of resellers might not benefit _literally personally_—in their own personal salary—from buying from your cheaper factory system. And so it would go.\n\nvisitor:  But why would the legislators go along with that?\n\ncecie:  Because the carpenters would have a big, concentrated incentive to figure out how to make legislators do it—maybe by hiring very persuasive people, or by subtle bribery, or by not-so-subtle bribery.\n\nInsofar as occupational licensing works to the benefit of professionals at the expense of consumers, occupational licensing represents a kind of regulatory capture, which happens when a few regulatees have a much more concentrated incentive to affect the regulation process. Regulatory capture in turn is a kind of commons problem, since every citizen shares the benefits of non-captured regulation, but no individual citizen has a sufficient incentive to unilaterally spend their life attending to that particular regulatory problem. So occupational licensing is regulatory capture is a commons problem is a coordination problem.\n\nvisitor:  Then… the upshot is that it’s impossible for your country to _test_ a functional hospital design _in the first place?_ The reformers can’t win the competition because they’re not legally allowed to try?\n\ncecie:  But of course. Though in this case, if you did manage to set up a test hospital working along more reasonable lines, you still wouldn’t be able to advertise your better results relative to any other hospitals. With just a few isolated exceptions, all of the other hospitals on Earth don’t publish patient outcome statistics in the first place.\n\nvisitor:  … But… then—_what are they even selling?_\n\nsimplicio:  Hold on. If you reward the doctors with the highest patient survival rates, won’t they just reject all the patients with poor prognoses?\n\nvisitor:  Obviously you don’t evaluate raw survival rates. You have Diagnosticians who estimate prognosis categories and are rated on their predictive accuracy, and Treatment Planners and Surgeons who are rated on their _relative_ outcomes, and you have the outcomes evaluated by a third party, and—\n\ncecie:  In our world, there’s no separation of powers where one person assigns patients a prognosis category and has their prediction record tracked, and another person does their best to treat them and has their treatment record tracked. So hospitals don’t publish any performance statistics, and patients choose the hospital closest to their house that takes their workplace’s insurance, and nobody has any financial incentive to decrease the number of patient deaths from sloppy surgeons or central line infections. When anesthesiologists in particular did happen to start tracking patient outcomes, they adopted some simple monitoring standards and subsequently decreased their fatality rates by a factor of _one hundred_.^[8](#footnote-8-definition)^ But that’s just anesthesiologists, not, say, cardiac surgeons.\n\nWith cardiac surgeons, a group of researchers recently figured out how to detect when the most senior cardiac surgeons were at conferences, and found that the death rates went down while the most senior cardiac surgeons were away.^[9](#footnote-9-definition)^ But our scientists have to use special tricks if they want to find out any facts like that.\n\nvisitor:  Do your _patients_ not care if they live or die?\n\ncecie:  Robin Hanson has a further thesis about how what people really want from medicine is reassurance rather than statistics. But I’m not sure that hypothesis is necessary to explain this particular aspect of the problem. If no hospital offers statistics, then you have no baseline to compare to if one hospital _does_ start offering statistics. You’d just be looking at an alarming-looking percentage for how many patients die, with no idea of whether that’s a better percentage or a worse percentage. Terrible marketing! Especially compared to that other hospital across town that just smiles at you reassuringly.\n\nNo hospital would benefit from being the _first_ to publish statistics, so none of them do.\n\nvisitor:  Your world has literally zero market demand for empirical evidence?\n\ncecie:  Not zero, no. But since publishing scary numbers would be bad marketing for _most_ patients, and hospitals are heavily regional, they all go by the majority preference to not hear about the statistics.\n\nvisitor:  I confess I’m having some trouble grasping the concept of a market consisting of opaque boxes allegedly containing goods, in which nobody publishes what is inside the boxes.\n\ncecie:  Hospitals don’t publish prices either, in most cases.\n\nvisitor:  …\n\ncecie:  Yeah, it’s pretty bad even by Earth standards.\n\nvisitor:  You literally don’t _have_ a healthcare market. Nobody knows what outcomes are being sold. Nobody knows what the prices are.\n\ncecie:  I guess we could call that Total Market Failure? As in, things have gone so wrong that there’s literally no supply-demand matching or price-equilibrating mechanism remaining, even though money is still changing hands.\n\nAnd while I wish that this phenomenon of “you simply don’t have a market” were only relevant to healthcare and not to other facets of our civilization… well, it’s not.\n\nvi. Absence of (meta-)competition\n---------------------------------\n\nvisitor:  I suppose I can imagine a hypothetical world in which _one_ country screws things up as badly as you describe. But your planet has multiple governments, I thought. Or did I misunderstand that? Why wouldn’t patients emigrate to—or just _visit_—countries that made better hospitals legal?\n\ncecie:  The forces acting on governments with high technology levels are mostly the same between countries, so all the governments of those countries tend to have their medical system screwed up in mostly the same way (not least because they’re imitating each other). Some aspects of dysfunctional insurance and payment policies are special to the US, but even the relatively functional National Health System in Britain still has failure of professional specialization. (Though they at least don’t require doctors to have philosophy degrees.)\n\nvisitor:  Is there not _one_ government that would allow a reasonably designed hospital staffed by specialists instead of generalists?\n\ncecie:  It wouldn’t be enough to just have one government’s okay. You’d need some way to initially train your workers, despite none of our world’s medical schools being set up to train them. A majority of legislators won’t benefit _personally_ from deciding to let you try your new hospital in their country. Furthermore, you couldn’t just go around raising money from rich countries for a venture in a poor country, because rich countries have elaborate regulations on who’s allowed to raise money for business ventures through equity sales. The fundamental story is that everything, everywhere, is covered with varying degrees of molasses, and to do any novel thing you have to get around all of the molasses streams _simultaneously_.\n\nvisitor:  So it’s impossible to test a functional hospital design _anywhere on the planet_?\n\ncecie:  But of course.\n\nvisitor:  I must still be missing something. I just don’t understand why all of the people with economics training on your planet can’t go off by themselves and establish their own hospitals. Do you literally have people occupying every square mile of land?\n\ncecie:  … How do I phrase this…\n\nAll useful land is already claimed by some national government, in a way that the international order recognizes, whether or not that land is inhabited. No relevant decisionmaker has a personal incentive to allow there to be unclaimed land. Those countries will defend even a very small patch of that claimed land using all of the military force their country has available, and the international order will see you as the aggressor in that case.\n\nvisitor:  Can you _buy_ land?\n\ncecie:  You can’t buy the sovereignty on the land. Even if you had a _lot_ of money, any country poor enough and desperate enough to consider your offer might just steal your stuff after you moved in.\n\nNegotiating the right to bring in weapons to defend yourself in this kind of scenario would be even more unthinkable, and would spark international outrage that could prevent you from trading with other countries.\n\nTo be clear, it’s not that there’s a global dictator who prevents new countries from popping up; but every potentially useful part of every land is under _some_ system’s control, and all of those systems would refuse you the chance to set up your own alternative system, for very similar reasons.\n\nvisitor:  So there’s no way for your planet to _try_ different ways of doing things, _anywhere_. You literally cannot run experiments about things like this.\n\ncecie:  Why would there be? Who would decide that, and how would they personally benefit?\n\nvisitor:  That sounds _extremely_ alarming. I mean, difficulties of adoption are one thing, but not even being able to _try_ new things and see what happens… Shouldn’t everyone on your planet be able to detect at a glance how horrible things have become? Can this type of disaster really stand up to _universal_ agreement that something is wrong?\n\ncecie:  I’m afraid that our civilization doesn’t have a sufficiently stirring and narratively satisfying conception of the valor of “testing things” that our people would be massively alarmed by its impossibility. And now, Visitor, I hope we’ve bottomed out the general concept of why people can’t do things differently—the local system’s equilibrium is broken, _and_ the larger system’s equilibrium makes it impossible to flee the game.\n\nvisitor:  Okay, look… despite everything you’ve said so far, I still have some trouble understanding why doctors and parents can’t just _not_ kill the babies. I manage to get up every single morning and successfully not kill any babies. It’s not as hard as it sounds.\n\ncecie:  I worry you’re starting to think like Simplicio. You can’t just _not_ kill babies and expect to get away with it.\n\nsimplicio:  I actually agree with Cecie here. The evil people behind the system hate those who defy them by behaving differently; there’s no way they’d countenance anyone departing from the norm. What we really need is a revolution, so we can depose our corrupt overlords, and finally be free to coordinate, and…!\n\ncecie:  There’s no need to add in any evil conspiracy hypotheses here.\n\nIt’s sufficient to note that the system is _in equilibrium_ and it has _causes_ for the equilibrium settling there—causes, if not justifications. You can’t go against the system’s default without going against the forces that underpin that default. A doctor who gives a baby a nutrition formula that isn’t FDA-approved will lose their job. A hospital that doesn’t fire that kind of doctor will be sued. A scientist that writes proposals for a big, expensive, definitive study won’t get a grant, and while they were busy writing those failed grant proposals, they’ll have lost their momentum toward tenure. So no, you can’t just try out a competing policy of not killing babies. Not more than once.\n\nvisitor:  _Have_ you tried?\n\ncecie:  No.\n\nvisitor:  But—\n\ncecie:  Anyway, from my perspective, it’s no surprise if you don’t yet feel like you understand. We’ve only _begun_ to survey the malfunctions of the whole system, which would further include the FDA, and the clinical trials, and the _p_-hacking. And the way venture capital is structured, and equity-market regulations. And the insurance companies, and the tax code. And the corporations who contract with the insurance companies. And the corporations’ employees. And the politicians. And the voters.\n\nvisitor:  … Consider me impressed that your planet managed to reach this level of dysfunction without _actually physically bursting into flames_.\n\n* * *\n\nNext: [**Moloch's Toolbox** part 2](https://www.lesserwrong.com/posts/PRAyQaiMWg2La7XQy/moloch-s-toolbox-2-2).\n\nThe full book will be available November 16th. You can go to [equilibriabook.com](https://equilibriabook.com) to pre-order the book, or sign up for notifications about new chapters and other developments.\n\n* * *\n\n1.  Carl Shulman notes that the Affordable Care Act linked federal payments to hospitals with reducing central-line infections ([source](https://www.washingtonpost.com/news/wonk/wp/2013/05/31/the-cost-curve-is-bending-does-obamacare-deserve-the-credit/)), which was probably a factor in the change. [↩](#footnote-1-return)\n    \n2.  Around a thousand infants are born with short bowel syndrome per year in the United States, of whom two-thirds develop parenteral nutrition-associated liver disease ([source](http://journals.sagepub.com/doi/abs/10.1177/0148607114527772)). See [Park, Nespor, and Kerner Jr](https://www.nature.com/jp/journal/v31/n1s/full/jp2010182a.html) for a 2011 review of the academic literature, and [Koch, Cohen, and Carroll](http://rockcenter.nbcnews.com/_news/2013/06/07/18833434-drug-treatment-omegaven-that-could-save-infants-lives-not-yet-approved-by-fda) and [Madrzyk](https://www.dailyherald.com/article/20110118/news/701199905/) for news coverage. [↩](#footnote-2-return)\n    \n3.  See Tabarrok’s “[Assessing the FDA via the Anomaly of Off-Label Drug Prescribing](http://www.independent.org/pdf/tir/tir_05_1_tabarrok.pdf),” which cites the widespread practice of off-label prescription as evidence that the FDA’s efficacy trial requirements are unnecessary. [↩](#footnote-3-return)\n    \n4.  See the “[Report Likelihoods, Not _p_-Values](https://arbital.com/p/likelihoods_not_pvalues/?l=505)” FAQ, or, in dialogue form: “[Likelihood Functions, _p_-Values, and the Replication Crisis](https://arbital.com/p/likelihoods_not_pvalues/?l=4xx).” [↩](#footnote-4-return)\n    \n5.  From Schmidt and Hunter’s “[Select on Intelligence](http://www.blackwellreference.com/public/tocnode?id=g9780631215066_chunk_g97806312150662)”: “Intelligence is the major determinant of job performance, and therefore hiring people based on intelligence leads to marked improvements in job performance.” See also psychologist Stuart Ritchie’s discussion of IQ [in _Vox_](https://www.vox.com/2016/5/25/11683192/iq-testing-intelligence).\n    \n    Software engineer Alyssa Vance adds:\n    \n    > I’ll note that, as far as I can tell, the informal consensus at least among the best-informed people in software is that hiring has tons of obvious irrationality even when there’s definitely no external cause; see [\\[1\\]](https://sockpuppet.org/blog/2015/03/06/the-hiring-post/) and [\\[2\\]](https://danluu.com/programmer-moneyball/). In terms of Moloch’s toolbox, the obvious reason for that is that interviewers are rarely judged on the quality of the people they accept, and when they are, certainly aren’t paid more or less based on it. (Never mind the people they reject. “Nobody ever got fired because of the later performance of someone they turned down.”) Their incentive, insofar as they have one, is to hire people who they’d most prefer to be on the same floor with all day long. [↩](#footnote-5-return)\n    \n6.  Compare psychiatrist Scott Alexander’s account, in “[Against Tulip Subsidies](https://slatestarcodex.com/2015/06/06/against-tulip-subsidies/)”:\n    \n    > In America, aspiring doctors do four years of undergrad in whatever area they want (I did Philosophy), then four more years of medical school, for a total of eight years post-high school education. In Ireland, aspiring doctors go straight from high school to medical school and finish after five years. I’ve done medicine in both America and Ireland. The doctors in both countries are about equally good. When Irish doctors take the American standardized tests, they usually do pretty well. Ireland is one of the approximately 100% of First World countries that gets better health outcomes than the United States. There’s no evidence whatsoever that American doctors gain anything from those three extra years of undergrad. And why would they? Why is having a philosophy degree under my belt supposed to make me any better at medicine? \\[…\\]\n    > \n    > I’ll make another confession. Ireland’s medical school is five years as opposed to America’s four because the Irish spend their first year teaching the basic sciences—biology, organic chemistry, physics, calculus. When I applied to medical school in Ireland, they offered me an accelerated four year program on the grounds that I had surely gotten all of those in my American undergraduate work. I hadn’t. I read some books about them over the summer and did just fine.\n    > \n    > Americans take eight years to become doctors. Irishmen can do it in four, and achieve the same result. Each year of higher education at a good school—let’s say an Ivy, doctors don’t study at Podunk Community College—costs about $50,000. So American medical students are paying an extra $200,000 for…what?\n    > \n    > Remember, a modest amount of the current health care crisis is caused by doctors’ crippling level of debt. Socially responsible doctors often consider less lucrative careers helping the needy, right up until the bill comes due from their education and they realize they have to make a lot of money right now. We took one look at that problem and said “You know, let’s make doctors pay an extra $200,000 for no reason.”\n    \n    For a more general discussion of the evidence that college is chiefly a costly signal of pre-existing ability, rather than a mechanism for building skills and improving productivity, see Bryan Caplan’s argument in “[Is College Worth It?](https://www.cato.org/events/college-worth-it)”, also summarized [by Roger Barris](http://www.economicmanblog.com/2017/02/25/college-capital-or-signal/). [↩](#footnote-6-return)\n    \n7.  See, e.g., Scott Alexander’s “[My IRB Nightmare](http://slatestarcodex.com/2017/08/29/my-irb-nightmare/).” [↩](#footnote-7-return)\n    \n8.  From Hyman and Silver, “[You Get What You Pay For](http://scholarlycommons.law.wlu.edu/cgi/viewcontent.cgi?article=1469&context=wlulr)”:\n    \n    > By the 1950s, death rates ranged between 1 and 10 per 10,000 encounters. Anesthesia mortality stabilized at this rate for more than two decades. Mortality and morbidity rates fell again after a 1978 article reframed the issue of anesthesia safety as one of human factor analysis. In the mid-1980s, the American Society of Anesthesiologists (ASA) promulgated standards of optimal anesthesia practice that relied heavily on systems-based approaches for preventing errors. Because patients frequently sued anesthetists when bad outcomes occurred and because deviations from the ASA guidelines made the imposition of liability much more likely, anesthetists had substantial incentives to comply.\n    > \n    > \\[… W\\]e should consider why anesthesia mortality stabilized at a rate more than one hundred times higher than its current level for more than two decades. The problem was not lack of information. To the contrary, anesthesia safety was studied extensively during the period. A better hypothesis is that anesthetists grew accustomed to a mortality rate that was exemplary by health care standards, but that was still higher than it should have been. From a psychological perspective, this low frequency encouraged anesthetists to treat each bad outcome as a tragic but unforeseen and unpreventable event. Indeed, anesthetists likely viewed each individual bad outcome as the manifestation of an irreducible baseline rate of medical mishap.\n    \n    Hyman and Silver note other possible factors behind the large change, e.g., the fact that the person responsible for mishaps was often easy to identify since there tended to be only one anesthetist per procedure, and that “because surgical patients had no on-going relationships with their anesthetist, victims were particularly likely to sue.” [↩](#footnote-8-return)\n    \n9.  See Jena, Prasad, Goldman, and Romley, “[Mortality and Treatment Patterns Among Patients Hospitalized With Acute Cardiovascular Conditions During Dates of National Cardiology Meetings](http://jamanetwork.com/journals/jamainternalmedicine/fullarticle/2038979).” [↩](#footnote-9-return)"
    },
    "voteCount": 114
  },
  {
    "_id": "QdppEcbhLTZqDDtDa",
    "url": null,
    "title": "Unifying the Simulacra Definitions",
    "slug": "unifying-the-simulacra-definitions",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "Rationality"
      },
      {
        "name": "Simulacrum Levels"
      }
    ],
    "tableOfContents": {
      "sections": [
        {
          "title": "Different Definitions of Simulacra Levels",
          "anchor": "Different_Definitions_of_Simulacra_Levels",
          "level": 1
        },
        {
          "title": "Actions versus Systems",
          "anchor": "Actions_versus_Systems",
          "level": 1
        },
        {
          "title": "Motivations versus Processes",
          "anchor": "Motivations_versus_Processes",
          "level": 1
        },
        {
          "title": "Simulacra Level of a System",
          "anchor": "Simulacra_Level_of_a_System",
          "level": 1
        },
        {
          "title": "Level 0 Exists",
          "anchor": "Level_0_Exists",
          "level": 1
        },
        {
          "title": "Level 3 and The War Against Knowledge",
          "anchor": "Level_3_and_The_War_Against_Knowledge",
          "level": 1
        },
        {
          "title": "Conclusion and the Unity of Level 4",
          "anchor": "Conclusion_and_the_Unity_of_Level_4",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        },
        {
          "anchor": "comments",
          "level": 0,
          "title": "30 comments"
        }
      ],
      "headingsCount": 9
    },
    "contents": {
      "markdown": "Epistemic Status: Confident this is the perspective I find most useful. This is intended to both be a stand-alone post and to be the second post in the Simulacra sequence, with the first being [Simulacra Levels and their Interactions](https://thezvi.wordpress.com/2020/06/15/simulacra-and-covid-19/). It should be readable on its own, but easier having read the previous post.\n\nSimulacra levels are difficult to understand.\n\nThis is not without cause. This is complex and bizarre stuff.\n\nSimulacra levels are a map of the metaphors we use to create metaphoric maps of both territory and the map itself.\n\nThe text that coined the term Simulacra levels does not help matters. The term was first referenced locally [by Ben Hoffman in this post](http://benjaminrosshoffman.com/excerpts-from-a-larger-discussion-about-simulacra/), but this was not the original source.\n\nThe original source of the term is a [super-dense work of French philosophy.](https://www.amazon.com/Simulacra-Simulation-Body-Theory-Materialism/dp/0472065211) It requires the reader to pause after every sentence. It’s not clear that a proper review would be shorter than the book itself.\n\nThus, I’m still working through the book. The more I read [Jean Baudrillard](https://en.wikipedia.org/wiki/Jean_Baudrillard)‘s further assertions, the less they seem deserving of engagement. He is opposed for nonsensical reasons not only to the concept of capitalism, but the concepts of money, value and trade, and even urbanization and mass production. He blames these for the rise of simulacra, whereas they are the primary forces *opposed to* simulacra. \n\nUpon parsing many of his super-dense sentences, I find many of them to be outright false. I find many others to be based on models and frameworks very different from my own, and that are assumed rather than specified in the text. The idea that capitalism isn’t the cause of all the world’s problems (never mind whether it’s the solution) does not seem to parse in his mind. I find many others to be downright absurd, or to be carrying water for the agendas of History’s Greatest Villains. \n\nThis is a case where I strongly endorse taking the concepts that are useful and leaving the remaining giant mess behind. \n\nBaudrillard’s definition will be kept. Beyond that, I’m tossing essentially everything else away.\n\nThe goal of this post is to reconcile Baudillard’s definition with the Lion definition used in my previous posts, integrating the relation of a simulacra to reality with the motivational explanatory framework. This deals with the dueling definition problem so it doesn’t get in the way down the line, and I hope it helps explain why higher level activity systematically has such strange and hostile relationships to physical reality (and what Baudillard calls ‘profound’ reality.)\n\n### **Different Definitions of Simulacra Levels**\n\nAn additional confusion is that there are now multiple competing definitions, which seem superficially to point at different things, although I claim that both definitions are fully compatible once properly understood.\n\nFirst, there are simulacra levels as defined in Baudrillard. The closest thing he gives to a compact full definition is this:\n\n*   Such would be the successive phases of the image (as we pass from levels 1 to 4):\n    *   It is the reflection of a profound reality.\n    *   It masks and denatures a profound reality.\n    *   It masks the absence of a profound reality.\n    *   It has no relation to any reality whatsoever: It is its own pure simulacrum.\n\nProfound reality is a weird term that is doing several distinct things. On a basic level, it means concrete physical reality unmediated by any symbols of any kind. It is what is, full stop. Lying masks and denatures that reality by representing it as something other than what it is, but this is less of an offense than there not being an underlying reality of importance in the third stage, or being cut free from that underlying reality entirely, as we are in the fourth stage.\n\nOn a continental philosophical level, there is this idea that anything mass produced, or anything that interacts with money, trade or other systematic motivations, rather than being fully intrinsic and local and spontaneous, or something like that, loses this something vital that Baudrillard calls ‘profound reality.’ \n\nI don’t think that this second angle is *entirely* nonsense. There is something important that can be distorted or lost when commodification sets in. Polanyi’s [The Great Transformation](https://smile.amazon.com/Great-Transformation-Political-Economic-Origins/dp/080705643X/ref=sr_1_1?crid=18H5UY3LDJ9ZH&dchild=1&keywords=the+great+transformation&qid=1595075676&s=books&sprefix=the+great+trans%2Caps%2C161&sr=1-1) is the best source I know about to make the general case for why this is inherently concerning. It also means that the person interacting with underlying/profound reality then exchanges the fruits of that interaction with someone else in exchange for something symbolic. Rather than gain the physical rewards, those are exchanged for rewards that are based on the symbolic associations of what has been created. This throws away any value for much of the underlying physical reality. The less connected production is with consumption, the bigger this concern.\n\nThe difference I have with Baudrillard here is that I do not think this phenomenon is central to what is happening. And I am not eager to dismiss the many-leveled benefits of such systems. The discipline of the market, the need to match demand with satisfactory supply and the reward for doing so, not only are the main ways we have, in historical terms, an insanely great abundance of lots of nice things. They also keep us connected to the underlying reality, and keep our simulacrum (and maze) levels lower. \n\nIt is precisely when this market discipline is lost or distorted that things get out of hand. Such systems force upon us symbols at all, bringing us firmly into the first level, as opposed to having no symbols and avoiding the scale entirely. And once on the scale, the slope upwards is slippery. But these forces also form one of our strongest defenses against rising to levels beyond that.\n\nIf I was going to write the symbolic description of Simulacra levels in my own words, I would say this:\n\nLevel 1: A symbol corresponds to the key elements of underlying physical reality.\n\nLevel 2: A symbol pretends to correspond to underlying physical reality, but instead distorts key elements.\n\nLevel 3: A symbol pretends to be a distorted version of underlying physical reality (that is in turn pretending to be the underlying physical reality), but instead only corresponds as necessary to maintain the plausibility claim that this is the case. \n\nLevel 4: A symbol no longer pretends to be a version of anything other than other symbols. It has no relationship to the underlying physical reality.\n\nOr more compactly:\n\nLevel 1: Symbols describe reality.\n\nLevel 2: Symbols pretend to describe reality.\n\nLevel 3: Symbols pretend to pretend to describe reality.\n\nLevel 4: Symbols need not pretend to describe reality.\n\nOr a variation/alternative:\n\nLevel 1: Symbols accurately describe reality.\n\nLevel 2: Symbols inaccurately describe reality.\n\nLevel 3: Symbols claim to describe reality.\n\nLevel 4: Symbols no longer claim to describe reality.\n\nA concrete example suggested by Michael Vassar:\n\nLevel 1: A court reflects justice.\n\nLevel 2: A corrupt judge distorts justice.\n\nLevel 3: A Soviet show trial conceals the absence of real Soviet courts.\n\nLevel 4: A trial by ordeal or trial by combat lacks and denies the concept of justice entirely.\n\nContrast that with the newer definition that’s based on what it means to say “There’s a lion across the river”, as described in [Simulacra Levels and their Interactions:](https://thezvi.wordpress.com/2020/06/15/simulacra-and-covid-19/)\n\nLevel 1: There’s a lion across the river.\n\nLevel 2: I don’t want to go (or have other people go) across the river.\n\nLevel 3: I’m with the popular kids who are too cool to go across the river.\n\nLevel 4: A firm stance against trans-river expansionism focus grouped well with undecided voters in my constituency.\n\nOr alternatively, and isomorphic to the Lion definition, from [my previous simulacra post](https://thezvi.wordpress.com/2020/06/15/simulacra-and-covid-19/):\n\n“There’s a pandemic headed our way from China” means…\n\nLevel 1:  “There’s a pandemic headed our way from China.”\n\nLevel 2: “I want you to act as if you think there might be a pandemic on our way from China” while hoping to still be interpreted by the listener as meaning “There’s a pandemic headed our way from China.”\n\nLevel 3: “I wish to associate with the group that claims there is a pandemic headed our way from China.”\n\nLevel 4: “It is advantageous for me to say there is a pandemic headed our way from China.”\n\nSee [the previous post](https://thezvi.wordpress.com/2020/06/15/simulacra-and-covid-19/) for more details and variations of meaning via this definition.\n\nCareful reading of Baudrillard confirms my suspicion that both definitions point at the same thing while highlighting different aspects. \n\nThey grasp different parts of the elephant. \n\nFrom here on, we will call the levels from the Lion definition L-1, L-2, L-3 and L-4, merged with the Pandemic definition. I will call the levels from the original Baudrillard definition B-1, B-2, B-3 and B-4. The claim is that L-1 = B-1, L-2 = B-2, L-3 = B-3, L-4 = C-4. \n\nThis is relatively easy to see, and relatively uncontroversial, for levels 1 and 2. It is less so for level 3 and even less so for level 4.\n\nThe key differences are that they deal with lower-level actions versus higher-level systems, and that they deal with cashed-out motivations versus processes.   \n\n### **Actions versus Systems**\n\n**The Lion and Pandemic definitions** deal centrally with the *motivations for, and the meanings of, individual statements, actions and systems.*\n\n**The Baudrillard definition** deals centrally with the *default or central interpretation of statements, actions and systems*. It is the expectation of general interpretation and thus of purpose. \n\nOne can usefully think of the relative importance of each simulacra level at any scale – of an individual statement or action, of an interaction, of a person or group, of a system or concept, or of a civilization or the world.\n\nHigher levels of grouping and abstraction exist at the simulacra level that is the default motivation and interpretation of their lower-abstraction more discrete actions, statements and systems.\n\n### **Motivations versus Processes**\n\nThe Lion definition:\n\nLevel 1: There’s a lion across the river.\n\nLevel 2: I don’t want to go (or have other people go) across the river.\n\nLevel 3: I’m with the popular kids who are too cool to go across the river.\n\nLevel 4: A firm stance against trans-river expansionism focus grouped well with undecided voters in my constituency.\n\nThe Baudrillard definition:\n\nLevel 1: It is the reflection of a profound reality.\n\nLevel 2: It masks and denatures a profound reality.\n\nLevel 3: It masks the absence of a profound reality.\n\nLevel 4: It has no relation to any reality whatsoever: It is its own pure simulacrum\n\n**The Lion and Pandemic definitions** deal with the motivation behind an action or communication. Was it about communicating true information, updating someone else’s a model, sending a symbolic signal or creating useful associations?\n\n**The Baudrillard definition** deals with the process by which the statement relates to what he calls the ‘profound reality.’ Does it deal with it directly, distort it, hide its absence or ignore it entirely?\n\nThese two patterns go hand in hand. \n\nAt level 1, one directly deals with reality in order to communicate true information. One does not have to pretend.\n\nAt level 2, one distorts reality – in Baudrillard’s words, masks and denatures it – in order to convince others that what one is representing as reality is actual reality. One pretends.\n\nAt level 3, one hides the absence of reality in order to invoke a symbolic meaning, usually for the purposes of signaling. There needs to be a sufficiently strong sense of association to the underlying reality that the signal is understood and holds meaning, but not so strong an association that the signal can be confused for an underlying map. Reality must be absent, but in a way that is deniable. One pretends to pretend.\n\nAt level 4, one engages in pure simulacra, with no relation to the underlying reality at all. There is no object-level cashing out at all. The underlying reality is something to be sculpted by changing associations and symbolic meanings. Consequences of a statement or action are in terms of the consequence *to a simulacra,* as measured on the third and fourth levels. At most, one pretends to be offering pure level-3 simulacra. One does not pretend to pretend to pretend to be on the object level, rather one stops pretending, period. \n\nMore than that. One does not merely stop pretending. One *forgets that there was an underlying reality to begin with*, and loses the ability to think about the underlying reality and guide it to better outcomes, except by doing so through agents operating at the levels in between. \n\n### **Simulacra Level of a System**\n\nAll of these are phrased above in terms of an individual, but apply equally to a group, system or civilization.\n\nEven more than people, groups, systems and civilizations have a mix of all levels. Moving up the level chain has instrumental rewards, and happens continuously unless there is sufficient push back. This is similar to [the rise of maze levels](https://thezvi.wordpress.com/2020/01/18/the-road-to-mazedom/). Existing systems must maintain some amount of low-level grounding, as well. Without sufficient grounding, doom quickly follows. Things collapse well before the amount of level 1 activity can reach zero.\n\nA person can exist mostly or solely on one level. A larger group almost never does. When I speak below about being at a level, that means the most dominant one. It does not mean that the others are not present.\n\nWhen a group, system or civilization is still sufficient in level 1, or has regained that footing, its symbols map directly to reality. Words have meaning.\n\nWhen a group, system or civilization proceeds sufficiently into level 2, that means it is standard and/or wise to assume that level 2 motives and actions are dominant. Claims cease to be taken at face value by default. Trust is destroyed. The assumption is that someone is likely to be out to sell you a bill of goods.\n\nIt would not parse that someone would say something because it is true. It would only make sense to say something because it would be beneficial for others to believe it is true. And thus, even straightforward claims are interpreted this way. Still, because all claims must *pretend,* a relatively strong link to the underlying reality is maintained. \n\nWhen a group, system or civilization proceeds sufficiently into level 3, that means it is standard slash wise to assume that level 3 motives and actions are dominant. Claims cease to be taken not only at face value, but also cease to be taken as being claims about the world at all. It is assumed that claims are made because it is beneficial to be seen making a claim, or for one’s side to be seen making such a claim, due to its symbolic benefits.\n\nBut this is not a complete transformation. Not yet. Those symbolic benefits still have to be seen as tracing back to underlying ‘profound’ reality. \n\n[Everybody knows](https://thezvi.wordpress.com/2019/07/02/everybody-knows/), when things reach level three, that statements are made based primarily on their utility in the game. It’s no longer a big deal to say that which is not. That’s fine. The cool kids don’t want to cross the river and this is their slogan, so you repeat the slogan. \n\nThe link to the underlying reality is tenuous, but still exists – if one can expose others as not being able to pretend, thus showing they have failed to pretend to pretend, they lose face. The symbols mask the lack of an underlying reality, *but need plausible deniability while doing so.* You need not believe, when claiming there is a lion across the river, that there is a lion across the river. But it would be bad to be seen knowing that there wasn’t a lion across the river, or being known to have no reason to think there was a lion across the river, and saying it anyway.\n\nThis maintains a weak link to underlying reality, so Level 3 never fully succeeds at existing purely on its own terms. Doing so transforms it into Level 4.\n\nWhen a group, system or civilization proceeds sufficiently into level 4, that means it is standard slash wise to assume that level 4 motives and actions are dominant. Claims cease to be taken as anything other than symbolic moves. Any impact on the underlying physical reality via their accuracy or lack thereof is purely coincidental. \n\nThe institutional memory of the object level is lost in all practical senses. The Level 4 parts of the system can’t see the Level 1 parts of the system. A sufficiently strongly Level 4 person, for whom level 4 has become truly part of them, almost always has the same issue. Level 4 doesn’t use logic or physical causation, so they’ve discarded those parts of their system of thought and no longer believe in them.\n\nAs I noted in the previous post, Level 4 is especially difficult for myself and many like myself to grok. It seems profoundly alien, perhaps evil. \n\n### **Level 0 Exists**\n\nIt is worth noting here that B-0 and L-0 also exist, are important, and are congruent.\n\nThat’s where you don’t say there is a lion across the river in Lion-0, you simply *don’t cross the river because you don’t want to get eaten by a lion.* You are also in Symbolic-0, because you don’t have a symbol at all. Others can see that, and consider that maybe crossing the river is not a great idea, even if they can’t figure out why.\n\nThe key is that, on Level 0, that has nothing to do with your decision to not cross. The moment you don’t cross because of how others might interpret that decision, you’re a symbol, and thus in Symbolic-1, and communicating a map of the world, and thus in Lion-1. L-0 is [what you are in the dark](https://tvtropes.org/pmwiki/pmwiki.php/Main/WhatYouAreInTheDark). We can call this The Hermit.\n\nIt is in this sense that Baudrillard is right to put the ‘blame’ for simulacra on capitalism or urbanization or mass production. These are the methods by which we have nice things and get to interact with fellow human beings. If one is alone in the forest, there is no need for symbols at all and the equilibrium is stable. The only way to entirely avoid manipulation of symbols is to not have any symbols. The price seems rather high. \n\n### **Level 3 and The War Against Knowledge**\n\nIt is important to note that Level 3 is at war with knowledge. \n\nThis is more than the “Level 3 sees knowledge as composed of things it can use for something else.” Level 3 is actively destructive of knowledge. \n\nAt level 3, the following two things are blameworthy, creating two ways in which knowledge is a liability.\n\n(These two are not everything that is blameworthy, of course. One can be blamed for other things as well – one can be part of the outgroup, be the designated scapegoat, etc etc.)\n\nOne blameworthy thing is *not invoking the right symbols.*\n\nThis is the “composed of things that can be used for something else” aspect. Caring about what is true creates an alternative incentive that prevents one from invoking the proper symbols, and casts doubt on whether those symbols mean what they seem to mean. \n\nInvoking symbols that are technically false rather than those that are technically true is, if anything, *a stronger move in the game.* This is why. It signals more strongly one’s costly sending of the appropriate signals, without room for misinterpretation as a lower level action. By repeating the lie, we show ourselves loyal. By getting others to repeat it, we drive them towards being and identifying as loyal, and get them to show others they are loyal, and demonstrate our power over both people and symbols.\n\nThe other blameworthy thing is *knowing that what you say is false.* What is blameworthy *is knowledge itself.*\n\n(Or, perhaps more precisely, *other people knowing* that you know what you say is false, and thus anyone else’s knowledge of our having knowledge, as opposed to knowledge itself, but that’s also true of any other blame system.)\n\nWhat did the President know, and when did he know it?\n\nThus, the shift in communication from explicit to implicit. The focus on having only deniable, tacit knowledge.\n\nThe follower who needs explicit instruction is a poor follower indeed. Specifying everything to be done is impractical, and makes it clear you have not only knowledge but responsibility. Much better to *work towards* the goals of the group, to pile on symbols that help win the game.\n\nThus does this structure drive everyone away from knowledge. The easiest way, by far, to pretend not to know things is to not know them.\n\nThus, Level 3 is not merely unconcerned with the profound reality. Level 3 actively symbolizes the *absence* of a profound reality. They are not merely orthogonal to an accurate map. They oppose it.\n\nThis situation is not stable. It relies on a lack of common knowledge. It also relies on a lack of individual knowledge. It also doesn’t present stable incentives and thus is not an equilibrium.\n\nTo be sustained, it requires sufficiently powerful residues stuck in the first two levels, who misunderstand what is going on. This is the force that requires people to pretend to pretend, when their actions are exposed to the public.\n\nWhen there are insufficient naive forces to appeal to or worry about, the mask of pretending is dropped. People stop pretending to pretend.\n\n“Facts don’t matter” is true at both levels three and four. But *acknowledging* that “facts don’t matter” and *creating common knowledge* of this will make short work of level three. It moves all actors up to levels three and four. This shatters the link between symbol and reality entirely. This moves us collectively to Level 4.\n\nLevel 4 is then not at war with knowledge the way level 3 is at war with knowledge. Level 4 doesn’t acknowledge that knowledge is a thing. Thus, there is no need to symbolize its absence. \n\n### **Conclusion and the Unity of Level 4**\n\nThis hopefully provided additional perspective on simulacra levels. Ideally it provided at least some justification for the additional associations and implications I’ve placed upon levels three and four, and made clear how I think the Lion definition integrates with the original definition. \n\nI hope the whole system is also looking less like an elegant 2×2 with extra weird stuff piled on top of it that seems like it has an axe to grind, and more like a coherent system. In particular, I hope that it is now clearer why level 3 actively opposes knowledge, and level 4 loses access to logic and the ability to observe and analyze and optimize the physical world.\n\nI hope that this will all become clearer as these posts continue. I’m especially excited by the next one, but felt I needed to get this one out of the way first, as the confusions it tries to clear up would otherwise have gotten in the way. It was necessary to tackle it first."
    },
    "voteCount": 25
  },
  {
    "_id": "v6NBpQc2AzWjzxJ4S",
    "url": null,
    "title": "The Four Children of the Seder as the Simulacra Levels",
    "slug": "the-four-children-of-the-seder-as-the-simulacra-levels",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "Rationality"
      },
      {
        "name": "Simulacrum Levels"
      },
      {
        "name": "Religion"
      }
    ],
    "tableOfContents": {
      "sections": [
        {
          "title": "Level One – The Wise Child",
          "anchor": "Level_One___The_Wise_Child",
          "level": 1
        },
        {
          "title": "Level Two – The Wicked Child",
          "anchor": "Level_Two___The_Wicked_Child",
          "level": 1
        },
        {
          "title": "“What is this service of yours?!”",
          "anchor": "_What_is_this_service_of_yours___",
          "level": 3
        },
        {
          "title": "Level Three – The Simple Child",
          "anchor": "Level_Three___The_Simple_Child",
          "level": 1
        },
        {
          "title": "Level Four – The One Who Does Not Know How to Ask",
          "anchor": "Level_Four___The_One_Who_Does_Not_Know_How_to_Ask",
          "level": 1
        },
        {
          "title": "Too Smart For Questions",
          "anchor": "Too_Smart_For_Questions",
          "level": 2
        },
        {
          "title": "Level Five – The Child Who Is Not There",
          "anchor": "Level_Five___The_Child_Who_Is_Not_There",
          "level": 1
        },
        {
          "title": "Conclusion, Goals and Takeaways",
          "anchor": "Conclusion__Goals_and_Takeaways",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        },
        {
          "anchor": "comments",
          "level": 0,
          "title": "17 comments"
        }
      ],
      "headingsCount": 10
    },
    "contents": {
      "markdown": "Previously: [Unifying the Simulacra Definitions](https://thezvi.wordpress.com/2020/08/03/unifying-the-simulacra-definitions/), [Simulacra Levels and their Interactions](https://thezvi.wordpress.com/2020/06/15/simulacra-and-covid-19/), [On Negative Feedback and Simulacra](https://thezvi.wordpress.com/2020/05/03/on-negative-feedback-and-simulacra/)\n\nSimulacra levels are complex, counter-intuitive and difficult to understand.\n\nThus, it is good and right to continue exploring them partly via story and metaphor.\n\nThe metaphor here will be that of the four children from Jewish Passover Seder.\n\nThe Jewish Seder tells us of four generations of children: The wise child, the wicked child, the simple child, and the one who does not know how to ask.\n\nThe story is _profoundly weird_ and does not, on its face, make much sense. Yet every year it is told anyway. What is going on here?\n\nMany attempts have been made to interpret it.\n\nA while back I wrote the first [rationalist seder](https://drive.google.com/file/d/0B-gfWUAglgYhbWFJRFF1ekJvNGphMGFsWFo4NFRVUGhYSDhn/view?usp=sharing) (later versions can be found [here](https://docs.google.com/document/d/1PPz6iSBY9_18_T-qOO-sJnQb-eJfIdFcNj0W939F9IY/edit)). At the time, the story of the four children did not make sense to me. Why this narrative of decline and fall, of wisdom as something that can only decay? \n\nTo make sense of the story of the children and to tie it to the themes I wanted to focus on, I told a reversed story and substituted in generations of rationalists and truth seekers. \n\nIn this story, we first learn how to ask, then we are simple, then we are instrumental, then we seek to fully understand, and then finally in a fifth stage we can transcend. We can be great because we stand on the shoulders of giants. \n\nReversing the order of development is reasonably common, as is an implied fifth child. When I was googling for details of what the sons say, [the first hit was a reversed-order story of the children as stages of psychological development](https://www.psychologytoday.com/us/blog/the-intelligent-divorce/201204/passover-four-sons-five-characters#:~:text=The%20Four%20Sons%3A,pivotal%20role%20in%20the%20seder.), with a fifth stage beyond the four listed.\n\nThese are fine tales, worthy of telling. Today, I bring a different story.\n\nI bring the story that I now believe was _originally intended._\n\nThe four children _are the four simulacra levels. _\n\nThe wise child represents level 1. They want to know how the Seder works.\n\nThe wicked child represents level 2. They want to know what the Seder can get them.\n\nThe simple child represents level 3. They want to know what the Seder symbolizes.\n\nThe child who does not know how to ask represents level 4. They don’t know things anymore.\n\nThis hypothesis and the analysis that follows _could_ be me doing what [Scott Alexander](https://unsongbook.com/interlude-%D7%9E-miss-american-pie/) often did and cherry picking to find entertaining and potentially enlightening connections that were clearly never intended. But I actually don’t think so.[ ](http://unsongbook.com/)\n\nI believe this is the primary original intent of the story. This makes the four children, and in particular the fourth child, _make sense._ [This is not a coincidence because nothing is ever a coincidence.](http://unsongbook.com/)\n\nQuotes are taken [from an Orthodox Haggadah excerpt](https://www.chabad.org/holidays/passover/pesach_cdo/aid/1486118/jewish/The-Four-Children-Explained.htm), which is the third hit on a Google search of “the four children passover.” The second hit is reform, so it doesn’t count. The first hit, as noted above, was Psychology Today doing its own thing, which really shouldn’t have been in the highlight box.\n\nYou are encouraged to click through to the sources, or even better perform your own search or pick up and read the section from your own Haggadah, to verify that I am not engaging in cherry picking and to consider additional perspectives.\n\n### **Level One – The Wise Child**\n\nThe Wise Child lives in object-level reality. She cares about understanding the territory, and knows the map is a means to that end. She wants the facts.\n\nShe asks this question:\n\n“What are the testimonies, the statutes, and the laws that G‑d, our G‑d, has commanded to you?” (deut. 6:20)\n\nA naturalist might interpret this question as “how does the physical world work?”\n\nAs she communicates, thus shall you communicate to her. She wants to know the facts, so you give her the facts.\n\nYou should respond to him as the Torah commands, “We were slaves to Pharaoh in Egypt, etc.” and also instruct him in all the laws of Passover, up to and including its final law: “After eating the Passover offering, one should not then conclude the meal with dessert which would wash away the taste of the Passover offering.”\n\nWhen one cares about the object level, one cares about every detail. The final law, a requirement with a specific physical purpose, is stressed here to illustrate that. \n\nThe final law is likely the final law _so that it can be the final law in this passage._ Dessert in the Seder is part of step 13 of 15. It’s not a natural place to put a final law.\n\nThe act and purpose matter in the Wise Child’s object-level literal senses. We wish to remember the taste of the Passover offering, so despite having an explicit phase of the meal for dessert, we must be careful that this dessert does not wash away the taste of the offering.\n\nThe act and purpose also matter directly as metaphor, in the more important meaning of both this law and its explanation. We finish the ceremony with joyful songs, but joyful songs that remind us of our struggles and do not hide the truth of our world – we know what the numbers are, the strong prey upon the weak then we all fall to the Angel of Death. Actions have consequences.\n\nWe also explicitly remind the Wise Child, that merely observing commandments without understanding them is not sufficient, for to do so would allow not merely them but our other actions and maps to cease to be anchored by reality:\n\nSo we tell the Wise Child:\n\nIt is true that the essence of the soul transcends the “natural order” of the person—the intellect and emotions—and therefore is blind to distinctions between commandments. It is likewise true that one can observe commandments without understanding them but simply because of the innate, essence-connection between the soul and G‑d. One can “pass over” and bypass the complications and limitations of self.\n\nBut it is G‑d’s will that we experience commandments within the “natural order” of our psyche, within our intellect and emotions. The transcendent “Passover” of our souls then finds expression within and permeates the “laws” of our minds and hearts (The Rebbe).\n\nThe very name of the holiday – Passover – _is superficially_ about the Exodus from Egypt and the concept that the Angel of Death ‘passed over’ Jewish houses during the tenth plague. But that never really made sense as a justification for the name of the entire holiday. This does.\n\nWhat the name is really for is a warning to avoid this trap of ‘passing over’ the object level, not forming a [gears-level understanding](https://www.lesswrong.com/posts/B7P97C27rvHPz3s9B/gears-in-understanding), and allowing our maps to become disconnected from profound reality.\n\nWithout discussion and argument, the Seder is hardly a Seder at all.\n\nWe must remain anchored in the object level, in our [profound reality](https://thezvi.wordpress.com/2020/08/03/unifying-the-simulacra-definitions/), if we wish to remain wise.\n\nInevitably, we lose sight of this, and proceed to level two. Thus, the second generation.\n\n### **Level Two – The Wicked Child**\n\nThe Wicked Child cares not about the first level, the obligation to the truth — as embodied by the Torah and the Passover story and Passover service.\n\nInstead, the Wicked Child cares about what effect the service, and the story that we tell at the Seder, will have on others – to be at the second level is to draw a distinction between what you believe and do, and what you seek others to believe and do.\n\nHe cares not about whether the service _reflects_ reality. He cares about in what way the service could _mask and denature_ reality, and _what he can get out of_ this service.\n\nHe thus asks:\n\n**“What is this service of yours?!”**\n\n**He says of** **_yours_****—implying that it is not for him. By excluding himself from the community, he denies the essential principle** of Judaism, the obligation to fulfill the commandments of the Torah.\n\n**You should also “blunt his teeth”** (speak harshly to him) **and say to him**:\n\n**“It is because of this** that I would fulfill His commandments, such as this Passover offering, matzah and _maror_ **that G‑d acted for** **_me_** **when I left Egypt** ([Exodus 13:8)](https://www.chabad.org/9874#v8)—for me, but not for _him_. If he \\[the wicked child\\] **had been there, he would not have been redeemed.**”\n\nAs he speaks on the second level, so we need to respond to him on the second level.\n\nThus, the first thing we note about the Wicked Child is that he has _separated himself from_ this central principle of Judaism, the obligation to the truth. We put his failure to be at level one front and center. That’s how important this is.\n\nYet we do not give up on him. One cannot have level one without the inevitability of level two. To care about what we believe, for any reason, is to invite others to care about what we believe, for their own selfish reasons.\n\nIncentives will always be a thing.\n\nWe must constantly remind everyone that we seek truth and to understand and manipulate the object level not (merely) for its own sake, but [because this is how we all survive](https://www.lesswrong.com/posts/SGR4GxFK7KmW7ckCB/something-to-protect) and have nice things. Without this, all is lost.\n\nThus, we speak back to him in his own language of consequences _to him_. We seek truth _because truth saves us._ We fulfill the obligations of reality and tell its stories that connect us to its profound reality – we are the people of the book – because they grant us freedom and life.\n\nIf the Wicked Child had been there, he would not have taken such action, would neither have been of help to or earned the help of the community, and thus _he would not have been saved. _\n\nThis is the whole quest. It is the central mission. Once they become wise to this, the child can study the details on their own:\n\nAs the Talmud states, a Jew cannot lose his Jewishness. Regardless of the degree of his disengagement from Judaism, the Jewish spark lives on within him.\n\nKabbalah teaches that the wicked child, _second_ of the four children, corresponds to the _second_ of the Four Cups. This means that the bulk of the Haggadah is recited over the cup related to the wicked child! Clearly, befriending and educating the wicked child is a central aspect of the Haggadah. For this effort helps bring about the ultimate realization of the Egyptian Exodus.\n\nThe Jewish spark here represents this drive towards truth in all of us. Of course this cannot be fully extinguished. Reality is that which, when you stop believing in it, doesn’t go away. A sufficiently powerful smackdown from reality will wake anyone (who survives it) up.\n\nIt can, however, be _suspended_ indefinitely under the wrong conditions.\n\nThus, we spend the bulk of the Seder speaking primarily to the Wicked Child.\n\nIn each generation the wicked child must be convinced of the need to choose wisdom. The wicked child follows from the wise child, as the second level follows from the first. Only by continuously maintaining right incentives and norms, and hammering the necessary messages into everyone’s heads over and over, can we ensure the wicked children among us ultimately choose wisdom.\n\nThis is not a struggle that happens once. It happens continuously for each of us that still thinks reality is a thing. Each of us _who still believes that others believe that one thing is and another is not,_ is tempted continuously by the ability to say that which is not in order to get others to believe that which is not. \n\nThis fits with my model that, while higher-simulacra-levels are always present to some extent, past societies have mostly succeeded at keeping the focus on the object level and thus preventing things on the whole from degenerating further.\n\nOr, that those that have failed at this task have fallen soon thereafter.\n\nWhen the community fails at this task, the Wicked Children grow up and remain wicked. They continuously work to mask and denature the grand reality. Words become less and less often and less and less substantively a reflection of reality, and more and more a mask of that reality – the mask the speaker wishes to place upon it. In turn, people’s expectations adjust.\n\nThings then give way to the third generation.\n\n### **Level Three – The Simple Child**\n\nThe Simple Child is not born simple. Nor is she stupid. The Simple Child is responding to incentives. She plays the game laid out before her.\n\nRaised by and around the wicked, The Simple Child lacks the expectation that symbols line up with reality. Those around her have been pretending the whole time. She wants to know how to pretend to do this pretending.\n\nShe does not have _or seek_ a useful model of physical reality. Such a model does not seem like it would be useful.\n\nShe notices instead that rewards and punishments in such a world are best navigated through asking what signals to send. So she seeks to understand symbols well enough to send the right signals.\n\nThus, the simple child asks the most basic question: “What is this?”, or “What is this celebration about?”\n\n**You shall say to him**: “We are commemorating the fact that **with a strong hand G‑d took us out of Egypt, from the house of slaves**” ([Exodus 13:14)](https://www.chabad.org/9874#v14).\n\nAs she speaks to you, so shall you speak to her. She wants to know _what this symbol means._ So we tell her what it means, and what and who is to be raised or lowered in status.\n\nWe don’t actually answer the question! We do not tell her _what this is. _\n\nShe isn’t really asking for that information. She isn’t ready for the answer. We don’t have that kind of time. We will. But not now. Not tonight.\n\nBut this is all rather tragic. Did we give up on her so easily? Has all been lost by this point? Can we not do better than to get her to think of us as her in-group whose actions should be imitated and signals sent?\n\nThis is one of the biggest problems of our age. If someone seeks to be nothing but a partisan, how does one get them to be more than that? If everyone is being judged on their partisanship, how is one to free them from that? To snap them out of it?\n\nThe text does not seem to have an answer. The Haggadahs I have used don’t even try to answer. This particular version advises:\n\nWe tell the simpleton how the Exodus occurred and how he too can experience a personal “Exodus”: Just as G‑d used a strong hand to “overcome” the attribute of justice, we too must use a strong hand to overcome those aspects of our personalities that impede our spiritual growth. We then experience a spiritual liberation from our personal enslavements.\n\nThat does not seem likely to get us much of anywhere. We’re talking in mumbo-jumbo in the hopes it will symbolically resonate. All we hold out is the promise of ‘spiritual liberation.’\n\nIt seems that all the Rabbis believe we can do, at this point, is damage control. Thus, we spend so much time trying to rescue the Wicked Child. That’s where there is still some hope. The Simple Child, in this model, is mostly a lost cause.\n\nBut we offer a way out. We note that we are commemorating _a fact. _\n\nWe link our explanation back to a concrete origin, as a first step in reorienting her attention. It’s a trick that just might work.\n\nThe ‘spiritual liberation’ is exactly this – to notice reality and be liberated from being trapped in meaningless symbols. To think for one’s self.\n\nThat’s why there is no talk about the Wise Child’s spiritual liberation. There is no need.\n\nThus, this model says the goal is purely to get the Simple Child to _pay attention._ The promises we make to her are to get her to participate at all, to be present. After that, she can be exposed to the arguments and discussions, to the details. She can notice what is actually going on, and think more on that level.\n\nThere is hope. Room to grow. She can still ask questions and care about the answers. Remember her opening question. She asks, what is this? Thus, she still knows on some level that there is a this and it has a what.\n\nWhat she is unable to do, if she is not helped out of her trap, is pass this remaining understanding along. The fourth generation is coming.\n\n### **Level Four – The One Who Does Not Know How to Ask**\n\nIt is frequently pointed out that the name of the fourth generation is profoundly weird.\n\nHave you ever met a _child_ who did not know how to ask?\n\nI have not. I’ve met _adults_ who _no longer_ know how to ask. Who have fully integrated level four. Who have _forgotten._ The fourth level ceases to know that the first level exists.\n\nThere is the temptation to not engage with the name. To treat it as some sort of metaphor.\n\nThe temptation is wrong. The fourth generation _does not know how to ask._\n\nThat does not _quite_ mean “_literally_ does not know how to ask anything at all”. But it also kind of does mean that.\n\nAsking requires realizing that there exist questions and answers. It requires believing that those questions and answers matter. That there is a ‘there there’ under all that.\n\nHe does not know that some things are while other things are not. If answers don’t matter, there can be no questions.\n\nEven if he did somehow want that information, _he doesn’t know how to ask about actual things._ Everything is a symbol referencing another symbol. There’s no way to get those symbols to reference the physical world. Thus, no way to ask a question.\n\nThis is the giveaway that we’ve been talking about simulacrum levels.\n\nThe one who does not know how to ask cannot ask for wisdom. For them, wisdom isn’t a thing.\n\nAnd they can’t ask how reality works. For them, reality isn’t a thing.\n\nWhat is to be done about this? We must talk in a way he might understand, that might cause him to realize there are things to be understood.\n\nThus:\n\nAs for The One Who Knows Not How To Ask—you must open up \\[the conversation\\] for him.\n\nAs it is written: You shall tell your child on that day: “It is because of this that G‑d acted for me when I left Egypt” (Exodus 13:8).\n\nWhat we are trying to communicate here is _basic cause and effect._ That there is a _this_ and it caused a _that._ Because of this, G-d acted for me when I left Egypt. The very idea of logic, of consequence, is lost upon him. Recover those, together with the idea that some things are and others are not, and the child can learn how to ask. All that matters, for now, is teaching this most basic lesson.\n\nTheir need to leave Egypt (which in Hebrew is literally “the narrow place”), is here about the need to realize this. Because we know things and seek knowledge, our world exists and can expand. We can do things, go places, not be trapped. We can be free.\n\nTwo levels. Because of these actions, things happened. Because of knowledge, one can take actions that do things.\n\nThe child’s participation in the Seder is not about any of that; they are just employing systems that attend the rituals that those around them participate in. They go through all the motions, but have no idea what they are doing.\n\nWhat about alternative interpretations of this stage?\n\nI have heard the suggestion that the fourth child is very young, and does not yet know how to speak. This seems clearly wrong.\n\nIf that was what was going on, the child would have a different name – the child who cannot (yet) speak – and our advice for them would be different. The child being unable to speak doesn’t make sense in the context of the text telling you to start the conversation for them. If they can’t talk, trying to start a conversation about the Exodus would be quite pointless. \n\nAnother reason to reject this interpretation is that this child does not yet know how to talk, but _does know how to ask._ He doesn’t know the words, but if you hang around a child who hasn’t yet learned to talk and pay attention it’s clear they can ask about basic things without words. \n\nAnother alternative interpretation, from the same Haggadah as above, is this angle:\n\n#### **Too Smart For Questions**\n\nThis fourth child may be a ritually observant Jew who fulfills all the customs of the Seder. But his Judaism is cold and dry. He does not feel a need for spiritual liberation. He has no questions about or real interest in the Exodus because he does not think of himself as being in exile.\n\nHe claims that he is not the excitable type and thus excuses his lifeless Jewish practice. Yet while he cannot muster any excitement for Judaism, he is easily exercised and engaged by material ambitions. He does not realize that his heart and mind are in exile, oblivious to the spiritual content of life.\n\nWe cannot begin by telling this Jew what G‑d did (as we tell the simple child); we must first inspire him to seek spiritual liberation. We therefore tell him:\n\n“G‑d did this for me when _I left Egypt_”—you too are in need of leaving Egypt.\n\nThe key insight here is that _we cannot begin the way we did with the Simple Child, by conveying information._ It won’t work! The Simple Child has redirected her curiosity, and does not yet much value information, but still understands that information is a thing.\n\nInformation would only bounce off The One Who Does Not Know How To Ask. Not being able to ask is merely a symptom. Spiritual liberation again means realizing knowledge exists at all, and is the necessary first step.\n\nHowever, I think the rest of this is importantly wrong. And it can be wrong in two ways.\n\nFirst, this child may be _misidentified. _\n\nIf the child is instead Simple, going through the ritual without feeling makes sense. The simple child can be told what this is and what to do, and then they go through the motions. It certainly would not occur to them to seek ‘spiritual revelation’ because life at the third level has no spiritual aspect.\n\nIf the child is instead Wicked, that is another potential explanation for this data. They are there to avoid punishment, or to score points, rather than to have the experience and/or better themselves.\n\nThe second way this is wrong is the most common mistake when those outside it try to model level four. It is the idea that he is easily exercised and engaged by material ambitions— that those sufficiently at level 4 are doing what the rest of us are doing, engaging in actions because of their model’s guess as to their consequences, in order to achieve particular ends.\n\nThat’s not how level 4 works. Such people don’t have goals. They have systems. The fourth child truly is lifeless and unexcited. When such people seem excited, it is because their systems think being excited is the next move, the way deep learning might suggest excitement be expressed at particular points. Nothing more.\n\nSuch strategies do often cash out in material ambitions, but that is not because such ambitions excited the person or a plan was formed to get them. The idea of having a plan or ambitions, or of there being a physical thing to be ambitious about, doesn’t parse for them the same way it does for others.\n\nThen there’s this other note:\n\nThe fourth child may actually want to ask but lacks confidence and fears being seen as a fool. The Haggadah instructs us to be sensitive to such people and to put them at ease by initiating conversation with them until they are comfortable sharing their thoughts confidently and clearly (R. Shlomo Alkabetz; Chida).\n\nThat is _definitely_ not the fourth child. The issue lies elsewhere.\n\nIt’s certainly a thing that happens. But the child it would be happening to would be the Wise child.\n\nKnowledge is desired. There’s social issues in the way, but that is _our fault. _\n\nThis is, of course, how it all begins. Children do not start out not knowing how to ask. The problem is caused by the adults who do not know how to answer.\n\nWe have somehow taught this child that asking questions can mean being a fool and that this is bad. We’ve answered his questions by telling him what we want them to see, or what the ritual response to their statement is, rather than by explaining what is and what is not. Without answers, what is a question?\n\nIt’s on us to fix it. Not them. The prescription here is a good idea, but seems importantly non-central. What is most important is taking away this idea that asking questions is bad or foolish, and setting up an expectation that questions get answers. If seek means ye might find, perhaps then ye will seek.\n\nOtherwise, engaging them in conversation will seem like torture rather than opening them up. It’s calling on kids unprompted in class to interrogate and humiliate them. It’s grading kids on ‘class participation’ where participation means [guessing the teacher’s password](https://www.lesswrong.com/posts/NMoLJuDJEms7Ku9XS/guessing-the-teacher-s-password). It is being polite at the dinner table until you can ask to be excused. If those around you will only respond to your level one inquiries with level three or four answers, either because that is all they know or they assume that is what you must seek, _then you too do not know how to ask._\n\nThus, once things move along sufficiently, the full _generation_ does not know how to ask, even those who remain wise, wicked or simple. When they attempt to ask, no answers come. Meaningful questioning ceases.\n\nThis is a common failure mode.\n\n### **Level Five – The Child Who Is Not There**\n\nDespite the failings of the four children, they all did the most important thing of all.\n\nThey showed up. They are present at the Seder.\n\nThat is important because, in this story and metaphor, the Seder (literally ‘order’) represents civilization. It is the ability to know things and pass on that knowledge. Also therefore to accomplish meaningful things, to gather the fruits of our labor.\n\nThe fourth generation _still sits down with_ the first one. They work together. To some extent, they must listen. This maintains an anchor.\n\nWithout the first generation’s renewal and participation, the process cannot be sustained.\n\nAs the generations progress, it becomes harder to draw the children into wisdom. Those who are drawn in become less rewarded for it, and more punished. The wicked understand, acknowledge and value the Wise—they depend on the Wise for their own cynical gain. The simple don’t see the point of wisdom. Those who do not know how to ask don’t even know wisdom is a thing.\n\n[Finally, there is the child who is not there.](https://thezvi.wordpress.com/2020/05/23/mazes-sequence-summary/) Not only do they not know how to ask, they are not connected to those that do. Value in the physical world ceases to be sustained at all. All is lost.\n\n### **Conclusion, Goals and Takeaways**\n\nThere were a few distinct goals here.\n\nThe first was that when I realized this lined up, it felt too good not to explore and share. Other goals were not necessary, and could be figured out later.\n\nThe second was to provide another look at the elephant that provides additional intuition pumps. When something is confusing, the more distinct ways to illustrate both the key points and the details around them, the more likely any given person is to find one that resonates. This also provides additional potential names and references for the levels.\n\nThe third was to reinforce in particular the idea that there is something profound that is lost at the fourth level, and to provide help understanding what that is and how that could be. That the fourth level loses its logical facilities. This version puts that so front and center that the loss of logic is explicit and much of the rest of the model is implicit. And it’s important enough that it has survived two thousand years of looking like nonsense.\n\nThe fourth, similar to the third, was to provide additional support for the idea of progression through the stages. And to look at how this first attempt tried to halt and even reverse that progression, in the hopes that we can use those strategies and/or find ways to do better.\n\nThis was a fun one. No doubt there are many other similar attempts out there. I can think of several but am curious what people come up with on their own. What are some others, real or fictional?\n\nIs GPT-3 a simulation of the child who does not know how to ask?\n\nI have now produced [a book-long sequence on Moral Mazes](https://thezvi.wordpress.com/2020/05/23/mazes-sequence-summary/), and a succession of posts on Simulacra levels. The central hope is to use this as background common knowledge concepts and jargon vocabulary going forward, and that others can do so as well."
    },
    "voteCount": 28
  },
  {
    "_id": "Jq73GozjsuhdwMLEG",
    "url": null,
    "title": "Superstimuli and the Collapse of Western Civilization",
    "slug": "superstimuli-and-the-collapse-of-western-civilization",
    "author": "Eliezer_Yudkowsky",
    "question": false,
    "tags": [
      {
        "name": "Superstimuli"
      },
      {
        "name": "Evolution"
      },
      {
        "name": "Rationality"
      },
      {
        "name": "Akrasia"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "At [least](http://news.xinhuanet.com/english/2005-11/01/content_3714003.htm) [three](http://phuze.com/2007/02/28/chinese-gamer-dies-playing-online-games.html) [people](http://news.bbc.co.uk/2/hi/technology/4137782.stm) have died playing online games for days without rest.  People have lost their spouses, jobs, and children to World of Warcraft. If people have the right to play video games - and it's hard to imagine a more fundamental right - [then the market is going to respond](/lw/h0/burchs_law/) by supplying the most _engaging_ video games that can be sold, to the point that exceptionally engaged consumers are removed from the gene pool.\n\nHow does a consumer product become so _involving_ that, after 57 hours of using the product, the consumer would rather use the product for one more hour than eat or sleep?  (I suppose one could argue that the consumer makes a rational decision that they'd rather play Starcraft for the next hour than live out the rest of their lives, but let's just not go there.  Please.)\n\nA candy bar is a _superstimulus:_ it contains more concentrated sugar, salt, and fat than anything that exists in the ancestral environment.   A candy bar matches taste buds that evolved in a hunter-gatherer environment, but it matches those taste buds much more strongly than anything that actually existed _in_ the hunter-gatherer environment.  The signal that once reliably correlated to healthy food has been hijacked, blotted out with a point in tastespace that wasn't in the training dataset - an impossibly distant outlier on the old ancestral graphs.  Tastiness, formerly representing the evolutionarily identified correlates of healthiness, has been reverse-engineered and perfectly matched with an artificial substance.  Unfortunately there's no equally powerful market incentive to make the resulting food item as healthy as it is tasty.  We can't taste healthfulness, after all.\n\nThe now-famous [Dove Evolution](http://www.youtube.com/watch?v=iYhCn0jf46U) video shows the painstaking construction of another superstimulus: an ordinary woman transformed by makeup, careful photography, and finally extensive Photoshopping, into a billboard model - a beauty impossible, unmatchable by human women in the unretouched real world.  Actual women are killing themselves (e.g. supermodels using cocaine to keep their weight down) to keep up with competitors that literally don't exist.\n\nAnd likewise, a video game can be so much more _engaging_ than mere reality, even through a simple computer monitor, that someone will play it without food or sleep until they literally die.  I don't know all the tricks used in video games, but I can guess some of them - challenges poised at the critical point between ease and impossibility, intermittent reinforcement, feedback showing an ever-increasing score, social involvement in massively multiplayer games.\n\nIs there a limit to the market incentive to make video games more engaging?  You might hope there'd be no incentive past the point where the players lose their jobs; after all, they must be able to pay their subscription fee.  This would imply a \"sweet spot\" for the addictiveness of games, where the mode of the bell curve is having fun, and only a few unfortunate souls on the tail become addicted to the point of losing their jobs.  As of 2007, playing World of Warcraft for 58 hours straight until you literally die _is_ still the exception rather than the rule.   But video game manufacturers compete against each other, and if you can make your game 5% more addictive, you may be able to steal 50% of your competitor's customers.  You can see how this problem could get a _lot_ worse.\n\nIf people have the right to be tempted - and that's what free will is all about - [the market is going to respond](/lw/h0/burchs_law/) by supplying as much temptation as can be sold.  The incentive is to make your stimuli 5% more tempting than those of your current leading competitors.  This continues well beyond the point where the stimuli become ancestrally anomalous superstimuli.  Consider how our standards of product-selling feminine beauty have changed since the advertisements of the 1950s.  And as candy bars demonstrate, the market incentive also continues well beyond the point where the superstimulus begins wreaking collateral damage on the consumer.\n\nSo why don't we just say no?  A key assumption of free-market economics is that, in the absence of force and fraud, people can always refuse to engage in a harmful transaction.  (To the extent this is true, a free market would be, not merely the _best_ policy on the whole, but a policy with [few or no downsides](/lw/gz/policy_debates_should_not_appear_onesided/).)\n\nAn organism that regularly passes up food will die, as some video game players found out the hard way.  But, on some occasions in the ancestral environment, a typically beneficial (and therefore tempting) act may in fact be harmful.  Humans, as organisms, have an unusually strong ability to perceive these special cases using abstract thought.  On the other hand we also tend to imagine lots of special-case consequences that don't exist, like ancestor spirits commanding us not to eat perfectly good rabbits.\n\nEvolution seems to have struck a compromise, or perhaps just aggregated new systems on top of old.  _Homo sapiens_ are still tempted by food, but our oversized prefrontal cortices give us a _limited_ ability to resist temptation.  Not unlimited ability - our ancestors with too much willpower probably starved themselves to sacrifice to the gods, or failed to commit adultery one too many times.  The video game players who died must have exercised willpower (in some sense) to keep playing for so long without food or sleep; the evolutionary hazard of self-control.\n\nResisting any temptation takes conscious expenditure of [an exhaustible supply of mental energy](http://www.stanford.edu/group/SITE/archive/SITE_2006/Web%20Session%207/Ozdenoren_Paper.pdf).  It is not in fact _true_ that we can \"just say no\" - not _just_ say no, without cost to ourselves. Even humans who won the birth lottery for willpower or foresightfulness still pay a price to resist temptation.  The price is just more easily paid.\n\nOur limited willpower evolved to deal with ancestral temptations; it may not operate well against enticements beyond anything known to hunter-gatherers.  Even where we successfully resist a superstimulus, it seems plausible that the effort required would deplete willpower much faster than resisting ancestral temptations.\n\nIs public display of superstimuli a negative externality, even to the people who say no?  Should we ban chocolate cookie ads, or storefronts that openly say \"Ice Cream\"?\n\nJust because a problem exists doesn't show (without further justification and a substantial burden of proof) that the government can fix it.  The regulator's career incentive does not focus on products that combine low-grade consumer harm with addictive superstimuli; it focuses on products with failure modes spectacular enough to get into the newspaper.  [Conversely, just because the government may _not_ be able to fix something, doesn't mean it _isn't_ going wrong.](/lw/h2/blue_or_green_on_regulation/)\n\nI leave you with a final argument from fictional evidence:  Simon Funk's online novel [After Life](http://interstice.com/%7Esimon/AfterLife/) depicts (among other plot points) the planned extermination of biological _Homo sapiens_ \\- not by marching robot armies, but by artificial children that are much cuter and sweeter and more fun to raise than real children. Perhaps the demographic collapse of advanced societies happens because the market supplies ever-more-tempting alternatives to having children, while the attractiveness of changing diapers remains constant over time.  Where are the advertising billboards that say \"BREED\"?  Who will pay professional image consultants to make arguing with sullen teenagers seem more alluring than a vacation in Tahiti?\n\n\"In the end,\" Simon Funk wrote, \"the human species was simply marketed out of existence.\""
    },
    "voteCount": 101
  },
  {
    "_id": "As9E3HfgED2zkTAfB",
    "url": null,
    "title": "A vote against spaced repetition",
    "slug": "a-vote-against-spaced-repetition",
    "author": "ancientcampus",
    "question": false,
    "tags": [
      {
        "name": "Spaced Repetition"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "LessWrong seems to be a big fan of spaced-repetition flashcard programs like Anki, Supermemo, or Mnemosyne. I used to be. After using them religiously for 3 years in medical school, I now categorically advise against using them for large volumes of memorization.\n\n\\[A caveat before people get upset: I think they appropriate in certain situations, and I have not tried to use them to learn a language, which seems its most popular use. More at the bottom.\\]\n\nA bit more history: I and 30 other students tried using Mnemosyne (and some used Anki) for multiple tests. At my school, we have a test approximately every 3 weeks, and each test covers about 75 pages of high-density outline-format notes. Many stopped after 5 or so such tests, citing that they simply did not get enough returns from their time. I stuck with it longer and used them more than anyone else, using them for 3 years.\n\nIncidentally, I failed my first year and had to repeat.\n\nBy the end of that third year (and studying for my Step 1 boards, a several-month process), I lost faith in spaced-repetition cards as an effective tool for my memorization demands. I later met with a learning-skills specialist, who felt the same way, and had better reasons than my intuition/trial-and-error:\n\n*   Flashcards are less useful to learning the “big picture”\n*   Specifically, if you are memorizing a large amount of information, there is often a hierarchy, organization, etc that can make leaning the whole thing easier, and you loose the constant visual reminder of the larger context when using flashcards.\n*   Flashcards do not take advantage of spatial, mapping, or visual memory, all of which the human mind is much better optimized for. It is not so well built to memorize pairs between seemingly arbitrary concepts with few to no intuitive links. My preferred methods are, in essence, hacks that use your visual and spatial memory rather than rote.\n\nHere are examples of the typical kind of things I memorize every day and have found flashcards to be surprisingly worthless for:\n\n*   The definition of Sjögren's syndrome\n*   The contraindications of Metronidazole\n*   The significance of a rise in serum αFP\n\nHere is what I now use in place of flashcards:\n\n1.  Ven diagrams/etc, to compare and contrast similar lists. (This is more specific to medical school, when you learn subtly different diseases.)\n2.  Mnemonic pictures. I have used this myself for years to great effect, and later learned it was taught by my study-skills expert, though I'm surprised I haven't found them formally named and taught anywhere else. The basic concept is to make a large picture, where each detail on the picture corresponds to a detail you want to memorize.\n3.  Memory palaces. I recently learned how to properly use these, and I'm a true believer. When I only had the general idea to “pair things you want to memorize with places in your room” I found it worthless, but after I was taught a lot of do's and don'ts, they're now my favorite way to memorize any list of 5+ items. If there's enough demand on LW I can write up a summary.\n\nSpaced repetition is still good for knowledge you need to retrieve *immediately*, when a 2-second delay would make it useless. I would still consider spaced-repetition to memorize some of the more rarely-used notes on the treble and bass clef, if I ever decide to learn to sight-read music properly. I make no comment on it's usefulness to learn a foreign language, as I haven't tried it, but if I were to pick one up I personally would start with a rosetta-stone-esque program.\n\nYour mileage may vary, but after seeing so many people try and reject them, I figured it was enough data to share. Mnemonic pictures and memory palaces are slightly time consuming when you're learning them. However, if someone has the motivation and discipline to make a stack of flashcards and study them every day indefinitely, then I believe learning and using those skills is a far better use of time."
    },
    "voteCount": 61
  },
  {
    "_id": "yffPyiu7hRLyc7r23",
    "url": null,
    "title": "Final Words",
    "slug": "final-words",
    "author": "Eliezer_Yudkowsky",
    "question": false,
    "tags": [
      {
        "name": "Fiction"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "Sunlight enriched air already alive with curiosity, as dawn rose on Brennan and his fellow students in the place to which Jeffreyssai had summoned them.\n\nThey sat there and waited, the five, at the top of the great glassy crag that was sometimes called Mount Mirror, and more often simply left unnamed.  The high top and peak of the mountain, from which you could see all the lands below and seas beyond.\n\n(Well, not _all_ the lands below, nor seas beyond.  So far as anyone knew, there was no place in the world from which all the world was visible; nor, equivalently, any kind of vision that would see through all obstacle-horizons.  In the end it was the top only of one particular mountain: there were other peaks, and from their tops you would see other lands below; even though, in the end, it was all a single world.)\n\n\"What do you think comes next?\" said Hiriwa.  Her eyes were bright, and she gazed to the far horizons like a lord.\n\nTaji shrugged, though his own eyes were alive with anticipation.  \"Jeffreyssai's last lesson doesn't have any obvious sequel that I can think of.  In fact, I think we've learned just about everything that I knew the _beisutsukai_ masters know.  What's left, then -\"\n\n\"Are the _real_ secrets,\" Yin completed the thought.\n\nHiriwa and Taji and Yin shared a grin, among themselves.\n\nStyrlyn wasn't smiling.  Brennan suspected rather strongly that Styrlyn was older than he had admitted.\n\nBrennan wasn't smiling either.  He might be young, but he kept high company, and had witnesssed some of what went on behind the curtains of the world.  Secrets had their price, always, that was the barrier that made them secrets; and Brennan thought he had a good idea of what this price might be.\n\nThere was a cough from behind them, at a moment when they had all happened to be looking in any other direction but that one.\n\nAs one, their heads turned.\n\nJeffreyssai stood there, in a casual robe that looked more like glass than any proper sort of mirrorweave.\n\nJeffreyssai stood there and looked at them, a strange abiding sorrow in those inscrutable eyes.\n\n\"Sen...sei,\" Taji started, faltering as that bright anticipation stumbled over Jeffreyssai's return look.  \"What's next?\"\n\n\"Nothing,\" Jeffreyssai said abruptly.  \"You're finished.  It's done.\"\n\nHiriwa, Taji, and Yin all blinked, a perfect synchronized gesture of shock.  Then, before their expressions could turn to outrage and objections -\n\n\"Don't,\" Jeffreyssai said.  There was real pain in it.  \"Believe me, it hurts me more than it hurts you.\"  He might have been looking at them; or at something far away, or long ago.  \"I don't know exactly what roads may lie before you - but yes, I know that you're not ready.  That I'm sending you out unprepared.  That everything I taught you is incomplete.  I know that what I said is not what you heard.  That I left out the one most important thing.  That the rhythm at the center of everything is missing and astray.  I know that you will harm yourself in the course of trying to use what I taught.  So that I, personally, will have shaped, in some fashion unknown to me, the very knife that will cut you...\"\n\n\"...that's the hell of being a teacher, you see,\" Jeffreyssai said.  Something grim flickered in his expression.  \"Nonetheless, you're _done_.  Finished, for now.  What lies between you and mastery is not another classroom.  We are fortunate, or perhaps not fortunate, that the road to power does not wend only through lecture halls.  Else the quest would be boring to the bitter end.  Still, I _cannot_ teach you; and so it is a moot point whether I would if I could.  There is no master here whose art is entirely inherited.  Even the _beisutsukai_ have never discovered how to teach certain things; it is possible that such an event has been prohibited.  And so you can only arrive at mastery by using to the fullest the techniques you have already learned, facing challenges and apprehending them, mastering the tools you have been taught _until they shatter in your hands -_\"\n\nJeffreyssai's eyes were hard, as though steeled in acceptance of unwelcome news.\n\n\"\\- and you are left in the midst of wreckage absolute.  That is where I, your teacher, am sending you.  You are not _beisutsukai_ masters.  I cannot create masters.  I have never known how to create masters.  Go forth, then, and fail.\"\n\n\"But -\" said Yin, and stopped herself.\n\n\"Speak,\" said Jeffreyssai.\n\n\"But then why,\" she said, \"why teach us anything in the first place?\"\n\nBrennan's eyelids flickered some tiny amount.\n\nIt was enough for Jeffreyssai.  \"Answer her, Brennan, if you think you know.\"\n\n\"Because,\" Brennan said, \"if we were not taught, there would be no chance at all of our becoming masters.\"\n\n\"Even so,\" said Jeffreyssai.  \"If you were not taught - then when you failed, you might simply think you had reached the limits of Reason itself.  You would be discouraged and bitter within your disaster.  You might not even realize when you had failed.  No; you have been shaped into something that may emerge from the wreckage, determined to remake your Art.  And then you may remember much that will help you.  I cannot create masters, but if you had not been taught, your chances would be - less.\"  His gaze passed over the group.  \"It should be obvious, but understand that you cannot provoke the moment of your crisis artificially.  To teach you something, the catastrophe must come to you as a surprise. You must go as far as you can, as best you can, and fail honestly.  The higher road begins after the Art seems to fail you; though the reality will be that it was you who failed your Art.\"\n\nBrennan made the gesture with his hand that indicated a question; and Jeffreyssai nodded in reply.\n\n\"Is this the only way in which Bayesian masters come to be, sensei?\"\n\n\"I do not know,\" said Jeffreyssai, from which the overall state of the evidence was obvious enough.  \"But I doubt there would ever be a road to mastery that goes only through the monastery.  We are the heirs in this world of mystics as well as scientists, just as the Competitive Conspiracy inherits from chessplayers alongside cagefighters.  We have turned our impulses to more constructive uses - but we must still stay on our guard against old failure modes.\"\n\nJeffreyssai took a breath.  \"Three flaws above all are common among the _beisutsukai._  The first flaw is to look just the slightest bit harder for flaws in arguments whose conclusions you would rather not accept.  If you cannot contain this aspect of yourself then every flaw you know how to detect will make you that much stupider.  This is the challenge which determines whether you possess the art or its opposite:  Intelligence, to be useful, must be used for something other than defeating itself.\"\n\n\"The second flaw is cleverness.  To invent great complicated plans and great complicated theories and great complicated arguments - or even, perhaps, plans and theories and arguments which are commended too much by their elegance and too little by their realism.  There is a widespread saying which runs:  'The vulnerability of the _beisutsukai_ is well-known; they are prone to be too clever.'  Your enemies will know this saying, if they know you for a _beisutsukai,_ so _you_ had best remember it also.  And you may think to yourself:  'But if I could never try anything clever or elegant, would my life even be worth living?'  This is why cleverness is still our chief vulnerability even after its being well-known, like offering a Competitor a challenge that seems fair, or tempting a Bard with drama.\"\n\n\"The third flaw is underconfidence, though it will seem to you like modesty or humility.  You have learned so many flaws in your own nature, some of them impossible to fix, that you may think that the rule of wisdom is to confess your own inability.  You may question yourself, without resolution or testing to determine the self-answers.  You may refuse to decide, pending further evidence, when a quick decision is necessary.  You may take advice you should not take.  Jaded cynicism and sage despair are less fashionable than once they were, but you may still be tempted by them.  Or you may simply - lose momentum.\"\n\nJeffreyssai fell silent then.\n\nHe looked from each of them, one to the other, with quiet intensity.\n\nAnd said at last, \"Those are my final words to you.  If and when we meet next, you and I - if and when you return to this place, Brennan, or Hiriwa, or Taji, or Yin, or Styrlyn - I will no longer be your teacher.\"\n\nAnd Jeffreyssai turned and walked swiftly away, heading back toward the glassy tunnel that had emitted him.\n\nEven Brennan was shocked.  For a moment they were all speechless.\n\nThen -\n\n\"Wait!\" cried Hiriwa.  \"What about our final words to you?  I never said -\"\n\n\"I will tell you what my _sensei_ told me,\" Jeffreyssai's voice came back as he disappeared.  \"You can thank me after you return, if you return.  One of you at least seems likely to come back.\"\n\n\"No, wait, I -\"  Hiriwa fell silent.  In the mirrored tunnel, the fractured reflections of Jeffreyssai were already fading.  She shook her head.  \"Never... mind, then.\"\n\nThere was a brief, uncomfortable silence, as the five of them looked at each other.\n\n\"Good heavens,\" Taji said finally.  \"Even the Bardic Conspiracy wouldn't try for that much drama.\"\n\nYin suddenly laughed.  \"Oh, this was nothing.  You should have seen my send-off when I left Diamond Sea University.\"  She smiled.  \"I'll tell you about it sometime - if you're interested.\"\n\nTaji coughed.  \"I suppose I should go back and... pack my things...\"\n\n\"I'm already packed,\" Brennan said.  He smiled, ever so slightly, when the other three turned to look at him.\n\n\"Really?\" Taji asked.  \"What was the clue?\"\n\nBrennan shrugged with artful carelessness.  \"Beyond a certain point, it is futile to inquire how a _beisutsukai_ master knows a thing -\"\n\n\"Come off it!\" Yin said.  \"You're not a _beisutsukai_ master _yet._\"\n\n\"Neither is Styrlyn,\" Brennan said.  \"But he has already packed as well.\"  He made it a statement rather than a question, betting double or nothing on his image of inscrutable foreknowledge.\n\nStyrlyn cleared his throat.  \"As you say.  Other commitments call me, and I have already tarried longer than I planned.  Though, Brennan, I do feel that you and I have certain mutual interests, which I would be happy to discuss with you -\"\n\n\"Styrlyn, my most excellent friend, I shall be happy to speak with you on any topic you desire,\" Brennan said politely and noncommitally, \"if we should meet again.\"  As in, not now.  He certainly wasn't selling out his Mistress this early in their relationship.\n\nThere was an exchange of goodbyes, and of hints and offers.\n\nAnd then Brennan was walking down the road that led toward or away from Mount Mirror (for every road is a two-edged sword), the glassy pebbles clicking under his feet.\n\nHe strode out along the path with purpose, vigor, and determination, just in case someone was watching.\n\nSome time later he stopped, stepped off the path, and moved just far enough away to prevent anyone from finding him unless they were deliberately following.\n\nThen Brennan sagged back against a tree-trunk.  It was a sparse clearing, with only a few trees poking out of the ground; not much present in the way of distracting scenery, unless you counted the red-tinted stream flowing out of a dark cave-mouth.  And Brennan deliberately faced away from that, leaving only the far grey of the horizons, and the blue sky and bright sun.\n\n_Now what?_\n\nHe had thought that the Bayesian Conspiracy, of all the possible trainings that existed in this world, would have cleared up his uncertainty about what to do with the rest of his life.\n\nPower, he'd sought at first.  Strength to prevent a repetition of the past.  \"If you don't know what you need, take power\" - so went the proverb.  He had gone first to the Competitive Conspiracy, then to the _beisutsukai._\n\nAnd now...\n\nNow he felt more lost than ever.\n\nHe could think of things that made him happy, but nothing that he really _wanted._\n\nThe passionate intensity that he'd come to associate with his Mistress, or with Jeffreyssai, or the other figures of power that he'd met... a life of pursuing small pleasures seemed to pale in comparison, next to that.\n\nIn a city not far from the center of the world, his Mistress waited for him (in all probability, assuming she hadn't gotten bored with her life and run away).  But to merely return, and then drift aimlessly, waiting to fall into someone else's web of intrigue... no.  That didn't seem like enough.\n\nBrennan plucked a blade of grass from the ground and stared at it, half-unconsciously looking for anything interesting about it; an old, old game that his very first teacher had taught him, what now seemed like ages ago.\n\n_Why did I believe that going to Mount Mirror would tell me what I wanted?_\n\nWell, decision theory did require that your utility function be consistent, but...\n\n_If the beisutsukai knew what I wanted, would they even tell me?_\n\nAt Mount Mirror they taught doubt.  So now he was falling prey to the third besetting sin of which Jeffreyssai had spoken, lost momentum, for he had learned to question the image that he held of himself in his mind.\n\n_Are you seeking power because that is your true desire, Brennan?_\n\n_Or because you have a picture in your mind, of the role_ _that you play as an ambitious young man, and you think it is what someone playing your role would do?_\n\nAlmost everything he'd done up until now, even going to Mount Mirror, had probably been the latter.\n\nAnd when he blanked out the old thoughts and tried to see the problem as though for the first time...\n\n...nothing much came to mind.\n\n_What do I want?_\n\nMaybe it wasn't reasonable to expect the _beisutsukai_ to tell him outright.  But was there anything they had taught him by which he might answer?\n\nBrennan closed his eyes and thought.\n\n_First, suppose there is something I would passionately desire.  Why would I not know what it is?_\n\n_Because I have not yet encountered it, or ever imagined it?_\n\n_Or because there is some reason I would not admit it to myself?_\n\nBrennan laughed out loud, then, and opened his eyes.\n\nSo simple, once you thought of it that way.  So obvious in retrospect.  _That_ was what they called a silver-shoes moment, and yet, if he hadn't gone to Mount Mirror, it wouldn't ever have occurred to him.\n\nOf _course_ there was something he wanted.  He knew _exactly_ what he wanted.  Wanted so desperately he could taste it like an sharp tinge on his tongue.\n\nIt just hadn't come to mind earlier, because... if he acknowledged his desire explicitly... then he also had to see that it was _difficult_.  High, high, above him.  Far out of his reach.  \"Impossible\" was the word that came to mind, though it was not, of course, physically impossible.\n\nBut once he asked himself if he preferred to wander aimlessly through his life - once it was put that way, the answer became obvious.  Pursuing the unattainable would make for a hard life, but not a sad one.  He could think of things that made him happy, either way.  And in the end - it was what he wanted.\n\nBrennan stood up, and took his first steps, in the exact direction of Shir L'or, the city that lies in the center of the world.  He had a plot to hatch, and he did not know who would be part of it.\n\nAnd then Brennan almost stumbled, when he realized that Jeffreyssai had already known.\n\n_One of you at least seems likely to come back..._\n\nBrennan had thought he was talking about Taji.  Taji had probably thought he was talking about Taji.  It was what Taji said he wanted.  But how reliable of an indicator was that, really?\n\nThere was a proverb about that very road he had just left:  _Whoever sets out from Mount Mirror seeking the impossible, will surely return.  \n_\n\nWhen you considered Jeffreyssai's last warning - and that the proverb said nothing of _succeeding_ at the impossible task itself - it was a less optimistic saying than it sounded.\n\nBrennan shook his head wonderingly.  How could Jeffreyssai possibly have known before Brennan knew himself?\n\nWell, beyond a certain point, it is futile to inquire how a _beisutsukai_ master knows a thing -\n\nBrennan halted in mid-thought.\n\nNo.\n\nNo, if he was going to become a _beisutsukai_ master himself someday, then he ought to figure it out.\n\nIt was, Brennan realized, a _stupid_ proverb.\n\nSo he walked, and this time, he thought about it carefully.\n\nAs the sun was setting, red-golden, shading his footsteps in light."
    },
    "voteCount": 119
  },
  {
    "_id": "BcYfsi7vmhDvzQGiF",
    "url": null,
    "title": "Taboo \"Outside View\"",
    "slug": "taboo-outside-view",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "Inside/Outside View"
      },
      {
        "name": "Forecasting & Prediction"
      },
      {
        "name": "Community"
      },
      {
        "name": "Rationality"
      },
      {
        "name": "Rationalist Taboo"
      },
      {
        "name": "Distinctions"
      },
      {
        "name": "Updated Beliefs (examples of)"
      }
    ],
    "tableOfContents": {
      "sections": [
        {
          "title": "What does “Outside view” mean now?",
          "anchor": "What_does__Outside_view__mean_now_",
          "level": 1
        },
        {
          "title": "Big List O’ Things People Describe As Outside View:",
          "anchor": "Big_List_O__Things_People_Describe_As_Outside_View_",
          "level": 2
        },
        {
          "title": "Big List O’ Things People Describe As Inside View:",
          "anchor": "Big_List_O__Things_People_Describe_As_Inside_View_",
          "level": 2
        },
        {
          "title": "What did “Outside view” mean originally?",
          "anchor": "What_did__Outside_view__mean_originally_",
          "level": 1
        },
        {
          "title": "This expansion of meaning is bad",
          "anchor": "This_expansion_of_meaning_is_bad",
          "level": 1
        },
        {
          "title": "Taboo Outside View; use this list of words instead",
          "anchor": "Taboo_Outside_View__use_this_list_of_words_instead",
          "level": 1
        },
        {
          "title": "Conclusion",
          "anchor": "Conclusion",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        },
        {
          "anchor": "comments",
          "level": 0,
          "title": "25 comments"
        }
      ],
      "headingsCount": 9
    },
    "contents": {
      "markdown": "> No one has ever seen an AGI takeoff, so any attempt to understand it must use these outside view considerations.\n\n—\\[Redacted for privacy\\]\n\n> What? That’s exactly backwards. If we had lots of experience with past AGI takeoffs, using the outside view to predict the next one would be a lot more effective.\n\n—My reaction\n\nTwo years ago I wrote a [deep-dive summary](https://aiimpacts.org/evidence-on-good-forecasting-practices-from-the-good-judgment-project-an-accompanying-blog-post/) of [*Superforecasting*](https://www.amazon.co.uk/Superforecasting-Science-Prediction-Philip-Tetlock/dp/1511358491) and the associated scientific literature. I learned about the “Outside view” / “Inside view” distinction, and the evidence supporting it. At the time I was excited about the concept and wrote: “*...I think we should do our best to imitate these best-practices, and that means using the outside view far more than we would naturally be inclined.*”\n\nNow that I have more experience, I think the concept is doing more harm than good in our community. The term is easily abused and its meaning has expanded too much. I recommend we permanently [taboo](https://www.lesswrong.com/tag/rationalist-taboo) “Outside view,” i.e. stop using the word and use more precise, less confused concepts instead. This post explains why. \n\nWhat does “Outside view” mean now?\n==================================\n\nOver the past two years I’ve noticed people (including myself!) do lots of different things in the name of the Outside View. I’ve compiled the following lists based on fuzzy memory of hundreds of conversations with dozens of people:\n\nBig List O’ Things People Describe As Outside View:\n---------------------------------------------------\n\n*   [**Reference class forecasting**](https://en.wikipedia.org/wiki/Reference_class_forecasting), the practice of computing a probability of an event by looking at the frequency with which similar events occurred in similar situations. Also called comparison class forecasting. \\[EDIT: [Eliezer rightly points out](https://forum.effectivealtruism.org/posts/wYpARcC4WqMsDEmYR/taboo-outside-view?commentId=CKPghNbk4RgnFsvZ3) that sometimes **reasoning by analogy** is undeservedly called reference class forecasting; reference classes are supposed to be held to a much higher standard, in which your sample size is larger and the analogy is especially tight.\\]\n*   **Trend extrapolation**, e.g. “AGI implies insane GWP growth; let’s forecast AGI timelines by extrapolating GWP trends.”\n*   **Foxy aggregation**, the practice of using multiple methods to compute an answer and then making your final forecast be some intuition-weighted average of those methods.\n*   **Bias correction, in others or in oneself**, e.g. “There’s a selection effect in our community for people who think AI is a big deal, and one reason to think AI is a big deal is if you have short timelines, so I’m going to bump my timelines estimate longer to correct for this.”\n*   **Deference to wisdom of the many**, e.g. expert surveys, or appeals to the efficient market hypothesis, or to conventional wisdom in some fairly large group of people such as the EA community or Western academia.\n*   **Anti-weirdness heuristic**, e.g. “How sure are we about all this AI stuff? It’s pretty wild, it sounds like science fiction or doomsday cult material.”\n*   **Priors**, e.g. “This sort of thing seems like a really rare, surprising sort of event; I guess I’m saying the prior is low / the outside view says it’s unlikely.” Note that I’ve heard this said even in cases where the prior is *not* generated by a reference class, but rather from raw intuition.\n*   **Ajeya’s timelines model** ([transcript of interview](https://www.lesswrong.com/posts/CuDYhLLXq6FuHvGZc/axrp-episode-7-5-forecasting-transformative-ai-from), [link to model](https://drive.google.com/drive/u/0/folders/15ArhEPZSTYU8f012bs6ehPS6-xmhtBPP))\n*   **… and probably many more I don’t remember**\n\nBig List O’ Things People Describe As Inside View:\n--------------------------------------------------\n\n*   **Having a gears-level model**, e.g. “Language data contains enough structure to learn human-level general intelligence with the right architecture and training setup; GPT-3 + recent theory papers indicate that this should be possible with X more data and compute…”\n*   **Having any model at all**, e.g. “I model AI progress as a function of compute and clock time, with the probability distribution over how much compute is needed shifting 2 OOMs lower each decade…”\n*   **Deference to wisdom of the few**, e.g. “the people I trust most on this matter seem to think…”\n*   **Intuition-based-on-detailed-imagining**, e.g. “When I imagine scaling up current AI architectures by 12 OOMs, I can see them continuing to get better at various tasks but they still wouldn’t be capable of taking over the world.”\n*   **Trend extrapolation combined with an argument for why that particular trend is the one to extrapolate**, e.g. “Your timelines rely on extrapolating compute trends, but I don’t share your inside view that compute is the main driver of AI progress.”\n*   **Drawing on subject matter expertise**, e.g. “my inside view, based on my experience in computational neuroscience, is that we are only a decade away from being able to replicate the core principles of the brain.”\n*   **Ajeya’s timelines model** (Yes, this is on both lists!)\n*   **… and probably many more I don’t remember**\n\nWhat did “Outside view” mean originally?\n========================================\n\nAs far as I can tell, *it basically meant reference class forecasting.* Kaj Sotala tells me the original source of the concept (cited by [the Overcoming Bias post](https://www.overcomingbias.com/2007/07/beware-the-insi.html) that brought it to our community) was [this paper](http://bear.warrington.ufl.edu/brenner/mar7588/Papers/kahneman-lovallo-mansci1993.pdf). Relevant quote: “The outside view is ... essentially ignores the details of the case at hand, and involves no attempt at detailed forecasting of the future history of the project. Instead, it focuses on the statistics of a class of cases chosen to be similar in relevant respects to the present one.” If you look at the text of *Superforecasting,* the “it basically means reference class forecasting” interpretation holds up. Also, “Outside view” redirects to “[reference class forecasting](https://en.wikipedia.org/wiki/Reference_class_forecasting)” in Wikipedia.\n\nTo head off an anticipated objection: I am not claiming that there is no  underlying pattern to the new, expanded meanings of “outside view” and “inside view.” I even have a few ideas about what the pattern is. For example, priors are sometimes based on reference classes, and even when they are instead based on intuition, that too can be thought of as reference class forecasting in the sense that intuition is often just unconscious, fuzzy pattern-matching, and pattern-matching is arguably a sort of reference class forecasting. And Ajeya’s model can be thought of as inside view relative to e.g. GDP extrapolations, while also outside view relative to e.g. deferring to Dario Amodei.\n\nHowever, it’s easy to see patterns everywhere if you squint. These lists are still pretty diverse. I could print out all the items on both lists and then mix-and-match to create new lists/distinctions, and I bet I could come up with several at least as principled as this one.\n\nThis expansion of meaning is bad\n================================\n\nWhen people use “outside view” or “inside view” without clarifying which of the things on the above lists they mean, I am left ignorant of what exactly they are doing and how well-justified it is. People say “On the outside view, X seems unlikely to me.” I then ask them what they mean, and sometimes it turns out they are using some reference class, complete with a dataset. (Example: [Tom Davidson’s four reference classes for TAI](https://www.openphilanthropy.org/blog/report-semi-informative-priors)). Other times it turns out they are just using the anti-weirdness heuristic. Good thing I asked for elaboration! \n\nSeparately, various people seem to think that the appropriate way to make forecasts is to (1) use some outside-view methods, (2) use some inside-view methods, but only if you feel like you are an expert in the subject, and then (3) do a weighted sum of them all using your intuition to pick the weights. This is *not* Tetlock’s advice, nor is it the lesson from the forecasting tournaments, *especially* if we use the nebulous modern definition of “outside view” instead of the original definition. (For my understanding of his advice and those lessons, see [this post](https://aiimpacts.org/evidence-on-good-forecasting-practices-from-the-good-judgment-project-an-accompanying-blog-post/), part 5. For an entire book written by Yudkowsky on why the aforementioned forecasting method is bogus, see *Inadequate Equilibria,* especially [this chapter](https://www.lesswrong.com/posts/o28fkhcZsBhhgfGjx/status-regulation-and-anxious-underconfidence). Also, I wish to emphasize that I myself was one of these people, at least sometimes, up until recently when I noticed what I was doing!)\n\nFinally, I think that too often the good epistemic standing of reference class forecasting is illicitly transferred to the other things in the list above. I already gave the example of the anti-weirdness heuristic; my second example will be bias correction: I sometimes see people go “There’s a bias towards X, so in accordance with the outside view I’m going to bump my estimate away from X.” *But this is a different sort of bias correction.* To see this, notice how they used *intuition* to decide how much to bump their estimate, and they [didn’t consider other biases towards or away from X](https://slatestarcodex.com/2019/07/17/caution-on-bias-arguments/). The original lesson was that biases could be corrected *by using reference classes*. Bias correction via intuition may be a valid technique, but it shouldn’t be called the outside view.\n\nI feel like it’s gotten to the point where, like, only 20% of uses of the term “outside view” involve reference classes. It seems to me that “outside view” has become an [applause light](https://www.lesswrong.com/posts/dLbkrPu5STNCBLRjr/applause-lights) and a smokescreen for over-reliance on intuition, the anti-weirdness heuristic, deference to crowd wisdom, correcting for biases in a way that is itself a gateway to more bias... \n\n![](https://lh6.googleusercontent.com/IjZQMAOgRVhrGW1tMj9j5EviSsg3hXlOD12zKVTOOnJfyY4tT7gzXOcHfUC-aZWRmT6lPXWSVOv43WSE5AvzF7hSl5vTqKcprBjj0j7iqaNxl1IFPn9I10bjUguQl471Vk4aGkca)\n\nI considered advocating for a return to the original meaning of “outside view,” i.e. reference class forecasting. But instead I say:\n\nTaboo Outside View; use this list of words instead\n==================================================\n\nI’m *not* recommending that we stop using reference classes! I love reference classes! I also love trend extrapolation! In fact, *for literally every tool on both lists above, I think there are situations where it is appropriate to use that tool.* Even the anti-weirdness heuristic.\n\nWhat I ask is that we stop using the *words* “outside view” and “inside view.” I encourage everyone to instead be more specific. Here is a big list of more specific words that I’d love to see, along with examples of how to use them:\n\n*   **Reference class forecasting**\n    *   “I feel like the best reference classes for AGI make it seem pretty far away in expectation.”\n    *   “I don’t think there are any good reference classes for AGI, so I think we should use other methods instead.”\n*   **Analogy**\n    *   Analogy is like a reference class but with lower standards; sample size can be small and the similarities can be weaker.\n    *   “I’m torn between thinking of AI as a technology vs. as a new intelligent species, but I lean towards the latter.”\n*   **Trend extrapolation**\n    *   “The GWP trend seems pretty relevant and we have good data on it”\n    *   “I claim that GPT performance trends are a better guide to AI timelines than compute or GWP or anything else, because they are more directly related.”\n*   **Foxy aggregation (a.k.a. multiple methods)**\n    *   “OK that model is pretty compelling, but to stay foxy I’m only assigning it 50% weight.”\n*   **Bias correction**\n    *   “I feel like things generally take longer than people expect, so I’m going to bump my timelines estimate to correct for this. How much? Eh, 2x longer seems good enough for now, but I really should look for data on this.”\n*   **Deference**\n    *   “I’m deferring to the markets on this one.”\n    *   “I think we should defer to the people building AI.”\n*   **Anti-weirdness heuristic**\n    *   “How sure are we about all this AI stuff? The anti-weirdness heuristic is screaming at me here.”\n*   **Priors**\n    *   “This just seems pretty implausible to me, on priors.”\n    *   (Ideally, say whether your prior comes from intuition or a reference class or a model. Jia points out “on priors” has similar problems as “on the outside view.”)\n*   [**Independent impression**](https://www.overcomingbias.com/2008/04/naming-beliefs.html)\n    *   i.e. what your view would be if you weren’t deferring to anyone.\n    *   “My independent impression is that AGI is super far away, but a lot of people I respect disagree.”\n*   **“It seems to me that…” **\n    *   i.e. what your view would be if you weren’t deferring to anyone *or* trying to correct for your own biases.\n    *   “It seems to me that AGI is just around the corner, but I know I’m probably getting caught up in the hype.”\n    *   Alternatively: “I feel like…”\n    *   Feel free to end the sentence with “...but I am not super confident” or “...but I may be wrong.”\n*   **Subject matter expertise**\n    *   “My experience with X suggests…”\n*   **Models**\n    *   “The best model, IMO, suggests that…” and “My model is…”\n    *   (Though beware, I sometimes hear people say “my model is...” when all they really mean is “I think…”)\n*   **Wild guess (a.k.a. Ass-number)**\n    *   “When I said 50%, that was just a wild guess, I’d probably have said something different if you asked me yesterday.”\n*   **Intuition**\n    *   “It’s not just an ass-number, it’s an intuition! Lol. But seriously though I have thought a lot about this and my intuition seems stable.”\n\nConclusion\n==========\n\nWhenever you notice yourself saying “outside view” or “inside view,” imagine a tiny Daniel Kokotajlo hopping up and down on your shoulder chirping “*Taboo outside view.*” \n\n*Many thanks to the many people who gave comments on a draft: Vojta, Jia, Anthony, Max, Kaj, Steve, and Mark. Also thanks to various people I ran the ideas by earlier.*"
    },
    "voteCount": 145
  },
  {
    "_id": "G7HgP9KTWAMSv6oEJ",
    "url": null,
    "title": "Bets and updating",
    "slug": "bets-and-updating",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "Betting"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "Suppose  the US presidential election is tomorrow.\nYou currently assign a probability of 50% to each outcome. (We are ignoring the small possibility that neither of the main party candidates will win).\n\nA man approaches you, and offers you a bet of \\$10, at 2-1 odds. In other words, if candidate one wins, he pays you $20, if candidate two wins, you pay him $10.\n\nShould you accept this bet? What if the bet was for \\$10000 instead?\nAssume that your utility is linear in dollars (or assume that the bet is for utilons instead, whatever). If not, why not? Try to think about this before reading on.\n\nThe answer is that it depends on your priors - in particular, it depends on how you interpret the evidence of being offered the bet. In general, if someone offers you a large bet on some outcome, it's probably safe to assume they have access to a reasonable amount of information about the outcome. Depending on how much information your own probability estimate is based on, you should update towards the odds they offered you.\n\nIf you update too little, and accept too many bets, you will lose a lot of money to people with better information than you. On the other hand, you can also go too far in the other direction. If your response to being offered a five-cent bet is to immediately update to accept their probabilities (and refuse the bet), you will be very easy to fool (although hard to exploit by betting).\n\n---\n\nNow suppose there are two superintelligences, Omega and Omicron.\nThey are both excellent at modelling both you and the presidential election.\nOmega has a strong preference for money, and a weak preference for having you believe false things about the presidential election.\nOmicron has this swapped - it wants you to believe that actual outcome of the election (which it has predicted) is extremely unlikely, and has a weak preference for money.\n\nOmega executes the following plan: It looks through a large number of possible bets, looking for ones that will give it a lot of money (according to its predicted outcome of the election). For each of them, it predicts whether you will, if offered, take the bet, or simply update your belief to be more accurate (if the bet is such that Omega wins money, it must be \"in the correct direction\"). It finds the best bet you will take (if any), and offers you this bet.\n\nOmicron does a similar thing, but instead looks for bets that you *won't* take - bets which will instead cause you to update strongly in the wrong direction, and *not* take the bet (since it doesn't want to give you money). Again, it finds the best bet you *won't* take (if any) and approaches you.\n\nYou are approached by someone and offered a bet, although you don't know if it's Omega or Omicron. What should your policy be?\n\nIf you accept a bet, the bet is likely to have come from Omega, and thus be extremely costly for you. So you shouldn't take the bet.\n\nOn the other hand, if you update strongly in the direction of the bet, it most likely came from Omicron, and this means it's probably an update in the wrong direction. So you shouldn't update.\n\nThis leaves you in the perplexing situation of believing that the bet is probably extremely good, but not wanting to take it."
    },
    "voteCount": 13
  },
  {
    "_id": "ts4KmAR8aJoGMawLb",
    "url": null,
    "title": "[LINK] Bets do not (necessarily) reveal beliefs",
    "slug": "link-bets-do-not-necessarily-reveal-beliefs",
    "author": null,
    "question": false,
    "tags": [],
    "tableOfContents": null,
    "contents": {
      "markdown": "When does a bet fail to reveal your true beliefs? When it hedges a risk in your portfolio.\n\nIf this claim does not immediately strike you as obviously true, you may benefit from reading [this post](http://noahpinionblog.blogspot.ca/2013/05/bets-do-not-necessarily-reveal-beliefs.html) by econblogger Noah Smith. Excerpt:\n\n> ...Alex Tabarrok famously declared that \"[a bet is a tax on bullshit](http://marginalrevolution.com/marginalrevolution/2012/11/a-bet-is-a-tax-on-bullshit.html)\".\n> \n> But this idea, attractive as it is, is not quite true. The reason is something that I've decided to call the Fundamental Error of Risk. It's a mistake that most people make (myself often included!), and that an intro finance class spends months correcting. The mistake is looking at the risk and return of single assets instead of total portfolios. Basically, the risk of an asset - which includes a bet! - is based mainly on how that asset relates to _other assets in your portfolio_."
    },
    "voteCount": 15
  },
  {
    "_id": "LzyN9wzEdfS3j5SmT",
    "url": null,
    "title": "Tricky Bets and Truth-Tracking Fields",
    "slug": "tricky-bets-and-truth-tracking-fields",
    "author": null,
    "question": false,
    "tags": [],
    "tableOfContents": null,
    "contents": {
      "markdown": "While visiting Oxford for MIRI’s [November 2013 workshop](http://intelligence.org/workshops/#november-2013), I had the pleasure of visiting a meeting of “Skeptics in the Pub” in the delightfully British-sounding town of _High Wycombe_ in _Buckinghamshire_. (Say that aloud in a British accent and try not to grin; I dare you!)\n\nI presented a mildly drunk intro to applied rationality, followed by a 2-hour Q&A that, naturally, wandered into the subject of why AI will inevitably eat the Earth. I must have been fairly compelling despite the beer, because at one point I noticed the bartenders were leaning uncomfortably over one end of the bar in order to hear me, ignoring thirsty customers at the other end.\n\nAnyhoo, at one point I was talking about the role of formal knowledge in applied rationality, so I explained [Solomonoff’s lightsaber](/lw/dhg/an_intuitive_explanation_of_solomonoff_induction/) and why it made me think the wave function never collapses.\n\nSomeone — I can’t recall who; let’s say “Bob” — wisely asked, “But if quantum interpretations all predict the same observations, what does it mean for you to say the wave function never collapses? What do you _anticipate_?” [\\[1\\]](#fn:1 \"see footnote\")\n\nNow, I don’t actually know whether the [usual proposals](http://plato.stanford.edu/entries/qm-manyworlds/#5) for experimental tests of collapse make sense, so instead I answered:\n\n> Well, I think theoretical physics is truth-tracking enough that it _eventually_ converges toward true theories, so one thing I anticipate as a result of favoring a no-collapse view is that a significantly greater fraction of physicists will reject collapse in 20 years, compared to today.\n\nHad Bob and I wanted to bet on whether the wave function collapses or not, that would have been an awfully tricky bet to settle. But if we roughly agree on the truth-trackingness of physics as a field, then we can use the consensus of physicists a decade or two from now as a proxy for physical truth, and bet on that instead.\n\nThis won’t work for some fields. For example, philosophy sometimes looks more like a random walk than a truth-tracking inquiry — or, more charitably, it tracks truth on the scale of _centuries_ rather than _decades_. For example, did you know that one year after the cover of _TIME_ asked “Is God dead?”, a philosopher named Alvin Plantinga launched a [renaissance in Christian philosophy](http://www.christianitytoday.com/ct/2008/july/13.22.html?paging=off), such that theism and Christian particularism were _more_ commonly defended by analytic philosophers in the 1970s than they were in the 1930s? I also have the impression that moral realism was a more popular view in the 1990s than it was in the 1970s, and that physicalism is less common today than it was in the 1960s, but I’m less sure about those.\n\nYou can also do this for bets that are hard to settle for a different kind of reason, e.g. an [apocalypse bet](/lw/ie/the_apocalypse_bet/). [\\[2\\]](#fn:2 \"see footnote\") Suppose Bob and I want to bet on whether smarter-than-human AI is technologically feasible. Trouble is, if it’s ever proven that superhuman AI is feasible, that event might overthrow the global economy, making it hard to collect the bet, or at least pointless.\n\nBut suppose Bob and I agree that AI scientists, or computer scientists, or technology advisors to first-world governments, or some other set of experts, is likely to converge toward the true answer on the feasibility of superhuman AI as time passes, as humanity learns more, etc. Then we can instead make a bet on whether it will be the case, 20 years from now, that a significantly increased or decreased fraction of those experts will think superhuman AI is feasible.\n\nOften, there won’t be acceptable polls of the experts at both times, for settling the bet. But domain experts typically have a general sense of whether some view has become more or less common in their field over time. So Bob and I could agree to poll a randomly chosen subset of our chosen expert community 20 years from now, asking them how common the view in question is at that time and how common it was 20 years earlier, and settle our bet that way.\n\nGetting the details right for this sort of long-term bet isn’t trivial, but I don't see a fatal flaw. Is there a fatal flaw in the idea that I’ve missed? [\\[3\\]](#fn:3 \"see footnote\")\n\n* * *\n\n1.  I can’t recall exactly how the conversation went, but it was _something_ like this. [ ↩](#fnref:1 \"return to article\")\n    \n2.  See also Jones, [How to bet on bad futures](http://econlog.econlib.org/archives/2012/09/how_to_bet_on_b.html). [ ↩](#fnref:2 \"return to article\")\n    \n3.  I also doubt I’m the first person to describe this idea in writing: please link to other articles making this point if you know of any. [ ↩](#fnref:3 \"return to article\")"
    },
    "voteCount": 16
  },
  {
    "_id": "msf7BHMrWTczbQckh",
    "url": null,
    "title": "Risk aversion does not explain people's betting behaviours",
    "slug": "risk-aversion-does-not-explain-people-s-betting-behaviours",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "Betting"
      },
      {
        "name": "Risk Management"
      },
      {
        "name": "World Modeling"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "Expected utility maximalisation is an excellent _prescriptive_ decision theory. It has all the [nice properties](http://en.wikipedia.org/wiki/Von_Neumann%E2%80%93Morgenstern_utility_theorem#The_axioms) that we want and need in a decision theory, and can be argued to be \"the\" ideal decision theory in some senses.\n\nHowever, it is completely wrong as a _descriptive_ theory of how humans behave. Those on this list are presumably aware of oddities like the [Allais paradox](http://en.wikipedia.org/wiki/Allais_paradox). But we may retain some notions that expected utility still has some descriptive uses, such as modelling [risk aversion](http://en.wikipedia.org/wiki/Risk_aversion). The story here is simple: each subsequent dollar gives less utility (the utility of money curve is concave), so people would need a premium to accept deals where they have a 50-50 chance of gaining or losing $100.\n\nAs a story or mental image, it's useful to have. As a formal model of human behaviour on small bets, it's spectacularly wrong. Matthew Rabin [showed why](http://users.nber.org/~rosenbla/econ311/syllabus/rabincallibration.pdf). If people are consistently slightly risk averse on small bets and expected utility theory is approximately correct, then they have to be massively, stupidly risk averse on larger bets, in ways that are clearly unrealistic. Put simply, the small bets behaviour forces their utility to become far too concave.\n\nFor illustration, let's introduce [Neville](http://en.wikipedia.org/wiki/Neville_Chamberlain). Neville is risk averse. He will reject a single 50-50 deal where he gains $55 or loses $50. He might accept this deal if he were really rich enough, and felt rich - say if he had $20 000 in capital, he would accept the deal. I hope I'm not painting a completely unbelievable portrait of human behaviour here! And yet expected utility maximalisation then predicts that if Neville had fifteen thousand dollars ($15 000) in capital, he would reject a 50-50 bet that either lost him fifteen hundred dollars ($1 500), or gained him a hundred and fifty thousand dollars ($150 000) - a ratio of a hundred to one between gains and losses!\n\nTo see this, first define define the marginal utility at $X dollars (MU($X)) as Neville's utility gain from one extra dollar (in other words, MU($X) = U($(X+1)) - U($X)). Since Neville is risk averse, MU($X) ≥ MU($Y) whenever Y>X. Then we get the following theorem:\n\n*   If Neville has $X and rejects a 50-50 deal where he gains $55 or loses $50, then MU($(X+55)) ≤ (10/11)*MU($(X-50)).\n\nThis theorem is a simple result of the fact that U($(X+55))-U($X) must be greater than 55\\*MU($(X+55)) (each dollar up from the Xth up to the (X+54)th must have marginal utility at least MU($(X+55))), while U($X)-U($(X-50)) must be less than 50\\*MU($(X-50)) (each dollar from the (X-50)th up to (X-1)th must have marginal utility at most MU($(X-50))). Since Neville rejects the deal, U($X) ≥ 1/2(U($(X+55)) + U($(X-50)), hence U($(X+55))-U($X) ≤ U($X)-U($(X-50)), hence 55\\*MU($(X+55)) ≤ 50\\*MU($(X-50)) and the result follows.\n\nHence if we scale Neville's utility so that MU($15000)=1, we know that MU($15105) ≤ 10/11, MU($15210) ≤ (10/11)^2^, MU($15315) ≤ (10/11)^3^, ... all the way up to to MU($19935) = MU($(15000 + 47*105)) ≤ (10/11)^47^. Summing the series of MU's from $15000 to $(15000+48*110) = $20040, we can see that\n\n*   U($20040) - U($15000) ≤ 105*(1+(10/11)+(10/11)^2^+...+(10/11)^47^) = 110*(1-(10/11)^48^)/(1-(10/11)) ≈ 1143.\n\nOne immediate result of that is that Neville, on $15000, will reject a 50-50 chance of losing $1144 versus gaining $5000. But it gets much worse! Let's assume that the bet is a 50-50 bet which involves losing $1500 - how far up in the benefits do we need to go before Neville will accept this bet? Now the marginal utilities below $15000 are bounded below, just as those above $15000 are bounded above. So summing the series down to $(15000-1500) = $13500 > $(15000 - 14*105):\n\n*   U($15000) - U($13500) ≥ 105*(1+(11/10)+...+(11/10)^13^) = 105*(1-(11/10)^14^)/(1-(11/10)) ≈ 2937.\n\nSo gaining $5040 from $15000 will net Neville (at most) 1143 utilons, while losing $1500 will lose him (at least) 2937. The marginal utility for dollars above the 20040th is at most (10/11)^47^ < 0.012. So we need to add at least (2937-1143-1)/0.012 ≈ 149416 extra dollars before Neville would accept the bet. So, as was said,\n\n*   If Neville had fifteen thousand dollars ($15 000), he would reject a 50-50 bet that either lost him fifteen hundred dollars ($1 500), or gained him a hundred a fifty thousand dollars ($150 000).\n\nThese bounds are not sharp - the real situation is worse than that. So expected utility maximisation is not a flawed model of human risk aversion on small bets - it's a completely ridiculous model of human risk aversion on small bets. Other variants such as [prospect theory](http://en.wikipedia.org/wiki/Prospect_theory) perform a better job at the descriptive task, though as usual in the social sciences, they are flawed as well."
    },
    "voteCount": 19
  },
  {
    "_id": "y5MxoeacRKKM3KQth",
    "url": null,
    "title": "Fallacies of Compression",
    "slug": "fallacies-of-compression",
    "author": "Eliezer_Yudkowsky",
    "question": false,
    "tags": [
      {
        "name": "Bucket Errors"
      },
      {
        "name": "Map and Territory"
      },
      {
        "name": "Philosophy of Language"
      },
      {
        "name": "Fallacy of Gray"
      },
      {
        "name": "Distinctions"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "\"The map is not the territory,\" as the saying goes.  The only life-size, atomically detailed, 100% accurate map of California is California.  But California has important regularities, such as the shape of its highways, that can be described using vastly less information—not to mention vastly less _physical material_—than it would take to describe every atom within the state borders.  Hence the _other_ saying:  \"The map is not the territory, but you can't fold up the territory and put it in your glove compartment.\"\n\nA paper map of California, at a scale of 10 kilometers to 1 centimeter (a million to one), doesn't have room to show the distinct position of two fallen leaves lying a centimeter apart on the sidewalk.  Even if the map tried to show the leaves, the leaves would appear as the same point on the map; or rather the map would need a feature size of 10 nanometers, which is a finer resolution than most book printers handle, not to mention human eyes.\n\nReality is very large—just the part we can see is billions of lightyears across.  But your map of reality is written on a few pounds of neurons, folded up to fit inside your skull.  I don't mean to be insulting, but your skull is tiny, comparatively speaking.\n\nInevitably, then, certain things that are distinct in reality, will be compressed into the same point on your map.\n\nBut what this [feels like from inside](/lw/no/how_an_algorithm_feels_from_inside/) is not that you say, \"Oh, look, I'm compressing two things into one point on my map.\"  What it _feels_ like from inside is that there is just _one_ thing, and you are seeing it.\n\nA sufficiently young child, or a sufficiently ancient Greek philosopher, would not know that there were such things as \"acoustic vibrations\" or \"auditory experiences\".  There would just be a single thing that happened when a tree fell; a single event called \"sound\".\n\nTo realize that there are _two_ distinct events, underlying _one_ point on your map, is an essentially _scientific_ challenge—a big, difficult scientific challenge.\n\nSometimes fallacies of compression result from confusing two known things under the same label—you know about acoustic vibrations, and you know about auditory processing in brains, but you call them both \"sound\" and so confuse yourself.  But the more dangerous fallacy of compression arises from having _no idea whatsoever_ that two distinct entities even _exist_.  There is just one mental folder in the filing system, labeled \"sound\", and everything thought about \"sound\" drops into that one folder.  It's not that there are two folders with the same label; there's just a single folder.  By default, the map is compressed; why would the brain create two mental buckets where one would serve?\n\nOr think of a mystery novel in which the detective's critical insight is that one of the suspects has an identical twin.  In the course of the detective's ordinary work, his job is just to observe that Carol is wearing red, that she has black hair, that her sandals are leather—but all these are _facts about_ Carol.  It's easy enough to question an individual fact, like WearsRed(Carol) or BlackHair(Carol).  Maybe BlackHair(Carol) is false.  Maybe Carol dyes her hair.  Maybe BrownHair(Carol).  But it takes a subtler detective to wonder if the Carol in WearsRed(Carol) and BlackHair(Carol)—the Carol file into which his observations drop—should be split into _two_ files.  Maybe there are two Carols, so that the Carol who wore red is not the same woman as the Carol who had black hair.\n\nHere it is the very act of _creating_ two different buckets that is the stroke of genius insight.  'Tis easier to question one's facts than one's ontology.\n\nThe map of reality contained in a human brain, unlike a paper map of California, can expand dynamically when we write down more detailed descriptions.  But what this feels like from inside is not so much zooming in on a map, as fissioning an indivisible atom—taking _one thing_ (it felt like one thing) and splitting it into two or more things.\n\nOften this manifests in the creation of new words, like \"acoustic vibrations\" and \"auditory experiences\" instead of just \"sound\".  Something about creating the new name seems to allocate the new bucket.  The detective is liable to start calling one of his suspects \"Carol-2\" or \"the Other Carol\" almost as soon as he realizes that there are two of them.\n\nBut expanding the map isn't always as simple as generating new city names.  It is a stroke of scientific insight to realize that such things as acoustic vibrations, or auditory experiences, even _exist._\n\nThe obvious modern-day illustration would be words like \"intelligence\" or \"consciousness\".  Every now and then one sees a press release claiming that a research has \"explained consciousness\" because a team of neurologists investigated a 40Hz electrical rhythm that might have something to do with cross-modality binding of sensory information, or because they investigated the reticular activating system that keeps humans awake.  That's an extreme example, and the usual failures are more subtle, but they are of the same kind.  The part of \"consciousness\" that people find most interesting is reflectivity, self-awareness, realizing that the person I see in the mirror is \"me\"; that and the hard problem of subjective experience as distinguished by Chalmers.  We also label \"conscious\" the state of being awake, rather than asleep, in our daily cycle.  But they are all different concepts going under the same name, and the underlying phenomena are different scientific puzzles.  You can explain being awake without explaining reflectivity or subjectivity.\n\nFallacies of compression also underlie the bait-and-switch technique in philosophy—you argue about \"consciousness\" under one definition (like the ability to think about thinking) and then apply the conclusions to \"consciousness\" under a different definition (like subjectivity).  Of course it may be that the two are the same thing, but if so, genuinely _understanding_ this fact would require _first_ a conceptual split and _then_ a genius stroke of reunification.\n\nExpanding your map is (I say again) a _scientific_ challenge: part of the art of science, the skill of inquiring into the world.  (And of course you cannot solve a scientific challenge by appealing to dictionaries, nor master a complex skill of inquiry by saying \"I can define a word any way I like\".)  Where you see a single confusing thing, with protean and self-contradictory attributes, it is a good guess that your map is cramming too much into one point—you need to pry it apart and allocate some new buckets.  This is not like _defining_ the single thing you see, but it _does_ often follow from figuring out how to talk about the thing without using a single mental handle.\n\nSo the skill of prying apart the map is linked to the [rationalist version](/lw/nv/replace_the_symbol_with_the_substance/) of [Taboo](/lw/nu/taboo_your_words/), and to the wise use of words; because words often represent the points on our map, the labels under which we file our propositions and the buckets into which we drop our information.  Avoiding a single word, or allocating new ones, is often part of the skill of expanding the map."
    },
    "voteCount": 62
  },
  {
    "_id": "rnFLc3E5Y4FP8TSGC",
    "url": null,
    "title": "The Biggest Problem in your Life",
    "slug": "the-biggest-problem-in-your-life",
    "author": null,
    "question": false,
    "tags": [],
    "tableOfContents": null,
    "contents": {
      "markdown": "Mathematician Richard Hamming used to ask scientists in other fields \"What are the most important problems in your field?\" partly so he could troll them by asking \"Why aren't you working on them?\" and partly because getting asked this question is really useful for focusing people's attention on what matters. CFAR developed the technique of \"Hamming Questions\" as different prompts to get your brain to (actually) think about the biggest problems, bottlenecks, and unspoken desires in _your_ life.\n\nPlan for the meetup:\n\n1\\. I will go through a list of prompts to get you to think of the various bugs in your life and the underlying \"Hamming problems\" that lie at the core of what's preventing you from being all you want to be.\n\n2\\. We will break into pairs, and each person will describe their Hamming problems and why they haven't solved them yet to another person, while looking in their eyes. The other person doesn't have to hear what's being said (I'll provide ear plugs for those who want them). The goal of saying this out loud is to let yourself be vulnerable and make the problem real in your attention.\n\n3\\. If enough people wish to discuss their Hamming problems, we will break into small groups where one person will present their problem and (hopefully) receive compassion, clarification, and feedback from the others.\n\nThese exercises will likely be somewhat outside the comfort zone for most of us, and are partly intended to stretch that comfort zone. You are very welcome to drop out after part 1 (which will take 30-45 by itself) or part 2; Luna has many lovely spaces to hang out in. However, if you choose to participate in any of the parts, please be committed and fully present. Also, please be on time. The success of the exercise depends a lot on the atmosphere created in the space.\n\nPlease bring:\n\n1\\. Something to write with/on, preferably paper-based and not electronic.\n\n2\\. Ear plugs, in case I misjudge the total number needed or you prefer your own.\n\n3\\. An open and introspective mindset.\n\nThank you to the Lunatics for graciously hosting this event at Luna Labs!\n\n(For more details and exact location, please join our [Google group](https://groups.google.com/forum/#!forum/overcomingbiasnyc))."
    },
    "voteCount": 2
  },
  {
    "_id": "L6Ktf952cwdMJnzWm",
    "url": null,
    "title": "Motive Ambiguity",
    "slug": "motive-ambiguity",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "Signaling"
      },
      {
        "name": "Moral Mazes"
      },
      {
        "name": "Tradeoffs"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "Central theme in: [Immoral Mazes Sequence](https://thezvi.wordpress.com/2020/05/23/mazes-sequence-summary/), but this generalizes.\n\nWhen looking to succeed, [pain is not the unit of effort](https://www.lesswrong.com/posts/bx3gkHJehRCYZAF3r/pain-is-not-the-unit-of-effort), and [money is a, if not the, unit of caring](https://www.lesswrong.com/posts/ZpDnRCeef2CLEFeKM/money-the-unit-of-caring#:~:text=To%20a%20first%20approximation%2C%20money,you%20care%20about%20the%20burrito.). \n\nOne is not always looking to succeed.\n\nHere is a common type of problem.\n\nYou are married, and want to take your spouse out to a romantic dinner. You can choose the place your spouse loves best, or the place you love best.\n\nA middle manager is working their way up the corporate ladder, and must choose how to get the factory to improve its production of widgets.  A middle manager must choose how to improve widget production. He can choose a policy that improperly maintains the factory and likely eventually it poisons the water supply, or a policy that would prevent that but at additional cost.\n\nA politician can choose between a bill that helps the general population, or a bill that helps their biggest campaign contributor.\n\nA start-up founder can choose between building a quality product without technical debt, or creating a hockey stick graph that will appeal to investors.\n\nYou can choose to make a gift yourself. This would be expensive in terms of your time and be lower quality, but be more thoughtful and cheaper. Or you could buy one in the store, which would be higher quality and take less time, but feel generic and cost more money. \n\nYou are cold. You can buy a cheap scarf, or a better but more expensive scarf. \n\nThese are trade-offs. Sometimes one choice will be made, sometimes the other.\n\nNow consider another type of problem.\n\nYou are married, and want to take your spouse out to a romantic dinner. You could choose a place you both love, or a place that only they love. You choose the place you don’t love, so they will know how much you love them. After all, you didn’t come here for the food.\n\nA middle manager must choose how to improve widget production. He can choose a policy that improperly maintains the factory and likely eventually poisons the water supply, or a policy that would prevent that at no additional cost. He knows that when he is up for promotion, management will want to know the higher ups can count on him to make the quarterly numbers look good and not concern himself with long term issues or what consequences might fall on others. If he cared about not poisoning the water supply, he would not be a reliable political ally. Thus, he chooses the neglectful policy. \n\nA politician can choose between two messages that affirm their loyalty: Advocating a beneficial policy, or advocating a useless and wasteful policy. They choose useless, because the motive behind advocating a beneficial policy is ambiguous. Maybe they wanted people to benefit!\n\nA start-up founder can choose between building a quality product without technical debt and creating a hockey stick graph with it, or building a superficially similar low-quality product with technical debt and using that. Both are equally likely to create the necessary graph, and both take about the same amount of effort, time and money. They choose the low-quality product, so the venture capitalists can appreciate their devotion to creating a hockey stick graph. \n\nYou can choose between making a gift and buying a gift.  You choose to make a gift, because you are rich and buying something from a store would be meaningless. Or you are poor, so you buy something from a store, because a handmade gift wouldn’t show you care. \n\nOld joke: One Russian oligarch says, “Look at my scarf! I bought it for ten thousand rubles.” The other says, “That’s nothing, I bought the same scarf for twenty thousand rubles.” \n\nWhat these examples have in common is that there is a strictly better action and a strictly worse action, in terms of physical consequences. In each case, the protagonist chooses the worse action _because it is worse._\n\nThis choice is made as a costly signal. In particular, to avoid _motive ambiguity. _\n\nIf you choose something better over something worse, you will be suspected of doing so _because_ it was better rather than worse. \n\nIf you choose something worse over something better, not only do you show how little you care about making the world better, you show that you care more _about people noticing and trusting this lack of caring_. It shows your values and loyalties. \n\nIn the first example, you care more about your spouse’s view of how much you care about their experience than you care about your own experience. \n\nIn the second example, [you care more about being seen as focused on your own success](https://thezvi.wordpress.com/2020/05/23/mazes-sequence-summary/#:~:text=The%20Immoral%20Mazes%20sequence%20is,for%20their%20own%20organizational%20advancement.) than you care about outcomes you won’t be responsible for.\n\nIn the third example, [you care more about being seen as loyal](https://en.wikipedia.org/wiki/Pledge_of_Allegiance) than about improving the world by being helpful.\n\nIn the fourth example, [you care about those making decisions over your fate believing that you will focus on the things they believe the next person deciding your fate will care about](https://thezvi.wordpress.com/2015/06/30/the-thing-and-the-symbolic-representation-of-the-thing/), so they can turn a profit. They don’t want you distracted by things like product quality. \n\nIn the old joke, the oligarchs want to show they have money to burn, and that they care a lot about showing they have lots of money to burn. That they _actively want_ to [Get Got](https://thezvi.wordpress.com/2017/09/23/out-to-get-you/) to show they don’t care. If someone thought the scarf was bought for [mundane utility](https://tvtropes.org/pmwiki/pmwiki.php/Main/MundaneUtility), that wouldn’t do at all. \n\nOne highly effective way to get many people to spend money is to give them a choice to either spend the money, or be slightly socially awkward and admit that they care about not spending the money. Don’t ask what the wine costs, it would ruin the evening.\n\nThe warning of [Out to Get You](https://thezvi.wordpress.com/2017/09/23/out-to-get-you/) is insufficiently cynical. The motive is often not to get your resources, and is instead purely to make your life worse.\n\n[Conflict theorists](https://slatestarcodex.com/2018/01/24/conflict-vs-mistake/) are often insufficiently cynical. We _hope_ the war is about whether to enrich the wealthy or help the people. Often the war is over whether to aim to destroy the wealthy, or aim to hurt the people.\n\nIn [simulacra terms](https://thezvi.wordpress.com/2020/06/15/simulacra-and-covid-19/), these effects are strongest when one desires to be seen as motivated on level three, but these dynamics are potentially present to an important extent for motivations at all levels. Note also that one is not motivated by this dynamic to destroy something unless you might plausibly favor it. If and only if [everybody knows](https://thezvi.wordpress.com/2019/07/02/everybody-knows/) you don’t care about poisoning the river, it is safe to not poison it. \n\nThis generalizes to time, to pain, to every preference. Hence anything that wants your loyalty will do its best to ask you to sacrifice and destroy everything you hold dear, because you care about it, to demonstrate you care more about other things.\n\nWorst of all, _none of this assumes a zero-sum mentality._ At all.\n\nSuch behavior _doesn’t even need one._\n\nIf one has a true zero-sum mentality, as many do, or one maps all results onto a zero-sum social dynamic, all of this is overthinking. All becomes simple. Your loss is my gain, so I want to cause you as much loss as possible.\n\nPain need not be the unit of effort if it is the unit of scoring.\n\nThe world would be better if people treated more situations like the first set of problems, and less situations like the second set of problems. How to do that?"
    },
    "voteCount": 79
  },
  {
    "_id": "S69ogAGXcc9EQjpcZ",
    "url": null,
    "title": "A Brief History of LessWrong\n",
    "slug": "a-brief-history-of-lesswrong-1",
    "author": "Ruby",
    "question": false,
    "tags": [
      {
        "name": "History"
      },
      {
        "name": "History of Rationality"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "In 2006, [Eliezer Yudkowsky](https://www.lesswrong.com/users/eliezer_yudkowsky), [Robin Hanson](https://www.lesswrong.com/users/robin_hanson2), and others began writing on _[Overcoming Bias](https://www.overcomingbias.com/about)_, a group blog with the general theme of how to move one’s beliefs closer to reality despite biases such as overconfidence and wishful thinking. In 2009, after the topics drifted more widely, Eliezer moved to a new community blog_, LessWrong_.\n\nLessWrong was seeded with series of daily blog posts written by Eliezer, originally known as _The Sequences_, and more recently compiled into an edited volume, _[Rationality: A-Z](https://www.lesswrong.com/rationality)._ These writings attracted a large community of readers and writers interested in the art of human rationality.\n\nIn 2015-2016 the site underwent a steady decline of activity leading some to declare the site dead. In 2017, a team led by Oliver Habryka took over the administration and development of the site, relaunching it on an [entirely new codebase](https://github.com/LessWrong2/Lesswrong2) later that year.\n\nThe new project, dubbed LessWrong 2.0, was the first time LessWrong had a full-time dedicated development [team](https://www.lesswrong.com/posts/aG74jJkiPccqdkK3c/the-lesswrong-team-page-under-construction) behind it instead of only volunteer hours. Site activity recovered from the 2015-2016 decline and [has remained at steady levels](https://www.lesswrong.com/posts/9dA6GfuDca3Zh3RMa/data-analysis-of-lw-activity-levels-age-distribution-of-user) since the launch.\n\nThe team behind LessWrong 2.0 has ambitions not limited to maintaining the original LessWrong community blog and forum. The LessWrong 2.0 team conceives of itself more broadly as an organization attempting to build community, culture, and technology which will drive intellectual progress on the world’s most pressing problems."
    },
    "voteCount": 22
  },
  {
    "_id": "fJKbCXrCPwAR5wjL8",
    "url": null,
    "title": "What is control theory, and why do you need to know about it?",
    "slug": "what-is-control-theory-and-why-do-you-need-to-know-about-it",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "Perceptual Control Theory"
      }
    ],
    "tableOfContents": {
      "sections": [
        {
          "title": "1. Alien Space Bats have abducted you.",
          "anchor": "1__Alien_Space_Bats_have_abducted_you_",
          "level": 1
        },
        {
          "title": "2. Two descriptions of the same thing that both make sense but don't fit together.",
          "anchor": "2__Two_descriptions_of_the_same_thing_that_both_make_sense_but_don_t_fit_together_",
          "level": 1
        },
        {
          "title": "3. Why it matters.",
          "anchor": "3__Why_it_matters_",
          "level": 1
        },
        {
          "title": "4. Conclusion.",
          "anchor": "4__Conclusion_",
          "level": 1
        },
        {
          "title": "5. Things I have not yet spoken of.",
          "anchor": "5__Things_I_have_not_yet_spoken_of_",
          "level": 1
        },
        {
          "title": "6. WARNING: Autonomous device",
          "anchor": "6__WARNING__Autonomous_device",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        },
        {
          "anchor": "comments",
          "level": 0,
          "title": "48 comments"
        }
      ],
      "headingsCount": 8
    },
    "contents": {
      "markdown": "This is long, but it's the shortest length I could cut from the material and have a complete thought.\n\n**1\\. Alien Space Bats have abducted you.**\n\nIn the spirit of [this posting](http://www.overcomingbias.com/2008/10/mundane-magic.html), I shall describe a magical power that some devices have. They have an intention, and certain means available to achieve that intention. They succeed in doing so, despite knowing almost nothing about the world outside. If you push on them, they push back. Their magic is not invincible: if you push hard enough, you may overwhelm them. But within their limits, they will push back against anything that would deflect them from their goal. And yet, they are not even aware that anything is opposing them. Nor do they act passively, like a nail holding something down, but instead they draw upon energy sources to actively apply whatever force is required. They do not know you are there, but they will struggle against you with all of their strength, precisely countering whatever you do. It seems that they have a sliver of that Ultimate Power of shaping reality, despite their almost complete ignorance of that reality. Just a sliver, not a whole beam, for their goals are generally simple and limited ones. But they pursue them relentlessly, and they absolutely will not stop until they are dead.\n\nYou look inside one of these devices to see how it works, and imagine yourself doing the same task...\n\n> _Alien Space Bats have abducted you. You find yourself in a sealed cell, featureless but for two devices on the wall. One seems to be some sort of meter with an unbreakable cover, the needle of which wanders over a scale marked off in units, but without any indication of what, if anything, it is measuring. There is a red blob at one point on the scale. The other device is a knob next to the meter, that you can turn. If you twiddle the knob at random, it seems to have some effect on the needle, but there is no fixed relationship. As you play with it, you realise that you very much want the needle to point to the red dot. Nothing else matters to you. Probably the ASBs' doing. But you do not know what moves the needle, and you do not know what turning the knob actually does. You know nothing of what lies outside the cell. There is only the needle, the red dot, and the knob. To make matters worse, the red dot also jumps along the scale from time to time, in no particular pattern, and nothing you do seems to have any effect on it. You don't know why, only that wherever it moves, you must keep the needle aligned with it._\n> \n> _Solve this problem._\n\nThat is what it is like, to be one of these magical devices. They are actually commonplace: you can find them everywhere.\n\nThey are the thermostat that keeps your home at a constant temperature, the cruise control that keeps your car at a constant speed, the power supply that provides a constant voltage to your computer's circuit boards. The magical thing is how little they need to know to perform their tasks. They have just the needle, the mark on the scale, the knob, and hardwired into them, a rule for how to turn the knob based only on what they see the needle and the red dot do. They do not need to sense the disturbing forces, or predict the effects of their actions, or learn. The thermostat does not know when the sun comes out. The cruise control does not know the gradient of the road. The power supply does not know why or when the mains voltage or the current demand will change. They model nothing, they predict nothing, they learn nothing. They do not know what they are doing. But they work.\n\nThese things are called control systems. A control system is a device for keeping a variable at a specified value, regardless of disturbing forces in its environment that would otherwise change it. It has two inputs, called the _perception_ and the _reference_, and one output, called the _output_ or the _action_. The output depends only on the perception and the reference (and possibly their past histories, integrals, or derivatives) and is such as to always tend to bring the perception closer to the reference.\n\nWhy is this important for LW readers?\n\n**2\\. Two descriptions of the same thing that both make sense but don't fit together.**\n\nI shall come to that via an autobiographical detour. In the mid-90's, I came across William Powers' book, [_Behavior: the Control of Perception_](http://www.amazon.com/Behavior-Perception-William-T-Powers/dp/0964712172/), in which he set out an analysis of human behaviour in terms of control theory. (Powers' profession was -- he is retired now -- control engineering.) It made sense to me, and it made nonsense of every other approach to psychology. He gave it the name of Perceptual Control Theory, or PCT, and the title of his book expresses the fundamental viewpoint: all of the behaviour of an organism is the output of control systems, and is performed with the purpose of controlling perceptions at desired reference values. Behaviour is the control of perception.\n\nThis is 180 degrees around from the behavioural stimulus-response view, in which you apply a stimulus (a perception) to the organism, and that causes it to emit a response (a behaviour). I shall come back to why this is wrong below. But there is no doubt that it is wrong. Completely, totally wrong. To this audience I can say, as wrong as theism. That wrong. Cognitive psychology just adds layers of processing between stimulus and response, and fares little better.\n\nI made a simulation of a walking robot whose control systems were designed according to the principles of PCT, and it works. It stands up, walks over uneven terrain, and navigates to food particles. (My earliest simulation is still on the web in the form of [this Java applet](http://www2.cmp.uea.ac.uk/~jrk/Robotics/Archy/Archy.html).) It resists a simulated wind, despite having no way to perceive it. It cannot see, sensing the direction of food only by the differential scent signals from its antennae. It walks on uneven terrain, despite having no perception of the ground other than the positions of its feet relative to its body.\n\nAnd then, a year or two ago, I came upon Overcoming Bias, and before that, Eliezer's [article on Bayes' theorem](http://yudkowsky.net/rational/bayes). (Anyone who has not read that article should do so: besides being essential background to OB and LW, it's a good read, and when you have studied it, you will intuitively know why a positive result on a screening test for a rare condition may not be telling you very much.) Bayes' theorem itself is a perfectly sound piece of mathematics, and has practical applications in those cases where you actually have the necessary numbers, such as in that example of screening tests.\n\nBut it was being put forward as something more than that, as a fundamental principle of reasoning, even when you don't have the numbers. Bayes' Theorem as the foundation of rationality, entangling one's brain with the real world, allowing the probability mass of one's beliefs to be pushed by the evidence, acting to funnel the world through a desired tunnel in configuration space. And it was presented as even more than a technique to be learned and applied well or badly, but as the essence of all successful action. Rationality not only wins, it wins by Bayescraft. Bayescraft is the single essence of any method of pushing probability mass into sharp peaks. This all made sense too.\n\nBut the two world-views did not seem to fit together. Consider the humble room thermostat, which keeps the temperature within a narrow range by turning the heating on and off (or in warmer climes, the air conditioning), and consider everything that it does _not_ do while doing the single thing that it does:\n\n*   The thermostat knows only one thing about its environment: the temperature.\n*   It has no model of its surroundings.\n*   It has no model of itself.\n*   It makes no predictions.\n*   It performs no Bayesian calculations.\n*   It has no priors.\n*   It has no utility function.\n*   It computes nothing but the difference between perception and reference, and its rule for what to do when they differ could hardly be simpler. Low temperature: turn on. High temperature: turn off.\n*   It does not think. It does nothing any more suggestive of thought than a single transistor is suggestive of a Cray.\n\nAnd yet despite that, it has a sliver of the [Ultimate Power](http://www.overcomingbias.com/2008/10/mundane-magic.html), the ability to funnel the world through its desired tunnel in configuration space. In short, control systems _win_ while being entirely arational. How is this possible?\n\nIf you look up subjects such as \"optimal control\", \"adaptive control\", or \"modern control theory\", you will certainly find a lot of work on using Bayesian methods to design control systems. However, the fact remains that the majority of all installed control systems are nothing but manually tuned [PID controllers](http://en.wikipedia.org/wiki/PID_controller). And I have never seen, although I have looked for it, any analysis of general control systems in Bayesian terms. (Except for one author, but despite having a mathematical background, I couldn't make head nor tail of what he was saying. I don't think it's me, because despite his being an eminent person in the field of \"intelligent control\", almost no-one cites his work.) So much for modern control theory. You can design things that way, but you usually don't have to, and it takes a lot more mathematics and computing power. I only mention it because anyone googling \"Bayes\" and \"control theory\" will find all that and may mistake it for the whole subject.\n\n**3\\. Why it matters.**\n\nIf this was only about cruise controls and room thermostats, it would just be a minor conundrum. But it is also about people, and all living organisms. The Alien Space Bat Prison Cell describes us just as much as it describes a thermostat. We have a large array of meter needles, red dots, and knobs on the walls of our cell, but it remains the case that we are held inside an unbreakable prison exactly the same shape as ourselves. We are brains in vats, the vat of our own body. No matter how we imagine we are reaching out into the world to perceive it directly, our perceptions are all just neural signals. We have reasons to think there is a world out there that causes these perceptions (and I am not seeking to cast doubt on that), but there is no direct access. All our perceptions enter us as neural signals. Our actions, too, are more neural signals, directed outwards -- we think -- to move our muscles. We can never [dig our way out of the cell](http://en.wikipedia.org/wiki/File:Flammarion.jpg). All that does is make a bigger cell, perhaps with more meters and knobs.\n\nWe do pretty well at controlling some of those needles, without having received the grace of Bayes. When you steer your car, how do you keep it directed along the intended path? By seeing through the windscreen how it is positioned, and doing whatever is necessary with the steering wheel in order to see what you want to see. You cannot do it if the windows are blacked out (no perception), if the steering linkage is broken (no action), or if you do not care where the car goes (no reference). But you can do it even if you do not know about the cross-wind, or the misadjusted brake dragging on one of the wheels, or the changing balance of the car according to where passengers are sitting. It would not help if you did. All you need is to see the actual state of affairs, and know what you want to see, and know how to use the steering wheel to get the view closer to the view you want. You don't need to know much about that last. Most people pick it up at once in their first driving lesson, and practice merely refines their control.\n\nConsider stimulus/response again. You can't sense the crosswind from inside a car, yet the angle of the steering wheel will always be just enough to counteract the cross-wind. The correlation between the two will be very high. A simple, measurable analogue of the task is [easily carried out on a computer](http://www.mindreadings.com/ControlDemo/BasicTrack.html). There is a mark on the screen that moves left and right, which the subject must keep close to a static mark. The position of the moving mark is simply the sum of the mouse position and a randomly drifting disturbance calculated by the program. So long as the disturbance is not too large and does not vary too rapidly, it is easy to keep the two marks fairly well aligned. The correlation between the mouse position (the subject's action) and the disturbance (which the subject cannot see) is typically around -0.99. (I just tried it and scored -0.987.) On the other hand, the correlation between mouse position and mark position (the subject's perception) will be close to zero.\n\nSo in a control task, the \"stimulus\" -- the perception -- is uncorrelated with the \"response\" -- the behaviour. To put that in different terminology, the mutual information between them is close to zero. But the behaviour is highly correlated with something that the subject cannot perceive.\n\nWhen driving a car, suppose you decide to change lanes? (Or in the tracking task, suppose you decide to keep the moving mark one inch to the left of the static mark?) Suddenly you do something different with the steering wheel. Nothing about your perception changed, yet your actions changed, because a reference signal inside your head changed.\n\nIf you do not know that you are dealing with a control system, it will seem mysterious. You will apply stimuli and measure responses, and end up with statistical mush. Since everyone else does the same, you can excuse the situation by saying that people are terribly complicated and you can't expect more. 0.6 is considered a high correlation in a psychology experiment, and 0.2 is considered publishable ([link](http://www.jstor.org/pss/2093765)). Real answers go _ping!!_ when you hit them, instead of slopping around like lumpy porridge. What is needed is to discover that a control system is present, what it is controlling, and how.\n\nThere are ways of doing that, but this is enough for one posting.\n\n**4\\. Conclusion.**\n\nConclusion of this posting, not my entire thoughts on the subject, not by a long way.\n\nMy questions to you are these.\n\nControl systems win while being arational. Either explain this in terms of Bayescraft, or explain why there is no such explanation.\n\nIf, as is speculated, a living organism's brain is a collection of control systems, is Bayescraft no more related to its physical working than arithmetic is? Our brains can learn to do arithmetic, but arithmetic is not how our brains work. Likewise, we can learn Bayescraft, or some [practical approximation](/lw/bn/twotier_rationalism/) to it, but do Bayesian processes have anything to do with the mechanism of brains?\n\nDoes Bayescraft necessarily have anything to do with the task of building a machine that ... can do something not to be discussed here yet?\n\n**5\\. Things I have not yet spoken of.**\n\nThe control system's designer who put the rule in, that tells it what output to emit given the perception and the reference: whether he supplied the rationality that is the real source of its miraculous power?\n\nHow to discover the presence of a control system and discern its reference, even if its physical embodiment remains obscure.\n\nHow to control a perception even when you don't know how.\n\nHierarchical arrangements of control systems as a method of building more complex control systems.\n\nSimple control systems win at their limited tasks while being arational. How much more is possible for arational systems built of control systems?\n\n**6\\. WARNING: Autonomous device**\n\nAfter those few thousand words of seriousness, a small dessert.\n\nExhibit A: [A supposedly futuristic warning sign.](http://www.flickr.com/photos/arenamontanus/264112997/in/set-72157594323393196/)\n\nExhibit B: A contemporary warning sign in an undergraduate control engineering lab: \"WARNING: These devices may start moving without warning, even if they appear powered off, and can exert sudden and considerable forces. Exercise caution in their vicinity.\"\n\nThey say the same thing."
    },
    "voteCount": 53
  },
  {
    "_id": "dcRY7XSnuARkHkA5D",
    "url": null,
    "title": "An Introduction to Control Theory",
    "slug": "an-introduction-to-control-theory",
    "author": "Vaniver",
    "question": false,
    "tags": [
      {
        "name": "Perceptual Control Theory"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "[Behavior: The Control of Perception](http://smile.amazon.com/Behavior-Perception-William-T-Powers/dp/0964712172/ref=nosim?tag=vglnk-c319-20) by William Powers applies control theory to psychology to develop a model of human intelligence that seems relevant to two of LW's primary interests: effective living for humans and value-preserving designs for artificial intelligence. It's been discussed on LW previously [here](/lw/dj/what_is_control_theory_and_why_do_you_need_to/), [here](/lw/11h/controlling_your_inner_control_circuits/), and [here](/lw/10u/ask_lesswrong_human_cognitive_enhancement_now/v80), as well as mentioned in Yvain's roundup of [5 years (and a week) of LW](http://slatestarcodex.com/2014/03/13/five-years-and-one-week-of-less-wrong/). I've found previous discussions unpersuasive for two reasons: first, they typically only have a short introduction to control theory and the mechanics of control systems, making it not quite obvious what specific modeling techniques they have in mind, and second, they often fail to communicate the differences between this model and competing models of intelligence. Even if you're not interested in its application to psychology, control theory is a widely applicable mathematical toolkit whose basics are simple and well worth knowing.\n\nBecause of the length of the material, I'll split it into three posts. In this post, I'll first give an introduction to that subject that's hopefully broadly accessible. The [next post](/lw/lk9/behavior_the_control_of_perception/) will explain the model Powers introduces in his book. In the [last post](/lw/lkb/control_theory_commentary/), I'll provide commentary on the model and what I see as its implications, for both LW and AI.\n\n* * *\n\n[Control theory](http://en.wikipedia.org/wiki/Control_theory) is a central tool of modern engineering. Briefly, most interesting things can be modeled as [dynamical systems](http://en.wikipedia.org/wiki/Dynamical_system), having both states and rules on how those states change with time. Consider the 3D position and velocity of a ball in a bowl (with friction); six numbers tell you where the ball is, its speed, and its direction of movement, and a formula tells you how you can predict what those six numbers will be in the next instant given where they are now. Those systems can be characterized by their **attractors**, states that are stable and are the endpoint for nearby states. The ball sitting motionless in the bottom of the bowl is an attractor- if it's already there, it will stay there, and if it's nearby (releasing from a centimeter away, for example), it will eventually end up there. The point of control theory is that adding a **control** to a dynamical system allows you to edit the total system dynamics so that the points you _want_ to be stable attractors _are_ stable attractors.\n\nLet's flesh out that sketch with an example. Suppose you want to keep a house within a specific temperature range. You have a sensor of the current temperature, a heater, and a cooler. A thermostat takes the sensor's output, compares it to the desired temperature range, and turns the heater on if the sensed temperature is below the desired temperature range, and turns if off if the sensed temperature is above the minimum of that range, and does the reverse with the cooler (turning it on if the sensor is above the desired max, and turning it off if it's below).\n\nMost interesting control systems are have a finer range of control values- instead of simply flipping a switch on or off, a car's cruise control can smoothly vary the amount of gas or brake that's being applied. A simple way to make a control system is to take the difference between the desired speed and actual speed, multiply it by some factor to go from units of distance per time to angle of pedal, and adjust the position of the pedals accordingly. (If the function is linear, it's called a linear controller.)\n\nLet's introduce more of the technical vocabulary. The thing we're measuring is the **input** (to the controller), the level we want it to be at is the **reference**, the difference between those is the **error**, and the adjustment the control system makes is the **output** or **feedback** (sometimes we'll talk about the **actuator** as the physical means by which the controller emits its output). None of them have to be single variables- they can be vectors, which allow us to describe arbitrarily complicated systems. (The six numbers that express the position and velocity of a ball, each in three dimensions, are an example of an input vector.) I'll often use the noun **state** to refer to the, well, _state_ of the system, and 'points in state-space' refers to those states as vectors. There's also a possible confusion in that the _plant_ (the system being controlled) and the _controller_ are mirrored- the controller's output is the plant's input, and the plant's output is the controller's input.\n\nControl systems naturally lend themselves to diagrams: here's the block diagram from the thermostat and cruise control:\n\n![](http://i.imgur.com/3V8yYDC.png)\n\nIn a block diagram, each block is some function of its inputs, and the arrows show what affects what. Moving left to right, the reference is the temperature you've set, the current temperature is subtracted (that's the point of the plus and minus sign), and the error goes into the yellow box which represents the function that converts from the error to the effort put into altering the system. That's the controller output arrow that goes into the green box (the house), which represents the external system. This is also a functional block, because it takes the controller output and any disturbances (often represented as another arrow pointing in from the top) and converts them into the system temperature. The arrow leading out of the house points both to the right- to remind you that this is the temperature you're living with- and back into the thermocouple, the sensor that measures the temperature to compare with the reference, and now we've finished our feedback loop.\n\nNow that we have a mathematical language for modeling lots of different systems, we can abstract away the specifics and prove properties about how those systems will behave given various controllers (i.e. feedback functions). Feedback functions convert the input and reference to the output, and are the mathematical abstraction of a physical controller. They can be arbitrary functions, but most of the mathematical edifice of control theory assumes that everything is continuous (but not necessarily linear). If you know the dynamics of a system, you can optimize your control function to match the system and be guaranteed to converge to the reference with a particular time profile. Rather than go deeply into the math, I'll discuss a few concepts that have technical meanings in control theory that are useful to think about.\n\nFirst is **convergence**: the system output will eventually match the reference. This means that any errors that get introduced into the system are transient (temporary), and ideally we know the time profile of how large those errors will be as time progresses. A common goal here is **exponential** convergence, which means that the error decreases with a rate proportional to its size. (If the temperature is off by 2 degrees, the rate at which the error is decreasing is twice that of the rate when the temperature is off by 1 degree.) A simple linear controller will, for simple state dynamics, accomplish exponential convergence. If your system doesn't converge, then you are not successfully controlling it, and if your reference changes unpredictably at a rate faster than your system can converge, then you are not going to be able to match your reference closely.\n\nSecond is **equilibrium**: a point when the forces are balanced. If the system is at an equilibrium state, then nothing will change. This is typically discussed along with steady state error: imagine that my house gets heated by the sun at a rate of 1° an hour, and rather than a standard air conditioner that's on or off I have a 'dimmer switch' for my AC. If my controller sets the AC rate at the difference between the reference temperature and the current temperature per hour, then when the house is at 30° and I want it to be at 23° it'll try to reduce the temperature by 7° an hour, but when the house is at 24° and I want it to be at 23° it will try to reduce the temperature by 1° an hour, which cancels the effect of the sun, and so the house is at equilibrium at 24°. (Most real controllers have an integrator in order to counteract this effect.)\n\nThird is **stability**: even when we know a point is an equilibrium, we want to know about the behavior in the neighborhood of that point. A stable equilibrium is one where a disturbance will be corrected; an unstable equilibrium is one where a disturbance will be amplified. Imagine a pendulum with the bob balanced at the bottom- tap it and it'll eventually be balanced at the bottom again, because that's a stable equilibrium (also called an attractor). Now imagine the bob balanced at the top- tap it, and it'll race away from that point, unlikely to return, because that's an unstable equilibrium (also called a repulsor).\n\nStability has a second meaning in control theory: a controller that applies _too much_ feedback will cause the system to go from a smaller positive error to a moderate negative error, and then again too much feedback will be applied, and the system will go from a moderate negative error to a huge positive error. Imagine a shower with a delay: you turn the temperature knob and five seconds later the temperature of the water coming out of the showerhead changes. If you react too strongly or too quickly, then you'll overreact, and the water that started out too hot will correct to being too cold by the time you've stopped turning the knob away from heat. That an overexuberant negative feedback controller can still lead to explosions is one of the interesting results of control theory, as is that making small, gradual, proportionate changes to the current state can be as effective as memory / implementing a delay in your controller. Sometimes, you achieve better control exactly because you applied _less_ control effort.\n\nIt's also worth mentioning here that basically all real controllers (even [amplifiers](http://en.wikipedia.org/wiki/Amplifier#Amplifier_circuit)!) are negative feedback controllers- positive feedback leads to explosions (literally), because \"positive feedback\" in this context means \"pushing the system in the direction of the error\" rather than its meaning in psychology.\n\n* * *\n\nSo what's the point of control systems? If we have a way of push the states of a system, we can effectively adjust the system dynamics to make _any_ state that we want a stable state. If we put a motor at the joint of a pendulum, we can adjust the acceleration of the bob so that it has an equilibrium at one radian away from vertical, rather than zero radians away from vertical. If we put a thermostat in a house, we can adjust the temperature changes of the house so that it returns to a comfortable range, instead of whatever the temperature is outside. (The point of control theory is to understand how the systems work, so we can make sure that our control systems do what we want them to do!)\n\nWhy are control systems interesting? Three primary reasons:\n\n1.  They're practical. Control systems are all over the place, from thermostats to cars to planes to satellites.\n2.  They can be adaptive. A **hierarchical** control system has a control loop which determines the parameters used in another control loop, and a natural application is adaptive control systems. When you launch a satellite, you might not have precise measurements of its rotational inertia, or you might expect that to change as it uses its fuel over its lifetime. One control system could observe how the satellite moves in response to its thrusters, and adjust the parameters of a rotational inertia model to correct errors to match the model to the observations. A second control system could use the inertia model created by the first control system to determine how to use the thrusters to adjust the satellite's alignment to match the desired rotation.\n3.  They give concrete mathematical models of how simple signal processing can create 'intentional' behavior, and of what it looks like to be intentional without an explicit model of reality. A [centrifugal governor](http://en.wikipedia.org/wiki/Centrifugal_governor) is not an agent in the LW sense of the term, but it is an agent in another sense of the term--an entity that performs actions on behalf of another. The governor is just some pieces of metal, it doesn't have a mind, it's not viewed with moral concern, it doesn't have goals as humans think of them, and it doesn't have even a rudimentary internal model of the dynamical system it's controlling, and it _still_ gets the job done. Controllers seem like a class of entities that are best modeled by intentionality, in that they alter the state of their external environment to match their desired internal environment based on their perceptions, and can do so in arbitrarily complicated and powerful ways, but while they do [steer the future](/lw/vb/efficient_crossdomain_optimization/) they don't seem to be cross-domain and they don't look like anthropomorphic models of \"human intelligence.\"\n\n[Next](/lw/lk9/behavior_the_control_of_perception/), B:CP on the use of control theory in psychology."
    },
    "voteCount": 36
  },
  {
    "_id": "Ba6buPA3u2btdKS82",
    "url": null,
    "title": "Without models",
    "slug": "without-models",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "World Modeling"
      },
      {
        "name": "Logic & Mathematics "
      },
      {
        "name": "Perceptual Control Theory"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "Followup to: [What is control theory?](/lw/dj/what_is_control_theory_and_why_do_you_need_to/)\n\nI mentioned in [my post testing the water on this subject](/lw/c0/the_ideas_youre_not_ready_to_post/934) that control systems are not intuitive until one has learnt to understand them. The point I am going to talk about is one of those non-intuitive features of the subject. It is (a) basic to the very idea of a control system, and (b) something that almost everyone gets wrong when they first encounter control systems.\n\nI'm going to address just this one point, not in order to ignore the rest, but because the discussion arising from [my last post](/lw/dj/what_is_control_theory_and_why_do_you_need_to/) has shown that this is presently the most important thing.\n\nThere is a great temptation to think that to control a variable -- that is, to keep it at a desired value in spite of disturbing influences -- the controller must contain a model of the process to be controlled and use it to calculate what actions will have the desired effect. In addition, it must measure the disturbances or better still, predict them in advance and what effect they will have, and take those into account in deciding its actions.\n\nIn terms more familiar here, the temptation to think that to bring about desired effects in the world, one must have a model of the relevant parts of the world and predict what actions will produce the desired results.\n\nHowever, this is absolutely wrong. This is not a minor mistake or a small misunderstanding; it is the pons asinorum of the subject.\n\nNote the word \"must\". It is not disputed that one can use models and predictions, only that one must, that the task inherently requires it.\n\n_A control system can work without having any model of what it is controlling._\n\nThe designer will have a model. For the room thermostat, he must know that the heating should turn on when the room is too cold and off when it is too hot, rather than the other way around, and he must arrange that the source of heat is powerful enough. The controller he designs does not know that; it merely does that. (Compare the similar [relationship between evolution and evolved organisms](http://www.overcomingbias.com/2007/11/adaptation-exec.html). How evolution works is not how the evolved organism works, nor is how a designer works how the designed system works.) For a cruise control, he must choose the parameters of the controller, taking into account the engine's response to the accelerator pedal. The resulting control system, however, contains no representation of that. According to the [HowStuffWorks](http://www.howstuffworks.com/cruise-control.htm) article, they typically use nothing more complicated than proportional or PID control. The parameters are chosen by the designer according to his knowledge about the system; the parameters themselves are not something the controller knows about the system.\n\nIt is possible to design control systems that do contain models, but it is not inherent to the task of control. [This](http://blg.eng.cam.ac.uk/t/pub/Public/Wolpert/Publications/WolMiaKaw98.pdf) is what model-based controllers look like. (Thanks to [Tom Talbot](/lw/dj/what_is_control_theory_and_why_do_you_need_to/al9) for that reference.) Pick up any book on model-based control to see more examples. There are signals within the control system that are designed to relate to each other in the same way as do corresponding properties of the world outside. That is what a model is. There is nothing even slightly resembling that in a thermostat or a cruise control. Nor is there in the knee-jerk tendon reflex. Whether there are models elsewhere in the human body is an empirical matter, to be decided by investigations such as those in the linked paper. To merely be entangled with the outside world is not what it is, to be a model.\n\nWithin the Alien Space Bat Prison Cell, the thermostat is flicking a switch one way when the needle is to the left of the mark, and the other when it is to the right. The cruise control is turning a knob by an amount proportional to the distance between the needle and the mark. Neither of them knows why. Neither of them knows what is outside the cell. Neither of them cares whether what they are doing is working. They just do it, and they work.\n\n_A control system can work without having any knowledge of the external disturbances._\n\nThe thermostat does not know that the sun is shining in through the window. It only knows the current temperature. The cruise control does not sense the gradient of the road, nor the head wind. It senses the speed of the car. It may be tuned for some broad characteristics of the vehicle, but it does not itself know those characteristics, or sense when they change, such as when passengers get in and out.\n\nAgain, it is possible to design controllers that do sense at least some of the disturbances, but it is not inherent to the task of control.\n\n_A control system can work without making any predictions about anything._\n\nThe room thermostat does not know that the sun is shining, nor the cruise control the gradient. A fortiori, they do not predict that the sun will come out in a few minutes, nor that there is a hill in the distance.\n\nIt is possible to design controllers that make predictions, but it is not an inherent requirement of the task of control. The fact that a controller works does not constitute a prediction, by the controller, that it will work. I am belabouring this point, because the error has already been belaboured.\n\nBut (it was maintained) doesn't the control system have an implicit model, implicit knowledge, and implicitly make predictions?\n\nNo. None of these things are true. The very concepts of implicit model, implicit knowledge, and implicit prediction are problematic. The phrases do have sensible meanings in some other contexts, but not here. An implicit model is one in which functional relationships are expressed not as explicit functions y=f(x), but as relations g(x,y)=k. Implicit knowledge is knowledge that one has but cannot express in words. Implicit prediction is an unarticulated belief about the effect of the actions one is taking.\n\nIn the present context, \"implicit\" is indistinguishable from \"not\". Just because a system was made a certain way in order to interact with some other system a certain way, it does not make the one a model of the other. As well say that a hammer is a model of a nail. The examples I am using, the thermostat and the cruise control, sense temperature and speed respectively, compare them with their set points, and apply a rule for determining their action. In the rule for a proportional controller:\n\n_output = constant × (reference - perception)_\n\nthere is no model of anything. The gain constant is not a model. The perception, the reference, and the output are not models. The equation relating them is my model of the controller. It is not the controller's model of anything: it is what the controller is.\n\nThe only knowledge these systems have is their perceptions and their references, for temperature or speed. They contain no \"implicit knowledge\".\n\nThey do not \"implicitly\" make predictions. The designer can predict that they will work. The controllers themselves predict nothing. They do what they do whether it works or not. Sometimes, in fact, these systems do not work. The thermostat will fail to control if the outside temperature is above the set point. The cruise control will fail to control on a sufficiently steep downhill gradient. They will not notice that they are not working. They will not behave any differently as a result. They will just carry on doing _o=c×(r-p)_, or whatever their output rule is.\n\nI don't know if anyone tried my [robot simulation applet](http://www2.cmp.uea.ac.uk/~jrk/Robotics/Archy/Archy.html) that I linked to, but I've noticed that people I show it to readily anthropomorphise it. (BTW, if its interface appears scrambled, resize the browser window a little and it should sort itself out.) They see the robot apparently going around the side of a hill to get to a food particle and think it planned that, when in fact it knows absolutely nothing about the shape of the terrain ahead. They see it go to one food particle rather than another and think it made a decision, when in fact it does not know how many food particles there are or where. There is almost nothing inside the robot, compared to what people imagine: no planning, no adaptation, no prediction, no sensing of disturbances, and no model of anything but its own geometry. The 6-legged version contains 44 proportional controllers. The 44 gain constants are not a model, they merely work.\n\n(A tangent: people look at other people and think they can see those other people's purposes, thoughts, and feelings. Are their projections any more accurate than they are when they look at that robot? If you think that they are, how do you know?)\n\nNow, I am not explaining control systems merely to explain control systems. The relevance to rationality is that they funnel reality into a narrow path in configuration space by entirely arational means, and thus constitute a proof by example that this is possible. This must raise the question, how much of the neural functioning of a living organism, human or lesser, operates by similar means? And how much of the functioning of an artificial organism must be designed to use these means? It appears inescapable that all of what a brain does consists of control systems. To what extent these may be model-based is an empirical question, and is not implied merely by the fact of control. Likewise, the extent to which these methods are useful in the design of artificial systems embodying the [Ultimate Art](http://www.overcomingbias.com/2008/10/mundane-magic.html).\n\nEvolution operates statistically; I would be entirely unsurprised by Bayesian analyses of evolution. But _how evolution works_ [is not](http://www.overcomingbias.com/2007/11/adaptation-exec.html) _how the evolved organism works_. That must be studied separately.\n\nI may post something more on the relationship between Bayesian reasoning and control systems neither designed by nor performing the same when I've digested [the material that Steve_Rayhawk pointed to](/lw/dj/what_is_control_theory_and_why_do_you_need_to/af3). For the moment, though, I'll just remark that \"Bayes!\" is merely a [mysterious answer](http://www.overcomingbias.com/2007/08/mysterious-answ.html), unless backed up by actual mathematical application to the specific case.\n\n**Exercises.**\n\n1\\. A room thermostat is set to turn the heating on at 20 degrees and off at 21. The ambient temperature outside is 10 degrees. You place a candle near the thermostat, whose effect is to raise its temperature 5 degrees relative to the body of the room. What will happen to (a) the temperature of the room and (b) the temperature of the thermostat?\n\n2\\. A cruise control is set to maintain the speed at 50 mph. It is mechanically connected to the accelerator pedal -- it moves it up and down, operating the throttle just as you would be doing if you were controlling the speed yourself. It is designed to disengage the moment you depress the brake. Suppose that that switch fails: the cruise control continues to operate when you apply the brake. As you gently apply the brake, what will happen to (a) the accelerator pedal, and (b) the speed of the car? What will happen if you attempt to keep the speed down to 40 mph?\n\n3\\. An employee is paid an hourly rate for however many hours he wishes to work. What will happen to the number of hours per week he works if the rate is increased?\n\n4\\. A target is imposed on a doctor's practice, of never having a waiting list for appointments more than four weeks long. What effect will this have on (a) how long a patient must wait to see the doctor, and (b) the length of the appointments book?\n\n5\\. What relates questions 3 and 4 to the subject of this article?\n\n6\\. Controller: _o = c×(r-p)_. Environment: _dp/dt = k×o + d_. _o_, _r_, and _p_ as above; _c_ and _k_ are constants; _d_ is an arbitrary function of time (the disturbance). How fast and how accurately does this controller reject the disturbance and track the reference?"
    },
    "voteCount": 31
  },
  {
    "_id": "cMKNFE8hWKNhnEXtM",
    "url": null,
    "title": "Control Theory Commentary",
    "slug": "control-theory-commentary",
    "author": "Vaniver",
    "question": false,
    "tags": [
      {
        "name": "Perceptual Control Theory"
      }
    ],
    "tableOfContents": {
      "sections": [
        {
          "title": "History",
          "anchor": "History",
          "level": 1
        },
        {
          "title": "Utility Theory",
          "anchor": "Utility_Theory",
          "level": 1
        },
        {
          "title": "Previous Discussion on LW",
          "anchor": "Previous_Discussion_on_LW",
          "level": 1
        },
        {
          "title": "Intelligent Action without Environmental Simulation",
          "anchor": "Intelligent_Action_without_Environmental_Simulation",
          "level": 1
        },
        {
          "title": "Preference Modeling and Conflicts",
          "anchor": "Preference_Modeling_and_Conflicts",
          "level": 1
        },
        {
          "title": "Reasoning About AI",
          "anchor": "Reasoning_About_AI",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        },
        {
          "anchor": "comments",
          "level": 0,
          "title": "15 comments"
        }
      ],
      "headingsCount": 8
    },
    "contents": {
      "markdown": "This is the third and final post in a sequence on control theory. In the [first post](/lw/lk0/an_introduction_to_control_theory/) I introduced the subject of control theory and stepped through some basics. In the [second post](/lw/lk9/behavior_the_control_of_perception/) I outlined Powers's model, as presented in [Behavior: The Control of Perception](http://smile.amazon.com/Behavior-Perception-William-T-Powers/dp/0964712172/ref=nosim?tag=vglnk-c319-20). This post is a collection of comments on the subject that are only somewhat related, and so I'll use section headings to separate them. I'll also explicitly note the _absence_ of a section on the design of control systems, which is where _most_ of the effort used in talking about them in industrial settings goes, and is probably relevant to philosophical discussions surrounding them.\n\nHistory\n-------\n\nFrom Wikipedia's [Cybernetics](http://en.wikipedia.org/wiki/Cybernetics) page:\n\n> Artificial intelligence (AI) was founded as a distinct discipline at a 1956 conference. After some uneasy coexistence, AI gained funding and prominence. Consequently, cybernetic sciences such as the study of neural networks were downplayed; the discipline shifted into the world of social sciences and therapy.\n\nI'm no historian of science, and it's not clear to me why this split happened. It seems likely that control theory was simply not a useful approach for many of the early problems researchers associated with AI, like natural language processing: Powers has a description of how neural circuits as he models them could solve the phoneme parsing problem (which seems very compatible with sophisticated approaches that use [Hidden Markov Models](http://en.wikipedia.org/wiki/Hidden_Markov_model)) but how one would go from parsing sounds to make words to parsing words to make concepts is not quite clear. It seems like there might be some difference in kind between the required circuitry, but perhaps not: one of the recent advances in machine learning is \"[deep learning](http://en.wikipedia.org/wiki/Deep_learning),\" which the ultra-simplified explanation of is \"neural nets, just dialed up to 11.\" It seems possible (certain, if you count NNs as a 'cybernetic' thing) that AI is moving back in the direction of cybernetics/control theory/etc., but possibly without much intellectual continuity. Did backpropagation spread from controls to AI, or was it independently invented? As mentioned before, I'm not a historian of science. People working in robotics, as my limited understanding goes, have always maintained a connection to engineering and cybernetics and so on, but the 'hardware' and 'software' fields diverged, where the roboticists sought to move from the first level up and the AI researchers sometimes sought to move from the top down, perhaps without the hierarchical view.\n\n[This article](http://nautil.us/issue/21/information/the-man-who-tried-to-redeem-the-world-with-logic) on Walter Pitts (an important early figure in cybernetics) describes the split thus:\n\n> Von Neumann was the first to see the problem. He expressed his concern to Wiener in a letter that anticipated the coming split between artificial intelligence on one side and neuroscience on the other. “After the great positive contribution of Turing-cum-Pitts-and-McCulloch is assimilated,” he wrote, “the situation is rather worse than better than before. Indeed these authors have demonstrated in absolute and hopeless generality that anything and everything … can be done by an appropriate mechanism, and specifically by a neural mechanism—and that even one, definite mechanism can be ‘universal.’ Inverting the argument: Nothing that we may know or learn about the functioning of the organism can give, without ‘microscopic,’ cytological work any clues regarding the further details of the neural mechanism.”\n\nUtility Theory\n--------------\n\nUtility theory is the mathematically correct way to behave in an uncertain world if you have preferences over consequences that can be collapsed onto the real line and can solve the maximization problem. So long as your values follow [four desirable rules](http://en.wikipedia.org/wiki/Von_Neumann%E2%80%93Morgenstern_utility_theorem), that describes your preferences. If we express those preferences as a _probabilistically relevant_ score, then we can entirely separate our module that expresses preferences over consequences and our module that predicts probabilities of consequences once we take a particular action, and this is a huge boon to mathematical decision-making.\n\nBut it turns out that decision-making under _certainty_ can often be a hard problem for humans. This is a black mark for the _descriptive_ application of utility theory to humans, but is explained by the control theory paradigm as multiple goals (i.e. control systems) conflicting. I _don't_ see this as a challenge to the prescriptive usefulness of utility theory: when presented with a choice, it is often better to make one than not make one--or, if one is delaying until additional information arrives, to know exactly what impact possible information could impact the decision through a [VoI calculation](/lw/85x/value_of_information_four_examples/). Even if you've identified the two terminal goals that are conflicting, it is probably better to explicitly short circuit one of those desires, determine the right tradeoff, or pull in a third direction rather than remain locked in conflict.\n\nIt also seems that utility maximization is mostly appropriate for an agent in the LW sense--a unitary entity that has a single preference function and plans how to achieve that (potentially complex) preference as well as possible. This requires a potentially immense amount of computing power, and it's not at all obvious that many of the \"systems\" with \"intelligence\" that we might be worried about will be described that way. When we look at, say, trading algorithms causing problems in the financial markets, utility maximization doesn't appear to be the right viewpoint for understanding why those algorithms behave the way they do, and game theory doesn't seem like the right approach to try to determine the correct reaction to their algorithms and their use.\n\nIt may also be helpful to separate out \"strong\" and \"weak\" senses in which an agent maximizes utility. The strong sense is that they actually have a known function that they use to value consequences, and simulate the future to determine the action that gets them the most value. The weak sense is that we can describe _any_ agent as behaving as though it is maximizing some utility function, by observing what it does and calling that the utility-maximizing action. As the names suggest, the strong sense is useful for predicting how an agent will behave, and the weak sense isn't.\n\nAs mentioned earlier, I don't think it's easy (or desirable) to dethrone utility as the premier prescriptive decision-making approach, if you have the self-editing ability to change your decision-making style and the computing power to solve the maximization problems it poses. But we may need to figure out where we're coming from to figure out how to get there. (In some sense, that's the premise of the Heuristics and Biases literature.)\n\nPrevious Discussion on LW\n-------------------------\n\nIt's not quite fair or reasonable to respond to comments and posts made years ago (and before I even found LW), especially in light of Yvain's roundup that had PCT listed with ideas that seemed outlandish before being partly absorbed into the LW consensus. One of the reasons why I bring the subject up again, with a longer treatment, is because I think I see a specific hole in the LW consensus that I might be well-suited to fill. So let's look at the list of links from the first post again: this book and Powers's Perceptual Control Theory have been discussed on LW [here](/lw/dj/what_is_control_theory_and_why_do_you_need_to/), [here](/lw/11h/controlling_your_inner_control_circuits/), and [here](/lw/10u/ask_lesswrong_human_cognitive_enhancement_now/v80), as well as mentioned in Yvain's roundup of [5 years (and a week) of LW](http://slatestarcodex.com/2014/03/13/five-years-and-one-week-of-less-wrong/).\n\nI feel like the primary criticisms (see [SilasBarta's comment](/lw/11h/controlling_your_inner_control_circuits/vuw) as an example) were about the presentation and the unexplained enthusiasm, rather than the invalidity or inappropriateness of the model, and the primary defenses were enthusiasm (as I recall, [this comment by pjeby](/lw/10u/ask_lesswrong_human_cognitive_enhancement_now/vlc) prompted me to buy and read the book, but I'm only familiar with one of the six things that he says it explains, which impairs my ability to understand why he thinks it's impressive!). I don't mean to fault people involved in that conversation on the PCT side for not explaining- even I see my two thousand words in the last post as an argument to read Powers' three hundred page long book rather than a full explanation (just like my two thousand words spent on the basics of controls wouldn't get you through an undergraduate level class on the subject, and are more of an argument to take that class).\n\nSince you can make an arbitrary function out of enough control loops, saying that human minds run on control loops doesn't constrain the possible behavior of humans much by itself, just like saying that a program is written in a high-level language doesn't constrain the possible behavior of that program much. I view PCT as describing the inherent modularity of the code, rather than what's possible to code, which helps quite a bit in figuring out how the code functions and where bugs might be hiding or how to edit it. Any model built in the controls framework will have to be very complicated to be fully functional--I feel like it's easier to understand the mechanics of a person's arm than the mechanics of their personality, but if we want to explain the arm at any meaningful level of detail we need a meaningful number of details!\n\nAnd, in terms of models, I think the way to think about PCT is as a competitor for utility. I don't know many LWers who model themselves or other humans as utility maximizers, but it seems like that remains the _default_ model for describing intelligent agents whenever we step up a level of abstraction (like when, say, we start talking about ethics or meta-ethics). As part of writing this post, I reread Yvain's sequence on the [Blue-Minimizing Robot](http://wiki.lesswrong.com/wiki/The_Blue-Minimizing_Robot). At parts, it seems to me to present a dilemma between either modeling intelligence as utility-optimization or arbitrary code, where the former can't be implemented and the latter can't be generalized. A control system framework seems like it finds a middle ground that can be both feasibly implemented and generalized. (It pays for that, of course, by not being _easily_ implemented or _easily_ generalized. Roboticists are finding that _walking_ is hard, and that's only level 5 in the hierarchy! On the input side, computer vision folks don't seem to be doing all that much better.)\n\nIdeally, this is where I would exhibit some example that demonstrates the utility of thinking this way: an ethical problem that utilitarianism can't answer well but a control theory approach can, or a self-help or educational problem that other methods couldn't resolve and this method can. But I don't have such an example ready to go, I'm not convinced that such an example even exists, and even if one exists and I have it, it's not clear to me that it would be convincing to others. Perhaps the closest thing I have to an example is my experience training in the Alexander Technique, which I see as being easy to describe from a control theory perspective, but is highly related to my internal experience and methodology of moving through the world, both of which are difficult to describe through a text-based medium. Further, even if it does become obvious that positive change is taking place, determining how much that positive change validates a control system-based explanation of what's going on underneath is it's own difficult task!\n\nA model fits the situation when easy problems are easy in the model and hard problems are hard in the model. A thermostat is simple, and a person is complex . The utilitarian approach says \"find the thing defined as the thing to be maximized, and then maximize it,\" and in some sense the utilitarian model for a thermostat is 'as simple' as the utilitarian model for a person- they've swept all the hard bits into the utility function, and give little guidance on how to actually go about finding that function. The control system approach says \"find the negative feedback loops, and edit them or their reference levels so they do what you want them to do,\" and the number of feedback loops involved in the thermostat is rightly far lower than the number of feedback loops involved in the person. If I could exhibit a simple model that solves a complex problem, then it seems to me that my model doesn't quite fit.^1^\n\nIntelligent Action without Environmental Simulation\n---------------------------------------------------\n\nThis is mostly covered by RichardKennaway's post [here](/lw/ek/without_models/), but is important enough to repeat. Typically, when we think about [optimization](http://en.wikipedia.org/wiki/Optimization_problem), we have some solution space (say, possible actions to take) and some objective function (over solutions, i.e. actions), and go through the solution space applying the objective function to points until we're satisfied that we have a point that's good enough. (If the relationship between the solution space and objective function is kind enough, we'll actually search for a proof that no better solutions exist than the one we picked.)\n\nA common mathematical approach is to model the world as having states, with the transition probability from state to state depending on the actions the robot takes (see [Markov Decision Processes](http://en.wikipedia.org/wiki/Markov_decision_process)). Typically, we want to find an optimal policy, i.e. a mapping from states of the world to actions that lead to the maximum possible accrual of value.\n\nBut the computational cost of modeling reality in that level of depth may not be worth it. To give a concrete example, there's a human movement neuroscience community that studies the questions of how muscles and joints and brains work (i.e. fleshing out the first five levels of the model we talked about in the last post), and one of the basic ideas in that field is that there's a well-behaved function that maps from the position of the joints in the arm to where the tip of the finger is. Suppose you want to press a button with your finger. You now have to solve the _inverse_ problem, where I give you a position for the tip of the finger (where the button is) and you figure out what position to put the joints in so that you touch the button. Even harder, you want to find the change in joint positions that represents the _least amount of effort_. This is a computationally hard problem, and one of the questions the community is debating is how the human nervous system solves this hard problem.\n\nMy favorite answer is \"it doesn't solve the hard problem.\" (Why would it? Effort spent in the muscles is measured in calories, and _so is effort spent in the nerves._)Instead of actually inverting the function and picking the best possible action out of all actions, there might be either stored approaches in memory or the brain might do some sort of gradient descent (easily implemented in control systems using the structure described in the last post), where the brain knows the difference between where the finger is and where it should be, moves each joint in a way that'll bring the finger closer to where it should be, and then corrects its approach as it gets closer. This path is _not_ guaranteed to be globally optimal, i.e. it does not solve the hard problem, but is locally optimal in muscular effort and probably optimal in _combined_ muscular and nervous calorie expenditure.\n\nPreference Modeling and Conflicts\n---------------------------------\n\nI'm not familiar with enough [Internal Family Systems Therapy](http://www.selfleadership.org/outline-of-the-Internal-family-systems-model.html) to speak to how closely it aligns with the control systems view, but I get the impression that the two share many deep similarities.\n\nBut it seems to me that if one hopes to preserve human values, it would help to work in the value space that humans have- and we can easily imagine control systems that compare the _relative position_ or _rate of change_ of various factors to some references. I recall a conversation with another rationalist about the end of Mass Effect 3, where (if I recall their position correctly, and it's been years so I'm not very confident that I do) they preferred a galactic restart to a stagnant 'maximally happy' galaxy, because the former offered opportunities for growth and the latter did not, and they saw life without growth as not worth living. From a utility maximization or efficiency point of view, this seems strange- why want the present to be _worse_ than it could be? But this is a common preference (that shows up in the Heuristics and Biases literature), that people often prefer an increasing series of payments to a decreasing series of payments, even though by exponential discounting they should prefer the decreasing series (where you get the largest payment first) if the amounts are the same with only the order reversed.\n\nReasoning About AI\n------------------\n\nI don't see all that much relevance to solving the general problem of value preservation (it seems _way_ easier to prove results about utility functions), but as mentioned in the conflicts section it does seems relevant to human value preservation if it's a good description of human values. There is the obvious caveat that we might _not_ want to preserve the fragmentary shattering of human values; a potential future person who wants the same things at the same strength as we do, but has their desires unified into a single introspectively accessible function with known tradeoffs between all values, will definitely be more efficient than current humans--potentially more human than the humans! But when I ask myself for a fictional example of immediate, unswerving confidence in one's values, the best example that comes to mind is the Randian hero (which is perhaps an argument for keeping the fragmentation around). As Roark says to Peter (emphasis mine):\n\n> If you want my advice, Peter, you've made a mistake already. By asking me. By asking anyone. Never ask people. Not about your work. **Don't you know what you want? How can you stand it, not to know?**\n\nBut leaving aside values, there's the question of predicting behavior. It seems to me that there are two facets--what sort of intensity of change we would expect, and how we should organize predictions of the future. It seems likely that local controllers will have, generally, local effects. I recall a conversation, many years ago, where another suggested that an AI in charge of illuminating the streets might decide to destroy the sun in order to prevent the street from being too bright. Or, I suggested, it might invest in some umbrellas, since that's a much more _proportionate_ response. I italicize proportionate because under a linear negative feedback controller that would be literally true- the more lumens between the sensor and the reference, the more control effort would be expended, in a one to one fashion. Controllers are a third class of intentional agents, different from both [satisficers](http://en.wikipedia.org/wiki/Satisficing) and maximizers, and a potentially useful one to have in one's mental toolkit.\n\nIf we know a system is an intelligent optimizer, and we have a range of possible futures that we're trying to estimate the probability of, we can expect that futures higher in the preference ordering of the optimizer are more likely. But if we also have an idea of what actuators the system has, we might expect futures where those actuators are used, or the direct effect of those actuators leads to a higher preference ordering, to be more likely, and this might be a better way for reasoning about those problems. I'm not sure how far I would take this argument, though; many examples abound of genetic algorithms and other metaheuristic optimization methods being clever in surprising ways, using their ability to simulate the future to find areas of the solution space that did not look especially promising to their human creators that turned out to have proverbial gold. It seems likely that superhuman intelligence is likely to rely heavily on numerical optimization, and even if the objective function is determined by control systems,^2^ as soon as optimizers are in the mix (perhaps as determining what control to apply to reduce an error) it makes sense to break out the conservative assumptions on their power. And actuators that might _seem_ simple, like sending plaintext through a cable to be read by people, are often in fact very complex.\n\n* * *\n\n1\\. gwern recently posted an Asimov essay called [Forget It!](http://www.kkbooks.net/ScienceFiction/Asimov37/27283.html), which discusses how an arithmetic textbook from 1797 managed to require over 500 pages to teach the subject. One might compare today's simple arithmetic model to their complex arithmetic model, apply my argument in this paragraph, and say \"but if you've managed to explain a long subject in a short amount of time, clearly you've lost a lot of the inherent complexity of the subject!\" I'd counter with Asimov's counter, that the arithmetic of today _really is simpler_ than the arithmetic they were doing then, and that the difference is not so much that the models of today are better, but that the _reality is simpler_ today and thus simpler models suffice for simpler problems. But perhaps this is a dodge because it depends on the definitions of \"model\" and \"reality\" that I'm using.\n\n2\\. A few years ago, I saw an optimization algorithm that designed the targeting of an ion beam (I believe?) used to deliver maximum radiation to a tumor while delivering minimum radiation to the surrounding tissue. The human-readable output was a dose probability curve, basically showing the radiation distribution that the tumor received and that the surrounding tissue received. The doctor would look at the curve, decide whether or not they liked it, and play with the meta-parameters of the optimization until the optimizer spat out dosage distributions that they were happy with. I thought this was terribly inefficient- even if the doctors thought they were optimizing a complex function of the distribution, they were probably doing something simple and easy to learn like area under the curve in particular regions or a simple integration, and then that could be optimized directly. The presenter disagreed, though I suspect they might have been disagreeing on the practicality of getting doctors to accept such a system rather than building one. As the [fable](http://www.snopes.com/business/genius/cakemix.asp) goes, \"instant\" cake mix requires that the customer break an egg because customers prefer to do at least _one_ thing as part of making the cake."
    },
    "voteCount": 19
  },
  {
    "_id": "zFQQEkx4c6bxdshr4",
    "url": null,
    "title": "5 Axioms of Decision Making",
    "slug": "5-axioms-of-decision-making",
    "author": "Vaniver",
    "question": false,
    "tags": [
      {
        "name": "Planning & Decision-Making"
      },
      {
        "name": "Decision Theory"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "This is part of a [sequence on decision analysis](/lw/8xr/decision_analysis_sequence/); the first post is a primer on [Uncertainty](/lw/8lb/uncertainty/).\n\nDecision analysis has two main parts: abstracting a real situation to math, and then cranking through the math to get an answer. We started by talking a bit about how probabilities work, and I'll finish up the inner math in this post. We're working from the inside out because it's easier to understand the shell once you understand the kernel. I'll provide an example of prospects and deals to demonstrate the math, but first we should talk about axioms. In order to be comfortable with using this method, there are five axioms1 you have to agree with, and if you agree with those axioms, then this method flows naturally. They are: **Probability, Order, Equivalence, Substitution,** and **Choice.**\n\nProbability\n\nYou must be willing to assign a probability to quantify any uncertainty important to your decision. You must have consistent probabilities.\n\nOrder\n\nYou must be willing to order outcomes without any cycles. This can be called transitivity of preferences: if you prefer A to B, and B to C, you must prefer A to C.\n\nEquivalence\n\nIf you prefer A to B to C, then there must exist a *p* where you are indifferent between a deal where you receive B with certainty and a deal where you receive A with probability *p* and C otherwise.\n\nSubstitution\n\nYou must be willing to substitute an uncertain deal for a certain deal or vice versa if you are indifferent between them by the previous rule. Also called \"do you really mean it?\"\n\nChoice\n\nIf you have a choice between two deals, both of which offer A or C, and you prefer A to C, then you must pick the deal with the higher probability of A.\n\nThese five axioms correspond to five actions you'll take in solving a decision problem. You assign **probabilities**, then you **order** outcomes, then you determine **equivalence** so you can **substitute** complicated deals for simple deals, until you're finally left with one obvious **choice**.\n\nYou might be uncomfortable with some of these axioms. You might say that your preferences genuinely cycle, or you're not willing to assign numbers to uncertain events, or you want there to be an [additional value for certainty](/lw/my/the_allais_paradox/) beyond the prospects involved. I can only respond that these axioms are prescriptive, not descriptive: you will be better off if you behave this way, but you must choose to.\n\nLet's look at an example:\n\nMy Little Decision  \n \n----------------------\n\nSuppose I enter a lottery for MLP toys. I can choose from two kinds of tickets: an A ticket has a 1/3 chance of giving me a Twilight Sparkle, a 1/3 chance of giving me an Applejack, and a 1/3 chance of giving me a Pinkie Pie. A B ticket has a 1/3 chance of giving me a Rainbow Dash, a 1/3 chance of giving me a Rarity, and a 1/3 chance of giving me a Fluttershy. There are two deals for me to choose between- the A ticket and the B ticket- and six prospects, which I'll abbreviate to TS, AJ, PP, RD, R, and FS.\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/f4bcdb0b06b529bac30c18bd6b5a025fd5bdbdb457600b18.jpg)\n\n(Typically, decision nodes are represented as squares, and work just like uncertainty nodes, and so A would be above B with a decision node pointing to both. I've displayed them side by side because I suspect it looks better for small decisions.)\n\nThe first axiom- probability- is already taken care of for us, because our model of the world is already specified. We are rarely that lucky in the real world. The second axiom- order- is where we need to put in work. I need to come up with a preference ordering. I think about it and come up with the ordering TS > RD > R = AJ > FS > PP. Preferences are *personal*\\- beyond requiring internal consistency, we shouldn't require or expect that everyone will think Twilight Sparkle is the best pony. Preferences are also a source of uncertainty if prospects satisfy multiple different desires, as you may not be sure about your indifference tradeoffs between those desires. Even when prospects have only one *measure*, that is, they're all expressed in the same unit (say, dollars), you could be uncertain about your risk sensitivity, which shows up in preference probabilities but deserves a post of its own.\n\nNow we move to axiom 3: I have an ordering, but that's not enough to solve this problem. I need a preference scoring to represent how much I prefer one prospect to another. I might prefer cake to chicken and chicken to death, but the second preference is far stronger than the first! To determine my scoring I need to imagine deals and assign indifference probabilities. There are a lot of ways to do this, but let's jump straight to the most sensible one: compare every prospect to a deal between the best and worst prospect.2\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/1b35646eeedb60772e243dde673a4a606f7740e83f398283.jpg)\n\nI need to assign a *preference* probability *p* such that I'm indifferent between the two deals presented: either RD with certainty, or a chance at TS (and PP if I don't get it). I think about it and settle on .9: I like RD close to how much I like TS.3 This indifference needs to be two-way: I need to be indifferent about trading a ticket for that deal for a RD, and I need to be indifferent about trading a RD for that deal.4 I repeat this process with the rest, and decide .6 for R and AJ and .3 for FS. It's useful to check and make sure that all the relationships I elicited before hold- I prefer R and AJ the same, and the ordering is all correct. I don't need to do this process for TS or PP, as *p* is trivially 1 or 0 in that case.\n\nNow that I have a preference scoring, I can move to axiom 4. I start by making things more complicated- I take all of the prospects that weren't TS or PP and turn them into deals of {*p* TS, 1-*p* PP}. (Pictured is just the expansion of the right tree; try expanding the tree for A. It's much easier.)\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/11847a8dd44d6119ad6015954e3ed7805c95e43650f18655.jpg)\n\nThen, using axiom 1 again, I rearrange this tree. The A tree (not shown) and B tree now have only two prospects, and I've expressed the probabilities of those prospects in a complicated way that I know how to simplify.\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/09ec87770039ebbfab1d948534cb5bbca8401df6c6f42737.jpg)\n\nAnd we have one last axiom to apply: choice. Deal *B* has a higher chance of the better prospect, and so I pick it. Note that that's the case even though my actual chance of receiving TS with deal B is 0%- this is just how I'm representing my preferences, and this computation is telling me that my probability-weighted preference for deal B is higher than my probability-weighted preference for deal A. Not only do I know that I should choose deal B, but I know how much better deal B is for me than deal A.5\n\nThis was a toy example, but the beauty of this method is that all calculations are local. That means we can apply this method to a problem of arbitrary size without changes. Once we have probabilities and preferences for the possible outcomes, we can propagate those from the back of the tree through every node (decision or uncertainty) until we know what to do everywhere. Of course, whether the method will have a runtime shorter than the age of the universe depends on the size of your problem. You could use this to decide which chess moves to play against an opponent whose strategy you can guess from the board configuration, but I don't recommend it.6 Typical real-world problems you would use this for are too large to solve with intuition but small enough that a computer (or you working carefully) can solve it exactly if you give it the right input.\n\nNext we start the meat of decision analysis: reducing the real world to math.\n\n* * *\n\n1\\. These axioms are [Ronald Howard](http://en.wikipedia.org/wiki/Ronald_A._Howard)'s 5 Rules of Actional Thought.\n\n2\\. Another method you might consider is comparing a prospect to its neighbors; RD in terms of TS and R, R in terms of RD and FS, FS in terms of R and PP. You could then unpack those into the preference probability\n\n3\\. Assigning these probabilities is tough, especially if you aren't comfortable with probabilities. Some people find it helpful to use a [probability wheel](http://www.stanford.edu/~savage/software.htm), where they can *see* what 60% looks like, and adjust the wheel until it matches what they feel. See also [1001 PredictionBook Nights](/lw/7z9/1001_predictionbook_nights/) and [This is what 5% feels like](http://rationalpoker.com/2011/04/21/this-is-what-5-feels-like/).\n\n4\\. In actual practice, deals often come with friction and people tend to be attached to what they have beyond the amount that they want it. It's important to make sure that you're actually coming up with an indifference value, *not* the worst deal you would be willing to make, and flipping the deal around and making sure you feel the same way is a good way to check.\n\n5\\. If you find yourself disagreeing with the results of your analysis, double check your math and make sure you agree with all of your elicited preferences. An unintuitive answer can be a sign of an error in your inputs or your calculations, but if you don't find either make sure you're not trying to [start with the bottom line](/lw/js/the_bottom_line/).\n\n6\\. There are supposedly [10120 possible games of chess](http://www.nybooks.com/articles/archives/2010/feb/11/the-chess-master-and-the-computer/?pagination=false), and this method would evaluate all of them. Even with computation-saving implementation tricks, you're better off with another algorithm."
    },
    "voteCount": 40
  },
  {
    "_id": "HnzB46zsL8ehdpLcd",
    "url": null,
    "title": "Checklists",
    "slug": "checklists",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "Checklists"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "[Checklists](http://www.newyorker.com/reporting/2007/12/10/071210fa_fact_gawande) are a rationality technique, [mentioned previously on OB](http://www.overcomingbias.com/2009/02/the-intervention-and-the-checklist-two-paradigms-for-improvement.html). Everyone knows this, but we don't hear about them as often as we should, possibly because they seem prosaic and boring.\n\nIn the context of doing something over and over, there is a checklist improvement cycle.\n\n*   You try to make the thing (e.g. the blog post, the rational decision, the mathematical proof)\n*   For each kind of error in your checklist, you search the thing for that kind of error, and fix it if it occurs.\n*   When something that passed your checklist turns out to have had an error, you add that kind of error to your checklist.\n\nThere are many caveats to this description: Some checklists are not primarily lists of errors, but primarily ordered procedures. You may want to complicate the cycle to track the cost and benefit of the items on the checklist. We're assuming that errors are eventually discovered. I want to pass over those caveats and claim that this kind of checklist-of-errors is very successful. If you agree, my question is: What feature or features of our minds are checklists compensating for? If we understood that, then we would be able to use checklists even more effectively.\n\nThe act of considering the current checklist item and the post/decision/proof simultaneously reminds me of \"Forced Association\", a creativity technique. So one idea is that by putting one's mind into several different states via forced association with the items on the checklist, we gain more independent chances to detect an error.\n\nEven if you haven't made any errors yet, and so your checklist is empty, conducting several searches for errors while wearing [De Bono's hats](http://en.wikipedia.org/wiki/De_Bono_Hats) (or another forced association list) might be a way to make fewer errors.\n\nIf you're consciously looking for a red minivan, then you will notice more red minivans. This \"noticing\" seems surprisingly spontaneous, unlike deliberately scanning a scene and considering \"Is that a red minivan? Is that a red minivan?\". Possibly this is because the noticing is being done by unconscious modules of our minds. A checklist breaks our search for errors into a sequence of searches for more specific kinds of errors. Possibly checklists are effective because the general concept \"error\" is too vague for those modules. By breaking it into easier chunks (e.g. \"ad hominem fallacy\", \"missing semicolon\"), we can start using those modules to get a more thorough search.\n\nI admit, I'm not sure how to use this \"modules\" idea to use checklists more effectively."
    },
    "voteCount": 19
  },
  {
    "_id": "mRsm8gibyuPWas7K5",
    "url": "https://www.lesswrong.com/tag/do-the-math-then-burn-the-math-and-go-with-your-gut",
    "title": "Gems from the Wiki: Do The Math, Then Burn The Math and Go With Your Gut",
    "slug": "gems-from-the-wiki-do-the-math-then-burn-the-math-and-go",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "Rationality"
      },
      {
        "name": "Probabilistic Reasoning"
      }
    ],
    "tableOfContents": {
      "sections": [
        {
          "title": "History",
          "anchor": "History",
          "level": 1
        },
        {
          "title": "See also",
          "anchor": "See_also",
          "level": 1
        },
        {
          "title": "External links",
          "anchor": "External_links",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        },
        {
          "anchor": "comments",
          "level": 0,
          "title": "3 comments"
        }
      ],
      "headingsCount": 5
    },
    "contents": {
      "markdown": "*During the* [*LessWrong 1.0 Wiki Import*](https://www.lesswrong.com/posts/ELN6FpRLoeLJPgx8z/the-wiki-is-dead-long-live-the-wiki-help-wanted) *we (the LessWrong team) discovered a number of great articles that most of the LessWrong team hadn't read before. Since we expect many others to also not have have read these, we are creating a series of the best posts from the Wiki to help give those hidden gems some more time to shine.*\n\n*The original wiki article was fully written by* [*riceissa*](https://www.lesswrong.com/users/riceissa)*, who I've added as a coauthor to this post. Thank you for your work on the wiki!*\n\n* * *\n\n\"**Do the math, then burn the math and go with your gut**\"[^1^](https://www.lesswrong.com/tag/do-the-math-then-burn-the-math-and-go-with-your-gut#fn1) is a procedure for decision-making that has been described by [Eliezer Yudkowsky](https://www.lesswrong.com/tag/eliezer-yudkowsky). The basic procedure is to go through the process of assigning numbers and probabilities that are relevant to some decision (\"do the math\") and then to throw away this calculation and instead make the final decision with one's gut feelings (\"burn the math and go with your gut\"). The purpose of the first step is to force oneself to think through all the details of the decision and to spot inconsistencies.\n\nHistory\n-------\n\nIn July 2008, Eliezer Yudkowsky wrote the blog post \"When (Not) To Use Probabilities\", which discusses the situations under which it is a bad idea to verbally assign probabilities. Specifically, the post claims that while theoretical arguments in favor of using probabilities (such as [Dutch book](https://en.wikipedia.org/wiki/Dutch_book) and [coherence](https://en.wikipedia.org/wiki/Coherence_(philosophical_gambling_strategy)) arguments) always apply, humans have evolved algorithms for reasoning under uncertainty that don't involve verbally assigning probabilities (such as using \"gut feelings\"), which in practice often perform better than actually assigning probabilities. In other words, the post argues in favor of using humans' non-verbal/built-in forms of reasoning under uncertainty even if this makes humans incoherent/subject to Dutch books, because forcing humans to articulate probabilities would actually lead to worse outcomes. The post also contains the quote \"there *are* benefits from trying to translate your gut feelings of uncertainty into verbal probabilities. It may help you spot problems like the conjunction fallacy. It may help you spot internal inconsistencies – though it may not show you any way to remedy them.\"[^2^](https://www.lesswrong.com/tag/do-the-math-then-burn-the-math-and-go-with-your-gut#fn2)\n\nIn October 2011, LessWrong user bentarm gave an outline of the procedure in a comment in the context of the [Amanda Knox case](https://en.wikipedia.org/wiki/Murder_of_Meredith_Kercher). The steps were: \"(1) write down a list of all of the relevant facts on either side of the argument. (2) assign numerical weights to each of the facts, according to how much they point you in one direction or another. (3) burn the piece of paper on which you wrote down the facts, and go with your gut.\" This description was endorsed by Yudkowsky in a follow-up comment. bentarm's comment claims that Yudkowsky described the procedure during summer of 2011.[^3^](https://www.lesswrong.com/tag/do-the-math-then-burn-the-math-and-go-with-your-gut#fn3)\n\nIn December 2016, [Anna Salamon](https://www.lesswrong.com/tag/anna-salamon) described the procedure parenthetically at the end of a blog post. Salamon described the procedure as follows: \"Eliezer once described what I take to be the a similar ritual for avoiding bucket errors, as follows: When deciding which apartment to rent (he said), one should first do out the math, and estimate the number of dollars each would cost, the number of minutes of commute time times the rate at which one values one's time, and so on. But at the end of the day, if the math says the wrong thing, one should do the right thing anyway.\"[^4^](https://www.lesswrong.com/tag/do-the-math-then-burn-the-math-and-go-with-your-gut#fn4)\n\nSee also\n--------\n\n*   [CFAR Exercise Prize](https://www.lesswrong.com/tag/cfar-exercise-prize) – Andrew Critch's Bayes game, described on this page, gives another technique for dealing with uncertainty in real-life situations\n\nExternal links\n--------------\n\n*   [A Facebook post by Julia Galef from May 2018 inquiring about this procedure](https://www.facebook.com/julia.galef/posts/10103884948339182)\n*   [\"Why we can’t take expected value estimates literally (even when they’re unbiased)\"](https://blog.givewell.org/2011/08/18/why-we-cant-take-expected-value-estimates-literally-even-when-theyre-unbiased/) (August 2011) by [GiveWell](https://www.lesswrong.com/tag/givewell) co-founder [Holden Karnofsky](https://www.lesswrong.com/tag/holden-karnofsky) makes a similar point: \"It’s my view that my brain instinctively processes huge amounts of information, coming from many different reference classes, and arrives at a prior; if I attempt to formalize my prior, counting only what I can name and justify, I can worsen the accuracy a lot relative to going with my gut. Of course there is a problem here: going with one’s gut can be an excuse for going with what one wants to believe, and a lot of what enters into my gut belief could be irrelevant to proper Bayesian analysis. There is an appeal to formulas, which is that they seem to be susceptible to outsiders’ checking them for fairness and consistency.\"\n*   [\"The Optimizer’s Curse & Wrong-Way Reductions\"](https://confusopoly.com/2019/04/03/the-optimizers-curse-wrong-way-reductions/) by Christian Smith discusses similar issues\n*   [Verbal overshadowing](https://en.wikipedia.org/wiki/Verbal_overshadowing) page on Wikipedia\n\n* * *\n\n1.  Qiaochu Yuan. [\"Qiaochu_Yuan comments on A Sketch of Good Communication\"](https://www.lesswrong.com/posts/yeADMcScw8EW9yxpH/a-sketch-of-good-communication/comment/puSysxsCG2i98XHMW). March 31, 2018. *LessWrong*.[↩](https://www.lesswrong.com/tag/do-the-math-then-burn-the-math-and-go-with-your-gut#fnref1)\n2.  Eliezer Yudkowsky. [\"When (Not) To Use Probabilities\"](https://www.lesswrong.com/posts/AJ9dX59QXokZb35fk/when-not-to-use-probabilities). July 23, 2008. *LessWrong*.[↩](https://www.lesswrong.com/tag/do-the-math-then-burn-the-math-and-go-with-your-gut#fnref2)\n3.  bentarm. [\"bentarm comments on Amanda Knox: post mortem\"](https://www.lesswrong.com/posts/Jx4gGbPi7GuydwvzB/amanda-knox-post-mortem/comment/aFe8RxLnH3JWtNJcD). October 21, 2011. *LessWrong*.[↩](https://www.lesswrong.com/tag/do-the-math-then-burn-the-math-and-go-with-your-gut#fnref3)\n4.  Anna Salamon. [\"'Flinching away from truth' is often about \\*protecting\\* the epistemology\"](https://www.lesswrong.com/posts/EEv9JeuY5xfuDDSgF/flinching-away-from-truth-is-often-about-protecting-the). December 20, 2016. *LessWrong*.[↩](https://www.lesswrong.com/tag/do-the-math-then-burn-the-math-and-go-with-your-gut#fnref4)"
    },
    "voteCount": 20
  }
]