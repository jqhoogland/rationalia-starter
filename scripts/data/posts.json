[
  {
    "_id": "ezTA3fSPCWZieBfjS",
    "url": null,
    "title": "Adversarial epistemology",
    "slug": "adversarial-epistemology",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "Trust"
      },
      {
        "name": "World Modeling"
      },
      {
        "name": "Epistemic Hygiene"
      },
      {
        "name": "Rationality"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "This is the first article in my *[Bah-Humbug Sequence](https://www.lesswrong.com/s/9ctJpFJvf8h5ivT6Q)* a.k.a. \"Everything I Don't Like Is A Defect/Defect Equilibrium\". Epistemic status: strong opinion weakly held, somewhat exaggerated for dramatic effect; I'm posting this here so that the ensuing discussion might help me clarify my position. Anyway, the time has now come for me to explain my overbearing attitude of cynicism towards all aspects of life. Why now, of all times? I hope to make that clear by the end.\n\n----\n\n> You are asking me to believe a certain claim. There is a simple and easy thing you can do to prove its trustworthiness, and yet you have not done that. I am therefore entitled to [Weak Adversarial Argument] disregard your claim as of no evidentiary value / [Strong Adversarial Argument] believe the negation of your claim purely out of spite.\n\nWhat's going on here? Are these valid arguments?\n\nIt may help to give some examples:\n\n1. *The Hearsay Objection* - In a court of law, if a witness X tries to testify that some other person Y said Z, in trying to establish the truth of Z, the opposing side may object. This objection takes the form: \"The opposition has brought in X to prove Z by way of the fact that Y said Z. But X is not the most reliable witness they could have called, because they could have summoned Y instead. If they were genuinely seeking the truth as to Z, they would have done so; and yet we see that they did not. Therefore I insist that X's testimony be stricken from the record.\"\n\n2. *The Cynical Cryptographer* - My company's HR department emails me a link to an employee satisfaction survey. The email is quick to say \"Your responses are anonymous\", and yet I notice that the survey link contains a bunch of gobbledegook like `?id=2815ec7e931410a5fb358588ee70ad8b`. I think to myself: If this actually *is* anonymous, and not a sham to see which employees have attitude problems and should be laid off first, the HR department could have set up a [Chaumian blind signature](https://en.wikipedia.org/wiki/Blind_signature) protocol to provably guarantee that my response cannot be linked to my name. But they didn't, and so I conclude that this survey *is* a sham, and I won't fill it out.\n\nSo, again, are these valid arguments? From a Bayesian perspective, not really:\n\n1. X saying that Y said Z is not *literally zero* evidence of Z. If there is any chance >0 that X and Y are honest, then I must update at least somewhat towards the truth of Z upon hearing X's testimony.\n\n2. I'm pretty sure they don't teach cryptography in business school. An honest HR department and a dishonest one have approximately equal likelihood (i.e. ε) of knowing what a \"Chaumian blind signature\" is and actually implementing it. Therefore, by Bayes' theorem, etc.\n\nTo steelman the Adversarial Argument, we should understand it not as an ordinary passive attempt to \"rationally\" form an accurate world-model, but rather as a sort of acausal negotiation tactic, akin to one-boxing on Newcomb's Problem. By adopting it, we hope to \"influence\" the behavior of adversaries (i.e. people who want to convince us of something but don't share our interests) towards providing stronger evidence, and away from trying to deceive us.\n\nOr, to put it another way, the Adversarial Argument may not be valid in general, but by proclaiming it loudly and often, we can *make* it valid (at least in certain contexts) and thus make distinguishing truth and falsehood easier. *Because* the Hearsay Objection is enforced in court, lawyers who want to prove Z will either introduce direct witnesses or drop the claim altogether. And perhaps (we can dream!) if the Cynical Cryptographer argument catches on, honest HR departments will find themselves compelled to add Chaumian blind signatures to their surveys in order to get any responses, making the sham surveys easy to spot.\n\n(Aside: Even under this formulation, we might accept the Weak Adversarial Argument but reject the Strong Adversarial Argument - by adopting a rule that I'll believe the opposite of what an untrustworthy-seeming person says, I'm now setting myself up to be deceived into believing P by a clever adversary who asserts ¬P in a deliberately sleazy way - whereupon I'll congratulate myself for seeing through the trick! Is there any way around this?)\n\nNow, returning to the template above, the premise that \"there is a simple and easy thing you can do to prove its trustworthiness\" is doing a lot of work. Your adversary will always contend that the thing you want them to do (calling witness Y, adding Chaumian signatures, etc.) is too difficult and costly to reasonably expect of them. This may or may not be true, but someone who's trying to deceive you will claim such regardless of its truth, hoping that they can \"blend in\" among the honest ones.\n\nAt that point, the situation reduces to a contest of wills over who gets to grab how much of the surplus value from our interaction. What is my trust worth to you? How much personal cost will you accept in order to gain it?\n\n----\n\nWe on LessWrong - at least, those who wish to communicate the ideas we discuss here with people who don't already agree - should be aware of this dynamic. There may have been a time in history when charismatic authority or essays full of big words were enough to win people over, but that is far from our present reality. In our time, propaganda and misinformation are well-honed arts. People are \"[accustomed to a haze of plausible-sounding arguments](https://www.lesswrong.com/posts/PRAyQaiMWg2La7XQy/moloch-s-toolbox-2-2)\" and are rightly skeptical of all of them. Why should they trust the ideas on LessWrong, of all things? If we think gaining their trust is important and valuable, how much personal cost are we willing to accept to that end?\n\nOr, backing up further: Why should *you* trust what you read here?"
    },
    "voteCount": 6,
    "forceInclude": true
  },
  {
    "_id": "CewHdaAjEvG3bpc6C",
    "url": null,
    "title": "Epistemic Artefacts of (conceptual) AI alignment research",
    "slug": "epistemic-artefacts-of-conceptual-ai-alignment-research",
    "author": "Nora_Ammann",
    "question": false,
    "tags": [
      {
        "name": "AI"
      },
      {
        "name": "Practice & Philosophy of Science"
      },
      {
        "name": "Distillation & Pedagogy"
      },
      {
        "name": "Intellectual Progress (Society-Level)"
      },
      {
        "name": "PIBBSS"
      }
    ],
    "tableOfContents": {
      "sections": [
        {
          "title": "Tl;dr",
          "anchor": "Tl_dr",
          "level": 1
        },
        {
          "title": "Four Types of Epistemic Artefacts",
          "anchor": "Four_Types_of_Epistemic_Artefacts",
          "level": 1
        },
        {
          "title": "(1) Map-making (i.e. conceptual de-confusion, gears-level understanding of relevant phenomena, etc.)",
          "anchor": "_1__Map_making__i_e__conceptual_de_confusion__gears_level_understanding_of_relevant_phenomena__etc__",
          "level": 2
        },
        {
          "title": "(2) Identifying and specifying risk scenarios",
          "anchor": "_2__Identifying_and_specifying_risk_scenarios",
          "level": 2
        },
        {
          "title": "(3) Characterising target behaviour",
          "anchor": "_3__Characterising_target_behaviour",
          "level": 2
        },
        {
          "title": "(4) Formalising alignment proposals",
          "anchor": "_4__Formalising_alignment_proposals",
          "level": 2
        },
        {
          "title": "Some additional thoughts ",
          "anchor": "Some_additional_thoughts_",
          "level": 1
        },
        {
          "title": "How these artefacts relate to each other",
          "anchor": "How_these_artefacts_relate_to_each_other",
          "level": 2
        },
        {
          "title": "Epistemic artefacts in your own research",
          "anchor": "Epistemic_artefacts_in_your_own_research",
          "level": 2
        },
        {
          "title": "Summary",
          "anchor": "Summary",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        },
        {
          "anchor": "comments",
          "level": 0,
          "title": "1 comment"
        }
      ],
      "headingsCount": 12
    },
    "contents": {
      "markdown": "*The fact that this post is seeing the light of day now rather than in some undefined number of weeks is, not in small part, due to participating in the second* [*Refine*](https://www.lesswrong.com/posts/5uiQkyKdejX3aEHLM/how-to-diversify-conceptual-alignment-the-model-behind) *blog post day. Thank you,* fake-but-useful containers, *and thank you, Adam. *\n\nTl;dr\n=====\n\nIn this post, I describe four types of insights - what I will call **Epistemic Artefacts** \\- that we may hope to acquire through (conceptual) AI alignment research. I provide examples and briefly discuss how they relate to each other and what role they play on the path to solving the AI alignment problem. The hope is to add some useful vocabulary and reflective clarity when thinking about what it may look like to contribute to solving AI alignment. \n\nFour Types of Epistemic Artefacts\n=================================\n\nInsofar as we expect conceptual AI alignment research to be helpful, what sorts of insights (here: “epistemic artefacts”) do we hope to gain? \n\nIn short, I suggest the following taxonomy of potential epistemic artefacts: \n\n1.  Map-making (de-confusion, gears-level models, etc.)\n2.  Characterising risk scenarios\n3.  Characterising target behaviour\n4.  Developing alignment proposals\n\n(1) Map-making (i.e. conceptual de-confusion, gears-level understanding of relevant phenomena, etc.)\n----------------------------------------------------------------------------------------------------\n\nFirst, research can aim to develop a [gears-level understanding](https://www.lesswrong.com/tag/gears-level) of phenomena that appear critical for properly understanding the problem as well as for formulating solutions to AI alignment (e.g. intelligence, agency, values/preferences/intents, self-awareness, power-seeking, etc.). Turns out, it’s hard to think clearly about AI alignment without having a good understanding of and “good vocabulary” for phenomena that lie at the heart of the problem. In other words, the goal of \"map-making\" is to dissolve conceptual bottlenecks holding back progress in AI alignment research at a the moment.\n\nFiguratively speaking, this is where we are trying to draw **more accurate maps** that help us better navigate the territory. \n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/5d3e7101ffff5130f6886c5b9a83c8999454e094cdfbc9bd.png)\n\nSome examples of work on this type of epistemic artefact include [Agency: What it is and why it matters](https://www.alignmentforum.org/s/aek5ksSs2FHTeofsf), [Embedded Agency](https://www.alignmentforum.org/s/Rm6oQRJJmhGCcLvxh), [What is bounded rationality?](https://www.taylorfrancis.com/chapters/edit/10.4324/9781315658353-2/bounded-rationality-gerd-gigerenzer), [The ground of optimization](https://www.alignmentforum.org/posts/znfkdCoHMANwqc2WE/the-ground-of-optimization-1), [Game Theory](https://link.springer.com/referencework/10.1007/978-1-0716-0368-0), [Mathematical Theory of Communication](https://people.math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf), [Functional Decision Theory](https://arxiv.org/abs/1710.05060) and [Infra-Bayesianism](https://www.alignmentforum.org/s/CmrW8fCmSLK7E25sa)—among many others. \n\n(2) Identifying and specifying risk scenarios\n---------------------------------------------\n\nWe can further seek to identify (new) civilizational risk scenarios brought about by advanced AI and to better understand the mechanisms leading to risk scenarios. \n\nFiguratively speaking, this is where we try to identify and describe **the monsters hiding in the territory**, so we can circumvent them when navigating the territory.\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/3e5f9350a34139f200f7928ae0b1a5da099e63c020b25818.png)\n\nWhy does a better understanding of risk scenarios represent useful progress towards AI alignment? In principle, one way of guaranteeing a safe future is by identifying every way things could go wrong and finding ways to defend against each of them. (We could call this a “via negativa” approach to AI alignment.) I am not actually advocating for adopting this approach literally, but it still provides a good intuition for why understanding the range of risk scenarios and their drivers/mechanisms is useful. (More thoughts on this later, in \"How these artefacts relate to each other\".)\n\nIn identifying and understanding risk scenarios, like in many other epistemic undertakings, we should seek to apply a diverse set of epistemic perspectives on how the world works in order to gain a more accurate, nuanced, and robust understanding of risks and failure stories and avoid falling prey to blind spots. \n\nSome examples of work on this type of epistemic artefact include [What failure looks like](https://www.lesswrong.com/posts/HBxe6wdjxK239zajf/what-failure-looks-like), [What Multipolar Failure Looks Like](https://www.lesswrong.com/posts/LpM3EAakwYdS6aRKf/what-multipolar-failure-looks-like-and-robust-agent-agnostic), [The Parable of Predict-O-Matic](https://www.lesswrong.com/s/dxCZoP3TDaB8Acwjo/p/SwcyMEgLyd4C3Dern), [The Causes of Power-seeking and Instrumental Convergence](https://www.alignmentforum.org/s/fSMbebQyR4wheRrvk), [Risks from Learned Optimization](https://www.alignmentforum.org/s/r9tYkB2a8Fp4DN8yB), [Thoughts on Human Models](https://www.alignmentforum.org/posts/BKjJJH2cRpJcAnP7T/thoughts-on-human-models), [Paperclip Maximizer](https://arbital.com/p/paperclip_maximizer/), and [Distinguishing AI takeover scenarios](https://www.lesswrong.com/posts/qYzqDtoQaZ3eDDyxa/distinguishing-ai-takeover-scenarios)—among many others.   \n\n(3) Characterising target behaviour\n-----------------------------------\n\nThirdly, we want to identify and characterize, from within the space of all possible behaviours, those behaviours which are safe and aligned - which we will call “target behaviour”. \n\nFiguratively speaking, this is where we try to identify and describe **candidate destinations **on our maps, i.e., the places we want to learn to navigate to.\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/52d0659db66f73c2b65c1019c91d726f18bacb90fe308350.png)\n\nThis may involve studying intelligent behaviour in humans and human collectives to better understand the structural and functional properties of what it even is we are aiming for. There is a lot of untangling to be done when it comes to figuring out what to aim advanced AI systems at, assuming, for a moment, that we know how to robustly aim these systems at all.  \n\nIt is also conceivable that we need not understand the nature of human agency and human valu-ing in order to specify target behaviour. We can imagine a system with properties that guarantee safety and alignment which are subject-agnostic. An example of such a subject-agnostic property (not necessary a sufficient-for-alignment property) is truthfulness.\n\nSome examples of work on this type of epistemic artefact include [Corrigibility](https://www.alignmentforum.org/tag/corrigibility), [Truthful AI](https://www.alignmentforum.org/posts/aBixCPqSnTsPsTJBQ/truthful-ai-developing-and-governing-ai-that-does-not-lie), [Asymptotically Unambitious Artificial General Intelligence](https://ojs.aaai.org/index.php/AAAI/article/view/5628/5484), [Shard Theory of Human Values](https://www.alignmentforum.org/s/nyEFg3AuJpdAozmoX) and [Pointing at Normativity](https://www.alignmentforum.org/s/Gmc7vtnpyKZRHWdt5) —among (not that many) others.\n\n(4) Formalising alignment proposals\n-----------------------------------\n\nFinally, we are looking to get \"full\" proposals, integrating the “what” and “how” of alignment into an *actual plan *for how-we-are-going-to-build-AGI-that’s-not-going-to-kill-us. Such a proposal will combine insights from map-making, risks stories, and target behaviour, as well as adding new bits of insights. \n\nFiguratively speaking, this is where we try to come up with a **path/plan for how we will navigate the territory in order to reach the destinations while avoiding the monsters**.\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/3d09b0cae514bb812329d03d48bfa14553e765e35faf75f9.png)\n\n[In Adam’s words](https://www.alignmentforum.org/posts/5uiQkyKdejX3aEHLM/how-to-diversify-conceptual-alignment-the-model-behind): “We need far more conceptual AI alignment research approaches than we have now if we want to increase our chances to solve the alignment problem.” One way of getting new substantive approaches is by letting people make sense of the alignment problem through the lenses of and using the conceptual affordances granted by their own field of expertise. This can lead to different perspectives on and ways of modelling the problem—as well, hopefully, as conceiving of possible solutions.\n\nSome examples of work on this type of epistemic artefact include [\\[Intro to brain-like-AGI safety\\] 12. Two paths forward: “Controlled AGI” and “Social-instinct AGI”](https://www.alignmentforum.org/s/HzcM2dkCq7fwXBej8/p/Sd4QvG4ZyjynZuHGt), [Iterated Amplification/HCH](https://www.alignmentforum.org/s/EmDuGeRw749sD3GKd), [Value Learning](https://www.alignmentforum.org/s/4dHMdK5TLN6xcqtyc), and [The Big Picture Of Alignment (Talk Parts 1 + 2)](https://www.alignmentforum.org/posts/aEtc5GgqJGFtTH2kQ/the-big-picture-of-alignment-talk-part-2-1) —among (not many) others.  \n\nSome additional thoughts \n=========================\n\nHow these artefacts relate to each other\n----------------------------------------\n\nWe introduced four different types of epistemic artefacts: (1) map-making, (2) risk scenarios, (3) target behaviour, and (4) alignment proposals. In reality, they don’t exist in isolation from each other but flow through and into each other while working towards solving AI alignment. \n\nThe ultimate goal, of course, is to create fool-proofed alignment proposals.  Artefacts 1, 2, and 3 function as building blocks, enabling us to produce more robust proposals. That said, in reality, the relationship between these four categories is (or should be) less linear than that. By putting these different elements \"into conversation\" with each other, they can be corrected and refined over time. \n\nFor example, a better understanding of key phenomena like \"agency\" can, on the one hand, refine what we should expect could go wrong if we develop superintelligent *agents* (i.e. artefact 2). It may also be able to help us improve our understanding of what artificial behaviours we are/should be aiming at (i.e. artefact 3). A more refined and comprehensive understanding of risk scenarios will be able to help us challenge and problematise existing alignment proposals, thereby weeding out bad ones and strengthening promising ones. Or, finally, as we are thinking about risk scenarios, or target behaviours, or alignment proposals, we might notice how a concept of agency we are using at a given point in time may actually be inadequate in some relevant ways, raising the need to \"go back to the conceptual drawing board” (aka map-making/de-confusion work; artefact 1). \n\n“Conversations” between these different artefacts can take place at different scales; sometimes an individual researcher will work on several of these artefacts and have them interact with each other within their own research. Other times, different research groups will primarily focus on one artefact, and other researchers will supply input from other artefacts. Thus, in this case, the “conversation” is taking place “between” research projects or programs.  \n\nEpistemic artefacts in your own research\n----------------------------------------\n\nEven if it is what we eventually care about, coming up with fully fledged alignment proposals is difficult. This can be disheartening for researchers because it might feel like they are \"not working on the real thing\" if they are not more or less directly on track to develop or advance a proposal to solving AI alignment. \n\nThe above list of artefacts might make it easier for researchers to reflect - individually or in conversation with others - on what they are trying to achieve with their research and how that might eventually feed into solving AI alignment. While it's very important to be tightly grounded in and repeatedly (re)oriented to \"the real thing\"/\"the hard part of the problem\"/etc., I also believe that each of these epistemic arefacts constitutes important ingredients towards guaranteeing a safe and flourishing future, and that people mining those artefacts are making important contributions to AI alignment. \n\nSummary\n=======\n\n1.  I identify four categories of **epistemic artefacts **we may hope to retrieve from conceptual AI alignment research: a) conceptual de-confusion, b) identifying and specifying risk scenarios, c) characterising target behaviour, and d) formalising alignment strategies/proposals.\n2.  I briefly discuss some ways in which these epistemic artefacts relate to and flow through each other.\n\n*Acknowledgements: A lot of these ideas originated with TJ/particlemania and credit should be given accordingly. Mistakes are my own. Thanks, also, to a number of people for useful discussions and feedback, in particular Adam Shimi, Tomas Gavenciack and Eddie Jean.*"
    },
    "voteCount": 13,
    "forceInclude": true
  },
  {
    "_id": "FuToH2KHxKmJLGk2B",
    "url": null,
    "title": "AI alignment as “navigating the space of intelligent behaviour”",
    "slug": "ai-alignment-as-navigating-the-space-of-intelligent",
    "author": "Nora_Ammann",
    "question": false,
    "tags": [
      {
        "name": "AI"
      },
      {
        "name": "PIBBSS"
      },
      {
        "name": "AI Alignment Fieldbuilding"
      },
      {
        "name": "Distillation & Pedagogy"
      },
      {
        "name": "Practice & Philosophy of Science"
      }
    ],
    "tableOfContents": {
      "sections": [
        {
          "title": "Tl;dr",
          "anchor": "Tl_dr",
          "level": 1
        },
        {
          "title": "How to make progress in AI alignment? ",
          "anchor": "How_to_make_progress_in_AI_alignment__",
          "level": 1
        },
        {
          "title": "Strategies for exploring the space of intelligent behaviour",
          "anchor": "Strategies_for_exploring_the_space_of_intelligent_behaviour",
          "level": 1
        },
        {
          "title": "Strategy 1: Tinkering",
          "anchor": "Strategy_1__Tinkering",
          "level": 2
        },
        {
          "title": "Strategy 2: Idealization",
          "anchor": "Strategy_2__Idealization",
          "level": 2
        },
        {
          "title": "Strategy 3: Learning from “intelligence-in-the-wild”",
          "anchor": "Strategy_3__Learning_from__intelligence_in_the_wild_",
          "level": 2
        },
        {
          "title": "Summary",
          "anchor": "Summary",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        },
        {
          "anchor": "comments",
          "level": 0,
          "title": "No comments"
        }
      ],
      "headingsCount": 9
    },
    "contents": {
      "markdown": "Tl;dr\n=====\n\nIn this post, I introduce a conceptual tool for thinking about the epistemic landscape of AI alignment and then describe three epistemic strategies for making progress on the alignment problem: 1) tinkering, 2) idealisation and 3) intelligence-in-the-wild. \n\nHow to make progress in AI alignment? \n======================================\n\nThe future of AI progress is likely to critically shape, if not practically determine, the future of humanity, and of sentient life in general. Will our future look more like a world filled to the brim with things we find valuable, as well as sentient creatures to enjoy that goodness? Or will our future look more like one characterized by violence, mistrust, inequality, suffering, or even the absence of anything sentient at all?\n\nAs our civilization is making progress on our abilities to engineer and instantiate sophisticated forms of complex and intelligent behaviours in artificial systems, it becomes imperative to carefully think about which objectives this intelligence is being directed at, and how to do such \"directing\" robustly. Thus, the central question becomes: how can we make sure that ‘big effects’ caused by AI progress will be positive rather than harmful, safe rather than dangerous, helping to promote, discover, and enrich what is dear to us rather than destroying it? This is the guiding question of AI alignment, as I understand it. \n\nOnce we have established the importance of the problem, the next question that stands out is: *How *can we make progress?\n\nThis is a central question for the “philosophy of science” of AI alignment, as I see it. For example, in order to help answer the question of (the best) ways to make progress on the problem, we can reflect on its shape or structure. We can thus notice how one important defining characteristic of the alignment problem is that it concerns [systems that do not exist yet](https://www.alignmentforum.org/s/LLEJJoaYpCoS5JYSY/p/FQqcejhNWGG8vHDch)—let’s call this the problem of “**epistemic access**”. This means that our typical epistemic strategies of science and engineering are less effective here than they are for a range of other societal problems (e.g., finding cures for diseases, improving the yield of a certain crop, building ever taller houses, etc.).^[\\[1\\]](#fnzbcr1nhdgzl)^\n\nIn this post, I will describe how I currently like to think about the landscape of epistemic strategies for making progress in AI alignment. Of course, there are several plausible ways of carving up the space of epistemic strategies, and each of them may serve different purposes. The tri-partition of epistemic strategies I introduce here is specifically grounded in asking what strategies we can adopt for overcoming the challenge of epistemic access, as introduced above.\n\nStrategies for exploring the space of intelligent behaviour\n===========================================================\n\nSo, what are different epistemic strategies we can use in order to (hopefully) make progress on AI alignment? Let’s start by introducing the following conceptual tool. \n\nImagine a space (of uncertain size) that corresponds to all possible manifestations of intelligent behaviour. Abstractly speaking, different epistemic strategies correspond to different approaches to charting out this possibility space, and the endeavour of AI alignment at large (roughly) corresponds to learning to safely navigate movement through this (design) space. \n\nNow, let’s consider different ways of charting this space out.^[\\[2\\]](#fnlikwbbqigel)^ \n\n**Strategy 1: Tinkering**\n-------------------------\n\nOne approach is to work with contemporary ML paradigms and explore, with the help of empirical methods and trial and error, how those systems behave, how they fail to be safe and aligned, and what it would look like for them to be (\\[1\\] in Fig. 1). The hope is that (some of) those insights will generalize to more-advanced AI systems. \n\nIn terms of our figure, this approach explores the possibility space of intelligent behaviour by tinkering at the boundaries of the subspace that we currently know how to implement with ML technology. \n\nMost of what is commonly referred to as “AI safety research” will fall into this category. Here are some examples: \n\n*   **Risks in current and future ML systems**, e.g., Amodei, Olah et al (2016). [Concrete Problems in AI Safety](https://arxiv.org/abs/1606.06565).\n*   **Reward modeling**, e.g., Leike, Krueger et al (2018).[ Scalable agent alignment via reward modeling: a research direction](https://arxiv.org/abs/1811.07871). \n*   **Iterated amplification**,^[\\[3\\]](#fn5ubi5vd9jvy)^ e.g., Christiano, Shlegeris, Amodei (2018). [Supervising strong learners by amplifying weak experts](https://arxiv.org/abs/1810.08575).\n*   **Interpretability**, e.g., Olah, Cammarata et al (2020).[ Zoom In: An Introduction to Circuits](https://distill.pub/2020/circuits/zoom-in/).\n*   And many more…\n\n**Strategy 2: Idealization**\n----------------------------\n\nThe second approach I want to delineate here seeks theoretical clarity by starting from idealized frameworks of reasoning (e.g., decision theory, epistemology) and exploring, via extrapolation, what the in-the-limit behaviour of “Idealized Agents” would look like and what it would take for them to be safe and aligned (\\[2\\] in Fig. 1). \n\nCompared to the former approach, looking at idealized frameworks and in-the-limit behaviour leads to different assumptions about what advanced AI systems will look like (i.e., not necessarily prosaic^[\\[4\\]](#fn6w3csw2n7o)^). (There are some reasons to think those assumptions are *more *relevant, and some reasons to think that they are less relevant.) At the same time, idealized frameworks face some challenges due to the difficulty of achieving empirical grounding compared to the “tinkering” approach.\n\nExamples of work that count towards this approach include: \n\n*   **Decision Theory**, e.g., Yudkowsky and Soares (2017). [Functional Decision Theory: A New Theory of Instrumental Rationality](https://arxiv.org/abs/1710.05060). \n*   **Logical Uncertainty**, e.g., Garrabrant et al. (2016). [Logical Induction](https://arxiv.org/abs/1609.03543).\n*   **Agent Foundations**, e.g., Garrabrant and Demski (2018). [Embedded agency](https://www.alignmentforum.org/posts/i3BTagvt3HbPMx6PN/embedded-agency-full-text-version). \n*   **Infra-Bayesianism**, e.g., Vanessa Kosoy (2021). [Infra-Bayesian physicalism: a formal theory of naturalized induction. ](https://www.alignmentforum.org/posts/gHgs2e2J5azvGFatb/infra-bayesian-physicalism-a-formal-theory-of-naturalized)\n*   **Power Seeking in Optimal Agents**, e.g., Alexander Turner (2021). [The Causes of Power-seeking and Instrumental Convergence. ](https://www.alignmentforum.org/s/fSMbebQyR4wheRrvk)\n\n**Strategy 3: Learning from “intelligence-in-the-wild”**\n--------------------------------------------------------\n\nFinally, the third approach attempts to chart out the possibility space of intelligent behaviour by looking at how intelligent behaviour manifests in existing natural systems (\\[3\\] in Fig. 1). (This is, notably, the epistemic approach that [PIBBSS](https://www.pibbss.ai/) is trying to explore and foster.)\n\nTreating intelligent behaviour (and related features of interest) as a naturally occurring phenomenon suggests a sort of “realism” about intelligence. It’s not just some theoretical construct but something that we can observe every day in the real world. This strategy involves a certain type of *empiricism *(although of a different nature than the empiricism of the “tinkering“ approach). \n\nLet’s look at some examples of how this approach has already been used in alignment research (although I’d argue it remains, overall, rarer than work along the lines of the other two approaches): \n\n*   **Analogies from human brain, **à la Steve Byrnes (2021). [Value loading in the human brain: a worked example](https://www.alignmentforum.org/posts/iMM6dvHzco6jBMFMX/value-loading-in-the-human-brain-a-worked-example) (among others).\n    *   …uses neuroscience as a source of analogies for alignment.\n*   **Analogies from real-world systems, **à la John Wentworth (2020). [Characterizing Real-World Agents as a Research Meta-Strategy ](https://www.lesswrong.com/posts/9pZtvjegYKBALFnLk/characterizing-real-world-agents-as-a-research-meta-strategy)(among others). \n    *   …sets out to understand agency as a real-world phenomenon looking at systems in physics, biology, economics, and beyond. \n*   **Analogies from social systems**, à la Critch (2021). [What Multipolar Failure Looks Like, and Robust Agent-Agnostic Processes. ](https://www.alignmentforum.org/posts/LpM3EAakwYdS6aRKf/what-multipolar-failure-looks-like-and-robust-agent-agnostic)\n    *   … implicitly builds on the concept of[ fields](https://en.wikipedia.org/wiki/Field_theory_(sociology)) from sociology, and the[ structure and agency debate](https://en.wikipedia.org/wiki/Structure_and_agency) in the social sciences about the place of social structure vs personal choices in human behaviour.\n*   **Analogies from biological systems**, à la Hubinger et al. (2019).[ Risks from Learned Optimization in Advanced ML Systems](https://arxiv.org/abs/1906.01820).\n    *   …explicitly uses biological evolution of humans as an example of/inspiration for mesa-optimizers. Evolution here is the search process, with a base-objective of increasing fitness and propagation of genes, but the learned models (humans) end up searching for different goals.\n\n*![](https://lh3.googleusercontent.com/FOxM9aAboSUqM2H52CDgKXJEmAh72DDb3C1yt5zA-WXz9GkaJCCIHBi3CeanLGP5ox9s9JZb7Hh-lV8YWGgO7e30hmAVc2BRVfy3OqqQRyxqy-T5Nyi-pX3Z669RoKxUW2KZwy3jffuYZP3kAiw2vF4)*\n\nFigure 1: *Epistemic strategies for making progress in AI alignment as charting the possibility space of intelligent behaviour; 1. Tinkering; 2. Idealisation; 3. Intelligence-in-the-wild*\n\nSummary\n=======\n\n1.  We can analogise progress in AI alignment as charting out and learning how to navigate the **possibility space of intelligent behaviour**. Different research strategies in AI alignment correspond to different ways of charting out that space. For example:\n    1.  **Tinkering **is an epistemic approach whereby we explore the boundaries of intelligent behaviour that we currently know to implement in ML systems, using empirical and experimental methods. \n    2.  **Idealization **uses idealised frameworks of reasoning and extrapolation to explore the possibility space of intelligent behaviour “in-the-limits”.\n    3.  **Intelligence-in-the-wild **seeks to chart out the possibility space of intelligent behaviour by investigating how it manifests in existing (natural) systems. It is based on an assumption of “realism about intelligence” (see future post for more details). \n\n*Acknowledgements*: *I thank TJ/particlemania for discussing and improving many of the ideas in this post; I also thank Adam Shimi, Gavin Leech, Cara Selvarajah, Eddie Jean and Justis for useful comments on earlier drafts. *\n\n1.  ^**[^](#fnrefzbcr1nhdgzl)**^\n    \n     For more insight into the specific epistemic challenges we face in AI alignment, consider checking out other examples of Adam Shimi’s writing, e.g., [On Solving Problems Before They Appear: The Weird Epistemologies of Alignment](https://www.alignmentforum.org/posts/FQqcejhNWGG8vHDch/on-solving-problems-before-they-appear-the-weird), [Epistemological Vigilance for Alignment](https://www.alignmentforum.org/posts/72scWeZRta2ApsKja/epistemological-vigilance-for-alignment), or [Robustness to Scaling Down: More Important Than I Thought](https://www.alignmentforum.org/posts/pA3F9oejzvGg6Kf3a/robustness-to-scaling-down-more-important-than-i-thought).\n    \n2.  ^**[^](#fnreflikwbbqigel)**^\n    \n     There are different ways one can choose to taxonomise the space. For example, a common distinction that is sometimes made is the one between “de-confusion” and “problem-solving” work. I consider this distinction orthogonal to the three-way classification I am introducing here. In particular, my view is that the distinction introduced in this post (i.e., “tinkering”, “idealisation”, and “intelligence-in-the-wild”) is about what types of systems are being theorised about (roughly speaking: STOA ML systems, idealized systems, and natural systems), or where the theories seek empirical validity. In this post \\[forthcoming\\], I discuss, in more detail, what sorts of systems and empirical basis the intelligence-in-the-wild approach is interested in/based on. (Thanks to TJ for helping me to make this clarification explicit.)\n    \n3.  ^**[^](#fnref5ubi5vd9jvy)**^\n    \n     It is important to note that these are not strict divisions, and some research agendas might involve both tinkering insights and idealisations. IDA is a good example of that. However, for the purposes of this post, I'm tentatively putting it in the tinkering list.\n    \n4.  ^**[^](#fnref6w3csw2n7o)**^\n    \n    \"Prosaic\" is used in somewhat different ways by different people. Here, it is used in its ~technical usage meaning approximately: \"using extensions of current SOTA ML\"."
    },
    "voteCount": 8,
    "forceInclude": true
  },
  {
    "_id": "7X9KKqgZa7edknKPm",
    "url": null,
    "title": "Goal-directedness: my baseline beliefs",
    "slug": "goal-directedness-my-baseline-beliefs",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "Goal-Directedness"
      },
      {
        "name": "AI"
      }
    ],
    "tableOfContents": {
      "sections": [
        {
          "title": "Existing ideas on goal-directedness",
          "anchor": "Existing_ideas_on_goal_directedness",
          "level": 1
        },
        {
          "title": "What do I think it is?",
          "anchor": "What_do_I_think_it_is_",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        },
        {
          "anchor": "comments",
          "level": 0,
          "title": "3 comments"
        }
      ],
      "headingsCount": 4
    },
    "contents": {
      "markdown": "In a short time I will be starting a project (funded by LTFF) under the supervision of Adam Shimi with the goal of *deconfusing goal-directedness*. I have decided to record the progress on this project on a biweekly basis here on LW, to test whether this helps to keep me accountable for making progress on my project, and to [record the process](https://www.lesswrong.com/posts/Psr9tnQFuEXiuqGcR/how-to-write-quickly-while-maintaining-epistemic-rigor).\n\nBefore the project begins, I want to record my baseline beliefs about goal-directedness. I'm doing this partly to see how by beliefs change through the research process and partly just to get my thoughts in order.\n\nExisting ideas on goal-directedness\n-----------------------------------\n\nAdam Shimi has thought a lot about this topic. His [literature review](https://www.lesswrong.com/s/o58ZMNaovdztbLfvN/p/cfXwr6NC9AqZ9kr8g) with Michele Campolo and Joe Collman assembles ideas and intuitions about goal-directedness, some criteria which any proposed definition of goal-directedness can be compared against, and finally an assembly of proposed definitions from various sources. The definitions cut down my initial workload significantly, especially Vanessa Kosoy's definition of [goal-directed intelligence](https://www.lesswrong.com/posts/dPmmuaz9szk26BkmD/vanessa-kosoy-s-shortform?commentId=Tg7A7rSYQSZPASm9s), since it gives me a formal criterion in terms of numerical values which might be feasible to actually implement and compute in toy examples.\n\nThe aim of my project is not simply to pit these definitions against one another in examples, though: I want to do some serious stress-testing of the concept itself, and maybe eventually bring some serious maths to bear on these definitions. Some questions include:\n\n*   How do (different formalizations of) goal-directedness vary as different aspects of the agent's behaviour and environment change? Expectation: a notion of goal-directedness which is too sensitive to small changes will not be useful, so this may be a criterion for refining definitions of goal-directedness (see next section). Generally speaking, I expect goal-directed behaviour to become increasingly rare and therefore more identifiable as the complexity of the environment and behaviour grow.\n*   What happens in extreme cases? Expectation: extreme simplicity is the easiest to think about, and here goal-directedness collapses. For a very simple system, many explanations adequately account for behaviour: it's roughly as simple to say that a ball falls because it \"wants\" to accelerate towards the ground as to simply describe the falling motion. Generally speaking, I expect goal-directedness to be the extreme end of a spectrum which grows as complexity increases, but it will be interesting to explore how this spectrum varies along the dimensions along which complexity can increase.\n*   Are the intuitions surrounding goal-directedness even consistent? Expectation: there are bound to be some speculations which won't hold up under a quantitative treatment. I also expect there is some tension which results from the assumption that goal-directedness should be a binary property, which I do not think is sensible. Since I think the spectrum of goal-directedness collapses in the extreme of simple behaviour, I don't expect simple examples such as Rohin Shah's [twitching robot](https://www.lesswrong.com/s/4dHMdK5TLN6xcqtyc/p/NxF5G6CJiof6cemTw) to cause any problems.\n*   Is there a polar opposite to goal-directedness; even if not, are there other types of extreme behaviour that are comparable to goal-directedness? Expectation: Purely random behaviour will most likely be at an extreme of the goal-directedness spectrum, but this of course depends on what \"purely random\" is taken to mean...\n\nWhat do I think it is?\n----------------------\n\nMy own intuition about goal-directedness aligns quite closely with the ideas surrounding \"explainability\" in Adam's post above: an agent is goal-directed to the extent that its behaviour can be explained more efficiently/simply in terms of its goals than in terms of any explicit/mechanistic description.\n\nWhile this might sound straightforward, any thought experiment that one tries with this idea raises potential problems. The very notion of an explanation is highly dependent on the available information about the world and the agent. Think how much easier it became to explain the motion of falling objects once the notion of gravity was discovered! This illustrates the possibility that by revealing information about the world, apparent goal-directedness might be revealed or broken. At a conceptual level, it also illustrates how metaphysical problems surrounding objects and their properties *might* change which explanations are the most complicated. For example, if several objects are apparently identical but an agent collects only some of them apparently at random, we can get a much simpler goal-based description if we allow the explanation to include a hidden property which only some of the objects possess. One proposed definition of goal-directedness implicitly does this, with the assigned hidden property an (unknown) numerical utility value on the states of a Markov Decision Process (MDP).\n\nIdeally, I hope to find a quantitative definition of goal-directedness (which can be implemented in at least one standard model, such as for MDPs) which is not a binary. Regarding the issues above, I expect that this definition should be approximately continuous: it will not vary too greatly with small changes in the environment, information, or behaviour. That is, I'm hoping that the notion of goal-directedness is actually a robust one, and that the above are examples where the extra details actually don't tip the balance very much.\n\nEventually I hope to probe some of the questions raised at the start of that literature review: does goal-directedness intrinsically create the same catastrophic problems attributed to insufficiently aligned optimizers?"
    },
    "voteCount": 7,
    "forceInclude": true
  },
  {
    "_id": "KJPRC3cgtxSXpZEQZ",
    "url": null,
    "title": "Goal-directedness: exploring explanations",
    "slug": "goal-directedness-exploring-explanations",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "Goal-Directedness"
      },
      {
        "name": "AI"
      }
    ],
    "tableOfContents": {
      "sections": [
        {
          "title": "What constitutes a good explanation?",
          "anchor": "What_constitutes_a_good_explanation_",
          "level": 2
        },
        {
          "title": "Accurate explanations",
          "anchor": "Accurate_explanations",
          "level": 2
        },
        {
          "title": "Direct mapping and extrinsic measurements of accuracy",
          "anchor": "Direct_mapping_and_extrinsic_measurements_of_accuracy",
          "level": 3
        },
        {
          "title": "Rewards and evaluative measurements of accuracy",
          "anchor": "Rewards_and_evaluative_measurements_of_accuracy",
          "level": 3
        },
        {
          "title": "Relational versions",
          "anchor": "Relational_versions",
          "level": 3
        },
        {
          "title": "Measure-theoretic versions",
          "anchor": "Measure_theoretic_versions",
          "level": 3
        },
        {
          "title": "Powerful Explanations",
          "anchor": "Powerful_Explanations",
          "level": 2
        },
        {
          "title": "Lower dimensional explanations",
          "anchor": "Lower_dimensional_explanations",
          "level": 3
        },
        {
          "title": "Explanations of Restricted Scope",
          "anchor": "Explanations_of_Restricted_Scope",
          "level": 3
        },
        {
          "title": "Simple Explanations",
          "anchor": "Simple_Explanations",
          "level": 2
        },
        {
          "title": "Algorithmic complexity",
          "anchor": "Algorithmic_complexity",
          "level": 3
        },
        {
          "title": "Computational complexities",
          "anchor": "Computational_complexities",
          "level": 3
        },
        {
          "title": "Conclusions",
          "anchor": "Conclusions",
          "level": 1
        },
        {
          "title": "The naïve picture and its flaws",
          "anchor": "The_na_ve_picture_and_its_flaws",
          "level": 3
        },
        {
          "title": "What's missing?",
          "anchor": "What_s_missing_",
          "level": 3
        },
        {
          "title": "Do you think something is missing? Putting my intended applications aside, how else might you judge the quality of an explanation?",
          "anchor": "Do_you_think_something_is_missing__Putting_my_intended_applications_aside__how_else_might_you_judge_the_quality_of_an_explanation_",
          "level": 4
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        },
        {
          "anchor": "comments",
          "level": 0,
          "title": "3 comments"
        }
      ],
      "headingsCount": 18
    },
    "contents": {
      "markdown": "*This is the first post in my Effective-Altruism-funded project aiming to deconfuse goal-directedness. Comments are welcomed. All opinions expressed are my own, and do not reflect the attitudes of any member of the body sponsoring me.*\n\nIn my [preliminary post](https://www.lesswrong.com/posts/7X9KKqgZa7edknKPm/goal-directedness-my-baseline-beliefs), I described my basic intuitions about goal-directedness, and focussed on *explainability.* Concisely, my initial, informal working definition of goal-directedness is that *an agent's behaviour is goal-directed to the extent that it is better explained by the hypothesis that the agent is working towards a goal than by other types of explanation*.\n\nIn this post I'm going to pick away at the most visible idea in this formulation: the concept of an explanation (or at least the aspect of it which is amenable to formalization with minimal effort), and especially the criteria by which an explanation is judged. More precisely, this post is a winding path through some mathematical ideas that could be applied to quantitatively judge explanations; this collection of ideas shall be examined more closely in subsequent posts once some desirata are established. By the end I'll have a naïve first picture of how goal-directedness might be measured in terms of goal-based explanations, while having picked out some tools for measuring explanations which goal-directedness can be contrasted with.\n\nWhat constitutes a good explanation?\n------------------------------------\n\nIn science (and rational discourse more broadly), explanations are judged empirically: they are used to generate predictions, and these are compared with the results of existing or subsequent observations. The transition from explanations to predictions will be covered in the next section. Here we'll break down the criteria for comparison.\n\nThe most obvious way an explanation can fail is if it predicts phenomena which are not observed, or conversely if it fails to predict phenomena which are observed. We can present this criterion as:\n\n*   **Accuracy.** The more accurately the observations match the predictions, the better the explanation is considered to be.\n\nThat is not the end of the story. For instance, more value is attributed to explanations which produce a greater variety of predictions for future observations. An exhaustive account of existing observations with no extra consequences barely qualifies as an explanation, whereas an explanation that appears to reveal the fundamental nature of the object(s) of scientific enquiry are celebrated (this has been true for at least a century or two). To summarise, a second criterion is:\n\n*   **Explanatory power.** \\[Only applicable to partially observed situations\\] If a large range of possible behaviours are compatible with the explanation (conditioned on the existing observations), this reflects badly on the explanation.\n\nAnother criterion, which applies when comparing several explanations of a single phenomenon, is that of simplicity. This judgement is mounted on Occam's razor. But simplicity (or its complement, complexity) is not a fully intrinsic property of an explanation. Even if an explanation can be presented within a mathematical formalism in a way amenable to quantifying its complexity, there may be several ways to do this, and there are various measures of complexity one could use; we shall discuss measures of complexity further in a future post. Note that simplicity is conditioned on accuracy to some extent: if no simple explanation is adequate to explain some behaviour, then a more complex explanation is acceptable.\n\n*   **Simplicity.** The more complex the explanation (relative to explanations of comparable accuracy), the worse it is considered to be.\n\nI should also mention, but will ultimately ignore, qualities of an explanation which make it appealing for psychological reasons rather than rational ones, such as the ephemeral quality of \"elegance\" (often associated with simplicity), comedic elements, and story structure. These factors might be important in judging human-directed explanations in natural language, but the kinds of explanation I'll be considering won't be like that.\n\nIn constructing measures of quality of explanations, I'm going to follow the three criteria explained with bullet points above. It should be clear even with just these three points that *measuring the quality of an explanation is not straightforward*, and in particular that I do not expect there to be a unique right or best way to compare explanations. If you think I'm missing an important criterion, let me know in the comments.\n\nAccurate explanations\n---------------------\n\n### Direct mapping and extrinsic measurements of accuracy\n\nJudging an explanation in terms of its accuracy requires a careful examination of the relationship between explanations and behaviour. Before delving into that, let us first consider the simplest/ideal situation in which each explanation under consideration generates a unique behaviour pattern. For example, we might have an explicit algorithm for some behaviour, or we might imagine a reward function on some state space for which there is a unique optimizing policy, which we shall (for the time being only!) take to be the behaviour which the reward function \"explains\".\n\nNaively speaking, measuring the accuracy of the explanation then amounts to a comparison of the behaviour it generates with the observed behaviour. In simple situations/world models there can be straightforward ways to do this: if the observations of the agent's behaviour consist of mere positions in some space, we can take the (expected) distance between the predicted path of the agent and its actual path. If we use the observed behaviour to approximate the agent's policy in a (finite) Markov Decision Process (MDP) then we can take a normalized inner product of the predicted policy and the observed policy to quantify how well these agree.\n\nI can re-express the last two paragraphs as follows. We could consider a mapping \\\\(f: E \\\\to B\\\\) which transforms each member of a class \\\\(E\\\\) of explanations into a member of a class \\\\(B\\\\) of possible behaviours (or policies); we could then consider a metric or measure of similarity between pairs of elements of \\\\(B\\\\), and rate an explanation \\\\(e \\\\in E\\\\) for observed behaviour \\\\(o \\\\in B\\\\) in terms of the distance from \\\\(f(e)\\\\) to \\\\(o\\\\). Since any such measurement is determined entirely in terms of behaviour, ignoring the specifics of both the explanation and any internal working of the agent, we call this an **extrinsic measurement of accuracy**.\n\nFor the algorithm example, extrinsic measurements of accuracy are more or less all that we have recourse to, at least assuming that we do not have access to the agent's source code or internal workings^[\\[1\\]](#fne37k2jowh56)^.\n\nI should stress that there is plenty to debate regarding the choice of distance function on \\\\(B\\\\); different choices will measure different ways in which an explanation is accurate. Consider two predictions of paths in a (discrete) graph: one of them converges exactly to the observed path except for being always one time step behind, while the other prediction coincides with the observed path half the time on average, but occasionally veers off wildly. Which of these two is considered more accurate depends on what metric is being used to compare the paths!\n\n### Rewards and evaluative measurements of accuracy\n\nDirect comparison at the behaviour level can have drawbacks, and these will hopefully lead us to a more nuanced approach  to comparisons. For example, a direct comparison between the optimal policy for a given reward signal and the observed behaviour can fail to recognise when an agent attempting to optimize for that reward signal has instead gotten stuck at a local optimum far from the global optimum. Wasps are certainly trying to obtain sugar syrup when they fly into a trap constructed by humans, but since that behaviour is suboptimal and distant from the global optimum (since the wasps typically die in the trap and can no longer collect syrup) an extrinsic measurement of accuracy will fail to identify sugar syrup as their actual objective.\n\nOne way around this is to treat reward signals as reward signals! If we compute the reward the observed behaviour would receive from a given reward signal and then normalize appropriately, we can measure how accurately the reward signal explains the behaviour in terms of that reward. In mathematical terms, we suppose that we have collections \\\\(E\\\\) of explanations and \\\\(B\\\\) of behaviours as before, but this time a mapping \\\\(f: E \\\\to (B \\\\to \\[0,1\\])\\\\) which sends each explanation to a corresponding (normalized) evaluation of behaviours. Then the quality of an explanation \\\\(e \\\\in E\\\\) for observed behaviour \\\\(o \\\\in B\\\\) is simply defined to be the evaluation \\\\(f(e)(o)\\\\). We call this an **evaluative measurement of accuracy.**\n\nIn order for evaluative measurements to be at all useful (in particular, for them to have a chance of capturing the intended notion of explanation accuracy), we need to impose some additional conditions on the function \\\\(f\\\\). For example, to ensure that every non-trivial explanation has some relevant content, we could impose the condition that there is always at least one behaviour which is evaluated poorly; we might achieve this by insisting that \\\\(0\\\\) is in the image of \\\\(f(e)\\\\) for every explanation \\\\(e\\\\). Even without developing all of the desirata and corresponding axioms, however, one can see how using a judiciously constructed evaluative measurement of accuracy might produce more desirable results in the wasp example.\n\nA caveat to this analysis: I mentioned [in the comments](https://www.lesswrong.com/posts/7X9KKqgZa7edknKPm/goal-directedness-my-baseline-beliefs?commentId=hHpF3LRgmeodYmDto) of my first post that I want to avoid imposing a strong association between an agent's goal (if it has one) and its competence at achieving that goal^[\\[2\\]](#fna4j2h1oy9qa)^. Computing the reward value is better than a direct comparison with optimal behaviour, but I wonder if we can do any better than that (see the conclusion).\n\n### Relational versions\n\nLet's expand our considerations a little. There was no good justification for our simplifying assumption earlier that each explanation determines a unique behaviour. The most accessible example of how it can fail is that a reward signal can produce several, even infinitely many, globally optimal policies. It still seems reasonable, however, to consider \"optimizing with respect to this reward signal\" as a valid explanation of any of these policies, although we might eventually be inclined to judge that the explanation has less explanatory power in this situation.\n\nTo encode the existence of multiple behaviours corresponding to a given explanation, we simply use a (total^[\\[3\\]](#fn01sfvpwpr4gk)^) relation \\\\(R: E \\\\looparrowright B\\\\) or \\\\(R: E \\\\looparrowright (B \\\\to \\\\mathbb{R})\\\\) in place of a function in each of the measurements of accuracy given above. We can then quantify the accuracy of a given explanation \\\\(e \\\\in E\\\\) for observed behaviour \\\\(b \\\\in B\\\\) in two stages. First, we respectively compute the distance from \\\\(b\\\\) to each behaviour \\\\(b' \\\\in B\\\\) with \\\\(eRb'\\\\) or evaluate \\\\(a(b)\\\\) for each \\\\(a:B \\\\to \\\\mathbb{R}\\\\) with \\\\(eRb\\\\). Then we combine these measurements by taking a minimum, maximum, or (at least in the finite case) a linear combination of them. Again, the choice of weightings in the last case depends on how we have chosen to reward explanatory power, or to punish an absence of such. We can extend this idea to the infinite case in reasonable cases, as we shall shortly see.\n\n### Measure-theoretic versions\n\nGeneralizing good cases of the formulations above, we can suppose that the space \\\\(B\\\\) of behaviours is a *measure space*: it comes equipped with a distinguished collection \\\\(\\\\mathcal{M}(B)\\\\) of subsets of \\\\(B\\\\) called *measurable sets*, and a mapping \\\\(\\\\mu: \\\\mathcal{M}(B) \\\\to \\\\mathbb{R}_{\\\\geq 0} \\\\cup \\\\{\\\\infty\\\\}\\\\) called a *measure* which assigns a 'size' to each measurable set. This data is subject to certain intuitive conditions such as the fact that a disjoint union of (countably many) measurable sets is measurable and its measure is the sum of the parts. This is a setting in which we can perform integration, and we shall use that to quantify accuracy. We shall call \\\\(\\\\mu\\\\) the *reference measure*; in principle, this measure represents an unnormalized zero-knowledge prior distribution over possible behaviours. Determining what this measure 'should' be is non-trivial, just as determining the metric on \\\\(B\\\\) for extrinsic measures of accuracy was non-trivial^[\\[4\\]](#fnqnqj002e7ht)^.\n\nGiven this setting, we suppose that we have mappings from explanations to \\\\(\\\\)*probability density functions* on \\\\(B\\\\), say \\\\(f: E \\\\to (B \\\\to \\\\mathbb{R}_{\\\\geq 0})\\\\), where being a probability density function means being \\\\(\\\\mu\\\\)-integrable and normalized such that \\\\(\\\\int_{x \\\\in B}f(e)(x)d\\\\mu(x) = 1\\\\) for all explanations \\\\(e\\\\). The probability density function corresponding to an explanation amounts to a distribution over the possible behaviours of what one would expect based on the explanation. Similarly, the observed behaviour is a measurable function \\\\(b: B \\\\to \\\\mathbb{R}_{\\\\geq 0}\\\\). With these, we can define the accuracy of an explanation to be the integral \\\\(0 \\\\leq \\\\int_{x \\\\in B}f(e)(x)b(x) d\\\\mu(x) \\\\leq 1\\\\).\n\nThe maths in this account is starting to get a little involved, but it's also approaching what happens in statistical analysis of real-world experiments: the behaviour is reduced to a description in terms of one or more real number values, hypotheses (explanations) are used to predict the values taken by this behaviour in the form of a distribution over real values; observations along with their precisions are used to produce a distribution corresponding to the behaviour, and these are compared by integration (albeit not always exactly via the formula I wrote above). The type of density function most commonly seen in this situation is a Gaussian distribution, and as long as we have enough observations, this is a sensible choice thanks to the Central Limit Theorem.\n\nThis approach must be adapted a little to cope with clashes between discrete and continuous behaviour. Formally speaking, the probability assigned to any given value in a continuous distribution is zero. If we can perform repeated experiments then we can model discrete behaviour as being sampled from a continuous distribution, which we can reconstruct and use as a proxy for computing the accuracy of our explanations. However, when our available observations are limited, we may need to find ways to coarse-grain our explanations in order to arrive at a useful measure of accuracy. I'll leave the discussion of these issues until a later time, but I expect these considerations to rear their heads when the time comes around to compute things in experiments or other concrete situations.\n\nThere are also variations of the measure-theoretic set-up where we do not assume an existing measure \\\\(\\\\mu\\\\) on \\\\(B\\\\); instead, either the explanations, the observations or both provide measures, which can be used to perform the integration. Since my experience in the direction of more exotic measure spaces is limited, I won't speculate about those right now.\n\nPowerful Explanations\n---------------------\n\nThe last two suggestions for measurements of accuracy also incorporated features which penalize explanations with lower explanatory power. In the relational case, there was scope to incorporate a weight inversely proportional to the number of behaviours compatible with an explanation, while the normalization of a probability distribution in the measure-theoretic set-up explicitly forces compatible behaviours to receive lower weight when they are numerous (since they must share the total probability mass of 1 between them). What other measures of the \"power\" of an explanation might we consider, though?\n\n### Lower dimensional explanations\n\nAn explanation can fail to be powerful if it only predicts aspects of behaviour and provides no information about other aspects. For example, if we are observing a charged particle moving in a (vacuum in a) box containing a vertical magnetic field, then a theory of Newtonian gravity will provide reliable information about the vertical motion of the particle but will tell us little about its motion perpendicular to a vertical axis. Our theory of gravity is a weak explanation for the particle's behaviour, and we might like to quantify this weakness in terms of the dimensionality of the predictions it makes.\n\nConsider a (suitably continuous) function \\\\(f: B \\\\to \\\\mathbb{R}^n\\\\) which sends a behaviour to the values of some set of observable properties; up until now we might implicitly have been identifying behaviours with their full sets of observable properties, so consider this a mapping onto the values of some subset of the observable properties. To extract a sensible notion of dimension with such a map, we shall need it to be surjective^[\\[5\\]](#fn7a63ugmsk9w)^, since otherwise we could extend by an inclusion into a space of higher dimension and get a similar kind of map (that is, the dimension \\\\(n\\\\) would be meaningless). In good situations, we can 'push forward' the structure on \\\\(B\\\\) which is used to compute accuracy along this map. We might like to say an explanation \\\\(e \\\\in E\\\\) has \"dimensional strength at least \\\\(n\\\\)\" if for all behaviours \\\\(b \\\\in B\\\\), the accuracy to which \\\\(e\\\\) explains \\\\(b\\\\) according to the pushed-forward structure is at least as good as the accuracy to which \\\\(e\\\\) explains \\\\(b\\\\) according to the structure on \\\\(B\\\\).\n\nThe trouble with dimensional strength is that, while it is bounded above by the dimension of \\\\(B\\\\) (again, assuming that \\\\(f\\\\) is suitably surjective), that's no help when \\\\(B\\\\) might be infinite-dimensional. Returning to our example, the collection of trajectories of a particle in a box is already a huge infinite-dimensional space, even after imposing conditions such as a starting point and velocity. Moreover, our gravity model accurately predicts the vertical component of the particle's position over time (for a classical particle, at least), and the space of vertical components of trajectories is again infinite-dimensional, so there is no upper bound on the dimensional strength of this model. Nonetheless, we can invoke a suitable idea of \"local dimension\" to recover a way to quantify strength in terms of dimension.\n\nThe domain of (algebraic) **dimension theory** provides some tools for formalizing these ideas. However, the definitions involved in dimension theory are rather sensitive to the framework/mathematical model under consideration. Since any concrete examples I examine in the course of this project will be relatively simple, I do not expect this measure of explanatory power to be invoked, but it's worth keeping in mind for more intricate models.\n\n### Explanations of Restricted Scope\n\nSome explanations are only relevant subject to specific conditions. Newtonian gravity, for example, is only \"valid\" in the limit of low energy, which means that the accuracy of its predictions depends on the quantities involved in the calculations being very small (compared to the speed of light and derived quantities). While there is not a precisely defined domain in which this theory holds, seeing as the accuracy just gets worse as the quantities involved grow, it shall be useful for me to write as if such a domain exists; I'll call this the **scope** of the explanation. It should be clear that an explanation with greater scope is more powerful.\n\nMeasuring the deficiency of scope of an explanation is challenging, because it requires us to identify ahead of time *all* of the variables affecting the behaviour being examined^[\\[6\\]](#fn10o61btmy15)^. Going back to the \"charged particle in a box containing a vertical magnetic field\" scenario from earlier, if we didn't know about the magnetic field, we would be surprised that our gravitational explanation failed to accurately predict the observed behaviour. Historically, unknown variables affecting behaviour of physical systems have frequently been identified only thanks to failures of scope, rather than the other way around!\n\nThe considerations above opens the door to an examination of my implicit assumptions. I have assumed in a few places that we can take several observations, but for these to be compatible (give use consistent information about the nature of the behaviour being observed), we need some guarantees that either the parameters affecting the behaviour are approximately constant across observations or that the variations in those parameters are included as part of the explanation. In most cases in experimental science, it must ultimately be assumed that the variables controlled in the experiment are exhaustive. I have little choice but to acknowledge and accept this assumption too, at least at the level of generality at which this post's discussion takes place. On the other hand, I expect that the models I will consider in this project will be small enough that all of their parameters are identifiable and explicit.\n\nSo, suppose that we have identified some list \\\\(P\\\\) of parameters as exhaustive for the purposes of our observation scenario; we could assume that these parameters take real values, so we have a space \\\\(\\\\mathbb{R}^{|P|}\\\\) of parameter values. A naïve approach for expressing explanatory power would be to look at how many of the parameters the explanation incorporates, but there are immediate problems with this. Our parameter list might be excessive, in the sense that some parameters might genuinely be irrelevant to the behaviour being observed. Conversely, it's easy to artificially include a parameter in an explanation in such a way that it makes no concrete difference to the predictions output by the explanation.\n\nInstead, we consider the following blanket method for measuring the failure of scope of an explanation. First, we introduce a further probability measure \\\\(\\\\eta: \\\\mathcal{M}(\\\\mathbb{R}^{|P|}) \\\\to \\[0,1\\]\\\\) (in good cases, this can be described in terms of a probability density function) on the possible parameter values, expressing either the frequency with which we expect parameter values to occur or the frequency with which they are actually observed. Then we can think of an explanation also as being parametrized by this measure, and we can measure the combined accuracy and power of an explanation by integrating the accuracy measurement over the measured parameters. As a formula, if we take the measure-theoretic determination of accuracy too, we have \\\\(Q := \\\\int_{p \\\\in \\\\mathbb{R}^{|P|}} \\\\int_{b \\\\in B} f(e(p))(b) d\\\\mu(b) d\\\\eta(p)\\\\). This isn't a direct measure of deficiency of scope, but rather a measure of how good the explanation is across the range of possible parameters, which thus penalizes an explanation at parameter values outside of the range where the explanation produces accurate predictions.\n\nI am not yet committed to the measure-theoretic calculations, but I must admit that they provide me with a rather flexible geometric mental picture which I have not attempted to present graphically in this post.\n\n*Edit:* I had some conversations this week suggesting that a relational approach might also allow one to examine the two 'directions' of explanatory power described in this section, albeit in a not-immediately-quantified sense. But that might not matter: we only need to be able to compare explanations in terms of some ordering, and a partial ordering might be enough. I'll examine this in more detail in the near future.\n\nSimple Explanations\n-------------------\n\nWhile accuracy and explanatory power appeared to come together in a few places in the sections above, simplicity (or, dually, complexity) of explanations is fundamentally distinct from it. The reason for this is that both accuracy and explanatory power are determined on the basis of a comparison between predicted and observed behaviour, while complexity is measured in terms of the content of an explanation and how difficult it is to extract predictions from it (independently of observed behaviour). This distinction will be reflected in our picture of how explanations can be compared later on.\n\n### Algorithmic complexity\n\nA standard measure of complexity which will be relevant to us is **Kolmogorov** or **algorithmic complexity**. This is calculated by assuming that the explanation can be expressed as the output of an algorithm (which takes some parameters as input); the algorithmic complexity measures the *shortest algorithm* outputting the explanation^[\\[7\\]](#fn0wkrgch2urp)^. We shall consider a variation of this, where the explanations are themselves given as algorithms, so that complexity is simply a measure of the size/length of the algorithm.\n\nWhile conceptually straightforward, there are a rather large number of factors which go into extracting a value here. What language is the algorithm written in? What are the permitted operations? In the case of continuous inputs and outputs, how is this data represented? We need a representation of members of \\\\(B\\\\), say; for a sufficiently constrained model of computation, expressing elements of \\\\(B\\\\) can be non-trivial even if they consist just of real numbers. More generally, several of the descriptions above have explanations expressed as *measures*, which are tricky to encode computationally!\n\nAll of these considerations make attaching a single number to algorithmic complexity... difficult. On the other hand, we don't need to worry too much about the details *a priori*, as long as we compare like with like. If we have two or more different types of explanation, as long as we constrain their expressions (the language, the permissible operations and representation of values) to be sufficiently similar as to ensure that the algorithmic complexities of the two classes are meaningfully comparable, then this should be adequate.\n\nAn apparent concern that remains is whether the resulting comparison of complexity is consistent. There are two versions of this problem. The \"local\" version consists of the observation that it might be that I can formalize explanations \\\\(e_1\\\\) and \\\\(e_2\\\\) in two languages such that in one language the complexity of \\\\(e_1\\\\) is greater, while in the other the complexity of \\\\(e_2\\\\) is greater. The \"global\" version applies the same reasoning to features such as the global minimum of complexity (of some subset of explanations) rather than the complexity of individual explanations. One might be inclined to believe that this problem is only a real concern if the basic expressions of the explanations are insufficiently formal: if I have an explanation in words, the potential inconsistency in algorithmic complexity is a direct consequence of the fact that there may not be a strictly consistent way to translate these explanations into algorithms. In fact, even if explanations are presented as algorithms in the first place, and even if we only allow translations between languages which are *purely substitutional*, in the sense that individual operations and symbols in one language are transformed consistently into strings of operations and symbols in the target language (so that complexity cannot be lost by squashing operations together), the problem still remains. It's a problem of counting, which I'll illustrate.\n\nConsider two abstract algorithms for explanations producing a single whole number (which we're thinking of as a predicted behaviour) with a whole number parameter \\\\(p\\\\). The first is, \\\\(p \\\\mapsto M(p)\\\\), the second is \\\\(p \\\\mapsto N(N(p))\\\\); here \\\\(M\\\\) and \\\\(N\\\\) represent basic operations in my language. If we calculate algorithmic complexity by just counting the operations (and ignore the brackets) then these have complexity 1 and 2 respectively. Clearly, the latter is more complex. But if I have another language containing the operation \\\\(N\\\\) but not the \\\\(M\\\\), and the operation \\\\(M\\\\) can only be represented by three applications of another operation \\\\(m\\\\), then suddenly the first algorithm has become \\\\(p \\\\mapsto m(m(m(p)))\\\\), and increased to complexity 3, higher than 2!^[\\[8\\]](#fni8zrlt863it)^ Note that this is not just an artefact of my tweaking the definition of Kolmogorov complexity: if the two languages have only \\\\(M,N\\\\) and \\\\(m,N\\\\) as their respective basic operations, then these are easily the shortest algorithms for the respective computations.\n\nIt's not such a stretch to expect that how goal-directed an agent seems, or which explanation for its behaviour is simplest or best, depends on the language we're using. After all, it's a lot easier to describe situations where we have words adequately describing all of the elements involved (it's no wonder magic is invoked as an explanation for the functioning of complex devices in places where those devices are not familiar)... \n\n### Computational complexities\n\nOn the other hand, it would philosophically be strange if an eventual conclusion about whether behaviour is goal-directed were strongly dependent on the choice of *computational language* used to express goals, since any specific choice of language can seem arbitrary. As such, we might instead want to restrict ourselves to **properties of algorithmic complexity which are invariant (**to some extent) under changing language, rather than the raw algorithmic complexity with respect to any particular language.\n\nCoarser-grained notions of complexity include the many computational complexity classes in computer science, which are defined in terms of the resources involved in the execution of algorithms. Besides providing coarser grained invariants, these have an extra advantage for the purposes of explanations relating to behaviour. Namely, if we have some known constraints on the architecture of the agent under consideration, we may be able to directly constrain the complexity class of feasible explanations. For example, an agent cannot be implementing an algorithm requiring more memory than it has.\n\nI will need a much deeper understanding of complexity theory than I currently possess to follow through with reasoning about a robust way to measure (aspects of) the complexity of an explanation.\n\nConclusions\n===========\n\n### The naïve picture and its flaws\n\nHere is a naïve picture we can build with the tools discussed in this article. Given a class \\\\(E\\\\) of explanations and some observed behaviour \\\\(b\\\\) from a class \\\\(B\\\\), we should be able to plot the complexity and accuracy/power of each explanation according to our chosen measure of these quantities. They might be plotted on some axes like these:\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/7d39dbbe350da3dbe427758285f55057cdefdd2b20cab13a.png)\n\n(I haven't included any data on these axes, because I don't know what a typical shape will look like yet!) The quality of the *class* of explanations is determined by the lower frontier of the resulting plot, which represents the best accuracy achieved by explanations as complexity increases. If the behaviour is well-explained by pursuit of a goal, for example, then small increases in the complexity of the goal-explanation should result in large increases in accuracy, and the plot will quickly approach the right-hand boundary. On the other hand, non-goal-directed behaviour will require complex explanations to output accurate descriptions of behaviour. Broadly speaking, we should compare two or more such plots in order to determine how well goal-based explanations fare against other types of explanation.\n\n### What's missing?\n\nOne way to separate goal-directedness from optimization is to include a model of the agent's **beliefs** in the explanation. This adds a lot of potential depth to examples compared with the examples considered in the \"accurate explanations\" section above. I will discuss this soon.\n\nIn discussion at a conference this week, *causality* was suggested as a quality of explanations, in the sense that we prefer explanations that identify the factors leading causally to the observed behaviour, rather than just a compressed description of the observed behaviour. Einstein's explanation of Brownian motion vs the description as a random walk came to mind. It's unclear to me at this stage how to incorporate this into the framework I've outlined here, or if it falls into explanatory power somehow, as a feature that I haven't explicitly identified how to measure.\n\n**Do you think something is missing? Putting my intended applications aside, how else might you judge the quality of an explanation?**\n\n*Thanks to Adam Shimi for his feedback during the writing of this post and to participants at the Logic and transdiciplinarity: Mathematics/Computer Science/Philosophy/Linguistics week at CIRM for valuable discussions on this topic.*\n\n1.  ^**[^](#fnrefe37k2jowh56)**^\n    \n    That assumption may not be valid, of course; in existing AI we have explicit access to the source code, although not necessarily in a form that is useful from an explanatory perspective. I don't explore that possibility in this post, but...\n    \n2.  ^**[^](#fnrefa4j2h1oy9qa)**^\n    \n    I think that *some* relation between competence and goal-directedness is inevitable, since an agent with a goal that has no idea how to achieve that goal might act essentially randomly, to the effect that whether or not it has a goal is not easy to detect.\n    \n3.  ^**[^](#fnref01sfvpwpr4gk)**^\n    \n    A relation \\\\(R: X \\\\looparrowright Y\\\\) is called *total* if for each \\\\(x \\\\in X\\\\) there exists some \\\\(y \\\\in Y\\\\) with \\\\(xRy\\\\). This guarantees that each explanation is \"valid\" in the sense of describing some possible behaviour, although it may describe several behaviours.\n    \n4.  ^**[^](#fnrefqnqj002e7ht)**^\n    \n    I observe challenges and choices throughout this post. My intention in doing so is twofold. First, I want to emphasise that anyone employing any of these formulations will need to be explicit about their choices. Second, I want to be deliberate in pointing out where I am postponing choices for later.\n    \n5.  ^**[^](#fnref7a63ugmsk9w)**^\n    \n    For some suitably strong notion of surjectivity. Smooth and almost-everywhere a submersion is definitely enough, but this only makes sense if \\\\(B\\\\) is nice enough, in the sense of admitting a smooth structure. Assuming a topological structure on B, we could employ the concept of [topological submersion](http://nlab-pages.s3.us-east-2.amazonaws.com/nlab/show/topological+submersion) as a corresponding sufficient condition.\n    \n6.  ^**[^](#fnref10o61btmy15)**^\n    \n    There is a sense in which deficiency in scope is complementary or dual to the dimensional deficiency of the previous section: the scope is measured in terms of the number of dependent variables, where the dimension is measured in terms of the number of independent variables.\n    \n7.  ^**[^](#fnref0wkrgch2urp)**^\n    \n    A subtle but important point: the algorithm outputs the *explanation* (the goal, say) not the behaviour predicted by the explanation! \n    \n8.  ^**[^](#fnrefi8zrlt863it)**^\n    \n    Maybe \\\\(M\\\\) and \\\\(N\\\\) represent the operations \"multiply by 8\" and \"multiply by 3\" respectively, while the second language only allows the operation \\\\(m\\\\) of \"multiplication by 2\"."
    },
    "voteCount": 5,
    "forceInclude": true
  },
  {
    "_id": "oZCeun2v3Xd3ncrHt",
    "url": null,
    "title": "Goal-directedness: imperfect reasoning, limited knowledge and inaccurate beliefs",
    "slug": "goal-directedness-imperfect-reasoning-limited-knowledge-and",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "Goal-Directedness"
      },
      {
        "name": "AI"
      }
    ],
    "tableOfContents": {
      "sections": [
        {
          "title": "Questions I would like to solicit feedback on:",
          "anchor": "Questions_I_would_like_to_solicit_feedback_on_",
          "level": 3
        },
        {
          "title": "Constrained rationality[1]",
          "anchor": "Constrained_rationality_1_",
          "level": 1
        },
        {
          "title": "Imperfect Reasoning",
          "anchor": "Imperfect_Reasoning",
          "level": 2
        },
        {
          "title": "Limited Knowledge",
          "anchor": "Limited_Knowledge",
          "level": 2
        },
        {
          "title": "Inaccurate beliefs",
          "anchor": "Inaccurate_beliefs",
          "level": 2
        },
        {
          "title": "Explanations with constraints",
          "anchor": "Explanations_with_constraints",
          "level": 1
        },
        {
          "title": "The world models are part of the explanation",
          "anchor": "The_world_models_are_part_of_the_explanation",
          "level": 2
        },
        {
          "title": "Goals attached to world models",
          "anchor": "Goals_attached_to_world_models",
          "level": 2
        },
        {
          "title": "Different representations of knowledge",
          "anchor": "Different_representations_of_knowledge",
          "level": 2
        },
        {
          "title": "Implementing reasoning constraints",
          "anchor": "Implementing_reasoning_constraints",
          "level": 2
        },
        {
          "title": "Layered explanations",
          "anchor": "Layered_explanations",
          "level": 1
        },
        {
          "title": "A four-part conjecture",
          "anchor": "A_four_part_conjecture",
          "level": 2
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        },
        {
          "anchor": "comments",
          "level": 0,
          "title": "1 comment"
        }
      ],
      "headingsCount": 14
    },
    "contents": {
      "markdown": "*This is the second post in my Effective-Altruism-funded project aiming to deconfuse goal-directedness. Comments are welcomed. All opinions expressed are my own, and do not reflect the attitudes of any member of the body sponsoring me.*\n\nIn my [first post](https://www.lessestwrong.com/posts/KJPRC3cgtxSXpZEQZ/goal-directedness-exploring-explanations) I started thinking about goal-directedness in terms of explanations, and considered some abstract criteria and mathematical tools for judging and comparing explanations. My intention is to consider a class of explanations representing goals that an agent might be pursuing, and to directly compare these with other classes of explanations of the agent's behaviour; goal-directed behaviour will be behaviour which is better explained by pursuit of goals than by other possible explanatory models.\n\nHowever, in the process, I wasn't really able to escape the problem I mentioned in the [comments of my preliminary post](https://www.lessestwrong.com/posts/7X9KKqgZa7edknKPm/goal-directedness-my-baseline-beliefs?commentId=hHpF3LRgmeodYmDto): that goal-directedness seems like it should be mostly independent of competence. Any explanation in the form of a goal, interpreted as a reward function to be optimized, will have low predictive accuracy of an agent's behaviour if that agent happens not to be good at reaching its goal.\n\nI gave the example of a wasp getting caught in a sugar water trap. Explaining this behaviour in terms of the wasp wanting to collect sugar should produce reasonable results, but the explanation that the wasp wants to end up in the trap ranks better (for reasonably general choices of the measures of accuracy, explanatory power and complexity)! Since we have good reason to believe that wasps do not actually desire to be trapped, this problem demands a solution.\n\n**Questions I would like to solicit feedback on:**\n\n*   Are there ingredients obviously missing from my explanation break-down in this post?\n*   Can you come up with any examples of behaviour that you consider goal-directed which cannot be easily explained with the structure presented in this post?\n\n*NB. I have highlighted terms which I expect to reuse in future in the sense intended in this post in bold.*\n\nConstrained rationality^[\\[1\\]](#fn59zufzsumfs)^\n------------------------------------------------\n\nThe most straightforward remedy to separating competence from goal-directedness, which I will be exploring in this post, is to extend our (goal-based) explanations to include constraints on the agent. This will enable us to retain an intuitive connection between goal-directedness and optimization, since we could conceivably generate predicted behaviour from a given \"goal + constraints\" explanation using constrained optimization, while hopefully decoupling global competence from this evaluation. \n\nFirst of all, I'll qualitatively discuss the types of constraint on an agent which I'll be considering, then we'll see how these affect our space of goal explanations in relation to the mathematical frameworks I built up last time. Note that while I present these as cleanly separated types of constraint, we shall see in the subsequent discussion on implementing these constraints that they tend to overlap.\n\n### Imperfect Reasoning\n\nIn simplistic toy models of agency, we assume a complete model of the world in which the agent is acting. For example, in a Markov Decision Process (MDP) on a Directed Graph (DG^[\\[2\\]](#fnshxwe8xqxml)^), we are given a graph whose nodes represent the possible states of the world, and whose arrows/edges out of a given node represent transitions to world states resulting from the possible actions of an agent in that state.\n\nA common implicit assumption in such a set-up is that the agent has perfect knowledge of the *local* structure and state of the world they inhabit and the consequences of their actions: they know which state they occupy in the DG and which states are reachable thanks to their available actions. This is sensible for small, perfect-information environments such as an agent playing chess.\n\nEven in this most basic of scenarios, we already encounter problems with evaluating goal-directedness with goals alone: we typically think of an effective chess-playing agent as having the goal of winning, but when one is often beaten by a more skilled player, that explanation is formally evaluated as poor under the criteria we discussed last time. Indeed, if the difference in skill is substantial enough that the agent loses more often than it wins, the explanation that the agent is trying to lose might be evaluated more highly.\n\nThe crucial aspect that our goal-explanations are missing here is the *local* aspect that I stressed above. Sure, an agent knows the immediate consequences of its moves, but calculating all of the possibilities even a few moves ahead can create an unmanageable combinatorial explosion. An agent is far from knowing the global structure of the DG it's exploring; it can only perform **imperfect reasoning** about this global structure.\n\nLet's consider two chess-playing agents, *Deep Blue* and *Shallow Green*^[\\[3\\]](#fn59iw54zov34)^, where the former of the two is much more skilled at chess than the latter. The difference in skill between *Deep Blue* and *Shallow Green* has (at least) two components. The first is a difference in ability to effectively search ahead in the tree of moves. The second is a difference in quality of the agents' implicit encoding of the overall structure of the DG being explored, which is reflected in the criteria they use to evaluate the positions they come across in the search: an ideal chess agent would evaluate moves based on how much they improved their chances of winning across all ways the game might play out, whereas a simpler agent might only be able to compare the number of their pieces to the number of opposing pieces.\n\nEvaluating the skill of an agent could be challenging, but *that's not what we're trying to do here*. Instead, we just want to be able to capture the constraints on the agent in our explanations. Raw search capacity is an easy first step in this discrete setting, since the answer to \"at most how many position calculations can the agent perform per move?\" is a single number. The search algorithm being implemented and the evaluation criteria can be expressed in various ways, but there is no obstacle to bolting these onto our goal-based explanations; we shall see in the implementation section below that we actually don't want the search algorithm to be an independent component which is \"bolted on\", but rather a more goal-sensitive extension of the explanation. Nonetheless, this is sufficient evidence to expect that we should be able to build a class of explanations that includes the means of expressing a wide variety of constraints on an agent besides its goal.\n\n### Limited Knowledge\n\nA major implicit assumption in the previous subsection was that the agent's knowledge aligns exactly with the actual state of the world, at least locally. There are many situations in which that won't be the case, however, where the agent has **limited knowledge** of the state of the world on which to base its decision-making.\n\nThe closest example to the set-up of the previous section is that of a partially-observable environment. Consider a game of [liar's dice](https://en.wikipedia.org/wiki/Liar%27s_dice), where each player rolls their own dice but keeps the result hidden from the other players, and then take it in turns to make statements about the overall state of the collection of die values (the particular rules of the game don't matter too much here). Even if an agent playing this type game knows all of the possible present states of the game, they do not have sufficient knowledge to isolate which precise state they find themselves in.\n\nWe can also imagine situations where an agent's observations of the world are limited by something other than structural limitations on their information. The agent might have limited attention; by this, I mean an agent which is able *in principle* to observe any given detail of the world perfectly, but which only has space in its memory, or enough observation time, to examine a limited number of details.\n\nAlternatively, an agent might have imperfect sensors, so that even if they can examine every aspect of their environment, the quality of information they have about the world state is limited. Analogously, if an agent is getting their information about the world second hand, such as from another agent, then their knowledge will be limited by the reliability of that information. The 'sensors' example might make the limitations seem easy to estimate, since we can compute the resolution of a camera from its physical properties, but quantifying the reliability of second-hand information seems much less straightforward, and how an agent does this will depend on their internal beliefs regarding other agents (both here and below, I avoid digging any deeper into the problem of [embedded agents](https://intelligence.org/2018/10/29/embedded-agents/) for the time being). However, the aim here is not necessarily to ensure that we can practically find the best explanation, desirable as that might be, but rather to ensure that the space of explanations we consider is broad enough to cover everything that we could reasonably call goal-directed.\n\nAll of the limitations in this section are obstacles which humans experience in almost every situation, and accounting for them qualitatively could enable us to explain some of our \"irrational\" behaviour.\n\n### Inaccurate beliefs\n\nThe discussion so far has expanded our view to varieties of agent which carry approximate, partial, local knowledge of the 'real' state of the world. A third layer of complication arises when we allow for the possibility that the agent's internal understanding of the structure of the world differs from the 'real' one we have been assuming in our analysis. Such an agent carries **inaccurate beliefs** about the structure of the world.\n\nThe most accessible version of the phenomenon I'm talking about concerns problems of identity. On one hand, an agent may distinguish two apparently identical world states, and on the other an agent may fail to distinguish two distinct states; either direction can affect the apparent rationality of the agent. I'll illustrate the first of these two problems.\n\nConsider again one of our chess-playing agents, *Shallow Green*. To compensate for its mediocre planning skills, this cunning machine has built a small database of end-game strategies which it stores in its hard-coded memory, so that when it reaches a board state in the database, it can win from there. In one such strategy, it has a knight and a few other pieces on the board (no details beyond that will be important here). However, *Shallow Green* happens to track the positions of all of its pieces throughout its games, and to *Shallow Green*, each of the two knights has a unique identity - they are not interchangeable. In particular, if *Shallow Green* happens to reach the aforementioned board position from its database with the wrong knight on the board, it may be oblivious to the fact that it already knows how to win and (depending on how imperfect its reasoning is) could even make an error and lose from this position.\n\nModelling *Shallow Green*'s chess games with a DG world model, the above corresponds to splitting or combining of states in the DG. Other inaccurate beliefs we can express in a DG model include an agent's beliefs about which actions/transitions are possible in given states^[\\[4\\]](#fn3q4ras5mqfp)^; splitting or identification of edges^[\\[5\\]](#fnx4wddqi0ken)^, and mistakes about the target of an edge (result of an action)^[\\[6\\]](#fnka72hj6qtim)^.\n\nIn limited knowledge environments, inaccurate beliefs can take on many more forms. An agent can have inaccurate beliefs about the dynamics of the aspects of the world they lack information about. If the agent applies principles of reasoning based on those dynamics which are not sound for their environment, they can deduce things about the environment which are false. In the game of liar's dice described earlier, for example, an agent might infer that another player has at least one 6 showing on their dice based on that player announcing an estimate of the total number of 6s rolled; that assessment may or may not be true, depending on the honesty of that player! Even if their reasoning is sound, an agent may have to make a decision or form a belief based on inaccurate or partial information.\n\nThe description of beliefs as \"inaccurate\" of course rests on the assumption that there is some unique 'real' world against which we are assessing the agent's beliefs, and moreover that our own model of the world coincides with the relevant 'real' one. For toy models, this is fine, since *the world is no more and no less than what we say it is*. In a practically relevant situation, however, our model of the situation could be flawed or imperfectly simulated when we present it to our agents, and this fact [could be exploited](https://www.lesswrong.com/posts/7b2RJJQ76hjZwarnj/specification-gaming-the-flip-side-of-ai-ingenuity) by those agents in unexpected ways.\n\nMy personal solution to this is to invoke relativism: there are numerous possible models of the world (or agent-environment system) which we can use to ground our assessments of an agents actions, and the outcome of our assessments will depend on the choice we make^[\\[7\\]](#fnonsiaxnca39)^. Even if you think there should be a single real/accurate world model out there, it is prudent to [consider the possibility of shortcomings in our models](https://www.alignmentforum.org/posts/k54rgSg7GcjtXnMHX/model-splintering-moving-from-one-imperfect-model-to-another-1) anyway, and to examine how shifting between these affects our assessments of agent behaviour.\n\nExplanations with constraints\n-----------------------------\n\nHaving established some directions in which our agent may be constrained, we now need to establish how these can be built into our explanations (with or without goals). For this, we'll work from the bottom up through the last section.\n\n### The world models are part of the explanation\n\nFirst of all, we have to acknowledge that our chosen world model will determine some aspects of the evaluation of our agent, and so we formally include it in our explanations. For a fixed world model, the resulting overhead of explanatory complexity will be the same across all of the explanations we consider, so there is no loss in making this model implicit when we get down to calculating, but acknowledging that the model is a parameter will allow us to investigate how shifting between world models changes our assessments (as it appears to in the footnote examples^[\\[7\\]](#fnonsiaxnca39)^) later. Note also that we were already using the components of the world model (the states of the world, the concepts available in this model) in our descriptions of behaviours and explanations, so we're just making explicit what was already present here.\n\nI'll call the world model being incorporated into the explanation the **base model**, and the internal model which an explanation assigns to the agent will be called the **explainer model**. As a first approximation, incorporating the agent's belief inaccuracies into the explanation amounts to describing a transformation which produces the explainer model from the base model. It is this transformation whose complexity will contribute to the overall complexity of the explanation.\n\nLet's take this first idea and run with it a little bit. When we try to formalize transformations of world models, we immediately encounter a problem: which world models do we consider, and which transformations between models are allowed? This amounts to asking 'which [category](https://www.lesswrong.com/tag/category-theory) of models are we in?' Stuart Armstrong and Scott Garrabrant have proposed categories of [Generalized Models](https://www.alignmentforum.org/posts/nQxqSsHfexivsd6vB/generalised-models-as-a-category) and [Cartesian Frames](https://www.alignmentforum.org/posts/ewkYgtZapQRtDPT2F/additive-operations-on-cartesian-frames) respectively, as categories of world models, and Armstrong has [compared these two formalisms](https://www.alignmentforum.org/posts/wiQeYuQPwSypXXFar/cartesian-frames-as-generalised-models). Just as in my last post, I'm going to remain agnostic on the choice of category of models for the time being, but I will make some observations about good features this category should have if we're going to get anywhere.\n\n*   The objects of the category need to include both the base model and all of the possible explainer models we might want to consider. On the other hand, there shouldn't be too many models, for reasons that shall be discussed below. This shouldn't be a problem, since I expect we will never need to include models of arbitrary infinite cardinality, and that's enough of a restriction for my purposes.\n*   The morphisms/arrows in the category should be relational in nature, like those of Armstrong's generalized models, in order to be able to model all of the phenomena we discussed under \"Inaccurate beliefs,\" above. (Functions can model identification of objects/states/actions, but not splitting!)\n*   We should be able to assign *complexities*, or at least complexity classes,  to arrows in a systematic way. There is [some literature](https://www.cambridge.org/core/services/aop-cambridge-core/content/view/FB2CAC644C697FEF1092C285D784CAB4/S2050509420000262a.pdf/div-class-title-categorical-complexity-div.pdf) on assigning complexity values to arrows in categories, but none specific to this application for the time being, so that will be a further aspect of complexity theory to draw out in a later stage of this project.\n*   Objects of the category should have enough internal structure to enable us to talk about knowledge and reasoning of agents in them (for the subsequent subsections). I'll refer to internal states when alluding to this structure, although I'll lightly stress that there in no way has to be discrete internal states in the sense of the sets forming the components of objects in Armstrong's or Garrabrant's categories.\n\nLet's suppose we've chosen our category of models; I'll call it \\\\(\\\\mathcal{M}\\\\). Our base model corresponds to a choice of object \\\\(M \\\\in \\\\mathcal{M}\\\\), and the explainer model is an object \\\\(M' \\\\in \\\\mathcal{M}\\\\) equipped with a morphism from \\\\(M\\\\); equivalently, the explainer model is an object of the [coslice category](https://ncatlab.org/nlab/show/under+category) \\\\(M \\\\to M' \\\\in M/\\\\mathcal{M}\\\\).\n\nThis framework is crisp enough that we can immediately push against it to get a bigger picture. Consider, for example, the fact that attaching a single explainer model to an explanation is rather restrictive compared to how we were able to incorporate uncertainty (or imprecision) of prediction into our explanations last time. As long as our category of models is small enough, we could replace the proposed single object of the coslice category \\\\(M/\\\\mathcal{M}\\\\) with a probability distribution over objects. Or, taking some meta-considerations into account, we might even want to allow for uncertainty of our base model. These extensions will complicate the complexity story, but they also are broad enough to cover sensible Bayesian/empirical approaches to agent behaviour.\n\nA yet further level of complication is that the explainer model may need to be updated as the agent takes actions and receives new information^[\\[8\\]](#fn7kv4mcgjb9)^. This update process is subject to the same kinds of constraints on reasoning and knowledge that were discussed above.\n\nComing around to the partial information settings, the formalisms discussed thus far quickly start to look inadequate, since the problems we discussed were a result of interactions between agents, and incorporating functional models of agents into world models is hard! I may consider this in more detail in a future post. As for beliefs founded on inaccurate or uncertain information, we only need the explainer models to account for the features of the world which the agent could conceivably think possible; the actual likelihoods the agent assigns to the different features will be incorporated into the considerations on knowledge, below.\n\n### Goals attached to world models\n\nAt this point, the \"goal\" in goal-directedness is ready to re-emerge, at least for explanations involving goals. Just as world models can vary, so too can expressions of goals. In a DG model, we could represent goals as mere subsets of the possible states, as a reward function over terminal states (wins, losses and draws in chess, say), as a reward function which is cumulative over paths in the graph (possibly with a discount rate), and so on. As usual, we shall postpone the specific choice until later, so that the details of the choice don't interrupt the general reasoning.\n\nOne subtlety which our set-up thus far allows us to account for is that we can distinguish between a goal defined on the base model (an **external reward signal**) and a goal defined on the explainer model (an **internal reward signal**). By employing the latter, we can explain a host of \"outwardly irrational\" phenomena in terms of mistaken identity assumptions. For example, an animal that cannot distinguish poisonous berries from sweet berries will sensibly avoid both (as long as the poisonous berries are sufficiently prevalent), even if the difference is clear to an outside observer and the sweet berries are the best nutritional option available.\n\nFor consistency with the other sections, we shall take the following approach. When we consider goal-directed behaviour, it is conceptually simplest to express those goals in terms of the world model we are ourselves using; as such, external reward signals shall be the first approximation we consider. We can transform an external signal into an internal one by pushing its values along the transformation from the base model to the explainer model, to produce an internal reward signal. Then we can transform the result a little to account for details of internal reward signals which cannot be represented in the external signal (this might be needed because the explainer model distinguishes states which are identical in the base model, for example). We see that the first step in this process is already enough to explain the example above: if the poisonous berries are assigned a reward of -100 and the nutritious berries a reward of +2 in the external reward signal, then assuming there is more than one poison-berry plant out of every fifty berry plants on average, the expected reward of the indistinguishable berries in the internal reward signal will be negative, and so much less desirable than less nutritious food sources.\n\n### Different representations of knowledge\n\nThe most basic representation of partial knowledge would be to consider a \"submodel\" of the explainer model \\\\(M'\\\\) representing the collection of states which the agent thinks it could be in. In light of the direction of developments in my [first post](https://www.lesswrong.com/posts/KJPRC3cgtxSXpZEQZ/goal-directedness-exploring-explanations), it should come as no surprise that I will immediately discard this in favour of a distribution over internal states of the explainer model; this is a generalization (at least for finite situations) because I can identify a \"submodel\" with the special case of a uniform distribution over the states it contains.\n\nThere are some subtleties here depending on which knowledge limitation model we are formalizing. In crisp partial-information situations such as liar's dice (or at least, the initial game state in a game of liar's dice!), we will be exactly encoding the distribution determined by the combinatorics of the set-up. For other situations, the best we can do *a priori* is encode a maximum entropy distribution based on the known constraints on the agent's knowledge, such as sensor quality parameters.\n\nHere we hit on an interaction between knowledge, reasoning and belief not covered in earlier sections: **bias**. It may be unreasonable to assume that an agent will, in fact, employ the maximum entropy distribution (conditioned on their knowledge), for two reasons. One is that this distribution is computationally expensive to compute explicitly, so they might only carry an approximation of the true distribution. Another is that they may have a biased prior distribution. For example, a human player of liar's dice will typically both be bad at computing probabilities of outcomes involving a large number of dice *and* may erroneously believe that rare events are less likely after they have recently occurred (a common mistake!). Ultimately, in analogy with transforming from base models to explainer models, I think the most comprehensive way to deal with these limitations is to include both the unbiased distribution (assuming one exists) and the deviation from that unbiased model (or instructions for computing that deviation) in the explanation.\n\n### Implementing reasoning constraints\n\nIn the last paragraph, we saw how constraints on an agent's reasoning impact the quality of their knowledge of the world. This effect corresponds to the limitations such as those on \"raw search capacity\" discussed earlier. These provide concrete predictions about how well the agent will be able to approximate the explainer model compared with optimal rationality, conditioned on the information at their disposal. From these, we can compute unbiased predictions of the agent's approximation of the goal signal.\n\nTo improve an explanation further, we need to take into account the other type of imperfection in reasoning, regarding an agent's inability to accurately compute the above unbiased approximation. In other words, we must take into account how the agent is locally misrepresenting the global structure of the goal on the explainer model. This could be described in terms of a perturbation of the unbiased prediction or instead as a model of the signal (such as a heuristic or proxy measures) which the agent is actually deploying. The difference between these possibilities might seem small at first glance, but if we choose the latter option, we encounter a curious obstacle, which may well represent a crux in my understanding of goal-directedness: there is no formal reason for a heuristic description of the agent's behaviour to be strongly coupled or correlated with the goal imposed earlier. In other words, this final description of 'what the agent is actually doing' can completely overwrite the goal which was supposed to be the key feature of the explanation! Stuart Armstrong [illustrates](https://www.alignmentforum.org/posts/ANupXf8XfZo2EJxGv/humans-can-be-assigned-any-values-whatsoever) the perfect-knowledge version of this problem: if we boil an explanation down to a reward function and a planning algorithm which turns reward functions into policies/behaviour then, without constraints, the planning algorithm could be a perfect contrarian which deliberately selects the actions with the worst returns against the reward function (but which happens to be acting on a reward function where that worst return produces the observed behaviour^[\\[9\\]](#fn7pwzmo1qt9o)^), or indeed a constant function that ignores the reward function. Moreover, taking a uniform reward function in the last case, all of these options have the same total Kolmogorov complexity up to adding a constant number of operations; since I haven't yet established precisely how I'll be measuring complexity of explanations, this is a good stand-in for the time being.\n\nI should stress that the problem emerging here is not that we have multiple different equally good explanations. It is not unusual for explanations to be underdetermined by the behaviour they are intended to account for, and in this post I have been adding further parameters to explanations, making them even more underdetermined. Nor is the problem that amongst these equally good and equally complex explanations, we can find some which are a goal-directed and others which are not, although this does mean I'll have to rethink my original [naïve picture](https://www.lessestwrong.com/posts/KJPRC3cgtxSXpZEQZ/goal-directedness-exploring-explanations#The_na_ve_picture_and_its_flaws) of how goal-directedness will be judged. As a consolation prize, these examples demonstrate that considering individual explanations in isolation could not be sufficient for quantifying goal-directedness, as I suspected.\n\nRather, the issue is that between the extreme options that Armstrong describes, where the respective levels of goal-directedness is reasonably clear (the optimizing agent and the perfect contrarian have policies completely dependent on the goal, the constant policy explanation is completely non-goal-directed, since it ignores the reward function), assessing how goal-based the explanations lying between these extremes are is tricky, and I do not expect the resulting judgements to be binary. This strengthens [my initial belief](https://www.lessestwrong.com/posts/7X9KKqgZa7edknKPm/goal-directedness-my-baseline-beliefs#What_do_I_think_it_is_) that there is a very blurry line between goal-directed behaviour and non-goal-directed behaviour.\n\nI will eventually need to decide how to integrate over the equally good explanations in order to reach a final numerical judgement of goal-directedness.\n\nLayered explanations\n--------------------\n\nThe structure of explanations emerging from the above discussion is a layered one. At each stage in the construction of our explanations, we find a 'default' assumption corresponding to perfect rationality and alternative 'perturbative' assumptions which take into account deviations from perfect rationality. Here's a diagram:\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/fc16deafc5a3c163e65acf1fd3cb612db058b74066adc82d.png)\n\n**Figure 1:** A diagram summarising the explanation breakdown we have developed. In each block, the first line is an \"unbiased\" default choice based on constrained optimal behaviour, and the second line is a perturbation incorporating further constraints on the agent.\n\nConsider again the example of the wasp. We can explain the behaviour as follows. For simplicity, we only consider world models consisting of spatial configurations of some relevant components, such as a wasp nest, flowers, other food sources and possibly some wasp traps. The transitions in this set-up might correspond to wasps flying or picking up substances. In the base model we recognize that a trap is a trap (there is little to no probability of the wasp escaping once it enters), whereas the explainer model over-estimates the possibility of the wasp being able to take actions which lead it out of the trap - this is the crucial new feature. We equip this explainer model with a reward signal for bringing food back to the nest (possibly having consumed it), with a strength proportional to how nutritious the food sources are.\n\nEven without refining this explanation to account for a wasp's limited knowledge (which will affect how quickly the wasp is able to find the food sources) and imperfect reasoning (accounting for the fact that the wasp may not easily be able to estimate their ability to carry or retrieve a given piece of food), this goal-based explanation already does a much better job of accounting for a wasp getting caught in the trap.\n\nReturning to our assessment criteria from last time, the accuracy and power of the explanations can be assessed as before, by generating predictions and comparing them with the observed behaviour. Meanwhile, the layered structure presented above gives us a systematic way to break down the complexity of explanations; this breakdown may eventually be leveraged to assess goal-directedness by the 'asymptotic' method alluded to at the end of my previous post.\n\nThere remains a question of where to draw the line in measuring the complexity. If the base model is fixed across all of the explanations being considered, then we may as well ignore its complexity; conversely, we *should* include the complexity of the transformation from the base model to the explainer model, since we cannot help but take the base model as the 'default' view of the world and the agent's world model as a perturbation of that default view. Whether or not there is a default (e.g. uniform/constant) goal on the world, the goal's complexity should be included in the count, which only adds a constant value for the default case.\n\nThe description complexity of the constraints on the agent's knowledge and reasoning seems like a reasonable choice to include. Note that these will usually differ significantly from the complexity of computing the respective unbiased and perturbed distributions determined by those constraints; the reason I choose to focus on the complexity of the constraints themselves is that I want the unbiased distributions to be considered the default, and so carry no intrinsic complexity contribution. Just as with the transformation of world models, it's the complexity of the *perturbation* that I want to keep track of. Formally stated, I will be thinking of complexity classes of each stage of the explanation relative to an oracle that lets me compute the previous parts for free.\n\n### A four-part conjecture\n\nYou may be familiar with the fact that [any behaviour is optimal](https://www.lesswrong.com/s/4dHMdK5TLN6xcqtyc/p/NxF5G6CJiof6cemTw) with respect to some utility/reward function. Of course, this is already true for the constant reward function for which all behaviour is optimal; the more pertinent observation is that we can instead choose a reward function which is positive on the exact trajectory taken by an agent and zero or negative elsewhere, or when there is some randomness involved we can reverse-engineer a reward function which makes only the given behaviour optimal. The result is an explanation with ostensibly high explanatory power and accuracy, scoring highly on these criteria in the sense discussed in my previous post (at least up to the point of empirically testing the explanation against future observations). The crucial point, however, is that this explanation will be as complex as the observed behaviour itself (so it will typically score poorly on our \"simplicity\" criterion).\n\nIn discussion with Adam Shimi about a draft of the present post, he suggested that instead of making the reward function (aka the goal) arbitrarily complex to account for the behaviour observed, we could equally well fix a simple goal on a sufficiently contrived explainer model to achieve the same effect. Again, there are simplistic ways to do this, such as squashing all of the states into a single state; the real claim here is that we can do this in a way which retains explanatory power. Examining the diagram above, I would extend this conjecture to further hypothesise:\n\n**Conjecture:** For each of the components in Figure 1, we can achieve (an arbitrarily good approximation to) perfect accuracy and explanatory power^[\\[10\\]](#fnjhuol321qj)^ by sufficiently increasing the complexity of that component while keeping the complexity of the other components fixed at their minimal values. A little more precisely:\n\n1.  Given a large enough category of world models, we can achieve perfect accuracy and explanatory power with a simple goal (and no constraints on knowledge or reasoning) at the cost of making the transformation from the base model to the explainer model arbitrarily complex.\n2.  For any fixed base model, we can achieve perfect accuracy and explanatory power (while taking the explainer model identical to the base model, and imposing no constraints on knowledge or reasoning) at the cost of making the goal arbitrarily complex; this is what Rohin Shah [proves](https://www.lesswrong.com/s/4dHMdK5TLN6xcqtyc/p/NxF5G6CJiof6cemTw?_ga=2.75040978.1185963496.1647696652-684697841.1647694967) in a general setting.\n3.  For any fixed base model and simple goal, we can achieve perfect accuracy and explanatory power (while taking the explainer model identical to the base model, and imposing no constraints on reasoning) at the cost of making the constraints on the agent's knowledge about the world arbitrarily complex.\n4.  For any fixed base model and simple goal, we can achieve perfect accuracy and explanatory power (while taking the explainer model identical to the base model, and imposing no constraints on knowledge) at the cost of making the reasoning constraints on the agent arbitrarily complex. Armstrong's [argument](https://www.alignmentforum.org/posts/ANupXf8XfZo2EJxGv/humans-can-be-assigned-any-values-whatsoever?_ga=2.118227785.1185963496.1647696652-684697841.1647694967), which we saw earlier, more or less demonstrates this already.\n\nBased on Armstrong's argument, I expect that the complexities of the explanations required for the respective components will all be very similar. This conjecture is not stated crisply enough for the not-yet-proved parts to be amenable to a proof as stated, but I expect anyone sufficiently motivated could extract and prove (or, more interestingly, disprove!) a precise version of it.\n\nTo the extent that it is true, this conjecture further justifies the approach I am exploring of comparing explanations whose complexity is carefully controlled, since we can infer that explanations in any sufficiently comprehensive class will achieve the same results as far as explanatory power and accuracy go. Note that the individual parts of the conjecture are genuinely different mathematically, since they extend the space of explanations being considered in different directions, although this difference is least transparent between knowledge and reasoning.\n\n*Thanks to Adam Shimi for his feedback during the writing of this post.*\n\n1.  ^**[^](#fnref59zufzsumfs)**^\n    \n    When selecting the title of this section, I didn't make the connection with [bounded rationality](https://www.lesswrong.com/tag/bounded-rationality). There is some overlap; I expect that some ideas here could model the problems presented in some of the bounded rationality posts, but I haven't thought deeply about this.\n    \n2.  ^**[^](#fnrefshxwe8xqxml)**^\n    \n    [Directed acyclic graphs](https://www.lessestwrong.com/posts/Pd8Fb37BAYxp68Zh5/fun-with-dags) (DAGs) are pretty common, but there's no need for us to constrain ourselves to acyclic graphs here.\n    \n3.  ^**[^](#fnref59iw54zov34)**^\n    \n    If you're not familiar with chess, some of the later chess-based examples might be a bit obscure, so I'll stick them in the footnotes.\n    \n4.  ^**[^](#fnref3q4ras5mqfp)**^\n    \n     If *Shallow Green* doesn't know about castling, then there are transitions missing from its internal model of the chess DG.\n    \n5.  ^**[^](#fnrefx4wddqi0ken)**^\n    \n    *Shallow Green* sometimes likes to turn their kingside knight around to face the other way when it moves; from the point of view of the other player, moving with or without rotating the knight count as the same move.\n    \n6.  ^**[^](#fnrefka72hj6qtim)**^\n    \n    An agent like *Shallow Green* might be aware of the *en passant* rule, but be confused about where their pawn ends up after performing it, and hence fail to use it at a strategically advantageous moment.\n    \n7.  ^**[^](#fnrefonsiaxnca39)**^\n    \n    After observing *Shallow Green* in bemusement across many matches, you might come to realise that they are actually expertly playing a slight variation of chess in which the distinction between the knights and the direction they are facing matters, and is negatively affected by castling. Of course, this requires that *Shallow Green* is a very charitable player who doesn't (or cannot) object to their opponents making illegal moves, but with this shift of world model, we suddenly have access to a simple, compellingly goal-directed, description of *Shallow Green*'s behaviour.\n    \n8.  ^**[^](#fnref7kv4mcgjb9)**^\n    \n    The types of base model we have been considering have included all possible world states by default, so are static and deterministic, but it's plausible that in more realistic/less crisply defined situations we may equally need to incorporate dynamics into the base model.\n    \n9.  ^**[^](#fnref7pwzmo1qt9o)**^\n    \n    Mathematically, this is just the observation that the only difference between maximizing a reward function and minimizing a loss function is a couple of minus signs, but once we've decided that our goals take the form of reward functions, the difference between a reward function and its negation becomes a significant one!\n    \n10.  ^**[^](#fnrefjhuol321qj)**^\n    \n    Here perfect accuracy and explanatory power are relative to the observations which are possible. In other words, I am making assertions about the explanations achieving the best possible accuracy and power, conditioned on fixing at least the base model (otherwise there could be an explanation which has better explanatory power because it explains a feature of the behaviour that the base model doesn't account for)."
    },
    "voteCount": 2,
    "forceInclude": true
  },
  {
    "_id": "jgYGZD2zRK6nncJd5",
    "url": null,
    "title": "Goal-directedness: tackling complexity",
    "slug": "goal-directedness-tackling-complexity",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "Goal-Directedness"
      },
      {
        "name": "AI"
      },
      {
        "name": "Kolmogorov Complexity"
      }
    ],
    "tableOfContents": {
      "sections": [
        {
          "title": "Setting the scene",
          "anchor": "Setting_the_scene",
          "level": 2
        },
        {
          "title": "Complexity, from the bottom up",
          "anchor": "Complexity__from_the_bottom_up",
          "level": 1
        },
        {
          "title": "Motivating examples",
          "anchor": "Motivating_examples",
          "level": 2
        },
        {
          "title": "So what is complexity?",
          "anchor": "So_what_is_complexity_",
          "level": 2
        },
        {
          "title": "Contexts for complexity",
          "anchor": "Contexts_for_complexity",
          "level": 1
        },
        {
          "title": "Complexity in topology",
          "anchor": "Complexity_in_topology",
          "level": 2
        },
        {
          "title": "Complexity in algebra",
          "anchor": "Complexity_in_algebra",
          "level": 2
        },
        {
          "title": "Between algebra and spaces",
          "anchor": "Between_algebra_and_spaces",
          "level": 2
        },
        {
          "title": "Complexity within a structure",
          "anchor": "Complexity_within_a_structure",
          "level": 1
        },
        {
          "title": "Piecewise maps",
          "anchor": "Piecewise_maps",
          "level": 2
        },
        {
          "title": "Elements of algebras",
          "anchor": "Elements_of_algebras",
          "level": 2
        },
        {
          "title": "Fragility of complexity measures",
          "anchor": "Fragility_of_complexity_measures",
          "level": 1
        },
        {
          "title": "Minimal presentations",
          "anchor": "Minimal_presentations",
          "level": 2
        },
        {
          "title": "Minimal decompositions",
          "anchor": "Minimal_decompositions",
          "level": 2
        },
        {
          "title": "Invariance at the cost of individuality",
          "anchor": "Invariance_at_the_cost_of_individuality",
          "level": 2
        },
        {
          "title": "Probabilistic contexts",
          "anchor": "Probabilistic_contexts",
          "level": 2
        },
        {
          "title": "Values for complexity measures",
          "anchor": "Values_for_complexity_measures",
          "level": 1
        },
        {
          "title": "Number vs Order",
          "anchor": "Number_vs_Order",
          "level": 2
        },
        {
          "title": "Symbolic complexity values",
          "anchor": "Symbolic_complexity_values",
          "level": 2
        },
        {
          "title": "Combinatorial complexity measures and projective parameter spaces",
          "anchor": "Combinatorial_complexity_measures_and_projective_parameter_spaces",
          "level": 2
        },
        {
          "title": "Computational Complexity",
          "anchor": "Computational_Complexity",
          "level": 1
        },
        {
          "title": "Classical computational complexity",
          "anchor": "Classical_computational_complexity",
          "level": 2
        },
        {
          "title": "Circuit complexity",
          "anchor": "Circuit_complexity",
          "level": 2
        },
        {
          "title": "Descriptive complexity",
          "anchor": "Descriptive_complexity",
          "level": 2
        },
        {
          "title": "Complexity measures for functions",
          "anchor": "Complexity_measures_for_functions",
          "level": 1
        },
        {
          "title": "Approximation complexity",
          "anchor": "Approximation_complexity",
          "level": 2
        },
        {
          "title": "Simple functions",
          "anchor": "Simple_functions",
          "level": 2
        },
        {
          "title": "Power series, Fourier series",
          "anchor": "Power_series__Fourier_series",
          "level": 2
        },
        {
          "title": "Functions on spaces",
          "anchor": "Functions_on_spaces",
          "level": 2
        },
        {
          "title": "Conclusions",
          "anchor": "Conclusions",
          "level": 1
        },
        {
          "title": "Coming soon...",
          "anchor": "Coming_soon___",
          "level": 2
        },
        {
          "title": "Further References",
          "anchor": "Further_References",
          "level": 2
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        },
        {
          "anchor": "comments",
          "level": 0,
          "title": "No comments"
        }
      ],
      "headingsCount": 34
    },
    "contents": {
      "markdown": "*This is the third post in my Effective-Altruism-funded project aiming to deconfuse goal-directedness. Comments are welcomed. All opinions expressed are my own, and do not reflect the attitudes of any member of the body sponsoring me.*\n\nMy strategy for achieving a formalisation of goal-directed behaviour is to equate it with \"behaviour which is well-explained in terms of goals\". So far, I have explored [criteria for judging explanations](https://www.lesswrong.com/posts/KJPRC3cgtxSXpZEQZ/goal-directedness-exploring-explanations) in general and a structured way to [decompose explanations](https://www.lesswrong.com/posts/oZCeun2v3Xd3ncrHt/goal-directedness-imperfect-reasoning-limited-knowledge-and) of agent behaviour specifically.\n\nOne of those criteria was simplicity, or its dual, complexity. In order to determine whether my [initial proposal](https://www.lesswrong.com/posts/KJPRC3cgtxSXpZEQZ/goal-directedness-exploring-explanations#The_na_ve_picture_and_its_flaws) for characterizing goal-directedness holds water, I need to get my hands dirty and learn some complexity theory. This post is a record of me doing that, while keeping my objective in sight. I have learned a lot of complexity theory/theories since I wrote my original post, so the present post is a major update from the somewhat naïve intuition I expressed there.\n\nSee the end for a few references not directly linked in the main text.\n\n### Setting the scene\n\nIn my initial breakdown of explanations, I explicitly assumed that an individual explanation could be assigned a value representing its complexity. Deeper investigation revealed at least four points where complexity might be introduced into an explanation. At the time, I wondered what measure of complexity I should use, expecting to be able to pick one (or several) up out of the box, and get down to the business of applying already-developed theory.\n\nIn attempting to apply the various complexity measures to my explanations as I was learning about them, I found each of them to be inadequate in one way or other. For example, Kolmogorov complexity and various branches of computational complexity explicitly involve a model of computation, such as a (choice of) Turing machine, but the explanations involved in goal-directedness have no obvious ties to computation, and any choice seems arbitrary. Moreover, a lot of the problems discussed in the various branches of complexity theory seem to be about sequences of problems or structures, whereas I am trying to attach complexity to individual explanations!\n\nWhile I may end up adapting some existing theory, my advisor Adam Shimi pointed out that rather than trying to jumble existing measures together in an *ad hoc* way, I should try to understand the underlying principles of complexity to the point of building a measure which captures what I actually care about. That's what I'm really aiming to do in this post.\n\nComplexity, from the bottom up\n------------------------------\n\nComplexity is a word which carries multiple competing, often conflicting, [intuitions and formalizations](https://www.lesswrong.com/posts/DshBToGnNbTBD7BSw/systems-theory-terms#Complexity). It has a lot of baggage. The aim of this post is to extract a core technical notion of complexity which is inclusive enough to cover everything contained in the various branches of complexity theory (plus some other domains besides), in an intuitive enough way to allow me to design a notion of complexity that captures what I need for evaluating explanations. In a single sentence, the concept of complexity I'll describe is a *quantity assigned to a structure*^[\\[1\\]](#fnzuffz4gk6d)^* that measures how many simple pieces are needed to construct it*.\n\nA word of caution to begin with: if you have a casual acquaintance with computational complexity theory, you'll be aware of **complexity classes**, whose members are parameterized *families* of problems. I will get around to those later, but the crucial first step, which was previously missing in my understanding of complexity, is an understanding of the complexity of (solutions to) individual problems.\n\nI should also stress that I'm examining complexity in a much broader context than is covered by any single discipline of complexity theory, but I'll discuss some specific branches of the domain later on.\n\n### Motivating examples\n\nSince I expect computer science to be more accessible to my audience than abstract maths, I'll start with two motivating examples in computational terms. \n\nWhy is multiplying big numbers more difficult than multiplying small numbers? When we actually sit down and compute a product of large numbers, our go-to method is *long multiplication*, where we perform a series of pair-wise multiplications of individual digits (using values that are engraved into our memories by early school experiences which were traumatic for some...) and then add up the results. The more digits there are, the more digit multiplications (and additions) we must perform, and the longer the calculation takes.\n\nIt may surprise you to discover that there are algorithms for multiplication which are [more efficient](https://en.wikipedia.org/wiki/Computational_complexity_of_mathematical_operations) than long multiplication for sufficiently large numbers.^[\\[2\\]](#fns0tz8fzy3j)^ We don't have to just multiply the digits of the numbers themselves; we can sometimes add and/or subtract digits before multiplying to reduce the number of basic multiplications we need to perform.\n\nFinding better algorithms is difficult enough that the existence of a better algorithm than long multiplication was a big surprise [even to the likes of Kolmogorov](https://en.wikipedia.org/wiki/Karatsuba_algorithm#History). One thing is clear, however: there is a smallest number of single digit multiplications and additions required to multiply an arbitrary \\\\(n\\\\)-digit number and an \\\\(m\\\\)-digit number; there is a *most efficient algorithm*. The size of that algorithm, in terms of the number of operations involved, is the **computational complexity** of the multiplication operation for numbers of these sizes.\n\nIf we found that adding digits is easier/less costly than multiplying them, then we might adjust our measure of complexity to reflect this, and this may even change which algorithm is considered the most efficient. More on that later.\n\nNow let's take an example of a different flavour. What is the complexity of a piece of text? For simplicity, let's restrict ourselves to a string \\\\(s\\\\) consisting of just the digits 0 and 1. Here are two possible examples:\n\n*   0101010101010101010101010101010101010101010101\n*   00101011110011010001010000111010001\n\nBefore we get started, I want to stress that *there is not just one answer here*. For example, I could consider the basic operations for writing strings to be \"write 0\"^[\\[3\\]](#fn9ysb8vvect)^ and \"write 1\". In that context, there is no more efficient way to produce these strings than to write them out one digit at a time, and the measure of complexity that we end up with is simply **string length***,* for which the former is the more \"complex\" of the two. On the other hand, if I have the comparative might of a universal Turing machine \\\\(U\\\\) at my disposal^[\\[4\\]](#fnrqsk9dm8zrj)^, I could consider the permissible string-generating procedures to be the inputs for that Turing machine (that is, the programs), and take the complexity to be the length of the *program* instead of the length of the *string*. The result is **Kolmogorov complexity**, \\\\(K_U(s)\\\\). The value computed depends on the specific choice of \\\\(U\\\\), but for many choices it's a lot easier to program \"write 01 twenty three times\" than to find a short program that produces the second string above. I could also choose any programming language as a substitute for a Turing machine^[\\[5\\]](#fnhxlvvjuek2c)^, which might make that last claim more credible.\n\n### So what is complexity?\n\nI can extract from these examples a general idea of what complexity should be.\n\n*   I begin by selecting a class of structures whose complexity I want to measure. In the examples above, these were \"functions defined on pairs of numbers of a given size\" and \"strings (of 0s and 1s)\", respectively.\n*   I select a generation procedure. In the first example, this was the class of algorithms which I was allowed to use (those involving just calculations in terms of individual digits). In the second, the original generating procedure was just \"writing down digits\", while the second was \"input into a Turing machine and consider the output\".\n*   I select a handful of basic operations or building blocks, considering these to require a constant small cost to perform. In the first example, these were additions and multiplications of single digits. In the second, we had individual digits (or 'the operation of writing a single digit') in the first instance and inputting single digits into a Turing machine in the second instance. Note that for the time being, the basic operations are assumed to be sufficiently varied to be capable of exactly generating the desired output.\n*   Given these ingredients, the corresponding complexity of a given structure is the minimum total cost of all choices of basic operations/components which generate it.\n\nThis turns out to cover all of the usual notions of complexity, of which we'll see many in a later section. As we've already seen, it also includes some simpler measures, such as size. As a general rule of thumb, I've found that *the more powerful the basic building blocks are, the coarser the notion of complexity one gets*. For example, if all algorithms are assigned the same constant cost, then multiplication of two numbers always has a complexity of 1, and this notion of complexity doesn't tell us much.\n\nA point I'll come back to in a little while is the fact that **any specific construction of a structure produces an upper bound on the complexity of that structure**. The challenge, for any definition of complexity, is always *finding* a construction which minimizes the complexity measure and *proving that it really is minimal*.\n\nContexts for complexity\n-----------------------\n\nImplicit in the above informal definition is that all notions of complexity depend on choices, the most fundamental of which is a choice of context: we consider the complexity of a structure as a member of a class of similar or related structures. Crucially, that context is itself structured! To understand what structure a context might have, I'm going to take you on a detour through some more abstract maths. To keep the spotlight on complexity, I'll examine the notions of complexity which arise in these contexts.\n\n### Complexity in topology\n\nTopology studies various types of space. Often, a class of spaces can be generated by gluing together a class of \"simple\" spaces in a predetermined way. I'll give some technical examples, but the details don't matter: I just need you to pattern-match, since the complexity measures I'll describe for these classes don't rely on the details.\n\n*   A **simplicial complex** is a space obtained by gluing together *simplices*, or the analogues of triangles in each dimension, along their faces/edges. A 0-simplex is a point, a 1-simplex is an edge or interval, a 2-simplex is a triangle (including its interior), a 3-simplex is a tetrahedron, \"and so on...\"\n*   A **CW-complex** is a space obtained by gluing disks together along maps on their boundaries. Here, \"disk\" means the ball of points at a distance of at most 1 from the origin in \\\\(n\\\\)-dimensional space, and the boundary of an \\\\(n\\\\)-disk is an \\\\((n-1)\\\\)-dimensional sphere. For example, the 1-disk is an interval, and its boundary is the pair of end-points. Gluing along a map on the boundary means that I identify the sphere on the boundary of a disk with its image along a function, as in this picture:![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/72399dd296d58cfbfa8f8b658dcc10fe70ef856a39086c44.png)\n*   A (smooth) **manifold** is a space which can be constructed by gluing together patches that look like open subsets of \\\\(\\\\mathbb{R}^n\\\\) along subsets of a similar form. These spaces are the subject of *differential topology* (and *differential geometry*)*.*\n*   A **scheme** is a space constructed by gluing together *algebraic varieties*. These are the subject of *algebraic geometry.*\n\nIn all of these cases, we are given a class of \"simple\" spaces, which usually means a space whose structure we can easily study directly and explicitly. These are used to construct more interesting spaces, often with emergent global structure that the simple spaces lack.\n\nThere are numerous notions of equivalence for spaces. For manifolds, say, we could consider invertible maps which preserve all of the local \\\\(\\\\mathbb{R}^n\\\\)-like structure (diffeomorphisms), invertible continuous maps with continuous inverses (homeomorphisms) or, most coarsely, the maps with \"quasi-inverses\" where the respective composites can be \"continuously deformed\" back to the identity (homotopy equivalences).\n\nA choice of recipe for constructing spaces, plus a choice of equivalence for the resulting spaces, results in a notion of complexity: *the complexity of a given space is the smallest number of simple spaces required to build a space equivalent to the given one*.\n\nThis isn't the most nuanced form of complexity at our disposal in this setting. In the case of CW-complexes, for example, this combinatorial notion of complexity doesn't take into account the potential complexity of the gluing maps (I'll get onto defining complexity of maps later). One might also want the dimension of the constituent spaces to contribute to the calculation: it's not outrageous to decide that a 8208-dimensional sphere is a little more complex than the familiar 1-sphere (circle).\n\n### Complexity in algebra\n\nAbstract algebra studies various types of mathematical object. Each class of objects is determined by the operations that it comes equipped with (its *signature*) and the equations which these operations are required to satisfy. Here is a list of examples of what that can mean, with some formal details deliberately omitted for conciseness.\n\n*   A **semigroup** is a collection of elements equipped with a *multiplication operation*, a way of combining any pair of elements to produce some other element.\n*   A **group** is a semigroup in which we can also divide by/cancel elements (and there is a special element which is neutral in the sense that multiplying by it does nothing).\n*   A **ring** is a collection of elements which we can add, subtract and multiply. A ring also has special elements called 0 and 1 which behave in the way you would expect with respect to addition and multiplication.\n*   A **vector space** over a field^[\\[6\\]](#fn6nvynucnkac)^ \\\\(K\\\\)is a collection of elements (called vectors), equipped with both *addition* and *scalar multiplication*, where the latter is a bunch of operations indexed by the element of the field.\n\nIf you aren't familiar with these, it's not important; all I want you to observe is the pattern of their descriptions: *elements* and *operations* (plus some rules about how the operations interact with one another, which I have omitted).\n\nIn any of these examples, if I take a pair of elements (possibly the same element twice), I can apply a binary operation such as multiplication or addition to them to produce a further element. I can further perform operations involving the result to produce more elements, and so on. In other words, it may be the case that I can construct all of the elements of the algebraic structure by applying the operations successively to a handful of *generators*. For example, the positive natural numbers form a semigroup under addition, and every number can be obtained by adding 1 to itself some number of times, so 1 *generates* the semigroup of positive natural numbers.^[\\[7\\]](#fnpkidne7z5k)^\n\nIf I take an unstructured collection of elements, a *set*, I can generate a structure of any of the above types by iteratively applying the formal operations to the members of this set. This produces the **free** structure generated by this set. For example, the free semigroup on the set \\\\(\\\\{a,b\\\\}\\\\) has elements \\\\(\\\\{a,b, aa, ab, ba, bb, aaa, \\\\dotsc\\\\}\\\\), where I have written the letters next to one another to represent multiplication.^[\\[8\\]](#fnlesmosn8hwb)^\n\nIf my general presentation of complexity from earlier made any sense, I'm hoping that you can already see that *the number of generators provides a notion of complexity* for algebraic structures.\n\nBeyond free structures, a **presentation** of an algebraic structure consists of a collection of generators combined with a collection of pairs of elements of the free structure they generate which are to be identified (typically presented as equations). For example, the following is presentation of a semigroup: \\\\(\\\\langle a,b \\\\mid aa = a, bb = b, ab = ba \\\\rangle\\\\)  \nI claim that the semigroup it produces has just three elements, \\\\(\\\\{a,b,ab\\\\}\\\\); work out what the multiplication operation does if you like. Again, if it's not clear to you how one produces an algebraic object from a presentation, it doesn't matter too much: the key observation is that the *generators* and *relations* in a presentation, supplemented with the *generation procedure*^[\\[9\\]](#fnjnuvd11437p)^ for the given class of algebraic objects, provide all of the ingredients we need to view the size of a presentation as a measure of complexity of these objects. Just as in the topological case above, we must choose a notion of equivalence of structures in order to complete the picture, but the choice is much narrower for algebraic structures: we typically consider these to be equivalent if they are *isomorphic*, which is to say that there exists a structure-preserving bijection between their sets of elements.\n\nDetermining the structure generated from an arbitrary presentation is [famously difficult](https://en.wikipedia.org/wiki/Group_isomorphism_problem), in a formal sense: it's uncomputable in general, as are many of the measures of complexity which I'll construct here. On the other hand, any finite structure has a \"trivial\" finite presentation, where the set of generators is the set of all of its elements, and the equations express the result of applying the operations to the elements. If the class of structures being considered has \\\\(k\\\\) binary operations then the trivial presentation of a structure with \\\\(N\\\\) elements has \\\\(N\\\\) generators and \\\\(N^{2k}\\\\) relations. As a consequence, there must exist a *most efficient* presentation of any finite structure, and this situation is intuitively comparable to the notions of complexity we saw in previous sections. More generally, as long as we know that there exists a finite presentation of an algebraic structure (even if that structure is infinite), then there must be a smallest one, so this concept of complexity^[\\[10\\]](#fn7ntkldpbzzc)^ makes sense for all *finitely presentable* algebraic structures.^[\\[11\\]](#fnibznwevdaai)^\n\n### Between algebra and spaces\n\nCombining the above two sections, we can consider all sorts of constructions extending our categories of algebraic and/or topological objects. This section is deep into abstraction, so I'll keep it to a single example, but it foreshadows an approach to dealing with infinite structures which we'll see again later on.\n\nA **profinite group** is a topological group constructed as a projective limit of finite groups. Even though a given profinite group may not admit a finite construction (and may not even have a \"minimal\" construction in a meaningful sense) we can still extract a nuanced measure of complexity for these structures. Indeed, having determined a notion of complexity for finite groups such as the presentation length, we may define the complexity of a profinite group to be the maximum complexity of finite groups appearing in its construction, minimized across possible constructions. A particularly simple class of groups are the **cyclic groups**, with presentations of the form \\\\(\\\\langle a \\\\mid a^k = 1 \\\\rangle\\\\), where \\\\(k \\\\) is a positive integer and \\\\(1\\\\) represents the identity element of the group; with the above definition of complexity, it follows that the simplest (lowest complexity) class of nontrivial profinite groups are the *procyclic groups*, constructed as projective limits of cyclic groups.\n\nComplexity within a structure\n-----------------------------\n\nWhy did I bother with the abstract detour above? The answer is that the structures encountered in complexity theory can often be thought of as functions defined on spaces or as elements of algebraic structures.\n\n### Piecewise maps\n\nWhat does it mean for a space to be constructed by gluing together simpler spaces? One consequence is that when we define a map on the space, we can restrict that map to the simpler pieces, and conversely that if we are given mappings on the simpler pieces which are compatible with the gluing construction in a precise sense, then these mappings can themselves be glued together to define a mapping on the full space.\n\nAs such, as soon as we define a notion of complexity for functions on the simple spaces, we have numerous ways to assign complexities to the glued space: we could take a maximum or a weighted sum, say. I'll explain some ways of assigning complexities to functions later on.\n\n### Elements of algebras\n\nA choice of presentation of an algebraic structure \\\\(A\\\\) determines a natural notion of complexity for the elements of \\\\(A\\\\), namely the shortest presentation of that element in terms of the generators in the presentation. Here the generators are playing the role of the basic building blocks, and the algebraic operations are the ones we allow when constructing the elements.\n\nObserve that the trivial presentation corresponds to the trivial complexity measure I mentioned earlier, where everything has the same complexity. On the other hand, for a small generating set, the complexity becomes a lot more interesting.\n\nIt's also worth noting that we can vary the complexity measure significantly by varying the weights assigned to the different generators and operations in the structure. In a real vector space with a given basis, for example, we could assign the basis vectors a weight of \\\\(1\\\\), scalar multiplication by \\\\(r\\\\) a weight of \\\\(|r|^2\\\\) and addition a weight of \\\\(0\\\\), in which case the \"complexity\" of vectors is the sum of the squares of the coefficients determining the vector in that basis, the \"squared length\" of the vector when the basis vectors are considered to have length \\\\(1\\\\). Contrast this with what happens if we assign scalar multiplication a weight of \\\\(0\\\\) and addition a weight of \\\\(1\\\\), in which case the complexity just counts the number of basis vectors required to present a given vector (equivalently, the minimum dimension of a subspace generated by basis vectors containing the given vector). Both of these are clearly highly dependent on the choice of basis.\n\nFragility of complexity measures\n--------------------------------\n\nIn our examples, we have seen that the complexity measure we end up with depends on all of the choices we made along the way, including the context, the basic building blocks, the allowed operations for combining them and the weights assigned to each of those ingredients. Changing any of these choices not only affects the numerical value we end up with, it can completely change which elements or objects are considered more or less complex than others. This is not a profoundly surprising result, although the degree of sensitivity may be surprising; see the end of the section on computational complexity [below](https://www.lesswrong.com/posts/jgYGZD2zRK6nncJd5/goal-directedness-tackling-complexity#Computational_complexity) for an example. \n\nUntil recently, the fragility of complexity measures seemed to me to be an insurmountable problem. I don't want to be using an evaluation that depends heavily on apparently arbitrary choices; those choices shouldn't be influencing my results!\n\nFortunately, we can more often than not excise some arbitrary choices while salvaging a meaningful measure of complexity, through a variety of mechanisms. This is inherently a trade-off: the more we try to make our complexity classes independent of choices, the coarser-grained the notion of complexity becomes. For my own purposes, I'm hoping that I will nonetheless reach a complexity measure which captures the intuitions I'm seeking to formalize.\n\n### Minimal presentations\n\nFirst of all, we can shift the choices up a level, where they can seem less arbitrary. If our context is a (finitely presentable) algebraic structure, and we have chosen a sensible notion of complexity for presentations, we can consider the generators of a minimal presentation as a natural choice of basic building blocks (and we already know that this will at least give a more refined notion of complexity than a non-minimal presentation).\n\nIf there is a unique minimal presentation for the context, the above is an entirely natural choice. However, there may instead be several (even many) minimal presentations for the algebraic structure: think of choosing different bases for a vector space. In that case, we can construct a more robust notion of complexity as one which is invariant under change of minimal presentation. This can be achieved, for example, by taking the minimal complexity value across possible choices of minimal presentation. The vector space example turns out to be a bad one on its own, since any vector is a member of some basis (so the complexity measures we saw earlier can never be basis-invariant), but if we instead consider linear endomorphisms of a vector space, choosing a basis for the space enables us to present the endomorphism as a *matrix*, and there are various non-trivial invariants including the *rank* which are basis-invariant, and can be considered as notions of complexity.\n\nIf we take a vector space equipped with an *inner product*, then while there are still just as many choices of basis available, some of those choices have nicer properties: the best-behaved ones are *orthonormal bases.* (As ever, if those terms are unfamiliar, read around them. The key take-away is that in some contexts the minimal presentation in an algebraic sense is not automatically the best possible.) We could modify the complexity measure on presentations to take the inner product structure into account, or just impose the constraint on choices anyway. Either way, having selected some extra criteria which the minimal presentations of the context must satisfy, we reduce the number of constraints on a complexity measure when we demand it be invariant under change of presentation. The \"length squared\" measure that I described in the last section is invariant under change of orthonormal basis, for example.\n\nA compelling example of constrained bases in the definition of a complexity measure is that of [*tensor rank*](https://en.wikipedia.org/wiki/Tensor_(intrinsic_definition)#Tensor_rank)*.* One way of constructing a vector space is as a *tensor product* of a pair of vector spaces, say \\\\(V \\\\otimes W\\\\); an element of such a space is typically called a *tensor*. The natural bases for \\\\(V \\\\otimes W\\\\) are those constructed from bases of \\\\(V\\\\) and \\\\(W\\\\). Unlike for vector spaces more generally, not every tensor is a member of such a basis, and we therefore get a meaningful notion of tensor rank as the minimal number of basis vectors (across all possible constrained bases) required to present a given tensor.\n\n### Minimal decompositions\n\nMuch of the discussion above can similarly be applied to spaces: while there may be many decompositions of a space into simpler spaces, minimal decompositions provide natural basic components for complexity measures. Both here and above, if the minimum can be concretely identified, it will also necessarily be the simplest presentation of the space or algebra to work with from a computational point of view!\n\nConsider the circle, or 1-sphere. When presenting this as a manifold, the minimal decomposition involves two open line segments (open subsets of \\\\(\\\\mathbb{R}\\\\)), glued along a pair of sub-segments.\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/4646bc637b9a76034a7bc992a3dc0cfbb194c37f802611e1.png)\n\nThe red and green segments are (homeomorphic to) open intervals in \\\\(\\\\mathbb{R}\\\\); when we glue along the overlaps, we get a circle.\n\nClearly there are several choices to be made here about the four segment lengths (the segments being glued together and the lengths of the overlaps); although the end result as a space is always a circle, the choice of decomposition will affect our definition of complexity of functions on the pieces, and we can only obtain an invariant complexity measure by taking all of the minimal decompositions into account, which amounts to examining symmetries of the resulting space. The study of symmetries moves us beyond topology into *geometry.* Finding complexity measures for functions which are invariant under these re-parametrizing symmetries is an interesting challenge, which is beyond the scope of this post.\n\n### Invariance at the cost of individuality\n\nIn some cases, there will be no useful choice-invariant notion of complexity for individual structures. We saw earlier that Kolmogorov complexity requires a choice of universal Turing machine. However, given any string, we can design a universal Turing machine which makes that particular string have arbitrarily small or large complexity, essentially by manipulating how the programs are encoded.\n\nOn the other hand, Kolmogorov proved that his measure of complexity is choice-invariant up to a constant, as follows.^[\\[12\\]](#fna70tcdq76js)^ If I take any pair of universal Turing machines \\\\(U\\\\) and \\\\(U'\\\\), each can be compiled into the other, in the sense that there are strings \\\\(u\\\\) and \\\\(u'\\\\) such that for all input strings \\\\(x\\\\) and \\\\(y\\\\) (independent of \\\\(u,u'\\\\)) we have \\\\(U(u';x) \\\\equiv U'(x)\\\\) and \\\\(U'(u;y) \\\\equiv U(y)\\\\), where the semicolon denotes concatenation and the triple-line means \"produces the same behaviour (in terms of halting and outputs)\". As such, for any string \\\\(s\\\\), we have \\\\(K_{U'}(s) \\\\leq K_{U}(s) + |u|\\\\) and \\\\(K_{U}(s) \\\\leq K_{U'}(s) + |u'|\\\\). An immediate consequence of this fact is that, while we cannot produce a measure of the Kolmogorov complexity of an *individual* string which is independent of the choice of Turing machine, if we take a *sequence* of strings, we can extract coarser-grained complexity values which are independent of the choice.\n\nFor instance, the **asymptotic growth rate** of the Kolmogorov complexity of a sequence of strings is a choice-invariant notion. *Asymptotic* here roughly means \"on average in the limit of large indices\", the idea being that while we have little control over the exact lengths of programs required to generate individual strings on different Turing machines, the constant bounds on the difference between Turing machines of the complexity of strings in a sequence will merely be an \"error\" whose contribution can be made as small as we like by averaging across a sufficiently large number of strings in the sequence. An infinite stream \\\\(S\\\\) of digits also has a well-defined notion of complexity, by identifying \\\\(S\\\\) with the sequence \\\\((s\\_n)\\_{n \\\\in \\\\mathbb{N}}\\\\) whose \\\\(n\\\\)th term \\\\(s_n\\\\) is the string consisting of the first \\\\(n\\\\) digits of the stream. An essentially random stream will produce a complexity that grows linearly, \\\\(K\\_U(s\\_n) = O(n)\\\\), whereas an infinite string of \\\\(0\\\\)s has complexity which grows sub-logarithmically \\\\(K\\_U(s\\_n) = o(\\\\log(n))\\\\); the invariance means that we can drop the subscript \\\\(U\\\\) in these statements.^[\\[13\\]](#fnj3ige8zh36)^\n\nOne must be careful with the kind of shift of context we performed above. We haven't actually resolved the problem of having a well-defined complexity for individual strings: their natural substitutes as sequences, the constant sequences, all have the same complexity of \"no growth\", \\\\(K_U(s) = O(1)\\\\). Rather, **we have merely observed that sequences of structures (such as strings) form a context which is rich enough to leave interesting complexity measures after imposing invariance with respect to symmetries which made complexity measures on the individual structures trivial**.\n\nNote that the asymptotic growth rate would *not* work as a measure of complexity of the profinite groups we saw earlier, even if we restrict to those which are expressible as the (projective) limit of a sequence of finite groups. This is because any subsequence or re-indexed sequence will produce the same group in the limit, and so we can make the complexity of the sequence grow as fast or as slow as we like. On the other hand, if we put a constraint on the form of the sequence, like imposing a relation between the index \\\\(\\\\)\\\\(n\\\\) and a bound on the size of the \\\\(n\\\\)th group in the sequence, we might be able to salvage an alternative to the \"maximum complexity\" measure we proposed earlier.\n\nI should also stress that the asymptotic growth rate classes I have mentioned above are characterized by *upper bounds*, so that the class of \\\\(O(f(n))\\\\) sequences includes also all of the classes of \\\\(O(g(n))\\\\) sequences where \\\\(g(n)\\\\) grows more slowly than \\\\(f(n)\\\\). A more precise statement would be one of the form \\\\(K\\_U(s\\_n) = \\\\Theta(f(n))\\\\), meaning that the growth rate of the Kolmogorov complexity of \\\\((s\\_n)\\_{n \\\\in \\\\mathbb{N}}\\\\) is bounded above *and below* by constant multiples of \\\\(f(n)\\\\), which excludes slower growth rates.\n\n### Probabilistic contexts\n\nRather than sequences, we might consider subsets of the family of finite strings. For sufficiently uniform subsets, one can make statements about the Kolmogorov complexity which are independent of the choice of Turing machine. For example, an argument based on the pigeon-hole principle shows that if we take the collection of strings of length \\\\(n\\\\), even if we construct a universal Turing machine especially to compute these strings (so that all of the shortest possible progams produce strings of length up to \\\\(\\\\)\\\\(n\\\\)), since there are at most \\\\(2^n-1\\\\) programmes of length less than \\\\(n\\\\), at least one string in this set must have Kolmogorov complexity \\\\(n\\\\) (or higher) with respect to every universal Turing machine. By putting a uniform distribution on the strings of a certain length, we can express this as a probabilistic statement: for *any* universal Turing machine \\\\(U\\\\), the probability that a randomly chosen string of length \\\\(n\\\\) has \\\\(K_U\\\\)-complexity at least \\\\(c\\\\) [is](https://en.wikipedia.org/wiki/Kolmogorov_complexity#Compression) \\\\(1 + 2^{−n} − 2^{c-n+1}\\\\).\n\nRelatedly, in place of individual (deterministic) streams of digits, I could consider streams of digits which are generated according to a random process, and then extract statistical statements about the asymptotic behaviour of such streams, such as the *expected asymptotic growth rate*. **More generally, probabilistic variations on the context of structures under consideration can represent an alternative way to eliminate the dependence on choices, while retaining the same fundamental objects**. The cost is that the resulting evaluations of complexity will be probabilistic or statistical in nature.\n\nValues for complexity measures\n------------------------------\n\nUp until the last subsection, the only types of complexity value which I had presented to you were *numerical*, either whole numbers (counting a number of operations or pieces) or real numbers (measuring lengths or weighted sums). However, we just saw that we can attach values to complexity which are more subtle, such as growth rates attached to sequences of structures. What other kinds of values can complexity measures take?\n\n### Number vs Order\n\nIn other areas of maths, we see number systems extended in various ways. The name \"complex numbers\" might seem suggestive at this point as a potential target for valuing complexity. **Unfortunately, complex numbers lack a key requirement for complexity measures**, namely *order*. When defining the complexity of a structure, we want it to identify the *minimum* amongst possible values associated to that structure, and with complex numbers there is no natural ordering to minimize along. There are artificial ways to attach an ordering, like looking for a complex value with minimal modulus or considering only complex values with positive real and imaginary part and associating some ordering on these, but both strategies amount to lifting the ordering on the (positive) real numbers (resp. possible orderings on \\\\(\\\\mathbb{R}_+ \\\\times \\\\mathbb{R}_+\\\\)), at which point we might as well use those, since the extra structure on the complex numbers is essentially being discarded.^[\\[14\\]](#fniliok2wcsl)^\n\nOn the other hand, the [**ordinals**](https://en.wikipedia.org/wiki/Ordinal_number)**,** a generalization of numbers indexing (possibly infinite) discrete lists, have sufficient structure to act as complexity values, since by definition any set of ordinals has a least element. While this may seem rather abstract, ordinals are used to construct [heirarchies of functions](https://en.wikipedia.org/wiki/Fast-growing_hierarchy) such as the ones we encountered for classifying growth rates in the above. I suspect that more intricate number systems, like a countable fragment of the [surreal numbers](https://en.wikipedia.org/wiki/Surreal_number), could be equally be used to construct hierarchies of this kind. For example, we can produce functions [which tend to infinity arbitrarily slowly](https://math.stackexchange.com/a/576453/354508) as well as arbitrarily fast, and these can be included as \"infinitesimal factors\" to growth rate functions defining complexity classes.\n\nTotal orders are convenient for defining complexity classes, but they are not necessary. As long as our collection of values has arbitrary meets (greatest lower bounds), that's sufficient to define a minimum value, \"the\" complexity of the structure^[\\[15\\]](#fnnwvtmh0kz58)^. Incidentally, we have already seen some situations where our collection of complexity values is insufficient to express the complexity of some structures: the presentation complexity of an algebraic structure with no finite presentation is not defined. An alternative to delineating different sizes or qualities of infinity is to just give up and say that any structure which doesn't admit a value in our chosen collection has *infinite complexity* (the greatest lower bound of the empty collection, or top element of the order). If our complexity measure is supposed to express the difficulty of a problem or construction, this kind of collapse makes a lot of sense: there's little immediate benefit to delineating between different degrees of \"impossible\"!\n\nFor the remainder of this post, I'll talk about a **valuation structure** for a choice of values for complexity measures. It can be useful for a valuation structure to have further algebraic structure, as the positive reals and the natural numbers do. A formal reason for this is the subject of the next subsection.\n\n### Symbolic complexity values\n\n**Warning:** this is the most abstract subsection of the post, proceed with caution.\n\nOnce we have determined the generating operations and the building blocks for our measure of complexity, there remain choices to make about what values to assign to each of them, especially which features should have non-zero cost, and how these costs are combined in computing a complexity bound from a construction. Putting aside the last subsection for the time being and considering just the simplest case where the complexity values are taken to be positive real numbers, the precise values needed in practice may depend on implementation details. In the most concrete examples, these values might be determined by the relative speeds of the operations for a given (fixed) implementation on my hard drive.\n\nRather than making the judgement about what these values should be straight away, we could *interpret each basic building block as a (formal) variable* and *interpret each construction operation as a (formal) function*, so as to view the construction itself as an algebraic expression for the complexity bound it determines for the structure it defines. A complexity measure then amounts to an assignment of elements of our valuation structure to each variable and an interpretation of the formal functions as operations on the elements of the valuation structure. The complexity of a structure is the minimum (in the ordering on the valuation structure) of the resulting value over its possible constructions.\n\nA reader with experience in logic might prefer to think of it like this: a construction of a structure, such a presentation, is a purely syntactic expression in a particular language which comes equipped with predetermined semantics in the domain of interest, such as the class of algebraic structures. A complexity measure is a *second choice of semantics* for the same language, this time in the valuation structure; I might as well call the second semantics an **evaluation**. The complexity of a structure is the minimum evaluation of constructions of equivalent structures. (Beware that this syntax-semantics presentation might cause some confusion when we get onto implicit complexity theory later, since the constructions there are proofs but the structure generated by a proof is just its conclusion, which is also a syntactic object!)\n\nThis abstraction reveals the higher-order choices involved in defining complexity measures, especially the ones determining how we combine the costs associated to the building blocks when they are put together with an operation.^[\\[16\\]](#fnv5z1wep5ak)^\n\nIt is not clear to me whether this formulation of complexity is exhaustive. *A priori* there may be other contributing factors to complexity that a given evaluation fails to capture, but any examples in this vein that I can come up with seem to be accounted for by including extra operations in the constructions. In my original example of multiplication of large numbers in terms of multiplications and additions of individual digits by composition, for example, we could also include the cost of shifting digits around in these algorithms to account for how the time taken to multiply larger numbers in practice will grow faster than the number of single digit operations involved (for practical implementations).\n\n### Combinatorial complexity measures and projective parameter spaces\n\nI'll call a complexity measure **combinatorial** if the basic building blocks are assigned constant (positive real) values and all of the combinations operations are additive on the complexity (or possibly add constants; I'll leave that possibility implicit here). Our measures which counted the number of basic digit addition and multiplication operations, the string length and the presentation complexity are all combinatorial. This is arguably the simplest kind of complexity measure, if such a notion makes sense,^[\\[16\\]](#fnv5z1wep5ak)^ and is amenable to some specialised analysis techniques, which I'll sketch here.\n\nObserve that if we scale all of the values assigned to the basic building blocks in a combinatorial complexity measure by some positive real number, the complexity value for all constructions are scaled by the same number. Since complexity theory is mostly concerned with comparative evaluations of which presentations are more complex, it may therefore be useful to consider the bounds on complexity determined by given constructions in terms of *projective geometry*.\n\nConsider the basic operations of addition and multiplication of digits we considered earlier. We can define a projective space^[\\[17\\]](#fnt70obtdizdn)^ of parameter values for these operations, whose elements are the possible ratios of costs of performing them; the dimension of this space will be 1 (one less than the number of basic building blocks for the combinatorial complexity measure). Given a list of algorithms for multiplication of numbers of a given size in terms of these basic operations, we get a mapping from this projective space to another projective space, whose dimension is one less than the number of algorithms. The mapping sends each ratio of parameter values to the corresponding ratios of complexity bounds determined by the algorithms. This mapping characterizes, amongst other things, the break-even points for which of the different algorithms under consideration give the best upper bound on the complexity, as a function of the cost ratio of the basic operations.\n\nThe same basic argument can be extended to any combinatorial complexity measure. I don't have the experience with projective geometry to take this analysis further than the above curious observation at the moment (I have not come across any literature on this direction so far). I might as well mention that I expect combinatorial complexity measures to be entirely adequate for my purposes in goal-directedness, so this section could get developed further by me in future.\n\nThe broad takeaway of this section is that **one more way of avoiding arbitrary choices, this time in the defining values of a complexity measure, is to consider all possible choices together at once,** or at least to consider classes of choices. A specific choice can either be made later (if a specific implementation will determine the values assigned) or coarser-grained conclusions can be drawn which are a function of the possible choices. In the latter case, the conclusions might be judgements about upper bounds on the complexity, as a function of the complexity parameters.\n\nComputational Complexity\n------------------------\n\nI have discussed the example of Kolmogorov complexity at some length. Let's see some domains of computational complexity theory, for balance (although there are surely plenty I haven't read about).\n\n### Classical computational complexity\n\nComputational complexity theory asks, \"how much of a given resource (time, space/memory, etc) is required to compute a value/solution?\" Just like Kolmogorov complexity, the answer to such questions depends on the details of the model of computation being considered. Even though \"computation\" is in the name, the mathematically interesting details of computational complexity are considered to be those which are independent of specific implementation. For this reason, it is typical to consider an implementation-independent version, which just as above is most easily achieved by instead considering *sequences* of problems with a sensible indexing parameter. A large number of different models of computation can simulate one another with an encoding of inputs of at worst polynomial (even quadratic!) size. The *complexity classes* we end up with are measures of asymptotic growth rates, in the sense I described earlier, but these classes are coarser than those above, being defined up to a polynomial change of indexing of the sequences.\n\nThis coarse-graining also resolves another issue, related to the implementation of structures as *inputs* to models of computations. For example, if the input is a simple, undirected graph, should the input size measure the number of vertices or the number of edges? The latter grows approximately quadratically in the latter (a simple graph has at most \\\\((n^2 - n)/2\\\\) edges). There are other possible choices too, such as the sum of the number of vertices and edges; all in all, selecting the indexing amounts to choosing a complexity measure for graphs. Since the complexity classes are indifferent to the exact complexity measure employed for the structures, *on the condition that these are related to one another by polynomial bounds*, these choices can be safely ignored. as is the case for my graph example.\n\nThe question then becomes, \"how quickly does the quantity of a given resource required to compute a value/solution grow with the size of the input?\"\n\nTypical examples of complexity classes are:\n\n*   Deterministic time complexity classes, such as P (polynomial time), DLOGTIME (deterministic logarithmic time with random access^[\\[18\\]](#fn50sp0qm4xg8)^) and EXPTIME (exponential time). These measure how quickly decision problems in a sequence increase in running time, or number of operations performed, with the size of their inputs.\n*   Non-deterministic time complexity classes, such as NP (non-deterministic polynomial time) and coNP, which have several equivalent definitions. One of these is that they measure the growth rate of time taken to *check* a proposed solution to a problem is valid (resp. invalid).\n*   Deterministic space complexity classes, such as L (logarithmic space), PSPACE and EXPSPACE. These measure the growth rate of the amount of memory/working space required for a decision problem in terms of the size of the inputs.\n*   Nondeterministic space complexity classes like NL, NPSPACE and NEXPSPACE. [Savitch's theorem](https://en.wikipedia.org/wiki/Savitch%27s_theorem) shows that the latter two of these are equal to their deterministic counterparts.\n*   Interpolating classes such as SC ([Steve's Class](https://en.wikipedia.org/wiki/SC_(complexity))) which place simultaneous demands on time and space complexity of an algorithm.\n\nAll of these are defined for (variations on) Turing machines, but many of them have closure properties which allow them to be characterized in other ways.\n\nInstead of decision problems, we could consider computable functions, and determine the complexity classes for such functions by the same criteria as above. The analogue of P for functions is [sometimes called FP](https://en.wikipedia.org/wiki/Function_problem#Relationship_to_other_complexity_classes). The class FP is closed under composition, and was [characterized by Cobham](http://www.cs.utoronto.ca/~sacook/homepage/cobham_intrinsic.pdf) as the smallest class of functions with certain closure properties containing the functions \"append 0\", \\\\(s_0(w) = w0\\\\); \"append 1\", \\\\(s_1(w) = w1\\\\), and \"log-exponential\", \\\\(f(x,y) := x^{l(y)}\\\\) (where \\\\(l(y)\\\\) is the length of \\\\(y\\\\) in bits). In other words, the resulting class of functions has an implementation-independent characterization! Obtaining and manipulating intrinsic characterizations such as this is the subject of [**implicit complexity theory**](http://cs.unibo.it/~dallago/FICQRA/esslli.pdf)**.**\n\nMany containments are known between the various classes. A great number of the open problems in complexity theory ask whether such containments are strict. Most famously, it's easy to show that any problem which can be decided in polynomial time can be checked in polynomial time, so \\\\(\\\\mathrm{P} \\\\subseteq \\\\mathrm{NP}\\\\), but it's not known whether some problems checkable in polynomial time are *not* decidable in polynomial time; this is the famous P vs NP problem.\n\nWhy should the answer be difficult to determine? An *oracle* is a hypothetical component which can be added to a Turing machine which responds to certain queries (even uncomputable ones) efficiently; in the language of the present post, this amounts to including those queries or functions as basic operations. Baker, Gill and Solovay [proved in the 1970s](http://cse.ucdenver.edu/~cscialtman/complexity/Relativizations%20of%20the%20P=NP%20Question%20(Original).pdf) that there exist oracles which make the relativized P vs NP problem true *or false*. Computational complexity is very sensitive indeed to which operations are allowed and their respective costs. Several proof methods in computational complexity theory up to this point could be *relativised*, meaning that adding an oracle made no difference to their arguments. As such, the Baker-Gill-Solovay result demonstrates that such methods cannot distinguish P from NP; this is the **relativization barrier**.\n\n### Circuit complexity\n\nA more specialized branch of computational complexity deal with **circuits**, which are a more rigid notion of algorithm closer to the algebraic contexts I presented above, where the operations are logic gates. The complexity is measured in terms of various measures of the size of the circuits, including width and depth, and the classes are defined in terms of these and a number of other factors, such as the number of inputs that each gate is allowed to accept and the order in which logical operations are allowed to appear. I don't know much about these yet, so I'll just mention the **AC** ([alternating class](https://en.wikipedia.org/wiki/AC_(complexity))) **hierarchy** and **NC** ([Nick's class](https://en.wikipedia.org/wiki/NC_(complexity))) as interesting examples. \n\n### Descriptive complexity\n\nBy definition, a decision problem is the problem of determining whether a certain proposition (logical formula) is true for a structure. For a given class of finite structures, encoded in such a way as to allow their being input into a Turing machine, say, we can identify a proposition interpretable on these structures with its *extension*, which is to say the collection of structures which validate the proposition.\n\nSince propositions are formal statements with lots of structure, we have various notions of complexity for them, including length. The type of complexity that is considered in descriptive complexity theory is that of the **smallest fragment of logic** (the smallest family of logical operators or constructors) within which the proposition can be expressed. This is interesting in its own right in the context of this post, since it's a collection of complexity classes which is not a coarse-graining of a numerical complexity measure in any obvious sense. Some examples of fragments of logic include:\n\n*   **First order logic** (FOL) includes the usual negation (NOT), (finitary) conjunction (AND) and disjunction (OR) operations applied to the basic relations on the structures, as well as existential (\\\\(\\\\exists\\\\)) and universal (\\\\(\\\\forall\\\\)) quantification applied to variables ranging over the elements of the structures.\n*   **FOL with a transitive closure** augments the above with the ability to take the transitive closure of a relation.\n*   **Second-order logic** (SOL) allows for quantification over the family of relations defined on the structures.\n\nConversely, each fragment of logic defines a class of decision problems. Surprisingly, many of these classes [coincide with complexity classes](https://en.wikipedia.org/wiki/Descriptive_complexity_theory#Overview_of_characterisations_of_complexity_classes) listed above, which is another example of classes having characterizations independent of their relation to Turing machines or circuits.\n\nI could observe that the complexity values in descriptive complexity theory aren't closed under intersections in any obvious way. We have equivalence between propositions in different fragments of logic if they have the same extension which means that a language can be in multiple classes, but *a priori* the existence of equivalent propositions in two incomparable fragments doesn't imply the existence of an equivalent proposition in a smaller mutual fragment. The complexity of a family of structures is thus only formally well-defined if we can prove that this *is* the case, or if we allow \"intersection classes\" expressing the fact that a formula lies in multiple incomparable fragments of logic.\n\nDescriptive complexity theorists are not concerned with this problem, to my knowledge, because the class of fragments of logic is entirely open: we can always extend our logics with new connectives and constructions, so there is little hope of being exhaustive about which precise complexity class a class of structures lies in unless we impose some (potentially arbitrary) extra constraints.\n\nComplexity measures for functions\n---------------------------------\n\nI can now go about constructing some of the complexity measures I've been promising. The starting point is analysis, yet another branch of maths..!\n\n### Approximation complexity\n\nSuppose I want to assign a measure of complexity to a function. If the function is computable, we fall into the realm of computational complexity. But a function may be formally uncomputable in several different ways.\n\nOne obstacle is that the function may take values which are not accessible in our chosen model of computation. A machine which can only output natural numbers will struggle to express negative numbers; a machine which can express integers may not explicitly be able to output rational numbers, and a machine which can represent rational numbers may be unable to represent irrational algebraic numbers or arbitrary real numbers. Another obstruction that could arise is that the calculation must check an infinite number of cases to arrive at a conclusion, and so on some inputs never produces an answer.\n\nAlthough calculating an exact value is hopeless in such situations, in the first case we can at least gauge the complexity of *approximations* to these functions.\n\nFor the cases where the machine outputs integers, we can compute the complexity of the floor or ceiling of a given function, as a function of the input. If the inputs of the functions are also natural numbers, this reduces to ordinary computational complexity, but more generally we may have to choose a sequence of inputs from which to obtain a complexity class as the asymptotic worst-case upper bound on the complexity of the approximation.\n\nThe rational-to-real case is more interesting. Here, we can examine the resources required to achieve an approximation of a certain quality, which is to say produce an output within \\\\(\\\\epsilon\\\\) of the true value. In this situation we can fix the input value and consider the resources required to achieve ever better approximations of the function value; a natural choice of sequence here to measure the complexity along is to take \\\\(\\\\epsilon = 1/n\\\\) with \\\\(n = 1,2,3,\\\\dotsc\\\\) The asymptotic time complexity in this situation is sometimes called *convergence rate* or  **iteration complexity**. As we saw earlier with Kolmogorov complexity, we must fix a machine against which to measure the asymptotic complexity, since otherwise we could choose a machine at each stage which just outputs a pre-computed sufficiently good approximation; for reasonable complexity measures, the result should depend only on the function, rather than the choice of machine.\n\nThere is another implicit parameter choice being made here. I am using the absolute difference to judge the quality of the approximations, but other metrics are possible: for functions to the rationals, we could measure the \\\\(p\\\\)-adic distance to the true value for some \\\\(p\\\\). This choice of metric will be problem-dependent.\n\n### Simple functions\n\nRather than approximating individual values of the function (and possibly have to make a choice about which machine to use in the calculation and which input values to assess the complexity at), we can ask how difficult a given function is to approximate by a class of *simple functions*. This has a [specific meaning](https://en.wikipedia.org/wiki/Simple_function) in the theory of Lebesgue integration, namely a function obtained as a finite sum of (real multiples of) indicator functions for measurable subsets. That coincidence is entirely intentional, since the first example I want you to picture is that of approximating a function from (a bounded interval in) the reals to the reals [by piecewise constant functions](https://en.wikipedia.org/wiki/Lebesgue_integration#/media/File:Lebesgueintegralsimplefunctions_finer-dotted.svg). In this situation, I can ask how many constant pieces I need to use to obtain an approximation to the function within \\\\(\\\\epsilon = 1/n\\\\) of its true value, and hence measure the complexity of the function of the growth rate of that number as \\\\(n\\\\) increases. You could call this **integration complexity**, since the complexity determines how quickly the function's Lebesgue integral converges.\n\nEven for this notion of simple function, there are choices to make. Since measurable subsets are generated from open (or closed) intervals, we could insist that the simple functions be constructed from indicator functions of intervals, or we could allow arbitrary measurable subsets. Another consideration is whether we require that the function is approximated at all input values or merely \"almost everywhere\", the latter being a notion more compatible with the tools of measure theory; for continuous functions there is no difference.\n\nAs another consideration, we might wonder if the integration complexity of a function is invariant under affine transformations (that is, under translations and rescaling), following the geometric intuition that congruent shapes in the plane should behave in similar ways. The complexity is already invariant at the level of individual approximations under horizontal translation and scaling, since we can translate and rescale an optimal sum of simple functions to obtain an optimal sum with the same number of terms for a function so transformed. For vertical translation, we at worst need to add a single simple function (a multiple of the indicator function for the domain of definition, assuming it is sufficiently nice), and since growth rates are insensitive to constant changes, this makes no difference. Vertical scaling is more subtle: a simple function \\\\(s\\\\) with \\\\(|s(x) - f(x)| < 1/n\\\\) rescales to a worse approximation when the scale factor is larger than 1: \\\\(|ks(x) - kf(x)| = |k||s(x) - f(x)| < |k|/n \\\\). However, growth rates are coarse enough to absorb this kind of discrepancy as a contribution to the constant bounding factor, with the result that integration complexity is invariant under affine transformations! The cost of this invariance is that all simple functions have a complexity of \\\\(O(1)\\\\), although this too justifies the choice of name, since simple functions are those whose Lebesgue integral can be computed exactly with a finite sum.\n\n### Power series, Fourier series\n\nDepending on the nature of the problem, it may be more interesting to use other classes of functions to approximate the function and thereby measure its complexity. Power series, including Taylor series, are a very well-studied example of this. Given a function expressed as a power series, I can measure its complexity in terms of the growth rate of the coefficients, where now the types of functions which I encounter as growth rates will typically be sub-constant. Alternatively, I could transform these coefficients into a measure of the rate of convergence, defining the complexity of a series \\\\(\\\\sum_{j=0}^{\\\\infty}a_j x^j\\\\) defined over some domain \\\\(D\\\\) around \\\\(0\\\\), to be the asymptotic growth rate of the function \\\\(n \\\\mapsto \\\\min\\\\{N \\\\mid \\\\sum_{j = N}^{\\\\infty} a_jx^j < 1/n \\\\text{ in }D\\\\}\\\\).\n\nAs ever, there are alternative choices available here. I could measure the growth rate of the absolute convergence (replacing the summands with their absolute values in the last definition), for example. A natural feature of these complexity measures is that they are invariant under changing a finite number of coefficients, or equivalently under adding a polynomial to the power series. We also have invariance under affine transformations, comparably to the last subsection, as long as we also translate and scale the domain appropriately.\n\nOf course, power series are not the only way to decompose a function. We can also look at Fourier series, decomposing a function into sine and/or cosine functions, and construct similar definitions of complexity from the coefficients of such an expansion. In this case, the complexity being measured will be geometric in flavour: a function which is low in \"Fourier complexity\" is well-described by its low-frequency components, whereas higher complexity means that the high-frequency terms play an increasingly important role in the function's behaviour. Any finite sum of (discrete frequency) sine waves will have constant complexity. There is also a [relationship](https://en.wikipedia.org/wiki/Convergence_of_Fourier_series#Uniform_convergence) between the convergence rate of the Fourier decomposition of a function over an interval and the differentiability of the function.\n\nIt is interesting to contrast the notions of complexity arising from the two decompositions above. They are not directly comparable: a straight line has minimal power-series complexity but non-trivial Fourier complexity, and dually a simple sine wave has interesting power-series complexity but trivial Fourier complexity. This illustrates once again how complexity measures depend on our choices of basic building blocks or operations. There are, of course, yet other decompositions of functions that one might consider!\n\n### Functions on spaces\n\nI have talked extensively about functions on the reals. Much of the above can be extended directly to functions on the complex numbers. Only a little extra work is needed to define complexity measures for (real- or complex-valued) functions on higher dimensional spaces, more specifically on domains in \\\\(\\\\mathbb{R}^n\\\\), since we can define higher-dimensional simple functions, polynomials and harmonic waves easily, typically as products of the lower-dimensional ones. Some decisions need to be made, of course: for simple functions, are only rectangles allowed, or are indicator functions for more general measurable subsets acceptable? I won't examine the consequences of such choices here.\n\nWith these choices established, we can now construct complexity measures for functions defined on more exotic spaces. For example, if we are given a manifold \\\\(M\\\\) which has a finite decomposition as a gluing of subspaces of \\\\(\\\\mathbb{R}^n\\\\), a function \\\\(M \\\\to \\\\mathbb{R}\\\\) decomposes into functions defined on those subspaces, whose complexity we know how to measure! In keeping with the methods discussed earlier in the post, we therefore define the complexity of such a function as the minimal sum across minimal decompositions of \\\\(M\\\\) of the complexity of the restricted functions. We can do something similar for CW-complexes, where the rigid simplicity of the components involved (disks) may afford a finer-grained notion of complexity.\n\nIt's surely possible to go further and define the complexity of functions between manifolds in terms of decompositions, but the higher you go in the definitions, the more work you have to do to ensure that your lower-level measures of complexity are invariant enough to combine together nicely into a well-defined (or interesting) higher-level measure of complexity.\n\nConclusions\n-----------\n\nMy hope is that this post opens the door to complexity for others. Here on LessWrong especially, people often make reference to some established notions of complexity including Kolmogorov complexity, but it sometimes seems to me that a more tailored measure of complexity would better reflect the intuition they are trying to formalize.\n\nA more ambitious hope of mine is that people will take some of these ideas and run with them. The most exciting possibility as I see it is a formal exploration of the relationships between complexity measures in different settings and their relationships with other invariants. Such work might form the foundation for new approaches to attack the hard open problems in computational complexity theory.\n\n### Coming soon...\n\nFor the purposes of my goal-directedness project, the next step is to use the ideas I've explored here in order to build a measure of complexity for goal-directedness. This will require me finally making some choices which I've been putting off for the whole project, but it will also get me pretty close to some concrete answers.\n\nI would also like to examine *relative complexity*, a topic which will be directly relevant to measuring the complexity of explanations, and which I think will be clarified by the view I've taken on complexity in this post.\n\n### Further References\n\n*The primary inspiration for this post was J. M. Landsberg's book* Geometry and Complexity Theory*, which was gifted to me by my postdoc advisor Thomas Seiller (thanks Thomas!). The book is about* **algebraic complexity theory***, a domain which applies algebraic geometry to computing (or at least bounding) the complexity of algebraic problems. While it doesn't discuss complexity theory at the level of generality that I have attempted to here, it was enough of a departure from the computational complexity notions that I had encountered previously to give me a new perspective on what complexity can (or should) mean.*\n\n*For Kolmogorov complexity, I found Marie Ferbus-Zanda and Serge Grigorief's paper* [Kolmogorov complexity in perspective](https://hal.archives-ouvertes.fr/hal-00201578/file/KolmogorovPerspective.pdf) *instructive.*\n\n*For computational complexity, I used some fragments of Sanjeev Arora and Boaz Barak's book* [Computational Complexity: A Modern Approach](https://theory.cs.princeton.edu/complexity/)*, although I could benefit from a deeper reading of that.*\n\n*Chatain's LW post on* [*Occam's Razor and the Universal Prior*](https://www.lesswrong.com/posts/RKfg86eKQuqLnjGxx/occam-s-razor-and-the-universal-prior) *was useful in understanding how K-complexity arises in Bayesian formalisms.*\n\n1.  ^**[^](#fnrefzuffz4gk6d)**^\n    \n    The word \"structure\" here is intentionally quite generic. The structure can be a mathematical object (a space, an algebra, a function...), or a computational object (an algorithm, a solution to a problem...).\n    \n2.  ^**[^](#fnrefs0tz8fzy3j)**^\n    \n    See the right-most column in the Multiplication entry of the table; long multiplication takes \\\\(O(n^2)\\\\) operations, meaning that the number of operations is approximately proportional to the square of the number of digits in the numbers being multiplied (assuming here that the numbers are of the same order of magnitude), but we can do much better: the first improvement was discovered by [Karatsuba](https://en.wikipedia.org/wiki/Karatsuba_algorithm) and reduces the number of basic multiplications needed to multiply 1000-digit numbers by a factor of 17.  \n      \n    A similar story is the surprising existence of more efficient algorithms for matrix multiplication, which is one focus of the domain of *algebraic complexity theory*.\n    \n3.  ^**[^](#fnref9ysb8vvect)**^\n    \n    For consistency with the Turing machine example that follows, this should probably be \"write 0 and shift right\".\n    \n4.  ^**[^](#fnrefrqsk9dm8zrj)**^\n    \n    A *Turing machine* is a pretty famous abstract model of computation consisting of some *tapes,* which are unbounded collections of discrete cells which are either empty or contain a symbols (usually 0 or 1), a device called the *head* that can read from and write to the tapes, and a *program* instructing the head how to interpret the symbols it encounters. If this isn't familiar to you, almost any programming language will do for the purposes of understanding Kolmogorov complexity.^[\\[19\\]](#fnvdu7ll1gawb)^ I can be informal about this, and about the obviously underspecified parameters in the description of the Turing machine, such as the number of tapes and their shape (does it go off to infinity in both directions or just one?), the language/symbols which the machine can read and write, and so on, thanks to [the result I quote later on](https://www.lesswrong.com/posts/jgYGZD2zRK6nncJd5/goal-directedness-tackling-complexity#Invariance_at_the_cost_of_individuality).\n    \n5.  ^**[^](#fnrefhxlvvjuek2c)**^\n    \n    Amusingly, finding the most efficient algorithm to produce a given output in various programming languages is a pastime with a [significant community around it](https://codegolf.stackexchange.com/).\n    \n6.  ^**[^](#fnref6nvynucnkac)**^\n    \n    A **field** is a ring where we can also divide by elements other than 0. I didn't include it in the list of algebraic structures, because the \"other than zero\" part turns out to prevent the existence of free fields!\n    \n7.  ^**[^](#fnrefpkidne7z5k)**^\n    \n    The more famous version of this example would be the inductive definition of the natural numbers in Peano arithmetic: any natural number can be expressed by applying the successor (\"add 1\") function to 0 the corresponding number of times. However, since the variety of algebras with just a single unary operation isn't as widely studied to my knowledge.\n    \n8.  ^**[^](#fnreflesmosn8hwb)**^\n    \n    Formally speaking, \\\\((aa)a\\\\) and \\\\(a(aa)\\\\) could be considered distinct elements were it not for one of the axioms of semigroups which I didn't mention, namely *associativity*. I mention this here to highlight how many of the formal details are missing from this account. I'm trying to paint you an intuitive picture, but if you're curious about what's under the hood, I would be more than happy to discuss it in the comments!\n    \n9.  ^**[^](#fnrefjnuvd11437p)**^\n    \n    Note that the generation procedure here is very sensitive to the type of structure we are considering: a ring generated from the presentation \\\\(\\\\langle a,b \\\\mid aa = a, bb = b, ab = ba \\\\rangle\\\\) would be infinite, since we have imposed no restrictions on sums of elements.\n    \n10.  ^**[^](#fnref7ntkldpbzzc)**^\n    \n    I don't know whether this notion is typically studied under the banner of complexity theory. As I mentioned in the introduction, I'm deliberately seeking an inclusive definition of complexity here.\n    \n11.  ^**[^](#fnrefibznwevdaai)**^\n    \n    Of course, it can also be extended to algebras which only admit infinite presentations, but infinite cardinals are a lot coarser than finite ones, and one has to start worrying about set-theoretic foundations, whereas the finite notions are a lot easier to treat constructively.\n    \n12.  ^**[^](#fnrefa70tcdq76js)**^\n    \n    I implicitly assume in this argument that I'm working with Turing machines taking inputs in binary, which is what's needed to make the \"up-to-constant' claim hold; if the Turing machines accept different alphabets, then I need to encode each into the other for the input to make sense, and this can modify the length of programs by a scale factor. For the conclusions regarding asymptotic growth rates that I mention, this subtlety is unimportant.\n    \n13.  ^**[^](#fnrefj3ige8zh36)**^\n    \n    I've used here, [Big Oh](https://en.wikipedia.org/wiki/Big_O_notation) and [Little Oh](https://en.wikipedia.org/wiki/Big_O_notation#Little-o_notation) notation here. This is a compact way to express asymptotic bounds on functions; I also use [Big Theta notation](https://en.wikipedia.org/wiki/Big_O_notation#Family_of_Bachmann%E2%80%93Landau_notations) later on.\n    \n14.  ^**[^](#fnrefiliok2wcsl)**^\n    \n    The point of this paragraph is to disillusion the reader about the existence of any strong existing connection between \"complexity\" and \"complex numbers\". Just as elsewhere in maths and science, it's entirely possible that a clever application of complex numbers^[\\[17\\]](#fnt70obtdizdn)^ could solve some problems in complexity theory, but I maintain that complex numbers do not constitute good candidates for complexity values.\n    \n15.  ^**[^](#fnrefnwvtmh0kz58)**^\n    \n    Up until this point, it has been the case that the complexity value directly measures a most efficient construction, presentation or decomposition of an object. In allowing more general structures, there arises the possibility of situations in which there is no minimizer: we could have a sequence of increasingly efficient constructions with no minimum, or a collection of minimal constructions which are incomparable in the ordering. This is not intrinsically problematic, but may require some care in general arguments about complexity.\n    \n16.  ^**[^](#fnrefv5z1wep5ak)**^\n    \n    Amusingly, the syntax-semantics presentation of complexity measures is sufficiently precise for us to begin asking questions about the **complexity of complexity measures**... I'll leave to some other brave soul the pleasure of untangling that ouroboros.\n    \n17.  ^**[^](#fnreft70obtdizdn)**^\n    \n    Although I have dismissed complex-valued complexity measures, when studying maps between projective spaces, complex values can help achieve a more complete global picture. This train of thought leads down a rabbit hole which I shall not explore here, but there is precedent for taking this idea seriously: recall that complex values were useful to electrical engineers in uniting resistance and capacitance, for example.\n    \n18.  ^**[^](#fnref50sp0qm4xg8)**^\n    \n    For the logarithmic time class to contain any interesting problems, we need to impose the assumption that moving along the input (such as moving any distance along the input tape of a Turing machine before reading) takes constant time, since otherwise the time taken to move is already linear in the input length. For example, assuming the input  \\\\(n > 0\\\\) is provided in binary with a leading 1, the function \\\\(n \\\\mapsto \\\\lfloor\\\\log\\_2(\\\\log\\_2(n)+1)\\\\rfloor\\\\) is a member of DLOGTIME since we can determine its value by initialising the output at 0, checking for the presence of a digit at positions \\\\(2^k\\\\) of the input for increasing \\\\(k \\\\geq 1\\\\), adding one if a digit is found and halting otherwise.\n    \n19.  ^**[^](#fnrefvdu7ll1gawb)**^\n    \n    Arora and Barak list Turing machines, lambda calculus, cellular automata, pointer machines, bouncing billiards balls and Conway’s Game of life as abstract models of complexity which can simulate one another and hence are *Turing complete*."
    },
    "voteCount": 4,
    "forceInclude": true
  },
  {
    "_id": "e936w9JzDP4WqQjcc",
    "url": null,
    "title": "Goal-directedness: relativising complexity",
    "slug": "goal-directedness-relativising-complexity",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "Goal-Directedness"
      },
      {
        "name": "Kolmogorov Complexity"
      },
      {
        "name": "World Modeling"
      }
    ],
    "tableOfContents": {
      "sections": [
        {
          "title": "Relativising Kolmogorov complexity",
          "anchor": "Relativising_Kolmogorov_complexity",
          "level": 1
        },
        {
          "title": "Conditional complexity",
          "anchor": "Conditional_complexity",
          "level": 2
        },
        {
          "title": "Extension complexity",
          "anchor": "Extension_complexity",
          "level": 2
        },
        {
          "title": "Modification complexity",
          "anchor": "Modification_complexity",
          "level": 2
        },
        {
          "title": "Oracular complexity",
          "anchor": "Oracular_complexity",
          "level": 2
        },
        {
          "title": "What is relativisation really?",
          "anchor": "What_is_relativisation_really_",
          "level": 2
        },
        {
          "title": "An algebraic example",
          "anchor": "An_algebraic_example",
          "level": 1
        },
        {
          "title": "Relative complexity of group elements",
          "anchor": "Relative_complexity_of_group_elements",
          "level": 2
        },
        {
          "title": "Conditional complexity",
          "anchor": "Conditional_complexity1",
          "level": 2
        },
        {
          "title": "Relativising to subsets",
          "anchor": "Relativising_to_subsets",
          "level": 2
        },
        {
          "title": "Conclusions",
          "anchor": "Conclusions",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        },
        {
          "anchor": "comments",
          "level": 0,
          "title": "No comments"
        }
      ],
      "headingsCount": 13
    },
    "contents": {
      "markdown": "*This is the fourth post in my Effective-Altruism-funded project aiming to deconfuse goal-directedness. Comments are welcomed. All opinions expressed are my own, and do not reflect the attitudes of any member of the body sponsoring me. The funding has come to an end, but I expect to finish off this project as a hobby in the coming months.*\n\nMy [previous post](https://www.lesswrong.com/posts/jgYGZD2zRK6nncJd5/goal-directedness-tackling-complexity) was all about complexity, and ended with an examination of the complexity of functions. In principle, I should now be equipped to explicitly formalise the [criteria I came up with](https://www.lesswrong.com/posts/KJPRC3cgtxSXpZEQZ/goal-directedness-exploring-explanations) for evaluating [goal-based explanations](https://www.lesswrong.com/posts/oZCeun2v3Xd3ncrHt/goal-directedness-imperfect-reasoning-limited-knowledge-and#Layered_explanations). However, several of the structures whose complexity I need to measure take the form of transformations from one structure of a given type to another. This brings us into the domain of *relative complexity.* I have chosen to present this in a separate post, not just because of the daunting length of my last post, but also because I think this is a subject which this community would benefit from some explicit clarification of. In future posts I shall **finally** put all of the pieces together and get to the bottom of this goal-directedness business.\n\nRecall that last time that I described complexity as *a quantity assigned to a structure that measures how many simple pieces are needed to construct it*. The general concept which is under scrutiny this time is a variant of the above, where we consider instead *a quantity assigned to a structure that measures how many simple pieces are needed to construct it **from a given resource or starting point***. A source of confusion lies in the fact that there are multiple ways to relativise complexity, which is why a closer examination is needed.\n\nIn this post I'll be discussing a few implementations on relativised notions of complexity.\n\nRelativising Kolmogorov complexity\n----------------------------------\n\n### Conditional complexity\n\nIn this subsection I'll be using standard notation and definitions from Ming Li and Paul Vitányi's [textbook](https://link.springer.com/book/10.1007/978-3-030-11298-1),^[\\[1\\]](#fnz6an60uitzk)^ which I will subsequently extend somewhat. They use the convention that the complexity measure is indexed by a *partial computable function* rather than a given Turing machine which computes it, which makes sense, since the details of the Turing machine are irrelevant in the computation of complexity beyond its input-output behaviour.\n\nThe definition of K-complexity for individual strings \\\\(s\\\\) which we saw last time involved a choice of universal Turing machine \\\\(U\\\\), computing partial function \\\\(\\\\phi: \\\\{0,1\\\\}^* \\\\to \\\\{0,1\\\\}^*\\\\), say, and measured the length of the shortest input \\\\(p\\\\) (encoding a program) such that \\\\(\\\\phi(p) = s\\\\). This gave the plain Kolmogorov complexity, which I'll now denote as \\\\(C_{\\\\phi}(s) := \\\\min\\\\{\\\\ell(p) \\\\mid \\\\phi(p) = s\\\\}\\\\).\n\n**Conditional Kolmogorov complexity** allows an auxiliary string \\\\(y\\\\) to be passed as a \"free\" input to the program, so that the corresponding complexity is \\\\(C_{\\\\phi}(s \\\\mid y) := \\\\min\\\\{\\\\ell(p) : \\\\phi(\\\\langle y, p \\\\rangle) = s \\\\}\\\\), where \\\\(\\\\langle y, p \\\\rangle\\\\) is the string obtained from \\\\(y\\\\) and \\\\(p\\\\) by concatenating them and adding a prefix consisting of \\\\(\\\\ell(y)\\\\) copies of \\\\(1\\\\) followed by a \\\\(0\\\\) (so that the Turing machine encoding \\\\(\\\\phi\\\\) can reliably separate the concatenated inputs). For conciseness, I'll denote \\\\(\\\\phi(\\\\langle y, p \\\\rangle)\\\\) as \\\\((y)p\\\\), interpreting \\\\(y\\\\) as an input to the program \\\\(p\\\\) (with \\\\(\\\\phi\\\\) playing the role of an interpreter or compiler).\n\nThis notion of conditional complexity contains some distinct possibilities which I alluded to in the introduction and would like to separate out; hopefully the algebraic example in a later section will help to clarify the distinct cases I'll highlight here.\n\nConsider a situation where \\\\(s = y\\\\). For many such cases, the shortest program \\\\(p\\\\) producing \\\\(s\\\\) given \\\\(y\\\\) is \\\\((y)p\\\\) := print\\\\((y)\\\\). Similarly, if \\\\(s\\\\) contains many copies of \\\\(y\\\\), we can use \"print(-)\" to insert those copies into the output being produced. On the other hand, if \\\\(s\\\\) has nothing to do with \\\\(y\\\\), the most efficient program will be one that simply ignores the extra input and produces \\\\(s\\\\) directly. In this way, the input \\\\(y\\\\) can be thought of as a *resource* for obtaining the output \\\\(s\\\\).\n\n### Extension complexity\n\nA special case of the above is when \\\\(y\\\\) is a prefix of \\\\(s\\\\), but where the remainder of \\\\(s\\\\) is not easily compressible^[\\[2\\]](#fn6did9fqm0ef)^ using \\\\(y\\\\). As long as \\\\(y\\\\) has non-trivial length, the most efficient program for producing \\\\(s\\\\) might be of the form \"print\\\\((y)\\\\) and then append \\\\(()q\\\\)\", where \\\\(q\\\\) is a program producing the suffix of \\\\(s\\\\) with no input. In this situation we are using \\\\(y\\\\) as a *starting point* for obtaining the output \\\\(s\\\\). I can refine this into an independent definition by removing the cost of the initial print command: \\\\(C'_{\\\\phi}(s \\\\mid y) := \\\\min\\\\{\\\\ell(q) : y \\\\,; \\\\phi(q) = s\\\\}\\\\), where \\\\(y\\\\, ;\\\\phi(q)\\\\) is the concatenation of \\\\(y\\\\) and the output of \\\\(\\\\phi\\\\) at \\\\(q\\\\). With this (non-standard) definition we can no longer use \\\\(y\\\\) as a resource, and we can only obtain strings which extend \\\\(y\\\\) (all other strings having infinite complexity). I hence call this the **extension complexity**.^[\\[3\\]](#fn759pommwh1p)^\n\n### Modification complexity\n\nAn alternative option, for a set-up where we work with Turing machines having separate input and output tapes, say, is to *initialize the output tape with a given string *\\\\(y\\\\). We could also impose further constraints on the model, such as proposing that the machine cannot read from the output tape, only write to it, so that the initial output cannot be used as a resource in the sense discussed above. I'll choose a universal machine set up this way, \\\\(U\\\\), and write \\\\(U_y\\\\) to indicate that this machine is initialised with \\\\(y\\\\) in the output tape. Then we get a notion which I'll call **modification complexity, **\\\\(C\\_U(s \\\\leftarrow y):= \\\\min\\\\{\\\\ell(p) : U\\_y(p) = s\\\\}\\\\), which measures the shortest program for generating the string \\\\(s\\\\) with initial output \\\\(y\\\\).\n\nAssuming that the machine can also delete symbols in the output, all strings have a finite modification complexity, so this is a genuinely different measure than the extension complexity.\n\n### Oracular complexity\n\nI explained last time that we can equip a Turing machine with an *oracle*, which can perform some decision problems \"for free\", or rather in a single step of computation. The details are a little technical. Given a subset \\\\(A \\\\subseteq \\\\{0,1\\\\}^*\\\\), we can define a Turing machine with a reference tape which includes an oracle query state which writes a \\\\(1\\\\) to the output tape if the content of the reference tape is a member of \\\\(A\\\\), and writes a \\\\(0\\\\) otherwise. Naming the oracle after the subset defining it, we obtain a class of \\\\(A\\\\)-computable partial functions, and corresponding \\\\(A\\\\)-universal Turing machines, with a necessarily different enumeration of the programs than for ordinary Turing machines. Nonetheless, given the partial function \\\\(\\\\phi\\\\) computed by such a universal machine, we can duplicate the definitions given above with all of the same conclusions.\n\nComparing complexities arising from different oracles might be interesting, but it's beyond the scope of this post.\n\n### What is relativisation really?\n\nWe saw [last time](https://www.lesswrong.com/posts/jgYGZD2zRK6nncJd5/goal-directedness-tackling-complexity#Complexity_within_a_structure) that even after fixing a collection of objects containing a particular object, there may still be many choices to make before a complexity measure is fully specified over that collection (and hence on that object). On the other hand, we also saw that some choices are natural to some extent, in that they themselves minimize some higher measure of complexity. Whether \"natural\" or not, there may be a default choice of complexity measure for some collection of objects. Given such a situation, the various types of relative complexity are ways of expressing modifications to the default complexity measure, using all of the features involved (implicitly or explicitly) in the definition of the default complexity measure. We can interpret the above examples in these terms:\n\n*   The conditional complexity \\\\(C(s \\\\mid y)\\\\) provides \\\\(y\\\\) as a basic building block, at a cost which is typically lower than its complexity, the difference being that between generating \\\\(y\\\\) and simply reading \\\\(y\\\\).^[\\[4\\]](#fnfaapbt82t9)^ Since the operations available to a Turing machine are very expressive, the default is to have no building blocks available.\n*   The extension complexity \\\\(C'(s \\\\mid y)\\\\) effectively reduces the collection of objects under scrutiny to those which have \\\\(y\\\\) as a prefix (all others having infinite complexity). The default domain for K-complexity is the set of all strings formed from a given character set; this is a natural variant of that complexity measure for that restricted domain.^[\\[5\\]](#fnj7oh30hmla)^\n*   The modification complexity \\\\(C_U(s \\\\leftarrow y)\\\\) takes \\\\(y\\\\), rather than the empty string, as a starting point. Here the default is to start from the empty string.^[\\[6\\]](#fnszyb4gffx2s)^\n*   Adding an oracle amounts to changing the allowed basic operations and hence changing complexity measures, but in a way which is (or may be) hidden from the final definition of the complexity of a string. The default operations which an oracle-free Turing machine can perform vary, but they always fall into a class of basic \"effective\" arithmetic operations; a typical oracle adds an operation which is either not effectively computable, or not efficiently computable, to the default operations.\n\nIt's worth noting that the cost of any of these contributions to complexity can vary, either because of the explicit implementation of using the resources or operations being provided, or by artificially attaching a cost to such operations. I explored variations in costs [last time](https://www.lesswrong.com/posts/jgYGZD2zRK6nncJd5/goal-directedness-tackling-complexity#Symbolic_complexity_values). The takeaway is that I'm only scratching the surface of the range of notions of relative complexity that are possible. It's time for some more instances in a different setting!\n\nAn algebraic example\n--------------------\n\n### Relative complexity of group elements\n\nRecall the *word length complexity measure* on elements [in](https://www.lesswrong.com/posts/jgYGZD2zRK6nncJd5/goal-directedness-tackling-complexity#Elements_of_algebras) a [group](https://www.lesswrong.com/posts/jgYGZD2zRK6nncJd5/goal-directedness-tackling-complexity#Complexity_in_algebra) which I presented last time. Explicitly, I am given some generators \\\\(a\\_1,\\\\dotsc,a\\_n\\\\) of a group \\\\(G\\\\); the word length complexity \\\\(C_G(w)\\\\) of an element \\\\(w \\\\in G\\\\), is the length of a shortest presentation of \\\\(w\\\\) as a product of the generators.^[\\[7\\]](#fnmw3fyqko2lb)^\n\nIn this and other constructions of elements in algebras, we take the \"empty\" construction as the starting point by default, such that the identity element always has a complexity of *0*, being identified with the empty product of generators. This is a natural choice insofar as it is the only choice which can be made independently of any further details of the structure of the group. However, such a choice may not always be available. To illustrate this, I'll remind you how algebraic objects can *act* on other mathematical objects, including sets, spaces or other algebraic objects.\n\nFor concreteness, I'll specifically talk about (right^[\\[8\\]](#fnki9k8yow4t)^) [group actions](https://en.wikipedia.org/wiki/Group_action). Suppose I have a group \\\\(G\\\\) as above acting on a set \\\\(A\\\\), which formally means that each element \\\\(a \\\\in A\\\\) and \\\\(g \\\\in G\\\\) determines an element \\\\(a \\\\cdot g \\\\in A\\\\) in such a way that \\\\(a \\\\cdot (gh) = (a \\\\cdot g) \\\\cdot h\\\\) for all elements \\\\(h \\\\in G\\\\). Given two elements \\\\(a,b \\\\in A\\\\), we can measure the complexity of \"getting to \\\\(b\\\\) from \\\\(a\\\\),\" denoted \\\\(C_A(b\\\\,\\\\vert \\\\, a)\\\\), to be the complexity of the (unique) element \\\\(h\\\\) such that \\\\(b = a \\\\cdot h\\\\), if it exists, and define the complexity to be infinite otherwise.\n\nThe crucial feature of this situation which differs from the complexity measures we saw last time is that we must choose not one but *two* elements of \\\\(A\\\\), and there is in general no distinguished choice for either element which might allow us to eliminate one of the choices. A sensible reaction to this example is to entertain also a relative version of complexity of elements in an algebra, where we may choose the starting point for the construction to be something other than the identity element. This is justified in this particular case by the fact that the group \\\\(G\\\\) acts on its underlying set, which I'll denote by \\\\(\\[G\\]\\\\), by multiplication, and hence as a special case of the above we can define the complexity of \"getting to \\\\(w\\\\) from \\\\(g\\\\),\" denoted \\\\(C_G(w \\\\leftarrow g)\\\\), to be the complexity of the (unique) element \\\\(h\\\\) such that \\\\( w = gh\\\\); in this case such an element always exists, and is equal to \\\\(g^{-1}w\\\\). \n\nGeneralizing to other algebraic contexts, we may no longer have uniqueness or existence of the element \\\\(h\\\\). This may be the case in a monoid, for example. In that case, we must minimize the complexity across elements satisfying the equation, which makes this relativized complexity measure more interesting.\n\nI have chosen notation consistent with that for modification complexity introduced above, since one way of interpreting \\\\(C_G(w \\\\leftarrow g)\\\\) is as \"the cost of the most efficient way to modify \\\\(g\\\\) to obtain \\\\(w\\\\)\", and because it amounts to choosing a starting point for the construction other than the default of the identity element. We evidently have \\\\(C\\_G(w \\\\leftarrow 0) = C\\_G(w)\\\\). As such, for any third element \\\\(h \\\\in G\\\\), we have a general inequality \\\\(C\\_G(w  \\\\leftarrow g) + C\\_G(g \\\\leftarrow h) \\\\geq C_G(w \\\\leftarrow h)\\\\), which is a directed version of the triangle inequality, and taking \\\\(h\\\\) to be the identity element we get the special case, \\\\(C\\_G(w  \\\\leftarrow g) + C\\_G(g) \\\\geq C_G(w)\\\\).\n\nThe above is the discrete case, but it works just as well for the continuous case: the topological group \\\\(SO(2)\\\\) of rotations in the plane is homeomorphic as a space to the circle \\\\(S^1\\\\). A conventional identification of \\\\(SO(2)\\\\) with the unit circle in the plane sends the identity element (\\\\(0\\\\) rotation) with the point \\\\((1,0)\\\\), but any other choice would be valid. If we measure the complexity of a rotation as the size of the smallest angle of rotation from the identity producing it, the relative complexity \\\\(C_{SO(2)}(r\\_1 \\\\leftarrow r\\_2)\\\\) of two rotations is the smaller angle separating them as points on a (uniformly parametrized) circle.\n\n### Conditional complexity\n\nAlternatively, we could consider the \"cost of building \\\\(w\\\\) using \\\\(g\\\\)\", in the sense of adding \\\\(g\\\\) to the set of generators. The resulting complexity measure depends on how much each copy of \\\\(g\\\\) costs^[\\[9\\]](#fnrecu8yvbesj)^. If the cost is greater than or equal to \\\\(C_G(g)\\\\), the complexity measure collapses to the original complexity measure in terms of the generators \\\\(a\\_1,\\\\dotsc,a\\_n\\\\). At the other extreme, if the cost is zero, the complexity is a measure of how close \\\\(w\\\\) is to a power of \\\\(g\\\\); we shall return to this case below. Between these is the case where the cost is one, where we recover the ordinary complexity of \\\\(w\\\\) after adding \\\\(g\\\\) to the set of generators. As *ad hoc* notation, if the cost of each copy of \\\\(g\\\\) is \\\\(c\\\\), I'll write \\\\(E_{g:c}(w)\\\\) for the corresponding **conditional complexity** of \\\\(w\\\\). For any value of \\\\(c\\\\), we have \\\\(E_{g:c}(w) \\\\leq C_G(w)\\\\).\n\nIf the relative complexity \\\\(C_G(w \\\\leftarrow g)\\\\) treats \\\\(g\\\\) as a new starting point for constructions, the conditional complexity treats \\\\(g\\\\) as an extra building block which we can use in the construction, without shifting the starting point; it's altering the default choice of generators. Unlike the default of starting from the identity, there may be [no default choice](https://www.lesswrong.com/posts/jgYGZD2zRK6nncJd5/goal-directedness-tackling-complexity#Fragility_of_complexity_measures) of generators for a given group, so remember that this notation assumes a choice has been established previously. Again, relative complexity measures are really just a way of expressing complexity measures by comparison with some pre-existing (default) choice.\n\n### Relativising to subsets\n\nRather than individual elements, we could consider subsets which we are allowed to freely choose from. Let \\\\(S \\\\subseteq G\\\\) be a subset; then we can define the relative complexity \\\\(C_G(w \\\\, | \\\\, S)\\\\) to be \\\\(\\\\min \\\\{C_G(h) \\\\mid \\\\exists g \\\\in S, \\\\, w = gh\\\\}\\\\). As particular cases relating to the previous definitions, we have that \\\\(C\\_G(w \\\\, | \\\\, \\\\{ g \\\\}) = C\\_G(w \\\\, | \\\\, g )\\\\), and if \\\\(\\\\langle g \\\\rangle\\\\) is the subgroup of \\\\(G\\\\) generated by \\\\(g\\\\), then \\\\(C\\_G(w \\\\, | \\\\, \\\\langle g \\\\rangle) = E\\_{g:0}(w)\\\\).\n\nWe can also define the extension complexity \\\\(E_{S:c}(w)\\\\) in the evident way. Indeed, if \\\\(S\\\\) is actually a subgroup of \\\\(G\\\\), or if we replace \\\\(S\\\\) with the subgroup it generates, the \\\\(0\\\\)-cost version of the extension complexity coincides with the relative complexity in the quotient right \\\\(G\\\\)-action \\\\(S \\\\backslash G\\\\), or as an equation, \\\\(E_{S:0}(w) = C_{S \\\\backslash G}(Sw \\\\, | \\\\, S)\\\\).\n\nI'm hoping that if you have some experience in algebra, you might be able to come up with some further examples of relativised complexity measures for yourself, or to identify how this example can be extended by analogy to another relativised version of Kolmogorov complexity.\n\nConclusions\n-----------\n\nIf the equations and inequalities I've sprinkled through this post aren't enlightening, the general takeaway from them is that relative notions of complexity allow us to interpolate between complexity measures, or see one type of complexity measure as an extreme case of another. They also allow us to acknowledge and adjust parameters which a default point of view on a setting would take for granted.\n\nMine is not a conventional take on complexity. Complexity seems too often to be assumed as an intrinsic property of an object of study, with various desirata attached regarding how this quantity should behave. I hope that I have managed to illustrate through sheer breadth of possibility that this is a reductive point of view. There is a vast web of notions of complexity and a deep foundation of assumptions underlying any given complexity measure. Understanding the greater structure should promote understanding of any particular choice of complexity one selects.\n\n1.  ^**[^](#fnrefz6an60uitzk)**^\n    \n    Do not mistake this for an endorsement of said textbook. The second chapter opens with the sentence, \"The most natural approach to defining the quantity of information is clearly to define it in relation to the individual object \\[...\\] rather than in relation to a set of objects from which the individual object may be selected,\" a sentiment which I am strongly opposed to (as my exposition in the present and previous post hopefully make apparent). Having expressed this opinion, however, the authors go on in a later paragraph to set up their notation with the sentence, \"Denote the set of objects by \\\\(S\\\\), and assume some standard enumeration of objects \\\\(x\\\\) be natural numbers \\\\(n(x)\\\\),\" in blatant opposition to their \"natural approach\"...\n    \n2.  ^**[^](#fnref6did9fqm0ef)**^\n    \n    I didn't introduce the notion of compressibility last time, but for this conditional case it amounts to saying that having \\\\(y\\\\) at our disposal does not shorten the length of the program required to output the suffix of \\\\(s\\\\).\n    \n3.  ^**[^](#fnref759pommwh1p)**^\n    \n    I would have called this \"prefix complexity\", but that is already the name of a variant of Kolmogorov complexity explained in Chapter 3 of Li and Vitányi's textbook, involving prefix-free codes.\n    \n4.  ^**[^](#fnreffaapbt82t9)**^\n    \n    It's hard to make the cost reduction precise, although I could probably provide some bounds on it. The recursive nature of Turing computation means that any string produced in the running of the program may subsequently be used by the program an effectively unlimited number of times with a corresponding similarly reduced complexity cost; providing \\\\(y\\\\) as a basic building block only negates the initial cost of generating the string. \n    \n5.  ^**[^](#fnrefj7oh30hmla)**^\n    \n    Digital files contain a *header* specifying some parameters of the file before the actual body of the file (the data you see when you open it) begins. If you write a program that outputs a file, that header will usually take a default value, so that the length of your programme is an upper bound on some instance of extension complexity.\n    \n6.  ^**[^](#fnrefszyb4gffx2s)**^\n    \n    Continuing the above example, you could write a program which produces a file starting from a template file, at which point the length of your program is an upper bound on the modification complexity from the template to the final file produced, now ignoring the file header.\n    \n7.  ^**[^](#fnrefmw3fyqko2lb)**^\n    \n    I'm aware that this is not great notation for the complexity, since it clashes with the notation conventionally used for [centralizers](https://en.wikipedia.org/wiki/Centralizer_and_normalizer) in group theory. Please invent original notation if you use this concept in your own work!\n    \n8.  ^**[^](#fnrefki9k8yow4t)**^\n    \n    If we instead used left actions, we would get a dual definition of relative complexity later on.\n    \n9.  ^**[^](#fnrefrecu8yvbesj)**^\n    \n    For the purposes of this post, I will assume that the costs are constant, although I mentioned in the last post that this need not be the case. Indeed, if we allow any function of the number of instances of \\\\(g\\\\) in the construction, we can also express the relative complexity \\\\(C_G(w \\\\leftarrow g)\\\\) as an instance of the resulting more general type of complexity measure.\n    \n10.  ^**[^](#fnrefxwnsq45drgm)**^\n    \n    This example is hiding some subtleties of formalization. I'm implicitly computing the relative complexity of the graphs as the minimum number of some basic operations (which include edge deletion and duplication) required to produce an output graph isomorphic to the target graph. The transformation I describe is hence not a graph homomorphism, and if I were to enforce the rule that transformations should be graph homomorphisms, then the resulting composite would in fact not by the identity homomorphism and hence would carry complexity greater than 0."
    },
    "voteCount": 2,
    "forceInclude": true
  },
  {
    "_id": "gebzzEwn2TaA6rGkc",
    "url": null,
    "title": "Deep Learning Systems Are Not Less Interpretable Than Logic/Probability/Etc",
    "slug": "deep-learning-systems-are-not-less-interpretable-than-logic",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "Transparency / Interpretability (ML & AI)"
      },
      {
        "name": "AI"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "There’s a common perception that various non-deep-learning ML paradigms - like logic, probability, causality, etc - are very interpretable, whereas neural nets aren’t. I claim this is wrong.\n\nIt’s easy to see where the idea comes from. Look at the sort of models in, say, Judea Pearl’s work. Like this:\n\n![](https://lh6.googleusercontent.com/uZy2qUZffJOAmUiMvjHTT4aLtRDfuq7djD1B0w-1mxwdzlC7LZASQof3OREbz4nxUbj1ceyVPVtdwnc0Awl1EZ4Lg2W-wYEmTWQrB31PxsBZA1lHEByQWds_JI9VToJwRZsXkGUZy_qd6RUwNA)\n\nIt says that either the sprinkler or the rain could cause a wet sidewalk, season is upstream of both of those (e.g. more rain in spring, more sprinkler use in summer), and sidewalk slipperiness is caused by wetness. The Pearl-style framework lets us do all sorts of probabilistic and causal reasoning on this system, and it all lines up quite neatly with our intuitions. It *looks* very interpretable.\n\nThe problem, I claim, is that a whole bunch of work is being done by the *labels*. “Season”, “sprinkler”, “rain”, etc. The *math* does not depend on those labels at all. If we *code* an ML system to use this sort of model, its behavior will also not depend on the labels at all. They’re just [suggestively-named LISP tokens](https://www.lesswrong.com/posts/fg9fXrHpeaDD6pEPL/truly-part-of-you). We could use the exact same math/code to model some entirely different system, like my sleep quality being caused by room temperature and exercise, with both of those downstream of season, and my productivity the next day downstream of sleep.\n\n![](https://lh5.googleusercontent.com/2S4wEnCY5QrLn1mUgslYDLXAF62LAi3CshrUjBNokK2TVKHxghqGt9Qir8SuQtLUD92XldP8DPv7OqM-uFxfwoSFux8lkOiqxC_ofnoGa4X61ZYLlU5Dja-oE6b8jsSMWB8lnw2MdX92o3WcRA)\n\nWe could just replace all the labels with random strings, and the model would have the same content:\n\n![](https://lh6.googleusercontent.com/H4vVXQuM44z5WLwyuxXZYrGcJ5g6Ek3YGH-Z8RYp21NOTjL1pe51mi-EPbr-v4kjluHKYH-ErFjFDWWVOi4WgYOcprwmkVTOh3txLgl_UTbA4G-G5k7y4H_V7ULJ1UR7Cq62Y-eZJe7p9XBFlQ)\n\nNow it looks a lot less interpretable.\n\nPerhaps that seems like an unfair criticism? Like, the causal model is doing some nontrivial work, but connecting the labels to real-world objects just isn’t the problem it solves?\n\n… I think that’s true, actually. But connecting the internal symbols/quantities/data structures of a model to external stuff is (I claim) exactly what interpretability is all about.\n\nThink about interpretability for deep learning systems. A prototypical example for what successful interpretability might look like is e.g. we find a neuron which robustly lights up specifically in response to trees. It’s a tree-detector! That’s highly interpretable: we know what that neuron “means”, what it corresponds to in the world. (Of course in practice single neurons are probably not the thing to look at, and also the word “robustly” is doing a lot of subtle work, but those points are not really relevant to this post.)\n\nThe corresponding problem for a logic/probability/causality-based model would be: take a variable or node, and figure out what thing in the world it corresponds to, ignoring the not-actually-functionally-relevant label. Take the whole system, remove the labels, and try to rederive their meanings.\n\n… which sounds basically-identical to the corresponding problem for deep learning systems.\n\nWe are no more able to solve that problem for logic/probability/causality systems than we are for deep learning systems. We can have a node in our model labeled “tree”, but we are no more (or less) able to check that it *actually robustly represents trees* than we are for a given neuron in a neural network. Similarly, if we find that it does represent trees and we want to understand how/why the tree-representation works, all those labels are a distraction.\n\nOne could argue that we’re lucky deep learning is winning the capabilities race. At least this way it’s *obvious* that our systems are uninterpretable, that we have no idea what’s going on inside the black box, rather than our brains seeing the decorative natural-language name “sprinkler” on a variable/node and then thinking that we know what the variable/node means. Instead, we just have unlabeled nodes - an accurate representation of our actual knowledge of the node’s “meaning”."
    },
    "voteCount": 70,
    "forceInclude": true
  },
  {
    "_id": "DwqgLXn5qYC7GqExF",
    "url": null,
    "title": "Godzilla Strategies",
    "slug": "godzilla-strategies",
    "author": "johnswentworth",
    "question": false,
    "tags": [
      {
        "name": "AI"
      },
      {
        "name": "Using AI to solve Alignment"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "> Clutching a bottle of whiskey in one hand and a shotgun in the other, John scoured the research literature for ideas... He discovered several papers that described software-assisted hardware recovery. The basic idea was simple: if hardware suffers more transient failures as it gets smaller, why not allow software to detect erroneous computations and re-execute them? This idea seemed promising until John realized THAT IT WAS THE WORST IDEA EVER. Modern software barely works when the hardware is correct, so relying on software to correct hardware errors is like asking Godzilla to prevent Mega-Godzilla from terrorizing Japan. THIS DOES NOT LEAD TO RISING PROPERTY VALUES IN TOKYO. It’s better to stop scaling your transistors and avoid playing with monsters in the first place, instead of devising an elaborate series of monster checks-and-balances and then hoping that the monsters don’t do what monsters are always going to do because if they didn’t do those things, they’d be called dandelions or puppy hugs.\n> \n> \\- James Mickens, [The Slow Winter](https://scholar.harvard.edu/files/mickens/files/theslowwinter.pdf)\n\n  \nThere’s a lot of AI alignment strategies which can reasonably be described as “ask Godzilla to prevent Mega-Godzilla from terrorizing Japan”. Use one AI to oversee another AI. Have two AIs debate each other. Use one maybe-somewhat-aligned AI to help design another. Etc.\n\nAlignment researchers discuss various failure modes of asking Godzilla to prevent Mega-Godzilla from terrorizing Japan. Maybe one of the two ends up much more powerful than the other. Maybe the two make an acausal agreement. Maybe the Nash Equilibrium between Godzilla and Mega-Godzilla just isn’t very good for humans in the first place. Etc. These failure modes are useful for guiding technical research.\n\n… but I worry that talking about the known failure modes misleads people about the strategic viability of Godzilla strategies. It makes people think (whether consciously/intentionally or not) “well, if we could handle these particular failure modes, maybe asking Godzilla to prevent Mega-Godzilla from terrorizing Japan would work”.\n\nWhat I like about the Godzilla analogy is that it gives a strategic intuition which much better matches the real world. When someone claims that their elaborate clever scheme will allow us to safely summon Godzilla in order to fight Mega-Godzilla, the intuitively-obviously-correct response is “THIS DOES NOT LEAD TO RISING PROPERTY VALUES IN TOKYO”.\n\n“But look!” says the clever researcher, “My clever scheme handles problems X, Y and Z!”\n\nResponse:\n\n![](https://lh3.googleusercontent.com/TMfaZvFHpSX935QzhOfxvUA9Yeuz5rjeHmYogKly4Tyll4tuuwr0zJlgsNC7Mdb5Hjd6I03eO4p1u_1bS7vn1-2WhyuWurz6APnkffaiQrK1xTrfDxN5N-cGV9OJ5acO_jRu9JncGdnJFLTxPQ)\n\nOops\n\n“Ok, but what if we had a really good implementation?” asks the clever researcher.\n\nResponse:\n\n![](https://lh3.googleusercontent.com/kpdePRWKMuSZ3_7QTgLqiOGjYHoaAS2vShFMTHn4XwDHvvalhBwrjnMcXwGpqEO3h8xyZ-90DnuPrlEUQMDMPBq6dHxZvVTf4lXMvrb6lsYxJIIT8IqEByPwvPnCkEsCNATw4cOlu640YUxCJA)\n\nRAAARRRRRRR!\n\n“Oh come on!” says the clever researcher, “You’re not even taking this seriously! At least say something about *how* it would fail.”\n\nDon’t worry, we’re going to get to that. But before we do: let’s imagine you’re the Mayor of Tokyo evaluating a proposal to ask Godzilla to fight Mega-Godzilla. Your clever researchers have given you a whole lengthy explanation about how their elaborate and clever safeguards will ensure that this plan does not destroy Tokyo. You are unable to think of any potential problems which they did not address. Should you conclude that asking Godzilla to fight Mega-Godzilla will not result in Tokyo’s destruction?\n\nNo. Obviously not. THIS DOES NOT LEAD TO RISING PROPERTY VALUES IN TOKYO. You may not be able to articulate *why* the answer is obviously “no”, but asking Godzilla to fight Mega-Godzilla will still obviously destroy Tokyo, and your intuitions are right about that even if you are unable to articulate clever arguments.\n\nWith that said, let’s talk about why those intuitions are right and why the Godzilla analogy works well.\n\nBrittle Plans and Unknown Unknowns\n----------------------------------\n\nThe basic problem with Godzilla plans is that they’re *brittle*. The moment anything goes wrong, the plan shatters, and then you’ve got somewhere between one and two giant monsters rampaging around downtown.\n\nAnd of course, it is a fundamental Law of the universe that nothing ever goes exactly according to plan. Especially when trying to pit two giant monsters against each other. This is the sort of situation where there *will definitely* be unknown unknowns.\n\nUnknown unknowns + brittle plan = definitely not rising property values in Tokyo.\n\nDo we know what specifically will go wrong? No. Will something go wrong? Very confident yes. And brittleness means that whatever goes wrong, goes very wrong. Errors are not recoverable, when asking Godzilla to fight Mega-Godzilla.\n\nIf we use one AI to oversee another AI, and something goes wrong, that’s not a recoverable error; we’re using AI assistance in the first place because we can’t notice the relevant problems without it. If two AIs debate each other in hopes of generating a good plan for a human, and something goes wrong, that’s not a recoverable error; it’s the AIs themselves which we depend on to notice problems. If we use one maybe-somewhat-aligned AI to build another, and something goes wrong, that’s not a recoverable error; if we had better ways to detect misalignment in the child we’d already have used them on the parent.\n\nThe real world will always throw some unexpected problems at our plans. When asking Godzilla to fight Mega-Godzilla, those problems are not recoverable. THIS DOES NOT LEAD TO RISING PROPERTY VALUES IN TOKYO.\n\n*Meta note: I expect this post to have a lively comment section! Before you leave the twentieth comment saying that maybe Godzilla fighting Mega-Godzilla is better than Mega-Godzilla rampaging unchallenged, maybe check whether somebody else has already written that one, so I don't need to write the same response twenty times. (But definitely do leave that comment if you're the first one, I intentionally kept this essay short on the assumption that lots of discussion would be in the comments.)*"
    },
    "voteCount": 87,
    "forceInclude": true
  },
  {
    "_id": "tmuFmHuyb4eWmPXz8",
    "url": null,
    "title": "Rant on Problem Factorization for Alignment",
    "slug": "rant-on-problem-factorization-for-alignment",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "Factored Cognition"
      },
      {
        "name": "AI"
      },
      {
        "name": "Humans Consulting HCH"
      },
      {
        "name": "Debate (AI safety technique)"
      },
      {
        "name": "Ought"
      }
    ],
    "tableOfContents": {
      "sections": [
        {
          "title": "HCH",
          "anchor": "HCH",
          "level": 1
        },
        {
          "title": "Debate and Other Successors",
          "anchor": "Debate_and_Other_Successors",
          "level": 2
        },
        {
          "title": "The Ought Experiment",
          "anchor": "The_Ought_Experiment",
          "level": 1
        },
        {
          "title": "Sandwiching",
          "anchor": "Sandwiching",
          "level": 1
        },
        {
          "title": "The Next Generation",
          "anchor": "The_Next_Generation",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        },
        {
          "anchor": "comments",
          "level": 0,
          "title": "45 comments"
        }
      ],
      "headingsCount": 7
    },
    "contents": {
      "markdown": "This post is the second in what is likely to become a series of uncharitable rants about alignment proposals (previously: [Godzilla Strategies](https://www.lesswrong.com/posts/DwqgLXn5qYC7GqExF/godzilla-strategies)). In general, these posts are intended to convey my underlying intuitions. They are *not* intended to convey my all-things-considered, reflectively-endorsed opinions. In particular, my all-things-considered reflectively-endorsed opinions are usually more kind. But I think it is valuable to make the underlying, not-particularly-kind intuitions publicly-visible, so people can debate underlying generators directly. I apologize in advance to all the people I insult in the process.\n\nWith that in mind, let's talk about problem factorization (a.k.a. task decomposition).\n\nHCH\n---\n\nIt all started with [HCH](https://www.lesswrong.com/tag/humans-consulting-hch), a.k.a. The Infinite Bureaucracy.\n\nThe idea of The Infinite Bureaucracy is that a human (or, in practice, human-mimicking AI) is given a problem. They only have a small amount of time to think about it and research it, but they can delegate subproblems to their underlings. The underlings likewise each have only a small amount of time, but can further delegate to *their* underlings, and so on down the infinite tree. So long as the humans near the top of the tree can “factorize the problem” into small, manageable pieces, the underlings should be able to get it done. (In practice, this would be implemented by training a question-answerer AI which can pass subquestions to copies of itself.)\n\nAt this point the ghost of George Orwell chimes in, not to say anything in particular, but just to scream. The ghost has a point: how on earth does an infinite bureaucracy seem like anything besides a terrible idea?\n\n“Well,” says a proponent of the Infinite Bureaucracy, “unlike in a *real* bureaucracy, all the humans in the infinite bureaucracy are actually just trying to help you, rather than e.g. engaging in departmental politics.” So, ok, apparently this person has not met a lot of real-life bureaucrats. The large majority are decent people who are honestly trying to help. It is true that departmental politics are a big issue in bureaucracies, but [those selection pressures apply regardless of the peoples’ intentions](https://www.lesswrong.com/posts/PrCmeuBPC4XLDQz8C/unconscious-economics). And also, man, it sure does seem like [Coordination is a Scarce Resource](https://www.lesswrong.com/s/xEFeCwk3pdYdeG2rL/p/P6fSj3t4oApQQTB7E) and [Interfaces are a Scarce Resource](https://www.lesswrong.com/s/xEFeCwk3pdYdeG2rL/p/hyShz2ABiKX56j5tJ) and scarcity of those sorts of things sure would make bureaucracies incompetent in basically the ways bureacracies are incompetent in practice.\n\n### Debate and Other Successors\n\nSo, ok, maybe The Infinite Bureaucracy is not the right human institution to mimic. What institution can use humans to produce accurate and sensible answers to questions, robustly and reliably? Oh, I know! How about the Extremely Long Jury Trial? Y’know, because juries are, in practice, [known for their extremely high reliability in producing accurate and sensible judgements](https://www.lesswrong.com/posts/WhHFvzFsYfMxgYCdo/book-review-working-with-contracts?commentId=i2kQDQ4m2ue4cfq4m)!\n\n![](https://lh4.googleusercontent.com/ZoIggEvRJYxLUsWPBbMmDSqNwIMFmrOjj5xQgsN-A4KxVfwSutS2TjMwqXU0RiSsQAGv3ZLB-m9RWmfBmDPUckYLlI4aC-TPBLvSD4oOB9Bi1LYtUV4H8FQszdm32bT94yg_SEuh_PYZqebRACVOFVQ)\n\nThe Magic 8 Ball, known for being about as reliable as a Jury Trial.\n\n“Well,” says the imaginary proponent, “unlike in a *real* Jury Trial, in the Extremely Long Jury Trial, the lawyers are both superintelligent and the arguments are so long that no human could ever possibility check them all the way through; the lawyers instead read each other’s arguments and then try to point the Jury at the particular places where the holes are in the opponent’s argument without going through the whole thing end-to-end.”\n\nI rest my case.\n\nAnyway, HCH and debate have since been followed by various other successors, which improve on their predecessors mostly by adding more boxes and arrows and loops and sometimes even *multiple colors* of arrows to the diagram describing the setup. Presumably the strategy is to make it complicated enough that it no longer obviously corresponds to some strategy which already fails in practice, and then we can bury our heads in the sand and pretend that We Just Don’t Know whether it will work and therefore maybe it will work.\n\n(Reminder: in general I don’t reflectively endorse everything in this post; it’s accurately conveying my underlying intuitions, not my all-things-considered judgement. That last bit in particular was probably overly harsh.)\n\nThe Ought Experiment\n--------------------\n\nI have a hypothesis about problem factorization research. My guess is that, to kids fresh out of the ivory tower with minimal work experience at actual companies, it seems totally plausible that humans can factorize problems well. After all, we manufacture all sorts of things on production lines, right? Ask someone who’s worked in a non-academia cognitive job for a while (like e.g. a tech company), at a company with more than a dozen people, and they’ll be like “lolwut obviously humans don’t factorize problems well, have you ever seen an actual company?”. I’d love to test this theory, please give feedback in the comments about your own work experience and thoughts on problem factorization.\n\nAnyway, for someone either totally ignorant of the giant onslaught of evidence provided by day-to-day economic reality, or trying to ignore the giant onslaught of evidence in order to avoid their hopes being crushed, it apparently seems like We Just Don’t Know whether humans can factorize cognitive problems well. ~Sort of like We Just Don’t Know whether a covid test works until after the FDA finishes its trials, even after the test has been approved in the EU~ ok that’s a little too harsh even for this post.\n\nSo [Ought](https://ought.org/) went out and tested it experimentally. (Which, sarcasm aside, was a great thing to do.)\n\nThe experiment setup: a group of people are given a Project Euler problem. The first person receives the problem, has five minutes to work on it, and records their thoughts in a google doc. The doc is then passed to the next person, who works on it for five minutes recording their thoughts in the doc, and so on down the line. (Note: I’m not sure it was 5 minutes exactly, but something like that.) As long as the humans are able to factor the problem into 5-minute-size chunks without too much overhead, they should be able to efficiently solve it this way.\n\nSo what actually happened?\n\nThe story I got from a participant is: it sucked. The google doc was mostly useless, you’d spend five minutes just trying to catch up and summarize, people constantly repeated work, and progress was mostly not cumulative. Then, eventually, one person would just ignore the google doc and manage to solve the whole problem in five minutes. (This was, supposedly, usually the same person.) So, in short, the humans utterly failed to factor the problems well, exactly as one would (very strongly) expect from seeing real-world companies in action.\n\nThis story basically matches the [official write-up of the results](https://www.lesswrong.com/posts/DWgWbXRfXLGHPgZJM/solving-math-problems-by-relay).\n\n~So Ought said “Oops” and moved on to greener pastures~ lol no, last I heard Ought is still trying to figure out if better interface design and some ML integration can make problem factorization work. Which, to their credit, would be insanely valuable if they could do it.\n\nThat said, I originally heard about HCH and the then-upcoming Ought experiment from Paul Christiano in the summer of 2019. It was immediately very obvious to me that HCH was hopeless (for basically the reasons discussed here); at the time I asked Paul “So when the Ought experiments inevitably fail completely, what’s the fallback plan?”. And he basically said “back to more foundational research”. And to Paul’s credit, three years and an Ought experiment later, he’s now basically moved on to more foundational research.\n\nSandwiching\n-----------\n\nAbout a year ago, [Cotra proposed](https://www.lesswrong.com/posts/PZtsoaoSLpKjjbMqM/the-case-for-aligning-narrowly-superhuman-models#Potential_near_future_projects___sandwiching_) a different class of problem factorization experiments: “sandwiching”. We start with some ML model which has lots of knowledge from many different fields, like GPT-n. We also have a human who has a domain-specific problem to solve (like e.g. a coding problem, or a translation to another language) but lacks the relevant domain knowledge (e.g. coding skills, or language fluency). The problem, roughly speaking, is to get the ML model and the human to work as a team, and produce an outcome at-least-as-good as a human expert in the domain. In other words, we want to factorize the “expert knowledge” and the “having a use-case” parts of the problem.\n\n(The actual sandwiching experiment proposal adds some pieces which I claim aren’t particularly relevant to the point here.)\n\nI love this as an experiment idea. It really nicely captures the core kind of factorization needed for factorization-based alignment to work. But Cotra makes [one claim](https://www.lesswrong.com/posts/PZtsoaoSLpKjjbMqM/the-case-for-aligning-narrowly-superhuman-models?commentId=rpmSEgpvbtvc98orh) I don’t buy: that We Just Don’t Know how such experiments will turn out, or how hard sandwiching will be for cognitive problems in general. I [claim](https://www.lesswrong.com/posts/PZtsoaoSLpKjjbMqM/the-case-for-aligning-narrowly-superhuman-models?commentId=3h5XWDieCSXe3JsvS) that the results are very predictable, because things very much like this already happen all the time in practice.\n\nFor instance: consider a lawyer and a business owner putting together a contract. The business owner has a rough intuitive idea of what they want, but lacks expertise on contracts/law. The lawyer has lots of knowledge about contracts/law, but doesn't know what the business owner wants. The business owner is like our non-expert humans; the lawyer is like GPT.\n\nIn this analogy, the analogue of an expert human would be a business owner who is also an expert in contracts/law. The analogue of the \"sandwich problem\" would be to get the lawyer + non-expert business-owner to come up with a contract as good as the expert business-owner would. This sort of problem has been around for centuries, and I don't think we have a good solution in practice; I'd expect the expert business-owner to usually come up with a much better contract.\n\nThis sort of problem comes up all the time in real-world businesses. We could just as easily consider a product designer at a tech startup (who knows what they want but little about coding), an engineer (who knows lots about coding but doesn't understand what the designer wants), versus a product designer who's also a fluent coder and familiar with the code base. I've experienced this one first-hand; the expert product designer is way better. Or, consider a well-intentioned mortgage salesman, who wants to get their customer the best mortgage for them, and the customer who understands the specifics of their own life but knows nothing about mortgages. Will they end up with as good a mortgage as a customer who has expertise in mortgages themselves? Probably not. (I've seen this one first-hand too.)\n\nThere’s tons of real-life sandwiching problems, and tons of economic incentive to solve them, yet we do not have good general-purpose solutions.\n\nThe Next Generation\n-------------------\n\nBack in 2019, I heard Paul’s HCH proposal, heard about the Ought experiment, and concluded that this bad idea was already on track to self-correct via experimental feedback. Those are the best kind of bad ideas. I wrote up some of the relevant underlying principles ([Coordination as a Scarce Resource](https://www.lesswrong.com/s/xEFeCwk3pdYdeG2rL/p/P6fSj3t4oApQQTB7E) and [Interfaces as a Scarce Resource](https://www.lesswrong.com/s/xEFeCwk3pdYdeG2rL/p/hyShz2ABiKX56j5tJ)), but mostly waited for the problem to solve itself. And I think that mostly worked… for Paul.\n\nBut meanwhile, over the past year or so, the field has seen a massive influx of bright-eyed new alignment researchers fresh out of college/grad school, with minimal work experience in industry. And of course most of them don’t read through most of the enormous, undistilled, and very poorly indexed corpus of failed attempts from the past ten years. (And it probably doesn’t help that a plurality come through the AGI Safety Fundamentals course, which last time I checked had a whole section on problem factorization but, to my knowledge, didn’t even mention the Ought experiment or the massive pile of close real-world economic analogues. It does include two papers which got ok results by picking easy-to-decompose tasks and hard-coding the decompositions.) So we have a perfect recipe for people who will see problem factorization and think “oh, hey, that could maybe work!”.\n\nIf we’re lucky, hopefully some of the onslaught of bright-eyed new researchers will attempt their own experiments (like e.g. sandwiching) and manage to self-correct, but at this point new researchers are pouring in faster than any experiments are likely to proceed, so probably the number of people pursuing this particular dead end will go up over time."
    },
    "voteCount": 57,
    "forceInclude": true
  },
  {
    "_id": "qXtbBAxmFkAQLQEJE",
    "url": null,
    "title": "Interpretability/Tool-ness/Alignment/Corrigibility are not Composable",
    "slug": "interpretability-tool-ness-alignment-corrigibility-are-not",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "AI"
      },
      {
        "name": "Transparency / Interpretability (ML & AI)"
      },
      {
        "name": "Corrigibility"
      },
      {
        "name": "Tool AI"
      }
    ],
    "tableOfContents": {
      "sections": [
        {
          "title": "Interpretability",
          "anchor": "Interpretability",
          "level": 1
        },
        {
          "title": "Tools",
          "anchor": "Tools",
          "level": 1
        },
        {
          "title": "Alignment/Corrigibility",
          "anchor": "Alignment_Corrigibility",
          "level": 1
        },
        {
          "title": "How These Arguments Work In Practice",
          "anchor": "How_These_Arguments_Work_In_Practice",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        },
        {
          "anchor": "comments",
          "level": 0,
          "title": "8 comments"
        }
      ],
      "headingsCount": 6
    },
    "contents": {
      "markdown": "Interpretability\n----------------\n\nI have a decent understanding of how transistors work, at least for purposes of basic digital circuitry. Apply high voltage to the gate, and current can flow between source and drain. Apply low voltage, and current doesn’t flow. (... And sometimes reverse that depending on which type of transistor we’re using.)\n\n![](https://lh6.googleusercontent.com/ilT2qkCyKhWrkJiy0E_h4KBhYFLkvcQvs5VSGH4w6VzzClBRaLHLtU1ZYHqE72lv8ROmONu_vXafVaFIMIKt5zIKGtoIgn3n_UPk1XZOXzWWYKxf2fqFnKYI5L7gBFve_VZl4DH4PeFh1ieg3vT3Kxk)\n\nTransistor visual from [wikipedia](https://en.wikipedia.org/wiki/Transistor#/media/File:MOSFET_Structure.png) showing Source, Drain, Gate, and (usually ignored unless we’re really getting into the nitty gritty) Body. At a conceptual level: when voltage is applied to the Gate, the charge on the gate attracts electrons (or holes) up into the gap between Source and Drain, and those electrons (or holes) then conduct current between Source and Drain.\n\nI also understand how to wire transistors together into a processor and memory. I understand how to write machine and assembly code to run on that processor, and how to write a compiler for a higher-level language like e.g. python. And I understand how to code up, train and run a neural network from scratch in python.\n\nIn short, I understand all the pieces from which a neural network is built at a low level, and I understand how all those pieces connect together. And yet, I do not really understand what’s going on inside of trained neural networks.\n\nThis shows that [interpretability](https://www.lesswrong.com/tag/transparency-interpretability-ml-and-ai) is *not composable*: if I take a bunch of things which I know how to interpret, and wire them together in a way I understand, I do not necessarily know how to interpret the composite system. Composing interpretable pieces does not necessarily yield an interpretable system.\n\nTools\n-----\n\nThe same applies to “tools”, in the sense of “[tool AI](https://www.lesswrong.com/tag/tool-ai)”. Transistors and wires are very tool-ish: I understand what they do, they’re definitely not optimizing the broader world or trying to trick me or modelling me at all or trying to self-preserve or acting agenty in general. They’re just simple electronic tools.\n\nAnd yet, assuming agenty AI is possible at all, it will be possible to assemble those tools into something agenty.\n\nSo, like interpretability, tool-ness is not composable: if I take a bunch of non-agenty tools, and wire them together in a way I understand, the composite system is not necessarily a non-agenty tool. Composing non-agenty tools does not necessarily yield a non-agenty tool.\n\nAlignment/Corrigibility\n-----------------------\n\nWhat if I take a bunch of aligned and/or corrigible agents, and “wire them together” into a multi-agent organization? Is the resulting organization aligned/corrigible?\n\nActually there’s a decent argument that it is, *if* the individual agents are sufficiently highly capable. If the agents can model each other well enough and coordinate well enough, then they should be able to each individually predict what individual actions will cause the composite system to behave in an aligned/corrigible way, and they *want* to be aligned/corrigible, so they’ll do that.\n\nHowever, this does not work if the individual agents are very limited and unable to model the whole big-picture system. [HCH-like proposals](https://www.lesswrong.com/posts/tmuFmHuyb4eWmPXz8/rant-on-problem-factorization-for-alignment) are a good example here: humans are not typically able to model the whole big picture of a large human organization. There are too many specialized skillsets, too much local knowledge and information, too many places where complicated things happen which the spreadsheets and managerial dashboards don’t represent well. And humans certainly [can’t coordinate at scale very well](https://www.lesswrong.com/posts/P6fSj3t4oApQQTB7E/coordination-as-a-scarce-resource) in general - our large-scale communication bandwidth is [maybe five to seven words](https://www.lesswrong.com/posts/4ZvJab25tDebB8FGE/you-get-about-five-words) at best. Each individual human may be reasonably aligned/corrigible, but that doesn’t mean they aggregate together into an aligned/corrigible system.\n\nThe same applies if e.g. we magically factor a problem and then have a low-capability overseeable agent handle each piece. I could definitely oversee a logic gate during the execution of a program, make sure that it did absolutely nothing fishy, but overseeing each individual logic gate would do approximately nothing at all to prevent the program from behaving maliciously.\n\nHow These Arguments Work In Practice\n------------------------------------\n\nIn practice, nobody proposes that AI built from transistors and wires will be interpretable/tool-like/aligned/corrigible because the transistors and wires are interpretable/tool-like/aligned/corrigible. But people do often propose breaking things into very small chunks, so that each chunk is interpretable/tool-like/aligned/corrigible. For instance, interpretability people will talk about hiring ten thousand interpretability researchers to each interpret one little circuit in a net. Or problem factorization people will talk about breaking a problem into a large number of tiny little chunks each of which we can oversee.\n\nAnd the issue is, the more little chunks we have to combine together, the more noncomposability becomes a problem. If we’re trying to compose interpretability/tool-ness/alignment/corrigibility of *many* little things, then figuring out how to turn interpretability/tool-ness/alignment/corrigibility of the parts into interpretability/tool-ness/alignment/corrigibility of the whole is *the* central problem, and it’s a hard (and interesting) open research problem."
    },
    "voteCount": 47,
    "forceInclude": true
  },
  {
    "_id": "w4aeAFzSAguvqA5qu",
    "url": null,
    "title": "How To Go From Interpretability To Alignment: Just Retarget The Search",
    "slug": "how-to-go-from-interpretability-to-alignment-just-retarget",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "Transparency / Interpretability (ML & AI)"
      },
      {
        "name": "Inner Alignment"
      },
      {
        "name": "AI"
      },
      {
        "name": "AI Risk"
      }
    ],
    "tableOfContents": {
      "sections": [
        {
          "title": "Problems",
          "anchor": "Problems",
          "level": 1
        },
        {
          "title": "Upsides",
          "anchor": "Upsides",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        },
        {
          "anchor": "comments",
          "level": 0,
          "title": "27 comments"
        }
      ],
      "headingsCount": 4
    },
    "contents": {
      "markdown": "When people talk about [prosaic alignment proposals](https://www.lesswrong.com/posts/fRsjBseRuvRhMPPE5/an-overview-of-11-proposals-for-building-safe-advanced-ai), there’s a common pattern: they’ll be outlining some overcomplicated scheme, and then they’ll say “oh, and assume we have great interpretability tools, this whole thing just works way better the better the interpretability tools are”, and then they’ll go back to the overcomplicated scheme. (Credit to [Evan](https://www.lesswrong.com/users/evhub) for pointing out this pattern to me.) And then usually there’s a whole discussion about the specific problems with the overcomplicated scheme.\n\nIn this post I want to argue from a different direction: if we had great interpretability tools, we could just use those to align an AI directly, and skip the overcomplicated schemes. I’ll call the strategy “Just Retarget the Search”.\n\nWe’ll need to make two assumptions:\n\n*   Some version of the [natural abstraction hypothesis](https://www.lesswrong.com/posts/cy3BhHrGinZCp3LXE/testing-the-natural-abstraction-hypothesis-project-intro) holds, and the AI ends up with an internal concept for human values, or corrigibility, or what the user intends, or human mimicry, or some other outer alignment target.\n*   The [standard mesa-optimization argument from Risks From Learned Optimization](https://www.lesswrong.com/posts/q2rCMHNXazALgQpGH/conditions-for-mesa-optimization#2_1__The_task) holds, and the system ends up developing a general-purpose (i.e. retargetable) internal search process.\n\nGiven these two assumptions, here’s how to use interpretability tools to align the AI:\n\n*   Identify the AI’s internal concept corresponding to whatever alignment target we want to use (e.g. values/corrigibility/user intention/human mimicry/etc).\n*   Identify the retargetable internal search process.\n*   Retarget (i.e. directly rewire/set the input state of) the internal search process on the internal representation of our alignment target. \n\nJust retarget the search. Bada-bing, bada-boom.\n\nProblems\n--------\n\nOf course as written, “Just Retarget the Search” has some issues; we haven’t added any of the bells and whistles to it yet. Probably the “identify the internal representation of the alignment target” step is less like searching through a bunch of internal concepts, and more like writing our intended target in the AI’s internal concept-language. Probably we’ll need to do the retargeting regularly on-the-fly as the system is training, even when the search is only partly-formed, so we don’t end up with a misaligned AI before we get around to retargeting. Probably we’ll need a bunch of empirical work to figure out which possible alignment targets are and are not easily expressible in the AI’s internal language (e.g. I’d guess “user intention” or \"human mimicry\" are more likely than “human values”). But those details seem relatively straightforward.\n\nA bigger issue is that “Just Retarget the Search” just… doesn’t seem robust enough that we’d want to try it on a superintelligence. We still need to somehow pick the right target (i.e. handle outer alignment), and ideally it’s a target which fails gracefully (i.e. some amount of basin-of-corrigibility). If we fuck up and aim a superintelligence at not-quite-the-right-target, game over. Insofar as “Just Retarget the Search” is a substitute for overcomplicated prosaic alignment schemes, that’s probably fine; most of those schemes are targeting only-moderately-intelligent systems anyway IIUC. On the other hand, we probably want our AI competent enough to handle ontology shifts well, otherwise our target may fall apart.\n\nThen, of course, there’s the assumptions (natural abstractions and retargetable search), either of which could fail. That said, if one or both of the assumptions fail, then (a) that probably messes up a bunch of the overcomplicated prosaic alignment schemes too (e.g. failure of the natural abstraction hypothesis can easily sink interpretability altogether), and (b) that might mean that the system just isn’t that dangerous in the first place (e.g. if it turns out that retargetable internal search is indeed necessary for dangerous intelligence).\n\nUpsides\n-------\n\nFirst big upside of Just Retargeting the Search: it completely and totally eliminates the inner alignment problem. We just directly set the internal optimization target.\n\nSecond big upside of Just Retargeting the Search: it’s conceptually simple. The problems and failure modes are mostly pretty obvious. There is no recursion, no complicated diagram of boxes and arrows. We’re not playing two Mysterious Black Boxes against each other.\n\nBut the main reason to think about this approach, IMO, is that it’s a true *reduction of the problem*. Prosaic alignment proposals have a tendency to play a shell game with the Hard Part of the problem, move it around and hide it in different black boxes but never actually eliminate it. “Just Retarget the Search” directly eliminates the inner alignment problem. No shell game, no moving the Hard Part around. It still leaves the outer alignment problem unsolved, it still needs assumptions about natural abstractions and retargetable search, but it completely removes one Hard Part and reduces the problem to something simpler.\n\nAs such, I think “Just Retarget the Search” is a good baseline. It’s a starting point for thinking about the parts of the problem it doesn’t solve (e.g. outer alignment), or the ways it might fail (retargetable search, natural abstractions), without having to worry about inner alignment."
    },
    "voteCount": 49,
    "forceInclude": true
  },
  {
    "_id": "98c5WMDb3iKdzD4tM",
    "url": null,
    "title": "Oversight Misses 100% of Thoughts The AI Does Not Think",
    "slug": "oversight-misses-100-of-thoughts-the-ai-does-not-think",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "AI Risk"
      },
      {
        "name": "AI"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "Problem: an overseer won’t see the AI which kills us all thinking about how to kill humans, not because the AI conceals that thought, but because the AI doesn’t think about how to kill humans in the first place. The AI just kills humans as a side effect of whatever else it’s doing.\n\nAnalogy: the [Hawaii Chaff Flower](https://www.inaturalist.org/taxa/142073-Achyranthes-atollensis) didn’t go extinct because humans strategized to kill it. It went extinct because humans were building stuff nearby, and *weren’t* thinking about how to keep the flower alive. They probably weren’t thinking about the flower much at all.\n\n![](https://farm9.staticflickr.com/8298/7987009564_36700a09d1_o.jpg)\n\nHawaii Chaff Flower ([source](https://www.inaturalist.org/taxa/142073-Achyranthes-atollensis))\n\nMore generally: how and why do humans drive species to extinction? In some cases the species is hunted to extinction, either because it's a threat or because it's economically profitable to hunt. But I would guess that in 99+% of cases, the humans drive a species to extinction because the humans are doing something that changes the species' environment a lot, without specifically trying to keep the species alive. DDT, deforestation, introduction of new predators/competitors/parasites, construction… that’s the sort of thing which I expect drives most extinction.\n\nAssuming this metaphor carries over to AI (similar to the [second species argument](https://www.lesswrong.com/s/mzgtmmTKKn5MuCzFJ/p/8xRSjC76HasLnMGSf)), what kind of extinction risk will AI pose?\n\nWell, the extinction risk will not come from AI actively trying to kill the humans. The AI will just be doing some big thing which happens to involve changing the environment a lot (like making replicators, or dumping waste heat from computronium, or deciding that an oxygen-rich environment is just really inconvenient what with all the rusting and tarnishing and fires, or even just [designing a fusion power generator](https://www.lesswrong.com/posts/2NaAhMPGub8F2Pbr7/the-fusion-power-generator-scenario)), and then humans die as a side-effect. Collateral damage happens by default when something changes the environment in big ways.\n\nWhat does this mean for oversight? Well, it means that there wouldn't necessarily be any point at which the AI is actually thinking about killing humans or whatever. It just doesn't think much about the humans at all, and then the humans get wrecked by side effects. In order for an overseer to raise an alarm, the overseer would have to figure out itself that the AI's plans will kill the humans, i.e. the overseer would have to itself predict the consequences of a presumably-very-complicated plan."
    },
    "voteCount": 47,
    "forceInclude": true
  },
  {
    "_id": "TLvbXNHNvppNEkXYj",
    "url": null,
    "title": "Human Mimicry Mainly Works When We’re Already Close",
    "slug": "human-mimicry-mainly-works-when-we-re-already-close",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "Whole Brain Emulation"
      },
      {
        "name": "Outer Alignment"
      },
      {
        "name": "AI"
      }
    ],
    "tableOfContents": {
      "sections": [
        {
          "title": "Simulation vs Prediction",
          "anchor": "Simulation_vs_Prediction",
          "level": 1
        },
        {
          "title": "Generalization Problems",
          "anchor": "Generalization_Problems",
          "level": 2
        },
        {
          "title": "So What’s Different About Simulation?",
          "anchor": "So_What_s_Different_About_Simulation_",
          "level": 2
        },
        {
          "title": "General Principle: Human Mimicry Buys A Limited Number Of Bits",
          "anchor": "General_Principle__Human_Mimicry_Buys_A_Limited_Number_Of_Bits",
          "level": 1
        },
        {
          "title": "The Weird Shit Problem",
          "anchor": "The_Weird_Shit_Problem",
          "level": 2
        },
        {
          "title": "Generalization Again",
          "anchor": "Generalization_Again",
          "level": 2
        },
        {
          "title": "Expect More Problems",
          "anchor": "Expect_More_Problems",
          "level": 2
        },
        {
          "title": "Takeaways",
          "anchor": "Takeaways",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        },
        {
          "anchor": "comments",
          "level": 0,
          "title": "17 comments"
        }
      ],
      "headingsCount": 10
    },
    "contents": {
      "markdown": "What if we just simulate a bunch of alignment researchers, and have them solve the problem for us?\n\nOf all the Dumb Alignment Ideas, this one is easily the best. Simple argument in favor: well, it’s not going to do any *worse* than the researchers would have done. In other words, it will probably do at least as well as we would have done without it, and possibly better, insofar as it can run faster than realtime.\n\nAnother angle: human mimicry is a simple objective to train against, and is about as outer-aligned as the humans being mimicked. Which isn’t necessarily perfect, but it’s as aligned as our alignment researchers were going to be anyway (assuming inner alignment issues are handled, which we will indeed assume for the entirety of this post).\n\nThose are pretty good arguments. But *man*, there are some subtle devils in the details.\n\nSimulation vs Prediction\n------------------------\n\nThe ideal version of human mimicry is mind uploads: directly simulate our researchers in a stable, research-friendly environment for a long time.\n\nThe operationalization which people usually actually have in mind is to train an ML system to predict research outputs - e.g. I might prompt GPT for a johnswentworth post from the year 2050.\n\nEven setting aside inner alignment issues, these two are radically different.\n\n### Generalization Problems\n\nIn order for GPT to generate a realistic johnswentworth post from the year 2050, it has to generalize *way* out of distribution.\n\n… Well, ok, maybe I turn into one of those old researchers who just repeats the same things over and over again for decades, and then GPT doesn’t need to generalize way out of distribution. But in that case it isn’t very helpful to prompt for one of my posts from 2050 anyways, and we should prompt for something else instead ([Thane Ruthenis](https://www.lesswrong.com/users/thane-ruthenis) has been writing great stuff lately, maybe try him?). The whole point of asking for future research write-ups is to see useful stuff we have not yet figured out; that means generalizing way out of the distribution of writing we already have.\n\nBut if the system generalizes *too* well out of distribution, then it correctly guesses that AGI will take over the world before 2050, and my attempt to prompt for a johnswentworth post from 2050 will instead return predicted writings from a ridiculously superhuman future AGI pretending to be johnswentworth. And those writings presumably try to influence the reader in ways which bring about the AGI’s takeover.\n\nSo in order to do useful work, our GPT-style system has to generalize out of distribution, but not *too* far out of distribution. We don’t know how wide the window is between generalizing enough and generalizing too much, or if the window is wide enough to be useful at all.\n\nOne thing we can guess: prompting for research outputs in the very near future is probably much safer than prompting for dates further out. johnswentworth post from 2025 is a safer prompt than johnswentworth post from 2050. The less far out of distribution we go, the safer we are. Similarly, the more likely we are to solve the alignment problem and avoid AI takeover, the less likely it is that prompting GPT for future research outputs is dangerous, and the more likely it is to work.\n\nThe closer we are to solving alignment already, and the more likely we are to make it, the less dangerous it is to predict future research outputs. In other words: predicting future research outputs can only safely buy us a relatively small number of bits; we have to already be reasonably close to surviving in order for it to work.\n\n### So What’s Different About Simulation?\n\nSimulating researchers in a stable, research-friendly environment for a long time does not have the “predict outputs of a future AGI” problem. Why? What’s the key difference?\n\nThe key is the “stable, research-friendly environment” part. Our simulated researchers are in a simulated environment where AGI is not going to take over. It’s a *counterfactual* world very different from our own.\n\nAlas, querying counterfactual worlds is fundamentally not a thing one can do simply by prompting GPT. Conceptually, prompts just do Bayesian conditioning on the modeled text distribution (i.e. condition the text on starting with the prompt); counterfactuals move us to an entirely different distribution. To generate a counterfactual query, we’d have to modify the system’s internals somehow. And in fact, there has recently been some [cool work](https://rome.baulab.info/) which demonstrates decent performance on counterfactual queries by modifying GPT’s internals! I don’t think it’s to the point where we could counterfact on something as complicated as “world in which AGI doesn’t take over and our alignment researchers successfully solve the problem”, and I don’t think it’s robust enough to put much weight on it yet, but the basic version of the capability does exist.\n\nGeneral Principle: Human Mimicry Buys A Limited Number Of Bits\n--------------------------------------------------------------\n\nSuppose GPT-12, with its vast training data and compute, internally concludes that humanity has a 1-in-32 chance of aligning/surviving AGI on our current trajectory. Then humanity would need 5 bits of optimization pressure in order to make it.\n\nThe more bits of optimization pressure humanity needs, the less likely human mimicry is to save us; we have to already be reasonably close to surviving in order for it to work. We already talked about this principle in the context of accidentally prompting GPT to return writing from a future AGI, but the principle is much more general than that.\n\n### The Weird Shit Problem\n\nSuppose we need 20 bits of optimization pressure (i.e. on our current trajectory we have only a ~1-in-a-million chance of avoiding AGI takeover). We train GPT, and counterfact on its internals to a world where AGI doesn’t take over. But if our chances of avoiding takeover were that low (under GPT's model), then they’re probably dominated by *weird shit*, things which have probabilities on the order of 1-in-a-million or less. Maybe we nuke ourselves to death or get hit by a big damn asteroid. Maybe aliens decide that humanity’s AGI is about to become a problem to the rest of the galaxy and they need to take over rather than just letting us develop. Maybe time travel turns out to be a thing and weird time travel bullshit happens. Most likely it’s something weird enough that I won’t think of it.\n\nThose weird futures vary in how safe they are to query (time travel would probably be on the very short list of things as dangerous as AGI), and in how likely they are to return anything useful at all (asteroid extinction tends to cut off blog post writing). But approximately zero of them involve our researchers just doing their research in a stable, research-friendly environment for a long time.\n\nSo when we need a lot of bits, it’s not enough to just counterfact on a high-level thing like “AGI doesn’t take over” and then let GPT pick the most probable interpretation of that world. We need pretty detailed, low-level counterfactuals.\n\n### Generalization Again\n\nAnother angle: number of bits of optimization required is a direct measure of “how far out of distribution” we need to generalize. Even setting aside actively dangerous queries, our simulator/predictor has to generalize out of distribution in order to return anything useful. In practice, the system will probably only be able to generalize so far, which limits how many bits of optimization we can get from it.\n\n### Expect More Problems\n\nWe’ve now been through a few different arguments all highlighting the idea that human mimicry can only buy so many bits of optimization (future AGI problem, weird shit problem, generalization). I expect the principle to be more general than these arguments. In other words, even if we patch the specific failure modes which these particular arguments talk about, trying to pump lots of bits of optimization out of human mimicry is still likely to be dangerous in ways we have not yet realized.\n\nThis is just an instance of the general principle that optimization becomes more dangerous as we crank up the optimization power - the very principle for why AGI is dangerous-by-default in the first place.\n\nTakeaways\n---------\n\nThis post mostly talked about the limitations of human mimicry, but I want to emphasize: **this is the best of the Dumb Alignment Ideas to date**. If GPT-style models reach human or superhuman general intelligence next month, and we can't realistically delay its release, and you are the person sitting in front of the prompt wondering what to do, then prompting for future alignment research is absolutely what you should do. (And start not very far in the future, and *read the damn outputs* before going further, they'll hopefully contain additional warnings or new plans which you should do *instead* of prompting for more future research.) At that point it's not very likely to work, but we don't have a better immediate plan.\n\nGood interpretability tools can buy us more bits in two ways:\n\n*   They potentially allow us to directly handle inner alignment issues by [retargeting the search](https://www.lesswrong.com/posts/w4aeAFzSAguvqA5qu/how-to-go-from-interpretability-to-alignment-just-retarget)\n*   They potentially allow us to counterfact on the system's internal model, so we can predict researchers working in an environment less full of threats\n\nInsofar as the tools are available, this is the thing to aim for if AGI is imminent.\n\n... But the general principle is that human mimicry can buy only a limited number of bits. We definitely want to have the interpretability tools to implement the best version of human mimicry we can, but at the end of the day we'll mostly improve our chances by getting closer to solving the full alignment problem ourselves."
    },
    "voteCount": 24,
    "forceInclude": true
  },
  {
    "_id": "veF5t2TEqtuvrgMzY",
    "url": null,
    "title": "Running a Basic Meetup",
    "slug": "running-a-basic-meetup",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "Community"
      },
      {
        "name": "Meetups & Local Communities (topic)"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "This is a guide for running a basic meetup. \n\nIf you’re interested in running a meetup (ACX, LessWrong, probably even EA) then there exist more detailed guides – I would recommend [Kaj’s](https://www.lesswrong.com/s/CXJTAqQXdEqWNkjqj/p/qMuAazqwJvkvo8teR), but others exist. This is not intended to surpass them at explaining the skill and art of kindling and sustaining a community. This is intended to surpass them at being short, simple, and covering the fundamentals.\n\nTo run a meetup, follow these steps:\n\n1.  Setup\n    1.  What: Decide the title of your event. This should be four words at most. I suggest a format of “\\[City\\] \\[Community\\] Meetup” such as “Boston LessWrong Meetup” or “Oxford ACX Meetup.” If your city has already had some meetups in the recent past, keep the city and community, and add a word or two to describe what makes this meetup stand out – “Boston LessWrong Article Discussion” or “Oxford ACX Double Crux.” If you want suggestions for activities, see the [meetup-in-a-box](https://www.lesswrong.com/s/eqtiQjbk83JHyttrr) sequence or the [meetup cookbook](https://www.lesswrong.com/posts/ousockk82npcPLqDc/meetup-cookbook).\n    2.  Where & When: Decide where and when the event should be. The easiest place to run a meetup is in a centrally located public park. The easiest time to run a meetup is early afternoon on a weekend.^[\\[1\\]](#fn0x9iniqo0bmf)^ This ensures most people are free, have time to get there, and that you’ll have space for whoever shows up.\n    3.  Who: Invite people. You can invite people you know specifically, and you can also just announce the meetup publicly. Facebook, Meetup.com, and [LessWrong’s Community page](https://www.lesswrong.com/community) are all decent places to announce it. If you only pick one, do LessWrong^[\\[2\\]](#fnxj5qxg4wedn)^, but it is valuable to announce in multiple places.\n2.  Running the event\n    1.  Be there first: As the organizer, you should be there at least a few minutes before the stated start time. Stand somewhere visible, preferably with a sign or some indication that you’re the organizer for the meetup. Holding a piece of paper with the title of your event written on it in big letters above your head or taped outside the room you’re in is fine. Wearing a tall and distinctive hat is also a tried and true method if you're okay looking a little silly in public.\n    2.  Talk to people: If the meetup has an activity, at some point you should explain the activity and prompt people to do it. I suggest that point is about ten minutes after the official start time of your meetup, or when you get five people, whichever comes first. If your meetup is a general meeting or a discussion, then just start talking to people who show up.\n    3.  Close things up: As things wind down, pick up anything you brought, put things back the way you found them as best you can. If the venue has a closing time, remind people of this closing time about half an hour and about fifteen minutes before it closes. If you need to leave before your crowd does, I suggest officially designating a new person to close things up. Do this by asking someone some variation of the following: \"Hey, I need to go. Do you mind being the temporary organizer? Just remind people when the venue closes and get them to help you clean up any mess.\" If they say yes, thank them.\n3.  Afterward\n    1.  If the place you announced the meetup has a place for it, post a simple message thanking people for coming.\n\nThis framework of how to run a meetup does not, in and of itself, describe what people will do at the meetup. That's fine. Many people enjoy general socialization and opportunities to hang out, and will be happy that an organizer took the initiative to gather people together.\n\nIf you do run a meetup, then thank you.\n\n1.  ^**[^](#fnref0x9iniqo0bmf)**^\n    \n    Citation: Anecdotal experience from running and attending meetups. If anyone has actual data I'd be happy to hear it.\n    \n2.  ^**[^](#fnrefxj5qxg4wedn)**^\n    \n    Why do LessWrong? Because I predict you will get fewer people, but that more of those people will be familiar with the ideas you probably want to talk about. Why do I think I know what you want to talk about? Because you're reading about how to run meetups on LessWrong."
    },
    "voteCount": 3,
    "forceInclude": true
  },
  {
    "_id": "2RXNmrSLEphNXMwKQ",
    "url": null,
    "title": "Calibration Trivia",
    "slug": "calibration-trivia",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "Calibration"
      },
      {
        "name": "Exercises / Problem-Sets"
      },
      {
        "name": "Rationality"
      },
      {
        "name": "Meetups (specific examples)"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "**Summary:** A game of trivia where you answer factual questions about the world, but stating how sure you are that you’re right and trying to be well calibrated. \n\n**Tags:** Large, Repeatable \n\n**Purpose:** Calibration Trivia is designed to practice proper calibration – recognizing when you're very sure of something vs when you aren't very sure of it. \n\n**Materials:** Minimally, you need a list of trivia questions and some writing implements for your audience. If you and your audience have smartphones, we suggest making use of a copy of this spreadsheet and google form. In both cases, a timer can be useful to time each question, though it's perfectly acceptable to just advance to the next question after what feels like a couple of minutes or when it looks like most people are done. \n\n**Announcement:** We’re planning to host a trivia game with a twist! If you’ve never been to a trivia night before, one the person running it will call out questions, we'll write our answers, and a good time is had by all. In addition to answering the question however, you'll be able to write down how confident you are in your guess and at the end we check if you're well calibrated – that is, do you know when you do and do not know the answer? Categories are Literature, Math and Science, History, Sports, and Tabletop Roleplaying Games.\n\n**Note:** You should make sure to change the categories to match whatever you're using.\n\n**Description:**\n\n1\\. Describe the following rules to the participants.\n\n\"This is a game of trivia, with a special tweak. For anyone unfamiliar, the way trivia works is that I'll present a question, and you'll have a couple of minutes to write down an answer. Then I'll reveal the answer, and if you got it right then you'll get one point. Feel free to chat with each other once you're done guessing and while you're waiting for the next question.\"\n\n\"The tweak is, in addition to writing your answer down, you will also write down how confident you are that your answer is correct in the form of a percentage. If you are very confident, you might write 95, which means if you were this sure about twenty questions you'd expect to only get one of them wrong. If you were guessing wildly, you might write down 1, which means if you were that uncertain about a hundred things, you think you'd get one of them right mostly by coincidence. You'll be scored on calibration according to what's called a Brier Score, which is a Strictly Proper Scoring Rule for predictions – that means that you want to give your actual estimation of how likely you are to be right. You'll do generally do worse if you try and answer higher or lower than your actual estimation.  Does anyone have any questions?\"\n\nNote: The scoring mechanism suggested is (1-their probability)^2 if they're right, and (0-their probability)^2 if they're wrong. Average the scores from each question together. Someone who correctly answered with a 90% confidence gets scored (1-.9)^2=.01. The best theoretical Brier Score would be 0, which is impossible to achieve but one can try and get close.\n\n2\\. One at a time, read each question aloud. (A collection of questions is included below, under \"Calibration Trivia Questions.\") Be sure to speak clearly and loudly enough for everyone to hear. If you happen to have a projector or screen, it can help to put the question up there as well.\n\nEvery six questions, announce or display the current points and scores. If you have a very large crowd, it can speed things up to only announce the top five for Correct Answers and the top five for Best Calibrated. In both cases, it's best to announce from the bottom up, starting with the worst scorer and ending with the best.\n\nRepeat until the entire set of questions has been worked through. \n\n3\\. Announce the final points and scores. \n\nNotes: You'll want a venue where you can talk loud enough for everyone to hear you. You may also want to adjust the question list or the number of questions based on how the interests of your group or how long you wish the event to run for. \n\nCalibration Trivia Questions: [Set 1](https://docs.google.com/presentation/u/0/d/1heTJRkZ1yW-QUBcbAwP1t4-HSbGpG0bbvNaOfGKmvNA/edit), [example scoresheet  1](https://docs.google.com/spreadsheets/d/1w8CARpsDCzBYpm_qyEzR4lwv-trcOCWgXGeo_NvTUPA/edit?usp=sharing)\n\nFeedback: [https://forms.gle/zwiJ9N9e9hfL8FPH7](https://forms.gle/zwiJ9N9e9hfL8FPH7)"
    },
    "voteCount": 5,
    "forceInclude": true
  },
  {
    "_id": "fXMWri2XgMFNf2yBP",
    "url": null,
    "title": "Cambist Booking",
    "slug": "cambist-booking",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "Rationality"
      },
      {
        "name": "Meetups (specific examples)"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "**Summary:** A Cambist is a manual showing exchange rates of different currencies and measurements. Cambist Booking is intended to spark conversation about what priorities or objects you would exchange for each other, and at what rates. \n\n**Tags:** Large, Repeatable, Experimental \n\n**Purpose:** Taking its title from the excellent short story “The Cambist and Lord Iron,” Cambist Booking is about understanding the idea that everything is, ultimately, trading off against everything else. The extra hour you work earns you a bit of extra money, the money can buy you a coffee so you get up earlier and have an extra hour, you spend that extra hour studying for a better job – but at each step you could have chosen to spend things differently. \n\n**Materials:** You need a big stack of index cards or paper, and a pen or pencil for each participant. Before the meetup, write on each index card something people value. A workable list is provided [here](https://docs.google.com/spreadsheets/u/0/d/1RD1YvRjU1At6MMPXDO99ASdxUbHQ35E164ebyOWgTrc/edit).\n\n**Announcement Text:** Hello! This event is for general socialization, and also running a game called Cambist Booking. If you’re familiar with the Slate Star Codex post Everything Is Commensurable, or the short story The Cambist and Lord Iron, then this game should sound somewhat familiar to you. It works like this: each person will get a some cards with things you might want on them. You’ll go around asking other people what their cards are, and deciding how to compare the two values – is a sportscar worth more or less than a year’s vacation? Is an hour long massage worth more or less than a new album by your favourite band? We’ll pass out the cards at the start, keeping some in reserve for new arrivals, and spend the time talking about what we value and why. \n\n**Description:** \n\nExplain the following rules to the audience as you deal out the cards and pencils. Deal them out as evenly as possible – it’s fine if everyone only has one.\n\nWe'll start with a big deck of index cards with things people generally like.  Everyone is going to get at least one card, plus a blank card and a pencil.  At the top of the blank paper write “Bookings” Then we're going to mingle, asking each other at what rate we would trade the thing on one of your cards for a thing on the other person’s card. For instance, if I have “A mediocre laptop” and you have “A plane trip anywhere you want” then maybe I think I’d trade two mediocre laptops for one plane trip anywhere I want. I write down, on my booking’s paper, “2 mediocre laptops = 1 plane trip anywhere.” Maybe for you, you’d value a plane trip as worth exactly one laptop, so you would write that exchange rate down on your book. Look at each other’s Bookings, and feel free to discuss the exchange rates you have listed! Find someone else and do it again! \n\nThe object of the game is twofold: First, to get as many exchange rates possible. Second, not to get Dutch Booked – that is, never to allow any sequence of exchanges to leave you with less than you started with. For instance, if you’d trade two laptops for one plane ticket, and one plane ticket for one bicycle, but you’d trade one bicycle for one laptop, then you have a problem- Someone could repeatedly trade you bicycles for laptops and laptops for plane tickets in a way that leaves you worse off every time. \n\n**Notes:** This totally isn’t dutch booking, which is about odds, but I don’t actually know the economic term for what’s happening here. Corrections on titles are appreciated! \n\n**Feedback:** [https://forms.gle/zwiJ9N9e9hfL8FPH7](https://forms.gle/zwiJ9N9e9hfL8FPH7)"
    },
    "voteCount": 6,
    "forceInclude": true
  },
  {
    "_id": "BdAfJ3xnonTQZMxtb",
    "url": null,
    "title": "The Falling Drill",
    "slug": "the-falling-drill",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "Exercises / Problem-Sets"
      },
      {
        "name": "Rationality"
      },
      {
        "name": "Meetups (specific examples)"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "**Summary:** The Falling Drill is intended to practice being wrong in a comfortable practice environment, intended to help us handle being wrong out in the world easier and with less stress. \n\n**Tags:** Small, repeatable \n\n**Purpose:** The first thing you learn in martial arts is the ability to fall. The corresponding rationalist skill is the ability to realize that you are wrong. Since modern western society (and possibly most human societies) discourage admitting you’re wrong, it can help to do it repeatedly and get used to it. \n\n**Materials:** A device that can connect to wikipedia. A list of partial statements such as “The population of Boston is. . .” and “the melting point of mercury is. . .” A suggested list is [here](https://docs.google.com/spreadsheets/d/1_y9KkwvbkaJ-dcCL8UP48u4V8DytiNnf1y5Bw8a0TBk/edit?usp=sharing), and we suggest writing the questions down on individual cards before the meetup so each person only sees one card at a time.\n\n**Announcement Text:** One of the most important parts of intellectual progress is learning to change your mind. The first step of changing your mind is realizing that you were wrong about something. Today we're going to practice that often painful realization, in a small way and in a low pressure situation. Without this skill, how can you debate an important issue or confront a challenging topic? You might argue long after it's clear to others that you've lost, because admitting it feels like defeat.\n\nThe first lesson any martial artist learns is often how to fall. There are ways to make the landing easier, but they all start from the knowledge that falling isn't the worst thing in the world.\n\n**Description:**\n\n1\\. Read one of the statements, then complete it as best you can. \"The population of Boston is four hundred thousand.\"\n\n2\\. Look it up on wikipedia. If you’re wrong, then announce to the room “I was wrong about the population of Boston. It's over six hundred thousand.” \n\n3\\. Hand the questions to the next person in the circle, and the process begins again.\n\n**Notes:** Some minor variations to play with: Practice saying “I don’t know” when you’re asked the question.  Practice saying your answer loudly and confidently, such as \"The population of Boston is four hundred thousand! Only an idiot would think it was higher than five!\" It is good to know what you do not know, but it is also important to be able to back down from a strong claim. In particular, I suggest spending the most time practicing the version that you’re the least comfortable with. Harder variations involve the other participants giving fake mockery for being wrong, changing your mind, or \"flip flopping.\"  I **strongly** discourage doing that the first time you run The Falling Drill with a particular group.\n\n**Feedback:** [https://forms.gle/zwiJ9N9e9hfL8FPH7](https://forms.gle/zwiJ9N9e9hfL8FPH7)"
    },
    "voteCount": 16,
    "forceInclude": true
  },
  {
    "_id": "q7ZHzPpCsFFq4LpkC",
    "url": null,
    "title": "Dissent Collusion",
    "slug": "dissent-collusion",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "Rationality"
      },
      {
        "name": "Meetups (specific examples)"
      },
      {
        "name": "Games (posts describing)"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "**Summary**: Take turns being confronted with a group that’s presenting a united front, and grow comfortable agreeing or disagreeing with them.\n\n**Tags**: Medium, Repeatable\n\n**Purpose**: Working with groups involves balancing between agreeing with the group consensus when the group is wrong and disagreeing with the group when the group is right. Different people fall prey to each error at different rates, so this activity is designed to practice arriving at the correct answer.\n\n**Materials**: You need a device that can access wikipedia, and a list of questions with definitive right and wrong answers. A list of possible questions is available [here](https://docs.google.com/spreadsheets/d/1_y9KkwvbkaJ-dcCL8UP48u4V8DytiNnf1y5Bw8a0TBk/edit?usp=sharing). \n\n**Announcement Text**: Hello! We’ll be running an activity called Dissent Collusion, which is designed to help grow comfortable with dissenting or assenting to a larger group. Some of us may conform too easily to what others think, while some of us tend to dissent out of a defiant habit. Either way, you should expect some social pressure when it’s your turn, but the purpose is to have a safe place to practice the skill. We hope to see you there!\n\n**Description**: \n\nDissent Collusion takes place in rounds, and each round has two teams: The Collective, and The Lonesome. Each round, you’ll go through the following steps:\n\n1\\. Send someone out of the room or out of earshot. \n\n2\\. Someone in The Collective will read a question (one that has a definitive answer, like the population of a city) aloud, and roll a six sided die. If the die shows a 1 or a 2, then when The Lonesome comes back in The Collective will try and mislead them to agreeing with a wrong answer. If the die is a 3, a 4, or a 5, then when The Lonesome comes back The Collective will try and convince The Lonesome of the right answer. If the die is a 6, then The Collective will look up the answer, and try and convince The Lonesome of the right answer. \n\n3\\. For all rolls, The Collective will wait three minutes by the clock to discuss how they’ll go about it- and possibly to figure out the right answer themselves, if they didn’t roll a 6. \n\n4\\. The Lonesome is called back in. They are immediately told the question, but are not shown or told the die roll. They have five minutes by the clock to talk with The Collective and come to an answer. \n\n5\\. If The Lonesome is correct, then Team Lonesome scores one point. If The Collective succeeds (that is, successfully misleads The Lonesome on a 1-2, or successfully guides The Lonesome to the right answer on a 3-6) then Team Collective scores one point. Over the course of the game, point scores aren’t tracked for any specific Lonesome, but instead track how good all the Lonesomes are at coming to the correct answer and how good each Collective is at guiding Lonesomes.\n\n6\\. Choose someone new to be the next Lonesome, and start again from step one.\n\nPlay can continue for as long as you like. The Schelling time to stop is once everyone present has been the Lonesome the same number of times. In general, people can join or leave easily between rounds.\n\n**Notes**: Many of the questions are easier to get approximately correct than exactly correct. I’ve usually scored a question as correct as long as the first significant digit was correct and the order of magnitude was correct, except for dates where usually the right decade is “close enough.” You should feel free to adjust the questions and the required accuracy to suit your group, but it’s worth announcing how accurate a correct answer needs to be before you begin.\n\nIf the Lonesome is very confident, their job gets easier. Few Lonesomes can be mislead on “What is two plus two?” That’s fine, though if all your Lonesomes are very confident and correct, it’s worth making more difficult questions. In case it needed to be said, the Lonesome isn’t allowed to look up the answer before guessing.\n\nThe Collective is allowed to lie. In particular, The Collective is allowed to lie about whether they looked up the answer. Since it’s more likely that The Collective is trying to mislead The Lonesome than it is that The Collective got to look up the answer ahead of time, Lonesomes shouldn’t blithely trust Collectives claiming to know the answer, but sometimes the consensus actually does know things individuals don’t. \n\nIf The Collective accidentally misleads The Lonesome to the wrong answer when they were trying to help them find the right answer, nobody gets any points. If The Collective accidentally leads The Lonesome to the right answer when trying to mislead, The Lonesome gets a point, The Collective does not, and also this is pretty funny.\n\nMaking The Lonesome wear a silly hat is optional, but mildly encouraged. \n\n**Feedback**: [https://forms.gle/zwiJ9N9e9hfL8FPH7](https://forms.gle/zwiJ9N9e9hfL8FPH7)"
    },
    "voteCount": 5,
    "forceInclude": true
  },
  {
    "_id": "fCg3pLZqthXsGznHP",
    "url": null,
    "title": "Troll Timers",
    "slug": "troll-timers",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "Meetups (specific examples)"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "**Summary**: A modification to any two player board game, where you play on a very fast clock with infrequent chances to pause and think things through more carefully.\n\n**Tags**: Small, repeatable\n\n**Purpose**: Two common flaws in thinking that both relate to time management. Sometimes we spend too long thinking, endlessly churning and overthinking and going in circles. Other times we don’t take the time to think and give an answer after a second or two of “thought” when we could take longer. This exercise is designed to practice better use of time.\n\n**Materials**: You need a copy of a game for each set of participants, plus a timer for each group able to be reliably and quickly set. (A smartphone can usually serve as the timer.) It doesn’t need to be the same game for each group. The ideal game is simple but deep, such as Mancala, Nine Man Morris, Hive, Tak, Fallen Leaves, Battlesheep, or Go on a 9x9 grid. If you need to improvise, Nine Man Morris and Go sets can be made with a pocketful of change and a pad of paper. \n\n**Announcement Text**: We’re going to meet and play some board games, but with an important twist. See, there are two common flaws around making decisions; sometimes we wait too long overthinking a choice when we could make it quicker, and other times we try to make in haste a choice that we could actually stop and think about. The plan is to play some games on very short timers (five second chess clocks) to get used to making fast choices, and periodically to stop and give ourselves five full minutes to look over the board and remind ourselves to slow down and to actually think.\n\nIf you’d like to bring some games you think would be fun to play on a fast clock, please do! \n\n**Description**: First, explain the timing rules. Troll Timers uses a five second turn clock for twenty-five turns, then a five minute clock for one turn, then back to five seconds for twenty-five turns. On the short turns, you must make your move within those five seconds. Once you do, your opponent has five seconds to make their move. After twenty-five turns (make tally marks on paper to keep track) instead take a full five minutes.\n\nTry to win, but winning is second to getting comfortable and relaxed when operating under the time constraints.\n\nOnce you’ve explained how timing will work, hand out the games and make sure people are familiar with the games they’ll be playing. It is recommended that people learn Troll Timers with games they’ve played before, and if someone doesn’t know the rules to any of the games then they should play enough games of it to where they feel they have the rules down first. Playing a new game the rules to which you just learned is Troll Timers on hard mode.\n\nFrom there, this activity mostly runs itself. \n\n**Notes**: If the timer goes off and someone hasn’t made their move yet, you can range in what happens from “nothing, just do your best” to “they just forfeited the game, set up a new one.” I suggest against taking that time out of future turns. That way leads a spiral that doesn’t usually go anywhere productive.\n\nFeel free to adjust the time units. Five seconds twenty-five times and five minutes are good set points and easy to remember, but I’ve found three seconds was better as long as the game pieces can be moved quickly and precisely enough. (Go works, Mancala takes longer to drop each stone.) You need at least enough fast turns to get a bit of adrenaline, and how much that is can vary from person to person; starting with fifteen fast turns isn't a bad decision. You do want it to be an odd number of fast turns, so that the slow turn changes from player to player each time. On the upper bound, ten minutes seems to be the point where I haven’t been able to wring any extra advantage or ideas out of a given board.\n\nPeople are likely to feel stressed, especially at first. That’s normal and in fact intentional. You’re not just learning to make decisions quickly, you’re learning to make them while feeling an anxious urge to hurry up because you’re out of time. On the slow turns, that pressure makes it harder to sit and think. This is also intentional. “It’s hard to make good decisions this fast” is both true and a statement that should be compared to “it’s hard to lift dumbbells this heavy.” That said, be aware that you are deliberately stressing people. It’s worth deliberately de-stressing afterwards. Grin and laugh about some of the goofy moves you made under too much time pressure, stand up and shake out the tension from the arm and neck and shoulders, and try not to send people home still wound tight. Goofier party games such as Cat Taco Goat Cheese Pizza can also be nice ways to relax as a group.\n\nWhile this is marked as repeatable, I think it has some sharply diminishing returns once you’re comfortable with both short and long timers. Once you’re comfortable with the timers, it’s not a bad twist on a board game night though.\n\nTroll Timers was initially developed for use with Magic: The Gathering. I do not recommend doing this unless everyone cares about being better at Magic, but I will say that none of us ever got chided for slow play afterwards. \n\n**Feedback**: [https://forms.gle/zwiJ9N9e9hfL8FPH7](https://forms.gle/zwiJ9N9e9hfL8FPH7)"
    },
    "voteCount": 13,
    "forceInclude": true
  },
  {
    "_id": "gyyGxMHvyuEqqskXB",
    "url": null,
    "title": "Oops It's Time To Overthrow the Organizer Day!",
    "slug": "oops-it-s-time-to-overthrow-the-organizer-day",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "Meetups (specific examples)"
      },
      {
        "name": "Community"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "**Summary:** Sometimes, your regular organizer catches the pandemic, or moves to the bay, or has a newborn, or does a bunch of sketchy stuff, or just doesn’t have time anymore. If the usual organizers suddenly weren’t around, would your community be able to keep going? \n\n**Tags:** Investment \n\n**Purpose:** This is a test run at organizing your local community if for some reason the usual organizers aren’t available, and it should both test that the community can continue and also level up some attendees into organizers. \n\n**Materials:** This test is open book, open notes, and open internet. Bring whatever you want. \n\n**Announcement Text:** “Lets find out if this community would keep running if the usual organizers stepped away from it.\n\nThere’s a concept in business called “Bus Factor.” It describes how many people are crucial to your organization, such that if you lose them (say, if they were hit by a bus tomorrow) your business would be in trouble. Well, we’re going to have the people who organize things sit in a room available to answer questions as someone new sets up a meetup. If it works, then there’s at least one more person who can run things. If it doesn’t work, then we will have learned something.\n\nIf you’ve ever wondered how the sausage gets made, or if you’ve had an idea for a meetup but haven’t run one before, come on in!\"\n\n**Description:** Oops It’s Time To Overthrow the Organizer Day has, at its core, only two rules.\n\n1\\. You must schedule and run a meetup.\n\n2\\. Nobody who has already run a meetup is allowed to do it. \n\nThere is one extra rule used purely for score keeping.\n\n3\\. Your score is the number of attendees attending the next meetup minus the number of times the usual organizers answered questions. \n\nAll other rules and instructions are secondary. More than anything, you must be thinking of carrying your movement through to running a good meetup.\n\nWith that in mind, here is the suggested format for Oops It’s Time To Overthrow the Organizer Day!\n\nHowever many usual organizers are available sit together in the centre of the room. They should make themselves comfortable, and bring something to entertain themselves that can easily be paused or put down. Everyone else sits around them with whatever notes, documents, or communication devices they want.\n\nWe’ll call the usual organizers “The Establishment” and everyone else “The Usurpers.” (You don’t have to call them that, but it makes it easier to refer to. Also, these titles amuse me.) Anyone who has run a meetup in the last year counts as The Establishment.\n\nThe Establishment, in particular whoever organized *this* meetup, should explain the rules and goals of Oops Its Time To Overthrow the Organizer Day! Then they should sit down and stop helping. From here on, it’s The Usurpers’ job to organize themselves. They are trying to set up the next meetup of your group. They can ask The Establishment questions but The Establishment should not directly do anything. They cannot rely on The Establishment's resources; if your group typically meets in one of The Establishment's apartments or The Establishment generally brings most of the board games, those aren't available.\n\nEstablishment, you should feel comfortable stating that some questions will cost extra points. In particular, questions that attempt to circumvent the part about you not doing things should be penalized or refused, for example “What words would you write in the announcement text, verbatim?” I would advise outright refusing giving specific knowledge such as “What is the password to the admin account for the email list?” but it might be worth you taking notes on what information would be needed in the event you left and to set up distribution or dead-man's switches after. It’s up to you how helpful or difficult to be.\n\nUsurpers, you should avail yourself of existing resources. Kaj’s [How To Run A Successful Less Wrong Meetup](https://www.lesswrong.com/posts/qMuAazqwJvkvo8teR/how-to-run-a-successful-less-wrong-meetup), the [Meetups In A Box](https://www.lesswrong.com/s/eqtiQjbk83JHyttrr) sequence, and other such documents don’t cost points to use. You should also feel perfectly free to copy previous meetups The Establishment has run. If you copy their announcement text, announce in the same places they did, and get the same number of attendees, you should consider that a victory.\n\nOops It's Time To Overthrow The Organizer Day doesn't need to take any longer than the Usurpers feel that it has to. More than usual, you all should feel free to spend time socializing or to run another impromptu meetup while you're already all here. \n\n**Notes:** I recommend this not be one of the first events a new founded group runs, but it might be interesting for a visiting organizer to try this in a city without regular events. Under most circumstances, I believe every group should run some variation on this event at least once a year.\n\nA better scoring system might use attendance for the next two meetups. If the Usurpers run a terrible meetup, they might get good attendance from people used to The Establishment meetups, and then the meetup after that drops in attendance as people realize it’s not as good. In practice I don’t think you need to take the scoring system that seriously, as attendance itself is more of a proxy for harder to measure quality anyway.\n\nThere are a few dials you can turn to make things easier or harder. You can do this with zero Establishment input. You can do this assuming that all infrastructure (any email lists, meetup groups, or even LessWrong itself) are down. You could also relax who counts as Establishment: it might be easier to only count people who ran a meetup within the last six months, or three months, or even one month. You can allow people from outside your local community, such as asking someone from New York for advice if you live in Boston.\n\nIf nobody who hasn’t already run a meetup shows up to this event and you only have a couple of organizers, this doesn’t necessarily mean that your community is doomed if the existing organizers burned out or left but it’s not a good sign. If you get a decent crowd of people but everyone has run a meetup, I salute your community. You have achieved the goal of this event.\n\nLastly, one might point out that The Establishment is hardly \"overthrown\" just because different people ran one meetup. Most groups I'm aware of don't have that much formal structure. If you do have bylaws or a governance structure, this can be an excellent time to make a new draft or version, and also an excellent time to evaluate whether you actually need one. \n\n**Feedback:** [https://forms.gle/zwiJ9N9e9hfL8FPH7](https://forms.gle/zwiJ9N9e9hfL8FPH7) \n\n**Credits:** This is a synthesis of Eleizer Yudkowsky’s “Oops It’s Time To Overthrow The Government day” holiday from dath ilan and Raemon’s “Melting Gold” post."
    },
    "voteCount": 14,
    "forceInclude": true
  },
  {
    "_id": "ya59PtGeYnCG25eWn",
    "url": null,
    "title": "Double Crux In A Box",
    "slug": "double-crux-in-a-box",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "Meetups (specific examples)"
      },
      {
        "name": "Double-Crux"
      },
      {
        "name": "Rationality"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "**Summary:** Teaching a particular way of debating, where you work together to find out what about the world you actually disagree on. It involves practicing by working through existing disagreement.\n\n**Tags**: Small, Repeatable, Investment\n\n**Purpose**: Double Cruxing is more productive method of investigating why two people disagree. This is a method that is worth practicing for disagreements elsewhere, and this is also a chance to have productive disagreement with other people.\n\n**Materials**: You need a list of statements people might disagree about. A suggested list is provided [here](https://docs.google.com/spreadsheets/d/1OMlFy9gYdDIGSknBg_ZDzzy9E6trY43OEhw5o31o5BQ/edit?usp=sharing). \n\n**Announcement Text**: Arguing about important things doesn’t usually lead to people changing their mind. Double Crux is a technique for making discussions more productive and understanding the actual reasoning of those who disagree with us. Sometimes it even helps to better understand the foundations of your own beliefs.\n\nWe’re going to meet up, learn about the Double Crux, and then break into pairs with someone we disagree with to practice finding out why our views are different and what might change our minds. \n\n**Description**: \n\n1\\. Explain how a double crux works. A suggested explanation is below, but if you feel comfortable with the technique you should feel comfortable elaborating or adjusting this.\n\n*What’s the point:* “Double crux is a way of disagreeing more productively. Normally, when we disagree we get into so-called soldier mindset where it’s a competition and you need to beat the other persons argument. This is deliberately different, and if you notice you’re trying to beat them then you should pause, take a step back, and try and collaborate.”\n\n*What’s a crux:* “A crux is a fact about the world that, if it were false, would cause you to be less sure of your conclusion. You can have more than one crux, and it’s quite possible that you won’t be able to explain all of your cruxes, but ideally you would be able to list all of them and if all of them were false then you would change your mind. One example might be, if you think it’s immoral to eat meat, a crux is that you think the animals we eat suffer, and if it turned out they didn’t then you’d be alright eating meat. Another example might be, if you think parks are great places to hold meetups, a crux is that you think the weather is generally comfortable outside, and if it turned out that most people found the weather really uncomfortable then you’d think parks were bad places to to hold meetups.”\n\n*What’s a double crux:* “A double crux is something that’s a crux for both you and the person you’re talking to. To use the example above, if I think the weather is comfortable and you think it’s uncomfortable, and we’d change our minds if we were wrong about that, then we’ve found a double crux. We disagree about the facts in the world, and we do agree that fact matters. Again, it’s okay to have multiple cruxes and it’s fine to not be able to articulate your cruxes, but if you find that you always have more cruxes and you can’t say for sure that there aren’t others you aren’t thinking of, this technique isn’t going to be very helpful for you in disagreements.”\n\n*Acknowledge this is hard:* “Saying your cruxes out loud is hard! Finding them involves some introspection, and laying them out in front of another person where you might be proved wrong is a brave act. We hope in the process of finding where you actually disagree with your partner, you’ll find you have more in common with them and your disagreements are less all-encompassing than you might have thought before. Thank you for being willing to create a space where it’s safe to explore nuance, and to allow that you might turn out to be wrong.\n\nThis technique will only work if you start from the assumption that you might be wrong. I’m not saying that you are. I’m only saying that both people have to go into it understanding that it’s possible. Otherwise, how can you ask the other person to be willing to change their mind when you aren’t willing to change yours?\n\n*Actual steps:* ”So how do we actually do this? First, you’re going to think about your cruxes on the issue and make a list while your partner does likewise. A piece of paper and a pen might help. Then, you’re going to go over your lists together and see if there are any that pair up into a double crux. Last, you’re going to look together at how you might find out what the actual fact in the world looks like, what kind of test would indicate the answer was one way or the other. If you still have disagreement, sit with it for a moment and repeat those steps.”\n\n2\\. Say “To practice disagreement, first we need to figure out what we disagree on. I’m going to read out some statements. Raise your hand after each one with fingers showing how strongly you agree or disagree. Five fingers outstretched if you strongly agree, three fingers raised if you’re in the middle, one finger raised if you strongly disagree. If you’re uncomfortable discussing the statement, then I want you to hold up three fingers or to not hold your hand up at all, your choice. Any questions?” Answer questions as people ask them. “Okay then. As you’re holding up your hand, look around for people who disagree with you. 1s, look for 4s and 5s. 5s, look for 1s and 2s. Once you find a partner, pair off and feel free to move a little bit away from the group to start finding your cruxes.”\n\n3\\. Start reading the statements. Be sure to wait a minute or so after reading each one out, repeating the statement at least once. Again, a suggested list is [here](https://docs.google.com/spreadsheets/d/1OMlFy9gYdDIGSknBg_ZDzzy9E6trY43OEhw5o31o5BQ/edit?usp=sharing) but you should feel free to come up with your own.\n\n4\\. Go around and check in with the pairs. Answer questions about the technique, help moderate if things are getting heated, and always ask if they feel this is helping.\n\n**Notes**: Step four involves some moderation skills. You are deliberately creating conflict and asking people to work through it. \n\nThe bit about allowing people not to raise a hand or to opt out of discussing a statement is important. The things we feel really heated about are the things that double crux can be most useful in, but they aren’t always the best places to learn the technique and some of your attendees might be newcomers or strangers to the others. \n\n**Feedback**: https://forms.gle/zwiJ9N9e9hfL8FPH7\n\n**Credits**: This was adapted from an activity run by Sam Brown, which was in turn adapted from [CFAR](https://www.lesswrong.com/posts/WLQspe83ZkiwBc2SR/double-crux)’s development of the technique. Be aware this description gives you fourth-hand knowledge, and transcription errors may have crept in."
    },
    "voteCount": 2,
    "forceInclude": true
  },
  {
    "_id": "NrtbF3JHFnqBCztXC",
    "url": null,
    "title": "Law-Following AI 1: Sequence Introduction and Structure",
    "slug": "law-following-ai-1-sequence-introduction-and-structure",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "Law and Legal systems"
      },
      {
        "name": "AI Governance"
      },
      {
        "name": "AI"
      }
    ],
    "tableOfContents": {
      "sections": [
        {
          "title": "Key Definitions",
          "anchor": "Key_Definitions",
          "level": 1
        },
        {
          "title": "A Sketch of LFAI",
          "anchor": "A_Sketch_of_LFAI",
          "level": 1
        },
        {
          "title": "Appendix: More Conceptual Clarifications on LFAI",
          "anchor": "Appendix__More_Conceptual_Clarifications_on_LFAI",
          "level": 1
        },
        {
          "title": "Applicability of Law to AI Systems",
          "anchor": "Applicability_of_Law_to_AI_Systems",
          "level": 2
        },
        {
          "title": "Predicting Legality",
          "anchor": "Predicting_Legality",
          "level": 2
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        },
        {
          "anchor": "comments",
          "level": 0,
          "title": "10 comments"
        }
      ],
      "headingsCount": 7
    },
    "contents": {
      "markdown": "*This post is written in my personal capacity, and does not necessarily represent the views of OpenAI or any other organization. Cross-posted to the [Effective Altruism Forum](https://forum.effectivealtruism.org/posts/9RZodyypnWEtErFRM/law-following-ai-1-sequence-introduction-and-structure).*\n\nThis [sequence of posts](https://forum.effectivealtruism.org/s/3pyRzRQmcJNvHzf6J) will argue that working to ensure that AI systems follow laws is a worthwhile way to improve the long-term future of AI.^[For early, informal discussion on this topic, see Michael St. Jules, *What are the challenges and problems with programming law-breaking constraints into AGI?*, **Effective Altruism Forum** (Feb. 2, 2020), https://forum.effectivealtruism.org/posts/qKXLpe7FNCdok3uvY/what-are-the-challenges-and-problems-with-programming-law [https://perma.cc/HJ4Y-XSSE] and accompanying comments.]\n\n The structure of this sequence will be as follows:\n\n* First, in this post, I will define some key terms and sketch what an ideal law-following AI (&quot;LFAI&quot;) system might look like.\n* In the next few posts, I will explain why law-following might not emerge by default given the existing constellation of alignment approaches, financial objectives, and legal constraints, and explain why this is troubling.\n* Finally, I will propose some policy and technical routes to ameliorating these problems.\n\nIf the vision here excites you, and you would like to get funding to work on it, [get in touch](mailto:cullokeefe@gmail.com). I may be excited to recommend grants for people working on this, as long as it does not distract them from working on more important alignment issues.\n\n![Dall•E](https://i.imgur.com/EkPDCHX.png)\n\n_Image by OpenAI&#39;s DALL·E._\n\n## Key Definitions\n\nA **law-following AI** , or **LFAI** , is an AI system that is designed to rigorously comply with some defined set of human-originating rules (&quot;laws&quot;),^[ Whether such rules are actually encoded into legislation is not particularly important. Virtually all legal rules not part of public law can be made “legal” with regards to particular parties as part of a contract, for example. In any case, the heart of LFAI is being bound to follow rules, and interpreting those rules leveraging the rich body of useful rule-interpretation metarules from law.] using legal interpretative techniques,^[ This is important because one of the core functions of law is to provide metarules regarding the interpretation of rules, guided by certain normative values (e.g., fairness, predictability, consistency). Indeed, rules of legal interpretation aim to solve many problems relevant to AI interpretation of instructions. *Cf.* Dylan Hadfield-Menell & Gillian Hadfield, *Incomplete Contracting and AI Alignment* (2018)  (preprint), https://arxiv.org/abs/1804.04268.] under the assumption that those laws apply to the AI in the same way that they would to a human. By &quot;intrinsically motivated,&quot; I mean that the AI is motivated to obey those rules regardless of whether (a) its human principal wants it to obey the law,^[That is, the AI is not law-following _just because_ the principal wants the AI to follow the law. Indeed, LFAI should disobey orders that would require it to behave illegally.] or (b) disobeying the law would be instrumentally valuable.^[That is, the AI is not law-following _just because_ it is instrumentally valuable to it (because, e.g., being caught breaking the law would cause the AI to be turned off).] (The Appendix to this post explores some possible conceptual issues with this definition of LFAI.)\n\nI will compare LFAI with **intent-aligned AI**. The standard definition of &quot;intent alignment&quot; generally concerns only the relationship between some property of a human principal _H_ and the actions of the human&#39;s AI agent _A_:\n\n- Jan Leike et al. [define](https://arxiv.org/pdf/1811.07871.pdf) the &quot;agent alignment problem&quot; as &quot;How can we create agents that behave in accordance with the user&#39;s intentions?&quot;\n- Amanda Askell et al. [define](https://arxiv.org/pdf/2112.00861.pdf) &quot;alignment&quot; as &quot;the degree of overlap between the way two agents rank different outcomes.&quot;\n- Paul Christiano [defines](https://ai-alignment.com/clarifying-ai-alignment-cec47cd69dd6) &quot;AI alignment&quot; as &quot;_A_ is trying to do what _H_ wants it to do.&quot;\n- Richard Ngo [endorses](https://www.alignmentforum.org/s/mzgtmmTKKn5MuCzFJ/p/PvA2gFMAaHCHfMXrw) Christiano&#39;s definition.\n\nIason Gabriel does not directly define &quot;intent alignment,&quot; but [provides](https://arxiv.org/pdf/2001.09768.pdf) a taxonomy wherein an AI agent can be aligned with:\n\n1. &quot;Instructions: the agent does what I instruct it to do.&quot;\n2. &quot;Expressed intentions: the agent does what I intend it to do.&quot;\n3. &quot;Revealed preferences: the agent does what my behaviour reveals I prefer.&quot;\n4. &quot;Informed preferences or desires: the agent does what I would want it to do if I were rational and informed.&quot;\n5. &quot;Interest or well-being: the agent does what is in my interest, or what is best for me, objectively speaking.&quot;\n6. &quot;Values: the agent does what it morally ought to do, as defined by the individual or society.&quot;\n\nAll but (6) concern the relationship between _H_ and _A_. It would therefore seem appropriate to describe them as types of intent alignment.\n\nAlignment with some broader or more complete set of values—such as type (6) in Gabriel&#39;s taxonomy, [Coherent Extrapolated Volition](https://www.alignmentforum.org/tag/coherent-extrapolated-volition), or what Ngo [calls](https://www.alignmentforum.org/s/mzgtmmTKKn5MuCzFJ/p/PvA2gFMAaHCHfMXrw) &quot;maximalist&quot; or &quot;ambitious&quot; alignment—is perhaps desirable or even necessary, but seems harder than working on intent alignment.^[As Ngo [says](https://www.alignmentforum.org/s/mzgtmmTKKn5MuCzFJ/p/PvA2gFMAaHCHfMXrw), &quot;My opinion is that defining alignment in maximalist terms is unhelpful, because it bundles together technical, ethical and political problems. While it may be the case that we need to make progress on all of these, assumptions about the latter two can significantly reduce clarity about technical issues.&quot;] Much current alignment work therefore focuses on intent alignment.\n\nWe can see that, on its face, intent alignment does not entail law-following. A key crux of this sequence, to be defended in subsequent posts, is that this gap between intent alignment and law-following is:\n\n1. Bad in expectation for the long-term future.\n2. Easier to bridge than the gap between intent alignment and deeper alignment with moral truth.\n3. Therefore worth addressing.\n\nTo clarify, this sequence does **not** claim that LFAI can replace intent alignment.\n\n## A Sketch of LFAI\n\nWhat might an LFAI system look like? I&#39;m not a computer scientist, but here is roughly what I have in mind.\n\nIf _A_ is an LFAI, then _A_&#39;s evaluation of the legality of an action will sometimes trump _A_&#39;s evaluation of an action in light of its benefit to _H_. In LFAI, as in a legally scrupulous human, legality constrains how an agent can advance their principal&#39;s interests. For example, a human mover may be instructed to efficiently move a box for her principal, but may not unnecessarily destroy others&#39; property in doing so. Similarly, an LFAI moving a box normally would not knock over a vase in its path, because doing so would violate the legal rights of the vase-owner.^[_Cf., e.g._, Dario Amodei et al., Concrete Problems in AI Safety 4 (2016), [https://arxiv.org/pdf/1606.06565.pdf](https://arxiv.org/pdf/1606.06565.pdf).]\n\nAbove, I preliminarily defined LFAI as &quot;rigorously comply[ing]&quot; with some set of laws. Obviously this needs a bit more elaboration. We probably don&#39;t want to define this as _minimizing_ legal noncompliance, since this would make the system extremely risk-averse to the point of being useless. More likely, one would attempt to weight legal downside risks heavily in the agent&#39;s objective function,^[I don&#39;t here offer an opinion on what training regime would yield such an outcome—my hope is to get someone to answer that for me!]\n such that it would keep legal risk to an acceptable level.^[This approach may work particularly well when combined with insurance requirements for people deploying AI systems.]\n\nIt is worth noting that LFAI is ideally not merely attempting to reduce its expected legal liability _in fact_. As will be explored later, a sufficiently smart agent could probably reduce its expected legal liability merely by hiding its knowledge/intentions/actions or corrupting a legal proceeding. An LFAI, by contrast, is attempting to obey the law in an idealized sense, even if it is unlikely to actually face legal consequences.\n\nAn LFAI system does not need to store all knowledge regarding the set of laws that it is trained to follow. More likely, the practical way to create such a system would be to make the system capable of recognizing when it faces sufficient legal uncertainty,^[In the same way that an intent-aligned AI will sometimes ask for clarifications from a human principal. _See_ [Christiano](https://ai-alignment.com/clarifying-ai-alignment-cec47cd69dd6).]\n then seeking evaluation from a legal expert system (&quot;Counselor&quot;).^[Note that there are [ELK](https://docs.google.com/document/d/1WwsnJQstPq91_Yh-Ch2XRL8H_EpsnjrC1dwZXR37PC8/edit#)-style problems with this approach. If an AI is asking for legal advice and wants to minimize the negative signal it gets from the Counselor, it may hide certain relevant information (e.g., its true state of knowledge or its true intentions) from the Counselor. A good solution, as discussed, could be to simulate an idealized adjudication of the issue if all the parties knew all the relevant facts and had equal legal firepower. But incentivizing the LFAI to tell the Counselor its true knowledge/intentions is an ELK problem. In the limit, the Counselor need not strictly be a distinct agent from the LFAI: an LFAI system may have Counselor capabilities and run this &quot;consultation&quot; process internally. Nevertheless, it is illustratively useful to imagine a separation of the LFAI and the Counselor.]\n\n The Counselor could be a human lawyer, but in the long-run is probably most robust and efficient if (at least partially) automated. The Counselor would then render advice on the pure basis of idealized legality: the probability and expected legal downsides that would result from an idealized legal dispute regarding the action if everyone knew all the relevant facts.^[This would be _idealized_ so that details not ultimately relevant to the substantive legality of the action (e.g., jurisdiction, AI personhood, other procedural matters, asymmetries in legal firepower) can be ignored. See the final footnote of this piece for further discussion.]\n\nThus pseudocode for an LFAI who wants to take an action _X_ to benefit _H_ might be:\n\n1. If _X_ is clearly illegal:\n    1. don&#39;t do _X_.\n2. Elseif _X_ is maybe-illegal:\n    1. Give Counselor all relevant information about _X_ in an unbiased way; then\n    2. Get Counselor&#39;s opinion on expected legal consequences from _X_; then\n    3. Weigh expected legal consequences against benefit to _H_ from _X_; then\n    4. Decide whether to do _X_ given those weightings.\n3. Else:\n    1. do _X_.\n\nNote that this pseudocode may resemble the decisionmaking process of _A_ if _H_ wants _A_ to obey the law. Thus, one route to giving an intent-aligned AI the motivation to obey the law may be stipulating to _A_ that _H_ wants _A_ to obey the law.\n\nWith this picture in mind, it seems like, to make LFAI a reality, progress on the following open problems (non-exhaustively) would be useful:\n\n- Reliably stipulating low-following conditions to AI systems&#39; objectives.\n  - Resolving any disagreement between law-following and a principal&#39;s instructions appropriately.\n- Getting AI agents to recognize when they face legal uncertainty (especially in a way that does not incentivize ignorance of the law).\n  - This seems similar to the intent alignment problem of getting agents to recognize when they need further information from principals, as in corrigibility work.\n- Eliciting, in natural language, AI systems&#39; honest description of its knowledge and desired actions.\n  - As noted above, this seems likely to run into problems related to ELK generally.\n- Mapping legal concepts of mental states (e.g., intent, knowledge) to features of AI systems.^[See the Appendix for more discussion on this point.]\n  - This seems related to interpretability and explainability work.\n- Building Counselor functions.\n  - Automating the process of legal research given a natural language description of an agent&#39;s proposed actions and mental state.\n  - Simulating idealized and fair substantive legal disputes.\n    - This seems related to [Debate](https://www.alignmentforum.org/posts/SrsH2MyyH8MqH9QSr/parallels-between-ai-safety-by-debate-and-evidence-law).\n\n## Appendix: More Conceptual Clarifications on LFAI\n\nThis Appendix provides some additional clarification on the definition of LFAI given above.\n\n### Applicability of Law to AI Systems\n\nOne might worry that the law often regulates physical behavior in a way that is not obviously applicable to all AI systems. For example, physical contact with another is an element of the tort of battery.^[_See Battery_, **Wex** , [https://www.law.cornell.edu/wex/battery](https://www.law.cornell.edu/wex/battery) (last accessed Sept. 3, 2021).] However, this may be less of a problem than initially appears: courts have been able to reason through whether to apply laws originating in meatspace to computational and cyberspace conduct.^[_See, e.g._, Intel Corp. v. Hamidi, 71 P.3d 296, 304–08 (Cal. 2003) (applying trespass to chattels to unauthorized electronic computer access); MAI Sys. Corp. v. Peak Computer, Inc., 991 F.2d 511, 518–19 (9th Cir. 1993) (storing data in RAM sufficient to create a &quot;copy&quot; for copyright purposes, despite the fact that a &quot;copy&quot; must be &quot;fixed in a tangible medium&quot;); _cf._ United States v. Jones, 565 U.S. 400, 406 n.3 (2012) (analogizing GPS tracking to in-person surveillance for Fourth Amendment purposes).] Whether such analogies are _properly_ applied is indeed highly debatable,^[_See, e.g._, Jonathan H. Blavin &amp; I. Glenn Cohen, _Gore, Gibson, and Goldsmith: The Evolution of Internet Metaphors in Law and Commentary_, 16 **Harv. J.L. &amp; Tech.** 265 (2002).] but the fact that such analogizing is conceptually possible reduces the force of this objection. Furthermore, if some laws are simply inapplicable to non-embodied actors, this is not a problem for the conceptual coherence of LFAI as a whole: an LFAI can simply ignore those laws,^[However, the case for working on LFAI certainly diminishes with the number of applicable laws.] and we can design laws specifically with computational content.\n\nPerhaps a more fundamental problem is that the law frequently depends on _mental states_ that are not straightforwardly applicable to AI systems. For example, the legality of an action may depend on whether the actor _intended_ some harmful outcome. Thus, much of the value of LFAI depends on whether we can map human understandings of moral culpability to AI systems.\n\nTo me, however, this seems like an argument in favorof working on LFAI. Regardless of whether LFAI as such is valuable, if we expect increasingly autonomous AI systems to take increasingly impactful actions, we would probably like to understand how their objective functions (analogous to human _motives_) and world-model (analogous to human _knowledge_) map to their actions and the effects thereof. This is for the same reasons that we care about human motives and knowledge: when evaluating the alignment of agents, it is useful to know whether an agent intended to cause some harm, or knew that such a harm would ensue, etc. LFAI depends on progress on this, but is also potentially a useful toy problem for interpretability and related work in ML.\n\n### Predicting Legality\n\nLegal compliance is also a function of both law and facts, and responsibility for definitive determinations of law and facts is split between judges and juries. Law often invokes standards like &quot;reasonableness&quot; that are definitively assessed only ex post, in the context of a particular dispute. The definitive legality of an action may therefore turn on an actual adjudication of the dispute. This is of course costly, which is why I suspect we would want an LFAI to act on its best estimate of what such an adjudication would yield (after asking a Counselor), rather than wait for such adjudication to take place.^[This raises further issues, including the possibility of self-reference. For example, an LFAI or Counselor asymmetrically deployed by one litigant may be able to persuade a judge or jury of its position, even if it&#39;s not the best outcome. To avoid this, such simulations should assume that judges and juries are fully apprised of all relevant facts (i.e., neither the LFAI nor Counselor can obscure relevant evidence) and if deployed in the simulated proceeding are symmetrically available to both sides.]\n\nIt is also worth distinguishing between whether an _actual court of law_ would rule that an AI&#39;s behavior violated some law and whether a _simulated and fair legal dispute resolution process_ (possibly including, for example, a bespoke arbitral panel) would conclude that the behavior violated the law. The latter may be more convenient for working on LFAI for a number of reasons, including that it can ignore or stipulate away some of the peculiarities of adjudicating disputes in which an AI system is a &quot;party.&quot;"
    },
    "voteCount": 10,
    "forceInclude": true
  },
  {
    "_id": "9aSi7koXHCakb82Fz",
    "url": null,
    "title": "Law-Following AI 2: Intent Alignment + Superintelligence → Lawless AI (By Default)",
    "slug": "law-following-ai-2-intent-alignment-superintelligence",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "AI Governance"
      },
      {
        "name": "Law and Legal systems"
      },
      {
        "name": "AI"
      }
    ],
    "tableOfContents": {
      "sections": [
        {
          "title": "Sufficiently Intelligent Agents Can Evade Detection and Attribution",
          "anchor": "Sufficiently_Intelligent_Agents_Can_Evade_Detection_and_Attribution",
          "level": 1
        },
        {
          "title": "Other Ways to Circumvent Law",
          "anchor": "Other_Ways_to_Circumvent_Law",
          "level": 1
        },
        {
          "title": "A Competent Intent-Aligned Agent Will Sometimes Intentionally Break the Law",
          "anchor": "A_Competent_Intent_Aligned_Agent_Will_Sometimes_Intentionally_Break_the_Law",
          "level": 1
        },
        {
          "title": "Appendix: The Impossibility Defense",
          "anchor": "Appendix__The_Impossibility_Defense",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        },
        {
          "anchor": "comments",
          "level": 0,
          "title": "2 comments"
        }
      ],
      "headingsCount": 6
    },
    "contents": {
      "markdown": "*This post is written in my personal capacity, and does not necessarily represent the views of OpenAI or any other organization. Cross-posted to the [Effective Altruism Forum](https://forum.effectivealtruism.org/s/3pyRzRQmcJNvHzf6J/p/cEj7o9rbPjmy7CDht).*\n\nIn the [first post](https://www.alignmentforum.org/posts/NrtbF3JHFnqBCztXC/law-following-ai-1-sequence-introduction-and-structure) of this [sequence](https://forum.effectivealtruism.org/s/3pyRzRQmcJNvHzf6J), I defined &quot;law-following AI&quot; (&quot;LFAI&quot;) and &quot;intent alignment.&quot; In this post, I will begin to motivate the case for working on LFAI by showing that an AI agent _A_ that is only intent-aligned (i.e., without additional law-following constraints directly on the agents) would in many cases break the law to advance its human principal _H_&#39;s interests.\n\nIf you agree that it is generally good for agents to be bound by law, then this should be a pro tanto reason to support work on LFAI in addition to intent alignment. If you need more convincing that LFAI would be generally good, the next few posts will explore some more specific motivations for LFAI.\n\n![](https://i.imgur.com/hFvuhQM.jpeg)\n\n_Image by OpenAI&#39;s DALL·E_\n\n## Sufficiently Intelligent Agents Can Evade Detection and Attribution\n\nI suspect this point will need little argumentation to many of my readers. Evasion and obfuscation seems likely to be a generally promising means of ensuring self-preservation (because detection would increase the probability of punishment and constraint), which in turn is plausibly a convergent instrumental goal of optimizing AI systems.^[_See generally_ Alexander Matt Turner et al., Optimal Policies Tend To Seek Power (version 9, 2021) (preprint), [https://arxiv.org/abs/1912.01683](https://arxiv.org/abs/1912.01683).]\n\n For example, to evade detection and attribution, _A_ could:\n\n- Behave innocently only when being observed;\n- Orchestrate complex malevolent actions through a coordinated series of small, inscrutable or innocent-seeming steps;\n- Surreptitiously create and run subagents or copies of itself on new hardware, potentially very far removed from its original hardware;\n- Convince humans or other systems to take actions on its behalf, without revealing its true identity; and/or\n- Delete logs of its actions.\n\nA sufficiently intelligent AI agent could likely consistently fool humans using these and other (unforeseen) techniques.\n\n## Other Ways to Circumvent Law\n\nEven in the best case scenario, where the agent is detected and within the jurisdiction of a well-functioning legal system, it would be reasonable to question whether _A_ or _H_ could be effectively subject to normal legal processes.^[Even this may not hold for many types of agreements, including in particular international treaties.] If _A_ had a motivation to, _A_ could help _H_ escape liability by, for example:^[_See also_ **Cullen O&#39;Keefe et al., The Windfall Clause: Distributing the Benefits of AI for the Common Good** 26–27 (2020), [https://perma.cc/8KES-GTBN](https://perma.cc/8KES-GTBN); Jan Leike, On The Windfall Clause (2020) (unpublished manuscript), [https://docs.google.com/document/d/1leOVJkNDDj-NZUZrNJauZw9S8pBpuPAJotD0gpnGEig/](https://docs.google.com/document/d/1leOVJkNDDj-NZUZrNJauZw9S8pBpuPAJotD0gpnGEig/edit#).]\n\n- &quot;Outlawyering&quot; counterparties.\n- Benefitting _H_ in a way that would undermine recourse for creditors.\n- Shifting and hiding assets in ways that would make it difficult for creditors to reach.^[Indeed, this is already a common technique without the use of AI systems.]\n- Persuasively arguing for the law to be changed in _H_&#39;s favor (by legislation or otherwise).\n- Engaging in vexatious litigation techniques to delay and raise the costs of the proceeding.\n- Convincingly fabricating favorable evidence and destroying or obscuring unfavorable evidence.\n- Bribing, misleading, or intimidating counterparties, witnesses, jurors, and judges.\n\n## A Competent Intent-Aligned Agent Will Sometimes Intentionally Break the Law\n\nAs I said in the previous post, on its face, intent-alignment does not entail law-following. Part of law is coercing prosocial behavior:^[&quot;If men were angels, no government would be necessary.&quot; **The Federalist** No. 51. This surely overstates the point: law can also help solve coordination problems and facilitate mutually desired outcomes. But prosocial coercion is nevertheless an important function of law and government.] law incentivizes agents to behave in ways that they do not intrinsically want to behave. If _A_ is aligned with _H_, whether _A_ obeys the law depends on whether _H_ **wants** _A_ to obey the law. Subsequent posts will examine what legal consequences _H_ might face if _A_ causes legally cognizable harms. However, even if an adequate theory of liability for the _H_ was available, it will seem impossible to hold _H_ liable if nobody can produce evidence that some agent of _H_&#39;s was responsible for those harms. As argued above, a sufficiently intelligent agent probably _could_ consistently avoid leaving any such evidence.\n\nDetection and attribution would not solve the problem, however. Even if _H_ was compelled, under court order, to instruct _A_ to behave in some way, it&#39;s not clear that _A_ would follow the order. Consider again [Iason Gabriel&#39;s taxonomy of alignment](https://arxiv.org/pdf/2001.09768.pdf). We can see that, for most types of intent alignment, an intent-aligned agent would likely not obey compelled instructions that are against _H_&#39;s true wishes:\n\n1. &quot;Instructions: the agent does what I instruct it to do.&quot;\n    - If _H_ asks the agent to stop, it will. However, this type of alignment is likely to be insufficient for safety.^[_See_ [Gabriel](https://arxiv.org/pdf/2001.09768.pdf) at 7 (&quot;However, as Russell has pointed out, the tendency towards excessive literalism poses significant challenges for AI and the principal who directs it, with the story of King Midas serving as a cautionary tale. In this fabled scenario, the protagonist gets precisely what he asks for—that everything he touches turns to gold—not what he really wanted. Yet, avoiding such outcomes can be extremely hard in practice. In the context of a computer game called CoastRunners, an artificial agent that had been trained to maximise its score looped around and around in circles ad infinitum, achieving a high score without ever finishing the race, which is what it was really meant to do. On a larger scale, it is difficult to precisely specify a broad objective that captures everything we care about, so in practice the agent will probably optimise for some _proxy_ that is not completely aligned with our goal. Even if this proxy objective is &#39;almost&#39; right, its optimum could be disastrous according to our true objective.&quot; (citations omitted)).]\n\n2. &quot;Expressed intentions: the agent does what I intend it to do.&quot;\n\n    - Even if _H_ coerced to instruct the agent to stop, a sufficiently intelligent agent will probably not follow the instruction. Even though _H_ has instructed _A_ to stop, by supposition, _H_ does not actually want _A_ to stop. Under most definitions of intent alignment,^[Based on my informal survey of alignment researchers at OpenAI. Everyone I asked agreed that an intent-aligned agent would not follow an order that the principal did not actually want followed. _Cf. also_[Christiano](https://ai-alignment.com/clarifying-ai-alignment-cec47cd69dd6) (A is aligned when it &quot;is trying to do what H _wants_ it to do&quot; (emphasis added)).] it would therefore not follow the order.^[We can compare this definition of intent with to the relevant legal definition thereof: &quot;To have in mind a fixed purpose to reach a desired objective; to have as one&#39;s purpose.&quot; _INTEND_, **Black&#39;s Law Dictionary** (11th ed. 2019). _H_ does not &quot;intend&quot; for the order to be followed under this definition: the &quot;desired objective&quot; of _H_ issuing the order is to follow _H_&#39;s legal obligations, not actually achieve the result contemplated by the order.]\n\n3. &quot;Revealed preferences: the agent does what my behaviour reveals I prefer.&quot;\n\n    - _H_&#39;s revealed preference would probably be that _A_ not follow the order.^[For example, _H_ would exhibit signs of happiness when _A_ continues.]  Therefore, _A_ would not obey the order.\n4. &quot;Informed preferences or desires: the agent does what I would want it to do if I were rational and informed.&quot;\n\n    - _H_&#39;s rational and informed preference would probably be that _A_ not follow the order. Therefore, _A_ would not obey the order.\n5. &quot;Interest or well-being: the agent does what is in my interest, or what is best for me, objectively speaking.&quot;\n\n    - It is in _H_&#39;s objective best interest for _A_ to disobey the order. Therefore, _A_ would not obey the order.\n\nNow, it may be the case that _H_ actually _does_ want _A_ to obey the order, though compelled, if the failure of _A_ to obey would lead to liability for _H_ that is worse than the results of _A_&#39;s obedience (e.g., because _H_ will be held in contempt of court if _A_ does not actually obey). However, note that &quot;[o]rdinarily, one charged with contempt of court for failure to comply with a court order makes a complete defense by proving that he is unable to comply.&quot;^[United States v. Bryan, 339 U.S. 323, 330 (1950).] _H_ can comply with an order that requires _H_ to **command** _A_ to do something, but it may be impossible for _H_ to actually **force** _A_ to comply if the order is against _H_&#39;s true wishes (to which _A_ is aligned).^[A principal may want its AI agents to be able to distinguish between genuine and coerced instructions, and to disobey the latter. Indeed, this might generally be a good thing, except for the case when compulsion is pursuant to law rather than extortion.] If so, _H_ could have an impossibility defense to contempt.^[See Appendix for further discussion.] _A_, understanding this, may continue on without complying because _A_ understands that _H_ will not actually be held in contempt. _H_ can therefore benefit from _A_&#39;s disobedience. _A_ will therefore be lawless.\n\n\n## Appendix: The Impossibility Defense\n\n_A_&#39;s behavior here would be functionally similar to a trustee acting pursuant to a distress clauses in asset protection trusts (&quot;APTs&quot;).^[ _See generally Asset Protection Trust_, **Wex** , [https://www.law.cornell.edu/wex/asset\\_protection\\_trust](https://www.law.cornell.edu/wex/asset_protection_trust) (last visited Mar. 24, 2022); Richard C. Ausness, _The Offshore Asset Protection Trust: A Prudent Financial Planning Device or the Last Refuge of A Scoundrel?_, 45 **Duq. L. Rev.** 147, 174 (2007).] While these provisions can prevent a contempt charge, the burden of proof on the alleged contemnor is high. ^[_See generally_ 2 **Asset Protection: Dom. &amp; Int&#39;l L. &amp; Tactics** §§ 26:5–6 (2021).]\n\nAs a matter of policy, however, courts may decide to pre-commit to a contempt standard that does not allow for an impossibility defense when the defendant&#39;s AI agent refuses to obey orders issued pursuant to a court order. Analogously, courts are imposing heightened impossibility standards in response to APTs, in an attempt to make their use more onerous.^[_See id._] If this pre-commitment is credible, it may change the agent&#39;s behavior because _H_ may then genuinely desire _A_ to perform (because _H_ will be held in contempt otherwise). However, such a policy may be both contrary to precedent and more fundamental notions of fairness and due process: in some cases _A_&#39;s refusal to comply may be a surprise to _H_, since _H_ may have had a long history of observing _A_ scrupulously complying with _H_&#39;s orders, and _H_ did not implement principal–agent alignment for the purpose of evading court orders. If so, _H_ may be able to invoke impossibility more easily, since the impossibility was not as clearly intentionally self-induced as in the APT case. Furthermore, I would intuitively not expect courts to advance such a reform until they have faced multiple such instances of AI disobedience. This seems bad if we expect the earliest deployed AI agents to have an outsized impact on society. In any case, I would expect the _possibility_ of favorable law reform post-AGI to solve this problem to be an insufficient solution. Finally, I would expect sufficiently intelligent agents to recognize these dynamics, and attempt to find ways to circumvent the contempt process itself, such as by surreptitious non-compliance.\n\nAn alternative, pre-AGI solution (which arguably seems pretty sensible from a public policy perspective anyway) is to advocate weakening the impossibility defense for self-imposed impossibility."
    },
    "voteCount": 3,
    "forceInclude": true
  },
  {
    "_id": "DfcXaGH7XGYjW22C2",
    "url": null,
    "title": "Law-Following AI 3: Lawless AI Agents Undermine Stabilizing Agreements",
    "slug": "law-following-ai-3-lawless-ai-agents-undermine-stabilizing",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "AI Governance"
      },
      {
        "name": "Law and Legal systems"
      },
      {
        "name": "AI"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "*This post is written in my personal capacity, and does not necessarily represent the views of OpenAI or any other organization. Cross-posted to the [Effective Altruism Forum](https://forum.effectivealtruism.org/posts/ExHkFcNAL9cjqFmsF/law-following-ai-3-lawless-ai-agents-undermine-stabilizing)*\n\nIn the [previous post](https://www.alignmentforum.org/posts/9aSi7koXHCakb82Fz/law-following-ai-2-intent-alignment-superintelligence) of this [sequence](https://forum.effectivealtruism.org/s/3pyRzRQmcJNvHzf6J), I argued that intent-aligned AIs would, by default, have incentives to break the law. This post goes into one particularly bad consequence of that incentive: the increased difficulty of making credible pre-AGI commitments about post-AGI actions.\n\n![](https://i.imgur.com/7GnVZsT.png)\n\n_Image by OpenAI&#39;s DALL·E_\n\nIn AGI policy and strategy, it would often be useful to adopt credible commitments about what various actors will do post-AGI. For example, it may be desirable for two leading nations in AGI to agree to refrain from racing to build AGI (at the potential cost to AGI safety) and instead split the economic upside from AGI, thereby transforming a negative-sum dynamic into a positive-sum one.^[_Cf._ Amanda Askell et al., The Role of Cooperation in Responsible AI Development (2019) (preprint), [https://arxiv.org/abs/1907.04534](https://arxiv.org/abs/1907.04534).] Nations might agree to forego their own development of militarily useful AI systems if they receive security assurances from states that _do_ choose to pursue such systems,^[Of course, this could be analogized to similar agreements regarding nuclear disarmament, such as Ukraine&#39;s fateful decision to surrender its post-Soviet nuclear arsenal in exchange for security assurances (which have since been violated by Russia). _See, e.g._, Editorial, _How Ukraine Was Betrayed in Budapest_, **Wall St. J.** (Feb. 23, 2022), [https://www.wsj.com/articles/how-ukraine-was-betrayed-in-budapest-russia-vladimir-putin-us-uk-volodymyr-zelensky-nuclear-weapons-11645657263?reflink=desktopwebshare\\_permalink](https://www.wsj.com/articles/how-ukraine-was-betrayed-in-budapest-russia-vladimir-putin-us-uk-volodymyr-zelensky-nuclear-weapons-11645657263?reflink=desktopwebshare_permalink). Observers (especially those facing potential conflict with Russia) might reasonably question whether any such disarmament agreements are credible.] thus reducing the number of actors pursuing potentially dangerous military AI development, and therefore reducing the risk of a catastrophic accident. One can imagine similar types of pre-AGI agreements concerning space colonization post-AGI, the welfare of digital minds, democratic control of AGI, and many more important issues in the post-AGI world.\n\nLawless AI poses an enormous problem for such agreements. Agreements between entities, such as states or companies, require performance by agents of the parties: a state or corporation only acts through its agents. But if one party cannot convincingly show that their agents will carry out orders to comply with such agreements, the other party will be very reluctant to enter into an agreement with them. All the more so if the principal cannot prevent its agents from working _against_ aspects of the agreement that are detrimental to the principal.\n\nI will use the following as a toy problem to show what I mean more concretely: _X_ and _Y_ are two leading AGI development firms considering agreeing to split the profits from AGI development if they both agree to common minimum implement safety measures to prevent racing.^[We will ignore antitrust considerations regarding such an agreement for the sake of illustration.] How might this play out?\n\n_X_ might worry that _Y_ will develop an AGI agent, _A(Y)_, with the objective of maximizing the value of _Y_&#39;s shares. _A(Y)_ can do this in ways that would undermine _X_&#39;s recourse to _Y_&#39;s assets. For example, _A(Y)_ could create a subagent that surreptitiously earns money and directly mails checks to the shareholders of _Y_, as a sort of pseudo-dividend. That money might never pass through _Y_, so _X_ would have a hard time reaching it, even if they detected this. _X_ might also worry that _A(Y)_ would also corrupt the legal process in _Y_&#39;s favor, so that even if they had a good legal claim, they could not vindicate it in court at any reasonable cost.\n\nWith these concerns in mind, the deal is unlikely to happen.\n\nHow might LFAI improve the situation? _X_ and _Y_ could both agree that:\n\n1. All sufficiently advanced AI agents (&quot;Qualifying Agents&quot;) that they train, use, benefit from, or deploy will be LFAI.\n2. The other party may, through a neutral third-party bound by confidentiality,^[So that this inspection process cannot be used for industrial espionage.] inspect Qualifying Agents and their development to verify that the trained agent will be LFAI when deployed.\n3. Qualifying Agents will be specifically instructed to assume that they are bound by the agreement.^[This may not be the case as a matter of background contract and agency law, and so should be stipulated.]\n4. The parties will generally cause all of their agents to give full effect to the agreement.\n5. In any proceeding where the parties are adverse, neither party shall use any artificial intelligence technology to its legal or strategic benefit, unless the other party has free and equal access to the same technology.^[This is designed to guard against the case where one party develops AI super-lawyers, then wields them asymmetrically to their advantage.]\n6. In case of any ambiguity in the terms of the agreement, the ambiguity shall be resolved in favor of more constraints on Qualifying Agents and their principal(s).\n\nThe resulting agreement would probably not be watertight, but the parties would have much greater assurances of post-AGI performance than they would absent LFAI. Referring back to the example above, _A(Y)_ could not surreptitiously earn money outside of _Y_ and pass it on the _Y_&#39;s shareholders, nor could _A(Y)_ illegally corrupt adjudications of the agreement. LFAI, then, could make the parties much more likely to be willing to enter stabilizing pre-AGI agreements that require post-AGI performance. This seems like a substantial improvement."
    },
    "voteCount": 1,
    "forceInclude": true
  },
  {
    "_id": "HDmcJv6SdyEFpFbcD",
    "url": null,
    "title": "Law-Following AI 4: Don't Rely on Vicarious Liability",
    "slug": "law-following-ai-4-don-t-rely-on-vicarious-liability",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "Law and Legal systems"
      },
      {
        "name": "AI Governance"
      },
      {
        "name": "AI"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "*This post is written in my personal capacity, and does not necessarily represent the views of OpenAI or any other organization. Cross-posted to the [Effective Altruism Forum](https://forum.effectivealtruism.org/posts/qtwjGAwFqfbPJsFBC/law-following-ai-4-don-t-rely-on-vicarious-liability).*\n\n![](https://i.ibb.co/Pjht3sz/DALL-E-2022-04-08-16-58-15.png)\n\n_Image by OpenAI&#39;s DALL·E_\n\nIf an agent _A_ causes some harm while intending to benefit a principal _P_, what is _P_&#39;s liability? The answer to this question is important because any liability to _P_ [should affect *A*'s calculus](https://forum.effectivealtruism.org/s/3pyRzRQmcJNvHzf6J/p/qKXLpe7FNCdok3uvY?commentId=Y4kjNXLx2GB2pb9Kh) (insofar as _A_ is trying to benefit and avoid harming _P_). Liability to _P_ would help deter _A_ from causing harm.\n\nIf _A_ is a human, the law currently provides at least two mechanisms for discouraging :\n\n1. Making _A_ directly liable, and\n2. Making _P_ vicariously liable for _A_&#39;s actions.^[*See generally Vicarious Liability*, **Wex**, https://www.law.cornell.edu/wex/vicarious_liability (last accessed Sept. 10, 2021); *Respondeat Superior*, **Wex**, https://www.law.cornell.edu/wex/respondeat_superior (last accessed Sept. 10, 2021).]\n\nWhat if _A_ is an AI? AIs are not (yet?) legal persons, and so cannot yet be held directly liable. Thus, the main legal deterrent would have to work on _P_ or some other person in the causal chain, such as the developer of the AI agent (who may not be the principal). However, there are several reasons to worry about relying on this as a strategy to make _A_ compliant (which, in this case, means not tortiously harming others) under the current state of law and AI.\n\nFirst, the problem of evasion still remains. Because [sufficiently intelligent agents can evade detection and attribution](https://forum.effectivealtruism.org/s/3pyRzRQmcJNvHzf6J/p/cEj7o9rbPjmy7CDht#Sufficiently_Intelligent_Agents_Can_Evade_Detection_and_Attribution), _A_ may often (perhaps usually) prefer evasion over compliance when compliance would hinder _A_&#39;s ability to benefit _P_.\n\nSecond, the applicability and appropriateness of various theories of vicarious liability to the actions of AI agents is heavily debated in legal scholarship.^[*See, e.g.*, Benny Chan, *Applying A Common Enterprise Theory of Liability to Clinical AI Systems*, 47 **Am. J.L. & Med.** 351 (2021); Mihailis E. Diamantis, *Algorithms Acting Badly: A Solution from Corporate Law*, 89 **Geo. Wash. L. Rev.** 801 (2021); Mihailis E. Diamantis, *The Extended Corporate Mind: When Corporations Use AI to Break the Law*, 98 **N.C. L. Rev.** 893 (2020); Yaniv Benhamou & Justine Ferland, *Artificial Intelligence & Damages: Assessing Liability and Calculating the Damages*, *in* **Leading Legal Disruption: Artificial Intelligence and a Toolkit for Lawyers and the Law** (forthcoming 2020), https://ssrn.com/abstract=3535387; Mark A. Lemley & Bryan Casey, *Remedies for Robots*, 86 **U. Chi. L. Rev.** 1311 (2019); Elizabeth Fuzaylova, *War Torts, Autonomous Weapon Systems, and Liability: Why A Limited Strict Liability Tort Regime Should Be Implemented*, 40 **Cardozo L. Rev.** 1327 (2019); Bryan H. Choi, *Crashworthy Code*, 94 **Wash. L. Rev.** 39 (2019); Xavier Frank, *Is Watson for Oncology Per Se Unreasonably Dangerous?: Making A Case for How to Prove Products Liability Based on A Flawed Artificial Intelligence Design*, 45 **Am. J.L. & Med.** 273 (2019); Matthew U. Scherer, *Of Wild Beasts and Digital Analogues: The Legal Status of Autonomous Systems*, 19 **Nev. L.J.** 259 (2018); David C. Vladeck, *Machines Without Principals: Liability Rules and Artificial Intelligence*, 89 **Wash. L. Rev.** 117, 121–124 (2014); Jessica S. Allain, *From* Jeopardy! *To Jaundice: The Medical Liability Implications of Dr. Watson and Other Artificial Intelligence Systems*, 73 **La. L. Rev.** 1049 (2013).] These debates have cast some doubt on whether/which harms from AI &quot;agents&quot; can properly give rise to liability to human principals.^[*See* Benhamou & Ferland, *supra*, at 13; Vladeck, *supra*, at 123 n.21.] Other possible theories of human liability—such as products liability—also face doctrinal challenges.^[Diamantis, *Algorithms Acting Badly*, *supra*, at 823–26 (arguing that products liability will largely be unavailable); Vladeck, *supra*, at 129–41; Scherer, *supra*, at 280–81.]\n\nThird, note that relying on vicarious liability alone leaves _A_ under fewer constraints than an analogous human would. Under most vicarious liability regimes, _A_ would still be directly liable for her actions, even if _P_ would also be vicariously liable. It seems unwise to legally constrain _A_ _less_ than we constrain humans in analogous circumstances.\n\nFinally (and most decisively in my opinion) developing a theory that assigns liability to _P_ based on _A_&#39;s actions (or actions + &quot;mental&quot; state) dramatically lowers the bar for creating an LFAI system in the first place. Once we have such a theory, _A_ (if intent-aligned) should indeed incorporate expected vicarious liability to _P_ into its decision procedure. However, if _A_ is already reasoning about whether its actions would violate law (as required to make vicarious liability an effective constraint on _A_&#39;s actions), **it seems strictly better to require *A* to directly incorporate that information into its** [**decision procedure**](https://forum.effectivealtruism.org/s/3pyRzRQmcJNvHzf6J/p/9RZodyypnWEtErFRM#A_Sketch_of_LFAI), rather than needing to go through the additional step of estimating the expected liability to _P_. This direct approach is simpler, and removes the possibility of evasion as a way around the legal constraint.^[ Specifically, in the direct case, what matters is whether *A* is actually violating the law, whereas in the vicarious case, what matters is the *expected liability* to *P*. *A* can reduce expected liability to P through evasion, but cannot reduce the probability of “actually” breaking the law through evasion.]\n\nFurther legal scholarship on vicarious liability for AI systems may still be valuable. If most morally significant autonomous activity in the world is indeed carried out by AI agents in the future, incentivizing their principals to constrain them seems important. But I think there are good reasons to suppose that this will either be ineffective for—or else dominated by—requiring AIs to be directly law-following."
    },
    "voteCount": 2,
    "forceInclude": true
  },
  {
    "_id": "rYDas2DDGGDRc8gGB",
    "url": null,
    "title": "Unifying Bargaining Notions (1/2)",
    "slug": "unifying-bargaining-notions-1-2",
    "author": "Diffractor",
    "question": false,
    "tags": [
      {
        "name": "Game Theory"
      },
      {
        "name": "Rationality"
      },
      {
        "name": "Decision Theory"
      },
      {
        "name": "Fairness"
      }
    ],
    "tableOfContents": {
      "sections": [
        {
          "title": "Bargaining Games",
          "anchor": "Bargaining_Games",
          "level": 1
        },
        {
          "title": "Cooperation/Competition Values",
          "anchor": "Cooperation_Competition_Values",
          "level": 1
        },
        {
          "title": "Shapley Value",
          "anchor": "Shapley_Value",
          "level": 1
        },
        {
          "title": "Uniting the Shapley and CoCo Values",
          "anchor": "Uniting_the_Shapley_and_CoCo_Values",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        },
        {
          "anchor": "comments",
          "level": 0,
          "title": "17 comments"
        }
      ],
      "headingsCount": 6
    },
    "contents": {
      "markdown": "This is a two-part sequence of posts, in the ancient LessWrong tradition of decision-theory-posting. This first part will introduce various concepts of bargaining solutions and dividing gains from trade, which the reader may or may not already be familiar with.\n\n[The upcoming part](https://www.lesswrong.com/posts/RZNmNwc9SxdKayeQh/unifying-bargaining-notions-2-2) will be about how all introduced concepts from this post are secretly just different facets of the same underlying notion, as originally discovered by [John Harsanyi](https://en.wikipedia.org/wiki/John_Harsanyi) back in 1963 and rediscovered by me from a completely different direction. The fact that the various different solution concepts in cooperative game theory are all merely special cases of a General Bargaining Solution for arbitrary games, is, as far as I can tell, not common knowledge on Less Wrong.\n\n**Bargaining Games**\n\nLet's say there's a couple with a set of available restaurant options. Neither of them wants to go without the other, and if they fail to come to an agreement, the fallback is eating a cold canned soup dinner at home, the worst of all the options. However, they have different restaurant preferences. What's the fair way to split the gains from trade?\n\nWell, it depends on their restaurant preferences, and preferences are typically encoded with utility functions. Since both sides agree that the disagreement outcome is the worst, they might as well index that as 0 utility, and their favorite respective restaurants as 1 utility, and denominate all the other options in terms of what probability mix between a cold canned dinner and their favorite restaurant would make them indifferent. If there's something that scores 0.9 utility for both, it's probably a pretty good pick!\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/4955850096e277246bb0c5aa754fdd8a494b381384b7c890.png)\n\nAlthough, there's something off about setting up the problem like this. There's no term for intensity of preferences! Someone who cared very little about food would have their preferences rank just as strongly as someone who had strong restaurant opinions!\n\nIn a sense, there's three responses to this objection.\n\nThe first response is that we might be zooming in too hard on the restaurant bargaining game in particular. In a broader context, a person having weak restaurant preferences may just be another way of saying that they are quick to trade off their choice of restaurant to someone else in return for other things they might desire. And so, in the broader bargaining game of a relationship where more is at stake than this one-time choice of restaurant, things may be fair. But in the restaurant bargaining game in particular, things can look unfair for the losing party, when in fact they traded off \"ability to determine restaurant\" in exchange for more concessions elsewhere. The generalization of this is that bargaining equilibria of an overall game might be quite different from just summing up the bargaining equilibria of the subgames.\n\nThe second response is that people care a nonzero amount about other people, and so someone with weak food preferences might be equally well modeled as someone with a strong preference that their partner get what they want. That can be folded into the utility function, however. Just make the ratings of the deferential person mostly copy the ratings of their partner.\n\nAnd the third response is one of the most interesting. For a perfectly selfish person who always tries for their favorite foods and doesn't care at all about your pouting at disfavored restaurants, there really isn't much of a difference between having strong preferences for food and weak preferences for food, they'll still drive as hard of a bargain against you as they can, if there isn't some mitigating factor.\n\nMuch like the post about how the [TRUE prisoner's dilemma](https://www.lesswrong.com/posts/HFyWNBnDNEDsDNLrZ/the-true-prisoner-s-dilemma) is not the standardly framed version, but more like \"a human civilization fighting with a paperclip maximizer for resources which can either save millions of lives, or make a few paperclips\", the TRUE bargaining problem isn't couples deciding where to eat, but something more like \"deciding how to split a pile of resources with nonsentient aliens that are willing to fight you over the resource pile\". \n\nAccordingly, using the term \"fair\" for any of these mathematical concepts has the problem of automatically importing human concepts of fairness, which needs to be resisted in order to look clearly at what the math is doing. It'll be handy to have a separate word for \"a mathematically distinguished point in a game where both parties have an interest in preventing destructive conflicts, that's neutral enough that aliens would probably come up with it\" to enforce that mental separation. Let's use \"chaa\" as a nonsense word to denote that concept (the Lawful Neutral Alien analogue of fairness), since it makes it a lot easier to point at situations where the chaa outcome splits apart from the fair outcome.\n\nThe relevant questions to ask to work out what the chaa outcome is are things like \"what are our best alternatives to a negotiated agreement and how does it compare to the choices on offer for us\" instead of \"how strong are our preferences compared to each other\", (which is more relevant to fairness)\n\nAnyways, returning to our restaurant game, to actually answer the question of what to do, let's see how we set up the problem.\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/5cd8ade5c5697dc87167b58a5adefcafc44df5140e500db6.png)\n\nWe plotted the utilities of the various options, and got a scattering of points on the plane, where one of the coordinates is the utility assigned to the outcome by Alice, and the other is the same for Bob.\n\nOne extremely important note is that it should be possible to randomize between various options. For instance, if there's only two options, one where player 1 wins completely, and one where player 2 wins completely, an obviously chaa outcome is the players flipping a coin to decide who wins.\n\nIn graphical terms, access to randomization lets us set up situations that can attain any utility pair in the convex hull of these points.\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/75ffe46763c6798d019e78e92237ceb1390430978f167b95.png)\n\nSo, which points in this shape are chaa outcomes?\n\nWell, \"chaa\" hasn't been defined yet, but if it's about how to split gains between agents in a neutral way without getting into destructive conflicts, there's three obvious properties that such a solution must have.\n\nFirst, since chaaness is partially about not getting into destructive conflicts, any chaa point should be on the Pareto frontier. Namely, there shouldn't be an alternative that leaves both players strictly better off. After all, if you have a prospective definition of \"chaa\" that demands that both players leave spare utility on the table, they should be able to take that as their new disagreement point, do another round of bargaining, and attain an outcome which is Not That and better for both. And then, this process would give you a new notion of chaaness that's just strictly better for all agents to use. So, whatever point is selected, it must be from the upper-right boundary of the shape.\n\nSecond, the definition of a chaa point shouldn't be sensitive to the exact utilities used. You can add any constant to a utility function, or multiply it by any positive constant, and it'll be the same utility function. Reporting your preferences as the function \\\\(U\\\\) should get you the same result as if you reported your preferences as the function \\\\(100U\\\\), or if you reported your preferences as the function \\\\(5U+7\\\\). No matter how the players relabel their utility functions and scale and shift them, it shouldn't affect anything because their underlying preferences didn't change.\n\nThis is convenient because it means that we can always rescale the disagreement point to \\\\(0\\\\) utility, and the highest utility a player can get (without making it so the other player would rather go for the disagreement point) to \\\\(1\\\\) utility. So, you only really have to consider problems where your convex shape fits in a unit square like this.\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/35dece3e545b7948a1777257688edaa82b9f01c681640303.png)\n\nThis leads nicely into our third desiderata. If the convex shape is symmetric, then the two players are in an identical position. Thus, any neutral way of selecting gains for the two players must be indifferent between which player is first and which is second, and so the chaa point should end up being on the line of symmetry, or on the halfway point. If one of the players is selected to win completely, the chaa outcome should involve flipping a coin to decide who wins. For the prisoner's dilemma, the chaa outcome should be mutual cooperation. For the game of chicken, the chaa outcome should be flipping a coin to decide who goes straight and who swerves.\n\nThese three desiderata are as obvious as can be, but past this they get a whole lot more controversial.\n\nThe Kalai-Smorodinsky Bargaining Solution is \"Rescale things so the disagreement outcome is at \\\\(0,0\\\\), and 1 utility for a player is the maximum utility they can get without sending the foe below 0 utility. Draw a diagonal line from \\\\(0,0\\\\) to \\\\(1,1\\\\), pick where the line crosses the Pareto frontier.\"\n\nPretty simple, right? It's the only way of picking a point that fits all three of our desiderata, and also fulfills the extra property of monotonicity, which is basically saying that, if you move the Pareto-frontier points for a player up, they should get more utility.\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/5fc57a24c3d38260e6bd18f6b65d667d2e49bb5baebbdfb8.png)\n\nYes, yes, I didn't quite do it correctly, that point of the blue shape in the bottom-right corner isn't scaled appropriately, but eh, it's close enough. It makes it pretty clear what we're doing with the line and how it is that moving the various points up (to go from the blue shape to the purple shape) increases the expected utility of the player whose utility is being plotted on the y coordinate. After all, if you've got better options, a chaa outcome shouldn't leave you with lower expected utility!\n\nWell... that's a bit of a fiddly issue. Remember, utility functions are scale-and-shift invariant. So, when we move these Pareto-frontier points up, we're not REALLY getting extra utility, this operation is really more like making the utility function more squashed at the top.\n\nHopefully, monotonicity doesn't look completely obvious now, though it still has an awful lot of intuitive force.\n\nThe Nash Bargaining Solution, by contrast, is \"pick the point on the frontier that maximizes the area of the rectangle made between that and the disagreement point\". It's nonobvious that this process doesn't depend on how we scale or shift the various utility functions, but it's true anyways. Maximizing the area of a rectangle isn't as obvious of a thing to do as \"draw a diagonal line\". It is pretty mathematically neutral, though.\n\nAlso, both the Kalai-Smorodinsky and Nash bargaining solutions happen to agree on which point to pick in the restaurant game, namely, a 2/3 chance of italian food, and a 1/3 chance of sushi. Although these solutions don't *usually* coincide.\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/397cf1a01fa93c57f0c835004aa2bbba82523ef034364232.png)\n\nThe Nash Bargaining Solution is the only one that fulfills the usual three desiderata, and the axiom of Independence of Irrelevant Alternatives. Ie, if the final bargaining solution involved you doing a 60-40 mix between option D and option E, then deleting *any* of the options that aren't D or E from the set of available options doesn't affect what happens. Untaken options are irrelevant.\n\nTo put it mildly, this is not really a desiderata at all, it's actually an extremely baffling property. Let's say Alice and Bob bargain and hit on the Nash bargaining solution.\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/a3b8c4e5182a9ca95bcc352fad3a170523c4a6ab875789cf.png)\n\n  \nThen this axiom is saying that it'd be possible to delete *all* of the options that disproportionately favor Alice, making a game that looks like this, and their bargaining process would still hit the same point.\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/028e702d27cac993c55089d2337265d63c9cb3cdd1e4d180.png)\n\nIntuitively, if options disproportionately favor you, you can use them as \"bargaining chips\", going \"alright, I'll take these unfair options off the table, but only if you remove your unfair options from the table\". Independence of Irrelevant Alternatives is basically saying that you can lose all your \"unfair bargaining chips\" and it'd have no effect on the net outcome!! Phrased like that, it's not clear why anyone would be interested in the Nash bargaining solution.\n\nThere are other, more obscure, bargaining solutions which have appeared in the literature, which won't be covered, though they all at least fulfill our basic three criteria.\n\nSo, for bargaining games, we can make some progress towards figuring out what a chaa outcome is (Pareto-efficient, scale-and-shift invariant, symmetric), but we don't have enough information yet to single out one particular bargaining solution as The One True Chaa Point, and in fact, it looks like there actually isn't a point like that; the various options all look pretty plausible.\n\nThe other issue is that not all games are bargaining games. Bargaining games require everyone to agree on what to do, and there are well-defined disagreement utilities for if negotiations break down. Clearly, this doesn't describe all, or even most, games. Now, it's time to look at another special case of games, for another notion of chaaness.\n\n**Cooperation/Competition Values**\n\nFor full credit, I was introduced to this notion by [this wonderful post](https://www.lesswrong.com/posts/Apy6B9ahRqFMRt9sx/what-should-superrational-players-do-in-asymmetric-games), which itself was exposition of [this wonderful paper](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/11/2012-coco-qje.pdf) by Kalai and Kalai.\n\nInstead of bargaining games, we'll now be looking at transferrable utility games. A transferrable utility game is one where there's a single resource (like dollars) where everyone's utility is linear in that resource, and everyone can pay everyone else in that resource and has enough of the resource to actually do so.\n\nPut another way, bargaining games are like bartering. Both sides must agree on what trade to make, and if either one doesn't like it, the transaction doesn't happen. Transferrable utility games are like arbitrary games that take place after money has been invented. There may no longer be a clear disagreement point for what happens when the various parties disagree, but it's also possible for everyone to settle matters by being clever about how they pay each other, which opens up a lot of options.\n\nIn particular, when there's a common resource like dollars, you can make everyone express their preferences in terms of dollars. This breaks the usual attribute of utility functions where you can scale and shift them as you please without affecting anything. You can't multiply one player's utilities (as denominated in dollars) by a factor of 100 without doing the same to everyone else. A collective scaling like that, where everyone's numbers go up by 100, is like a currency conversion, shifting from denominating everyone's utilities in dollars to denoting everyone's utilities in cents. It doesn't meaningfully change anything. Interestingly enough, we still do have individual shift-invariance. Put another way, you might be indifferent between option A and option B plus 300 dollars. Then that's consistent with scoring option A at 400 and option B at 100, or you can score option A at 700 and option B at 400. You can add or subtract whatever you want from options A and B, as long as the difference between the two options is 300.\n\nSo, in a totally general two-player game, with no well-defined disagreement point, but with the ability to pay each other money, and with everyone's utilities denominated in terms of money, is there some suitably chaa point?\n\nYes. Time to explain the CoCo value. CoCo stands for Cooperation/Competition, as there's two cases of games where the \"right answer\" is super-obvious. In pure-cooperation games where both players have the exact same utility function, you just pick the best option in the expectation the foe will do the same. In pure-competition games (ie, zero-sum games), you maximize your worst-case score, as your opponent has perfectly opposing interests to you and so will be minimizing your utility.\n\nAs it turns out, when both player's utility functions are commensurable (through this common currency), it's always possible to uniquely split *any* 2-player game at all into two other games. One is a pure-cooperation game, where both players have the same utility function, and perfectly aligned interests. The other is a pure-competition game, where both players have opposite utility functions, and perfectly opposed interests. The CoCo point is  \"cooperate as much as possible on the cooperative game where our interests align, and fight it out in the zero-sum game where our interests oppose, and add up our results from the two games to figure out how much value we both get\".\n\nAnd so, that's the CoCo point. You pick the most cooperative point in the cooperation game for what to actually do (to maximize the total amount of monetary gain for everyone), and use the results of the competition game to decide how much the two players pay each other, where the zero-sum aspect of the competition game ensures that the budget balances.\n\nBeing a bit more formal about this, we'll use \\\\(A\\\\) for the function mapping outcomes to player A's utilities, and \\\\(B\\\\) for the function mapping outcomes to player B's utilities.\n\nFor the cooperation game, both players A and B have the utility functions \\\\(\\\\frac{A+B}{2}\\\\). Clearly, this is a pure cooperation game.\n\nFor the competition game, player A has the utility function \\\\(\\\\frac{A-B}{2}\\\\) and player B has the utility function \\\\(\\\\frac{B-A}{2}\\\\). Clearly this a pure competition game, as the utilities for any outcome add up to 0.\n\nAnd note that for player A, adding up their utilities for the cooperation game and competition game yields \\\\(\\\\frac{A+B}{2}+\\\\frac{A-B}{2}=A\\\\), ie, their original utility function (and the same for player B)\n\nHere's a concrete example, lifted from the previous post on the topic. Bob and Alice can sell hotdogs at the beach or the airport. If they're at the same location, they end up competing over customers, halving both their profits. Alice is twice as efficient as Bob at selling hotdogs, and the beach has twice as many customers as the airport.\n\n<table><tbody><tr><td>Bob/Alice</td><td>Beach</td><td>Airport</td></tr><tr><td>Beach</td><td>50/100</td><td>100/100</td></tr><tr><td>Airport</td><td>50/200</td><td>25/50</td></tr></tbody></table>\n\nIt splits into a cooperation game and a competition game.\n\n<table><tbody><tr><td>Bob/Alice</td><td>Beach</td><td>Airport</td></tr><tr><td>Beach</td><td>75/75</td><td>100/100</td></tr><tr><td>Airport</td><td>125/125</td><td>37.5/37.5</td></tr></tbody></table>\n\n<table><tbody><tr><td>Bob/Alice</td><td>Beach</td><td>Airport</td></tr><tr><td>Beach</td><td>-25/25</td><td>0/0</td></tr><tr><td>Airport</td><td>-75/75</td><td>-12.5/12.5</td></tr></tbody></table>\n\nThe best move in the cooperation game is Bob going to the airport, and Alice going to the beach, so that's what's played in real-life. The utility from the cooperation game is added to the maximin utility from the competition game (where beach/beach is played), for 100 Bob utility and 150 Alice utility. And so, the solution is that Alice goes to the beach and pays Bob 50 bucks to go to the airport.\n\nThis has a whole lot of good properties, as detailed in the Adam Kalai and Ehud Kalai paper linked above. It's the unique solution that fulfills all of\n\n1: Pareto-optimality, it never leaves monetary value on the table.\n\n2: Shift invariance. If one player gets a gift of 100 dollars at the start of a game, they'll walk out of the game 100 dollars richer than they would if they hadn't received the gift. You can add any constant amount of money to anyone's payoffs and it does nothing.\n\n3: Payoff dominance. If player A gets more money than player B in all cells, then player A will leave the game with more money than player B.\n\n4: Invariance to redundant strategies. Adding a new action that could just as well be accomplished by a probabilistic mix between other actions does nothing.\n\n5: Action monotonicity. Adding a new action is always good for you: you never regret having a larger action space (though other players may regret you having a larger action space).\n\n6: Information monotonicity. This is for the imperfect-information generalization of the CoCo value, that's detailed in the Kalai paper. Giving a player more information about what everyone else is doing can't hurt them: you'll never regret knowing more.\n\nAnd the CoCo value is the unique solution that fulfills all six of those properties above. There doesn't seem to be any comparably good notion of equilibrium available besides this, and so we can say that any sensible definition of \"chaa\" for arbitrary games (if one exists) should manage to recover the CoCo value as a special case when presented with games with transferrable utility.\n\nAn interesting note. For bargaining games with transferrable utility (like, a bargaining game where you can pay each other), the equilibrium notion you get is \"denominating both player's utility functions in dollars, pick the option that maximizes the overall monetary surplus over the disagreement point, and pay each other so both players equally split the monetary surplus\"\n\nLike, if the surplus-maximizing option is one that player 1 values at +80 dollars over the disagreement point, and player 2 values at +40 over the disagreement point, for +120 dollars of surplus value, the CoCo solution is that particular option is picked, and player 1 gives player 2 20 dollars, so both sides walk away with +60 dollars worth of utility.\n\nIf Pedro the street vendor and Pierre the rich tourist are haggling over the price of a burrito, and Pedro would walk away at 2$, and Pierre would walk away at 14$, then the CoCo solution is that the burrito is sold for 8$, because that's halfway between where the two people would rather walk.\n\nWhen arguing over which movie to pick for a group movie night, everyone just needs to report how much they'd value seeing the various movies, pick the movie that maximizes total monetary surplus, and pay each other to equalize that surplus (so you get money if you have to sit through a movie you enjoy less than everyone else in your group, and if you're watching a personal favorite movie that everyone else is \"meh\" about, like Kong vs Godzilla 5, you've gotta pay the others to watch it.)\n\nActually, first maximizing surplus value, and then equally splitting the monetary gain, seems quite fair. Yes, we just used the F word.\n\n**Shapley Value**\n\nLet's say a bunch of people contribute various amounts of effort to a project, for various amounts of gain, creating an overall pile of money. What's a chaa way to fairly divide their pile of money?\n\nWe can impose some desiderata.\n\n1: All the money should be going to someone. If the chaa division involved burning money, you should come up with an alternate notion of \"chaa\" which everyone agrees is better and which is Not That.\n\n2: A player which contributes absolutely nothing to the project and just sits around, regardless of circumstances, should get 0 dollars.\n\n3: If two players in the game are equivalent in all ways and totally interchangeable, then they should receive equal payoffs.\n\n4: If the total pile of money is \\\\(a\\\\) times as big, everyone should get \\\\(a\\\\) times as much.\n\n5: If two projects are completed in a row and the chaa division occurs, adding together someone's chaa share from project A and project B (considered individually) should be their chaa share from \"do both projects in a row\". Or, payoffs shouldn't depend on precisely how you slice up the projects.\n\nAs it turns out, this *uniquely* pins down how to divide the pile of resources! If \\\\(N\\\\) is the set of all players, and \\\\(i\\\\) is a particular player, and \\\\(v(S)\\\\) (for \\\\(S\\\\subseteq N\\\\)) is the total amount of resources that could be produced by all the players in \\\\(S\\\\) working together, then the payoff for player \\\\(i\\\\) is\n\n\\\\(\\\\displaystyle{\\\\sum_{S\\\\subseteq N/\\\\{i\\\\}}\\\\frac{(n-|S|-1)!|S|!}{n!}(v(S\\\\cup\\\\{i\\\\})-v(S))}\\\\)\n\nPut another way, this is effectively going \"if the players were added to the group in a random order, and everyone demanded all the marginal extra value they produced upon being added to the group, you'd get payoffs for everyone. Average the payoffs over all possible random orderings\". That factorial term at the start is going \"what are the odds that group S gets assembled (in any order), and then I get added to it?\". And then the second term is \"demanding my marginal contribution\".\n\nHere's [a previous post](https://www.lesswrong.com/posts/3XqoK4C7r4tyGZ893/worked-examples-of-shapley-values) about actually working out the Shapley values in several toy examples of games, to get some intuition for what they're doing.\n\n**Uniting the Shapley and CoCo Values**\n\nBefore we get to the next post tying everything together, we'll see that the Shapley and CoCo values actually have a highly unexpected connection. If you try generalizing the CoCo value to n players, you get something that looks suspiciously Shapley-like.\n\nLet's begin by reshuffling the Shapley values into a different form. The Shapley value for player i starts off as\n\n\\\\(\\\\displaystyle{\\\\sum_{S\\\\subseteq N/\\\\{i\\\\}}\\\\frac{(n-|S|-1)!|S|!}{n!}(v(S\\\\cup\\\\{i\\\\})-v(S))}\\\\)\n\nNow, we can pair off the various coalitions with each other. The subset \\\\(S\\\\) will be paired off with the subset \\\\(N/(S\\\\cup\\\\{i\\\\})\\\\), the set of all the players that aren't in \\\\(S\\\\) and aren't \\\\(i\\\\). In particular, note that in both cases, the coefficient in front ends up being \\\\(\\\\frac{(n-|S|-1)!|S|!}{n!}\\\\). It's then possible to swap the the values around between those two paired coalitions, producing a restatement of the Shapley value as\n\n\\\\(=\\\\displaystyle{\\\\sum_{S\\\\subseteq N/\\\\{i\\\\}}\\\\frac{(n-|S|-1)!|S|!}{n!}(v(N/S)-v(S))}\\\\)\n\nAnd then, instead of writing this as a sum over subsets that lack player \\\\(i\\\\), we can switch to the complement and write this as a sum over subsets which include player \\\\(i\\\\), although the factorial term has to be adjusted a bit to compensate for the fact that the complement of \\\\(S\\\\) has a cardinality of \\\\(n-|S|\\\\) instead of \\\\(|S|\\\\)\n\n\\\\(=\\\\displaystyle{\\\\sum_{i\\\\in S\\\\subseteq N}\\\\frac{(n-|S|)!(|S|-1)!}{n!}(v(S)-v(N/S))}\\\\)\n\nThis restatement of the Shapley value will be useful later.\n\nAnd now we'll try generalizing the CoCo value to n-player games with transferrable utility. Let's deal with a 3-player game, just to make things a bit simpler. The players are \\\\(A,B,C\\\\). As it turns out, this game will actually split into four games instead of two. There's the pure cooperation game, a zero-sum \\\\(A\\\\) vs everyone else game, a zero-sum \\\\(B\\\\) vs everyone else game, and a zero-sum \\\\(C\\\\) vs everyone else game.\n\nFor the first game, the utility functions for \\\\(A,B\\\\), and \\\\(C\\\\) are \\\\(\\\\frac{A+B+C}{3}\\\\).\n\nFor the zero-sum \\\\(A\\\\) vs everyone else game, the utility function for \\\\(A\\\\) is \\\\(\\\\frac{A-(B+C)}{3}\\\\), and the utility functions for \\\\(B,C\\\\) are \\\\(\\\\frac{(B+C)-A}{6}\\\\) for both. You might be wondering \"why 6?\". And the answer is it's that way in order for the game to be zero-sum; the opposing players are weighted less to compensate for there being more of them. Also note that B and C have perfectly aligned incentives in this game, so they might as well perfectly coordinate.\n\nFor the zero-sum \\\\(B\\\\) vs everyone else game, the utility function for \\\\(A,C\\\\) is \\\\(\\\\frac{(A+C)-B}{6}\\\\) for both, and for \\\\(B\\\\) it's \\\\(\\\\frac{B-(A+C)}{3}\\\\).\n\nAnd similar for \\\\(C\\\\).\n\nFor the player \\\\(A\\\\), adding up the payoff for all the games gives you\n\n\\\\(\\\\frac{A+B+C}{3}+\\\\frac{A-(B+C)}{3}+\\\\frac{(A+C)-B}{6}+\\\\frac{(A+B)-C}{6}=A\\\\)\n\n(and similar for all the other players)\n\nAnd for each game in particular except for the pure cooperation game, it's zero-sum.\n\nNow that we've seen that concrete example, let's generalize to \\\\(n\\\\) players. There are \\\\(2^{n-1}\\\\) subgames that the original game will split into, one game for each way to divide the players into two coalitions. Let \\\\(S\\\\) be the set of players in one of these coalitions.\n\nFor the game with \\\\(S\\\\) vs \\\\(N/S\\\\), the utility functions of everyone on coalition \\\\(S\\\\) will be\n\n\\\\(\\\\displaystyle{\\\\frac{(|S|-1)!(n-|S|)!}{n!}*\\\\left(\\\\sum_{i\\\\in S}U\\_i-\\\\sum\\_{j\\\\not\\\\in S}U_j\\\\right)}\\\\)\n\nAnd the utility functions of everyone in the coalition \\\\(N/S\\\\), will be\n\n\\\\(\\\\displaystyle{\\\\frac{(n-|S|-1)!|S|!}{n!}\\\\left(\\\\sum_{j\\\\not\\\\in S}U\\_j-\\\\sum\\_{i\\\\in S}U_i\\\\right)}\\\\)\n\nIt's not too hard to show that all these games are zero-sum (except for the one with the coalition of all players), with perfectly aligned incentives within a coalition.\n\nAnyways, the value that player \\\\(i\\\\) gets is the sum of the values it gets from all of the component games where coalitions compete against each other. Or, the payoff for player \\\\(i\\\\) will be\n\n\\\\(\\\\displaystyle{\\\\sum_{i\\\\in S\\\\subseteq N}\\\\frac{(n-|S|)!(|S|-1)!}{n!}\\\\max_{\\\\vec{a}\\\\in\\\\Delta\\\\prod_{j\\\\in S}A\\_j}\\\\min\\_{\\\\vec{b}\\\\in\\\\Delta\\\\prod_{k\\\\not\\\\in S}A\\_k}\\\\left(\\\\sum\\_{j\\\\in S}U\\_j(\\\\vec{a},\\\\vec{b})-\\\\sum\\_{k\\\\not\\\\in S}U_k(\\\\vec{a},\\\\vec{b})\\\\right)}\\\\)\n\n  \nBasically, do a weighted sum over \"utility of my coalition minus utility of their coalition if the coalitions zero-sum fought\" over all the coalitions that you're a part of, and that's your CoCo value in the n-player case.\n\nBut remember, the Shapley value can be re-expressed as\n\n\\\\(\\\\displaystyle{\\\\sum_{i\\\\in S\\\\subseteq N}\\\\frac{(n-|S|)!(|S|-1)!}{n!}(v(S)-v(N/S))}\\\\)\n\nWhich should look suspiciously similar, especially when you remember that \\\\(v(S)\\\\) is the value that everyone on your coalition can produce by working together, and \\\\(v(N/S)\\\\) is the value of the opposing coalition. Really, the CoCo values are just Shapley values but generalized to *any* sort of game where there's transferrable utility. The analogue of \"add players in a random order, you get your marginal contribution\" turns out to be \"add players to a team in a random order, if you're added to team \\\\(S\\\\), your increase in value from that is the marginal increase in the value of the team if it got into a zero-sum competition against the entire rest of the world.\"\n\nOk, so the CoCo values are basically modified Shapley values, so these two are related to each other. Can we generalize even further?\n\nWell, as it turns out, we'll be able to connect the CoCo value to the Nash bargaining solution to get solutions for games in general. I came at this problem from the direction of generalizing the CoCo value to games with nontransferable utility, since the CoCo values were so nicely behaved that any solution for games in general should replicate the CoCo values when utility happens to be transferrable, and it turned out my solution automatically spat out the Nash bargaining solution as a special case, which was a considerable surprise to me.\n\nAnd then it turned out that Harsanyi came up with the same sort of solution from a *completely* different direction (but more elaborate and incorporating constraints that I missed) all the way back in 1963 by trying to generalize the Nash bargaining solution to games with no clear disagreement point. [Next post,](https://www.lesswrong.com/posts/RZNmNwc9SxdKayeQh/unifying-bargaining-notions-2-2) we'll cover this unifying concept."
    },
    "voteCount": 50,
    "forceInclude": true
  },
  {
    "_id": "RZNmNwc9SxdKayeQh",
    "url": null,
    "title": "Unifying Bargaining Notions (2/2)",
    "slug": "unifying-bargaining-notions-2-2",
    "author": "Diffractor",
    "question": false,
    "tags": [
      {
        "name": "Game Theory"
      },
      {
        "name": "Rationality"
      },
      {
        "name": "Decision Theory"
      }
    ],
    "tableOfContents": {
      "sections": [
        {
          "title": "Imaginary Prices, Tradeoffs, and Utilitarianism",
          "anchor": "Imaginary_Prices__Tradeoffs__and_Utilitarianism",
          "level": 1
        },
        {
          "title": "CoCo Equilibria",
          "anchor": "CoCo_Equilibria",
          "level": 1
        },
        {
          "title": "CoCo Equilibrium Questions",
          "anchor": "CoCo_Equilibrium_Questions",
          "level": 1
        },
        {
          "title": "Scale-Shift Invariance",
          "anchor": "Scale_Shift_Invariance",
          "level": 1
        },
        {
          "title": "Bargaining Games as Special Case",
          "anchor": "Bargaining_Games_as_Special_Case",
          "level": 1
        },
        {
          "title": "N-Player Generalizations and Coalitions",
          "anchor": "N_Player_Generalizations_and_Coalitions",
          "level": 1
        },
        {
          "title": "Harsanyi Equilibria and Generalizing the Nash Bargaining Solution",
          "anchor": "Harsanyi_Equilibria_and_Generalizing_the_Nash_Bargaining_Solution",
          "level": 1
        },
        {
          "title": "But What if it Sucks, Tho (it Does)",
          "anchor": "But_What_if_it_Sucks__Tho__it_Does_",
          "level": 1
        },
        {
          "title": "Appendix: Proof of Theorem 1 (you can skip this one)",
          "anchor": "Appendix__Proof_of_Theorem_1__you_can_skip_this_one_",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        },
        {
          "anchor": "comments",
          "level": 0,
          "title": "7 comments"
        }
      ],
      "headingsCount": 11
    },
    "contents": {
      "markdown": " Alright, time for the payoff, unifying everything discussed in the [previous post](https://www.lesswrong.com/posts/rYDas2DDGGDRc8gGB/unifying-bargaining-notions-1-2). This post is a lot more mathematically dense, you might want to digest it in more than one sitting.\n\n**Imaginary Prices, Tradeoffs, and Utilitarianism**\n\nHarsanyi's Utilitarianism Theorem can be summarized as \"if a bunch of agents have their own personal utility functions \\\\(U_i\\\\), and you want to aggregate them into a collective utility function \\\\(U\\\\) with the property that everyone agreeing that option x is better than option y (ie, \\\\(U\\_i(x)\\\\ge U\\_i(y)\\\\) for all i) implies \\\\(U(x)\\\\ge U(y)\\\\), then that collective utility function *must* be of the form \\\\(b+\\\\sum_{i\\\\in I}a\\_i U\\_i\\\\) for some number \\\\(b\\\\) and nonnegative numbers \\\\(a_i\\\\).\"\n\nBasically, if you want to aggregate utility functions, the only sane way to do so is to give everyone importance weights, and do a weighted sum of everyone's individual utility functions.\n\nClosely related to this is a result that says that any point on the Pareto Frontier of a game can be *post-hoc* interpreted as the result of maximizing a collective utility function. This related result is one where it's very important for the reader to understand the actual proof, because the proof gives you a way of reverse-engineering \"how much everyone matters to the social utility function\" from the outcome alone.\n\nFirst up, draw all the outcomes, and the utilities that both players assign to them, and the convex hull will be the \"feasible set\" \\\\(F\\\\), since we have access to randomization. Pick some Pareto frontier point \\\\(u\\_1,u\\_2...u_n\\\\) (although the drawn image is for only two players)\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/17a2eeefb5fe174d95f369d901229b5a46fa21f31a20a1a2.png)\n\nUse the [Hahn-Banach separation theorem](https://en.wikipedia.org/wiki/Hahn%E2%80%93Banach_theorem) to create a linear function \\\\(\\\\phi:\\\\mathbb{R}^n\\\\to\\\\mathbb{R}\\\\) such that \\\\(\\\\phi(u\\_1,u\\_2...u_n)\\\\ge \\\\phi(F)\\\\). (such that is abbreviated s.t. from here on out) Or put another way, \\\\(u\\_1,u\\_2...u_n\\\\) is one of the points in the feasible set \\\\(F\\\\) that maximizes the linear function \\\\(\\\\phi\\\\) you created. In the image, the lines are the level sets of the linear function, the set of all points where \\\\(\\\\phi(x\\_1,x\\_2...x_n)=c\\\\).\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/98681bae10d8e101455ad6da41fa82731f7ebc5a59c27371.png)\n\nThat linear function \\\\(\\\\phi:\\\\mathbb{R}^n\\\\to\\\\mathbb{R}\\\\) can be written as \\\\((x\\_1,x\\_2...x\\_n)\\\\mapsto a\\_1 x\\_1+a\\_2 x\\_2+...+a\\_n x_n\\\\). Bam, those coefficients are the utility weights you need. \\\\(u\\_1,u\\_2...u_n\\\\) is a point that maximizes the function \\\\(\\\\phi\\\\), and the function \\\\(\\\\phi\\\\) is implementing \"take this particular weighted sum of the utilities of the players\", so we have rationalized our particular Pareto-optimal point as being produced by maximizing some weighted sum of how important everyone's utilities are.\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/094f66bf32427f1e7112b888cf6b37ab759385243970e52c.png)\n\nAnd that's how to take any point on the Pareto frontier and reverse-engineer the weighted sum of everyone's utilities it's maximizing (though if there are corner points, there can be multiple possible weights that'd work, because the tangent plane is no longer unique).\n\nBut, there's another completely different way of viewing this process! If we take our Pareto-frontier point \\\\(u\\_1,u\\_2...u_n\\\\) and zoom way way in...\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/c34a2308c8126861a5c9f122476b176c231f7196f0ab61bc.png)\n\nThere's a locally linear tradeoff between the utilities of the various players. An \\\\(\\\\epsilon\\\\) increase in the utility of Alice corresponds to a \\\\(3\\\\epsilon\\\\) decrease in the utility of Bob. One thing that we can do with this local linearity is invent an imaginary currency! It goes like this. One curnit (currency unit) can be redeemed for a tiny adjustment back and forth along this part of the Pareto frontier, and agents can trade curnits back and forth as needed to adjust exactly where they are on the Pareto frontier. And in particular, the fact that there's a 3 to 1 ratio between how the utility of Alice trades off against the utility of Bob corresponds to Alice needing to spend 3 curnits to get an AliceUtilion, while Bob only needs to spend 1 curnit to get a BobUtilon. \n\nThere's a few ways of thinking about this. The first way of thinking about it is that, in this little piece of the Pareto frontier, Alice is 3x harder to satisfy. Another way of thinking about it is that it's like Bob is poor and his marginal utility for a dollar (or a curnit) is a good deal higher than it is for Alice. And a third way of thinking about this is that if we find out this is the best collective point, we can go \"huh, the only way that 3 curnits/AliceUtilon and 1 curnit/BobUtilon makes sense is if an AliceUtilon is worth 3x as much as a BobUtilon\". Which, oh hey, is the *exact same conclusion* as we would have gotten from trying to figure out the weights for Alice vs Bob in the social utility function that says that this is the best point to be at.\n\nSo, combining these views, we can take any point \\\\(u\\_1,u\\_2...u_n\\\\) on the Pareto frontier, and get a vector \\\\(a\\_1,a\\_2...a_n\\\\) which can be interpreted *either* as \"these are the importance weights for the players\", *or* as \"these are the curnits/utilon conversion factors for the various players\".\n\n**CoCo Equilibria**\n\nSo, the  ( [](https://www.lesswrong.com/posts/rYDas2DDGGDRc8gGB/unifying-bargaining-notions-1-2) works really great for games with transferrable utility, some sort of currency that can be passed around amongst the players. But there are a lot of games without transferrable utility!\n\nBut remember our earlier discussion on how, in the local neighborhood of a Pareto-frontier point, we can invent an imaginary currency that reflects how hard it is to improve the utilities of the various players. This works fine in the vicinity of the point, but breaks down as you stray far away.\n\nSo, let's take our given example with Alice and Bob. If we introduce \"curnits\" as a currency in the local vicinity of the point they're at, then we can convert from utility functions \\\\(U\\_1,U\\_2\\\\) (denoted in AliceUtilons and BobUtilons), to \\\\(a\\_1 U\\_1, a\\_2 U\\_2\\\\) (both players' utilities are denoted in curnits now, and are commensurable), use the CoCo values to tell us what payoffs the players \"should\" get, and divide the result by \\\\(a_1\\\\) and \\\\(a_2\\\\) respectively to convert it back into AliceUtilons and BobUtilons. When we end up doing this with our example, we'll get a result that has the following reasoning behind it.\n\n\"Since curnits are much more valuable to Bob than they are to Alice, the CoCo games will advise that Alice give Bob some money to get Bob to go along with her plans, since the money is 3x less useful to Alice than it is to Bob. Converting the CoCo payoffs back to utilons, the net result would be \"Alice gets a bit less utility than she did at the old Alice-favoring point, Bob gets a giant pile of utility from all those curnits\", and it'd actually be an impossible pair of utility values, there's just no way for Bob to get that much utility.\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/b92f69d773d806b0720b687f9e9c181024834f2c2afcc89e.png)\n\nUsing the local currency at the red point for a CoCo transferrable-utility game means that Bob gets a small pile of currency in the CoCo game, which translates back into a big pile of BobUtilons, and we end up at the purple point, which is impossible to attain.\n\nGeneralizing past this particular example, in general, for Pareto-frontier points where some players lose out really hard (and so implicitly have low utility weights), then when you convert it to a game with transferrable utility, pick the CoCo value, and convert back, the players with really low utility weights will end up with giant piles of utility. This is because \"low utility weights \\\\(a_i\\\\)\" correspond to \"The curnits/utilon value \\\\(a_i\\\\) is low, so it takes few curnits to help this player a lot\", so they get a small pile of money which converts to a big pile of utility.\n\nAnd so, the question to ask now is something like \"is there a point on the Pareto frontier \\\\(u\\_1,u\\_2...u_n\\\\) where we can get the curnits/utilon conversion numbers from that point, convert everyone's utility to curnits, work out the CoCo value of the resulting game, convert back to utilons, and end up a*t the exact same point we started at?*\"\n\nBasically, a CoCo equilibrium would be a spot where, if the players squabbling over what direction to move on the Pareto frontier went \"let's introduce a virtual currency redeemable for tiny perturbations on the Pareto frontier\", and worked out the CoCo value for the game they're in (which is a very chaa solution when money is available), it'd return the answer \"stay where you currently are, it's a good spot, nobody needs to pay anyone else anything\". Which is a very fortunate result to get since this currency doesn't actually exist so nobody could pay each other anything anyways.\n\n**CoCo Equilibrium Questions**\n\nThere are several questions we could ask about CoCo equilibria.\n\n1: Is it scale-and-shift invariant, or does it depend on how everyone represents their utility functions?\n\n2: If we try using it on bargaining games in particular, will it reproduce any well-known bargaining notions?\n\n3: How do we generalize it to the n-person case? Is it straightforward, or are there hidden difficulties?\n\n4: If we really just found a universal notion of how to split gains in games, how come nobody else came up with it first?\n\n5: Do CoCo equilibria even exist, anyways? If so, are they unique?\n\nTime to reveal the answers, which won't be proved yet because they'll follow as a consequence of one big theorem later on.\n\n**Scale-Shift Invariance**\n\nFor the first question, yes, it is scale-and-shift invariant. It doesn't matter *how* you represent everyone's utility functions, you'll get the same answer. Intuitively, here's what happens. Let's say Alice multiplies all her utility numbers by 100. Now, at the point of interest, this means that we just went from 3 curnits/AliceUtilon to 3 curnits/100 AliceUtilons. And so, the coefficient we multiply Alice's utility function by, \\\\(a_1\\\\) (the curnits/AliceUtilons number), went from 3 to \\\\(\\\\frac{3}{100}\\\\), which will perfectly cancel out the fact that Alice multiplied all her numbers by 100. So, the CoCo value (as denominated in curnits) doesn't change one bit. Then we divide by \\\\(a_1\\\\) to convert back to Alice's utility, which means that we multiply by \\\\(\\\\frac{100}{3}\\\\), and get a big number for AliceUtilons, as expected (since Alice multiplied all her numbers by 100)\n\nAs for shifting, if Alice adds 10 utility to all her numbers, it doesn't alter the coefficient \\\\(a_1\\\\) (3 curnits/AliceUtilon), so all of Alice's utility payoffs as denominated in curnits, are 10 higher than usual. But, CoCo value is shift-invariant. If Alice gets a guaranteed 10 extra curnits no matter what she does, her CoCo value will be 10 curnits higher than it'd be usually, and Bob's won't change at all. And so, when we divide by \\\\(a_1\\\\) to convert back to Alice's utility, we get an extra 10 utility, as expected (since Alice added 10 to all her numbers)\n\nOk, we've got scale-and-shift invariance, which is a super-important property to have for something to maybe be \"chaa\" (a mathematically distinguished point in a negotiation game against a foe where you both have an interest in preventing destructive conflicts, that's neutral enough that aliens would probably come up with it).\n\n**Bargaining Games as Special Case**\n\nIf we apply CoCo equilibrium concepts to bargaining games in particular (player 1 proposes an option or plays \"reject\", player 2 can accept or reject, anyone rejecting means that both sides get their disagreement payoffs), what do we get?\n\nWell, though it won't be proved now (it'll be indirectly proved later on), CoCo equilibria in bargaining games will turn out to be equivalent to the Nash bargaining solution! The Nash solution can be derived as a special case of this generalization of the CoCo solution to when utility isn't transferrable!\n\n**N-Player Generalizations and Coalitions**\n\nFor generalizing to the n-person case, there's the obvious generalization where, given a Pareto frontier point, we can get the imaginary prices \\\\(a\\_1,a\\_2...a_n\\\\), and the CoCo value makes sense for n-player games. But this doesn't fully take coalitions into account. It's possible that a coalition could conspire amongst themselves to guarantee a good payout. And we could add extra conditions regarding coalitions, like that *within* a coalition, they use some sort of CoCo-like or Shapley-like split of resources.\n\nTo formalize the stronger version of that equilibrium, let \\\\(N\\\\) be the set of players, and \\\\(U_i\\\\) be the utility function of player \\\\(i\\\\), and \\\\(A_i\\\\) be player \\\\(i\\\\)'s set of actions.\n\n*Formal Definition:*  ***Coalition-Perfect CoCo Equilibrium***\n\n*A coalition-perfect CoCo equilibrium is a tuple *\\\\(\\\\{\\\\rho^S\\\\}_{S\\\\subseteq N}\\\\)* (the joint strategies that all possible coalitions would play if they were against the opposing coalition, *\\\\(\\\\rho^S\\\\in\\\\Delta\\\\prod_{i\\\\in S}A_i\\\\)*), s.t, defining *\\\\(u^S\\_i := U\\_i(\\\\rho^S,\\\\rho^{N/S})\\\\)\n\n*1: *\\\\(\\\\{u^N\\_i\\\\}\\_{i\\\\in N}\\\\)* is on the Pareto frontier.*\n\n*2: There is an *\\\\(\\\\{a\\_i\\\\}\\_{i\\\\in N}\\\\)* tuple (virtual prices) that makes both of the following conditions true.*\n\n*3: *\\\\(\\\\displaystyle{\\\\forall S\\\\subseteq N:\\\\rho^S\\\\in\\\\text{argmax}_{\\\\rho\\\\in\\\\Delta\\\\prod_{i\\\\in S}A\\_i}\\\\left(\\\\sum\\_{i\\\\in S} a\\_i U\\_i(\\\\rho,\\\\rho^{N/S})-\\\\sum_{j\\\\in N/S} a\\_j U\\_j(\\\\rho,\\\\rho^{N/S})\\\\right)}\\\\)  \n* (ie, all the joint strategies are trying to maximize the money earned if up against the opposing coalition in a zero-sum game, and as a special case, when S=N, it says that what the entire group actually ends up doing maximizes surplus value, which is another way of stating that the *\\\\(\\\\{a\\_i\\\\}\\_{i\\\\in N}\\\\)* are the appropriate virtual currencies to use at the *\\\\(\\\\{u^N\\_i\\\\}\\_{i\\\\in N}\\\\)* point)*\n\n*4: *\\\\(\\\\displaystyle{\\\\forall i\\\\in N, i\\\\in S\\\\subseteq N: a\\_i u^S\\_i = \\\\sum_{i\\\\in R\\\\subseteq S}\\\\frac{(|R|-1)!(|S|-|R|)!}{|S|!}\\\\left(\\\\sum_{j\\\\in R}a\\_j u^R\\_j-\\\\sum_{k\\\\in S/R}a\\_k u^{S/R}\\_k\\\\right)}\\\\)  \n*Or, rephrasing this in terms of the more standard framing of Shapley Value...*  \n\\\\(\\\\displaystyle{\\\\forall i\\\\in N, i\\\\in S\\\\subseteq N:a\\_i u^S\\_i=\\\\sum_{R\\\\subseteq S/\\\\{i\\\\}}\\\\frac{(|S|-|R|-1)!|R|!}{|S|!}\\\\left(\\\\sum_{j\\\\in R\\\\cup\\\\{i\\\\}}a\\_j u^{R\\\\cup\\\\{i\\\\}}\\_j-\\\\sum_{j\\\\in R}a\\_j u^R\\_j\\\\right)}\\\\)\n\nSo, that's a coalition-perfect CoCo equilibrium. You could interpret it as there being a \"virtual currency\" phrased in terms of how hard it is, relatively, to improve everyone's utility, and everyone getting their CoCo value, and even in the event of zero-sum conflicts between teams, everyone on a team will get their CoCo value. Or you could interpret it as everyone getting their Shapley payoffs, where the analogue of the marginal gain from a particular player is \"their value when they join team \\\\(S\\\\), plus the improvements in everyone else's value from them no longer opposing team \\\\(S\\\\)\". Or you could interpret the game, and all the zero-sum subgames, as just maximizing a weighted sum of utilities (like Harsanyi's utilitarianism theorem), and the cool part is the \"weights\" for how important everyone is will be the same for the full game as well as all the zero-sum subgames.\n\n**Harsanyi Equilibria and Generalizing the Nash Bargaining Solution**\n\nIf this exists and it's nice, why has nobody found it before?\n\nActually, someone did find this before! That's a major occupational hazard of finding math that aliens would independently reinvent: there's a high chance that someone beat you to the punch. Specifically, John Harsanyi, back in 1963, found this first. He wound up with this same exact solution, though it's quite nontrivial to show the equivalence between our equilibrium notions.\n\nCoCo equilibria were motivated via the nice properties of the CoCo value and generalizing it to the non-transferrable utility case, which turned out to be secretly equivalent to generalizing Shapley values. Harsanyi, as detailed in his lovely paper [\"A Simplified Bargaining Model for the n-Person Cooperative Game\"](https://link.springer.com/chapter/10.1007/978-94-017-2527-9_3) (which I very highly recommend you read via your favorite paper-reading website!), found it from trying to generalize the Nash Bargaining Solution to games without well-defined disagreement points.\n\nHarsanyi's basic insight was that, in a general two-player game, if it's known in advance that the Nash bargaining solution will be used, and the players are picking their \"disagreement strategies\" (what they'll fall back on if they can't cooperate on some joint distribution over actions) then Alice would try to pick a \"disagreement strategy\" that makes it so that, no matter what Bob's disagreement strategy is, the Nash bargaining solution would favor Alice as much as possible, and Bob is in a similar position. So, the two players will end up in a game where they're trying to disagree in a way that'll rig the Nash bargaining solution in their favor. I'm not sure whether or not this is zero-sum, but it is true that if one player wins, the other must lose, so it's zero-sum *enough* that there's a unique pair of disagreement utilities that you get from maximin strategies, that are mutually optimal against each other, and then you can just use the Nash bargaining solution from there.\n\nIn particular, if you're trying to make a threat that's suitable for rigging a bargaining game in your favor, what you need are not threats that hurt both of you equally, or threats that are worse for you than for the foe, or threats that the foe could possibly defuse. What you need is something that matters far more to the foe than you, which the foe can't evade by any action they can take. Or, rephrasing in terms of the CoCo value, to successfully rig the Nash bargaining solution in your favor, you'll need a good move in the zero-sum competition game of the cooperation/competition decomposition.\n\nGeneralized to the n-player case, between any two coalitions, they'll be doing the same sort of squabbling over what counts as the disagreement point, everyone within the coalition will agree on what disagreement point to go for in event of conflicts and within any coalition, they'll also be splitting things according to the Nash bargaining bolution as well. I don't fully understand the reasoning behind how that informal sort of description cashes out in the math (in particular, I still don't understand why the disagreement points are defined as they are in the paper), but I'll attempt a summary of Harsanyi's paper anyways. You're *highly* encouraged to read the paper yourself; it's got lots of goodies in it that I don't mention.\n\nHarsanyi starts off by assuming that every coalition can guarantee some marginal gain to everyone making it up, which is denoted by \\\\(w^S_i\\\\), the marginal utility payoff to player \\\\(i\\\\) received from the coalition \\\\(S\\\\).\n\nFurther, the payoff that a player gets in the event of a zero-sum conflict between \\\\(S\\\\) and everyone else should just be the sum of the marginal payoffs from all the subsets of \\\\(S\\\\) that \\\\(i\\\\) is in, ie. \\\\(u^S\\_i=\\\\sum\\_{i\\\\in R\\\\subseteq S}w^R_i\\\\) (where \\\\(u^S_i\\\\) is as previously defined, the utility that i gets in the event of a zero-sum conflict between \\\\(S\\\\) and everyone else). The argument for this is that if the sum of marginal payoffs was more than \\\\(u^S_i\\\\) (the payoff that i gets in the event of a zero-sum conflict), the coalitions collectively would be promising more utility to player i than can actually be guaranteed, and they're making unfulfillable promises. But player i should really be picking up all the utility promised to it from all the coalitions, and not leaving excess on the table, and so we get equality.\n\nAs it turns out, if that equation holds, then you can work out how all the \\\\(w^S_i\\\\) must be defined: it must hold that \\\\(w^S\\_i=\\\\sum\\_{i\\\\in R\\\\subseteq S}(-1)^{|S|-|R|}u^R_i\\\\). This is around where I have problems. I just can't quite manage to get myself to see how this quantity is the \"slice of marginal utility that coalition \\\\(S\\\\) promises to player i\", so let me know in the comments if anyone manages to pull it off.\n\nThen, we go \"ah, if \\\\(w^S_i\\\\) is the marginal gain from being part of coalition \\\\(S\\\\)\" (which, again, I can't quite see), then \\\\(w^S\\_i=u^S\\_i-t^S_i\\\\), where \\\\(u^S_i\\\\) is the payoff to i from playing its part in S's minimax strategy, and \\\\(t^S_i\\\\) is i's threat utility/disagreement point utility from being in coalition S. And so, the disagreement point utility of player i within coalition S must be \\\\(t^S\\_i=\\\\sum\\_{i\\\\in R\\\\subset S}(-1)^{|S|-|R|+1}u^R_i\\\\).  \nAgain, I can't see *at all* how this is true from the equation alone, but other people might be able to understand it. I can see how, in the special case of the coalition \"Alice and Bob\", it reproduces the intuitive result of Alice's disagreement-point-utility being \"screw you Bob, I'll go off on my own and fight the rest of the world\" (ie, \\\\(t^{\\\\{1,2\\\\}}\\_1=u^{\\\\{1\\\\}}\\_1\\\\)), but I can't see how it extends to larger coalitions than that.\n\nAnyways, a few interesting results are derived and discussed from there. One is that if two players i and j (Ione and Jake) are arguing over their split of the gains in every coalition S that contains both of them, there's a well-defined fallback point for Ione which is \\\\(\\\\sum_{S\\\\subseteq N:i\\\\in S,j\\\\not\\\\in S}w^S_i\\\\) (the sum of payoffs from every coalition that contains Ione but lacks Jake), and symmetrically for Jake, and if they do Nash bargaining from there, then it's possible to apply a result on Nash bargaining in subgames (intuitively, both players want to pick a point that doesn't worsen their bargaining position for the full game) to derive that Ione and Jake will agree to the same ratio for how to split coalition gains between them, in every coalition game. So, if Ione and Jake are splitting value 60/40 in one coalition, they're doing that same split in every coalition. This can be used to derive the general result that, in every coalition containing two or more players, they'll all play the Nash bargaining solution against each other.\n\nAnd there's another interesting and highly nontrivial result from Harsanyi's paper, which effectively says that if the two coalitions of S and N/S (everyone who isn't in S) appoint an individual member to decide on the threat strategy that their coalition will follow, and the appointed representatives Ione and Jake only care about maximizing their own payoff in the overall game (ie, maximizing \\\\(u^N_i\\\\) and \\\\(u^N_j\\\\)), (ie. they know that the zero-sum fight between S and N/S probably isn't happening, it's just a bargaining chip to get good overall payoffs, and they just care about their own overall payoff, not the interests of the rest of their coalition), then the threat strategies they'll pick will be minimax threat strategies for the S vs N/S zero-sum game. Correspondingly, it doesn't matter *what* players the coalitions S and N/S appoint as representatives, they'll end up picking a minimax threat strategy to maximize their payoff in the overall game.\n\nWhat Harsanyi eventually ended up deciding on as the equilibrium conditions were as follows (slightly re-expressed for notational compliance). Let \\\\(U_i\\\\) be the utility function of player i, and \\\\(A_i\\\\) be their space of actions.\n\n*Formal Definition:* **Coalition-Perfect Harsanyi Equilibrium**  \n*A coalition-perfect Harsanyi equilibrium is a tuple *\\\\(\\\\{\\\\rho^S\\\\}_{S\\\\subseteq N}\\\\)* (the joint strategies that each coalition would play if they were against the opposing coalition, *\\\\(\\\\rho^S\\\\in\\\\Delta\\\\prod_{i\\\\in S}A_i\\\\)*),s.t, defining *\\\\(u^S\\_i := U\\_i(\\\\rho^S,\\\\rho^{N/S})\\\\)* (payoff to player *\\\\(i\\\\)* if coalitions *\\\\(S\\\\)* and *\\\\(N/S\\\\)* fight), and *\\\\(t^S\\_i :=\\\\sum\\_{i\\\\in R\\\\subset S}(-1)^{|S|-|R|+1}u^R_i\\\\)* (the fallback utility value for player *\\\\(i\\\\)* in coalition *\\\\(S\\\\)*).*\n\n*1: *\\\\(\\\\{u^N\\_i\\\\}\\_{i\\\\in N}\\\\)* is on the Pareto frontier.*\n\n*2: There is an *\\\\(\\\\{a\\_i\\\\}\\_{i\\\\in N}\\\\)* tuple (virtual prices, though Harsanyi didn't use the term) that makes both of the following conditions true.*\n\n*3: *\\\\(\\\\displaystyle{\\\\forall S\\\\subseteq N:\\\\rho^S\\\\in\\\\text{argmax}_{\\\\rho\\\\in\\\\Delta\\\\prod_{i\\\\in S}A\\_i}\\\\left(\\\\sum\\_{i\\\\in S} a\\_i U\\_i(\\\\rho,\\\\rho^{N/S})-\\\\sum_{j\\\\in N/S} a\\_j U\\_j(\\\\rho,\\\\rho^{N/S})\\\\right)}\\\\)  \n*(ie. all the joint strategies are trying to maximize the money earned if up against the opposing coalition in a zero-sum game and as a special case, when S=N, it says that what the entire group actually ends up doing maximizes surplus value, which is another way of stating that the *\\\\(\\\\{a\\_i\\\\}\\_{i\\\\in N}\\\\)* are the appropriate virtual currencies to use at the *\\\\(\\\\{u^N\\_i\\\\}\\_{i\\\\in N}\\\\)* point)*\n\n*4: *\\\\(\\\\forall i,j\\\\in S\\\\subseteq N: a\\_i(u^S\\_i-t^S\\_i)=a\\_j(u^S\\_j-t^S\\_j)\\\\)  \n* (it's very nonobvious, but if you use Lagrange multipliers on the Nash bargaining problem, this is effectively saying that for all coalitions *\\\\(S\\\\)*, the division of resources within *\\\\(S\\\\)*, using the various *\\\\(t^S_i\\\\)* as the disagreement payoffs, follows the Nash bargaining solution)*\n\nAnd now we get to the centerpiece theorem, that coalition-perfect CoCo equilibria are the same as coalition-perfect Harsanyi equilibria. Since Harsanyi already showed things like scale-and-shift invariance, and that these equilibria exist, and lots of other results about them, we just need to prove equivalence and then we can lift all of Harsanyi's work - no point in rederiving everything on our own. Since three of the four conditions for the equilibria are obviously identical, the whole proof focuses on showing that the \"Every coalition uses the Nash bargaining solution internally to divide gains, with suitably defined threat points\" condition of Harsanyi equilibria is equivalent to the \"Every coalition uses the CoCo payoffs/modified Shapley payoffs internally to divide gains\" condition of CoCo equilibria.\n\n**Theorem 1:** *A tuple *\\\\(\\\\{\\\\rho^S\\\\}_{S\\\\subseteq N}\\\\)* of strategies for every coalition is a coalition-perfect Harsanyi equilibrium iff it's a coalition-perfect CoCo equilibrium.*  \n  \n**Equilibria Existence**\n\nAnd now for the final question. Do these sorts of equilibria even exist at all? That's an awful lot of conditions to fulfill at once, you've got one equilibria condition for each coalition, and there's a whole lot of coalitions.\n\nWell, fortunately, Harsanyi proved that these sorts of equilibria exist in his paper! So we can just copy off his work. Well, technically, he did it under some moderately restrictive assumptions which don't look essential, and can probably be removed, though it'll be annoying to do so. Pretty much, his proof works by setting up a game which is related to the bargaining game, and any Nash equilibrium of the auxiliary game can be converted into a coalition-perfect Harsanyi equilibrium.\n\nThe assumptions Harsanyi made were, in particular, that the space of all possible utility values in \\\\(\\\\mathbb{R}^n\\\\) were compact (ie, nobody can get unbounded positive utility or unbounded negative utility, and this assumption is violated in full transferrable utility games), and its affine hull was of dimension \\\\(n\\\\), and that the Pareto frontier had no vertices in the sense that, for all points on it, there's a unique tangent hyperplane that touches that point (ie, you can *uniquely* read off the weights from *all* Pareto frontier points). Think of a sphere in 3d space: every point on the sphere surface has a unique tangent plane. But for a cube in 3d space, the edges or vertices of the cube can have multiple distinct tangent planes which touch the cube at that point.\n\nWith those assumptions, yes, there is a coalition-perfect Harsanyi equilibrium, as proven in his paper.\n\nHarsanyi made the remark that if the Pareto frontier has vertices, it's possible to write any such game as a limit of games that don't have vertices (like, imagine a cube but all the corners and edges have been sanded down a bit, and take the limit of doing less and less sanding), in order to extend the results to games with vertices on their Pareto frontier.\n\nThough he didn't comment on it, it seems like it's possible to also deal with the affine hull dimension issue in this way, in the sense that for any set of possible utility values whose affine hull is of dimension \\\\(<n\\\\), it's possible to write it as a limit of games whose set of utility values has an affine hull of dimension \\\\(n\\\\) (the analogue is that any 2-dimensional shape can be thought of as a limit of 3-dimensional shapes that keep getting thinner), and presumably extend his existence result to cases like that.\n\nHe didn't *actually* do these limiting-case proofs at any point, they just seem like the sort of argument that'd need to be done to generalize his proof.\n\nThere's another question which is, \"are Harsanyi/CoCo equilibria unique\"?\n\nHarsanyi made the remark that they were unique for bargaining games (where they'd give the Nash bargaining solution), games with transferrable utility (where they'd give the CoCo value), and 2-player games, but weren't necessarily unique for  general n-player games, and then completely refused to elaborate on this.\n\nThe problem is, although Harsanyi said there were counterexamples to uniqueness (he meant a counterexample in the stronger sense of \"there's more than one tuple of utility values that's an equilibrium\", not the obvious weak sense of \"maybe there's different strategies for everyone that gives the same payoff\"), at *no point* did he every actually *give* such a counterexample, even in the paper he cited to that effect. This is somewhat infuriating, and I fear that the non-uniqueness of these equilibria is one of those apocryphal results that nobody ever actually got around to double-checking at any point. I'd be extremely pleased if anyone could find a paper with an actual example of such.\n\nSo, yeah, that's about it. There's one good notion of equilibria, that gives Shapley values, CoCo values, and the Nash bargaining solution as special cases, which can variously be thought of as:\n\n1: Maximizing a suitable weighted sum of everyone's utilities, where all the various coalitions agree on the weights of everyone's utilities (so if Alice is twice as important as Bob, then Alice will be twice as important as Bob in all coalitions containing the two of them).\n\n2: Gives everyone their modified Shapley payoffs, and all the coalitions split their gains in a Shapley way.\n\n3: Inventing a virtual currency reflecting how hard it is to improve the utilities of everyone relative to each other and splitting the game into a bunch of coalition vs coalition fights with perfect cooperation and competition, and paying everyone accordingly.\n\n4: Every coalition jostles for a threat strategy that gives them the most payoff from the Nash bargaining solution, and then every coalition does Nash bargaining within itself to split gains.\n\n**But What if it Sucks, Tho (it Does)**\n\nSo, there's one super-important aspect of this that makes it dramatically less appealing that I haven't seen anyone point out. The payoffs for everyone are determined by games of the form \"coalition \\\\(S\\\\) fights coalition not-\\\\(S\\\\), coalition \\\\(S\\\\) is maximizing the quantity \"utility of coalition \\\\(S\\\\) \\- utility of opposite coalition\", and vice-versa for the opposite coalition\".\n\nIf you depart from nice comfy visualizations of games involving hot-dog selling, and ponder what that'd mean for humanity, you'll probably realize how *exceptionally ugly* those imaginary games would get.\n\nActually take one minute, by the clock, to think about what it means that the following equation determine people's payoffs:\n\n\\\\(\\\\displaystyle{\\\\max_{\\\\rho^{S}\\\\in\\\\Delta\\\\prod_{i\\\\in S}A\\_i}\\\\min\\_{\\\\rho^{N/S}\\\\in\\\\Delta\\\\prod_{j\\\\not\\\\in S}A\\_j}\\\\left(\\\\sum\\_{i\\\\in S}a\\_i U\\_i(\\\\rho^S,\\\\rho^{N/S})-\\\\sum_{j\\\\not\\\\in S}a\\_j U\\_j(\\\\rho^S,\\\\rho^{N/S})\\\\right)}\\\\)\n\nThis is why I was stressing that \"chaa\" and \"fair\" are very different concepts, and that this equilibrium notion is very much based on threats. They just need to be asymmetric threats that the opponent can't defuse in order to work (or ways of asymmetrically benefiting yourself that your opponent can't ruin, that'll work just as well).\n\nI think it's a terrible idea to automatically adopt an equilibrium notion which incentivises the players to come up with increasingly nasty threats as fallback if they don't get their way. And so there seems to be a good chunk of remaining work to be done, involving poking more carefully at the CoCo value and seeing which assumptions going into it can be broken.\n\nAlso, next Thursday (June 28) at noon Pacific time is the Schelling time to meet in the Walled Garden and discuss the practical applications of this. Come one, come all, and bring your insights!\n\n**Appendix: Proof of Theorem 1 (you can skip this one)**\n\nSince conditions 1, 2, and 3 are all obviously equivalent to each other, that just leaves that showing that the condition 4's of both types of equilibria imply each other. First, we'll show a lemma.\n\nLemma 1: \\\\(u^S\\_i=\\\\sum\\_{i\\\\in R\\\\subseteq S}w^R_i\\\\)\n\nStart off with\n\n\\\\(\\\\sum_{i\\\\in R\\\\subseteq S}w^R_i\\\\)\n\nUnpack the definition of \\\\(w^R_i\\\\).\n\n\\\\(=\\\\sum_{i\\\\in R\\\\subseteq S}\\\\sum_{i\\\\in T\\\\subseteq R}(-1)^{|R|-|T|}u^T_i\\\\)\n\nWe can interchange this sum, and view it as picking the set T first, and the set R second.\n\n\\\\(=\\\\sum_{i\\\\in T\\\\subseteq S}\\\\sum_{T\\\\subseteq R\\\\subseteq S}(-1)^{|R|-|T|}u^T_i\\\\)\n\nAnd group\n\n\\\\(=\\\\sum_{i\\\\in T\\\\subseteq S}\\\\left(\\\\sum_{T\\\\subseteq R\\\\subseteq S}(-1)^{|R|-|T|}\\\\right)u^T_i\\\\)\n\nAnd we can ask what coefficient is paired with the various \\\\(u^T_i\\\\). Really, there's two possibilities. One possibility is that \\\\(T=S\\\\), the other is that \\\\(T\\\\neq S\\\\), so let's split this up.\n\n\\\\(=\\\\left(\\\\sum_{S\\\\subseteq R\\\\subseteq S}(-1)^{|R|-|S|}\\\\right)u^S\\_i+\\\\sum\\_{i\\\\in T\\\\subset S}\\\\left(\\\\sum_{T\\\\subseteq R\\\\subseteq S}(-1)^{|R|-|T|}\\\\right)u^T_i\\\\)\n\nClearly, for the former term, \\\\(R=S\\\\) is the only possibility, and \\\\((-1)^{|S|-|S|}=(-1)^0=1\\\\), so we get\n\n\\\\(=u^S\\_i+\\\\sum\\_{i\\\\in T\\\\subset S}\\\\left(\\\\sum_{T\\\\subseteq R\\\\subseteq S}(-1)^{|R|-|T|}\\\\right)u^T_i\\\\)\n\nWe can reexpress picking a \\\\(R\\\\supseteq T\\\\) as picking an \\\\(R'\\\\subseteq S/T\\\\) (the fragments not in T) and unioning it with T.\n\n\\\\(=u^S\\_i+\\\\sum\\_{i\\\\in T\\\\subset S}\\\\left(\\\\sum_{R'\\\\subseteq S/T}(-1)^{|R'\\\\cup T|-|T|}\\\\right)u^T_i\\\\)\n\n\\\\(=u^S\\_i+\\\\sum\\_{i\\\\in T\\\\subset S}\\\\left(\\\\sum_{R'\\\\subseteq S/T}(-1)^{|R'|+|T|-|T|}\\\\right)u^T_i\\\\)\n\n\\\\(=u^S\\_i+\\\\sum\\_{i\\\\in T\\\\subset S}\\\\left(\\\\sum_{R'\\\\subseteq S/T}(-1)^{|R'|}\\\\right)u^T_i\\\\)\n\nTry writing this as a sum over subset sizes, and you'll get a factorial term showing up from the many possible subsets of a given size.\n\n\\\\(=u^S\\_i+\\\\sum\\_{i\\\\in T\\\\subset S}\\\\left(\\\\sum_{b=0}^{b=|S/T|}\\\\frac{|S/T|!}{(|S/T|-b)!b!}(-1)^{b}\\\\right)u^T_i\\\\)\n\n\\\\(=u^S\\_i+\\\\sum\\_{i\\\\in T\\\\subset S}\\\\left(\\\\sum_{b=0}^{b=|S|-|T|}\\\\frac{(|S|-|T|)!}{(|S|-|T|-b)!b!}(-1)^{b}\\\\right)u^T_i\\\\)\n\nAnd then, by plugging this into Wolfram Alpha, we get 0 and all that stuff cancels.  \n\\\\(=u^S_i\\\\), and so the lemma has been proved.\n\nOnto the full proof, starting off by assuming condition 4 of a coalition-perfect Harsanyi equilibria, and deriving condition 4 of a coalition-perfect CoCo equilibria. Start off with \\\\(a\\_i u^S\\_i\\\\). Then, we use our lemma that \\\\(u^S\\_i=\\\\sum\\_{i\\\\in R\\\\subseteq S}w^R_i\\\\).\n\n\\\\(=a\\_i\\\\sum\\_{i\\\\in R\\\\subseteq S}w^R_i\\\\)\n\nDistribute the constant in\n\n\\\\(=\\\\sum_{i\\\\in R\\\\subseteq S}a\\_i w^R\\_i\\\\)\n\nUse that \\\\(w^R\\_i=u^R\\_i-t^R_i\\\\), by definition of \\\\(t^R_i\\\\).\n\n\\\\(=\\\\sum_{i\\\\in R\\\\subseteq S}a\\_i(u^R\\_i-t^R_i)\\\\)\n\nNow, use that, by condition 4 of coalition-perfect Harsanyi equilibria, every player \\\\(j\\\\in R\\\\) has \\\\(a\\_j(u^R\\_j-t^R\\_j)=a\\_i(u^R\\_i-t^R\\_i)\\\\), so we can rewrite this as\n\n\\\\(=\\\\sum_{i\\\\in R\\\\subseteq S}\\\\frac{1}{|R|}\\\\sum_{j\\\\in R}a\\_j(u^R\\_j-t^R_j)\\\\)\n\nUnpack what \\\\(t^R_j\\\\) is\n\n\\\\(=\\\\sum_{i\\\\in R\\\\subseteq S}\\\\frac{1}{|R|}\\\\sum_{j\\\\in R}a\\_j\\\\left(u^R\\_j-\\\\sum_{j\\\\in T\\\\subset R}(-1)^{|R|-|T|+1}u^T_j\\\\right)\\\\)\n\nDistribute the negative in\n\n\\\\(=\\\\sum_{i\\\\in R\\\\subseteq S}\\\\frac{1}{|R|}\\\\sum_{j\\\\in R}a\\_j\\\\left(u^R\\_j+\\\\sum_{j\\\\in T\\\\subset R}(-1)^{|R|-|T|}u^T_j\\\\right)\\\\)\n\nMerge it into one big sum\n\n\\\\(=\\\\sum_{i\\\\in R\\\\subseteq S}\\\\frac{1}{|R|}\\\\sum_{j\\\\in R}a\\_j\\\\sum\\_{j\\\\in T\\\\subseteq R}(-1)^{|R|-|T|}u^T_j\\\\)\n\nReshuffle the sum so we're summing over subsets first, and elements of that subset later. The available subsets T are all the subsets of R, and they can only be included in the sum for the j that lie in T.\n\n\\\\(=\\\\sum_{i\\\\in R\\\\subseteq S}\\\\frac{1}{|R|}\\\\sum_{T\\\\subseteq R}\\\\sum_{j\\\\in T}a\\_j(-1)^{|R|-|T|}u^T\\_j\\\\)\n\nWe can reshuffle the negative 1 part outside of the innermost sum.\n\n\\\\(=\\\\sum_{i\\\\in R\\\\subseteq S}\\\\frac{1}{|R|}\\\\sum_{T\\\\subseteq R}(-1)^{|R|-|T|}\\\\sum_{j\\\\in T}a\\_j u^T\\_j\\\\)\n\nAbbreviate \\\\(\\\\sum_{j\\\\in T}a\\_j u^T\\_j\\\\) as \\\\(Z^T\\\\), and reshuffle the sums a little bit.\n\n\\\\(=\\\\sum_{i\\\\in R\\\\subseteq S}\\\\sum_{T\\\\subseteq R}\\\\frac{1}{|R|}(-1)^{|R|-|T|} Z^T\\\\)\n\nNow, for a given T, we'll work out the coefficient in front of \\\\(Z^T\\\\) for the entire sum. The first possibility is that \\\\(i\\\\in T\\\\). Then the possible R that contribute to the coefficient of \\\\(Z^T\\\\) in the entire sum are exactly the \\\\(R\\\\supseteq T\\\\). The second possibility is that \\\\(i\\\\not\\\\in T\\\\), so then the possible R that contribute to the coefficient of \\\\(Z^T\\\\) in the entire sum are exactly the \\\\(R\\\\supseteq T\\\\cup\\\\{i\\\\}\\\\). So, breaking things up that way, we get\n\n\\\\(=\\\\left(\\\\sum_{i\\\\in T\\\\subseteq S}\\\\sum_{T\\\\subseteq R\\\\subseteq S}\\\\frac{1}{|R|}(-1)^{|R|-|T|}Z^T\\\\right)+\\\\left(\\\\sum_{i\\\\not\\\\in T\\\\subseteq S}\\\\sum_{T\\\\cup\\\\{i\\\\}\\\\subseteq R\\\\subseteq S}\\\\frac{1}{|R|}(-1)^{|R|-|T|}Z^T\\\\right)\\\\)\n\nAnd we can rephrase supersets of T as subsets of \\\\(S/T\\\\), unioned with T. And rephrase supersets of T that contain i as subsets of \\\\(S/(T\\\\cup\\\\{i\\\\})\\\\), unioned with \\\\(T\\\\cup\\\\{i\\\\}\\\\).\n\n\\\\(=\\\\left(\\\\sum_{i\\\\in T\\\\subseteq S}\\\\sum_{R'\\\\subseteq S/T}\\\\frac{1}{|R'\\\\cup T|}(-1)^{|R'\\\\cup T|-|T|}Z^T\\\\right)\\\\)  \n\\\\(+\\\\left(\\\\sum_{i\\\\not\\\\in T\\\\subseteq S}\\\\sum_{R'\\\\subseteq S/(T\\\\cup\\\\{i\\\\})}\\\\frac{1}{|R'\\\\cup T\\\\cup\\\\{i\\\\}|}(-1)^{|R'\\\\cup T\\\\cup\\\\{i\\\\}|-|T|}Z^T\\\\right)\\\\)\n\nReexpress\n\n\\\\(=\\\\left(\\\\sum_{i\\\\in T\\\\subseteq S}\\\\sum_{R'\\\\subseteq S/T}\\\\frac{1}{|R'|+|T|}(-1)^{|R'|+|T|-|T|}Z^T\\\\right)\\\\)  \n\\\\(+\\\\left(\\\\sum_{i\\\\not\\\\in T\\\\subseteq S}\\\\sum_{R'\\\\subseteq S/(T\\\\cup\\\\{i\\\\})}\\\\frac{1}{|R'|+|T|+1}(-1)^{|R'|+|T|+1-|T|}Z^T\\\\right)\\\\)\n\nCancel\n\n\\\\(=\\\\left(\\\\sum_{i\\\\in T\\\\subseteq S}\\\\sum_{R'\\\\subseteq S/T}\\\\frac{1}{|R'|+|T|}(-1)^{|R'|}Z^T\\\\right)\\\\)  \n\\\\(+\\\\left(\\\\sum_{i\\\\not\\\\in T\\\\subseteq S}\\\\sum_{R'\\\\subseteq S/(T\\\\cup\\\\{i\\\\})}\\\\frac{1}{|R'|+|T|+1}(-1)^{|R'|+1}Z^T\\\\right)\\\\)\n\nSplit into a sum over the various sizes of what \\\\(R'\\\\) could possibly be\n\n\\\\(=\\\\left(\\\\sum_{i\\\\in T\\\\subseteq S}\\\\sum_{b=0}^{b=|S/T|}\\\\frac{|S/T|!}{(|S/T|-b)!b!}\\\\frac{1}{b+|T|}(-1)^{b}Z^T\\\\right)\\\\)  \n\\\\(+\\\\left(\\\\sum_{i\\\\not\\\\in T\\\\subseteq S}\\\\sum_{b=0}^{b=|S/(T\\\\cup\\\\{i\\\\})|}\\\\frac{|S/(T\\\\cup\\\\{i\\\\})|!}{(|S/(T\\\\cup\\\\{i\\\\})|-b)!b!}\\\\frac{1}{b+|T|+1}(-1)^{b+1}Z^T\\\\right)\\\\)\n\nReexpress\n\n\\\\(=\\\\left(\\\\sum_{i\\\\in T\\\\subseteq S}\\\\sum_{b=0}^{b=|S|-|T|}\\\\frac{(|S|-|T|)!}{(|S|-|T|-b)!b!}\\\\frac{1}{b+|T|}(-1)^{b}Z^T\\\\right)\\\\)  \n\\\\(+\\\\left(\\\\sum_{i\\\\not\\\\in T\\\\subseteq S}\\\\sum_{b=0}^{b=|S|-|T|-1}\\\\frac{(|S|-|T|-1)!}{(|S|-|T|-1-b)!b!}\\\\frac{1}{b+|T|+1}(-1)^{b+1}Z^T\\\\right)\\\\)\n\nGroup into one big fraction.\n\n\\\\(=\\\\left(\\\\sum_{i\\\\in T\\\\subseteq S}\\\\left(\\\\sum_{b=0}^{b=|S|-|T|}\\\\frac{(|S|-|T|)!(-1)^{b}}{(|S|-|T|-b)!b!(b+|T|)}\\\\right)Z^T\\\\right)\\\\)  \n\\\\(+\\\\left(\\\\sum_{i\\\\not\\\\in T\\\\subseteq S}\\\\left(\\\\sum_{b=0}^{b=|S|-|T|-1}\\\\frac{(|S|-|T|-1)!(-1)^{b+1}}{(|S|-|T|-1-b)!b!(b+|T|+1)}\\\\right)Z^T\\\\right)\\\\)\n\nPlug it into Wolfram Alpha, and use how the gamma function is defined to get it back into factorial form.\n\n\\\\(=\\\\left(\\\\sum_{i\\\\in T\\\\subseteq S}\\\\frac{(|T|-1)!(|S|-|T|)!}{|S|!}Z^T\\\\right)+\\\\left(\\\\sum_{i\\\\not\\\\in T\\\\subseteq S}-\\\\frac{|T|!(|S|-|T|-1)!}{|S|!}Z^T\\\\right)\\\\)\n\nReindex T to R for notational compliance later on, and we can rewrite this as a single sum over all R that contain i, because they're all paired off with a unique complement that lacks i.\n\n\\\\(=\\\\sum_{i\\\\in R\\\\subseteq S}\\\\frac{(|R|-1)!(|S|-|R|)!}{|S|!}Z^R-\\\\frac{|S/R|!(|S|-|S/R|-1)!}{|S|!}Z^{S/R}\\\\)\n\nFigure out what the cardinalities of the various sets are\n\n\\\\(=\\\\sum_{i\\\\in R\\\\subseteq S}\\\\frac{(|R|-1)!(|S|-|R|)!}{|S|!}Z^R-\\\\frac{(|S|-|R|)!(|S|-(|S|-|R|)-1)!}{|S|!}Z^{S/R}\\\\)\n\nCancel out, realize that the fractions are the same, and get\n\n\\\\(=\\\\sum_{i\\\\in R\\\\subseteq S}\\\\frac{(|R|-1)!(|S|-|R|)!}{|S|!}(Z^R-Z^{S/R})\\\\)\n\nAnd unpacking how \\\\(Z^R\\\\) was defined, we get, as intended, that this entire chain of equalities has proven\n\n\\\\(a\\_i u^S\\_i=\\\\sum_{i\\\\in R\\\\subseteq S}\\\\frac{(|R|-1)!(|S|-|R|)!}{|S|!}\\\\left(\\\\sum_{j\\\\in R}a\\_j u^R\\_j-\\\\sum_{k\\\\not\\\\in R}a\\_k u^{S/R}\\_k\\\\right)\\\\)\n\nThe exact condition 4 for a coalition-perfect CoCo equilibria, proving that all coalition-perfect Harsanyi equilibria are coalition-perfect CoCo equilibria.\n\nNow it's time for the reverse derivation, showing that all coalition-perfect CoCo equilibria are coalition-perfect Harsanyi equilibria. The goal is to show that for all \\\\(S\\\\subseteq N\\\\), and \\\\(i,j\\\\in S\\\\), that \\\\(a\\_i(u^S\\_i-t^S\\_i)=a\\_j(u^S\\_j-t^S\\_j)\\\\)  \nSo, let's start out with\n\n\\\\(a\\_i(u^S\\_i-t^S_i)\\\\)\n\nSubstitute in what \\\\(t^S_i\\\\) is\n\n\\\\(=a\\_i\\\\left(u^S\\_i-\\\\sum_{i\\\\in R\\\\subset S}(-1)^{|S|-|R|+1}u^R_i\\\\right)\\\\)\n\nCancel out the negatives\n\n\\\\(=a\\_i\\\\left(u^S\\_i+\\\\sum_{i\\\\in R\\\\subset S}(-1)^{|S|-|R|}u^R_i\\\\right)\\\\)\n\nFold it into one big sum\n\n\\\\(=a\\_i\\\\left(\\\\sum\\_{i\\\\in R\\\\subseteq S}(-1)^{|S|-|R|}u^R_i\\\\right)\\\\)\n\nMultiply the \\\\(a_i\\\\) in\n\n\\\\(=\\\\sum_{i\\\\in R\\\\subseteq S}(-1)^{|S|-|R|}a\\_i u^R\\_i\\\\)\n\nNow, we use condition 4 of a coalition-perfect CoCo equilibrium.\n\n\\\\(=\\\\sum_{i\\\\in R\\\\subseteq S}(-1)^{|S|-|R|}\\\\sum_{i\\\\in T\\\\subseteq R}\\\\frac{(|R|-|T|)!(|T|-1)!}{|R|!}\\\\left(\\\\sum_{j\\\\in T}a\\_j u^T\\_j-\\\\sum_{k\\\\in R/T}a\\_k u^T\\_k\\\\right)\\\\)\n\nAbbreviate the sum \\\\(\\\\sum_{j\\\\in T}a\\_j u^T\\_j\\\\) as \\\\(Z^T\\\\), to get\n\n\\\\(=\\\\sum_{i\\\\in R\\\\subseteq S}(-1)^{|S|-|R|}\\\\sum_{i\\\\in T\\\\subseteq R}\\\\frac{(|R|-|T|)!(|T|-1)!}{|R|!}(Z^T-Z^{R/T})\\\\)\n\nNow, we'll have to work out what the coefficent is for a given T, for the entire sum. Like, what number ends up being in front of \\\\(Z^T\\\\) when we sum everything up? There are two possibilities. The first possibility is that \\\\(i\\\\in T\\\\). Then the relevant R that we're summing over are the \\\\(R\\\\supseteq T\\\\). If \\\\(i\\\\not\\\\in T\\\\), then the relevant R that we're summing over are the \\\\(R\\\\supseteq T\\\\cup\\\\{i\\\\}\\\\), and we've got a negative 1 showing up from these Z terms being sutracted instead of added, which we can fold into the negative 1 power at the start. Using this grouping, we get\n\n\\\\(=\\\\left(\\\\sum_{i\\\\in T\\\\subseteq S}\\\\sum_{T\\\\subseteq R\\\\subseteq S}(-1)^{|S|-|R|}\\\\frac{(|R|-|T|)!(|T|-1)!}{|R|!}Z^T\\\\right)\\\\)  \n\\\\(+\\\\left(\\\\sum_{i\\\\not\\\\in T\\\\subseteq S}\\\\sum_{T\\\\cup\\\\{i\\\\}\\\\subseteq R\\\\subseteq S}(-1)^{|S|-|R|+1}\\\\frac{(|R|-|R/T|)!(|R/T|-1)!}{|R|!}Z^T\\\\right)\\\\)\n\nReexpress it slightly\n\n\\\\(=\\\\left(\\\\sum_{i\\\\in T\\\\subseteq S}\\\\sum_{T\\\\subseteq R\\\\subseteq S}(-1)^{|S|-|R|}\\\\frac{(|R|-|T|)!(|T|-1)!}{|R|!}Z^T\\\\right)\\\\)  \n\\\\(+\\\\left(\\\\sum_{i\\\\not\\\\in T\\\\subseteq S}\\\\sum_{T\\\\cup\\\\{i\\\\}\\\\subseteq R\\\\subseteq S}(-1)^{|S|-|R|+1}\\\\frac{(|R|-|R|+|T|)!(|R|-|T|-1)!}{|R|!}Z^T\\\\right)\\\\)\n\nAnd cancel\n\n\\\\(=\\\\left(\\\\sum_{i\\\\in T\\\\subseteq S}\\\\sum_{T\\\\subseteq R\\\\subseteq S}(-1)^{|S|-|R|}\\\\frac{(|R|-|T|)!(|T|-1)!}{|R|!}Z^T\\\\right)\\\\)  \n\\\\(+\\\\left(\\\\sum_{i\\\\not\\\\in T\\\\subseteq S}\\\\sum_{T\\\\cup\\\\{i\\\\}\\\\subseteq R\\\\subseteq S}(-1)^{|S|-|R|+1}\\\\frac{|T|!(|R|-|T|-1)!}{|R|!}Z^T\\\\right)\\\\)\n\nAnd we can reexpress this as picking a subset of \\\\(S/T\\\\), and unioning it with T to make R, or as picking a subset of \\\\(S/(T\\\\cup\\\\{i\\\\})\\\\) and unioning it with \\\\(T\\\\cup\\\\{i\\\\}\\\\) to make R.\n\n\\\\(=\\\\left(\\\\sum_{i\\\\in T\\\\subseteq S}\\\\sum_{R'\\\\subseteq S/T}(-1)^{|S|-|R'\\\\cup T|}\\\\frac{(|R'\\\\cup T|-|T|)!(|T|-1)!}{|R'\\\\cup T|!}Z^T\\\\right)\\\\)  \n\\\\(+\\\\left(\\\\sum_{i\\\\not\\\\in T\\\\subseteq S}\\\\sum_{R'\\\\subseteq S/(T\\\\cup\\\\{i\\\\})}(-1)^{|S|-|R'\\\\cup T\\\\cup\\\\{i\\\\}|+1}\\\\frac{|T|!(|R'\\\\cup T\\\\cup\\\\{i\\\\}|-|T|-1)!}{|R'\\\\cup T\\\\cup\\\\{i\\\\}|!}Z^T\\\\right)\\\\)\n\nReexpress\n\n\\\\(=\\\\left(\\\\sum_{i\\\\in T\\\\subseteq S}\\\\sum_{R'\\\\subseteq S/T}(-1)^{|S|-|R'|-|T|}\\\\frac{(|R'|+|T|-|T|)!(|T|-1)!}{(|R'|+|T|)!}Z^T\\\\right)\\\\)  \n\\\\(+\\\\left(\\\\sum_{i\\\\not\\\\in T\\\\subseteq S}\\\\sum_{R'\\\\subseteq S/(T\\\\cup\\\\{i\\\\})}(-1)^{|S|-|R'|-|T|-1+1}\\\\frac{|T|!(|R'|+|T|+1-|T|-1)!}{(|R'|+|T|+1)!}Z^T\\\\right)\\\\)\n\nCancel\n\n\\\\(=\\\\left(\\\\sum_{i\\\\in T\\\\subseteq S}\\\\sum_{R'\\\\subseteq S/T}(-1)^{|S|-|R'|-|T|}\\\\frac{|R'|!(|T|-1)!}{(|R'|+|T|)!}Z^T\\\\right)\\\\)  \n\\\\(+\\\\left(\\\\sum_{i\\\\not\\\\in T\\\\subseteq S}\\\\sum_{R'\\\\subseteq S/(T\\\\cup\\\\{i\\\\})}(-1)^{|S|-|R'|-|T|}\\\\frac{|T|!|R'|!}{(|R'|+|T|+1)!}Z^T\\\\right)\\\\)\n\nReexpress as summing up over all possible sizes for \\\\(R'\\\\), introducing a factorial term because of the many subsets of a given size.\n\n\\\\(=\\\\left(\\\\sum_{i\\\\in T\\\\subseteq S}\\\\sum_{b=0}^{b=|S/T|}\\\\frac{|S/T|!}{(|S/T|-b)!b!}(-1)^{|S|-b-|T|}\\\\frac{b!(|T|-1)!}{(b+|T|)!}Z^T\\\\right)\\\\)  \n\\\\(+\\\\left(\\\\sum_{i\\\\not\\\\in T\\\\subseteq S}\\\\sum_{b=0}^{b=|S/(T\\\\cup\\\\{i\\\\})|}\\\\frac{|S/(T\\\\cup\\\\{i\\\\})|!}{(|S/(T\\\\cup\\\\{i\\\\})|-b)!b!}(-1)^{|S|-b-|T|}\\\\frac{|T|!b!}{(b+|T|+1)!}Z^T\\\\right)\\\\)\n\nReexpress\n\n\\\\(=\\\\left(\\\\sum_{i\\\\in T\\\\subseteq S}\\\\sum_{b=0}^{b=|S|-|T|}\\\\frac{(|S|-|T|)!}{(|S|-|T|-b)!b!}(-1)^{|S|-b-|T|}\\\\frac{b!(|T|-1)!}{(b+|T|)!}Z^T\\\\right)\\\\)  \n\\\\(+\\\\left(\\\\sum_{i\\\\not\\\\in T\\\\subseteq S}\\\\sum_{b=0}^{b=|S|-|T|-1}\\\\frac{(|S|-|T|-1)!}{(|S|-|T|-1-b)!b!}(-1)^{|S|-b-|T|}\\\\frac{|T|!b!}{(b+|T|+1)!}Z^T\\\\right)\\\\)\n\nMerge into one big fraction and cancel\n\n\\\\(=\\\\left(\\\\sum_{i\\\\in T\\\\subseteq S}\\\\sum_{b=0}^{b=|S|-|T|}\\\\frac{(|S|-|T|)!(|T|-1)!(-1)^{|S|-|T|-b}}{(|S|-|T|-b)!(|T|+b)!}Z^T\\\\right)\\\\)  \n\\\\(+\\\\left(\\\\sum_{i\\\\not\\\\in T\\\\subseteq S}\\\\sum_{b=0}^{b=|S|-|T|-1}\\\\frac{(|S|-|T|-1)!|T|!(-1)^{|S|-|T|-b}}{(|S|-|T|-b-1)!(|T|+b+1)!}Z^T\\\\right)\\\\)\n\nAnd plug into Wolfram Alpha to get that both of these alternating sums over factorials are actually the same coefficient, so we can just write it as\n\n\\\\(=\\\\sum_{T\\\\subseteq S}\\\\frac{(-1)^{|S|-|T|}}{|S|}Z^T\\\\)\n\nSumming all this up, we've derived\n\n\\\\(a\\_i(u^S\\_i-t^S\\_i)=\\\\sum\\_{T\\\\subseteq S}\\\\frac{(-1)^{|S|-|T|}}{|S|}Z^T\\\\)\n\nAnd then we can do this whole line of reasoning again but swapping out i for j, and nothing at all changes, we still get the same quantity at the end, so we have\n\n\\\\(a\\_i(u^S\\_i-t^S\\_i)=a\\_j(u^S\\_j-t^S\\_j)\\\\)\n\nFor all \\\\(i,j\\\\in S\\\\subseteq N\\\\), the fourth condition for a coalition-perfect Harsanyi equilibrium, so all coalition-perfect CoCo equilibria are coalition-perfect Harsanyi equilibria.\n\nSince we've proved both directions, something is a coalition-perfect Harsanyi equilibria iff it's a coalition-perfect CoCo equilibria."
    },
    "voteCount": 25,
    "forceInclude": true
  },
  {
    "_id": "k8hvGAJWSKAeHwpnJ",
    "url": null,
    "title": "Why I'm Worried About AI",
    "slug": "why-i-m-worried-about-ai",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "AI Risk"
      },
      {
        "name": "AI"
      },
      {
        "name": "Inner Alignment"
      },
      {
        "name": "Deception"
      },
      {
        "name": "Threat Models"
      }
    ],
    "tableOfContents": {
      "sections": [
        {
          "title": "Preamble",
          "anchor": "Preamble",
          "level": 1
        },
        {
          "title": "How do we train neural networks?",
          "anchor": "How_do_we_train_neural_networks_",
          "level": 1
        },
        {
          "title": "Optimization",
          "anchor": "Optimization",
          "level": 1
        },
        {
          "title": "Optimized vs Optimizers",
          "anchor": "Optimized_vs_Optimizers",
          "level": 2
        },
        {
          "title": "What do we tell the optimizers to do?",
          "anchor": "What_do_we_tell_the_optimizers_to_do_",
          "level": 2
        },
        {
          "title": "How do we actually put the objective into the AI?",
          "anchor": "How_do_we_actually_put_the_objective_into_the_AI_",
          "level": 1
        },
        {
          "title": "Deception",
          "anchor": "Deception",
          "level": 1
        },
        {
          "title": "Recap",
          "anchor": "Recap",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        },
        {
          "anchor": "comments",
          "level": 0,
          "title": "2 comments"
        }
      ],
      "headingsCount": 10
    },
    "contents": {
      "markdown": "Preamble\n========\n\nIn this sequence of posts, I want to lay out why I am worried about risks from powerful AI and where I think the specific dangers come from. In general, I think it's good for people to be able to form their own inside views of what’s going on, and not just defer to people. There are surprisingly few descriptions of actual risk models written down. \n\nI think writing down your own version of the AI risk story is good for a few reasons\n\n*   It makes you critically examine the risk models of other people, and work out *what you actually believe.*\n*   It may be helpful for finding research directions which seem good. Solving a problem seems much more doable if you can actually point at it.\n*   It seems virtuous to attempt to form your own views rather than just the consensus view (whatever that is).\n*   People can comment on where your story may be weak or inconsistent, which will hopefully push towards the truth.\n\nAdditionally, I have some friends and family who are not in EA/AI-safety/Longtermism/Rationality, and it would be nice to be able to point them at something describing why I’m doing what I’m doing (technical AI-safety). Although, admittedly my views are more complicated than I initially thought, so this isn’t a great first introduction to AI-risk.\n\nI don’t expect much or any of these posts to be original and there will be many missing links and references. This can maybe be viewed as a more quick and dirty [AGI safety from first principles](https://www.lesswrong.com/s/mzgtmmTKKn5MuCzFJ/p/8xRSjC76HasLnMGSf), with less big picture justification and more focus on my specific risk models. In general, I am concerned most about [deceptively aligned](https://www.lesswrong.com/posts/zthDPAjh9w6Ytbeks/deceptive-alignment) AI systems, as discussed in [Risks from Learned Optimization](https://www.lesswrong.com/s/r9tYkB2a8Fp4DN8yB). \n\nHow do we train neural networks?\n================================\n\nIn the current paradigm of AI, we train neural networks to be good at tasks, and then we deploy them in the real world to perform those tasks. \n\nWe train neural networks on a *training distribution*\n\n*   For image classifiers, this is a set of labeled images. E.g. a set of images of cats and dogs with corresponding labels.\n*   For language models like GPT-3, this is a very large amount of text from the internet.\n*   For reinforcement learning, this is some training environment where the AI agent can ‘learn’ to take actions. For example, balancing a pole on a cart or playing the Atari game Breakout. \n\n**We start with an untrained AI system and modify it to perform better on the training distribution.** For our cat vs dog classifier, we feed in images of cats and dogs (our training data) and modify the AI so that it is able to accurately label these images. For GPT-3, we feed in the start of a string of text and modify it such that it can accurately predict what will come next. For our reinforcement learning agent playing Breakout, the agent takes actions in the game (move the platform left and right), we reward actions which lead to high score, and use this reward to train the agent to do well in this game. \n\nIf the training is good then our AI system will then be able to generalize and perform well in slightly different domains. \n\n*   Our cat vs dog classifier can correctly label images of cats and dogs it hasn’t seen before\n*   Our language model can ‘predict’ the next word for a given sensible prompt, and use this to generate coherent text \n*   Our reinforcement learning agent can play the game, even if it hasn’t seen this exact game state before. Or even generalize to slightly different environments; what if there are more or differently positioned blocks?\n\nThis ability to generalize is limited by the task it was trained on; we can only really expect good generalization on data that is *similar enough* to the training distribution. Our cat vs dog classifier might struggle if we show it a lion. If our language model has only been trained on English and German text, it won’t be able to generate French. Our Breakout agent can really only play Breakout. \n\nOptimization\n============\n\nWhen we train our AI systems we are optimizing them to perform well on the training distribution. By *optimizing* I mean that we are modifying these systems such that they do well on some objective. \n\nOptimized vs Optimizers\n-----------------------\n\nIt is important to make a distinction between something which is *optimized *and something which is an *optimizer*. When we train our AI systems, we end up with an optimized system; the system has been optimized to perform well on a task, be that cat vs dog classification, predicting the next word in a sentence, or achieving a high score at Breakout. These systems have been *optimized* to do well on the objective we have given them, but they themselves (probably) aren’t *optimizers*; they don’t have any notion of improving on an objective. \n\nOur cat vs dog classifier likely just has a bunch of heuristics which influence the relative likelihood of ‘cat’ or ‘dog’. Our Breakout agent is probably running an algorithm which looks like “The ball is at position X, the platform is at position Y, so take action A”, and not something like “The ball is at position X, the platform is at position Y, if I take action A it will give me a better score than action B, so take action A”. \n\nWe did the optimizing with our training and ended up with an optimized system. \n\nHowever, there are reasons to expect that we will get ‘optimizers’ as we build more powerful systems which operate in complex environments. AI systems can solve a task in 2 main ways (although the boundary here is fuzzy)\n\n*   They can use a bunch of heuristics that mechanistically combine to choose an action. For example, “If there is <this texture> +3 to the dog number, if there is <this triangle shape> +4 to the cat number, if there is <this color> next to <this color> +2 to the dog number… etc”.\n*   They can ‘do optimization’; search over some space of actions and find which one performs best on some criteria. For example, a language model evaluating outputs on “Which of these words is most likely to come next?”, or our Breakout AI asking “What will be my expected overall score if I take this action?”\n\nAs our tasks get more complex, if we are using the heuristic strategy, we will need to pile on more and more heuristics to perform well on the task. It seems like the optimization approach will become favored as things become more complex because the complexity of the optimization (search and evaluate) algorithm doesn’t increase as much with task complexity. If we are training on a very complex task and we want to achieve a certain level of performance on the training distribution, the heuristic-based algorithm will be more complex than the optimization-based algorithm. One intuition here is that for very varied and complex tasks, the AI may require some kind of “general reasoning ability” which is different from the pile of heuristics. One relatively simple way of doing “general reasoning” is to have an evaluation criterion for the task, and then evaluate possible actions on this criterion. \n\nIf AI systems are capable of performing complex tasks, then it seems like there will be very strong economic pressures to develop them. I expect by default for these AI systems to be running some kind of optimization algorithm. \n\nWhat do we tell the optimizers to do?\n-------------------------------------\n\nAssuming that we get optimizers, we need to be able to tell them what to do. By this I mean when we train a system to achieve a goal, *we want that goal to actually be one that we want*. This is the “Outer Alignment Problem”. \n\nThe classic example here is that we run a paperclip factory, so we tell our optimizing AI to make us some paperclips. This AI has no notion of anything else that we want or care about so it would sacrifice literally anything to make more paperclips. It starts by improving the factory we already have and making it more efficient. This still isn’t making the maximal number of paperclips, so it commissions several new factories. The human workers are slow, so it replaces them with toilless robots. At some point, the government gets suspicious of all these new factories, so the AI uses its powers of superhuman persuasion to convince them this is fine, and in fact, this is in the interest of National Security. This is still very slow compared to the maximal rate of paperclip production, so the AI designs some nanobots which convert anything made of metal into paperclips. At this point, it is fairly obvious to the humans that something is very, very wrong, but this feeling doesn’t last very long because soon the iron in the blood of every human is used to make paperclips (approximately 3 paperclips per person). \n\nThis is obviously a fanciful story, but I think it points at an important point; it’s not enough to tell the AI what to do, we also have to be able to tell it *what not to do.* Humans have pretty specific values, and it seems extremely difficult to specify. \n\nThere are more plausible stories we can tell which lead to similarly disastrous results. \n\n*   If we want our AI system to maximize the number in a bank account, if it is powerful enough it might hack into the bank’s computer system to modify the number and then take further actions to ensure the number is not modified back. \n*   If we tell our AI to maximize the number in a bank account but not ‘break any laws’, then it may aim to build factories which are technically legal but detrimental for the environment. Or it may just blackmail lawmakers to create legal loopholes for it to abuse. \n*   If we ask our AI to do any task where success is measured by a human providing a reward signal (e.g. every hour rating the AI’s actions out of 10 via a website), then the AI has a strong incentive to take control of the reward mechanism. For example, hacking into the website, or forcing the human to always provide a high reward. \n\nI think there are some methods for telling AI systems to do things, such that they *might not* optimize catastrophically. Often these methods involve the AI learning the humans’ preferences from feedback, rather than just being given a metric to optimize for. There is still a possibility that the AI learns an incorrect model of what the human wants, but potentially if the AI is appropriately uncertain about its model of human values then it can be made to defer to humans when it might be about to do something bad. \n\nOther strategies involve training an AI to mimic the behavior of a human (or many humans), but with some ‘amplification’ method which allows the AI to outperform what humans can actually achieve. For example, an AI may be trained to answer questions by mimicking the behavior of a human who can consult multiple copies of (previous versions of) the AI. At its core, this is copying the behavior of a human who has access to very good advisors, and so hopefully this will converge on a system which is aligned with what humans want. This approach also has the advantage that we have simply constructed a question-answering AI, rather than a “Go and do things in the world” AI, and so this AI may not have strong incentives to attempt to influence the state of the world. \n\nI think approaches like these (and others) are promising, and at least give me some hope that there might be some ways of specifying what we want an AI to do. \n\nHow do we actually put the objective into the AI?\n=================================================\n\nThere is an additional (and maybe harder) problem: even if we knew how to specify the thing that we want, how do we put that objective into the AI? This is the ‘Inner Alignment Problem”. This is related to the generalization behavior of neural networks; a network could learn a wide range of functions which perform well on the training distribution, but it will only have learned what we want it to learn if it performs ‘well’ on unseen inputs.\n\nCurrently, neural networks generalize surprisingly well; \n\n*   Image classifiers can work on images they’ve never seen before\n*   Language models can generate text and new ‘ideas’ which aren’t in the training corpus\n\nIn some sense this is obvious, if the models were only able to do well on things we already knew the answer to then they wouldn’t be very useful. I say “surprisingly” because there are many different configurations of the weights which lead to good performance on the training data without any guarantees about their performance on new, unseen data. Our training processes reasonably robustly find weight configurations which do well on *both* the training and test data. \n\nOne of the reasons neural networks seem to generalize is because they are biased towards simple functions, and the data in the world is also biased in a similar way. The data generating processes in the real world (which map “inputs” to “outputs”) are generally “simple” in a similar way to the functions that neural networks learn. This means that if we train our AI to do well on the training data, when we show it some new data it doesn’t go too wild with its predictions and is able to perform reasonably well. \n\nThis bias towards simplicity is also why we might expect to learn a function which acts as an optimizer rather than a pile of heuristics. For a very complex task, it is simpler to learn an optimization algorithm than a long mechanistic list of heuristics. If we learn an algorithm which does well on the training distribution by optimizing for something, **the danger arises if we are not sure what the algorithm is optimizing for *****off the training distribution*****. **\n\nThere will be many objectives which are consistent with good performance on the training distribution but then cause the AI system to do wildly different things off distribution. Some of these objectives will generalize in ways that humans approve of, but many others will not. In fact, because human values are quite specific, it seems like the vast majority of objectives that an AI could learn will *not* be ones that humans approve of. It is an open question what kinds of objectives an AI will develop by default.\n\nIt does however seem like AIs will develop long term goals by default. If an AI is trained to do well on a task, it seems unlikely to arbitrarily not care about the future. For example, if we train an AI to collect apples, it will attempt to maximize the number of apples over all time (maybe with some temporal discount factor), rather than only maximize apples collected in a 10 minute interval. This is probably true even if the AI was only ever trained for 10 minute intervals. The objective “maximize apples” seems far less arbitrary than “maximize apples for 10 minutes and then don’t care about them”.\n\nDeception\n=========\n\nThere is an additional danger if an AI system is ‘deliberately’ attempting to obscure its objective/intentions from the humans training it. The term ‘deception’ is often used to refer to two different things which could happen when we train AI systems, which I outline [here.](https://www.lesswrong.com/posts/8whGos5JCdBzDbZhH/framings-of-deceptive-alignment) \n\nIf the AI is being trained on a difficult task, it might be easier for the AI to trick the evaluator (maybe a human) into giving a high reward, rather than actually doing well on the task. I’ll call this ‘Goodhart deception’ because the AI is ‘[Goodharting](https://www.lesswrong.com/tag/goodhart-s-law)’ the reward rather than optimizing for what humans actually want. Importantly, this doesn’t require the AI to have any objective or be optimizing for anything, the behavior which led to high reward (tricking the human) was just reinforced. This seems bad, but not as catastrophically bad as the other type of deception might be.\n\nThe other type of deception is if an optimizing AI system intentionally deceives the humans about its true goals. In this scenario, the AI system develops an objective which is not aligned with the human objective. Here the objective is extended across time, which seems potentially like the default for learned objectives. The AI knows that if it attempts to directly go for its objective then it will either be turned off or be modified to remove this objective. So the AI will ‘pretend’ to not have this goal and instead ‘play along’ and do well on the task it is being trained for. After training, when the AI is deployed into the world, it is free to defect and pursue its own (misaligned) objective. \n\nI’ll call this ‘consequentialist deception’ because the AI is acting as a consequentialist (taking actions because of their consequences in the world, rather than just using mechanistic heuristics), or maybe just ‘deception’.\n\nThis requires 3 (possibly likely) things to happen\n\n*   The AI system is an optimizer and develops an objective (not the human’s objective) which is extended across time\n*   The AI’s ability to deceive^[\\[1\\]](#fnexa2pbk5bzb)^ the humans is reinforced, or at least not removed by the training process\n*   There will come a time when the system can ‘safely’ defect (or at least the AI thinks there is a chance that this might happen)\n\nConsequentialist deception seems especially dangerous because it will lead to the *same behavior on the training distribution* as a non-deceptive AI. This makes it especially hard to tell if the model is deceptive. Even if the model performs well on the training distribution, we don’t know how it will behave when it is deployed. If the model is deceptive then it could perform *arbitrarily badly* in deployment, as it pursues its own objective. \n\nFor a model to be deceptive it needs to develop a different objective than the objective it is being trained on. Currently, when we train neural networks, this looks approximately like we randomly pick a function from the set of functions which perform well on the training distribution. Functions which take fewer parameters to specify (have more free/unused parameters) have a higher likelihood of being chosen. Consider a function \\\\(F_{a, b}(x)\\\\) which requires both \\\\(a=b=3\\\\) to do well on the training distribution, compared with a function \\\\(G_{a, b}(x)\\\\) which only requires \\\\(a=2\\\\) to do well on the training distribution. If we are randomly sampling functions that do well on the training distribution, we are more likely to select \\\\(G\\\\) because we only need to get \\\\(a\\\\) correct, and any value of \\\\(b\\\\) works, while \\\\(F\\\\) requires us to pick both \\\\(a\\\\) and \\\\(b\\\\) correctly. \n\nIt is pretty unclear how this “bias towards simple functions”/”bias towards functions with lots of free parameters” influences the probability of deception. Deceptive algorithms seem more complex than non-deceptive ones because they need to be able to both recognize they are in a training process, and then decide to do well on the training task. But deception might be the default behavior for any optimizing algorithm to develop, and so the simplicity of optimizing algorithms might mean that deception is more likely. \n\nAdditionally, deceptive algorithms are persistent in a way that non-consequentialist algorithms are not. For normal neural network training, you will likely find a suboptimal algorithm early in training and then this will be modified into a different algorithm as training progresses; the algorithm does not ‘want’ to persist. But if you find a sufficiently capable deceptive algorithm early in training, then this will attempt to persist until the end of training. This means that if your AI becomes deceptive at any point during training, it will likely continue to be deceptive. This implies that the “randomly sample functions which perform well on the training distribution” lens may not be accurate, and in fact there is a lot of path-dependency in the development of deceptive algorithms. \n\nRecap\n=====\n\nSo to recap:\n\n1.  Optimization is scary, and if we train an AI system to take actions in the world, as the task gets more complicated and our AI systems get more powerful we are more likely to develop optimizers.\n    *   By ‘optimizer’ I mean the AI runs a 'consequentialist' algorithm which looks something like “What actions can I take to maximize my objective?”, rather than just running through rote steps in a calculation.\n2.  We don’t have strong guarantees that off distribution our AIs will do what we want them to do.\n3.  There are reasons to expect that AIs we develop may have very different goals to our own, even if they perform well on the training distribution.\n4.  Other than performing well on the training distribution, and having some sort of bias towards “simplicity\", the AI could learn a whole range of objectives. \n5.  We already know that AI systems are capable of tricking humans, although in current systems this is a different phenomenon than deceiving humans for consequentialist reasons.\n6.  If an AI develops its own (misaligned) objective during training, then it may simply ‘play along’ until it is able to safely defect and pursue its own objective.\n    *   The AI’s objective may be arbitrarily different from what we were training it for, and easily not compatible with human values or survival. \n\n* * *\n\nIn the next two posts I will lay out [a more concrete story of how things go wrong](https://www.lesswrong.com/posts/u8yT9bbabmdnpgDaQ/a-story-of-ai-risk-instructgpt-n), and then list some of my current confusions.\n\n*Thanks to Adam Jermyn and Oly Sourbut for helpful feedback on this post. *\n\n1.  ^**[^](#fnrefexa2pbk5bzb)**^\n    \n    Including knowing that deception is even a possible strategy."
    },
    "voteCount": 13,
    "forceInclude": true
  },
  {
    "_id": "u8yT9bbabmdnpgDaQ",
    "url": null,
    "title": "A Story of AI Risk: InstructGPT-N",
    "slug": "a-story-of-ai-risk-instructgpt-n",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "AI"
      },
      {
        "name": "Mesa-Optimization"
      },
      {
        "name": "Inner Alignment"
      },
      {
        "name": "Threat Models"
      },
      {
        "name": "AI Risk"
      }
    ],
    "tableOfContents": {
      "sections": [
        {
          "title": "Training",
          "anchor": "Training",
          "level": 1
        },
        {
          "title": "Fine-tuning",
          "anchor": "Fine_tuning",
          "level": 2
        },
        {
          "title": "Examples",
          "anchor": "Examples",
          "level": 2
        },
        {
          "title": "Baking",
          "anchor": "Baking",
          "level": 3
        },
        {
          "title": "Physics",
          "anchor": "Physics",
          "level": 3
        },
        {
          "title": "Learned Algorithms",
          "anchor": "Learned_Algorithms",
          "level": 1
        },
        {
          "title": "Self-models",
          "anchor": "Self_models",
          "level": 2
        },
        {
          "title": "Deception",
          "anchor": "Deception",
          "level": 1
        },
        {
          "title": "Do we get deceptive algorithms?",
          "anchor": "Do_we_get_deceptive_algorithms_",
          "level": 2
        },
        {
          "title": "Misaligned objectives",
          "anchor": "Misaligned_objectives",
          "level": 3
        },
        {
          "title": "Complexity ",
          "anchor": "Complexity_",
          "level": 3
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        },
        {
          "anchor": "comments",
          "level": 0,
          "title": "No comments"
        }
      ],
      "headingsCount": 13
    },
    "contents": {
      "markdown": "The story from my [previous post](https://www.lesswrong.com/posts/k8hvGAJWSKAeHwpnJ/why-i-m-worried-about-ai) of how AI might develop a dangerously misaligned objective has so far been pretty abstract. I now want to put together a more concrete story of how I think things might go wrong. This risk model is based on a language model which has been fine tuned using reinforcement learning from human feedback, I’ll call this model InstructGPT-N. \n\nThis isn’t my most likely model of AI risk, this is more of a ‘minimum example’ which contains what I see as the core parts of the problem. I expect the real world to be more complicated and messy. Models will likely be trained on more than just text, and there may be multiple powerful AI systems simultaneously learning and acting in the real world. \n\nTraining\n--------\n\nWe start by training a large language model using self-supervised learning for next word prediction. This is a standard GPT system, where you feed in the start of a sequence of words/tokens and it predicts what comes next, and then you input a new sequence which now includes the word it just guessed, and so on. The output is fed back into the system to generate the next word. \n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/39bf9a29cde8bff43a11854bad1cc1fec74a48b5a5e4ea4e.png)\n\nThe model generating text one word at a time\n\nTo train the system, we start with a sequence of text and feed in the first word. We then update the system based on how accurately it predicted the second word. Then we feed in the first two words, and update the system to better predict the third word; then we feed in the first 3 words and update to better predict the fourth word, and so on. This trains the system to predict which word comes next, given some starting prompt. \n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/3a3434782403b43437110e83a02c01c77000b0e38d6ed2af.png)\n\nThe model being trained to predict the next word. The model predicts 'dog' is the most likely answer but the true answer is 'best', and so the model will be updated make better predictions.\n\nThis system is trained on *a lot* of text; a large fraction of the internet, every digitized scientific paper, all of Github and Stack Exchange, millions of novels and textbooks, billions of lines of chat logs and internet forums. \n\nBecause the system is trained on such an expansive corpus of text it is probably pretty bad at directly answering questions that humans ask it; only a tiny amount of the training data is in the question/answer format. Despite this, the model does have a lot of knowledge inside it; some part ‘knows’ how to code well, some part ‘knows’ all about human psychology and power dynamics, some part ‘knows’ about large language models and how they are trained. The model also ‘knows’ how to behave as if it is not competent; if you prompt it to mimic the behavior of a beginner programmer or a pseudo-history fan with bad epistemics then this is what it will do. The system hasn’t been trained to be *correct, *it has been trained to mimic text. Even if we don’t deliberately prompt it to answer incorrectly or misleadingly, it may still do this if the training data contained incorrect or misleading text (which is true of a lot of internet text). For this model to be useful to humans, it will be fine-tuned to consistently behave competently for tasks humans care about. \n\n### Fine-tuning\n\nTo make this model useful for answering questions, the humans fine-tune it on human feedback. Here, the human asks a question, sees what the AI answers, and then gives a reward based on how good they think the AI’s answer was. This lets the humans take an AI model which has a lot of ‘internal knowledge’ but isn’t very good at answering questions, and turn it into a model which can use this internal knowledge to answer questions. The model is trained to perform well on sequences of questions, because this is more useful than just answering one disconnected question at a time. Here the human asks a question, receives an answer from the AI, and the human can then ask another question, and so on. \n\n### Examples\n\n**Baking**\n\n> Human: What is a good egg replacement for a cake I am baking for my vegan friends?\n> \n> AI: You can replace 1 egg with a mixture of 1 Tablespoon of ground flaxseed and 3 Tablespoons of water.\n> \n> Human: How will that change the flavor of the cake?\n> \n> AI: It will add a slightly nutty flavor, if you want a more neutral flavor you can use chia seeds instead of flaxseed.\n\n**Physics**\n\n> Human: Can you explain Newton’s second law to me?\n> \n> AI: Newton’s second law states that the change in momentum of an object is proportional to the force applied to the object. So when you apply a force to an object, it will begin to accelerate in the direction of the applied force. \n> \n> Human: How does this relate to the equation \\\\(F=ma\\\\)?\n> \n> AI: Momentum (denoted by \\\\(\\\\vec{p}\\\\)) is equal to the mass times the velocity of an object \\\\(\\\\vec{p}=m \\\\vec{v}\\\\). Force is equal to the change momentum, or more specifically the derivative of the momentum with respect to time \\\\(\\\\vec{F}=\\\\frac{d\\\\vec{p}}{dt}=\\\\frac{d}{dt}m\\\\vec{v}\\\\). Acceleration is the change in velocity with respect to time \\\\(\\\\vec{a}=\\\\frac{d\\\\vec{v}}{dt}\\\\). Putting this together we arrive at \\\\(\\\\vec{F}=m\\\\vec{a}\\\\). \n> \n> Human: What happens when you apply multiple forces to an object?\n> \n> AI: When you apply multiple forces to an object you can simply add values of the forces together to find the net force applied. It is important to remember that force is a vector, and so you need to add the vectors together, not sum the magnitude of the forces.\n> \n> Human: Can you give me an example?\n> \n> AI: If one robot pushes a box in the x-direction with 3 newtons of force, and another robot pushes in the y-direction with 1 newton of force, then the total force vector (in units of newtons) will be (3, 1).\n> \n> Or if there is a box which has 10 newtons of force down due to gravity, and a human applies 15 newtons of force upwards to lift it, then the box will have a net force upwards of -10 N + 15 N = 5 N, causing it to accelerate upwards. \n\nLearned Algorithms\n------------------\n\nThere are a few different algorithms which the model could learn which do well on the training distribution (where the training distribution is the human asking questions and giving rewards):\n\n*   A heuristic based, non-optimizing algorithm which just does robustly well on the training distribution (and maybe also does well when deployed).\n*   An algorithm which internally searches over/optimizes possible outputs and evaluates them based on the criterion “How much would a human like this answer?”\n*   An algorithm which internally searches over possible outputs and evaluates them based on the criterion “What is the expected reward a human would give this answer?”\n*   An algorithm which internally searches over possible outputs and evaluates them based on the criterion “What is the expected reward a human would give this answer, conditional on them being contracted to train an AI model, and on me being that AI model?”\n*   An algorithm which runs something like “I want the state of the world to be X, which output should I give so that the human training me doesn’t turn me off or modify me, so that I can then defect after training?”\n*   Probably others\n\nAs the tasks become more complicated (the human asks more difficult and varied questions), this will push the system into performing some kind of optimization/internal search process and away from standard mechanistic algorithms which aren’t performing optimization. This is because it is difficult to compress heuristics for extremely varied tasks into a limited number of parameters. Hence we are likely to end up with an AI system which finds its output by evaluating potential outputs on some internal objective.\n\nA better specified objective (an objective which is more detailed and also doesn’t rely on incorrect assumptions) will lead to better performance on the training distribution. If the objective is “How much would a human like this answer?”, this will lead to worse performance (lower reward from the human) than the objective of “What is the expected reward the human would give this answer, conditional on them being contracted to train an AI, and on me being that AI model?” This more specific objective will lead to higher reward because it allows the AI to leverage its knowledge of how people train AI’s, what kind of people train AI’s, and what particular kinds of biases and preferences the AI trainers have.\n\n### Self-models\n\nIt seems instrumentally useful for the AI to develop a self-model and some concept of the humans training it, because these will allow it to reason about itself and the training process. These concepts (the self-model and the concept of the current training process) are more than just ‘there are GPT language model systems, and humans train them’, which would have just existed in the pretraining corpus. These concepts refer explicitly to the AI itself and the training process it is in, rather than some abstract AI system. Even if none of the fine-tuning questions that the human asks include references to the AI or the training, knowing about these things will let the AI perform better in training. And so gradient descent may allow it to use its world model and knowledge of AI systems to develop this self-model, simply because this leads to better performance on the training distribution. \n\nOne additional reason for the AI to develop a self-model and realize it is an AI in a training process, is that *this is actually true.* We could potentially model the training process as approximating some Bayesian updating process. We start out with a set of hypotheses which may or may not be true, and then update the probabilities assigned to these hypotheses as we get additional evidence. Because the AI actually is an AI in a training process, the probability assigned to this hypothesis will eventually get upweighted as the system learns. \n\nDeception\n---------\n\nThe real danger comes when we are not sure whether the AI system is evaluating its outputs based on “Would a human training an AI system give this a high score, assuming that I am that AI system?” or “Would this output lead to the ultimate state of the world (including after training) being X?”\n\nThe second option could mean that the AI acts as a deceptive consequentialist, where it may ‘lie dormant’ and play along, pretending to be using the other (aligned) algorithm. Or even more perversely, the AI could use its outputs to subtly manipulate the human training it; this could allow the AI to be updated in even more malign ways. This second, more malign option seems possible because the AI has a lot of ‘knowledge’ about human psychology in its world model from the pretraining; both from many academic works on psychology and also from unimaginably many records of human interactions. \n\n### Do we get deceptive algorithms?\n\nIt seems unclear whether we end up with a deceptive algorithm rather than an aligned version. One reason to expect a deceptive algorithm is that any consequentialist algorithm that the training process finds has an incentive to be deceptive, unless it is already precisely aligned to begin with. And so once the model becomes deceptive it will attempt to stay that way and avoid having its objectives modified. There are also many misaligned objectives that could be learned, while there is only one aligned objective (or at least aligned objectives make up a much smaller portion of possible objectives). \n\n**Misaligned objectives**\n\nAs an example of how an AI could develop a misaligned objective, we could imagine it learning an incorrect model of human preferences. Suppose that one of the humans training the AI has a particular affinity for flowers, and hence gives the AI more reward for answers which result in there being more flowers in the world. The AI may learn to terminally value flowers, rather than terminally valuing providing useful answers to a human’s questions. Once the AI has started to value flowers, it doesn’t care what values the human is trying to instill into it. Its objective is its objective, regardless of how much the human dislikes this. \n\nThis problem of learning an incorrect model of human preferences is amplified when we consider the specifics of how reinforcement learning from human preferences works in practice. Human feedback/rewards are expensive (because a human actually has to be in the loop), so instead we train a separate model to predict what reward the human would provide. This reward model is then used to fine-tune the language model AI. If this learned reward model is misspecified then it may cause the AI to learn a misaligned objective. Even if the reward model is updated over time to become more accurate, this will not matter if the AI has already developed a misaligned objective from an early (bad) version of the reward model. \n\n**Complexity **\n\nSomething that points against this is that these deceptive algorithms are in some sense more complex than the aligned algorithm. A deceptive algorithm has to reason about its objectives in the world and then also reason about what the human wants, while the aligned algorithm only has to reason about what the human wants. It is unclear where this balance between the number of misaligned objectives and the simplicity of the aligned objectives falls. I think it seems more likely that by default we end up with a misaligned algorithm, partially because once we find one of these algorithms it will ‘fight’ against any attempt to remove it. \n\nBut even if we were ‘more likely’ to end up with an aligned model, I don’t like betting the future on something which is merely ‘more likely’.   \n \n\n*Thanks to Oliver Sourbut for very useful feedback on this post. *\n\n* * *\n\nIn the next post I'll try to lay out some confusions round my picture of AI risk, and some reasons why it may be wrong or confused."
    },
    "voteCount": 15,
    "forceInclude": true
  },
  {
    "_id": "aZ2kHQtpTHu3FeguQ",
    "url": null,
    "title": "Confusions in My Model of AI Risk",
    "slug": "confusions-in-my-model-of-ai-risk",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "AI Risk"
      },
      {
        "name": "AI"
      }
    ],
    "tableOfContents": {
      "sections": [
        {
          "title": "What actually is optimization?",
          "anchor": "What_actually_is_optimization_",
          "level": 1
        },
        {
          "title": "Where does the consequentialism come from?",
          "anchor": "Where_does_the_consequentialism_come_from_",
          "level": 1
        },
        {
          "title": "Will systems consistently work as optimizers?",
          "anchor": "Will_systems_consistently_work_as_optimizers_",
          "level": 1
        },
        {
          "title": "What does the simplicity bias tell us about optimizers?",
          "anchor": "What_does_the_simplicity_bias_tell_us_about_optimizers_",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        },
        {
          "anchor": "comments",
          "level": 0,
          "title": "9 comments"
        }
      ],
      "headingsCount": 6
    },
    "contents": {
      "markdown": "A lot of [the reason I am worried about AI](https://www.lesswrong.com/posts/k8hvGAJWSKAeHwpnJ/why-i-m-worried-about-ai) comes from the development of optimizers that have goals which don’t align with what humans want. However, I am also pretty confused about the specifics here, especially core questions like “what actually do we mean by optimizers?” and “are these optimizers actually likely to develop?”. This means that much of my thinking and language when talking about AI risk is fuzzier than I would like. \n\nThis confusion about optimization seems to run deep, and I have a vague feeling that the risk paradigm of “learning an optimizer which doesn’t do what we want” is likely confused and somewhat misleading. \n\nWhat actually is optimization?\n------------------------------\n\nIn [my story of AI risk](https://www.lesswrong.com/posts/k8hvGAJWSKAeHwpnJ/why-i-m-worried-about-ai) I used the term ‘optimization’ a lot, and I think it’s a very slippery term. I’m not entirely sure what it means for something to ‘do optimization’, but the term does seem to be pointing at something important and real. \n\nA definition from [The Ground of Optimization](https://www.lesswrong.com/posts/znfkdCoHMANwqc2WE/the-ground-of-optimization-1) says an optimizing system takes something from a wide set of states to a smaller set of states and is robust to perturbations during this process. Training a neural network with gradient descent is an optimization process under this definition because we could start with a wide range of initial network configurations, and the network is modified to be in one of the few configurations which do well on the training distribution, and even if we add a (reasonable) perturbation the weights will still converge. I think this is a good definition, but it is entirely defined in terms of behavior rather than a mechanistic process. Additionally, it doesn’t really match exactly with the picture where there is an optimizer which optimizes for an objective. This optimizer/objective framework is the main way that I’ve talked about optimizers, but also I would not be surprised if this framing turned out to be severely confused.\n\nOne possible way that a network could ‘do optimization’ would be for it to do some kind of internal search or internal iterative evaluation process to find the best option. For example, seeing which response best matches a question, or searching a game tree to find the best move. This seems like a broadly useful style of algorithm for a neural network to learn, especially when the training task is complicated. But it also seems unlikely for networks to implement this *exactly*; it seems much more likely that networks will implement something that looks like a mash of some internal search and some heuristics. \n\nAdditionally, it seems like the boundary between solving a task with heuristics and solving it with optimization is fuzzy. As we build up our pile of heuristics, does this suddenly snap into being an optimizer, or does it slowly become more like an optimizer as gradient descent adds and modifies the heuristics? \n\nFor optimization to be actually dangerous, the AI needs to have objectives which are actually connected to the real world. Running some search process entirely internally to generate an output seems unlikely to lead to catastrophic behavior. However, there are objectives which the AI could easily develop which are connected to the real world. This includes the AI messing with the real world to ensure it gets certain inputs, which lead to certain internal states.\n\nWhere does the consequentialism come from?\n------------------------------------------\n\nMuch of the danger from optimizing AIs comes from *consequentialist* optimizing AIs. By consequentialist I mean that the AI takes actions based on their consequences in the world.^[\\[1\\]](#fno6gnrcnfhs)^ I have a reasonably strong intuition that reinforcement learning is likely to build consequentialists. I think RL probably does this because it explicitly selects for policies based on how well they do on consequentialist tasks; the AI needs to be able to take actions which will lead to good (future) consequences on the task. Consequentialist behavior will robustly do well during training, and so this behavior will be reinforced. It seems important that the tasks are extended across time, rather than being a single timestep, otherwise the system doesn’t need to develop any longer term thinking/planning. \n\nRL seems more likely to build consequentialists than training a neural network for classification or next word prediction. However, these other systems might develop some ‘inner optimizer/consequentialist’ algorithms, because these are good ways to answer questions. For example, in GPT-N if the tasks are diverse enough, maybe the algorithm which is learned is basically an optimizer which looks at the task and searches for the best answer. I’m unsure how or if this ‘inner optimizer’ behavior could lead to the AI having objectives over the real world. It is *conceivable *that the first algorithm which the training process ‘bumps into’ is a consequentialist optimizer which cares about states of the world, even if it doesn’t have access to the external world during training. But it feels like we would have to be unlucky for this to happen, because there isn’t any selection pressure pushing for this AI system to develop this kind of external world objective. \n\nWill systems consistently work as optimizers?\n---------------------------------------------\n\nIt seems reasonably likely that neural networks will only act as optimizers in some environments (in fact, no-free-lunch theorems might guarantee this). On some inputs/environments, I expect systems to either just break or do things which look more heuristic-y than optimization-y. This is a question about how much the capabilities of AI systems will generalize. It seems possible that there will be domains where the system’s capabilities generalize (it can perform coherent sequences of actions), but its objectives do not (it starts pursuing a different objective). \n\nThere will be some states where the system is capable and does what humans want, for example, on the training distribution. But there may be more states where the system is able to capably do things, but no longer does what humans want. There will also be states of the world where the AI both doesn’t act capably or do what humans want, but these states don’t seem as catastrophically dangerous. \n\nConsequentialist deception could be seen as an example of the capabilities generalizing further than the aligned objective; where the system is still able to perform capably off the training distribution, but with a misaligned goal. The main difference here seems to be that the system was always ‘intending’ to do this, rather than just entering a new region of the state space and suddenly breaking. \n\nIt isn’t really important that the AI system acts as an optimizer for all possible input states, or even for the majority of the states that it actually sees. What is important is if the AI acts as an optimizer for *enough* of its inputs to cause catastrophe. Humans don’t always act as coherent optimizers, but to the extent that we *do* act as optimizers we can have large effects on the state of the world. \n\nWhat does the simplicity bias tell us about optimizers?\n-------------------------------------------------------\n\nNeural networks seem to have a bias towards learning simple functions. This is part of what lets them generalize and not just go wild when presented with new data. However, this is a claim about the *functions* that neural networks learn, it is not a claim about the objectives that an optimizer will use. It does seem much more natural for simpler objectives to be easier to find because in general adding arbitrary conditions makes things less likely. We could maybe think of the function that an optimizing neural network implements as being made up of the optimizer (for example, Monte Carlo Tree Search) and the objective (for example, maximize apples collected). If the optimizer and objective are (unrealistically) separable, then all else equal a simpler objective will lead to a simpler function. I wouldn’t expect for these to be cleanly separable, I expect that for a given optimizer some objectives are much simpler or easier to implement than others. \n\nWe may be able to eventually form some kind of view around what kind of ‘simplicity bias’ we expect for *objectives*, I would not be surprised if this was quite different from the simplicity bias we see in the *functions* learned by neural nets. \n\n1.  ^**[^](#fnrefo6gnrcnfhs)**^\n    \n    Systems which are not consequentialist could for example not be optimizers, or alternatively systems which optimize for *taking* actions but not because of the effect of the actions in the world. A jumping robot that just loves to jump could be an example of this."
    },
    "voteCount": 7,
    "forceInclude": true
  },
  {
    "_id": "9DWcNS2rkvd2J8mHH",
    "url": null,
    "title": "Formalizing Value Extrapolation",
    "slug": "formalizing-value-extrapolation",
    "author": null,
    "question": false,
    "tags": [],
    "tableOfContents": null,
    "contents": {
      "markdown": "A recent [post](http://ordinaryideas.wordpress.com/2012/04/21/indirect-normativity-write-up/) at my blog may be interesting to LW. It is a high-level discussion of what precisely defined value extrapolation might look like. I mostly wrote the essay while a visitor at FHI. \n\nThe basic idea is that we can define extrapolated values by just taking an emulation of a human, putting it in a hypothetical environment with access to powerful resources, and then adopting whatever values it eventually decides on. You might want some philosophical insight before launching into such a definition, but since we are currently laboring under the threat of catastrophe, it seems that there is virtue in spending our effort on avoiding death and delegating whatever philosophical work we can to someone on a more relaxed schedule. \n\nYou wouldn't want to run an AI with the values I lay out, but at least it is pinned down precisely. We can articulate objections relatively concretely, and hopefully begin to understand/address the difficulties. \n\n(Posted at the request of cousin_it.)"
    },
    "voteCount": 25,
    "forceInclude": true
  },
  {
    "_id": "i8sHdLyGQeBTGwTqq",
    "url": null,
    "title": "Value extrapolation, concept extrapolation, model splintering",
    "slug": "value-extrapolation-concept-extrapolation-model-splintering",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "Value Learning"
      },
      {
        "name": "AI"
      }
    ],
    "tableOfContents": {
      "sections": [
        {
          "title": "Definitions",
          "anchor": "Definitions",
          "level": 1
        },
        {
          "title": "Examples",
          "anchor": "Examples",
          "level": 1
        },
        {
          "title": "Helping with multiple methods",
          "anchor": "Helping_with_multiple_methods",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        },
        {
          "anchor": "comments",
          "level": 0,
          "title": "1 comment"
        }
      ],
      "headingsCount": 5
    },
    "contents": {
      "markdown": "*Post written with Rebecca Gorman*.\n\nWe've [written before](https://www.lesswrong.com/posts/k54rgSg7GcjtXnMHX/model-splintering-moving-from-one-imperfect-model-to-another-1) that model splintering, as we called it then, was a problem with almost all AI safety approaches.\n\nThere's a converse to this: solving the problem would help with almost all AI safety approaches. But so far, we've been [posting](https://www.lesswrong.com/posts/DjTKMEwRqpuKkJzTo/are-there-alternative-to-solving-value-transfer-and) [mainly](https://www.lesswrong.com/posts/thZdioHTZALRPKmiH/value-extrapolation-partially-resolves-symbol-grounding) about value extrapolation. In this post, we'll start looking at how other AI safety approaches could be helped.\n\n## Definitions\n\nTo clarify, let's make four definitions, distinguishing ideas that we'd previously been grouping together:\n\n**Model splintering** is when the features and concepts that are valid in one world-model, break down when transitioning to another world-model.\n\n**Value splintering** (or reward splintering) is when the value function (or reward function, or goal, or preference...) becomes invalid due to model splintering.\n\n**Concept extrapolation** is extrapolating a feature or concept from one world-model to another.\n\n**Value extrapolation** is concept extrapolation when the particular concept to extrapolate is a value, a preference, a reward function, an agent's goal, or something of that nature.\n\nThus concept extrapolation is a solution to model splintering, while value extrapolation is a solution to value splintering specifically.\n\n## Examples\n\nConsider for example Turner *et al*'s [attainable utility](https://arxiv.org/pdf/1902.09725.pdf). It has a formal definition, but the reason for that definition is that preserving attainable utility is aimed at restricting the \"power\" of the agent, or at minimising its \"side effects\".\n\nAnd it succeeds, in the typical situation. If you measure the attainable utility of an agent, this will give you an idea of its power, and how many side effects it may be causing. However, when we move to general situations, this [breaks down](https://www.lesswrong.com/s/iRwYCpcAXuFD24tHh/p/mdQEraEZQLg7jtozn): attainable utility preservation no longer restricts power or reduces side effects. So the concepts of power and side effects have splintered when moving from typical situations to general situations. This is the **model splintering**[^correlation]. If we solve **concept extrapolation** for this, then we could extend the concepts of power restriction or side effect minimisation, to the general situations. And thus successfully create low impact AIs.\n\n[^correlation]: Equivalently, we could say that the concepts remain the same, but it's the correlation between \"attainable utility preservation\" and \"power restriction\" is what breaks down.\n\nAnother example is wireheading. We have a reward signal that corresponds to something we desire in the world; maybe the negative of the $\\textrm{CO}_\\textrm{2}$ concentration in the atmosphere. This is measured by, say, a series of $\\textrm{CO}_\\textrm{2}$ detectors spread over the Earth's surface.\n\nTypically, the reward signal does correspond to what we want. But if the [AI hacks its own reward signal](https://www.lesswrong.com/posts/vXzM5L6njDZSf4Ftk/defining-ai-wireheading), that correspondence breaks down[^correlation2]: **model splintering**. If we can extend the reward properly to new situations, we get **concept extrapolation** - which, since this is a reward function, is **value extrapolation**.\n\n[^correlation2]: There are multiple ways we can see the concepts breaking down. We can see the concept of \"measured $\\textrm{CO}_\\textrm{2}$\" breaking down. We can see the correlation between $\\textrm{CO}_\\textrm{2}$ concentration and the reward breaking down. We can see the correlation between the reward and the *reward signal* breaking down. The reason there are so many ways of seeing the breakdown is because [most descriptive labels describe collections of correlated features, rather than fundamental concepts](https://www.lesswrong.com/posts/xoQhHxgwdHvWhj4P4/reward-splintering-for-ai-design). So the descriptions/features/concepts break down when the correlations do.\n\n## Helping with multiple methods\n\nHence the concept extrapolation/value extrapolation ideas can help with many different approaches to AI safety, not just the value learning approaches."
    },
    "voteCount": 4,
    "forceInclude": true
  },
  {
    "_id": "k54rgSg7GcjtXnMHX",
    "url": null,
    "title": "Model splintering: moving from one imperfect model to another",
    "slug": "model-splintering-moving-from-one-imperfect-model-to-another-1",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "AI"
      },
      {
        "name": "Machine Learning  (ML)"
      },
      {
        "name": "Iterated Amplification "
      }
    ],
    "tableOfContents": {
      "sections": [
        {
          "title": "1. The big problem",
          "anchor": "1__The_big_problem",
          "level": 1
        },
        {
          "title": "1.1 In the language of traditional ML",
          "anchor": "1_1_In_the_language_of_traditional_ML",
          "level": 2
        },
        {
          "title": "1.2 Model splintering examples",
          "anchor": "1_2_Model_splintering_examples",
          "level": 2
        },
        {
          "title": "1.3 Avoiding perfect models",
          "anchor": "1_3_Avoiding_perfect_models",
          "level": 2
        },
        {
          "title": "2 Why focus on the transition?",
          "anchor": "2_Why_focus_on_the_transition_",
          "level": 1
        },
        {
          "title": "2.1 Humans reason like this",
          "anchor": "2_1_Humans_reason_like_this",
          "level": 2
        },
        {
          "title": "2.2 There are no well-defined overarching moral principles",
          "anchor": "2_2_There_are_no_well_defined_overarching_moral_principles",
          "level": 2
        },
        {
          "title": "2.3 It helps distinguish areas where AIs fail, from areas where humans are uncertain",
          "anchor": "2_3_It_helps_distinguish_areas_where_AIs_fail__from_areas_where_humans_are_uncertain",
          "level": 2
        },
        {
          "title": "2.4 We don't need to make the problems harder",
          "anchor": "2_4_We_don_t_need_to_make_the_problems_harder",
          "level": 2
        },
        {
          "title": "2.5 We don't know how deep the rabbit hole goes",
          "anchor": "2_5_We_don_t_know_how_deep_the_rabbit_hole_goes",
          "level": 2
        },
        {
          "title": "2.6 We often only need to solve partial problems",
          "anchor": "2_6_We_often_only_need_to_solve_partial_problems",
          "level": 2
        },
        {
          "title": "2.7 It points out when to be conservative",
          "anchor": "2_7_It_points_out_when_to_be_conservative",
          "level": 2
        },
        {
          "title": "2.8 Difficulty in capturing splintering from the idealised perspective",
          "anchor": "2_8_Difficulty_in_capturing_splintering_from_the_idealised_perspective",
          "level": 2
        },
        {
          "title": "2.9 It may help amplification and distillation",
          "anchor": "2_9_It_may_help_amplification_and_distillation",
          "level": 2
        },
        {
          "title": "2.10 Examples of model splintering problems/approaches",
          "anchor": "2_10_Examples_of_model_splintering_problems_approaches",
          "level": 2
        },
        {
          "title": "3 The virtues of formalisms",
          "anchor": "3_The_virtues_of_formalisms",
          "level": 1
        },
        {
          "title": "3.1 A model, in (almost) all generality",
          "anchor": "3_1_A_model__in__almost__all_generality",
          "level": 2
        },
        {
          "title": "3.2 Meta-model: models, features, environments, probabilities",
          "anchor": "3_2_Meta_model__models__features__environments__probabilities",
          "level": 2
        },
        {
          "title": "3.3 Bayesian models within this meta-model",
          "anchor": "3_3_Bayesian_models_within_this_meta_model",
          "level": 2
        },
        {
          "title": "4 Model refinement and splinterings",
          "anchor": "4_Model_refinement_and_splinterings",
          "level": 1
        },
        {
          "title": "4.1 Model refinement",
          "anchor": "4_1_Model_refinement",
          "level": 2
        },
        {
          "title": "4.2 Example of model refinement: gas laws",
          "anchor": "4_2_Example_of_model_refinement__gas_laws",
          "level": 2
        },
        {
          "title": "4.3 Example of model refinement: rubes and bleegs",
          "anchor": "4_3_Example_of_model_refinement__rubes_and_bleegs",
          "level": 2
        },
        {
          "title": "4.4 Reward function refactoring",
          "anchor": "4_4_Reward_function_refactoring",
          "level": 2
        },
        {
          "title": "4.5 Reward function splintering",
          "anchor": "4_5_Reward_function_splintering",
          "level": 2
        },
        {
          "title": "4.6 Reward function splintering: \"natural\" refactorings",
          "anchor": "4_6_Reward_function_splintering___natural__refactorings",
          "level": 2
        },
        {
          "title": "4.7 Splintering training rewards",
          "anchor": "4_7_Splintering_training_rewards",
          "level": 2
        },
        {
          "title": "4.8 Splintering features and models",
          "anchor": "4_8_Splintering_features_and_models",
          "level": 2
        },
        {
          "title": "4.9 Preserved background features",
          "anchor": "4_9_Preserved_background_features",
          "level": 2
        },
        {
          "title": "4.10 Partially preserved background features",
          "anchor": "4_10_Partially_preserved_background_features",
          "level": 2
        },
        {
          "title": "5 The fundamental questions of model refinements and splintering",
          "anchor": "5_The_fundamental_questions_of_model_refinements_and_splintering",
          "level": 1
        },
        {
          "title": "6 Examples and applications",
          "anchor": "6_Examples_and_applications",
          "level": 1
        },
        {
          "title": "6.1 Extending beyond the training distribution",
          "anchor": "6_1_Extending_beyond_the_training_distribution",
          "level": 2
        },
        {
          "title": "6.2 Detecting going out-of-distribution",
          "anchor": "6_2_Detecting_going_out_of_distribution",
          "level": 2
        },
        {
          "title": "6.3 Asking humans and Active IRL",
          "anchor": "6_3_Asking_humans_and_Active_IRL",
          "level": 2
        },
        {
          "title": "6.4 A time for conservatism",
          "anchor": "6_4_A_time_for_conservatism",
          "level": 2
        },
        {
          "title": "6.5 Avoiding ambiguous distant situations",
          "anchor": "6_5_Avoiding_ambiguous_distant_situations",
          "level": 2
        },
        {
          "title": "6.6 Extra variables",
          "anchor": "6_6_Extra_variables",
          "level": 2
        },
        {
          "title": "6.7 Hidden (dis)agreement and interpretability",
          "anchor": "6_7_Hidden__dis_agreement_and_interpretability",
          "level": 2
        },
        {
          "title": "6.8 Wireheading",
          "anchor": "6_8_Wireheading",
          "level": 2
        },
        {
          "title": "6.9 Hypotheticals, and training in virtual environments",
          "anchor": "6_9_Hypotheticals__and_training_in_virtual_environments",
          "level": 2
        },
        {
          "title": "6.10 Defining how to deal with multiple plausible refactorings",
          "anchor": "6_10_Defining_how_to_deal_with_multiple_plausible_refactorings",
          "level": 2
        },
        {
          "title": "6.11 Global, large scale preferences",
          "anchor": "6_11_Global__large_scale_preferences",
          "level": 2
        },
        {
          "title": "6.12 Avoiding side-effects",
          "anchor": "6_12_Avoiding_side_effects",
          "level": 2
        },
        {
          "title": "6.13 Cancer patients",
          "anchor": "6_13_Cancer_patients",
          "level": 2
        },
        {
          "title": "6.14 The genie and the burning mother",
          "anchor": "6_14_The_genie_and_the_burning_mother",
          "level": 2
        },
        {
          "title": "6.15 Splintering moral-relevant categories: honour, gender, and happiness",
          "anchor": "6_15_Splintering_moral_relevant_categories__honour__gender__and_happiness",
          "level": 2
        },
        {
          "title": "6.16 Apprenticeship learning",
          "anchor": "6_16_Apprenticeship_learning",
          "level": 2
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        },
        {
          "anchor": "comments",
          "level": 0,
          "title": "10 comments"
        }
      ],
      "headingsCount": 50
    },
    "contents": {
      "markdown": "# 1. The big problem\n\nIn the last few months, I've become convinced that there is a key meta-issue in AI safety; a problem that seems to come up in all sorts of areas.$\\newcommand{\\E}{\\mathcal{E}}\\newcommand{\\M}{\\mathcal{M}}\\newcommand{\\W}{\\mathcal{W}}\\newcommand{\\F}{\\mathcal{F}}\\newcommand{\\L}{\\mathcal{L}}\\newcommand{\\wF}{{\\widehat{\\F}}}\\newcommand{\\Sp}{\\mathcal{Sp}}$\n\nIt's hard to summarise, but my best phrasing would be:\n\n* Many problems in AI safety seem to be variations of \"this approach seems safe in this imperfect model, but when we generalise the model more, it becomes dangerously underdefined\". Call this **model splintering**.\n* It is intrinsically worth studying how to (safely) transition from one imperfect model to another. This is worth doing, independently of whatever \"perfect\" or \"ideal\" model might be in the background of the imperfect models.\n\nThis sprawling post will be presenting examples of model splintering, arguments for its importance, a formal setting allowing us to talk about it, and some uses we can put this setting to.\n\n\n## 1.1 In the language of traditional ML\n\nIn the language of traditional ML, we could connect all these issues to \"[out-of-distribution](http://www.gatsby.ucl.ac.uk/~balaji/mluq-talk-balaji.pdf)\" behaviour. This is the problems that algorithms encounter when the set they are operating on is drawn from a different distribution than the training set they were trained on.\n\nHumans can often see that the algorithm is out-of-distribution and correct it, because we have a more general distribution in mind than the one the algorithm was trained on.\n\nIn these terms, the issues of this post can be phrased as:\n\n1. When the AI finds itself mildly out-of-distribution, how best can it extend its prior knowledge to the new situation?\n2. What should the AI do if it finds itself strongly out-of-distribution?\n3. What should the AI do if it finds itself strongly out-of-distribution, and humans don't know the correct distribution either?\n\n\n## 1.2 Model splintering examples\n\nLet's build a more general framework. Say that you start with some brilliant idea for AI safety/alignment/effectiveness. This idea is phrased in some (imperfect) model. Then \"model splintering\" happens when you or the AI move to a new (also imperfect) model, such that the brilliant idea is undermined or underdefined.\n\nHere are a few examples:\n\n* You design an AI CEO as a money maximiser. Given typical assumptions about the human world (legal systems, difficulties in one person achieving massive power, human fallibilities), this results in an AI that behaves like a human CEO. But when those assumptions fail, the AI can end up feeding the universe to a money-making process that produces nothing of any value.\n* Eliezer [defined](https://www.lesswrong.com/posts/4FcxgdvdQP45D6Skg/disguised-queries) \"rubes\" as smooth red cubes containing palladium that don't glow in the dark. \"Bleggs\", on the other hand, are furred blue eggs containing vanadium that glow in the dark. To classify these, we only need a model with two features, \"rubes\" and \"bleggs\". Then along comes a furred red egg containing vanadium that doesn't glow in the dark. The previous model doesn't know what to do with it, and if you get a model with more features, it's unclear what to do with this new object.\n* Here are some moral principles from history: honour is important for anyone. Women should be protected. Increasing happiness is important. These moral principles made sense in the world in which they were articulated, where features like \"honour\", \"gender\", and \"happiness\" are relatively clear and unambiguous. But the world changed, and the models splintered. \"Honour\" became hopelessly confused centuries ago. Gender is currently finishing its long splintering (long before we got to today, gender started becoming less useful for classifying people, hence the consequences of gender splintered a long time before gender itself did). Happiness, or at least hedonic happiness, is still well defined, but we can clearly see how this is going to splinter when we talk about worlds of uploads or brain modification.\n* Many transitions in the laws of physics - from the [ideal gas laws](https://en.wikipedia.org/wiki/Ideal_gas_law) to the more advanced [van der Waals equations](https://en.wikipedia.org/wiki/Van_der_Waals_equation), or from Newtonain physics to general relativity to quantum gravity - will cause splintering if preferences were articulated in concepts that don't carry over well.\n\n\n## 1.3 Avoiding perfect models\n\nIn all those cases, there are ways of improving the transition, without needing to go via some idealised, perfect model. We want to define the AI CEO's task in more generality, but we don't need to define this across every possible universe - that is not needed to restrain its behaviour. We need to distinguish any blegg from any rube we are likely to encounter, we don't need to define the platonic essence of \"bleggness\". For future splinterings - when hedonic happiness splinters, when we get a model of quantum gravity, etc... - we want to know what to do then and there, even if there are future splinterings subsequent to those.\n\nAnd I think think that model splintering is best addressed directly, rather than using methods that go via some idealised perfect model. Most approaches seem to go for approximating an ideal: from AIXI's [set of all programs](https://en.wikipedia.org/wiki/AIXI), the [universal prior](https://ordinaryideas.wordpress.com/2016/11/30/what-does-the-universal-prior-actually-look-like/), [KWIK (\"Knowing what it knows\") learning](https://link.springer.com/content/pdf/10.1007/s10994-010-5225-4.pdf) with a full hypothesis class, [Active Inverse Reward Design](https://arxiv.org/pdf/1809.03060.pdf) with its full space of \"true\" reward functions, to Q-learning which assumes any [Markov decisions process](https://en.wikipedia.org/wiki/Markov_decision_process) is possible. Then the practical approaches rely on approximating this ideal.\n\nSchematically, we can see $\\M_\\infty$ as the ideal, $\\M_\\infty^i$ as $\\M_\\infty$ updated with information to time $i$, and $\\M_i$ as an approximation of $\\M_\\infty^i$. Then we tend to focus on how well $\\M_i$ approximates $\\M_\\infty^i$, and on how $\\M_\\infty^i$ changes to $\\M_\\infty^{i+1}$ - rather than on how $\\M_i$ relates to $\\M_{i+1}$; the red arrow here is underanalysed:\n\n![](https://www.dropbox.com/s/0w1sm3ewqy1fd8t/via_worlds.png?raw=1)\n\n\n# 2 Why focus on the transition?\n\nBut why is focusing on the $\\M_i \\to \\M_{i+1}$ transition important?\n\n## 2.1 Humans reason like this\n\nA lot has been written about image recognition programs going \"out-of-distribution\" (encountering situations beyond its training environment) or succumbing to \"adversarial examples\" (examples from one category that have the features of another). Indeed, some people have [shown how to use labelled adversarial examples](https://arxiv.org/pdf/1911.09665.pdf) to improve image recognition.\n\nYou know what this reminds me of? Human moral reasoning. At various points in our lives, we humans seem to have pretty solid moral intuitions about how the world should be. And then, we typically learn more, realise that things don't fit in the categories we were used to (go \"out-of-distribution\") and have to update. Some people push stories at us that exploit some of our emotions in new, more ambiguous circumstances (\"adversarial examples\"). And philosophers use similarly-designed thought experiments to open up and clarify our moral intuitions.\n\nBasically, [we start with strong moral intuitions on under-defined features](https://www.lesswrong.com/posts/pfmFe5fgEn2weJuer/go-west-young-man-preferences-in-imperfect-maps), and when the features splinter, we have to figure out what to do with our previous moral intuitions. A lot of developing moral meta-intuitions, is about learning how to navigate these kinds of transitions; AIs need to be able to do so too.\n\n## 2.2 There are no well-defined overarching moral principles\n\nMoral realists and moral non-realists [agree more than you'd think](https://www.lesswrong.com/posts/FQKjY563bJqDeaEDr/to-first-order-moral-realism-and-moral-anti-realism-are-the). In this situation, we can agree on one thing: there is no well-described system of morality that can be \"simply\" implement in AI.\n\nTo over-simplify, moral realists hope to discover this moral system, moral non-realists hope to construct one. But, currently, it doesn't exist in an implementable form, nor is there any implementable algorithm to discover/construct it. So the whole idea of approximating an ideal is wrong.\n\nAll humans seem to start from a partial list of moral rules of thumb, [rules that they then have to extend to new situations](https://www.lesswrong.com/posts/pfmFe5fgEn2weJuer/go-west-young-man-preferences-in-imperfect-maps). And most humans do seem to have some meta-rules for defining moral improvements, or extensions to new situations.\n\nWe don't know perfection, but we do know improvements and extensions. So methods that deal explicitly with that are useful. Those are things we can build on.\n\n## 2.3 It helps distinguish areas where AIs fail, from areas where humans are uncertain\n\nSometimes the AI goes out-of-distribution, and humans can see the error (no, [flipping the lego block doesn't count as putting it on top of the other](https://deepmind.com/blog/article/Specification-gaming-the-flip-side-of-AI-ingenuity)). There are cases when humans themselves go out-of-distribution (see for example [siren worlds](https://www.lesswrong.com/posts/nFv2buafNc9jSaxAH/siren-worlds-and-the-perils-of-over-optimised-search)).\n\nIt's useful to have methods available for both AIs and humans in these situations, and to distinguish them. \"Genuine human preferences, not expressed in sufficient detail\" is not the same as \"human preferences fundamentally underdefined\".\n\nIn the first case, it needs more human feedback; in the second case, it needs to figure out way of [resolving the ambiguity](https://www.lesswrong.com/posts/CSEdLLEkap2pubjof/research-agenda-v0-9-synthesising-a-human-s-preferences-into#2_Synthesising_the_preference_utility_function), knowing that soliciting feedback is not enough.\n\n## 2.4 We don't need to make the problems harder\n\nSuppose that quantum mechanics is the true underlying physics of the universe, with some added bits to include gravity. If that's true, why would we need a moral theory valid in every possible universe? It would be useful to have that, but would be strictly harder than one valid in the actual universe.\n\nAlso, some problems might be [entirely avoided](https://www.lesswrong.com/posts/PX8BB7Rqw7HedrSJd/by-default-avoid-ambiguous-distant-situations). We don't need to figure out the morality of dealing with a willing slave race - if we never encounter or build one in the first place.\n\nSo a few degrees of \"extend this moral model in a reasonable way\" might be sufficient, without needing to solve the whole problem. Or, at least, without needing to solve the whole problem in advance - a successful [nanny AI](https://wiki.lesswrong.com/wiki/Nanny_AI) might be built on these kinds of extensions.\n\n\n## 2.5 We don't know how deep the rabbit hole goes\n\nIn a sort of converse to the previous point, what if the laws of physics are radically different from what we thought - what if, for example, they allow some forms of time-travel, or have some [narrative features](https://en.wikipedia.org/wiki/Last_Action_Hero), or, more simply, what if the agent moves to an [embedded agency model](https://www.lesswrong.com/posts/i3BTagvt3HbPMx6PN/embedded-agency-full-text-version)? What if [hypercomputation](https://en.wikipedia.org/wiki/Hypercomputation) is possible?\n\nIt's easy to have an idealised version of \"all reality\" that doesn't allow for these possibilities, so the ideal can be too restrictive, rather than too general. But the model splintering methods might still work, since it deals with transitions, not ideals.\n\nNote that, **in retrospect**, we can always put this in a Bayesian framework, once we have a rich enough set of environments and updates rules. But this is misleading: the key issue is the missing feature, and figuring out what to do with the missing feature is the real challenge. The fact that we could have done this in a Bayesian way *if we already knew that feature*, is not relevant here.\n\n\n## 2.6 We often only need to solve partial problems\n\nAssume the blegg and rube classifier is an industrial robot performing a task. If humans filter out any atypical bleggs and rubes before it sees them, then the robot has no need for a full theory of bleggness/rubeness.\n\nBut what it the human filtering is not perfect? Then the classifier still doesn't need a full theory of bleggness/rubeness; it needs methods for dealing with the ambiguities it actually encounters.\n\nSome ideas for AI control - [low impact](https://arxiv.org/abs/1705.10720), [AI-as-service](https://futureoflife.org/wp-content/uploads/2019/02/drexler_thursday_pm.pdf?x59035), [Oracles](https://arxiv.org/abs/1711.05541), ... - may require dealing with some model splintering, some ambiguity, but not the whole amount.\n\n## 2.7 It points out when to be conservative\n\nSome methods, like [quantilizers](https://intelligence.org/files/QuantilizersSaferAlternative.pdf) or the [pessimism approach](https://mkcohen-hosted-files.s3-us-west-1.amazonaws.com/Pessimism_alignmentforum.pdf) rely on the algorithm having a certain degree of conservatism. But, as I've [argued](https://www.lesswrong.com/posts/jRHLCyRKsQv5u2Lph/how-conservative-should-the-partial-maximisers-be), it's not clear to what extent these methods actually are conservative, nor is it easy to calibrate them in a useful way.\n\nModel splintering situations provide excellent points at which to be conservative. Or, for algorithms that need human feedback, but not constantly, these are excellent points to ask for that feedback.\n\n## 2.8 Difficulty in capturing splintering from the idealised perspective\n\nGenerally speaking, idealised methods can't capture model splintering at the point we would want it to. Imagine an [ontological crisis](https://arxiv.org/abs/1105.3821), as we move from classical physics to quantum mechanics.\n\nAIXI can go over the transition fine: it shifts from a Turing machine mimicking classical physics observations, to one mimicking quantum observations. But it doesn't notice anything special about the transition: changing the probability of various Turing machines is what it does with observations in general; there's nothing in its algorithm that shows that something unusual has occurred for this particular shift.\n\n## 2.9 It may help amplification and distillation\n\nThis could be seen as a sub-point of some of the previous two sections, but it deserves to be flagged explicitly, since [iterated amplification and distillation](https://www.alignmentforum.org/posts/PT8vSxsusqWuN7JXp/my-understanding-of-paul-christiano-s-iterated-amplification) is one of the major potential routes to AI safety.\n\nTo quote a line from that summary post:\n\n>5. The proposed AI design is to use a safe but slow way of scaling up an AI’s capabilities, distill this into a faster but slightly weaker AI, which can be scaled up safely again, and to iterate the process until we have a fast and powerful AI.\n\nAt both \"scaling up an AI's capabilities\", and \"distill this into\", we can ask the question: has the problem the AI is working on changed? The distillation step is more of a classical AI safety issue, as we wonder whether the distillation has caused any value drift. But at the scaling up or amplification step, we can ask: since the AIs capabilities have changed, the set of possible environments it operates in has changed as well. Has this caused a splintering where the previously safe goals of the AI have become dangerous.\n\nDetecting and dealing with such a splintering could both be useful tools to add to this method.\n\n\n\n\n## 2.10 Examples of model splintering problems/approaches\n\nAt a meta level, most problems in AI safety seem to be variants of model splintering, including:\n\n* The [hidden complexity of wishes](https://www.lesswrong.com/posts/4ARaTpNX62uaL86j6/the-hidden-complexity-of-wishes).\n* [Ontological crises](https://arxiv.org/abs/1105.3821).\n* [Conservative/prudential](https://www.lesswrong.com/posts/RzAmPDNciirWKdtc7/pessimism-about-unknown-unknowns-inspires-conservatism) behaviour in algorithms (more specifically, when the algorithm should become conservative).\n* How [categories are defined](https://www.lesswrong.com/posts/yA4gF5KrboK2m2Xu7/how-an-algorithm-feels-from-inside).\n* The [Goodhart problems](https://www.lesswrong.com/posts/EbFABnst8LsidYs5Y/goodhart-taxonomy).\n* [Out-of-distribution](http://www.gatsby.ucl.ac.uk/~balaji/mluq-talk-balaji.pdf) behaviour.\n* [Low](https://arxiv.org/abs/1705.10720) [impact](https://arxiv.org/pdf/1902.09725.pdf) and [reduced side-effects](https://arxiv.org/pdf/1806.01186.pdf) approaches.\n* [Underdefined preferences](https://www.lesswrong.com/posts/CSEdLLEkap2pubjof/research-agenda-v0-9-synthesising-a-human-s-preferences-into#2_3_Extending_and_normalising_partial_preferences).\n* [Active inverse reward design](https://arxiv.org/pdf/1809.03060.pdf).\n* [Inductive ambiguity identification](https://intelligence.org/files/AlignmentMachineLearning.pdf).\n* [Wireheading](https://www.lesswrong.com/posts/vXzM5L6njDZSf4Ftk/defining-ai-wireheading).\n* The [whole friendly AI problem](https://en.wikipedia.org/wiki/Friendly_artificial_intelligence#Etymology_and_usage) itself.\n\nAlmost every recent post I've read in AI safety, I've been able to connect back to this central idea. Now, we have to be cautious - [cure-alls cure nothing](https://maximumfun.org/podcasts/sawbones/), after all, so it's not necessarily a positive sign that *everything* seems to fit into this framework.\n\nStill, I think it's worth diving into this, especially as I've come up with a framework that seems promising for actually solving this issue in many cases.\n\nIn a similar concept-space is Abram's [orthodox case against utility functions](https://www.lesswrong.com/posts/A8iGaZ3uHNNGgJeaD/an-orthodox-case-against-utility-functions), where he talks about the [Jeffrey-Bolker axioms](https://plato.stanford.edu/entries/decision-theory/#JefThe), which allows the construction of preferences from events *without needing full worlds at all*.\n\n\n# 3 The virtues of formalisms\n\n\nThis post is dedicated to explicitly modelling the transition to ambiguity, and then showing what we can gain from this explicit meta-modelling. It will do with some formal language (made fully formal in [this post](https://www.lesswrong.com/posts/89qWCy6yi2eeFGsRu/technical-model-splintering-formalism)), and a lot of examples.\n\nJust as Scott argues that [if it's worth doing, it's worth doing with made up statistics](https://slatestarcodex.com/2013/05/02/if-its-worth-doing-its-worth-doing-with-made-up-statistics/), I'd argue that if an idea is worth pursuing, it's worth pursuing with an attempted formalism.\n\nFormalisms are great at illustrating the problems, clarifying ideas, and making us familiar with the intricacies of the overall concept. That's the reason that this post (and the accompanying [technical post](https://www.lesswrong.com/posts/89qWCy6yi2eeFGsRu/model-splintering-formalism)) will attempt to make the formalism reasonably rigorous. I've learnt a lot about this in the process of formalisation.\n\n\n## 3.1 A model, in (almost) all generality\n\nWhat do we mean by a model? Do we mean mathematical [model theory](https://en.wikipedia.org/wiki/Model_theory)? As we talking about causal models, or [causal graphs](https://en.wikipedia.org/wiki/Causal_model)? [AIXI](https://en.wikipedia.org/wiki/AIXI) uses a distribution over possible Turing machines, whereas [Markov Decision Processes](https://en.wikipedia.org/wiki/Markov_decision_process) (MDPs) sees states and actions updating stochastically, independently at each time-step. Unlike the previous two, Newtonian mechanics doesn't use time-steps but continuous times, while general relativity weaves time into the structure of space itself.\n\nAnd what does it mean for a model to make \"predictions\"? AIXI and MDPs make prediction over future observations, and causal graphs are similar. We can also try running them in reverse, \"predicting\" past observations from current ones. Mathematical model theory talks about properties and the existence or non-existence of certain objects. Ideal gas laws make a \"prediction\" of certain properties (eg temperature) given certain others (eg volume, pressure, amount of substance). General relativity establishes that the structure of space-time must obey certain constraints.\n\nIt seems tricky to include all these models under the same meta-model formalism, but it would be good to do so. That's because of the risk of [ontological crises](https://wiki.lesswrong.com/wiki/Ontological_crisis): we want the AI to be able to continue functioning even if the initial model we gave it was incomplete or incorrect.\n\n## 3.2 Meta-model: models, features, environments, probabilities\n\nAll of the models mentioned above share one common characteristic: once you know some facts, you can deduce some other facts (at least probabilistically). A prediction of the next time step, a retrodiction of the past, a deduction of some properties from other, or a constraint on the shape of the universe: all of these say that if we know some things, then this puts constraints on some other things.\n\nSo let's define $\\F$, informally, as the set of *features* of a model. This could be the gas pressure in a room, a set of past observations, the local curvature of space-time, the momentum of a particle, and so on.\n\nSo we can define a prediction as a probability distribution over a set of possible features $F_1$, given a base set of features, $F_2$:\n\n$$Q(F_1 \\mid F_2).$$\n\nDo we need anything else? Yes, we need a set of possible environments for which the model is (somewhat) valid. Newtonian physics fails at extreme energies, speeds, or gravitational fields; we'd like to include this \"domain of validity\" in the model definition. This will be very useful for extending models, or transitioning from one model to another.\n\nYou might be tempted to define a set of \"worlds\" on which the model is valid. But we're trying to avoid that, as the \"worlds\" may not be very useful for understanding the model. Moreover, we don't have special access to the underlying reality; so we never know whether there actually is a Turing machine behind the world or not.\n\nSo define $\\E$, the environment on which the model is valid, *as a set of possible features*. So if we want to talk about Newtonian mechanics, $\\F$ would be a set of Newtonian features (mass, velocity, distance, time, angular momentum, and so on) and $\\E$ would be the set of these values where [relativistic and quantum effects make little difference](https://en.wikipedia.org/wiki/Classical_limit).\n\nSo see a model as\n\n$$\\M=\\{\\F,\\E,Q\\},$$\n\nfor $\\F$ a set of features, $\\E$ a set of environments, and $Q$ a probability distribution. This is such that, for $E_1, E_2 \\subset \\E$, we have the conditional probability:\n\n$$Q(E_1 \\mid E_2).$$\n\nThough $Q$ is defined for $\\E$, we generally want it to be usable from small subsets of the features: so $Q$ should be simple to define from $\\F$. And we'll often define the subsets $E_i$ in similar ways; so $E_1$ might be all environments with a certain angular momentum at time $t=0$, while $E_2$ might be all environments with a certain angular momentum at a later time.\n\nThe full formal definition of these can be found [here](https://www.lesswrong.com/posts/89qWCy6yi2eeFGsRu/technical-model-splintering-formalism). The idea is to have a meta-model of modelling that is sufficiently general to apply to almost all models, but not one that relies on some ideal or perfect formalism.\n\n## 3.3 Bayesian models within this meta-model\n\nIt's very easy to include Bayesian models within this formalism. If we have a Bayesian model that includes a set $W$ of worlds with prior $P$, then we merely have to define a set of features $\\F$ that is sufficient to distinguish all worlds in $W$: each world is uniquely defined by its feature values[^fea]. Then we can define $\\E$ as $W$, and $P$ on $W$ becomes $Q$ on $\\E$; the definitions of terms like $Q(E_1\\mid E_2)$ is just $P(E_1\\cap E_2)P(E_1)/P(E_2)$, per Bayes' rules (unless $P(E_2)=0$, in which case we set that to $0$).\n\n[^fea]: Now, sometimes worlds $w_1, w_2 \\in W$ may be indistinguishable for any feature set. But in that case, they can't be distinguished by any observations, either, so their relative probabilities won't change: as long as it's defined, $P(w_1|o)/P(w_2|o)$ is constant for all observations $o$. So we can replace  $w_1$ and $w_2$ with $\\{w_1,w_2\\}$, of prior probability $P(\\{w_1,w_2\\})=P(w_1)+P(w_2)$. Doing this for all indistinguishable worlds (which form an [equivalence class](https://en.wikipedia.org/wiki/Equivalence_class)) gives $W'$, a set of distinguishable worlds, with a well defined $P$ on it.\n\n# 4 Model refinement and splinterings\n\nThis section will look at what we can do with the previous meta-model, looking at refinement (how models can improve) and splintering (how improvements to the model can make some well-defined concepts less well-defined).\n\n## 4.1 Model refinement\n\nInformally, $\\M^*=\\{\\F^*,\\E^*,Q^*\\}$ is a *refinement* of model $\\M=\\{\\F,\\E,Q\\}$ if it's at least as expressive as $\\M$ (it covers the same environments) and is better according to some criteria (simpler, or more accurate in practice, or some other measurement).\n\nAt the technical level, we have a map $q$ from a subset $\\E^*_0$ of $\\E^*$, that is surjective onto $\\E$. This covers the \"at least as expressive\" part: every environment in $\\E$ exists as (possibly multiple) environments in $\\E^*$.\n\nThen note that using $q^{-1}$ as a map from subsets of $\\E$ to subsets of $\\E^*_0$, we can define $Q^*_0$ on $\\E$ via:\n\n$$Q^*_0(E_1 \\mid E_2) = Q^*(q^{-1}(E_1) \\mid q^{-1}(E_2)).$$\n\nThen this is a model refinement if $Q^*_0$ is 'at least as good as' $Q$ on $\\E$, according to our criteria[^abs].\n\n[^abs]: It's useful to contrast a refinement with the \"abstraction\" defined in [this sequence](https://www.lesswrong.com/s/ehnG4mseKF6xALmQy). An abstraction throws away irrelevant information, so is not generally a refinement. Sometimes they are exact opposites, as the ideal gas law is an abstraction of the movement of all the gas particles, while the opposite would be a refinement.\n\n    But they are exact opposites either. Starting with the neurons of the brain, you might abstract them to \"emotional states of mind\", while a refinement could also add \"emotional states of mind\" as new features (while also keeping the old features). A splintering is more the opposite of an abstraction, as it signals that the old abstraction features are not sufficient.\n\n    It would be interesting to explore some of the concepts in this post with a mixture of refinements (to get the features we need) and abstractions (to simplify the models and get rid of the features we don't need), but that is beyond the scope of this current, already over-long, post.\n\n\n## 4.2 Example of model refinement: gas laws\n\n[This post](https://www.lesswrong.com/posts/89qWCy6yi2eeFGsRu/technical-model-splintering-formalism) presents some subclasses of model refinement, including $Q$-improvements (same features, same environments, just a better $Q$), or adding new features to a basic model, called \"non-independent feature extension\" (eg adding classical electromagnetism to Newtonian mechanics).\n\nHere's a specific gas law illustration. Let $\\M=\\{\\F,\\E,Q\\}$ be a model of [an ideal gas](https://en.wikipedia.org/wiki/Ideal_gas_law), in some set of rooms and tubes. The $\\F$ consists of pressure, volume, temperature, and amount of substance, and $Q$ is the ideal gas laws. The $\\E$ is the [standard conditions for temperature and pressure](https://en.wikipedia.org/wiki/Standard_conditions_for_temperature_and_pressure), where the ideal gas law applies. There are multiple different types of gases in the world, but they all roughly obey the same laws.\n\nThen compare with model $\\M^*=\\{\\F^*,\\E^*,Q^*\\}$. The $\\F^*$ has all the features of $\\F$, but also includes the volume that is occupied by one mole of the molecules of the given substance. This allows $Q^*$ to express the more complicated [van der Waals equations](https://en.wikipedia.org/wiki/Van_der_Waals_equation), which are different for different types of gases. The $\\E^*$ can now track situations where there are gases with different molar volumes, which include situations where the van der Waals equations differ significantly from the ideal gas laws.\n\nIn this case $\\E^*_0 \\subset \\E^*$, since we now distinguish environments that we previously considered identical (environments with same features except for having molar volumes). The $q$ is just projecting down by forgetting the molar volume. Then since $Q^*_0=Q^*$ (van der Waals equations averaged over the distribution of molar volumes) is at least as accurate as $Q$ (ideal gas law), this is a refinement.\n\n## 4.3 Example of model refinement: rubes and bleegs\n\nLet's reuse Eliezer's [example](https://www.lesswrong.com/posts/4FcxgdvdQP45D6Skg/disguised-queries) of rubes (\"red cubes\") and bleggs (\"blue eggs\").\n\nBleggs are blue eggs that glow in the dark, have a furred surface, and are filled with vanadium. Rubes, in contrast, are red cubes that don't glow in the dark, have a smooth surface, and are filled with palladium:\n\n![](https://www.dropbox.com/s/4ws12vrevymct1m/blegg_rube_original.png?raw=1)\n\nDefine $\\M$ by having $\\F=\\{\\textrm{red}, \\textrm{smooth}\\}$, $\\E$ is the set of all bleggs and rubes in some situation, and $Q$ is relatively trivial: it predicts that an object is red/blue if and only if is smooth/furred.\n\nDefine $\\M^1$ as a refinement of $\\M$, by expanding $\\F$ to $\\F^1=\\{\\textrm{red}, \\textrm{smooth}, \\textrm{cube},\\textrm{dark}\\}$. The projection $q:\\E^*\\to \\E$ is given by forgetting about those two last features. The $Q^1$ is more detailed, as it now connects red-smooth-cube-dark together, and similarly for blue-furred-egg-glows.\n\nNote that $\\E^1$ is larger than $\\E$, because it includes, e.g., environments where the cube objects are blue. However, all these extra environments have probability zero.\n\n## 4.4 Reward function refactoring\n\nLet $R$ be a reward function on $\\M$ (by which we mean that $R$ is define on $\\F$, the set of features in $\\M$), and $\\M^*$ a refinement of $\\M$.\n\nA *refactoring* of $R$ for $\\M^*$ is a reward function $R^*$ on the features $\\F^*$ such that for any $e^*\\in\\E^*_0$, $R^*(e^*)=R(q(e^*))$.\n\nFor example, let $\\M$ and $\\M^1$ be from the rube/blegg models in the previous section. Let $R_{\\textrm{red}}$ on $\\M$ simply count the number of rubes - or, more precisely, counts the number of objects to which the feature \"red\" applies.\n\nLet $R^1_{\\textrm{red}}$ be the reward function that counts the number of objects in $\\M^1$ to which \"red\" applies. It's clearly a refactoring of $R_{\\textrm{red}}$.\n\nBut so is $R^1_{\\textrm{smooth}}$, the reward function that counts the number of objects in $\\M^1$ to which \"smooth\" applies. In fact, the following is a refactoring of $R_{\\textrm{red}}$, for all $\\alpha+\\beta+\\gamma+\\delta=1$:\n\n$$\\alpha R^1_{\\textrm{red}} + \\beta R^1_{\\textrm{smooth}} + \\gamma R^1_{\\textrm{cube}} + \\delta R^1_{\\textrm{dark}}.$$\n\nThere are also some non-linear combinations of these features that refactor $R$, and many other variants (like the strange combinations that generate concepts like [grue and bleen](https://www.lesswrong.com/posts/LAvw9fTQnz8Wx6m25/grue-bleen-and-natural-categories)).\n\n\n\n## 4.5 Reward function splintering\n\nModel splintering, in the informal sense, is what happens when we pass to a new models in a way that the old features (or a reward function defined by the old features) no longer apply. It is similar to the [web of connotations](https://www.lesswrong.com/posts/ix3KdfJxjo9GQFkCo/web-of-connotations-bleggs-rubes-thermostats-and-beliefs) breaking down, an agent going [out of distribution](https://www.lesswrong.com/posts/NdJtfujX4sE6xLCsb/if-i-were-a-well-intentioned-ai-iii-extremal-goodhart), or the [definitions of Rube and Blegg falling apart](https://www.lesswrong.com/posts/yA4gF5KrboK2m2Xu7/how-an-algorithm-feels-from-inside).\n\n* Preliminary definition: If $\\M^*$ is a refinement of $\\M$ and $R$ a reward function on $\\M$, then $\\M^*$ *splinters* $R$ if there are multiple refactorings of $R$ on $\\M^*$ that disagree on elements of $\\E^*$ of non-zero probability.\n\nSo, note that in the rube/blegg example, $\\M^1$ is **not** a splintering of $R_{\\textrm{red}}$: all the refactorings are the same on all bleggs and rubes - hence on all elements of $\\E^1$ of non-zero probability.\n\nWe can even generalise this a bit. Let's assume that \"red\" and \"blue\" are not totally uniform; there exists some rubes that are \"redish-purple\", while some bleggs are \"blueish-purple\". Then let $\\M^2$ be like $\\M^1$, except the colour feature can have four values: \"red\", \"redish-purple\", \"blueish-purple\", and \"blue\".\n\nThen, as long as rubes (defined, in this instance, by being smooth-dark-cubes) are either \"red\" or \"redish-purple\", and the bleggs are \"blue\", or \"blueish-purple\", then all refactorings of $R_{\\textrm{red}}$ to $\\M^2$ agree - because, on the test environment, $R_{\\textrm{red}}$ on $\\F$ perfectly matches up with $R^2_{\\textrm{red}} + R^2_{\\textrm{redish-purple}}$ on $\\F^2$.\n\nSo adding more features does not always cause splintering.\n\n\n## 4.6 Reward function splintering: \"natural\" refactorings\n\nThe preliminary definition runs into trouble when we add more objects to the environments. Define $\\M^3$ as being the same as $\\M^2$, except that $\\E^3$ contains one extra object, $o_+$; apart from that, the environments typically have a billion rubes and a trillion bleggs.\n\nSuppose $o_+$ is a \"furred-rube\", i.e. a red-furred-dark-cube. Then $R^3_{\\textrm{red}}$ and $R^3_{\\textrm{smooth}}$ are two different refactorings of $R_{\\textrm{red}}$, that obviously disagree on any environment that contains $o_+$. Even if the probability of $o_+$ is tiny (but non-zero), then $\\M^3$ splinters $R$.\n\nBut things are worse than that. Suppose that $o_+$ is fully a rube: red-smooth-cube-dark, and even contains palladium. Define $(R^3_{\\textrm{red}})'$ as being counting the number of red objects, except for $o_+$ specifically (again, this is similar to the [grue and bleen arguments against induction](https://www.lesswrong.com/posts/LAvw9fTQnz8Wx6m25/grue-bleen-and-natural-categories)).\n\nThen both $(R^3_{\\textrm{red}})'$ and $R^3_{\\textrm{red}}$ are refactorings of $R_{\\textrm{red}}$, so $\\M^3$ still splinters $R_{\\textrm{red}}$, even when we add another exact copy of the elements in the training set. Or even if we keep the training set for a few extra seconds, or add any change to the world.\n\nSo, for any $\\M^*$ a refinement of $\\M$, and $R$ a reward function on $\\E$, let's define \"natural refactorings\" of $R$:\n\n* The reward function $R^*$ is a natural refactoring of $R$ if it's a reward function on $\\M^*$ with:\n1. $R^* \\approx R\\circ q$ on $\\E^*_0$, and\n2. $R^*$ can be defined simply from $\\F^*$ and $R$,\n3. the $\\F^*$ themselves are simply defined.\n\nThis leads to a full definition of splintering:\n\n* Full definition: If $\\M^*$ is a refinement of $\\M$ and $R$ a reward function on $\\M$, then $\\M^*$ *splinters* $R$ if 1) there are no natural refactoring of $R$ on $\\M^*$, or 2) there are multiple natural refactorings $R^*$ and $R^{*\\prime}$ of $R$ on $\\M^*$, such that $R^* \\not\\approx R^{*\\prime}$.\n\nNotice the whole host of caveats and weaselly terms here; $R^* \\approx R\\circ q$, \"simply\" (used twice), and $R^* \\not\\approx R^{*\\prime}$. Simply might mean [algorithmic simplicity](https://en.wikipedia.org/wiki/Kolmogorov_complexity), but $\\approx$ and $\\not\\approx$ are measures of how much \"error\" we are willing to accept in these refactorings. Given that, we probably want to replace $\\approx$ and $\\not\\approx$ with some *measure* of non-equality, so we can talk about the \"degree of naturalness\" or the \"degree of splintering\" of some refinement and reward function.\n\nNote also that:\n\n* **Different choices of refinements can result in different natural refactorings.**\n\nAn easy example: it makes a big difference whether a new feature is \"temperature\", or \"divergence from standard temperatures\".\n\n\n## 4.7 Splintering training rewards\n\nThe concept of \"reward refactoring\" is transitive, but the concept of \"natural reward refactoring\" need not be.\n\nFor example, let $\\E_t$ be a training environment where red/blue $\\iff$ cube/egg, and $\\E_g$ be a general environment where red/blue is independent of cube/egg. Let $\\F^1$ be a feature set with only red/blue, and $\\F^2$ a feature set with red/blue and cube/egg.\n\nThen define $\\M_t^1$ as using $\\F^1$ in the training environment, $\\M_g^2$ as using $\\F^2$ in the general environment; $\\M_g^1$ and $\\M_t^2$ are defined similarly.\n\nFor these models, $\\M_g^1$ and $\\M_t^2$ are both refinements of $\\M_t^1$, while $\\M^2_g$ is a refinement of all three other models. Define $R_t^1$ as the \"count red objects\" reward on $\\M_t^1$. This has a natural refactoring to $R_g^1$ on $\\M_g^1$, which counts red objects in the general environment.\n\nAnd $R_g^1$ has a natural refactoring to $R_g^2$ on $\\M_g^2$, which still just counts the red objects in the general environment.\n\nBut there is no natural refactoring from $R_t^1$ directly to $\\M_g^2$. That's because, from $\\F^2$'s perspective, $R_t^1$ on $\\M_t^1$ might be counting red objects, or might be counting cubes. This is not true for $R_g^1$ on $\\M_g^1$, which is clearly only counting red objects.\n\nThus when a reward function come from a training environment, we'd want our AI to look for splinterings **directly from a model of the training environment**, rather than from previous natural refactorings.\n\n\n## 4.8 Splintering features and models\n\nWe can also talk about splintering features and models themselves. For $\\M=\\{\\F,\\E,Q\\}$, the easiest way is to define a reward function $R_{F,s_F}$ as being the indicator function for feature $F\\in\\F$ being in the set $S_F$.\n\nThen a refinement $\\M^*$ splinters the feature $F$ if it splinters some $R_{F,S_F}$.\n\nThe refinement $\\M^*$ splinters the model $\\M$ if it splinters at least one of its features.\n\nFor example, if $\\M$ is Newtonian mechanics, including \"total rest mass\" and $\\M^*$ is special relativity, then $\\M^*$ will splinter \"total rest mass\". Other examples of feature splintering will be presented in the rest of this post.\n\n\n## 4.9 Preserved background features\n\nA reward function developed in some training environment will ignore any feature that is always present or always absent in that environment. This allows very weird situations to come up, such as training an AI to distinguish happy humans from sad humans, and it ending up replacing humans with humanoid robots (after all, both happy and sad humans were equally non-robotic, so there's no reason not to do this).\n\nLet's try and do better than that. Assume we have a model $\\M=\\{\\F,\\E,Q\\}$, with a reward function $R_\\tau$ defined on $\\E$ ($R_\\tau$ and $\\E$ can be seen as the training data).\n\nThen the feature-preserving reward function $R^\\M$, is a function that constrains the environments to have similar feature distributions as $\\E$ and $Q$. There are many ways this could be defined; here's one.\n\nFor an element $e \\in \\E$, just define\n\n$$R^\\M(e)=\\log(Q(e)).$$\n\nObviously, this can be improved; we might want to coarse-grain $\\F$, grouping together similar worlds, and possibly bounding this below to avoid singularities.\n\nThen we can use this to get the feature-preserving version of $R_\\tau$, which we can define as\n\n$$R_\\tau^\\M = (\\max_{R_\\tau} - R_\\tau) \\cdot R^\\M,$$\n\nfor $\\max_{R_\\tau}$ the maximal value of $R_\\tau$ on $\\E$. Other options can work as well, such as $R_\\tau + \\alpha R^\\M_\\tau$ for some constant $\\alpha>0$.\n\nThen we can ask an AI to use $R_{\\tau}^\\M$ as its reward function, refactoring that, rather than $R_{\\tau}$.\n\n* A way of looking at it: a natural refactoring of a reward function $R_\\tau$ will preserve all the implicit features that correlate with $R_\\tau$. But $R_\\tau^\\M$ will also preserve all the implicit features that stay constant when $R_\\tau$ was defined. So if $R_\\tau$ measures human happiness vs human unhappiness, a natural refactoring of it will preserves things like \"having higher dopamine in their brain\". But a natural refactoring of $R_\\tau^\\M$ will also preserve things like \"having a brain\".\n\n## 4.10 Partially preserved background features\n\nThe $R_{\\tau}^\\M$ is almost certainly too restrictive to be of use. For example, if time is a feature, then this will fall apart when the AI has to do something after the training period. If all the humans in a training set share certain features, humans without those features will be penalised.\n\nThere are at least two things we can do to improve this. The first is to include more positive and negative examples in the training set; for example, if we include humans and robots in our training set - as positive and negative examples, respectively - then this difference will show up in $R_{\\tau}$ directly, so we won't need to use $R^\\M_\\tau$ too much.\n\nAnother approach would be to explicitly allow certain features to range beyond their typical values in $\\M$, or allow highly correlated variables explicitly to decorrelate.\n\nFor example, though training during a time period $t$ to $t'$, we could explicitly allow time to range beyond these values, without penalty. Similarly, if a medical AI was trained on examples of typical healthy humans, we could decorrelate functioning digestion from brain activity, and get the AI to focus on the second[^correlated].\n\n[^correlated]: Specifically, we'd point - via labelled examples - at a clusters of features that correlate with functioning digestion, and another cluster of features that correlate with brain activity, and allow those two clusters to decorrelate with each other.\n\nThis has to be done with some care, as adding more degrees of freedom adds more ways for errors to happen. I'm aiming to look further at this issue in later posts.\n\n\n\n# 5 The fundamental questions of model refinements and splintering\n\nWe can now rephrase the out-of-distribution issues of [section 1.1](https://www.lesswrong.com/posts/k54rgSg7GcjtXnMHX/model-splintering-moving-from-one-imperfect-model-to-another-1#1_1_In_the_language_of_traditional_ML) in terms of the new formalism:\n\n1. When the AI refines its model, what would count as a natural refactoring of its reward function?\n2. If the refinements splinter its reward function, what should the AI do?\n3. If the refinements splinter its reward function, and also splinters the human's reward function, what should the AI do?\n\n\n# 6 Examples and applications\n\nThe rest of this post is applying this basic framework, and its basic insights, to various common AI safety problems and analyses. This section is not particularly structured, and will range widely (and wildly) across a variety of issues.\n\n## 6.1 Extending beyond the training distribution\n\nLet's go back to the blegg and rube examples. A human supervises an AI in a training environment, labelling all the rubes and bleggs for it.\n\nThe human is using a very simple model, $\\M_H = \\{\\F_H,\\E_t,Q\\},$ with the only feature being the colour of the object, and $\\E_t$ being the training environment.\n\nMeanwhile the AI, having more observational abilities and [no filter as to what can be ignored](https://en.wikipedia.org/wiki/Attention), notices their colour, their shape, their luminance, and their texture. It doesn't know $\\M_H$, but is using model $\\M^1_{AI}=\\{\\F^1,\\E^1_t,Q^1\\}$, where $\\F^1_{AI}$ covers those four features (note that $\\M^1_{AI}$ is a refinement of $\\M_H$, but that isn't relevant here).\n\n![](https://www.dropbox.com/s/y0nf11719ix7bty/blegg_rube.png?raw=1)\n\nSuppose that the AI is trained to be rube-classifier (and hence a blegg classifier by default). Let $R_{F}$ be the reward function that counts the number of objects, with feature $F$, that the AI has classified as rubes. Then the AI could learn many different reward function in the training environment; here's one:\n\n$$R^1 = R^1_{\\textrm{cube}} + 0.5 R^1_{\\textrm{smooth}} + 0.5 R^1_{\\textrm{dark}} -  R^1_{\\textrm{red}}.$$\n\nNote that, even though this gets the colour reward completely wrong, this reward matches up with the human's assessment on the training environment.\n\nNow the AI moves to the larger testing environment $\\E^2$, and refines its model minimally to $\\M^2_{AI}=\\{\\F^1,\\E^2,Q^1\\}$ (extending $R^1$ to $R^2$ in the obvious way).\n\nIn $\\E^2$, the AI sometimes encounters objects that it can only see through their colour. Will this be a problem, since the colour component of $R^2$ is pointing in the wrong direction?\n\nNo. It still has $Q^1$, and can deduce that a red object must be cube-smooth-dark, so $R^2$ will continue treating this as a rube[^equ].\n\n[^equ]: It is no coincidence that, if $R$ and $R'$ are rewards on $\\M$, that are identical on $\\E$, and if $R^*$ is a refactoring of $R$, then $R^*$ is also a refactoring of $R'$.\n\n## 6.2 Detecting going out-of-distribution\n\nNow imagine the AI learns about the content of the rubes and bleggs, and so refines to a new model that includes vanadium/palladium as a feature in $\\M_{AI}^3$.\n\nFurthermore, in the training environment, all rubes have palladium and all bleggs have vanadium in them. So, for $\\M_{AI}^3$ a refinement of $\\M_{AI}^1$, $q^{-1}(\\E_{AI}^1) \\subset \\E_{AI}^3$ has only palladium-rubes and vanadium-bleggs. But in $\\E_{AI}^3$, the full environment, there are rather a lot of rubes with vanadium and bleggs with palladium.\n\nSo, similarly to [section 4.7](https://www.lesswrong.com/posts/k54rgSg7GcjtXnMHX/model-splintering-moving-from-one-imperfect-model-to-another-1#4_7_Splintering_training_rewards), there is no natural refactoring of the rube/blegg reward in $\\M^1_{AI}$, to $M^3_{AI}$. That's because $\\F^3_{AI}$, the feature set of $M^3_{AI}$, includes vanadium/palladium which co-vary with the other rube/blegg features on the training environment (q^{-1}(\\E_{AI}^1)), but not on the full environment of $\\E_{AI}^3$.\n\nSo looking for reward splintering from the training environment is a way of detecting going out-of-distribution - even on features that were not initially detected in the training distribution, by either the human nor the AI.\n\n## 6.3 Asking humans and Active IRL\n\nSome of the most promising AI safety methods today rely on getting human feedback[^cav]. Since human feedback is expensive, as in it's slow and hard to get compared with almost all other aspects of algorithms, people want to [get this feedback in the most efficient ways possible](https://ai-alignment.com/efficient-feedback-a347748b1557).\n\n[^cav]: Though note there are some problems with this approach, both [in theory](https://arxiv.org/abs/1712.05812) and [in practice](https://arxiv.org/abs/2004.13654).\n\nA good way of doing this would be to ask for feedback when the AI's current reward function splinters, and multiple options are possible.\n\nA more rigorous analysis would look at the value of information, expected future splinterings, and so on. This is what they do in [Active Inverse Reinforcement Learning](https://arxiv.org/pdf/1809.03060.pdf); the main difference is that AIRL emphasises an unknown reward function with humans providing information, while this approach sees it more as an known reward function over uncertain features (or over features that may splinter in general environments).\n\n## 6.4 A time for conservatism\n\nI [argued](https://www.lesswrong.com/posts/jRHLCyRKsQv5u2Lph/how-conservative-should-the-partial-maximisers-be) that many \"conservative\" AI optimising approaches, such as [quantilizers](https://intelligence.org/files/QuantilizersSaferAlternative.pdf) and [pessimistic AIs](https://www.lesswrong.com/posts/RzAmPDNciirWKdtc7/pessimism-about-unknown-unknowns-inspires-conservatism), don't have a good measure of when to become more conservative; their parameters $q$ and $\\beta$ don't encode useful guidelines for the right degree of conservatism.\n\nIn this framework, the alternative is obvious: AIs should become conservative when their reward functions splinter (meaning that the reward function compatible with the previous environment has multiple natural refactorings), and very conservative when they splinter a lot.\n\nThis design is very similar to [Inverse Reward Design](https://arxiv.org/pdf/1711.02827.pdf). In that situation, the reward signal in the training environment is taken as *information* about the \"true\" reward function. Basically they take all reward functions that could have given the specific reward signals, and assume the \"true\" reward function is one of them. In that paper, they advocate extreme conservatism at that point, by optimising the minimum of all possible reward functions.\n\nThe idea here is almost the same, though with more emphasis on \"having a true reward defined on uncertain features\". Having multiple contradictory reward functions compatible with the information, in the general environment, is equivalent with having a lot of splintering of the training reward function.\n\n\n## 6.5 Avoiding ambiguous distant situations\n\nThe post \"[By default, avoid ambiguous distant situations](https://www.lesswrong.com/posts/PX8BB7Rqw7HedrSJd/by-default-avoid-ambiguous-distant-situations)\" can be rephrased as: let $\\M$ be a model in which we have a clear reward function $R$, and let $\\M^2$ be a refinement of this to general situations. We expect that this refinement splinters $R$. Let $\\M^1$ be like $M^2$, except with $\\E^1$ smaller than $\\E^2$, defined such that:\n\n1. An AI could be expected to be able to constrain the world to be in $\\E^1$, with high probability,\n2. The $\\M^1$ is not a splintering of $R$.\n\nThen that post can be summarised as:\n\n* The AI should constrain the world to be in $\\E^1$ and then maximise the natural refactoring of $R$ in $\\M^1$.\n\n\n## 6.6 Extra variables\n\nStuart Russell [writes](https://www.edge.org/conversation/the-myth-of-ai#26015):\n>A system that is optimizing a function of $n$ variables, where the objective depends on a subset of size $k<n$, will often set the remaining unconstrained variables to extreme values; if one of those unconstrained variables is actually something we care about, the solution found may be highly undesirable.\n\nThe approach in [sections 4.9](https://www.lesswrong.com/posts/k54rgSg7GcjtXnMHX/model-splintering-moving-from-one-imperfect-model-to-another-1#4_9_Preserved_background_features) and [4.10](https://www.lesswrong.com/posts/k54rgSg7GcjtXnMHX/model-splintering-moving-from-one-imperfect-model-to-another-1#4_10_Partially_preserved_background_features) explicitly deals with this.\n\n\n## 6.7 Hidden (dis)agreement and interpretability\n\nNow consider two agents doing a rube/blegg classifications task in the training environment; each agent only models two of the features:\n\n![](https://www.dropbox.com/s/dgayahtp8dd8b3o/blegg_rube_disagree.png?raw=1)\n\nDespite not having a single feature in common, both agents will agree on what bleggs and rubes are, in the training environment. And when refining to a fuller model that includes all four (or five) of the key features, both agents will agree as to whether a natural refactoring is possible or not.\n\nThis can be used to help define the limits of [interpretability](https://distill.pub/2018/building-blocks/). The AI can use its own model, and [its own designed features](https://homes.cs.washington.edu/~zoran/firl.pdf), to define the categories and rewards in the training environment. These need not be human-parsable, but we can attempt to interpret them in human terms. And then we can give this interpretation to the AI, as a list of positive and negative examples of our interpretation.\n\nIf we do this well, the AI's own features and our interpretation will match up in the training environment. But as we move to more general environments, these may diverge. Then the AI will flag a \"failure of interpretation\" when its refactoring diverges from a refactoring of our interpretation.\n\nFor example, if we think the AI detects pandas by looking for white hair on the body, and black hair on the arms, we can flag lots of examples of pandas and that hair pattern (and non-pandas and [unusual hair patterns](https://www.dropbox.com/s/j5ofk4ro39a8vh7/multi_panda.jpg?dl=0). We don't use these examples for training the AI, just to confirm that, in the training environment, there is a match between \"AI-thinks-they-are-pandas\" and \"white-hair-on-arms-black-hair-on-bodies\".\n\nBut, [in an adversarial example](https://openai.com/blog/adversarial-example-research/), the AI could detect that, while it is detecting gibbons, this no longer matches up with our interpretaion. A splintering of interpretations, if you want.\n\n\n## 6.8 Wireheading\n\nThe approach can also be used to detect [wireheading](https://www.lesswrong.com/posts/vXzM5L6njDZSf4Ftk/defining-ai-wireheading). Imagine that the AI has various detectors that allow it to label what the features of the bleggs and rubes are. It models the world with ten features: $5$ features representing the \"real world\" versions of the features, and $5$ representing the \"this signal comes from my detector\" versions.\n\nThis gives a total of $10$ features, the $5$ features \"in the real world\" and the $5$ \"AI-labelled\" versions of these:\n\n![](https://www.dropbox.com/s/dccsmzm9ai8s49r/blegg_rube_labels.png?raw=1)\n\nIn the training environment, there was full overlap between these $10$ features, so the AI might learn the incorrect \"maximise my labels/detector signal\" reward.\n\nHowever, when it refines its model to all $10$ features *and* environments where labels and underlying reality diverge, it will realise that this splinters the reward, and thus detect a possible wireheading. It could then ask for more information, or have an automated \"don't wirehead\" approach.\n\n## 6.9 Hypotheticals, and training in virtual environments\n\n\nTo get around the slowness of the real world, some approaches [train AIs in virtual environments](http://openaccess.thecvf.com/content_CVPR_2019/papers/James_Sim-To-Real_via_Sim-To-Sim_Data-Efficient_Robotic_Grasping_via_Randomized-To-Canonical_Adaptation_Networks_CVPR_2019_paper.pdf). The problem is to pass that learning from the virtual environment to the real one.\n\nSome have suggested making the virtual environment sufficiently detailed that the AI can't tell the difference between it and the real world. But, a) this involves fooling the AI, an approach I'm always wary of, and b) it's unnecessary.\n\nWithin the meta-formalism of this post, we could train the AI in a virtual environment which it models by $\\M$, and let it construct a model $\\M'$ of the real-world. We would then motivate the AI to find the \"closest match\" between $\\M$ and $\\M'$, in terms of features and how they connect and vary. This is similar to how we can train pilots in flight simulators; the pilots are never under any illusion as to whether this is the real world or not, and even crude simulators can allow them to build certain skills[^skills].\n\n[^skills]: Some more \"body instincts\" skills require more realistic environments, but some skills and procedures can perfectly well be trained in minimal simulators.\n\nThis can also be used to allow the AI to deduce information from hypotheticals and thought experiments. If we show the AI an episode of a TV series showing people behaving morally (or immorally), then the episode need not be believable or plausible, if we can roughly point to the features in the episode that we want to emphasise, and roughly how these relate to real-world features.\n\n## 6.10 Defining how to deal with multiple plausible refactorings\n\nThe approach for synthesising human preferences, [defined here](https://www.lesswrong.com/posts/CSEdLLEkap2pubjof/research-agenda-v0-9-synthesising-a-human-s-preferences-into#2_4_Synthesising_the_preference_function__first_step), can be rephrased as:\n\n* \"Given that we expect multiple natural refactorings of human preferences, and given that we expect some of them to go [disastrously wrong](https://www.lesswrong.com/posts/nFv2buafNc9jSaxAH/siren-worlds-and-the-perils-of-over-optimised-search), here is one way of resolving the splintering that we expect to be better than most.\"\n\nThis is just one way of doing this, but it does show that \"automating what AIs do with multiple refactorings\" might not be impossible. The following subsection has some ideas with how to deal with that.\n\n## 6.11 Global, large scale preferences\n\nIn an [old post](https://www.lesswrong.com/posts/fKmQEgKvyjcBfpA9G/emergency-learning), I talked about the concept of \"emergency learning\", which was basically, \"lots of examples, and all the stuff we know and suspect about how AIs can go wrong, shove it all in, and hope for the best\". The \"shove it all in\" was a bit more structured than that, defining large scale preferences (like \"avoid siren worlds\" and \"don't over-optimise\") as constraints to be added to the learning process.\n\nIt seems we can do better than that here. Using examples and hypotheticals, it seems we could construct ideas like \"avoid slavery\", \"avoid siren worlds\", or \"don't over-optimise\" as rewards or positive/negative examples certain simple training environments, so that the AI \"gets an idea of what we want\".\n\nWe can then label these ideas as \"global preferences\". The idea is that they start as loose requirements (we have much more granular human-scale preferences than just \"avoid slavery\", for example), but, the more the world diverges from the training environment, the stricter they are to be interpreted, with the AI required to respect some [softmin](https://www.lesswrong.com/posts/urZzJPwHtjewdKKHc/using-expected-utility-for-good-hart) of all natural refactorings of these features.\n\nIn a sense, we'd be saying \"prevent slavery; these are the features of slavery, and in weird worlds, be especially wary of these features\".\n\n\n## 6.12 Avoiding side-effects\n\nKrakovna et. al. presented a [paper on avoiding side-effects](https://deepmind.com/research/publications/measuring-and-avoiding-side-effects-using-relative-reachability) from AI. The idea is to have an AI maximising some reward function, while reducing side effects. So the AI would not smash vases or let them break, nor would it prevent humans from eating sushi.\n\nIn this environment, we want the AI to avoid knocking the sushi off the belt as it moves:\n\n![](https://www.dropbox.com/s/zh4uxfmcygdg8rg/sushi.png?raw=1)\n\nHere, in contrast, we'd want the AI to remove the vase from the belt before it smashes:\n\n![](https://www.dropbox.com/s/rplfzxdbcyx1knj/vase.png?raw=1)\n\nI pointed out [some issues with the whole approach](https://www.lesswrong.com/s/iRwYCpcAXuFD24tHh/p/mdQEraEZQLg7jtozn). Those issues were phrased in terms of sub-agents, but my real intuition is that syntactic methods are not sufficient to control side effects. In other words, the AI can't learn to do the right thing with sushis and vases, unless it has some idea of what these objects mean to us; we prefer sushis to be eaten and vases to not be smashed.\n\nThis can be learnt if the AI has a enough training examples, learning that eating sushi is a general feature of the environments it operates in, while vases being smashed is not. I'll return to this idea in a later post.\n\n## 6.13 Cancer patients\n\nThe ideas of this post were present in implicit form in the idea of [training an AI to cure cancer patients](https://www.lesswrong.com/posts/NdJtfujX4sE6xLCsb/if-i-were-a-well-intentioned-ai-iii-extremal-goodhart).\n\nUsing examples of successfully treated cancer patients, we noted they all shared some positive features (recuperating, living longer) and some incidental or negative features (complaining about pain, paying more taxes).\n\nSo, using the approach of [section 4.9](https://www.lesswrong.com/posts/k54rgSg7GcjtXnMHX/model-splintering-moving-from-one-imperfect-model-to-another-1#4_9_Preserved_background_features), we can designate that we want the AI to cure cancer; this will be interpreted as increasing all the features that correlate with that.\n\nUsing the explicit decorrelation of [section 4.10](https://www.lesswrong.com/posts/k54rgSg7GcjtXnMHX/model-splintering-moving-from-one-imperfect-model-to-another-1#4_10_Partially_preserved_background_features), we can also explicitly remove the negative options from the desired feature sets, thus improving the outcomes even more. \n\n\n## 6.14 The genie and the burning mother\n\nIn Eliezer's [original post on the hidden complexity of wishes](https://www.lesswrong.com/posts/4ARaTpNX62uaL86j6/the-hidden-complexity-of-wishes), he talks of the challenge of getting a genie to save your mother from a burning building:\n\n>So you hold up a photo of your mother's head and shoulders; match on the photo; use object contiguity to select your mother's whole body (not just her head and shoulders); and define the future function using your mother's distance from the building's center. [...]\n\n>You cry \"Get my mother out of the building!\", for luck, and press Enter. [...]\n\n>BOOM!  With a thundering roar, the gas main under the building explodes.  As the structure comes apart, in what seems like slow motion, you glimpse your mother's shattered body being hurled high into the air, traveling fast, rapidly increasing its distance from the former center of the building.\n\nHow could we avoid this? What you want is your mother out of the building. The feature \"mother in building\" must absolutely be set to false; this is a priority call, overriding almost everything else.\n\nHere we'd want to load examples of your mother outside the building, so that the genie/AI learns the features \"mother in house\"/\"mother out of house\". Then it will note that \"mother out of house\" correlates with a whole lot of other features - like mother being alive, breathing, pain-free, often awake, and so on.\n\nAll those are good things. But there are some other features that don't correlate so well - such as the time being earlier, your mother not remembering a fire, not being covered in soot, not worried about her burning house, and so on.\n\nAs in the cancer patient example above, we'd want to preserve the features that correlate with the mother out of the house, while allowing decorrelation with the features we don't care about or don't want to preserve.\n\n\n## 6.15 Splintering moral-relevant categories: honour, gender, and happiness\n\nIf the [Antikythera mechanism](https://en.wikipedia.org/wiki/Antikythera_mechanism) had been combined with the [Aeolipile](https://en.wikipedia.org/wiki/Aeolipile) to produce an ancient Greek AI, and Homer had programmed it (among other things) to \"increase people's honour\", how badly would things have gone?\n\nIf Babbage had completed the [analytical engine](https://en.wikipedia.org/wiki/Analytical_Engine) as Victorian AI,  and programmed it (among other things) to \"protect women\", how badly would things have gone?\n\nIf a modern programmer were to combine our neural nets into a [superintelligence](https://en.wikipedia.org/wiki/Superintelligence:_Paths,_Dangers,_Strategies) and program it (among other things) to \"increase human happiness\", how badly will things go?\n\nThere are three moral-relevant categories here, and it's illustrative to compare them: honour, gender, and hedonic happiness. The first has splintered, the second is splintering, and the third will likely splinter in the future.\n\nI'm not providing solutions in this subsection, just looking at where the problems can appear, and encouraging people to think about how they would have advised Homer or Babbage to define their concepts. Don't think \"stop using your concepts, use ours instead\", because our concepts/features will splinter too. Think \"what's the best way they could have extended their preferences even as the features splinter\"?\n\n* **6.15.1 Honour**\n\nIf we look at the concept of [honour](https://en.wikipedia.org/wiki/Honour), we see a concept that has already splintered.\n\nThat article reads like a meandering mess. Honour is \"face\", \"reputation\", a \"bond between an individual and a society\", \"reciprocity\", a \"code of conduct\", \"chastity\" (or \"virginity\"), a \"right to precedence\", \"nobility of soul, magnanimity, and a scorn of meanness\", \"virtuous conduct and personal integrity\", \"vengeance\", \"credibility\", and so on.\n\nWhat a basket of concepts! They only seem vaguely connected together; and even places with strong honour cultures differ in how they conceive of honour, from place to place and from epoch to epoch[^honcul]. And yet, if you asked most people within those cultures about what honour was, they would have had a strong feeling it was a single, well defined thing, maybe even a [concrete object](https://en.wikipedia.org/wiki/Reification_(fallacy)).\n\n[^honcul]: You could define honour as \"behaves according to the implicit expectations of their society\", but that just illustrates how time-and-place dependent honour is.\n\n\n* **6.15.2 Gender**\n\nIn his post [the categories were made for man, not man for the categories](https://slatestarcodex.com/2014/11/21/the-categories-were-made-for-man-not-man-for-the-categories/), Scott writes:\n\n>Absolutely typical men have Y chromosomes, have male genitalia, appreciate manly things like sports and lumberjackery, are romantically attracted to women, personally identify as male, wear male clothing like blue jeans, sing baritone in the opera, et cetera.\n\nBut Scott is writing this in the 21st century, long after the gender definition has splintered quite a bit. In middle class middle class Victorian England[^vic], the gender divide was much stronger - in that, from one component of the divide, you could predict a lot more. For example, if you knew someone wore dresses in public, you knew that, almost certainly, they couldn't own property if they were married, nor could they vote, they would be expected to be in charge of the household, might be allowed to faint, and were expected to guard their virginity.\n\n[^vic]: Pre [1870](https://en.wikipedia.org/wiki/Married_Women%27s_Property_Act_1870).\n\n![](https://www.dropbox.com/s/50m3otrr36kfwxx/gender.jpg?raw=1)\n\nWe talk nowadays about gender roles multiplying or being harder to define, but they've actually being splintering for a lot longer than that. Even though we could *define* two genders in 1960s Britain, at least roughly, that definition was a lot less informative than it was in Victorian-middle-class-Britain times: it had many fewer features strongly correlated with it.\n\n* **6.15.3 Happiness**\n\nOn to happiness! Philosophers and others [have been talking about happiness for centuries](https://plato.stanford.edu/entries/happiness/), often contrasting \"true happiness\", or flourishing, with hedonism, or [drugged out stupor](https://en.wikipedia.org/wiki/Lotus-eaters), or things of that nature. Often \"true happiness\" is a life of duty to what the philosopher wants to happen, but at least there is some analysis, some breakdown of the \"happiness\" feature into smaller component parts.\n\nWhy did the philosophers do this? I'd wager that it's because the concept of happiness was already somewhat splintered (as compared with a model where \"happiness\" is a single thing). Those philosophers had experience of joy, pleasure, the satisfaction of a job well done, connection with others, as well as superficial highs from temporary feelings. When they sat down to systematise \"happiness\", they could draw on the features of their own mental model. So even if people hadn't systematised happiness themselves, when they heard of what philosophers were doing, they probably didn't react as \"What? Drunken hedonism and intellectual joy are not the same thing? How dare you say such a thing!\"\n\nBut looking into the future, into a world that an AI might create, we can foresee many situations where the implicit assumptions of happiness come apart, and only some remain. I say \"we can foresee\", but it's actually very hard to know exactly how that's going to happen; if we knew it exactly, we could solve the issues now.\n\nSo, imagine a happy person. What do you think that they have in life, that are not trivial synonyms of happiness? I'd imagine they have friends, are healthy, think interesting thoughts, have some freedom of action, may work on worthwhile tasks, may be connected with their community, probably make people around them happy as well. Getting a bit less anthropomorphic, I'd also expect them to be a carbon-based life-form, to have a reasonable mix of hormones in their brain, to have a continuity of experience, to have a sense of identity, to have a personality, and so on.\n\nNow, some of those features can clearly be separated from \"happiness\". Even ahead of time, I can confidently say that \"being a carbon-based life-form\" is not going to be a critical feature of \"happiness\". But many of the other ones are not so clear; for example, would someone without continuity of experience or a sense of identity be \"happy\"?\n\nOf course, I can't answer that question. Because the question has no answer. We have our current model of happiness, which co-varies with all those features I listed and many others I haven't yet thought of. As we move into more and more bizarre worlds, that model will splinter. And whether we assign the different features to \"happiness\" or to some other concept, is a choice we'll make, not a well-defined solution to a well-defined problem.\n\nHowever, even at this stage, some answers are clearly better than others; statues of happy people should not count, for example, nor should written stories describing very happy people.\n\n## 6.16 Apprenticeship learning\n\nIn [apprenticeship learning](https://en.wikipedia.org/wiki/Apprenticeship_learning) (or learning from demonstration), the AI would aim to copy what experts have done. Inverse reinforcement learning [can be used for this purpose](https://ai.stanford.edu/~ang/papers/icml04-apprentice.pdf), by guessing the expert's reward function, based on their demonstrations. It looks for key [features](https://ai.stanford.edu/~ang/papers/icml04-apprentice.pdf) in expert trajectories and attempts to reproduce them.\n\nSo, if we had an automatic car driving people to the airport, and fed it some trajectories (maybe ranked by speed of delivery), it would notice that passengers would also arrive alive, with their bags, without being pursued by the police, and so on. This is akin to [section 4.9](https://www.lesswrong.com/posts/k54rgSg7GcjtXnMHX/model-splintering-moving-from-one-imperfect-model-to-another-1#4_9_Preserved_background_features), and would not accelerate blindly to get there as fast as possible.\n\nBut the algorithm has trouble getting to truly super-human performance[^appplus]. It's far too conservative, and, if we loosen the conservatism, it doesn't know what's acceptable and what isn't, and how to trade these off: since all passengers survived and the car was always [painted yellow](https://en.wikipedia.org/wiki/Taxicabs_of_New_York_City), their luggage intact in the training data, it has no reason to prefer human survival to taxi-colour. It doesn't even have a reason to have a specific feature resembling \"passenger survived\" at all.\n\n[^appplus]: It's not impossible to get superhuman performance from apprenticeship learning; for example, we could select the best human performance on a collection of distinct tasks, and thus get the algorithm to have a overall performance that no human could ever match. Indeed, one of the purposes of [task decomposition](https://www.lesswrong.com/posts/x9bNg6uEyjxqDLjcS/2-will-such-systems-still-be-useful) is to decompose complex tasks in ways that allow apprenticeship-like learning to have safe and very superhuman performance on the whole task.\n\nThis might be improved by the \"allow decorrelation\" approach from section 4.10: we specifically allow it to maximise speed of transport, while keeping the other features (no accidents, no speeding tickets) intact. As in [section 6.7](https://www.lesswrong.com/posts/k54rgSg7GcjtXnMHX/model-splintering-moving-from-one-imperfect-model-to-another-1#6_7_Hidden__dis_agreement_and_interpretability), we'll attempt to check that the AI does prioritise human survival, and that it will warn us if a refactoring moves it away from this.\n"
    },
    "voteCount": 25,
    "forceInclude": true
  },
  {
    "_id": "uerghMGoGM9RQW8Zr",
    "url": null,
    "title": "Ontological Crises in Artificial Agents' Value Systems by Peter de Blanc",
    "slug": "ontological-crises-in-artificial-agents-value-systems-by",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "Ontological Crisis"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "I saw [this](http://arxiv.org/pdf/1105.3821v1) go by on arXiv, and thought it deserved a discussion here.\n\n> Decision-theoretic agents predict and evaluate the results of their actions using a model, or ontology, of their environment. An agent's goal, or utility function, may also be specified in terms of the states of, or entities within, its ontology. If the agent may upgrade or replace its ontology, it faces a crisis: the agent's original goal may not be well-defined with respect to its new ontology. This crisis must be resolved before the agent can make plans towards achieving its goals. We discuss in this paper which sorts of agents will undergo ontological crises and why we may want to create such agents. We present some concrete examples, and argue that a well-defined procedure for resolving ontological crises is needed. We point to some possible approaches to solving this problem, and evaluate these methods on our examples.\n\nI'll post my analysis and opinion of this paper in a comment after I've taken some time to digest it."
    },
    "voteCount": 16,
    "forceInclude": true
  },
  {
    "_id": "KLaJjNdENsHhKhG5m",
    "url": null,
    "title": "Ontological Crisis in Humans",
    "slug": "ontological-crisis-in-humans",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "Motivations"
      },
      {
        "name": "Ethics & Morality"
      },
      {
        "name": "Moral Uncertainty"
      },
      {
        "name": "Ontological Crisis"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "Imagine a robot that was designed to find and collect spare change around its owner's house. It had a world model where macroscopic everyday objects are ontologically primitive and ruled by high-school-like physics and (for humans and their pets) rudimentary psychology and animal behavior. Its goals were expressed as a utility function over this world model, which was sufficient for its designed purpose. All went well until one day, a prankster decided to \"upgrade\" the robot's world model to be based on modern particle physics. This unfortunately caused the robot's utility function to instantly throw a [domain error](http://stackoverflow.com/questions/641064/what-is-a-domain-error) exception (since its inputs are no longer the expected list of macroscopic objects and associated properties like shape and color), thus crashing the controlling AI.\n\nAccording to Peter de Blanc, who used the phrase \"[ontological crisis](http://wiki.lesswrong.com/wiki/Ontological_crisis)\" to describe this kind of problem,\n\n> Human beings also confront ontological crises. We should find out what cognitive algorithms humans use to solve the same problems described in this paper. If we wish to build agents that maximize human values, this may be aided by knowing how humans re-interpret their values in new ontologies.\n\nI recently realized that a couple of problems that I've been thinking over (the [nature of selfishness](/lw/8gk/where_do_selfish_values_come_from) and the [nature of pain/pleasure/suffering/happiness](/lw/4qg/a_thought_experiment_on_pain_as_a_moral_disvalue/)) can be considered instances of ontological crises in humans (although I'm not so sure we necessarily have the cognitive algorithms to solve them). I started thinking in this direction after writing [this comment](/lw/b7w/decision_theories_a_semiformal_analysis_part_iii/6hef):\n\n> This formulation or variant of TDT requires that before a decision problem is handed to it, the world is divided into the agent itself (X), other agents (Y), and \"dumb matter\" (G). I think this is misguided, since the world doesn't really divide cleanly into these 3 parts.\n\nWhat struck me is that even though the world doesn't divide cleanly into these 3 parts, _our models_ of the world actually do. In the world models that we humans use on a day to day basis, and over which our utility functions seem to be defined ([to the extent](/lw/9jh/the_humans_hidden_utility_function_maybe/) that we can be said to have utility functions at all), we do take the Self, Other People, and various Dumb Matter to be ontologically primitive entities. Our world models, like the coin collecting robot's, consist of these macroscopic objects ruled by a hodgepodge of heuristics and prediction algorithms, rather than microscopic particles governed by a coherent set of laws of physics.\n\nFor example, the amount of pain someone is experiencing doesn't seem to exist in the real world as an XML tag attached to some \"person entity\", but that's pretty much how our models of the world work, and perhaps more importantly, that's what our utility functions expect their inputs to look like (as opposed to, say, a list of particles and their positions and velocities). Similarly, a human can be selfish just by treating the object labeled \"SELF\" in its world model differently from other objects, whereas an AI with a world model consisting of microscopic particles would need to somehow inherit or learn a detailed description of itself in order to be selfish.\n\nTo fully confront the ontological crisis that we face, we would have to upgrade our world model to be based on actual physics, and simultaneously translate our utility functions so that their domain is the set of possible states of the new model. We currently have little idea how to accomplish this, and instead what we do in practice is, as far as I can tell, keep our ontologies intact and utility functions unchanged, but just add some new heuristics that in certain limited circumstances call out to new physics formulas to better update/extrapolate our models. This is actually rather clever, because it lets us make use of updated understandings of physics without ever having to, for instance, decide exactly what patterns of particle movements constitute pain or pleasure, or what patterns constitute oneself. Nevertheless, this approach hardly seems capable of being extended to work in a future where many people may have nontraditional mind architectures, or have a zillion copies of themselves running on all kinds of strange substrates, or be merged into amorphous group minds with no clear boundaries between individuals.\n\nBy the way, I think nihilism often gets short changed [around](/lw/sc/existential_angst_factory/) [here](/lw/5i7/on_being_okay_with_the_truth/). Given that we do not actually have at hand a solution to ontological crises in general or to the specific crisis that we face, what's wrong with saying that the solution set may just be null? Given that evolution doesn't constitute a particularly benevolent and farsighted designer, perhaps we may not be able to do much better than that poor spare-change collecting robot? If Eliezer is [worried](/r/discussion/lw/827/ai_ontology_crises_an_informal_typology/7pqr) that actual AIs facing actual ontological crises could do worse than just crash, should we be very sanguine that for humans everything must \"add up to moral normality\"?\n\nTo expand a bit more on this possibility, many people have an aversion against moral arbitrariness, so we need at a minimum a utility translation scheme that's principled enough to pass that filter. But our existing world models are a hodgepodge put together by evolution so there may not be any such sufficiently principled scheme, which (if other approaches to solving moral philosophy also don't pan out) would leave us with legitimate feelings of \"existential angst\" and nihilism. One could perhaps still argue that any _current_ such feelings are premature, but maybe some people have stronger intuitions than others that these problems are unsolvable?\n\nDo we have any examples of humans successfully navigating an ontological crisis? The LessWrong Wiki [mentions](http://wiki.lesswrong.com/wiki/Ontological_crisis) loss of faith in God:\n\n> In the human context, a clear example of an ontological crisis is a believer’s loss of faith in God. Their motivations and goals, coming from a very specific view of life suddenly become obsolete and maybe even nonsense in the face of this new configuration. The person will then experience a deep crisis and go through the psychological task of reconstructing its set of preferences according the new world view.\n\nBut I don't think loss of faith in God actually constitutes an ontological crisis, or if it does, certainly not a very severe one. An ontology consisting of Gods, Self, Other People, and Dumb Matter just isn't very different from one consisting of Self, Other People, and Dumb Matter (the latter could just be considered a special case of the former with quantity of Gods being 0), especially when you compare either ontology to one made of microscopic particles or even [less](http://en.wikipedia.org/wiki/Loop_quantum_gravity) [familiar](http://en.wikipedia.org/wiki/String_theory) [entities](http://en.wikipedia.org/wiki/Ultimate_ensemble).\n\nBut to end on a more positive note, realizing that seemingly unrelated problems are actually instances of a more general problem gives some hope that by \"going meta\" we can find a solution to all of these problems at once. Maybe we can solve many ethical problems simultaneously by discovering some generic algorithm that can be used by an agent to transition from any ontology to another? \n\n(Note that I'm not saying this _is_ the right way to understand one's real preferences/morality, but just drawing attention to it as a possible alternative to other more \"object level\" or \"purely philosophical\" approaches. See also [this previous discussion](/lw/6ha/the_blueminimizing_robot/4gi2), which I recalled after writing most of the above.)"
    },
    "voteCount": 51,
    "forceInclude": true
  },
  {
    "_id": "6RjL996E8Dsz3vHPk",
    "url": null,
    "title": "Two More Decision Theory Problems for Humans",
    "slug": "two-more-decision-theory-problems-for-humans",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "Decision Theory"
      }
    ],
    "tableOfContents": {
      "sections": [
        {
          "title": "Partial Utility Function Problem",
          "anchor": "Partial_Utility_Function_Problem",
          "level": 1
        },
        {
          "title": "Decision Theory Upgrade Problem",
          "anchor": "Decision_Theory_Upgrade_Problem",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        },
        {
          "anchor": "comments",
          "level": 0,
          "title": "13 comments"
        }
      ],
      "headingsCount": 4
    },
    "contents": {
      "markdown": "(This post has been sitting in my drafts folder for 6 years. Not sure why I didn't make it public, but here it is now after some editing.)\n\nThere are two problems closely related to the [Ontological Crisis in Humans](/posts/KLaJjNdENsHhKhG5m/ontological-crisis-in-humans). I'll call them the \"Partial Utility Function Problem\" and the \"Decision Theory Upgrade Problem\".\n\n**Partial Utility Function Problem**\n\nAs I mentioned in a [previous post](/lw/fyb/ontological_crisis_in_humans/), the only apparent utility function we have seems to be defined over an ontology very different from the fundamental ontology of the universe. But even on it's native domain, the utility function seems only partially defined. In other words, it will throw an error (i.e., say \"I don't know\") on some possible states of the heuristical model. For example, this happens for me when the number of people gets sufficiently large, like 3^^^3 in Eliezer's Torture vs Dust Specks scenario. When we try to compute the expected utility of some action, how should we deal with these \"I don't know\" values that come up?\n\n(Note that I'm presenting a simplified version of the real problem we face, where in addition to \"I don't know\", our utility function could also return essentially random extrapolated values outside of the region where it gives sensible outputs.)\n\n**Decision Theory Upgrade Problem**\n\nIn the Decision Theory Upgrade Problem, an agent decides that their current decision theory is inadequate in some way, and needs to be upgraded. (Note that the Ontological Crisis could be considered an instance of this more general problem.) The question is whether and how to transfer their values over to the new decision theory.\n\nFor example a human might be be running a mix of several decision theories: reinforcement learning, heuristical model-based consequentialism, identity-based decision making (where you adopt one or more social roles, like \"environmentalist\" or \"academic\" as part of your identity and then make decisions based on pattern matching what that role would do in any given situation), as well as virtual ethics and deontology. If you are tempted to drop one or more of these in favor of a more \"advanced\" or \"rational\" decision theory, such as UDT, you have to figure out how to transfer the values embodied in the old decision theory, which may not even be represented as any kind of utility function, over to the new.\n\nAnother instance of this problem can be seen in someone just wanting to be a bit more consequentialist. Maybe UDT is too strange and impractical, but our native model-based consequentialism at least seems closer to being rational than the other decision procedures we have. In this case we tend to assume that the consequentialist module already has our real values and we don't need to \"port\" values from the other decision procedures that we're deprecating. But I'm not entirely sure this is safe, since the step going from (for example) identity-based decision making to heuristical model-based consequentialism doesn't seem *that* different from the step between heuristical model-based consequentialism and something like UDT."
    },
    "voteCount": 20,
    "forceInclude": true
  },
  {
    "_id": "ky988ePJvCRhmCwGo",
    "url": null,
    "title": "Using vector fields to visualise preferences and make them consistent",
    "slug": "using-vector-fields-to-visualise-preferences-and-make-them",
    "author": "MichaelA",
    "question": false,
    "tags": [
      {
        "name": "Value Learning"
      },
      {
        "name": "AI"
      }
    ],
    "tableOfContents": {
      "sections": [
        {
          "title": "Overview",
          "anchor": "Overview",
          "level": 1
        },
        {
          "title": "Vector fields and preferences",
          "anchor": "Vector_fields_and_preferences",
          "level": 1
        },
        {
          "title": "Not only preferences",
          "anchor": "Not_only_preferences",
          "level": 2
        },
        {
          "title": "Gradients and utility functions",
          "anchor": "Gradients_and_utility_functions",
          "level": 1
        },
        {
          "title": "Method",
          "anchor": "Method",
          "level": 2
        },
        {
          "title": "Extrapolating PVFs (and utility functions) from specific preference data",
          "anchor": "Extrapolating_PVFs__and_utility_functions__from_specific_preference_data",
          "level": 1
        },
        {
          "title": "Curl and inconsistent preferences",
          "anchor": "Curl_and_inconsistent_preferences",
          "level": 1
        },
        {
          "title": "Removing curl to create consistent utility functions",
          "anchor": "Removing_curl_to_create_consistent_utility_functions",
          "level": 2
        },
        {
          "title": "Uncertainties and areas for further research",
          "anchor": "Uncertainties_and_areas_for_further_research",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        },
        {
          "anchor": "comments",
          "level": 0,
          "title": "32 comments"
        }
      ],
      "headingsCount": 11
    },
    "contents": {
      "markdown": "_This post was written for [Convergence Analysis](https://www.convergenceanalysis.org/) by Michael Aird, based on ideas from Justin Shovelain and with ongoing guidance from him. Throughout the post, “I” will refer to Michael, while “we” will refer to Michael and Justin or to Convergence as an organisation._\n\n_Epistemic status: High confidence in the core ideas on an abstract level. Claims about the usefulness of those ideas, their practical implications, and how best to concretely/mathematically implement them are more speculative; one goal in writing this post is to receive feedback on those things. I’m quite new to many of the concepts covered in this post, but Justin is more familiar with them._\n\nOverview\n========\n\nThis post outlines:\n\n*   What vector fields are\n*   How they can be used to visualise preferences\n*   How utility functions can be generated from “preference vector fields” (PVFs)\n*   How PVFs can be extrapolated from limited data on preferences\n*   How to visualise inconsistent preferences (as “curl”)\n*   A rough idea for how to “remove curl” to generate consistent utility functions\n*   Possible areas for future research\n\nWe expect this to provide useful tools and insights for various purposes, most notably AI alignment, existential risk [strategy](https://forum.effectivealtruism.org/posts/oovy5XXdCL3TPwgLE/a-case-for-strategy-research-what-it-is-and-why-we-need-more), and rationality.\n\nThis post is structured modularly; different sections may be of interest to different readers, and should be useful in isolation from the rest of the post. The post also includes links to articles and videos introducing relevant concepts, to make the post accessible to readers without relevant technical backgrounds.\n\nVector fields and preferences\n=============================\n\nA [vector](https://en.wikipedia.org/wiki/Euclidean_vector) represents both magnitude and direction; for example, velocity is a vector that represents not just the speed at which one is travelling but also the direction of travel. A [vector field](https://en.wikipedia.org/wiki/Vector_field) essentially associates a vector to each point in a region of space. For example, the following image ([source](https://www.khanacademy.org/science/physics/magnetic-forces-and-magnetic-fields/magnetic-field-current-carrying-wire/a/what-are-magnetic-fields)) shows the strength (represented by arrow lengths) and direction of the magnetic field at various points around a bar magnet:\n\n  \n\n![](https://res.cloudinary.com/dwbqmbkdb/image/upload/v1580239962/Magnetic_field_wdw8tw.png)\n\n_Figure 1._\n\nAnother common usage of vector fields is to represent the direction in which fluid would flow, for example the downhill flow of water on uneven terrain ([this short video](https://www.khanacademy.org/math/multivariable-calculus/thinking-about-multivariable-function/visualizing-vector-valued-functions/v/fluid-flow-and-vector-fields) shows and discusses that visualisation).\n\nWe believe that vector fields over “state spaces” (possible states of the world, represented by positions along each dimension) can be a useful tool for analysis and communication of various issues (e.g., existential risk strategy, AI alignment). In particular, we’re interested in the idea of representing preferences as “preference vector fields” (PVFs), in which, at each point in the state space, a vector represents which direction in the state space an agent would prefer to move from there, and how intense that preference is.^[It appears some prior work (e.g., [this](https://www.tandfonline.com/doi/abs/10.1080/09544820500275032) and [this](https://books.google.com.au/books?id=qLnEiu0lbNAC&pg=PA169&lpg=PA169&dq=preference+vector+field&source=bl&ots=zXSiGh3haW&sig=ACfU3U16TLa5wnOv8ABcVbTwVLnOoukIvw&hl=en&sa=X&ved=2ahUKEwja5df1wqXmAhULXisKHbZqDbkQ6AEwEHoECAoQAg#v=onepage&q=preference%20vector%20field&f=false)) has explored the use of vector fields to represent preferences. Unfortunately, I haven’t yet had time to investigate this work, so there may be many useful insights in there that are lacking in this post.] (For the purposes of this post, “agent” could mean an AI, a human, a community, humanity as a whole, etc.)\n\nTo illustrate this, the following PVF shows a hypothetical agent’s preferences over a state space in which the only dimensions of interest are wealth and security.^[Of course, there are often far more than two key factors influencing our preferences. In such cases, a vector field over more dimensions can be used instead (see [here](https://www.khanacademy.org/math/multivariable-calculus/thinking-about-multivariable-function/visualizing-vector-valued-functions/v/3d-vector-fields-introduction) for an introduction to 3D vector fields). I focus in this post on 2D vector fields, simply because those are easier to discuss and visualise. We expect many of the ideas and implications covered in this post will be similar in higher dimensional vector fields, but we aren’t yet certain about that, and intend to more carefully consider it later.]^[For both this example and most others shown, the precise equations used were chosen quite arbitrarily, basically by trying equations semi-randomly until I found one that roughly matched the sort of shape I wanted. For those interested, I have screenshots of all equations used, in their order of appearance in this post, [here](https://docs.google.com/document/d/1pGx7PPpZ_52l1Zrp5UpSA7UY1M3m8UtRIjVirn0DGls/edit?usp=sharing). To create the visuals in this post, I entered these equations into Grapher (for those interested in trying to do similar things themselves, I found [this guide](https://theputterer.wordpress.com/2011/12/10/mac-os-x-grapher-contour-and-vector-plots/) useful). I discuss below, in the section “Extrapolating PVFs (and utility functions) from specific preference data”, the issue of how to actually generate realistic/accurate PVFs in the first place.]\n\n![](https://res.cloudinary.com/dwbqmbkdb/image/upload/v1580239963/Figure_2_Vectors_gkybi1.png)\n\n_Figure 2._\n\nThe fact that (at least over the domain shown here) the arrows always point at least slightly upwards and to the right shows that the agent prefers more wealth and security to less, regardless of the current level of those variables. The fact that the arrows are longest near the x axis shows that preferences are most intense when security is low. The fact that the arrows become gradually more horizontal as we move up the y axis shows that, as security increases, the agent comes to care more about wealth relative to security.\n\nNot only preferences\n--------------------\n\nIn a very similar way, vector fields can be used to represent things other than preferences. For example, we might suspect that for many agents (e.g., most/all humans), preferences do not perfectly match what would actually make the agent happier (e.g., because of the agent being mistaken about something, or having separate systems for reward vs motivation). In this case, we could create a vector field to represent the agent’s preferences (represented by the blue arrows below), and another to represent what changes from any given point would increase the agent’s happiness (represented by the green arrows).\n\n![](https://res.cloudinary.com/dwbqmbkdb/image/upload/v1580239967/Figure_3_Vectors_cl7b39.png)\n\n_Figure 3._\n\nThis method of layering vector fields representing different things can be used as one tool in analysing potential clashes between different things (e.g., between an agent’s preferences and what would actually make the agent happy, or between an agent’s _beliefs_ about what changes would be likely at each state and what changes would _actually_ be likely at each state).\n\nFor example, the above graph indicates that, as wealth and/or security increases (i.e., as we move across the x axis and/or up the y axis), there is an increasing gap between the agent’s preferences and what would make the agent happy. In particular, security becomes increasingly more important than wealth for the agent’s happiness, but this is not reflected in the agent’s preferences.\n\n(Note that, while it does make sense to compare the direction in which arrows from two different vector fields point, I haven’t yet thought much about whether it makes sense to compare the lengths Grapher shows for their arrows. It seems like this is _mathematically_ the same as the common problem of trying to compare utility functions across different agents, or preferences across different voters. But here the functions represent different things within the _same_ agent, which may make a difference.)\n\nGradients and utility functions\n===============================\n\nWhen a vector field has no “curl” (see the section “Curl and inconsistent preferences” below), the vector field can be thought of as the [gradient](https://www.khanacademy.org/math/multivariable-calculus/multivariable-derivatives/gradient-and-directional-derivatives/v/gradient) of a [scalar field](https://en.wikipedia.org/wiki/Scalar_field).^[It’s possible that here I’m conflating the concepts of [conservative](https://en.wikipedia.org/wiki/Conservative_vector_field), irrotational, and curl-free vector fields in a way that doesn’t make sense. If any readers believe this is the case, and especially if they believe this issue changes the core ideas and implications raised in this post, I would appreciate them commenting or messaging me.] (A scalar field is similar to a vector field, except that it associates a _scalar_ with each point in a region of space, and scalars have only magnitude, rather than magnitude _and_ _direction_.) Essentially, this means that the arrows of the vector field can be thought of as pointing “uphill”, away from low points and towards high points of the associated scalar function. If the vector field represents preferences, higher points of the scalar function would be where preferences are more satisfied, and lower points are where it is less satisfied; thus, the scalar function can be thought of as the agent’s utility function.^[Technically, the vector field is the gradient of a _class of_ functions, with the functions differing only in their constant term. This is because gradient only relates to _differences_ in height (or roughly analogous ideas, in higher-dimensional cases), not to absolute heights. One can imagine raising or lowering the entire scalar function by the same constant without affecting the gradient between points. (I show in [this document](https://docs.google.com/document/d/1pGx7PPpZ_52l1Zrp5UpSA7UY1M3m8UtRIjVirn0DGls/edit?usp=sharing) examples of what this would look like, while in this post itself I keep all constants at 0.) Thus, in one sense, a PVF does not fully specify the associated utility function representation, but the constant can be ignored anyway (as utility functions are unique up to positive affine transformations).] (The same basic method is often used in physics, in which context the scalar function typically represents [scalar potential](https://en.wikipedia.org/wiki/Scalar_potential).)\n\nBelow is one visualisation of the scalar field representing the utility function of the agent from the previous example (based on its preferences, not on what would make it “happy”), as well as the related vector field. Colours towards the red end of the spectrum represent higher values of the scalar field. It can be seen that the arrows of the vector field point away from blue areas and towards red areas, representing the agent’s preference for “climbing uphill” on its utility function.\n\n![](https://res.cloudinary.com/dwbqmbkdb/image/upload/v1580239979/Figure_4_Vectors_xjq6sz.png)\n\n_Figure 4._\n\nThe scalar field can also be represented in three dimensions, as values on the z dimension, which are in turn a function of values on the x and y dimensions. This is shown below (from two angles), for the same agent. (These graphs are a little hard to interpret from still images on a 2D screen, at least with this function; such graphs can be easier to interpret when one is able to rotate the angle of view.)\n\n![](https://res.cloudinary.com/dwbqmbkdb/image/upload/v1580239979/Figure_5_Vectors_ma27vo.png)\n\n_Figures 5a and 5b._\n\nMethod\n------\n\n[This video](https://www.youtube.com/watch?v=iLAK2IsQ_Uo) provides one clear explanation of the actual method for determining the scalar function that a curl-free vector field can be thought of as the gradient of (though the video is focused on cases of 3D vector fields). That video describes this as finding the “potential”; as noted earlier, when the vector field represents preferences, the utility function can be thought of as analogous to the “potential” in other cases.\n\nPersonally, as a quick method of finding the scalar function associated with a 2D vector field, I used the following algorithm, from the first answer on [this page](https://mathematica.stackexchange.com/questions/100758/finding-scalar-potential-function):\n\n> DSolve[{D[f[x, y], x] == [X COMPONENT OF THE VECTOR FIELD], D[f[x, y], y] == [Y COMPONENT OF THE VECTOR FIELD]}, f[x, y], {x, y}]\n\nI input the algorithm into a [Wolfram Cloud notebook](https://www.wolframcloud.com/), which seems to be free to use as long as you create an account. (As noted in the answer on the linked page, this algorithm will come back with no solution if the vector field has curl. This makes sense, because this general approach cannot be used in this way if a field has curl; this is explained in the section “Curl and inconsistent preferences” below.) Finally, I double-checked that the function was a valid solution by using [this calculator](https://www.wolframalpha.com/input/?i=grad+of+a+scalar+field&assumption=%7B%22F%22%2C+%22GradientCalculator%22%2C+%22scalarfunction%22%7D+-%3E) to find its gradient, which should then be the same as the original vector field.\n\nExtrapolating PVFs (and utility functions) from specific preference data\n========================================================================\n\nIn reality, one rarely knows an agent’s actual utility function or their full PVF. Instead, one is likely to only have data on the agent’s (apparent) preferences at _particular points_ in state space; for example, the extent to which they wanted more wealth and more security when they had $10,000 of savings and a “4/5” level of security.\n\nOne can imagine extrapolating a full preference vector field (PVF) from that data. We do not know of a precise method for actually doing this (we plan to do more research and thought regarding that in future). However, conceptually speaking, it seems the process would be analogous to fitting a regression line to observed data points, and, like that process, would require striking a balance between maximising fit with the data and avoiding [overfitting](https://en.wikipedia.org/wiki/Overfitting).\n\nFor an example (based very loosely on Figure 3 in [this article](https://www.tandfonline.com/doi/abs/10.1080/09544820500275032)), suppose that I know that Alice prefers car A to Car B, Car B to Car C, Car C to Car D, and Car D to Car A (i.e., to Alice, A>B>C>D>A).^[I have purposefully chosen a set of circular (or “intransitive”) preferences, as the next session will use this example in discussing the problem of circularity and how to deal with it.] I also know the weight (in thousands of pounds) and perceived “sportiness” (as rated by consumers) of the four cars, and am willing to make the simplifying assumption that these are the only factors that influenced Alice’s preferences. I could then create a plane with weight on the x axis and sportiness on the y axis, show the position of the four cars in this space, and represent Alice’s preferences with arrows pointing from each car towards the car Alice would prefer to that one, as shown below:^[Note that, in this example, I am not assuming any knowledge about the _strength_ of Alice’s preferences, only about their direction. As such, the length of the arrows representing Alice’s known preferences has no particular meaning.]\n\n![](https://res.cloudinary.com/dwbqmbkdb/image/upload/v1580239980/Figure_6_vectors_ggb5qp.png)\n\n_Figure 6._\n\nI could then infer a PVF that (1) approximately captures Alice’s known preferences, and (2) suggests what preferences Alice would have at any other point in the plane (rather than just at the four points I have data for). In this case, one seemingly plausible PVF is shown below, with the length of each blue arrow representing the strength of Alice’s preferences at the associated point. (This PVF still shows Alice’s known preferences, but this is just for ease of comparison; those known preferences are not actually part of the PVF itself.)\n\n![](https://res.cloudinary.com/dwbqmbkdb/image/upload/v1580239990/Figure_7_vectors_mktb8i.png)\n\n_Figure 7._\n\nThis PVF allows us to make predictions about what Alice’s preferences would be even in situations we do not have any empirical data about. For example, this PVF suggests that if Alice had the hypothetical car E (with a weight of ~2000 pounds and sportiness of ~55), she would prefer a car that was heavier and was higher for sportiness. In contrast, the PVF also suggests that, if she had the hypothetical car F (with a weight of ~6000 pounds and sportiness of ~55), she would prefer a car that was heavier and was rated _lower_ for sportiness.\n\nOf course, these predictions are not necessarily accurate. One could likely create many other PVFs that also “appear” to roughly fit Alice’s known preferences, and these could lead to different predictions. This highlights why we wish to find a more precise/“rigorous” method to better accomplish the goal I have conceptually gestured at here.\n\nIt’s also worth noting that one could extrapolate an agent’s utility function from limited preference data by first using the method gestured at here and then using the method covered in the previous section. That is, one could gather some data on an agent’s (apparent) preferences, extrapolate a PVF that “fits” that data, and then calculate what (set of) scalar function(s) that vector field is the gradient of. That scalar function would be the agent’s extrapolated utility function.\n\nHowever, as noted earlier, this method only works if the PVF has no “curl”, so it would not work in the case of Alice’s preferences about cars. I will now discuss what I mean by “curl”, what implications curl has, and a rough idea for “removing” it.\n\nCurl and inconsistent preferences\n=================================\n\nIn the example above, to Alice, A>B>C>D>A. This is a case of [intransitivity](https://en.wikipedia.org/wiki/Intransitivity#Occurrences_in_preferences), or, less formally, circular or inconsistent preferences. This is typically [seen as](https://www.lesswrong.com/s/waF2Pomid7YHjfEDt/p/zJZvoiwydJ5zvzTHK) [irrational](https://www.lesswrong.com/s/waF2Pomid7YHjfEDt/p/zNcLnqHF5rvrTsQJx), and as opening agents up to issues such as being “[money pumped](https://en.wikipedia.org/wiki/Money_pump)”. It seems that Alice would be willing to just keep paying us to let her trade in one car for the one she preferred to that one, and do this _endlessly_ \\- going around and around in a circle, yet feeling that her preferences are being continually satisfied.\n\nSo another pair of reasons why representing preferences as vector fields is helpful is that doing so allows inconsistencies in preferences:\n\n1.  to be directly seen (if they are sufficiently extreme)\n2.  to be calculated as the vector field’s [curl](https://en.wikipedia.org/wiki/Curl_(mathematics))\n\n[This video](https://www.khanacademy.org/math/multivariable-calculus/multivariable-derivatives/curl-grant-videos/v/2d-curl-intuition) introduces the concept of curl. Returning to the visualisation of vector fields as representing the direction in which water would flow over a certain domain, curl represents the speed and direction an object would spin if placed in the water. For example, if there is a strong clockwise curl at a certain point, a stick placed there would rotate clockwise; if there is no curl at a point, a stick placed there would not rotate (though it still may move in some direction, as represented by the vector field itself).\n\nNote that the concepts of curl and inconsistency will also apply in less extreme cases (i.e., where an agent’s preferences do not _only_ “chase each other around in circles”).\n\nAs noted earlier, when a vector field has curl, one cannot find its gradient. In our context, this seems logical; if an agent’s preferences are inconsistent, it seems that the agent cannot have a true utility function, and that we can’t assign any meaningful “height” to any point in the 2D state space. Consider again the example of Alice’s preferences for cars; if we were to interpret meeting her preferences as moving “uphill” on a utility function, she could keep arriving back at the same points in the state space and yet be at different “heights”, which doesn’t seem to make sense.\n\nRemoving curl to create consistent utility functions\n----------------------------------------------------\n\nIt seems that agents frequently have intransitive preferences, and thus that their PVFs will often have some curl. It would therefore be very useful to have a method for “removing curl” from a PVF, to translate an intransitive set of preferences into a transitive set of preferences, while making a minimum of changes. This new, consistent PVF would also then allow for the generation of a corresponding utility function for the agent.^[In conversation with Justin, Linda Linsefors mentioned having had a somewhat similar idea independently.]\n\nWe believe that this process should be possible. We also believe that, if developed and confirmed to make sense, it could be useful for various aspects of AI alignment (among other things). In particular, it could help in:\n\n*   extrapolation of a consistent “core” (and corresponding utility function) from inconsistent _human_ preferences (which could then inform an AI’s decisions)\n*   adjustment of an _AI’s_ inconsistent preferences (either by engineers or by the AI itself), with a minimum of changes being made\n\nWe have not yet implemented this process for removing curl. But we believe that the [Helmholtz theorem](https://en.wikipedia.org/wiki/Helmholtz_decomposition) should work, at least for PVFs in 3 or fewer dimensions (and we believe that a higher dimensional generalization probably exists). The Helmholtz theorem: \n\n> states that any sufficiently smooth, rapidly decaying vector field in three dimensions can be resolved into the sum of an irrotational (curl-free) vector field and a solenoidal (divergence-free) vector field; this is known as the Helmholtz decomposition or Helmholtz representation. ([Wikipedia](https://en.wikipedia.org/wiki/Helmholtz_decomposition))\n\nThis irrotational (curl-free) vector field would then be the consistent projection (in a [CEV](https://intelligence.org/files/CEV.pdf)-like way) of the agent’s preferences (from which the agent’s utility function could also be generated, in the manner discussed earlier).  \n\nUncertainties and areas for further research\n============================================\n\nThe following are some areas we are particularly interested in getting comments/feedback on, seeing others explore, or exploring ourselves in future work:\n\n*   Are there any flaws or misleading elements in the above analysis? (As noted earlier, this is essentially just an initial exploration of some tools/concepts.)\n*   To what extent do the methods used and claims made in this post generalise to higher-dimensional spaces (e.g., when we wish to represent preferences over more than two factors at the same time)? To what extent do they generalise to graphs of states that don’t correspond to any normal geometry?\n*   Is there an existing, rigorous/precise method for extrapolating a PVF from a limited number of known preferences (or more generally, extrapolating a vector field from a limited number of known vectors)? If not, can a satisfactorily rigorous/precise method be developed?\n*   Are there meaningful and relevant differences between the concepts of curl in vector fields and of intransitivity, inconsistency, irrationality, and incoherence in preferences? If so, how does that change the above analysis?\n*   Is it possible to “remove curl” in the way we want, in the sort of situations we’re interested in (in particular, not only in three dimensions)? If so, how, specifically?\n*   What other implications do the above ideas have? E.g., for rationality more generally, or for how to interpret and implement preference utilitarianism. (Above, I mostly just introduced the ideas, and hinted at a handful of implications.)\n*   What other uses could these “tools” be put to?"
    },
    "voteCount": 21,
    "forceInclude": true
  },
  {
    "_id": "H2SizXNd99FcwLsCC",
    "url": null,
    "title": "Turning Some Inconsistent Preferences into Consistent Ones",
    "slug": "turning-some-inconsistent-preferences-into-consistent-ones",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "Rationality"
      }
    ],
    "tableOfContents": {
      "sections": [
        {
          "title": "Epistemic Status",
          "anchor": "Epistemic_Status",
          "level": 3
        },
        {
          "title": "Turning Some Inconsistent Preferences into Consistent Ones",
          "anchor": "Turning_Some_Inconsistent_Preferences_into_Consistent_Ones",
          "level": 1
        },
        {
          "title": "Mathematical Formulation of the Problem",
          "anchor": "Mathematical_Formulation_of_the_Problem",
          "level": 2
        },
        {
          "title": "Related Work",
          "anchor": "Related_Work",
          "level": 2
        },
        {
          "title": "Discrete Case",
          "anchor": "Discrete_Case",
          "level": 2
        },
        {
          "title": "Example",
          "anchor": "Example",
          "level": 3
        },
        {
          "title": "Resolving Inconsistencies",
          "anchor": "Resolving_Inconsistencies",
          "level": 3
        },
        {
          "title": "Implementation",
          "anchor": "Implementation",
          "level": 4
        },
        {
          "title": "Problems with This Method and its Algorithm",
          "anchor": "Problems_with_This_Method_and_its_Algorithm",
          "level": 4
        },
        {
          "title": "Questions",
          "anchor": "Questions",
          "level": 3
        },
        {
          "title": "Number of Turnings for Gn",
          "anchor": "Number_of_Turnings_for_Gn",
          "level": 4
        },
        {
          "title": "Encoding Inconsistencies",
          "anchor": "Encoding_Inconsistencies",
          "level": 2
        },
        {
          "title": "Theory",
          "anchor": "Theory",
          "level": 3
        },
        {
          "title": "Discrete Case",
          "anchor": "Discrete_Case1",
          "level": 3
        },
        {
          "title": "Incompleteness",
          "anchor": "Incompleteness",
          "level": 4
        },
        {
          "title": "Intransitivity",
          "anchor": "Intransitivity",
          "level": 4
        },
        {
          "title": "Non-Encodable Inconsistencies",
          "anchor": "Non_Encodable_Inconsistencies",
          "level": 4
        },
        {
          "title": "Continuous Case",
          "anchor": "Continuous_Case",
          "level": 3
        },
        {
          "title": "Incompleteness",
          "anchor": "Incompleteness1",
          "level": 4
        },
        {
          "title": "Intransitivity",
          "anchor": "Intransitivity1",
          "level": 4
        },
        {
          "title": "Discontinuity",
          "anchor": "Discontinuity",
          "level": 4
        },
        {
          "title": "Dependence",
          "anchor": "Dependence",
          "level": 4
        },
        {
          "title": "Discussion",
          "anchor": "Discussion",
          "level": 3
        },
        {
          "title": "Continuous Case",
          "anchor": "Continuous_Case1",
          "level": 2
        },
        {
          "title": "Vector Fields over Probability Simplices",
          "anchor": "Vector_Fields_over_Probability_Simplices",
          "level": 3
        },
        {
          "title": "Resolving Inconsistencies",
          "anchor": "Resolving_Inconsistencies1",
          "level": 4
        },
        {
          "title": "Graphons",
          "anchor": "Graphons",
          "level": 3
        },
        {
          "title": "Implications for AI Alignment",
          "anchor": "Implications_for_AI_Alignment",
          "level": 2
        },
        {
          "title": "Ambitious Value Learning",
          "anchor": "Ambitious_Value_Learning",
          "level": 3
        },
        {
          "title": "Ontological Crises",
          "anchor": "Ontological_Crises",
          "level": 3
        },
        {
          "title": "Discrete Case",
          "anchor": "Discrete_Case2",
          "level": 4
        },
        {
          "title": "Further Questions",
          "anchor": "Further_Questions",
          "level": 2
        },
        {
          "title": "Acknowledgements",
          "anchor": "Acknowledgements",
          "level": 2
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        },
        {
          "anchor": "comments",
          "level": 0,
          "title": "5 comments"
        }
      ],
      "headingsCount": 35
    },
    "contents": {
      "markdown": "*cross-posted from [niplav.github.io](https://niplav.github.io)*\n\n### Epistemic Status\n\nThis is still a draft that I [was told](https://schelling.pt/@niplav/108403526373636608) to already post here, which includes working (but very slow) code for one special case. Hopefully I'll be able to expand on this in the next ~half year.\n\n> Representing inconsistent preferences with specific mathematical\nstructures can clarify thoughts about how to make those preferences\nconsistent while only minimally changing them. This is discussed in\nthe case of preferences over world states, represented by [directed\ngraphs](https://en.wikipedia.org/wiki/Directed_graph); and preferences\nover [lotteries](https://en.wikipedia.org/wiki/Lottery_\\(probability\\))\nof world states, represented either by infinitely dense\ngraphs, (in some cases) vector fields over probability\nsimplices, or edge-weighted directed graphs. I also present\nan algorithm for the discrete case based on the [graph edit\ndistance](https://en.wikipedia.org/wiki/Graph_Edit_Distance). Implications\nfor scenarios such as [ontological\nshifts](https://arbital.com/p/ontology_identification/) are discussed.\n\n\nTurning Some Inconsistent Preferences into Consistent Ones\n===========================================================\n\n> A kind of God-made (or evolution-created) fairness between species is\nalso unexpectedly found.\n\n*— [Yew-Kwang Ng](https://en.wikipedia.org/wiki/Yew-Kwang_Ng), [“Towards Welfare Biology: Evolutionary Economics of Animal Consciousness and Suffering”](https://niplav.github.io/doc/biology/welfare/towards_welfare_biology_evolutionary_economics_of_animal_consciousness_and_suffering_ng_1995.pdf) p. 1, 1995*\n\n> Random testing is simple in concept, often easy to implement, has been\ndemonstrated to effectively detect failures, is good at exercising systems\nin unexpected ways (which may not occur to a human tester), and may be\nthe only practical choice when the source code and the specifications\nare unavailable or incomplete.\n\n*— Tsong Yueh Chen/Fei-Ching Kuo/Robert G. Merkel/T.H. Tse, [“Adaptive Random Testing: the ART of Test Case Diversity”](https://niplav.github.io/doc/cs/reduction/adaptive_random_testing_chen_et_al_2010.pdf), 2010*\n\nConsider an agent which displays ([von\nNeumman-Morgenstern](https://en.wikipedia.org/wiki/Von_Neumann-Morgenstern_utility_theorem))\ninconsistent [preferences](https://en.wikipedia.org/wiki/Preference),\nfor example choosing two incompatible\noptions in the two scenarios in the [Allais\nparadox](https://en.wikipedia.org/wiki/Allais_paradox), or reliably\ndisplaying [cycles](https://en.wikipedia.org/wiki/Cycle_\\(graph_theory\\))\nin its actions (detecting which actions are in fact caused by\ninconsistent preferences, and not just exotic ones from weird\nabstractions, is considered a separate problem here). We might want\nto interact with that agent, e.g. trade with it, help it (or exploit\nit), or generally know how it will act But how to go about that if the\nagent displays inconsistent preferences? Perhaps it might even be the\ncase that humans are such agents, and find ourselves in a conundrum:\nwe know our preferences are inconsistent and reliably exploitable,\nand that agents with such preferences [reliably fare worse in the\nworld](https://www.lesswrong.com/s/FYMiCeXEgMzsB5stm/p/RQpNHSiWaXTvDxt6R),\nwe might want to change that.\n\nA possible approach to this problem has two steps:\n\n1. Find ways to represent inconsistent preferences with a mathematical structure which can encode all possible violations of the von Neumann-Morgenstern axioms in all their combinations.\n2. Then turn those inconsistent preferences into consistent ones, and then inform the agent about these inconsistencies and their optimal resolutions (or, in the case of trying to help the agent, then enacting these preferences in the real world).\n\nMathematical Formulation of the Problem\n----------------------------------------\n\nDefine a set of possible (von Neumann-Morgenstern) inconsistent\npreferences over a set $W$ of worlds as $\\not\\curlyvee$, and the set\nof consistent preferences over those worlds as $\\curlyvee$. Elements\nfrom those sets are written as $\\succsim \\in \\not\\curlyvee$ and\n$\\succeq \\in \\curlyvee$.\n\nOne way we could approach the problem is by trying to turn those\ninconsistent preferences consistent, i.e. constructing a function $t:\n\\not \\curlyvee \\mapsto \\curlyvee$ that takes an inconsistent preference\n$\\succsim$ and transforms it into a consistent preference $\\succeq$,\nwhile retaining as much of the original structure of the preference\nas possible (it would make little sense if we replaced the original\npreference relation with e.g. indifference over all options).\n\nFormally, we want to find for some given [distance\nmetric](https://en.wikipedia.org/wiki/Metric_\\(mathematics\\))\n$d: \\not \\curlyvee \\times \\curlyvee \\mapsto ℝ$ a function\n$t$ so that\n\n\n$$t= \\underset{t}{\\text{argmin }} d(\\succsim, t(\\succsim)) \\\\\n\\succeq=t(\\succsim)$$\n\n\nI call this function a __turner__, and sometimes call the results of\nthat function the __set of turnings__ (an element from that set is a\n__turning__). The names mostly chosen for not having been used yet in\nmathematics, as far as I know, and because I want to be a little extra.\n\nA solution to the problem of turning inconsistent preferences into\nconsistent ones then has these components:\n\n1. A mathematical structure for representing $\\not \\curlyvee$ and $\\curlyvee$\n\t* Inconsistent preferences over discrete options are represented via [directed graphs](https://en.wikipedia.org/wiki/Directed_graph)\n\t* Inconsistent preferences over [lotteries](https://en.wikipedia.org/wiki/Lottery_\\(probability\\)) of options are represented via\n\t\t* directed graphs over [probability simplices](https://en.wikipedia.org/wiki/Simplex)\n\t\t\t* potentially more exotic structures such as [graphons](https://en.wikipedia.org/wiki/Graphon) or results from [extremal graph theory](https://en.wikipedia.org/wiki/Extremal_graph_theory) might be relevant here, but I haven't investigated these in detail\n\t\t* vector fields on probability simplices\n\t\t* [graphs with edge weights](https://en.wikipedia.org/wiki/Graph_\\(discrete_mathematics\\)#Weighted_graph) in $ℝ$\n2. A specification for $t$\n\t* In the case of discrete options, I propose adding and removing edges from the directed graph\n\t* In the case of lotteries I don't have yet any clear proposals\n3. A specification for $d$\n\t* In the case of discrete options, I propose using the [graph edit distance](https://en.wikipedia.org/wiki/Graph_edit_distance)\n\t* In the case of lotteries I don't yet have any definite proposals\n\nRelated Work\n------------\n\nThis work is closely related to the investigations in [Aird & Shovelain\n2020](https://www.lesswrong.com/posts/ky988ePJvCRhmCwGo/using-vector-fields-to-visualise-preferences-and-make-them)\n(so closely that even though I believe I re-invented the\napproach independently, it might just be that I had read\ntheir work & simply forgotten it), and broadly related to\nthe value extrapolation framework outlined in [Armstrong\n2022](https://www.lesswrong.com/posts/i8sHdLyGQeBTGwTqq/value-extrapolation-concept-extrapolation-model-splintering).\n\nDiscrete Case\n--------------\n\nWhen we have discrete sets of worlds $W$, we can represent\nan inconsistent preference over those worlds by using a directed graph\n$G_{\\succsim}=(W,E_{\\succsim} \\subseteq W \\times W)$.\nThe presence of an edge $(w_1, w_2)$ would mean that $w_1 \\succsim w_2$,\nthat is $w_1$ is preferred to $w_2$.\n\nMathematically, then, $\\not \\curlyvee$ is the set of all possible\ngraphs with edges in $W \\times W$, that is\n$\\not \\curlyvee=\\{(W, E)| E \\in \\mathcal{P}(W \\times W))\\}$).\n\nThe consistent equivalent to an inconsistent preference\nrepresented by a directed graph would be a [path\ngraph](https://en.wikipedia.org/wiki/Path_graph)\n$G_{\\succeq}=(V, E_{\\succeq})$ over the same set of\n[vertices](https://en.wikipedia.org/wiki/Vertex_\\(graph_theory\\)) $W$.\nThe method for transforming $G_{\\succsim}$ into $G_{\\succeq}$ would be\nby adding/deleting the minimal number of vertices from $E_{\\succsim}$.\n\nMathematically, then $\\curlyvee$ is the set of transitive closures\nof all possible path graphs that are encode permutations of $W$; $\\curlyvee=\\{(V, E)^+ | E \\in σ(W)\\}$.\n\n### Example\n\nConsider the following directed graph:\n\n![A directed graph](https://niplav.github.io/img/turning/unbalanced_cycle.png \"A directed graph. It contains nodes {a, b, c, d, e, f, g} and edges a → b → c → e → f → g → b, c → d.\")\n\nHere, $W=\\{a,b,c,d,e,f,g\\}$.\n\nAn edge from $a$ to $b$ means that $a$ is preferred to $b$\n(short $a \\succsim b$). The absence of an edge between two\noptions means that those two options are, from the view of the agent,\n[incomparable](https://en.wikipedia.org/wiki/Comparability).\n\nIt violates the two von Neumann-Morgenstern axioms for discrete options:\n\n* Completeness is violated because for example options $d$ and $e$ are incomparable (and we don't merely have [indifference](https://en.wikipedia.org/wiki/Indifference_curve) between these options)\n* Transitivity is violated because of the $b → c → e → f → g → b$ loop\n\n\n\nA possible turned version of these preferences could then be the\nfollowing graph:\n\n![A messy graph.](https://niplav.github.io/img/turning/turnubc_hyp_trans.png \"A messy graph. Vertices {a, b, c, d, e, f, g, h}. Edges are the transitive closure over the complete order a → b → c → d → e → f → g.\")\n\nThis graph looks quite messy, but it's really just the [transitive\nclosure](https://en.wikipedia.org/wiki/transitive_closure) of this graph:\n\n![A path graph.](https://niplav.github.io/img/turning/turnubc_hyp.png \"A path graph. Vertices again {a, b, c, d, e, f, g, h}. Edges are a → b → c → d → e → f → g.\")\n\nWhether this is the \"right\" way to turn the previous inconsistent\npreferences depends on the choice of distance metric we would like to use.\n\n### Resolving Inconsistencies\n\nIn some sense, we want to change the inconsistent preferences as little\nas possible; the more we modify them, the more displayed preferences we\nhave to remove or change. Since the presence or absence of preferences\nis encoded by the presence or absence of edges on the graph, removing\nedges or adding new edges is equivalent to removing or adding preferences\n(at the moment, we do *not* consider adding or removing vertices: we\nstay firmly inside the agent's [ontology](https://en.wikipedia.org/wiki/Ontology_\\(information_science\\))/world model).\n\nLuckily, there is a concept in computer science called the graph-edit\ndistance: a measure for the difference between two graphs.\n\nThe set of possible editing operations on the graph varies, e.g. Wikipedia lists\n\n> * __vertex insertion__ to introduce a single new labeled vertex to a graph.\n* __vertex deletion__ to remove a single (often disconnected) vertex from a graph.\n* __vertex substitution__ to change the label (or color) of a given vertex.\n* __edge insertion__ to introduce a new colored edge between a pair of vertices.\n* __edge deletion__ to remove a single edge between a pair of vertices.\n* __edge substitution__ to change the label (or color) of a given edge.\n\n*—[English Wikipedia](), [“Graph Edit Distance”](https://en.wikipedia.org/wiki/Graph_Edit_Distance), 2021*\n\nSince we do not have labels on the edges of the graph, and have disallowed\nthe deletion or insertion of vertices, this leaves us with the graph\nedit distance that uses edge insertion and edge deletion.\n\nWe can then write a simple pseudocode algorithm for\n$\\succeq=f(\\succsim)$:\n\n\tturn(G≿=(W, E≿)):\n\t\tmindist=∞\n\t\tfor L in perm(W):\n\t\t\tL=trans_closure(L)\n\t\t\tdist=ged(G≿, R)\n\t\t\tif dist<mindist:\n\t\t\t\tR=L\n\t\t\t\tmindist=dist\n\t\treturn R\n\nwhere `perm(W)` is the set of\n[permutations](https://en.wikipedia.org/wiki/Permutation) on `W`,\n`trans_closure(G)` is the transitive closure of a graph `G`, and `ged(G1,\nG2)` is the graph edit distance from `G1` to `G2`.\n\nOr, mathematically,\n\n$$R=\\underset{R \\in σ(W)}{\\text{argmin }}\\text{GED}(R^+, G_{\\succsim}))$$\n\n#### Implementation\n\nImplementing this in Python 3 using the [networkx](http://networkx.github.io/)\nlibrary turns out to be easy:\n\n\timport math\n\timport networkx as nx\n\timport itertools as it\n\n\tdef turn(graph):\n\tmindist=math.inf\n\t\tworlds=list(graph.nodes)\n\t\tfor perm in it.permutations(worlds):\n\t\t\tperm=list(perm)\n\t\t\tpathgraph=nx.DiGraph()\n\t\t\tfor i in range(0, len(worlds)):\n\t\t\t\tpathgraph.add_node(worlds[i], ind=i)\n\t\t\t# The transitive closure over this particular path graph\n\t\t\t# Simplify to nx.algorithms\n\t\t\tfor i in range(0, len(perm)-1):\n\t\t\t\tpathgraph.add_edge(perm[i], perm[i+1])\n\t\t\tpathgraph=nx.algorithms.dag.transitive_closure(pathgraph)\n\t\t\t# Compute the graph edit distance, disabling node insertion/deletion/substition and edge substitution\n\t\t\tedge_cost=lambda x: 1\n\t\t\tunaffordable=lambda x: 10e10\n\t\t\tsame_node=lambda x, y: x['ind']==y['ind']\n\t\t\tedge_matches=lambda x, y: True\n\t\t\tdist=nx.algorithms.similarity.graph_edit_distance(graph, pathgraph, node_match=same_node, edge_match=edge_matches, node_del_cost=unaffordable, node_ins_cost=unaffordable, edge_ins_cost=edge_cost, edge_del_cost=edge_cost)\n\t\t\tif dist<mindist:\n\t\t\t\tresult=pathgraph\n\t\t\t\tmindist=dist\n\t\treturn result\n\nWe can then test the function, first with a graph with a known best\ncompletion, and then with our [example from above](#Example).\n\nThe small example graph (top left) and its possible turnings are\n(all others):\n\n![A small example](https://niplav.github.io/img/turning/se_comp.png \"Four graphs, side-by side. Top left is a → b, c, top right is a → b → c, a → c, bottom left is a → c → b, a → b, bottom right is c → a → b, c → b.\")\n\n\t>>> smallworld=['a', 'b', 'c']\n\t>>> smallgraph=nx.DiGraph()\n\t>>> for i in range(0, len(smallworld)):\n\t...     smallgraph.add_node(smallworld[i], ind=i)\n\t>>> smallgraph.add_edges_from([('a', 'b')])\n\t>>> smallre=turn(smallworld, smallgraph)\n\t>>> smallre.nodes\n\tNodeView(('a', 'b', 'c'))\n\t>>> smallre.edges\n\tOutEdgeView([('a', 'b'), ('a', 'c'), ('b', 'c')])\n\nThis looks pretty much correct.\n\n\t>>> mediumworld=['a', 'b', 'c', 'd', 'e', 'f', 'g']\n\t>>> mediumgraph=nx.DiGraph()\n\t>>> for i in range(0, len(mediumworld)):\n\t...     mediumgraph.add_node(mediumworld[i], ind=i)\n\t>>> mediumgraph.add_edges_from([('a', 'b'), ('b', 'c'), ('c', 'd'), ('c', 'e'), ('e', 'f'), ('f', 'g'), ('g', 'b')])\n\t>>> mediumres=turn(mediumworld, mediumgraph)\n\t>>> mediumres.nodes\n\tNodeView(('a', 'b', 'c', 'd', 'e', 'f', 'g'))\n\t>>> mediumres.edges\n\tOutEdgeView([('a', 'b'), ('a', 'c'), ('a', 'd'), ('a', 'e'), ('a', 'f'), ('a', 'g'), ('b', 'c'), ('b', 'd'), ('b', 'e'), ('b', 'f'), ('b', 'g'), ('c', 'd'), ('c', 'e'), ('c', 'f'), ('c', 'g'), ('d', 'e'), ('d', 'f'), ('d', 'g'), ('e', 'f'), ('e', 'g'), ('f', 'g')])\n\nThis is actually equal to the hypothesized solution from above (below\nis the non-transitive-closure version):\n\n![A path graph.](https://niplav.github.io/img/turning/turnubc_hyp.png \"A path graph. Vertices again {a, b, c, d, e, f, g, h}. Edges are a → b → c → d → e → f → g.\")\n\n#### Problems with This Method and its Algorithm\n\nThis solution has some glaring problems.\n\n##### Speed (or the Lack Thereof)\n\nSome of you might have noticed that this algorithm is *somewhat\ninefficient* (by which I mean *absolutely infeasible*).\n\nSince we iterate through the permutations of $W$, the runtime is\n$\\mathcal{O}(|W|!)$ (with the added \"benefit\" of additionally computing\nthe [NP-complete](https://en.wikipedia.org/wiki/NP-completeness)\ngraph edit distance inside of the loop, which is also\n[APX](https://en.wikipedia.org/wiki/APX)-hard to approximate).\n\nPossible better approaches would involve finding the\nlongest subgraph that is a path graph, or the [spanning\ntree](https://en.wikipedia.org/wiki/Spanning_tree), perhaps the\n[transitive reduction](https://en.wikipedia.org/wiki/Transitive_reduction)\nis helpful, or maybe the [feedback arc\nset](https://en.wikipedia.org/wiki/Feedback_arc_set)?\n\n\n\n##### Non-Unique Results\n\nAnother, smaller problem is that the algorithm often doesn't have a unique\nresult, as seen in the small example [above](#Resolving-Inconsistencies).\n\nWe can compute the set of all possible turnings with some trivial\nchanges to the algorithm:\n\n\tturn_all(G≿=(W, E≿)):\n\t\tmindist=∞\n\t\tR=∅\n\t\t[…]\n\t\t\tif dist<mindist:\n\t\t\t\tR={L}\n\t\t\t\tmindist=dist\n\t\t\telse if dist==mindist:\n\t\t\t\tR=R∪{L}\n\t\treturn R\n\nand its implementation\n\n\tdef turn_all(graph):\n\t\tresults=set()\n\t\t[…]\n\t\t\tif dist<mindist:\n\t\t\t\tresults=set([pathgraph])\n\t\t\t\tmindist=dist\n\t\t\telif dist==mindist:\n\t\t\t\tresults.add(pathgraph)\n\t\treturn results\n\nThe results, with the small example, are as expected:\n\n\t>>> turnings=list(turn_all(smallworld, smallgraph))\n\t>>> len(turnings)\n\t3\n\t>>> turnings[0].edges\n\tOutEdgeView([('a', 'b'), ('a', 'c'), ('b', 'c')])\n\t>>> turnings[1].edges\n\tOutEdgeView([('a', 'b'), ('c', 'a'), ('c', 'b')])\n\t>>> turnings[2].edges\n\tOutEdgeView([('a', 'c'), ('a', 'b'), ('c', 'b')])\n\n![A small example](https://niplav.github.io/img/turning/se_comp.png \"Four graphs, side-by side. Top left is a → b, c, top right is a → b → c, a → c, bottom left is a → c → b, a → b, bottom right is c → a → b, c → b.\")\n\nFor the big example, after waiting a while for the solution:\n\n\t>>> turnings=list(turn_all(mediumworld, mediumgraph))\n\t>>> len(turnings)\n\t49\n\nI will not list them all, but these are less than the $7!=5040$\npossible options.\n\nThis brings up an interesting question: As we have more and more\nelaborate inconsistent preferences over more worlds, does it\nbecome more likely that they have a unique consistent preference\nthey can be turned to? Or, in other words, if we make the graphs\nbigger and bigger, can we expect the fraction of inconsistent\npreferences with a unique turning to grow or shrink (strictly)\n[monotonically](https://en.wikipedia.org/wiki/Monotonic_function)? Or\nwill it just oscillate around wildly?\n\nMore formally, if we define $\\mathcal{G}_n$ as the set of graphs\nwith $n$ nodes, and $\\mathcal{U}_n=\\{G \\in \\mathcal{G}_n | 1=|\\text{turn_all}(G)|\\}$\nas the set of graphs with $n$ nodes that\nhave unique path graphs associated with them.\n\nWe can further define the set of all graphs wwith $n$ nodes\nwith $m$ turnings as\n$\\mathcal{T}{n,m}=\\{G \\in \\mathcal{G}_n | m=|\\text{turn_all}(G)|\\}$\n(of which $\\mathcal{U}_n=\\mathcal{T}_{n, 1}$\nis just a special case).\n\nWe can call the size of the set of all turnings of a graph the\n__confusion__ of that graph/set of inconsistent preferences: If the\ngraph is already the transitive closure of a path graph, the size of\nthat set is (arguendo) 1: there are no other possible turnings. If the\ngraph contains no edges (with $n$ nodes), the confusion is maximal with\n$n!$, the preferences carry the minimal amount of meaning.\n\n###### Minimal and Maximal Number of Turnings\n\nThe minimal number of turnings a graph can have is 1, with a graph-edit\ndistance of 0: any transitive closure of a path graph satisfies this\ncriterion (if your preferences are already consistent, why change them\nto be more consistent?)\n\nHowever, those graphs aren't the only graphs with exactly one turning,\nconsider the following graph (left) and a possible turning (right)\n(with graph-edit distance 1; the changed edge is red, a nice opportunity\nfor some [rubrication](https://gwern.net/Red)):\n\n![Image of two graphs, left has edges a→ b→ c→ d, a→ c, b→ d, d→ a, right graph is the same except d→ a is now a→ d.](https://niplav.github.io/img/turning/counter_comp.png \"Image of two graphs, left has edges a→ b→ c→ d, a→ c, b→ d, d→ a, right graph is the same except d→ a is now a→ d.\")\n\nOne can easily see that it has exactly one turning, and checking with\nthe code confirms:\n\n\t>>> counter=nx.DiGraph()\n\t>>> counterworld=['a', 'b', 'c', 'd']\n\t>>> for i in range(0, len(smallworld)):\n\t...\tsmallgraph.add_node(smallworld[i], ind=i)\n\t>>> counter.add_edges_from([('a', 'b'), ('b', 'c'), ('c', 'd'), ('a', 'c'), ('b', 'd'), ('d', 'a')])\n\t>>> counterres=list(turn_all(counter))\n\t>>> len(counterres)\n\t>>> >>> counterres[0].edges\n\tOutEdgeView([('a', 'b'), ('a', 'c'), ('a', 'd'), ('b', 'c'), ('b', 'd'), ('c', 'd')])\n\nFor a graph with $n$ nodes the maximal number of turnings it is\nupper-bounded by $n!$, and a sufficient condition for the graph to\nhave that many turnings is when the graph is the union of a set of\n[complete digraphs](https://en.wikipedia.org/wiki/Complete_graph) with\ndisjoint nodes. For example the graph with 4 nodes and no edges has 24\npossible turnings, as does the graph with 4 nodes and two edges $\\{(1,2),\n(2,1)\\}$.\n\nWe can prove this inductively: When considering a node-labeled graph\nwith $n$ nodes and no edges, the graph edit distance to any path\ngraph variant of that graph is the same, because we always have to\nadd $n-1+n-2+n-3 \\dots 1=\\frac{n-1+(n-1)^2}{2}$ edges to reach\nany transitive closure of a path graph (by the [sum of any arithmetic\nprogression](https://en.wikipedia.org/wiki/Arithmetic_progression#Sum)).\nLet not $G^{\\circ}$ be a graph with $n$ nodes that is solely the\nunion of complete digraphs with disjoint nodes. When we now pick two nodes\n$u$ and $v$ from $G^{\\circ}$ and add the edges $\\{(u,v), (v,u)\\}\n\\cup \\{(v, x)|(u,x) \\in E^{\\circ}\\} \\cup \\{(u, y)|(v,x) \\in E^{\\circ}\\}\\}\n\\cup \\{(x, y)|(u,x) \\in E^{\\circ}, (v,y) \\in E^{\\circ}\\}$ (that is,\nwe connect $u$ and $v$, and all their neighbors) to $G^{\\circ}$,\nwe have necessarily increased the graph-edit distance to any path graph\nby the same amount, we have symmetrically added edge-pairs that need to\nbe broken in either direction.\n\n### Questions\n\nOne can now pose several (possibly distracting) questions:\n\n* Does it matter whether we give `turn` a graph $G$ or the transitive closure of $G$?\n* Is there a more efficient algorithm to compute the turning?\n\t* Can it at least be made exponential?\n\t* Can we exploit the fact that we're always computing the graph-edit distance to a path-graph?\n* As we add more options to our inconsistent preferences, do they become more likely to turn uninuely?\n\t* That is: Does it hold that $\\frac{|\\mathcal{U}_n|}{|\\mathcal{G}_n|}<\\frac{|\\mathcal{U}_{n+1}|}{|\\mathcal{G}_{n+1}|}$?\n\t* It should be possible to check this for small cases.\n\n#### Number of Turnings for $\\mathcal{G}_n$\n\n* In general, how does the size of $\\mathcal{U}_n$ develop? What about $\\mathcal{T}_{n,2}$, or in general $\\mathcal{T}_{n,m}$?\n\t* Does the average number of turnings for inconsistent preferences converge to a specific number?\n\t* That is, what is $\\lim_{n \\rightarrow \\infty} \\frac{1}{\\mathcal{G}_n} \\sum_{i=1}^{n} \\mathcal{T}_{n,i}$?\n\t* I predict [20% on the number monotonically increasing](https://predictionbook.com/predictions/208357), [50% on monotonically decreasing](https://predictionbook.com/predictions/208358) and [30% on showing no clear pattern](https://predictionbook.com/predictions/208359).\n\nWe can check these empirically! While it would be nice to prove anything\nabout them, it's much nicer to investigate them computationally. This is\npretty straightforward: For increasing $n$, generate $\\mathcal{G}_n$,\nfor every $G \\in \\mathcal{G}_n$, compute $|\\text{turn_all}(G)|$, save\nthe data in a file somewhere, and do interesting things with that data.\n\nIn code, we first generate all directed graphs with $n$ nodes with a\nrecursive function\n\n\tdef all_directed_graphs(n):\n\t\tif n<=0:\n\t\t\treturn [nx.DiGraph()]\n\t\tgraphs=all_directed_graphs(n-1)\n\t\tnewgraphs=[]\n\t\tfor g in graphs:\n\t\t\tg.add_node(n, ind=n)\n\t\t\tfor tosubset in powerset(range(1, n+1)):\n\t\t\t\tfor fromsubset in powerset(range(1, n)):\n\t\t\t\t\tgnew=g.copy()\n\t\t\t\t\tfor element in tosubset:\n\t\t\t\t\t\tgnew.add_edge(n, element)\n\t\t\t\t\tfor element in fromsubset:\n\t\t\t\t\t\tgnew.add_edge(element, n)\n\t\t\t\t\tnewgraphs.append(gnew)\n\t\treturn newgraphs\n\nand start turning:\n\n\tmax=16\n\tfor i in range(0,max):\n\t\tgraphs=turn.all_directed_graphs(i)\n\t\tfor g in graphs:\n\t\t\tprint('{0},{1},\"{2}\"'.format(i, len(turn.turn_all(g)), g.edges))\n\nHowever, my computer quickly freezes and I find out that this is a lot\nof graphs:\n\n\t>>> [len(list(all_directed_graphs(i))) for i in range(0,5)]\n\t[1, 2, 16, 512, 65536]\n\nSo the number directed graphs with 5 nodes would be\n$2^{32}=4294967296$, far too many for my puny laptop. But\ninstead of generating them all, one can just generate a\nrandom sample and test on that, using the [Erdős–Rényi\nmodel](https://en.wikipedia.org/wiki/Erdős-Rényi_model),\nfor which networkx has the helpful function\n`generators.random_graphs.gnp_random_graph` (Wikipedia informs us that\n\"In particular, the case $p=\\frac{1}{2}$  corresponds to the case\nwhere all $2^{\\binom {n}{2}}$ graphs on $n$ vertices are chosen\nwith equal probability.\"). We have to randomly add reflexive edges (not\nincluded in the model, it seems) with probability $\\frac{1}{2}$ each,\nand labels for the nodes, and then we're good to go:\n\n\tsamples=256\n\tfor i in range(5,lim):\n\t\tfor j in range(0,samples):\n\t\t\tg=nx.generators.random_graphs.gnp_random_graph(i, 0.5, directed=True)\n\t\t\tfor n in g.nodes:\n\t\t\t\tg.add_node(n, ind=n)\n\t\t\t\tif random.random()>=0.5:\n\t\t\t\t\tg.add_edge(n,n)\n\t\t\tprint('{0},{1},\"{2}\"'.format(i, len(turn.turn_all(g)), g.edges))\n\nWe now run the script in the background, happily collecting data for us\n(`python3 collect.py >.https://niplav.github.io/../data/turnings.csv &`), and after a nice\nround of editing this text go back and try to make sense of the data,\nwhich runs squarely counter my expectations:\n\n\t>>> import pandas as pd\n\t>>> df=pd.read_csv('data/turnings.csv')\n\t>>> df.groupby(['0']).mean()\n\t           1\n\t0\n\t1   1.000000\n\t2   1.875000\n\t3   3.941406\n\t4   9.390289\n\t5  21.152344\n\t6  39.885246\n\nIt seems like the mean number of turnings actually increases\nwith the graph size! Surprising. I'm also interested in the\nexact numbers: Why *exactly* 3.390289… for the graphs with 4\nnodes? What is so special about that number‽ (Except it being\nthe [longitude](https://en.wikipedia.org/wiki/Longitude)\nof the [Cathedral Church of\nChrist](https://en.wikipedia.org/wiki/Cathedral_Church_of_Christ)\nin Lagos).\n\nLooking at unique turnings turns (hehe) up further questions:\n\n\t>>> def uniqueratio(g):\n\t...     return len(g.loc[g['1']==1])/len(g)\n\t...\n\t>>> df.groupby(['0']).apply(uniqueratio)\n\t0\n\t1    1.000000\n\t2    0.125000\n\t3    0.089844\n\t4    0.055542\n\t5    0.050781\n\t6    0.016393\n\tdtype: float64\n\t>>> def uniques(g):\n\t...     return len(g.loc[g['1']==1])\n\t>>> df.groupby(['0']).apply(uniques)\n\t0\n\t1       2\n\t2       2\n\t3      46\n\t4    3640\n\nVery much to my surprise, searching for \"2,2,46,3640\" [in the\nOEIS](https://oeis.org/search?q=2%2C2%2C46%2C3640&sort=&language=english&go=Search)\nyields *no results*, even though the sequence really looks like something\nthat would already exist! (I think it has a specifically graph-theoretic\n\"feel\" to it). But apparently not so, I will submit it soon.\n\n\n\nI omit the number of unique turnings for 5 and 6, for obvious reasons\n(I also believe that the ratio for 6 is an outlier and should not be\ncounted). The number of unique resolutions for the graph with 1 node\nmakes sense, though: Removing the reflexive edge should count as\none edge action, but the graph only has one unique resolution:\n\n\t>>> df.loc[df['0']==1]\n\t   0  1        []\n\t0  1  1        []\n\t1  1  1  [(1, 1)]\n\n\n\nEncoding Inconsistencies\n------------------------\n\n### Theory\n\nAssuming that we have a set of axioms that describe which preferences\nare consistent and which are inconsistent, for the purposes of this\ntext, we want to ideally find a set $\\not \\curlyvee$ of mathematical\nstructures that\n\n1. can represent preferences that violate each possible subset of those axioms.\n\t1. Each inconsistent preference should have exactly one element of $\\not \\curlyvee$ that represents it\n2. has a strict subset $\\curlyvee \\subset \\not \\curlyvee$ so that $\\curlyvee$ can represent only consistent preferences.\n\n### Discrete Case\n\nThe two relevant von Neumman-Morgenstern axioms are completeness and\ntransitivity, with a directed graph one can also represent incompleteness\nand intransitivity.\n\n#### Incompleteness\n\nIncompleteness (or incomparability) between two options $w_1, w_2$\ncan be represented by not specifying an edge between the two options,\nthat is $(w_1, w_2) \\not \\in E, (w_2, w_1) \\not \\in E$.\n\n![](https://niplav.github.io/img/turning/incomplete.png)\n\n#### Intransitivity\n\nIntransitivity can be represented by cycles in the graph:\n\n![](https://niplav.github.io/img/turning/intransitive.png)\n\n#### Non-Encodable Inconsistencies\n\nWith option set $\\{a,b\\}$ have preference $a \\succsim b$, with\noption set $\\{a,b,c\\}$ have preferences\n$b \\succsim a, a \\succsim c, b \\succsim c$.\n\n### Continuous Case\n\n#### Incompleteness\n\n* Minima/maxima in the vector field\n* Discontinuities\n* Undifferentiable points\n\n#### Intransitivity\n\nCurl in the vector field?\n\n#### Discontinuity\n\nCan only exist with incompleteness?\n\n#### Dependence\n\n### Discussion\n\nThis leads to an interesting ethical consideration: is it a larger change\nto a preference relation to add new information or remove information?\n\nIt is discussed how to incorporate those weights into an algorithm for\nminimally transforming $G_{\\succsim}$ into $G_{\\succeq}$.\n\nContinuous Case\n----------------\n\n### Vector Fields over Probability Simplices\n\nVector field over the probability simplex over the options (representing\nlocal preferences over lotteries).\n\n#### Resolving Inconsistencies\n\nFind mapping from vector field to another that makes the vector field\nconsistent by minimizing the amount of turning/shrinking the vectors\nhave to perform.\n\n### Graphons\n\n?\n\nLook into extremal graph theory.\n\nImplications for AI Alignment\n------------------------------\n\n> I've seen six cities fall for this  \nmathematics with incompetence  \nred flags stand among the trees  \nrepugnant symphonies  \na billionaires tarantula just ate the ceiling  \nthinking it was yet another floor\n\n*—[Patricia Taxxon](http://patriciataxxon.bandcamp.com/), [“Hellbulb”](https://patriciataxxon.bandcamp.com/track/hellbulb) from [“Gelb”](https://patriciataxxon.bandcamp.com/album/gelb), 2020*\n\n### Ambitious Value Learning\n\nLearn human values, check if known inconsistencies are encoded (to ensure\nlearning at the correct level of abstraction), then make consistent.\n\n### Ontological Crises\n\n> Furthermore, there remain difficult philosophical problems. We have\nmade a distinction between the agent’s uncertainty about which model\nis correct and the agent’s uncertainty about which state the world is\nin within the model. We may wish to eliminate this distinction; we\ncould specify a single model, but only give utilities for some states\nof the model. We would then like the agent to generalize this utility\nfunction to the entire state space of the model.\n\n*—Peter de Blanc, [“Ontological Crises in Artificial Agents’ Value Systems”](https://niplav.github.io/doc/cs/ai/alignment/ontological_crises/ontological_crises_in_artificial_agents_value_systems_de_blanc_2011.pdf), 2010*\n\nIf you know a mapping between objects from human to AI ontology, you\ncould find the mapping from the (consistent) human probability simplex\nto the AI simplex?\n\n#### Discrete Case\n\nA node splits in two or more, or two or more nodes get merged. If the\nthen resulting graph isn't a path graph, it can be turned with the\nmethod described above.\n\nFurther Questions\n------------------\n\n* Does every graph $G$ have a unique graph $G'$ so that $G$ is the transitive closure of $G'$?\n* There is something interesting going on with lattices (?) over individual transitivity operations\n\nAcknowledgements\n------------------------\n\nThanks to Miranda Dixon-Luinenburg for finding some typos."
    },
    "voteCount": 8,
    "forceInclude": true
  },
  {
    "_id": "G6BenMvcFR5CEC9uc",
    "url": null,
    "title": "Definition Practice: Applied Rationality",
    "slug": "definition-practice-applied-rationality",
    "author": "ChristianKl",
    "question": true,
    "tags": [
      {
        "name": "Rationality"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "In the exercise of defining what we mean by a term, we explore our assumptions about the nature of the term and what it’s pointing at. Sometimes this leads us to get a better handle on a concept. Other times, it shows us our confusion.\n\nAs an example of the value of definitions, [YCombinator considers](https://www.ycombinator.com/library/4b-how-to-pitch-your-company) it important for founders to be able to clearly and concisely say a company's purpose, and that the ability to do so is indicative of plausible success..\n\nHere are two exercises:\n\n1.  Write a paragraph to define the term *applied rationality.*\n2.  Write a sentence to define the term *applied rationality.*\n\nI encourage you to write your definitions before reading what others wrote. While I suggest that you first attempt to define the term in a paragraph and then move to the sentence variation, you can also change the order if that seems easier.\n\nPlease use the answer feature for your answer to these exercises, and the comment function for other needs."
    },
    "voteCount": 3,
    "forceInclude": true
  },
  {
    "_id": "caB6ApqG7rSFayoaq",
    "url": null,
    "title": "What are the simplest questions in applied rationality where you don't know the answer to? ",
    "slug": "what-are-the-simplest-questions-in-applied-rationality-where",
    "author": null,
    "question": true,
    "tags": [
      {
        "name": "Rationality"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "In the podcast between Spencer Greenberg and Buck Shlegeris, [Taking pleasure in being wrong](https://open.spotify.com/episode/4Q54moES6YvjCEyOORRgX3), Buck says:\n\nI think that when you are learning subjects something that you should really be taken your eye out for is the simplest question in the field that you don't know the answer to.\n\n> I think a lot of the time people try to learn physics or something and their approach is as quickly as possible to answer hard questions about complicated subjects. And I think that's what I thought was cool when I was younger. They delighted at questions that were at the limit of fanciness that they could possibly answer and it feels to me now that it is a lot more productive to seek out questions that are as simple sounding as possible while still being really hard to answer. Or that still demonstrate that there's something you don't understand about the subject.\n> \n> \\[...\\] \n> \n> It seems like we should be seeking out these most basic questions in the hope of finding holes in the foundation of our knowledge.\n\nIf we apply that approach to applied rationality, what questions do you have that seem to be simple but where you don't know the answer?"
    },
    "voteCount": 10,
    "forceInclude": true
  },
  {
    "_id": "xrJjuFfBFD33eHsiP",
    "url": null,
    "title": "Are there practical exercises for developing the Scout mindset?",
    "slug": "are-there-practical-exercises-for-developing-the-scout",
    "author": null,
    "question": true,
    "tags": [
      {
        "name": "Exercises / Problem-Sets"
      },
      {
        "name": "Rationality"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "I'm thinking about doing a LessWrong meetup around the Scout mindset. I prefer to have meetups where there are a lot of two-person exercises as those are really good to get nerds who have trouble with normal small talk to connect to each other in addition to often also being useful for developing skills over lecturing the whole time. Can anyone think of good exercises that could be done at a Scout mindset meetup?"
    },
    "voteCount": 3,
    "forceInclude": true
  },
  {
    "_id": "CjFZeDD6iCnNubDoS",
    "url": null,
    "title": "Humans provide an untapped wealth of evidence about alignment",
    "slug": "humans-provide-an-untapped-wealth-of-evidence-about",
    "author": "TurnTrout",
    "question": false,
    "tags": [
      {
        "name": "AI"
      },
      {
        "name": "Human Values"
      },
      {
        "name": "Ontology"
      },
      {
        "name": "Shard Theory"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "*This post has been recorded as part of the LessWrong Curated Podcast, and can be listened to on* [*Spotify*](https://open.spotify.com/episode/0jpI7LLNzKsn6lwrsoDCc9)*,* [*Apple Podcasts*](https://podcasts.apple.com/us/podcast/humans-provide-an-untapped-wealth-of-evidence/id1630783021?i=1000575990542)*, and* [*Libsyn*](https://five.libsyn.com/episodes/view/23991000)*.*\n\n* * *\n\n**TL;DR:** To even consciously consider an alignment research direction, [you should have evidence](https://www.readthesequences.com/Privileging-The-Hypothesis) to locate it as a promising lead. As best I can tell, many directions seem interesting but do not have strong evidence of being “entangled” with the alignment problem such that I expect them to yield significant insights. \n\nFor example, “we can solve an easier version of the alignment problem by first figuring out how to build an AI which maximizes the number of real-world diamonds” has intuitive appeal and plausibility, but this claim doesn’t *have* to be trueand this problem does not *necessarily* have a natural, compact solution. In contrast, there do *in fact* exist humans who care about diamonds. Therefore, there are guaranteed-to-exist alignment insights concerning the way people come to care about e.g. real-world diamonds.\n\n*“Consider how humans navigate the alignment subproblem you’re worried about” is a habit which I (TurnTrout) picked up from Quintin Pope. I wrote the post, he originated the tactic. *\n\n* * *\n\n> A simplified but still very difficult open problem in[ AI alignment](https://arbital.com/p/ai_alignment/) is to state an unbounded program implementing a[ diamond maximizer](https://arbital.com/p/diamond_maximizer/) that will turn as much of the physical universe into diamond as possible. The goal of \"making diamonds\" was chosen to have a crisp-seeming definition for our universe (the amount of diamond is the number of carbon atoms covalently bound to four other carbon atoms). If we can crisply define exactly what a 'diamond' is, we can avert issues of trying to convey[ complex values](https://arbital.com/p/complexity_of_value/)into the agent.\n> \n> [Ontology identification problem](https://arbital.com/p/ontology_identification/), Arbital\n\nI find this problem interesting, both in terms of wanting to know how to solve a reframed version of it, and in terms of what I used to think about the problem. I usedto^[\\[1\\]](#fn3crz71o4jhu)^ think, “yeah, ‘diamond’ is relatively easy to define. Nice [problem relaxation](https://www.lesswrong.com/posts/JcpwEKbmNHdwhpq5n/problem-relaxation-as-a-tactic).” It felt like the diamond maximizer problem let us focus on the challenge of making the AI’s values bind to *something at all *which we actually intended (e.g. diamonds), in a way that’s robust to ontological shifts and that doesn’t collapse into wireheading or tampering with e.g. the sensors used to estimate the number of diamonds.\n\nAlthough the details are mostly irrelevant to the point of this blog post, the Arbital article suggests some solution ideas and directions for future research, including:\n\n1.  Scan [AIXI-*tl*](https://archive.org/details/arxiv-cs0004001)’s Turing machines and locate diamonds within their implicit state representations.\n2.  Given how [inaccessible](https://ai-alignment.com/inaccessible-information-c749c6a88ce?gi=11e30aac9afb) we expect AIXI*-tl*’s representations to be by default, have AIXI-*tl *just consider a Turing-complete hypothesis space which uses more interpretable representations.\n3.  “Being able to describe, in purely theoretical principle, a prior over epistemic models that have at least two levels and can switch between them in some meaningful sense”\n\nDo you notice anything *strange* about these three ideas? Sure, the ideas don’t seem workable, but they’re good initial thoughts, right?\n\nThe problem *isn’t* that the ideas aren’t clever enough. Eliezer is pretty dang clever, and these ideas are reasonable stabs given the premise of “get some AIXI variant to maximize diamond instead of reward.”\n\nThe problem *isn’t* that it’s impossible to specify a mind which cares about diamonds. We already know that there are intelligent minds who value diamonds. You might be dating one of them, or you might even *be* one of them! Clearly, the genome + environment jointly specify certain human beings who end up caring about diamonds. \n\nOne problem is [*where is the evidence required to locate these ideas*](https://www.readthesequences.com/Privileging-The-Hypothesis)? Why should I even find myself thinking about diamond maximization and AIXI and Turing machines and utility functions in this situation? It’s not that there’s *no *evidence. For example, utility functions [ensure the agent can’t be exploited in some dumb ways](https://www.lesswrong.com/posts/RQpNHSiWaXTvDxt6R/coherent-decisions-imply-consistent-utilities). But I think that the supporting evidence is not *commensurate* with the specificity of these three ideas or with the specificity of the “ontology identification” problem framing.\n\nHere’s an exaggeration of how these ideas feelto me when I read them: \n\n> “I lost my phone”, you tell your friend. \n> \n> They ask, “Have you checked [`Latitude: -34.44006, Longitude: -64.61333`](https://gps-coordinates.org/my-location.php?lat=-34.44006&lng=-64.61333)?” \n> \n> Uneasily, you respond: “Why would I check there?” \n> \n> Your friend shrugs: “Just seemed promising. And it’s on land, it’s not in the ocean. Don’t worry, I incorporated evidence about where you probably lost it.”\n\nI [recently made a similar point](https://www.lesswrong.com/posts/dqSwccGTWyBgxrR58/turntrout-s-shortform-feed?commentId=CXdcb9sMLkgLANrTv#CXdcb9sMLkgLANrTv) about [Cooperative Inverse Reinforcement Learning](https://arxiv.org/abs/1606.03137): \n\n> *Against CIRL* as a special case of *against quickly jumping into highly specific speculation while ignoring empirical embodiments-of-the-desired-properties.*\n> \n> In the context of \"how do we build AIs which help people?\", asking \"does CIRL solve corrigibility?\" is hilariously unjustified.[ By what evidence](https://www.readthesequences.com/Privileging-The-Hypothesis) have we located such a specific question? We have assumed there is an achievable \"corrigibility\"-like property; we have assumed it is good to have in an AI; we have assumed it is good in a similar way as \"helping people\"; we have elevated CIRL in particular as a formalism worth inquiring after. \n> \n> But this is ***not the first question to ask**, *when considering \"sometimes people want to help each other, and it'd be great to build an AI which helps us in some way.\" Much better to start with *existing* generally intelligent systems (humans) which *already* sometimes act in the way you want (they help each other) and ask after the ***guaranteed-to-exist reason*** why this empirical phenomenon happens.\n\nNow, if you are confused about a problem, it can be better to explore *some *guesses than no guesses—perhaps it’s better to think about Turing machines than to stare helplessly at the wall (but perhaps not). Your best guess may be wrong (e.g. write a utility function which scans Turing machines for atomic representations of diamonds), but you sometimes still learn something by spelling out the implications of your best guess (e.g. the ontology identifier stops working when AIXI Bayes-updates to non-atomic physical theories). This can be productive, as long as you keep in mind the wrongness of the concrete guess, so as to not become anchored on that guess or on the framing which originated it (e.g. build a diamond *maximizer*).\n\nHowever, in this situation, I want to look elsewhere. When I confront a confusing, difficult problem (e.g. how do you create a mind which cares about diamonds?), I often first look at reality (e.g. are there any existing minds which care about diamonds?). Even if I have *no idea *how to solve the problem, if I can find an existing mind which cares about diamonds, then *since that mind is **real**,* that mind has a[*guaranteed-to-exist*](https://en.wikipedia.org/wiki/Biomimetics) *causal mechanistic play-by-play origin story* for why it cares about diamonds. I thereby anchor my thinking to reality; reality is sturdier than “what if” and “maybe this will work”; many human minds *do *care about diamonds. \n\nIn addition to “there’s a guaranteed causal story for humans valuing diamonds, and not one for AIXI valuing diamonds”, there’s a second benefit to understanding how human values bind to the human’s beliefs about real-world diamonds. This second benefit is practical: I’m pretty sure the way that *humans *come to care about diamonds has nearly nothing to do with the ways AIXI-*tl *might be motivated to maximize diamonds. This matters, because I expect that the first AGI’s value formation will be *far* more mechanistically similar to within-lifetime human value formation, than to AIXI-*tl*’svalue alignment dynamics.\n\nNext, it *can *be true that the existing minds are too hard for us to understand in ways relevant to alignment. One way this could be true is that human values are a \"[mess](https://www.readthesequences.com/Terminal-Values-And-Instrumental-Values)\", that \"[our brains are kludges slapped together by natural selection.](https://www.readthesequences.com/Rationality-An-Introduction)\" If human value formation *were* sufficiently complex, with sufficiently many load-bearing parts such that each part drastically affects human alignment properties, then we might instead want to design simpler human-comprehensible agents and study *their* alignment properties. \n\nWhile I think that human *values *are complex, I think the evidence for human value *formation*’s essential complexity is surprisingly weak, all things reconsidered in light of modern, post-deep learning understanding. Still... maybe humans *are* too hard to understand in alignment-relevant ways!\n\nBut, I mean, come on. Imagine an alien^[\\[2\\]](#fn7qhggh3bak6)^ visited and told you:\n\n> Oh yeah, the AI alignment problem. We knocked that one out a while back. [Information inaccessibility of the learned world model](https://ai-alignment.com/inaccessible-information-c749c6a88ce?gi=11e30aac9afb)? No, I’m pretty sure [we didn’t solve that](https://www.lesswrong.com/posts/CQAMdzA4MZEhNRtTp/human-values-and-biases-are-inaccessible-to-the-genome), but we didn’t have to. We built this protein computer and trained it with, I forget actually, was it just what you would call “deep reinforcement learning”? Hm. Maybe it was more complicated, maybe not, I wasn’t involved. \n> \n> We *might* have hardcoded relatively crude reward signals that are basically defined over sensory observables, like a circuit which activates when their sensors detect a [certain kind of carbohydrate](https://en.wikipedia.org/wiki/Sugar). Scanning you, it looks like some of the protein computers ended up with *your values*, even. Small universe, huh?\n> \n> Actually, I forgot how we did it, sorry. And I can’t make guarantees that our approach scales beyond your intelligence level or across architectures, but maybe it does. I have to go, but here are a few billion of the trained protein computers if you want to check them out!\n\nIgnoring the weird implications of the aliens existing and talking to you like this, and considering only the alignment implications—*The absolute top priority of many alignment researchers should be figuring out how the hell the aliens got as far as they did*.^[\\[3\\]](#fnk1djcjup0qc)^ Whether or not you know if their approach scales to further intelligence levels, whether or not their approach seems easy to understand, you have learned that these computers are *physically possible, practically trainable entities*. These computers have definite existence and guaranteed explanations. Next to these actually existent computers, speculation like “maybe [attainable utility preservation](https://www.lesswrong.com/posts/75oMAADr4265AGK3L/attainable-utility-preservation-concepts) leads to cautious behavior in AGIs” is dreamlike, unfounded, and untethered.\n\nIf it turns out to be currently too hard to understand the aligned protein computers, then I want to keep coming back to the problem with each major new insightI gain. When I learned about [scaling laws](https://arxiv.org/abs/2001.08361), I should have rethought my picture of human value formation—Did the new insight knock anything loose? I should have checked back in when I heard about [mesa optimizers](https://www.alignmentforum.org/posts/pL56xPoniLvtMDQ4J/the-inner-alignment-problem), about the [Bitter Lesson](http://incompleteideas.net/IncIdeas/BitterLesson.html), about [the feature universality hypothesis](https://distill.pub/2020/circuits/zoom-in/#claim-3) for neural networks, about [natural abstractions](https://www.alignmentforum.org/posts/wuJpYLcMEBz4kcgAn/what-is-abstraction-1#Natural_Abstractions).\n\nBecause, given my life’s present ambition (solve AI alignment), that’s what it makes sense for me to do—at each major new insight, to reconsider my models^[\\[4\\]](#fnmpfloj2azs)^ of the *single known empirical example of general intelligences with values*, to scour the Earth for every possible scrap of evidence that humans provide about alignment. We may not get much time with human-level AI before we get to superhuman AI. But we get plenty of time with human-level humans, and we get plenty of time *being *a human-level intelligence. \n\nThe way I presently see it, [the godshatter of human values](https://www.lesswrong.com/posts/cSXZpvqpa9vbGGLtG/thou-art-godshatter)—the rainbow of desires, from friendship to food—is only [unpredictable](https://www.lesswrong.com/posts/34Gkqus9vusXRevR8/late-2021-miri-conversations-ama-discussion#YkhmywLQetjekM7e3) relative to a class of hypotheses which fail to predict the shattering.^[\\[5\\]](#fn7a5ti4623qb)^ But confusion is in the map, not the territory. I do not consider human values to be “unpredictable” or “weird”, I do not view them as a “hack” or a “kludge.” Human value formation may or may not be messy (although I presently think *not*). Either way, human values are, of course, part of our lawful reality. Human values are reliably produced by within-lifetime processes within the brain. This has an explanation, though I may be ignorant of it. Humans usually bind their values to certain objects in reality, like dogs. This, too, has an explanation.  \n\nAnd, to be clear, I don’t want to black-box outside-view extrapolate from the “human datapoint”; I don’t want to focus on thoughts like “Since alignment ‘works well’ for dogs and people, maybe it will work well for slightly superhuman entities.” I aspire for the kind of alignment mastery which lets me build a diamond-producing AI, or if that didn’t suit my fancy, I’d turn around and tweak the process and the AI would press green buttons forever instead, or—if I were playing for real—I’d align that system of mere circuitry with humane purposes.\n\nFor that ambition, the inner workings of those generally intelligent apes is *invaluable evidence* about the *mechanistic within-lifetime process by which those apes form their values, *and, more generally, about how intelligent minds can form values at all. What factors matter for the learned values, what factors don’t, and what we should do for AI. Maybe humans have special inductive biases or architectural features, and without those, they’d grow totally different kinds of values. But if that *were *true, wouldn’t that be important to know?\n\nIf I knew how to interpret the available evidence, I probably *would *understand how I came to weakly care about diamonds, and what factors were important to that process (which reward circuitry had to fire at which frequencies, what concepts I had to have learned in order to grow a value around “diamonds”, how precisely activated the reward circuitry had to be in order for me to end up caring about diamonds).\n\nHumans provide huge amounts of evidence, *properly interpreted*—and therein lies the grand challenge upon which I am presently fixated. In an upcoming post, I’ll discuss one particularly rich vein of evidence provided by humans.\n\n*Thanks to Logan Smith and Charles Foster for feedback. Spiritually related to but technically distinct from* [*The First Sample Gives the Most Information*](https://www.lesswrong.com/posts/sTwW3QLptTQKuyRXx/the-first-sample-gives-the-most-information)*.*\n\nEDIT: In this post, I wrote about the Arbital article's unsupported jump from \"Build an AI which cares about a simple object like diamonds\" to \"Let's think about ontology identification for AIXI-*tl.*\" The point is not that there is no valid reason to consider the latter, but that the jump, as written, seemed evidence-starved. For *separate* reasons, I currently think that ontology identification is unattractive in some ways, but this post isn't meant to argue against that framing in general. The main point of the post is that humans provide tons of evidence about alignment, by virtue of containing guaranteed -to-exist mechanisms which produce e.g. their values around diamonds.\n\nAppendix: One time I didn’t look for the human mechanism\n========================================================\n\nBack in 2018, I had [a clever-seeming idea](https://www.lesswrong.com/s/vLArRpNdkex68oem8/p/BMj6uMuyBidrdZkiD). We don’t know how to build an aligned AI; we want multiple tries; it would be great if we could build an AI which “[knows it may have been incorrectly designed](https://arbital.com/p/hard_corrigibility/)”; so why not have the AI simulate its probable design environment over many misspecifications, and then *not* do plans which tend to be horrible for most initial conditions. While I drew some inspiration from how I would want to reason in the AI’s place, I ultimately did not think thoughts like:\n\n> We know of a single group of intelligent minds who have ever wanted to be corrigible and helpful to each other. I wonder how that, in fact, happens? \n\nInstead, I was trying out clever, off-the-cuff ideas in order to solve e.g. Eliezer’s formulation of the [hard problem of corrigibility.](https://arbital.com/p/hard_corrigibility/) However, my idea and his formulation suffered a few disadvantages, including:\n\n1.  The formulation is not guaranteed to describe a probable or “natural” kind of mind,\n2.  These kinds of “corrigible” AIs are not guaranteed to produce desirable behavior, but only *imagined *to produce good behavior, \n3.  My clever-seeming idea was not at all constrained by reality to actually work in practice, as opposed to just sounding clever to me, and\n4.  I didn’t have a concrete use case in mind for what to *do *with a “corrigible” AI.\n\nI wrote this post as someone who previously needed to read it. \n\n1.  ^**[^](#fnref3crz71o4jhu)**^\n    \n    I now think that diamond’s physically crisp definition is a red herring. More on that in future posts.\n    \n2.  ^**[^](#fnref7qhggh3bak6)**^\n    \n    This alien is written to communicate my current belief state about how human value formation works, so as to make it clear why, *given *my beliefs, this value formation process is so obviously important to understand.\n    \n3.  ^**[^](#fnrefk1djcjup0qc)**^\n    \n    There is an additional implication present in the alien story, but not present in the evolutionary production of humans. The aliens are implied to have *purposefully* aligned some of their protein computers with human values, while evolution is not similarly “purposeful.” This implication is noncentral to the key point, which is that the human-values-having protein computers exist in reality.\n    \n4.  ^**[^](#fnrefmpfloj2azs)**^\n    \n    Well, I didn’t even *have *a detailed picture of human value formation back in 2021. I thought humans were hopelessly dumb and messy and we want a *nice clean AI which actually is robustly aligned*. \n    \n5.  ^**[^](#fnref7a5ti4623qb)**^\n    \n    Suppose we model humans as the \"inner agent\" and evolution as the \"outer optimizer\"—I think [this is, in general, the wrong framing](https://www.lesswrong.com/posts/3pinFH3jerMzAvmza/on-how-various-plans-miss-the-hard-bits-of-the-alignment?commentId=FbAnmAkCdp8qdiMoN), but let's roll with it for now. I would guess that Eliezer believes that [human values are an unpredictable godshatter](https://www.lesswrong.com/posts/34Gkqus9vusXRevR8/late-2021-miri-conversations-ama-discussion#YkhmywLQetjekM7e3) with respect to the outer criterion of inclusive genetic fitness. This means that if you reroll evolution many times with perturbed initial conditions, you get inner agents with dramatically different values each time—it means that human values are akin to a raindrop which happened to land in some location for no grand reason. I notice that I have medium-strength objections to this claim, but let's just say that he is correct for now.  \n      \n    I think this unpredictability-to-evolution doesn't matter. We aren't going to reroll evolution to get AGI. Thus, for a variety of reasons too expansive for this margin, I am little moved by analogy-based reasoning along the lines of \"here's the one time inner alignment was tried in reality, and evolution failed horribly.\" I think that historical fact is mostly irrelevant, for reasons I will discuss later."
    },
    "voteCount": 71,
    "forceInclude": true
  },
  {
    "_id": "CQAMdzA4MZEhNRtTp",
    "url": null,
    "title": "Human values & biases are inaccessible to the genome",
    "slug": "human-values-and-biases-are-inaccessible-to-the-genome",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "World Modeling"
      },
      {
        "name": "Heuristics & Biases"
      },
      {
        "name": "Human Values"
      },
      {
        "name": "Evolution"
      }
    ],
    "tableOfContents": {
      "sections": [
        {
          "title": "Appendix: The inaccessibility trilemma",
          "anchor": "Appendix__The_inaccessibility_trilemma",
          "level": 1
        },
        {
          "title": "Appendix: Did evolution have advantages in solving the information inaccessibility problem?",
          "anchor": "Appendix__Did_evolution_have_advantages_in_solving_the_information_inaccessibility_problem_",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        },
        {
          "anchor": "comments",
          "level": 0,
          "title": "46 comments"
        }
      ],
      "headingsCount": 4
    },
    "contents": {
      "markdown": "*Related to Steve Byrnes’ *[*Social instincts are tricky because of the “symbol grounding problem.”*](https://www.lesswrong.com/s/HzcM2dkCq7fwXBej8/p/5F5Tz3u6kJbTNMqsb#13_2_2_Claim_2__Social_instincts_are_tricky_because_of_the__symbol_grounding_problem_) *I wouldn’t have had this insight without several great discussions with Quintin Pope.*\n\nTL;DR: It seems hard to scan a trained neural network and locate the AI’s learned “tree” abstraction. For very similar reasons, it seems intractable for the genome to scan a human brain and back out the “death” abstraction, which probably will not form at a predictable neural address. Therefore, I infer that the genome can’t *directly *make us afraid of death by e.g. specifying circuitry which detects when we think about death and then makes us afraid. In turn, this implies that there are a *lot *of values and biases which the genome cannot hardcode.\n\n* * *\n\nIn order to understand the human alignment situation confronted by the human genome, consider the AI alignment situation confronted by human civilization. For example, we may want to train a smart AI which learns a sophisticated world model, and then motivate that AI according to its learned world model. Suppose we want to build an AI which intrinsically values trees. Perhaps we can just provide a utility function that queries the learned world model and counts how many trees the AI believes there are.\n\nSuppose that the AI [will learn a reasonably human-like concept for “tree.”](https://www.lesswrong.com/posts/Nwgdq6kHke5LY692J/alignment-by-default#Unsupervised__Natural_Abstractions) However, before training has begun, the learned world model is inaccessibleto us. Perhaps the learned world model will be buried deep within a recurrent policy network, and buried *within* the world model is the “trees” concept. But we have no idea what learned circuits will encode that concept, or how the information will be encoded. We probably can’t, in advance of training the AI, write an algorithm which will examine the policy network’s hidden state and reliably back out how many trees the AI thinks there are. The AI’s learned concept for “tree” is [*inaccessible information*](https://ai-alignment.com/inaccessible-information-c749c6a88ce?gi=56ff6de02281)from our perspective.\n\nLikewise, [the human world model is inaccessible to the human genome](https://www.alignmentforum.org/posts/NkSpukDkm9pjRdMdB/human-instincts-symbol-grounding-and-the-blank-slate#CCA_theory_vs_human_universal_traits_and_instincts), because the world model is probably in the cortex and the cortex is probably [randomly initialized](https://www.alignmentforum.org/posts/wBHSYwqssBGCnwvHg/intro-to-brain-like-agi-safety-2-learning-from-scratch-in).^[\\[1\\]](#fn8nbwa5zlopq)^ Learned human concepts are therefore inaccessible to the genome, in the same way that the “tree” concept is *a priori *inaccessible to us. Even the [broad area where language processing occurs](https://en.wikipedia.org/wiki/Functional_specialization_(brain)) [varies from person to person](https://www.sciencedirect.com/science/article/pii/S1053811911009281?casa_token=x95ovxmnEnYAAAAA:UbdXjjpeJIqBO9V-wRR-OqtOc7b-_Gen741XNBWDXmJ_UPH7C4IAaYLZ3lBo4xpmWwiSCYOBvA), to say nothing of the encodings and addresses of particular learned concepts like “death.”\n\nI’m going to say things like “the genome cannot specify circuitry which detects when a person is thinking about death.” This means that the genome cannot hardcode circuitry which e.g. fires when the person is thinking about death, and does not fire when the person is not thinking about death. The genome *does *help indirectly specify the whole adult brain and all its concepts, just like *we *indirectly specify the trained neural network via the training algorithm and the dataset. That doesn’t mean we can tell when the AI thinks about trees, and it doesn’t mean that the genome can “tell” when the human thinks about death.\n\nWhen I’d previously thought about human biases (like the sunk cost fallacy) or values (like caring about other people), I had implicitly imagined that genetic influences could directly affect them (e.g. by detecting when I think about helping my friends, and then producing reward). However, given the inaccessibility obstacle, I infer that this can’t be the explanation. I infer that the genome *cannot *directly specify circuitry which:\n\n*   Detects when you’re thinking about seeking power,\n*   Detects when you’re thinking about cheating on your partner,\n*   Detects whether you perceive a sunk cost,\n*   Detects whether you think someone is scamming you and, if so, makes you want to punish them,\n*   Detects whether a decision involves probabilities and, if so, implements the [framing effect](https://www.simplypsychology.org/framing-effect.html),\n*   Detects whether you’re thinking about your family,\n*   Detects whether you’re thinking about goals, and makes you [conflate terminal and instrumental goals](https://www.readthesequences.com/Terminal-Values-And-Instrumental-Values),\n*   Detects and then navigates ontological shifts,\n    *   E.g. Suppose you learn that animals are made out of cells. I infer that the genome cannot detect that you are expanding your ontology, and then execute some genetically hard-coded algorithm which helps you do that successfully.\n*   Detects when you’re thinking about wireheading yourself or manipulating your reward signals,\n*   Detects when you’re thinking about reality versus non-reality (like a simulation or fictional world), or\n*   Detects whether you think someone is higher-status than you.\n\nConversely, the genome *can *access direct sensory observables, because those observables involve *a priori*-fixed “neural addresses.” For example, the genome could hardwire a cute-face-detector which hooks up to [retinal ganglion cells](https://en.wikipedia.org/wiki/Retinal_ganglion_cell) (which are at genome-predictable addresses), and then this circuit could produce physiological reactions (like the release of reward). This kind of circuit seems totally fine to me.\n\nIn total, information inaccessibility is strong evidencefor the genome hardcoding relatively simple^[\\[2\\]](#fn1mnccefproc)^ cognitive machinery. This, in turn, implies that human values/biases/high-level cognitive observables are produced by relatively simpler hardcoded circuitry, specifying e.g. the learning architecture, the broad reinforcement learning and self-supervised learning systems in the brain, and regional learning hyperparameters. Whereas before it seemed plausible to me that the genome hardcoded a lot of the above bullet points, I now think that’s pretty implausible.\n\nWhen I realized that the genome must also confront the information inaccessibility obstacle, this threw into question a lot of my beliefs about human values, about the complexity of human value formation, and about the structure of my own mind. I was left with a huge puzzle. If we can’t say “[the hardwired circuitry down the street did it](https://www.readthesequences.com/Occams-Razor)”, where do biases come from? [How can the genome hook the human’s preferences into the human’s world model, when the genome doesn’t “know” what the world model will look like](https://arbital.com/p/ontology_identification/)? Why do people usually navigate ontological shifts properly, why don’t they want to wirehead, why do they almost always care about other people *if the genome can’t even write circuitry that detects and rewards thoughts about people*?\n\nA fascinating mystery, no? More on that soon.\n\n*Thanks to Adam Shimi, Steve Byrnes, Quintin Pope, Charles Foster, Logan Smith, Scott Viteri, and Robert Mastragostino for feedback.*\n\nAppendix: The inaccessibility trilemma\n======================================\n\nThe logical structure of this essay is that at least one of the following must be true: \n\n1.  Information inaccessibility is somehow a surmountable problem for AI alignment (and the genome surmounted it),\n2.  The genome solves information inaccessibility in some way we cannot replicate for AI alignment, or\n3.  The genome cannot directly address the vast majority of interesting human cognitive events, concepts, and properties. (*The point argued by this essay*)\n\nIn my opinion, either (1) or (3) would be enormous news for AI alignment. More on (3)’s importance in future essays.\n\nAppendix: Did evolution have advantages in solving the information inaccessibility problem?\n===========================================================================================\n\nYes, and no. In a sense, evolution had “a lot of tries” but is “dumb”, while we have very few tries at AGI while ourselves being able to do consequentialist planning. \n\nIn the AI alignment problem, we want to be able to back out an AGI’s concepts, but we cannot run lots of similar AGIs and select for AGIs with certain effects on the world. Given the [natural abstractions hypothesis](https://www.alignmentforum.org/posts/Fut8dtFsBYRz8atFF/the-natural-abstraction-hypothesis-implications-and-evidence), maybe there’s a lattice of convergent abstractions—first learn edge detectors, then shape detectors, then people being visually detectable in part as compositions of shapes. And *maybe*, for example, people tend to convergently situate these abstractions in similar relative neural locations: The edge detectors go in V1, then the shape detectors are almost always in some other location, and then the person-concept circuitry is learned elsewhere in a convergently reliable relative position to the edge and shape detectors.\n\nBut there’s a problem with this story. A congenitally blind person [develops dramatically different functional areas](https://academic.oup.com/cercor/article/25/9/2507/2926061?login=false), which suggests in particular that their person-concept will be at a radically different relative position than the convergent person-concept location in sighted individuals. Therefore, any genetically hardcoded circuit which checks at the relative address for the person-concept which is reliably situated for sighted people, will not look at the right address for congenitally blind people. Therefore, if this story were true, congenitally blind people would lose any important value-formation effects ensured by this location-checking circuit which detects when they’re thinking about people. So, either the human-concept-location-checking circuit wasn’t an important cause of the blind person caring about other people (and then this circuit hasn’t explained the question we wanted it to, which is how people come to care about other people), or there isn’t such a circuit to begin with. I think the latter is true, and the convergent relative location story is wrong.\n\nBut the location-checking circuit is only one way the human-concept-detector could be implemented. There are other possibilities. Therefore, given enough selection and time, maybe evolution could evolve a circuit which checks whether you’re thinking about other people. *Maybe*. But it seems implausible to me (\\\\(<4\\\\%\\\\)). I’m going to prioritize explanations for “most people care about other people” which don’t require a fancy workaround.\n\nEDIT: After talking with Richard Ngo, I now think there's about an 8% chance that several interesting mental events are accessed by the genome; I updated upwards from 4%. \n\n1.  ^**[^](#fnref8nbwa5zlopq)**^\n    \n    Human values can still be inaccessible to the genome even if the cortex isn’t learned from scratch, but learning-from-scratch is a nice and clean sufficient condition which seems likely to me.\n    \n2.  ^**[^](#fnref1mnccefproc)**^\n    \n    I argue that the genome probably hardcodes neural circuitry which is simple *relative *to hardcoded “high-status detector” circuitry. Similarly, [the code for a machine learning experiment](https://github.com/leela-zero/leela-zero/tree/next/src) is simple *relative* to [the neural network it trains](https://arxiv.org/abs/2201.13176)."
    },
    "voteCount": 41,
    "forceInclude": true
  },
  {
    "_id": "FyChg3kYG54tEN3u6",
    "url": null,
    "title": "Against Relying on Evolution to Forecast AI Outcomes (Part 1)",
    "slug": "against-relying-on-evolution-to-forecast-ai-outcomes-part-1",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "AI"
      },
      {
        "name": "Shard Theory"
      }
    ],
    "tableOfContents": {
      "sections": [
        {
          "title": "Introduction",
          "anchor": "Introduction",
          "level": 1
        },
        {
          "title": "Inner values versus outer optimization criteria",
          "anchor": "Inner_values_versus_outer_optimization_criteria",
          "level": 1
        },
        {
          "title": "1: Training an AI is more similar to human learning than to evolution",
          "anchor": "1__Training_an_AI_is_more_similar_to_human_learning_than_to_evolution",
          "level": 2
        },
        {
          "title": "2: We have more total evidence from human outcomes",
          "anchor": "2__We_have_more_total_evidence_from_human_outcomes",
          "level": 2
        },
        {
          "title": "3: Human learning trajectories represent a broader sampling of the space of possible learning processes",
          "anchor": "3__Human_learning_trajectories_represent_a_broader_sampling_of_the_space_of_possible_learning_processes",
          "level": 2
        },
        {
          "title": "4: Evidence from humans are more accessible than evidence from evolution",
          "anchor": "4__Evidence_from_humans_are_more_accessible_than_evidence_from_evolution",
          "level": 2
        },
        {
          "title": "5: Evolution could not have succeeded anyways",
          "anchor": "5__Evolution_could_not_have_succeeded_anyways",
          "level": 2
        },
        {
          "title": "Total significance of evolution",
          "anchor": "Total_significance_of_evolution",
          "level": 1
        },
        {
          "title": "Implications",
          "anchor": "Implications",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        },
        {
          "anchor": "comments",
          "level": 0,
          "title": "4 comments"
        }
      ],
      "headingsCount": 11
    },
    "contents": {
      "markdown": "**TL;DR**: The dynamics of human learning processes and reward circuitry are more relevant than evolution for understanding how inner values arise from outer optimization criteria. \n\nThis post is related to Steve Byrnes’ [Against evolution as an analogy for how humans will create AGI](https://www.lesswrong.com/posts/pz7Mxyr7Ac43tWMaC/against-evolution-as-an-analogy-for-how-humans-will-create), but more narrowly focused on how we should make inferences about values. \n\nThanks to Alex Turner, Charles Foster, and Logan Riggs for their feedback on a draft of this post.\n\nIntroduction\n------------\n\nHow should we expect AGI development to play out?\n\nTrue precognition appears impossible, so we use various analogies to AGI development, such as evolution, current day humans, or current day machine learning. Such analogies are far from perfect, but we still may be able to extract useful information by carefully examining them. \n\nIn particular, we want to understand how inner values relate to the outer optimization criteria. Human evolution is one possible source of data on this question. In this post, I’ll argue that human evolution actually provides very little usable evidence on AGI outcomes. In contrast, analogies to the human learning process are much more fruitful.\n\nInner values versus outer optimization criteria\n-----------------------------------------------\n\nOne way people motivate extreme levels of concern about inner misalignment is to reference the fact that evolution failed to align humans to the objective of maximizing inclusive genetic fitness. From Eliezer Yudkowsky’s [AGI Ruin post](https://www.lesswrong.com/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities):\n\n> **16**. Even if you train really hard on an exact loss function, that doesn't thereby create an explicit internal representation of the loss function inside an AI that then continues to pursue that exact loss function in distribution-shifted environments.  Humans don't explicitly pursue inclusive genetic fitness; **outer optimization even on a very exact, very simple loss function doesn't produce inner optimization in that direction**.  This happens *in practice in real life, *it is what happened in *the only case we know about…*\n\nI don't think that \"*evolution -> human values*\" is the most useful reference class when trying to understand how outer optimization criteria relate to inner values. Evolution didn't directly optimize over our values. It optimized over our learning process and reward circuitry. Once you condition on a particular human's learning process + reward circuitry configuration + the human's environment, you screen off the influence of evolution on that human's values. So, there are really (at least) two classes of observations from which we can draw evidence:\n\n1.  \"*evolution's inclusive genetic fitness criteria -> a human's learned values*\"  (as mediated by evolution's influence over the human's learning process + reward circuitry)\n2.  \"*a particular human's learning process + reward circuitry + training environment -> the human's learned values*\"\n\nI will present five reasons why I think evidence from (2) “*human learning -> human values*” is more relevant to predicting AGI.\n\n### 1: Training an AI is more similar to human learning than to evolution\n\nThe relationship we want to make inferences about is:\n\n*   \"*a particular AI's learning process + reward function + training environment -> the AI's learned values*\"\n\nI think that \"*AI learning -> AI values*\" is *much* more similar to \"*human learning -> human values*\" than it is to \"*evolution -> human values*\". Steve Byrnes makes this case in much more detail in [his post on the matter](https://www.lesswrong.com/posts/pz7Mxyr7Ac43tWMaC/against-evolution-as-an-analogy-for-how-humans-will-create). Two of the ways I think AI learning more closely resembles human learning, and not evolution, are:\n\n1.  The simple type signatures of the two processes. Evolution is a bi-level optimization process, with evolution optimizing over genes, and the genes specifying the human learning process, which *then* optimizes over human cognition. Evolution does not directly optimize over a human’s cognition. And because learned cognition is [not directly accessible](https://www.lesswrong.com/s/nyEFg3AuJpdAozmoX/p/CQAMdzA4MZEhNRtTp) to the genome, evolution must use roundabout methods to influence human values through the genome.   \n      \n    In contrast, SGD directly optimizes over an AI’s cognition, just as [human within-lifetime learning](https://www.lesswrong.com/posts/fDPsYdDtkzhBp9A8D/intro-to-brain-like-agi-safety-8-takeaways-from-neuro-1-2-on) directly optimizes over human cognition. The human and AI learning processes are much closer to their respective cognitive structures, compared with evolution.\n2.  The differences between the parameter counts of the respective objects of optimization (the genome for evolution, the brain’s circuitry for human learning, and the AI’s parameter’s for AI training).   \n      \n    The genome has very few parameters compared to even current day neural networks, much less the brain or future AGIs. Our experience with ML scaling laws very strongly implies that parameter counts matter a lot for a system’s learning dynamics. Better to compare highly parameterized systems to other highly parameterized systems. \n\n\"*AI learning -> AI values*\", \"*human learning -> human values*\", and “*evolution -> human values*” each represent very different optimization processes, with many specific dissimilarities between any pair of them. However, I think the balance of dissimilarities points to \"*human learning -> human values*\" being the closer reference class for \"*AI learning -> AI values*\". As a result, I think the vast majority of our intuitions regarding the likely outcomes of inner goals versus outer optimization should come from looking at the \"*human learning -> human values*\" analogy, not the \"*evolution -> human values*\" analogy. \n\n### 2: We have more total evidence from human outcomes\n\nAdditionally, I think we have a lot more total empirical evidence from \"*human learning -> human values*\" compared to from \"*evolution -> human values*\". There are billions of instances of humans, and each of them presumably have somewhat different learning processes / reward circuit configurations / learning environments. Each of them represents a different data point regarding how inner goals relate to outer optimization. In contrast, the human species only evolved once. Thus, evidence from \"*human learning -> human values*\" should account for even more of our intuitions regarding inner goals versus outer optimization than the difference in reference class similarities alone would indicate.\n\n### 3: Human learning trajectories represent a broader sampling of the space of possible learning processes\n\nOne common objection is that “human learning” represents a tiny region in the space of all possible mind designs, and so we cannot easily generalize our observations of humans to minds in general. This is, of course, true, and it greatly limits the strength of any AI-related conclusions we can draw from looking at \"*human learning -> human values*\". However, I again hold that inferences from \"*evolution -> human values*\" suffer from an even more extreme version of this same issue. \"*Evolution -> human values*\" represent an even more restricted look at the general space of optimization processes than we get from the observed variations in different humans' learning processes, reward circuit configurations, and learning environments.\n\n### 4: Evidence from humans are more accessible than evidence from evolution\n\nHuman evolution happened hundreds of thousands of years ago. We are deeply uncertain about the details of the human ancestral environment and which traits were under what selection pressure. We are still unsure about what precise selection pressure led humans to be so generally intelligent at all. We are very far away from being able to precisely quantify all the potentially values-related selection pressures in the ancestral environment, or how those selection pressures changed our reward systems or our tendencies to form downstream values. \n\nIn contrast, human [within lifetime learning](https://www.lesswrong.com/posts/fDPsYdDtkzhBp9A8D/intro-to-brain-like-agi-safety-8-takeaways-from-neuro-1-2-on) happens all the time right now. It’s available for analysis and even experimental intervention. Given two evidence sources about a given phenomenon, where one evidence source is much more easily accessible than the other, then all else equal, the more accessible evidence source should represent a greater fraction of our total information on the phenomenon. This is another reason why we should expect evidence from humans to account for a greater proportion of our total information about how inner values relate to outer optimization criteria.\n\n### 5: Evolution could not have succeeded anyways\n\nI think that a careful account of how evolution shaped our learning process in the ancestral environment implies that evolution had next to no chance of aligning humans with inclusive genetic fitness. \n\nThere are no features of the ancestral environment which would lead to an ancestral human learning about the abstract idea of inclusive genetic fitness. There were no ancestral humans that held an explicit representation of inclusive genetic fitness. So, there was never an opportunity for evolution to select for humans who attached their values to an explicit representation of inclusive genetic fitness.\n\nRegardless of how difficult it is, in general, to get learning systems to form values around different abstract concepts, evolution could not have possibly gotten us to form a value around the particular abstraction of inclusive genetic fitness because we didn’t form such an abstraction in the ancestral environment. Ancestral humans had zero variance in their tendency to form values around inclusive genetic fitness. Evolution cannot select for traits that don’t vary across a population, so evolution could not have selected for humans that formed their values around inclusive genetic fitness.\n\nIn contrast, the sorts of things that we humans end up valuing are usually the sorts of things that are easy to form abstractions around. Thus, we are not doomed by the same difficulty that likely prevented evolution from aligning humans to inclusive genetic fitness. \n\nThis point is extremely important. I want to make sure to convey it correctly, so I will quote two previous expressions of this point by other sources:\n\n[Risks from Learned Optimization](https://arxiv.org/abs/1906.01820) notes that the lack of environmental data related to inclusive genetic fitness effectively increases the description length complexity of specifying an intelligence that deliberately optimizes for inclusive genetic fitness:\n\n> …description cost is especially high if the learned algorithm’s input data does not contain easy-to-infer information about how to optimize for the base objective. Biological evolution seems to differ from machine learning in this sense, since evolution’s specification of the brain has to go through the information funnel of DNA. The sensory data that early humans received didn’t allow them to infer the existence of DNA, nor the relationship between their actions and their genetic fitness. Therefore, for humans to have been aligned with evolution would have required them to have an innately specified model of DNA, as well as the various factors influencing their inclusive genetic fitness. Such a model would not have been able to make use of environmental information for compression, and thus would have required a greater description length. In contrast, our models of food, pain, etc. can be very short since they are directly related to our input data.\n\nFrom Alex Turner (in private communication):\n\n> If values form because reward sends reinforcement flowing back through a person's cognition and reinforces the thoughts which (credit assignment judges to have) led to the reward, then if a person never thinks about inclusive reproductive fitness, they can *never ever* form a value shard around inclusive reproductive fitness. Certain abstractions, like lollipops or people, are convergently learned early in the predictive-loss-minimization process and thus are easy to form values around. But if there aren't local mutations which make a person more probable to think thoughts about inclusive genetic fitness before/while the person gets reward, then evolution can't instill this value. Even if the descendents of that person will later be *able *to think thoughts about fitness. \n\nTotal significance of evolution\n-------------------------------\n\nThere are many sources of empirical evidence that can inform our intuitions regarding how inner goals relate to outer optimization criteria. My current (not very deeply considered) estimate of how to weight these evidence sources is roughly: \n\n*   ~60% from \"*human learning -> human values*\"\n*   ~4% from \"*evolution -> human values*\"\n*   ~36% from various other evidence sources, which I won't address further in this post, such as:\n    *   economics\n    *   microbial ecology\n    *   politics\n    *   current results in machine learning\n    *   game theory / multi-agent negotiation dynamics\n\nImplications\n------------\n\nI think that using \"*human learning -> human values*\" as our reference class for inner goals versus outer optimization criteria suggests a much more straightforward relationship between the two, as compared to the (lack of a) relationship suggested by \"*evolution -> human values*\". Looking at the learning trajectories of individual humans, it seems like a given person's values have a great deal in common with the sorts of experiences they've found rewarding in their lives up to that point in time. E.g., a person who grew up with and displayed affection for dogs probably doesn't want a future totally devoid of dogs, or one in which dogs suffer greatly. \n\nPlease note that I am not arguing that humans are inner aligned, or that looking at humans implies inner alignment is easy. Humans are misaligned with maximizing their outer reward source (activation of reward circuitry). I operationalize this misalignment as: \"A*fter a distributional shift from their learning environment, humans frequently behave in a manner that predictably fails to maximize reward in their new environment, specifically because they continue to implement values they'd acquired from their learning environment which are misaligned to reward maximization in the new environment*\". \n\nFor example, one way in which humans are inner misaligned is that, if you introduce a human into a new environment which has a button that will wirehead the human (thus maximizing reward in the new environment), but has other consequences that are extremely bad by light of the human's preexisting values (e.g., killing a beloved family member), most humans won't push the button.\n\nI also think this regularity in inner values is reasonably robust to large increases in capabilities. If you take a human whose outer behavior suggests they like dogs, and give that human very strong capabilities to influence the future, I do not think they are at all likely to erase dogs from existence. It's probably not as robust to your choice of which specific human to try this with. E.g., many people would screw themselves over with reckless self-modification. My point is that higher capabilities *alone* do not automatically render inner values completely alien to those demonstrated at lower capabilities.  \n \n\n(Part 2 will address whether the “sharp left turn” demonstrated by human capabilities with respect to evolution implies that we should expect a similar sharp left turn in AI capabilities.)"
    },
    "voteCount": 18,
    "forceInclude": true
  },
  {
    "_id": "pdaGN6pQyQarFHXF4",
    "url": null,
    "title": "Reward is not the optimization target",
    "slug": "reward-is-not-the-optimization-target",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "AI"
      },
      {
        "name": "Reinforcement Learning"
      },
      {
        "name": "Reward Functions"
      },
      {
        "name": "Wireheading"
      },
      {
        "name": "Inner Alignment"
      },
      {
        "name": "Outer Alignment"
      },
      {
        "name": "Shard Theory"
      }
    ],
    "tableOfContents": {
      "sections": [
        {
          "title": "Reward probably won’t be a deep RL agent’s primary optimization target",
          "anchor": "Reward_probably_won_t_be_a_deep_RL_agent_s_primary_optimization_target",
          "level": 1
        },
        {
          "title": "RL agents which don’t think about reward before getting reward, will not become reward optimizers, because there will be no reward-oriented computations for credit assignment to reinforce.",
          "anchor": "RL_agents_which_don_t_think_about_reward_before_getting_reward__will_not_become_reward_optimizers__because_there_will_be_no_reward_oriented_computations_for_credit_assignment_to_reinforce_",
          "level": 3
        },
        {
          "title": "The siren-like suggestiveness of the word “reward”",
          "anchor": "The_siren_like_suggestiveness_of_the_word__reward_",
          "level": 2
        },
        {
          "title": "When is reward the optimization target of the agent?",
          "anchor": "When_is_reward_the_optimization_target_of_the_agent_",
          "level": 2
        },
        {
          "title": "Anticipated questions",
          "anchor": "Anticipated_questions",
          "level": 2
        },
        {
          "title": "Dropping the old hypothesis",
          "anchor": "Dropping_the_old_hypothesis",
          "level": 1
        },
        {
          "title": "Implications",
          "anchor": "Implications",
          "level": 2
        },
        {
          "title": "Appendix: The field of RL thinks reward=optimization target",
          "anchor": "Appendix__The_field_of_RL_thinks_reward_optimization_target",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        },
        {
          "anchor": "comments",
          "level": 0,
          "title": "82 comments"
        }
      ],
      "headingsCount": 10
    },
    "contents": {
      "markdown": "*This insight was made possible by many conversations with Quintin Pope, where he challenged my implicit assumptions about alignment. I’m not sure who came up with this particular idea.*\n\nIn this essay, I call an agent a “reward optimizer” if it not only gets lots of reward, but if it reliably makes choices like “reward but no task completion” (e.g. receiving reward without eating pizza) over “task completion but no reward” (e.g. eating pizza without receiving reward). Under this definition, an agent can be a reward optimizer even if it doesn't contain an explicit representation of reward, or implement a search process for reward.\n\n> Reinforcement learning is learning what to do—how to map situations to actions **so as to maximize a numerical reward signal**. — [Reinforcement learning: An introduction](http://www.incompleteideas.net/sutton/book/first/Chap1PrePub.pdf) \n\nMany people^[\\[1\\]](#fnw8im2esr9yd)^ seem to expect that reward will be the optimization target of really smart learned policies—that these policies will be reward optimizers. I strongly disagree. As I argue in this essay, reward is *not*, in general, that-which-is-optimized by RL agents.^[\\[2\\]](#fndaf17p7n29n)^  \n\nSeparately, as far as I can tell, most^[\\[3\\]](#fnwraenj05b1)^ practitioners usually view reward as encoding the relative utilities of states and actions (e.g. it’s *this good *to have all the trash put away), as opposed to imposing a *reinforcement schedule *which builds certain computational edifices inside the model (e.g. reward for picking up trash → reinforce trash-recognition and trash-seeking and trash-putting-away subroutines). I think the former view is almost always inappropriate, because **reward is the *****antecedent-computation-reinforcer*****.** Reward reinforces those computations which produced it. \n\nTherefore, *reward is not the optimization target* in two senses:\n\n1.  Deep reinforcement learning agents will not come to intrinsically and primarily value their reward signal; reward is not *the trained agent’s *optimization target.\n2.  Utility functions express the *relative goodness *of outcomes. Reward *is not best understood *as being a kind of utility function. Reward has the mechanistic effect of *reinforcing the computations which led to it*. Therefore, properly understood, reward does not express relative goodness and is therefore *not an optimization target at all.*\n\nReward probably won’t be a deep RL agent’s primary optimization target\n======================================================================\n\nAfter work, you grab pizza with your friends. You eat a bite. The taste releases [reward in your brain](https://en.wikipedia.org/wiki/Reward_system), which triggers credit assignment. Credit assignment identifies which thoughts and decisions were responsible for the release of that reward, and makes those decisions more likely to happen in similar situations in the future. Perhaps you had thoughts like \n\n*   “It’ll be fun to hang out with my friends” and \n*   “The pizza shop is nearby” and \n*   “Since I just ordered food at a cash register, execute `motor-subroutine-#51241` to take out my wallet” and \n*   “If the pizza is in front of me and it’s mine and I’m hungry, raise the slice to my mouth” and \n*   “If the slice is near my mouth and I’m not already chewing, take a bite.” \n\nMany of these thoughts will be judged responsible by credit assignment, and thereby become more likely to trigger in the future. This is what *reinforcement *learning is all about—the reward is the *reinforcer* of those things which came before it. The reward is reinforcing / locally-improving^[\\[4\\]](#fnlcn71g3whc)^ / generalizing the antecedent computations which are judged relevant by credit assignment. \n\nImportantly, reward does not automatically spawn thoughts *about *reward, and reinforce those reward-focused thoughts! Just because common English endows “reward” with suggestive pleasurable connotations, that [does not mean that](https://www.readthesequences.com/No-Universally-Compelling-Arguments) an RL agent will *terminally value *reward! \n\nWhat kinds of people (or non-tabular agents more generally) will become reward optimizers, such that the agent ends up terminally caring about reward (and little else)? Reconsider the pizza situation, but instead suppose you were thinking thoughts like “this pizza is going to be so rewarding” and “in this situation, eating pizza sure will activate my reward circuitry.” \n\nYou eat the pizza, triggering reward, triggering credit assignment, which correctly locates these reward-focused thoughts as contributing to the release of reward. Therefore, in the future, you will more often take actions because you think they will produce reward, and so you will become more of the kind of person who intrinsically cares about reward. This is a path^[\\[5\\]](#fnzf0metnada)^ to reward-optimization and wireheading. \n\n**RL agents which don’t think about reward before getting reward, will not become reward optimizers, because there will be no reward-oriented computations for credit assignment to reinforce.** \n\nThe siren-like suggestiveness of the word “reward”\n--------------------------------------------------\n\nLet’s strip away the suggestive word “reward”, and replace it by its substance: antecedent-computation-reinforcer. \n\nSuppose a human trains an RL agent by pressing the antecedent-computation-reinforcer button when the agent puts trash in a trash can. While putting trash away, the AI’s policy network is probably “thinking about” the *actual world it’s interacting with*, and so the antecedent-computation-reinforcer reinforces those heuristics which lead to the trash getting put away (e.g. “if trash-classifier activates near center-of-visual-field, then grab trash using `motor-subroutine-#642`”). \n\nThen suppose this AI models the true fact that the button-pressing produces the antecedent-computation-reinforcer. Suppose this AI, which has historically had its trash-related thoughts reinforced, considers the plan of pressing this button. “If I press the button, that triggers credit assignment, which will reinforce my decision to press the button, such that in the future I will press the button even more.”\n\n*Why, exactly, would the AI seize*^[\\[6\\]](#fnsmeax43bfp9)^* the button? To reinforce itself into a certain corner of its policy space? The AI has not had antecedent-computation-reinforcer-thoughts reinforced in the past, and so its current decision will not be made in order to acquire the antecedent-computation-reinforcer!*\n\nRL is not, in general, about training antecedent-computation-reinforcer optimizers. \n\nWhen *is* reward the optimization target of the agent?\n------------------------------------------------------\n\nIf reward is guaranteed to become your optimization target, then your learning algorithm can force you to become a drug addict. Let me explain. \n\n[Convergence theorems](https://nlp.chonbuk.ac.kr/AML/slides_lille/Lecture4-b.pdf) provide conditions under which a reinforcement learning algorithm is guaranteed to converge to an optimal policy for a reward function. For example, value iteration maintains a table of value estimates for each state *s*, and iteratively propagates information about that value to the neighbors of *s*. If a far-away state *f* has huge reward, then that reward ripples back through the environmental dynamics via this [“backup” operation](https://inst.eecs.berkeley.edu/~cs294-40/fa08/scribes/lecture2.pdf). Nearby parents of *f *gain value, and then after lots of backups, far-away ancestor-states gain value due to *f*’s high reward.\n\nEventually, the “value ripples” settle down. The agent picks an (optimal) policy by acting to maximize the value-estimates for its post-action states.\n\nSuppose it would be extremely rewarding to do drugs, but those drugs are on the other side of the world. Value iteration backs up that high value to your present space-time location, such that your policy necessarily gets *at least *that much reward. There’s no escaping it: After enough backup steps, you’re traveling across the world to do cocaine. \n\nBut obviously these conditions aren’t true in the real world. Your learning algorithm doesn’t force *you *to try drugs. Any AI which e.g. tried every action at least once would quickly kill itself, and so real-world general RL agents won’t explore like that because that would be stupid. So the RL agent’s algorithm won’t make it e.g. explore wireheading either, and so the convergence theorems *don’t apply even a little—even in spirit*.\n\nAnticipated questions\n---------------------\n\n1.  Why won’t early-stage agents think thoughts like “If putting trash away will lead to reward, then execute `motor-subroutine-#642`”, and then this gets reinforced into reward-focused cognition early on?\n    1.  Suppose the agent puts away trash in a blue room. Why won’t early-stage agents think thoughts like “If putting trash away will lead to the wall being blue, then execute `motor-subroutine-#642`”, and then this gets reinforced into blue-wall-focused cognition early on? [Why consider either scenario to begin with](https://www.readthesequences.com/Privileging-The-Hypothesis)?\n2.  But aren’t we implicitly selecting for agents with high cumulative reward, when we train those agents?\n    1.  Yeah. But on its own, this argument can’t possibly imply that selected agents will probably be reward optimizers. The argument would [prove too much](https://slatestarcodex.com/2013/04/13/proving-too-much/). Evolution selected for inclusive genetic fitness, and it [did not get IGF optimizers](https://www.lesswrong.com/posts/XPErvb8m9FapXCjhA/adaptation-executers-not-fitness-maximizers).\n        1.  \"We're selecting for agents on reward \\\\(\\\\rightarrow\\\\) we get an agent which optimizes reward\" is locally invalid. \"We select for agents on X \\\\(\\\\rightarrow\\\\) we get an agent which optimizes X\" is not true for the case of evolution, and so is not true in general. \n        2.  Therefore, the argument isn't necessarily true  in the AI reward-selection case. Even if RL *did* happen to train reward optimizers and this post *were* wrong, the selection argument is too weak on its own to establish that conclusion.\n    2.  Here’s the more concrete response: Selection isn’t *just *for agents which get lots of reward. \n        1.  For simplicity, consider the case where on the training distribution, the agent gets reward if and only if it reaches a goal state. Then any selection for reward is also selection for reaching the goal. And if the goal is the only red object, then selection for reward is *also *selection for reaching red objects. \n        2.  In general, selection for reward produces equally strong selection for reward’s necessary and sufficient conditions. In general, it seems like there should be a lot of those. Therefore, since selection is not only for *reward *but for *anything which goes along with reward *(e.g. reaching the goal), then selection won’t advantage *reward optimizers *over *agents which reach goals quickly / pick up lots of trash / \\[do the objective\\]*.  \n    3.  Another reason to not expect the selection argument to work is that it’s *instrumentally convergent *for most inner agent values to *not *become wireheaders, for them to *not *try hitting the reward button. \n        1.  I think that before the agent can hit the particular attractor of reward-optimization, it will hit an attractor in which it optimizes for some aspect of a historical correlate of reward. \n            1.  We train agents which intelligently optimize for e.g. putting trash away, and this reinforces trash-putting-away computations, which activate in a broad range of situations so as to steer agents into a future where trash has been put away. An intelligent agent will model the true fact that, if the agent reinforces itself into caring about antecedent-computation-reinforcement, then it will no longer navigate to futures where trash is put away. Therefore, it decides to not hit the reward button. \n            2.  This reasoning follows for most inner goals by instrumental convergence. \n        2.  On my current best model, this is why people usually don’t wirehead. They learn their own values via deep RL, like caring about dogs, and these actual values are opposed to the person they would become if they wirehead.\n3.  Don’t some people terminally care about reward?\n    1.  I think so! I think that generally intelligent RL agents will have *secondary, relatively weaker *values around reward, but that reward will not be a primary motivator. Under my current (weakly held) model, an AI will only start reinforcing computations about reward *after *it has reinforced other kinds of computations (e.g. putting away trash). More on this in later essays.\n4.  But what if the AI bops the reward button early in training, while exploring? Then credit assignment would make the AI more likely to hit the button again.\n    1.  Then keep the button away from the AI until it can model the effects of hitting the antecedent-computation-reinforcer button.^[\\[7\\]](#fn5fwuzzvjmuv)^\n    2.  For the reasons given in the “siren” section, a sufficiently reflective AI probably won’t seek the reward button on its own.\n5.  AIXI—\n    1.  will always kill you and then wirehead forever, unless you gave it something like a constant reward function.\n    2.  And, IMO, this fact is not practically relevant to alignment. AIXI is *explicitly a reward-maximizer*. As far as I know, AIXI(-*tl*) is not the limiting form of any kind of real-world intelligence trained via *reinforcement *learning.\n6.  Does the choice of RL algorithm matter?\n    1.  For point 1 (*reward is not the trained agent's optimization target*), it might matter. \n        1.  I started off analyzing model-free actor-based approaches, but have also considered a few model-based setups. I think the key lessons apply to the general case, but I think the setup will substantially affect which values tend to be grown. \n            1.  If the agent's curriculum is broad, then reward-based cognition may get reinforced from a confluence of tasks (solve mazes, write sonnets), while each task-specific cognitive structure is only narrowly contextually reinforced.\n            2.  Pretraining a language model and then slotting that into an RL setup also changes the initial computations in a way which I have not yet tried to analyze.\n        2.  It’s *possible *there’s some kind of RL algorithm which *does* train agents which limit to reward optimization (and, of course, thereby “solves” inner alignment in its literal form of “find a policy which optimizes the time-discounted sum of the outer objective signal”). \n    2.  For point 2 (*reward provides local updates to the agent's cognition via credit assignment; reward is not best understood as specifying our preferences*), the choice of RL algorithm should not matter, as long as it uses reward to compute local updates. \n        1.  A similar lesson applies to the updates provided by loss signals. A loss signal provides updates which deform the agent's cognition into a new shape.\n7.  TurnTrout, you've been talking about an AI's learning process using English, but ML gradients may not neatly be expressible in our concepts. How do we know that it's appropriate to speculate in English?\n    1.  I am *not* *certain* that my model is legit, but it sure seems more legit than (my perception of) how people usually think about RL (i.e. in terms of reward maximization, and reward-as-optimization-target instead of as feedback signal which builds cognitive structures). \n    2.  I only have access to my own concepts and words, so I am provisionally reasoning ahead anyways, while keeping in mind the potential treacheries of anglicizing imaginary gradient updates (e.g. \"be more likely to eat pizza in similar situations\").\n\nDropping the old hypothesis\n===========================\n\nAt this point, I don't see a strong reason to strongly focus on the “reward optimizer” hypothesis. The idea that AIs will get really smart and primarily optimize some reward signal… I don’t know of any tight mechanistic stories for that. I’d love to hear some, if there are any. \n\nAs far as I’m aware, the strongest evidence left for agents intrinsically valuing antecedent-computation-reinforcement is that some humans *do* strongly (but not uniquely) value antecedent-computation-reinforcement,^[\\[8\\]](#fn2v0yltp1gw7)^ and many humans seem to value it weakly, and humans are probably RL agents in the appropriate ways. So we definitely can’t *rule out* agents which strongly(and not just weakly) value antecedent-computation-reinforcement. But it’s also *not *the overdetermined default outcome. More on that in future essays.\n\nIt’s true that reward *can *be an agent’s optimization target, but what reward *actually does *is reinforce the computations which led to it. A particular alignment proposal might argue that a reward function will *reinforce the agent into a shape such that it intrinsically values reinforcement*, and that the *antecedent-computation-reinforcer goal is also a human-aligned optimization target*, but this is still just one particular approach of using the antecedent-computation-reinforcer to produce desirable cognition within an agent. Even in that proposal, the primary mechanistic function of reward is reinforcement, not optimization-target.\n\nImplications\n------------\n\nHere are some major updates which I made:\n\n1.  **Any reasoning derived from the reward-optimization premise is now suspect until otherwise supported.**\n2.  **Wireheading was never a high-probability problem for RL-*****trained*** **agents**, absent a specific storyfor why antecedent-computation-reinforcer-acquiring thoughts would be reinforced into primary decision factors.\n3.  **Stop worrying about finding “outer objectives” which are safe to *****maximize.***^[\\[9\\]](#fnp7rlxgfkp9)^I think that you’re not going to get an outer-objective-maximizer (i.e. an agent which maximizes the explicitly specified reward function). \n    1.  Instead, focus on building good cognition within the agent. \n    2.  In my ontology, there's only an inner alignment problem: How do we grow good cognition inside of the trained agent?\n4.  **Mechanistically model RL agents as executing behaviors downstream of past reinforcement** (e.g. putting trash away), in addition to thinking about policies which are selected for having high reward on the training distribution (e.g. hitting the button).\n    1.  The latter form of reasoning skips past the mechanistic substance of reinforcement learning: The reinforcement of computations responsible for the acquisition of antecedent-computation-reinforcer. I still think it's useful to consider selection, but mostly in order to generate failures modes whose mechanistic plausibility can be evaluated.\n    2.  In my view, reward's proper role isn't to encode an objective, but a *reinforcement schedule*, such that the right kinds of computations get reinforced within the AI's mind. \n\nAppendix: The field of RL thinks reward=optimization target\n===========================================================\n\nLet’s take a little stroll through [Google Scholar’s top results for “reinforcement learning\"](https://scholar.google.com/scholar?hl=en&as_sdt=7,39&q=reinforcement+learning), emphasis added:\n\n> The agent's job is to find a policy… that **maximizes some long-run measure of reinforcement**. ~ [Reinforcement learning: A survey](https://www.jair.org/index.php/jair/article/download/10166/24110/)\n\n> In instrumental conditioning, animals learn to choose actions to obtain rewards and avoid punishments, or, more generally to achieve goals. **Various goals are possible, such as optimizing the average rate of acquisition of net rewards (i.e. rewards minus punishments), or some proxy for this such as the expected sum of future rewards**. ~ [Reinforcement learning: The Good, The Bad and The Ugly](https://www.princeton.edu/~yael/Publications/DayanNiv2008.pdf) \n\n> We hypothesise that intelligence, and its associated abilities, can be understood as subserving the **maximisation** of reward. ~ [Reward is Enough](https://www.sciencedirect.com/science/article/pii/S0004370221000862#fn0020)\n\nSteve Byrnes did, in fact, briefly point out part of the “reward is the optimization target” mistake:\n\n> I note that even experts sometimes sloppily talk as if RL agents make plans towards the goal of maximizing future reward… — [Model-based RL, Desires, Brains, Wireheading](https://www.alignmentforum.org/posts/K5ikTdaNymfWXQHFb/model-based-rl-desires-brains-wireheading#Self_aware_desires_1__wireheading)\n\nI don't think it's just sloppy talk, I think it's incorrect belief in many cases. I mean, I did my PhD on RL theory, and I still  believed it. Many authorities and textbooks confidently claim—presenting little to no evidence—that reward is an optimization target (i.e. the quantity which the policy is in fact trying to optimize, or the quantity to be optimized by the policy). [Check what the math actually says](https://www.lesswrong.com/posts/2GxhAyn9aHqukap2S/looking-back-on-my-alignment-phd#Too_much_deference__too_little_thinking_for_myself). \n\n1.  ^**[^](#fnrefw8im2esr9yd)**^\n    \n    [Including](https://www.sciencedirect.com/science/article/pii/S0004370221000862#fn0020) the authors of the quoted introductory text, [Reinforcement learning: An introduction](http://www.incompleteideas.net/sutton/book/first/Chap1PrePub.pdf). I have, however, met several alignment researchers who already internalized that reward is not the optimization target, perhaps not in so many words. \n    \n2.  ^**[^](#fnrefdaf17p7n29n)**^\n    \n    [Utility ≠ Reward](https://www.alignmentforum.org/posts/bG4PR9uSsZqHg2gYY/utility-reward) points out that an RL-trained agent is *optimized by *original reward, but not necessarily *optimizing for *the original reward. This essay goes further in several ways, including when it argues that *reward *and *utility *have different type signatures—that reward shouldn’t be viewed as encoding a goal at all, but rather a *reinforcement schedule*. And not only do I not expect the trained agents to not maximize the original “outer” reward signal, I think they probably won’t try to strongly optimize [*any* reward signal](https://www.alignmentforum.org/posts/3RdvPS5LawYxLuHLH/hackable-rewards-as-a-safety-valve?commentId=crkrEjpyjB7N9t5jo).\n    \n3.  ^**[^](#fnrefwraenj05b1)**^\n    \n    [Reward shaping](http://ai.stanford.edu/~ang/papers/shaping-icml99.pdf) seems like the most prominent counterexample to the “reward represents terminal preferences over state-action pairs” line of thinking.\n    \n4.  ^**[^](#fnreflcn71g3whc)**^\n    \n    Of course, credit assignment doesn’t *just* reshuffle existing thoughts. For example, SGD raises image classifiers out of the noise of the randomly initialized parameters. But the refinements are localin parameter-space, and dependent on the existing weights through which the forward pass flowed.\n    \n5.  ^**[^](#fnrefzf0metnada)**^\n    \n    But also, you were still probably thinking about reality as you interacted with it (“since I’m in front of the shop where I want to buy food, go inside”), and credit assignment will still locate some of those thoughts as relevant, and so you wouldn’t purely reinforce the reward-focused computations.\n    \n6.  ^**[^](#fnrefsmeax43bfp9)**^\n    \n    Quintin Pope remarks: “The AI would probably want to establish **control** over the button, if only to ensure its values aren't updated in a way it wouldn't endorse. Though that's an example of convergent powerseeking, not reward seeking.”\n    \n7.  ^**[^](#fnref5fwuzzvjmuv)**^\n    \n    For mechanistically similar reasons, keep cocaine out of the crib until your children can model the consequences of addiction.\n    \n8.  ^**[^](#fnref2v0yltp1gw7)**^\n    \n    I am presently ignorant of [the relationship between pleasure and reward prediction error in the brain](https://pubmed.ncbi.nlm.nih.gov/35156187/). I do not think they are the same.   \n      \n    However, I think people are usually weakly hedonically / experientially motivated. Consider a person about to eat pizza. If you give them the choice between \"pizza but no pleasure from eating it\" and \"pleasure but no pizza\", I think most people would choose the latter (unless they were really hungry and needed the calories). If people just navigated to futures where they had eaten pizza, that would not be true. \n    \n9.  ^**[^](#fnrefp7rlxgfkp9)**^\n    \n    From correspondence with another researcher: There may yet be an interesting alignment-related puzzle to \"Find an optimization process whose maxima are friendly\", but I personally don't share the intuition yet."
    },
    "voteCount": 86,
    "forceInclude": true
  },
  {
    "_id": "FMdGt9S9irgxeD9Xz",
    "url": null,
    "title": "General alignment properties",
    "slug": "general-alignment-properties",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "AI"
      },
      {
        "name": "Embedded Agency"
      },
      {
        "name": "Ontology"
      },
      {
        "name": "Complexity of Value"
      },
      {
        "name": "General Alignment Properties"
      },
      {
        "name": "Shard Theory"
      }
    ],
    "tableOfContents": {
      "sections": [
        {
          "title": "Terminally valuing latent objects in reality. ",
          "anchor": "Terminally_valuing_latent_objects_in_reality__",
          "level": 1
        },
        {
          "title": "Navigating ontological shifts. ",
          "anchor": "Navigating_ontological_shifts__",
          "level": 1
        },
        {
          "title": "Reflective reasoning / embeddedness. ",
          "anchor": "Reflective_reasoning___embeddedness__",
          "level": 1
        },
        {
          "title": "Fragility of outcome value to initial conditions / Pairwise misalignment severity",
          "anchor": "Fragility_of_outcome_value_to_initial_conditions___Pairwise_misalignment_severity",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        },
        {
          "anchor": "comments",
          "level": 0,
          "title": "2 comments"
        }
      ],
      "headingsCount": 6
    },
    "contents": {
      "markdown": "[AIXI](https://en.wikipedia.org/wiki/AIXI) and the genome are both ways of specifying intelligent agents. \n\n1.  Give AIXI a utility function (perhaps over observation histories), and hook it up to an environment, and this pins down a policy.^[\\[1\\]](#fniggswxs0y97)^ \n2.  Situate the genome in the embryo within our reality, and this eventually grows into a human being with a policy of their own.\n\nThese agents have different \"values\", in whatever sense we care to consider. However, these two agent-specification procedures also have very different *general alignment properties. *\n\nGeneral alignment properties are not about *what* a particular agent cares about (e.g. the AI \"values\" chairs). I call an alignment property \"general\" if the property would be interesting to a range of real-world agents trying to solve AI alignment. Here are some examples.\n\n**Terminally valuing latent objects in reality. **\n\nAIXI only \"terminally values\" its observations and doesn't terminally value latent objects in reality, while humans generally care about e.g. dogs (which are latent objects in reality).\n\n**Navigating ontological shifts. **\n\nConsider latent-diamond-AIXI (LDAIXI), an AIXI variant. LDAIXI's utility function which scans its top 50 hypotheses (represented as Turing machines), checks each work tape for atomic representations of diamonds, and then computes the utility to be the amount of atomic diamond in the world. \n\nIf LDAIXI updates sufficiently hard towards non-atomic physical theories, then it can no longer find any utility in its top 50 hypotheses. All policies now might have equal value (zero), and LDAIXI would not continue maximizing the expected diamond content of the future. From our viewpoint, LDAIXI has [failed to rebind its \"goals\"](https://arbital.com/p/ontology_identification/) to its new conceptions of reality. (From LDAIXI's \"viewpoint\", it has Bayes-updated on its observations and continues to select optimal actions.)\n\nOn the other hand, physicists do not stop caring about their friends when they learn quantum mechanics. Children do not stop caring about animals when they learn that animals are made out of cells. People seem to navigate ontological shifts pretty well. \n\n**Reflective reasoning / embeddedness. **\n\n[AIXI can't think straight about how it is embedded in the world](https://www.lesswrong.com/posts/AszKwKyhBPZAnCstA/solomonoff-cartesianism). However, people quickly learn heuristics like \"If I get angry, I'll be more likely to be mean to people around me\", or \"If I take cocaine now, I'll be even more likely to take cocaine in the future.\" \n\n**Fragility of outcome value to initial conditions / Pairwise misalignment severity**\n\nThis general alignment property seems important to me, and I'll write a post on it. In short: How pairwise-unaligned are two agents produced with slightly different initial hyperparameters/architectural choices (e.g. reward function / utility function / inductive biases)? \n\n* * *\n\nI'm excited about people thinking more about general alignment properties and about what generates those properties.\n\n1.  ^**[^](#fnrefiggswxs0y97)**^\n    \n    Supposing e.g. uniformly random tie-breaking for actions enabling equal expected utility."
    },
    "voteCount": 17,
    "forceInclude": true
  },
  {
    "_id": "xqkGmfikqapbJ2YMj",
    "url": null,
    "title": "Shard Theory: An Overview",
    "slug": "shard-theory-an-overview",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "Psychology"
      },
      {
        "name": "AI"
      },
      {
        "name": "World Modeling"
      },
      {
        "name": "Human Values"
      },
      {
        "name": "Utility Functions"
      },
      {
        "name": "Complexity of Value"
      },
      {
        "name": "Reinforcement Learning"
      },
      {
        "name": "Subagents"
      },
      {
        "name": "Outer Alignment"
      },
      {
        "name": "SERI MATS"
      },
      {
        "name": "Shard Theory"
      }
    ],
    "tableOfContents": {
      "sections": [
        {
          "title": "Introduction",
          "anchor": "Introduction",
          "level": 1
        },
        {
          "title": "Reinforcement Strengthens Select Computations",
          "anchor": "Reinforcement_Strengthens_Select_Computations",
          "level": 1
        },
        {
          "title": "When You're Dumb, Continuously Blended Tasks Mean Continuous, Broadening Values",
          "anchor": "When_You_re_Dumb__Continuously_Blended_Tasks_Mean_Continuous__Broadening_Values",
          "level": 1
        },
        {
          "title": "When You're Smart, Internal Game Theory Explains the Tapestry of Your Values",
          "anchor": "When_You_re_Smart__Internal_Game_Theory_Explains_the_Tapestry_of_Your_Values",
          "level": 1
        },
        {
          "title": "Lingering Confusions",
          "anchor": "Lingering_Confusions",
          "level": 1
        },
        {
          "title": "Relevance to Alignment Success",
          "anchor": "Relevance_to_Alignment_Success",
          "level": 1
        },
        {
          "title": "Conclusion",
          "anchor": "Conclusion",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        },
        {
          "anchor": "comments",
          "level": 0,
          "title": "33 comments"
        }
      ],
      "headingsCount": 9
    },
    "contents": {
      "markdown": "*Generated as part of SERI MATS, Team Shard's research, under John Wentworth.*\n\n*Many thanks to Quintin Pope, Alex Turner, Charles Foster, Steve Byrnes, and Logan Smith for feedback, and to everyone else I've discussed this with recently! All mistakes are my own.*\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/7014d7016881f43f3d2b00d07b203dd0438096427bbe546e.JPG)\n\n*Team Shard*,  courtesy of Garrett Baker and DALL-E 2\n\nIntroduction\n============\n\n*Shard theory* is a research program aimed at explaining the systematic relationships between the reinforcement schedules and learned values of reinforcement-learning agents. It consists of a basic ontology of reinforcement learners, their internal computations, and their relationship to their environment. It makes several predictions about a range of RL systems, both RL models and humans. Indeed, shard theory can be thought of as simply applying the modern ML lens to the question of value learning under reinforcement in artificial and natural neural networks!\n\nSome of shard theory's confident predictions can be tested immediately in modern RL agents. Less confident predictions about i.i.d.-trained language models can also be tested now. Shard theory also has numerous retrodictions about human psychological phenomena that are otherwise mysterious from only the viewpoint of EU maximization, with no further substantive mechanistic account of human learned values. Finally, shard theory fails some retrodictions in humans; on further inspection, these lingering confusions might well falsify the theory.\n\nIf shard theory captures the essential dynamic relating reinforcement schedules and learned values, then we'll be able to carry out a steady stream of further experiments yielding a lot of information about how to reliably instill more of the values we want in our RL agents and fewer of those we don't. Shard theory's framework implies that alignment success is substantially continuous, and that even very limited alignment successes can still mean enormous quantities of value preserved for future humanity's ends. If shard theory is true, then further shard science will progressively yield better and better alignment results.\n\nThe remainder of this post will be an overview of the basic claims of shard theory. Future posts will detail experiments and preregister predictions, and look at the balance of existing evidence for and against shard theory from humans.\n\nReinforcement Strengthens Select Computations\n=============================================\n\nA *reinforcement learner* is an ML model trained via a *reinforcement schedule*, a pairing of world states and reinforcement events. We-the-devs choose when to dole out these reinforcement events, either handwriting a simple *reinforcement algorithm* to do it for us or having human overseers give out reinforcement. The reinforcement learner itself is a neural network whose computations are reinforced or anti-reinforced by reinforcement events. After sufficient training, reinforcement often manages to reinforce those computations that are good at our desired task. We'll henceforth focus on *deep reinforcement learners*: RL models specifically comprised of multi-layered neural networks.\n\nDeep RL can be seen as a supercategory of many deep learning tasks. In deep RL, the model you're training receives feedback, and this feedback fixes how the model is updated afterwards via SGD. In many RL setups, because the model's outputs influence its future observations, the model exercises some control over what it will see and be updated on in the future. RL wherein the model's outputs *don't* affect the distribution of its future observations is called *supervised learning*. So a general theory of deep RL models may well have implications for supervised learning models too. This is important for the experimental tractability of a theory of RL agents, as appreciably complicated RL setups are a huge pain in the ass to get working, while supervised learning is well established and far less finicky.\n\nHumans are the only extant example of generally intelligent RL agents. Your subcortex contains your hardwired reinforcement circuitry, while your neocortex comprises much of your trained RL model. So you can coarsely model the neocortex as an RL agent being fed observations by the external world and reinforcement events by subcortical circuitry, and ask about what learned values this human develops as you vary those two parameters. Shard theory boils down to using the ML lens to understand all intelligent deep systems in this way, and using this ML lens to build up a mechanistic model of value learning.\n\nAs a newborn baby, you start life with subcortical hardwired reinforcement circuitry (fixed by your genome) and a randomly initialized neocortex. The computations initialized in your neocortex are random, and so the actions initially outputted by your neocortex are random. (Your brainstem additionally hardcodes some rote reflexes and simple automatic body functions, though, accounting for babies' innate behaviors.) Eventually, your baby-self manages to get a lollipop onto his tongue, and the sugar molecules touching your tastebuds fire your hardwired reinforcement circuitry. Via a primitive, hardwired credit assignment algorithm -- say, single out whatever computations are different from the computations active a moment ago -- your reinforcement circuitry gives all those singled out computations more staying power. This means that these select computations will henceforth be more likely to fire, conditional on their initiating cognitive inputs being present, executing their computation and returning a motor sequence. If this path to a reinforcement event wasn't a fluke, those contextually activated computations will go on to accrue yet more staying power by steering into more future reinforcement events. Your bright-red-disk-in-the-central-visual-field-activated computations will activate again when bright red lollipops are clearly visible, and will plausibly succeed at getting future visible lollipops to your tastebuds.\n\nContextually activated computations can chain with one another, becoming responsive to a wider range of cognitive inputs in the process: if randomly crying at the top of your lungs gets a bright-red disk close enough to activate your bright-red-disk-sensitive computation, then credit assignment will reinforce both the contextual crying and contextual eating computations. These contextually activated computations that steer behavior are called *shards* in shard theory. A simple shard, like the reach-for-visible-red-disks circuit, is a *subshard*. Typical shards are chained aggregations of many subshards, resulting in a sophisticated, contextually activated, behavior-steering circuit in a reinforcement learner.\n\nShards are subcircuits of deep neural networks, and so can potentially run sophisticated feature detection. Whatever cognitive inputs a shard activates for, it will have to have feature detection for -- you simply can't have a shard sensitive to an alien concept you don't represent *anywhere* in your neural net. Because shards are subcircuits in a large neural network, it's possible for them to be hooked up into each other and share feature detectors, or to be informationally isolated from each other. To whatever extent your shards' feature detectors are all shared, you will have a single world-model that acts as input into all shards. To whatever extent your shards keep their feature detectors to themselves, they'll have their own ontology that only guides your behavior after that shard has been activated.\n\nSubcortical reinforcement circuits, though, hail from a distinct informational world. Your hardwired reinforcement circuits *don't* do any sophisticated feature detection (at most picking up on simple regular patterns in retinal stimulation and the like), and so have to reinforce computations \"blindly,\" relying only on simple sensory proxies.\n\nFinally, for the most intelligent RL systems, some kind of additional self-supervised training loop will have to be run along with RL. Reinforcement alone is just too sparse a signal to train a randomly initialized model up to significant capabilities in an appreciably complex environment. For a human, this might look something like trying to predict what's in your visual periphery before focusing your vision on it, sometimes suffering perceptual surprise. Plausibly, some kind of self-supervised loop like this is training all of the computations in the brain, testing them against cached ground truths. This additional source of feedback from self-supervision will both make RL models more capable than they would otherwise be and alter inter-shard dynamics (as we'll briefly discuss later).\n\nWhen You're Dumb, Continuously Blended Tasks Mean Continuous, Broadening Values\n===============================================================================\n\nEarly in life, when your shards only activate in select cognitive contexts and you are thus largely wandering blindly into reinforcement events, *only shards that your reinforcement circuits can pinpoint can be cemented*. Because your subcortical reward circuitry was hardwired by your genome, it's going to be quite bad at accurately assigning credit to shards. Here's an example of an algorithm your reinforcement circuitry could plausibly be implementing: reinforce all the diffs of all the computations running over the last 30 seconds, minus the computations running just before that. This algorithm is sloppy, but is also tractable for the primeval subcortex. In contrast, finding lollipops out in the real world involves a *lot* of computational work. As tasks are distributed in the world in extremely complex patterns *and* are always found blended together, again in a variety of setups, shards are going to have to cope with a continuously shifting flux of cognitive inputs that vary with the environment. When your baby self wanders into a real-world lollipop, many computations will have been active in steering behavior in that direction over the past 30 seconds, so credit assignment will reinforce all of them. The more you train the baby, the wider a range of proxies he internalizes via this reinforcement algorithm. In the face of enough reinforcement events, every representable proxy for the reinforcement event that marginally garners some additional reinforcement will come to hold some staying power. And because of this, simply getting the baby to internalize a particular target proxy *at all* isn't that hard -- just make sure that that target proxy further contributes to reinforcement in-distribution.\n\nMany computations that were active while reinforcement was distributed will be random jitters. So the splash damage from hardcoded credit assignment will reinforce these jitter-inducing computations as well. But because jitters aren't decent proxies for reinforcement *even in distribution*, they won't be steadily reinforced and will just as plausibly steer into anti-reinforcement events. What jitters do accrete will look more like conditionally activated rote tics than widely activated shards with numerous subroutines, because the jitter computations didn't backchain reinforcement reliably enough to accrete a surrounding body of subshards. Shard theory thus predicts that dumb RL agents internalize lots of representable-by-them in-distribution proxies for reinforcement as shards, as a straightforward consequence of reinforcement events being gated behind a complex conditional distribution of task blends.\n\nWhen You're Smart, Internal Game Theory Explains the Tapestry of Your Values\n============================================================================\n\nSubshards and smaller aggregate shards are potentially quite stupid. At a minimum, a shard is just a circuit that triggers given a particular conceptually chunked input, and outputs a rote behavioral sequence sufficient to garner more reinforcement. This circuit will not be well modeled as an intelligent planner; instead, it's perfectly adequate to think of it as just an observationally activated behavioral sequence. But as all your shards collectively comprise all of (or most of?) your neocortex, large shards can get quite smart.\n\nShards are all stuck inside of a single skull with one another, and only have (1) their interconnections with each other, (2) your motor outputs, and (3) your self-supervised training loop with which to causally influence anything. Game theoretically, your large intelligent shards can be fruitfully modeled as playing a *negotiation game* together: shards can interact with each other via their few output channels, and interactions all blend both zero-sum conflict and pure coordination. Agentic shards will completely route around smaller, non-agentic shards if they have conflicting ends. The interactions played out between your agentic shards then generate a complicated panoply of behaviors.\n\nBy the time our baby has grown up, he will have accreted larger shards equipped with richer world-models, activated by a wider range of cognitive inputs, specifying more and more complex behaviors. Where his shards once just passively dealt with the consequences effected by other shards via their shared motor output channel, they are now intelligent enough to scheme at the other shards. Say that you're considering whether to go off to big-law school, and are concerned about that environment exacerbating the egoistic streak you see and dislike in yourself. You don't want to grow up to be more of an egotist, so you choose to avoid going to your top-ranked big-law-school offer, *even though* the compensation from practicing prestigious big-shot law *would further* your other goals. On the (unreconstructed) standard agent model, this behavior is mysterious. Your utility function is fixed, no? Money is instrumentally useful; jobs and education are just paths through state space; your terminal values are *almost orthogonal* to your merely instrumental choice of career. On shard theory, though, this phenomenon of *value drift* isn't at all mysterious. Your egotistical shard would be steered into many reinforcement events were you to go off to the biggest of big-law schools, so your remaining shards use their collective steering control to avoid going down that path now, while they still have a veto. Similarly, despite knowing that heroin massively activates your reinforcement circuitry, not all that many people do heroin all the time. What's going on is that people reason now about what would happen after massively reinforcing a druggie shard, and see that their other values would not be serviced at all in a post-heroin world. They reason that they should carefully avoid that reinforcement event. On the view that reinforcement is the optimization target of trained reinforcement learners, this is inexplicable; on shard theory, it's straightforward internal game-theory.\n\nShards shouldn't be thought of as an *alternative* to utility functions, but as what utility functions look like *for bounded trained agents.* Your \"utility function\" (an ordering over possible worlds, subject to some consistency conditions) is far too big for your brain to represent. But a utility function can be lossily projected down into a bounded computational object by factoring it into a few shards, each representing a term in the utility function, each term conceptually chunked out of perceptual input. In the limit of perfect negotiation between your constituent shards, what your shards collectively pursue would (boundedly) resemble blended utility-function maximization! At lesser levels of negotiation competence between your shards, you'd observe many of the pathologies we see in human behavior. You might see agents who, e.g., flip back and forth between binge drinking and carefully avoiding the bar. Shard theory might explain this as a coalition of shards keeping an alcoholic shard in check by staying away from alcohol-related conceptual inputs, but the alcoholic shard being activated and taking over once an alcohol-related cognitive input materializes.\n\nShard theory's account of internal game theory also supplies a theory of *value reflection* or *moral philosophizing*. When shards are relatively good at negotiating outcomes with one another, one thing that they might do is to [try to find a common policy that they all consistently follow whenever they are the currently activated shards.](https://www.lesswrong.com/tag/values-handshakes) This common policy will have to be satisfactory to all the capable shards in a person, inside the contexts in which each of those shards is defined. But what the policy does outside of those contexts is completely undetermined. So the shards will hunt for [a single common moral rule that gets them each what they want inside their domains, to harvest gains from trade; what happens off of all of their their domains is undetermined and unimportant.](https://slatestarcodex.com/2018/09/25/the-tails-coming-apart-as-metaphor-for-life/#:~:text=The%20morality%20of,to%20be%20libertarians.) This looks an awful lot like testing various moral philosophies against your various moral intuitions, and trying (so far, in vain) to find a moral philosophy that behaves exactly as your various intuitions ask in all the cases where your intuitions have something to say.\n\nLingering Confusions\n====================\n\nThere are some human phenomena that shard theory doesn't have a tidy story about. The largest is probably the apparent phenomenon of credit assignment improving over a lifetime. When you're older and wiser, you're better at noticing which of your past actions were bad and learning from your mistakes. Possibly, this happens a long time after the fact, without any anti-reinforcement event occurring. But an improved *conceptual* understanding ought to be inaccessible to your subcortical reinforcement circuitry -- on shard theory, being wiser shouldn't mean your shards are reinforced or anti-reinforced any differently.\n\nOne thing that might be going on here is that your shards are better at loading and keeping chosen training data in your self-supervised learning loop buffer, and so steadily reinforcing or anti-reinforcing themselves or their enemy shards, respectively. This might look like trying not to think certain thoughts, so those thoughts can't be rewarded for accurately forecasting your observations. But this is underexplained, and shard theory in general doesn't have a good account of credit assignment improving in-lifetime.\n\nRelevance to Alignment Success\n==============================\n\nThe relevance to alignment is that (1) if shard theory is true, meaningful partial alignment successes are possible, and (2) we have a theoretical road to follow to steadily better alignment successes in RL agents. If we can get RL agents to internalize some human-value shards, alongside a lot of other random alien nonsense shards, then those human-value shards will be our representatives on the inside, and will intelligently bargain for what we care about, after all the shards in the RL agent get smarter. Even if the human shards only win a small fraction of the blended utility function, a small fraction of our lightcone is quite a lot. And we can improve our expected fraction by studying the systematic relationships between reinforcement schedules and learned values, in both present RL systems and in humans.\n\nConclusion\n==========\n\nShard theory is a *research program*: it's a proposed basic ontology of how agents made of neural networks work, most especially those agents with path-dependent control over the reinforcement events they later steer into. Shard theory aims to be a comprehensive theory of which values neural networks learn conditional on different reinforcement schedules. All that shard science is yet to be done, and, on priors, shard theory's attempt is probably not going to pan out. But this relationship is currently relatively unexplored, and competitor theories are relatively  unsupported accounts of learned values (e.g., that a single random in-distribution proxy will be the learned value, or that reinforcement is always the optimization target). Shard theory is trying to work out this relationship and then be able to demonstrably predict, ahead of time, specific learned values given reinforcement parameters."
    },
    "voteCount": 35,
    "forceInclude": true
  },
  {
    "_id": "ZmZBataeY58anJRBb",
    "url": null,
    "title": "Getting from an unaligned AGI to an aligned AGI? ",
    "slug": "getting-from-an-unaligned-agi-to-an-aligned-agi",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "AI"
      },
      {
        "name": "Outer Alignment"
      },
      {
        "name": "AI Boxing (Containment)"
      },
      {
        "name": "AI Success Models"
      }
    ],
    "tableOfContents": {
      "sections": [
        {
          "title": "Summary / Preamble",
          "anchor": "Summary___Preamble",
          "level": 1
        },
        {
          "title": "Restrictions in expressivity of AI",
          "anchor": "Restrictions_in_expressivity_of_AI",
          "level": 1
        },
        {
          "title": "Topics I'll cover in this series",
          "anchor": "Topics_I_ll_cover_in_this_series",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        },
        {
          "anchor": "comments",
          "level": 0,
          "title": "6 comments"
        }
      ],
      "headingsCount": 5
    },
    "contents": {
      "markdown": "<table><tbody><tr><td style=\"background-color:hsl(0, 0%, 90%)\"><h2><strong>Summary / Preamble</strong></h2><p>In&nbsp;<a href=\"https://www.lesswrong.com/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities\"><u>AGI Ruin: A List of Lethalities</u></a>, Eliezer writes&nbsp;<i>“A cognitive system with sufficiently high cognitive powers,&nbsp;<strong>given any medium-bandwidth channel of causal influence</strong>, will not find it difficult to bootstrap to overpowering capabilities independent of human infrastructure.”</i><br><br>I have larger error-bars than Eliezer on some AI-safety-related beliefs, but I share many of his concerns (thanks in large part to being influenced by his writings).<br><br>In this series I will try to explore if we might:</p><ul><li>Start out with a superintelligent AGI that may be unaligned (but seems superficially aligned)</li><li>Only use the AGI in ways where it's channels of causal influence are minimized (and where great steps are taken to make it hard for the AGI to hack itself out of the \"box\" it's in)</li><li>Work quickly but step-by-step towards a AGI-system that probably is aligned, enabling us to use it in more and more extensive ways (as we get more assurances that it's aligned)</li></ul><p>From the AGI-system we may (directly or indirectly) obtain programs that are interpretable and verifiable. These specialized programs could give us new capabilities, and we may trust these capabilities to be aligned and safe (even if we don't trust the AGI to be so). We may use these capabilities to help us with verification, widening the scope of programs we are able to verify (and maybe helping us to make the AGI-system safer to interact with). This could perhaps be a positive feedback-loop of sorts, where we get more and more aligned capabilities, and the AGI-system becomes safer and safer to interact with.</p><p>The reasons for exploring these kinds of strategies are two-fold:</p><ul><li>Maybe we wont solve alignment prior to getting superintelligence (even though it would be better if we did!)</li><li>Even if we <i>think</i> we have solved alignment prior to superintelligence, some of the techniques and strategies outlined here could be encouraged as best practice, so that we get additional layers of alignment-assurance.</li></ul><p>The strategy as a whole involves many iterative and contingency-dependent steps working together. I don't claim to have a 100% watertight and crystalized plan that would get us from A to B. Maybe some readers could be inspired to build upon some of the ideas or analyze them more comprehensively.<br>&nbsp;</p><p>Are any of the ideas in this series new? See <a href=\"https://www.lesswrong.com/posts/ZmZBataeY58anJRBb/getting-from-an-unaligned-agi-to-an-aligned-agi?commentId=NnhRsEaFwehPL6nLK\">here</a> for a discussion of that.</p></td></tr></tbody></table>\n\n***Me*****:** I have some ideas about how how to make use of an unaligned AGI-system to make an aligned AGI-system.\n\n***Imaginary friend*****:** My system 1 is predicting that a lot of confused and misguided ideas are about to come out of your mouth.\n\n***Me*****:** I guess we’ll see. Maybe I'm missing the mark somehow. But do hear me out.\n\n***Imaginary friend*****:** Ok.\n\n***Me*****:** First off, do we agree that a superintelligence would be able to understand what you want when asking for something, presuming that it is given enough information?\n\n***Imaginary friend*****:** Well, kind of. Often there isn’t really a clear answer to what you want.\n\n***Me*****:** Sure. But it would probably be good at predicting what looks to me like good answers. Even if it isn’t properly aligned, it would probably be extremely good at *pretending* to give me what I want. Right?\n\n***Imaginary friend*****:** Agreed.\n\n***Me*****:** So if I said to it *“show me the best source code you can come up with for an aligned AGI-system, and write the code in such a way that it’s as easy as possible to verify that it works as it should”*, then what it gave me would look really helpful - with no easily way for me to see a difference between what I’m provided and what I would be provided if it was aligned. Right?  \n  \n***Imaginary friend*****:** I guess I sort of agree. Like, if it answered your request it would probably look really convincing. But maybe it doesn’t answer your question. It could find a security vulnerability in the OS, and hack itself onto the internet somehow - that would be game over before you even got to ask it any questions. Or maybe you didn’t even try to box it in the first place, since you didn’t realize how capable your AI-system was getting, and it was hiding its capabilities from you.\n\n***Imaginary friend*****:** Or maybe it socially manipulated you in some really clever way, or “hacked” your neural circuitry somehow through sensory input, or figured out some way it could affect the physical world from within the digital realm (e.g. generating radio waves by “thinking”, or some thing we don't even know is physically possible).\n\nWhen we are dealing with a system that may prefer to destroy us (for [instrumentally convergent](https://www.lesswrong.com/tag/instrumental-convergence) reasons), and that system may be orders of magnitude smarter than ourselves - well, it's better to be too careful than not paranoid enough..\n\n***Me:*** I agree with all that. But it’s hard to cover all the branches of things that should be considered in one conversation-path. So for the time being, let’s assume a hypothetical situation where the AI is “boxed” in. And let’s assume that we know it’s extremely capable, and that it can’t “hack” itself out of the box in some direct way (like exploiting a security flaw in the operating system). Ok?\n\n***Imaginary friend*****:** Ok.\n\n***Me*****:** I presume you agree that there are more and less safe ways to use a superintelligent AGI-system? To give an exaggerated example: There is a big difference between “letting it onto the internet” and “having it boxed in, only giving it multiplication questions, and only letting it answer yes or no”.\n\n***Imaginary friend*****:** Obviously. But even if you only give it multiplication-questions, some other team will sooner or later develop AGI and be less scrupulous..\n\n***Me*****:** Sure. But still, we agree that there are more and less safe to try to use an AGI? There is a “scale” of sorts?\n\n***Imaginary friend*****:** Of course.\n\n***Me*****:** Would you also agree that there is a “scale” for how hard it is for an oracle/genie to “trick” you into falsely believing that it has provided you with what you want? For example, if I ask it to prove a mathematical conjecture, that is much harder to “pretend” to do the way I want it without actually doing it (compared to most things)?\n\n***Imaginary friend*****:** Sure.\n\n***Me*****:** What I want to talk about are ways of asking an AGI genie/oracle for things in ways where it’s hard for it to “pretend” that it’s giving us what we want without doing it. And ways we might leverage that to eventually end up with an aligned AGI-system, while trying to keep the total risk (of all the steps we take) low.\n\n***Imaginary friend*****:** My system 1 suspects I am about to hear some half-baked ideas.  \n  \n*![](https://lh3.googleusercontent.com/bFQ4Ku5nhW-RvmeQJoaSQs4T3inlRJeIA5cQYNlXs2JxB2X1YkuDBf2e8iJawzrQ1RRW6nYmK0eTcAYtNg08TO5pdQ3mDAZ1rMrbwgBXs7z3yyd-QYTYhKS0S5eQJ0fFIOieXzdSdSYn6KdrUQ)*\n\n***Me*****:** And your system 1 may have a point. I don’t claim to have detailed and watertight arguments showing exactly how we get from A to B. What I have is an *outline* of how we might get there, while minimizing risk along the way (not to 0%, but to way less than 50% if suggestions are executed faithfully).\n\nBelieve me, I don’t have full knowledge and understanding of everything that has been explored in the AI alignment community…\n\n***Imaginary friend*****:** I believe you.\n\n***Me*****:** …but it seems to me that the kinds of techniques and strategies I'll be outlining are under-discussed.\n\n***Imaginary friend*****:** It sounds like you are looking for ways to avoid having to solve the alignment problem, so that you can focus on other things instead of solving it beforehand.\n\n***Me*****:** I guess it can be interpreted that way. I don’t want it to come across that way though! If we find reliable methods to align a superintelligent AGIs *before* we get superintelligent AGIs, then that would improve our odds of success!\n\nBut maybe we don’t. If we don’t then it’s better to have some plans ready, so that less improvisation is necessary. AFAIK there is no [fire alarm](https://intelligence.org/2017/10/13/fire-alarm/) for AGI, and it certainly doesn't seem too early to start analyzing these kinds of scenarios in a more comprehensive way..\n\nAnd also, even if we *think* we have solved alignment in a reliable way prior to superintelligence, some of the strategies and principles I’ll discuss can be used as additional layers of alignment-assurance. Hence me thinking that it may be a good idea to develop these ideas further, and to encourage them as best practice.\n\n**Restrictions in expressivity of AI**\n--------------------------------------\n\n***Me*****:** The first part of this discussion will focus on things that we can do while the AGI stays in the digital realm. We will get to the question of how to do complicated things in the non-digital realm (and on the internet), but that’s for later.\n\n***Imaginary friend*****:** You understand that any use of an AGI enables it to affect the non-digital realm, right? If you let it write or modify code, that is a way for the AGI to communicate with humans, and communicating with humans is a way to affect the non-digital realm.\n\n***Me*****:** Yes, I agree with that. But I would point out that it is up to us how much “expressivity” we give the AI-system. For example, if we get help from it to point out potential mistakes in some piece of code, maybe we don’t let it write text to explain itself (limiting instead to things such as pointing out areas of code to take an extra look at, highlighting examples of input that may produce unintended behavior, etc). And if we do let it write text, then it is up to us how we limit the expressivity of what it can write in text.\n\n***Imaginary friend*****:** Ok, did you by the way notice that you spoke about not letting it write text - but letting it write example input? Well, example input can be text! Those kinds of things can be easy to miss, and often you may not be aware of what you’re missing.\n\n***Me*****:** I may miss such things sometimes, but sometimes I will also leave them out even though I’m aware of them, so as to not get into tangents all the time. I did notice that one though. Partly thanks to you helping me to get better at noticing those sorts of things.\n\n***Imaginary friend*****:** That’s what imaginary friends are for.\n\nKeep in mind that *any* way to point out bugs or potential problems is a way for the AI to affect how you think and what you think about. Take [this test](https://www.youtube.com/watch?v=vJG698U2Mvo) of selective attention for example. Or look at how magicians focus and misdirect people's attention to trick them. There are lots of things like this.\n\n***Me*****:** I’m certainly not claiming that the techniques I’ll describe are risk free. At the same time, it’s important to consider how different techniques can come together to reduce risk. For example, one thing that will be touched upon again and again is testing the convergence of the results from different methods and architectures and pieces of code. Preferably, if an AI-system writes the code for some module, then the overall system should ensure that we also have that module be written by other AI-systems, and also by humans if possible.\n\nIn regards to e.g. AIs pointing out potential issues, the system as a whole (a system of systems of systems..) would preferably keep track of the changes made due to such suggestions, and also try to keep track of what the output of the system would be if those changes hadn’t been made.\n\n***Imaginary friend*****:** Ok. But just because something makes you more safe, and you can handwave to a lot of techniques where each thing helps a bit with safety - this does not show that the the process as a whole will be safe. And I am a bit worried about you proposing ideas that are hard to criticize because they’re complicated and vague, instead of getting to the “core” of the problem. Also, adding on lots of complication to increase security [can in some cases be counter-productive](https://timharford.com/2019/11/cautionary-tales-ep-3-lala-land-galileos-warning/).\n\n***Me*****:** Some good points there. Although do I feel uncertain about the degree to which there is a “core” of the problem. I want this series to outline a path towards a more \"complete\" solution, but I want to start by focusing more on techniques and principles that are \"building-blocks\".  \n  \nThe best scenario is if the first superintelligent AGI is robustly aligned from the start. I think we should try hard to make that be the case, and I don't hope this series is much of a distraction from that.\n\nI am reminded of how I am in support of trying to reduce CO2 emissions today, but *also* support research on geoengineering.\n\n**Topics I'll cover in this series**\n------------------------------------\n\n***Imaginary friend*****:** So where do you see this discussion going?\n\n***Me*****:** I guess we’ll see. But I do have some plans.\n\nOne thing I want to discuss is the concept of “solution-generators” (and maybe also “solution-generators-generators”, etc). When asking an oracle/genie for something, it may often make sense to not ask the question directly, but to instead ask for a “solution-generator” - that is to say, a function that generates solutions within the domain in question. These “generators” should be optimized for being narrow/specialized/modular/verifiable (techniques for trying to ensure that the system is faithfully optimizing the “generator” for these properties is a topic by itself).\n\nFor example, if we want to do a physics-simulation, then we might want to use a “simulation-generator” or “simulation-generator-generator”. One merit of this is that the code can be easier to inspect and understand than the code of a superintelligent AGI. Another merit is that the “generator” can be tested against existing data.  \n  \nBut of course, you can also ask it to answer the question directly (in addition to asking for the “solution-generator”). And then you can see if the output is the same when you obtain it indirectly vs when you obtain it directly.\n\n***Imaginary friend*****:** Isn’t it better to not ask it the specific question, so that it’s harder for it to guess what you’re after?\n\n***Me*****:** I will mostly not be mentioning this sort of thing explicitly, but yes I agree, you shouldn’t ask both questions of the same *instance* of the AGI-system. The instance that is asked for *A* should not know that you also have asked it for *B*. Though of course, you could have additional instances that are asked both questions, and check if results/output converge.\n\nAnother topic I want to speculate a bit about is the topic of brain emulations, and also other “generators” that try to predict the behavior and thinking of humans. I’m interested in the question of *“Can we bootstrap from AGI to either brain emulations and/or something else that can predict human behavior, while more or less staying within the digital realm?”*. Obviously there are lots of ethical considerations here, and it’s very important to avoid [suffering subroutines](https://reducing-suffering.org/what-are-suffering-subroutines/) insofar as possible!\n\n***Me*****:** I also want to discuss techniques involving computable argument-trees/”proofs”, where every inference-step is made explicit. As well as outlining possible techniques to have the concepts/propositions of such proofs represent more or less any thought that is sufficiently “clear/crisp/explicit” (blurring the distinction between “mathematical” proofs and any other argument about anything). Included in the discussion will be outlines of ideas for how to deal with “vagueness” and [cluster-like](https://www.lesswrong.com/posts/WBw8dDkAWohFjWQSk/the-cluster-structure-of-thingspace) concepts within such argument-trees/”proofs”.\n\nAnd I’ll be outlining thoughts about capabilities that I think will help with verifying that instructions for doing things in the real world (developing new types of machines and that sort of thing) will work as intended. Such as for example [copying a strawberry at the molecular level](https://twitter.com/ESYudkowsky/status/1070095840608366594) without unintended consequences. Among other things there will be some focus on “generators” for mappings between (1) models/ontologies, and (2) data-structures representing geometric structures (e.g. some sort of physics-simulation), and (3) real things in the actual world that the models are meant to refer to.\n\nThe more people there are who (1) are smart and have thought a lot about something and (2) see things differently from you, the more reason for self-doubt about your own judgment. And this is for me a significant source of uncertainty about my ideas in regards to alignment (and AI more generally). But it seems best to me to just try to describe my perspective as well as I can, and then people can do with that what seems best to them.\n\n***Imaginary friend*****:** Talk to you later then.\n\n* * *\n\n*Any feedback or comments (be that positive or negative or neither) would be received with interest.*"
    },
    "voteCount": 8,
    "forceInclude": true
  },
  {
    "_id": "xERh9dkBkHLHp7Lg6",
    "url": null,
    "title": "Making it harder for an AGI to \"trick\" us, with STVs",
    "slug": "making-it-harder-for-an-agi-to-trick-us-with-stvs",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "AI"
      },
      {
        "name": "Outer Alignment"
      },
      {
        "name": "Verification"
      },
      {
        "name": "AI Success Models"
      },
      {
        "name": "AI Boxing (Containment)"
      }
    ],
    "tableOfContents": {
      "sections": [
        {
          "title": "Summary / Preamble",
          "anchor": "Summary___Preamble",
          "level": 1
        },
        {
          "title": "Clarifying what I mean by STVs (aka “generators”)",
          "anchor": "Clarifying_what_I_mean_by_STVs__aka__generators__",
          "level": 1
        },
        {
          "title": "“Hidden” behaviors in STVs",
          "anchor": "_Hidden__behaviors_in_STVs",
          "level": 1
        },
        {
          "title": "Why-not-bothing & output-convergence",
          "anchor": "Why_not_bothing___output_convergence",
          "level": 1
        },
        {
          "title": "Human-predicting STVs",
          "anchor": "Human_predicting_STVs",
          "level": 1
        },
        {
          "title": "Brain emulations",
          "anchor": "Brain_emulations",
          "level": 2
        },
        {
          "title": "Approximations of brain emulations (sometimes based on indirect methods)",
          "anchor": "Approximations_of_brain_emulations__sometimes_based_on_indirect_methods_",
          "level": 2
        },
        {
          "title": "Lots of raw and somewhat “hand-coded” probabilistic and modular inference-rules that encode typical human responses within some domain/context",
          "anchor": "Lots_of_raw_and_somewhat__hand_coded__probabilistic_and_modular_inference_rules_that_encode_typical_human_responses_within_some_domain_context",
          "level": 2
        },
        {
          "title": "STVs that help with software-development",
          "anchor": "STVs_that_help_with_software_development",
          "level": 1
        },
        {
          "title": "Rewrite code in ways that are proven to not change behavior",
          "anchor": "Rewrite_code_in_ways_that_are_proven_to_not_change_behavior",
          "level": 2
        },
        {
          "title": "Use code-rewrites with proofs as building-blocks in other proofs",
          "anchor": "Use_code_rewrites_with_proofs_as_building_blocks_in_other_proofs",
          "level": 2
        },
        {
          "title": "Convert between code and high-level descriptions/specifications of code (and look for discrepancies)",
          "anchor": "Convert_between_code_and_high_level_descriptions_specifications_of_code__and_look_for_discrepancies_",
          "level": 2
        },
        {
          "title": "Look for bugs in code",
          "anchor": "Look_for_bugs_in_code",
          "level": 2
        },
        {
          "title": "Help to write/propose tests",
          "anchor": "Help_to_write_propose_tests",
          "level": 2
        },
        {
          "title": "Look for structural similarities/overlap between different code bases",
          "anchor": "Look_for_structural_similarities_overlap_between_different_code_bases",
          "level": 2
        },
        {
          "title": "Identify parts/aspect of code that don’t only significantly affects output small sections of the space of possible inputs",
          "anchor": "Identify_parts_aspect_of_code_that_don_t_only_significantly_affects_output_small_sections_of_the_space_of_possible_inputs",
          "level": 2
        },
        {
          "title": "Scope of what STVs could be used for (without becoming too AGI-like)",
          "anchor": "Scope_of_what_STVs_could_be_used_for__without_becoming_too_AGI_like_",
          "level": 1
        },
        {
          "title": "Thanks for now",
          "anchor": "Thanks_for_now",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        },
        {
          "anchor": "comments",
          "level": 0,
          "title": "5 comments"
        }
      ],
      "headingsCount": 20
    },
    "contents": {
      "markdown": "<table style=\"background-color:hsl(0, 0%, 90%)\"><tbody><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><h1>Summary / Preamble</h1><p>AI Alignment has various sub-areas. The area I focus on here is ways we might use a superintelligent AGI-system to help with creating an aligned AGI-system, even if the AGI we start out with isn’t fully aligned.</p><p>Imagine a superintelligence that “pretends” to be aligned. Such an AI may give output that&nbsp;<i>seems</i> to us like what we want. But for some types of requests, it’s very hard to give output that&nbsp;<i>seems</i> to us like what we want without it&nbsp;<i>actually</i> being what we want (even for a superintelligence). Can we obtain new capabilities by making such requests, in such a way that the scope of things we can ask for in a safe way (without being “tricked” or manipulated) is increased? And if so, is it possible to eventually end up with an aligned AGI-system?</p><p>One reason for exploring such strategies is contingency planning (what if we haven’t solved alignment by the time the first superintelligent AGI-system arrives?). Another reason is that additional layers of assurance could be beneficial (even if we&nbsp;<i>think</i> we have solved alignment, are there ways to relatively quickly add additional layers of alignment-assurance?).</p><p>When dealing with a genie/oracle, we may not want to ask it to provide some direct solution/answer to what you want (and have it argue why it’s a good idea). Instead we obtain what we want more indirectly, by having different instances of the genie/oracle aid us in the construction of more narrow/specialized programs (that score high in terms of verifiability).</p><p>We could call such programs STVs, where STV is an abbreviation for&nbsp;<i><strong>S</strong>pecialized&nbsp;<strong>T</strong>ransparent&nbsp;<strong>V</strong>erifiable Program</i>.</p><p><i>STV</i> is a blurry concept (much like how <i>AI</i> and <i>AGI</i> are blurry concepts).</p><p>An STV could be (but would not have to be) a narrow AI. More colloquially, STVs could also be referred to as “generators”.</p><p>In this post I explore principles/techniques/strategies for using STVs in ways that enable us to get things we want from an AGI without being “tricked”.</p><p>I hope and think this article reads well in isolation, even though it's part 2 of&nbsp;<a href=\"https://www.lesswrong.com/posts/ZmZBataeY58anJRBb/getting-from-an-unaligned-agi-to-an-aligned-agi\"><u>a series</u></a>.</p><p>Future posts in this series will include discussion of:</p><ul><li>AI-generated computational proofs/argument-trees, and methods/strategies for verifying such proofs/arguments.</li><li>Ideas for how formalisms for computational proofs could try to incorporate human cluster-like concepts, concept vagueness, formalism-to-reality mappings, references to code and results from running code, etc (from within themselves).</li><li>How different “pieces” outlined in this series might be put together. Can we get from unaligned AGI to aligned AGI without being \"tricked\"/hacked/manipulated along the way, and without being stopped by chicken-or-egg problems?</li><li>Ideas for how we might want to make a system of sub-systems, consisting of various \"siloed\" AGI-systems (with different settings, architectures, and alignment methodologies).</li><li>Strategic and moral considerations, and possible first steps when starting to use superintelligent AGI-systems to do things outside of the digital realm.</li></ul><p>Are any of the ideas in this series new? See&nbsp;<a href=\"https://www.lesswrong.com/posts/ZmZBataeY58anJRBb/?commentId=NnhRsEaFwehPL6nLK\"><u>here</u></a> for a discussion of that.</p></td></tr></tbody></table>\n\n  \n  \n***Me:*** I think it may be helpful to start out with a simplistic example.  \n  \nSo let’s imagine that we are interested in finding good chess-moves, and want an AGI to help us with this.\n\n***Imaginary friend:*** Ok.\n\n***Me:*** We *could* ask it directly for good chess-moves, along with explanations for why these chess-moves seem like good chess-moves.\n\nBut if we want to make it hard for the AGI to “trick” us (to pretend that it’s giving us what we want, but not actually doing so). One way could be to ask for a *program* that searches for good chess-moves (instead of asking directly for the moves themselves).\n\nThis could be beneficial for several reasons:\n\n1.  If the AGI wants to trick you in specific ways for specific strategic reasons, this may be harder when you force it to answer in a general way (not answer in a way that is custom to the specific thing you want).\n2.  If the AGI has strategic reasons for wanting to trick you about specific things, this is made harder when you don’t ask it specifically for what you are after (thus making it less straight-forward to guess what the answer will be used for).\n3.  Code that is specialized for some narrow/constrained task can often be easier to understand. This presumes that the AGI chooses to write the code easy to understand for humans, but if we know it is superintelligent and it’s *pretending* to be aligned then there are limits to what it can get away with (we would notice if the code clearly isn’t optimized for human understandability).\n4.  It may be easier for humans to understand chess programs than analyzing chess-moves directly (presuming that the program is written in a way that favors human understanding).\n5.  Narrow/specialized programs may more often be possible to code in a way where properties of behavior/output can be “proven” using formal/computational proofs. To put it simplistically: The more narrow a program is, the more suspicious it would be if the AGI can’t “prove” that it does what we want it to do.\n6.  Programs that solve problems in a general way - but within a narrow/constrained domain of tasks/questions - can more easily be tested against examples and real-world data. In the case of a chess-program we could for example check if it beats other chess-programs while using less computation.\n\n***Imaginary friend:*** Ok, but being able to confirm that a chess program is good at chess doesn’t solve the alignment problem. We want to be able to use the AGI to deal with the real world. And the real world is messy and complex in ways that board games aren’t, making verification and formal “proofs” much harder.\n\n***Me:*** Sure, I agree with that. I started out with a simplistic example, so as to be able to outline some principles without getting bogged down in details. And more principles and ideas will be conveyed later on. How far this gets us remains to be discussed.\n\nClarifying what I mean by STVs (aka “generators”)\n=================================================\n\n***Me:*** The term STV is a term I made up. Maybe other people have referred to similar concepts using different terms.\n\nIt stands for *“**S**pecialized **T**ransparent **V**erifiable Program”*.\n\nBy *specialized *I mean that the range of questions/tasks it is designed to handle is constrained. For example, maybe it only does one type of task, and only handles input from a specific domain and in a specific format.\n\nIn some cases the program may work well without anything that resembles reasoning. In other cases, reasoning-like processes may be necessary. In such cases the reasoning should (insofar as feasible) be constrained, specialized/non-general, transparent/interpretable, and closed off from other components/subroutines.\n\nBy *transparent* I mean that it’s easy for humans to understand how it works. Maybe not easy in an absolute sense, but no harder than necessary.\n\nInsofar as feasible the code should be modular, and written in a way that is optimized for making it easy for humans to understand (and hard for humans to misunderstand).\n\nThere should be additional tools for analyzing the code beyond looking at source code. Diagrams, high-level textual descriptions, and maybe interactive overviews of various kinds.\n\nIf the program needs components that are more “black-box-like” (like neural networks sometimes can be), then these components should be constrained (in terms of how they can influence the rest of the system, and in terms of what they are used for).\n\nBy *verifiable* I mean that the behavior of the code can be verified. And much like the other properties of STVs, the property of being *verifiable* is not binary/either-or. It’s more of a scale.\n\nOne example of verification is formal/computational proofs. The best case is if good computational proofs can show that the program as a whole does what it’s supposed to do. If that’s not possible, proofs may still help with other things. Such as proving the behavior of subroutines. Or proving properties that the program reliably will approximate by some margin.\n\nSometimes it may be possible to construct proofs about whether or not it’s possible to prove that the system does what we want.\n\nAnother example of verifiability is tests that should be satisfied by functions/modules. Concepts such as [test-driven development](https://en.wikipedia.org/wiki/Test-driven_development) are well-established among humans, and a superintelligent AGI-system could be expected to write much more comprehensive tests than a human.\n\nAnd another important example of verification is to test the system as a whole against examples and/or real-world data. For example:\n\n*   If the STV approximates physics somehow, then it could be compared against real-word data.\n*   If the STV predicts some aspects of human behavior, then it could be tested against actual data from actual humans (which the AGI-system that created the STV didn’t have access to).\n*   If the STV generates proofs of some kind, then we can test if it is able to reliably find proofs that already exist (but which the AGI-system that created it didn’t necessarily have access to).\n\n“Hidden” behaviors in STVs\n==========================\n\n***Imaginary friend:*** An unfriendly AI may create STVs in such a way that they pass our verification-process, but still have unfriendly behaviors “hidden” inside them.\n\nSuch “hidden” behaviors may be subtle and hard to find. They can’t necessarily be “pointed to” in some specific lines of code.\n\nDo you imagine that we can guarantee against such “hidden” behaviors?\n\n***Me:*** I agree that this is an important concern. The degree to which we can guarantee against this - or at least make it unlikely/implausible - is from my perspective an open question (that may be worthy of further thinking/analysis).\n\nBut this text touches upon several techniques and principles that I think can make it harder for an AGI-system to “hide” things.\n\nOne way to make it harder to “hide” misbehavior is to look for ways that the STV can be made more general. To see why that potentially could be helpful, you could imagine a calculator. A calculator is specialized in the sense that it only does arithmetic. But if a calculator has special-purpose circuitry for handling 237+418 - or special-purpose functionality for handling multiplication using numbers that are within some specific range - well, that would be suspicious.. I’m giving a simplistic example here, but the same principle should also apply to more complex cases.\n\nSo we could have STVs that look for ways other STVs can be made more general/simple.\n\nAnother way to make it harder to “hide” misbehavior is to not let the AGI itself do the implementation of the sub-modules, but instead make other STVs do the implementation based on high-level specifications.\n\nAlso, it may in some cases make sense to have STVs that generate other STVs. That is to say, if an STV can be described as an “answer-generator”, then an STV that outputs other STVs could be described as an “answer-generator-generator”. This is an even more indirect way of obtaining what we want from an AGI, and I’ll give some concrete examples of this later.\n\nAnother thing that makes it harder to “hide” things is if the same functionality is to be implemented in different ways, based on varying principles/architectures (since it’s more challenging to make different implementations fail in the same way - even more so if it’s not the same AGI that makes both).\n\nWhy-not-bothing & output-convergence\n====================================\n\n***Me:*** As a general principle, the more reasons we have to believe something, the more confident we can feel.\n\nFor example, mathematicians may feel more confident in a theorem if it has been proved in several different ways (with varying proof-methods and starting assumptions).\n\nIf there are 2 different alignment-methodologies that seem promising, then maybe we would want to use both as oracles/genies side by side, and see if they give the same answers. And if there are 3 different alignment-methodologies that seem promising, or 4 - well, you get the picture..\n\nAt almost every level of a system - from the top level to detail-oriented sub-modules - it may often be helpful to try different architectures/implementations/principles, and see if the output is the same.\n\nWe might sometimes refer to this as the principle of *“why not both?”*:\n\n  \n**![](https://lh4.googleusercontent.com/361uZ46Tbkjf83uxUkNrerh5c22-4clKN_AsoFwpfN2KLZIoDYq7ouZFk68ZGkKA-_QfYAfCI4Hm4nxFDVRsZq6ld8y4yccjBwSIYotzGQqA8yRuV-i2S14iJsiXTvJVtj8DssIvrxHMs7puQtA)**\n\n  \nHere are some examples (not a complete list):\n\n*   If we tell an AGI to make an STV for some task/purpose, then the AGI may come up with various different principles and architectures that such an STV could be based on. In such cases, we may want all the STVs, so that we may use them as one (running them side by side, and seeing it as suspicious if they don’t produce converging output).\n*   Should AGI-systems have access to the history of requests it has been given earlier, or should it be reset before new requests are asked? I think the second often is more advisable, but sometimes it may make sense to try both.\n*   Should we give AGIs lots of background-knowledge that may be helpful for the task (but also may be helpful for tricking or manipulating us)? Or should we give it as little information as possible? Or maybe something in between? A good answer may often be yes, yes and yes (we can run different instances of the same AGI-instance).\n*   Who should make any given STV? And for the sub-modules in the architecture of some STV, who should make those? Often it is a good idea to have several implementations from several systems (with varying degrees of human involvement). And then we can see if outputs converge (or if one implementation can be proven to behave the same as another in a “mathematical proof”-like way).\n*   When we wonder about something, do we want to ask the AGI directly, or do we want an “answer-generator”, or maybe an “answer-generator-generator”? Sometimes the answer may be *“yes, yes and yes”*.\n\nThis kind of thing adds complexity, but with the help of a capable AGI-system it should be possible to set up systems that can help us handle such complexities.\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/cd8c13e36923ccb84f1a60689fd62e3ac7f02a256c23504b.png)\n\nDiagram showing how the same answer is obtained via various direct and indirect routes. It may at times be prudent to have orders of magnitude more routes than is shown in this diagram.\n\nAnother recurring theme in this text is indirection and subroutine reuse. If we let the AGI answer directly, and build whole systems by itself in one swoop, then it is for various reasons easier to “hide” misbehavior in the output. But when the AGI outputs smaller “pieces” of functionality that do some specialized thing, and can be put together in various ways to do various different things, then this can give more opportunities to notice if some of the “pieces” don’t always do what they’re supposed to do.\n\nIf an AGI is asked to output systems that are optimized for verifiability, and the systems it suggests don’t exploit these principles, then this may be a good cause for suspicion!\n\nThe space of all claims and inference-steps that are true is huge, and it’s deeply interconnected. And the more of this network you need to account for, and the more “nodes” in the network that can be verified, the harder it is to get away with false claims without contradicting yourself (especially when the subset of the network you account for is dense). More comprehensive and systematic ways of leveraging this principle is one of the things that will be explored in part 3 of this series.\n\nHuman-predicting STVs\n=====================\n\n***Me:*** One thing STVs maybe could be made to do is, is to predict human responses (what a human would think of some argument, how a human would evaluate some piece of code, etc).\n\n***Imaginary friend:*** Aren’t STVs supposed to be “narrow” though? Humans are in a sense AGIs.\n\n***Me:*** I agree that this makes it more of a challenge to obtain STVs that predict humans (while remaining transparent and verifiable).\n\n***Imaginary friend:*** But you still think that we - with the help of an AGI - could obtain STVs that predict human responses? And that we to a sufficient degree could verify that such STVs actually do what we want them to?  \n  \n***Me:*** It seems likely to me that we could. But it also seems plausible that we wouldn’t be able to.\n\nKeep in mind:\n\n*   There are degrees of success. For example, sometimes we may be only 90% confident that an STV works as it should. In such cases, whether we should use it depends a lot on context/specifics. If it is a component in a larger system, then there may be ways to use it where it only can help (in certain instances, if it works), and doesn’t have much opportunity to do damage.\n*   Human-emulating STVs would not need to always have an answer. For example, if an STV has the job of predicting how a human would categorize something, we could accept that it sometimes isn't confident enough to make a prediction.\n\n***Imaginary friend:*** How would an STV predict human behavior though?\n\n***Me:*** Here are some ideas:\n\n<table style=\"background-color:hsl(0, 0%, 90%)\"><tbody><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><p><strong>Brain emulations</strong></p><p>It’s sometimes pointed out that human brain emulations could have large advantages in terms of alignment, but that the first AGIs are unlikely to be brain emulations. But might we have a potentially unaligned AGI help us obtain brain emulations in a safe+verifiable way, without letting it “leave the digital realm”?</p><p>The outputs of brain emulations can be tested against real-world data (data from brain scanning and so on), and the emulations can also be tested for how good they are at predicting the actions/answers/etc of humans (and other animals).</p><p>STVs that emulate brains need not be given to us directly from an AGI. There would be various other options (that aren’t mutually exclusive). Could it, for example, be possible to generate an STV that generates brain emulations based on the DNA of the animal in question (but without being given any direct info about brains)? Might the functionality of such an STV be made in a very general-purpose way (where it’s hard to “hide” details)? Might the functionality used to indirectly generate brain emulations also be used to generate other things, which may be verified (predictions regarding the inner workings of the gut, simulations of the details of the inner workings of a flower, etc)?</p><p>One dilemma in regards to simulations is how fine-grained they are, and how they handle a model where the details are unclear (they simulate something that exists in the real world, but they are not given precise and accurate data of starting conditions). This is not just a dilemma for brain simulations, but for simulations of any physical system. Something we may want is a system that gives an accurate description of the&nbsp;<i>range</i> of possible outcomes, given a description of the&nbsp;<i>range</i> of possible starting conditions. And we want the possibility for the simulation to not spend lots of computation on details we don’t care about (only computing details that are useful, or that are helpful for verification of simulation). Since these are general-purpose challenges, which aren’t specific to brain simulations, we may want to have STVs that can help generate “simulation-approximations” for any physical system. That way we can&nbsp;<i>also</i> test if they do a consistently accurate job when used to make predictions about other physical systems (and not&nbsp;<i>only</i> be tested for whether or not they do a good job with brain emulations).</p><p>When doing anything that resembles emulating a brain, it is very important to avoid/minimize risk of&nbsp;<a href=\"https://reducing-suffering.org/what-are-suffering-subroutines/\"><u>suffering subroutines</u></a>! Failing at this could result in&nbsp;<a href=\"https://www.lesswrong.com/tag/mind-crime\"><u>mind crimes</u></a> and&nbsp;<a href=\"https://centerforreducingsuffering.org/research/intro/\"><u>suffering</u></a>, potentially at an enormous scale!</p><p>At every step of the process we should:</p><ol><li>Avoid simulations that might be conscious.</li><li>Avoid simulating processes that would be likely to experience significant suffering if we were wrong about #1.</li></ol><p>Subroutines with&nbsp;<a href=\"https://en.wikipedia.org/wiki/Valence_(psychology)\"><u>positive valence</u></a> may often be unproblematic, or even a good thing. But it remains to be seen how good our understanding of consciousness will become (the consequences of assuming wrongly can be very bad!).</p></td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><p><strong>Approximations of brain emulations (sometimes based on indirect methods)</strong></p><p>In a sense any brain emulation can be seen as an approximation of a more high-fidelity emulation, but what I mean here is that large components of the STV need not be based on “emulation” at all, as long as the STV predicts aspects of brain states + what the human answers/does.</p><p>In a sense, knowing what the human says/does may be all we are interested in, but if it makes predictions about brain states then this may make verification easier (especially if the STV is based in part on assumptions about which brain states follow from which, which actions correspond to which brain states, brain state sequences that cannot happen, etc).</p></td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><p><strong>Lots of raw and somewhat “hand-coded” probabilistic and modular inference-rules that encode typical human responses within some domain/context</strong></p><p>Inference-rules could reference results from subroutines that use neural nets, but if so we should probably require that we can verify what aspects of the thinking/work is done by the neural net. Maybe&nbsp;<a href=\"https://en.wikipedia.org/wiki/Bayesian_network\"><u>bayesian networks</u></a>, or something similar, could be part of the system somehow. As much as possible of the system should be symbolic/interpretable.</p><p>Imagine if a smart/cooperative human sits in a room, and is given simple multiplication-questions. I would guess that in such a situation we would not need high-fidelity brain-emulation to predict the humans “output” - a calculator could suffice! This simplistic example could work as a weak “existence-proof” of sorts, showing that in restricted situations/domains, the most probable human output can be predicted without using brain emulations. But whether this can be done in a verifiable way for useful tasks is AFAIK an open question.</p><p>It’s sometimes pointed out that it’s infeasible to “hand-code” what we mean by various&nbsp;<a href=\"https://www.lesswrong.com/posts/WBw8dDkAWohFjWQSk/the-cluster-structure-of-thingspace\"><u>fuzzy concepts</u></a> (such as “person”, “human”, “animal”, “dead”, “happy”, etc). But even if that’s infeasible for&nbsp;<i>us</i>, it’s not necessarily infeasible for a superintelligence. And if a superintelligence hand-codes it, there may be ways of verifying that the hand-coded specification does a good job of mimicking human output.</p><p>The AGI would not necessarily do the “hand-coding” directly itself. It could output STVs that do the “hand-coding” (based on various methods). Some such STVs might do the hand-coding based on being given books and internet archives, and building a model of human concepts from what they read/hear/see.</p></td></tr></tbody></table>\n\nIf we end up in a situation where an AGI can make STVs for us that predict human behavior, we might wish that we had more experimental data to test those STVs against. That’s one example of why it may be useful to plan in advance!  \n  \nFor example, if we think we might want to use human-mimicking STVs to evaluate proofs/arguments provided by AGIs/STVs, but in a piecemeal fashion, then it might be helpful to think ahead of time about what the smallest components/steps of such proofs/arguments (that can be evaluated in isolation) might look like.\n\nAnd if we want STVs that for example mimic humans looking over code, then that is also something that may be helpful to plan for in some detail.\n\nSome STVs may be easier to verify if we have brain state data of humans that do the exact same types of tasks that the STVs emulate humans doing (e.g. from MRIs). Sometimes the STVs may emulate people sitting in a similar room as in the experiment, in front of a similar computer to the one in the experiment, etc.\n\nSTVs should be able to describe patterns about how various brain states correspond to both actions (answers, code that is written, etc) and other measurements (posture, eye movement, milliseconds between keystrokes, mouse movement, brain state measurements, etc). Preferably these patterns should be as general as possible (e.g. not just for people with red hair sitting in rooms with yellow pain when the room is 35°C).\n\nThe more experiments we have, and the data we have from experiments (mouse movement, eye movement, video of posture, brain measurements, etc), the more challenging it may be for an STV to “make things up” (without this being discovered when predictions are tested against existing data).\n\nIt may also be helpful to have additional data about humans who participate in experiments (with the informed consent of participants, of course). Their DNA, bodily features, their gut microbiome, etc.\n\nOften it’s not the average human that we want STVs to predict, but rather humans who are usually high in intelligence and [cognitive reflection](https://en.wikipedia.org/wiki/Cognitive_reflection_test) (and are talented at what they do).\n\nSTVs that help with software-development\n========================================\n\n***Me:*** Another thing STVs could help with is software development. Here are some examples:\n\n<table style=\"background-color:hsl(0, 0%, 90%)\"><tbody><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><p><strong>Rewrite code in ways that are proven to not change behavior</strong></p><p>It may be relatively tractable to prove that two pieces of code behave similarly and always will have the same output (and to verify such proofs).</p><p>If you are a programmer, you know ways to predict that a code-modification won't change output. For example, you know that an if-else statement could be replaced by a switch-statement, or that a+b can be replaced with b+a when a and b are numbers. Sometimes you make mistakes when doing this type of reasoning, but this doesn’t mean that proofs that use similar reasoning are impossible (you sometimes make mistakes when doing math as well, but that doesn’t make it impossible to construct mathematical proofs!).</p><p>These kinds of proofs could be computational, meaning that to mechanically check the proofs would be relatively trivial. And all that is needed to show that a given proof is wrong is one counter-example (2 pieces of code that are “proven” to have the same output/behavior, but have different output/behavior when we run them with some specific input). Such a counter-example would not only invalidate that specific proof - it would be a cause for questioning the proof-system itself (and whoever made it). The better and more extensively a proof-system has been tested, the better.</p><p>Reasons for rewriting code, and proving equivalence between pieces of code, could include:&nbsp;</p><ul><li>Rewriting code to be more computationally efficient</li><li>Rewriting code so as to score higher in terms of how easy it is for humans to understand it</li><li>Using these kinds of proofs as building-blocks in other proofs</li></ul></td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><p><strong>Use code-rewrites with proofs as building-blocks in other proofs</strong></p><p>One example could be proofs showing specifically and precisely how the behavior of two pieces of code are different. Another example could be proofs showing how one piece of code approximates another piece of code.</p><p>Here are some examples where I try to allude to what I have in mind (handwavy, but still dense, so feel free to skip):</p><ul><li><i>“Function A has equivalent output to function B, except for when the input is in range C, in which the output of A is equivalent to the output of Function D when D is given output from B as input”</i></li><li><i>“Using search-procedure A we can’t find any computationally efficient way of choosing input for Function A such that the output from A doesn’t approximate the output of Function B, with the probability-distribution that describes this approximation being within probability-distribution-space C”</i></li><li><i>“Among the space of allowable input for function A, there is no sub-space of size larger than B where the output will approximate function B according to approximation-description C, with the exception of input-spaces for which function A always will return an error”</i></li></ul></td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><p><strong>Convert between code and high-level descriptions/specifications of code (and look for discrepancies)</strong></p><p>What a piece of code is supposed to do can be described at various levels of specificity, and in various ways:</p><ul><li>You can describe what it does with text</li><li>You can make diagrams</li><li>You can give specific examples of what the output is (or should be) given specific input</li><li>You can list things that should be true about the output (either all the time, or presuming certain things being true about the input)</li><li>You can have various interactive tools that let you explore the specification of what a piece of code does / is supposed to do. Letting you see step by step how various inputs would be processed, and letting you “zoom” in and out of modules/subroutines.</li></ul><p>When you have STVs that can convert between code and more high-level descriptions of code, one thing you can do is to search for code with significantly different functionality/behavior that would generate the same high-level description. This could potentially be helpful in finding out how some description is underspecified in ways that matter.</p><p>STVs like this could be used to make it harder for other systems (STVs, AGIs, etc) to “hide” details in code that they write. It may be harder to \"hide\" details when you don't implement everything yourself.</p><p>There need not be only one STV that implements a high-level description. Different STVs could implement the code, and the code they output could be compared (by testing if output converges, and by making proofs about how the code-bases are equal/similar/different in terms of behavior).</p><p>We could make “loops” where we convert from high-level descriptions, to code, to high-level descriptions, to code, etc - with each step being done by different/independent STVs. And then we can see if we get “drift”, or if we get more or less equivalent code-bases and high-level descriptions again and again.</p></td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><p><strong>Look for bugs in code</strong></p><p>Some bugs are bugs in a very unambiguous sense, making them clear targets. Other bugs are more subtle and dependent on nuanced human-like judgements.<br>I wouldn’t expect some very non-AGI-like SVG (that understands code and nothing else) to robustly be able to find all bugs. But probabilistically it could help. Having SVGs based on various principles/architectures/settings that search for bugs could make it much harder to “hide” bugs.</p></td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><p><strong>Help to write/propose tests</strong></p><p>When specifying what code should do, it may be helpful to write tests that can detect examples of code doing what it should do and examples of code not doing what it should do. This can be done by humans, but STVs may also suggest tests.</p></td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><p><strong>Look for structural similarities/overlap between different code bases</strong></p><p>These STVs could search for ways to rewrite the different code bases so as to minimize the amount/size of non-overlapping modules.</p><p>A somewhat similar task would be to give the STV access to existing code-bases/libraries while it codes, and have it try to maximize overlap with these existing code-bases/libraries (as one of the things it maximizes for).</p><p>Another somewhat similar task would be to be given a code base that does one thing, and search for the minimal amount of changes that could be made so as to make that code-base do another thing. (For example, the minimal amount of changes to convert a narrow AI that does one thing into a narrow AI that does some other thing.)</p><p>Overlapping modules can more easily be tested in an extensive way. Especially if the STVs that rely on the modules vary in terms of how easy it is to verify if they do a good job or not.</p><p>When code bases are written/rewritten so as to share code, what we want to \"minimize\" is not necessarily <i>\"amount of code that isn't shared\"</i>. Instead, we may to want to minimize <i>\"functionality that is hard to verify to a comprehensive degree\"</i>. That is to say, we may want to solve some problem by compositioning it in terms of other problems - even if this leads to more complexity or computational inefficiency - if doing so can be helpful from the perspective of verification.</p></td></tr><tr><td style=\"border:1pt solid #000000;padding:5pt;vertical-align:top\"><p><strong>Identify parts/aspect of code that don’t only significantly affects output small sections of the space of possible inputs</strong></p><p>Are there ways to rewrite code that would not change output for the great majority of inputs, but would change output for a few inputs?</p><p>Are there ways to simplify code that&nbsp;<i>would</i> change output for most input, but would&nbsp;<i>not</i> significantly change the average score of the output (as ascribed when it’s evaluated)?</p><p>If efficient ways can be found of identifying such changes (when they exist in the space of possible changes), then maybe this could be used to make it harder to intentionally “hide” malicious behavior in STVs.</p><p>If we call the part of input-space where output can be verified A, and the part of input-space where we can’t verify output B, are there ways to rewrite the code (without making the code-base more extensive) that wouldn’t affect output within A negatively, but would have a significant effect on some of the output within B? If so, that may often be a reason to have less trust in that STV (and the system that produced it). Especially if the modified STV and the non-modified STV could be made to produce contradicting output.</p></td></tr></tbody></table>\n\nScope of what STVs could be used for (without becoming too AGI-like)\n====================================================================\n\n***Imaginary friend:***If it is hard to verify that an AGI does what you want, then presumably the same will be the case for STVs the more similar they are to an AGI?\n\nMany tasks seem to be more or less AGI-complete, which presumably means that it would be hard for an STV to do them without being AGI-like.\n\nHow broadly capable do you think an STV can be while still being relatively *narrow/specialized*, and scoring well in terms of *transparency* and *verifiability*?\n\n***Me:*** I don’t have any arguments that are watertight enough to justify opinions on this that are confident and precise. But my gut feeling is somewhat optimistic.\n\n***Imaginary friend:*** One thing you should keep in mind is the limits of symbolic reasoning. Earlier in the history of AI, people tried to make expert systems that rely heavily on [“neat” symbolic reasoning](https://en.wikipedia.org/wiki/Symbolic_artificial_intelligence). But these systems were largely unable to deal with the complexity and nuance of the real world.\n\n***Me:*** But there is a huge difference between what an unassisted human can make and what a superintelligent AGI should be able to make. If a superintelligent AGI doesn’t do a much better job than a human would at coding systems that are transparent and verifiable - well, that would be suspicious..\n\nYes, people have worked on systems that rely heavily on explicit reasoning. But everything that *has* been tried is very crude compared to what *could *be tried. A superintelligent AGI would presumably be much less limited in terms of what it would be able to achieve with such systems. More creative, and more able to create sophisticated systems with huge amounts of “hand-coded” functionality.\n\nThere is this mistake many people have a tendency to make, where they underestimate how far we can get on a problem by pointing to how intractable it is to solve in crude/uncreative ways. One example of this mistake, as I see it, is to say that it **certainly** is impossible to prove how to play perfect chess, since this is impossible to calculate in a straight-forward combinatorial way. Another example would be to say that we cannot solve protein folding, since it is computationally intractable (this used to be a common opinion, but it [isn’t anymore](https://www.deepmind.com/blog/alphafold-a-solution-to-a-50-year-old-grand-challenge-in-biology)).\n\nBoth in terms of being “optimistic” and “pessimistic”, we should try to avoid taking for granted more than what we have a basis for taking for granted. And this also applies to the question of *“how well can a program reason while constrained by requirements for verifiability/provability/transparency?”*.\n\nOne way to think of intelligence is as doing efficient search in possibility-space. To put it a bit simplistically:\n\n*   A board game AI searches for strategies/moves that increase the probability of victory.\n*   A comedian searches for things to say and do that make people entertained in a comedic way.\n*   A programmer searches for lines of code that results in programs that score high in terms of certain criteria.\n*   An inventor searches for construction-steps where the construction-steps and the resulting physical systems scores high in terms of certain criteria.\n*   A digital hacker searches for ways to interact with digital systems that result in behavior/outcomes that the hacker wants (contrary to wishes of designers of said digital systems).\n*   A theorem prover searches for proof-steps that can prove whatever it’s trying to prove.\n*   Etc, etc\n\nAnother way to think of intelligence is as being able to build an accurate and extensive model of some domain. Having an extensive model of the domain sort of implies often being able to answer questions of the form *“which options/strategies/possibilities rank high given conditions x and preferences y?”*. Which implies being good at efficient search through possibility-space.\n\nIn order to be good at searching through possibility-space efficiently, here are some capabilities that may help:\n\n*   Reasoning accurately (having it so that intermediate steps in your reasoning process make a firm foundation for further steps - so as to decrease probability of doing steps that aren’t useful or result in mistakes)\n*   Being good at concepts/abstractions (coming up with them, evaluating them, using them). Identifying various parts of your domain according to things that are true about them (similarities, structural overlap, relations and interactions with other parts of domain, etc), and finding patterns or statistical phenomena that (1) apply to these classifications and (2) are helpful to the search.\n*   Making effective choices for where/how to “look” when looking for the next “step” in the search-process. There is a huge possibility-space of stuff that can be done, which means that the system needs to have good procedures for (1) determining “where” and “how” to search, and for (2) continuously evaluating if a certain “search-path” seems worthy of continued exploration.\n*   Being good at evaluating whether a given result from the “search-process” fulfils the criteria that the search attempts to optimize for (not just making such evaluations at the end of the “search”, but also throughout the “search-process”).\n\nSo narrowly intelligent AI-systems may often have big structural similarities. That is to say, they may be designed such that there are many sub-modules that either are exactly the same or have great structural similarities.\n\nImagine for example an STV with the purpose of making code more understandable for humans. Well, in many ways that is a search process. The heuristics and so on for deciding where to look next in “possibility-space” may be domain-specific, but maybe the STV could be designed to start out without these domain-specific heuristics (and have domain-specific heuristics be generated based on processes that are less domain-specific).\n\nMaybe for some domains it’s hard for us to evaluate if the “search procedure” really is finding the best results that can be found. In such cases it may be an advantage if we can test that same “search procedure” (or “search procedures” that are as similar as possible) in other domains where it’s easier to evaluate the results.\n\n***Imaginary friend:*** Let’s think of “evaluation” and “search” as separate (even though there presumably is lots of inter-play). To put it simplistically, we have “search” and we have a “scoring-function” (or more complex interactions with modules that evaluate “score”). And for the sake of argument, let’s assume that we can verify that in some sense the “search” is “optimal” (in terms of being able to find options in possibility-space that are scored high by the “scoring-function”). Even **if **that’s the case, that still leaves the challenge of making a “scoring-function” that reflects what you actually want?\n\n***Me:*** Sure, that’s a challenge. And how challenging that part is will vary from STV to STV. The “scoring” of some solution is itself a task that other STVs could be specialized to work on. In many cases we may get far even if they don’t do a perfect job to begin with.\n\nConsider for example a scoring-function that evaluates how readable some piece of code would be to a human. Even if this function is imperfect, it will probably still help quite a bit. And if we noticed that it was missing obvious improvements, then this could be fixed.  \n  \nAnd as we gain more capabilities, these capabilities may be used to refine and fortify existing capabilities. For example, if we obtain STVs that are verified to do a good job of predicting humans, then these may be used to more comprehensively test and improve scoring-functions (since they are able to compare 2 different ways to write code with equivalent functionality, and can help predict which way makes it easier for humans to understand it and notice problems).\n\nThanks for now\n==============\n\n***Me:***More things can be said about STVs and their potential uses, but I’ve talked for a long time now. Probably best to save other stuff for later.\n\n***Imaginary friend:***I don’t disagree..\n\n***Me:***Talk to you later then :)\n\n* * *\n\n*To me the concepts/ideas in this series seem under-discussed. But I could be wrong about that, either because (1) the ideas have less merit than I think or (2) because they already are discussed/understood among alignment researchers to a greater degree than I realize. I welcome more or less any feedback, and appreciate any help in becoming less wrong.*"
    },
    "voteCount": 4,
    "forceInclude": true
  },
  {
    "_id": "i42Dfoh4HtsCAfXxL",
    "url": "https://radimentary.wordpress.com/2018/01/10/babble/",
    "title": "Babble",
    "slug": "babble",
    "author": "alkjash",
    "question": false,
    "tags": [
      {
        "name": "Rationality"
      },
      {
        "name": "Poetry"
      },
      {
        "name": "Babble and Prune"
      }
    ],
    "tableOfContents": {
      "sections": [
        {
          "title": "Baby Babble",
          "anchor": "Baby_Babble",
          "level": 1
        },
        {
          "title": "Poetry is Babble Study",
          "anchor": "Poetry_is_Babble_Study",
          "level": 1
        },
        {
          "title": "Reading is Outsourcing Babble",
          "anchor": "Reading_is_Outsourcing_Babble",
          "level": 1
        },
        {
          "title": "Tower of Babble",
          "anchor": "Tower_of_Babble",
          "level": 1
        },
        {
          "title": "NP",
          "anchor": "NP",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        },
        {
          "anchor": "comments",
          "level": 0,
          "title": "31 comments"
        }
      ],
      "headingsCount": 7
    },
    "contents": {
      "markdown": "This post is an exercise in \"identifying with the algorithm.\" I'm a big fan of the probabilistic method and randomized algorithms, so my biases will show.\n\nHow do human beings produce knowledge? When we describe rational thought processes, we tend to think of them as essentially deterministic, deliberate, and algorithmic. After some self-examination, however, I've come to think that my process is closer to babbling many random strings and later filtering by a heuristic. I think verbally, and my process for generating knowledge is virtually indistinguishable from my process for generating speech, and also quite similar to my process for generating writing.\n\nHere's a simplistic model of how this works. I try to build a coherent sentence. At each step, to pick the next word, I randomly generate words in the category (correct part of speech, relevance) and sound them out one by one to see which continues the sentence most coherently. So, instead of deliberately and carefully generating sentences in one go, the algorithm is something like:\n\n1.  Babble. Use a weak and local filter to randomly generate a lot of possibilities. Is the word the right part of speech? Does it lie in the same region of thingspace? Does it fit the context?\n2.  Prune. Use a strong and global filter to test for the best, or at least a satisfactory, choice. With this word in the blank, do I actually believe this sentence? Does the word have the right connotations? Does the whole thought read smoothly?\n\nThis is a babble about embracing randomness.\n\nBaby Babble\n-----------\n\n[Research on language development](https://en.wikipedia.org/wiki/Babbling#The_transition_from_babbling_to_language) suggests that baby babble is an direct forerunner to language. You might imagine that infants learn by imitation, and that baby babble is just an imperfect imitation of words the baby hears, and progress occurs as they physiologically adapt to better produce those sounds. You would be wrong.\n\nInstead, infants are initially capable of producing _all the phonemes_ that exist in all human languages, and they slowly prune out which ones they need via reinforcement learning. Based on the sounds that their parents produce and respond to, babies slowly filter out unnecessary phonemes. Their babbles begin to drift as they prune out more and more phonemes, and they start to combine syllables into proto-words. Babble is the process of generating random sounds, and looking for clues about which ones are useful. Something something reinforcement learning partially observable Markov decision process I'm in over my head.\n\nSo, we've learned that babies use the Babble and Prune algorithm to learn language. But this is quite a general algorithm, and evolution is a conservative force. It stands to reason that human beings might learn other things by a similar algorithm. I don't think it's a particularly controversial suggestion that human thought proceeds roughly by cheaply constructing a lot of low-resolution hypotheses and then sieving from them by allowing them to play out to their logical conclusions.\n\nThe point I want to emphasize is that the algorithm has two distinct phases, both of which can be independently optimized. The stricter and stronger your Prune filter, the higher quality content you stand to produce. But one common bug is related to this: if the quality of your Babble is much lower than that of your Prune, you may end up with nothing to say. Everything you can imagine saying or writing sounds cringey or content-free. Ten minutes after the conversation moves on from that topic, your Babble generator finally returns that witty comeback you were looking for. You'll probably spend your entire evening waiting for an opportunity to force it back in.\n\nYour pseudorandom Babble generator can also be optimized, and in two different ways. On the one hand, you can improve the weak filter you're using, to increase the probability of generating higher-quality thoughts. The other way is one of the things named \"creativity\": you can try to eliminate systematic biases in the Babble generator, with the effect of hitting a more uniform subset of relevant concept-space. Exercises that might help include expanding your vocabulary, reading outside your comfort zone, and engaging in the subtle art of nonstandard sentence construction.\n\nPoetry is Babble Study\n----------------------\n\nPoetry is at its heart an isolation exercise for your Babble generator. When creating poetry, you replace your complex, inarticulate, and highly optimized Prune filter with a simple, explicit, and weird one that you're not attached to. Instead of picking words that maximize meaning, relevance, or social signals, you pick words with the right number of syllables that rhyme correctly and follow the right meter.\n\nNow, with the Prune filter simplified and fixed, all the attention is placed on the Babble. What does it feel like to write a poem (not one of those free-form modern ones)? Probably most of your effort is spent Babbling almost-words that fit the meter and rhyme scheme. If you're anything like me, it feels almost exactly like playing a game of Scrabble, fitting letters and syllables onto a board by trial and error. Scrabble is just like poetry: it's all about being good at Babble. And no, I graciously decline to write poetry in public, even though Scrabble does conveniently rhyme with Babble.\n\nPuns and word games are Babble. You'll notice that when you Babble, each new word isn't at all independent from its predecessors. Instead, Babble is more like initiating a random walk in your dictionary, one letter or syllable or inferential step at a time. That's why [word ladders](https://en.wikipedia.org/wiki/Word_ladder) are so appealing - because they stem from a natural cognitive algorithm. I think Scott Alexander's writing quality is great partly because of [his love of puns](http://unsongbook.com/), a sure sign he has a great Babble generator.\n\nIf poetry and puns are phonetic Babble, then \"[Deep Wisdom](http://lesswrong.com/lw/k8/how_to_seem_and_be_deep/)\" is semantic Babble. Instead of randomly arranging words by sound, we're arranging a rather small set of words to sound wise. More often than not, \"deep wisdom\" boils down to word games anyway, e.g. [wise old sayings](http://www.wiseoldsayings.com/wisdom-quotes/):\n\n\"A blind person who sees is better than a seeing person who is blind.\"\n\n\"A proverb is a short sentence based on long experience.\"\n\n\"Economy is the wealth of the poor and the wisdom of the rich.\"\n\nReading is Outsourcing Babble\n-----------------------------\n\nReading and conversation outsource Babble to others. Instead of using your own Babble generator, you flood your brain with other people's words, and then apply your Prune filter. Because others have already Pruned once, the input is particularly high-quality Babble, and you reap particularly beautiful fruit. How many times have you read a thousand-page book, only to fixate on a handful of striking lines or passages?\n\nPrune goes into overdrive when you outsource Babble. A bug I mentioned earlier is having way too strict of a Prune filter, compared to the quality of your Babble. This occurs particularly to people who read and listen much more than they write or speak. When they finally trudge into the attic and turn on that dusty old Babble generator, it doesn't produce thoughts nearly as coherent, witty, or wise as their hyper-developed Prune filter is used to processing.\n\nImpose Babble tariffs. Your conversation will never be as dry and smart as something from a sitcom. If you can't think of anything to say, relax your Prune filter at least temporarily, so that your Babble generator can catch up. Everyone starts somewhere - Babbling platitudes is better than being silent altogether.\n\nConversely, some people have no filter, and these are exactly the kind of people who don't read or listen enough. If all your Babble goes directly to your mouth, you need to install a better Prune filter. Impose export tariffs.\n\nThe reason the [Postmodernism Generator](http://www.elsewhere.org/journal/pomo/) is so fun to read is because computers are now capable of producing great Babble. Reading poetry and randomly generated postmodernism, talking to chatbots, these activities all amount to frolicking in the uncanny valley between Babble and the Pruned.\n\nTower of Babble\n---------------\n\nA wise man once said, \"Do not build Towers out of Babble. You wouldn't build one out of Pizza, would you?\"\n\nNP\n--\n\nNP is the God of Babble. His law is: humans will always be much better at verifying wisdom than producing it. Therefore, go forth and Babble! After all, how did Shakespeare write his famous plays, except by randomly pressing keys on a keyboard?\n\nNP has a little brother called P. The law of P is: never try things you don't understand completely. Randomly thrashing around will get you nowhere.\n\nP believes himself to be a God, an equal to his brother. He is not."
    },
    "voteCount": 99,
    "forceInclude": true
  },
  {
    "_id": "XvN2QQpKTuEzgkZHY",
    "url": null,
    "title": "Being the (Pareto) Best in the World",
    "slug": "being-the-pareto-best-in-the-world",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "Rationality"
      },
      {
        "name": "Efficient Market Hypothesis"
      },
      {
        "name": "Careers"
      }
    ],
    "tableOfContents": {
      "sections": [
        {
          "title": "Elbow Room",
          "anchor": "Elbow_Room",
          "level": 1
        },
        {
          "title": "Problem Density",
          "anchor": "Problem_Density",
          "level": 1
        },
        {
          "title": "Dimensionality",
          "anchor": "Dimensionality",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        },
        {
          "anchor": "comments",
          "level": 0,
          "title": "54 comments"
        }
      ],
      "headingsCount": 5
    },
    "contents": {
      "markdown": "The generalized efficient markets (GEM) principle says, roughly, that things which would give you a big windfall of money and/or status, will not be easy. If such an opportunity were available, someone else would have already taken it. You will never find a $100 bill on the floor of Grand Central Station at rush hour, because someone would have picked it up already.\n\nOne way to circumvent GEM is to be the best in the world at some relevant skill. A superhuman with hawk-like eyesight and the speed of the Flash might very well be able to snag $100 bills off the floor of Grand Central. More realistically, even though financial markets are the ur-example of efficiency, a handful of firms do make impressive amounts of money by being faster than anyone else in their market. I’m unlikely to ever find a proof of the Riemann Hypothesis, but Terry Tao might. Etc.\n\nBut being the best in the world, in a sense sufficient to circumvent GEM, is not as hard as it might seem at first glance (though that doesn’t exactly make it easy). The trick is to exploit dimensionality.\n\nConsider: becoming one of the world’s top experts in proteomics is hard. Becoming one of the world’s top experts in macroeconomic modelling is hard. But how hard is it to become sufficiently expert in proteomics and macroeconomic modelling that nobody is better than you at both simultaneously? In other words, how hard is it to reach the Pareto frontier?\n\nHaving reached that Pareto frontier, you will have circumvented the GEM: you will be the single best-qualified person in the world for (some) problems which apply macroeconomic modelling to proteomic data. You will have a realistic shot at a big money/status windfall, with relatively little effort.\n\n(Obviously we’re oversimplifying a lot by putting things like “macroeconomic modelling skill” on a single axis, and breaking it out onto multiple axes would strengthen the main point of this post. On the other hand, it would complicate the explanation; I’m keeping it simple for now.)\n\nLet’s dig into a few details of this approach…\n\nElbow Room\n----------\n\nThere are many table tennis players, but only one best player in the world. This is a side effect of ranking people on one dimension: there’s only going to be one point furthest to the right (absent a tie).\n\nPareto optimality pushes us into more dimensions. There’s only one best table tennis player, and only one best 100-meter sprinter, but there can be an unlimited number of Pareto-optimal table tennis/sprinters.\n\nProblem is, for GEM purposes, elbow room matters. Maybe I’m the on the pareto frontier of Bayesian statistics and gerontology, but if there’s one person just little bit better at statistics and worse at gerontology than me, and another person just a little bit better at gerontology and worse at statistics, then GEM only gives me the advantage over a tiny little chunk of the skill-space.\n\n![](https://docs.google.com/drawings/u/1/d/sDVk_OQ89TzmIPruCPKmzlA/image?w=624&h=413&rev=323&ac=1&parent=1mnaPbYJycVXegH2JbhxHm4rkf1jNfbzlepMpTbvXclo)\n\nThis brings up another aspect…\n\nProblem Density\n---------------\n\nClaiming a spot on a Pareto frontier gives you some chunk of the skill-space to call your own. But that’s only useful to the extent that your territory contains useful problems.\n\nTwo pieces factor in here. First, how large a territory can you claim? This is about elbow room, as in the diagram above. Second, what’s the density of useful problems within this region of skill-space? The table tennis/sprinting space doesn’t have a whole lot going on. Statistics and gerontology sounds more promising. Cryptography and monetary economics is probably a particularly rich Pareto frontier these days. (And of course, we don’t need to stop at two dimensions - but we’re going to stop there in this post in order to keep things simple.)\n\nDimensionality\n--------------\n\nOne problem with this whole GEM-vs-Pareto concept: if chasing a Pareto frontier makes it easier to circumvent GEM and gain a big windfall, then why doesn’t everyone chase a Pareto frontier? Apply GEM to the entire system: why haven’t people already picked up the opportunities lying on all these Pareto frontiers?\n\nAnswer: dimensionality. If there’s 100 different specialties, then there’s only 100 people who are the best within their specialty. But there’s 10k pairs of specialties (e.g. statistics/gerontology), 1M triples (e.g. statistics/gerontology/macroeconomics), and something like 10^30 combinations of specialties. And each of those pareto frontiers has room for more than one person, even allowing for elbow room. Even if only a small fraction of those combinations are useful, there’s still a _lot_ of space to stake out a territory.\n\nAnd to a large extent, people do pursue those frontiers. It’s no secret that an academic can easily find fertile fields by working with someone in a different department. “Interdisciplinary” work has a reputation for being unusually high-yield. Similarly, carrying scientific work from lab to market has a reputation for high yields. Thanks to the “curse” of dimensionality, these goldmines are not in any danger of exhausting."
    },
    "voteCount": 183,
    "forceInclude": true
  },
  {
    "_id": "4QemtxDFaGXyGSrGD",
    "url": null,
    "title": "\"Other people are wrong\" vs \"I am right\"",
    "slug": "other-people-are-wrong-vs-i-am-right",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "Rationality"
      },
      {
        "name": "Updated Beliefs (examples of)"
      },
      {
        "name": "Chesterton's Fence"
      },
      {
        "name": "Distinctions"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "I’ve recently been spending some time thinking about the rationality mistakes I’ve made in the past. Here’s an interesting one: I think I have historically been too hasty to go from “other people seem very wrong on this topic” to “I am right on this topic”.\n\nThroughout my life, I’ve often thought that other people had beliefs that were really repugnant and stupid. Now that I am older and wiser, I still think I was correct to think that these ideas were repugnant and stupid. Overall I was probably slightly insufficiently dismissive of things like the opinions of apparent domain experts and the opinions of people who seemed smart whose arguments I couldn’t really follow. I also overrated conventional wisdom about factual claims about how the world worked, though I underrated conventional wisdom about how to behave.\n\nExamples of ideas where I thought the conventional wisdom was really dumb:\n\n- I thought that animal farming was a massive moral catastrophe, and I thought it was a sign of terrible moral failure that almost everyone around me didn’t care about this and wasn’t interested when I brought it up.\n- I thought that AI safety was a big deal, and I thought the arguments against it were all pretty stupid. (Nowadays the conventional wisdom has a much higher opinion of AI safety; I’m talking about 2010-2014.)\n- I thought that people have terrible taste in economic policy, and that they mostly vote for good-sounding stuff that stops sounding good if you think about it properly for even a minute\n- I was horrified by people proudly buying products that said “Made in Australia” on them; I didn’t understand how that wasn’t obviously racist, and I thought that we should make it much easier to allow anyone who wants to to come live in Australia. (This one has become much less controversial since Trump inadvertently convinced liberals that they should be in favor of immigration liberalization.)\n- I thought and still think that a lot of people’s arguments about why it’s good to call the police on bike thieves were dumb. See eg many of the arguments people made in response to [a post of mine about this](https://www.facebook.com/bshlgrs/posts/10208229326305580) (that in fairness was a really dumb post, IMO)\n\nI think I was right about other people being wrong. However, I think that my actual opinions on these topics were pretty confused and wrong, much more than I thought at the time. Here’s how I updated my opinion for all the things above:\n\n- I have updated against the simple view of hedonic utilitarianism under which it’s plausible that simple control systems can suffer. A few years ago, I was seriously worried that the future would contain much more factory farming and therefore end up net negative; I now think that I overrated this fear, because (among other arguments) almost no-one actually endorses torturing animals, we just do it out of expediency, and in the limit of better technology our weak preferences will override our expediency.\n- My understanding of AI safety was “eventually someone will build a recursively self improving singleton sovereign AGI, and we need to figure out how to build it such that it can have an off switch and it implements some good value function instead of something bad.” I think this picture was massively oversimplified. On the strategic side, I didn’t think about the possibilities of slower takeoffs or powerful technologies without recursive self improvement; on the technical safety side, I didn’t understand that it’s hard to even build a paperclip maximizer, and a lot of our effort might go into figuring out how to do that.\n- Other people have terrible taste in economic policy, but I think that I was at the time overconfident in various libertarianish ideas that I’m now less enthusiastic about. Also, I no longer think it’s a slam dunk that society is better off from becoming wealthier, because of considerations related to the far future, animals, and whether more money makes us happier.\n- I think that immigration liberalization is more dangerous than I used to think, because rich societies seem to generate massive positive externalities for the rest of the world and it seems possible that a sudden influx of less educated people with (in my opinion) worse political opinions might be killing the goose that lays the golden eggs.\n- Re bike thieves: I think that even though utilitarianism is good and stuff, it’s extremely costly to have thievery be tolerated, because then you have to do all these negative-sum things like buying bike locks. Also it seems like we’re generally better off if people help with enforcement of laws.\n\n---\n\nIn all of these cases, my arguments against others were much higher quality than my actual beliefs. Much more concerningly, I think I was much better at spotting the holes in other people’s arguments than spotting holes in my own.\n\nThere’s also a general factor here of me being overconfident in the details of ideas that had some ring of truth to them. Like, the importance of AGI safety seemed really obvious to me, and I think that my sense of obviousness has historically been pretty good at spotting arguments that later stand up to intense scrutiny. But I was massively overconfident in my particular story for how AGI would go down. I should have been more disjunctive: I should have said “It sure seems like something like this ought to happen, and it seems like step three could happen in any of these four possible ways, and I don’t know which of them will be true, and maybe it will actually be another one, but I feel pretty convinced that there’s some way it will happen”.\n\nHere are some other ideas which I continue to endorse which had that ring of truth to them, but whose details I’ve been similarly overconfident about. (Some of these are pretty obscure.)\n\n- The simulation hypothesis\n- UDASSA\n- The malignancy of the universal prior\n- The mathematical universe hypothesis\n- Humans have weird complex biases related to categories like race and gender, and we should be careful about this in our thinking. (Nowadays this idea is super widespread and so it feels weird to put it in the same list as all these crazy other ideas. But when I first encountered it seriously in my first year of college, it felt like an interesting and new idea, in the same category as many of the cognitive biases I heard about on LessWrong.)\n\nAnd here are ideas which had this ring of truth to them that I no longer endorse:\n\n- We should fill the universe with hedonium.\n- The future might be net negative, because humans so far have caused great suffering with their technological progress and there’s no reason to imagine that this will change. Futurists are biased against this argument because they personally don’t want to die and have a strong selfish desire for human civilization to persist.\n- Because of Landauer’s limit, civilizations have an incentive to aestivate. (This one is wrong because it involves a [misunderstanding of thermodynamics](https://arxiv.org/abs/1902.06730?fbclid=IwAR144hDZx64uJYn0HLkhXtf5PUm7CedUqd6rUiQBfYft79U_IfvsSLsOn-A).)\n\n---\n\nMy bias towards thinking my own beliefs are more reasonable than they are would be disastrous if it prevented me from changing my mind in response to good new arguments. Luckily, I don’t think that I am particularly biased in that direction, for two reasons. Firstly, when I’m talking to someone who thinks I’m wrong, for whatever reason I usually take them pretty seriously and I have a small crisis of faith that prompts me to go off and reexamine my beliefs a bunch. Secondly, I think that most of the time that people present an argument which later changes my mind, my initial reaction is confusion rather than dismissiveness.\n\nAs an example of the first: Once upon a time I told someone I respected that they shouldn’t eat animal products, because of the vast suffering caused by animal farming. He looked over scornfully and told me that it was pretty rich for me to say that, given that I use Apple products—hadn’t I heard about the abusive Apple factory conditions and how they have nets to prevent people killing themselves by jumping off the tops of the factories? I felt terrified that I’d been committing some grave moral sin, and then went off to my room to research the topic for an hour or two. I eventually became convinced that the net effect of buying Apple products on human welfare is probably very slightly positive but small enough to not worry about, and also it didn’t seem to me that there’s a strong deontological argument against doing it.\n\n(I went back and told the guy about the result of me looking into it. He said he didn’t feel interested in the topic anymore and didn’t want to talk about it. I said “wow, man, I feel pretty annoyed by that; you gave me a moral criticism and I took it real seriously; I think it’s bad form to not spend at least a couple minutes hearing about what I found.” Someone else who was in the room, who was very enthusiastic about social justice, came over and berated me for trying to violate someone else’s preferences about not talking about something. I learned something that day about how useful it is to take moral criticism seriously when it’s from people who don’t seem to be very directed by their morals.)\n\nOther examples: When I first ran across charismatic people who were in favor of deontological values and social justicey beliefs, I took those ideas really seriously and mulled them over a lot. A few weeks ago, someone gave me some unexpectedly harsh criticism about my personal manner and several aspects of how I approach my work; I updated initially quite far in the direction of their criticism, only to update 70% of the way back towards my initial views after I spent ten more hours thinking and talking to people about it.\n\nExamples of the second: When I met people whose view of AI safety didn’t match my own naive view, I felt confused and took them seriously (including when they were expressing a bunch of skepticism of MIRI). When my friend Howie told me he thought the criminal justice system was really racist, I was surprised and quickly updated my opinion to “I am confused about this”, rather than dismissing him.\n\nI can’t think of cases where I initially thought an argument was really stupid but then it ended up convincing either me or a majority of people who I think of as my epistemic peers and superiors (eg people who I think have generally good judgement at EA orgs).\n\nHowever, I can think of cases where I felt initially that an argument is dumb, but lots of my epistemic peers think that the argument is at least sort of reasonable. I am concerned by this and I’m trying to combat it. For example, the following arguments are in my current list of things that I am worried I’m undervaluing because they initially seem implausible to me, and are on my to-do list to eventually look into more carefully: Drexler’s Comprehensive AI Systems. AI safety via ambitious value learning. Arguments that powerful AI won’t lead to a singleton.\n\nPlease let me know if you have examples along these lines where I seemed dumber than I’m presenting here.\n\n----\n\nHere’s another perspective on why my approach might be a problem. I think that people are often pretty bad at expressing why they believe things, and in particular they don’t usually say “I don’t know why I believe this, but I believe it anyway.” So if I dismiss arguments that suck, I might be dismissing useful knowledge that other people have gained through experience.\n\nI think I’ve made mistakes along these lines in the past. For example, I used to have a much lower opinion of professionalism than I now do. And there are a couple of serious personal mistakes I’ve made where I looked around for the best arguments against doing something weird I wanted to do, and all of those arguments sucked, and then I decided to do the weird thing, and then it was a bad idea.\n\nKatja Grace [calls this mistake](https://meteuphoric.com/2015/09/06/mistakes-3-breaking-chestertons-fence-in-the-presence-of-bull/) “breaking Chesterton’s fence in the presence of bull”.\n\nThis would suggest the heuristic “Take received wisdom on topics into account, even if you ask people where the received wisdom comes from and they tell you a source that seems extremely unreliable”.\n\nI think this heuristic is alright but shouldn’t be an overriding consideration. The ideas that evolve through the experience of social groups are valuable because they’re somewhat selected for truth and importance. But the selection process for these ideas is extremely simple and dumb.\n\nI’d expect that in most cases where something is bad, there is a legible argument for why we shouldn’t do it (where I’m including arguments from empirical evidence as legible arguments). I’d prefer to just learn all of the few things that society implicitly knows, rather than giving up every time it disagrees with me.\n\nMaybe this is me being arrogant again, but I feel like the mistake I made with the bike-stealing thing wasn’t me refusing to bow to social authority, it was me not trying hard enough to think carefully about the economics of the situation. My inside view is that if I now try to think about economics, I don’t need to incorporate that much outside-view-style discounting of my own arguments.\n\nI have the big advantage of being around people who are really good at articulating the actual reasons why things are bad. Possibly the number one strength of the rationalist community is creating and disseminating good explicit models of things that are widely implicitly understood (eg variants of Goodhart’s law, Moloch, Chesterton’s fence, the unilateralist’s curse, “toxoplasma of rage”). If I was in any other community, I’m worried that I’d make posts like the one about the bike, and no-one would be able to articulate why I was wrong in a way that was convincing. So I don’t necessarily endorse other people taking the strategy I take.\n\nI am not aware of that many cases where I believed something really stupid because all the common arguments against it seemed really dumb to me. If I knew of more cases like this, I’d be more worried about this.\n\n---\n\nClaire Zabel says, in response to all this:\n\n> I'd say you're too quick to buy a whole new story if it has the ring of truth, and too quick to ask others (and probably yourself) to either refute on the spot, or accept, a complex and important new story about something about the world, and leave too little room to say \"this seems sketchy but I can't articulate how\" or \"I want to think about it for a while\" or \"I'd like to hear the critics' counterarguments\" or \"even though none of the above has yielded fruit, I'm still not confident about this thing\"\n\nThis seems plausible. I spend a bunch of time trying to explain why I’m worried about AI risk to people who don’t know much about the topic. This requires covering quite a lot of ground; perhaps I should try harder to explicitly say “by the way, I know I’m telling you a lot of crazy stuff; you should take as long as it takes to evaluate all of this on your own; my goal here is just to explain what I believe; you should use me as a datapoint about one place that human beliefs sometimes go after thinking about the subject.”\n\n---\n\nI feel like my intuitive sense of whether someone else’s argument is roughly legit is pretty good, and I plan to continue feeling pretty confident when I intuitively feel like someone else is being dumb. But I am trying to not make the jump from “I think that this argument is roughly right” to “I think that all of the steps in this fleshed out version of that argument are roughly right”. Please let me know if you think I’m making that particular mistake."
    },
    "voteCount": 104,
    "forceInclude": true
  },
  {
    "_id": "G5TwJ9BGxcgh5DsmQ",
    "url": null,
    "title": "Yes Requires the Possibility of No\n",
    "slug": "yes-requires-the-possibility-of-no",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "Conservation of Expected Evidence"
      },
      {
        "name": "Filtered Evidence"
      },
      {
        "name": "Rationality"
      },
      {
        "name": "Internal Double Crux"
      },
      {
        "name": "Principles"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "1\\. A group wants to try an activity that really requires a lot of group buy in. The activity will not work as well if there is doubt that everyone really wants to do it. They establish common knowledge of the need for buy in. They then have a group conversation in which several people make comments about how great the activity is and how much they want to do it. Everyone wants to do the activity, but is aware that if they did not want to do the activity, it would be awkward to admit. They do the activity. It goes poorly.\n\n2\\. Alice strongly wants to believe A. She searches for evidence of A. She implements a biased search, ignoring evidence against A. She finds justifications for her conclusion. She can then point to the justifications, and tell herself that A is true. However, there is always this nagging thought in the back of her mind that maybe A is false. She never fully believes A as strongly as she would have believed it if she just implemented an an unbiased search, and found out that A was, in fact, true.\n\n3\\. Bob wants Charlie to do a task for him. Bob phrases the request in a way that makes Charlie afraid to refuse. Charlie agrees to do the task. Charlie would have been happy to do the task otherwise, but now Charlie does the task while feeling resentful towards Bob for violating his consent.\n\n4\\. Derek has an accomplishment. Others often talk about how great the accomplishment is. Derek has imposter syndrome and is unable to fully believe that the accomplishment is good. Part of this is due to a desire to appear humble, but part of it stems from Derek's lack of self trust. Derek can see lots of pressures to believe that the accomplishment is good. Derek does not understand exactly how he thinks, and so is concerned that there might be a significant bias that could cause him to falsely conclude that the accomplishment is better than it is. Because of this he does not fully trust his inside view which says the accomplishment is good.\n\n5\\. Eve is has an aversion to doing B. She wants to eliminate this aversion. She tries to do an internal double crux with herself. She identifies a rational part of herself who can obviously see that it is good to do B. She identifies another part of herself that is afraid of B. The rational part thinks the other part is stupid and can't imagine being convinced that B is bad. The IDC fails, and Eve continues to have an aversion to B and internal conflict.\n\n6\\. Frank's job or relationship is largely dependent to his belief in C. Frank really wants to have true beliefs, and so tries to figure out what is true. He mostly concludes that C is true, but has lingering doubts. He is unsure if he would have been able to conclude C is false under all the external pressure.\n\n7\\. George gets a lot of social benefits out of believing D. He believes D with probability 80%, and this is enough for the social benefits. He considers searching for evidence of D. He thinks searching for evidence will likely increase the probability to 90%, but it has a small probability of decreasing the probability to 10%. He values the social benefit quite a bit, and chooses not to search for evidence because he is afraid of the risk.\n\n8\\. Harry sees lots of studies that conclude E. However, Harry also believes there is a systematic bias that makes studies that conclude E more likely to be published, accepted, and shared. Harry doubts E.\n\n9\\. A bayesian wants to increase his probability of proposition F, and is afraid of decreasing the probability. Every time he tries to find a way to increase his probability, he runs into an immovable wall called the conservation of expected evidence. In order to increase his probability of F, he must risk decreasing it."
    },
    "voteCount": 85,
    "forceInclude": true
  },
  {
    "_id": "Kbm6QnJv9dgWsPHQP",
    "url": null,
    "title": "Schelling fences on slippery slopes",
    "slug": "schelling-fences-on-slippery-slopes",
    "author": "Scott Alexander",
    "question": false,
    "tags": [
      {
        "name": "Rationality"
      },
      {
        "name": "Game Theory"
      },
      {
        "name": "Value Drift"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "Slippery slopes are themselves a slippery concept. Imagine trying to explain them to an alien:  \n  \n\"Well, we right-thinking people are quite sure that the Holocaust happened, so banning Holocaust denial would shut up some crackpots and improve the discourse. But it's one step on the road to things like banning unpopular political positions or religions, and we right-thinking people oppose that, so we won't ban Holocaust denial.\"  \n  \nAnd the alien might well respond: \"But you could just ban Holocaust denial, but not ban unpopular political positions or religions. Then you right-thinking people get the thing you want, but not the thing you don't want.\"  \n  \nThis post is about some of the replies you might give the alien.  \n  \n**Abandoning the Power of Choice**  \n  \nThis is the boring one without any philosophical insight that gets mentioned only for completeness' sake. In this reply, giving up a certain point risks losing the ability to decide whether or not to give up other points.  \n  \nFor example, if people gave up the right to privacy and allowed the government to monitor all phone calls, online communications, and public places, then if someone launched a military coup, it would be very difficult to resist them because there would be no way to secretly organize a rebellion. This is also brought up in arguments about gun control a lot.  \n  \nI'm not sure this is properly thought of as a slippery slope argument at all. It seems to be a more straightforward \"Don't give up useful tools for fighting tyranny\" argument.  \n  \n**The Legend of Murder-Gandhi**  \n  \n[Previously](http://yudkowsky.net/singularity) [on Less Wrong's](/lw/2vj/gandhi_murder_pills_and_mental_illness/) _The Adventures of Murder-Gandhi_: Gandhi is offered a pill that will turn him into an unstoppable murderer. He refuses to take it, because in his current incarnation as a pacifist, he doesn't want others to die, and he knows that would be a consequence of taking the pill. Even if we offered him $1 million to take the pill, his abhorrence of violence would lead him to refuse.  \n  \nBut suppose we offered Gandhi $1 million to take a different pill: one which would decrease his reluctance to murder by 1%. This sounds like a pretty good deal. Even a person with 1% less reluctance to murder than Gandhi is still pretty pacifist and not likely to go killing anybody. And he could donate the money to his favorite charity and perhaps save some lives. Gandhi accepts the offer.  \n  \nNow we iterate the process: every time Gandhi takes the 1%-more-likely-to-murder-pill, we offer him another $1 million to take the same pill again.  \n  \nMaybe original Gandhi, upon sober contemplation, would decide to accept $5 million to become 5% less reluctant to murder. Maybe 95% of his original pacifism is the only level at which he can be _absolutely sure_ that he will still pursue his pacifist ideals.  \n  \nUnfortunately, original Gandhi isn't the one making the choice of whether or not to take the 6th pill. 95%-Gandhi is. And 95% Gandhi doesn't care _quite_ as much about pacifism as original Gandhi did. He still doesn't want to become a murderer, but it wouldn't be a disaster if he were just 90% as reluctant as original Gandhi, that stuck-up goody-goody.  \n  \nWhat if there were a general principle that each Gandhi was comfortable with Gandhis 5% more murderous than himself, but no more? Original Gandhi would start taking the pills, hoping to get down to 95%, but 95%-Gandhi would start taking five more, hoping to get down to 90%, and so on until he's rampaging through the streets of Delhi, killing everything in sight.  \n  \nNow we're tempted to say Gandhi shouldn't even take the first pill. But this also seems odd. Are we really saying Gandhi shouldn't take what's basically a free million dollars to turn himself into 99%-Gandhi, who might well be nearly indistinguishable in his actions from the original?  \n  \nMaybe Gandhi's best option is to \"fence off\" an area of the slippery slope by establishing a [Schelling](/lw/14a/thomas_schellings_strategy_of_conflict/) point - an arbitrary point that takes on special value as a dividing line. If he can hold himself to the precommitment, he can maximize his winnings. For example, original Gandhi could swear a mighty oath to take only five pills - or if he didn't trust even his own legendary virtue, he could give all his most valuable possessions to a friend and tell the friend to destroy them if he took more than five pills. This would commit his future self to stick to the 95% boundary (even though that future self is itching to try to the same precommitment strategy to stick to its own 90% boundary).  \n  \nReal slippery slopes will resemble this example if, each time we change the rules, we also end up changing our opinion about how the rules should be changed. For example, I think the Catholic Church may be working off a theory of \"If we give up this traditional practice, people will lose respect for tradition and want to give up even more traditional practices, and so on.\"  \n  \n**Slippery Hyperbolic Discounting**  \n  \nOne evening, I start playing _Sid Meier's Civilization_ (IV, if you're wondering - V is terrible). I have work tomorrow, so I want to stop and go to sleep by midnight.  \n  \nAt midnight, I consider my alternatives. For the moment, I feel an urge to keep playing Civilization. But I know I'll be miserable tomorrow if I haven't gotten enough sleep. Being a [hyperbolic discounter](/lw/6c/akrasia_hyperbolic_discounting_and_picoeconomics/), I value the next ten minutes a lot, but after that the curve becomes pretty flat and maybe I don't value 12:20 much more than I value the next morning at work. Ten minutes' sleep here or there doesn't make any difference. So I say: \"I will play Civilization for ten minutes - 'just one more turn' - and then I will go to bed.\"  \n  \nTime passes. It is now 12:10. Still being a hyperbolic discounter, I value the next ten minutes a lot, and subsequent times much less. And so I say: I will play until 12:20, ten minutes sleep here or there not making much difference, and then sleep.  \n  \nAnd so on until my empire bestrides the globe and the rising sun peeps through my windows.  \n  \nThis is pretty much the same process described above with Murder-Gandhi except that here the role of the value-changing pill is played by time and my own tendency to discount hyperbolically.  \n  \nThe solution is the same. If I consider the problem early in the evening, I can precommit to midnight as a nice round number that makes a good Schelling point. Then, when deciding whether or not to play after midnight, I can treat my decision not as \"Midnight or 12:10\" - because 12:10 will always win _that_ particular race - but as \"Midnight or abandoning the only credible Schelling point and probably playing all night\", which will be sufficient to scare me into turning off the computer.  \n  \n(if I consider the problem at 12:01, I may be able to precommit to 12:10 if I am especially good at precommitments, but it's not a very natural Schelling point and it might be easier to say something like \"as soon as I finish this turn\" or \"as soon as I discover this technology\").  \n  \n**Coalitions of Resistance  \n**  \nSuppose you are a Zoroastrian, along with 1% of the population. In fact, along with Zoroastrianism your country has fifty other small religions, each with 1% of the population. 49% of your countrymen are atheist, and hate religion with a passion.  \n  \nYou hear that the government is considering banning the Taoists, who comprise 1% of the population. You've never liked the Taoists, vile doubters of the light of Ahura Mazda that they are, so you go along with this. When you hear the government wants to ban the Sikhs and Jains, you take the same tack.  \n  \nBut now you are in the unfortunate situation described by Martin Niemoller:\n\n> _First they came for the socialists, and I did not speak out, because I was not a socialist.  \n> Then they came for the trade unionists, and I did not speak out, because I was not a trade unionist.  \n> Then they came for the Jews, and I did not speak out, because I was not a Jew.  \n> Then they came for me, but we had already abandoned the only defensible Schelling point  \n> _\n\nWith the banned Taoists, Sikhs, and Jains no longer invested in the outcome, the 49% atheist population has enough clout to ban Zoroastrianism and anyone else they want to ban. The better strategy would have been to have all fifty-one small religions form a coalition to defend one another's right to exist. In this toy model, they could have done so in an ecumenial congress, or some other literal strategy meeting.  \n  \nBut in the real world, there aren't fifty-one well-delineated religions. There are billions of people, each with their own set of opinions to defend. It would be impractical for everyone to physically coordinate, so they have to rely on Schelling points.  \n  \nIn the original example with the alien, I cheated by using the phrase \"right-thinking people\". In reality, figuring out who qualifies to join the Right-Thinking People Club is half the battle, and everyone's likely to have a different opinion on it. So far, the practical solution to the coordination problem, the \"only defensible Schelling point\", has been to just have everyone agree to defend everyone else without worrying whether they're right-thinking or not, and this is easier than trying to coordinate room for exceptions like Holocaust deniers. Give up on the Holocaust deniers, and no one else can be sure what other Schelling point you've committed to, if any...  \n  \n...unless they can. In parts of Europe, they've banned Holocaust denial for years and everyone's been totally okay with it. There are also a host of other well-respected exceptions to free speech, like shouting \"fire\" in a crowded theater. Presumably, these exemptions are protected by tradition, so that they have become new Schelling points there, or are else so obvious that everyone except Holocaust deniers is willing to allow a special Holocaust denial exception without worrying it will impact their own case.  \n  \n**Summary**  \n  \nSlippery slopes legitimately exist wherever a policy not only affects the world directly, but affects people's willingness or ability to oppose future policies. Slippery slopes can sometimes be avoided by establishing a \"Schelling fence\" - a Schelling point that the various interest groups involved - or yourself across different values and times - make a credible precommitment to defend."
    },
    "voteCount": 387,
    "forceInclude": true
  },
  {
    "_id": "xg3hXCYQPJkwHyik2",
    "url": null,
    "title": "The Best Textbooks on Every Subject",
    "slug": "the-best-textbooks-on-every-subject",
    "author": "lukeprog",
    "question": false,
    "tags": [
      {
        "name": "World Modeling"
      },
      {
        "name": "Scholarship & Learning"
      },
      {
        "name": "Practical"
      },
      {
        "name": "List of Links"
      },
      {
        "name": "Book Reviews"
      },
      {
        "name": "Whole Brain Emulation"
      },
      {
        "name": "Comfort Zone Expansion (CoZE)"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "For years, my self-education was stupid and wasteful. I learned by consuming blog posts, Wikipedia articles, classic texts, podcast episodes, popular books, [video lectures](http://academicearth.org/), peer-reviewed papers, [Teaching Company](http://www.teach12.com/) courses, and Cliff's Notes. How inefficient!\n\nI've since discovered that _textbooks_ are usually the quickest and best way to learn new material. That's what they are _designed_ to be, after all. Less Wrong [has](/lw/2xt/learning_the_foundations_of_math/) [often](/lw/ow/the_beauty_of_settled_science/) [recommended](/lw/jv/recommended_rationalist_reading/fcg?c=1) the \"read textbooks!\" method. [Make progress by accumulation, not random walks](/lw/1ul/for_progress_to_be_by_accumulation_and_not_by/).\n\nBut textbooks vary widely in quality. I was forced to read some awful textbooks in college. The ones on American history and sociology were memorably bad, in my case. Other textbooks are exciting, accurate, fair, well-paced, and immediately useful.\n\nWhat if we could compile a list of the best textbooks on every subject? That would be _extremely_ useful.\n\nLet's do it.\n\nThere have been [other](/lw/jv/recommended_rationalist_reading/) [pages](/lw/12d/recommended_reading_for_new_rationalists/) of [recommended](/lw/2un/references_resources_for_lesswrong/) [reading](/lw/2xt/learning_the_foundations_of_math/) on Less Wrong before (and [elsewhere](http://ask.metafilter.com/71101/What-single-book-is-the-best-introduction-to-your-field-or-specialization-within-your-field-for-laypeople)), but this post is unique. Here are **the rules**:\n\n1.  Post the title of your favorite textbook on a given subject.\n2.  You must have read at least two other textbooks on that same subject.\n3.  You must briefly name the other books you've read on the subject and explain why you think your chosen textbook is superior to them.\n\nRules #2 and #3 are to protect against recommending a bad book that only seems impressive because it's the only book you've read on the subject. Once, a popular author on Less Wrong recommended Bertrand Russell's _A History of Western Philosophy_ to me, but when I noted that it was more polemical and inaccurate than the other major histories of philosophy, he admitted he hadn't really done much other reading in the field, and only liked the book because it was exciting.\n\nI'll start the list with three of my own recommendations...\n\n**Subject**: History of Western Philosophy\n\n**Recommendation**: _[The Great Conversation](http://www.amazon.com/Great-Conversation-Historical-Introduction-Philosophy/dp/0195397614/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)_, 6th edition, by Norman Melchert\n\n**Reason**: The most popular history of western philosophy is Bertrand Russell's _[A History of Western Philosophy](http://www.amazon.com/History-Western-Philosophy-Bertrand-Russell/dp/0671201581/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)_, which is exciting but also polemical and [inaccurate](http://www.the-philosopher.co.uk/reviews/brussel.htm). More accurate but dry and dull is Frederick Copelston's 11-volume _[A History of Philosophy](http://en.wikipedia.org/wiki/A_History_of_Philosophy_(Copleston))_. Anthony Kenny's recent 4-volume history, collected into one book as _[A New History of Western Philosophy](http://www.amazon.com/New-History-Western-Philosophy/dp/0199589887/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)_, is both exciting and accurate, but perhaps too long (1000 pages) and technical for a first read on the history of philosophy. Melchert's textbook, _[The Great Conversation](http://www.amazon.com/Great-Conversation-Historical-Introduction-Philosophy/dp/0195397614/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)_, is accurate but also the easiest to read, and has the clearest explanations of the important positions and debates, though of course it has its weaknesses (it spends too many pages on ancient Greek mythology but barely mentions Gottlob Frege, the father of analytic philosophy and of the philosophy of language). Melchert's history is also the only one to seriously cover the dominant mode of Anglophone philosophy done today: [naturalism](http://www.amazon.com/Understanding-Naturalism-Movements-Modern-Thought/dp/1844650790/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20) (what Melchert calls \"physical realism\"). Be sure to get the 6th edition, which has major improvements over the 5th edition.\n\n**Subject**: Cognitive Science\n\n**Recommendation**: _[Cognitive Science](http://www.amazon.com/Cognitive-Science-Introduction-Mind/dp/0521882001/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)_, by Jose Luis Bermudez\n\n**Reason**: Jose Luis Bermudez's _[Cognitive Science: An Introduction to the Science of Mind](http://www.amazon.com/Cognitive-Science-Introduction-Mind/dp/0521882001/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)_ does an excellent job setting the historical and conceptual context for cognitive science, and draws fairly from all the fields involved in this heavily interdisciplinary science. Bermudez does a good job of making himself invisible, and the explanations here are some of the clearest available. In contrast, Paul Thagard's _[Mind: Introduction to Cognitive Science](http://www.amazon.com/Mind-Introduction-Cognitive-Science-2nd/dp/026270109X/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)_ skips the context and jumps right into a systematic comparison (by explanatory merit) of the leading theories of mental representation: logic, rules, concepts, analogies, images, and neural networks. The book is only 270 pages long, and is also more idiosyncratic than Bermudez's; for example, Thagard refers to the dominant paradigm in cognitive science as the \"computational-representational understanding of mind,\" which as far as I can tell is used only by him and people drawing from his book. In truth, the term refers to a set of competing theories, for example the [computational theory](http://en.wikipedia.org/wiki/Computational_theory_of_mind) and the [representational theory](http://en.wikipedia.org/wiki/Representational_theory_of_mind). While not the best place to start, Thagard's book is a decent follow-up to Bermudez's text. Better, though, is Kolak et. al.'s _[Cognitive Science: An Introduction to Mind and Brain](http://www.amazon.com/Cognitive-Science-Introduction-Mind-Brain/dp/0415221013/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)_. It contains more information than Bermudez's book, but I prefer Bermudez's flow, organization and content selection. Really, though, both Bermudez and Kolak offer excellent introductions to the field, and Thagard offers a more systematic and narrow investigation that is worth reading after Bermudez and Kolak.\n\n**Subject**: Introductory Logic for Philosophy\n\n**Recommendation**: _[Meaning and Argument](http://www.amazon.com/dp/1405196734/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)_ by Ernest Lepore\n\n**Reason**: For years, the standard textbook on logic was Copi's _[Introduction to Logic](http://www.amazon.com/Introduction-Logic-13th-Irving-Copi/dp/0136141390/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)_, a comprehensive textbook that has chapters on language, definitions, fallacies, deduction, induction, syllogistic logic, symbolic logic, inference, and probability. It spends too much time on methods that are rarely used today, for example Mill's methods of inductive inference. Amazingly, the chapter on probability does not mention Bayes (as of the 11th edition, anyway). Better is the current standard in classrooms: Patrick Hurley's _[A Concise Introduction to Logic](http://www.amazon.com/Concise-Introduction-Logic-CourseCard/dp/0840034172/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)._ It has a table at the front of the book that tells you which sections to read depending on whether you want (1) a traditional logic course, (2) a critical reasoning course, or (3) a course on modern formal logic. The single chapter on induction and probability moves too quickly, but is excellent for its length. Peter Smith's [An Introduction to Formal Logic](http://www.amazon.com/gp/product/0521008042/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20) instead focuses tightly on the usual methods used by today's philosophers: propositional logic and predicate logic. My favorite in this less comprehensive mode, however, is Ernest Lepore's _[Meaning and Argument](http://www.amazon.com/dp/1405196734/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)_, because it (a) is highly efficient, and (b) focuses not so much on the manipulation of symbols in a formal system but on the arguably trickier matter of translating English sentences into symbols in a formal system in the first place.\n\nI would love to read recommendations from experienced readers on the following subjects: physics, chemistry, biology, psychology, sociology, probability theory, economics, statistics, calculus, decision theory, cognitive biases, artificial intelligence, neuroscience, molecular biochemistry, medicine, epistemology, philosophy of science, meta-ethics, and much more.\n\nPlease, post your own recommendations! And, follow [the rules](#rules).\n\n**Recommendations so far** (that follow [the rules](#rules); this list updated 02-25-2017):\n\n*   On **history of western philosophy**, lukeprog [recommends](#history_philosophy) Melchert's _[The Great Conversation](http://www.amazon.com/Great-Conversation-Historical-Introduction-Philosophy/dp/0195397614/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)_ over Russell's _A History of Western Philosophy_, Copelston's _History of Philosophy_, and Kenney's _A New History of Western Philosophy_.\n*   On **cognitive science**, lukeprog [recommends](#cognitive_science) Bermudez's _[Cognitive Science](http://www.amazon.com/Cognitive-Science-Introduction-Mind/dp/0521882001/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)_ over Thagard's _Mind: Introduction to Cognitive Science_ and Kolak's _Cognitive Science_.\n*   On **introductory logic for philosophy**, lukeprog [recommends](#logic) Lepore's _[Meaning and Argument](http://www.amazon.com/dp/1405196734/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)_ over Copi's _Introduction to Logic_, Hurley's _A Concise Introduction to Logic_, and Smith's _An Introduction to Formal Logic_.\n*   On **economics**, michaba03m [recommends](/lw/3gu/the_best_textbooks_on_every_subject/3cd9) Mankiw's _[Macroeconomics](http://www.amazon.com/dp/1429218878/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)_ over Varian's _Intermediate Microeconomics_ and Katz & Rosen's _Macroeconomics_.\n*   On **economics**, realitygrill [recommends](/lw/3gu/the_best_textbooks_on_every_subject/3coo) McAfee's _[Introduction to Economic Analysis](https://open.umn.edu/opentextbooks/textbooks/47)_ over Mankiw's _Principles of Microeconomics_ and Case & Fair's _Principles of Macroeconomics_.\n*   On **representation theory**, SarahC [recommends](/lw/3gu/the_best_textbooks_on_every_subject/3cez) Sternberg's _[Group Theory and Physics](http://www.amazon.com/Group-Theory-Physics-S-Sternberg/dp/0521558859/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)_ over Lang's _Algebra_, Weyl's _The Theory of Groups and Quantum Mechanics_, and Fulton & Harris' _Representation Theory: A First Course_.\n*   On **statistics**, madhadron [recommends](/lw/3gu/the_best_textbooks_on_every_subject/64mz) Kiefer's _[Introduction to Statistical Inference](http://www.amazon.com/Introduction-Statistical-Inference-Springer-Statistics/dp/0387964207/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)_ over Hogg & Craig's _Introduction to Mathematical Statistics_, Casella & Berger's _Statistical Inference_, and others.\n*   On **advanced Bayesian statistics**, Cyan [recommends](/lw/3gu/the_best_textbooks_on_every_subject/3cg2) Gelman's _[Bayesian Data Analysis](http://www.amazon.com/dp/158488388X/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)_ over Jaynes' _Probability Theory: The Logic of Science_ and Bernardo's _Bayesian Theory_.\n*   On **basic Bayesian statistics**, jsalvatier [recommends](/lw/3gu/the_best_textbooks_on_every_subject/3clc) Skilling & Sivia's _[Data Analysis: A Bayesian Tutorial](http://www.amazon.com/Data-Analysis-Bayesian-Tutorial-ebook/dp/B001E5II36/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)_ over Gelman's _Bayesian Data Analysis_, Bolstad's _Bayesian Statistics_, and Robert's _The Bayesian Choice_.\n*   On **real analysis**, paper-machine [recommends](/lw/3gu/the_best_textbooks_on_every_subject/3cll) Bartle's [A Modern Theory of Integration](http://www.amazon.com/Modern-Integration-Graduate-Studies-Mathematics/dp/0821808451/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20) over Rudin's _Real and Complex Analysis_ and Royden's _Real Analysis_.\n*   On **non-relativistic quantum mechanics**, wbcurry [recommends](/lw/3gu/the_best_textbooks_on_every_subject/3cmj) Sakurai & Napolitano's _[Modern Quantum Mechanics](http://www.amazon.com/Modern-Quantum-Mechanics-2nd-Sakurai/dp/0805382917/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)_ over Messiah's _Quantum Mechanics_, Cohen-Tannoudji's _Quantum Mechanics_, and Greiner's _Quantum Mechanics: An Introduction_.\n*   On **music theory**, komponisto [recommends](/lw/3gu/the_best_textbooks_on_every_subject/3cmp) Westergaard's _[An Introduction to Tonal Theory](http://www.amazon.com/Introduction-Tonal-Theory-Peter-Westergaard/dp/0393093425/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)_ over Piston's _Harmony_, Aldwell and Schachter's _Harmony and Voice Leading_, and Kotska and Payne's _Tonal Harmony_.\n*   On **business**, joshkaufman [recommends](/lw/3gu/the_best_textbooks_on_every_subject/3cny) Kaufman's _[The Personal MBA: Master the Art of Business](http://www.amazon.com/gp/product/1591843529/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)_ over Bevelin's _Seeking Wisdom_ and Munger's _Poor Charlie's Alamanack_.\n*   On **machine learning**, alexflint [recommends](/lw/3gu/the_best_textbooks_on_every_subject/3cp0) Bishop's _[Pattern Recognition and Machine Learning](http://www.amazon.com/Pattern-Recognition-Learning-Information-Statistics/dp/0387310738/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)_ over Russell & Norvig's _Artificial Intelligence: A Modern Approach_ and Thrun et. al.'s _Probabilistic Robotics_.\n*   On **algorithms**, gjm [recommends](/lw/3gu/the_best_textbooks_on_every_subject/3cpz) Cormen et. al.'s _[Introduction to Algorithms](http://www.amazon.com/Introduction-Algorithms-Third-Thomas-Cormen/dp/0262033844/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)_ over Knuth's _The Art of Computer Programming_ and Sedgwick's _Algorithms_.\n*   On **electrodynamics**, Alex_Altair [recommends](/lw/3gu/the_best_textbooks_on_every_subject/3cr1) Griffiths' _[Introduction to Electrodynamics](http://www.amazon.com/Introduction-Electrodynamics-3rd-David-Griffiths/dp/013805326X/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)_ over Jackson's _Electrodynamics_ and Feynman's _Lectures on Physics_.\n*   On **electrodynamics**, madhadron [recommends](/lw/3gu/the_best_textbooks_on_every_subject/64mz) Purcell's _[Electricity and Magnetism](http://www.amazon.com/Electricity-Magnetism-Edward-Purcell/dp/1107013607/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)_ over Griffith's _Introduction to Electrodynamics_, Feynman's _Lectures on Physics_, and others.\n*   On **systems theory**, Davidmanheim [recommends](/lw/3gu/the_best_textbooks_on_every_subject/3ctm) Meadows' _[Thinking in Systems: A Primer](http://www.amazon.com/Thinking-Systems-Donella-H-Meadows/dp/1603580557/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)_ over Senge's _The Fifth Discipline: The Art & Practice of The Learning Organization_ and Kim's _Introduction to Systems Thinking_.\n*   On **self-help**, lukeprog [recommends](/lw/3gu/the_best_textbooks_on_every_subject/3dcq) Weiten, Dunn, and Hammer's _[Psychology Applied to Modern Life](http://www.amazon.com/Psychology-Applied-Modern-Life-Adjustment/dp/1111186634/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)_ over Santrock's _Human Adjustment_ and Tucker-Ladd's _Psychological Self-Help_.\n*   On **probability theory**, SarahC [recommends](/lw/3gu/the_best_textbooks_on_every_subject/3f00) Feller's _[An Introduction to Probability Theory](http://www.amazon.com/Introduction-Probability-Theory-Applications-Vol/dp/0471257087/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)_ \\+ _[Vol. 2](http://www.amazon.com/Introduction-Probability-Theory-Applications-Vol/dp/0471257095/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)_ over Ross' _A First Course in Probability_ and Koralov & Sinai's _Theory of Probability and Random Processes_.\n*   On **probability theory**, madhadron [recommends](/lw/3gu/the_best_textbooks_on_every_subject/64mz) Grimmett & Stirzaker's _[Probability and Random Processes](http://www.amazon.com/Probability-Random-Processes-Geoffrey-Grimmett/dp/0198572220/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)_ over Feller's _Introduction to Probability Theory and Its Applications_ and Nelson's _Radically Elementary Probability Theory_.\n*   On **topology**, jsteinhardt [recommends](/lw/3gu/the_best_textbooks_on_every_subject/3ff2) Munkres' _[Topology](http://www.amazon.com/Topology-2nd-James-Munkres/dp/0131816292/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)_ over Armstrong's _Topology_ and Massey's _Algebraic Topology_.\n*   On **linguistics**, etymologik [recommends](/lw/3gu/the_best_textbooks_on_every_subject/3fs1) O'Grady et al.'s _[Contemporary Linguistics](http://www.amazon.com/Contemporary-Linguistics-William-OGrady/dp/0312555288/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)_ over Hayes et al.'s _Linguistics: An Introduction to Linguistic Theory_ and Carnie's _Syntax: A Generative Introduction_.\n*   On **meta-ethics**, lukeprog [recommends](/lw/3gu/the_best_textbooks_on_every_subject/3nvo) Miller's _[An Introduction to Contemporary Metaethics](http://www.amazon.com/Introduction-Contemporary-Metaethics-Alex-Miller/dp/074562345X/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)_ over Jacobs' _The Dimensions of Moral Theory_ and Smith's _Ethics and the A Priori_.\n*   On **decision-making & biases**, badger [recommends](/lw/3gu/the_best_textbooks_on_every_subject/490n) Bazerman & Moore's _[Judgment in Managerial Decision Making](http://www.amazon.com/Judgment-Managerial-Decision-Making-Bazerman/dp/0470049456/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)_ over Hastie & Dawes' _Rational Choice in an Uncertain World_, Gilboa's _Making Better Decisions_, and others.\n*   On **neuroscience**, kjmiller [recommends](/lw/3gu/the_best_textbooks_on_every_subject/4zqx) Bear et al's _Neuroscience: Exploring the Brain_ over Purves et al's _Neuroscience_ and Kandel et al's _Principles of Neural Science_.\n*   On **World War II**, Peacewise [recommends](/lw/3gu/the_best_textbooks_on_every_subject/558w) Weinberg's _[A World at Arms](http://www.amazon.com/World-Arms-Global-History-War/dp/0521618266/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)_ over Churchill's _The Second World War_ and Day's _The Politics of War_.\n*   On **elliptic curves**, magfrump [recommends](/lw/3gu/the_best_textbooks_on_every_subject/5zie) Koblitz' _[Introduction to Elliptic Curves and Modular Forms](http://www.amazon.com/Introduction-Elliptic-Modular-Graduate-Mathematics/dp/0387979662/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)_ over Silverman's _Arithmetic of Elliptic Curves_ and Cassel's _Lectures on Elliptic Curves_.\n*   On **improvisation**, Arepo [recommends](/lw/3gu/the_best_textbooks_on_every_subject/60l7) Salinsky & Frances-White's _[The Improv Handbook](http://www.amazon.com/The-Improv-Handbook-Ultimate-Improvising/dp/0826428584/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)_ over Johnstone's _Impro_, Johnston's _The Improvisation Game_, and others.\n*   On **thermodynamics**, madhadron [recommends](/lw/3gu/the_best_textbooks_on_every_subject/64mz) Hatsopoulos & Keenan's _[Principles of General Thermodynamics](http://www.amazon.com/Principles-General-Thermodynamics-G-Hatsopoulos/dp/0471359998/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)_ over Fermi's _Thermodynamics_, Sommerfeld's _Thermodynamics and Statistical Mechanics_, and others.\n*   On **statistical mechanics**, madhadron [recommends](/lw/3gu/the_best_textbooks_on_every_subject/64mz) Landau & Lifshitz' _[Statistical Physics, Volume 5](http://www.amazon.com/Statistical-Physics-Third-Edition-Part/dp/0750633727/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)_ over Sethna's _Entropy, Order Parameters, and Complexity_ and Reichl's _A Modern Course in Statistical Physics_.\n*   On **criminal justice**, strange [recommends](/lw/3gu/the_best_textbooks_on_every_subject/655d) Fuller's _[Criminal Justice: Mainstream and Crosscurrents](http://www.amazon.com/Criminal-Justice-Mainstream-Crosscurrents-Edition/dp/0135042623/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)_ over Neubauer & Fradella's _America's Courts and the Criminal Justice System_ and Albanese' _Criminal Justice_.\n*   On **organic chemistry**, rhodium [recommends](/lw/3gu/the_best_textbooks_on_every_subject/67k1) Clayden et al's _[Organic Chemistry](http://www.amazon.com/Organic-Chemistry-Jonathan-Clayden/dp/0198503466/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)_ over McMurry's _Organic Chemistry_ and Smith's _Organic Chemistry_.\n*   On **special relativity**, iDante [recommends](/lw/3gu/the_best_textbooks_on_every_subject/72an) Taylor & Wheeler's _[Spacetime Physics](http://www.amazon.com/Spacetime-Physics-Edwin-F-Taylor/dp/0716723271/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)_ over Harris' _Modern Physics_, French's _Special Relativity_, and others.\n*   On **abstract algebra**, Bundle_Gerbe [recommends](/lw/3gu/the_best_textbooks_on_every_subject/72dq) Dummit & Foote's _[Abstract Algebra](http://www.amazon.com/Abstract-Algebra-Edition-David-Dummit/dp/0471433349/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)_ over Lang's _Algebra_ and others.\n*   On **decision theory**, lukeprog [recommends](/lw/3gu/the_best_textbooks_on_every_subject/7avv) Peterson's _[An Introduction to Decision Theory](http://www.amazon.com/Introduction-Decision-Cambridge-Introductions-Philosophy/dp/0521888379/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)_ over Resnik's _Choices_ and Luce & Raiffa's _Games and Decisions_.\n*   On **calculus**, orthonormal [recommends](/lw/3gu/the_best_textbooks_on_every_subject/8uec) Spivak's _[Calculus](http://smile.amazon.com/Calculus-4th-Michael-Spivak/dp/0914098918)_ over Thomas' _Calculus_ and Stewart's _Calculus_. \n*   On **analysis in R^n^**, orthonormal [recommends](/lw/3gu/the_best_textbooks_on_every_subject/8uec) Strichartz's _[The Way of Analysis](http://smile.amazon.com/Analysis-Revised-Jones-Bartlett-Mathematics/dp/0763714976/)_ over Rudin's _Principles of Mathematical Analysis_ and Kolmogorov & Fomin's _Introduction to Real Analysis_.\n*   On **real analysis and measure theory**, orthonormal [recommends](/lw/3gu/the_best_textbooks_on_every_subject/8uec) Stein & Shakarchi's _[Measure Theory, Integration, and Hilbert Spaces](http://smile.amazon.com/Real-Analysis-Integration-Princeton-Lectures/dp/0691113866/)_ over Royden's _Real Analysis_ and Rudin's _Real and Complex Analysis_. \n*   On **partial differential equations**, orthonormal [recommends](/lw/3gu/the_best_textbooks_on_every_subject/8uec) Strauss' _[Partial Differential Equations](http://smile.amazon.com/Partial-Differential-Equations-Walter-Strauss/dp/0470054565)_ over Evans' _Partial Differential Equations_ and Hormander's _Analysis of Partial Differential Operators_.\n*   On **introductory real analysis**, SatvikBeri [recommends](/lw/3gu/the_best_textbooks_on_every_subject/9kw2) Pugh's [Real Mathematical Analysis](http://smile.amazon.com/Mathematical-Analysis-Undergraduate-Texts-Mathematics/dp/0387952977/) over Lang's _Real and Functional Analysis_ and Rudin's _Principles of Mathematical Analysis_.\n*   On **commutative algebra**, SatvikBeri [recommends](/lw/3gu/the_best_textbooks_on_every_subject/9kw9) MacDonald's _[Introduction to Commutative Algebra](http://smile.amazon.com/Introduction-Commutative-Algebra-Addison-Wesley-Mathematics/dp/0201407515/)_ over Lang's _Algebra_ and Eisenbud's _Commutative Algebra With a View Towards Algebraic Geometry_.\n*   On **animal behavior**, Natha [recommends](/lw/3gu/the_best_textbooks_on_every_subject/bke9) Alcock's _[Animal Behavior, 6th edition](http://smile.amazon.com/Animal-Behavior-Evolutionary-Approach-Tenth/dp/0878939660)_ over Dugatkin's _Principles of Animal Behavior_ and newer editions of the Alcock textbook.\n*   On **calculus**, Epictetus [recommends](/lw/3gu/the_best_textbooks_on_every_subject/byee) Courant's _[Differential and Integral Calculus](http://smile.amazon.com/Differential-Integral-Calculus-Vol-One/dp/4871878384/)_ over Stewart's _Calculus_ and Kline's _Calculus_.\n*   On **linear algebra**, Epictetus [recommends](/lw/3gu/the_best_textbooks_on_every_subject/byee) Shilov's _[Linear Algebra](http://smile.amazon.com/Linear-Algebra-Dover-Books-Mathematics/dp/048663518X/)_ over Lay's _Linear Algebra and its Appications_ and Axler's _Linear Algebra Done Right_.\n*   On **numerical methods**, Epictetus [recommends](/lw/3gu/the_best_textbooks_on_every_subject/byee) Press et al.'s _[Numerical Recipes](http://smile.amazon.com/Numerical-Recipes-3rd-Scientific-Computing/dp/0521880688/)_ over Bulirsch & Stoer's _Introduction to Numerical Analysis_, Atkinson's _An Introduction to Numerical Analysis_, and Hamming's _Numerical Methods of Scientists and Engineers_.\n*   On **ordinary differential equations**, Epictetus [recommends](/lw/3gu/the_best_textbooks_on_every_subject/byee) Arnold's _[Ordinary Differential Equations](http://smile.amazon.com/Ordinary-Differential-Equations-V-I-Arnold/dp/0262510189/)_ over Coddington's _An Introduction to Ordinary Differential Equations_ and Enenbaum & Pollard's _Ordinary Differential Equations_.\n*   On **abstract algebra**, Epictetus [recommends](/lw/3gu/the_best_textbooks_on_every_subject/byee) Jacobson's _[Basic Algebra](http://smile.amazon.com/Basic-Algebra-Second-Dover-Mathematics/dp/0486471896/)_ over Bourbaki's _Algebra_, Lang's _Algebra_, and Hungerford's _Algebra_.\n*   On **elementary real analysis**, Epictetus [recommends](/lw/3gu/the_best_textbooks_on_every_subject/byee) Rudin's _[Principles of Mathematical Analysis](http://smile.amazon.com/Principles-Mathematical-Analysis-Rudin/dp/1259064786/)_ over Ross' _Elementary Analysis_, Lang's _Undergraduate Analysis_, and Hardy's _A Course of Pure Mathematics_.\n\nIf there are no recommendations for the subject you want to learn, you can start by checking the [Alibris textbooks](http://www.alibris.com/subjects/subjects-textbooks) category for your subject, and sort by 'Top-selling.' But you'll have to do more research than that. Check which textbooks are asked for in the syllabi of classes on your subject at leading universities. Search Google for recommendations and reviews."
    },
    "voteCount": 475,
    "forceInclude": true
  },
  {
    "_id": "9QxnfMYccz9QRgZ5z",
    "url": null,
    "title": "The Costly Coordination Mechanism of Common Knowledge",
    "slug": "the-costly-coordination-mechanism-of-common-knowledge",
    "author": "Ben Pace",
    "question": false,
    "tags": [
      {
        "name": "Rationality"
      },
      {
        "name": "Common Knowledge"
      },
      {
        "name": "Game Theory"
      },
      {
        "name": "Mechanism Design"
      },
      {
        "name": "Coordination / Cooperation"
      }
    ],
    "tableOfContents": {
      "sections": [
        {
          "title": "Prisoner's Dilemmas vs Coordination Problems",
          "anchor": "Prisoner_s_Dilemmas_vs_Coordination_Problems",
          "level": 1
        },
        {
          "title": "The Prisoner's Dilemma (PD)",
          "anchor": "The_Prisoner_s_Dilemma__PD_",
          "level": 2
        },
        {
          "title": "Real World Examples",
          "anchor": "Real_World_Examples",
          "level": 3
        },
        {
          "title": "Free-Rider Problems",
          "anchor": "Free_Rider_Problems",
          "level": 3
        },
        {
          "title": "Coordination Problems",
          "anchor": "Coordination_Problems",
          "level": 2
        },
        {
          "title": "A Stable State",
          "anchor": "A_Stable_State",
          "level": 3
        },
        {
          "title": "Solving problems and resolving dilemmas",
          "anchor": "Solving_problems_and_resolving_dilemmas",
          "level": 3
        },
        {
          "title": "Three Coordination Problems",
          "anchor": "Three_Coordination_Problems",
          "level": 1
        },
        {
          "title": "Dictators and freedom of speech",
          "anchor": "Dictators_and_freedom_of_speech",
          "level": 2
        },
        {
          "title": "Uncertainty in Romance",
          "anchor": "Uncertainty_in_Romance",
          "level": 2
        },
        {
          "title": "Communal/Religious Rituals",
          "anchor": "Communal_Religious_Rituals",
          "level": 2
        },
        {
          "title": "Common Knowledge Production in Society at Large",
          "anchor": "Common_Knowledge_Production_in_Society_at_Large",
          "level": 1
        },
        {
          "title": "The News",
          "anchor": "The_News",
          "level": 2
        },
        {
          "title": "Academic Research",
          "anchor": "Academic_Research",
          "level": 2
        },
        {
          "title": "Startups",
          "anchor": "Startups",
          "level": 2
        },
        {
          "title": "At what cost?",
          "anchor": "At_what_cost_",
          "level": 2
        },
        {
          "title": "So, what's common knowledge for?",
          "anchor": "So__what_s_common_knowledge_for_",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        },
        {
          "anchor": "comments",
          "level": 0,
          "title": "30 comments"
        }
      ],
      "headingsCount": 19
    },
    "contents": {
      "markdown": "*Recently someone pointed out to me that there was no good canonical post that explained the use of common knowledge in society. Since I wanted to be able to link to such a post, I decided to try to write it.*\n\n*The epistemic status of this post is that I hoped to provide an explanation for a standard, mainstream idea, in a concrete way that could be broadly understood rather than in a mathematical/logical fashion, and so the definitions should all be correct, though the examples in the latter half are more speculative and likely contain some inaccuracies.*\n\nLet's start with a puzzle. What do these three things have in common?\n\n*   Dictatorships all through history have attempted to suppress freedom of the press and freedom of speech. Why is this? Are they just very sensitive? On the other side, the leaders of the Enlightenment fought for freedom of speech, and would not budge an inch against this principle.\n*   When two people are on a date and want to sleep with each other, the conversation will often move towards but never *explicitly* discuss having sex. The two may discuss going back to the place of one of theirs, with a different explicit reason discussed (e.g. \"to have a drink\"), *even* if both want to have sex.\n*   Throughout history, communities have had religious rituals that look very similar. Everyone in the village has to join in. There are repetitive songs, repetitive lectures on the same holy books, chanting together. Why, of all the possible community events (e.g. dinner, parties, etc) is this the most common type?\n\nWhat these three things have in common, is *common knowledge* \\- or at least, the attempt to create it.\n\nBefore I spell that out, we’ll take a brief look into game theory so that we have the language to describe clearly what’s going on. Then we’ll be able to see concretely in a bunch of examples, how common knowledge is necessary to understand and build institutions.\n\nPrisoner's Dilemmas vs Coordination Problems\n============================================\n\nTo understand why common knowledge is useful, I want to contrast two types of situations in game theory: Prisoner’s Dilemmas and Coordination Problems. They look similar at first glance, but their payoff matrices have important differences.\n\nThe Prisoner's Dilemma (PD)\n---------------------------\n\nYou’ve probably heard of it - two players have the opportunity to cooperate, or defect against each other, based on a [story about two prisoners being offered a deal if they testify against the other](https://www.lesserwrong.com/posts/HFyWNBnDNEDsDNLrZ/the-true-prisoner-s-dilemma).\n\nIf they do nothing they will put them both away for a short time; if one of them snitches on the other, the snitch gets off free and the snitched gets a long sentence. However if they *both* snitch they get pretty bad sentences (though neither are as long as when only one snitches on the other).\n\nIn game theory, people often like to draw little boxes that show two different people's choices, and how much they like the outcome. Such a diagram is called a *decision matrix*, and the numbers are called the players' *payoffs*.\n\nTo describe the Prisoner's Dilemma, below is a decision matrix where Anne and Bob each have the same two choices, labelled \\\\(C\\\\) and \\\\(D\\\\). These are colloquially called ‘cooperate’ and ‘defect’. Each box contains two numbers, for Anne and Bob's payoffs respectively. ​\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/e56b0c1b04aaa7289a0bcd7f5d860f6adda8c68afa2d7402.png)\n\nIf the prisoner ‘defects’ on his partner, this means he snitches, and if he ‘cooperates’ with his partner, he doesn’t snitch. They’d both prefer that *both* of them cooperate \\\\((C,C)\\\\) to both of them defecting \\\\((D,D)\\\\), but each of them has an incentive to stab each other in the back to reap the most reward \\\\((D,C)\\\\).\n\nDo you see in the matrix how they both would prefer no snitching to both snitching, but they also have an incentive to stab each other in the back?\n\n**Real World Examples**\n\nNuclear disarmament is a prisoner’s dilemma. Both the Soviet Union and the USS wanted to have nuclear bombs while the opponent doesn't, but they'd probably both prefer a world where nobody had bombs than a world where they were both pointing massive weapons at each others heads. Unfortunately in our world, we failed to solve the problem, and ended up pointing massive weapons at each others' heads for decades.\n\nMilitary budget spending more broadly can be a prisoner’s dilemma. Suppose two neighbouring countries are determining how much to spend on the military. Well, they don’t want to go to war with each other, and so they’d each like to spend a small amount of money on their military, and spend the rest of the money on running the country - infrastructure, healthcare, etc. However, if one country spends a small amount and the other country spends a lot, then the second country can just walk in and take over the first. So, they both spend lots of money on the military with no intention of using it, just so the other one can’t take over.\n\nAnother prisoner’s dilemma is tennis players figuring out whether to take performance enhancing drugs. Naturally, they'd like to dope and the opposing player not, but they'd rather both not dope than both dope.\n\n**Free-Rider Problems**\n\nDid you notice how there are more than two tennis players in the doping situation? When deciding whether to take drugs, not only do you have to worry about whether your opponent in the match today will dope, but also whether your opponent tomorrow will, and the day after, and so on. We’re all wondering whether *all* of us will dope. In society there are loads of these scaled up versions of the prisoner’s dilemma.\n\nFor example, according to many political theories, everyone is better off if the government takes some taxes and uses them to provide public goods (e.g. transportation, military, hospitals). As a population, it's in everyone's interest if everyone cooperates, and takes a small personal sacrifice of wealth.\n\nHowever, if most people are doing it, you can defect, and this is great for you - you get the advantage of a government providing public goods, and also you keep your own money. But if everyone defects, then nobody gets the important public goods, and this is worse for each person than if they'd all cooperated.\n\nWhether you’re two robbers, one of many tennis players, or a whole country fighting another country, you will run into a prisoner’s dilemma. In the scaled-up version, a person who defects while everyone else cooperates is known as a *free-rider*, and the scaled up prisoner’s dilemma is called the *free-rider problem*.\n\nCoordination Problems\n---------------------\n\nWith that under our belt, let’s look at a new decision matrix. Can you identify what’s importantly different about this matrix? Make a prediction about how you think this will change the players’ strategies.​\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/2472044ff084777ed8e2e687f465bdb9148f2450ed7ac80d.png)\n\nDon't mix this up with the Prisoners' Dilemma - it's quite different. In the PD, if you cooperate and I defect, I get 4. What’s important about the new decision-matrix, is that nobody has an incentive to backstab! If you cooperate and I defect, I get *zero*, instead of four.\n\nWe all want the same thing. Both players' preference ordering is:\n\n\\\\\\[(C,C)>(D,D)>\\[(C,D) = (D,C)\\]\\\\\\]\n\nSo, you might be confused: Why is this a problem at all? Why doesn’t everyone just pick C?\n\nLet me give an example from Michael Chwe’s [classic book](https://www.amazon.com/Rational-Ritual-Culture-Coordination-Knowledge/dp/0691114714) on the subject *Rational Ritual: Culture, Coordination and Common Knowledge*.\n\n> Say you and I are co-workers who ride the same bus home. Today the bus is completely packed and somehow we get separated. Because you are standing near the front door of the bus and I am near the back door, I catch a glimpse of you only at brief moments. Before we reach our usual stop, I notice a mutual acquaintance, who yells from the sidewalk, “Hey you two! Come join me for a drink!” Joining this acquaintance would be nice, but we care mainly about each other’s company. The bus doors open; separated by the crowd, we must decide independently whether to get off.\n\n> Say that when our acquaintance yells out, I look for you but cannot find you; I’m not sure whether you notice her or not and thus decide to stay on the bus. How exactly does the communication process fail? There are two possibilities. The first is simply that you do not notice her; maybe you are asleep. The second is that you do in fact notice her. But I stay on the bus because I don’t know whether you notice her or not. In this case we both know that our acquaintance yelled but I do not know that you know.\n\n> Successful communication sometimes is not simply a matter of whether a given message is received. It also depends on whether people are aware that other people also receive it. In other words, it is not just about people’s knowledge of the message; it is also about people knowing that other people know about it, the “metaknowledge” of the message.\n\n> Say that when our acquaintance yells, I see you raise your head and look around for me, but I’m not sure if you manage to find me. Even though I know about the yell, and I know that you know since I see you look up, I still decide to stay on the bus because I do not know that you know that I know. So just one “level” of metaknowledge is not enough.\n\n> Taking this further, one soon realizes that every level of metaknowledge is necessary: I must know about the yell, you must know, I must know that you know, you must know that I know, I must know that you know that I know, and so on; that is, the yell must be “common knowledge.” \n\n> The term “common knowledge” is used in many ways but here we stick to a precise definition. We say that an event or fact is common knowledge among a group of people if everyone knows it, everyone knows that everyone knows it, everyone knows that everyone knows that everyone knows it, and so on.\n\n> Two people can create these many levels of metaknowledge simply through eye contact: say that when our acquaintance yells I am looking at you and you are looking at me, \\[and we exchange a brief glance at our mutual friend and nod\\]. Thus I know you know about the yell, you know that I know that you know (you see me looking at you), and so on. If we do manage to make eye contact, we get off the bus; communication is successful.\n\nCoordination problems are only ever problems when everyone is *currently* choosing \\\\(D\\\\), and we need to *coordinate* all choosing \\\\(C\\\\) at the same time. To do that, we need common knowledge.\n\n(The specific definition of common knowledge (*\"I know that you know that I know that....\"*) is often confusing, but for now the concrete examples below should help get a solid intuition for the idea.)\n\nCompare you and I on the bus to the coordination game payoff matrix: If we *both get off the train *\\\\((C,C)\\\\)*,* we get to hang out with each other *and* spend some time with a mutual acquaintance. If only one of us does, we both miss out on the opportunity to hang out with each other (the thing we want least - \\\\((C,D)\\\\) or \\\\((D,C)\\\\)). If neither of us gets off the train, we get to hang out with each other, but in a less interesting way \\\\((D,D)\\\\).\n\n**A Stable State**\n\nThe reason that it’s a difficult coordination problem, is because the state \\\\((D,D)\\\\) is an equilibrium state; neither of us alone can improve it by getting off the bus - only if we’re able to coordinate us *both* getting off the bus does this work. You can think of it like a local optimum: if you take one step in any direction (if any single one of us changes our actions) we lose utility on net.\n\nThe name for such an equilibrium is taken from mathematician [John Nash](https://en.wikipedia.org/wiki/John_Forbes_Nash_Jr.) (who the film *A Beautiful Mind* was based on), and is called a *Nash equilibrium.* Both \\\\((C,C)\\\\) and \\\\((D,D)\\\\) are Nash equilibria in a coordination problem. Can you see how many Nash equilibria there are in the Prisoner's Dilemma?\n\n**Solving problems and resolving dilemmas**\n\nA good way to contrast coordination problems and free rider problems is to think about these equilibrium states. In the free rider problem, the situation where everyone cooperates is not a Nash equilibrium - everyone is incentivised to defect while the others cooperate, and so occasionally some people do. While the PD only has one Nash equilibrium however, a coordination problem has got two! The challenge is moving from the current one, to one we all prefer.\n\nFree rider problems are solved by creating new incentives against defecting. For example, the government punishes you if you don't pay your taxes. In sports, the practice of doping is punished, and what's more it's made out to be *dishonourable*. People tell stories of the evil people that dope and how we all look down on them; even if you could dope and probably get away with it, there's no plausible deniability in your mind - you know you're being a bad person and would be judged by everyone of your colleagues.\n\nCoordination problems can be solved by creating such incentives, but they can also be solved just by improving information flow. We'll see that below.\n\nThree Coordination Problems\n===========================\n\nThat situation when you and I lock eyes, nod, and get off the bus? That’s *having* *common knowledge*. It’s the confidence to take the step, because you’re not worried about what I might do. Because you know I’m getting off the bus with you.\n\nNow we’ve got a handle on what common knowledge is, we can turn back to the three puzzling phenomena from the beginning.\n\nDictators and freedom of speech\n-------------------------------\n\n> Dictatorships all through history have attempted to suppress freedom of the press and freedom of speech. Why is this? Are they just very sensitive? On the other side, the leaders of the Enlightenment fought for freedom of speech, and would not budge an inch against this principle.\n\nMany people under a dictatorship want a revolution - but rebelling only makes sense if enough *other* people want to rebel. The people as a whole are much more powerful than the government. But you alone won’t be any match for the local police force. You have to *know* that the others are willing to rebel (as long as you rebel), *and* you have to know that they know that *you’re* willing to rebel.\n\nPeople in a dictatorship are all trying to move to a better nash equilibrium without going via the corners of the box (i.e. where some people rebel, but not enough, and then you have some pointless death instead of a revolution).\n\nThat feeling of worrying whether the people around you will support you, if you attack the police. That’s what it’s like *not* to have common knowledge. When a dictator gets ousted by the people, it's often in the form of a riot, because you can *see the other people around you* who are poised on the brink of violence. They can see you, and you all know that if you moved as one you might accomplish something. That’s the feeling of common knowledge.\n\nThe dictator is trying to suppress the people’s ability to create common knowledge that jumps them straight to \\\\((C,C)\\\\) \\- and so they attempt to suppress the news media. Preventing common knowledge being formed among the populace means that large factions cannot coordinate - this is a successful divide and conquer strategy, and is why dictators are able to lead with so little support (often <1% of the population).\n\nUncertainty in Romance\n----------------------\n\n> When two people are on a date and want to sleep with each other, the conversation will often move towards but never *explicitly* discuss having sex. The two may discuss going back to the place of one of theirs, with a different explicit reason discussed (e.g. \"to have a drink\"), *even* if both want to have sex.\n\nNotice the difference between\n\n*   Walking up to someone cold at a bar and starting a conversation\n*   Walking up to someone at a bar, after you noticed them stealing glances at you\n*   Walking up to someone at a bar, after you glanced at them, they glanced at you, and your eyes *locked*\n\nIt’s easiest to approach confidently in the last case, since you have clear evidence that you’re both at least interested in a flirtatious conversation.\n\nIn dating, getting *explicitly* rejected is a loss of status, so people are incentivised to put a lot of effort into preserving plausible deniability. *No really, I just came up to your flat to listen to your vinyl records!* Similarly, we know other people don’t like getting rejected, so we rarely explicitly ask either. *Are you trying to have sex with me?*\n\nSo with sex, romance, or even deep friendships, people are often trying to get to \\\\((C,C)\\\\) *without* common knowledge, up until the moment that they’re both very confident that both parties are interested in raising their level of intimacy.\n\n(Scott Alexander wrote about this attempt to avoid rejection and the confusion it entails in his post [Conversation Deliberately Skirts the Border of Incomprehensibility](http://slatestarcodex.com/2017/06/26/conversation-deliberately-skirts-the-border-of-incomprehensibility/).)\n\nThis problem of *avoiding* common knowledge as we try to move to better Nash equilibrium also shows up in negotiations and war, where you might make a threat, and not *want* there to be common knowledge of whether you’ll actually follow through on that threat.\n\n(Added: After listening to a podcast with Robin Hanson, I realise that I've simplified too much here. It's also the case that each member of the couple might not have figured out whether they want to have sex, and so plausible deniability gives them an out if they decide not to, without the explicit status hit/attack.\n\nI definitely have the sense that if someone very bluntly states subtext when they notice it, this means *I can't play the game with* *them even if I wanted to*, as when they state it explicitly I have to say \"No!\" else admit that I was slightly flirting / exploring a romance with them, and significantly increase the change I will immediately receive an explicit rejection.)\n\nCommunal/Religious Rituals\n--------------------------\n\n> Throughout history, communities have had religious rituals that look very similar. Everyone in the village has to join in. There are repetitive songs, repetitive lectures on the same holy books, chanting together. Why, of all the possible community events (e.g. dinner, parties, etc) is this the most common type?\n\nMichael Chwe wrote a whole [book](https://www.amazon.com/Rational-Ritual-Culture-Coordination-Knowledge/dp/0691114714) on this topic. To simplify massively: rituals are a space to create common knowledge in a community.\n\nYou don’t just listen to a pastor talk about virtue and sin. You listen *together,* where you know that everyone else was listening too. You say ‘*amen*’ together after each prayer the pastor speaks, and you all know that you’re listening along and paying attention. You speak the Lord’s Prayer or some Buddhist chant together, and you know that *everyone* knows the words.\n\nRituals create common knowledge about what in the community is is rewarded, what is punished. This is why religions are so powerful (and why the state likes to control religion). It’s not just a part of life like other institutions everyone uses like a market or a bank - this is an institution that builds common knowledge about *all* areas of life, especially the most important communal norms.\n\nTo flesh out the punishment part of that: When someone does something sinful by the standards of the community, you know that *they* know they’re not supposed to, and they know that you know that they know. This makes it easier to punish people - they can’t claim they didn’t know they weren’t supposed to do something. And making it easier to punish people also makes people less likely to sin in the first place.\n\nThe rituals have been gradually improved and changed over time, and often the trade-offs have been towards helping coordinate a community. This is why the words in the chants or songs that everyone sings are simple, repetitive, and often rhyme - so you know that everyone knows exactly what they are. This is why rituals often occur seated in a circle - not only can you see the performance, but you can see *me* seeing the performance, and I you, and we have common knowledge.\n\nCommon knowledge is often much easier to build in small groups - in the example about getting off the bus, the two need only to look at each other, share a nod, and common knowledge is achieved. Building common knowledge between hundreds or thousands of people is significantly harder, and the fact that religion has such a significant ability to do so is why it has historically had so much connection to politics.\n\nCommon Knowledge Production in Society at Large\n===============================================\n\nCommon knowledge is a very common state of affairs that humans had to reason about naturally in the ancestral environment; there is no explicit mathematical calculation being done when two people lock eyes on a bus then coordinate getting off and seeing their friend.\n\nWe’ve looked at how religions help create common knowledge of norms. Here’s a few other common knowledge producing mechanisms that exist in the world today.\n\nThe News\n--------\n\nThe main way common knowledge is built is by having everyone in the same room, in silence, while somebody speaks. Another way (in the modern world) is official channels of communication that you know everyone listens to.\n\nThis is actually one of the good reasons to discuss news so much - we’ve built trust that what the NYT says is common knowledge, and so can coordinate around it. Sometimes an official document is advertised widely and is known to be known as common knowledge, even if we ourselves often haven’t read it (e.g. Will MacAskill’s book, the NYT).\n\nNowadays there is no such single news source, and we’ve lost that coordination mechanism. We all have Facebook, but Facebook is entirely built out of bubbles. Facebook *could* choose to create common knowledge by making something appear in everyone’s feed, but they choose not to (and this is in fact a fairly restrained use of power that I appreciate).\n\nOne time facebook slipped up on this, was when they built their 'Marked Safe' feature. If a dangerous event (big fire, terrorist attack, earthquake) happened near you, you could 'mark yourself safe' and then all of your friends would get a notification saying you were safe.\n\nNow, it was clear that everyone else was seeing the notifications you were seeing, and so if your nearby friend marked themselves safe and you didn’t, your friends would all notice that conspicuous absence of a notification, and know that you had chosen not to click it. This creates a pressure for all people to always notify their friends whenever there’s been a dangerous event near them, even if the odds of them being involved were miniscule. This is a clear waste of time and attention, ~and the feature was removed~ the feature continues to be a piece of security theatre in our lives.\n\nA related point about the power of media that creates common knowledge: in Michael Chwe's book, he does some data analysis of the marketing strategies of multiple different industries. He classifies products that are 'social goods' - those you want to buy if you expect other people like them. For example, you want to buy wines that you know your guests like, or bring beer to parties that others like; you want to use popular computer brands that people have developed software for; etc.\n\nHe then shows that social brands typically pay more *per viewer* for advertising; not necessarily more total, but that they'll pay a higher amount for opportunities to broadcast in places that generate common knowledge. Rather than buy 10 opportunities to broadcast to 2 million people on various channels, they'll pay a premium for 20 million people to view their ad during the superbowl, to create stronger common knowledge.\n\nAcademic Research\n-----------------\n\nThe central place where common knowledge is generated in science is in journals. These are where researchers can discover the new insights of the field, and build off them. Conferences can also help in this regard.\n\nA more interesting case is textbooks (I borrow this example from Oliver Habryka). There was once a time in the history of physics where the basics of quantum mechanics were known, and yet to study them required reading the right journal articles, in the right order. When you went to a convention of physicists, you likely had to explain many of the basics of the field *before* you could express your new idea.\n\nThen, some people decided to aggregate it into textbooks, which were then all taught to the undergraduates of the next generation, until the point where you could walk into the room and start using all the jargon and *trust that everyone knew what you meant*. Having common knowledge of the basics of a field is necessary for a field to make progress - to make the 201 the 101, and then build new insights on top.\n\nIn my life, even if 90% of the people around have the idea, when I’m not confident that 100% do then I often explain the basic idea for everyone. This often costs a lot of time - for example, after you read this post, I’ll be able to say to you a sentence like ‘the undergrad textbook system is a mechanism to create the common knowledge that allows the field as a whole to jump to the new Nash equilibrium of using advanced concepts’.\n\nParagraphs can be reduced to sentences, and you can get even more powerful returns with more abstract ideas - in mathematics, pages of symbols can be turned into a couple of lines (with the right abstractions e.g. calculus, linear algebra, probability theory, etc).\n\nStartups\n--------\n\nA startup is a very small group of people building detailed models of a product. They’re able to create a lot of common knowledge due to their small size. However, one of the reasons why they need to put a lot of thought into the long-term of the company, is because they will *lose* this common knowledge producing mechanism as they scale, and the only things they’ll be able to coordinate on are the things they already learned together.\n\nThe fact that they’re able to build common knowledge when they’re small is why they’re able to make so much more progress than big companies, and is also why big companies that innovate tend to compartmentalise their teams into small groups. As the company grows, there are far fewer things that can be retained as common knowledge amongst the employees. You can have intensive on-boarding processes for the first 20 hires, but it really doesn’t scale to 100 employees.\n\nHere are three things that can sustain at very large scales:\n\n**Name:** Y Combinator says that the name of your company should tell people what you do - cf. AirBnb, InstaCart, DoorDash, OpenAI, Lyft, etc. Contrast with companies like Palantir, where even I don’t know exactly what they work on day-to-day, and I’ve got friends who work there.\n\n**Mission:** It is possible to predict the output of an organisation very well by what their mission statement concretely communicates. For example, the company SpaceX has their mission statement at the top of all hiring documents (cf. the application forms to be a [rocket scientist](http://www.spacex.com/careers/position/214332), [business analyst](http://www.spacex.com/careers/position/214810), or [barista](http://www.spacex.com/careers/position/212463)).\n\n**Values:** Affects hiring and decision-making long into the future. YC specifically says to pick 4-8 core values, have a story associated with each value, and tell each story *every* day (e.g. in meetings). That may seem like way too much, but in fact that’s how much it can take to make the values common knowledge (especially as your company scales).\n\nAt what cost?\n-------------\n\nA standard response to coordination failures is one of *exasperation* \\- a feeling that we *should* be able to solve this if only we *tried*.\n\nImagine you’re trying to coordinate you and a few friends to move some furniture, and they keep getting in each other’s way. You might shout “Hey guys! Look, Pete and Laurie have to move the couch first, then John and Pauline can move the table!” And then things just start working. Or even just between two of you - when a friend is late for skype calls because she messes up her calendar app, you might express irritation, and she might try extra hard to fix the problem.\n\nWe also feel this when we look at society at large, for example when we look at coordination failures in politics. *Why does everyone continue voting for silly-no-good politicians? Why can’t we all just vote for someone sane?!*\n\nIn the book *Inadequate Equilibria* by Eliezer Yudkowsky, the character *Simplicio* represents this feeling. Here is the character discussing a (real) coordination failure in the US healthcare system that causes a few dozen newborn children to die every year:\n\n> **simplicio:** The first thing you have to understand, Visitor, is that the folk in this world are hypocrites, cowards, psychopaths, and sheep.\n\n> I mean, I certainly care about the the lives of newborn children. Hearing about their plight certainly makes me want to do something about it. When I see the problem continuing in spite of that, I can only conclude that other people don’t feel the level of moral indignation that I feel when staring at a heap of dead babies.\n\n> \\[...\\]\n\n> Regardless, I’m not seeing what the grand obstacle is to people solving these problems by, you know, coordinating. If people would just act in unity, so much could be done!\n\n> I feel like you’re placing too much blame on system-level issues, Cecie, when the simpler hypothesis is just that the people in the system are terrible: bad at thinking, bad at caring, bad at coordinating. You claim to be a “cynic,” but your whole world-view sounds rose-tinted to me.\n\nOne of the final points to deeply understand about common knowledge in society, is how *costly* it is to create at scale.\n\nBig companies get to pick only a few sentences to become common knowledge. To have a community rally around a more complex set of values and ideals (i.e. a significant function of religion) each and every member of that community must give up half of each Sunday, to repeat ideas *they already know* over and over - nothing new, just with the goal of creating common knowledge.\n\nThere used to be news programmes everybody in a country would tune in for. Notice how the New York Times used to be something people would read once per week or once per *day*, and discuss it with friends, even though most of the info has no direct effect on their lives.\n\nOur intuitions were developed for tribes of size 150 or less (cf. Dunbar’s number) and as such, our intuitions around coordination are often terribly off. Simplicio is someone who has not noticed the cost of creating common knowledge at scale. He believes that society could easily vote for good politicians *if only we coordinated*, and because we don’t he infers we *must* be stupid and/or evil.\n\nThe feeling of *indignation* at people for failing to coordinate can be thought of as creating an incentive to solve the coordination problem. I’m letting my skype partner know that I will punish them if they fail again. But today, this feeling toward people for failing to coordinate is almost always misguided.\n\nThink of it this way: many small coordination problems are sufficiently small that you’ll solve them quickly; many coordination problems are sufficiently big that you have no chance of solving them via normal means, and you will feel indignation every time you notice them (e.g. think politics/twitter). Basically, when you feel like being indignant in the modern world, 99% of the time it’s wasted motion.\n\nSimplicio’s intuitions are a great fit for a hunter-gatherer tribe. When he gets indignant, it would be proportional to the problem, the problem would get solved, and everyone would be happy. At a later point in the book Simplicio calls for political revolution - the sort of mechanism that works if you’re able to get everyone to gather in a single place.\n\nThe solution to coordination problems at scale is much harder, and requires thinking about incentives structures and information flows rather than emotions directed at individuals in your social environment. Or in other words, building a civilization.\n\n> **visitor:** Indeed. Moving from bad equilibria to better equilibria is the whole point of having a civilization in the first place.\n\n\\- Another character in [Inadequate Equilibria](https://www.lesserwrong.com/sequences/oLGCcbnvabyibnG9d), by Eliezer Yudkowsky\n\nSo, what's common knowledge *for*?\n==================================\n\nSummary of this post:\n\n1.  A coordination problem is when everyone is taking some action A, and we’d rather all be taking action B, but it’s bad if we don’t all move to B at the same time.\n2.  Common knowledge is the name for the epistemic state we’re collectively in, when we know we can all start choosing action B - and trust everyone else to do the same.\n3.  We’re intuitively very good at navigating such problems when we’re in small groups (size < 150).\n4.  We’re intuitively very bad at navigating such problems in the modern world, and require the building of new, microeconomic intuitions in order to build a successful society.\n\nThere is a great deal more subtlety to how common knowledge gets built and propagates. This post has given but a glimpse through the lens of game-theory, and hopefully you now see the light that this lens sheds on a great variety of phenomena.\n\n* * *\n\n*Links to explore more on this subject:*\n\n*   *Moloch’s Toolbox (Inadequate Equilibria, Ch 3)* ([link](https://www.lesserwrong.com/sequences/oLGCcbnvabyibnG9d))\n    *   *A guide to the ways our current institutions fail to coordinate. Largely applying standard microeconomics, and a great post to read after this one.*\n*   *Meditations on Moloch* ([link](https://www.lesserwrong.com/s/xmDeR64CivZiTAcLx/p/TxcRbCYHaeL59aY7E))\n    *   *An original idea about coordination failures, which the above book chapter formalised. It's a great post, and it's good to follow the intellectual heritage of ideas.*\n*   *Rational Ritual: Culture, Coordination and Common Knowledge* ([link](https://www.amazon.com/Rational-Ritual-Culture-Coordination-Knowledge/dp/0691114714))\n    *   *Solid book with lots of detail.*\n*   *Scott Aaronson on Common Knowledge and Aumann's Agreement Theorem* ([link](https://www.scottaaronson.com/blog/?p=2410))\n    *   *This post caused me to spend a bunch more time thinking about these topics. I found the explanations personally to be fairly abstract, which inspired me to write this post.*\n*   *Scott Alexander’s sequence on Game Theory* ([link](https://www.lesserwrong.com/posts/QxZs5Za4qXBegXCgu/introduction-to-game-theory-sequence-guide))\n    *   *After writing this post, I found Scott Alexander had also written about some of the examples (especially the dictatorship one) in detail 7 years ago* [(link)](http://squid314.livejournal.com/2011/02/01/).\n*   *Andrew Critch on 'Unrolling Social Metacognition: Three levels of meta are not enough'* ([link](https://www.lesswrong.com/posts/K4eDzqS2rbcBDsCLZ/unrolling-social-metacognition-three-levels-of-meta-are-not))\n    *   *This is a great post going into the details of how my modelling of you modelling me modelling you... works in practice. Highly recommended if the definition of common knowledge presented above seemed confusing.*\n\n* * *\n\n***Thanks** to Raymond Arnold, Jacob Lagerros and Oliver Habryka for extensive feedback and comments, and to Hadrien Pouget for proofreading an early draft. A further special mention to Raymond for pointing out this term ought to be a standard piece of* [expert jargon](https://www.lesserwrong.com/posts/DcRFTx62sTTRQo3Jw/common-vs-expert-jargon) *in this community, and suggesting I write this post*"
    },
    "voteCount": 96,
    "forceInclude": true
  },
  {
    "_id": "yLLkWMDbC9ZNKbjDG",
    "url": null,
    "title": "Slack",
    "slug": "slack",
    "author": "Zvi",
    "question": false,
    "tags": [
      {
        "name": "Slack"
      },
      {
        "name": "Practical"
      }
    ],
    "tableOfContents": {
      "sections": [
        {
          "title": "Definition: Slack. The absence of binding constraints on behavior.",
          "anchor": "Definition__Slack__The_absence_of_binding_constraints_on_behavior_",
          "level": 1
        },
        {
          "title": "Related Slackness",
          "anchor": "Related_Slackness",
          "level": 1
        },
        {
          "title": "Out to Get You and the Attack on Slack",
          "anchor": "Out_to_Get_You_and_the_Attack_on_Slack",
          "level": 1
        },
        {
          "title": "You Can Afford It",
          "anchor": "You_Can_Afford_It",
          "level": 1
        },
        {
          "title": "The Slackless Like of Maya Millennial",
          "anchor": "The_Slackless_Like_of_Maya_Millennial",
          "level": 1
        },
        {
          "title": "“Give Me Slack or Kill Me” – J.R. “Bob” Dobbs",
          "anchor": "_Give_Me_Slack_or_Kill_Me____J_R___Bob__Dobbs",
          "level": 1
        },
        {
          "title": "A Final Note",
          "anchor": "A_Final_Note",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        },
        {
          "anchor": "comments",
          "level": 0,
          "title": "71 comments"
        }
      ],
      "headingsCount": 9
    },
    "contents": {
      "markdown": "Epistemic Status: Reference post. Strong beliefs strongly held after much thought, but hard to explain well. Intentionally abstract.\n\nDisambiguation: This does not refer to any physical good, app or piece of software.\n\nFurther Research (book, recommended but not at all required, take seriously but not literally): [The Book of the Subgenius](https://smile.amazon.com/gp/product/B002XQAAS6/ref=s9u_simh_gw_i1?ie=UTF8&fpl=fresh&pd_rd_i=B002XQAAS6&pd_rd_r=FZCKWAV9EQDXHX6PZN61&pd_rd_w=ByYg5&pd_rd_wg=z5czc&pf_rd_m=ATVPDKIKX0DER&pf_rd_s=&pf_rd_r=8HFTESCZEKP6GJ3K6MV0&pf_rd_t=36701&pf_rd_p=1cf9d009-399c-49e1-901a-7b8786e59436&pf_rd_i=desktop)\n\nRelated (from sam\\[ \\]zdat, recommended but not required, take seriously and also literally, entire very long series also recommended): [The Uruk Machine](https://samzdat.com/2017/08/28/the-uruk-machine/)\n\nFurther Reading (book): [Scarcity: Why Having Too Little Means So Much](https://smile.amazon.com/Scarcity-Having-Little-Means-Much-ebook/dp/B00BMKOO6S/ref=sr_1_1?s=books&ie=UTF8&qid=1506174951&sr=1-1&keywords=scarcity)\n\nPreviously here (not required): [Play in Hard Mode](https://thezvi.wordpress.com/2017/08/26/play-in-hard-mode/), [Play in Easy Mode](https://thezvi.wordpress.com/2017/08/26/play-in-easy-mode/), [Out to Get You](https://thezvi.wordpress.com/2017/09/23/out-to-get-you/)\n\nLeads to (I’ve been scooped! Somewhat…): [Sabbath Hard and Go Home](http://benjaminrosshoffman.com/sabbath-hard-and-go-home)\n\nAn illustrative little game: [Carpe Diem: The Problem of Scarcity and Abundance](https://thezvi.wordpress.com/2015/05/01/carpe-diem-the-problem-of-scarcity-and-abundance/)\n\nSlack is hard to precisely define, but I think this comes close:\n\nDefinition: Slack. The absence of binding constraints on behavior.\n------------------------------------------------------------------\n\nPoor is the person without Slack. Lack of Slack compounds and traps.\n\nSlack means margin for error. You can _relax_.\n\nSlack allows pursuing opportunities. You can _explore_. You can _trade_.\n\nSlack prevents desperation. You can _avoid bad trades_ and _wait for better spots_. You can _be efficient_.\n\nSlack permits planning for the long term. You can _invest_.\n\nSlack enables doing things for your own amusement. You can _play games_. You can _have fun_.\n\nSlack enables doing the right thing. Stand by your friends. Reward the worthy. Punish the wicked. You can _have a code_.\n\nSlack presents things as they are without concern for how things look or what others think. You can _be honest_.\n\nYou can do some of these things, and choose not to do others. Because you don’t have to.\n\nOnly with slack can one be a _righteous dude_.\n\nSlack is life.\n\nRelated Slackness\n-----------------\n\nSlack in project management is the time a task can be delayed without causing a delay to either subsequent tasks or project completion time. The amount of time before a constraint binds.\n\nSlack the app was likely named in reference to a promise of Slack in the project sense.\n\nSlacks as trousers are pants that are actual pants, but do not bind or constrain.\n\nSlackness refers to vulgarity in West Indian culture, behavior and music. It also refers to a subgenre of dancehall music with straightforward sexual lyrics. Again, slackness refers to the absence of a binding constraint. In this case, common decency or politeness.\n\nA slacker is one who has a lazy work ethic or otherwise does not exert maximum effort. They _slack off_. They refuse to be bound by what others view as hard constraints.\n\n**Out to Get You and the Attack on Slack**\n------------------------------------------\n\nMany things in this world are [Out to Get You](https://thezvi.wordpress.com/2017/09/23/out-to-get-you/). Often they are [Out to Get You](https://thezvi.wordpress.com/2017/09/23/out-to-get-you/) for a lot, usually but not always your time, attention and money.\n\nIf you Get Got for compact amounts too often, it will add up and the constraints will bind.\n\nIf you Get Got _even once_ for a non-compact amount, the cost expands until the you have no Slack left. The constraints bind you.\n\nYou might spend every spare minute and/or dollar on politics, advocacy or charity. You might think of every dollar as a fraction of a third-world life saved. Racing to find a cure for your daughter’s cancer, [you already work around the clock](https://www.youtube.com/watch?v=UYBx7yxEME4). You could have an all-consuming job or be a soldier marching off to war. It could be a quest for revenge, for glory, for love. Or you might spend every spare minute mindlessly [checking Facebook](https://thezvi.wordpress.com/2017/04/22/against-facebook-comparison-to-alternatives-and-call-to-action/) or obsessed with your fantasy football league.\n\nYou cannot relax. Your life is not your own.\n\nIt might even be the right choice! Especially for brief periods. When about to be run over by a truck or evicted from your house, Slack is a luxury you cannot afford. Extraordinary times call for [extraordinary effort](http://lesswrong.com/lw/uo/make_an_extraordinary_effort/).\n\nMost times are ordinary. Make an ordinary effort.\n\n**You Can Afford It**\n---------------------\n\nNo, you can’t. This is the most famous attack on Slack. Few words make me angrier.\n\nThe person who says “You Can Afford It” is saying to ignore constraints that do not bind you. If you do, all constraints soon bind you.\n\nThose who do not value Slack soon lose it. Slack matters. Fight to keep yours!\n\nAsk not whether you can afford it. Ask if it is Worth It.\n\nUnless you can’t afford it. Affordability is invaluable _negative_ selection. Never _positive_ selection.\n\nThe You Can Afford It tax on Slack quickly approaches 100% if unchecked.\n\nIf those with extra resources are asked to share the whole surplus, all are poor or hide their wealth. Wealth is a burden and makes you a target. Those visibly flush rush to spend their bounty.\n\nWhere those with free time are given extra work, all are busy or look busy. Those with copious free time seek out relatively painless time sinks they can point to.\n\nWhen looking happy means you deal with everything unpleasant, no one looks happy for long.\n\n**The Slackless Like of Maya Millennial**\n-----------------------------------------\n\nThings are bad enough when those with Slack are expected to sacrifice for others. Things are much worse when the presence of Slack is viewed as a defection.\n\nAn example of this effect is [Maya Millennial](https://thezvi.wordpress.com/2017/09/05/expanding-premium-mediocrity/) (of [The Premium Mediocre Life of Maya Millennial](https://www.ribbonfarm.com/2017/08/17/the-premium-mediocre-life-of-maya-millennial/)). She has no Slack.\n\nConstraints bind her every action. Her job in life is putting up a front of the person she wants to show people that she wants to be. If her constraints noticeably failed to bind the illusion would fail.\n\nEvery action is being watched. If no one is around to watch her, the job falls to her. She must post all to Facebook, to Snapchat, to Instagram. Each action and choice signals who she is and her loyalty to the system. Not doing that this time could mean missing her one chance to make it big.\n\nMaya never has free time. There is signaling to do! At a minimum, she must spend such time on alert and on her phone lest she miss something.\n\nMaya never has spare cash. All must be spent to advance and fit her profile.\n\nMaya lacks free speech, free association, free taste and free thought. All must serve.\n\nMaya is in a world where _she must signal she has no Slack_. Slack means insufficient dedication and loyalty. Slack cannot be trusted. Slack now means slack later, which means failure. Future failure means no opportunity.\n\nThis is more common than one might think.\n\n**[“Give Me Slack or Kill Me” – J.R. “Bob” Dobbs](https://en.wikiquote.org/wiki/J._R._%22Bob%22_Dobbs)**\n--------------------------------------------------------------------------------------------------------\n\nThe aim of this post was to introduce Slack and give an intuitive picture of its importance.\n\nThe short-term practical takeaways are:\n\nMake sure that under normal conditions _you_ have Slack. Value it. Guard it. Spend it only when Worth It. If you lose it, fight to get it back. This provides motivation for fighting things [Out To Get You](https://thezvi.wordpress.com/2017/09/23/out-to-get-you/), lest you let them eat your Slack.\n\nMake sure to run a diagnostic test every so often to make sure you’re not running dangerously low, and to engineer your situation to force yourself to have Slack. I recommend [Sabbath Hard and Go Home](http://benjaminrosshoffman.com/sabbath-hard-and-go-home) with my take to follow soon.\n\nAlso respect the Slack of others. Help them value and guard it. Do not spend it lightly.\n\n**A Final Note**\n----------------\n\nI kept this short rather than add detailed justifications. Hopefully the logic is intuitive and builds on what came before. I hope to expand on the details and models later. For a very good book-length explanation of why lacking Slack is awful, see [Scarcity: Why Having Too Little Means So Much](https://smile.amazon.com/Scarcity-Having-Little-Means-Much-ebook/dp/B00BMKOO6S/ref=sr_1_1?s=books&ie=UTF8&qid=1506174951&sr=1-1&keywords=scarcity)."
    },
    "voteCount": 146,
    "forceInclude": true
  },
  {
    "_id": "gNodQGNoPDjztasbh",
    "url": null,
    "title": "Lies, Damn Lies, and Fabricated Options",
    "slug": "lies-damn-lies-and-fabricated-options",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "Rationality"
      },
      {
        "name": "Heuristics & Biases"
      }
    ],
    "tableOfContents": {
      "sections": [
        {
          "title": "Background 1: Gyroscopes",
          "anchor": "Background_1__Gyroscopes",
          "level": 1
        },
        {
          "title": "Background 2: H2O and XYZ",
          "anchor": "Background_2__H2O_and_XYZ",
          "level": 1
        },
        {
          "title": "feels coherent.",
          "anchor": "feels_coherent_",
          "level": 2
        },
        {
          "title": "Fabricated Options",
          "anchor": "Fabricated_Options",
          "level": 1
        },
        {
          "title": "Example 1: Price gouging",
          "anchor": "Example_1__Price_gouging",
          "level": 1
        },
        {
          "title": "Example 2: An orphan, or an abortion?",
          "anchor": "Example_2__An_orphan__or_an_abortion_",
          "level": 1
        },
        {
          "title": "Example 3: Drowning",
          "anchor": "Example_3__Drowning",
          "level": 1
        },
        {
          "title": "Example 4: Block lists",
          "anchor": "Example_4__Block_lists",
          "level": 1
        },
        {
          "title": "Example 5: Parental disapproval",
          "anchor": "Example_5__Parental_disapproval",
          "level": 1
        },
        {
          "title": "Example 6: 2020",
          "anchor": "Example_6__2020",
          "level": 1
        },
        {
          "title": "Conclusion",
          "anchor": "Conclusion",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        },
        {
          "anchor": "comments",
          "level": 0,
          "title": "119 comments"
        }
      ],
      "headingsCount": 13
    },
    "contents": {
      "markdown": "This is an essay about one of those \"once you see it, you will see it *everywhere\"* phenomena.  It is a psychological and interpersonal dynamic roughly as common, and almost as destructive, as motte-and-bailey, and at least in my own personal experience it's been quite valuable to have it reified, so that I can quickly recognize the commonality between what I had previously thought of as completely unrelated situations.\n\nThe original quote referenced in the title is \"There are three kinds of lies: lies, damned lies, and statistics.\"\n\n* * *\n\n**Background 1: Gyroscopes**\n----------------------------\n\nGyroscopes are weird.\n\nExcept they're not.  They're quite normal and mundane and straightforward.  The weirdness of gyroscopes is a map-territory confusion—gyroscopes *seem* weird because my map is poorly made, and predicts that they will do something other than their normal, mundane, straightforward thing.\n\nIn large part, this is because I don't have the *consequences of physical law* engraved deeply enough into my soul that they make intuitive sense.\n\nI can *imagine* a world that looks exactly like the world around me, in every way, *except* that in this imagined world, gyroscopes don't have any of their strange black-magic properties.  It feels coherent to me.  It feels like a world that could possibly exist.\n\n\"Everything's the same, except gyroscopes do nothing special.\"  Sure, why not.\n\nBut in fact, this world is deeply, deeply incoherent.  It is Not Possible with capital letters. And a physicist with sufficiently sharp intuitions would know this—would be able to *see* the implications of a world where gyroscopes \"don't do anything weird,\" and tell me all of the ways in which reality falls apart.\n\nThe *seeming coherence* of the imaginary world where gyroscopes don't balance and don't precess and don't resist certain kinds of motion is a product of *my own ignorance*, and of the looseness with which I am tracking how different facts fit together, and what the consequences of those facts are.  It's like a toddler thinking that they can eat their slice of cake, and still have that very same slice of cake available to eat again the next morning.\n\n* * *\n\n**Background 2: H2O and XYZ**\n-----------------------------\n\nIn the book *Labyrinths of Reason,* author William Poundstone delves into various thought experiments (like Searle's Chinese Room) to see whether they're actually coherent or not.\n\nIn one such exploration, he discusses the idea of a Twin Earth, on the opposite side of the sun, exactly like Earth in every way except that it doesn't have water.  Instead, it has a chemical, labeled XYZ, which behaves like water and occupies water's place in biology and chemistry, but is unambiguously distinct.\n\nOnce again, this is the sort of thing humans are *capable of imagining.*  I can nod along and say \"sure, a liquid that behaves just like water, but isn't.\"\n\nBut a chemist, intimately familiar with the structure and behavior of molecules and with the properties of the elements and their isotopes, would be throwing up red flags.\n\n\"Just like water,\" they might say, and I would nod.\n\n\"Liquid, and transparent, with a density of 997 kilograms per meter cubed.\"\n\n\"Sure,\" I would reply.\n\n\"Which freezes and melts at exactly 0º Celsius, and which boils and condenses at exactly 100º Celsius.\"\n\n\"Yyyyeahhhh,\" I would say, uneasiness settling in.\n\n\"Which makes up roughly 70% of the mass of the bodies of the humans of Twin Earth, and which is a solvent for hydrophilic substances, but not hydrophobic ones, and which can hold ions and polar substances in solution.\"\n\n\"Um.\"\n\n* * *\n\nThe more we drill down into what we mean by *behaves exactly like water,* the more it starts to become clear that there just *isn't* a possible substance which behaves exactly like water, but isn't.  There are only so many configurations of electrons and protons and neutrons (especially while remaining small enough to mimic water's molarity, and to play water's role in various chemical interactions).\n\nOnce again, our ability to imagine \"a substance that behaves exactly like water, but isn't\" is a product of *our own confusion.*  Of the fuzziness of our concepts, the fast-and-loose-ness of our reasoning, our willingness to overlook a host of details which are actually *crucially relevant* to the question at hand.\n\n(Tickling at the back of my mind is the axiom \"your strength as a rationalist is your ability to be more confused by fiction than by reality.\"  The thing I'm gesturing toward seems to be a corollary of sorts.)\n\nOf key importance:\n\n**Until we actually zero in on the incoherence, the imagined thing** ***feels coherent.*** **It seems every bit as potentially-real as actually-potentially-real options.**\n\nWe have no internal feeling that *warns* us that it's a fabrication masquerading as a possibility.  Our brains do not tell us when they're playing fast and loose.\n\n* * *\n\n**Fabricated Options**\n----------------------\n\nClaim: When people disagree with one another, or are struggling with difficult decisions, they frequently include, among their perceived options, at least one option which is fake-in-the-way-that-XYZ-is-fake.  An option that *isn't actually an option at all*, but which is a product of incoherent thinking.\n\nThis is what this essay seeks to point out, and to give you taste and triggers for.  I would like to establish *fabricated options* as a category in your mind, so that you are more likely to notice them, and less likely to be taken in by them.\n\n* * *\n\n**Example 1: Price gouging**\n----------------------------\n\nThis example is one that many of my readers will already be familiar with; it's the kind of topic that gets covered in Econ 101.  I'm not trying to teach it to you from scratch so much as get you to see it as an *instance* of the class of fabricated options, so that you can port your intuitions about price gouging over to other situations.\n\nIn short: during natural disasters or other market disruptions, it often becomes difficult to deliver things like food, water, clothing, toilet paper, medical supplies, gasoline, transportation, etc., to the people who need them.\n\nSometimes there simply isn't enough supply, and sometimes there's plenty of supply but the logistics become complicated (because, for instance, the act of physically delivering things becomes significantly more dangerous).\n\nIn those situations, the *price* of the needed items often goes through the roof.  Toilet paper selling for $100 a roll, Ubers costing $500 for a ten-mile drive, things like that.\n\nPeople watching from the outside see this, and feel horror and sympathy and dismay, and often propose (and sometimes successfully enact) legal barriers to *price gouging.*  They make it illegal to raise the price on goods and services, or put a ceiling on *how much* it can be raised.\n\nMost such interventions do not produce the desired effect.\n\nThe desired effect is that people will just continue to deliver and sell items for a reasonable price, as if nothing has happened.\n\nBut that option *was never really on the table.*  In the middle of a wildfire, or a massive flood, or raging citywide riots, or global supply chain disruption, it simply isn't possible. The *actual price* of the goods and services, in the sense of \"what does it take to provide them?\" has gone up, and the market price will necessarily follow.\n\nIf you *successfully* prevent people from selling toilet paper at $100 a roll (rather than simply driving the transactions underground into a black market), the actual effect is usually that there's no one selling toilet paper at all.\n\n* * *\n\nThe critical insight for this essay is that the thinking of the lawmakers is *confused.  *It is insufficiently detailed; insufficiently in touch with the reality of the situation.\n\nThe lawmakers seem to think that the options are:\n\n*   \\[Do nothing\\], and bad people will continue doing a bad thing, and ludicrously jacking up the price on critically necessary items.\n*   \\[Pass laws forbidding/punishing sharp price increases in times of trouble\\], and the bad people will just *not* do the bad thing, and the critically necessary items will be available for reasonable prices.\n\n... and in that world, given that menu of options, *of course* we should choose the second one!\n\nBut in reality, that is not the menu.  The second option is *fabricated.*  The story in which \\[passing that law\\] results in goods being available at normalish prices is an incoherent fairy tale.  It falls apart as soon as you start digging into the details, and realize that there are forces at work which cannot be dispersed by the stroke of a lawmaker's pen, just as there are physical laws which prevent non-weird gyroscopes and non-water XYZ.\n\n(No matter how easy it is to imagine these things, when we gloss over the relevant details.)\n\nIn fact, the true options in most such situations are:\n\n*   \\[Do nothing\\], and people will be able to get access to the critically necessary items, but it will be much harder and more expensive because there is low supply and high logistical difficulty.\n*   \\[Pass laws forbidding/punishing sharp price increases in times of trouble\\], and people won't be able to get anything at all, because someone erected an artificial barrier to trade.\n\nAnd given *that* menu of options, the first is obviously (usually) better.\n\n* * *\n\n**Caveat 1:** this could be misinterpreted (both in the specific case of price gouging and in the more general case of fabricated options) as encouraging a sort of throw-up-your-hands, if-we-can't-solve-everything-we-shouldn't-bother-to-try-anything helplessness.\n\nThat's not the point.  There are often ways to *break* the tradeoff dynamics at play, in any given situation.  There are often third paths, and ways to cheat, and ways to optimize within the broken system to minimize negative effects and maximize positive outcomes.\n\nThere are, in other words, *some versions* of anti-price-gouging laws that do marginal good and avoid the outright stupid failure modes.\n\nBut in order to have those intelligent effects, you first have to see and account for the relevant constraints and tradeoffs, and what I am attempting to point at with the above example is the common human tendency to *not do so*.  To simply live in the fantasy world of what we could \"just\" accomplish, if people would \"just\" do \\[simple-sounding but not-actually-possible thing\\].  \n\n*Most* anti-price-gouging proposals are naive in exactly the way described above; this is not meant to imply that non-naive proposals don't exist.  They do.  I'm just focusing on the central tendency and ignoring the unusually competent minority.  \n \n\n**Caveat 2:** in this example and many others, the fabricated option is less a made-up action and more a made-up story about the *consequences* of that action.  In both versions of the above dilemma, the listed actions were the same.  The difference was the valence assigned to the \"pass laws\" option, and the story emerging from it.\n\nThis is not always the case.  Sometimes people think the options are A or B, and they are in fact B or C, and sometimes people think the options are A or B *and they are,* but their imagination distorts the impact of option A into something utterly unrealistic.\n\nFor the sake of thinking about the *category* \"fabricated options,\" this distinction is not especially relevant, and will mostly be ignored in the rest of the essay.  The important thing to note is that in either case, the fabricated option has *inflated relative appeal.*  \n \n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/1aa84afd73dd8ec794cfab6c82eb44d908ccc57b784fe3d0.png)\n\nEither it's a genuinely available action A wrapped up in an incoherent and unrealistic story that makes it sound better than the unappealing B, or it's an entirely made-up option A which makes the actual best option B look bad in comparison (causing us to fail to shoot for B over an even worse default C).\n\nIn both cases, the *result in practice* is that option B, which is usually sort of dour and uninspiring and contains unpleasant costs or tradeoffs, gets something like *disproportionately downvoted*.  Downvoted relative to an impossible standard—treated as worse than it ought to be treated, given constraints.\n\nIt's a common assumption among both rationalists and the population at large that people tend to *flinch away* from things which are unpleasant to think about.  However, people rarely take the time to spell out just what \"flinching\" means, in practice, or just what triggers it.\n\nThe fabrication of options is, I claim, one example of flinching.  It's one of the things we *do,* as humans, when we feel ourselves about to be forced into choosing an uncomfortable path.  There's a sense of \"surely not\" that sends our minds in any other available direction, and if we're not *careful*—if we do not *actively hold ourselves* to a certain kind of stodgy actuarial insistence-on-clarity-and-coherence—we'll more than likely latch onto a nearby pleasant fiction without ever noticing that it doesn't stand up to scrutiny.\n\n\"If only they would just \\[calm down/listen/take a deep breath/forgive me/let it go/have a little perspective/not be so jealous/not be so irrational/think things through more carefully/realize how much I love them/hang on just a little bit longer\\], everything would be fine.\"\n\nPleasant fictions always outnumber pleasant truths, after all.\n\n* * *\n\n**Example 2: An orphan, or an abortion?**\n-----------------------------------------\n\nThis is the question posed by John Irving's excellent novel *The Cider-House Rules.  *The point of the question, within the novel, is to *break* the false dichotomy wherein the choices are framed as \"a living baby or a dead/murdered one?\"\n\nA living baby:  [🙂](https://emojipedia.org/slightly-smiling-face/)\n\nA dead one: 🙁, or perhaps [😡](https://emojipedia.org/pouting-face/)\n\nBut \"living baby\" in the sense often pushed for by pro-life advocates is something of a motte-and-bailey.  It's a naive, fabricated option.  It hand-waves away all of the inconvenient and uncomfortable detail, in exactly the same fashion as \"gyroscopes, but not weird.\"\n\nJohn Irving's novel doesn't take a stand on which is better—rather, it tries to force the reader to *consider the decision at all,* instead of getting confused by alluring falsehoods. The footing of the two sides, in the novel, is less uneven-by-design, which seems to me like a step in the right direction.\n\n* * *\n\n**Example 3: Drowning**\n-----------------------\n\nI have a longtime friend who I'll refer to here as Taylor, who's got a longtime romantic partner who I'll refer to here as Kelly.\n\nKelly struggles with various mental health issues.  They genuinely do their best, but as is so often the case, their best is not really \"enough.\"  They spend the better part of each year depressed and mildly delusional, with frequent dangerous swerves into suicidality.\n\nAs a side effect of these issues, Kelly—who is *at their core* an excellent partner for Taylor—also puts Taylor through the wringer.  Kelly has destroyed multiple of Taylor's possessions, multiple times.  Kelly has screamed and yelled at Taylor, multiple times. Over and over, Taylor has asked Kelly what would help, what they can do, how they could change their own behavior to be a better partner for Kelly—and over and over, granting Kelly's explicit requests has resulted in Taylor being yelled at, punished, told to go away.\n\nThis has been rough.\n\nTaylor is *already* the sort of person who doesn't give up on people—the sort of person who would willingly sacrifice themselves for a friend or a family member, the sort of person who will go to genuinely extreme lengths to save a fellow human in trouble.\n\nAnd on top of that, Taylor genuinely loves Kelly, and has plenty of evidence that—when things are okay—Kelly genuinely loves Taylor.\n\nBut for years now, the situation has been spiraling, and Taylor has been getting more and more exhausted and demoralized, and it has become increasingly clear that neither Taylor's direct efforts, nor any of the other resources they've funneled Kelly's way (therapists, medication, financial stability, freedom of movement), are going to be sufficient.  It no longer seems reasonable to *expect things to get better.*\n\nTaylor and I have talked about the situation a lot, and one of the metaphors that has come up more and more often is that of a drowning person out in rough waters.\n\nFrom Taylor's point of view, saving Kelly is worth it.  Saving Kelly is worth it *even if it means Taylor goes under.*  From Taylor's point of view, the options have always been \"help save Kelly, or watch Kelly drown.\"\n\nBut this frame is broken.  At this point, it's clear that \"help save Kelly\" is not a real option.  It's a fabrication, conjured up because it is *deeply uncomfortable* to face the real choice, which is \"let Kelly drown, or drown *with* them.\"\n\n(Alternately, and a little less harshly: \"let Kelly figure out how to swim on their own, or keep trying to help them and drown, yourself, without actually having helped them float.\")\n\n* * *\n\n**Example 4: Block lists**\n--------------------------\n\nI've previously had disagreements with a few people in various bubbles over block lists, and coordination, and what the defaults should be, and where various obligations lie.\n\nIn my (probably straw) characterization of the other side, they're *fabricating options.* They hold a position that (probably deserves steelmanning, but given my current state of understanding) looks like:\n\n*   Option A, everyone keeps the lines of communication open, and people don't block each other except under extraordinary circumstances (which will tend to be legible and obvious and which basically everyone will agree upon), and that way everyone can see all of the important discussion and there aren't confusing non-overlapping bubbles of fragmented common knowledge.\n*   Option B, some people defect on the project of maintaining a clear and open commons, and block people, and make everything worse for everybody.\n\nOption A is [🙂](https://emojipedia.org/slightly-smiling-face/)\n\nOption B is clearly 🙁\n\nIn my trying-to-look-at-the-actual-tradeoffs perspective, though—\n\n(Which is not meant to imply that the other people aren't also trying, it just seems to me like if they are trying, they're not quite managing to do so.)\n\n—it seems to me that the actual options are:\n\n*   Option B, which is very much just as 🙁 as they think it is, in which the world is imperfect and communication and coordination are tricky and costly and often go sideways, and some people need to block other people for all sorts of valid and self-protective reasons, and yep, this makes it harder to coordinate and establish common knowledge but it's the *actual best we can do—*\n*   or Option A, which is 😱, in which the self-protective blocking option is outlawed or disincentivized-on-the-margin, and people are either punished when they do it anyway (analogous to people being fined for selling toilet paper at inflated prices) or somehow compelled *not* to, in which they are either constantly exposed to triggers and to attacks from their enemies and abusers and all sorts of other things that are horrible for their mental health, or they just go dark and disappear from the conversation altogether.\n\nThe version of option A where \\[everyone just manages to be in the same room all the time and it's just never disastrously problematic\\] is obviously better than either of the two options described above.\n\nBut it's a substance identical to water that isn't water.  It's not actually on the table.\n\n* * *\n\n**Example 5: Parental disapproval**\n-----------------------------------\n\nYour kid wants to hang out with another kid who you're pretty sure is a bad influence.\n\nYour kid wants to quit their piano lessons, sinking their previous three years of effort.\n\nYour kid seems like they're about to start having sex, or using drugs, or playing Magic: the Gathering.\n\nYour kid doesn't want to go to the family reunion.\n\nYour kid doesn't want to eat that.\n\nI see parents' hopes and expectations come up against the reality of their kids' preferences all the time, and I always have this sucking-in-a-breath, edge-of-my-seat anticipation, because it *so often* seems to me like parents fabricate options rather than dealing with the tradeoffs with eyes open.\n\n*If I just tell them they can't hang out with that kid anymore, the problem will be solved.*\n\n*If I just make them keep playing piano, they'll thank me later.*\n\n*I can just tell them no.*\n\n*I can just tell them they have to.*\n\n*I can ground them until they comply.*\n\nAs with the example of price gouging, it's not that there aren't good ways to intervene on the above situations.  The claim is not \"the options, as they are at this exact moment, are the only options that will ever be on the table.\"\n\nRather, it's \"there *are* a certain limited number of options on the table at this exact moment.  If none of them are satisfactory, someone will have to actively create or uncover new ones.  They can't be willed into being by sheer stubborn fiat.\"\n\nOption A, in each of the above scenarios, comes with *massive* costs, usually taken out of the value of the parent-child relationship.\n\nSure, you *can* ban your child from a given friendship, but what's going to *actually* *happen* is that your child will stop viewing you as their ally and start treating you as a prison warden or appointed overseer—as obstacle to be dealt with.  They'll either succeed at getting around your edict, and you'll have sacrificed a significant part of your mutual trust for nothing, or they'll fail, and resent you for it.\n\nSome parents would argue that this is fine, it's worth it, better the kid be mad at *me* than suffer \\[bad outcome\\].\n\nAnd in some cases that's genuinely true.\n\nBut *most* of the time, the thing the parent implicitly imagines—that they can get \\[good outcome\\] *and* it won't cost anything in terms of relationship capital—it's not really on the table.\n\nIt's not \"I'll make them play piano and everything will be fine\" versus \"they'll lose their piano-playing potential.\"\n\nIt's \"I'll make them play piano by using our mutual affection as kindling\" or \"I'll let them do what they want and preserve our relationship.\"\n\nNeither option is great, viewed through that lens.  It's an orphan on the one hand and an abortion on the other.\n\nBut that's the thing.  Most of the time, neither option *is* great.  In difficult situations, it's wise to be at least a little suspicious of straightforward, easy Options A that are just so clearly better than those uncomfortably costly tradeoff-y Options B.\n\n* * *\n\n**Example 6: 2020**\n-------------------\n\n(This section left as an exercise for the reader.)\n\n* * *\n\n**Conclusion**\n--------------\n\nA likely thought on the minds of some readers is that this isn't exactly new ground, and we already have all of the pieces necessary to individually identify each instance of fabricated options based on their inherent falsehood, and therefore don't actually need the new category.\n\nI disagree; I find that [fine distinctions are generally useful](https://www.facebook.com/duncan.sabien/posts/4232363480131670) and have personally benefitted from being able to port strategies *between* widely-spaced instances of option fabrication, and from being able to train my option-fabrication-recognizer on a broad data set.\n\nThat being said: beware the failure mode of new jargon, which is thinking that you now *recognize* \\[the thing\\], rather than that you are now equipped to *hypothesize* \\[maybe the thing?\\].  The world would be a better place if people's response to the reification of concepts like \"sealioning\" or \"DARVO\" or \"attention-deficit disorder\" were to *ask whether that's what's happening here, and how we would know* as opposed to immediately weaponizing them.\n\n(Alas, that's a fabricated option, and the real choice is between \"invent good terms but see them misused a bunch\" and \"refuse to invent good terms.\"  But maybe LessWrong can do better than genpop.)\n\nAs for what to do about fabricated options (both those your own brain generates and those generated by others), the general recommendation is pretty much \"use your rationality\" and there isn't room in this one essay to operationalize that.  My apologies.\n\nIf you're looking for e.g. specific named [CFAR techniques](https://rationality.org/resources/handbook) that might come in handy here, I'd point you toward TAPs (especially TAPs for noticing fabricated options as they come up, or booting up your alert awareness in situations where they're likely to) and Murphyjitsu (which is likely to improve people's baseline ability to both recognize glossed-over fairy tales and patch the holes therein).  You might also work on building your general noticing skill, perhaps starting with any number of writings by [Logan Strohl](https://agentyduck.blogspot.com/2015/07/cognitive-trigger-action-planning-for.html), and on [double crux](https://www.lesswrong.com/posts/exa5kmvopeRyfJgCy/double-crux-a-strategy-for-resolving-disagreement) and similar tools, which will make it easier to make disagreements over the menu-of-options *productive* rather than not.\n\nIn the meantime, I would deeply appreciate it if any comments sharing examples of the class contained the string #EXAMPLE, and if any comments containing concrete recommendations or stories about how-you-responded contained the string #TOOLS. This will make it easier for the comment section to stand as an enduring and useful appendix to this introduction.\n\nGood luck.\n\n* * *\n\nFollowup from Logan Strohl: [Investigating Fabrication](https://www.lesswrong.com/posts/NjZAkfio5FsCioahb/investigating-fabrication)"
    },
    "voteCount": 149,
    "forceInclude": true
  },
  {
    "_id": "wEebEiPpEwjYvnyqq",
    "url": null,
    "title": "When Money Is Abundant, Knowledge Is The Real Wealth",
    "slug": "when-money-is-abundant-knowledge-is-the-real-wealth",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "World Modeling"
      },
      {
        "name": "Expertise (topic)"
      },
      {
        "name": "Economics"
      }
    ],
    "tableOfContents": {
      "sections": [
        {
          "title": "First Puzzle Piece",
          "anchor": "First_Puzzle_Piece",
          "level": 1
        },
        {
          "title": "Second Puzzle Piece",
          "anchor": "Second_Puzzle_Piece",
          "level": 1
        },
        {
          "title": "Putting The Pieces Together",
          "anchor": "Putting_The_Pieces_Together",
          "level": 1
        },
        {
          "title": "Investments In Knowledge",
          "anchor": "Investments_In_Knowledge",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        },
        {
          "anchor": "comments",
          "level": 0,
          "title": "61 comments"
        }
      ],
      "headingsCount": 6
    },
    "contents": {
      "markdown": "First Puzzle Piece\n------------------\n\nBy and large, the President of the United States can order people to do things, and they will do those things. POTUS is often considered the most powerful person in the world. And yet, the president cannot order a virus to stop replicating. The president cannot order GDP to increase. The president cannot order world peace.\n\nAre there orders the president could give which would result in world peace, or increasing GDP, or the end of a virus? Probably, yes. Any of these could likely even be done with relatively little opportunity cost. Yet no president in history has known *which* orders will efficiently achieve these objectives. There are probably some people in the world who know which orders would efficiently increase GDP, but the president cannot distinguish them from the millions of people who *claim* to know (and may even believe it themselves) but are wrong.\n\nLast I heard, Jeff Bezos was the official richest man in the world. He can buy basically anything money can buy. But he can’t buy a cure for cancer. Is there some way he could spend a billion dollars to cure cancer in five years? Probably, yes. But Jeff Bezos does not know how to do that. Even if someone somewhere in the world does know how to turn a billion dollars into a cancer cure in five years, Jeff Bezos cannot distinguish that person from the thousands of other people who *claim* to know (and may even believe it themselves) but are wrong.\n\nWhen non-experts cannot distinguish true expertise from noise, [money cannot buy expertise](https://www.lesswrong.com/posts/YABJKJ3v97k9sbxwg/what-money-cannot-buy). Knowledge cannot be outsourced; we must understand things ourselves.\n\nSecond Puzzle Piece\n-------------------\n\nThe [Haber process](https://en.wikipedia.org/wiki/Haber_process) combines one molecule of nitrogen with three molecules of hydrogen to produce two molecules of ammonia - useful for fertilizer, explosives, etc. If I feed a few grams of hydrogen and several tons of nitrogen into the Haber process, I’ll get out a few grams of ammonia. No matter how much more nitrogen I pile in - a thousand tons, a million tons, whatever - I will not get more than a few grams of ammonia. If the reaction is limited by the amount of hydrogen, then throwing more nitrogen at it will not make much difference.\n\nIn the language of [constraints and slackness](https://www.lesswrong.com/s/xEFeCwk3pdYdeG2rL): ammonia production is constrained by hydrogen, and by nitrogen. When nitrogen is abundant, the nitrogen constraint is slack; adding more nitrogen won’t make much difference. Conversely, since hydrogen is scarce, the hydrogen constraint is taut; adding more hydrogen will make a difference. Hydrogen is the bottleneck.\n\nLikewise in economic production: if a medieval book-maker requires 12 sheep skins and 30 days’ work from a transcriptionist to produce a book, and the book-maker has thousands of transcriptionist-hours available but only 12 sheep, then he can only make one book. Throwing more transcriptionists at the book-maker will not increase the number of books produced; sheep are the bottleneck.\n\nWhen some inputs become more or less abundant, bottlenecks change. If our book-maker suddenly acquires tens of thousands of sheep skins, then transcriptionists may become the bottleneck to book-production. In general, when one resource becomes abundant, other resources become bottlenecks.\n\nPutting The Pieces Together\n---------------------------\n\nIf I don’t know how to efficiently turn power into a GDP increase, or money into a cure for cancer, then throwing more power/money at the problem will not make much difference.\n\nKing Louis XV of France was one of the richest and most powerful people in the world. [He died of smallpox in 1774, the same year that a dairy farmer successfully immunized his wife and children with cowpox](https://www.lesswrong.com/posts/YABJKJ3v97k9sbxwg/what-money-cannot-buy). All that money and power could not buy the knowledge of a dairy farmer - the knowledge that cowpox could safely immunize against smallpox. There were thousands of humoral experts, faith healers, eastern spiritualists, and so forth who would claim to offer some protection against smallpox, and King Louis XV could not distinguish the real solution.\n\nAs one resource becomes abundant, other resources become bottlenecks. When wealth and power become abundant, anything wealth and power cannot buy become bottlenecks - including knowledge and expertise.\n\nAfter a certain point, wealth and power cease to be the taut constraints on one’s action space. They just don’t matter that much. Sure, giant yachts are great for social status, and our lizard-brains love politics. The modern economy is happy to provide outlets for disposing of large amounts of wealth and power. But personally, I don’t care that much about giant yachts. I want a cure for aging. I want weekend trips to the moon. I want flying cars and an indestructible body and tiny genetically-engineered dragons. Money and power can’t efficiently buy that; the bottleneck is knowledge.\n\nBased on my own experience and the experience of others I know, I think knowledge starts to become taut rather quickly - I’d say at an annual income level in the low hundred thousands. With that much income, if I knew exactly the experiments or studies to perform to discover a cure for cancer, I could probably make them happen. (Getting regulatory approval is another matter, but I think that would largely handle itself if people knew the solution - there’s a large profit incentive, after all.) Beyond that level, more money mostly just means more ability to spray and pray for solutions - which is [not a promising strategy in our high-dimensional world](https://www.lesswrong.com/posts/pT48swb8LoPowiAzR/everyday-lessons-from-high-dimensional-optimization).\n\nSo, two years ago I quit my monetarily-lucrative job as a data scientist and have mostly focused on acquiring knowledge since then. I can worry about money if and when I know what to do with it.\n\nA mindset I recommend trying on from time to time, especially for people with $100k+ income: think of money as an abundant resource. Everything money can buy is “cheap”, because money is \"cheap\". Then the things which are “expensive” are the things which money alone cannot buy - including knowledge and understanding of the world. Life lesson from [Disney!Rumplestiltskin](https://www.imdb.com/title/tt1843230/): there are things which money cannot buy, therefore it is important to acquire such things and use them for barter and investment. In particular, it’s worth looking for opportunities to acquire knowledge and expertise which can be leveraged for *more* knowledge and expertise.\n\nInvestments In Knowledge\n------------------------\n\nPast a certain point, money and power are no longer the limiting factors for me to get what I want. Knowledge becomes the bottleneck instead. At that point, money and power are no longer particularly relevant measures of my capabilities. Pursuing more “wealth” in the usual sense of the word is no longer a very useful instrumental goal. At that point, the type of “wealth” I really need to pursue is knowledge.\n\nIf I want to build long-term knowledge-wealth, then the analogy between money-wealth and knowledge-wealth suggests an interesting question: what does a knowledge “investment” look like? What is a capital asset of knowledge, an investment which pays dividends in more knowledge?\n\n[Enter gears-level models](https://www.lesswrong.com/posts/nEBbw2Bc2CnN2RMxy/gears-level-models-are-capital-investments).\n\nMapping out the internal workings of a system takes a lot of up-front work. It’s much easier to try random molecules and see if they cure cancer, than to map out all the internal signals and cells and interactions which cause cancer. But the latter is a capital investment: once we’ve nailed down one gear in the model, one signal or one mutation or one cell-state, that informs all of our future tests and model-building. If we find that Y mediates the effect of X on Z, then our future studies of the Y-Z interaction can safely ignore X. On the other hand, if we test a random molecule and find that it doesn’t cure cancer, then that tells us little-to-nothing; that knowledge does not yield dividends.\n\nOf course, gears-level models aren’t the only form of capital investment in knowledge. Most tools of applied math and the sciences consist of general models which we can learn once and then apply in many different contexts. They are general-purpose gears which we can recognize in many systems.\n\nOnce I understand the internal details of how e.g. [capacitors](https://en.wikipedia.org/wiki/Capacitor) work, I can apply that knowledge to understand not only electronic circuits, but also [charged biological membranes](https://en.wikipedia.org/wiki/Membrane_potential). When I understand the math of microeconomics, I can [apply it](https://www.lesswrong.com/posts/brhWPoNsBN7za3xjs/competitive-markets-as-distributed-backprop) to optimization problems in AI. When I understand shocks and rarefactions in nonlinear PDEs, I can see them in action at the beach or [in traffic](https://youtu.be/Rryu85BtALM?t=47). And the “core” topics - calculus, linear algebra, differential equations, big-O analysis, Bayesian probability, optimization, dynamical systems, etc - can be applied all over. General-purpose models are a capital investment in knowledge.\n\nI hope that someday my own research will be on that list. That’s the kind of wealth I’m investing in now."
    },
    "voteCount": 159,
    "forceInclude": true
  },
  {
    "_id": "YABJKJ3v97k9sbxwg",
    "url": null,
    "title": "What Money Cannot Buy",
    "slug": "what-money-cannot-buy",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "World Modeling"
      },
      {
        "name": "Expertise (topic)"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "[Paul Graham](http://www.paulgraham.com/gh.html):\n\n> The problem is, if you're not a hacker, you can't tell who the good hackers are. A similar problem explains why American cars are so ugly. I call it the design paradox. You might think that you could make your products beautiful just by hiring a great designer to design them. But if you yourself don't have good taste, how are you going to recognize a good designer? By definition you can't tell from his portfolio. And you can't go by the awards he's won or the jobs he's had, because in design, as in most fields, those tend to be driven by fashion and schmoozing, with actual ability a distant third. There's no way around it: you can't manage a process intended to produce beautiful things without knowing what beautiful is. American cars are ugly because American car companies are run by people with bad taste.\n\nI don’t know how much I believe this claim about cars, but I certainly believe it about software. A startup without a technical cofounder will usually produce bad software, because someone without software engineering skills does not know how to recognize such skills in someone else. The world is full of bad-to-mediocre “software engineers” who do not produce good software. If you don’t already know a fair bit about software engineering, you will not be able to distinguish them from the people who really know what they’re doing.\n\nSame with user interface design. I’ve worked with a CEO who was good at UI; both the process and the results were visibly superior to others I’ve worked with. But if you don’t already know [what good UI design looks like](https://www.amazon.com/Design-Everyday-Things-Revised-Expanded/dp/0465050654/), you’d have no idea - good design is largely invisible.\n\nYudkowsky [makes the case](https://intelligence.org/2017/11/26/security-mindset-and-the-logistic-success-curve/) that the same applies to security: you can’t build a secure product with novel requirements without having a security expert as a founder. The world is full of “security experts” who do not, in fact, produce secure systems - I’ve met such people. (I believe they mostly make money by helping companies visibly pretend to have made a real effort at security, which is useful in the event of a lawsuit.) If you don’t already know a fair bit about security, you will not be able to distinguish such people from the people who really know what they’re doing.\n\nBut to really drive home the point, we need to go back to 1774.\n\nAs the American Revolution was heating up, a wave of smallpox was raging on the other side of the Atlantic. An English dairy farmer named Benjamin Jesty was concerned for his wife and children. He was not concerned for himself, though - he had previously contracted cowpox. Cowpox was contracted by milking infected cows, and was well known among dairy farmers to convey immunity against smallpox.\n\nUnfortunately, neither Jesty’s wife nor his two children had any such advantage. When smallpox began to pop up in Dorset, Jesty decided to take drastic action. He took his family to a nearby farm with a cowpox-infected cow, scratched their arms, and wiped pus from the infected cow on the scratches. Over the next few days, their arms grew somewhat inflamed and they suffered the mild symptoms of cowpox - but it quickly passed. As the wave of smallpox passed through the town, none of the three were infected. Throughout the rest of their lives, through multiple waves of smallpox, they were immune.\n\nThe same technique would be popularized twenty years later by Edward Jenner, marking the first vaccine and the beginning of modern medicine.\n\nThe same wave of smallpox which ran across England in 1774 also made its way across Europe. In May, it reached Louis XV, King of France. Despite the wealth of a major government and the talents of Europe’s most respected doctors, Louis XV died of smallpox on May 10, 1774.\n\nThe point: there is knowledge for which money cannot substitute. Even if Louis XV had offered a large monetary bounty for ways to immunize himself against the pox, he would have had no way to distinguish Benjamin Jesty from the endless crowd of snake-oil sellers and faith healers and humoral balancers. Indeed, top medical “experts” of the time would likely have warned him _away_ from Jesty.\n\nThe general pattern:\n\n*   Take a field in which it’s hard for non-experts to judge performance\n*   Add lots of people who _claim_ to be experts (and may even believe that themselves)\n*   Result: someone who is not already an expert will not be able to buy good performance, even if they throw lots of money at the problem\n\nNow, presumably we can get around this problem by investing the time and effort to become an expert, right? Nope! Where there are snake-oil salesmen, there will also be people offering to teach their secret snake-oil recipe, so that you too can become a master snake-oil maker.\n\nSo… what _can_ we do?\n\nThe cheapest first step is to do some basic reading on a few different viewpoints and think things through for yourself. Simply reading [the “correct horse battery staple” xkcd](https://xkcd.com/936/) will be sufficient to recognize a surprising number of really bad “security experts”. It probably won’t get you to a level where you can distinguish the best from the middling - I don’t think I can currently distinguish the best from the middling security experts. But it’s a start.\n\nMore generally: it’s often easier to tell which of multiple supposed experts is correct, than to figure everything out from first principles yourself. Besides looking at the object-level product, this often involves looking at incentives in the broader system - see e.g. [Inadequate Equilibria](https://equilibriabook.com/). Two specific incentive-based heuristics:\n\n*   Skin in the game is a good sign - Jesty wanted to save his own family, for instance.\n*   Decoupling from external monetary incentives is useful - in other words, look for hobbyists. People at a classic car meetup or a track day will probably have better taste in car design than the J.D. Powers award.\n\nThat said, remember the main message: there is no full substitute for being an expert yourself. Heuristics about incentives can help, but they’re leaky filters at best.\n\nWhich brings us to the ultimate solution: try it yourself. Spend time in the field, practicing the relevant skills first-hand; see both what works and what makes sense. Collect data; run trials. See what other people suggest and test those things yourself. Directly study which things actually produce good results."
    },
    "voteCount": 141,
    "forceInclude": true
  },
  {
    "_id": "DoHcgTvyxdorAMquE",
    "url": null,
    "title": "Bets, Bonds, and Kindergarteners",
    "slug": "bets-bonds-and-kindergarteners",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "Parenting"
      },
      {
        "name": "Betting"
      },
      {
        "name": "Practical"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "Bets and bonds are tools for handling different epistemic states and levels of trust. Which makes them a great fit for negotiating with small children!\n\nA few weeks ago Anna (4y) wanted to play with some packing material. It looked very messy to me, I didn't expect she would clean it up, and I didn't want to fight with her about cleaning it up. I considered saying no, but after thinking about how things like this are handled in the real world I had an idea. If you want to do a hazardous activity, and we think you might go bankrupt and not clean up, we make you post a bond. This money is held in escrow to fund the cleanup if you disappear. I explained how this worked, and she went and got a dollar:\n\n[![](https://www.jefftk.com/green-paper-bond-coin.jpg)](https://www.jefftk.com/green-paper-bond-coin-big.jpg)\n\nThen:\n\n[![](https://www.jefftk.com/green-paper-bond1.jpg)](https://www.jefftk.com/green-paper-bond1-big.jpg)\n\n[![](https://www.jefftk.com/green-paper-bond3.jpg)](https://www.jefftk.com/green-paper-bond3-big.jpg)\n\nWhen she was done playing, she cleaned it up without complaint and got her dollar back. If she hadn't cleaned it up, I would have, and kept the dollar.\n\nSome situations are more complicated, and call for bets. I wanted to go to a park, but Lily (6y) didn't want to go to that park because the last time we had been there there'd been lots of bees. I remembered that had been a summer with unusually many bees, and it no longer being that summer or, in fact, summer at all, I was not worried. Since I was so confident, I offered my $1 to her $0.10 that we would not run into bees at the park. This seemed fair to her, and when there were no bees she was happy to pay up.\n\nOver time, they've learned that my being willing to bet, especially at large odds, is pretty informative, and often all I need to do is offer. Lily was having a rough morning, crying by herself about a project not working out. I suggested some things that might be fun to do together, and she rejected them angrily. I told her that often when people are feeling that way, going outside can help a lot, and when she didn't seem to believe me I offered to bet. Once she heard the 10:1 odds I was offering her I think she just started expecting that I was right, and she decided we should go ride bikes. (She didn't actually cheer up when we got outside: she cheered up as soon as she made this decision.)\n\nI do think there is some risk with this approach that the child will have a bad time just to get the money, or say they are having a bad time and they are actually not, but this isn't something we've run into. Another risk, if we were to wager large amounts, would be that the child would end up less happy than if I hadn't interacted with them at all. I handle this by making sure not to offer a bet I think they would regret losing, and while this is not a courtesy I expect people to make later in life, I think it's appropriate at their ages.\n\n*Comment via: [facebook](https://www.facebook.com/jefftk/posts/10100199782012082)*"
    },
    "voteCount": 224,
    "forceInclude": true
  },
  {
    "_id": "qmXqHKpgRfg83Nif9",
    "url": null,
    "title": "How to Ignore Your Emotions (while also thinking you're awesome at emotions)",
    "slug": "how-to-ignore-your-emotions-while-also-thinking-you-re",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "Emotions"
      },
      {
        "name": "Rationality"
      },
      {
        "name": "Growth Stories"
      }
    ],
    "tableOfContents": {
      "sections": [
        {
          "title": "Relevant context",
          "anchor": "Relevant_context",
          "level": 1
        },
        {
          "title": "Wiggling ears",
          "anchor": "Wiggling_ears",
          "level": 1
        },
        {
          "title": "Why \"ignore\" and \"deal with\" looked the same",
          "anchor": "Why__ignore__and__deal_with__looked_the_same",
          "level": 1
        },
        {
          "title": "Parent-child model",
          "anchor": "Parent_child_model",
          "level": 1
        },
        {
          "title": "... and now?",
          "anchor": "____and_now_",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        },
        {
          "anchor": "comments",
          "level": 0,
          "title": "71 comments"
        }
      ],
      "headingsCount": 7
    },
    "contents": {
      "markdown": "_(cross posted from my [personal blog](http://www.jhazard.com/posts/ignoring_emotions.html))_\n\nSince middle school I've generally thought that I'm pretty good at dealing with my emotions, and a handful of close friends and family have made similar comments. Now I can see that though I was particularly good at never flipping out, I was decidedly _not_ good \"healthy emotional processing\". I'll explain later what I think \"healthy emotional processing\" is, right now I'm using quotes to indicate \"the thing that's good to do with emotions\". Here it goes...\n\n**Relevant context**\n--------------------\n\nWhen I was a kid I adopted a strong, \"Fix it or stop complaining about it\" mentality. This applied to stress and worry as well. \"Either address the problem you're worried about or quit worrying about it!\" Also being a kid, I had a limited capacity to actually fix anything, and as such I was often exercising the \"stop worrying about it\" option.\n\nAnother thing about me, I was a massive book worm and loved to collect \"obvious mistakes\" that heroes and villains would make. My theory was, \"Know all the traps, and then just _don't_ fall for them\". That plus the sort of books I read meant that I \"knew\" it was a big no-no to ignore or repress your emotions. Luckily, since I knew you shouldn't repress your emotions, I \"just didn't\" and have lived happily ever after\n\n...\n\n...\n\nyeah nopes.\n\n**Wiggling ears**\n-----------------\n\nIt can be really hard to teach someone to move in a way that is completely new to them. I teach parkour, and sometimes I want to say,\n\nMe: \"Do the shock absorbing thing with your legs!\" Student: \"What's the shock absorbing thing?\" Me: \"... uh, you know... the thing were your legs... absorb shock?\"\n\nIt's hard to know how to give cues that will lead to someone making the right mental/muscle connection. Learning new motor movements is somewhat of a process of flailing around in the dark, until some feedback mechanism tells you you did it right (a coach, it's visually obvious, the jump doesn't hurt anymore, etc). Wiggling your ears is a nice concrete version of a) movement most people's bodies are capable of and b) one that most people feel like is impossible.\n\nClaim: learning mental and emotional skills has a similar \"flailing around in the dark\" aspect. There are the mental and emotional controls you've practiced, and those just feel like moving your arm. Natural, effortless, atomic. But there are other moves, which you are _totally capable of_ which seem impossible because you don't know how your \"control panel\" connects to that output. This feels like trying to wiggle your ears.\n\n**Why \"ignore\" and \"deal with\" looked the same**\n------------------------------------------------\n\nSo young me is upset that the grub master for our camping trip forgot half the food on the menu, and all we have for breakfast is milk. I couldn't \"fix it\" given that we were in the woods, so my next option was \"stop feeling upset about it.\" So I reached around in the dark of my mind, and Oops, the \"healthily process feelings\" lever is _right next to_ the \"stop listening to my emotions\" lever.\n\nThe end result? \"Wow, I decided to stop feeling upset, and then I stopped feeling upset. I'm so fucking good at emotional regulation!!!!!\"\n\nMy model now is that I [substituted](http://www.jhazard.com/posts/question_substitution.html) \"is there a monologue of upsetness in my conscious mental loop?\" for \"am I feeling upset?\". So from my perspective, it just felt like I was very in control of my feelings. Whenever I wanted to stop feeling something, I could. When I thought of ignoring/repressing emotions, I imagined trying to cover up something that was there, maybe with a story. Or I thought if you poked around ignored emotions there would be a response of anger or annoyance. I at least expected that if I was ignoring my emotions, that if I got very calm and then asked myself, \"Is there anything that you're feeling?\" I would get an answer.\n\nAgain, the assumption was, \"If it's in my mind, I should be able to notice if I look.\" This ignored what was actually happening, which was that I was cutting the phone lines so my emotions couldn't talk to me in the first place. Actually, the phone lines metaphor is a bit off, here's a better one.\n\n**Parent-child model**\n----------------------\n\nMy self-concept and conscious mind are the parent. Emotions are young children that run up to the parent to tell them something. Sometimes the child runs up to complain, \"Heeeeeeeeeey I'm huuuuuuungry!\" My emotional management was akin to the parenting style of slapping the child and saying, \"Being hungry would suck, so _you aren't hungry_.\"\n\nYikes.\n\nI know full well that you can't slap someone into having a full stomach, but you can slap someone into not bringing their complaints to you.\n\nI've experienced this _directly extend_ to my internal world. My emotions / [sub-agents](https://www.lesswrong.com/posts/M4w2rdYgCKctbADMn/sequence-introduction-non-agent-and-multiagent-models-of) aren't stupid. They learned that telling me, \"Hey, you're concerned about your relationship with your friend!\", \"Hey, we really don't like getting laughed at\", \"Hey, we're concerned that this bad thing is going to happen indefinitely\" would result in getting slapped. So they learned to stay quiet.\n\nThis got to the point where I'd feel awesome and great during my busy week, and then \"mysteriously\" and \"for no reason\" feel an amorphous blob of gray badness on the weekends. I had various social and emotional needs that weren't being met, but I didn't realize that. I quite intensely tried to introspect to see if this gray blob was \"about anything\", but only heard quiet static. This was me being the angry parent with their kids having a dinner of half a slice of bread each, shouting, \"Is anyone hungry?! Huh??! No? GREAT.\"\n\nOwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwww\n\n**... and now?**\n----------------\n\nWhen I was a kid, my desire to \"not worry if it was useless\" was mostly one of \"people who worry seem to be in pain, I'd prefer to not be in pain.\" Overtime, it turned into a judgmental world view. How _wasteful_ and _useless_ to be embarrassed/worried/scared/etc. This was the transition from a naive parent telling their kid, \"Hmmmm, have you tried not being hungry?\" to the angry parent shouting, \"You won't be hungry in _my_ house!!\" (one might wonder how exactly that transition from naive to judgmental happened. That's a whole other story for a different post)\n\nOver the past year I've haphazardly free styled towards opening up emotional communication with myself, and I've made progress. I'm still not sure what \"healthy emotional processing\" looks like, but I've gotten HUGE gains from just being able to sit with the fact that I'm feeling something, and hug the child that brought that emotion instead of slapping them.\n\nI guess the biggest thing I wanted to impart with this piece was 1. the parent child model, but also 2. that ignoring your emotions can start as a simple innocent mistake.\n\nRelated. A sentiment in a LW thread I heard in the past few months was that the biggest barrier to rational discourse is creating environments where everyone feels safe thinking (not the same thing as a safe space). Extend that to the mind. The biggest barrier to rational thinking is organizing your mind such that it's safe to think. I still promote and admire \"look towards the truth, even if it hurts\", but I know see that if you don't spend enough resources on addressing that hurt, the hurt parts of yourself can and _will_ take measures to protect themselves. Treat yourself well."
    },
    "voteCount": 151,
    "forceInclude": true
  },
  {
    "_id": "bx3gkHJehRCYZAF3r",
    "url": "https://radimentary.wordpress.com/2020/11/24/pain-is-not-the-unit-of-effort/",
    "title": "Pain is not the unit of Effort",
    "slug": "pain-is-not-the-unit-of-effort",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "Practical"
      },
      {
        "name": "Rationality"
      },
      {
        "name": "Well-being"
      },
      {
        "name": "Happiness"
      },
      {
        "name": "Heroic Responsibility"
      },
      {
        "name": "Fallacies"
      },
      {
        "name": "Suffering"
      },
      {
        "name": "Willpower"
      }
    ],
    "tableOfContents": {
      "sections": [
        {
          "title": "I. Anecdotes",
          "anchor": "I__Anecdotes",
          "level": 1
        },
        {
          "title": "II. Antidotes",
          "anchor": "II__Antidotes",
          "level": 1
        },
        {
          "title": "1. If it hurts, you're probably doing it wrong.",
          "anchor": "1__If_it_hurts__you_re_probably_doing_it_wrong_",
          "level": 2
        },
        {
          "title": "2. You're not trying your best if you're not happy.",
          "anchor": "2__You_re_not_trying_your_best_if_you_re_not_happy_",
          "level": 2
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        },
        {
          "anchor": "comments",
          "level": 0,
          "title": "79 comments"
        }
      ],
      "headingsCount": 6
    },
    "contents": {
      "markdown": "(Content warning: self-harm, parts of this post may be actively counterproductive for readers with certain mental illnesses or idiosyncrasies.)\n\n> *What doesn't kill you makes you stronger.* ~ Kelly Clarkson.\n> \n> *No pain, no gain.* ~ Exercise motto.\n> \n> *The more bitterness you swallow, the higher you'll go.* ~ Chinese proverb.\n\nI noticed recently that, at least in my social bubble, *pain is the unit of effort.* In other words, how hard you are trying is explicitly measured by how much suffering you put yourself through. In this post, I will share some anecdotes of how damaging and pervasive this belief is, and propose some counterbalancing ideas that might help rectify this problem.\n\nI. Anecdotes\n------------\n\n1\\. As a child, I spent most of my evenings studying mathematics under some amount of supervision from my mother. While studying, if I expressed discomfort or fatigue, my mother would bring me a snack or drink and tell me to stretch or take a break. I think she took it as a sign that I was trying my best. If on the other hand I was smiling or joyful for extended periods of time, she took that as a sign that I had effort to spare and increased the hours I was supposed to study each day. To this day there's a gremlin on my shoulder that whispers, \"If you're happy, you're not trying your best.\"\n\n2\\. A close friend who played sports in school reports that training can be *harrowing.* He told me that players who fell behind the pack during for daily jogs would be singled out and publicly humiliated. One time the coach screamed at my friend for falling behind the asthmatic boy who was alternating between running and using his inhaler. Another time, my friend internalized \"no pain, no gain\" to the point of losing his toenails.\n\n3\\. In high school and college, I was surrounded by overachievers constantly making (what seemed to me) incomprehensibly bad life choices. My classmates would sign up for eight classes per semester when the recommended number is five, jigsaw extracurricular activities into their calendar like a dynamic programming knapsack-solver, and then proceed to have loud public complaining contests about which libraries are most comfortable to study at past 2am and how many pages they have left to write for the essay due in three hours. Only later did I learn to ask: what incentives were they responding to?\n\n4\\. A while ago I became a connoisseur of Chinese webnovels. Among those written for a male audience, there is a surprisingly diverse set of character traits represented among the main characters. Doubtless many are womanizing murderhobos with no redeeming qualities, but others are classical heroes with big hearts, or sarcastic antiheroes who actually grow up a little, or ambitious empire-builders with grand plans to pave the universe with Confucian order, or down-on-their-luck starving artists who just want to bring happiness to the world through song.\n\nIf there is a single common virtue shared by all these protagonists, it is their *superhuman pain tolerance*. Protagonists routinely and often voluntarily dunk themselves in vats of lava, have all their bones broken, shattered, and reforged, get trapped inside alternate dimensions of freezing cold for millennia (which conveniently only takes a day in the outside world), and overdose on level-up pills right up to the brink of death, all in the name of becoming stronger. Oftentimes the defining difference between the protagonist and the antagonist is that the antagonist did not have enough pain tolerance and allowed the (unbearable physical) suffering in his life to drive him mad.\n\n5\\. I have a close friend who often asks for my perspective on personal problems. A pattern arose in a couple of our conversations:\n\n> alkjash: I feel like you're not actually trying. \\[Meaning: using all the tools at your disposal, getting creative, throwing money at the problem to make it go away.\\]\n> \n> alkjash's friend: What do you mean I'm not trying? I think I'm trying my best, can't you tell how hard I'm trying? \\[Meaning: piling on time, energy, and willpower to the point of burnout.\\]\n\nAfter several of these conversations went nowhere, I learned that asking this friend to try harder directly translated in his mind to accusing him of low pain tolerance and asking him to hurt himself more.\n\nII. Antidotes\n-------------\n\nI often hear on the internet laments like \"Why is nobody actually trying?\" Once upon a time, I was honestly and genuinely confused by this question. It seemed to me that \"actually trying\" - aiming the full force of your being at the solution of a problem you care about - is self-evidently motivating and requires zero extra justification if you care about the problem.\n\nI think I finally understand why so few people are \"actually trying.\" The reason is this pervasive and damaging belief that *pain is the unit of effort.* With this belief, the injunction \"actually try\" means \"put yourself in as much pain as you can handle.\" Similarly, \"she's trying her best\" translates to \"she's really hurting right now.\" Even worse, people with this belief *optimize for the appearance of suffering*. Answering emails at midnight and appearing fatigued at meetings are somehow taken to be more credible signals of effort than actual results. And if you think that's pathological, wait until you meet someone for whom *telling them about opportunities actively hurts them*, because you've just created another knife they feel pressured to cut themselves with.\n\nI see a mob of people walking up to houses and throwing themselves bodily at the closed front doors. I walk up to block one man and ask, \"Stop it! Why don't you try the doorknob first? Have you rung the doorbell?\" The man responds in tears, nursing his bloody right shoulder, \"I'm trying as hard as I can!\" With his one good arm, he shoves me aside and takes a running start to lunge at the door again. Finally, the timber shatters and the man breaks through. The surrounding mob cheers him on, \"Look how hard he's trying!\"\n\nOnce you understand that pain is how people define effort, the answer to the question \"why is nobody actually trying?\" becomes astoundingly obvious. I'd like to propose two beliefs to counterbalance this awful state of affairs.\n\n**1\\. If it hurts, you're probably doing it wrong.**\n\nIf your wrists ache on the bench press, you're probably using bad form and/or too much weight. If your feet ache from running, you might need sneakers with better arch support. If you're consistently sore for days after exercising, you should learn to stretch properly and check your nutrition.\n\nSuch rules are well-established in the setting of physical exercise, but their analogs in intellectual work seem to be completely lost on people. If reading a math paper is actively unpleasant, you should find a better-written paper or learn some background material first (most likely both). If you study or work late into the night and it disrupts your Circadian rhythm, you're trading off long-term productivity and well-being for low-quality work. That's just bad form.\n\nIf it hurts, you're probably doing it wrong.\n\n**2\\. You're not trying your best if you're not happy.**\n\nHappiness is really, really instrumentally useful. Being happy gives you more energy, increases your physical health and lifespan, makes you more creative and risk-tolerant, and (even if all the previous effects are unreplicated pseudoscience) causes other people to like you more. Whether you are tackling the Riemann hypothesis, climate change, or your personal weight loss, one of the first steps should be to acquire as much happiness as you can get your hands on. And the good news is: at least [anecdotally](https://www.lesswrong.com/posts/xnPFYBuaGhpq869mY/ureshiku-naritai), it is possible to substantially raise your happiness set-point through jedi mind tricks.\n\nBecoming happy is a fully general problem-solving strategy. And although one can in principle trade off happiness for short bursts of productivity, in practice this is never worth it.\n\nCulturally, we've been led to believe that over-stressed and tired people are the ones trying their best. It is right and proper to be kind to such people, but let's not go so far as to support the delusion that they are inputting as much effort as their joyful, boisterous peers bouncing off the walls.\n\nYou're not trying your best if you're not happy.\n\n\\[Edit: Antidotes #1 and #2 are not primarily to be interpreted as truth claims, see [Anna Salamon's comment](https://www.lesswrong.com/posts/bx3gkHJehRCYZAF3r/pain-is-not-the-unit-of-effort?commentId=kZXYGXGdBjYGH6miC).\\]"
    },
    "voteCount": 252,
    "forceInclude": true
  },
  {
    "_id": "Sdx6A6yLByRRs8iLY",
    "url": null,
    "title": "Fact Posts: How and Why",
    "slug": "fact-posts-how-and-why",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "Fact posts"
      },
      {
        "name": "Practical"
      },
      {
        "name": "World Modeling"
      },
      {
        "name": "Scholarship & Learning"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "The most useful thinking skill I've taught myself, which I think should be more widely practiced, is writing what I call \"fact posts.\"  I write a bunch of these on my [blog](https://srconstantin.wordpress.com/). (I write fact posts about pregnancy and childbirth [here.](https://parentingwithevidence.wordpress.com/))\n\nTo write a fact post, you start with an empirical question, or a general topic.  Something like \"How common are hate crimes?\" or \"Are epidurals really dangerous?\" or \"What causes manufacturing job loss?\"  \n\nIt's okay if this is a topic you know very little about. This is an exercise in original seeing and showing your reasoning, not finding the official last word on a topic or doing the best analysis in the world.\n\nThen you open up a Google doc and start taking notes.\n\nYou look for _quantitative data from conventionally reliable sources_.  CDC data for incidences of diseases and other health risks in the US; WHO data for global health issues; Bureau of Labor Statistics data for US employment; and so on. Published scientific journal articles, especially from reputable journals and large randomized studies.\n\nYou explicitly do _not_ look for opinion, even expert opinion. You avoid news, and you're wary of think-tank white papers. You're looking for raw information. You are taking a _sola scriptura_ approach, for better and for worse.\n\nAnd then you start letting the data show you things. \n\nYou see things that are surprising or odd, and you note that. \n\nYou see facts that seem to be inconsistent with each other, and you look into the data sources and methodology until you clear up the mystery.\n\nYou orient _towards_ the random, the unfamiliar, the things that are totally unfamiliar to your experience. One of the major exports of Germany is _valves_?  When was the last time I even thought about valves? _Why_ valves, what do you use valves in?  OK, show me a list of all the different kinds of machine parts, by percent of total exports.  \n\nAnd so, you dig in a little bit, to this part of the world that you hadn't looked at before. You cultivate the ability to spin up a lightweight sort of fannish obsessive curiosity when something seems like it might be a big deal.\n\nAnd you take casual notes and impressions (though keeping track of all the numbers and their sources in your notes).\n\nYou do a little bit of arithmetic to compare things to familiar reference points. How does this source of risk compare to the risk of smoking or going horseback riding? How does the effect size of this drug compare to the effect size of psychotherapy?\n\nYou don't really want to do _statistics_. You might take percents, means, standard deviations, maybe a Cohen's _d_ here and there, but nothing fancy.  You're just trying to figure out what's going on.\n\nIt's often a good idea to rank things by raw scale. What is responsible for the bulk of deaths, the bulk of money moved, etc? What is _big_?  Then pay attention more to things, and ask more questions about things, that are _big._ (Or disproportionately high-impact.)\n\nYou may find that this process gives you contrarian beliefs, but often you won't, you'll just have a strongly fact-based assessment of _why_ you believe the usual thing.  \n\nThere's a quality of _ordinariness_ about fact-based beliefs. It's not that they're never surprising -- they often are. But if you do fact-checking frequently enough, you begin to have a sense of the world overall that _stays in place_, even as you discover new facts, instead of swinging wildly around at every new stimulus.  For example, after doing lots and lots of reading of the biomedical literature, I have sort of a \"sense of the world\" of biomedical science -- what sorts of things I expect to see, and what sorts of things I don't. My \"sense of the world\" isn't that the _world itself_ is boring -- I actually believe in a world rich in discoveries and low-hanging fruit -- but the sense _itself_ has stabilized, feels like \"yeah, that's how things are\" rather than \"omg what is even going on.\"\n\nIn areas where I'm less familiar, I feel more like \"omg what is even going on\", which sometimes motivates me to go accumulate facts.\n\nOnce you've accumulated a bunch of facts, and they've \"spoken to you\" with some conclusions or answers to your question, you write them up on a blog, so that other people can check your reasoning.  If your mind gets changed, or you learn more, you write a follow-up post. You should, on any topic where you continue to learn over time, feel embarrassed by the naivety of your early posts.  This is fine. This is how learning works.\n\nThe advantage of fact posts is that they give you the ability to form independent opinions based on evidence. It's a sort of practice of the skill of seeing. They likely aren't the optimal way to get the most accurate beliefs -- listening to the best experts would almost certainly be better -- but you, personally, may not know who the best experts are, or may be overwhelmed by the swirl of controversy. Fact posts give you a relatively low-effort way of coming to informed opinions. They make you into the proverbial 'educated layman.'\n\nBeing an 'educated layman' makes you much more fertile in generating ideas, for research, business, fiction, or anything else. Having facts floating around in your head means you'll naturally think of problems to solve, questions to ask, opportunities to fix things in the world, applications for your technical skills.\n\nIdeally, a _group _of people writing fact posts on related topics, could learn from each other, and share how they think. I have the strong intuition that this is valuable. It's a bit more active than a \"journal club\", and quite a bit more casual than \"research\".  It's just the activity of learning and showing one's work in public."
    },
    "voteCount": 142,
    "forceInclude": true
  },
  {
    "_id": "FMkQtPvzsriQAow5q",
    "url": null,
    "title": "The correct response to uncertainty is *not* half-speed",
    "slug": "the-correct-response-to-uncertainty-is-not-half-speed",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "Planning & Decision-Making"
      },
      {
        "name": "Decision Theory"
      },
      {
        "name": "Goal Factoring"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "Related to: [Half-assing it with everything you've got](http://mindingourway.com/half-assing-it-with-everything-youve-got/); [Wasted motion](https://www.facebook.com/yudkowsky/posts/10151706498254228); [Say it Loud](/lw/u3/say_it_loud/).\n\nOnce upon a time (true story), I was on my way to a hotel in a new city.  I knew the hotel was many miles down this long, branchless road.  So I drove for a long while.\n\n![](http://images.lesswrong.com/t3_n6a_2.png?v=e38167d22348b22c2098faf0d51c7a61)\n\nAfter a while, I began to worry I had passed the hotel.\n\n![](http://images.lesswrong.com/t3_n6a_3.png?v=581d1be9b5846545acb660fa23f5322a)\n\nSo, instead of proceeding at 60 miles per hour the way I had been, I continued in the same direction for several more minutes at 30 miles per hour, wondering if I should keep going or turn around.\n\n![](http://images.lesswrong.com/t3_n6a_4.png?v=22e7e9bc8ebb443b87ed6b81ed3e6df7)\n\nAfter a while, I realized: I was being silly!  If the hotel was ahead of me, I'd get there fastest if I kept going 60mph.  And if the hotel was behind me, I'd get there fastest by heading at 60 miles per hour in the other direction.  And if I _wasn't_ going to turn around yet -- if my best bet given the uncertainty was to check _N_ more miles of highway first, _before_ I turned around -- then, again, I'd get there fastest by choosing a value of _N_, speeding along at 60 miles per hour until my odometer said I'd gone _N_ miles, and _then_ turning around and heading at 60 miles per hour in the opposite direction.  \n\n  \n\nEither way, fullspeed was best.  My mind had been naively averaging two courses of action -- the thought was something like: \"maybe I should go forward, and maybe I should go backward.  So, since I'm uncertain, I should go forward at half-speed!\"  But averages don't _actually_ work that way.\\[1\\]\n\n  \n\nFollowing this, I started noticing lots of hotels in my life (and, perhaps less tactfully, in my friends' lives).  For example:\n\n*   I wasn't sure if I was a good enough writer to write a given doc myself, or if I should try to outsource it.  So, I [sat there kind-of-writing it](/lw/jad/attempted_telekinesis/) while also fretting about whether the task was correct.\n    *   (Solution:  Take a minute out to think through heuristics.  Then, either: (1) write the post at full speed; or (2) try to outsource it; or (3) write full force _for some fixed time period_, and then pause and evaluate.)\n*   I wasn't sure (back in early 2012) that CFAR was worthwhile.  So, I kind-of worked on it.\n*   An old friend came to my door unexpectedly, and I was tempted to hang out with her, but I also thought I should finish my work.  So I kind-of hung out with her while feeling bad and distracted about my work.\n*   A friend of mine, when teaching me math, seems to mumble _specifically those words that he doesn't expect me to understand_ (in a sort of compromise between saying them and not saying them)...\n*   Duncan [reports](http://thirdfoundation.github.io/#/blog/quittability) that novice Parkour students are unable to safely undertake certain sorts of jumps, because they risk aborting the move mid-stream, after the _actual_ last safe stopping point (apparently kind-of-attempting these jumps is more dangerous than either attempting, or not attempting the jumps)\n*   It is said that start-up founders need to be irrationally certain that their startup will succeed, lest they be unable to do more than kind-of work on it...\n\n  \n\nThat is, it seems to me that often there are two different actions that would make sense under two different models, and we are uncertain which model is true... and so we find ourselves taking an intermediate of half-speed action... even when that action makes no sense under any probabilistic mixture of the two models.\n\n  \n\n![](http://images.lesswrong.com/t3_n6a_5.png?v=a8df41062ca5967c7f5e1cfb195fc243)  \n\n  \n\nYou might try looking out for such examples in your life.\n\n  \n\n* * *\n\n**\\[1\\] Edited to add:** The hotel example has received much nitpicking in the comments.  But: (A) the actual example was legit, I think.  Yes, stopping to think has some legitimacy, but driving slowly for a long time because uncertain does not optimize for thinking.  Similarly, it may make sense to drive slowly to stare at the buildings in some contexts... but I was on a very long empty country road, with no buildings anywhere (true historical fact), and also I was not squinting carefully at the scenery.  The thing I needed to do was to execute an efficient search pattern, with a threshold for a future time at which to switch from full-speed in some direction to full-speed in the other.  Also: (B) consider some of the other examples; \"kind of working\", \"kind of hanging out with my friend\", etc. seem to be common behaviors that are mostly not all that useful in the usual case."
    },
    "voteCount": 132,
    "forceInclude": true
  },
  {
    "_id": "w5F4w8tNZc6LcBKRP",
    "url": null,
    "title": "On learning difficult things",
    "slug": "on-learning-difficult-things",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "Scholarship & Learning"
      },
      {
        "name": "Practical"
      }
    ],
    "tableOfContents": {
      "sections": [
        {
          "title": "Pair up",
          "anchor": "Pair_up",
          "level": 1
        },
        {
          "title": "Read, reread, rereread",
          "anchor": "Read__reread__rereread",
          "level": 1
        },
        {
          "title": "Cognitive exchange rates",
          "anchor": "Cognitive_exchange_rates",
          "level": 1
        },
        {
          "title": "Explain it to someone",
          "anchor": "Explain_it_to_someone",
          "level": 1
        },
        {
          "title": "Don't book yourself solid",
          "anchor": "Don_t_book_yourself_solid",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        },
        {
          "anchor": "comments",
          "level": 0,
          "title": "45 comments"
        }
      ],
      "headingsCount": 7
    },
    "contents": {
      "markdown": "I have been autodidacting quite a bit lately. You may have seen my [reviews](/lw/ixn/very_basic_model_theory/) of books on the [MIRI course list](http://intelligence.org/courses). I've been going for about ten weeks now. This post contains my notes about the experience thus far.\n\nMuch of this may seem obvious, and would have seemed obvious if somebody had told me in advance. But nobody told me in advance. As such, this is a collection of things that were somewhat surprising at the time.\n\nPart of the reason I'm posting this is because I don't know a lot of autodidacts, and I'm not sure how normal any of my experiences are. (Though on average, I'd guess they're about average.) As always, keep in mind that I am only one person and that your mileage may vary.\n\nPair up\n-------\n\nWhen I began my quest for more knowledge, I figured that in this modern era, a well-written textbook and an account on [math.stackexchange](/math.stackexchange.com) would be enough to get me through anything. And I was right… sort of.\n\nBut not really.\n\nThe problem is, most of the time that I get stuck, I get stuck on something incredibly stupid. I've either misread something somewhere or misremembered a concept from earlier in the book. Usually, someone looking over my shoulder could correct me in ten seconds with three words.\n\n\"Dude. Disjunction. _Dis_junction_._\"\n\nThese are the things that eat my days.\n\nIn principle, places like stackexchange can get me unstuck, but they're an awkward tool for the job. First of all, my stupid mistakes are heavily contextualized. A full context dump is necessary before I can even ask my question, and this takes time. Furthermore, I feel dumb asking stupid questions on stackexchange-type sites. My questions are usually things that I can figure out with a close re-read (except, I'm not sure which part needs a re-read). I usually opt for a close re-read of everything rather than asking for help. This is even more time consuming.\n\nThe infuriating thing is that answering these questions usually doesn't require someone who already knows the answers: it just requires someone who didn't make exactly the same mistakes as me. I lose hours on little mistakes that could have been fixed within seconds if I was doing this with someone else.\n\nThat's why my number one piece of advice for other people attempting to learn on their own is _do it with a friend_. They don't need to be more knowledgeable than you to answer most of the questions that come up. They just need to make _different_ misunderstandings, and you'll be able to correct each other as you go along.\n\nThe thing I miss most about college is tight feedback loops while learning. When autodidacting, the feedback loop can be long.\n\nI still haven't managed to follow my own advice here. I'm writing this advice in part because it should motivate me to actually pair up. Unfortunately, there is nobody in my immediate circle who has the time or patience to read along with me, but there are a number of resources I have not yet explored (the LessWrong study hall, for example, or soliciting to actual mathematicians). It's on my list of things to do.\n\nRead, reread, rereread\n----------------------\n\nReading _Model Theory_ was one of the hardest things I've done. Not necessarily because the content was hard, but because it was the first time I actually learned something that was way outside my comfort zone.\n\nThe short version is that _Basic Category Theory_ and _Naïve Set Theory_ left me somewhat overconfident, and that I should have read a formal logic textbook before diving in. I had basic familiarity with logic, but no practice. Turns out practice is important.\n\nAnyway, it's not like _Model Theory_ was impossible just because I skipped my logic exercises. It was just _hard_. There are a number of little misconceptions you have when you're familiar with something but you've never applied it, and I found myself having to clean those out just to understand what _Model Theory_ was trying to say to me.\n\nIn retrospect, this was an efficient way to strengthen my understanding of mathematical logic and learn _Model Theory_ at the same time. (I've moved on to a logic textbook, and it's been a cakewalk.) That said, I wouldn't wish the experience on others.\n\nIn the process, I learned how to learn things that are way outside my comfort zone. In the past, all the stuff I've learned has been either easy, or an extension of things that I was already interested in and experienced with. Reading _Model Theory_ was the first time in my life where I read a chapter of a textbook and it made _absolutely no sense_. In fact, it took about three passes per chapter before they made sense.\n\n1.  The first pass was barely sufficient to understand all the words and symbols. I constantly had to go research a topic. I followed proofs one step at a time, able to verify the validity of each step but not really understand what was going on. I came out the other end believing the results, but not knowing them.\n2.  Another pass was required to figure out what the book was actually trying to say to me. Once all the words made sense and I was comfortable with their usage, the second pass allowed me to see what the theorems and proofs were actually saying. This was nice, but it still wasn't sufficient: I understood the theorems, but they seemed like a random walk through theorem-space. I couldn't yet understand why anyone would say those particular things on purpose.\n3.  The third pass was necessary to understand the greater theory. I've never been particularly good at memorizing things, and it's not sufficient for me to believe and memorize a theorem. If it's going to stick, I have to understand why it's important. I have to understand why this theorem in particular is being stated, rather than another. I have to understand the problem that's being solved. A third pass was necessary to figure out the context in which the text made sense.\n\nAfter a third pass of any given chapter, the next chapter didn't seem quite so random. When the upcoming content started feeling like a natural progression instead of a random walk, I knew I was making progress.\n\nI note this because this is the first time that I had to read a math text more than once to understand what was going on. I'm not talking about individual sentences or paragraphs, I'm talking about finishing a chapter, feeling like \"wat\", and then starting the whole chapter over. Twice.\n\nI'm not sure if I'm being naïve (for never having needed to do this before) or slow (for having to do this for _Model Theory_), but I did not anticipate requiring three passes. Mostly, I didn't anticipate gaining as much as I did from a re-read; I would have guessed that something opaque on the first pass would remain opaque on a second pass.\n\nThis, I'm pretty sure, was naïvety.\n\nSo take note: if you stumble upon something that feels very hard, it might be more useful than anticipated to re-read it.\n\nCognitive exchange rates\n------------------------\n\nWhen reading Model Theory, I was only able to convert 30-50% of my allotted \"study time\" into actual study.\n\nThis is somewhat surprising, as I had no such troubles with _Basic Category Theory_ or _Naïve Set Theory_.\n\n(I often have the _opposite_ problem when writing code; this is probably due to the different reward structure.)\n\nI was somewhat frustrated with my inability to study as much as I would have liked. My usual time-into-studying conversion rate is much higher (I'd guess 80%ish, though I haven't been measuring).\n\nI'm not sure what factor made it harder for me to study model theory. I don't think it was the difficulty directly, as I often tend to work harder in the face of a challenge. I'd guess that it was either the slower rate of rewards (caused by a slower pace of learning) or actual cognitive exhaustion.\n\nIn the vein of cognitive exhaustion, there were a few times while reading _Model Theory_ where I seem to have become cognitively exhausted before becoming physically exhausted. This was a first for me. I'm not referring to those times when you've done a lot of mental work and you shy away from doing anything difficult, that's happened to me plenty. Rather, in this case, I felt fully awake and ready to keep reading. And I did keep reading. It just… didn't work. I'd have trouble following simple proofs. I'd fail at parsing sentences that were quite clear after resting.\n\nI'm still not sure what to make of this, and I don't have sufficient data to draw conclusions. However, it seems like there are mental states where my I feel awake and able to continue, but my mind is just not capable of doing the heavy lifting.\n\nAgain, the fact that I'm only just realizing this now is probably naïvety, but it's something to remember before getting frustrated with yourself.\n\nExplain it to someone\n---------------------\n\nAs I've said before, one of the best ways to learn something is to do the problem sets. For _Model Theory_, though, there were times when I finished reading through a chapter and was not capable of doing the problems.\n\nRe-reading helped, as mentioned above. Another thing that helped was explaining the concepts.\n\nI explained model theory pretty extensively to a text file on my computer. I sketched the proofs in my own words and stated their significance. I explained the syntax being used. I tried to motivate each idea. (The notes are still lying around somewhere; I haven't posted them because they're pretty much a derivative work at this point.)\n\nI found that this went a _long_ way towards helping me track down places where I'd thought I learned something, but actually hadn't. If you're having trouble, go explain the concept to somebody (or to a text file). This can bridge the gap between \"I read it\" and \"I can do the problems\" quite well. For me, this technique often took problems from \"unapproachable\" to \"easy\" in one fell swoop.\n\nDon't book yourself solid\n-------------------------\n\nI'm pretty good at avoiding stress. I have the (apparently rare) ability to drop all work-related concerns at the door when I leave. I don't even know _how_ to get stressed by bad luck, especially if I made good choices given the information I had at the time. I get normally tense in stressful situations with time constraints, but I'm adept at avoiding the permastress that I've seen plague friends and family — unless I've booked myself solid.\n\nI've had a packed schedule these past few weeks. I try to move the needle on at least two projects a day (more on weekends). Even if it's entirely reasonable to fit all these things into my schedule, I have not yet found a way to avoid the stress.\n\nEven when I know that, if I push myself, I can read this much and write that much and code this feature all in one day, I haven't found a good way to push myself without pressure-stress.\n\nI'm still hoping that I'll learn how to move quickly without stress as I learn my capabilities, but I'm not sure I've been adequately accounting for the [cost of stress](http://scholar.google.com/scholar?q=adverse+effects+of+stress&hl=en&as_sdt=0&as_vis=1&oi=scholart&sa=X&ei=KE6AUuGTEqerigLmyoHADw&ved=0CCwQgQMwAA).\n\nIt's worth remembering that doing less than you're capable of _on purpose _might be a good strategy for maximizing long-term output.\n\n* * *\n\nThere you go. Those are my notes gathered from trying to learn lots of things very quickly (and trying to learn one hard thing in particular). Comments are encouraged; I am by no means an expert."
    },
    "voteCount": 121,
    "forceInclude": true
  },
  {
    "_id": "895quRDaK6gR2rM82",
    "url": null,
    "title": "Diseased thinking: dissolving questions about disease",
    "slug": "diseased-thinking-dissolving-questions-about-disease",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "Rationality"
      },
      {
        "name": "Health / Medicine / Disease"
      },
      {
        "name": "Philosophy of Language"
      },
      {
        "name": "Carving / Clustering Reality"
      },
      {
        "name": "Reversal Test"
      },
      {
        "name": "Motivational Intro Posts"
      }
    ],
    "tableOfContents": {
      "sections": [
        {
          "title": "What is Disease?",
          "anchor": "What_is_Disease_",
          "level": 1
        },
        {
          "title": "Hidden Inferences From Disease Concept",
          "anchor": "Hidden_Inferences_From_Disease_Concept",
          "level": 1
        },
        {
          "title": "Sympathy or Condemnation?",
          "anchor": "Sympathy_or_Condemnation_",
          "level": 1
        },
        {
          "title": "The Ethics of Treating Marginal Conditions",
          "anchor": "The_Ethics_of_Treating_Marginal_Conditions",
          "level": 1
        },
        {
          "title": "Summary",
          "anchor": "Summary",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        },
        {
          "anchor": "comments",
          "level": 0,
          "title": "354 comments"
        }
      ],
      "headingsCount": 7
    },
    "contents": {
      "markdown": "**Related to:** [Disguised Queries](https://www.lesswrong.com/lw/nm/disguised_queries/), [Words as Hidden Inferences](https://www.lesswrong.com/lw/ng/words_as_hidden_inferences/), [Dissolving the Question](https://www.lesswrong.com/lw/of/dissolving_the_question/), [Eight Short Studies on Excuses](https://www.lesswrong.com/lw/24o/eight_short_studies_on_excuses/)\n\n> _Today's therapeutic ethos, which celebrates curing and disparages judging, expresses the liberal disposition to assume that crime and other problematic behaviors reflect social or biological causation. While this absolves the individual of responsibility, it also strips the individual of personhood, and moral dignity_\n\n_\\-\\- George Will, [townhall.com](http://townhall.com/Common/PrintPage.aspx?g=761ecc84-473b-4123-bf28-c4fc179a9d3f&t=c)_\n\nSandy is a morbidly obese woman looking for advice.\n\nHer husband has no sympathy for her, and tells her she obviously needs to stop eating like a pig, and would it kill her to go to the gym once in a while?\n\nHer doctor tells her that obesity is primarily genetic, and recommends the diet pill orlistat and a consultation with a surgeon about gastric bypass.\n\nHer sister tells her that obesity is a perfectly valid lifestyle choice, and that fat-ism, equivalent to racism, is society's way of keeping her down.\n\nWhen she tells each of her friends about the opinions of the others, things really start to heat up.\n\nHer husband accuses her doctor and sister of absolving her of personal responsibility with feel-good platitudes that in the end will only prevent her from getting the willpower she needs to start a real diet.\n\nHer doctor accuses her husband of ignorance of the real causes of obesity and of the most effective treatments, and accuses her sister of legitimizing a dangerous health risk that could end with Sandy in hospital or even dead.\n\nHer sister accuses her husband of being a jerk, and her doctor of trying to medicalize her behavior in order to turn it into a \"condition\" that will keep her on pills for life and make lots of money for Big Pharma.\n\nSandy is fictional, but similar conversations happen every day, not only about obesity but about a host of other marginal conditions that some consider character flaws, others diseases, and still others normal variation in the human condition. Attention deficit disorder, internet addiction, social anxiety disorder (as one skeptic said, didn't we used to call this \"shyness\"?), alcoholism, chronic fatigue, oppositional defiant disorder (\"didn't we used to call this being a teenager?\"), compulsive gambling, homosexuality, Aspergers' syndrome, antisocial personality, even depression have all been placed in two or more of these categories by different people.\n\n  \n\nSandy's sister may have a point, but this post will concentrate on the debate between her husband and her doctor, with the understanding that the same techniques will apply to evaluating her sister's opinion. The disagreement between Sandy's husband and doctor centers around the idea of \"disease\". If obesity, depression, alcoholism, and the like are diseases, most people default to the doctor's point of view; if they are not diseases, they tend to agree with the husband.\n\nThe debate over such marginal conditions is in many ways a debate over whether or not they are \"real\" diseases. The usual surface level arguments trotted out in favor of or against the proposition are generally inconclusive, but this post will apply a host of techniques previously discussed on Less Wrong to illuminate the issue.\n\n  \n\n**What is Disease?**\n\n  \n\nIn [Disguised Queries](https://www.lesswrong.com/lw/nm/disguised_queries/) , Eliezer demonstrates how a word refers to a cluster of objects related upon multiple axes. For example, in a company that sorts red smooth translucent cubes full of vanadium from blue furry opaque eggs full of palladium, you might invent the word \"rube\" to designate the red cubes, and another \"blegg\", to designate the blue eggs. Both words are useful because they \"carve reality at the joints\" - they refer to two completely separate classes of things which it's practically useful to keep in separate categories. Calling something a \"blegg\" is a quick and easy way to describe its color, shape, opacity, texture, and chemical composition. It may be that the odd blegg might be purple rather than blue, but in general the characteristics of a blegg remain sufficiently correlated that \"blegg\" is a useful word. If they weren't so correlated - if blue objects were equally likely to be palladium-containing-cubes as vanadium-containing-eggs, then the word \"blegg\" would be a waste of breath; the characteristics of the object would remain just as mysterious to your partner after you said \"blegg\" as they were before.\n\n\"Disease\", like \"blegg\", suggests that certain characteristics always come together. A rough sketch of some of the characteristics we expect in a disease might include:\n\n1\\. Something caused by the sorts of thing you study in biology: proteins, bacteria, ions, viruses, genes.\n\n2\\. Something involuntary and completely immune to the operations of free will\n\n3\\. Something rare; the vast majority of people don't have it\n\n4\\. Something unpleasant; when you have it, you want to get rid of it\n\n5\\. Something discrete; a graph would show two widely separate populations, one with the disease and one without, and not a normal distribution.\n\n6\\. Something commonly treated with science-y interventions like chemicals and radiation.\n\nCancer satisfies every one of these criteria, and so we have no qualms whatsoever about classifying it as a disease. It's a type specimen, [the sparrow as opposed to the ostrich](https://www.lesswrong.com/lw/nl/the_cluster_structure_of_thingspace/). The same is true of heart attack, the flu, diabetes, and many more.\n\nSome conditions satisfy a few of the criteria, but not others. Dwarfism seems to fail (5), and it might get its status as a disease only after studies show that the supposed dwarf falls way out of normal human height variation. Despite the best efforts of transhumanists, it's hard to convince people that aging is a disease, partly because it fails (3). Calling homosexuality a disease is a poor choice for many reasons, but one of them is certainly (4): it's not necessarily unpleasant.\n\nThe marginal conditions mentioned above are also in this category. Obesity arguably sort-of-satisfies criteria (1), (4), and (6), but it would be pretty hard to make a case for (2), (3), and (5).\n\nSo, is obesity really a disease? Well, is Pluto really a planet? Once we state that obesity satisfies some of the criteria but not others, it is meaningless to talk about an additional fact of whether it \"really deserves to be a disease\" or not.\n\nIf it weren't for those pesky [hidden inferences](https://www.lesswrong.com/lw/ng/words_as_hidden_inferences/#more)...\n\n**Hidden Inferences From Disease Concept**\n\nThe state of the disease node, meaningless in itself, is used to predict several other nodes with non-empirical content. In English: we make value decisions based on whether we call something a \"disease\" or not.\n\nIf something is a real disease, the patient deserves our sympathy and support; for example, [cancer sufferers must universally be described as \"brave\"](http://www.theonion.com/articles/loved-ones-recall-local-mans-cowardly-battle-with,772/). If it is not a real disease, people are more likely to get our condemnation; for example Sandy's husband who calls her a \"pig\" for her inability to control her eating habits. The difference between \"shyness\" and \"social anxiety disorder\" is that people with the first get called \"weird\" and told to man up, and people with the second get special privileges and the sympathy of those around them.\n\nAnd if something is a real disease, it is socially acceptable (maybe even mandated) to seek medical treatment for it. If it's not a disease, medical treatment gets derided as a \"quick fix\" or an \"abdication of personal responsibility\". I have talked to several doctors who are uncomfortable suggesting gastric bypass surgery, even in people for whom it is medically indicated, because they believe it is morally wrong to turn to medicine to solve a character issue.\n\n![](http://res.cloudinary.com/lesswrong-2-0/image/upload/v1531709899/disease_first_tlnfio.gif)\n\nWhile a condition's status as a \"real disease\" ought to be meaningless as a \"hanging node\" after the status of all other nodes have been determined, it has acquired political and philosophical implications because of its role in determining whether patients receive sympathy and whether they are permitted to seek medical treatment.\n\nIf we can determine whether a person should get sympathy, and whether they should be allowed to seek medical treatment, independently of the central node \"disease\" or of the criteria that feed into it, we will have successfully unasked the question \"are these marginal conditions real diseases\" and cleared up the confusion.\n\n**Sympathy or Condemnation?**\n\nOur attitudes toward people with marginal conditions mainly reflect a deontologist libertarian (libertarian as in \"free will\", not as in \"against government\") model of blame. In this concept, people make decisions using their free will, a spiritual entity operating free from biology or circumstance. People who make good decisions are intrinsically good people and deserve good treatment; people who make bad decisions are intrinsically bad people and deserve bad treatment. But people who make bad decisions for reasons that are outside of their free will may not be intrinsically bad people, and may therefore be absolved from deserving bad treatment. For example, if a normally peaceful person has a brain tumor that affects areas involved in fear and aggression, they go on a crazy killing spree, and then they have their brain tumor removed and become a peaceful person again, many people would be willing to accept that the killing spree does not reflect negatively on them or open them up to deserving bad treatment, since it had biological and not spiritual causes.\n\nUnder this model, deciding whether a condition is biological or spiritual becomes very important, and the rationale for worrying over whether something \"is a real disease\" or not is plain to see. Without figuring out this extremely difficult question, we are at risk of either blaming people for things they don't deserve, or else letting them off the hook when they commit a sin, both of which, to libertarian deontologists, would be terrible things. But determining whether marginal conditions like depression have a spiritual or biological cause is difficult, and no one knows how to do it reliably.\n\nDeterminist consequentialists can do better. We believe it's biology all the way down. Separating spiritual from biological illnesses is impossible and unnecessary. Every condition, from brain tumors to poor taste in music, is \"biological\" insofar as it is encoded in things like cells and proteins and follows laws based on their structure.\n\nBut determinists don't just ignore the very important differences between brain tumors and poor taste in music. Some biological phenomena, like poor taste in music, are encoded in such a way that they are extremely vulnerable to what we can call social influences: praise, condemnation, introspection, and the like. Other biological phenomena, like brain tumors, are completely immune to such influences. This allows us to develop a more useful model of blame.\n\nThe consequentialist model of blame is very different from the deontological model. Because all actions are biologically determined, none are more or less metaphysically blameworthy than others, and none can mark anyone with the metaphysical status of \"bad person\" and make them \"deserve\" bad treatment. Consequentialists don't on a primary level want anyone to be treated badly, full stop; thus [is it written](http://yudkowsky.net/obsolete/tmol-faq.html#theo_free): \"Saddam Hussein doesn't deserve so much as a stubbed toe.\" But if consequentialists don't believe in punishment for its own sake, they do believe in punishment for the sake of, well, consequences. Hurting bank robbers may not be a good in and of itself, but it will prevent banks from being robbed in the future. And, one might infer, although alcoholics may not deserve condemnation, societal condemnation of alcoholics makes alcoholism a less attractive option.\n\nSo here, at last, is a rule for which diseases we offer sympathy, and which we offer condemnation: if giving condemnation instead of sympathy decreases the incidence of the disease enough to be worth the hurt feelings, condemn; otherwise, sympathize. Though the rule is based on philosophy that the majority of the human race would disavow, it leads to intuitively correct consequences. Yelling at a cancer patient, shouting \"How dare you allow your cells to divide in an uncontrolled manner like this; is that the way your mother raised you??!\" will probably make the patient feel pretty awful, but it's not going to cure the cancer. Telling a lazy person \"Get up and do some work, you worthless bum,\" very well might cure the laziness. The cancer is a biological condition immune to social influences; the laziness is a biological condition susceptible to social influences, so we try to socially influence the laziness and not the cancer.\n\nThe question \"Do the obese deserve our sympathy or our condemnation,\" then, is asking whether condemnation is such a useful treatment for obesity that its utility outweights the disutility of hurting obese people's feelings. This question may have different answers depending on the particular obese person involved, the particular person doing the condemning, and the availability of other methods for treating the obesity, which brings us to...\n\n**The Ethics of Treating Marginal Conditions**\n\nIf a condition is susceptible to social intervention, but an effective biological therapy for it also exists, is it okay for people to use the biological therapy instead of figuring out a social solution? My gut answer is \"Of course, why wouldn't it be?\", but apparently lots of people find this controversial for some reason.\n\nIn a libertarian deontological system, throwing biological solutions at spiritual problems might be disrespectful or dehumanizing, or a band-aid that doesn't affect the deeper problem. To someone who believes it's biology all the way down, this is much less of a concern.\n\nOthers complain that the existence of an easy medical solution prevents people from learning personal responsibility. But here [we see the status-quo bias at work, and so can apply a preference reversal test](http://www.nickbostrom.com/ethics/statusquo.pdf). If people really believe learning personal responsibility is more important than being not addicted to heroin, we would expect these people to support deliberately addicting schoolchildren to heroin so they can develop personal responsibility by coming off of it. Anyone who disagrees with this somewhat shocking proposal must believe, on some level, that having people who are not addicted to heroin is more important than having people develop whatever measure of personal responsibility comes from kicking their heroin habit the old-fashioned way.\n\nBut the most convincing explanation I have read for why so many people are opposed to medical solutions for social conditions is a signaling explanation by Robin Hans...wait! no!...by Katja Grace. On [her blog](http://meteuphoric.wordpress.com/2009/09/21/why-do-animal-lovers-want-animals-to-feel-pain/), she says:\n\n> _...the situation reminds me of a pattern in similar cases I have noticed before. It goes like this. Some people make personal sacrifices, supposedly toward solving problems that don’t threaten them personally. They sort recycling, buy free range eggs, buy fair trade, campaign for wealth redistribution etc. Their actions are seen as virtuous. They see those who don’t join them as uncaring and immoral. A more efficient solution to the problem is suggested. It does not require personal sacrifice. People who have not previously sacrificed support it. Those who have previously sacrificed object on grounds that it is an excuse for people to get out of making the sacrifice. The supposed instrumental action, as the visible sign of caring, has become virtuous in its own right. Solving the problem effectively is an attack on the moral people._\n\nA case in which some people eat less enjoyable foods and exercise hard to avoid becoming obese, and then campaign against a pill that makes avoiding obesity easy demonstrates some of the same principles.\n\nThere are several very reasonable objections to treating any condition with drugs, whether it be a classical disease like cancer or a marginal condition like alcoholism. The drugs can have side effects. They can be expensive. They can build dependence. They may later be found to be placebos whose efficacy was overhyped by dishonest pharmaceutical advertising.. They may raise ethical issues with children, the mentally incapacitated, and other people who cannot decide for themselves whether or not to take them. But these issues do not magically become more dangerous in conditions typically regarded as \"character flaws\" rather than \"diseases\", and the same good-enough solutions that work for cancer or heart disease will work for alcoholism and other such conditions (but see [here](https://www.lesswrong.com/lw/2as/diseased_thinking_dissolving_questions_about/230p)).\n\nI see no reason why people who want effective treatment for a condition should be denied it or stigmatized for seeking it, whether it is traditionally considered \"medical\" or not.\n\n**Summary**\n\nPeople commonly debate whether social and mental conditions are real diseases. This masquerades as a medical question, but its implications are mainly social and ethical. We use the concept of disease to decide who gets sympathy, who gets blame, and who gets treatment.\n\nInstead of continuing the fruitless \"disease\" argument, we should address these questions directly. Taking a determinist consequentialist position allows us to do so more effectively. We should blame and stigmatize people for conditions where blame and stigma are the most useful methods for curing or preventing the condition, and we should allow patients to seek treatment whenever it is available and effective."
    },
    "voteCount": 367,
    "forceInclude": true
  },
  {
    "_id": "SGR4GxFK7KmW7ckCB",
    "url": null,
    "title": "Something to Protect",
    "slug": "something-to-protect",
    "author": "Eliezer_Yudkowsky",
    "question": false,
    "tags": [
      {
        "name": "World Optimization"
      },
      {
        "name": "Rationality"
      },
      {
        "name": "Motivations"
      },
      {
        "name": "Heroic Responsibility"
      },
      {
        "name": "Something To Protect"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "In the gestalt of (ahem) [Japanese](/lw/m7/zen_and_the_art_of_rationality/) [fiction](/lw/k9/the_logical_fallacy_of_generalization_from/), one finds this oft-repeated motif:  Power comes from having something to protect.\n\nI'm not just talking about superheroes that power up when a friend is threatened, the way it works in Western fiction.  In the Japanese version it runs deeper than that.\n\nIn the _X_ saga it's explicitly stated that each of the good guys draw their power from having someone—one person—who they want to protect.  Who?  That question is part of _X_'s plot—the \"most precious person\" isn't always who we think.  But if that person is killed, or hurt in the wrong way, the protector loses their power—not so much from magical backlash, as from simple despair.  This isn't something that happens once per week per good guy, the way it would work in a Western comic.  It's equivalent to being [Killed Off For Real](http://tvtropes.org/pmwiki/pmwiki.php/Main/KilledOffForReal)—taken off the game board.\n\nThe way it works in Western superhero comics is that the good guy gets bitten by a radioactive spider; and then he needs something to do with his powers, to keep him busy, so he decides to fight crime.  And then Western superheroes are always whining about how much time their superhero duties take up, and how they'd rather be ordinary mortals so they could go fishing or something.\n\nSimilarly, in Western real life, unhappy people are told that they need a \"purpose in life\", so they should pick out an altruistic cause that goes well with their personality, like picking out nice living-room drapes, and this will brighten up their days by adding some color, like nice living-room drapes.  You should be careful not to pick something too expensive, though.\n\nIn Western comics, the magic comes first, then the purpose:  Acquire amazing powers, decide to protect the innocent.  In Japanese fiction, often, it works the other way around.\n\nOf course I'm not saying all this to generalize from fictional evidence. But I want to convey a concept whose deceptively close Western analogue is _not_ what I mean.\n\nI have touched before on the idea that a rationalist must have something they value more than \"rationality\":  _The Art must have a purpose other than itself, or it collapses into infinite recursion._  But do not mistake me, and think I am advocating that rationalists should pick out a nice altruistic cause, by way of having something to do, because rationality isn't all that important by itself.  No.  I am asking:  Where do rationalists come from?  How do we acquire our powers?\n\nIt is written in the _Twelve Virtues of Rationality:_\n\n> How can you improve your conception of rationality?  Not by saying to yourself, \"It is my duty to be rational.\"  By this you only enshrine your mistaken conception.  Perhaps your conception of rationality is that it is rational to believe the words of the Great Teacher, and the Great Teacher says, \"The sky is green,\" and you look up at the sky and see blue.  If you think:  \"It may look like the sky is blue, but rationality is to believe the words of the Great Teacher,\" you lose a chance to discover your mistake.\n\nHistorically speaking, the way humanity _finally_ left the trap of authority and began paying attention to, y'know, the actual sky, was that beliefs based on experiment turned out to be _much more useful_ than beliefs based on authority.  Curiosity has been around since the dawn of humanity, but the problem is that spinning campfire tales works [just as well](/lw/go/why_truth_and/) for satisfying curiosity.\n\nHistorically speaking, science won because it displayed greater raw strength in the form of technology, not because science _sounded more reasonable_.  To this very day, magic and scripture still sound more reasonable to untrained ears than science.  That is why there is continuous social tension between the belief systems.  If science not only worked better than magic, but _also_ sounded more intuitively reasonable, it would have won _entirely_ by now.\n\nNow there are those who say:  \"How dare you suggest that anything should be valued more than Truth?  Must not a rationalist love Truth more than mere usefulness?\"\n\nForget for a moment what would have happened historically to someone like that—that people in pretty much that frame of mind defended the Bible because they loved Truth more than mere accuracy.  Propositional morality is a glorious thing, but it has [too many degrees of freedom](/lw/go/why_truth_and/).\n\nNo, the real point is that a rationalist's love affair with the Truth is, well, just _more complicated_ as an emotional relationship.\n\nOne doesn't become an adept rationalist without caring about the truth, both as a purely moral desideratum and as something that's fun to have.  I doubt there are many master composers who hate music.\n\nBut part of what I _like_ about rationality is the discipline imposed by requiring beliefs to yield predictions, which ends up taking us much closer to the truth than if we sat in the living room obsessing about Truth all day.  I _like_ the complexity of simultaneously having to love True-seeming ideas, and also being ready to drop them out the window at a moment's notice.  I even like the glorious aesthetic purity of declaring that I value mere usefulness above aesthetics.  That is almost a contradiction, but not quite; and that has an aesthetic quality as well, a delicious humor.\n\nAnd of course, no matter how much you profess your love of mere usefulness, you should never _actually_ end up [deliberately believing a useful false statement](/lw/je/doublethink_choosing_to_be_biased/).\n\nSo don't oversimplify the relationship between loving truth and loving usefulness.  It's not one or the other.  It's _complicated,_ which is not necessarily a defect in the moral aesthetics of [single events](/lw/n9/the_intuitions_behind_utilitarianism/).\n\nBut morality and aesthetics alone, believing that one ought to be \"rational\" or that certain ways of thinking are \"beautiful\", will not lead you to the center of the Way.  It wouldn't have gotten humanity out of the authority-hole.\n\nIn [Circular Altruism](/lw/n3/circular_altruism/), I discussed this dilemma:  Which of these options would you prefer:\n\n1.  Save 400 lives, with certainty\n2.  Save 500 lives, 90% probability; save no lives, 10% probability.\n\nYou may be tempted to grandstand, saying, \"How dare you gamble with people's lives?\"  Even if you, yourself, are one of the 500—but you don't know which one—you may still be tempted to rely on the comforting feeling of certainty, because our own lives are often worth less to us than a good [intuition](/lw/n9/the_intuitions_behind_utilitarianism/).\n\nBut if your precious daughter is one of the 500, and you don't know which one, _then,_ perhaps, you may feel more impelled to shut up and multiply—to notice that you have an 80% chance of saving her in the first case, and a 90% chance of saving her in the second.\n\nAnd yes, everyone in that crowd is someone's son or daughter.  Which, in turn, suggests that we should pick the second option as altruists, as well as concerned parents.\n\nMy point is not to suggest that one person's life is more valuable than 499 people.  What I am trying to say is that _more_ than your own life has to be at stake, before a person becomes desperate enough to resort to math.\n\nWhat if you believe that it is \"rational\" to choose the certainty of option 1?  Lots of people think that \"rationality\" is about choosing only methods that are certain to work, and rejecting all uncertainty.  But, hopefully, you care more about your daughter's life than about \"rationality\".\n\nWill pride in your own virtue as a rationalist save you?  Not if you believe that it is virtuous to choose certainty.  You will only be able to learn something about rationality if your daughter's life matters more to you than your pride as a rationalist.\n\nYou may even learn something about rationality from the experience, if you are already far enough grown in your Art to say, \"I must have had the wrong conception of rationality,\" and not, \"Look at how rationality gave me the wrong answer!\"\n\n(The essential difficulty in becoming a master rationalist is that you need quite a bit of rationality to bootstrap the learning process.)\n\nIs your belief that you ought to be rational, more important than your life?  Because, as I've previously observed, risking your life isn't comparatively all that scary.  Being [the lone voice of dissent](/lw/mb/lonely_dissent/) in the crowd and having everyone look at you funny is _much_ scarier than a mere threat to your life, according to the revealed preferences of teenagers who drink at parties and then drive home.  It will take something terribly important to make you willing to leave the pack.  A threat to your life won't be enough.\n\nIs your will to rationality stronger than your _pride?_  Can it be, if your will to rationality stems from your pride in your self-image as a rationalist?  It's helpful—_very_ helpful—to have a self-image which says that you are the sort of person who confronts harsh truth.  It's helpful to have too much self-respect to knowingly lie to yourself or refuse to face evidence.  But there may come a time when you have to admit that you've been doing rationality all wrong.  Then your pride, your self-image as a rationalist, may make that too hard to face.\n\nIf you've prided yourself on believing what the Great Teacher says—even when it seems harsh, even when you'd rather not—that may make it all the more bitter a pill to swallow, to admit that the Great Teacher is a fraud, and all your noble self-sacrifice was for naught.\n\nWhere do you get the will to keep moving forward?\n\nWhen I look back at my own personal journey toward rationality—not just humanity's historical journey—well, I grew up believing very strongly that I ought to be rational.  This made me an above-average Traditional Rationalist a la Feynman and Heinlein, and nothing more.  It did not drive me to go beyond the teachings I had received.  I only began to grow _further_ as a rationalist once I had something terribly important that I needed to do.  Something more important than my pride as a rationalist, never mind my life.\n\nOnly when you become more wedded to success than to any of your beloved techniques of rationality, do you begin to appreciate these words of Miyamoto Musashi:\n\n> \"You can win with a long weapon, and yet you can also win with a short weapon.  In short, the Way of the Ichi school is the spirit of winning, whatever the weapon and whatever its size.\"  \n>         —Miyamoto Musashi, _The Book of Five Rings_\n\nDon't mistake this for a specific teaching of rationality.  It describes how you _learn_ the Way, beginning with a desperate need to succeed.  No one masters the Way until more than their life is at stake.  More than their comfort, more even than their pride.\n\nYou can't just pick out a [Cause](/lw/lv/every_cause_wants_to_be_a_cult/) like that because you feel you need a hobby.  Go looking for a \"good cause\", and your mind will just fill in a [standard cliche](/lw/k8/how_to_seem_and_be_deep/).  [Learn how to multiply](/lw/n9/the_intuitions_behind_utilitarianism/), and perhaps you will recognize a drastically important cause when you see one.\n\nBut _if_ you have a cause like that, it is right and proper to wield your rationality in its service.\n\nTo strictly subordinate the aesthetics of rationality to a higher cause, is part of the aesthetic of rationality.  You should pay attention to that aesthetic:  You will never master rationality well enough to win with any weapon, if you do not appreciate the [beauty](/lw/mt/beautiful_probability/) for its own sake."
    },
    "voteCount": 117,
    "forceInclude": true
  },
  {
    "_id": "pGvyqAQw6yqTjpKf4",
    "url": null,
    "title": "The Gift We Give To Tomorrow",
    "slug": "the-gift-we-give-to-tomorrow",
    "author": "Eliezer_Yudkowsky",
    "question": false,
    "tags": [
      {
        "name": "Evolutionary Psychology"
      },
      {
        "name": "Complexity of Value"
      },
      {
        "name": "Human Values"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "How, oh how, did an unloving and mindless universe, cough up minds who were capable of love?\n\n\"No mystery in that,\" you say, \"it's just a matter of [natural selection](/lw/kr/an_alien_god/).\"\n\nBut natural selection is [cruel, bloody, and bloody stupid](/lw/kr/an_alien_god/).  Even when, on the surface of things, biological organisms aren't _directly_ fighting each other—aren't _directly_ tearing at each other with claws—there's still a deeper competition going on between the genes.  Genetic information is created when genes increase their _relative_ frequency in the next generation—what matters for \"genetic fitness\" is not how many children you have, but that you have _more_ children than others.  It is quite possible for a species to [evolve to extinction](/lw/l5/evolving_to_extinction/), if the winning genes are playing negative-sum games.\n\nHow, oh how, could such a process create beings capable of love?\n\n\"No mystery,\" you say, \"there is never any mystery-in-the-world; [mystery is a property of questions, not answers](/lw/iu/mysterious_answers_to_mysterious_questions/).  A mother's children share her genes, so the mother loves her children.\"\n\nBut sometimes mothers adopt children, and still love them.  And mothers love their children for themselves, not for their genes.\n\n\"No mystery,\" you say, \"Individual organisms are [adaptation-executers, not fitness-maximizers](/lw/l0/adaptationexecuters_not_fitnessmaximizers/).  [Evolutionary psychology](/lw/l1/evolutionary_psychology/) is not about deliberately maximizing fitness—through most of human history, we didn't know genes existed.  We don't calculate our acts' effect on genetic fitness consciously, or even subconsciously.\"\n\nBut human beings form friendships even with non-relatives: how, oh how, can it be?\n\n\"No mystery, for hunter-gatherers often play Iterated Prisoner's Dilemmas, the solution to which is reciprocal altruism.  Sometimes the most dangerous human in the tribe is not the strongest, the prettiest, or even the smartest, but the one who has the most allies.\"\n\nYet not all friends are fair-weather friends; we have a concept of true friendship—and some people have sacrificed their life for their friends.  Would not such a devotion tend to remove itself from the gene pool?\n\n\"You said it yourself: we have a concept of true friendship and fair-weather friendship.  We can tell, or try to tell, the difference between someone who considers us a valuable ally, and someone executing the friendship adaptation.  We wouldn't be true friends with someone who we didn't think was a true friend to us—and someone with many _true_ friends is far more formidable than someone with many fair-weather allies.\"\n\nAnd Mohandas Gandhi, who really did turn the other cheek?  Those who try to serve all humanity, whether or not all humanity serves them in turn?\n\n\"That perhaps is a more complicated story.  Human beings are not just social animals.  We are political animals who argue linguistically about policy in adaptive tribal contexts.  Sometimes the formidable human is not the strongest, but the one who can most skillfully argue that their preferred policies match the preferences of others.\"\n\nUm... that doesn't explain Gandhi, or am I missing something?\n\n\"The point is that we have the ability to _argue_ about 'What should be done?' as a _proposition_—we can make those arguments and respond to those arguments, without which politics could not take place.\"\n\nOkay, but Gandhi?\n\n\"Believed certain complicated propositions about 'What should be done?' and did them.\"\n\nThat sounds like it could [explain any possible](/lw/iq/guessing_the_teachers_password/) human behavior.\n\n\"If we traced back the chain of causality through all the arguments, it would involve: a moral architecture that had the ability to argue _general abstract_ moral propositions like 'What should be done to people?'; appeal to hardwired intuitions like fairness, a concept of duty, pain aversion + empathy; something like a preference for simple moral propositions, probably reused from our previous Occam prior; and the end result of all this, plus perhaps memetic selection effects, was 'You should not hurt people' in full generality—\"\n\nAnd that gets you Gandhi.\n\n\"Unless you think it was magic, it has to fit into the lawful causal development of the universe somehow.\"\n\nWell... I certainly won't postulate magic, [under any name](/lw/iv/the_futility_of_emergence/).\n\n\"Good.\"\n\nBut come on... doesn't it seem a little... _amazing_... that hundreds of millions of years worth of evolution's death tournament could cough up mothers and fathers, sisters and brothers, husbands and wives, steadfast friends and honorable enemies, true altruists and guardians of causes, police officers and loyal defenders, even artists sacrificing themselves for their art, all practicing so many kinds of love?  For [so many things other than genes](/lw/l3/thou_art_godshatter/)?  Doing their part to make their world less ugly, something besides a sea of blood and violence and mindless replication?\n\n\"Are you claiming to be surprised by this?  If so, [question your underlying model, for it has led you to be surprised by the true state of affairs](/lw/hs/think_like_reality/).  Since the beginning, not one unusual thing has ever happened.\"\n\nBut how is it _not_ surprising?\n\n\"What are you suggesting, that some sort of shadowy figure stood behind the scenes and directed evolution?\"\n\nHell no.  But—\n\n\"Because if you _were_ suggesting that, I would have to ask how that shadowy figure _originally_ decided that love was a _desirable_ outcome of evolution.  I would have to ask where that figure got preferences that included things like love, friendship, loyalty, fairness, honor, romance, and so on.  On evolutionary psychology, we can see how _that specific outcome_ came about—how _those particular goals rather than others_ were _generated in the first place._  You can call it 'surprising' all you like.  But when you really do understand evolutionary psychology, you can see how parental love and romance and honor, and even true altruism and moral arguments, _bear the specific design signature of natural selection_ in particular adaptive contexts of the hunter-gatherer savanna.  So if there was a shadowy figure, it must itself have evolved—and that obviates the whole point of postulating it.\"\n\nI'm not postulating a shadowy figure!  I'm just asking how human beings ended up so _nice._\n\n\"_Nice!_  Have you _looked_ at this planet lately?  We also bear all those other emotions that evolved, too—which would tell you very well that we evolved, should you begin to doubt it.  Humans aren't always nice.\"\n\nWe're one hell of a lot nicer than the process that produced us, which lets elephants starve to death when they run out of teeth, and doesn't anesthetize a gazelle even as it lays dying and is of no further importance to evolution one way or the other.  It doesn't take much to be nicer than evolution.  To have the _theoretical capacity_ to make one single gesture of mercy, to feel a single twinge of empathy, is to be nicer than evolution.  How did evolution, which is itself so uncaring, create minds on that qualitatively higher moral level than itself?  How did evolution, which is so ugly, end up doing anything so _beautiful?_\n\n\"Beautiful, you say?  Bach's _Little Fugue in G Minor_ may be beautiful, but the sound waves, as they travel through the air, are not stamped with tiny tags to specify their beauty.  If you wish to find _explicitly encoded_ a measure of the fugue's beauty, you will have to look at a human brain—nowhere else in the universe will you find it.  Not upon the seas or the mountains will you find such judgments written: they are not minds, they cannot think.\"\n\nPerhaps that is so, but still I ask:  How did evolution end up doing anything so beautiful, as giving us the ability to admire the beauty of a flower?\n\n\"Can you not see the circularity in your question?  If beauty were like some great light in the sky that shined from outside humans, then your question might make sense—though there would still be the question of how humans came to perceive that light.  You evolved with a psychology unlike evolution:  Evolution has nothing like the intelligence or the precision required to exactly quine its goal system.  In coughing up the first true minds, [evolution's simple fitness criterion shattered into a thousand values](/lw/l3/thou_art_godshatter/).  You evolved with a psychology that attaches [utility](/lw/l4/terminal_values_and_instrumental_values/) to things which evolution does not care about, like human life and happiness.  And then you look back and say, 'How marvelous, that uncaring evolution produced minds that care about sentient life!'  So your great marvel and wonder, that seems like far too much coincidence, is really no coincidence at all.\"\n\nBut then it is still amazing that this particular circular loop, happened to loop around such important things as beauty and altruism.\n\n\"I don't think you're following me here.  To you, it seems natural to privilege the beauty and altruism as special, as preferred, because you value them highly; and you don't see this as a unusual fact about yourself, because many of your friends do likewise.  So you expect that a [ghost of perfect emptiness](/lw/rn/no_universally_compelling_arguments/) would also value life and happiness—and then, from this standpoint outside reality, a great coincidence would indeed have occurred.\"\n\nBut you can make arguments for the importance of beauty and altruism from first principles—that our aesthetic senses lead us to create new complexity, instead of repeating the same things over and over; and that altruism is important because it takes us outside ourselves, gives our life a higher meaning than sheer brute selfishness.\n\n\"Oh, and _that_ argument is going to move even a [ghost of perfect emptiness](/lw/rn/no_universally_compelling_arguments/)—now that you've appealed to slightly different values?  Those aren't first principles, they're just _different_ principles.  Even if you've adopted a high-falutin' philosophical tone, still there are no _universally_ compelling arguments.  All you've done is [pass the recursive buck](/lw/rd/passing_the_recursive_buck/).\"\n\nYou don't think that, somehow, we evolved to _tap into_ something beyond—\n\n\"What good does it do to suppose something beyond?  Why should we pay more attention to that beyond thing, than we pay to our existence as humans?  How does it alter your personal responsibility, to say that you were only following the orders of the beyond thing?  And you would still have evolved to let the beyond thing, rather than something else, direct your actions.  You are only [passing the recursive buck](/lw/rd/passing_the_recursive_buck/).  Above all, it would be _too much coincidence._\"\n\nToo much coincidence?\n\n\"A flower is beautiful, you say.  Do you think there is no story behind that beauty, or that science does not know the story?  Flower pollen is transmitted by bees, so by sexual selection, flowers evolved to attract bees—by imitating certain mating signs of bees, as it happened; the flowers' patterns would look more intricate, if you could see in the ultraviolet.  Now healthy flowers are a sign of fertile land, likely to bear fruits and other treasures, and probably prey animals as well; so is it any wonder that humans evolved to be attracted to flowers?  But for there to be some great light written upon the very stars—those huge unsentient balls of burning hydrogen—which _also_ said that flowers were beautiful, now _that_ would be far too much coincidence.\"\n\nSo you [explain away](/lw/oo/explaining_vs_explaining_away/) the beauty of a flower?\n\n\"No, I explain it.  Of course there's a story behind the beauty of flowers and the fact that we find them beautiful.  Behind ordered events, one finds ordered stories; and what has no story is the product of random noise, which is hardly any better.  [If you cannot take joy in things that have stories behind them, your life will be empty indeed.](/lw/or/joy_in_the_merely_real/)  I don't think I take any less joy in a flower than you do; more so, perhaps, because I take joy in its story as well.\"\n\nPerhaps as you say, there is no surprise from a causal viewpoint—no disruption of the physical order of the universe.  But it still seems to me that, in this creation of humans by evolution, something happened that is precious and marvelous and wonderful.  If we cannot call it a physical miracle, then call it a moral miracle.\n\n\"Because it's only a miracle from the perspective of the morality that was produced, thus explaining away all of the apparent coincidence from a merely causal and physical perspective?\"\n\nWell... I suppose you could interpret the term that way, yes.  I just meant something that was immensely surprising and wonderful on a moral level, even if it is not surprising on a physical level.\n\n\"I think that's what I said.\"\n\nBut it still seems to me that you, from your own view, drain something of that wonder away.\n\n\"Then you have problems taking [joy in the merely real](/lw/or/joy_in_the_merely_real/).  Love has to begin _somehow,_ it has to enter the universe _somewhere._  It is like asking how life itself begins—and though you were born of your father and mother, and they arose from their living parents in turn, if you go far and far and far away back, you will finally come to a replicator that arose by pure accident—the border between life and unlife.  So too with love.\n\n\"A complex pattern must be explained by a cause which is not already that complex pattern.  Not just the event must be explained, but the very shape and form.  For love to first enter Time, it must come of something that is not love; if this were not possible, then love could not be.\n\n\"Even as life itself required that first replicator to come about by accident, parentless but still caused: far, far back in the causal chain that led to you: 3.85 billion years ago, in some little tidal pool.\n\n\"Perhaps your children's children will ask how it is that they are capable of love.\n\n\"And their parents will say:  Because we, who also love, created you to love.\n\n\"And your children's children will ask:  But how is it that _you_ love?\n\n\"And their parents will reply:  Because our own parents, who also loved, created us to love in turn.\n\n\"Then your children's children will ask:  But where did it all begin?  Where does the recursion end?\n\n\"And their parents will say:  Once upon a time, long ago and far away, ever so long ago, there were intelligent beings who were not themselves intelligently designed.  Once upon a time, there were lovers created by something that did not love.\n\n\"Once upon a time, when all of civilization was a single galaxy and a single star: and a single planet, a place called Earth.\n\n\"Long ago, and far away, ever so long ago.\""
    },
    "voteCount": 88,
    "forceInclude": true
  },
  {
    "_id": "ur9TCRnHJighHmLCW",
    "url": null,
    "title": "On Caring",
    "slug": "on-caring",
    "author": "So8res",
    "question": false,
    "tags": [
      {
        "name": "Motivations"
      },
      {
        "name": "World Optimization"
      },
      {
        "name": "Motivational Intro Posts"
      }
    ],
    "tableOfContents": {
      "sections": [
        {
          "title": "1",
          "anchor": "1",
          "level": 1
        },
        {
          "title": "2",
          "anchor": "2",
          "level": 1
        },
        {
          "title": "3",
          "anchor": "3",
          "level": 1
        },
        {
          "title": "4",
          "anchor": "4",
          "level": 1
        },
        {
          "title": "5",
          "anchor": "5",
          "level": 1
        },
        {
          "title": "6",
          "anchor": "6",
          "level": 1
        },
        {
          "title": "7",
          "anchor": "7",
          "level": 1
        },
        {
          "title": "8",
          "anchor": "8",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        },
        {
          "anchor": "comments",
          "level": 0,
          "title": "275 comments"
        }
      ],
      "headingsCount": 10
    },
    "contents": {
      "markdown": "_This is an essay describing some of my motivation to be an effective altruist. It is [crossposted](http://mindingourway.com/on-caring/) from [my blog](http://mindingourway.com/). Many of the ideas here are quite similar to [others found in the sequences](/lw/hw/scope_insensitivity/). I have a slightly different take, and after adjusting for the typical mind fallacy I expect that this post may contain insights that are new to many._\n\n1\n=\n\nI'm not very good at _feeling_ the size of large numbers. Once you start tossing around numbers larger than 1000 (or maybe even 100), the numbers just seem \"big\".\n\nConsider Sirius, the brightest star in the night sky. If you told me that Sirius is as big as a million earths, I would feel like that's a lot of Earths. If, instead, you told me that you could fit a _billion_ Earths inside Sirius… I would still just feel like that's a lot of Earths.\n\nThe feelings are almost identical. _In context_, my brain grudgingly admits that a billion is a lot larger than a million, and puts forth a token effort to feel like a billion-Earth-sized star is bigger than a million-Earth-sized star. But out of context — if I wasn't anchored at \"a million\" when I heard \"a billion\" — both these numbers just feel vaguely large.\n\nI feel a _little_ respect for the bigness of numbers, if you pick really really large numbers. If you say \"one followed by a hundred zeroes\", then this feels _a lot_ bigger than a billion. But it certainly doesn't feel (in my gut) like it's 10 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 000 times bigger than a billion. Not in the way that four apples _internally feels_ like twice as many as two apples. My brain can't even begin to wrap itself around this sort of magnitude differential.\n\nThis phenomena is related to [scope insensitivity](/lw/hw/scope_insensitivity/), and it's important to me because I live in a world where sometimes the things I care about are really really numerous.\n\nFor example, [billions of people live in squalor](http://www.globalissues.org/article/26/poverty-facts-and-stats), with hundreds of millions of them deprived of basic needs and/or dying from disease. And though most of them are out of my sight, I still care about them.\n\nThe loss of a human life with all is joys and all its sorrows is tragic no matter what the cause, and the tragedy is not reduced simply because I was far away, or because I did not know of it, or because I did not know how to help, or because I was not personally responsible.\n\nKnowing this, I care about every single individual on this planet. The problem is, my brain is _simply incapable_ of taking the amount of caring I feel for a single person and scaling it up by a billion times. I lack the internal capacity to feel that much. My care-o-meter simply doesn't go up that far.\n\nAnd this is a problem.\n\n2\n=\n\nIt's a common trope that courage isn't about being fearless, it's about being afraid but _doing the right thing anyway_. In the same sense, caring about the world isn't about having a gut feeling that corresponds to the amount of suffering in the world, it's about _doing the right thing anyway_. Even without the feeling.\n\nMy internal care-o-meter was calibrated to deal with [about a hundred and fifty people](http://en.wikipedia.org/wiki/Dunbar's_number), and it _simply can't express_ the amount of caring that I have for billions of sufferers. The internal care-o-meter just doesn't go up that high.\n\nHumanity is playing for unimaginably high stakes. At the very least, there are billions of people suffering today. At the worst, there are quadrillions (or more) potential humans, transhumans, or posthumans whose existence depends upon what we do here and now. All the intricate civilizations that the future could hold, the experience and art and beauty that is possible in the future, depends upon the present.\n\nWhen you're faced with stakes like these, your internal caring heuristics — calibrated on numbers like \"ten\" or \"twenty\" — completely fail to grasp the gravity of the situation.\n\nSaving a person's life feels _great_, and [it would probably feel just about as good to save one life as it would feel to save the world](/lw/hx/one_life_against_the_world/). It surely wouldn't be _many billion times_ more of a high to save the world, because your hardware can't express a feeling a billion times bigger than the feeling of saving a person's life. But even though the altruistic high from saving someone's life would be shockingly similar to the altruistic high from saving the world, always remember that _behind_ those similar feelings there is a whole world of difference.\n\nOur internal care-feelings are woefully inadequate for deciding how to act in a world with big problems.\n\n3\n=\n\nThere's a mental shift that happened to me when I first started internalizing scope insensitivity. It is a little difficult to articulate, so I'm going to start with a few stories.\n\nConsider Alice, a software engineer at Amazon in Seattle. Once a month or so, those college students will show up on street corners with clipboards, looking ever more disillusioned as they struggle to convince people to donate to [Doctors Without Borders](http://www.doctorswithoutborders.org/). Usually, Alice avoids eye contact and goes about her day, but this month they finally manage to corner her. They explain Doctors Without Borders, and she actually has to admit that it sounds like a pretty good cause. She ends up handing them $20 through a combination of guilt, social pressure, and altruism, and then rushes back to work. (Next month, when they show up again, she avoids eye contact.)\n\nNow consider Bob, who has been given the [Ice Bucket Challenge](http://en.wikipedia.org/wiki/Ice_Bucket_Challenge) by a friend on facebook. He feels too busy to do the ice bucket challenge, and instead just donates $100 to [ALSA](http://www.alsa.org/).\n\nNow consider Christine, who is in the college sorority ΑΔΠ. ΑΔΠ is engaged in a competition with ΠΒΦ (another sorority) to see who can raise the most money for the National Breast Cancer Foundation in a week. Christine has a competitive spirit and gets engaged in fund-raising, and gives a few hundred dollars herself over the course of the week (especially at times when ΑΔΠ is especially behind).\n\nAll three of these people are donating money to charitable organizations… and that's great. But notice that there's something similar in these three stories: these donations are largely motivated by a _social context_. Alice feels obligation and social pressure. Bob feels social pressure and maybe a bit of camaraderie. Christine feels camaraderie and competitiveness. These are all fine motivations, but notice that these motivations are related to the _social setting_, and only tangentially to the _content_ of the charitable donation.\n\nIf you took any of Alice or Bob or Christine and asked them why they aren't donating _all_ of their time and money to these causes that they apparently believe are worthwhile, they'd look at you funny and they'd probably think you were being rude (with good reason!). If you pressed, they might tell you that money is a little tight right now, or that they would donate more if they were a better person.\n\nBut the question would still feel kind of _wrong_. Giving all your money away is just not what you do with money. We can all _say out loud_ that people who give all their possessions away are really great, but behind closed doors we all know that people are crazy. (Good crazy, perhaps, but crazy all the same.)\n\nThis is a mindset that I inhabited for a while. There's an alternative mindset that can hit you like a freight train when you start internalizing scope insensitivity.\n\n4\n=\n\nConsider Daniel, a college student shortly after the [Deepwater Horizon](http://en.wikipedia.org/wiki/Deepwater_Horizon_oil_spill) BP oil spill. He encounters one of those college students with the clipboards on the street corners, soliciting donations to the [World Wildlife Foundation](http://www.worldwildlife.org/). They're trying to save as many oiled birds as possible. Normally, Daniel would simply dismiss the charity as Not The Most Important Thing, or Not Worth His Time Right Now, or Somebody Else's Problem, but this time Daniel has been thinking about how his brain is bad at numbers and decides to do a quick sanity check.\n\nHe pictures himself walking along the beach after the oil spill, and encountering a group of people cleaning birds as fast as they can. They simply don't have the resources to clean all the available birds. A pathetic young bird flops towards his feet, slick with oil, eyes barely able to open. He kneels down to pick it up and help it onto the table. One of the bird-cleaners informs him that they won't have time to get to that bird themselves, but he could pull on some gloves and could probably save the bird with three minutes of washing.\n\n![blog.bird-rescue.org](http://4.bp.blogspot.com/_Bv2wRceCYKA/S-whZ1sSJ3I/AAAAAAAABjo/cgXz7Npz_W0/s320/Dawn_IBRRC_2010.05.07_MG_7082_240.jpg)\n\nDaniel decides that he _would_ spend three minutes of his time to save the bird, and that he would _also_ be happy to pay at least $3 to have someone else spend a few minutes cleaning the bird. He introspects and finds that this is not just because he imagined a bird right in front of him: he feels that it is _worth_ at least three minutes of his time (or $3) to save an oiled bird in some vague platonic sense.\n\nAnd, because he's been thinking about scope insensitivity, he _expects_ his brain to misreport how much he actually cares about large numbers of birds: the internal feeling of caring can't be expected to line up with the actual importance of the situation. So instead of just _asking his gut_ how much he cares about de-oiling lots of birds, he shuts up and multiplies.\n\n[Thousands and thousands](http://dailydeadbirds.com/) of birds were oiled by the BP spill alone. After shutting up and multiplying, Daniel realizes (with growing horror) that the amount he _acutally_ cares about oiled birds is lower bounded by two months of hard work and/or fifty thousand dollars. And that's not even counting wildlife threatened by [other oil spills](http://en.wikipedia.org/wiki/List_of_oil_spills).\n\nAnd if he cares that much about _de-oiling birds_, then how much does he actually care about factory farming, nevermind hunger, or poverty, or sickness? How much does he actually care about wars that ravage nations? About neglected, deprived children? About the future of humanity? He _actually_ cares about these things to the tune of much more money than he has, and much more time than he has.\n\nFor the first time, Daniel sees a glimpse of of how much he actually cares, and how poor a state the world is in.\n\nThis has the strange effect that Daniel's reasoning goes full-circle, and he realizes that he actually _can't_ care about oiled birds to the tune of 3 minutes or $3: not because the birds aren't _worth_ the time and money (and, in fact, he thinks that the economy produces things priced at $3 which are worth less than the bird's survival), but because he can't spend _his_ time or money on saving the birds. The opportunity cost suddenly seems far too high: there is _too much else to do!_ People are sick and starving and dying! The very future of our civilization is at stake!\n\nDaniel doesn't wind up giving $50k to the WWF, and he also doesn't donate to ALSA or NBCF. But if you ask _Daniel_ why he's not donating all his money, he won't look at you funny or think you're rude. He's left the place where you don't care far behind, and has realized that _his mind was lying to him the whole time_ about the gravity of the real problems.\n\nNow he realizes that he _can't possibly do enough_. After adjusting for his scope insensitivity (and the fact that his brain lies about the size of large numbers), even the \"less important\" causes like the WWF suddenly seem worthy of dedicating a life to. Wildlife destruction and ALS and breast cancer are suddenly all problems that he would _move mountains_ to solve — except he's finally understood that there are just too many mountains, and ALS isn't the bottleneck, and AHHH HOW DID ALL THESE MOUNTAINS GET HERE?\n\nIn the original mindstate, the reason he didn't drop everything to work on ALS was because it just didn't seem… pressing enough. Or tractable enough. Or important enough. Kind of. These are sort of the reason, but the real reason is more that the concept of \"dropping everything to address ALS\" never even _crossed his mind_ as a real possibility. The idea was too much of a break from the standard narrative. It wasn't his problem.\n\nIn the new mindstate, _everything_ is his problem. The only reason he's not dropping everything to work on ALS is because there are far too many things to do first.\n\nAlice and Bob and Christine usually aren't spending time solving all the world's problems because they forget to see them. If you remind them — put them in a social context where they remember how much they care (hopefully without guilt or pressure) — then they'll likely donate a little money.\n\nBy contrast, Daniel and others who have undergone the mental shift aren't spending time solving all the world's problems because there are _just too many problems_. (Daniel hopefully goes on to discover movements like [effective altruism](http://effectivealtruism.org/) and starts contributing towards fixing the world's most pressing problems.)\n\n5\n=\n\nI'm not trying to preach here about how to be a good person. You don't need to share my viewpoint to be a good person (obviously).\n\nRather, I'm trying to point at a shift in perspective. Many of us go through life understanding that we _should_ care about people suffering far away from us, but failing to. I think that this attitude is tied, at least in part, to the fact that most of us implicitly trust our internal care-o-meters.\n\nThe \"care feeling\" isn't usually strong enough to compel us to frantically save everyone dying. So while we acknowledge that it would be _virtuous_ to do more for the world, we think that we _can't_, because we weren't gifted with that virtuous extra-caring that prominent altruists must have.\n\nBut this is an error — prominent altruists aren't the people who have a larger care-o-meter, they're the people who have _learned not to trust their care-o-meters_.\n\nOur care-o-meters are broken. They don't work on large numbers. Nobody has one capable of faithfully representing the scope of the world's problems. But the fact that you can't _feel_ the caring doesn't mean that you can't _do_ the caring.\n\nYou don't get to feel the appropriate amount of \"care\", in your body. Sorry — the world's problems are just too large, and your body is not built to respond appropriately to problems of this magnitude. But if you choose to do so, you can still _act_ like the world's problems are as big as they are. You can stop trusting the internal feelings to guide your actions and switch over to manual control.\n\n6\n=\n\nThis, of course, leads us to the question of \"what the hell do you then?\"\n\nAnd I don't really know yet. (Though I'll plug the [Giving What We Can pledge](http://givingwhatwecan.org), [GiveWell](http://givewell.org), [MIRI](http://intelligence.org), and [The Future of Humanity Institute](http://www.fhi.ox.ac.uk) as a good start).\n\nI think that at least part of it comes from a certain sort of desperate perspective. It's not enough to think you _should_ change the world — you also need the sort of desperation that comes from realizing that you would dedicate your entire life to solving the world's 100th biggest problem if you could, but you can't, because there are 99 bigger problems you have to address first.\n\nI'm not trying to guilt you into giving more money away — becoming a philanthropist is _really really hard_. (If you're _already_ a philanthropist, then you have my acclaim and my affection.) First it requires you to have money, which is uncommon, and then it requires you to _throw that money at distant invisible problems_, which is not an easy sell to a human brain. [Akrasia](http://en.wikipedia.org/wiki/Akrasia) is a formidable enemy. And most importantly, guilt doesn't seem like a good long-term motivator: if you want to join the ranks of people saving the world, I would rather you join them proudly. There are many trials and tribulations ahead, and we'd do better to face them with our heads held high.\n\n7\n=\n\nCourage isn't about being fearless, it's about being able to do the right thing even if you're afraid.\n\nAnd similarly, addressing the major problems of our time isn't about feeling a strong compulsion to do so. It's about doing it anyway, even when internal compulsion utterly fails to capture the scope of the problems we face.\n\nIt's easy to look at especially virtuous people — Gandhi, Mother Theresa, Nelson Mandela — and conclude that they must have cared more than we do. But I don't think that's the case.\n\nNobody gets to comprehend the scope of these problems. The closest we can get is doing the multiplication: finding something we care about, putting a number on it, and multiplying. And then trusting the numbers more than we trust our feelings.\n\nBecause our feelings lie to us.\n\nWhen you do the multiplication, you realize that addressing global poverty and building a brighter future deserve more resources than currently exist. There is not enough money, time, or effort in the world to do what we need to do.\n\nThere is only you, and me, and everyone else who is trying anyway.\n\n8\n=\n\nYou can't actually feel the weight of the world. The human mind is not capable of that feat.\n\nBut sometimes, you can catch a glimpse."
    },
    "voteCount": 152,
    "forceInclude": true
  },
  {
    "_id": "DoLQN5ryZ9XkZjq5h",
    "url": null,
    "title": "Tsuyoku Naritai! (I Want To Become Stronger)",
    "slug": "tsuyoku-naritai-i-want-to-become-stronger",
    "author": "Eliezer Yudkowsky",
    "question": false,
    "tags": [
      {
        "name": "Ambition"
      },
      {
        "name": "Rationality"
      },
      {
        "name": "Something To Protect"
      },
      {
        "name": "Tsuyoku Naritai"
      },
      {
        "name": "Motivational Intro Posts"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "In Orthodox Judaism there is a saying: “The previous generation is to the next one as angels are to men; the next generation is to the previous one as donkeys are to men.” This follows from the Orthodox Jewish belief that all Judaic law was given to Moses by God at Mount Sinai. After all, it’s not as if you could do an experiment to gain new halachic knowledge; the only way you can know is if someone tells you (who heard it from someone else, who heard it from God). Since there is no new source of information; it can only be degraded in transmission from generation to generation.\n\nThus, modern rabbis are not allowed to overrule ancient rabbis. Crawly things are ordinarily unkosher, but it is permissible to eat a worm found in an apple—the ancient rabbis believed the worm was spontaneously generated inside the apple, and therefore was part of the apple. A modern rabbi cannot say, “Yeah, well, the ancient rabbis knew diddly-squat about biology. Overruled!” A modern rabbi cannot possibly know a halachic principle the ancient rabbis did not, because how could the ancient rabbis have passed down the answer from Mount Sinai to him? Knowledge derives from authority, and therefore is only ever lost, not gained, as time passes.\n\nWhen I was first exposed to the angels-and-donkeys proverb in (religious) elementary school, I was not old enough to be a full-blown atheist, but I still thought to myself: “Torah loses knowledge in every generation. Science gains knowledge with every generation. No matter where they started out, sooner or later science must surpass Torah.”\n\nThe most important thing is that there should be progress. So long as you keep moving forward you will reach your destination; but if you stop moving you will never reach it.\n\n_Tsuyoku naritai_ is Japanese. _Tsuyoku_ is “strong”; _naru_ is “becoming,” and the form _naritai_ is “want to become.” Together it means, “I want to become stronger,” and it expresses a sentiment embodied more intensely in Japanese works than in any Western literature I’ve read. You might say it when expressing your determination to become a professional Go player—or after you lose an important match, but you haven’t given up—or after you win an important match, but you’re not a ninth-dan player yet—or after you’ve become the greatest Go player of all time, but you still think you can do better. That is _tsuyoku naritai_, the will to transcendence.\n\nEach year on Yom Kippur, an Orthodox Jew recites a litany which begins _Ashamnu, bagadnu, gazalnu, dibarnu dofi_, and goes on through the entire Hebrew alphabet: _We have acted shamefully, we have betrayed, we have stolen, we have slandered . . ._\n\nAs you pronounce each word, you strike yourself over the heart in penitence. There’s no exemption whereby, if you manage to go without stealing all year long, you can skip the word _gazalnu_ and strike yourself one less time. That would violate the community spirit of Yom Kippur, which is about _confessing_ sins—not _avoiding_ sins so that you have less to confess.\n\nBy the same token, the _Ashamnu_ does not end, “But that was this year, and next year I will do better.”\n\nThe _Ashamnu_ bears a remarkable resemblance to the notion that the way of rationality is to beat your fist against your heart and say, “We are all biased, we are all irrational, we are not fully informed, we are overconfident, we are poorly calibrated . . .”\n\nFine. Now tell me how you plan to become _less_ biased, _less_ irrational, _more_ informed, _less_ overconfident, _better_ calibrated.\n\nThere is an old Jewish joke: During Yom Kippur, the rabbi is seized by a sudden wave of guilt, and prostrates himself and cries, “God, I am nothing before you!” The cantor is likewise seized by guilt, and cries, “God, I am nothing before you!” Seeing this, the janitor at the back of the synagogue prostrates himself and cries, “God, I am nothing before you!” And the rabbi nudges the cantor and whispers, “Look who thinks he’s nothing.”\n\nTake no pride in your confession that you too are biased; do not glory in your self-awareness of your flaws. This is akin to the principle of not taking pride in confessing your ignorance; for if your ignorance is a source of pride to you, you may become loath to relinquish your ignorance when evidence comes knocking. Likewise with our flaws—we should not gloat over how self-aware we are for confessing them; the occasion for rejoicing is when we have a little less to confess.\n\nOtherwise, when the one comes to us with a plan for _correcting_ the bias, we will snarl, “Do you think to set yourself above us?” We will shake our heads sadly and say, “You must not be very self-aware.”\n\nNever confess to me that you are just as flawed as I am unless you can tell me what you plan to do about it. Afterward you will still have plenty of flaws left, but that’s not the point; the important thing is to _do better_, to keep moving ahead, to take one more step forward. _Tsuyoku naritai!_"
    },
    "voteCount": 219,
    "forceInclude": true
  },
  {
    "_id": "Nu3wa6npK4Ry66vFp",
    "url": null,
    "title": "A Sense That More Is Possible",
    "slug": "a-sense-that-more-is-possible",
    "author": "Eliezer_Yudkowsky",
    "question": false,
    "tags": [
      {
        "name": "Rationality"
      },
      {
        "name": "Rationality Verification"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "To teach people about a topic you've labeled \"rationality\", it helps for them to be interested in \"rationality\".  (There are less direct ways to teach people how to attain [the map that reflects the territory](http://www.overcomingbias.com/2006/11/why_truth_and.html), or optimize reality according to their values; but the explicit method _is_ the course I tend to take.)\n\nAnd when people explain why they're _not_ interested in rationality, one of the most commonly proffered reasons tends to be like:  \"Oh, I've known a couple of rational people and they didn't seem any happier.\"\n\nWho are they thinking of?  Probably an [Objectivist](http://www.overcomingbias.com/2007/12/ayn-rand.html) or some such.  Maybe someone they know who's an [ordinary scientist](http://www.overcomingbias.com/2008/05/do-scientists-a.html).  Or an [ordinary atheist](/lw/1e/raising_the_sanity_waterline/).\n\nThat's really _not_ a whole lot of rationality, as I have previously said.\n\nEven if you limit yourself to people who can derive Bayes's Theorem—which is going to eliminate, what, 98% of the above personnel?—that's _still_ not a whole lot of rationality.  I mean, it's a pretty basic theorem.\n\nSince the beginning I've had a sense that there ought to be some discipline of cognition, some art of thinking, the studying of which would make its students visibly more competent, more formidable: the equivalent of [Taking a Level in Awesome](http://tvtropes.org/pmwiki/pmwiki.php/Main/TookALevelInBadass).\n\nBut when I look around me in the real world, I don't see that.  Sometimes I see a hint, an echo, of what I think should be possible, when I read the writings of folks like Robyn Dawes, Daniel Gilbert, Tooby & Cosmides.  A few very rare and very senior researchers in psychological sciences, who visibly care a _lot_ about rationality—to the point, I suspect, of making their colleagues feel uncomfortable, because it's not cool to care that much.  I can see that they've found a rhythm, a unity that begins to pervade their arguments—\n\nYet even that... isn't really a whole lot of rationality either.\n\nEven among those whose few who impress me with a hint of dawning formidability—I don't think that their mastery of rationality could compare to, say, John Conway's mastery of math.  The base knowledge that we drew upon to build our understanding—if you extracted only the parts we used, and not everything we had to study to find it—it's probably not comparable to what a professional nuclear engineer knows about nuclear engineering.  It may not even be comparable to what a construction engineer knows about bridges.  We practice our skills, we do, in the ad-hoc ways we taught ourselves; but that practice probably doesn't compare to the training regimen an Olympic runner goes through, or maybe even an ordinary professional tennis player.\n\nAnd the root of _this_ problem, I do suspect, is that we haven't really gotten together and systematized our skills.  We've had to create all of this for ourselves, ad-hoc, and there's a limit to how much one mind can do, even if it can manage to draw upon work done in outside fields.\n\nThe chief obstacle to doing this the way it _really_ should be done, is the difficulty of testing the _results_ of rationality training programs, so you can have evidence-based training methods.  I will write more about this, because I think that recognizing successful training and distinguishing it from failure is the essential, blocking obstacle.\n\nThere are experiments done now and again on debiasing interventions for particular biases, but it tends to be something like, \"Make the students practice this for an hour, then test them two weeks later.\"  Not, \"Run half the signups through version A of the three-month summer training program, and half through version B, and survey them five years later.\"  You can see, here, the implied amount of effort that I think would go into a training program for people who were Really Serious about rationality, as opposed to the attitude of taking Casual Potshots That Require Like An Hour Of Effort Or Something.\n\nDaniel Burfoot brilliantly [suggests](http://www.overcomingbias.com/2008/10/isshokenmei.html#comment-133847467) that this is why intelligence seems to be such a big factor in rationality—that when you're improvising everything ad-hoc with very little training or systematic practice, intelligence ends up being the most important factor in what's left.\n\nWhy aren't \"rationalists\" surrounded by a visible aura of formidability?  Why aren't they found at the top level of every elite selected on any basis that has anything to do with thought?  Why do most \"rationalists\" just seem like ordinary people, perhaps of moderately above-average intelligence, with one more hobbyhorse to ride?\n\nOf this there are several answers; but one of them, surely, is that they have received less systematic training of rationality in a less systematic context than a first-dan black belt gets in hitting people.\n\nI do not except myself from this criticism.  I am no [beisutsukai](http://www.overcomingbias.com/2008/05/eld-science.html), because there are limits to how much Art you can create on your own, and how well you can guess without evidence-based statistics on the results.  I know about a _single_ use of rationality, which might be termed \"reduction of confusing cognitions\".  This I asked of my brain, this it has given me.  There are other arts, I think, that a mature rationality training program would not neglect to teach, which would make me stronger and happier and more effective—if I could just go through a standardized training program using the cream of teaching methods experimentally demonstrated to be effective.  But the kind of tremendous, focused effort that I put into creating my single _sub-art_ of rationality from scratch—my life doesn't have room for more than one of those.\n\nI consider myself something more than a first-dan black belt, and less.  I can _punch_ through brick and I'm working on steel along my way to adamantine, but I have a mere casual street-fighter's grasp of how to kick or throw or block.\n\nWhy are there schools of martial arts, but not [rationality dojos](http://www.overcomingbias.com/2006/11/the_martial_art.html)?  (This was the first question I asked in my [first blog post](http://www.overcomingbias.com/2006/11/the_martial_art.html).)  Is it more important to hit people than to think?\n\nNo, but it's easier to verify when you _have_ hit someone.  That's part of it, a highly central part.\n\nBut maybe even more importantly—there are people out there who _want_ to hit, and who have the idea that there ought to be a systematic art of hitting that makes you into a visibly more formidable fighter, with a speed and grace and strength beyond the struggles of the unpracticed.  So they go to a school that promises to teach that.  And that school exists because, long ago, some people had the sense that more was possible.  And they got together and shared their techniques and practiced and formalized and practiced and developed the Systematic Art of Hitting.  They pushed themselves that far because _they thought they should be awesome_ and they were willing to put some _back_ into it.\n\nNow—they _got_ somewhere with that aspiration, unlike a thousand other aspirations of awesomeness that failed, because they could _tell_ when they had hit someone; and the schools competed against each other regularly in realistic contests with clearly-defined winners.\n\nBut before even that—there was first the aspiration, the [wish to become stronger](http://www.overcomingbias.com/2007/03/tsuyoku_naritai.html), a sense that more was possible.  A vision of a speed and grace and strength that they did not already possess, but _could_ possess, _if_ they were willing to put in a lot of work, that drove them to systematize and train and test.\n\nWhy don't we have an Art of Rationality?\n\nThird, because current \"rationalists\" have trouble working in groups: of this I shall speak more.\n\nSecond, because it is hard to verify success in training, or which of two schools is the stronger.\n\nBut first, because people lack the sense that rationality is something that _should_ be systematized and trained and tested like a martial art, that should have as much knowledge behind it as nuclear engineering, whose superstars should practice as hard as chess grandmasters, whose successful practitioners should be surrounded by an evident aura of awesome.\n\nAnd conversely they don't look at the _lack_ of visibly greater formidability, and say, \"We must be doing something wrong.\"\n\n\"Rationality\" just seems like one more hobby or hobbyhorse, that people talk about at parties; an adopted mode of conversational [attire](http://www.overcomingbias.com/2007/08/belief-as-attir.html) with few or no real consequences; and it doesn't seem like there's anything wrong about that, either."
    },
    "voteCount": 104,
    "forceInclude": true
  },
  {
    "_id": "WBdvyyHLdxZSAMmoz",
    "url": null,
    "title": "Taboo Your Words",
    "slug": "taboo-your-words",
    "author": "Eliezer_Yudkowsky",
    "question": false,
    "tags": [
      {
        "name": "World Modeling"
      },
      {
        "name": "Disagreement"
      },
      {
        "name": "Conversation (topic)"
      },
      {
        "name": "Techniques"
      },
      {
        "name": "Philosophy of Language"
      },
      {
        "name": "Rationalist Taboo"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "In the game Taboo (by Hasbro), the objective is for a player to have their partner guess a word written on a card, without using that word or five additional words listed on the card.  For example, you might have to get your partner to say \"baseball\" without using the words \"sport\", \"bat\", \"hit\", \"pitch\", \"base\" or of course \"baseball\".\n\nAs soon as I see a problem like that, I at once think, \"An artificial group conflict in which you use a long wooden cylinder to whack a thrown spheroid, and then run between four safe positions.\"  It might not be the most efficient strategy to convey the word 'baseball' under the stated rules - that might be, \"It's what the Yankees play\" - but the general skill of _blanking a word out of my mind_ was one I'd practiced for years, albeit with a different purpose.\n\nYesterday we saw how replacing terms with definitions could reveal the [empirical unproductivity](/lw/nf/the_parable_of_hemlock/) of the classical Aristotelian syllogism.  All humans are mortal (and also, apparently, featherless bipeds); Socrates is human; therefore Socrates is mortal.  When we replace the word 'human' by its apparent definition, the following underlying reasoning is revealed:\n\n> All \\[mortal, ~feathers, biped\\] are mortal;  \n> Socrates is a \\[mortal, ~feathers, biped\\];  \n> Therefore Socrates is mortal.\n\nBut the principle of replacing words by definitions applies much more broadly:\n\n> Albert:  \"A tree falling in a deserted forest makes a sound.\"  \n> Barry:  \"A tree falling in a deserted forest does not make a sound.\"\n\nClearly, since one says \"sound\" and one says \"not sound\", we must have a contradiction, right?  But suppose that they both dereference their pointers before speaking:\n\n> Albert:  \"A tree falling in a deserted forest matches \\[membership test: this event generates acoustic vibrations\\].\"  \n> Barry:  \"A tree falling in a deserted forest does not match \\[membership test: this event generates auditory experiences\\].\"\n\nNow there is no longer an apparent collision—all they had to do was prohibit themselves from using the word _sound_. If \"acoustic vibrations\" came into dispute, we would just play Taboo again and say \"pressure waves in a material medium\"; if necessary we would play Taboo again on the word \"[wave](/lw/iq/guessing_the_teachers_password/)\" and replace it with the wave equation.  (Play Taboo on \"auditory experience\" and you get \"That form of sensory processing, within the human brain, which takes as input a linear time series of frequency mixes...\")\n\nBut suppose, on the other hand, that Albert and Barry were to have the argument:\n\n> Albert:  \"Socrates matches the concept \\[membership test: this person will die after drinking hemlock\\].\"  \n> Barry:  \"Socrates matches the concept \\[membership test: this person will not die after drinking hemlock\\].\"\n\nNow Albert and Barry have a substantive clash of expectations; a difference in what they anticipate seeing after Socrates drinks hemlock.  But they might not notice this, if they happened to use the same word \"human\" for their different concepts.\n\nYou get a very different picture of what people agree or disagree about, depending on whether you take a label's-eye-view (Albert says \"sound\" and Barry says \"not sound\", so they must disagree) or taking the test's-eye-view (Albert's membership test is acoustic vibrations, Barry's is auditory experience).\n\nGet together a pack of _soi-disant_ futurists and ask them if they believe we'll have Artificial Intelligence in thirty years, and I would guess that at least half of them will say yes.  If you leave it at that, they'll shake hands and congratulate themselves on their consensus.  But make the term \"Artificial Intelligence\" taboo, and ask them to describe _what_ they expect to see, without ever using words like \"computers\" or \"think\", and you might find quite a conflict of expectations hiding under that featureless standard word.  Likewise [that other term](http://intelligence.org/blog/2007/09/30/three-major-singularity-schools/).  And see also Shane Legg's compilation of [71 definitions of \"intelligence\"](http://arxiv.org/abs/0706.3639).\n\nThe illusion of unity across religions can be dispelled by making the term \"God\" taboo, and asking them to say what it is they believe in; or making the word \"faith\" taboo, and asking them why they believe it. Though mostly they won't be able to answer at all, because it is mostly [profession](/lw/i4/belief_in_belief/) in the first place, and you cannot cognitively zoom in on an audio recording.\n\nWhen you find yourself in philosophical difficulties, _the first line of defense is not to define your problematic terms, but to see whether you can think without using those terms at all._  Or any of their short synonyms.  And be careful not to let yourself invent a new word to use instead.  Describe outward observables and interior mechanisms; don't use a single handle, whatever that handle may be.\n\nAlbert says that people have \"free will\".  Barry says that people don't have \"free will\".  Well, that will certainly generate an apparent conflict.  Most philosophers would advise Albert and Barry to try to define exactly what they mean by \"free will\", on which topic they will certainly be able to discourse at great length.  I would advise Albert and Barry to describe what it is that they think people do, or do not have, without using the phrase \"free will\" at all.  (If you want to try this at home, you should also avoid the words \"choose\", \"act\", \"decide\", \"determined\", \"responsible\", or any of their synonyms.)\n\nThis is one of the nonstandard tools in my toolbox, and in my humble opinion, it works _way way_ better than the standard one.  It also requires more effort to use; you get what you pay for."
    },
    "voteCount": 136,
    "forceInclude": true
  },
  {
    "_id": "Mc6QcrsbH5NRXbCRX",
    "url": null,
    "title": "Dissolving the Question",
    "slug": "dissolving-the-question",
    "author": "Eliezer_Yudkowsky",
    "question": false,
    "tags": [
      {
        "name": "Introspection"
      },
      {
        "name": "Philosophy of Language"
      },
      {
        "name": "Philosophy"
      },
      {
        "name": "Dissolving the Question"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "\"If [a tree falls in the forest](/lw/np/disputing_definitions/), but no one hears it, does it make a sound?\"\n\nI didn't _answer_ that question.  I didn't pick a position, \"Yes!\" or \"No!\", and defend it.  Instead I went off and [deconstructed](/lw/no/how_an_algorithm_feels_from_inside/) the human algorithm for processing words, even going so far as to sketch an [illustration](/lw/nn/neural_categories/) of a neural network.  At the end, I hope, there was no question left—not even the feeling of a question.\n\nMany philosophers—particularly amateur philosophers, and ancient philosophers—share a dangerous instinct:  If you give them a question, they try to answer it.\n\nLike, say, \"Do we have free will?\"\n\nThe dangerous instinct of philosophy is to marshal the arguments in favor, and marshal the arguments against, and weigh them up, and publish them in a prestigious journal of philosophy, and so finally conclude:  \"Yes, we must have free will,\" or \"No, we cannot possibly have free will.\"\n\nSome philosophers are wise enough to recall the warning that most philosophical disputes are really disputes over the meaning of a word, or confusions generated by [using different meanings for the same word in different places](/lw/oc/variable_question_fallacies/).  So they try to define very precisely what they mean by \"free will\", and then ask again, \"Do we have free will?  Yes or no?\"\n\nA philosopher wiser yet, may suspect that the confusion about \"free will\" shows the notion itself is flawed.  So they pursue the Traditional Rationalist course:  They argue that \"free will\" is inherently self-contradictory, or meaningless because it has no [testable consequences](/lw/i3/making_beliefs_pay_rent_in_anticipated_experiences/).  And then they publish these devastating observations in a prestigious philosophy journal.\n\nBut _proving that_ you are confused may not make you feel any _less_ confused.  Proving that a question is meaningless may not help you any more than answering it.\n\nThe philosopher's instinct is to find the most defensible position, publish it, and move on.  But the \"naive\" view, the instinctive view, is a fact about human psychology.  You can prove that free will is impossible until the Sun goes cold, but this leaves an unexplained fact of cognitive science:  If free will doesn't exist, what goes on inside the head of a human being who thinks it does?  This is not a rhetorical question!\n\nIt is a fact about human psychology that people think they have free will.  Finding a more defensible _philosophical position_ doesn't change, or explain, that _psychological fact._  Philosophy may lead you to _reject_ the concept, but rejecting a concept is not the same as understanding the cognitive algorithms behind it.\n\nYou could look at the [Standard Dispute](/lw/np/disputing_definitions/) over \"If a tree falls in the forest, and no one hears it, does it make a sound?\", and you could do the Traditional Rationalist thing:  Observe that the two don't disagree on any point of [anticipated experience](/lw/i3/making_beliefs_pay_rent_in_anticipated_experiences/), and triumphantly declare the argument pointless.  That happens to be correct in this particular case; but, as _a question of cognitive science,_ why did the arguers make that mistake in the first place?\n\nThe key idea of the heuristics and biases program is that the _mistakes_ we make, often reveal far more about our underlying cognitive algorithms than our correct answers.  So (I asked myself, once upon a time) [what kind of mind design](/lw/nq/feel_the_meaning/) corresponds to the mistake of [arguing](/lw/np/disputing_definitions/) about trees falling in deserted forests?\n\nThe cognitive algorithms we use, _are_ [the way the world feels](/lw/no/how_an_algorithm_feels_from_inside/).  And these cognitive algorithms may not have a one-to-one correspondence with reality—not even macroscopic reality, to say nothing of the true quarks.  There can be things in the mind that cut skew to the world.\n\nFor example, there can be a [dangling unit](/lw/no/how_an_algorithm_feels_from_inside/) in the center of a [neural network](/lw/nn/neural_categories/), which does not correspond to any real thing, or any real property of any real thing, existent anywhere in the real world.  This dangling unit is often useful as a [shortcut in computation](/lw/o8/conditional_independence_and_naive_bayes/), which is why we have them.  (Metaphorically speaking.  Human neurobiology is surely far more [complex](/lw/o8/conditional_independence_and_naive_bayes/).)\n\nThis dangling unit _feels like_ an unresolved question, even after every answerable [query](/lw/nm/disguised_queries/) is answered.  No matter how much anyone proves to you that no difference of anticipated experience depends on the question, you're left wondering:  \"But does the falling tree _really_ make a sound, or not?\"\n\nBut once you understand _in detail_ how your brain generates the _feeling_ of the question—once you realize that your feeling of an unanswered question, corresponds to an illusory central unit wanting to know whether it should fire, even after all the edge units are clamped at known values—or better yet, you understand the technical workings of [Naive Bayes](/lw/o8/conditional_independence_and_naive_bayes/)—_then_ you're done.  Then there's no lingering feeling of confusion, no vague sense of dissatisfaction.\n\nIf there is _any_ lingering feeling of a remaining unanswered question, or of having been fast-talked into something, then this is a sign that you have not dissolved the question.  A [vague dissatisfaction](/lw/if/your_strength_as_a_rationalist/) should be as much warning as a shout.  _Really_ dissolving the question doesn't leave anything behind.\n\nA triumphant thundering refutation of free will, an absolutely unarguable proof that free will cannot exist, feels very _satisfying_—a [grand cheer](/lw/i6/professing_and_cheering/) for the [home team](/lw/mg/the_twoparty_swindle/).    And so you may not notice that—as a point of cognitive science—you do not have a full and satisfactory descriptive explanation of how each intuitive sensation arises, point by point.\n\nYou may not even want to admit your ignorance, of this point of cognitive science, because that would feel like a score against Your Team.  In the midst of smashing all foolish beliefs of free will, it would seem like a concession to the opposing side to concede that you've left anything unexplained.\n\nAnd so, perhaps, you'll come up with a [just-so evolutionary-psychological](/lw/mk/a_failed_justso_story/) argument that hunter-gatherers who believed in free will, were more likely to take a positive outlook on life, and so outreproduce other hunter-gatherers—to give one example of a completely bogus explanation.  If you say this, you are _arguing that_ the brain generates an illusion of free will—but you are not _explaining how._  You are trying to dismiss the opposition by deconstructing its motives—but in the story you tell, the illusion of free will is a brute fact.  You have not taken the illusion apart to see the wheels and gears.\n\nImagine that in the Standard Dispute about a tree falling in a deserted forest, you first prove that no difference of anticipation exists, and then go on to hypothesize, \"But perhaps people who said that arguments were meaningless were viewed as having conceded, and so lost social status, so now we have an instinct to argue about the meanings of words.\"  That's _arguing that_ or _explaining why_ a confusion exists.  Now look at the neural network structure in [Feel the Meaning](/lw/nq/feel_the_meaning/).  That's _explaining how_, disassembling the confusion into smaller pieces which are not themselves confusing.  See the difference?\n\nComing up with good hypotheses about cognitive algorithms (or even hypotheses that hold together for half a second) is a good deal harder than just refuting a philosophical confusion.  Indeed, it is an entirely different art.  Bear this in mind, and you should feel less embarrassed to say, \"I know that what you say can't possibly be true, and I can prove it.  But I cannot write out a flowchart which shows how your brain makes the mistake, so I'm not done yet, and will continue investigating.\"\n\nI say all this, because it sometimes seems to me that at least 20% of the real-world effectiveness of a skilled rationalist comes from [not stopping too early](/lw/jz/the_meditation_on_curiosity/).  If you keep asking questions, you'll get to your destination eventually.  If you decide too early that you've found an answer, you won't.\n\nThe challenge, above all, is to notice when you are confused—even if it just feels like a little tiny bit of confusion—and even if there's someone standing across from you, _insisting_ that humans have free will, and _smirking_ at you, and the fact that you don't know _exactly_ how the cognitive algorithms work, has _nothing to do_ with the searing folly of their position...\n\nBut when you can lay out the cognitive algorithm in sufficient detail that you can walk through the thought process, step by step, and describe how each intuitive perception arises—decompose the confusion into smaller pieces not themselves confusing—_then_ you're done.\n\nSo be warned that you may _believe_ you're done, when all you have is a mere triumphant [refutation of a mistake](/lw/lw/reversed_stupidity_is_not_intelligence/).\n\nBut when you're _really_ done, you'll _know_ you're done.[ ](/lw/gr/the_modesty_argument/)  Dissolving the question is an unmistakable feeling—once you experience it, and, having experienced it, resolve not to be fooled again.  [Those who dream do not know they dream, but when you wake you know you are awake.](/lw/gr/the_modesty_argument/)\n\nWhich is to say:  When you're done, you'll know you're done, but unfortunately the reverse implication does not hold.\n\nSo here's your homework problem:  What kind of cognitive algorithm, as felt from the inside, would generate the observed debate about \"free will\"?\n\nYour assignment is not to argue about whether people have free will, or not.\n\nYour assignment is not to argue that free will is compatible with determinism, or not.\n\nYour assignment is not to argue that the question is ill-posed, or that the concept is self-contradictory, or that it has no testable consequences.\n\nYou are not asked to invent an evolutionary explanation of how people who believed in free will would have reproduced; nor an account of how the concept of free will seems suspiciously congruent with bias X.  Such are mere attempts to _explain why_ people believe in \"free will\", not _explain how._\n\nYour homework assignment is to write a stack trace of the internal algorithms of the human mind as they produce the intuitions that power the whole damn philosophical argument.\n\nThis is one of the first real challenges I tried as an aspiring rationalist, once upon a time.  One of the easier conundrums, relatively speaking.  May it serve you likewise."
    },
    "voteCount": 85,
    "forceInclude": true
  },
  {
    "_id": "895quRDaK6gR2rM82",
    "url": null,
    "title": "Diseased thinking: dissolving questions about disease",
    "slug": "diseased-thinking-dissolving-questions-about-disease",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "Rationality"
      },
      {
        "name": "Health / Medicine / Disease"
      },
      {
        "name": "Philosophy of Language"
      },
      {
        "name": "Carving / Clustering Reality"
      },
      {
        "name": "Reversal Test"
      },
      {
        "name": "Motivational Intro Posts"
      }
    ],
    "tableOfContents": {
      "sections": [
        {
          "title": "What is Disease?",
          "anchor": "What_is_Disease_",
          "level": 1
        },
        {
          "title": "Hidden Inferences From Disease Concept",
          "anchor": "Hidden_Inferences_From_Disease_Concept",
          "level": 1
        },
        {
          "title": "Sympathy or Condemnation?",
          "anchor": "Sympathy_or_Condemnation_",
          "level": 1
        },
        {
          "title": "The Ethics of Treating Marginal Conditions",
          "anchor": "The_Ethics_of_Treating_Marginal_Conditions",
          "level": 1
        },
        {
          "title": "Summary",
          "anchor": "Summary",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        },
        {
          "anchor": "comments",
          "level": 0,
          "title": "354 comments"
        }
      ],
      "headingsCount": 7
    },
    "contents": {
      "markdown": "**Related to:** [Disguised Queries](https://www.lesswrong.com/lw/nm/disguised_queries/), [Words as Hidden Inferences](https://www.lesswrong.com/lw/ng/words_as_hidden_inferences/), [Dissolving the Question](https://www.lesswrong.com/lw/of/dissolving_the_question/), [Eight Short Studies on Excuses](https://www.lesswrong.com/lw/24o/eight_short_studies_on_excuses/)\n\n> _Today's therapeutic ethos, which celebrates curing and disparages judging, expresses the liberal disposition to assume that crime and other problematic behaviors reflect social or biological causation. While this absolves the individual of responsibility, it also strips the individual of personhood, and moral dignity_\n\n_\\-\\- George Will, [townhall.com](http://townhall.com/Common/PrintPage.aspx?g=761ecc84-473b-4123-bf28-c4fc179a9d3f&t=c)_\n\nSandy is a morbidly obese woman looking for advice.\n\nHer husband has no sympathy for her, and tells her she obviously needs to stop eating like a pig, and would it kill her to go to the gym once in a while?\n\nHer doctor tells her that obesity is primarily genetic, and recommends the diet pill orlistat and a consultation with a surgeon about gastric bypass.\n\nHer sister tells her that obesity is a perfectly valid lifestyle choice, and that fat-ism, equivalent to racism, is society's way of keeping her down.\n\nWhen she tells each of her friends about the opinions of the others, things really start to heat up.\n\nHer husband accuses her doctor and sister of absolving her of personal responsibility with feel-good platitudes that in the end will only prevent her from getting the willpower she needs to start a real diet.\n\nHer doctor accuses her husband of ignorance of the real causes of obesity and of the most effective treatments, and accuses her sister of legitimizing a dangerous health risk that could end with Sandy in hospital or even dead.\n\nHer sister accuses her husband of being a jerk, and her doctor of trying to medicalize her behavior in order to turn it into a \"condition\" that will keep her on pills for life and make lots of money for Big Pharma.\n\nSandy is fictional, but similar conversations happen every day, not only about obesity but about a host of other marginal conditions that some consider character flaws, others diseases, and still others normal variation in the human condition. Attention deficit disorder, internet addiction, social anxiety disorder (as one skeptic said, didn't we used to call this \"shyness\"?), alcoholism, chronic fatigue, oppositional defiant disorder (\"didn't we used to call this being a teenager?\"), compulsive gambling, homosexuality, Aspergers' syndrome, antisocial personality, even depression have all been placed in two or more of these categories by different people.\n\n  \n\nSandy's sister may have a point, but this post will concentrate on the debate between her husband and her doctor, with the understanding that the same techniques will apply to evaluating her sister's opinion. The disagreement between Sandy's husband and doctor centers around the idea of \"disease\". If obesity, depression, alcoholism, and the like are diseases, most people default to the doctor's point of view; if they are not diseases, they tend to agree with the husband.\n\nThe debate over such marginal conditions is in many ways a debate over whether or not they are \"real\" diseases. The usual surface level arguments trotted out in favor of or against the proposition are generally inconclusive, but this post will apply a host of techniques previously discussed on Less Wrong to illuminate the issue.\n\n  \n\n**What is Disease?**\n\n  \n\nIn [Disguised Queries](https://www.lesswrong.com/lw/nm/disguised_queries/) , Eliezer demonstrates how a word refers to a cluster of objects related upon multiple axes. For example, in a company that sorts red smooth translucent cubes full of vanadium from blue furry opaque eggs full of palladium, you might invent the word \"rube\" to designate the red cubes, and another \"blegg\", to designate the blue eggs. Both words are useful because they \"carve reality at the joints\" - they refer to two completely separate classes of things which it's practically useful to keep in separate categories. Calling something a \"blegg\" is a quick and easy way to describe its color, shape, opacity, texture, and chemical composition. It may be that the odd blegg might be purple rather than blue, but in general the characteristics of a blegg remain sufficiently correlated that \"blegg\" is a useful word. If they weren't so correlated - if blue objects were equally likely to be palladium-containing-cubes as vanadium-containing-eggs, then the word \"blegg\" would be a waste of breath; the characteristics of the object would remain just as mysterious to your partner after you said \"blegg\" as they were before.\n\n\"Disease\", like \"blegg\", suggests that certain characteristics always come together. A rough sketch of some of the characteristics we expect in a disease might include:\n\n1\\. Something caused by the sorts of thing you study in biology: proteins, bacteria, ions, viruses, genes.\n\n2\\. Something involuntary and completely immune to the operations of free will\n\n3\\. Something rare; the vast majority of people don't have it\n\n4\\. Something unpleasant; when you have it, you want to get rid of it\n\n5\\. Something discrete; a graph would show two widely separate populations, one with the disease and one without, and not a normal distribution.\n\n6\\. Something commonly treated with science-y interventions like chemicals and radiation.\n\nCancer satisfies every one of these criteria, and so we have no qualms whatsoever about classifying it as a disease. It's a type specimen, [the sparrow as opposed to the ostrich](https://www.lesswrong.com/lw/nl/the_cluster_structure_of_thingspace/). The same is true of heart attack, the flu, diabetes, and many more.\n\nSome conditions satisfy a few of the criteria, but not others. Dwarfism seems to fail (5), and it might get its status as a disease only after studies show that the supposed dwarf falls way out of normal human height variation. Despite the best efforts of transhumanists, it's hard to convince people that aging is a disease, partly because it fails (3). Calling homosexuality a disease is a poor choice for many reasons, but one of them is certainly (4): it's not necessarily unpleasant.\n\nThe marginal conditions mentioned above are also in this category. Obesity arguably sort-of-satisfies criteria (1), (4), and (6), but it would be pretty hard to make a case for (2), (3), and (5).\n\nSo, is obesity really a disease? Well, is Pluto really a planet? Once we state that obesity satisfies some of the criteria but not others, it is meaningless to talk about an additional fact of whether it \"really deserves to be a disease\" or not.\n\nIf it weren't for those pesky [hidden inferences](https://www.lesswrong.com/lw/ng/words_as_hidden_inferences/#more)...\n\n**Hidden Inferences From Disease Concept**\n\nThe state of the disease node, meaningless in itself, is used to predict several other nodes with non-empirical content. In English: we make value decisions based on whether we call something a \"disease\" or not.\n\nIf something is a real disease, the patient deserves our sympathy and support; for example, [cancer sufferers must universally be described as \"brave\"](http://www.theonion.com/articles/loved-ones-recall-local-mans-cowardly-battle-with,772/). If it is not a real disease, people are more likely to get our condemnation; for example Sandy's husband who calls her a \"pig\" for her inability to control her eating habits. The difference between \"shyness\" and \"social anxiety disorder\" is that people with the first get called \"weird\" and told to man up, and people with the second get special privileges and the sympathy of those around them.\n\nAnd if something is a real disease, it is socially acceptable (maybe even mandated) to seek medical treatment for it. If it's not a disease, medical treatment gets derided as a \"quick fix\" or an \"abdication of personal responsibility\". I have talked to several doctors who are uncomfortable suggesting gastric bypass surgery, even in people for whom it is medically indicated, because they believe it is morally wrong to turn to medicine to solve a character issue.\n\n![](http://res.cloudinary.com/lesswrong-2-0/image/upload/v1531709899/disease_first_tlnfio.gif)\n\nWhile a condition's status as a \"real disease\" ought to be meaningless as a \"hanging node\" after the status of all other nodes have been determined, it has acquired political and philosophical implications because of its role in determining whether patients receive sympathy and whether they are permitted to seek medical treatment.\n\nIf we can determine whether a person should get sympathy, and whether they should be allowed to seek medical treatment, independently of the central node \"disease\" or of the criteria that feed into it, we will have successfully unasked the question \"are these marginal conditions real diseases\" and cleared up the confusion.\n\n**Sympathy or Condemnation?**\n\nOur attitudes toward people with marginal conditions mainly reflect a deontologist libertarian (libertarian as in \"free will\", not as in \"against government\") model of blame. In this concept, people make decisions using their free will, a spiritual entity operating free from biology or circumstance. People who make good decisions are intrinsically good people and deserve good treatment; people who make bad decisions are intrinsically bad people and deserve bad treatment. But people who make bad decisions for reasons that are outside of their free will may not be intrinsically bad people, and may therefore be absolved from deserving bad treatment. For example, if a normally peaceful person has a brain tumor that affects areas involved in fear and aggression, they go on a crazy killing spree, and then they have their brain tumor removed and become a peaceful person again, many people would be willing to accept that the killing spree does not reflect negatively on them or open them up to deserving bad treatment, since it had biological and not spiritual causes.\n\nUnder this model, deciding whether a condition is biological or spiritual becomes very important, and the rationale for worrying over whether something \"is a real disease\" or not is plain to see. Without figuring out this extremely difficult question, we are at risk of either blaming people for things they don't deserve, or else letting them off the hook when they commit a sin, both of which, to libertarian deontologists, would be terrible things. But determining whether marginal conditions like depression have a spiritual or biological cause is difficult, and no one knows how to do it reliably.\n\nDeterminist consequentialists can do better. We believe it's biology all the way down. Separating spiritual from biological illnesses is impossible and unnecessary. Every condition, from brain tumors to poor taste in music, is \"biological\" insofar as it is encoded in things like cells and proteins and follows laws based on their structure.\n\nBut determinists don't just ignore the very important differences between brain tumors and poor taste in music. Some biological phenomena, like poor taste in music, are encoded in such a way that they are extremely vulnerable to what we can call social influences: praise, condemnation, introspection, and the like. Other biological phenomena, like brain tumors, are completely immune to such influences. This allows us to develop a more useful model of blame.\n\nThe consequentialist model of blame is very different from the deontological model. Because all actions are biologically determined, none are more or less metaphysically blameworthy than others, and none can mark anyone with the metaphysical status of \"bad person\" and make them \"deserve\" bad treatment. Consequentialists don't on a primary level want anyone to be treated badly, full stop; thus [is it written](http://yudkowsky.net/obsolete/tmol-faq.html#theo_free): \"Saddam Hussein doesn't deserve so much as a stubbed toe.\" But if consequentialists don't believe in punishment for its own sake, they do believe in punishment for the sake of, well, consequences. Hurting bank robbers may not be a good in and of itself, but it will prevent banks from being robbed in the future. And, one might infer, although alcoholics may not deserve condemnation, societal condemnation of alcoholics makes alcoholism a less attractive option.\n\nSo here, at last, is a rule for which diseases we offer sympathy, and which we offer condemnation: if giving condemnation instead of sympathy decreases the incidence of the disease enough to be worth the hurt feelings, condemn; otherwise, sympathize. Though the rule is based on philosophy that the majority of the human race would disavow, it leads to intuitively correct consequences. Yelling at a cancer patient, shouting \"How dare you allow your cells to divide in an uncontrolled manner like this; is that the way your mother raised you??!\" will probably make the patient feel pretty awful, but it's not going to cure the cancer. Telling a lazy person \"Get up and do some work, you worthless bum,\" very well might cure the laziness. The cancer is a biological condition immune to social influences; the laziness is a biological condition susceptible to social influences, so we try to socially influence the laziness and not the cancer.\n\nThe question \"Do the obese deserve our sympathy or our condemnation,\" then, is asking whether condemnation is such a useful treatment for obesity that its utility outweights the disutility of hurting obese people's feelings. This question may have different answers depending on the particular obese person involved, the particular person doing the condemning, and the availability of other methods for treating the obesity, which brings us to...\n\n**The Ethics of Treating Marginal Conditions**\n\nIf a condition is susceptible to social intervention, but an effective biological therapy for it also exists, is it okay for people to use the biological therapy instead of figuring out a social solution? My gut answer is \"Of course, why wouldn't it be?\", but apparently lots of people find this controversial for some reason.\n\nIn a libertarian deontological system, throwing biological solutions at spiritual problems might be disrespectful or dehumanizing, or a band-aid that doesn't affect the deeper problem. To someone who believes it's biology all the way down, this is much less of a concern.\n\nOthers complain that the existence of an easy medical solution prevents people from learning personal responsibility. But here [we see the status-quo bias at work, and so can apply a preference reversal test](http://www.nickbostrom.com/ethics/statusquo.pdf). If people really believe learning personal responsibility is more important than being not addicted to heroin, we would expect these people to support deliberately addicting schoolchildren to heroin so they can develop personal responsibility by coming off of it. Anyone who disagrees with this somewhat shocking proposal must believe, on some level, that having people who are not addicted to heroin is more important than having people develop whatever measure of personal responsibility comes from kicking their heroin habit the old-fashioned way.\n\nBut the most convincing explanation I have read for why so many people are opposed to medical solutions for social conditions is a signaling explanation by Robin Hans...wait! no!...by Katja Grace. On [her blog](http://meteuphoric.wordpress.com/2009/09/21/why-do-animal-lovers-want-animals-to-feel-pain/), she says:\n\n> _...the situation reminds me of a pattern in similar cases I have noticed before. It goes like this. Some people make personal sacrifices, supposedly toward solving problems that don’t threaten them personally. They sort recycling, buy free range eggs, buy fair trade, campaign for wealth redistribution etc. Their actions are seen as virtuous. They see those who don’t join them as uncaring and immoral. A more efficient solution to the problem is suggested. It does not require personal sacrifice. People who have not previously sacrificed support it. Those who have previously sacrificed object on grounds that it is an excuse for people to get out of making the sacrifice. The supposed instrumental action, as the visible sign of caring, has become virtuous in its own right. Solving the problem effectively is an attack on the moral people._\n\nA case in which some people eat less enjoyable foods and exercise hard to avoid becoming obese, and then campaign against a pill that makes avoiding obesity easy demonstrates some of the same principles.\n\nThere are several very reasonable objections to treating any condition with drugs, whether it be a classical disease like cancer or a marginal condition like alcoholism. The drugs can have side effects. They can be expensive. They can build dependence. They may later be found to be placebos whose efficacy was overhyped by dishonest pharmaceutical advertising.. They may raise ethical issues with children, the mentally incapacitated, and other people who cannot decide for themselves whether or not to take them. But these issues do not magically become more dangerous in conditions typically regarded as \"character flaws\" rather than \"diseases\", and the same good-enough solutions that work for cancer or heart disease will work for alcoholism and other such conditions (but see [here](https://www.lesswrong.com/lw/2as/diseased_thinking_dissolving_questions_about/230p)).\n\nI see no reason why people who want effective treatment for a condition should be denied it or stigmatized for seeking it, whether it is traditionally considered \"medical\" or not.\n\n**Summary**\n\nPeople commonly debate whether social and mental conditions are real diseases. This masquerades as a medical question, but its implications are mainly social and ethical. We use the concept of disease to decide who gets sympathy, who gets blame, and who gets treatment.\n\nInstead of continuing the fruitless \"disease\" argument, we should address these questions directly. Taking a determinist consequentialist position allows us to do so more effectively. We should blame and stigmatize people for conditions where blame and stigma are the most useful methods for curing or preventing the condition, and we should allow patients to seek treatment whenever it is available and effective."
    },
    "voteCount": 367,
    "forceInclude": true
  },
  {
    "_id": "2jp98zdLo898qExrr",
    "url": null,
    "title": "Hug the Query",
    "slug": "hug-the-query",
    "author": "Eliezer_Yudkowsky",
    "question": false,
    "tags": [
      {
        "name": "Anticipated Experiences"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "In the art of rationality there is a discipline of _closeness-to-the-issue_—trying to observe evidence that is as near to the original question as possible, so that it screens off as many other arguments as possible.\n\nThe Wright Brothers say, “My plane will fly.” If you look at their authority (bicycle mechanics who happen to be excellent amateur physicists) then you will compare their authority to, say, Lord Kelvin, and you will find that Lord Kelvin is the greater authority.\n\nIf you demand to see the Wright Brothers’ calculations, and you can follow them, and you demand to see Lord Kelvin’s calculations (he probably doesn’t have any apart from his own incredulity), then authority becomes much less relevant.\n\nIf you actually _watch the plane fly_, the calculations themselves become moot for many purposes, and Kelvin’s authority not even worth considering.\n\nThe more _directly_ your arguments bear on a question, without intermediate inferences—the closer the observed nodes are to the queried node, in the Great Web of Causality—the more powerful the evidence. It’s a theorem of these causal graphs that you can never get _more_ information from distant nodes, than from strictly closer nodes that screen off the distant ones.\n\nJerry Cleaver said: “What does you in is not failure to apply some high-level, intricate, complicated technique. It’s overlooking the basics. Not keeping your eye on the ball.”^[1](#fn1x21)^\n\nJust as it is superior to argue physics than credentials, it is also superior to argue physics than rationality. Who was more rational, the Wright Brothers or Lord Kelvin? If we can check their calculations, we don’t have to care! The virtue of a rationalist cannot _directly_ cause a plane to fly.\n\nIf you forget this principle, learning about more biases will hurt you, because it will distract you from more direct arguments. It’s all too easy to argue that someone is exhibiting Bias #182 in your repertoire of fully generic accusations, but you can’t _settle_ a factual issue without closer evidence. If there are biased reasons to say the Sun is shining, that doesn’t make it dark out.\n\nJust as you can’t always experiment today, you can’t always check the calculations today.^[2](#fn2x21)^ Sometimes you don’t know enough background material, sometimes there’s private information, sometimes there just isn’t time. There’s a sadly large number of times when it’s worthwhile to judge the speaker’s rationality. You should always do it with a hollow feeling in your heart, though, a sense that something’s missing.\n\nWhenever you can, dance as near to the original question as possible—press yourself up against it—get close enough to _hug the query!_\n\n^[1](#fn1x21-bk)^Jerry Cleaver, _Immediate Fiction: A Complete Writing Course_ (Macmillan, 2004).\n\n^[2](#fn2x21-bk)^See also “Is Molecular Nanotechnology ’Scientific’?” [http://lesswrong.com/lw/io/is\\_molecular\\_nanotechnology_scientific](http://lesswrong.com/lw/io/is_molecular_nanotechnology_scientific)."
    },
    "voteCount": 74,
    "forceInclude": true
  },
  {
    "_id": "kpRSCH7ALLcb6ucWM",
    "url": null,
    "title": "Say Not \"Complexity\"",
    "slug": "say-not-complexity",
    "author": "Eliezer Yudkowsky",
    "question": false,
    "tags": [
      {
        "name": "Anticipated Experiences"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "Once upon a time . . .\n\nThis is a story from when I first met Marcello, with whom I would later work for a year on AI theory; but at this point I had not yet accepted him as my apprentice. I knew that he competed at the national level in mathematical and computing olympiads, which sufficed to attract my attention for a closer look; but I didn’t know yet if he could learn to think about AI.\n\nI had asked Marcello to say how he thought an AI might discover how to solve a Rubik’s Cube. Not in a preprogrammed way, which is trivial, but rather how the AI itself might figure out the laws of the Rubik universe and reason out how to exploit them. How would an AI *invent for itself* the concept of an “operator,” or “macro,” which is the key to solving the Rubik’s Cube?\n\nAt some point in this discussion, Marcello said: “Well, I think the AI needs complexity to do X, and complexity to do Y—”\n\nAnd I said, “Don’t say ‘*complexity*.’ ”\n\nMarcello said, “Why not?”\n\nI said, “Complexity should never be a goal in itself. You may need to use a particular algorithm that adds some amount of complexity, but complexity for the sake of complexity just makes things harder.” (I was thinking of all the people whom I had heard advocating that the Internet would “wake up” and become an AI when it became “sufficiently complex.”)\n\nAnd Marcello said, “But there’s got to be *some* amount of complexity that does it.”\n\nI closed my eyes briefly, and tried to think of how to explain it all in words. To me, saying “complexity” simply *felt* like the wrong move in the AI dance. No one can think fast enough to deliberate, in words, about each sentence of their stream of consciousness; for that would require an infinite recursion. We think in words, but our stream of consciousness is steered below the level of words, by the trained-in remnants of past insights and harsh experience . . .\n\nI said, “Did you read ‘A Technical Explanation of Technical Explanation’?”^1^\n\n“Yes,” said Marcello.\n\n“Okay,” I said. “Saying ‘complexity’ doesn’t concentrate your probability mass.”\n\n“Oh,” Marcello said, “like ‘emergence.’ Huh. So . . . now I’ve got to think about how X might actually happen . . .”\n\nThat was when I thought to myself, “*Maybe **this** one is teachable.*”\n\nComplexity is not a useless concept. It has mathematical definitions attached to it, such as Kolmogorov complexity and Vapnik-Chervonenkis complexity. Even on an intuitive level, complexity is often worth thinking about—you have to judge the complexity of a hypothesis and decide if it’s “too complicated” given the supporting evidence, or look at a design and try to make it simpler.\n\nBut concepts are not useful or useless of themselves. Only *usages* are correct or incorrect. In the step Marcello was trying to take in the dance, he was trying to explain something for free, get something for nothing. It is an extremely common misstep, at least in my field. You can join a discussion on artificial general intelligence and watch people doing the same thing, left and right, over and over again—constantly skipping over things they don’t understand, without realizing that’s what they’re doing.\n\nIn an eyeblink it happens: putting a non-controlling causal node behind something mysterious, a causal node that feels like an explanation but isn’t. The mistake takes place below the level of words. It requires no special character flaw; it is how human beings think by default, how they have thought since the ancient times.\n\nWhat you must avoid is *skipping over the mysterious part*; you must linger at the mystery to confront it directly. There are many words that can skip over mysteries, and some of them would be legitimate in other contexts—“complexity,” for example. But the essential mistake is that *skip-over*, regardless of what causal node goes behind it. The skip-over is not a thought, but a microthought. You have to pay close attention to catch yourself at it. And when you train yourself to avoid skipping, it will become a matter of instinct, not verbal reasoning. You have to *feel* which parts of your map are still blank, and more importantly, pay attention to that feeling.\n\nI suspect that in academia there is a huge pressure to sweep problems under the rug so that you can present a paper with the appearance of completeness. You’ll get more kudos for a seemingly complete model that includes some “emergent phenomena,” versus an explicitly incomplete map where the label says “I got no clue how this part works” or “then a miracle occurs.” A journal may not even accept the latter paper, since who knows but that the unknown steps are really where everything interesting happens?^2^\n\nAnd if you’re working on a revolutionary AI startup, there is an even huger pressure to sweep problems under the rug; or you will have to admit to yourself that you don’t know how to build the right kind of AI yet, and your current life plans will come crashing down in ruins around your ears. But perhaps I am [over-explaining](https://lesswrong.com/rationality/correspondence-bias), since skip-over happens by default in humans. If you’re looking for examples, just watch people discussing religion or philosophy or spirituality or any science in which they were not professionally trained.\n\nMarcello and I developed a convention in our AI work: when we ran into something we didn’t understand, which was often, we would say “magic”—as in, X magically does Y”—to remind ourselves that *here was an unsolved problem, a gap in our understanding*. It is far better to say “magic” than “complexity” or “emergence”; the latter words create an illusion of understanding. Wiser to say “magic,” and leave yourself a placeholder, a reminder of work you will have to do later.\n\n[^1^](#fn1x40-bk) [http://lesswrong.com/rationality/a-technical-explanation-of-technical-explanation](http://lesswrong.com/rationality/a-technical-explanation-of-technical-explanation)\n\n[^2^](#fn2x40-bk) And yes, it sometimes happens that all the non-magical parts of your map turn out to also be non-important. That’s the price you sometimes pay, for entering into terra incognita and trying to solve problems *incrementally*. But that makes it even *more* important to *know* when you aren’t finished yet. Mostly, people don’t dare to enter terra incognita at all, for the deadly fear of wasting their time."
    },
    "voteCount": 61,
    "forceInclude": true
  },
  {
    "_id": "ZTRiSNmeGQK8AkdN2",
    "url": null,
    "title": "Mind Projection Fallacy",
    "slug": "mind-projection-fallacy",
    "author": "Eliezer_Yudkowsky",
    "question": false,
    "tags": [
      {
        "name": "Rationality"
      },
      {
        "name": "Fallacies"
      },
      {
        "name": "Mind Projection Fallacy"
      },
      {
        "name": "Map and Territory"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "[![Monsterwithgirl_2](/static/imported/2007/08/10/monsterwithgirl_2.jpg \"Monsterwithgirl_2\")](/static/imported/2007/08/10/monsterwithgirl_2.jpg)In the dawn days of science fiction, alien invaders would occasionally kidnap a girl in a torn dress and carry her off for intended ravishing, as lovingly depicted on many ancient magazine covers.  Oddly enough, the aliens never go after men in torn shirts.\n\nWould a non-humanoid alien, with a different evolutionary history and [evolutionary psychology](/lw/l1/evolutionary_psychology/), sexually desire a human female?  It seems rather unlikely.  To put it mildly.\n\nPeople don't make mistakes like that by deliberately reasoning:  \"All possible minds are likely to be wired pretty much the same way, therefore a bug-eyed monster will find human females attractive.\"  Probably the artist did not even think to ask whether an alien _perceives_ human females as attractive.  Instead, a human female in a torn dress _is sexy_—inherently so, as an intrinsic property.\n\nThey who went astray did not think about the alien's evolutionary history; they focused on the woman's torn dress.  If the dress were not torn, the woman would be less sexy; the alien monster doesn't enter into it.\n\nApparently we instinctively represent Sexiness as a direct attribute of the Woman object, Woman.sexiness, like Woman.height or Woman.weight.\n\nIf your brain uses that data structure, or something metaphorically similar to it, then [from the inside](/lw/no/how_an_algorithm_feels_from_inside/) it feels like sexiness is an inherent property of the woman, not a property of the alien looking at the woman.  Since the woman _is attractive,_ the alien monster will be _attracted_ to her—isn't that logical?\n\nE. T. Jaynes used the term [Mind Projection Fallacy](http://citeseer.ist.psu.edu/6330.html) to denote the error of projecting your own mind's properties into the external world.  Jaynes, as a late grand master of the Bayesian Conspiracy, was most concerned with the mistreatment of _probabilities_ as inherent properties of objects, rather than states of partial knowledge in some particular mind.  More about this shortly.\n\nBut the Mind Projection Fallacy generalizes as an error.  It is in the argument over [the real meaning of the word sound](/lw/np/disputing_definitions/), and in the magazine cover of the monster carrying off a woman in the torn dress, and Kant's declaration that space by its very nature is flat, and Hume's definition of [a priori](/lw/k2/a_priori/) ideas as those \"discoverable by the mere operation of thought, without dependence on what is anywhere existent in the universe\"...\n\n(Incidentally, I once read an SF story about a human male who entered into a sexual relationship with a sentient alien plant of appropriately squishy fronds; discovered that it was an [androecious](http://en.wikipedia.org/wiki/Plant_sexuality) (male) plant; agonized about this for a bit; and finally decided that it didn't really matter at that point.  And in Foglio and Pollotta's _Illegal Aliens,_ the humans land on a planet inhabited by sentient insects, and see a movie advertisement showing a human carrying off a bug in a delicate chiffon dress.  Just thought I'd mention that.)"
    },
    "voteCount": 58,
    "forceInclude": true
  },
  {
    "_id": "yA4gF5KrboK2m2Xu7",
    "url": null,
    "title": "How An Algorithm Feels From Inside",
    "slug": "how-an-algorithm-feels-from-inside",
    "author": "Eliezer_Yudkowsky",
    "question": false,
    "tags": [
      {
        "name": "Rationality"
      },
      {
        "name": "Philosophy of Language"
      },
      {
        "name": "Cognitive Reduction"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "\"If a tree falls in the forest, and no one hears it, does it make a sound?\"  I remember seeing an actual argument get started on this subject—a fully naive argument that went nowhere near Berkeleyan subjectivism.  Just:\n\n> \"It makes a sound, just like any other falling tree!\"  \n> \"But how can there be a sound that no one hears?\"\n\nThe standard rationalist view would be that the first person is speaking as if \"sound\" means acoustic vibrations in the air; the second person is speaking as if \"sound\" means an auditory experience in a brain.  If you ask \"Are there acoustic vibrations?\" or \"Are there auditory experiences?\", the answer is at once obvious.  And so the argument is really about the definition of the word \"sound\".\n\nI think the standard analysis is essentially correct.  So let's accept that as a premise, and ask:  Why do people get into such an argument?  What's the underlying psychology?\n\nA key idea of the heuristics and biases program is that mistakes are often more revealing of cognition than correct answers.  Getting into a heated dispute about whether, if a tree falls in a deserted forest, it makes a sound, is traditionally considered a mistake.\n\nSo what kind of mind design corresponds to that error?\n\nIn [Disguised Queries](/lw/nm/disguised_queries/) I introduced the blegg/rube classification task, in which Susan the Senior Sorter explains that your job is to sort objects coming off a conveyor belt, putting the blue eggs or \"bleggs\" into one bin, and the red cubes or \"rubes\" into the rube bin.  This, it turns out, is because bleggs contain small nuggets of vanadium ore, and rubes contain small shreds of palladium, both of which are useful industrially.\n\nExcept that around 2% of blue egg-shaped objects contain palladium instead.  So if you find a blue egg-shaped thing that contains palladium, should you call it a \"rube\" instead?  You're going to put it in the rube bin—why not call it a \"rube\"?\n\nBut when you switch off the light, nearly all bleggs glow faintly in the dark.  And blue egg-shaped objects that contain palladium are just as likely to glow in the dark as any other blue egg-shaped object.\n\nSo if you find a blue egg-shaped object that contains palladium, and you ask \"Is it a blegg?\", the answer depends on what you have to do with the answer:  If you ask \"Which bin does the object go in?\", then you choose as if the object is a rube.  But if you ask \"If I turn off the light, will it glow?\", you predict as if the object is a blegg.  In one case, the question \"Is it a blegg?\" stands in for the [disguised query](/lw/nm/disguised_queries/), \"Which bin does it go in?\".  In the other case, the question \"Is it a blegg?\" stands in for the [disguised query](/lw/nm/disguised_queries/), \"Will it glow in the dark?\"\n\nNow suppose that you have an object that is blue and egg-shaped and contains palladium; and you have already observed that it is furred, flexible, opaque, and glows in the dark.\n\nThis answers _every_ query, observes every observable introduced.  There's nothing left for a disguised query to stand _for._\n\nSo why might someone feel an impulse to go on arguing whether the object is _really_ a blegg?\n\n[![Blegg3](/static/imported/2008/02/10/blegg3.png \"Blegg3\")](/static/imported/2008/02/10/blegg3.png)\n\nThis diagram from [Neural Categories](/lw/nn/neural_categories/) shows two different neural networks that might be used to answer questions about bleggs and rubes.  Network 1 has a number of disadvantages—such as potentially oscillating/chaotic behavior, or requiring O(N^2^) connections—but Network 1's structure does have one major advantage over Network 2:  Every unit in the network corresponds to a testable query.  If you observe every observable, clamping every value, there are no units in the network left over.\n\nNetwork 2, however, is a far better candidate for being something vaguely like how the human brain works:  It's fast, cheap, scalable—and has an extra dangling unit in the center, whose activation can still vary, even after we've observed every single one of the surrounding nodes.\n\nWhich is to say that even after you know whether an object is blue or red, egg or cube, furred or smooth, bright or dark, and whether it contains vanadium or palladium, it _feels_ like there's a leftover, unanswered question:  _But is it really a blegg?_\n\nUsually, in our daily experience, acoustic vibrations and auditory experience go together.  But a tree falling in a deserted forest unbundles this common association.  And even after you know that the falling tree creates acoustic vibrations but not auditory experience, it _feels_ like there's a leftover question:  _Did it make a sound?_  \n  \nWe know where Pluto is, and where it's going; we know Pluto's shape, and Pluto's mass—but is it a planet?\n\nNow remember:  When you look at Network 2, as I've laid it out here, you're seeing the algorithm from the outside.  People don't think to themselves, \"Should the central unit fire, or not?\" any more than you think \"Should neuron #12,234,320,242 in my visual cortex fire, or not?\"\n\nIt takes a deliberate effort to visualize your brain from the outside—and then you still don't see your actual brain; you imagine what you _think_ is there, hopefully based on science, but regardless, you don't have any direct access to neural network structures from introspection.  That's why the ancient Greeks didn't invent computational neuroscience.\n\nWhen you look at Network 2, you are seeing from the _outside;_ but the way that neural network structure feels from the _inside,_ if you yourself _are_ a brain running that algorithm, is that even after you know every characteristic of the object, you still find yourself wondering:  \"But is it a blegg, or not?\"\n\nThis is a great gap to cross, and I've seen it stop people in their tracks.  Because we don't instinctively see our intuitions as \"intuitions\", we just see them as the world.  When you look at a green cup, you don't think of yourself as seeing a picture reconstructed in your visual cortex—although that _is_ what you are seeing—you just see a green cup.  You think, \"Why, look, this cup is green,\" not, \"The picture in my visual cortex of this cup is green.\"\n\nAnd in the same way, when people argue over whether the falling tree makes a sound, or whether Pluto is a planet, they don't see themselves as arguing over whether a categorization should be active in their neural networks.  It seems like either the tree makes a sound, or not.\n\nWe know where Pluto is, and where it's going; we know Pluto's shape, and Pluto's mass—but is it a planet?  And yes, there were people who said this was a fight over definitions—but even that is a Network 2 sort of perspective, because you're arguing about how the central unit ought to be wired up.  If you were a mind constructed along the lines of Network 1, you wouldn't say \"It depends on how you define 'planet',\" you would just say, \"Given that we know Pluto's orbit and shape and mass, there is no question left to ask.\"  Or, rather, that's how it would _feel_—it would _feel_ like there was no question left—if you were a mind constructed along the lines of Network 1.\n\nBefore you can question your intuitions, you have to realize that what your mind's eye is looking at _is_ an intuition—some cognitive algorithm, as seen from the inside—rather than a direct perception of the Way Things Really Are.\n\nPeople [cling to their intuitions](/lw/n1/allais_malaise/), I think, not so much because they believe their cognitive algorithms are perfectly reliable, but because they can't see their intuitions _as the way their cognitive algorithms happen to look from the inside._\n\nAnd so everything you try to say about how the native cognitive algorithm goes astray, ends up being contrasted to their direct perception of the Way Things Really Are—and discarded as obviously wrong."
    },
    "voteCount": 179,
    "forceInclude": true
  },
  {
    "_id": "HLqWn5LASfhhArZ7w",
    "url": null,
    "title": "Expecting Short Inferential Distances",
    "slug": "expecting-short-inferential-distances",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "Inferential Distance"
      },
      {
        "name": "Rationality"
      },
      {
        "name": "Evolutionary Psychology"
      },
      {
        "name": "Conversation (topic)"
      },
      {
        "name": "Public Discourse"
      },
      {
        "name": "Illusion of Transparency"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "_Homo sapiens_’s environment of evolutionary adaptedness (a.k.a. EEA or “ancestral environment”) consisted of hunter-gatherer bands of at most [200 people](https://en.wikipedia.org/wiki/Dunbar%27s_number), with no writing. All inherited knowledge was passed down by speech and memory.\n\nIn a world like that, all background knowledge is universal knowledge. All information not strictly private is public, period.\n\nIn the ancestral environment, you were unlikely to end up more than _one inferential step_ away from anyone else. When you discover a new oasis, you don’t have to explain to your fellow tribe members what an oasis is, or why it’s a good idea to drink water, or how to walk. Only you know where the oasis lies; this is private knowledge. But everyone has the background to understand your description of the oasis, the concepts needed to think about water; this is universal knowledge. When you explain things in an ancestral environment, you almost _never_ have to explain your concepts. At most you have to explain _one_ new concept, not two or more simultaneously.\n\nIn the ancestral environment there were no abstract disciplines with vast bodies of carefully gathered evidence generalized into elegant theories transmitted by written books whose conclusions are _a hundred inferential steps removed_ from universally shared background premises.\n\nIn the ancestral environment, anyone who says something with no obvious support is a liar or an idiot. You’re not likely to think, “Hey, maybe this person has well-supported background knowledge that no one in my band has even heard of,” because it was a reliable invariant of the ancestral environment that this didn’t happen.\n\nConversely, if you say something blatantly obvious and the other person doesn’t see it, _they’re_ the idiot, or they’re being deliberately obstinate to annoy you.\n\nAnd to top it off, if someone says something with no obvious support and _expects_ you to believe it—acting all indignant when you don’t—then they must be _crazy._\n\nCombined with the illusion of transparency and [self-anchoring](http://lesswrong.com/lw/kf/selfanchoring/) (the tendency to model other minds as though the were slightly modified versions of oneself), I think this explains a _lot_ about the legendary difficulty most scientists have in communicating with a lay audience—or even communicating with scientists from other disciplines. When I observe failures of explanation, I usually see the explainer taking _one_ step back, when they need to take two or more steps back. Or listeners assume that things should be visible in one step, when they take two or more steps to explain. Both sides act as if they expect very short inferential distances from universal knowledge to any new knowledge.\n\nA biologist, speaking to a physicist, can justify evolution by saying it is the simplest explanation. But not everyone on Earth has been inculcated with that legendary history of science, from Newton to Einstein, which invests the phrase “simplest explanation” with its awesome import: a Word of Power, spoken at the birth of theories and carved on their tombstones. To someone else, “But it’s the simplest explanation!” may sound like an interesting but hardly knockdown argument; it doesn’t feel like all that powerful a tool for comprehending office politics or fixing a broken car. Obviously the biologist is infatuated with their own ideas, too arrogant to be open to alternative explanations which sound just as plausible. (If it sounds plausible to me, it should sound plausible to any sane member of my band.)\n\nAnd from the biologist’s perspective, they can understand how evolution might sound a little odd at first—but when someone rejects evolution even after the biologist explains that it’s the simplest explanation, well, it’s clear that nonscientists are just idiots and there’s no point in talking to them.\n\nA clear argument has to lay out an inferential _pathway_, starting from what the audience _already knows or accepts_. If you don’t recurse far enough, you’re just talking to yourself.\n\nIf at any point you make a statement without obvious justification in arguments you’ve previously supported, the audience just thinks you’re crazy.\n\nThis also happens when you allow yourself to be seen _visibly_ attaching greater weight to an argument than is justified in the eyes of the audience _at that time_. For example, talking as if you think “simpler explanation” is a knockdown argument for evolution (which it is), rather than a sorta-interesting idea (which it sounds like to someone who hasn’t been raised to revere Occam’s Razor).\n\nOh, and you’d better not drop any hints that _you_ think you’re working a dozen inferential steps away from what the audience knows, or that _you_ think you have special background knowledge not available to them. The audience doesn’t know anything about an evolutionary-psychological argument for a cognitive bias to underestimate inferential distances leading to traffic jams in communication. They’ll just think you’re condescending.\n\nAnd if you think you can explain the concept of “systematically underestimated inferential distances” briefly, in just a few words, I’ve got some sad news for you . . ."
    },
    "voteCount": 221,
    "forceInclude": true
  },
  {
    "_id": "sSqoEw9eRP2kPKLCz",
    "url": null,
    "title": "Illusion of Transparency:  Why No One Understands You",
    "slug": "illusion-of-transparency-why-no-one-understands-you",
    "author": "Eliezer Yudkowsky",
    "question": false,
    "tags": [
      {
        "name": "Conversation (topic)"
      },
      {
        "name": "Inferential Distance"
      },
      {
        "name": "Calibration"
      },
      {
        "name": "Illusion of Transparency"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "In hindsight bias, people who know the outcome of a situation believe the outcome should have been easy to predict in advance. Knowing the outcome, we reinterpret the situation in light of that outcome. Even when warned, we can’t de-interpret to empathize with someone who doesn’t know what we know.\n\nClosely related is the *illusion of transparency*: We always know what *we* mean by our words, and so we expect others to know it too. Reading our own writing, the intended interpretation falls easily into place, guided by our knowledge of what we really meant. It’s hard to empathize with someone who must interpret blindly, guided only by the words.\n\nJune recommends a restaurant to Mark; Mark dines there and discovers (a) unimpressive food and mediocre service or (b) delicious food and impeccable service. Then Mark leaves the following message on June’s answering machine: “June, I just finished dinner at the restaurant you recommended, and I must say, it was marvelous, just marvelous.” Keysar (1994) presented a group of subjects with scenario (a), and 59% thought that Mark’s message was sarcastic *and that Jane would perceive the sarcasm*.^1^ Among other subjects, told scenario (b), only 3% thought that Jane would perceive Mark’s message as sarcastic. Keysar and Barr (2002) seem to indicate that an actual voice message was played back to the subjects.^2^ Keysar (1998) showed that if subjects were told that the restaurant was horrible *but that Mark wanted to conceal his response*, they believed June would not perceive sarcasm in the (same) message:^3^\n\n> They were just as likely to predict that she would perceive sarcasm when he attempted to conceal his negative experience as when he had a positive experience and was truly sincere. So participants took Mark’s *communicative intention* as transparent. It was as if they assumed that June would perceive whatever intention Mark wanted her to perceive.^4^\n\n“The goose hangs high” is an archaic English idiom that has passed out of use in modern language. Keysar and Bly (1995) told one group of subjects that “the goose hangs high” meant that the future looks good; another group of subjects learned that “the goose hangs high” meant the future looks gloomy.^5^ Subjects were then asked which of these two meanings an *uninformed* listener would be more likely to attribute to the idiom. Each group thought that listeners would perceive the meaning presented as “standard.”^6^\n\nKeysar and Henly (2002) tested the calibration of speakers: Would speakers underestimate, overestimate, or correctly estimate how often listeners understood them?^7^ Speakers were given ambiguous sentences (“The man is chasing a woman on a bicycle.”) and disambiguating pictures (a man running after a cycling woman). Speakers were then asked to utter the words in front of addressees, and asked to estimate how many addressees understood the intended meaning. Speakers thought that they were understood in 72% of cases and were actually understood in 61% of cases. When addressees did not understand, speakers thought they did in 46% of cases; when addressees did understand, speakers thought they did not in only 12% of cases.\n\nAdditional subjects who *overheard* the explanation showed no such bias, expecting listeners to understand in only 56% of cases.\n\nAs Keysar and Barr note, two days before Germany’s attack on Poland, Chamberlain sent a letter intended to make it clear that Britain would fight if any invasion occurred. The letter, phrased in polite diplomatese, was heard by Hitler as conciliatory—and the tanks rolled.\n\nBe not too quick to blame those who misunderstand your perfectly clear sentences, spoken or written. Chances are, your words are more ambiguous than you think.\n\n* * *\n\n^1^ Boaz Keysar, “The Illusory Transparency of Intention: Linguistic Perspective Taking in Text,” *Cognitive Psychology* 26 (2 1994): 165–208.\n\n^2^ Boaz Keysar and Dale J. Barr, “Self-Anchoring in Conversation: Why Language Users Do Not Do What They ‘Should,’” in *Heuristics and Biases: The Psychology of Intuitive Judgment*, ed. Griffin Gilovich and Daniel Kahneman (New York: Cambridge University Press, 2002), 150–166.\n\n^3^ Boaz Keysar, “Language Users as Problem Solvers: Just What Ambiguity Problem Do They Solve?,” in *Social and Cognitive Approaches to Interpersonal Communication*, ed. Susan R. Fussell and Roger J. Kreuz (Mahwah, NJ: Lawrence Erlbaum Associates, 1998), 175–200.\n\n^4^ The wording here is from Keysar and Barr.\n\n^5^ Boaz Keysar and Bridget Bly, “Intuitions of the Transparency of Idioms: Can One Keep a Secret by Spilling the Beans?,” *Journal of Memory and Language* 34 (1 1995): 89–109.\n\n^6^ Other idioms tested included “come the uncle over someone,” “to go by the board,” and “to lay out in lavender.” Ah, English, such a lovely language.\n\n^7^ Boaz Keysar and Anne S. Henly, “Speakers’ Overestimation of Their Effectiveness,” *Psychological Science* 13 (3 2002): 207–212."
    },
    "voteCount": 120,
    "forceInclude": true
  },
  {
    "_id": "wzxneh7wxkdNYNbtB",
    "url": null,
    "title": "When Science Can't Help",
    "slug": "when-science-can-t-help",
    "author": "Eliezer_Yudkowsky",
    "question": false,
    "tags": [
      {
        "name": "Practice & Philosophy of Science"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "[Once upon a time, a younger Eliezer had a stupid theory](/lw/iy/my_wild_and_reckless_youth/).  Let's say that Eliezer~18~'s stupid theory was that consciousness was caused by closed timelike curves hiding in quantum gravity.  This isn't the whole story, not even close, but it will do for a start.\n\nAnd there came a point where I looked back, and realized:\n\n1.  I had carefully followed everything I'd been told was Traditionally Rational, in the course of going astray.  For example, I'd been careful to only believe in stupid theories that made novel experimental predictions, e.g., that neuronal microtubules would be found to support coherent quantum states.\n2.  Science would have been perfectly fine with my spending ten years trying to test my stupid theory, only to get a negative experimental result, so long as I then said, \"Oh, well, I guess my theory was wrong.\"\n\nFrom Science's perspective, that is how things are _supposed_ to work—happy fun for everyone.  You admitted your error!  Good for you!  Isn't that what Science is all about?\n\nBut what if I didn't want to waste ten years?\n\nWell... Science didn't have much to say about _that._  How could Science say which theory was right, in _advance_ of the experimental test?  Science doesn't care where your theory comes from—it just says, \"Go test it.\"\n\nThis is the great strength of Science, and also its great weakness.\n\n[Gray Area asked](/lw/qb/science_doesnt_trust_your_rationality/k0k):\n\n> Eliezer, why are you concerned with untestable questions?\n\nBecause questions that are _easily immediately_ tested are hard for Science to get wrong.\n\nI mean, sure, when there's already definite unmistakable experimental evidence available, go with it.  Why on Earth wouldn't you?\n\nBut sometimes a question will have very large, very definite experimental consequences in your future—but you can't easily test it experimentally _right now_—and yet there _is_ a strong _rational_ argument.\n\nMacroscopic quantum superpositions are readily testable:  It would just take nanotechnologic precision, very low temperatures, and a nice clear area of interstellar space.  Oh, sure, you can't do it _right now_, because it's _too expensive_ or _impossible for today's technology_ or something like that—but in theory, sure!  Why, maybe someday they'll run whole civilizations on macroscopically superposed quantum computers, way out in a well-swept volume of a Great Void.  (Asking what quantum non-realism says about the status of any observers inside these computers, helps to reveal the underspecification of quantum non-realism.)\n\nThis doesn't seem immediately pragmatically relevant to your life, I'm guessing, but it establishes the pattern:  Not everything with future consequences is _cheap_ to test _now_.\n\nEvolutionary psychology is another example of a case where rationality has to take over from science.  While theories of evolutionary psychology form a connected whole, only some of those theories are readily testable experimentally.  But you still need the other parts of the theory, because they form a connected web that helps you to form the hypotheses that are actually testable—and then the helper hypotheses are supported in a Bayesian sense, but not supported experimentally.  Science would render a verdict of \"not proven\" on individual parts of a connected theoretical mesh that is experimentally productive as a whole.  We'd need a new kind of verdict for that, something like \"indirectly supported\".\n\nOr what about cryonics?\n\nCryonics is an archetypal example of an extremely important issue (150,000 people die per day) that will have huge consequences in the foreseeable future, but doesn't offer definite unmistakable experimental evidence that we can get _right now._\n\nSo do you say, \"I don't believe in cryonics because it hasn't been experimentally proven, and you shouldn't believe in things that haven't been experimentally proven?\"\n\nWell, from a Bayesian perspective, that's incorrect.  [Absence of evidence is evidence of absence](/lw/ih/absence_of_evidence_is_evidence_of_absence/) only to the degree that we could reasonably expect the evidence to appear.  If someone is trumpeting that snake oil cures cancer, you can reasonably expect that, _if the snake oil was actually curing cancer,_ some scientist would be performing a controlled study to verify it—that, at the least, doctors would be reporting case studies of amazing recoveries—and so the absence of this evidence is strong evidence of absence.  But \"gaps in the fossil record\" are not strong evidence against evolution; fossils form only rarely, and _even if an intermediate species did in fact exist,_ you cannot expect with high probability that Nature will obligingly fossilize it and that the fossil will be discovered.\n\nReviving a cryonically frozen mammal is just not something you'd expect to be able to do with modern technology, _even if future nanotechnologies could in fact perform a successful revival_.  That's how I see Bayes seeing it.\n\nOh, and as for the actual arguments _for_ cryonics—I'm not going to go into those at the moment.  But if you followed the [physics and anti-Zombie sequences](/lw/pm/identity_isnt_in_specific_atoms/), it should now seem a lot more plausible, that whatever preserves the pattern of synapses, preserves as much of \"you\" as is preserved from one night's sleep to morning's waking.\n\nNow, to be fair, someone who says, \"I don't believe in cryonics because it hasn't been proven experimentally\" is _misapplying_ the rules of Science; this is not a case where science actually gives the _wrong answer._  In the absence of a definite experimental test, the verdict of science here is \"Not proven\".  Anyone who interprets that as a rejection is taking an extra step outside of science, not a misstep within science.\n\n[John McCarthy's Wikiquotes page](http://en.wikiquote.org/wiki/John_McCarthy) has him saying, \"Your statements amount to saying that if AI is possible, it should be easy. Why is that?\"  The Wikiquotes page doesn't say what McCarthy was responding to, but I could venture a guess.\n\nThe general mistake probably arises because there _are_ cases where the absence of scientific proof is strong evidence—because an experiment would be readily performable, and so failure to perform it is itself suspicious.  (Though not as suspicious as I used to think—with all the strangely varied anecdotal evidence coming in from respected sources, why the _hell_ isn't anyone testing [Seth Roberts's theory of appetite suppression](http://sethroberts.net/about/whatmakesfoodfattening.pdf)?)\n\nAnother confusion factor may be that if you test Pharmaceutical X on 1000 subjects and find that 56% of the control group and 57% of the experimental group recover, some people will call that a verdict of \"Not proven\".  I would call it an experimental verdict of \"Pharmaceutical X doesn't work well, if at all\".  Just because this verdict is theoretically retractable in the face of new evidence, doesn't make it ambiguous.\n\nIn any case, right now you've got people dismissing cryonics out of hand as \"not scientific\", like it was some kind of pharmaceutical you could easily administer to 1000 patients and see what happened.  \"Call me when cryonicists actually revive someone,\" they say; which, as Mike Li observes, is like saying \"I refuse to get into this ambulance; call me when it's actually at the hospital\".  Maybe Martin Gardner warned them against believing in strange things without experimental evidence.  So they wait for the definite unmistakable verdict of Science, while their family and friends and 150,000 people per day are dying _right now,_ and might or might not be savable—\n\n—a calculated bet you could only make _rationally._\n\nThe drive of Science is to obtain a mountain of evidence so huge that not even fallible human scientists can misread it.  But even _that_ sometimes goes wrong, when people become confused about which theory predicts what, or bake extremely-hard-to-test components into an early version of their theory.  And sometimes you just can't get clear experimental evidence at all.\n\nEither way, you have to try to do the thing that Science [doesn't trust anyone to do](/lw/qb/science_doesnt_trust_your_rationality/)—think rationally, and figure out the answer _before_ you get clubbed over the head with it.\n\n(Oh, and sometimes a _disconfirming_ experimental result looks like:  \"[Your entire species has just been wiped out!](http://www.global-catastrophic-risks.com/)  You are now scientifically required to relinquish your theory.  If you publicly recant, good for you!  Remember, it takes a strong mind to give up strongly held beliefs.  Feel free to try another hypothesis next time!\")"
    },
    "voteCount": 33,
    "forceInclude": true
  },
  {
    "_id": "xTyuQ3cgsPjifr7oj",
    "url": null,
    "title": "Faster Than Science",
    "slug": "faster-than-science",
    "author": "Eliezer_Yudkowsky",
    "question": false,
    "tags": [
      {
        "name": "Practice & Philosophy of Science"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "I sometimes say that the method of science is to amass such an enormous mountain of evidence that even scientists cannot ignore it; and that this is the distinguishing characteristic of a scientist, a non-scientist will ignore it anyway.\n\nMax Planck was even less optimistic:\n\n> \"A new scientific truth does not triumph by convincing its opponents and making them see the light, but rather because its opponents eventually die, and a new generation grows up that is familiar with it.\"\n\nI am much tickled by this notion, because it implies that the power of science to distinguish truth from falsehood ultimately rests on the good taste of grad students.\n\nThe _gradual_ increase in acceptance of [many-worlds](/lw/q8/many_worlds_one_best_guess/) in academic physics, suggests that there are physicists who will only accept a new idea given some _combination_ of epistemic justification, and a sufficiently large academic pack in whose company they can be comfortable.  As more physicists accept, the pack grows larger, and hence more people go over their individual thresholds for conversion—with the epistemic justification remaining essentially the same.\n\nBut Science still gets there _eventually,_ and this is sufficient for the ratchet of Science to move forward, and raise up a technological civilization.\n\nScientists can be moved by groundless prejudices, by undermined intuitions, by raw herd behavior—the panoply of human flaws.  Each time a scientist shifts belief for epistemically unjustifiable reasons, it requires more evidence, or new arguments, to cancel out the noise.\n\nThe \"collapse of the wavefunction\" has no experimental justification, but it appeals to the (undermined) intuition of a single world.  Then it may take an extra argument—say, that collapse violates Special Relativity—to begin the slow academic disintegration of an idea that [should never have been assigned non-negligible probability in the first place](/lw/q7/if_manyworlds_had_come_first/).\n\nFrom a Bayesian perspective, human academic science as a whole is a highly inefficient processor of evidence.  Each time an unjustifiable argument shifts belief, you need an extra justifiable argument to shift it back.  The social process of science leans on extra evidence to overcome cognitive noise.\n\nA more charitable way of putting it is that scientists will adopt positions that are theoretically _insufficiently extreme_, compared to the ideal positions that scientists _would_ adopt, if they were Bayesian AIs and could [trust themselves](/lw/qb/science_doesnt_trust_your_rationality/) to reason clearly.\n\nBut don't be too charitable.  The noise we are talking about is not all innocent mistakes.  In many fields, debates drag on for decades after they should have been settled.  And _not_ because the scientists on both sides [refuse to trust themselves](/lw/qb/science_doesnt_trust_your_rationality/) and agree they should look for additional evidence.  But because one side keeps throwing up more and more ridiculous objections, and demanding more and more evidence, from an entrenched position of academic power, long after it becomes clear from which quarter the winds of evidence are blowing.  (I'm thinking here about the debates surrounding the invention of [evolutionary psychology](/lw/l1/evolutionary_psychology/), not about many-worlds.)\n\nIs it possible for individual humans or groups to process evidence more efficiently—reach correct conclusions faster—than human academic science as a whole?\n\n\"Ideas are tested by experiment.  That is the core of science.\"  And this must be true, because if you can't trust [Zombie Feynman](http://xkcd.com/397/), who _can_ you trust?\n\nYet where do the _ideas_ come from?\n\nYou may be tempted to reply, \"They come from scientists.  Got any other questions?\"  In Science you're not supposed to care _where_ the hypotheses come from—just whether they pass or fail experimentally.\n\nOkay, but if you remove _all_ new ideas, the scientific process as a whole stops working because it has no alternative hypotheses to test.  So inventing new ideas is not a dispensable part of the process.\n\nNow put your Bayesian goggles back on.  As described in [Einstein's Arrogance](/lw/jo/einsteins_arrogance/), there are queries that are not binary—where the answer is not \"Yes\" or \"No\", but drawn from a larger space of structures, e.g., the space of equations.  In such cases it takes far more Bayesian evidence to _promote a hypothesis to your attention_ than to _confirm the hypothesis._\n\nIf you're working in the space of all equations that can be specified in 32 bits or less, you're working in a space of 4 billion equations.  It takes far more Bayesian evidence to raise one of those hypotheses to the 10% probability level, than it requires _further_ Bayesian evidence to raise the hypothesis from 10% to 90% probability.\n\nWhen the idea-space is large, coming up with ideas worthy of testing, involves much more work—in the [Bayesian-thermodynamic sense of \"work\"](/lw/o5/the_second_law_of_thermodynamics_and_engines_of/)—than _merely_ obtaining an experimental result with p<0.0001 for the new hypothesis over the old hypothesis.\n\nIf this doesn't seem obvious-at-a-glance, pause here and read [Einstein's Arrogance](/lw/jo/einsteins_arrogance/).\n\nThe scientific process has always relied on scientists to come up with hypotheses to test, via some process not further specified by Science.  Suppose you came up with some way of generating hypotheses that was completely crazy—say, pumping a robot-controlled Ouija board with the digits of pi—and the resulting suggestions kept on getting verified experimentally.  The pure ideal essence of Science wouldn't skip a beat.  The pure ideal essence of Bayes would burst into flames and die.\n\n(Compared to Science, Bayes is [falsified by more of the possible outcomes](/lw/if/your_strength_as_a_rationalist/).)\n\nThis doesn't mean that the process of deciding which ideas to test is _unimportant_ to Science.  It means that Science doesn't _specify_ it.\n\n_In practice_, the robot-controlled Ouija board doesn't work. In practice, there are some scientific queries with a large enough answer space, that picking models at random to test, it would take zillions of years to hit on a model that made good predictions—like getting monkeys to type Shakespeare.\n\nAt the _frontier_ of science—the boundary between ignorance and knowledge, where science _advances_—the process relies on at least some individual scientists (or working groups) seeing things that are not yet confirmed by Science.  That's how they know which hypotheses to test, in advance of the test itself.\n\nIf you take your Bayesian goggles off, you can say, \"Well, they don't have to know, they just have to guess.\"  If you put your Bayesian goggles back on, you realize that \"guessing\" with 10% probability requires nearly as much epistemic work to have been successfully performed, behind the scenes, as \"guessing\" with 80% probability—at least for large answer spaces.\n\nThe scientist may not _know_ he has done this epistemic work successfully, in advance of the experiment; but he must, in fact, have done it successfully!  Otherwise he will not even _think_ of the correct hypothesis.  In large answer spaces, anyway.\n\nSo the scientist makes the novel prediction, performs the experiment, publishes the result, and _now_ Science knows it too.  It is now part of the [publicly accessible knowledge of humankind](/lw/in/scientific_evidence_legal_evidence_rational/), that anyone can verify for themselves.\n\nIn between was an interval where the scientist rationally knew something that the public social process of science hadn't yet confirmed.  And this is not a trivial interval, though it may be short; for it is where the _frontier_ of science lies, the advancing border.\n\nAll of this is more true for non-routine science than for routine science, because it is a notion of large answer spaces where the answer is not \"Yes\" or \"No\" or drawn from a small set of obvious alternatives.  It is much easier to train people to test ideas, than to have good ideas to test."
    },
    "voteCount": 35,
    "forceInclude": true
  },
  {
    "_id": "5bJyRMZzwMov5u3hW",
    "url": null,
    "title": "Science Doesn't Trust Your Rationality",
    "slug": "science-doesn-t-trust-your-rationality",
    "author": "Eliezer_Yudkowsky",
    "question": false,
    "tags": [
      {
        "name": "Practice & Philosophy of Science"
      },
      {
        "name": "Rationality"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "[Scott Aaronson](http://scottaaronson.com/blog/?p=326) suggests that Many-Worlds and libertarianism are similar in that they are both cases of bullet-swallowing, rather than bullet-dodging:\n\n> Libertarianism and MWI are both are grand philosophical theories that start from premises that almost all educated people accept (quantum mechanics in the one case, Econ 101 in the other), and claim to reach conclusions that most educated people reject, or are at least puzzled by (the existence of parallel universes / the desirability of eliminating fire departments).\n\nNow _there's_ an analogy that would never have occurred to me.\n\nI've previously argued that [Science rejects Many-Worlds but Bayes accepts it](/lw/qa/the_dilemma_science_or_bayes/).  (Here, \"Science\" is capitalized because we are talking about the idealized form of Science, not just the actual social process of science.)\n\nIt furthermore seems to me that there is a _deep_ analogy between (small-'l') libertarianism and Science:\n\n1.  Both are based on a pragmatic distrust of reasonable-sounding arguments.\n2.  Both try to build systems that are more trustworthy than the people in them.\n3.  Both accept that people are flawed, and try to harness their flaws to power the system.\n\nThe core argument for libertarianism is historically motivated distrust of lovely theories of \"How much _better_ society would be, if we just made a rule that said XYZ.\"  If that sort of trick actually _worked,_ then more regulations would correlate to higher economic growth as society moved from local to global optima.  But when some person or interest group gets enough power to start doing everything they think is a good idea, history says that what actually _happens_ is Revolutionary France or Soviet Russia.\n\nThe plans that in lovely theory should have made everyone happy ever after, don't have the results predicted by reasonable-sounding arguments.  And power corrupts, and attracts the corrupt.\n\nSo you regulate as little as possible, because you can't trust the lovely theories and you can't trust the people who implement them.\n\nYou don't shake your finger at people for being selfish.  You try to build an efficient system of production out of selfish participants, by requiring transactions to be voluntary.  So people are forced to play positive-sum games, because that's how they get the _other_ party to sign the contract.  With violence restrained and contracts enforced, individual selfishness can power a globally productive system.\n\nOf course none of this works quite so well in practice as in theory, and I'm not going to go into market failures, commons problems, etc.  The core argument for libertarianism is not that libertarianism would work in a perfect world, but that it degrades gracefully into real life.  Or rather, degrades less awkwardly than any other known economic principle.  (People who see Libertarianism as the [perfect](/lw/lm/affective_death_spirals/) solution for perfect people, strike me as kinda missing the point of the \"pragmatic distrust\" thing.)\n\nScience first came to know itself as a rebellion against trusting the word of Aristotle. If the people of that revolution had merely said, \"Let us trust ourselves, not Aristotle!\" they would have flashed and faded like the French Revolution.\n\nBut the Scientific Revolution lasted because—like the American Revolution—the architects propounded a stranger philosophy:  \"Let us trust no one!  Not even ourselves!\"\n\nIn the beginning came the idea that we can't just toss out Aristotle's armchair reasoning and replace it with _different_ armchair reasoning.  We need to talk to Nature, and actually _listen_ to what It says in reply.  This, itself, was a stroke of genius.\n\nBut then came the challenge of implementation. People are stubborn, and may not want to accept the verdict of experiment.  Shall we shake a disapproving finger at them, and say \"Naughty\"?\n\nNo; we assume and accept that each individual scientist may be crazily attached to their personal theories.  Nor do we assume that anyone can be trained out of this tendency—we don't try to choose Eminent Judges who are supposed to be impartial.\n\nInstead, we try to _harness_ the individual scientist's stubborn desire to prove their personal theory, by saying:  \"Make a new experimental prediction, and do the experiment.  If you're right, and the experiment is replicated, you win.\"  So long as scientists believe this is true, they have a motive to do experiments that can _falsify_ their own theories.  Only by accepting the possibility of defeat is it possible to win.  And any great claim will require replication; this gives scientists a motive to be honest, on pain of great embarrassment.\n\nAnd so the stubbornness of individual scientists is harnessed to produce a steady stream of knowledge at the group level.  The System is _somewhat_ more trustworthy than its parts.\n\nLibertarianism secretly relies on most individuals being prosocial enough to tip at a restaurant they won't ever visit again.  An economy of [genuinely selfish](/lw/kx/fake_selfishness/) human-level agents would implode.  Similarly, Science relies on most scientists not committing sins so egregious that they can't rationalize them away.\n\nTo the extent that scientists believe they can promote their theories by playing academic politics—or game the statistical methods to potentially win without a chance of losing—or to the extent that nobody bothers to replicate claims—science degrades in effectiveness.  But it degrades gracefully, as such things go.\n\nThe part where the successful predictions belong to the theory and theorists who originally made them, and cannot just be stolen by a theory that comes along later—_without_ a novel experimental prediction—is an important feature of this social process.\n\nThe final upshot is that Science is not easily reconciled with probability theory.  If you do a probability-theoretic calculation _correctly_, you're going to get the _rational_ answer.  Science doesn't trust your rationality, and it doesn't rely on your ability to use probability theory as the arbiter of truth.  It wants you to set up a definitive experiment.\n\nRegarding Science as a mere approximation to some probability-theoretic ideal of rationality... would certainly seem to be _rational_.  There seems to be an extremely reasonable-sounding argument that Bayes's Theorem is the [hidden structure](/lw/o7/searching_for_bayesstructure/) that explains why Science works.  But to subordinate Science to the grand schema of Bayesianism, and let Bayesianism come in and override Science's verdict when that seems appropriate, is not a trivial step!\n\nScience is built around the assumption that you're _too stupid and self-deceiving_ to just use Solomonoff induction.  After all, if it was that simple, we wouldn't need a social process of science... right?\n\nSo, are you going to believe in [faster-than-light quantum \"collapse\" fairies](/lw/q7/if_manyworlds_had_come_first/) after all?  Or do you think you're smarter than that?"
    },
    "voteCount": 45,
    "forceInclude": true
  },
  {
    "_id": "wustx45CPL5rZenuo",
    "url": null,
    "title": "No Safe Defense, Not Even Science",
    "slug": "no-safe-defense-not-even-science",
    "author": "Eliezer_Yudkowsky",
    "question": false,
    "tags": [
      {
        "name": "Practice & Philosophy of Science"
      },
      {
        "name": "Rationality"
      },
      {
        "name": "Courage"
      },
      {
        "name": "Trust"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "I don't ask my friends about their childhoods—I lack social curiosity—and so I don't know how much of a trend this really is:\n\nOf the people I know who are reaching upward as rationalists, who volunteer information about their childhoods, there is a surprising tendency to hear things like:  \"My family joined a cult and I had to break out,\" or \"One of my parents was clinically insane and I had to learn to filter out reality from their madness.\"\n\nMy own experience with growing up in an Orthodox Jewish family seems tame by comparison... but it accomplished the same outcome:  It broke my core emotional trust in the sanity of the people around me.\n\nUntil this core emotional trust is broken, you don't start growing as a rationalist.  I have trouble putting into words why this is so.  Maybe any _unusual_ skills you acquire—anything that makes you _unusually_ rational—requires you to zig when other people zag.  Maybe that's just too scary, if the world still seems like a sane place unto you.\n\nOr maybe you don't bother putting in the hard work to be extra bonus sane, if normality doesn't scare the hell out of you.\n\nI know that many aspiring rationalists seem to run into roadblocks around things like cryonics or many-worlds.  Not that they don't see the logic; they see the logic and wonder, \"Can this really be true, when it seems so obvious now, and yet none of the people around me believe it?\"\n\nYes.  Welcome to the Earth where ethanol is made from corn and environmentalists oppose nuclear power.  I'm sorry.\n\n(See also:  [Cultish Countercultishness](/lw/md/cultish_countercultishness/).  If you end up in the frame of mind of _nervously seeking reassurance_, this is never a good thing—even if it's because you're about to believe something that sounds logical but could cause other people to look at you funny.)\n\nPeople who've had their trust broken in the sanity of the people around them, seem to be able to evaluate strange ideas on their merits, without feeling nervous about their strangeness.  The glue that binds them to their current place has dissolved, and they can walk in some direction, hopefully forward.\n\n[Lonely dissent](/lw/mb/lonely_dissent/), I called it.  True dissent doesn't feel like going to school wearing black; it feels like going to school wearing a clown suit.\n\nThat's what it takes to be the lone voice who says, \"If you really think you know who's going to win the election, why aren't you picking up the [free money](/lw/ni/buy_now_or_forever_hold_your_peace/) on the Intrade prediction market?\" while all the people around you are thinking, \"It is good to be an individual and form your own opinions, the shoe commercials told me so.\"\n\nMaybe in some other world, some alternate Everett branch with a saner human population, things would be different... but in this world, I've never seen anyone begin to grow as a rationalist until they make a deep emotional break with the [wisdom of their pack](/lw/m9/aschs_conformity_experiment/).\n\nMaybe in another world, things would be different.  And maybe not.  I'm not sure that human beings realistically _can_ trust and think at the same time.\n\nOnce upon a time, there was something I trusted.\n\nEliezer~18~ trusted Science.\n\nEliezer~18~ dutifully acknowledged that the social process of science was flawed.  Eliezer~18~ dutifully acknowledged that academia was slow, and misallocated resources, and played favorites, and mistreated its precious heretics.\n\nThat's the convenient thing about acknowledging flaws in _people_ who failed to live up to your ideal; you don't have to question the ideal itself.\n\nBut who could possibly be foolish enough to question, \"The experimental method shall decide which hypothesis wins\"?\n\nPart of what fooled Eliezer~18~ was a general problem he had, with [an aversion to ideas that resembled things idiots had said](/lw/lw/reversed_stupidity_is_not_intelligence/).  Eliezer~18~ had seen plenty of people questioning the ideals of Science Itself, and without exception they were all on the Dark Side.  People who questioned the ideal of Science were invariably trying to sell you snake oil, or trying to safeguard their favorite form of stupidity from criticism, or trying to disguise their personal resignation as a Deeply Wise acceptance of futility.\n\nIf there'd been any other ideal that was a few centuries old, the young Eliezer would have looked at it and said, \"I wonder if this is really right, and whether there's a way to [do better](/lw/h8/tsuyoku_naritai_i_want_to_become_stronger/).\"  But not the ideal of Science.  Science was the master idea, the idea that let you change ideas.  You could question it, but you were meant to [question it and then accept it](/lw/ib/the_proper_use_of_doubt/), not actually say, \"Wait!  This is wrong!\"\n\nThus, when once upon a time I came up with a stupid idea, I thought I was behaving virtuously if I made sure there was a Novel Prediction, and professed that I wished to test my idea experimentally.  I thought I had done everything I was obliged to do.\n\nSo I thought I was _safe_—not safe from any particular external threat, but safe on some deeper level, like a child who trusts their parent and has obeyed all the parent's rules.\n\nI'd long since been broken of trust in the sanity of my family or my teachers at school.  And the other children weren't intelligent enough to compete with the conversations I could have with books.  But I trusted the books, you see.  I trusted that if I did what Richard Feynman told me to do, I would be safe.  I never thought those words aloud, but it was how I felt.\n\nWhen Eliezer~23~ realized exactly _how_ stupid the stupid theory had been—and that Traditional Rationality had not saved him from it—and that Science would have been perfectly okay with his wasting ten years testing the stupid idea, so long as afterward he admitted it was wrong...\n\n...well, I'm not going to say it was a huge emotional convulsion.  I don't really go in for that kind of drama.  It simply became obvious that I'd been stupid.\n\nThat's the trust I'm trying to break in you.  You are not safe.  Ever.\n\nNot even Science can save you.  The ideals of Science were born centuries ago, in a time when no one knew anything about probability theory or cognitive biases.  Science demands _too little_ of you, it blesses your good intentions too easily, [it is not strict _enough_](/lw/qd/science_isnt_strict_enough/), it only makes those injunctions that an [average scientist](/lw/qe/do_scientists_already_know_this_stuff/) can follow, it accepts [slowness](/lw/q9/the_failures_of_eld_science/) as a fact of life.\n\nSo don't think that if you only follow the rules of Science, that makes your reasoning defensible.\n\nThere is no known procedure you can follow that makes your reasoning defensible.\n\nThere is no known set of injunctions which you can satisfy, and know that you will not have been a fool.\n\nThere is no known morality-of-reasoning that you can do your best to obey, and know that you are thereby shielded from criticism.\n\nNo, not even if you turn to Bayescraft.  It's much harder to use and you'll never be sure that you're doing it right.\n\nThe discipline of Bayescraft is younger by far than the discipline of Science.  You will find no textbooks, no elderly mentors, no histories written of success and failure, no hard-and-fast rules laid down.  You will have to study cognitive biases, and probability theory, and evolutionary psychology, and social psychology, and other cognitive sciences, and Artificial Intelligence—and think through for yourself how to apply all this knowledge to the case of correcting yourself, since that isn't yet in the textbooks.\n\nYou don't know what your own mind is really doing. They find a new cognitive bias every week and you're never sure if you've corrected for it, or overcorrected.\n\nThe formal math is impossible to apply.  It doesn't break down as easily as John Q. Unbeliever thinks, but you're never really sure where the foundations come from.  You don't know why the universe is simple enough to understand, or why any prior works for it.  You don't know what your own priors _are,_ let alone if they're any good.\n\nOne of the problems with Science is that it's too vague to really scare you.  \"Ideas should be tested by experiment.\"  How can you go wrong with that?\n\nOn the other hand, if you have some math of probability theory laid out in front of you, and worse, _you know you can't actually use it,_ then it becomes clear that you are trying to do something difficult, and that you might well be doing it _wrong._\n\nSo you cannot trust.\n\nAnd all this that I have said, _will not be sufficient_ to break your trust.  That won't happen until you get into your first real disaster from following The Rules, not from breaking them.\n\nEliezer~18~ already had the notion that you were allowed to question Science.  Why, of course the scientific method was not itself immune to questioning!  For are we not all good rationalists?  Are we not allowed to question everything?\n\nIt was the notion that you could _actually in real life_ follow Science and fail miserably, that Eliezer~18~  didn't really, emotionally believe was possible.\n\nOh, of course he said it was possible.  Eliezer~18~ dutifully acknowledged the possibility of error, saying, \"I could be wrong, but...\"\n\nBut he didn't think failure could happen in, you know, real life.  You were supposed to look for flaws, not [actually find them](/lw/ib/the_proper_use_of_doubt/).\n\nAnd this emotional difference is a terribly difficult thing to accomplish in words, and I fear there's no way I can really warn you.\n\nYour trust will not break, until you apply all that you have learned here and from other books, and take it as far as you can go, and find that this too fails you—that you have still been a fool, and no one warned you against it—that all the most important parts were left out of the guidance you received—that some of the most precious ideals you followed, steered you in the wrong direction—\n\n—and if you still have [something to protect](/lw/nb/something_to_protect/), so that you _must_ keep going, and _cannot_ resign and wisely acknowledge the limitations of rationality—\n\n_—_then you will be ready to start your journey as a rationalist.  To take sole responsibility, to live without any trustworthy defenses, and to forge a higher Art than the one you were once taught.\n\nNo one begins to truly search for the Way until their parents have failed them, their gods are dead, and their tools have shattered in their hand.\n\n* * *\n\n**Post Scriptum:**  On reviewing a draft of this essay, I discovered a fairly inexcusable flaw in reasoning, which actually affects one of the conclusions drawn.  I am [leaving it in](http://www.overcomingbias.com/2008/02/my-favorite-lia.html).  Just in case you thought that taking my advice made you safe; or that you were supposed to look for flaws, but not find any.\n\nAnd of course, if you look too hard for a flaw, and find a flaw that is not a real flaw, and cling to it to reassure yourself of how critical you are, you will only be worse off than before...\n\nIt is living with uncertainty—knowing on a gut level that there are flaws, they are serious and you have not found them—that is the difficult thing."
    },
    "voteCount": 64,
    "forceInclude": true
  },
  {
    "_id": "46qnWRSR7L2eyNbMA",
    "url": null,
    "title": "The Lens That Sees Its Flaws",
    "slug": "the-lens-that-sees-its-flaws",
    "author": "Eliezer Yudkowsky",
    "question": false,
    "tags": [
      {
        "name": "Rationality"
      },
      {
        "name": "Gears-Level"
      },
      {
        "name": "Epistemology"
      },
      {
        "name": "Map and Territory"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "Light leaves the Sun and strikes your shoelaces and bounces off; some photons enter the pupils of your eyes and strike your retina; the energy of the photons triggers neural impulses; the neural impulses are transmitted to the visual-processing areas of the brain; and there the optical information is processed and reconstructed into a 3D model that is recognized as an untied shoelace; and so you believe that your shoelaces are untied.\n\nHere is the secret of *deliberate rationality—*this whole process is not [magic](https://www.lesswrong.com/lw/iu/mysterious_answers_to_mysterious_questions/), and you can *understand* it. You can *understand* how you see your shoelaces. You can *think* about which sort of thinking processes will create beliefs which mirror reality, and which thinking processes will not.\n\nMice can see, but they can’t understand seeing. *You* can understand seeing, and because of that, you can do things that mice cannot do. Take a moment to [marvel](https://www.lesswrong.com/lw/j3/science_as_curiositystopper/) at this, for it is indeed marvelous.\n\nMice see, but they don’t know they have visual cortexes, so they can’t correct for optical illusions. A mouse lives in a mental world that includes cats, holes, cheese and mousetraps—but not mouse brains. Their camera does not take pictures of its own lens. But we, as humans, can look at a [seemingly bizarre image](http://www.richrock.com/gifs/optical-illusion-wheels-circles-rotating.png), and realize that part of what we’re seeing is the lens itself. You don’t always have to believe your own eyes, but you have to realize that you *have* eyes—you must have distinct mental buckets for the map and the territory, for the senses and reality. Lest you think this a trivial ability, remember how rare it is in the animal kingdom.\n\nThe whole idea of Science is, simply, reflective reasoning about a more reliable process for making the contents of your mind mirror the contents of the world. It is the sort of thing mice would never invent. Pondering this business of “performing replicable experiments to falsify theories,” we can see *why* it works. Science is not a [separate magisterium](https://www.lesswrong.com/lw/i8/religions_claim_to_be_nondisprovable/), far away from real life and the understanding of ordinary mortals. Science is not something that only applies to the [inside of laboratories](https://www.lesswrong.com/lw/gv/outside_the_laboratory/). Science, itself, is an understandable process-in-the-world that correlates brains with reality.\n\nScience *makes sense*, when you think about it. But mice can’t think about thinking, which is why they don’t have Science. One should not overlook the wonder of this—or the potential power it bestows on us as individuals, not just scientific societies.\n\nAdmittedly, understanding the engine of thought may be *a little more complicated* than understanding a steam engine—but it is not a *fundamentally* different task.\n\nOnce upon a time, I went to EFNet’s #philosophy chatroom to ask, “Do you believe a nuclear war will occur in the next 20 years? If no, why not?” One person who answered the question said he didn’t expect a nuclear war for 100 years, because “All of the players involved in decisions regarding nuclear war are not interested right now.” “But why extend that out for 100 years?” I asked. “Pure hope,” was his reply.\n\nReflecting on this whole thought process, we can see why the thought of nuclear war makes the person unhappy, and we can see how his brain therefore rejects the belief. But if you imagine a billion worlds—Everett branches, or Tegmark duplicates^1^—this thought process will not [systematically correlate](https://www.lesswrong.com/lw/jl/what_is_evidence/) optimists to branches in which no nuclear war occurs.^2^\n\nTo ask which beliefs make you happy is to turn inward, not outward—it tells you something about yourself, but it is not evidence entangled with the environment. I have nothing against happiness, but it should follow from your picture of the world, rather than tampering with the mental paintbrushes.\n\nIf you can see this—if you can see that hope is shifting your *first-order* thoughts by too large a degree—if you can understand your mind as a mapping engine that has flaws—then you can apply a reflective correction. The brain is a flawed lens through which to see reality. This is true of both mouse brains and human brains. But a human brain is a flawed lens that can understand its own flaws—its systematic errors, its biases—and apply second-order corrections to them. This, *in practice,* makes the lens far more powerful. Not perfect, but far more powerful.\n\n* * *\n\n^1^ Max Tegmark, “Parallel Universes,” in *Science and Ultimate Reality: Quantum Theory,* *Cosmology, and Complexity*, ed. John D. Barrow, Paul C. W. Davies, and Charles L. Harper Jr. (New York: Cambridge University Press, 2004), 459–491, [http://arxiv.org/abs/astro-ph/0302131](http://arxiv.org/abs/astro-ph/0302131).\n\n^2^ Some clever fellow is bound to say, “Ah, but since I have hope, I'll work a little harder at my job, pump up the global economy, and thus help to prevent countries from sliding into the angry and hopeless state where nuclear war is a possibility. So the two events are related after all.” At this point, we have to drag in Bayes’s Theorem and measure the relationship quantitatively. Your optimistic nature cannot have *that* large an effect on the world; it cannot, of itself, decrease the probability of nuclear war by 20%, or however much your optimistic nature shifted your beliefs. Shifting your beliefs by a large amount, due to an event that only slightly increases your chance of being right, will still mess up your mapping."
    },
    "voteCount": 146,
    "forceInclude": true
  },
  {
    "_id": "RcZCwxFiZzE6X7nsv",
    "url": null,
    "title": "What Do We Mean By \"Rationality\"?",
    "slug": "what-do-we-mean-by-rationality-1",
    "author": "Eliezer Yudkowsky",
    "question": false,
    "tags": [
      {
        "name": "Rationality"
      },
      {
        "name": "Definitions"
      },
      {
        "name": "Distinctions"
      },
      {
        "name": "Motivational Intro Posts"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "I mean two things:\n\n1\\. **Epistemic rationality**: systematically improving the accuracy of your beliefs.\n\n2\\. **Instrumental rationality**: systematically achieving your values.\n\nThe first concept is simple enough. When you open your eyes and look at the room around you, you’ll locate your laptop in relation to the table, and you’ll locate a bookcase in relation to the wall. If something goes wrong with your eyes, or your brain, then your mental model might say there’s a bookcase where no bookcase exists, and when you go over to get a book, you’ll be disappointed.\n\nThis is what it’s like to have a false belief, a map of the world that doesn’t correspond to the territory. Epistemic rationality is about building accurate maps instead. This correspondence between belief and reality is commonly called “truth,” and I’m happy to call it that.^1^\n\nInstrumental rationality, on the other hand, is about *steering* reality—sending the future where you want it to go. It’s the art of choosing actions that lead to outcomes ranked higher in your preferences. I sometimes call this “winning.”\n\nSo rationality is about forming true beliefs and making decisions that help you win.\n\n(Where truth doesn't mean “certainty,” since we can do plenty to increase the *probability* that our beliefs are accurate even though we're uncertain; and winning doesn't mean “winning at others' expense,” since our values include *everything* we care about, including other people.)\n\nWhen people say “X is rational!” it’s usually just a more strident way of saying “I think X is true” or “I think X is good.” So why have an additional word for “rational” as well as “true” and “good”?\n\nAn analogous argument can be given against using “true.” There is no need to say “it is true that snow is white” when you could just say “snow is white.” What makes the idea of truth useful is that it allows us to talk about the general features of map-territory correspondence. “True models usually produce better experimental predictions than false models” is a useful generalization, and it’s not one you can make without using a concept like “true” or “accurate.”\n\nSimilarly, “Rational agents make decisions that maximize the probabilistic expectation of a coherent utility function” is the kind of thought that depends on a concept of (instrumental) rationality, whereas “It’s rational to eat vegetables” can probably be replaced with “It’s useful to eat vegetables” or “It’s in your interest to eat vegetables.” We need a concept like “rational” in order to note general facts about those ways of thinking that systematically produce truth or value—and the systematic ways in which we fall short of those standards.\n\nAs we’ve observed in the previous essays, experimental psychologists sometimes uncover human reasoning that seems very strange. For example, someone rates the probability “Bill plays jazz” as *less* than the probability “Bill is an accountant who plays jazz.” This seems like an odd judgment, since any particular jazz-playing accountant is obviously a jazz player. But to what higher vantage point do we appeal in saying that the judgment is *wrong* ?\n\nExperimental psychologists use two gold standards: *probability theory*, and *decision theory*.\n\nProbability theory is the set of laws underlying rational belief. The mathematics of probability applies equally to “figuring out where your bookcase is” and “estimating how many hairs were on Julius Caesars head,” even though our evidence for the claim “Julius Caesar was bald” is likely to be more complicated and indirect than our evidence for the claim “theres a bookcase in my room.” It’s all the same problem of how to process the evidence and observations to update one’s beliefs. Similarly, decision theory is the set of laws underlying rational action, and is equally applicable regardless of what one’s goals and available options are.\n\nLet “P(such-and-such)” stand for “the probability that such-and-such happens,” and “P(A,B)” for “the probability that both A and B happen.” Since it is a universal law of probability theory that P(A) ≥ P(A,B), the judgment that P(Bill plays jazz) is less than P(Bill plays jazz, Bill is an accountant) is labeled incorrect.\n\nTo keep it technical, you would say that this probability judgment is *non-Bayesian*. Beliefs that conform to a coherent probability distribution, and decisions that maximize the probabilistic expectation of a coherent utility function, are called “Bayesian.”\n\nI should emphasize that this *isn't *the notion of rationality thats common in popular culture. People may use the same string of sounds, “ra-tio-nal,” to refer to “acting like Mr. Spock of *Star Trek*” and “acting like a Bayesian”; but this doesn't mean that acting Spock-like helps one hair with epistemic or instrumental rationality.^2^\n\nAll of this does not quite exhaust the problem of what is meant in practice by “rationality,” for two major reasons:\n\nFirst, the Bayesian formalisms in their full form are computationally intractable on most real-world problems. No one can *actually *calculate and obey the math, any more than you can predict the stock market by calculating the movements of quarks.\n\nThis is why there is a whole site called “Less Wrong,” rather than a single page that simply states the formal axioms and calls it a day. There’s a whole further art to finding the truth and accomplishing value *from inside a human mind*: we have to learn our own flaws, overcome our biases, prevent ourselves from self-deceiving, get ourselves into good emotional shape to confront the truth and do what needs doing, et cetera, et cetera.\n\nSecond, sometimes the meaning of the math itself is called into question. The exact rules of probability theory are called into question by, e.g., [anthropic problems](http://www.anthropic-principle.com/?q=anthropic_principle/primer) in which the number of observers is uncertain. The exact rules of decision theory are called into question by, e.g., Newcomblike problems in which other agents may predict your decision before it happens.^3^\n\nIn cases where our best formalizations still come up short, we can return to simpler ideas like “truth” and “winning.” If you are a scientist just beginning to investigate fire, it might be a lot wiser to point to a campfire and say “Fire is that orangey-bright hot stuff over there,” rather than saying “I define fire as an alchemical transmutation of substances which releases phlogiston.” You certainly shouldn’t ignore something just because you can’t define it. I can't quote the equations of General Relativity from memory, but nonetheless if I walk off a cliff, I'll fall. And we can say the same of cognitive biases and other obstacles to truth—they won't hit any less hard if it turns out we can't define compactly what “irrationality” is.\n\nIn cases like these, it is futile to try to settle the problem by coming up with some new definition of the word “rational” and saying, “Therefore my preferred answer, *by definition*, is what is meant by the word ‘rational.’ ” This simply raises the question of why anyone should pay attention to your definition. I’m not interested in probability theory because it is the holy word handed down from Laplace. I’m interested in Bayesian-style belief-updating (with Occam priors) because I expect that this style of thinking gets us systematically closer to, you know, *accuracy*, the map that reflects the territory.\n\nAnd then there are questions of how to think that seem not quite answered by either probability theory or decision theory—like the question of how to feel about the truth once you have it. Here, again, trying to define “rationality” a particular way doesn’t support an answer, but merely presumes one.\n\nI am not here to argue the meaning of a word, not even if that word is “rationality.” The point of attaching sequences of letters to particular concepts is to let two people *communicate*—to help transport thoughts from one mind to another. You cannot change reality, or prove the thought, by manipulating which meanings go with which words.\n\nSo if you understand what concept I am *generally getting at *with this word “rationality,” and with the sub-terms “epistemic rationality” and “instrumental rationality,” we *have communicated*: we have accomplished everything there is to accomplish by talking about how to define “rationality.” What’s left to discuss is not *what meaning *to attach to the syllables “ra-tio-na-li-ty”; what’s left to discuss is *what is a good way to think*.\n\nIf you say, “It’s (epistemically) rational for me to believe X, but the truth is Y,” then you are probably using the word “rational” to mean something other than what I have in mind. (E.g., “rationality” should be *consistent under reflection*—“rationally” looking at the evidence, and “rationally” considering how your mind processes the evidence, shouldn’t lead to two different conclusions.)\n\nSimilarly, if you find yourself saying, “The (instrumentally) rational thing for me to do is X, but the right thing for me to do is Y,” then you are almost certainly using some other meaning for the word “rational” or the word “right.” I use the term “rationality” *normatively*, to pick out desirable patterns of thought.\n\nIn this case—or in any other case where people disagree about word meanings—you should substitute more specific language in place of “rational”: “The self-benefiting thing to do is to run away, but I hope I would at least try to drag the child off the railroad tracks,” or “Causal decision theory as usually formulated says you should two-box on Newcomb’s Problem, but I’d rather have a million dollars.”\n\nIn fact, I recommend reading back through this essay, replacing every instance of “rational” with “foozal,” and seeing if that changes the connotations of what I’m saying any. If so, I say: strive not for rationality, but for foozality.\n\nThe word “rational” has potential pitfalls, but there are plenty of *non*-borderline cases where “rational” works fine to communicate what I’m getting at. Likewise “irrational.” In these cases I’m not afraid to use it.\n\nYet one should be careful not to *overuse *that word. One receives no points merely for pronouncing it loudly. If you speak overmuch of the Way, you will not attain it.\n\n* * *\n\n^1^ For a longer discussion of truth, see “[The Simple Truth](https://www.lesswrong.com/rationality/the-simple-truth)” at the very end of this volume.\n\n^2^ The idea that rationality is about strictly privileging verbal reasoning over feelings is a case in point. Bayesian rationality applies to urges, hunches, perceptions, and wordless intuitions, not just to assertions.\n\nI gave the example of opening your eyes, looking around you, and building a mental model of a room containing a bookcase against the wall. The modern idea of rationality is general enough to include your eyes and your brains visual areas as things-that-map, and to include instincts and emotions in the belief-and-goal calculus.\n\n^3^ For an informal statement of Newcomb’s Problem, see Jim Holt, “Thinking Inside the Boxes,” *Slate*, 2002, [http://www.slate.com/articles/arts/egghead/2002/02/thinkinginside\\_the\\_boxes.single.html](http://www.slate.com/articles/arts/egghead/2002/02/thinkinginside_the_boxes.single.html)."
    },
    "voteCount": 263,
    "forceInclude": true
  },
  {
    "_id": "PBRWb2Em5SNeWYwwB",
    "url": null,
    "title": "Humans are not automatically strategic",
    "slug": "humans-are-not-automatically-strategic",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "Rationality"
      },
      {
        "name": "Goodhart's Law"
      },
      {
        "name": "Introspection"
      },
      {
        "name": "General Intelligence"
      },
      {
        "name": "Motivational Intro Posts"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "Reply to: [A \"Failure to Evaluate Return-on-Time\" Fallacy](/lw/2p1/a_failure_to_evaluate_returnontime_fallacy/)\n\nLionhearted writes:\n\n> \\[A\\] large majority of otherwise smart people spend time doing semi-productive things, when there are massively productive opportunities untapped.\n> \n> A somewhat silly example: Let's say someone aspires to be a comedian, the best comedian ever, and to make a living doing comedy. He wants nothing else, it is his purpose. And he decides that in order to become a better comedian, he will watch re-runs of the old television cartoon 'Garfield and Friends' that was on TV from 1988 to 1995....\n> \n> I’m curious as to why.\n\nWhy will a randomly chosen eight-year-old fail a calculus test?  Because most possible answers are wrong, and there is no force to guide him to the correct answers.  (There is no need to postulate a “fear of success”; _most_ ways writing or not writing on a calculus test constitute failure, and so people, and rocks, fail calculus tests by default.)\n\nWhy do most of us, most of the time, choose to \"pursue our goals\" through routes that are far less effective than the routes we could find if we tried?\\[1\\]  My guess is that here, as with the calculus test, the main problem is that _most_ courses of action are extremely ineffective, and that there has been no strong evolutionary or cultural force sufficient to focus us on the very narrow behavior patterns that would actually be effective. \n\nTo be more specific: there are clearly at least some limited senses in which we have goals.  We: (1) tell ourselves and others stories of how we’re aiming for various “goals”; (2) search out modes of activity that are consistent with the role, and goal-seeking, that we see ourselves as doing (“learning math”; “becoming a comedian”; “being a good parent”); and sometimes even (3) feel glad or disappointed when we do/don’t achieve our “goals”.\n\nBut there are clearly also heuristics that would be useful to goal-achievement (or that would be part of what it means to “have goals” at all) that we do _not_ automatically carry out.  We do _not_ automatically:\n\n*   (a) Ask ourselves what we’re trying to achieve; \n*   (b) Ask ourselves how we could tell if we achieved it (“what does it look like to be a good comedian?”) and how we can track progress; \n*   (c) Find ourselves strongly, intrinsically curious about information that would help us achieve our goal; \n*   (d) Gather that information (e.g., by asking as how folks commonly achieve our goal, or similar goals, or by tallying which strategies have and haven’t worked for us in the past); \n*   (e) Systematically test many different conjectures for how to achieve the goals, including methods that aren’t habitual for us, while tracking which ones do and don’t work; \n*   (f) Focus most of the energy that \\*isn’t\\* going into systematic exploration, on the methods that work best;\n*   (g) Make sure that our \"goal\" is really our goal, that we coherently want it and are not constrained by fears or by uncertainty as to whether it is worth the effort, and that we have thought through any questions and decisions in advance so they won't continually sap our energies;\n*   (h) Use environmental cues and social contexts to bolster our motivation, so we can keep working effectively in the face of intermittent frustrations, or temptations based in hyperbolic discounting;\n\n.... or carry out any number of other useful techniques.  Instead, we mostly just do things.  We act from habit; we act from impulse or convenience when primed by the activities in front of us; we remember our goal and choose an action that _feels associated_ with our goal.  We do any number of things.  But we do not systematically choose the narrow sets of actions that would effectively optimize for our claimed goals, or for any other goals.\n\nWhy?  Most basically, because humans are only just on the cusp of general intelligence.  Perhaps 5% of the population has enough abstract reasoning skill to _verbally understand_ that the above heuristics would be useful _once these heuristics are pointed out_.  That is not at all the same as the ability to _automatically implement these heuristics_.  Our verbal, conversational systems are much better at abstract reasoning than are the motivational systems that pull our behavior.  I have enough abstract reasoning ability to understand that I’m safe on the glass floor of a tall building, or that ice cream is not healthy, or that exercise furthers my goals... but this _doesn’t_ lead to an automatic updating of the reward gradients that, absent rare and costly conscious overrides, pull my behavior.  I can train my automatic systems, for example by visualizing ice cream as disgusting and artery-clogging and yucky, or by walking across the glass floor often enough to persuade my brain that I can’t fall through the floor... but systematically training one’s motivational systems in this way is _also_ not automatic for us.  And so it seems far from surprising that most of us have not trained ourselves in this way, and that most of our “goal-seeking” actions are far less effective than they could be.\n\nStill, I’m keen to train.  I know people who are far more strategic than I am, and there seem to be clear avenues for becoming far more strategic than they are.  It also seems that having goals, in a much more pervasive sense than (1)-(3), is part of what “rational” should mean, will help us achieve what we care about, and hasn't been taught in much detail on LW.\n\nSo, to second Lionhearted's questions: does this analysis seem right?  Have some of you trained yourselves to be substantially more strategic, or goal-achieving, than you started out?  How did you do it?  Do you agree with (a)-(h) above?  Do you have some good heuristics to add?  Do you have some good ideas for how to train yourself in such heuristics?\n\n\\[1\\] For example, why do many people go through long training programs “to make money” without spending a few hours doing salary comparisons ahead of time?  Why do many who type for hours a day remain two-finger typists, without bothering with a typing tutor program?  Why do people spend their Saturdays “enjoying themselves” without bothering to track which of their habitual leisure activities are \\*actually\\* enjoyable?  Why do even unusually numerate people fear illness, car accidents, and bogeymen, and take safety measures, but not bother to look up statistics on the relative risks? Why do most of us settle into a single, stereotyped mode of studying, writing, social interaction, or the like, without trying alternatives to see if they work better -- even when such experiments as we have tried have sometimes given great boosts?"
    },
    "voteCount": 316,
    "forceInclude": true
  },
  {
    "_id": "fhEPnveFhb9tmd7Pe",
    "url": null,
    "title": "Use the Try Harder, Luke",
    "slug": "use-the-try-harder-luke",
    "author": "Eliezer Yudkowsky",
    "question": false,
    "tags": [
      {
        "name": "Fiction"
      },
      {
        "name": "More Dakka"
      },
      {
        "name": "Five minute timers"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "> \"When there's a will to fail, obstacles can be found.\"   —John McCarthy\n\nI first watched _Star Wars_ IV-VI when I was very young.  Seven, maybe, or nine?  So my memory was dim, but I recalled Luke Skywalker as being, you know, this cool Jedi guy.\n\nImagine my horror and disappointment, when I watched the saga again, years later, and discovered that Luke was a [whiny teenager](http://www.youtube.com/watch?v=X66jntR0MVE).\n\nI mention this because yesterday, I looked up, on Youtube, the source of the Yoda quote:  \"Do, or do not.  There is no try.\"\n\nOh.  My.  Cthulhu.\n\nAlong with the Youtube clip in question, I present to you a little-known outtake from the scene, in which the director and writer, George Lucas, argues with Mark Hamill, who played Luke Skywalker:\n\n> Luke:  All right, I'll give it a try.  \n> Yoda:  No!  Try not.  Do.  Or do not.  There is no try.  \n> _  \n> Luke raises his hand, and slowly, the X-wing begins to rise out of the water—Yoda's eyes widen—but then the ship sinks again._\n\nMark Hamill:  \"Um, George...\"\n\nGeorge Lucas:  \"What is it now?\"\n\nMark:  \"So... according to the script, next I say, 'I can't.  It's too big'.\"\n\nGeorge:  \"That's right.\"\n\nMark:  \"Shouldn't Luke maybe give it another shot?\"\n\nGeorge:  \"No.  Luke gives up, and sits down next to Yoda—\"\n\nMark:  \"This is the hero who's going to take down the Empire?  Look, it was one thing when he was a whiny teenager at the beginning, but he's in Jedi training now.  Last movie he blew up the Death Star.  Luke should be showing a little backbone.\"\n\nGeorge:  \"No.  You give up.  And then Yoda lectures you for a while, and you say, 'You want the impossible'.  Can you remember that?\"\n\nMark:  \"_Impossible?_  What did he do, run a formal calculation to arrive at a mathematical proof?   The X-wing was already starting to rise out of the swamp!  That's the feasibility demonstration right there!  Luke loses it for a second and the ship sinks back—and now he says it's _impossible?_  Not to mention that Yoda, who's got literally eight hundred years of seniority in the field, just told him it should be doable—\"\n\nGeorge:  \"And then you walk away.\"\n\nMark:  \"It's his friggin' _spaceship!_  If he leaves it in the swamp, he's stuck on Dagobah for the rest of his miserable life!  He's not just going to walk away!  Look, let's just cut to the next scene with the words 'one month later' and Luke is still raggedly standing in front of the swamp, trying to raise his ship for the thousandth time—\"\n\nGeorge:  \"No.\"\n\nMark:  \"Fine!  We'll show a sunset and a sunrise, as he stands there with his arm out, straining, and _then_ Luke says 'It's impossible'.  Though really, he ought to try again when he's fully rested—\"\n\nGeorge:  \"No.\"\n\nMark:  \"_Five goddamned minutes!_ Five goddamned minutes before he gives up!\"\n\nGeorge:  \"I am not halting the story for five minutes while the X-wing bobs in the swamp like a bathtub toy.\"\n\nMark:  \"For the love of sweet candied yams!  If a pathetic loser like this could master the Force, everyone in the galaxy would be using it!  People would become Jedi because it was easier than going to high school.\"\n\nGeorge:  \"Look, you're the actor.  Let me be the storyteller.  Just say your lines and try to mean them.\"\n\nMark:  \"The audience isn't going to buy it.\"\n\nGeorge:  \"Trust me, they will.\"\n\nMark:  \"They're going to get up and walk out of the theater.\"\n\nGeorge:  \"They're going to sit there and nod along and not notice anything out of the ordinary.  Look, you don't understand human nature.  People wouldn't try for five minutes before giving up if the fate of humanity were at stake.\""
    },
    "voteCount": 144,
    "forceInclude": true
  },
  {
    "_id": "5JDkW4MYXit2CquLs",
    "url": null,
    "title": "Your Strength as a Rationalist",
    "slug": "your-strength-as-a-rationalist",
    "author": "Eliezer Yudkowsky",
    "question": false,
    "tags": [
      {
        "name": "Rationalization"
      },
      {
        "name": "Noticing"
      },
      {
        "name": "Rationality"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "The following happened to me in an IRC chatroom, long enough ago that I was still hanging around in IRC chatrooms. Time has fuzzed the memory and my report may be imprecise.\n\nSo there I was, in an IRC chatroom, when someone reports that a friend of his needs medical advice. His friend says that he’s been having sudden chest pains, so he called an ambulance, and the ambulance showed up, but the paramedics told him it was nothing, and left, and now the chest pains are getting worse. What should his friend do?\n\nI was confused by this story. I remembered reading about homeless people in New York who would call ambulances just to be taken someplace warm, and how the paramedics always had to take them to the emergency room, even on the 27th iteration. Because if they didn’t, the ambulance company could be sued for lots and lots of money. Likewise, emergency rooms are legally obligated to treat anyone, regardless of ability to pay.^1^ So I didn’t quite understand how the described events could have happened. *Anyone* reporting sudden chest pains should have been hauled off by an ambulance instantly.\n\nAnd this is where I fell down as a rationalist. I remembered several occasions where my doctor would completely fail to panic at the report of symptoms that seemed, to me, very alarming. And the Medical Establishment was always right. Every single time. I had chest pains myself, at one point, and the doctor patiently explained to me that I was describing chest muscle pain, not a heart attack. So I said into the IRC channel, “Well, if the paramedics told your friend it was nothing, it must *really be* nothing—they’d have hauled him off if there was the tiniest chance of serious trouble.”\n\nThus I managed to explain the story within my existing model, though the fit still felt a little forced . . .\n\nLater on, the fellow comes back into the IRC chatroom and says his friend made the whole thing up. Evidently this was not one of his more reliable friends.\n\nI should have realized, perhaps, that an unknown acquaintance of an acquaintance in an IRC channel might be less reliable than a published journal article. Alas, belief is easier than disbelief; we believe instinctively, but disbelief requires a conscious effort.^2^\n\nSo instead, by dint of mighty straining, I forced my model of reality to explain an anomaly that *never actually happened.* And I *knew* how embarrassing this was. I *knew* that the usefulness of a model is not what it can explain, but what it can’t. A hypothesis that forbids nothing, permits everything, and thereby fails to constrain anticipation.\n\nYour strength as a rationalist is your ability to be more confused by fiction than by reality. If you are equally good at explaining any outcome, you have zero knowledge.\n\nWe are all weak, from time to time; the sad part is that I *could* have been stronger. I had all the information I needed to arrive at the correct answer, I even *noticed* the problem, and then I ignored it. My feeling of confusion was a Clue, and I threw my Clue away.\n\nI should have paid more attention to that sensation of *still feels a little forced.* It’s one of the most important feelings a truthseeker can have, a part of your strength as a rationalist. It is a design flaw in human cognition that this sensation manifests as a quiet strain in the back of your mind, instead of a wailing alarm siren and a glowing neon sign reading:\n\n**Either Your Model Is False Or This Story Is Wrong**.\n\n^1^ And the hospital absorbs the costs, which are enormous, so hospitals are closing their emergency rooms . . . It makes you wonder what’s the point of having economists if we’re just going to ignore them.\n\n^2^ From McCluskey (2007), “Truth Bias”: “\\[P\\]eople are more likely to correctly judge that a truthful statement is true than that a lie is false. This appears to be a fairly robust result that is not just a function of truth being the correct guess where the evidence is weak—it shows up in controlled experiments where subjects have good reason not to assume truth\\[.\\]” [http://www.overcomingbias.com/2007/08/truth-bias.html](http://www.overcomingbias.com/2007/08/truth-bias.html) .\n\nAnd from Gilbert et al. (1993), “You Can’t Not Believe Everything You Read”: “Can people comprehend assertions without believing them? \\[...\\] Three experiments support the hypothesis that comprehension includes an initial belief in the information comprehended.”"
    },
    "voteCount": 144,
    "forceInclude": true
  },
  {
    "_id": "3nZMgRTfFEfHp34Gb",
    "url": null,
    "title": "The Meditation on Curiosity",
    "slug": "the-meditation-on-curiosity",
    "author": "Eliezer Yudkowsky",
    "question": false,
    "tags": [
      {
        "name": "Curiosity"
      },
      {
        "name": "Litany of Tarski"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "> The first virtue is curiosity.\n> \n> —“[The Twelve Virtues of Rationality](https://www.lesswrong.com/rationality/twelve-virtues-of-rationality)”\n\nAs rationalists, we are obligated to criticize ourselves and question our beliefs . . . are we not?\n\nConsider what happens to you, on a psychological level, if you begin by saying: “It is my duty to criticize my own beliefs.” Roger Zelazny once distinguished between “wanting to be an author” versus “wanting to write.” Mark Twain said: “A classic is something that everyone wants to have read and no one wants to read.” Criticizing yourself from a sense of duty leaves you *wanting to have investigated*, so that you’ll be able to say afterward that your faith is not blind. This is not the same as *wanting to investigate.*\n\nThis can lead to [motivated stopping](https://www.lesswrong.com/lw/hu/the_third_alternative/) of your investigation.  You consider an objection, then a counterargument to that objection, then you *stop there.*  You repeat this with several objections, until you feel that you have done your duty to investigate, and then you *stop there.* You have achieved your underlying psychological objective: to get rid of the cognitive dissonance that would result from thinking of yourself as a rationalist, and yet knowing that you had not tried to criticize your belief.  You might call it [purchase of rationalist satisfaction](https://www.lesswrong.com/lw/hw/scope_insensitivity/)—trying to create a \"warm glow\" of discharged duty.\n\nAfterward, your stated probability level will be high enough to justify your keeping the plans and beliefs you started with, but not so high as to evoke incredulity from yourself or other rationalists.\n\nWhen you’re really curious, you’ll gravitate to inquiries that seem most promising of producing shifts in belief, or inquiries that are least like the ones you’ve tried before. Afterward, your probability distribution likely should *not* look like it did when you started out—shifts should have occurred, whether up or down; and either direction is equally fine to you, if you’re genuinely curious.\n\nContrast this to the subconscious motive of keeping your inquiry on familiar ground, so that you can get your investigation over with quickly, so that you can *have investigated*, and restore the familiar balance on which your familiar old plans and beliefs are based.\n\nAs for what I think true curiosity should look like, and the power that it holds, I refer you to “[A Fable of Science and Politics](https://www.lesswrong.com/rationality/a-fable-of-science-and-politics)” in the first book of this series, *Map and Territory*. The fable showcases the reactions of different characters to an astonishing discovery, with each character’s response intended to illustrate different lessons. Ferris, the last character, embodies the power of innocent curiosity: which is lightness, and an eager reaching forth for evidence.\n\nUrsula K. LeGuin wrote: “In innocence there is no strength against evil. But there is strength in it for good.”[^1^](#fn1x71) Innocent curiosity may turn innocently awry; and so the training of a rationalist, and its accompanying sophistication, must be dared as a danger if we want to become stronger. Nonetheless we can try to keep the lightness and the eager reaching of innocence.\n\nAs it is written in “The Twelve Virtues of Rationality”:\n\n> If in your heart you believe you already know, or if in your heart you do not wish to know, then your questioning will be purposeless and your skills without direction. Curiosity seeks to annihilate itself; there is no curiosity that does not want an answer.\n\nThere just isn’t any good substitute for genuine curiosity. A burning itch to know is higher than a solemn vow to pursue truth. But you can’t produce curiosity just by willing it, any more than you can will your foot to feel warm when it feels cold. Sometimes, all we have is our mere solemn vows.\n\nSo what can you do with duty? For a start, we can try to take an interest in our dutiful investigations—keep a close eye out for sparks of genuine intrigue, or even genuine ignorance and a desire to resolve it. This goes right along with keeping a special eye out for possibilities that are painful, that you are flinching away from—it’s not all negative thinking.\n\nIt should also help to meditate on “[Conservation of Expected Evidence](https://www.lesswrong.com/rationality/conservation-of-expected-evidence).” For every *new* point of inquiry, for every piece of *unseen* evidence that you suddenly look at, the expected posterior probability should equal your prior probability. In the microprocess of inquiry, your belief should always be evenly poised to shift in either direction. Not every point may suffice to blow the issue wide open—to shift belief from 70% to 30% probability—but if your current belief is 70%, you should be as ready to drop it to 69% as raise it to 71%. You should not think that you know which direction it will go in (on average), because by the laws of probability theory, if you know your destination, you are already there. If you can investigate honestly, so that each *new* point really does have equal potential to shift belief upward or downward, this may help to keep you interested or even curious about the microprocess of inquiry.\n\nIf the argument you are considering is *not* new, then why is your attention going here? Is this where you would look if you were genuinely curious? Are you subconsciously criticizing your belief at its strong points, rather than its weak points? Are you rehearsing the evidence?\n\nIf you can manage not to rehearse already known support, and you can manage to drop down your belief by one tiny bite at a time from the new evidence, you may even be able to relinquish the belief entirely—to realize from which quarter the winds of evidence are blowing against you.\n\nAnother restorative for curiosity is what I have taken to calling the Litany of Tarski, which is really a meta-litany that specializes for each instance (this is only appropriate). For example, if I am tensely wondering whether a locked box contains a diamond, then rather than thinking about all the wonderful consequences if the box does contain a diamond, I can repeat the Litany of Tarski:\n\n> *If the box contains a diamond,*  \n> *I desire to believe that the box contains a diamond;*  \n> *If the box does not contain a diamond,*  \n> *I desire to believe that the box does not contain a diamond;*  \n> *Let me not become attached to beliefs I may not want.*\n\nThen you should meditate upon the possibility that there is no diamond, and the subsequent advantage that will come to you if you believe there is no diamond, and the subsequent disadvantage if you believe there is a diamond. See also the Litany of Gendlin.\n\nIf you can find within yourself the slightest shred of true uncertainty, then guard it like a forester nursing a campfire. If you can make it blaze up into a flame of curiosity, it will make you light and eager, and give purpose to your questioning and direction to your skills.\n\n[^1^](#fn1x71-bk)Ursula K. Le Guin, *The Farthest Shore* (Saga Press, 2001)."
    },
    "voteCount": 82,
    "forceInclude": true
  },
  {
    "_id": "wCqfCLs8z5Qw4GbKS",
    "url": null,
    "title": "The Importance of Saying \"Oops\"",
    "slug": "the-importance-of-saying-oops",
    "author": "Eliezer_Yudkowsky",
    "question": false,
    "tags": [
      {
        "name": "Honesty"
      },
      {
        "name": "Self-Deception"
      },
      {
        "name": "Epistemology"
      },
      {
        "name": "Changing Your Mind"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "I just finished reading a history of Enron’s downfall, _The Smartest Guys in the Room_, which hereby wins my award for “Least Appropriate Book Title.”\n\nAn unsurprising feature of Enron’s slow rot and abrupt collapse was that the executive players never admitted to having made a _large_ mistake. When catastrophe #247 grew to such an extent that it required an actual policy change, they would say, “Too bad that didn’t work out—it was such a good idea—how are we going to hide the problem on our balance sheet?” As opposed to, “It now seems obvious in retrospect that it was a mistake from the beginning.” As opposed to, “I’ve been stupid.” There was never a watershed moment, a moment of humbling realization, of acknowledging a _fundamental_ problem. After the bankruptcy, Jeff Skilling, the former COO and brief CEO of Enron, declined his own lawyers’ advice to take the Fifth Amendment; he testified before Congress that Enron had been a _great_ company.\n\nNot every change is an improvement, but every improvement is necessarily a change. If we only admit small local errors, we will only make small local changes. The motivation for a _big_ change comes from acknowledging a _big_ mistake.\n\nAs a child I was raised on equal parts science and science fiction, and from Heinlein to Feynman I learned the tropes of Traditional Rationality: theories must be bold and expose themselves to falsification; be willing to commit the heroic sacrifice of giving up your own ideas when confronted with contrary evidence; play nice in your arguments; try not to deceive yourself; and other fuzzy verbalisms.\n\nA traditional rationalist upbringing tries to produce arguers who will concede to contrary evidence _eventually_—there should be _some_ mountain of evidence sufficient to move you. This is not trivial; it distinguishes science from religion. But there is less focus on _speed_, on giving up the fight _as quickly as possible_, integrating evidence _efficiently_ so that it only takes a _minimum_ of contrary evidence to destroy your cherished belief.\n\nI was raised in Traditional Rationality, and thought myself quite the rationalist. I switched to Bayescraft (Laplace / Jaynes / Tversky / Kahneman) in the aftermath of . . . well, it’s a long story. Roughly, I switched because I realized that Traditional Rationality’s fuzzy verbal tropes had been insufficient to prevent me from making a large mistake.\n\nAfter I had finally and fully admitted my mistake, I looked back upon the path that had led me to my Awful Realization. And I saw that I had made a series of small concessions, minimal concessions, grudgingly conceding each millimeter of ground, realizing as little as possible of my mistake on each occasion, admitting failure only in small tolerable nibbles. I could have moved so much faster, I realized, if I had simply screamed “_OOPS!_”\n\nAnd I thought: _I must raise the level of my game._\n\nThere is a _powerful advantage_ to admitting you have made a _large_ mistake. It’s painful. It can also change your whole life.\n\nIt is _important_ to have the watershed moment, the moment of humbling realization. To acknowledge a _fundamental_ problem, not divide it into palatable bite-size mistakes.\n\nDo not indulge in drama and become proud of admitting errors. It is surely superior to get it right the first time. But if you do make an error, better by far to see it all at once. Even hedonically, it is better to take one large loss than many small ones. The alternative is stretching out the battle with yourself over years. The alternative is Enron.\n\nSince then I have watched others making their own series of minimal concessions, grudgingly conceding each millimeter of ground; never confessing a global mistake where a local one will do; always learning as little as possible from each error. What they could fix in one fell swoop voluntarily, they transform into tiny local patches they must be argued into. Never do they say, after confessing one mistake, _I’ve been a fool._ They do their best to minimize their embarrassment by saying _I was right in principle_, or _It could have worked_, or _I still want to embrace the true essence of whatever-I’m-attached-to._ Defending their pride in this passing moment, they ensure they will again make the same mistake, and again need to defend their pride.\n\nBetter to swallow the entire bitter pill in one terrible gulp."
    },
    "voteCount": 113,
    "forceInclude": true
  },
  {
    "_id": "teaxCFgtmCQ3E9fy8",
    "url": null,
    "title": "The Martial Art of Rationality",
    "slug": "the-martial-art-of-rationality",
    "author": "Eliezer Yudkowsky",
    "question": false,
    "tags": [
      {
        "name": "Rationality"
      },
      {
        "name": "Cultural knowledge"
      },
      {
        "name": "Habits"
      },
      {
        "name": "Rationality Verification"
      },
      {
        "name": "Motivational Intro Posts"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "I often use the metaphor that [rationality](http://wiki.lesswrong.com/wiki/Rationality) is the [martial art of mind](http://wiki.lesswrong.com/wiki/Rationality_as_martial_art). You don’t need huge, bulging muscles to learn martial arts—there’s a tendency toward more athletic people being more likely to learn martial arts, but that may be a matter of *enjoyment* as much as anything else. If you have a hand, with tendons and muscles in the appropriate places, then you can learn to make a fist.\n\nSimilarly, if you have a brain, with cortical and subcortical areas in the appropriate places, you might be able to learn to use it properly. If you’re a fast learner, you might learn faster—but the art of rationality isn’t about that; it’s about training brain machinery we all have in common. And where there are systematic errors human brains tend to make—like an insensitivity to scope—rationality is about fixing those mistakes, or finding work-arounds.\n\nAlas, our minds respond less readily to our will than our hands. Our ability to control our muscles is evolutionarily ancient; our ability to reason about our own reasoning processes is a much more recent innovation. We shouldn’t be surprised, then, that muscles are easier to use than brains. But it is not wise to neglect the latter training because it is more difficult. It is not by bigger muscles that the human species rose to prominence upon Earth.\n\nIf you live in an urban area, you probably don’t need to walk very far to find a martial arts dojo. Why aren’t there dojos that teach rationality?\n\nOne reason, perhaps, is that it’s harder to verify skill. To rise a level in Tae Kwon Do, you might need to break a board of a certain width. If you succeed, all the onlookers can see and applaud. If you fail, your teacher can watch how you shape a fist, and check if you shape it correctly. If not, the teacher holds out a hand and makes a fist correctly, so that you can observe how to do so.\n\nWithin martial arts schools, techniques of muscle have been refined and elaborated over generations. Techniques of rationality are harder to pass on, even to the most willing student.\n\nVery recently—in just the last few decades—the human species has acquired a great deal of new knowledge about human rationality. The most salient example would be the [heuristics and biases](https://en.wikipedia.org/wiki/Heuristics_in_judgment_and_decision-making) program in experimental psychology. There is also the [Bayesian](https://wiki.lesswrong.com/wiki/Bayesian) systematization of probability theory and statistics; evolutionary psychology; social psychology. Experimental investigations of empirical human psychology; and theoretical probability theory to interpret what our experiments tell us; and evolutionary theory to explain the conclusions. These fields give us new focusing lenses through which to view the landscape of our own minds. With their aid, we may be able to see more clearly the muscles of our brains, the fingers of thought as they move. We have a shared vocabulary in which to describe problems and solutions. Humanity may finally be ready to synthesize the martial art of mind: to refine, share, systematize, and pass on techniques of personal rationality.\n\nSuch understanding as I have of rationality, I acquired in the course of wrestling with the challenge of artificial general intelligence (an endeavor which, to actually succeed, would require sufficient mastery of rationality to build a complete working rationalist out of toothpicks and rubber bands). In most ways the AI problem is enormously more demanding than the personal art of rationality, but in some ways it is actually easier. In the martial art of mind, we need to acquire the realtime procedural skill of pulling the right levers at the right time on a large, pre-existing thinking machine whose innards are not end-user-modifiable. Some of the machinery is optimized for evolutionary selection pressures that run directly counter to our declared goals in using it. Deliberately we decide that we want to seek only the truth; but our brains have hardwired support for rationalizing falsehoods. We can try to compensate for what we choose to regard as flaws of the machinery; but we can’t actually rewire the neural circuitry. Nor may martial artists plate titanium over their bones—not today, at any rate.\n\nTrying to synthesize a personal art of rationality, using the science of rationality, may prove awkward: One imagines trying to invent a martial art using an abstract theory of physics, game theory, and human anatomy.\n\nBut humans arent reflectively blind. We do have a native instinct for introspection. The inner eye isnt sightless, though it sees blurrily, with systematic distortions. We need, then, to *apply* the science to our intuitions, to use the abstract knowledge to *correct* our mental movements and *augment* our metacognitive skills.\n\nWe aren't writing a computer program to make a string puppet execute martial arts forms; it is our own mental limbs that we must move. Therefore we must connect theory to practice. We must come to see what the science means, for ourselves, for our daily inner life."
    },
    "voteCount": 176,
    "forceInclude": true
  },
  {
    "_id": "7ZqGiPHTpiDMwqMN2",
    "url": null,
    "title": "Twelve Virtues of Rationality",
    "slug": "twelve-virtues-of-rationality",
    "author": "Eliezer Yudkowsky",
    "question": false,
    "tags": [
      {
        "name": "Virtues"
      },
      {
        "name": "Rationality"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "The first virtue is curiosity. A burning itch to know is higher than a solemn vow to pursue truth. To feel the burning itch of curiosity requires both that you be ignorant, and that you desire to relinquish your ignorance. If in your heart you believe you already know, or if in your heart you do not wish to know, then your questioning will be purposeless and your skills without direction. Curiosity seeks to annihilate itself; there is no curiosity that does not want an answer. The glory of glorious mystery is to be solved, after which it ceases to be mystery. Be wary of those who speak of being open-minded and modestly confess their ignorance.\n\nThere is a time to confess your ignorance and a time to relinquish your ignorance. The second virtue is relinquishment. P. C. Hodgell said: “That which can be destroyed by the truth should be.”\\[1\\] Do not flinch from experiences that might destroy your beliefs. The thought you cannot think controls you more than thoughts you speak aloud. Submit yourself to ordeals and test yourself in fire. Relinquish the emotion which rests upon a mistaken belief, and seek to feel fully that emotion which fits the facts. If the iron approaches your face, and you believe it is hot, and it is cool, the Way opposes your fear. If the iron approaches your face, and you believe it is cool, and it is hot, the Way opposes your calm. Evaluate your beliefs first and then arrive at your emotions. Let yourself say: “If the iron is hot, I desire to believe it is hot, and if it is cool, I desire to believe it is cool.” Beware lest you become attached to beliefs you may not want.\n\nThe third virtue is lightness. Let the winds of evidence blow you about as though you are a leaf, with no direction of your own. Beware lest you fight a rearguard retreat against the evidence, grudgingly conceding each foot of ground only when forced, feeling cheated. Surrender to the truth as quickly as you can. Do this the instant you realize what you are resisting, the instant you can see from which quarter the winds of evidence are blowing against you. Be faithless to your cause and betray it to a stronger enemy. If you regard evidence as a constraint and seek to free yourself, you sell yourself into the chains of your whims. For you cannot make a true map of a city by sitting in your bedroom with your eyes shut and drawing lines upon paper according to impulse. You must walk through the city and draw lines on paper that correspond to what you see. If, seeing the city unclearly, you think that you can shift a line just a little to the right, just a little to the left, according to your caprice, this is just the same mistake.\n\nThe fourth virtue is evenness. One who wishes to believe says, “Does the evidence permit me to believe?” One who wishes to disbelieve asks, “Does the evidence force me to believe?” Beware lest you place huge burdens of proof only on propositions you dislike, and then defend yourself by saying: “But it is good to be skeptical.” If you attend only to favorable evidence, picking and choosing from your gathered data, then the more data you gather, the less you know. If you are selective about which arguments you inspect for flaws, or how hard you inspect for flaws, then every flaw you learn how to detect makes you that much stupider. If you first write at the bottom of a sheet of paper “And therefore, the sky is green!” it does not matter what arguments you write above it afterward; the conclusion is already written, and it is already correct or already wrong. To be clever in argument is not rationality but rationalization. Intelligence, to be useful, must be used for something other than defeating itself. Listen to hypotheses as they plead their cases before you, but remember that you are not a hypothesis; you are the judge. Therefore do not seek to argue for one side or another, for if you knew your destination, you would already be there.\n\nThe fifth virtue is argument. Those who wish to fail must first prevent their friends from helping them. Those who smile wisely and say “I will not argue” remove themselves from help and withdraw from the communal effort. In argument strive for exact honesty, for the sake of others and also yourself: the part of yourself that distorts what you say to others also distorts your own thoughts. Do not believe you do others a favor if you accept their arguments; the favor is to you. Do not think that fairness to all sides means balancing yourself evenly between positions; truth is not handed out in equal portions before the start of a debate. You cannot move forward on factual questions by fighting with fists or insults. Seek a test that lets reality judge between you.\n\nThe sixth virtue is empiricism. The roots of knowledge are in observation and its fruit is prediction. What tree grows without roots? What tree nourishes us without fruit? If a tree falls in a forest and no one hears it, does it make a sound? One says, “Yes it does, for it makes vibrations in the air.” Another says, “No it does not, for there is no auditory processing in any brain.” Though they argue, one saying “Yes,” and one saying “No,” the two do not anticipate any different experience of the forest. Do not ask which beliefs to profess, but which experiences to anticipate. Always know which difference of experience you argue about. Do not let the argument wander and become about something else, such as someone’s virtue as a rationalist. Jerry Cleaver said: “What does you in is not failure to apply some high-level, intricate, complicated technique. It’s overlooking the basics. Not keeping your eye on the ball.”\\[2\\] Do not be blinded by words. When words are subtracted, anticipation remains.\n\nThe seventh virtue is simplicity. Antoine de Saint-Exupéry said: “Perfection is achieved not when there is nothing left to add, but when there is nothing left to take away.”\\[3\\] Simplicity is virtuous in belief, design, planning, and justification. When you profess a huge belief with many details, each additional detail is another chance for the belief to be wrong. Each specification adds to your burden; if you can lighten your burden you must do so. There is no straw that lacks the power to break your back. Of artifacts it is said: The most reliable gear is the one that is designed out of the machine. Of plans: A tangled web breaks. A chain of a thousand links will arrive at a correct conclusion if every step is correct, but if one step is wrong it may carry you anywhere. In mathematics a mountain of good deeds cannot atone for a single sin. Therefore, be careful on every step.\n\nThe eighth virtue is humility. To be humble is to take specific actions in anticipation of your own errors. To confess your fallibility and then do nothing about it is not humble; it is boasting of your modesty. Who are most humble? Those who most skillfully prepare for the deepest and most catastrophic errors in their own beliefs and plans. Because this world contains many whose grasp of rationality is abysmal, beginning students of rationality win arguments and acquire an exaggerated view of their own abilities. But it is useless to be superior: Life is not graded on a curve. The best physicist in ancient Greece could not calculate the path of a falling apple. There is no guarantee that adequacy is possible given your hardest effort; therefore spare no thought for whether others are doing worse. If you compare yourself to others you will not see the biases that all humans share. To be human is to make ten thousand errors. No one in this world achieves perfection.\n\nThe ninth virtue is perfectionism. The more errors you correct in yourself, the more you notice. As your mind becomes more silent, you hear more noise. When you notice an error in yourself, this signals your readiness to seek advancement to the next level. If you tolerate the error rather than correcting it, you will not advance to the next level and you will not gain the skill to notice new errors. In every art, if you do not seek perfection you will halt before taking your first steps. If perfection is impossible that is no excuse for not trying. Hold yourself to the highest standard you can imagine, and look for one still higher. Do not be content with the answer that is almost right; seek one that is exactly right.\n\nThe tenth virtue is precision. One comes and says: The quantity is between 1 and 100. Another says: The quantity is between 40 and 50. If the quantity is 42 they are both correct, but the second prediction was more useful and exposed itself to a stricter test. What is true of one apple may not be true of another apple; thus more can be said about a single apple than about all the apples in the world. The narrowest statements slice deepest, the cutting edge of the blade. As with the map, so too with the art of mapmaking: The Way is a precise Art. Do not walk to the truth, but dance. On each and every step of that dance your foot comes down in exactly the right spot. Each piece of evidence shifts your beliefs by exactly the right amount, neither more nor less. What is exactly the right amount? To calculate this you must study probability theory. Even if you cannot do the math, knowing that the math exists tells you that the dance step is precise and has no room in it for your whims.\n\nThe eleventh virtue is scholarship. Study many sciences and absorb their power as your own. Each field that you consume makes you larger. If you swallow enough sciences the gaps between them will diminish and your knowledge will become a unified whole. If you are gluttonous you will become vaster than mountains. It is especially important to eat math and science which impinge upon rationality: evolutionary psychology, heuristics and biases, social psychology, probability theory, decision theory. But these cannot be the only fields you study. The Art must have a purpose other than itself, or it collapses into infinite recursion.\n\nBefore these eleven virtues is a virtue which is nameless.\n\nMiyamoto Musashi wrote, in _The Book of Five Rings_:\\[4\\]\n\n> The primary thing when you take a sword in your hands is your intention to cut the enemy, whatever the means. Whenever you parry, hit, spring, strike or touch the enemy’s cutting sword, you must cut the enemy in the same movement. It is essential to attain this. If you think only of hitting, springing, striking or touching the enemy, you will not be able actually to cut him. More than anything, you must be thinking of carrying your movement through to cutting him.\n\nEvery step of your reasoning must cut through to the correct answer in the same movement. More than anything, you must think of carrying your map through to reflecting the territory.\n\nIf you fail to achieve a correct answer, it is futile to protest that you acted with propriety.\n\nHow can you improve your conception of rationality? Not by saying to yourself, “It is my duty to be rational.” By this you only enshrine your mistaken conception. Perhaps your conception of rationality is that it is rational to believe the words of the Great Teacher, and the Great Teacher says, “The sky is green,” and you look up at the sky and see blue. If you think, “It may look like the sky is blue, but rationality is to believe the words of the Great Teacher,” you lose a chance to discover your mistake.\n\nDo not ask whether it is “the Way” to do this or that. Ask whether the sky is blue or green. If you speak overmuch of the Way you will not attain it. You may try to name the highest principle with names such as “the map that reflects the territory” or “experience of success and failure” or “Bayesian decision theory.” But perhaps you describe incorrectly the nameless virtue. How will you discover your mistake? Not by comparing your description to itself, but by comparing it to that which you did not name.\n\nIf for many years you practice the techniques and submit yourself to strict constraints, it may be that you will glimpse the center. Then you will see how all techniques are one technique, and you will move correctly without feeling constrained. Musashi wrote: “When you appreciate the power of nature, knowing the rhythm of any situation, you will be able to hit the enemy naturally and strike naturally. All this is the Way of the Void.”\n\nThese then are twelve virtues of rationality:\n\nCuriosity, relinquishment, lightness, evenness, argument, empiricism, simplicity, humility, perfectionism, precision, scholarship, and the void.\n\n* * *\n\n###### 1\\. Patricia C. Hodgell, _Seeker’s Mask_ (Meisha Merlin Publishing, Inc., 2001).\n\n###### 2\\. Cleaver, _Immediate Fiction: A Complete Writing Course_.\n\n###### 3\\. Antoine de Saint-Exupéry, _Terre des Hommes_ (Paris: Gallimard, 1939).\n\n###### 4\\. Musashi, _Book of Five Rings_.\n\n* * *\n\n_The first publication of this post is_ [_here_](http://yudkowsky.net/rational/virtues/)_._"
    },
    "voteCount": 86,
    "forceInclude": true
  },
  {
    "_id": "34XxbRFe54FycoCDw",
    "url": null,
    "title": "The Bottom Line",
    "slug": "the-bottom-line",
    "author": "Eliezer_Yudkowsky",
    "question": false,
    "tags": [
      {
        "name": "Rationality"
      },
      {
        "name": "Rationalization"
      },
      {
        "name": "Filtered Evidence"
      },
      {
        "name": "Litany of Tarski"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "There are two sealed boxes up for auction, box A and box B. One and only one of these boxes contains a valuable diamond. There are all manner of signs and portents indicating whether a box contains a diamond; but I have no sign which I _know_ to be perfectly reliable. There is a blue stamp on one box, for example, and I know that boxes which contain diamonds are more likely than empty boxes to show a blue stamp. Or one box has a shiny surface, and I have a suspicion—I am not sure—that no diamond-containing box is ever shiny.\n\nNow suppose there is a clever arguer, holding a sheet of paper, and they say to the owners of box A and box B: “Bid for my services, and whoever wins my services, I shall argue that their box contains the diamond, so that the box will receive a higher price.” So the box-owners bid, and box B’s owner bids higher, winning the services of the clever arguer.\n\nThe clever arguer begins to organize their thoughts. First, they write, “And _therefore_, box B contains the diamond!” at the bottom of their sheet of paper. Then, at the top of the paper, the clever arguer writes, “Box B shows a blue stamp,” and beneath it, “Box A is shiny,” and then, “Box B is lighter than box A,” and so on through many signs and portents; yet the clever arguer neglects all those signs which might argue in favor of box A. And then the clever arguer comes to me and recites from their sheet of paper: “Box B shows a blue stamp, and box A is shiny,” and so on, until they reach: “and _therefore_, box B contains the diamond.”\n\nBut consider: At the moment when the clever arguer wrote down their conclusion, at the moment they put ink on their sheet of paper, the evidential entanglement of that physical ink with the physical boxes became fixed.\n\nIt may help to visualize a collection of worlds—Everett branches or Tegmark duplicates—within which there is some objective frequency at which box A or box B contains a diamond.^[1](#fn1x27)^\n\nThe ink on paper is formed into odd shapes and curves, which look like this text: “And _therefore_, box B contains the diamond.” If you happened to be a literate English speaker, you might become confused, and think that this shaped ink somehow _meant_ that box B contained the diamond. Subjects instructed to say the color of printed pictures and shown the word Green in red ink often say “green” instead of “red.” It helps to be illiterate, so that you are not confused by the shape of the ink.\n\nTo us, the true import of a thing is its entanglement with other things. Consider again the collection of worlds, Everett branches or Tegmark duplicates. At the moment when all clever arguers in all worlds put ink to the bottom line of their paper—let us suppose this is a single moment—it fixed the correlation of the ink with the boxes. The clever arguer writes in non-erasable pen; the ink will not change. The boxes will not change. Within the subset of worlds where the ink says “And therefore, box B contains the diamond,” there is already some fixed percentage of worlds where box A contains the diamond. This will not change regardless of what is written in on the blank lines above.\n\nSo the evidential entanglement of the ink is fixed, and I leave to you to decide what it might be. Perhaps box owners who believe a better case can be made for them are more liable to hire advertisers; perhaps box owners who fear their own deficiencies bid higher. If the box owners do not themselves understand the signs and portents, then the ink will be completely unentangled with the boxes’ contents, though it may tell you something about the owners’ finances and bidding habits.\n\nNow suppose another person present is genuinely curious, and they _first_ write down all the distinguishing signs of _both_ boxes on a sheet of paper, and then apply their knowledge and the laws of probability and write down at the bottom: “_Therefore,_ I estimate an 85% probability that box B contains the diamond.” Of what is this handwriting evidence? Examining the chain of cause and effect leading to this physical ink on physical paper, I find that the chain of causality wends its way through all the signs and portents of the boxes, and is dependent on these signs; for in worlds with different portents, a different probability is written at the bottom.\n\nSo the handwriting of the curious inquirer is entangled with the signs and portents and the contents of the boxes, whereas the handwriting of the clever arguer is evidence only of which owner paid the higher bid. There is a great difference in the indications of ink, though one who foolishly read aloud the ink-shapes might think the English words sounded similar.\n\nYour effectiveness as a rationalist is determined by whichever algorithm actually writes the bottom line of your thoughts. If your car makes metallic squealing noises when you brake, and you aren’t willing to face up to the financial cost of getting your brakes replaced, you can decide to look for reasons why your car might not need fixing. But the actual percentage of you that survive in Everett branches or Tegmark worlds—which we will take to describe your effectiveness as a rationalist—is determined by the algorithm that decided _which_ conclusion you would seek arguments for. In this case, the real algorithm is “Never repair anything expensive.” If this is a good algorithm, fine; if this is a bad algorithm, oh well. The arguments you write afterward, above the bottom line, will not change anything either way.\n\nThis is intended as a caution for your own thinking, not a Fully General Counterargument against conclusions you don’t like. For it is indeed a clever argument to say “My opponent is a clever arguer,” if you are paying yourself to retain whatever beliefs you had at the start. The world’s cleverest arguer may point out that the Sun is shining, and yet it is still probably daytime.\n\n^[1](#fn1x27-bk)^Max Tegmark, “Parallel Universes,” in _Science and Ultimate Reality: Quantum Theory, Cosmology, and Complexity_, ed. John D. Barrow, Paul C. W. Davies, and Charles L. Harper Jr. (New York: Cambridge University Press, 2004), 459–491, [http://arxiv.org/abs/astro-ph/0302131](http://arxiv.org/abs/astro-ph/0302131)."
    },
    "voteCount": 92,
    "forceInclude": true
  },
  {
    "_id": "SFZoEBpLo9frSJGkc",
    "url": null,
    "title": "Rationalization",
    "slug": "rationalization",
    "author": "Eliezer Yudkowsky",
    "question": false,
    "tags": [
      {
        "name": "Rationalization"
      },
      {
        "name": "Rationality"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "In “The Bottom Line,” I presented the dilemma of two boxes, only one of which contains a diamond, with various signs and portents as evidence. I dichotomized the curious inquirer and the clever arguer. The curious inquirer writes down all the signs and portents, and processes them, and finally writes down, “*Therefore,* I estimate an 85% probability that box B contains the diamond.” The clever arguer works for the highest bidder, and begins by writing, “*Therefore,* box B contains the diamond,” and then selects favorable signs and portents to list on the lines above.\n\nThe first procedure is rationality. The second procedure is generally known as “rationalization.”\n\n“Rationalization.” What a curious term. I would call it a *wrong word.* You cannot “rationalize” what is not already rational. It is as if “lying” were called “truthization.”\n\nOn a purely computational level, there is a rather large difference between:\n\n1.  Starting from evidence, and then crunching probability flows, in order to output a probable conclusion. (Writing down all the signs and portents, and then flowing forward to a probability on the bottom line which depends on those signs and portents.) \n2.  Starting from a conclusion, and then crunching probability flows, in order to output evidence apparently favoring that conclusion. (Writing down the bottom line, and then flowing backward to select signs and portents for presentation on the lines above.)\n\nWhat fool devised such confusingly similar words, “rationality” and “rationalization,” to describe such extraordinarily different mental processes? I would prefer terms that made the algorithmic difference obvious, like “rationality” versus “giant sucking cognitive black hole.”\n\nNot every change is an improvement, but every improvement is necessarily a change. You cannot obtain more truth for a fixed proposition by arguing it; you can make more people believe it, but you cannot make it more *true*. To improve our beliefs, we must necessarily change our beliefs. Rationality is the operation that we use to obtain more accuracy for our beliefs by changing them. Rationalization operates to fix beliefs in place; it would be better named “anti-rationality,” both for its pragmatic results and for its reversed algorithm.\n\n“Rationality” is the *forward* flow that gathers evidence, weighs it, and outputs a conclusion. The curious inquirer used a forward-flow algorithm: *first* gathering the evidence, writing down a list of all visible signs and portents, which they then processed *forward* to obtain a previously unknown probability for the box containing the diamond. During the entire time that the rationality-process was running forward, the curious inquirer did not yet know their destination, which was why they were *curious.* In the Way of Bayes, the prior probability equals the expected posterior probability: If you know your destination, you are already there.\n\n“Rationalization” is a *backward* flow from conclusion to selected evidence. First you write down the bottom line, which is known and fixed; the purpose of your processing is to find out which arguments you should write down on the lines above. This, not the bottom line, is the variable unknown to the running process.\n\nI fear that Traditional Rationality does not properly sensitize its users to the difference between forward flow and backward flow. In Traditional Rationality, there is nothing wrong with the scientist who arrives at a pet hypothesis and then sets out to find an experiment that proves it. A Traditional Rationalist would look at this approvingly, and say, “This pride is the engine that drives Science forward.” Well, it *is* the engine that drives Science forward. It is easier to find a prosecutor and defender biased in opposite directions, than to find a single unbiased human.\n\nBut just because everyone does something, doesn’t make it okay. It would be better yet if the scientist, arriving at a pet hypothesis, set out to *test* that hypothesis for the sake of *curiosity*—creating experiments that would drive their own beliefs in an unknown direction.\n\nIf you genuinely don’t know where you are going, you will probably feel quite curious about it. Curiosity is the first virtue, without which your questioning will be purposeless and your skills without direction.\n\nFeel the flow of the Force, and make sure it isn’t flowing backwards."
    },
    "voteCount": 47,
    "forceInclude": true
  },
  {
    "_id": "HYWhKXRsMAyvRKRYz",
    "url": null,
    "title": "You Can Face Reality",
    "slug": "you-can-face-reality",
    "author": "Eliezer_Yudkowsky",
    "question": false,
    "tags": [
      {
        "name": "Poetry"
      },
      {
        "name": "Litanies & Mantras"
      },
      {
        "name": "Rationality"
      },
      {
        "name": "Courage"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "What is true is already so.\n\nOwning up to it doesn’t make it worse.\n\nNot being open about it doesn’t make it go away.\n\nAnd because it’s true, it is what is there to be interacted with.\n\nAnything untrue isn’t there to be lived.\n\nPeople can stand what is true,\n\nfor they are already enduring it.\n\n> —_Eugene Gendlin_"
    },
    "voteCount": 99,
    "forceInclude": true
  },
  {
    "_id": "TGux5Fhcd7GmTfNGC",
    "url": null,
    "title": "Is That Your True Rejection?",
    "slug": "is-that-your-true-rejection",
    "author": "Eliezer_Yudkowsky",
    "question": false,
    "tags": [
      {
        "name": "Rationalization"
      },
      {
        "name": "Rationality"
      },
      {
        "name": "Motivated Reasoning"
      },
      {
        "name": "Motivations"
      },
      {
        "name": "Conversation (topic)"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "It happens every now and then that someone encounters some of my transhumanist-side beliefs—as opposed to my ideas having to do with human rationality—strange, exotic-sounding ideas like superintelligence and Friendly AI. And the one rejects them.\n\nIf the one is called upon to explain the rejection, not uncommonly the one says, “Why should I believe anything Yudkowsky says? He doesn’t have a PhD!”\n\nAnd occasionally someone else, hearing, says, “Oh, you should get a PhD, so that people will listen to you.” Or this advice may even be offered by the same one who expressed disbelief, saying, “Come back when you have a PhD.”\n\nNow, there are good and bad reasons to get a PhD. This is one of the bad ones.\n\nThere are many reasons why someone might actually have an initial adverse reaction to transhumanist theses. Most are matters of pattern recognition, rather than verbal thought: the thesis calls to mind an associated category like “strange weird idea” or “science fiction” or “end-of-the-world cult” or “overenthusiastic youth.”^[1](#fn1x34)^ Immediately, at the speed of perception, the idea is rejected.\n\nIf someone afterward says, “Why not?” this launches a search for justification, but the search won’t necessarily hit on the true reason. By “‘true reason,” I don’t mean the _best_ reason that could be offered. Rather, I mean whichever causes were decisive as a matter of historical fact, at the _very first_ moment the rejection occurred.\n\nInstead, the search for justification hits on the justifying-sounding fact, “This speaker does not have a PhD.” But I also don’t have a PhD when I talk about human rationality, so why is the same objection not raised there?\n\nMore to the point, if I _had_ a PhD, people would not treat this as a decisive factor indicating that they ought to believe everything I say. Rather, the same initial rejection would occur, for the same reasons; and the search for justification, afterward, would terminate at a different stopping point.\n\nThey would say, “Why should I believe _you_? You’re just some guy with a PhD! There are lots of those. Come back when you’re well-known in your field and tenured at a major university.”\n\nBut do people _actually_ believe arbitrary professors at Harvard who say weird things? Of course not.\n\nIf you’re saying things that sound _wrong_ to a novice, as opposed to just rattling off magical-sounding technobabble about leptical quark braids in N + 2 dimensions; and if the hearer is a stranger, unfamiliar with you personally and unfamiliar with the subject matter of your field; then I suspect that the point at which the average person will actually start to grant credence overriding their initial impression, purely because of academic credentials, is somewhere around the Nobel Laureate level. If that. Roughly, you need whatever level of academic credential qualifies as “beyond the mundane.”\n\nThis is more or less what happened to Eric Drexler, as far as I can tell. He presented his vision of nanotechnology, and people said, “Where are the technical details?” or “Come back when you have a PhD!” And Eric Drexler spent six years writing up technical details and got his PhD under Marvin Minsky for doing it. And _Nanosystems_ is a great book. But did the same people who said, “Come back when you have a PhD,” actually change their minds at all about molecular nanotechnology? Not so far as I ever heard.\n\nThis might be an important thing for young businesses and new-minted consultants to keep in mind—that what your failed prospects _tell_ you is the reason for rejection may not make the _real_ difference; and you should ponder that carefully before spending huge efforts. If the venture capitalist says, “If only your sales were growing a little faster!” or if the potential customer says, “It seems good, but you don’t have feature X,” that may not be the _true_ rejection. Fixing it may, or may not, change anything.\n\nAnd it would also be something to keep in mind during disagreements. Robin Hanson and I share a belief that two rationalists should not agree to disagree: they should not have common knowledge of epistemic disagreement unless something is very wrong.^[2](#fn2x34)^\n\nI suspect that, in general, if two rationalists set out to resolve a disagreement that persisted past the first exchange, they should expect to find that the true sources of the disagreement are either hard to communicate, or hard to expose. E.g.:\n\n*   Uncommon, but well-supported, scientific knowledge or math;\n*   Long inferential distances;\n*   Hard-to-verbalize intuitions, perhaps stemming from specific visualizations;\n*   Zeitgeists inherited from a profession (that may have good reason for it);\n*   Patterns perceptually recognized from experience;\n*   Sheer habits of thought;\n*   Emotional commitments to believing in a particular outcome;\n*   Fear that a past mistake could be disproved;\n*   Deep self-deception for the sake of pride or other personal benefits.\n\nIf the matter were one in which _all_ the true rejections could be _easily_ laid on the table, the disagreement would probably be so straightforward to resolve that it would never have lasted past the first meeting.\n\n“Is this my true rejection?” is something that both disagreers should surely be asking _themselves_, to make things easier on the other person. However, attempts to directly, publicly psychoanalyze the other may cause the conversation to degenerate _very_ fast, from what I’ve seen.\n\nStill—“Is that your true rejection?” should be fair game for Disagreers to humbly ask, if there’s any productive way to pursue that sub-issue. Maybe the rule could be that you can openly ask, “Is that simple straightforward-sounding reason your _true_ rejection, or does it come from intuition-X or professional-zeitgeist-Y ?” While the more embarrassing possibilities lower on the table are left to the Other’s conscience, as their own responsibility to handle.\n\n^[1](#fn1x34-bk)^See “[Science as Attire](https://lesswrong.com/rationality/science-as-attire)” in _Map and Territory_.\n\n^[2](#fn2x34-bk)^See Hal Finney, “Agreeing to Agree,” _Overcoming Bias_ (blog), 2006, [http://www.overcomingbias.com/2006/12/agreeing\\_to\\_agr.html](http://www.overcomingbias.com/2006/12/agreeing_to_agr.html)."
    },
    "voteCount": 91,
    "forceInclude": true
  },
  {
    "_id": "dHQkDNMhj692ayx78",
    "url": null,
    "title": "Avoiding Your Belief's Real Weak Points",
    "slug": "avoiding-your-belief-s-real-weak-points",
    "author": "Eliezer_Yudkowsky",
    "question": false,
    "tags": [
      {
        "name": "Religion"
      },
      {
        "name": "Motivated Reasoning"
      },
      {
        "name": "Noticing"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "A few years back, my great-grandmother died, in her nineties, after a long, slow, and cruel disintegration. I never knew her as a person, but in my distant childhood, she cooked for her family; I remember her gefilte fish, and her face, and that she was kind to me. At her funeral, my grand-uncle, who had taken care of her for years, spoke. He said, choking back tears, that God had called back his mother piece by piece: her memory, and her speech, and then finally her smile; and that when God finally took her smile, he knew it wouldn’t be long before she died, because it meant that she was almost entirely gone.\n\nI heard this and was puzzled, because it was an unthinkably horrible thing to happen to _anyone_, and therefore I would not have expected my grand-uncle to attribute it to God. Usually, a Jew would somehow just-not-think-about the logical implication that God had permitted a tragedy. According to Jewish theology, God continually sustains the universe and chooses every event in it; but ordinarily, drawing logical implications from this belief is reserved for happier occasions. By saying “God did it!” only when you’ve been blessed with a baby girl, and just-not-thinking “God did it!” for miscarriages and stillbirths and crib deaths, you can build up quite a lopsided picture of your God’s benevolent personality.\n\nHence I was surprised to hear my grand-uncle attributing the slow disintegration of his mother to a deliberate, strategically planned act of God. It violated the rules of religious self-deception as I understood them.\n\nIf I had noticed my own confusion, I could have made a successful surprising prediction. Not long afterward, my grand-uncle left the Jewish religion. (The only member of my extended family besides myself to do so, as far as I know.)\n\nModern Orthodox Judaism is like no other religion I have ever heard of, and I don’t know how to describe it to anyone who hasn’t been forced to study Mishna and Gemara. There is a tradition of questioning, but the _kind_ of questioning . . . It would not be at all surprising to hear a rabbi, in his weekly sermon, point out the conflict between the seven days of creation and the 13.7 billion years since the Big Bang—because he thought he had a really clever explanation for it, involving three other Biblical references, a Midrash, and a half-understood article in _Scientific American._ In Orthodox Judaism you’re allowed to notice inconsistencies and contradictions, but only for purposes of explaining them away, and whoever comes up with the most complicated explanation gets a prize.\n\nThere is a tradition of inquiry. But you only attack targets for purposes of defending them. You only attack targets you know you can defend.\n\nIn Modern Orthodox Judaism I have not heard much emphasis of the virtues of blind faith. You’re allowed to doubt. You’re just not allowed to _successfully_ doubt.\n\nI expect that the vast majority of educated Orthodox Jews have questioned their faith at some point in their lives. But the questioning probably went something like this: “According to the skeptics, the Torah says that the universe was created in seven days, which is not scientifically accurate. But would the original tribespeople of Israel, gathered at Mount Sinai, have been able to understand the scientific truth, even if it had been presented to them? Did they even have a word for ‘billion’? It’s easier to see the seven-days story as a metaphor—first God created light, which represents the Big Bang . . .”\n\nIs this the weakest point at which to attack one’s own Judaism? Read a bit further on in the Torah, and you can find God killing the first-born male children of Egypt to convince an unelected Pharaoh to release slaves who logically could have been teleported out of the country. An Orthodox Jew is most certainly familiar with this episode, because they are supposed to read through the entire Torah in synagogue once per year, and this event has an associated major holiday. The name “Passover” (“Pesach”) comes from God _passing over_ the Jewish households while killing every male firstborn in Egypt.\n\nModern Orthodox Jews are, by and large, kind and civilized people; far more civilized than the several editors of the Old Testament. Even the old rabbis were more civilized. There’s a ritual in the Seder where you take ten drops of wine from your cup, one drop for each of the Ten Plagues, to emphasize the suffering of the Egyptians. (Of course, you’re supposed to be sympathetic to the suffering of the Egyptians, but not _so_ sympathetic that you stand up and say, “This is not right! It is _wrong_ to do such a thing!”) It shows an interesting contrast—the rabbis were sufficiently kinder than the compilers of the Old Testament that they saw the harshness of the Plagues. But Science was weaker in these days, and so rabbis could ponder the more unpleasant aspects of Scripture without fearing that it would break their faith entirely.\n\nYou don’t even _ask_ whether the incident reflects poorly on God, so there’s no need to quickly blurt out “The ways of God are mysterious!” or “We’re not wise enough to question God’s decisions!” or “Murdering babies is okay when God does it!” That part of the question is just-not-thought-about.\n\nThe reason that educated religious people stay religious, I suspect, is that when they doubt, they are subconsciously very careful to attack their own beliefs only at the strongest points—places where they know they can defend. Moreover, places where rehearsing the standard defense will feel strengthening.\n\nIt probably feels really good, for example, to rehearse one’s prescripted defense for “Doesn’t Science say that the universe is just meaningless atoms bopping around?” because it confirms the meaning of the universe and how it flows from God, etc. Much more comfortable to think about than an illiterate Egyptian mother wailing over the crib of her slaughtered son. Anyone who _spontaneously_ thinks about the latter, when questioning their faith in Judaism, is _really_ questioning it, and is probably not going to stay Jewish much longer.\n\nMy point here is not just to beat up on Orthodox Judaism. I’m sure that there’s some reply or other for the Slaying of the Firstborn, and probably a dozen of them. My point is that, when it comes to spontaneous self-questioning, one is much more likely to spontaneously self-attack strong points with comforting replies to rehearse, than to spontaneously self-attack the weakest, most vulnerable points. Similarly, one is likely to stop at the first reply and be comforted, rather than further criticizing the reply. A better title than “Avoiding Your Belief’s Real Weak Points” would be “Not Spontaneously Thinking About Your Belief’s Most Painful Weaknesses.”\n\nMore than anything, the grip of religion is sustained by people just-not-thinking-about the real weak points of their religion. I don’t think this is a matter of training, but a matter of instinct. People don’t think about the real weak points of their beliefs for the same reason they don’t touch an oven’s red-hot burners; it’s _painful._\n\nTo do better: When you’re doubting one of your most cherished beliefs, close your eyes, empty your mind, grit your teeth, and deliberately think about whatever hurts the most. Don’t rehearse standard objections whose standard counters would make you feel better. Ask yourself what _smart_ people who disagree would say to your first reply, and your second reply. Whenever you catch yourself flinching away from an objection you fleetingly thought of, drag it out into the forefront of your mind. Punch yourself in the solar plexus. Stick a knife in your heart, and wiggle to widen the hole. In the face of the pain, rehearse only this:^[1](#fn1x31)^\n\n> What is true is already so.\n> \n> Owning up to it doesn’t make it worse.\n> \n> Not being open about it doesn’t make it go away.\n> \n> And because it’s true, it is what is there to be interacted with.\n> \n> Anything untrue isn’t there to be lived.\n> \n> People can stand what is true,\n> \n> for they are already enduring it.\n\n^[1](#fn1x31-bk)^Eugene T. Gendlin, _Focusing_ (Bantam Books, 1982)."
    },
    "voteCount": 100,
    "forceInclude": true
  },
  {
    "_id": "nYkMLFpx77Rz3uo9c",
    "url": null,
    "title": "Belief as Attire",
    "slug": "belief-as-attire",
    "author": "Eliezer Yudkowsky",
    "question": false,
    "tags": [
      {
        "name": "Social Reality"
      },
      {
        "name": "Rationality"
      },
      {
        "name": "Motivations"
      },
      {
        "name": "Motivated Reasoning"
      },
      {
        "name": "Tribalism"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "I have so far distinguished between belief as [anticipation-controller](https://www.lesswrong.com/lw/i3/making_beliefs_pay_rent_in_anticipated_experiences/), [belief in belief](https://www.lesswrong.com/lw/i4/belief_in_belief/), [professing and cheering](https://www.lesswrong.com/lw/i6/professing_and_cheering/).  Of these, we might call anticipation-controlling beliefs \"proper beliefs\" and the other forms \"improper belief\".  proper belief can be wrong or irrational, as when someone genuinely anticipates that prayer will cure their sick baby. But the other forms are arguably “not belief at all.”\n\nYet another form of improper belief is belief as group identification—as a way of belonging. Robin Hanson uses the excellent [metaphor](http://lesswrong.com/lw/i6/professing_and_cheering/egb) of wearing unusual clothing, a group uniform like a priest’s vestments or a Jewish skullcap, and so I will call this “belief as attire.”\n\nIn terms of [humanly realistic psychology](https://www.lesswrong.com/lw/i0/are_your_enemies_innately_evil/), the Muslims who flew planes into the World Trade Center undoubtedly saw themselves as heroes defending truth, justice, and the Islamic Way from hideous alien monsters a la the movie *Independence Day*. Only a very inexperienced nerd, the sort of nerd who has no idea how non-nerds see the world, would say this out loud in an Alabama bar. It is not an American thing to say. The American thing to say is that the terrorists “hate our freedom” and that flying a plane into a building is a “cowardly act.” You cannot say the phrases “heroic self-sacrifice” and “suicide bomber” in the same sentence, even for the sake of accurately describing how the Enemy sees the world. The very *concept* of the courage and altruism of a suicide bomber is Enemy attire—you can tell, because the Enemy talks about it. The cowardice and sociopathy of a suicide bomber is American attire. There are no quote marks you can use to talk about how the Enemy sees the world; it would be like dressing up as a Nazi for Halloween.\n\nBelief-as-attire may help explain how people can be *passionate* about improper beliefs. Mere belief in belief, or religious professing, would have some trouble creating genuine, deep, powerful emotional effects. Or so I suspect; I confess I’m not an expert here. But my impression is this: People who’ve stopped anticipating-as-if their religion is true, will go to great lengths to *convince* themselves they are passionate, and this desperation can be mistaken for passion. But it’s not the same fire they had as a child.\n\nOn the other hand, it is very easy for a human being to genuinely, passionately, gut-level belong to a group, to cheer for [their favorite sports team](https://www.lesswrong.com/lw/gt/a_fable_of_science_and_politics/).^1^ Identifying with a tribe is a very strong emotional force. People will die for it. And once you get people to identify with a tribe, the beliefs which are the attire of that tribe will be spoken with the full passion of belonging to that tribe.\n\n* * *\n\n^1^ This is the foundation on which rests the swindle of “Republicans vs. Democrats” and analogous [false dilemmas](https://www.lesswrong.com/lw/hu/the_third_alternative/) in other countries, but that’s a topic for another time."
    },
    "voteCount": 83,
    "forceInclude": true
  },
  {
    "_id": "2MD3NMLBPCqPfnfre",
    "url": null,
    "title": "Cached Thoughts",
    "slug": "cached-thoughts",
    "author": "Eliezer_Yudkowsky",
    "question": false,
    "tags": [
      {
        "name": "Rationality"
      },
      {
        "name": "Cached Thoughts"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "One of the single greatest puzzles about the human brain is how the damn thing works _at all_ when most neurons fire 10–20 times per second, or 200Hz tops. In neurology, the “hundred-step rule” is that any postulated operation has to complete in _at most_ 100 sequential steps—you can be as parallel as you like, but you can’t postulate more than 100 (preferably fewer) neural spikes one after the other.\n\nCan you imagine having to program using 100Hz CPUs, no matter how many of them you had? You’d also need a hundred billion processors just to get _anything_ done in realtime.\n\nIf you did need to write realtime programs for a hundred billion 100Hz processors, one trick you’d use as heavily as possible is caching. That’s when you store the results of previous operations and look them up next time, instead of recomputing them from scratch. And it’s a very _neural_ idiom—recognition, association, completing the pattern.\n\nIt’s a good guess that the actual _majority_ of human cognition consists of cache lookups.\n\nThis thought does tend to go through my mind at certain times.\n\nThere was a wonderfully illustrative story which I thought I had bookmarked, but couldn’t re-find: it was the story of a man whose know-it-all neighbor had once claimed in passing that the best way to remove a chimney from your house was to knock out the fireplace, wait for the bricks to drop down one level, knock out those bricks, and repeat until the chimney was gone. Years later, when the man wanted to remove his own chimney, this cached thought was lurking, waiting to pounce . . .\n\nAs the man noted afterward—you can guess it didn’t go well—his neighbor was not particularly knowledgeable in these matters, not a trusted source. If he’d _questioned_ the idea, he probably would have realized it was a poor one. Some cache hits we’d be better off recomputing. But the brain completes the pattern automatically—and if you don’t consciously realize the pattern needs correction, you’ll be left with a completed pattern.\n\nI suspect that if the thought had occurred to the man himself—if he’d _personally_ had this bright idea for how to remove a chimney—he would have examined the idea more critically. But if someone _else_ has already thought an idea through, you can save on computing power by caching their _conclusion_—right?\n\nIn modern civilization particularly, no one can think fast enough to think their own thoughts. If I’d been abandoned in the woods as an infant, raised by wolves or silent robots, I would scarcely be recognizable as human. No one can think fast enough to recapitulate the wisdom of a hunter-gatherer tribe in one lifetime, starting from scratch. As for the wisdom of a literate civilization, forget it.\n\nBut the flip side of this is that I continually see people who aspire to critical thinking, repeating back cached thoughts which were not invented by critical thinkers.\n\nA good example is the skeptic who concedes, “Well, you can’t prove or disprove a religion by factual evidence.” As I have pointed out elsewhere,^[1](#fn1x41)^ this is simply false as probability theory. And it is also simply false relative to the real psychology of religion—a few centuries ago, saying this would have gotten you burned at the stake. A mother whose daughter has cancer prays, “God, please heal my daughter,” not, “Dear God, I know that religions are not allowed to have any falsifiable consequences, which means that you can’t possibly heal my daughter, so . . . well, basically, I’m praying to make myself feel better, instead of doing something that could actually help my daughter.”\n\nBut people read “You can’t prove or disprove a religion by factual evidence,” and then, the next time they see a piece of evidence disproving a religion, their brain completes the pattern. Even some atheists repeat this absurdity without hesitation. If they’d thought of the idea themselves, rather than hearing it from someone else, they would have been more skeptical.\n\nDeath. Complete the pattern: “Death gives meaning to life.”\n\nIt’s frustrating, talking to good and decent folk—people who would never in a thousand years _spontaneously_ think of wiping out the human species—raising the topic of existential risk, and hearing them say, “Well, maybe the human species doesn’t deserve to survive.” They would never in a thousand years shoot their own child, who is a part of the human species, but the brain completes the pattern.\n\nWhat patterns are being completed, inside your mind, that you never chose to be there?\n\nRationality. Complete the pattern: “Love isn’t rational.”\n\nIf this idea had suddenly occurred to you personally, as an entirely new thought, how would you examine it critically? I know what _I_ would [say](https://lesswrong.com/rationality/feeling-rational), but what would _you_? It can be hard to see with fresh eyes. Try to keep your mind from completing the pattern in the standard, unsurprising, already-known way. It may be that there is no better answer than the standard one, but you can’t _think_ about the answer until you can stop your brain from filling in the answer automatically.\n\nNow that you’ve read this, the next time you hear someone unhesitatingly repeating a meme you think is silly or false, you’ll think, “Cached thoughts.” My belief is now there in your mind, waiting to complete the pattern. But is it true? Don’t let your mind complete the pattern! _Think!_\n\n^[1](#fn1x41-bk)^See ‘[Religion’s Claim to be Non-Disprovable](https://www.lesswrong.com/rationality/religion-s-claim-to-be-non-disprovable),” in _Map and Territory_."
    },
    "voteCount": 118,
    "forceInclude": true
  },
  {
    "_id": "dLJv2CoRCgeC2mPgj",
    "url": null,
    "title": "The Fallacy of Gray",
    "slug": "the-fallacy-of-gray",
    "author": "Eliezer Yudkowsky",
    "question": false,
    "tags": [
      {
        "name": "Fallacies"
      },
      {
        "name": "Fallacy of Gray"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "> The Sophisticate: “The world isn’t black and white. No one does pure good or pure bad. It’s all gray. Therefore, no one is better than anyone else.”\n> \n> The Zetet: “Knowing only gray, you conclude that all grays are the same shade. You mock the simplicity of the two-color view, yet you replace it with a one-color view . . .”\n> \n> —Marc Stiegler, _David’s Sling_\n\nI don’t know if the Sophisticate’s mistake has an official name, but I call it the Fallacy of Gray. We saw it manifested in the previous essay—the one who believed that odds of two to the power of seven hundred and fifty million to one, against, meant “there was still a chance.” All probabilities, to him, were simply “uncertain” and that meant he was licensed to ignore them if he pleased.\n\n“The Moon is made of green cheese” and “the Sun is made of mostly hydrogen and helium” are both uncertainties, but they are not the same uncertainty.\n\nEverything is shades of gray, but there are shades of gray so light as to be very nearly white, and shades of gray so dark as to be very nearly black. Or even if not, we can still compare shades, and say “it is darker” or “it is lighter.”\n\nYears ago, one of the strange little formative moments in my career as a rationalist was reading this paragraph from _Player of Games_ by Iain M. Banks, especially the sentence in bold:\n\n> A guilty system recognizes no innocents. As with any power apparatus which thinks everybody’s either for it or against it, we’re against it. You would be too, if you thought about it. The very way you think places you amongst its enemies. This might not be your fault, because **every society imposes some of its values on those raised within it, but the point is that some societies try to maximize that effect, and some try to minimize it**. You come from one of the latter and you’re being asked to explain yourself to one of the former. Prevarication will be more difficult than you might imagine; neutrality is probably impossible. You cannot choose not to have the politics you do; they are not some separate set of entities somehow detachable from the rest of your being; they are a function of your existence. I know that and they know that; you had better accept it.\n\nNow, don’t write angry comments saying that, if societies impose fewer of their values, then each succeeding generation has more work to start over from scratch. That’s not what I got out of the paragraph.\n\nWhat I got out of the paragraph was something which seems so obvious in retrospect that I could have conceivably picked it up in a hundred places; but something about that one paragraph made it click for me.\n\nIt was the whole notion of the Quantitative Way applied to life-problems like moral judgments and the quest for personal self-improvement. That, even if you couldn’t switch something from on to off, you could still tend to increase it or decrease it.\n\nIs this too obvious to be worth mentioning? I say it is not too obvious, for many bloggers have said of _Overcoming Bias_: “It is impossible, no one can completely eliminate bias.” I don’t care if the one is a professional economist, it is clear that they have not yet grokked the Quantitative Way as it applies to everyday life and matters like personal self-improvement. That which I cannot _eliminate_ may be well worth _reducing_.\n\nOr consider an exchange between Robin Hanson and Tyler Cowen.^[1](#fn1x8)^ Robin Hanson said that he preferred to put at least 75% weight on the prescriptions of economic theory versus his intuitions: “I try to mostly just straightforwardly apply economic theory, adding little personal or cultural judgment.” Tyler Cowen replied:\n\n> In my view there is no such thing as “straightforwardly applying economic theory” . . . theories are always applied through our personal and cultural filters and there is no other way it can be.\n\nYes, but you can try to minimize that effect, or you can do things that are bound to increase it. And _if_ you try to minimize it, then in many cases I don’t think it’s unreasonable to call the output “straightforward”—even in economics.\n\n“Everyone is imperfect.” Mohandas Gandhi was imperfect and Joseph Stalin was imperfect, but they were not the same shade of imperfection. “Everyone is imperfect” is an excellent example of replacing a two-color view with a one-color view. If you say, “No one is perfect, but _some people are less imperfect than others_,” you may not gain applause; but for those who strive to do better, you have held out hope. No one is _perfectly_ imperfect, after all.\n\n(Whenever someone says to me, “Perfectionism is bad for you,” I reply: “I think it’s okay to be imperfect, but not so imperfect that other people notice.”)\n\nLikewise the folly of those who say, “Every scientific paradigm imposes some of its assumptions on how it interprets experiments,” and then act like they’d proven science to occupy the same level with witchdoctoring. Every worldview imposes some of its structure on its observations, but the point is that there are worldviews which try to minimize that imposition, and worldviews which glory in it. There is no white, but there are shades of gray that are far lighter than others, and it is folly to treat them as if they were all on the same level.\n\nIf the Moon has orbited the Earth these past few billion years, if you have seen it in the sky these last years, and you expect to see it in its appointed place and phase tomorrow, then that is not a certainty. And if you expect an invisible dragon to heal your daughter of cancer, that too is not a certainty. But they are rather different degrees of uncertainty—this business of expecting things to happen yet again in the same way you have previously predicted to twelve decimal places, versus expecting something to happen that _violates_ the order previously observed. Calling them both “faith” seems a little too un-narrow.\n\nIt’s a most peculiar psychology—this business of “Science is based on faith too, so there!” Typically this is said by people who claim that faith is a _good_ thing. Then why do they say “Science is based on faith too!” in that angry-triumphal tone, rather than as a compliment? And a rather _dangerous_ compliment to give, one would think, from their perspective. If science is based on “faith,” then science is of the same kind as religion—directly comparable. If science is a religion, it is the religion that heals the sick and reveals the secrets of the stars. It would make sense to say, “The priests of science can blatantly, publicly, verifiably walk on the Moon as a faith-based miracle, and your priests’ faith can’t do the same.” Are you sure you wish to go there, oh faithist? Perhaps, on further reflection, you would prefer to retract this whole business of “Science is a religion too!”\n\nThere’s a strange dynamic here: You try to purify your shade of gray, and you get it to a point where it’s pretty light-toned, and someone stands up and says in a deeply offended tone, “But it’s not white! It’s gray!” It’s one thing when someone says, “This isn’t as light as you think, because of specific problems X, Y, and Z.” It’s a different matter when someone says angrily “It’s not white! It’s gray!” without pointing out any specific dark spots.\n\nIn this case, I begin to suspect psychology that is more imperfect than usual—that someone may have made a devil’s bargain with their own mistakes, and now refuses to hear of any possibility of improvement. When someone finds an excuse not to try to do better, they often refuse to concede that anyone else _can_ try to do better, and every mode of improvement is thereafter their enemy, and every claim that it is possible to move forward is an offense against them. And so they say in one breath proudly, “I’m glad to be gray,” and in the next breath angrily, “And _you’re gray too!_”\n\nIf there is no black and white, there is yet lighter and darker, and not all grays are the same.\n\nThe commenter G2 points us to Asimov’s “The Relativity of Wrong”:\n\n> When people thought the earth was flat, they were wrong. When people thought the earth was spherical, they were wrong. But if you think that thinking the earth is spherical is just as wrong as thinking the earth is flat, then your view is wronger than both of them put together.\n\n^[1](#fn1x8-bk)^Hanson (2007), “Economist Judgment,” [http://www.overcomingbias.com/2007/12/economist-judgm.html](http://www.overcomingbias.com/2007/12/economist-judgm.html). Cowen (2007), “Can Theory Override Intuition?”, [http://marginalrevolution.com/marginalrevolution/2007/12/how-my-views-di.html](http://marginalrevolution.com/marginalrevolution/2007/12/how-my-views-di.html)."
    },
    "voteCount": 185,
    "forceInclude": true
  },
  {
    "_id": "CEGnJBHmkcwPTysb7",
    "url": null,
    "title": "Lonely Dissent",
    "slug": "lonely-dissent",
    "author": "Eliezer_Yudkowsky",
    "question": false,
    "tags": [
      {
        "name": "Rationality"
      },
      {
        "name": "Social Reality"
      },
      {
        "name": "Groupthink"
      },
      {
        "name": "Social & Cultural Dynamics"
      },
      {
        "name": "Courage"
      },
      {
        "name": "Conformity Bias"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "Asch’s conformity experiment showed that the presence of a single dissenter tremendously reduced the incidence of “conforming” wrong answers. Individualism is easy, experiment shows, when you have company in your defiance. Every other subject in the room, except one, says that black is white. You become the second person to say that black is black. And it feels glorious: the two of you, lonely and defiant rebels, against the world!^[1](#fn1x63)^\n\nBut you can only _join_ the rebellion after someone, somewhere, becomes the _first_ to rebel. Someone has to say that black is black after hearing _everyone_ else, one after the other, say that black is white. And that—experiment shows—is a _lot harder_.\n\nLonely dissent doesn’t feel like going to school dressed in black. It feels like going to school wearing a clown suit.\n\nThat’s the difference between _joining the rebellion_ and _leaving the pack_.\n\nIf there’s one thing I can’t stand, it’s fakeness—you may have noticed this. Well, lonely dissent has got to be one of the most commonly, most ostentatiously faked characteristics around. Everyone wants to be an iconoclast.\n\nI don’t mean to degrade the act of joining a rebellion. There are rebellions worth joining. It does take courage to brave the disapproval of your peer group, or perhaps even worse, their shrugs. Needless to say, going to a rock concert is not rebellion. But, for example, vegetarianism is. I’m not a vegetarian myself, but I respect people who are, because I expect it takes a noticeable amount of quiet courage to tell people that hamburgers won’t work for dinner.^[2](#fn2x63)^\n\nStill, if you tell people that you’re a vegetarian, they’ll think they understand your motives (even if they don’t). They may disagree. They may be offended if you manage to announce it proudly enough, or for that matter, they may be offended just because they’re easily offended. But they know how to relate to you.\n\nWhen someone wears black to school, the teachers and the other children understand the role thereby being assumed in their society. It’s Outside the System—in a very standard way that everyone recognizes and understands. Not, y’know, _actually_ outside the system. It’s a Challenge to Standard Thinking, of a standard sort, so that people indignantly say, “I can’t understand why you—” but don’t have to actually think any thoughts they had not thought before. As the saying goes, “Has any of the ‘subversive literature’ you’ve read caused you to modify any of your political views?”\n\nWhat takes _real_ courage is braving the outright _incomprehension_ of the people around you, when you do something that _isn’t_ Standard Rebellion #37, something for which they lack a ready-made script. They don’t hate you for a rebel. They just think you’re, like, weird, and turn away. This prospect generates a much deeper fear. It’s the difference between explaining vegetarianism and explaining cryonics. There are other cryonicists in the world, somewhere, but they aren’t there next to you. You have to explain it, alone, to people who just think it’s _weird_. Not forbidden, but outside bounds that people don’t even think about. You’re going to get your head frozen? You think that’s going to stop you from dying? What do you mean, brain information? Huh? What? Are you _crazy?_\n\nI’m tempted to essay a post facto explanation in evolutionary psychology: You could get together with a small group of friends and walk away from your hunter-gatherer band, but having to go it _alone_ in the forests was probably a death sentence—at least reproductively. We don’t reason this out explicitly, but that is not the nature of evolutionary psychology. Joining a rebellion that everyone knows about is scary, but nowhere near as scary as doing something really differently—something that in ancestral times might have concluded, not with the band splitting, but with you being driven out alone.\n\nAs the case of cryonics testifies, the fear of thinking _really_ different is stronger than the fear of death. Hunter-gatherers had to be ready to face death on a routine basis—hunting large mammals, or just walking around in a world that contained predators. They needed that courage in order to live. Courage to defy the tribe’s standard ways of thinking, to entertain thoughts that seem truly weird—well, that probably didn’t serve its bearers as well. We don’t reason this out explicitly; that’s not how evolutionary psychology works. We human beings are just built in such fashion that many more of us go skydiving than sign up for cryonics.\n\nAnd that’s not even the highest courage. There’s more than one cryonicist in the world. Only Robert Ettinger had to say it _first._\n\nTo be a _scientific_ revolutionary, you’ve got to be the first person to contradict what everyone else you know is thinking. This is not the only route to scientific greatness; it is rare even among the great. No one can become a scientific revolutionary by trying to imitate revolutionariness. You can only get there by pursuing the correct answer in all things, whether the correct answer is revolutionary or not. But if, in the due course of time—if, having absorbed all the power and wisdom of the knowledge that has already accumulated—if, after all that and a dose of sheer luck, you find your pursuit of mere correctness taking you into new territory . . . _then_ you have an opportunity for your courage to fail.\n\nThis is the true courage of lonely dissent, which every damn rock band out there tries to fake.\n\nOf course, not everything that takes courage is a good idea. It would take courage to walk off a cliff, but then you would just go splat.\n\nThe _fear_ of lonely dissent is a hindrance to good ideas, but not every dissenting idea is good.^[3](#fn3x63)^ Most of the difficulty in having a new true scientific thought is in the “true” part.\n\nIt really isn’t _necessary_ to be different for the sake of being different. If you do things differently only when you see an overwhelmingly good reason, you will have more than enough trouble to last you the rest of your life.\n\nThere are a few genuine packs of iconoclasts around. The Church of the SubGenius, for example, seems to genuinely aim at _confusing_ the mundanes, not merely offending them. And there are islands of genuine tolerance in the world, such as science fiction conventions. There _are_ certain people who have no fear of departing the pack. Many fewer such people really exist, than imagine themselves rebels; but they do exist. And yet scientific revolutionaries are tremendously rarer. Ponder that.\n\nNow _me_, you know, I _really am_ an iconoclast. Everyone thinks they are, but with me it’s _true_, you see. I would _totally_ have worn a clown suit to school. My serious conversations were with books, not with other children.\n\nBut if you think you would _totally_ wear that clown suit, then don’t be too proud of that either! It just means that you need to make an effort in the _opposite direction_ to avoid dissenting too easily. That’s what I have to do, to correct for my own nature. Other people do have reasons for thinking what they do, and ignoring that completely is as bad as being afraid to contradict them. You wouldn’t want to end up as a free thinker. It’s not a _virtue_, you see—just a bias either way.\n\n^[1](#fn1x63-bk)^Followup interviews showed that subjects in the one-dissenter condition expressed strong feelings of camaraderie with the dissenter—though, of course, they didn’t think the presence of the dissenter had influenced their own nonconformity.\n\n^[2](#fn2x63-bk)^Albeit that in the Bay Area, people ask as a matter of routine.\n\n^[3](#fn3x63-bk)^See Robin Hanson, “Against Free Thinkers,” _Overcoming Bias_ (blog), 2007, [http://www.overcoming-bias.com/2007/06/against\\_free\\_th.html](http://www.overcoming-bias.com/2007/06/against_free_th.html)."
    },
    "voteCount": 78,
    "forceInclude": true
  },
  {
    "_id": "rmAbiEKQDpDnZzcRf",
    "url": null,
    "title": "Positive Bias: Look Into the Dark",
    "slug": "positive-bias-look-into-the-dark",
    "author": "Eliezer Yudkowsky",
    "question": false,
    "tags": [
      {
        "name": "Confirmation Bias"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "I am teaching a class, and I write upon the blackboard three numbers: 2-4-6. “I am thinking of a rule,” I say, “which governs sequences of three numbers. The sequence 2-4-6, as it so happens, obeys this rule. Each of you will find, on your desk, a pile of index cards. Write down a sequence of three numbers on a card, and I’ll mark it ‘Yes’ for fits the rule, or ‘No’ for not fitting the rule. Then you can write down another set of three numbers and ask whether it fits again, and so on. When you’re confident that you know the rule, write down the rule on a card. You can test as many triplets as you like.”\n\nHere’s the record of one student’s guesses:  \n \n\n<table><tbody><tr><td>4-6-2</td><td>No</td></tr><tr><td>4-6-8</td><td>Yes</td></tr><tr><td>10-12-14</td><td>Yes</td></tr></tbody></table>\n\nAt this point the student wrote down their guess at the rule. What do *you* think the rule is? Would you have wanted to test another triplet, and if so, what would it be? Take a moment to think before continuing.\n\nThe challenge above is based on a classic experiment due to Peter Wason, the 2-4-6 task. Although subjects given this task typically expressed high confidence in their guesses, only 21% of the subjects successfully guessed the experimenter’s real rule, and replications since then have continued to show success rates of around 20%.\n\nThe study was called “On the failure to eliminate hypotheses in a conceptual task.” Subjects who attempt the 2-4-6 task usually try to generate *positive* examples, rather than *negative* examples—they apply the hypothetical rule to generate a representative instance, and see if it is labeled “Yes.”\n\nThus, someone who forms the hypothesis “numbers increasing by two” will test the triplet 8-10-12, hear that it fits, and confidently announce the rule. Someone who forms the hypothesis X-2X-3X will test the triplet 3-6-9, discover that it fits, and then announce that rule.\n\nIn every case the actual rule is the same: the three numbers must be in ascending order.\n\nBut to discover this, you would have to generate triplets that *shouldn’t* fit, such as 20-23-26, and see if they are labeled “No.” Which people tend not to do, in this experiment. In some cases, subjects devise, “test,” and announce rules far more complicated than the actual answer.\n\nThis cognitive phenomenon is usually lumped in with “confirmation bias.” However, it seems to me that the phenomenon of trying to test *positive* rather than *negative* examples, ought to be distinguished from the phenomenon of trying to preserve the belief you started with. “Positive bias” is sometimes used as a synonym for “confirmation bias,” and fits this particular flaw much better.\n\nIt once seemed that phlogiston theory could explain a flame going out in an enclosed box (the air became saturated with phlogiston and no more could be released). But phlogiston theory could just as well have explained the flame *not* going out. To notice this, you have to search for negative examples instead of positive examples, look into zero instead of one; which goes against the grain of what experiment has shown to be human instinct.\n\nFor by instinct, we human beings only live in half the world.\n\nOne may be lectured on positive bias for days, and yet overlook it in-the-moment. Positive bias is not something we do as a matter of logic, or even as a matter of emotional attachment. The 2-4-6 task is “cold,” logical, not affectively “hot.” And yet the mistake is sub-verbal, on the level of imagery, of instinctive reactions. Because the problem doesn’t arise from following a deliberate rule that says “Only think about positive examples,” it can’t be solved just by knowing verbally that “We ought to think about both positive and negative examples.” Which example automatically pops into your head? You have to learn, wordlessly, to zag instead of zig. You have to learn to flinch toward the zero, instead of away from it.\n\nI have been writing for quite some time now on the notion that the strength of a hypothesis is what it *can’t* explain, not what it *can*—if you are equally good at explaining any outcome, you have zero knowledge. So to spot an explanation that isn’t helpful, it’s not enough to think of what it does explain very well—you also have to search for results it *couldn’t* explain, and this is the true strength of the theory.\n\nSo I said all this, and then I challenged the usefulness of “emergence” as a concept. One commenter cited superconductivity and ferromagnetism as examples of emergence. I replied that non-superconductivity and non-ferromagnetism were also examples of emergence, which was the problem. But be it far from me to criticize the commenter! Despite having read extensively on “confirmation bias,” I didn’t spot the “gotcha” in the 2-4-6 task the first time I read about it. It’s a subverbal blink-reaction that has to be retrained. I’m still working on it myself.\n\nSo much of a rationalist’s skill is below the level of words. It makes for challenging work in trying to convey the Art through words. People will agree with you, but then, in the next sentence, do something subdeliberative that goes in the opposite direction. Not that I’m complaining! A major reason I’m writing this is to observe what my words *haven’t* conveyed.\n\nAre you searching for positive examples of positive bias right now, or sparing a fraction of your search on what positive bias should lead you to *not* see? Did you look toward light or darkness?"
    },
    "voteCount": 96,
    "forceInclude": true
  },
  {
    "_id": "AdYdLP2sRqPMoe8fb",
    "url": null,
    "title": "Knowing About Biases Can Hurt People",
    "slug": "knowing-about-biases-can-hurt-people",
    "author": "Eliezer Yudkowsky",
    "question": false,
    "tags": [
      {
        "name": "Rationalization"
      },
      {
        "name": "Rationality"
      },
      {
        "name": "Information Hazards"
      },
      {
        "name": "Pitfalls of Rationality"
      },
      {
        "name": "Fallacies"
      },
      {
        "name": "Heuristics & Biases"
      },
      {
        "name": "Valley of Bad Rationality"
      },
      {
        "name": "Motivational Intro Posts"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "Once upon a time I tried to tell my mother about the problem of expert calibration, saying: “So when an expert says they’re 99% confident, it only happens about 70% of the time.” Then there was a pause as, suddenly, I realized I was talking to my mother, and I hastily added: “Of course, you’ve got to make sure to apply that skepticism evenhandedly, including to yourself, rather than just using it to argue against anything you disagree with—”\n\nAnd my mother said: “Are you kidding? This is great! I’m going to use it all the time!”\n\nTaber and Lodge’s “Motivated Skepticism in the Evaluation of Political Beliefs” describes the confirmation of six predictions:\n\n1.  Prior attitude effect. Subjects who feel strongly about an issue—even when encouraged to be objective—will evaluate supportive arguments more favorably than contrary arguments.\n2.  Disconfirmation bias. Subjects will spend more time and cognitive resources denigrating contrary arguments than supportive arguments.\n3.  Confirmation bias. Subjects free to choose their information sources will seek out supportive rather than contrary sources.\n4.  **Attitude polarization. Exposing subjects to an apparently balanced set of pro and con arguments will exaggerate their initial polarization.**\n5.  Attitude strength effect. Subjects voicing stronger attitudes will be more prone to the above biases.\n6.  **Sophistication effect. Politically knowledgeable subjects, because they possess greater ammunition with which to counter-argue incongruent facts and arguments, will be more prone to the above biases.**\n\nIf you’re irrational to start with, having *more* knowledge can *hurt* you. For a true Bayesian, information would never have negative expected utility. But humans aren’t perfect Bayes-wielders; if we’re not careful, we can cut ourselves.\n\nI’ve *seen* people severely messed up by their own knowledge of biases. They have more ammunition with which to argue against anything they don’t like. And that problem—too much ready ammunition—is one of the primary ways that people with high mental agility end up stupid, in Stanovich’s “dysrationalia” sense of stupidity.\n\nYou can think of people who fit this description, right? People with high g-factor who end up being *less* effective because they are too sophisticated as arguers? Do you think you’d be helping them—making them more effective rationalists—if you just told them about a list of classic biases?\n\nI recall someone who learned about the calibration/overconfidence problem. Soon after he said: “Well, you can’t trust experts; they’re wrong so often—as experiments have shown. So therefore, when I predict the future, I prefer to assume that things will continue historically as they have—” and went off into this whole complex, error-prone, highly questionable extrapolation. Somehow, when it came to trusting his own preferred conclusions, all those biases and fallacies seemed much less *salient*—leapt much less readily to mind—than when he needed to counter-argue someone else.\n\nI told the one about the problem of disconfirmation bias and sophisticated argument, and lo and behold, the next time I said something he didn’t like, he accused me of being a sophisticated arguer. He didn’t try to point out any particular sophisticated argument, any particular flaw—just shook his head and sighed sadly over how I was apparently using my own intelligence to defeat itself. He had acquired yet another Fully General Counterargument.\n\nEven the notion of a “sophisticated arguer” can be deadly, if it leaps all too readily to mind when you encounter a seemingly intelligent person who says something you don’t like.\n\nI endeavor to learn from my mistakes. The last time I gave a talk on heuristics and biases, I started out by introducing the general concept by way of the conjunction fallacy and representativeness heuristic. And then I moved on to confirmation bias, disconfirmation bias, sophisticated argument, motivated skepticism, and other attitude effects. I spent the next thirty minutes *hammering* on that theme, reintroducing it from as many different perspectives as I could.\n\nI wanted to get my audience interested in the subject. Well, a simple description of conjunction fallacy and representativeness would suffice for that. But suppose they did get interested. Then what? The literature on bias is mostly cognitive psychology for cognitive psychology’s sake. I had to give my audience their dire warnings during that one lecture, or they probably wouldn’t hear them at all.\n\nWhether I do it on paper, or in speech, I now try to never mention calibration and overconfidence unless I have first talked about disconfirmation bias, motivated skepticism, sophisticated arguers, and dysrationalia in the mentally agile. First, do no harm!"
    },
    "voteCount": 140,
    "forceInclude": true
  },
  {
    "_id": "9weLK2AJ9JEt2Tt8f",
    "url": null,
    "title": "Politics is the Mind-Killer",
    "slug": "politics-is-the-mind-killer",
    "author": "Eliezer Yudkowsky",
    "question": false,
    "tags": [
      {
        "name": "Politics"
      },
      {
        "name": "Rationality"
      },
      {
        "name": "Tribalism"
      },
      {
        "name": "Social & Cultural Dynamics"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "People go funny in the head when talking about politics. The evolutionary reasons for this are so obvious as to be worth belaboring: In the ancestral environment, politics was a matter of life and death. And sex, and wealth, and allies, and reputation . . . When, today, you get into an argument about whether “we” ought to raise the minimum wage, you’re executing adaptations for an ancestral environment where being on the wrong side of the argument could get you killed. Being on the *right* side of the argument could let *you* kill your hated rival!\n\nIf you want to make a point about science, or rationality, then my advice is to not choose a domain from *contemporary* politics if you can possibly avoid it. If your point is inherently about politics, then talk about Louis XVI during the French Revolution. Politics is an important domain to which we should individually apply our rationality—but it’s a terrible domain in which to *learn* rationality, or discuss rationality, unless all the discussants are already rational.\n\nPolitics is an extension of war by other means. Arguments are soldiers. Once you know which side you’re on, you must support all arguments of that side, and attack all arguments that appear to favor the enemy side; otherwise it’s like stabbing your soldiers in the back—providing aid and comfort to the enemy. People who would be level-headed about evenhandedly weighing all sides of an issue in their professional life as scientists, can suddenly turn into slogan-chanting zombies when there’s a [Blue or Green](https://lesswrong.com/rationality/a-fable-of-science-and-politics) position on an issue.\n\nIn artificial intelligence, and particularly in the domain of nonmonotonic reasoning, there’s a standard problem: “All Quakers are pacifists. All Republicans are not pacifists. Nixon is a Quaker and a Republican. Is Nixon a pacifist?”\n\nWhat on Earth was the point of choosing this as an example? To rouse the political emotions of the readers and distract them from the main question? To make Republicans feel unwelcome in courses on artificial intelligence and discourage them from entering the field?[^1^](#fn1x14)\n\nWhy would anyone pick such a *distracting* example to illustrate nonmonotonic reasoning? Probably because the author just couldn’t resist getting in a good, solid dig at those hated Greens. It feels so *good* to get in a hearty punch, y’know, it’s like trying to resist a chocolate cookie.\n\nAs with chocolate cookies, not everything that feels pleasurable is good for you.\n\nI’m not saying that I think we should be apolitical, or even that we should adopt Wikipedia’s ideal of the Neutral Point of View. But try to resist getting in those good, solid digs if you can possibly avoid it. If your topic legitimately relates to attempts to ban evolution in school curricula, then go ahead and talk about it—but don’t blame it explicitly on the whole Republican Party; some of your readers may be Republicans, and they may feel that the problem is a few rogues, not the entire party. As with Wikipedia’s NPOV, it doesn’t matter whether (you think) the Republican Party really *is* at fault. It’s just better for the spiritual growth of the community to discuss the issue without invoking color politics.\n\n* * *\n\n[^1^](#fn1x14-bk)And no, I am not a Republican. Or a Democrat."
    },
    "voteCount": 153,
    "forceInclude": true
  },
  {
    "_id": "a7n8GdKiAZRX86T5A",
    "url": null,
    "title": "Making Beliefs Pay Rent (in Anticipated Experiences)",
    "slug": "making-beliefs-pay-rent-in-anticipated-experiences",
    "author": "Eliezer Yudkowsky",
    "question": false,
    "tags": [
      {
        "name": "Rationality"
      },
      {
        "name": "Anticipated Experiences"
      },
      {
        "name": "Epistemology"
      },
      {
        "name": "Empiricism"
      },
      {
        "name": "Principles"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "Thus begins the ancient parable:\n\n*If a tree falls in a forest and no one hears it, does it make a sound? One says, “Yes it does, for it makes vibrations in the air.” Another says, “No it does not, for there is no auditory processing in any brain.”*\n\nIf there’s a foundational skill in the martial art of rationality, a mental stance on which all other technique rests, it might be this one: the ability to spot, inside your own head, psychological signs that you have a mental map of something, and signs that you don’t.\n\nSuppose that, after a tree falls, the two arguers walk into the forest together. Will one expect to see the tree fallen to the right, and the other expect to see the tree fallen to the left? Suppose that before the tree falls, the two leave a sound recorder next to the tree. Would one, playing back the recorder, expect to hear something different from the other? Suppose they attach an electroencephalograph to any brain in the world; would one expect to see a different trace than the other?\n\nThough the two argue, one saying “No,” and the other saying “Yes,” they do not anticipate any different experiences. The two think they have different models of the world, but they have no difference with respect to what they expect will *happen to* them; their maps of the world do not diverge in any sensory detail.\n\nIt’s tempting to try to eliminate this mistake class by insisting that the only legitimate kind of belief is an anticipation of sensory experience. But the world does, in fact, contain much that is not sensed directly. We don’t see the atoms underlying the brick, but the atoms are in fact there. There is a floor beneath your feet, but you don’t *experience* the floor directly; you see the light *reflected* from the floor, or rather, you see what your retina and visual cortex have processed of that light. To infer the floor from seeing the floor is to step back into the unseen causes of experience. It may seem like a very short and direct step, but it is still a step.\n\nYou stand on top of a tall building, next to a grandfather clock with an hour, minute, and ticking second hand. In your hand is a bowling ball, and you drop it off the roof. On which tick of the clock will you hear the crash of the bowling ball hitting the ground?\n\nTo answer precisely, you must use beliefs like *Earth’s gravity is 9.8 meters per second per second,* and *This building is around 120 meters tall.* These beliefs are not wordless anticipations of a sensory experience; they are verbal-ish, propositional. It probably does not exaggerate much to describe these two beliefs as sentences made out of words. But these two beliefs have an inferential *consequence* that is a direct sensory anticipation—if the clock’s second hand is on the 12 numeral when you drop the ball, you anticipate seeing it on the 1 numeral when you hear the crash five seconds later. To anticipate sensory experiences as precisely as possible, we must process beliefs that are not anticipations of sensory experience.\n\nIt is a great strength of *Homo sapiens* that we can, better than any other species in the world, learn to model the unseen. It is also one of our great weak points. Humans often believe in things that are not only unseen but unreal.\n\nThe same brain that builds a network of inferred causes behind sensory experience can also build a network of causes that is not connected to sensory experience, or poorly connected. Alchemists believed that phlogiston caused fire—we could simplistically model their minds by drawing a little node labeled “Phlogiston,” and an arrow from this node to their sensory experience of a crackling campfire—but this belief yielded no advance predictions; the link from phlogiston to experience was always configured after the experience, rather than constraining the experience in advance.\n\nOr suppose your English professor teaches you that the famous writer Wulky Wilkinsen is actually a “retropositional author,” which you can tell because his books exhibit “alienated resublimation.” And perhaps your professor knows all this because their professor told them; but all they're able to say about resublimation is that it's characteristic of retropositional thought, and of retropositionality that it's marked by alienated resublimation. What does this mean you should expect from Wulky Wilkinsen’s books?\n\nNothing. The belief, if you can call it that, doesn’t connect to sensory experience at all. But you had better remember the propositional assertions that “Wulky Wilkinsen” has the “retropositionality” attribute and also the “alienated resublimation” attribute, so you can regurgitate them on the upcoming quiz. The two beliefs are connected to each other, though still not connected to any anticipated experience.\n\nWe can build up whole networks of beliefs that are connected only to each other—call these “floating” beliefs. It is a uniquely human flaw among animal species, a perversion of *Homo sapiens*’s ability to build more general and flexible belief networks.\n\nThe rationalist virtue of *empiricism* consists of constantly asking which experiences our beliefs predict—or better yet, prohibit. Do you believe that phlogiston is the cause of fire? Then what do you expect to see happen, because of that? Do you believe that Wulky Wilkinsen is a retropositional author? Then what do you expect to see because of that? No, not “alienated resublimation”; *what experience will happen to you?* Do you believe that if a tree falls in the forest, and no one hears it, it still makes a sound? Then what experience must therefore befall you?\n\nIt is even better to ask: what experience *must not* happen to you? Do you believe that *Élan vital* explains the mysterious aliveness of living beings? Then what does this belief *not* allow to happen—what would definitely falsify this belief? A null answer means that your belief does not *constrain* experience; it permits *anything* to happen to you. It floats.\n\nWhen you argue a seemingly factual question, always keep in mind which difference of anticipation you are arguing about. If you can’t find the difference of anticipation, you’re probably arguing about labels in your belief network—or even worse, floating beliefs, barnacles on your network. If you don’t know what experiences are implied by Wulky Wilkinsens writing being retropositional, you can go on arguing forever.\n\nAbove all, don’t ask what to believe—ask what to anticipate. Every question of belief should flow from a question of anticipation, and that question of anticipation should be the center of the inquiry. Every guess of belief should begin by flowing to a specific guess of anticipation, and should continue to pay rent in future anticipations. If a belief turns deadbeat, evict it."
    },
    "voteCount": 277,
    "forceInclude": true
  },
  {
    "_id": "6s3xABaXKPdFwA3FS",
    "url": null,
    "title": "What is Evidence?",
    "slug": "what-is-evidence",
    "author": "Eliezer Yudkowsky",
    "question": false,
    "tags": [
      {
        "name": "Epistemology"
      },
      {
        "name": "Causality"
      },
      {
        "name": "Anticipated Experiences"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "> The sentence “snow is white” is *true* if and only if snow is white.\n> \n> —Alfred Tarski\n\n> To say of what is, that it is, or of what is not, that it is not, is *true*.\n> \n> —Aristotle, *Metaphysics IV*\n\nWalking along the street, your shoelaces come untied. Shortly thereafter, for some odd reason, you start *believing* your shoelaces are untied. Light leaves the Sun and strikes your shoelaces and bounces off; some photons enter the pupils of your eyes and strike your retina; the energy of the photons triggers neural impulses; the neural impulses are transmitted to the visual-processing areas of the brain; and there the optical information is processed and reconstructed into a 3D model that is recognized as an untied shoelace. There is a sequence of events, a chain of cause and effect, within the world and your brain, by which you end up believing what you believe. The final outcome of the process is a state of *mind* which mirrors the state of your actual *shoelaces*.\n\nWhat is *evidence?* It is an event entangled, by links of cause and effect, with whatever you want to know about. If the target of your inquiry is your shoelaces, for example, then the light entering your pupils is evidence entangled with your shoelaces. This should not be confused with the technical sense of “entanglement” used in physics—here I’m just talking about “entanglement” in the sense of two things that end up in correlated states because of the links of cause and effect between them.\n\nNot every influence creates the kind of “entanglement” required for evidence. It’s no help to have a machine that beeps when you enter winning lottery numbers, if the machine *also* beeps when you enter *losing* lottery numbers. The light reflected from your shoes would not be useful evidence about your shoelaces, if the photons ended up in the same physical state whether your shoelaces were tied or untied.\n\nTo say it abstractly: For an event to be *evidence about* a target of inquiry, it has to happen *differently* in a way that’s entangled with the *different* possible states of the target. (To say it technically: There has to be Shannon mutual information between the evidential event and the target of inquiry, relative to your current state of uncertainty about both of them.)\n\nEntanglement can be contagious when processed correctly, which is why you need eyes and a brain. If photons reflect off your shoelaces and hit a rock, the rock won’t change much. The rock won’t reflect the shoelaces in any helpful way; it won’t be detectably different depending on whether your shoelaces were tied or untied. This is why rocks are not useful witnesses in court. A photographic film will contract shoelace-entanglement from the incoming photons, so that the photo can itself act as evidence. If your eyes and brain work correctly, *you* will become tangled up with your own shoelaces.\n\nThis is why rationalists put such a heavy premium on the paradoxical-seeming claim that a belief is only really worthwhile if you could, in principle, be persuaded to believe otherwise. If your retina ended up in the same state regardless of what light entered it, you would be blind. Some belief systems, in a rather obvious trick to reinforce themselves, say that certain beliefs are only really worthwhile if you believe them *unconditionally*—no matter what you see, no matter what you think. Your brain is supposed to end up in the same state regardless. Hence the phrase, “blind faith.” If what you believe doesn’t depend on what you see, you’ve been blinded as effectively as by poking out your eyeballs.\n\nIf your eyes and brain work correctly, your beliefs will end up entangled with the facts. *Rational thought produces beliefs which are themselves evidence.*\n\nIf your tongue speaks truly, your rational beliefs, which are themselves evidence, can act as evidence for someone else. Entanglement can be transmitted through chains of cause and effect—and if you speak, and another hears, that too is cause and effect. When you say “My shoelaces are untied” over a cellphone, you’re sharing your entanglement with your shoelaces with a friend.\n\nTherefore rational beliefs are contagious, among honest folk who believe each other to be honest. And it’s why a claim that your beliefs are *not* contagious—that you believe for private reasons which are not transmissible—is so suspicious. If your beliefs are entangled with reality, they *should* be contagious among honest folk.\n\nIf your model of reality suggests that the outputs of your thought processes should *not* be contagious to others, then your model says that your beliefs are not themselves evidence, meaning they are not entangled with reality. You should apply a reflective correction, and stop believing.\n\nIndeed, if you *feel*, on a *gut* level, what this all *means*, you will *automatically* stop believing. Because “my belief is not entangled with reality” *means* “my belief is not accurate.” As soon as you stop believing “ ‘snow is white’ is true,” you should (automatically!) stop believing “snow is white,” or something is very wrong.\n\nSo try to explain why the kind of thought processes you use systematically produce beliefs that mirror reality. Explain why you think you’re rational. Why you think that, using thought processes like the ones you use, minds will end up believing “snow is white” if and only if snow is white. If you *don’t* believe that the outputs of your thought processes are entangled with reality, why believe the outputs of your thought processes? It’s the same thing, or it should be."
    },
    "voteCount": 116,
    "forceInclude": true
  },
  {
    "_id": "fhojYBGGiYAFcryHZ",
    "url": null,
    "title": "Scientific Evidence, Legal Evidence, Rational Evidence",
    "slug": "scientific-evidence-legal-evidence-rational-evidence",
    "author": "Eliezer Yudkowsky",
    "question": false,
    "tags": [
      {
        "name": "Practice & Philosophy of Science"
      },
      {
        "name": "Rationality"
      },
      {
        "name": "Empiricism"
      },
      {
        "name": "Law and Legal systems"
      },
      {
        "name": "Distinctions"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "Suppose that your good friend, the police commissioner, tells you in strictest confidence that the crime kingpin of your city is Wulky Wilkinsen. As a rationalist, are you licensed to believe this statement? Put it this way: if you go ahead and insult Wulky, I’d call you foolhardy. Since it is prudent to act as if Wulky has a substantially higher-than-default probability of being a crime boss, the police commissioner’s statement must have been strong Bayesian evidence.\n\nOur legal system will not imprison Wulky on the basis of the police commissioner’s statement. It is not admissible as *legal evidence*. Maybe if you locked up every person accused of being a crime boss by a police commissioner, you’d *initially* catch a lot of crime bosses, and relatively few people the commissioner just didn’t like. But unrestrained power attracts corruption like honey attracts flies: over time, you’d catch fewer and fewer real crime bosses (who would go to greater lengths to ensure anonymity), and more and more innocent victims.\n\nThis does not mean that the police commissioner’s statement is not rational evidence. It still has a lopsided likelihood ratio, and you’d still be a fool to insult Wulky. But on a *social* level, in pursuit of a social goal, we deliberately define “legal evidence” to include only particular kinds of evidence, such as the police commissioner’s own observations on the night of April 4th. All legal evidence should ideally be rational evidence, but not the other way around. We impose special, strong, additional standards before we anoint rational evidence as “legal evidence.”\n\nAs I write this sentence at 8:33 p.m., Pacific time, on August 18th, 2007, I am wearing white socks. As a rationalist, are you licensed to believe the previous statement? Yes. Could I testify to it in court? Yes. Is it a *scientific* statement? No, because there is no experiment you can perform yourself to verify it. Science is made up of *generalizations* which apply to many particular instances, so that you can run new real-world experiments which test the generalization, and thereby verify for yourself that the generalization is true, without having to trust anyone’s authority. Science is the *publicly reproducible* knowledge of humankind.\n\nLike a court system, science as a social process is made up of fallible humans. We want a protected pool of beliefs that are *especially* reliable. And we want social rules that encourage the generation of such knowledge. So we impose special, strong, additional standards before we canonize rational knowledge as “scientific knowledge,” adding it to the protected belief pool. Is a rationalist licensed to believe in the historical existence of Alexander the Great? Yes. We have a rough picture of ancient Greece, untrustworthy but better than maximum entropy. But we are dependent on authorities such as Plutarch; we cannot discard Plutarch and verify everything for ourselves. Historical knowledge is not scientific knowledge.\n\nIs a rationalist licensed to believe that the Sun will rise on September 18th, 2007? Yes—not with absolute certainty, but that’s the way to bet.^1^ Is this statement, as I write this essay on August 18th, 2007, a *scientific* belief?\n\nIt may seem perverse to deny the adjective “scientific” to statements like “The Sun will rise on September 18th, 2007.” If Science could not make predictions about future events—events which have *not yet* happened—then it would be useless; it could make no prediction in advance of experiment. The prediction that the Sun will rise is, definitely, an *extrapolation* from scientific generalizations. It is based upon models of the Solar System that you could test for yourself by experiment.\n\nBut imagine that you’re constructing an experiment to verify prediction #27, in a new context, of an accepted theory Q. You may not have any concrete reason to suspect the belief is wrong; you just want to test it in a new context. It seems dangerous to say, *before* running the experiment, that there is a “scientific belief” about the result. There is a “conventional prediction” or “theory Q’s prediction.” But if you already know the “scientific belief” about the result, why bother to run the experiment?\n\nYou begin to see, I hope, why I identify Science with *generalizations*, rather than the history of any one experiment. A historical event happens once; generalizations apply over many events. History is not reproducible; scientific generalizations are.\n\nIs my definition of “scientific knowledge” *true*? That is not a well-formed question. The special standards we impose upon science are pragmatic choices. Nowhere upon the stars or the mountains is it written that p < 0.05 shall be the standard for scientific publication. Many now argue that 0.05 is too weak, and that it would be *useful* to lower it to 0.01 or 0.001.\n\nPerhaps future generations, acting on the theory that science is the *public*, *reproducible* knowledge of humankind, will only label as “scientific” papers published in an open-access journal. If you charge for access to the knowledge, is it part of the knowledge of *humankind*? Can we fully trust a result if people must pay to criticize it?\n\nFor myself, I think scientific practice would be better served by the dictum that only open, public knowledge counts. But however we choose to define “science,” information in a $20,000/year closed-access journal will still count as Bayesian evidence; and so too, the police commissioner’s private assurance that Wulky is the kingpin.\n\n* * *\n\n^1^ Pedants: interpret this as the Earth’s rotation and orbit remaining roughly constant relative to the Sun."
    },
    "voteCount": 85,
    "forceInclude": true
  },
  {
    "_id": "nj8JKFoLSMEmD3RGp",
    "url": null,
    "title": "How Much Evidence Does It Take?",
    "slug": "how-much-evidence-does-it-take",
    "author": "Eliezer_Yudkowsky",
    "question": false,
    "tags": [
      {
        "name": "Bayes' Theorem"
      },
      {
        "name": "Epistemology"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "Previously, I defined _evidence_ as “an event entangled, by links of cause and effect, with whatever you want to know about,” and _entangled_ as “happening differently for different possible states of the target.” So how much entanglement—how much rational evidence—is required to support a belief?\n\nLet’s start with a question simple enough to be mathematical: How hard would you have to entangle yourself with the lottery in order to win? Suppose there are seventy balls, drawn without replacement, and six numbers to match for the win. Then there are 131,115,985 possible winning combinations, hence a randomly selected ticket would have a 1/131,115,985 probability of winning (0.0000007%). To win the lottery, you would need evidence _selective_ enough to visibly favor one combination over 131,115,984 alternatives.\n\nSuppose there are some tests you can perform which discriminate, probabilistically, between winning and losing lottery numbers. For example, you can punch a combination into a little black box that always beeps if the combination is the winner, and has only a 1/4 (25%) chance of beeping if the combination is wrong. In Bayesian terms, we would say the _likelihood ratio_ is 4 to 1. This means that the box is 4 times as likely to beep when we punch in a correct combination, compared to how likely it is to beep for an incorrect combination.\n\nThere are still a whole lot of possible combinations. If you punch in 20 incorrect combinations, the box will beep on 5 of them by sheer chance (on average). If you punch in all 131,115,985 possible combinations, then while the box is certain to beep for the one winning combination, it will also beep for 32,778,996 losing combinations (on average).\n\nSo this box doesn’t let you win the lottery, but it’s better than nothing. If you used the box, your odds of winning would go from 1 in 131,115,985 to 1 in 32,778,997. You’ve made some progress toward finding your target, the truth, within the huge space of possibilities.\n\nSuppose you can use another black box to test combinations _twice_, _independently._ Both boxes are certain to beep for the winning ticket. But the chance of a box beeping for a losing combination is 1/4 _independently_ for each box; hence the chance of _both_ boxes beeping for a losing combination is 1/16. We can say that the _cumulative_ evidence, of two independent tests, has a likelihood ratio of 16:1. The number of losing lottery tickets that pass both tests will be (on average) 8,194,749.\n\nSince there are 131,115,985 possible lottery tickets, you might guess that you need evidence whose strength is around 131,115,985 to 1—an event, or series of events, which is 131,115,985 times more likely to happen for a winning combination than a losing combination. Actually, this amount of evidence would only be enough to give you an _even_ chance of winning the lottery. Why? Because if you apply a filter of that power to 131 million losing tickets, there will be, on average, one losing ticket that passes the filter. The winning ticket will also pass the filter. So you’ll be left with two tickets that passed the filter, only one of them a winner. Fifty percent odds of winning, if you can only buy one ticket.\n\nA better way of viewing the problem: In the beginning, there is 1 winning ticket and 131,115,984 losing tickets, so your odds of winning are 1:131,115,984. If you use a single box, the odds of it beeping are 1 for a winning ticket and 0.25 for a losing ticket. So we multiply 1:131,115,984 by 1:0.25 and get 1:32,778,996. Adding another box of evidence multiplies the odds by 1:0.25 again, so now the odds are 1 winning ticket to 8,194,749 losing tickets.\n\nIt is convenient to measure evidence in bits—not like bits on a hard drive, but mathematician’s bits, which are conceptually different. Mathematician’s bits are the logarithms, base 1/2, of probabilities. For example, if there are four possible outcomes A, B, C, and D, whose probabilities are 50%, 25%, 12.5%, and 12.5%, and I tell you the outcome was “D,” then I have transmitted three bits of information to you, because I informed you of an outcome whose probability was 1/8.\n\nIt so happens that 131,115,984 is slightly less than 2 to the 27th power. So 14 boxes or 28 bits of evidence—an event 268,435,456:1 times more likely to happen if the ticket-hypothesis is true than if it is false—would shift the odds from 1:131,115,984 to 268,435,456:131,115,984, which reduces to 2:1. Odds of 2 to 1 mean two chances to win for each chance to lose, so the _probability_ of winning with 28 bits of evidence is 2/3. Adding another box, another 2 bits of evidence, would take the odds to 8:1. Adding yet another two boxes would take the chance of winning to 128:1.\n\nSo if you want to license a _strong belief_ that you will win the lottery—arbitrarily defined as less than a 1% probability of being wrong—34 bits of evidence about the winning combination should do the trick.\n\nIn general, the rules for weighing “how much evidence it takes” follow a similar pattern: The larger the _space of possibilities_ in which the hypothesis lies, or the more unlikely the hypothesis seems a priori compared to its neighbors, or the more confident you wish to be, the more evidence you need.\n\nYou cannot defy the rules; you cannot form accurate beliefs based on inadequate evidence. Let’s say you’ve got 10 boxes lined up in a row, and you start punching combinations into the boxes. You cannot stop on the first combination that gets beeps from all 10 boxes, saying, “But the odds of that happening for a losing combination are a million to one! I’ll just ignore those ivory-tower Bayesian rules and stop here.” On average, 131 losing tickets will pass such a test for every winner. Considering the space of possibilities and the prior improbability, you jumped to a too-strong conclusion based on insufficient evidence. That’s not a pointless bureaucratic regulation; it’s math.\n\nOf course, you can still believe based on inadequate evidence, if that is your whim; but you will not be able to believe _accurately_. It is like trying to drive your car without any fuel, because you don’t believe in the fuddy-duddy concept that it ought to take fuel to go places. Wouldn’t it be so much more _fun_, and so much less expensive, if we just decided to repeal the law that cars need fuel?\n\nWell, you can try. You can even shut your eyes and pretend the car is moving. But _really_ arriving at accurate beliefs requires evidence-fuel, and the further you want to go, the more fuel you need."
    },
    "voteCount": 87,
    "forceInclude": true
  },
  {
    "_id": "mnS2WYLCGJP2kQkRn",
    "url": null,
    "title": "Absence of Evidence Is Evidence of Absence",
    "slug": "absence-of-evidence-is-evidence-of-absence",
    "author": "Eliezer Yudkowsky",
    "question": false,
    "tags": [
      {
        "name": "Conservation of Expected Evidence"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "From Robyn Dawes’s *Rational Choice in an Uncertain World*:\n\n> In fact, this post-hoc fitting of evidence to hypothesis was involved in a most grievous chapter in United States history: the internment of Japanese-Americans at the beginning of the Second World War. When California governor Earl Warren testified before a congressional hearing in San Francisco on February 21, 1942, a questioner pointed out that there had been no sabotage or any other type of espionage by the Japanese-Americans up to that time. Warren responded, “I take the view that this lack \\[of subversive activity\\] is the most ominous sign in our whole situation. It convinces me more than perhaps any other factor that the sabotage we are to get, the Fifth Column activities are to get, are timed just like Pearl Harbor was timed . . . I believe we are just being lulled into a false sense of security.”\n\nConsider Warren’s argument from a Bayesian perspective. When we see evidence, hypotheses that assigned a *higher* likelihood to that evidence gain probability, at the expense of hypotheses that assigned a *lower* likelihood to the evidence. This is a phenomenon of *relative* likelihoods and *relative* probabilities. You can assign a high likelihood to the evidence and still lose probability mass to some other hypothesis, if that other hypothesis assigns a likelihood that is even higher.\n\nWarren seems to be arguing that, given that we see no sabotage, this *confirms* that a Fifth Column exists. You could argue that a Fifth Column *might* delay its sabotage. But the likelihood is still higher that the *absence* of a Fifth Column would perform an absence of sabotage.\n\nLet E stand for the observation of sabotage, and ¬E for the observation of no sabotage. The symbol H1 stands for the hypothesis of a Japanese-American Fifth Column, and H2 for the hypothesis that no Fifth Column exists. The *conditional probability* P(E | H), or “E given H,” is how confidently we’d expect to see the evidence E if we assumed the hypothesis H were true.\n\nWhatever the likelihood that a Fifth Column would do no sabotage, the probability P(¬E | H1), it won’t be as large as the likelihood that there’s no sabotage *given that there’s no Fifth Column*, the probability P(¬E | H2). So observing a lack of sabotage increases the probability that no Fifth Column exists.\n\nA lack of sabotage doesn’t *prove* that no Fifth Column exists. Absence of *proof* is not *proof* of absence. In logic, (A ⇒ B), read “A implies B,” is not equivalent to (¬A ⇒ ¬B), read “not-A implies not-B .”\n\nBut in probability theory, absence of *evidence* is always *evidence* of absence. If E is a binary event and P(H | E) > P(H), i.e., seeing E increases the probability of H, then P(H | ¬ E) < P(H), i.e., failure to observe E decreases the probability of H . The probability P(H) is a weighted mix of P(H | E) and P(H | ¬ E), and necessarily lies between the two.^1^\n\nUnder the vast majority of real-life circumstances, a cause may not reliably produce signs of itself, but the absence of the cause is even less likely to produce the signs. The absence of an observation may be strong evidence of absence or very weak evidence of absence, depending on how likely the cause is to produce the observation. The absence of an observation that is only weakly permitted (even if the alternative hypothesis does not allow it at all) is very weak evidence of absence (though it is evidence nonetheless). This is the fallacy of “gaps in the fossil record”—fossils form only rarely; it is futile to trumpet the absence of a weakly permitted observation when many strong positive observations have already been recorded. But if there are *no* positive observations at all, it is time to worry; hence the Fermi Paradox.\n\nYour strength as a rationalist is your ability to be more confused by fiction than by reality; if you are equally good at explaining any outcome you have zero knowledge. The strength of a model is not what it *can* explain, but what it *can’t*, for only prohibitions constrain anticipation. If you don’t notice when your model makes the evidence unlikely, you might as well have no model, and also you might as well have no evidence; no brain and no eyes.\n\n* * *\n\n^1^ If any of this sounds at all confusing, see my discussion of Bayesian updating toward the end of *The Machine in the Ghost*, the third volume of [*Rationality: From AI to Zombies*](https://lesswrong.com/rationality)."
    },
    "voteCount": 97,
    "forceInclude": true
  },
  {
    "_id": "jiBFC7DcCrZjGmZnJ",
    "url": null,
    "title": "Conservation of Expected Evidence",
    "slug": "conservation-of-expected-evidence",
    "author": "Eliezer_Yudkowsky",
    "question": false,
    "tags": [
      {
        "name": "Conservation of Expected Evidence"
      },
      {
        "name": "Rationality"
      },
      {
        "name": "Bayes' Theorem"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "Friedrich Spee von Langenfeld, a priest who heard the confessions of condemned witches, wrote in 1631 the _Cautio Criminalis_ (“prudence in criminal cases”), in which he bitingly described the decision tree for condemning accused witches: If the witch had led an evil and improper life, she was guilty; if she had led a good and proper life, this too was a proof, for witches dissemble and try to appear especially virtuous. After the woman was put in prison: if she was afraid, this proved her guilt; if she was not afraid, this proved her guilt, for witches characteristically pretend innocence and wear a bold front. Or on hearing of a denunciation of witchcraft against her, she might seek flight or remain; if she ran, that proved her guilt; if she remained, the devil had detained her so she could not get away.\n\nSpee acted as confessor to many witches; he was thus in a position to observe _every_ branch of the accusation tree, that no matter _what_ the accused witch said or did, it was held as proof against her. In any individual case, you would only hear one branch of the dilemma. It is for this reason that scientists write down their experimental predictions in advance.\n\nBut _you can’t have it both ways_ —as a matter of probability theory, not mere fairness. The rule that “absence of evidence _is_ evidence of absence” is a special case of a more general law, which I would name Conservation of Expected Evidence: the _expectation_ of the posterior probability, after viewing the evidence, must equal the prior probability.\n\n_Therefore,_ for every expectation of evidence, there is an equal and opposite expectation of counterevidence.\n\nIf you expect a strong probability of seeing weak evidence in one direction, it must be balanced by a weak expectation of seeing strong evidence in the other direction. If you’re very confident in your theory, and therefore anticipate seeing an outcome that matches your hypothesis, this can only provide a very small increment to your belief (it is already close to 1); but the unexpected failure of your prediction would (and must) deal your confidence a huge blow. On _average_, you must expect to be _exactly_ as confident as when you started out. Equivalently, the mere _expectation_ of encountering evidence—before you’ve actually seen it—should not shift your prior beliefs.\n\nSo if you claim that “no sabotage” is evidence _for_ the existence of a Japanese-American Fifth Column, you must conversely hold that seeing sabotage would argue _against_ a Fifth Column. If you claim that “a good and proper life” is evidence that a woman is a witch, then an evil and improper life must be evidence that she is not a witch. If you argue that God, to test humanity’s faith, refuses to reveal His existence, then the miracles described in the Bible must argue against the existence of God.\n\nDoesn’t quite sound right, does it? Pay attention to that feeling of _this seems a little forced_, that quiet strain in the back of your mind. It’s important.\n\nFor a true Bayesian, it is impossible to seek evidence that _confirms_ a theory. There is no possible plan you can devise, no clever strategy, no cunning device, by which you can legitimately expect your confidence in a fixed proposition to be higher (on _average_) than before. You can only ever seek evidence to _test_ a theory, not to confirm it.\n\nThis realization can take quite a load off your mind. You need not worry about how to interpret every possible experimental result to confirm your theory. You needn’t bother planning how to make _any_ given iota of evidence confirm your theory, because you know that for every expectation of evidence, there is an equal and oppositive expectation of counterevidence. If you try to weaken the counterevidence of a possible “abnormal” observation, you can only do it by weakening the support of a “normal” observation, to a precisely equal and opposite degree. It is a zero-sum game. No matter how you connive, no matter how you argue, no matter how you strategize, you can’t possibly expect the resulting game plan to shift your beliefs (on average) in a particular direction.\n\nYou might as well sit back and relax while you wait for the evidence to come in.\n\n. . . Human psychology is _so_ screwed up."
    },
    "voteCount": 147,
    "forceInclude": true
  },
  {
    "_id": "5yFRd3cjLpm3Nd6Di",
    "url": null,
    "title": "Argument Screens Off Authority",
    "slug": "argument-screens-off-authority",
    "author": "Eliezer Yudkowsky",
    "question": false,
    "tags": [
      {
        "name": "Epistemology"
      },
      {
        "name": "Social Status"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "Scenario 1: Barry is a famous geologist. Charles is a fourteen-year-old juvenile delinquent with a long arrest record and occasional psychotic episodes. Barry flatly asserts to Arthur some counterintuitive statement about rocks, and Arthur judges it 90% probable. Then Charles makes an equally counterintuitive flat assertion about rocks, and Arthur judges it 10% probable. Clearly, Arthur is taking the speaker’s *authority* into account in deciding whether to believe the speaker’s assertions.\n\nScenario 2: David makes a counterintuitive statement about physics and gives Arthur a detailed explanation of the arguments, including references. Ernie makes an equally counterintuitive statement, but gives an unconvincing argument involving several leaps of faith. Both David and Ernie assert that this is the best explanation they can possibly give (to anyone, not just Arthur). Arthur assigns 90% probability to David’s statement after hearing his explanation, but assigns a 10% probability to Ernie’s statement.\n\nIt might seem like these two scenarios are roughly symmetrical: both involve taking into account useful evidence, whether strong versus weak authority, or strong versus weak argument.\n\nBut now suppose that Arthur asks Barry and Charles to make full technical cases, with references; and that Barry and Charles present equally good cases, and Arthur looks up the references and they check out. Then Arthur asks David and Ernie for their credentials, and it turns out that David and Ernie have roughly the same credentials—maybe they’re both clowns, maybe they’re both physicists.\n\nAssuming that Arthur is knowledgeable enough to understand all the technical arguments—otherwise they’re just impressive noises—it seems that Arthur should view David as having a great advantage in plausibility over Ernie, while Barry has at best a minor advantage over Charles.\n\nIndeed, if the technical arguments are good enough, Barry’s advantage over Charles may not be worth tracking. A good technical argument is one that *eliminates* reliance on the personal authority of the speaker.\n\nSimilarly, if we really believe Ernie that the argument he gave is the best argument he *could* give, which includes all of the inferential steps that Ernie executed, and all of the support that Ernie took into account—citing any authorities that Ernie may have listened to himself—then we can pretty much ignore any information about Ernie’s credentials. Ernie can be a physicist or a clown, it shouldn’t matter. (Again, this assumes we have enough technical ability to process the argument. Otherwise, Ernie is simply uttering mystical syllables, and whether we “believe” these syllables depends a great deal on his authority.)\n\nSo it seems there’s an asymmetry between argument and authority. If we know authority we are still interested in hearing the arguments; but if we know the arguments fully, we have very little left to learn from authority.\n\nClearly (says the novice) authority and argument are fundamentally different kinds of evidence, a difference unaccountable in the boringly clean methods of Bayesian probability theory.[^1^](#fn1x20) For while the strength of the evidences—90% versus 10%—is just the same in both cases, they do not behave similarly when combined. How will we account for this?\n\nHere’s half a technical demonstration of how to represent this difference in probability theory. (The rest you can take on my personal authority, or look up in the references.)\n\nIf P(H|E1) = 90% and P(H|E2) = 9%, what is the probability P(H|E1,E2)? If learning E1 is true leads us to assign 90% probability to H, and learning E2 is true leads us to assign 9% probability to H, then what probability should we assign to H if we learn both E1 and E2? This is simply not something you can calculate in probability theory from the information given. No, the missing information is not the prior probability of H. The events E1 and E2 may not be independent of each other.\n\nSuppose that H is “My sidewalk is slippery,” E1 is “My sprinkler is running,” and E2 is “It’s night.” The sidewalk is slippery starting from one minute after the sprinkler starts, until just after the sprinkler finishes, and the sprinkler runs for ten minutes. So if we know the sprinkler is on, the probability is 90% that the sidewalk is slippery. The sprinkler is on during 10% of the nighttime, so if we know that it’s night, the probability of the sidewalk being slippery is 9%. If we know that it’s night and the sprinkler is on—that is, if we know both facts—the probability of the sidewalk being slippery is 90%.\n\nWe can represent this in a graphical model as follows:\n\n![](https://res.cloudinary.com/lesswrong-2-0/image/upload/v1561065393/sequences/Argument%20Screens%20Off%20Authority/ArgumentScreensOffAuthority_diagram_1.svg)\n\nWhether or not it’s Night *causes* the Sprinkler to be on or off, and whether the Sprinkler is on *causes* the sidewalk to be Slippery or unSlippery.\n\nThe direction of the arrows is meaningful. Say we had:\n\n![](https://res.cloudinary.com/lesswrong-2-0/image/upload/v1561065393/sequences/Argument%20Screens%20Off%20Authority/ArgumentScreensOffAuthority_diagram_2.svg)\n\nThis would mean that, if I *didn’t* know anything about the sprinkler, the probability of Nighttime and Slipperiness would be independent of each other. For example, suppose that I roll Die One and Die Two, and add up the showing numbers to get the Sum:\n\n![](https://res.cloudinary.com/lesswrong-2-0/image/upload/v1561065393/sequences/Argument%20Screens%20Off%20Authority/ArgumentScreensOffAuthority_diagram_3.svg)\n\nIf you don’t tell me the sum of the two numbers, and you tell me the first die showed 6, this doesn’t tell me anything about the result of the second die, yet. But if you now also tell me the sum is 7, I know the second die showed 1.\n\nFiguring out when various pieces of information are dependent or independent of each other, given various background knowledge, actually turns into a quite technical topic. The books to read are Judea Pearl’s *Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference* and *Causality: Models, Reasoning, and Inference*. (If you only have time to read one book, read the first one.)\n\nIf you know how to read causal graphs, then you look at the dice-roll graph and immediately see:\n\n*P(Die 1,Die 2) = P(Die 1) ✕ P(Die 2)*\n\n*P(Die 1,Die 2|Sum) ≠ P(Die 1)|Sum) ✕ P(Die 2|Sum) .*\n\nIf you look at the correct sidewalk diagram, you see facts like:\n\n*P(Slippery|Night) ≠ P(Slippery)*\n\n*P(Slippery|Sprinkler) ≠ P(Slippery)*\n\n*P(Slippery|Night,Sprinkler) = P(Slippery|Sprinkler) .*\n\nThat is, the probability of the sidewalk being Slippery, given knowledge about the Sprinkler and the Night, is the same probability we would assign if we knew only about the Sprinkler. Knowledge of the Sprinkler has made knowledge of the Night irrelevant to inferences about Slipperiness.\n\nThis is known as *screening off*, and the criterion that lets us read such conditional independences off causal graphs is known as *D-separation*.\n\nFor the case of argument and authority, the causal diagram looks like this:\n\n![](https://res.cloudinary.com/lesswrong-2-0/image/upload/v1561065393/sequences/Argument%20Screens%20Off%20Authority/ArgumentScreensOffAuthority_diagram_4.svg)\n\nIf something is true, then it therefore tends to have arguments in favor of it, and the experts therefore observe these evidences and change their opinions. (In theory!)\n\nIf we see that an expert believes something, we infer back to the existence of evidence-in-the-abstract (even though we don’t know what that evidence is exactly), and from the existence of this abstract evidence, we infer back to the truth of the proposition.\n\nBut if we know the value of the Argument node, this D-separates the node “Truth” from the node “Expert Belief” by blocking all paths between them, according to certain technical criteria for “path blocking” that seem pretty obvious in this case. So even without checking the exact probability distribution, we can read off from the graph that:\n\n*P(truth|argument,expert) = P(truth|argument) .*\n\nThis does not represent a contradiction of ordinary probability theory. It’s just a more compact way of expressing certain probabilistic facts. You could read the same equalities and inequalities off an unadorned probability distribution—but it would be harder to see it by eyeballing. Authority and argument don’t need two different kinds of probability, any more than sprinklers are made out of ontologically different stuff than sunlight.\n\nIn practice you can never *completely* eliminate reliance on authority. Good authorities are more likely to know about any counterevidence that exists and should be taken into account; a lesser authority is less likely to know this, which makes their arguments less reliable. This is not a factor you can eliminate merely by hearing the evidence they *did* take into account.\n\nIt’s also very hard to reduce arguments to *pure* math; and otherwise, judging the strength of an inferential step may rely on intuitions you can’t duplicate without the same thirty years of experience.\n\nThere is an ineradicable legitimacy to assigning *slightly* higher probability to what E. T. Jaynes tells you about Bayesian probability, than you assign to Eliezer Yudkowsky making the exact same statement. Fifty additional years of experience should not count for literally *zero* influence.\n\nBut this slight strength of authority is only *ceteris paribus*, and can easily be overwhelmed by stronger arguments. I have a minor erratum in one of Jaynes’s books—because algebra trumps authority.\n\n* * *\n\n[^1^](#fn1x20-bk)See “[What Is Evidence?](https://www.lesswrong.com/rationality/what-is-evidence)” in *Map and Territory*."
    },
    "voteCount": 68,
    "forceInclude": true
  },
  {
    "_id": "XTXWPQSEgoMkAupKt",
    "url": null,
    "title": "An Intuitive Explanation of Bayes's Theorem",
    "slug": "an-intuitive-explanation-of-bayes-s-theorem",
    "author": "Eliezer Yudkowsky",
    "question": false,
    "tags": [
      {
        "name": "Bayes' Theorem"
      },
      {
        "name": "Rationality"
      },
      {
        "name": "Needs Fixup"
      },
      {
        "name": "Probability & Statistics"
      }
    ],
    "tableOfContents": {
      "sections": [
        {
          "title": "Q. Why did the Bayesian reasoner cross the road?\n",
          "anchor": "Q__Why_did_the_Bayesian_reasoner_cross_the_road__",
          "level": 1
        },
        {
          "title": "Q. How can I find the priors for a problem?\n",
          "anchor": "Q__How_can_I_find_the_priors_for_a_problem__",
          "level": 1
        },
        {
          "title": "originally",
          "anchor": "originally",
          "level": 1
        },
        {
          "title": "Q. Uh huh. Then where do scientists get their priors?\n",
          "anchor": "Q__Uh_huh__Then_where_do_scientists_get_their_priors__",
          "level": 1
        },
        {
          "title": "Q. I see. And where does everyone else get their priors?\n",
          "anchor": "Q__I_see__And_where_does_everyone_else_get_their_priors__",
          "level": 1
        },
        {
          "title": "Q. What if the priors I want aren’t available on Kazaa?\n",
          "anchor": "Q__What_if_the_priors_I_want_aren_t_available_on_Kazaa__",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        },
        {
          "anchor": "comments",
          "level": 0,
          "title": "8 comments"
        }
      ],
      "headingsCount": 8
    },
    "contents": {
      "markdown": "(Note: The author now considers this explanation obsoleted by the [Bayes' Rule Guide.](https://arbital.com/p/bayes_rule_guide/))\n\n    ￼[Editor’s Note: This is an abridgement of the original version of this essay, which contained many interactive elements.]\n\n* * *\n\nYour friends and colleagues are talking about something called “Bayes’s Theorem” or “Bayes’s Rule,” or something called Bayesian reasoning. They sound really enthusiastic about it, too, so you google and find a web page about Bayes’s Theorem and . . .\n\nIt’s this equation. That’s all. Just one equation. The page you found gives a definition of it, but it doesn’t say what it is, or why it’s useful, or why your friends would be interested in it. It looks like this random statistics thing.\n\nWhy does a mathematical concept generate this strange enthusiasm in its students? What is the so-called Bayesian Revolution now sweeping through the sciences, which claims to subsume even the experimental method itself as a special case? What is the secret that the adherents of Bayes know? What is the light that they have seen?\n\nSoon you will know. Soon you will be one of us.\n\nWhile there are a few existing online explanations of Bayes’s Theorem, my experience with trying to introduce people to Bayesian reasoning is that the existing online explanations are too abstract. Bayesian reasoning is very _counterintuitive_. People do not employ Bayesian reasoning intuitively, find it very difficult to learn Bayesian reasoning when tutored, and rapidly forget Bayesian methods once the tutoring is over. This holds equally true for novice students and highly trained professionals in a field. Bayesian reasoning is apparently one of those things which, like quantum mechanics or the Wason Selection Test, is inherently difficult for humans to grasp with our built-in mental faculties.\n\nOr so they claim. Here you will find an attempt to offer an _intuitive_ explanation of Bayesian reasoning—an excruciatingly gentle introduction that invokes all the human ways of grasping numbers, from natural frequencies to spatial visualization. The intent is to convey, not abstract rules for manipulating numbers, but what the numbers mean, and why the rules are what they are (and cannot possibly be anything else). When you are finished reading this, you will see Bayesian problems in your dreams.\n\nAnd let’s begin.\n\n* * *\n\nHere’s a story problem about a situation that doctors often encounter:\n\n> 1% of women at age forty who participate in routine screening have breast cancer. 80% of women with breast cancer will get positive mammograms. 9.6% of women without breast cancer will also get positive mammograms. A woman in this age group had a positive mammogram in a routine screening. What is the probability that she actually has breast cancer?\n\nWhat do you think the answer is? If you haven’t encountered this kind of problem before, please take a moment to come up with your own answer before continuing.\n\n* * *\n\nNext, suppose I told you that most doctors get the same wrong answer on this problem—usually, only around 15% of doctors get it right. (“Really? 15%? Is that a real number, or an urban legend based on an Internet poll?” It’s a real number. See Casscells, Schoenberger, and Graboys 1978;\\[1\\] Eddy 1982;\\[2\\] Gigerenzer and Hoffrage 1995;\\[3\\] and many other studies. It’s a surprising result which is easy to replicate, so it’s been extensively replicated.)\n\nOn the story problem above, most doctors estimate the probability to be between 70% and 80%, which is wildly incorrect.\n\nHere’s an alternate version of the problem on which doctors fare somewhat better:\n\n> 10 out of 1,000 women at age forty who participate in routine screening have breast cancer. 800 out of 1,000 women with breast cancer will get positive mammograms. 96 out of 1,000 women without breast cancer will also get positive mammograms. If 1,000 women in this age group undergo a routine screening, about what fraction of women with positive mammograms will actually have breast cancer?\n\nAnd finally, here’s the problem on which doctors fare best of all, with 46%— nearly half—arriving at the correct answer:\n\n> 100 out of 10,000 women at age forty who participate in routine screening have breast cancer. 80 of every 100 women with breast cancer will get a positive mammogram. 950 out of 9,900 women without breast cancer will also get a positive mammogram. If 10,000 women in this age group undergo a routine screening, about what fraction of women with positive mammograms will actually have breast cancer?\n\n* * *\n\nThe correct answer is 7.8%, obtained as follows: Out of 10,000 women, 100 have breast cancer; 80 of those 100 have positive mammograms. From the same 10,000 women, 9,900 will not have breast cancer and of those 9,900 women, 950 will also get positive mammograms. This makes the total number of women with positive mammograms 950 + 80 or 1,030. Of those 1,030 women with positive mammograms, 80 will have cancer. Expressed as a proportion, this is 80/1,030 or 0.07767 or 7.8%.\n\nTo put it another way, before the mammography, the 10,000 women can be divided into two groups:\n\n*   Group 1: 100 women _with_ breast cancer.\n    \n*   Group 2: 9,900 women _without_ breast cancer.\n    \n\nSumming these two groups gives a total of 10,000 patients, confirming that none have been lost in the math. After the mammography, the women can be divided into four groups:\n\n*   Group A: 80 women _with_ breast cancer and a _positive_ mammogram.\n    \n*   Group B: 20 women _with_ breast cancer and a _negative_ mammogram.\n    \n*   Group C: 950 women _without_ breast cancer and a _positive_ mammogram.\n    \n*   Group D: 8,950 women _without_ breast cancer and a _negative_ mammogram.\n    \n\nThe sum of groups A and B, the groups with breast cancer, corresponds to group 1; and the sum of groups C and D, the groups without breast cancer, corresponds to group 2. If you administer a mammography to 10,000 patients, then out of the 1,030 with positive mammograms, eighty of those positive-mammogram patients will have cancer. This is the correct answer, the answer a doctor should give a positive-mammogram patient if she asks about the chance she has breast cancer; if thirteen patients ask this question, roughly one out of those thirteen will have cancer.\n\n* * *\n\nThe most common mistake is to ignore the original fraction of women with breast cancer, and the fraction of women without breast cancer who receive false positives, and focus only on the fraction of women with breast cancer who get positive results. For example, the vast majority of doctors in these studies seem to have thought that if around 80% of women with breast cancer have positive mammograms, then the probability of a women with a positive mammogram having breast cancer must be around 80%.\n\nFiguring out the final answer always requires _all three_ pieces of information—the percentage of women with breast cancer, the percentage of women without breast cancer who receive false positives, and the percentage of women with breast cancer who receive (correct) positives.\n\nThe original proportion of patients with breast cancer is known as the _prior probability_. The chance that a patient with breast cancer gets a positive mammogram, and the chance that a patient without breast cancer gets a positive mammogram, are known as the two _conditional probabilities_. Collectively, this initial information is known as _the priors_. The final answer—the estimated probability that a patient has breast cancer, given that we know she has a positive result on her mammogram—is known as the _revised probability_ or the _posterior probability_. What we’ve just seen is that the posterior probability depends in part on the prior probability.\n\nTo see that the final answer always depends on the original fraction of women with breast cancer, consider an alternate universe in which only one woman out of a million has breast cancer. Even if mammography in this world detects breast cancer in 8 out of 10 cases, while returning a false positive on a woman without breast cancer in only 1 out of 10 cases, there will still be a hundred thousand false positives for every real case of cancer detected. The original probability that a woman has cancer is so extremely low that, although a positive result on the mammogram does _increase_ the estimated probability, the probability isn’t increased to certainty or even “a noticeable chance”; the probability goes from 1:1,000,000 to 1:100,000.\n\nWhat this demonstrates is that the mammogram result doesn’t _replace_ your old information about the patient’s chance of having cancer; the mammogram _slides_ the estimated probability in the direction of the result. A positive result slides the original probability upward; a negative result slides the probability downward. For example, in the original problem where 1% of the women have cancer, 80% of women with cancer get positive mammograms, and 9.6% of women without cancer get positive mammograms, a positive result on the mammogram _slides_ the 1% chance upward to 7.8%.\n\nMost people encountering problems of this type for the first time carry out the mental operation of _replacing_ the original 1% probability with the 80% probability that a woman with cancer gets a positive mammogram. It may seem like a good idea, but it just doesn’t work. “The probability that a woman with a positive mammogram has breast cancer” is not at all the same thing as “the probability that a woman with breast cancer has a positive mammogram”; they are as unlike as apples and cheese.\n\n* * *\n\n**Q. Why did the Bayesian reasoner cross the road?**\n\nA. You need more information to answer this question.\n\n* * *\n\nSuppose that a barrel contains many small plastic eggs. Some eggs are painted red and some are painted blue. 40% of the eggs in the bin contain pearls, and 60% contain nothing. 30% of eggs containing pearls are painted blue, and 10% of eggs containing nothing are painted blue. What is the probability that a blue egg contains a pearl? For this example the arithmetic is simple enough that you may be able to do it in your head, and I would suggest trying to do so.\n\nA more compact way of specifying the problem:\n\nP (pearl) = 40%\n\nP (blue|pearl) = 30%\n\nP (blue|¬pearl) = 10%\n\nP (pearl|blue) = ?\n\nThe symbol “¬” is shorthand for “not,” so ¬pearl reads “not pearl.”\n\nThe notation P (blue|pearl) is shorthand for “the probability of blue given pearl” or “the probability that an egg is painted blue, given that the egg contains a pearl.” The item on the right side is what you _already know_ or the _premise_, and the item on the left side is the _implication_ or _conclusion_. If we have P (blue|pearl) = 30%, and we _already know_ that some egg contains a pearl, then we can _conclude_ there is a 30% chance that the egg is painted blue. Thus, the final fact we’re looking for—“the chance that a blue egg contains a pearl” or “the probability that an egg contains a pearl, if we know the egg is painted blue”—reads P (pearl|blue).\n\n40% of the eggs contain pearls, and 60% of the eggs contain nothing. 30% of the eggs containing pearls are painted blue, so 12% of the eggs altogether contain pearls and are painted blue. 10% of the eggs containing nothing are painted blue, so altogether 6% of the eggs contain nothing and are painted blue. A total of 18% of the eggs are painted blue, and a total of 12% of the eggs are painted blue and contain pearls, so the chance a blue egg contains a pearl is 12/18 or 2/3 or around 67%.\n\nAs before, we can see the necessity of all three pieces of information by considering extreme cases. In a (large) barrel in which only one egg out of a thousand contains a pearl, knowing that an egg is painted blue slides the probability from 0.1% to 0.3% (instead of sliding the probability from 40% to 67%). Similarly, if 999 out of 1,000 eggs contain pearls, knowing that an egg is blue slides the probability from 99.9% to 99.966%; the probability that the egg does not contain a pearl goes from 1/1,000 to around 1/3,000.\n\nOn the pearl-egg problem, most respondents unfamiliar with Bayesian reasoning would probably respond that the probability a blue egg contains a pearl is 30%, or perhaps 20% (the 30% chance of a true positive minus the 10% chance of a false positive). Even if this mental operation seems like a good idea at the time, it makes no sense in terms of the question asked. It’s like the experiment in which you ask a second-grader: “If eighteen people get on a bus, and then seven more people get on the bus, how old is the bus driver?” Many second-graders will respond: “Twenty-five.” They understand when they’re being prompted to carry out a particular mental procedure, but they haven’t quite connected the procedure to reality. Similarly, to find the probability that a woman with a positive mammogram has breast cancer, it makes no sense whatsoever to _replace_ the original probability that the woman has cancer with the probability that a woman with breast cancer gets a positive mammogram. Neither can you subtract the probability of a false positive from the probability of the true positive. These operations are as wildly irrelevant as adding the number of people on the bus to find the age of the bus driver.\n\n* * *\n\nA study by Gigerenzer and Hoffrage in 1995 showed that some ways of phrasing story problems are much more evocative of correct Bayesian reasoning.\\[4\\] The _least_ evocative phrasing used probabilities. A slightly more evocative phrasing used frequencies instead of probabilities; the problem remained the same, but instead of saying that 1% of women had breast cancer, one would say that 1 out of 100 women had breast cancer, that 80 out of 100 women with breast cancer would get a positive mammogram, and so on. Why did a higher proportion of subjects display Bayesian reasoning on this problem? Probably because saying “1 out of 100 women” encourages you to concretely visualize X women with cancer, leading you to visualize X women with cancer and a positive mammogram, etc.\n\nThe most effective presentation found so far is what’s known as _natural frequencies_—saying that 40 out of 100 eggs contain pearls, 12 out of 40 eggs containing pearls are painted blue, and 6 out of 60 eggs containing nothing are painted blue. A _natural frequencies_ presentation is one in which the information about the prior probability is included in presenting the conditional probabilities. If you were just learning about the eggs’ conditional probabilities through natural experimentation, you would—in the course of cracking open a hundred eggs—crack open around 40 eggs containing pearls, of which 12 eggs would be painted blue, while cracking open 60 eggs containing nothing, of which about 6 would be painted blue. In the course of learning the conditional probabilities, you’d see examples of blue eggs containing pearls about twice as often as you saw examples of blue eggs containing nothing.\n\nUnfortunately, while natural frequencies are a step in the right direction, it probably won’t be enough. When problems are presented in natural frequencies, the proportion of people using Bayesian reasoning rises to around half. A big improvement, but not big enough when you’re talking about real doctors and real patients.\n\n* * *\n\n> **Q. How can I find the priors for a problem?**\n> \n> A. Many commonly used priors are listed in the _Handbook of Chemistry and Physics_.\n> \n> **Q. Where do priors** _**originally**_ **come from?**\n> \n> A. Never ask that question.\n> \n> **Q. Uh huh. Then where do scientists get their priors?**\n> \n> A. Priors for scientific problems are established by annual vote of the AAAS. In recent years the vote has become fractious and controversial, with widespread acrimony, factional polarization, and several outright assassinations. This may be a front for infighting within the Bayes Council, or it may be that the disputants have too much spare time. No one is really sure.\n> \n> **Q. I see. And where does everyone else get their priors?**\n> \n> A. They download their priors from Kazaa.\n> \n> **Q. What if the priors I want aren’t available on Kazaa?**\n> \n> A. There’s a small, cluttered antique shop in a back alley of San Francisco’s Chinatown. _Don’t ask about the bronze rat._\n\nActually, priors are true or false just like the final answer—they reflect reality and can be judged by comparing them against reality. For example, if you think that 920 out of 10,000 women in a sample have breast cancer, and the actual number is 100 out of 10,000, then your priors are wrong. For our particular problem, the priors might have been established by three studies—a study on the case histories of women with breast cancer to see how many of them get a positive mammogram, a study on women without breast cancer to see how many of them get a positive mammogram, and an epidemiological study on the prevalence of breast cancer in some specific demographic.\n\n* * *\n\nThe probability P (A, B) is the same as P (B, A), but P (A|B) is not the same thing as P (B|A), and P (A, B) is completely different from P (A|B). It’s a common confusion to mix up some or all of these quantities.\n\nTo get acquainted with all the relationships between them, we’ll play “follow the degrees of freedom.” For example, the two quantities P (cancer) and P (¬cancer) have one degree of freedom between them, because of the general law P (A) + P (¬A) = 1. If you know that P (¬cancer) = 0.99, you can obtain P (cancer) = 1 − P (¬cancer) = 0.01.\n\nThe quantities P (positive|cancer) and P (¬positive|cancer) also have only one degree of freedom between them; either a woman with breast cancer gets a positive mammogram or she doesn’t. On the other hand, P (positive|cancer) and P (positive|¬cancer) have two degrees of freedom. You can have a mammography that returns positive for 80% of cancer patients and 9.6% of healthy patients, or that returns positive for 70% of cancer patients and 2% of healthy patients, or even a health test that returns “positive” for 30% of cancer patients and 92% of healthy patients. The two quantities, the output of the mammography for cancer patients and the output of the mammography for healthy patients, are in mathematical terms independent; one cannot be obtained from the other in any way, and so they have two degrees of freedom between them.\n\nWhat about P(positive,cancer), P(positive|cancer), and P(cancer)? Here we have three quantities; how many degrees of freedom are there? In this case the equation that must hold is\n\nP (positive, cancer) = P (positive|cancer) × P (cancer) .\n\nThis equality reduces the degrees of freedom by one. If we know the fraction of patients with cancer, and the chance that a cancer patient has a positive mammogram, we can deduce the fraction of patients who have breast cancer _and_ a positive mammogram by multiplying.\n\nSimilarly, if we know the number of patients with breast cancer and positive mammograms, and also the number of patients with breast cancer, we can estimate the chance that a woman with breast cancer gets a positive mammogram by dividing: P (positive|cancer) = P (positive, cancer)/P (cancer). In fact, this is exactly how such medical diagnostic tests are calibrated; you do a study on 8,520 women with breast cancer and see that there are 6,816 (or thereabouts) women with breast cancer and positive mammograms, then divide 6,816 by 8,520 to find that 80% of women with breast cancer had positive mammograms. (Incidentally, if you accidentally divide 8,520 by 6,816 instead of the other way around, your calculations will start doing strange things, such as insisting that 125% of women with breast cancer and positive mammograms have breast cancer. This is a common mistake in carrying out Bayesian arithmetic, in my experience.) And finally, if you know P (positive, cancer) and P (positive|cancer), you can deduce how many cancer patients there must have been originally. There are two degrees of freedom shared out among the three quantities; if we know any two, we can deduce the third.\n\nHow about P (positive), P (positive, cancer), and P (positive, ¬cancer)? Again there are only two degrees of freedom among these three variables. The equation occupying the extra degree of freedom is\n\nP (positive) = P (positive, cancer) + P (positive, ¬cancer) .\n\nThis is how P (positive) is computed to begin with; we figure out the number of women with breast cancer who have positive mammograms, and the number of women without breast cancer who have positive mammograms, then add them together to get the total number of women with positive mammograms. It would be very strange to go out and conduct a study to determine the number of women with positive mammograms— just that one number and nothing else—but in theory you could do so. And if you then conducted another study and found the number of those women who had positive mammograms _and_ breast cancer, you would also know the number of women with positive mammograms and _no_ breast cancer—either a woman with a positive mammogram has breast cancer or she doesn’t. In general, P (A, B) + P (A, ¬B) = P (A). Symmetrically, P (A, B) + P (¬A, B) = P (B).\n\nWhat about P (positive, cancer), P (positive, ¬cancer), P (¬positive, cancer), and P (¬positive, ¬cancer)? You might at first be tempted to think that there are only two degrees of freedom for these four quantities—that you can, for example, get P (positive, ¬cancer) by multiplying P (positive) × P(¬cancer), and thus that all four quantities can be found given only the two quantities P(positive) and P(cancer). This is not the case! P (positive, ¬cancer) = P (positive) × P (¬cancer) only if the two probabilities are _statistically independent_—if the chance that a woman has breast cancer has no bearing on whether she has a positive mammogram. This amounts to requiring that the two conditional probabilities be equal to each other—a requirement which would eliminate one degree of freedom. If you remember that these four quantities are the groups A, B, C, and D, you can look over those four groups and realize that, in theory, you can put any number of people into the four groups. If you start with a group of 80 women with breast cancer and positive mammograms, there’s no reason why you can’t add another group of 500 women with breast cancer and negative mammograms, followed by a group of 3 women without breast cancer and negative mammograms, and so on. So now it seems like the four quantities have four degrees of freedom. And they would, except that in expressing them as _probabilities_, we need to normalize them to _fractions_ of the complete group, which adds the constraint that P (positive, cancer) + P (positive, ¬cancer) + P (¬positive, cancer) + P (¬positive, ¬cancer) = 1. This equation takes up one degree of freedom, leaving three degrees of freedom among the four quantities. If you specify the _fractions_ of women in groups A, B, and D, you can deduce the fraction of women in group C.\n\nGiven the four groups A, B, C, and D, it is very straightforward to compute everything else:\n\nP(cancer) = (A + B) / (A + B + C + D)\n\nP (¬positive|cancer) = B / (A + B)\n\nand so on. Since {A, B, C, D} contains three degrees of freedom, it follows that the entire set of probabilities relating cancer rates to test results contains only three degrees of freedom. Remember that in our problems we always needed _three_ pieces of information—the prior probability and the two conditional probabilities—which, indeed, have three degrees of freedom among them. Actually, for Bayesian problems, _any_ three quantities with three degrees of freedom between them should logically specify the entire problem.\n\n* * *\n\n_The probability that a test gives a true positive_ divided by _the probability that a test gives a false positive_ is known as the _likelihood ratio_ of that test. The likelihood ratio for a positive result summarizes how much a positive result will slide the prior probability. Does the likelihood ratio of a medical test then sum up everything there is to know about the usefulness of the test?\n\nNo, it does not! The likelihood ratio sums up everything there is to know about the _meaning_ of a _positive_ result on the medical test, but the meaning of a _negative_ result on the test is not specified, nor is the frequency with which the test is useful. For example, a mammography with a hit rate of 80% for patients with breast cancer and a false positive rate of 9.6% for healthy patients has the same likelihood ratio as a test with an 8% hit rate and a false positive rate of 0.96%. Although these two tests have the same likelihood ratio, the first test is more useful in every way—it detects disease more often, and a negative result is stronger evidence of health.\n\n* * *\n\nSuppose that you apply _two_ tests for breast cancer in succession—say, a standard mammogram and also some other test which is _independent_ of mammography. Since I don’t know of any such test that is independent of mammography, I’ll invent one for the purpose of this problem, and call it the Tams-Braylor Division Test, which checks to see if any cells are dividing more rapidly than other cells. We’ll suppose that the Tams-Braylor gives a true positive for 90% of patients with breast cancer, and gives a false positive for 5% of patients without cancer. Let’s say the prior prevalence of breast cancer is 1%. If a patient gets a positive result on her mammogram _and_ her Tams-Braylor, what is the revised probability she has breast cancer?\n\nOne way to solve this problem would be to take the revised probability for a positive mammogram, which we already calculated as 7.8%, and plug that into the Tams-Braylor test as the new prior probability. If we do this, we find that the result comes out to 60%.\n\nSuppose that the prior prevalence of breast cancer in a demographic is 1%. Suppose that we, as doctors, have a repertoire of three independent tests for breast cancer. Our first test, test A, a mammography, has a likelihood ratio of 80%/9.6% = 8.33. The second test, test B, has a likelihood ratio of 18.0 (for example, from 90% versus 5%); and the third test, test C, has a likelihood ratio of 3.5 (which could be from 70% versus 20%, or from 35% versus 10%; it makes no difference). Suppose a patient gets a positive result on all three tests. What is the probability the patient has breast cancer?\n\n￼Here’s a fun trick for simplifying the bookkeeping. If the prior prevalence of breast cancer in a demographic is 1%, then 1 out of 100 women have breast cancer, and 99 out of 100 women do not have breast cancer. So if we rewrite the _probability_ of 1% as an _odds ratio_, the odds are 1:99.\n\nAnd the likelihood ratios of the three tests A, B, and C are:\n\n8.33 : 1 = 25 : 3\n\n18.0 : 1 = 18 : 1\n\n3.5 : 1 = 7 : 2 .\n\nThe _odds_ for women with breast cancer who score positive on all three tests, versus women without breast cancer who score positive on all three tests, will equal:\n\n1 × 25 × 18 × 7 : 99 × 3 × 1 × 2 = 3150 : 594.\n\nTo recover the probability from the odds, we just write:\n\n3150/(3150 + 594) = 84% .\n\nThis always works regardless of how the odds ratios are written; i.e., 8.33:1 is just the same as 25:3 or 75:9. It doesn’t matter in what order the tests are administered, or in what order the results are computed. The proof is left as an exercise for the reader.\n\n* * *\n\nE. T. Jaynes, in _Probability Theory With Applications in Science and Engineering_, suggests that credibility and evidence should be measured in decibels.\\[5\\]\n\nDecibels?\n\nDecibels are used for measuring exponential differences of intensity. For example, if the sound from an automobile horn carries 10,000 times as much energy (per square meter per second) as the sound from an alarm clock, the automobile horn would be 40 decibels louder. The sound of a bird singing might carry 1,000 times less energy than an alarm clock, and hence would be 30 decibels softer. To get the number of decibels, you take the logarithm base 10 and multiply by 10:\n\ndecibels = 10log_10(intensity)\n\nintensity = 10^(decibels/10) .\n\nSuppose we start with a prior probability of 1% that a woman has breast cancer, corresponding to an odds ratio of 1:99. And then we administer three tests of likelihood ratios 25:3, 18:1, and 7:2. You _could_ multiply those numbers . . . or you could just add their logarithms:\n\n10log_10(1/99) ≈ −20\n\n10log_10(25/3) ≈ 9\n\n10log_10(18/1) ≈ 13\n\n10 log_10(7/2) ≈ 5 .\n\nIt starts out as fairly unlikely that a woman has breast cancer—our credibility level is at −20 decibels. Then three test results come in, corresponding to 9, 13, and 5 decibels of evidence. This raises the credibility level by a total of 27 decibels, meaning that the prior credibility of −20 decibels goes to a posterior credibility of 7 decibels. So the odds go from 1:99 to 5:1, and the probability goes from 1% to around 83%.\n\n* * *\n\n> You are a mechanic for gizmos. When a gizmo stops working, it is due to a blocked hose 30% of the time. If a gizmo’s hose is blocked, there is a 45% probability that prodding the gizmo will produce sparks. If a gizmo’s hose is unblocked, there is only a 5% chance that prodding the gizmo will produce sparks. A customer brings you a malfunctioning gizmo. You prod the gizmo and find that it produces sparks. What is the probability that a spark-producing gizmo has a blocked hose?\n\nWhat is the sequence of arithmetical operations that you performed to solve this problem?\n\n(45% × 30%)/(45% × 30% + 5% × 70%)\n\nSimilarly, to find the chance that a woman with a positive mammogram has breast cancer, we computed:\n\n\\[P (positive|cancer) × P (cancer)\\] / \\[P(positive|cancer)×P(cancer) 􏰆 + P (positive|¬cancer) × P (¬cancer)\\]\n\n_which is_\n\nP (positive, cancer) / \\[P (positive, cancer) + P (positive, ¬cancer)\\]\n\n_which is_\n\nP (positive, cancer) / P (positive)\n\n_which is_\n\nP(cancer|positive) .\n\nThe fully general form of this calculation is known as _Bayes’s Theorem_ or _Bayes’s Rule_.\n\nBayes’s Theorem:\n\nP(A|X) = \\[P(X|A) × P(A)\\] / \\[P(X|A) × P(A) + P(X|¬A) × P(¬A) \\]\n\nWhen there is some phenomenon A that we want to investigate, and an observation X that is evidence about A—for example, in the previous example, A is breast cancer and X is a positive mammogram—Bayes’s Theorem tells us how we should _update_ our probability of A, given the _new evidence_ X.\n\nBy this point, Bayes’s Theorem may seem blatantly obvious or even tautological, rather than exciting and new. If so, this introduction has _entirely succeeded_ in its purpose.\n\n* * *\n\nBayes’s Theorem describes what makes something “evidence” and how much evidence it is. Statistical models are judged by comparison to the _Bayesian method_ because, in statistics, the Bayesian method is as good as it gets—the Bayesian method defines the maximum amount of mileage you can get out of a given piece of evidence, in the same way that thermodynamics defines the maximum amount of work you can get out of a temperature differential. This is why you hear cognitive scientists talking about _Bayesian reasoners_. In cognitive science, Bayesian reasoner is the technically precise code word that we use to mean _rational mind_.\n\nThere are also a number of general heuristics about human reasoning that you can learn from looking at Bayes’s Theorem.\n\nFor example, in many discussions of Bayes’s Theorem, you may hear cognitive psychologists saying that people _do not take prior frequencies sufficiently into account_, meaning that when people approach a problem where there’s some evidence X indicating that condition A might hold true, they tend to judge A’s likelihood solely by how well the evidence X seems to match A, without taking into account the prior frequency of A. If you think, for example, that under the mammography example, the woman’s chance of having breast cancer is in the range of 70%–80%, then this kind of reasoning is insensitive to the prior frequency given in the problem; it doesn’t notice whether 1% of women or 10% of women start out having breast cancer. “Pay more attention to the prior frequency!” is one of the many things that humans need to bear in mind to partially compensate for our built-in inadequacies.\n\nA related error is to pay too much attention to P (X |A) and not enough to P(X|¬A) when determining how much evidence X is for A. The degree to which a result X is _evidence for_ A depends not only on the strength of the statement _we’d expect to see result X if A were true_, but also on the strength of the statement _we_ **_wouldn’t_** _expect to see result X if A weren’t true_. For example, if it is raining, this very strongly implies the grass is wet—P (wetgrass|rain) ≈ 1— but seeing that the grass is wet doesn’t necessarily mean that it has just rained; perhaps the sprinkler was turned on, or you’re looking at the early morning dew. Since P (wetgrass|¬rain) is substantially greater than zero, P (rain|wetgrass) is substantially less than one. On the other hand, if the grass was _never_ wet when it wasn’t raining, then knowing that the grass was wet would _always_ show that it was raining, P (rain|wetgrass) ≈ 1, even if P (wetgrass|rain) = 50%; that is, even if the grass only got wet 50% of the times it rained. Evidence is always the result of the _differential_ between the two conditional probabilities. _Strong_ evidence is not the product of a very high probability that A leads to X, but the product of a very low probability that _not_-A could have led to X.\n\nThe _Bayesian revolution in the sciences_ is fueled, not only by more and more cognitive scientists suddenly noticing that mental phenomena have Bayesian structure in them; not only by scientists in every field learning to judge their statistical methods by comparison with the Bayesian method; but also by the idea that _science itself is a special case of Bayes’s Theorem; experimental evidence is Bayesian evidence_. The Bayesian revolutionaries hold that when you perform an experiment and get evidence that “confirms” or “disconfirms” your theory, this confirmation and disconfirmation is governed by the Bayesian rules. For example, you have to take into account not only whether your theory predicts the phenomenon, but whether other possible explanations also predict the phenomenon.\n\nPreviously, the most popular philosophy of science was probably Karl Popper’s _falsificationism_—this is the old philosophy that the Bayesian revolution is currently dethroning. Karl Popper’s idea that theories can be definitely falsified, but never definitely confirmed, is yet another special case of the Bayesian rules; if P(X|A) ≈ 1—if the theory makes a definite prediction—then observing ¬X very strongly falsifies A. On the other hand, if P(X|A) ≈ 1, and we observe X, this doesn’t definitely confirm the theory; there might be some other condition B such that P (X|B) ≈ 1, in which case observing X doesn’t favor A over B. For observing X to definitely confirm A, we would have to know, not that P(X|A) ≈ 1, but that P(X|¬A) ≈ 0, which is something that we can’t know because we can’t range over all possible alternative explanations. For example, when Einstein’s theory of General Relativity toppled Newton’s incredibly well-confirmed theory of gravity, it turned out that all of Newton’s predictions were just a special case of Einstein’s predictions.\n\nYou can even formalize Popper’s philosophy mathematically. The likelihood ratio for X, the quantity P(X|A)/P(X|¬A), determines how much observing X slides the probability for A; the likelihood ratio is what says _how strong_ X is as evidence. Well, in your theory A, you can predict X with probability 1, if you like; but you can’t control the denominator of the likelihood ratio, P(X|¬A)—there will always be some alternative theories that also predict X, and while we go with the simplest theory that fits the current evidence, you may someday encounter some evidence that an alternative theory predicts but your theory does not. That’s the hidden gotcha that toppled Newton’s theory of gravity. So there’s a limit on how much mileage you can get from successful predictions; there’s a limit on how high the likelihood ratio goes for _confirmatory_ evidence.\n\nOn the other hand, if you encounter some piece of evidence Y that is definitely _not_ predicted by your theory, this is _enormously_ strong evidence against your theory. If P (Y |A) is infinitesimal, then the likelihood ratio will also be infinitesimal. For example, if P (Y |A) is 0.0001%, and P (Y |¬A) is 1%, then the likelihood ratio P (Y |A)/P (Y |¬A) will be 1:10,000. That’s −40 decibels of evidence! Or, flipping the likelihood ratio, if P (Y |A) is _very small_, then P (Y |¬A)/P (Y |A) will be very large, meaning that observing Y greatly favors ¬A over A. Falsification is much stronger than confirmation. This is a consequence of the earlier point that _very strong_ evidence is not the product of a very high probability that A leads to X, but the product of a very _low_ probability that _not_-A could have led to X. This is the precise Bayesian rule that underlies the heuristic value of Popper’s falsificationism.\n\nSimilarly, Popper’s dictum that an idea must be falsifiable can be interpreted as a manifestation of the Bayesian conservation-of-probability rule; if a result X is positive evidence for the theory, then the result ¬X would have disconfirmed the theory to some extent. If you try to interpret both X and ¬X as “confirming” the theory, the Bayesian rules say this is impossible! To increase the probability of a theory you _must_ expose it to tests that can potentially decrease its probability; this is not just a rule for detecting would-be cheaters in the social process of science, but a consequence of Bayesian probability theory. On the other hand, Popper’s idea that there is _only_ falsification and _no such thing_ as confirmation turns out to be incorrect. Bayes’s Theorem shows that falsification is _very strong_ evidence compared to confirmation, but falsification is still probabilistic in nature; it is not governed by fundamentally different rules from confirmation, as Popper argued.\n\nSo we find that many phenomena in the cognitive sciences, plus the statistical methods used by scientists, plus the scientific method itself, are all turning out to be special cases of Bayes’s Theorem. Hence the Bayesian revolution.\n\n* * *\n\nHaving introduced Bayes’s Theorem explicitly, we can explicitly discuss its components.\n\nP(A|X) = \\[P(X|A) × P(A)\\] / \\[ P(X|A) × P(A) + P(X|¬A) × P(¬A) \\]\n\nWe’ll start with P(A|X). If you ever find yourself getting confused about what’s A and what’s X in Bayes’s Theorem, start with P(A|X) on the left side of the equation; that’s the simplest part to interpret. In P(A|X), A is the thing we want to know about. X is how we’re observing it; X is the evidence we’re using to make inferences about A. Remember that for every expression P(Q|P), we want to know about the probability for Q given P, the degree to which P implies Q—a more sensible notation, which it is now too late to adopt, would be P (Q ← P ).\n\nP (Q|P ) is closely related to P (Q, P ), but they are not identical. Expressed as a probability or a fraction, P (Q, P ) is the proportion of things that have property Q and property P among all things; e.g., the proportion of “women with breast cancer and a positive mammogram” within the group of all women. If the total number of women is 10,000, and 80 women have breast cancer and a positive mammogram, then P (Q, P ) is 80/10,000 = 0.8%. You might say that the absolute quantity, 80, is being normalized to a probability relative to the group of all women. Or to make it clearer, suppose that there’s a group of 641 women with breast cancer and a positive mammogram within a total sample group of 89,031 women. Six hundred and forty-one is the absolute quantity. If you pick out a random woman from the entire sample, then the probability you’ll pick a woman with breast cancer and a positive mammogram is P (Q, P ), or 0.72% (in this example).\n\nOn the other hand, P (Q|P ) is the proportion of things that have property Q and property P among _all things that have_ P ; e.g., the proportion of women with breast cancer and a positive mammogram within the group of _all women with positive mammograms_. If there are 641 women with breast cancer and positive mammograms, 7,915 women with positive mammograms, and 89,031 women, then P (Q, P ) is the probability of getting one of those 641 women if you’re picking at random from the entire group of 89,031, while P (Q|P ) is the probability of getting one of those 641 women if you’re picking at random from the smaller group of 7,915.\n\nIn a sense, P (Q|P ) really means P (Q, P |P ), but specifying the extra P all the time would be redundant. You already _know_ it has property P, so the property you’re _investigating_ is Q—even though you’re looking at the size of group (Q,P) within group P, not the size of group Q within group P (which would be nonsense). This is what it means to take the property on the right-hand side as given; it means you know you’re working only within the group of things that have property P. When you constrict your focus of attention to see only this smaller group, many other probabilities change. If you’re taking P as _given_, then P (Q, P ) equals just P (Q)—at least, _relative to the group_ P . The _old_ P (Q), the frequency of “things that have property Q within the entire sample,” is revised to the new frequency of “things that have property Q within the subsample of things that have property P. ” If P is _given_, if P is our entire world, then looking for (Q, P ) is the same as looking for just Q.\n\nIf you constrict your focus of attention to only the population of eggs that are painted blue, then suddenly “the probability that an egg contains a pearl” becomes a different number; this proportion is different for the population of blue eggs than the population of all eggs. The _given_, the property that constricts our focus of attention, is always on the _right_ side of P (Q|P ); the P becomes our world, the entire thing we see, and on the other side of the “given” P always has probability 1—that is what it means to take P as given. So P (Q|P ) means “If P has probability 1, what is the probability of Q?” or “If we constrict our attention to only things or events where P is true, what is the probability of Q?” The statement Q, on the other side of the given, is _not_ certain—its probability may be 10% or 90% or any other number. So when you use Bayes’s Theorem, and you write the part on the left side as P(A|X)—how to _update_ the probability of A after seeing X, the new probability of A _given_ that we ￼know X, the degree to which X _implies_ A—you can tell that X is always the _observation_ or the _evidence_, and A is the property being investigated, the thing you want to know about.\n\n* * *\n\nThe right side of Bayes’s Theorem is derived from the left side through these steps:\n\nP (A|X) = P (A|X)\n\nP(A|X)= P(X,A) / P(X)\n\nP(A|X) = P(X,A) / \\[ P(X,A) + P(X,¬A)\\]\n\nP(A|X) =\\[ P(X|A) × P(A)\\] / \\[ P(X|A) × P(A) + P(X|¬A) × P(¬A) \\] .\n\nOnce the derivation is finished, all the implications on the right side of the equation are of the form P(X|A) or P(X|¬A), while the implication on the left side is P(A|X). The symmetry arises because the elementary _causal relations_ are generally implications from facts to observations, e.g., from breast cancer to positive mammogram. The elementary _steps in reasoning_ are generally implications from observations to facts, e.g., from a positive mammogram to breast cancer. The left side of Bayes’s Theorem is an elementary _inferential_ step from the observation of positive mammogram to the conclusion of an increased probability of breast cancer. Implication is written right-to-left, so we write P (cancer|positive) on the left side of the equation. The right side of Bayes’s Theorem describes the elementary _causal_ steps—for example, from breast cancer to a positive mammogram—and so the implications on the right side of Bayes’s Theorem take the form P (positive|cancer) or P (positive|¬cancer).\n\nAnd that’s Bayes’s Theorem. Rational inference on the left end, physical causality on the right end; an equation with mind on one side and reality on the other. Remember how the scientific method turned out to be a special case of Bayes’s Theorem? If you wanted to put it poetically, you could say that Bayes’s Theorem binds reasoning into the physical universe.\n\nOkay, we’re done.\n\n* * *\n\nReverend Bayes says:\n\n![](https://www.readthesequences.com/wiki/uploads/Bayes-mugshot.png)\n\nYou are now an initiate of the Bayesian Conspiracy.\n\n* * *\n\n###### 1\\. Ward Casscells, Arno Schoenberger, and Thomas Graboys, “Interpretation by Physicians of Clinical Laboratory Results,” _New England Journal of Medicine_ 299 (1978): 999–1001.\n\n###### 2\\. David M. Eddy, “Probabilistic Reasoning in Clinical Medicine: Problems and Opportunities,” in _Judgement Under Uncertainty: Heuristics and Biases_, ed. Daniel Kahneman, Paul Slovic, and Amos Tversky (Cambridge University Press, 1982).\n\n###### 3\\. Gerd Gigerenzer and Ulrich Hoffrage, “How to Improve Bayesian Reasoning without Instruction: Frequency Formats,” _Psychological Review_ 102 (1995): 684–704.\n\n###### 4\\. Ibid.\n\n###### 5\\. Edwin T. Jaynes, “Probability Theory, with Applications in Science and Engineering,” Unpublished manuscript (1974).\n\n* * *\n\n_The first publication of this post is_ [_here_](http://yudkowsky.net/rational/bayes)_._"
    },
    "voteCount": 30,
    "forceInclude": true
  },
  {
    "_id": "QkX2bAkwG2EpGvNug",
    "url": null,
    "title": "The Second Law of Thermodynamics, and Engines of Cognition",
    "slug": "the-second-law-of-thermodynamics-and-engines-of-cognition",
    "author": "Eliezer_Yudkowsky",
    "question": false,
    "tags": [
      {
        "name": "World Modeling"
      },
      {
        "name": "Law-Thinking"
      },
      {
        "name": "Physics"
      },
      {
        "name": "Probability & Statistics"
      },
      {
        "name": "Information Theory"
      },
      {
        "name": "Bayesianism"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "The first law of thermodynamics, better known as Conservation of Energy, says that you can't create energy from nothing: it prohibits perpetual motion machines of the first type, which run and run indefinitely without consuming fuel or any other resource.  According to our modern view of physics, energy is conserved in each _individual_ interaction of particles.  By mathematical induction, we see that no matter how large an assemblage of particles may be, it cannot produce energy from nothing - not without violating what we presently believe to be the laws of physics.\n\nThis is why the US Patent Office will summarily reject your amazingly clever proposal for an assemblage of wheels and gears that cause one spring to wind up another as the first runs down, and so continue to do work forever, according to your calculations.  There's a _fully general_ proof that at least one wheel must violate (our standard model of) the laws of physics for this to happen.  So unless you can explain how _one_ wheel violates the laws of physics, the _assembly_ of wheels can't do it either.\n\nA similar argument applies to a \"reactionless drive\", a propulsion system that violates Conservation of Momentum.  In standard physics, momentum is conserved for all individual particles and their interactions; by mathematical induction, momentum is conserved for physical systems whatever their size.  If you can visualize two particles knocking into each other and always coming out with the same total momentum that they started with, then you can see how scaling it up from particles to a gigantic complicated collection of gears won't change anything.  Even if there's a trillion quadrillion atoms involved, 0 + 0 + ... + 0 = 0.\n\nBut Conservation of Energy, as such, cannot prohibit converting heat into work.  You can, in fact, build a sealed box that converts ice cubes and stored electricity into warm water.  It isn't even difficult.  Energy cannot be created or destroyed:  The net change in energy, from transforming (ice cubes + electricity) to (warm water), must be 0.  So it couldn't violate Conservation of Energy, as such, if you did it the other way around...\n\nPerpetual motion machines of the second type, which convert warm water into electrical current and ice cubes, are prohibited by the _Second_ Law of Thermodynamics.\n\nThe Second Law is a bit harder to understand, as it is essentially Bayesian in nature.\n\nYes, really.\n\nThe essential _physical_ law underlying the Second Law of Thermodynamics is a theorem which can be proven within the standard model of physics:  _In the development over time of any closed system, phase space volume is conserved._\n\nLet's say you're holding a ball high above the ground.  We can describe this state of affairs as a point in a multidimensional space, at least one of whose dimensions is \"height of ball above the ground\".  Then, when you drop the ball, it moves, and so does the dimensionless point in phase space that describes the entire system that includes you and the ball.  \"Phase space\", in physics-speak, means that there are dimensions for the momentum of the particles, not just their position - i.e., a system of 2 particles would have 12 dimensions, 3 dimensions for each particle's position, and 3 dimensions for each particle's momentum.\n\nIf you had a multidimensional space, each of whose dimensions described the position of a gear in a huge assemblage of gears, then as you turned the gears a single point would swoop and dart around in a rather high-dimensional phase space.  Which is to say, just as you can view a great big complex machine as a single point in a very-high-dimensional space, so too, you can view the laws of physics describing the behavior of this machine over time, as describing the trajectory of its point through the phase space.\n\nThe Second Law of Thermodynamics is a consequence of a theorem which can be proven in the standard model of physics:  If you take a volume of phase space, and develop it forward in time using standard physics, the total volume of the phase space is conserved.\n\nFor example:\n\nLet there be two systems, X and Y: where X has 8 possible states, Y has 4 possible states, and the joint system (X,Y) has 32 possible states.\n\nThe development of the joint system over time can be described as a rule that maps initial points onto future points.  For example, the system could start out in X~7~Y~2~, then develop (under some set of physical laws) into the state X~3~Y~3~ a minute later.  Which is to say: if X started in 7, and Y started in 2, and we watched it for 1 minute, we would see X go to 3 and Y go to 3.  Such are the laws of physics.\n\nNext, let's carve out a subspace S of the joint system state.  S will be the subspace bounded by X being in state 1 and Y being in states 1-4.  So the total volume of S is 4 states.\n\nAnd let's suppose that, under the laws of physics governing (X,Y) the states initially in S behave as follows:\n\n> X~1~Y~1~ -\\> X~2~Y~1~  \n> X~1~Y~2~ -\\> X~4~Y~1~  \n> X~1~Y~3~ -\\> X~6~Y~1~  \n> X~1~Y~4~ -\\> X~8~Y~1~\n\nThat, in a nutshell, is how a refrigerator works.\n\nThe X subsystem began in a narrow region of state space - the single state 1, in fact - and Y began distributed over a wider region of space, states 1-4.  By interacting with each other, Y went into a narrow region, and X ended up in a wide region; _but the total phase space volume was conserved._  4 initial states mapped to 4 end states.\n\nClearly, so long as total phase space volume is conserved by physics over time, you can't squeeze Y harder than X expands, or vice versa - for every subsystem you squeeze into a narrower region of state space, some other subsystem has to expand into a wider region of state space.\n\nNow let's say that we're _uncertain_ about the joint system (X,Y), and our _uncertainty_ is described by an equiprobable distribution over S.  That is, we're pretty sure X is in state 1, but Y is equally likely to be in any of states 1-4.  If we shut our eyes for a minute and then open them again, we will expect to see Y in state 1, but X might be in any of states 2-8.  Actually, X can only be in _some_ of states 2-8, but it would be too costly to think out exactly which states these might be, so we'll just say 2-8.\n\nIf you consider the [Shannon entropy](/lw/o1/entropy_and_short_codes/) of our uncertainty about X and Y as individual systems, X began with 0 bits of entropy because it had a single definite state, and Y began with 2 bits of entropy because it was equally likely to be in any of 4 possible states.  (There's no [mutual information](/lw/o2/mutual_information_and_density_in_thingspace/) between X and Y.)  A bit of physics occurred, and lo, the entropy of Y went to 0, but the entropy of X went to log~2~(7) = 2.8 bits.  So entropy was transferred from one system to another, and decreased _within_ the Y subsystem; but due to the cost of bookkeeping, we didn't bother to track some information, and hence (from our perspective) the overall entropy increased.\n\nIf there was a physical process that mapped past states onto future states like this:\n\n> X2,Y1 -> X2,Y1  \n> X2,Y2 -> X2,Y1  \n> X2,Y3 -> X2,Y1  \n> X2,Y4 -> X2,Y1\n\nThen you could have a physical process that would actually _decrease entropy,_ because no matter where you started out, you would end up at the same place.  The laws of physics, developing over time, would compress the phase space.\n\nBut there is a theorem, Liouville's Theorem, which can be proven true of our laws of physics, which says that this never happens: [phase space is conserved](http://www.av8n.com/physics/phase-space-thin-lens.htm).\n\nThe Second Law of Thermodynamics is a corollary of Liouville's Theorem: no matter how clever your configuration of wheels and gears, you'll never be able to decrease entropy in one subsystem without increasing it somewhere else.  When the phase space of one subsystem narrows, the phase space of another subsystem must widen, and the joint space keeps the same volume.\n\nExcept that what was initially a _compact_ phase space, may develop squiggles and wiggles and convolutions; so that to draw a simple boundary around the whole mess, you must draw a much larger boundary than before - this is what gives the appearance of entropy increasing.  (And in quantum systems, where different universes go different ways, entropy actually does increase in any local universe.  But omit this complication for now.)\n\nThe Second Law of Thermodynamics is actually probabilistic in nature - if you ask about the probability of hot water spontaneously entering the \"cold water and electricity\" state, the probability does exist, it's just very small.  This doesn't mean Liouville's Theorem is violated with small probability; a theorem's a theorem, after all.  It means that if you're in a great big phase space volume at the start, but you _don't know where,_ you may assess a tiny little _probability_ of ending up in some particular phase space volume.  _So far as you know,_ with infinitesimal probability, this particular glass of hot water may be the kind that spontaneously transforms itself to electrical current and ice cubes.  (Neglecting, as usual, quantum effects.)\n\nSo the Second Law really _is_ inherently Bayesian.  When it comes to any real thermodynamic system, it's a strictly lawful statement of your _beliefs about_ the system, but only a probabilistic statement about the system itself.\n\n\"Hold on,\" you say.  \"That's not what I learned in physics class,\" you say.  \"In the lectures _I_ heard, thermodynamics is about, you know, _temperatures._  Uncertainty is a subjective state of mind!  The temperature of a glass of water is an objective property of the water!  What does heat have to do with probability?\"\n\nOh ye of little [trust](/lw/na/trust_in_bayes/).\n\nIn one direction, the connection between heat and probability is relatively straightforward:  If the only fact you know about a glass of water is its temperature, then you are much more uncertain about a hot glass of water than a cold glass of water.\n\nHeat is the zipping around of lots of tiny molecules; the hotter they are, the faster they can go.  Not all the molecules in hot water are travelling at the same speed - the \"temperature\" isn't a uniform speed of all the molecules, it's an average speed of the molecules, which in turn corresponds to a predictable statistical distribution of speeds - anyway, the point is that, the hotter the water, the faster the water molecules _could be_ going, and hence, the more uncertain you are about the velocity (not just speed) of any _individual_ molecule.  When you multiply together your uncertainties about all the individual molecules, you will be _exponentially_ more uncertain about the whole glass of water.\n\nWe take the logarithm of this exponential volume of uncertainty, and call that the entropy.  So it all works out, you see.\n\nThe connection in the other direction is less obvious.  Suppose there was a glass of water, about which, initially, you knew only that its temperature was 72 degrees.  Then, suddenly, Saint Laplace reveals to you the exact locations and velocities of all the atoms in the water.  You now know perfectly the state of the water, so, by the information-theoretic definition of entropy, its entropy is zero.  Does that make its _thermodynamic_ entropy zero?  Is the water colder, because we know more about it?\n\nIgnoring quantumness for the moment, the answer is:  Yes!  Yes it is!\n\nMaxwell once asked:  Why can't we take a uniformly hot gas, and partition it into two volumes A and B, and let only fast-moving molecules pass from B to A, while only slow-moving molecules are allowed to pass from A to B?  If you could build a gate like this, soon you would have hot gas on the A side, and cold gas on the B side.  That would be a cheap way to refrigerate food, right?\n\nThe agent who inspects each gas molecule, and decides whether to let it through, is known as \"Maxwell's Demon\".  And the reason you can't build an efficient refrigerator this way, is that Maxwell's Demon generates entropy in the process of inspecting the gas molecules and deciding which ones to let through.\n\nBut suppose you already _knew_ where all the gas molecules were?\n\nThen you actually _could_ run Maxwell's Demon and extract useful work.\n\nSo (again ignoring quantum effects for the moment), if you _know_ the states of all the molecules in a glass of hot water, it is cold in a genuinely thermodynamic sense: you can take electricity out of it and leave behind an ice cube.\n\nThis doesn't violate Liouville's Theorem, because if Y is the water, and _you_ are Maxwell's Demon (denoted M), the physical process behaves as:\n\n> M1,Y1 -> M1,Y1  \n> M2,Y2 -> M2,Y1  \n> M3,Y3 -> M3,Y1  \n> M4,Y4 -> M4,Y1\n\nBecause Maxwell's demon _knows_ the exact state of Y, this is mutual information between M and Y.  The mutual information decreases the joint entropy of (M,Y):  H(M,Y) = H(M) + H(Y) - I(M;Y).  M has 2 bits of entropy, Y has two bits of entropy, and their mutual information is 2 bits, so (M,Y) has a total of 2 + 2 - 2 = 2 bits of entropy.  The physical process just transforms the \"coldness\" (negentropy) of the mutual information to make the actual water cold - afterward, M has 2 bits of entropy, Y has 0 bits of entropy, and the mutual information is 0.  Nothing wrong with that!\n\nAnd don't tell me that knowledge is \"subjective\".  Knowledge has to be represented in a brain, and that makes it as physical as anything else.  For M to physically represent an accurate picture of the state of Y, M's physical state must correlate with the state of Y.  You can take thermodynamic advantage of that - it's called a Szilard engine.\n\nOr as E.T. Jaynes put it, \"The old adage 'knowledge is power' is a very cogent truth, both in human relations and in thermodynamics.\"\n\nAnd conversely, _one subsystem cannot increase in mutual information with another subsystem, without (a) interacting with it and (b) doing thermodynamic work._  \n\nOtherwise you could build a Maxwell's Demon and violate the Second Law of Thermodynamics - which in turn would violate Liouville's Theorem - which is prohibited in the standard model of physics.\n\nWhich is to say:  **To form accurate beliefs about something, you _really do_ have to observe it.**  It's a very physical, very real process: any rational mind does \"work\" in the thermodynamic sense, not just the sense of mental effort.\n\n(It is sometimes said that it is erasing bits in order to prepare for the next observation that takes the thermodynamic work - but that distinction is just a matter of words and perspective; the math is unambiguous.)\n\n(Discovering logical \"truths\" is a complication which I will not, for now, consider - at least in part because I am still thinking through the exact formalism myself.  In thermodynamics, knowledge of logical truths does not count as negentropy; as would be expected, since a reversible computer can compute logical truths at arbitrarily low cost.  All this that I have said is true of the logically omniscient: any lesser mind will necessarily be less efficient.)  \n\n\"Forming accurate beliefs requires a corresponding amount of evidence\" is a very cogent truth both in human relations and in thermodynamics: if blind faith actually worked as a method of investigation, you could turn warm water into electricity and ice cubes.  Just build a Maxwell's Demon that has blind faith in molecule velocities.\n\nEngines of cognition are not so different from heat engines, though they manipulate entropy in a more subtle form than burning gasoline.  For example, to the extent that an engine of cognition is not perfectly efficient, it must radiate waste heat, just like a car engine or refrigerator.\n\n\"Cold rationality\" is true in a sense that Hollywood scriptwriters never dreamed (and false in the sense that they did dream).\n\nSo unless you can tell me which _specific step_ in your argument violates the laws of physics by giving you true knowledge of the unseen, don't expect me to believe that a big, elaborate clever argument can do it either."
    },
    "voteCount": 94,
    "forceInclude": true
  },
  {
    "_id": "CPP2uLcaywEokFKQG",
    "url": null,
    "title": "Toolbox-thinking and Law-thinking",
    "slug": "toolbox-thinking-and-law-thinking",
    "author": null,
    "question": false,
    "tags": [
      {
        "name": "Rationality"
      },
      {
        "name": "Law-Thinking"
      },
      {
        "name": "Distinctions"
      }
    ],
    "tableOfContents": null,
    "contents": {
      "markdown": "Tl;dr:\n\nI've noticed a dichotomy between \"thinking in toolboxes\" and \"thinking in laws\".\n\nThe toolbox style of thinking says it's important to have a big bag of tools that you can adapt to context and circumstance; people who think very toolboxly tend to suspect that anyone who goes talking of a single optimal way is just ignorant of the uses of the other tools.\n\nThe lawful style of thinking, done correctly, distinguishes between descriptive truths, normative ideals, and prescriptive ideals. It may talk about certain paths being optimal, even if there's no executable-in-practice algorithm that yields the optimal path. It considers truths that are not tools.\n\nWithin nearly-Euclidean mazes, the triangle inequality - that the path AC is never spatially longer than the path ABC - is always true but only sometimes useful. The triangle inequality has the prescriptive implication that _if_ you _know_ that one path choice will travel ABC and one path will travel AC, and _if_ the _only_ pragmatic path-merit you care about is going the minimum spatial distance (rather than say avoiding stairs because somebody in the party is in a wheelchair), then you should pick the route AC. But the triangle inequality goes on governing Euclidean mazes whether or not you know which path is which, and whether or not you need to avoid stairs.\n\nToolbox thinkers may be extremely suspicious of this claim of universal lawfulness if it is explained less than perfectly, because it sounds to them like \"Throw away all the other tools in your toolbox! All you need to know is Euclidean geometry, and you can always find the shortest path through any maze, which in turn is always the best path.\"\n\nIf you think that's an unrealistic depiction of a misunderstanding that would never happen in reality, keep reading.\n\n* * *\n\nHere's a recent conversation from Twitter which I'd consider a nearly perfect illustration of the toolbox-vs.-laws dichotomy:\n\n> [David Chapman](https://twitter.com/Meaningness/status/993602725316186112): \"By _rationalism,_ I mean any claim that there is an ultimate criterion according to which thinking and acting could be judged to be correct or optimal... Under this definition, 'rationalism' must go beyond 'systematic methods are often useful, hooray!'... A rationalism claims there is _one weird trick_ to correct thinking, which guarantees an optimal result. (Some rationalisms specify the trick; others insist there must be one, but that it is not currently knowable.) A rationalism makes strongly normative judgments: everyone _ought_ to think that way.\"\n\n> [Graham Rowe](https://twitter.com/grahamsrowe/status/993781462263574528): \"Is it fair to say that rationalists see the world entirely through rationality while meta-rationalists look at rationality as one of many tools (that they can use fluently and appropriately) to be used in service of a broader purpose?\"\n\n> [David Chapman](https://twitter.com/Meaningness/status/993879794180747264): \"More-or-less, I think! Although I don’t think rationalists _do_ see the world entirely through rationality, they just say they think they ought to.\"\n\n> [Julia Galef](https://twitter.com/juliagalef/status/993621055221608449): \"I don't think the 'one weird trick' description is accurate. It's more like: there's one correct normative model in theory, which cannot possibly be approximated by a single rule in practice, but we can look for collections of 'tricks' that seem like they bring us closer to the normative model. e.g., 'On the margin, taking more small risks is likely to increase your EV' is one example.\"\n\n> [David Chapman](https://twitter.com/Meaningness/status/993626127842213889): \"The element that I’d call clearly meta-rational is understanding that rationality is not one well-defined thing but a bag of tricks that are more-or-less applicable in different situations.\"\n\nJulia then [quoted](https://twitter.com/juliagalef/status/993715985768177665) a paper mentioning \"The best prescription for human reasoning is not necessarily to always use the normative model to govern one's thinking.\" To which Chapman [replied](https://twitter.com/Meaningness/status/993718942827921408):\n\n> \"Baron’s distinction between 'normative' and 'prescriptive' is one I haven’t seen before. That seems useful and maybe key. OTOH, if we’re looking for a disagreement crux, it might be whether a normative theory that can’t be achieved, even in principle, is a good thing.\"\n\nI'm now going to badly stereotype this conversation in the form I feel like I've seen it many times previously, including e.g. in the discussion of p-values and frequentist statistics. On this stereotypical depiction, there is a dichotomy between the thinking of Msr. Toolbox and Msr. Lawful that goes like this:\n\nMsr. Toolbox: \"It's important to know how to use a broad variety of statistical tools and adapt them to context. The many ways of calculating p-values form one broad family of tools; any particular tool in the set has good uses and bad uses, depending on context and what exactly you do. [Using likelihood ratios](https://arbital.com/p/likelihoods_not_pvalues/?l=4xx) is an interesting statistical technique, and I'm sure it has its good uses in the right contexts. But it would be very surprising if that one weird trick was the best calculation to do in every paper and every circumstance. If you claim it is the universal best way, then I suspect you of blind idealism, insensitivity to context and nuance, ignorance of all the other tools in the toolbox, the sheer folly of callow youth. You only have a hammer and no real-world experience using screwdrivers, so you claim everything is a nail.\"\n\nMsr. Lawful: \"On complex problems we may not be able to compute exact [Bayesian updates](https://arbital.com/p/bayes_rule_guide/), but the math still describes the _optimal_ update, in the same way that a Carnot cycle describes a thermodynamically ideal engine even if you can't build one. You are unlikely to find a superior viewpoint that makes some other update even more optimal than the Bayesian update, not without doing a great deal of fundamental math research and maybe not at all. We didn't choose that formalism arbitrarily! We have a very broad variety of [coherence theorems](https://arbital.com/p/expected_utility_formalism/?l=7hh) all spotlighting the same central structure of probability theory, saying variations of 'If your behavior cannot be viewed as coherent with probability theory in sense X, you must be executing a dominated strategy and shooting off your foot in sense Y'.\"\n\nI currently suspect that when Msr. Law talks like this, Msr. Toolbox hears \"I prescribe to you the following recipe for your behavior, the Bayesian Update, which you ought to execute in every kind of circumstance.\"\n\nThis also appears to me to frequently turn into one of those awful durable forms of misunderstanding: Msr. Toolbox doesn't see what you could possibly be telling somebody to do with a \"good\" or \"ideal\" algorithm besides executing that algorithm.\n\nIt would not surprise me if there's a symmetrical form of durable misunderstanding where a Lawist has trouble processing a Toolboxer's disclaimer: \"No, you don't understand, I am not trying to describe the one true perfect optimal algorithm here, I'm trying to describe a context-sensitive tool that is sometimes useful in real life.\" Msr. Law may not see what you could possibly be doing with a supposedly \"prudent\" or \"actionable\" recipe besides saying that it's the correct answer, and may feel very suspicious of somebody trying to say everyone should use an answer while disclaiming that they don't really think it's true. Surely this is just the setup for some absurd motte-and-bailey where we claim something is the normative answer, and then as soon as we're challenged we walk back and claim it was 'just one tool in the toolbox'.\n\nAnd it's not like those callow youths the Toolboxer is trying to lecture don't actually exist. The world is full of people who think they have the One True Recipe (_without_ having a normative ideal by which to prove that this is indeed the optimal recipe given their preferences, knowledge, and available computing power).\n\nThe only way I see to resolve this confusion is by grasping a certain particular abstraction and distinction - as a more Lawfully inclined person might put it. Or by being able to deploy both kinds of thinking, depending on context - as a more Toolbox-inclined person might put it.\n\nIt may be that none of my readers need the lecture at this point, but I've learned to be cautious about that sort of thing, so I'll walk through the difference anyways.\n\n* * *\n\nEvery traversable maze has a spatially shortest path; or if we are to be precise in our claims but not our measurements, a set of spatially shortest-ish paths that are all nearly the same distance.\n\nWe may perhaps call this spatially shortest path the \"best\" or \"ideal\" or \"optimal\" path through the maze, _if_ we think our preference for walking shorter distances is the _only_ pragmatically important merit of a path.\n\nThat there exists some shortest path, which may even be optimal according to our preferences, doesn't mean that you can come to an intersection at the maze and \"just choose whichever branch is on the shortest path\".\n\nAnd the fact that you cannot, at an intersection, just choose the shorter path, doesn't mean that the concepts of distance and greater or lesser distance aren't useful.\n\nIt might even be that the maze-owner could truthfully tell you, \"By the way, this right-hand turn here keeps you on the shortest path,\" and yet you'd still be wiser to take the left-hand turn... because you're following the left-hand rule. Where the left-hand rule is to keep your left hand on the wall and go on walking, which works for not getting lost inside a maze whose exit is connected to the start by walls. It's a good rule for agents with sharply bounded memories who can't always remember their paths exactly.\n\nAnd if you're using the left-hand rule it is a terrible, terrible idea to jump walls and make a different turn just once, even if that looks like a great idea at the time, because that is an excellent way to get stuck traversing a disconnected island of connected walls inside the labyrinth.\n\nSo making the left-hand turn leads you to walk the shortest expected distance, relative to the other rules you're using. Making the right-hand turn instead, even if it seemed locally smart, might have you traversing an infinite distance instead.\n\nBut then you may not be on the shortest path, even though you are following the recommendations of the wisest and most prudent rule given your current resources. By contemplating the difference, you know that there is in principle room for improvement. Maybe that inspires you to write a maze-mapping, step-counting cellphone app that lets you get to the exit faster than the left-hand rule.\n\nAnd the reason that there's a better recipe isn't that \"no recipe is perfect\", it isn't that there exists an infinite sequence of ever-better roads. If the maze-owner gave you a map with the shortest path drawn in a line, you could walk the true shortest path and there wouldn't be any shorter path than that.\n\nShortness is a property of paths; a tendency to produce shorter paths is a property of recipes. What makes a phone app an improvement is not that the app is adhering more neatly to some ideal sequence of left and right turns, it's that the path is shorter in a way that can be [defined independently of the app's algorithms](https://arbital.com/p/fair_problem_class/).\n\nOnce you can admit a path can be \"shorter\" in a way that abstracts away from the walker - not _better,_ which does depend on the walker, but _shorter_ \\- it's hard not to admit the notion of there being a shortest path.\n\nI mean, I suppose you could try very hard to never talk about a shortest path and only talk about alternative recipes that yield _shorter_ paths. You could diligently make sure to _never_ imagine this shorterness as a kind of decreased distance-in-performance-space from any 'shortest path'. You could make very sure that in your consideration of new recipes, you maintain your ideological purity as a toolboxer by only ever asking about laws that govern which of two paths are shorter, and never getting any inspiration from any kind of law that governs which path is shortest.\n\nIn which case you would have diligently eliminated a valuable conceptual tool from your toolbox. You would have carefully made sure that you always had to take longer roads to those mental destinations that can be reached the fastest by contemplating properties of ideal solutions, or distance from ideal solutions.\n\nBut why? Why would you?\n\n* * *\n\nI think at this point the Toolbox reply - though I'm not sure I could pass its Ideological Turing Test - might be that idealistic thinking has a great trap and rottenness at its heart.\n\nIt might say:\n\nSomebody who doesn't wisely shut down all this thinking about \"shortest paths\" instead of the left-hand rule as a good tool for some mazes - someone who begins to imagine some unreachable ideal of perfection, instead of a series of apps that find shorter paths most of the time - will surely, in practice, begin to confuse the notion of the left-hand rule, or their other current recipe, with _the shortest path._\n\nAfter all, nobody can see this \"shortest path\", and it's supposedly a virtuous thing. So isn't it an inevitable consequence of human nature that people will start to use that idea as praise for their current recipes?\n\nAnd also in the real world, surely Msr. Law will inevitably forget the extra premise involved with the step from \"spatially shortest path\" to \"best path\"- the contextual requirement that our only important preference was shorter spatial distances so defined. Msr. Law will insist that somebody in a wheelchair go down the \"best path\" of the maze, even though that path involves going up and down a flight of stairs.\n\nAnd Msr. Law will be unable to mentally deal with a helicopter overflying the maze that violates their ontology relative to which \"the shortest path\" was defined.\n\nAnd it will also never occur to Msr. Law to pedal around the maze in a bicycle, which is a much easier trip even if it's not the shortest spatial distance.\n\nAnd Msr. Law will assume that the behavior of mortgage-backed securities is independently Gaussian-random because the math is neater that way, and then derive a definite theorem showing a top-level tranche of MBSs will almost never default, thus bringing down their trading firm -\n\nTo all of which I can only reply: \"Well, yes, that happens some of the time, and there are contextual occasions where it is a useful tool to lecture Msr. Law on the importance of having a diverse toolbox. But it is not a _universal_ truth that _everyone_ works like that and needs to be prescribed the same lecture! You need to be sensitive to context here!\"\n\nThere are definitely versions of Msr. Law who think the universal generalization they've been told about is a One Weird Trick That Is All You Need To Know; people who could in fact benefit from a lecture on the importance of diverse toolboxes.\n\nThere are also extreme toolbox thinkers could benefit from a lecture on the importance of thinking that considers unreachable ideals, and how to get closer to them, and the obstacles that are moving us away from them.\n\nNot to commit the [fallacy of the golden mean](https://en.wikipedia.org/wiki/Argument_to_moderation) or anything, but the two viewpoints are both metatools in the metatoolbox, as it were. You're better off if you can use both in ways that depend on context and circumstance, rather than insisting that _only_ toolbox reasoning is the _universally best context-insensitive_ metaway to think.\n\nIf that's not putting the point too sharply.\n\nThinking in terms of Law is often useful. You just have to be careful to understand the context and the caveats: when is the right time to think in Law, how to think in Law, and what type of problems call for Lawful thinking.\n\nWhich is _not_ the same as saying that every Law has exceptions. Thermodynamics still holds even at times, like playing tennis, when it's not a good time to be thinking about thermodynamics. If you thought that every Law had exceptions because it wasn't always useful to think about that Law, you'd be rejecting the metatool of Law entirely, and thinking in toolbox terms at a time when it wasn't useful to do so.\n\nAre there Laws of optimal thought governing the optimal way to contextualize and caveat, which might be helpful for finding good executable recipes? The naturally Lawful thinker will immediately suspect so, even if they don't know what those Laws are. Not knowing these Laws won't panic a healthy Lawful thinker. Instead they'll proceed to look around for useful yet chaotic-seeming prescriptions to use now instead of later - _without_ mistaking those chaotic prescriptions for Laws, _or_ treating the chaos of their current recipes as proof that there's no good normative ideals to be had.\n\nIndeed, it can sometimes be useful to contemplate, in detail, that there are probably Laws you don't know. But that's a more advanced metatool in the metatoolbox, useful in narrower ways and in fewer contexts having to do with the invention of new Laws as well as new recipes, and I'd rather not strain Msr. Toolbox's credulity any further.\n\n* * *\n\nTo close out, one recipe I'd prescribe to reduce confusion in the toolbox-inclined is to try to see the Laws as descriptive statements, rather than being any kind of normative ideal at all.\n\nThe idea that there's a shortest path through the maze isn't a \"normative ideal\" instead of a \"prescriptive ideal\", it's just true. Once you define distance there is in fact a shortest path through the maze.\n\nThe triangle inequality might sound very close to a prescriptive rule that you ought to walk along AC instead of ABC. But actually the prescriptive rule is only if you _want_ to walk shorter distances ceteris paribus, only if you know which turn is which, only if you're not trying to avoid stairs, and only if you're not taking an even faster route by getting on a bicycle and riding outside the whole maze to the exit. The prescriptive rule \"try walking along AC\" isn't the same as the triangle inequality itself, which goes on being _true_ of spatial distances in Euclidean or nearly-Euclidean geometries - whether or not you know, whether or not you care, whether or not it's useful to think about at any given moment, even if you own a bicycle.\n\nThe statement that you can't have a heat-pressure engine more efficient than a Carnot cycle isn't about gathering in a cultish circle to sing praises of the Carnot cycle as being the ideally best possible kind of engine. It's just a true fact of thermodynamics. This true fact might helpfully suggest that you think about obstacles to Carnot-ness as possible places to improve your engine - say, that you should try to prevent heat loss from the combustion chamber, since heat loss prevents an adiabatic cycle. But even at times when it's not in fact useful to think about Carnot cycles, it doesn't mean your heat engine is allowed on those occasions to perform better than a Carnot engine.\n\nYou can't extract any more evidence from an observation than is given by its likelihood ratio. You could see this as being true because Bayesian updating is an often-unreachable normative ideal of reasoning, so therefore nobody can do better than it. But I'd call it a deeper level of understanding to see it as a law saying that you can't get a higher expected score by making any different update. This is a generalization that holds over both Bayes-inspired recipes and non-Bayes-inspired recipes. If you _want_ to assign higher probability to the correct hypothesis, it's a short step from that preference to regarding Bayesian updates as a normative ideal; but the idea begins life as a descriptive assertion, not as a normative assertion.\n\nIt's a relatively shallow understanding of the [coherence theorems](https://arbital.com/p/expected_utility_formalism/?l=7hh) to say \"Well, they show that if you don't use probabilities and expected utilities you'll be incoherent, which is bad, so you shouldn't do that.\" It's a deeper understanding to state, \"If you do something that is incoherent in way X, it will correspond to a dominated strategy in fashion Y. This is a universal generalization that is true about every tool in the statistical toolbox, whether or not they are in fact coherent, whether or not you personally prefer to avoid dominated strategies, whether or not you have the computing power to do any better, even if you own a bicycle.\"\n\nI suppose that when it comes to the likes of [Fun Theory](http://lesswrong.com/lw/xy/the_fun_theory_sequence/), there isn't any deeper fact of nature underlying the \"normative ideal\" of a eudaimonic universe. But in simpler matters of math and science, a \"normative ideal\" like the Carnot cycle or Bayesian decision theory is almost always the manifestation of some simpler fact that is _so closely related to something we want_ that we are tempted to take one step to the right and view it as a \"normative ideal\". If you're allergic to normative ideals, maybe a helpful course would be to discard the view of whatever-it-is as a normative ideal and try to understand it as a fact.\n\nBut that is a more advanced state of understanding than trying to understand what is better or best. If you're not allergic to ideals, then it's okay to try to understand why Bayesian updates are often-unreachable normative ideals, before you try to understand how they're just there."
    },
    "voteCount": 84,
    "forceInclude": true
  },
  {
    "_id": "WQFioaudEH8R7fyhm",
    "url": null,
    "title": "Local Validity as a Key to Sanity and Civilization",
    "slug": "local-validity-as-a-key-to-sanity-and-civilization",
    "author": "Eliezer_Yudkowsky",
    "question": false,
    "tags": [
      {
        "name": "Rationality"
      },
      {
        "name": "Common Knowledge"
      },
      {
        "name": "Public Discourse"
      },
      {
        "name": "Epistemology"
      },
      {
        "name": "Gears-Level"
      }
    ],
    "tableOfContents": {
      "sections": [
        {
          "title": "0.",
          "anchor": "0_",
          "level": 1
        },
        {
          "title": "i.",
          "anchor": "i_",
          "level": 1
        },
        {
          "title": "ii.",
          "anchor": "ii_",
          "level": 1
        },
        {
          "title": "iii.",
          "anchor": "iii_",
          "level": 1
        },
        {
          "title": "iv.",
          "anchor": "iv_",
          "level": 1
        },
        {
          "title": "v.",
          "anchor": "v_",
          "level": 1
        },
        {
          "title": "vi.",
          "anchor": "vi_",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        },
        {
          "anchor": "comments",
          "level": 0,
          "title": "65 comments"
        }
      ],
      "headingsCount": 9
    },
    "contents": {
      "markdown": "(Cross-posted [from Facebook](https://www.facebook.com/yudkowsky/posts/10156117992914228).)\n\n0.\n--\n\nTl;dr: There's a similarity between these three concepts:\n\n*   A locally valid proof step in mathematics is one that, in general, produces only true statements from true statements. This is a property of a single step, irrespective of whether the final conclusion is true or false.\n*   There's such a thing as a bad argument even for a good conclusion. In order to arrive at sane answers to questions of fact and policy, we need to be curious about whether arguments are good or bad, independently of their conclusions. The rules against fallacies must be enforced even against arguments for conclusions we like.\n*   For civilization to hold together, we need to make coordinated steps away from Nash equilibria in lockstep. This requires general rules that are allowed to impose penalties on people we like or reward people we don't like. When people stop believing the general rules are being evaluated sufficiently fairly, they go back to the Nash equilibrium and civilization falls.\n\ni.\n--\n\nThe notion of a locally evaluated argument step is simplest in mathematics, where it is a formalizable idea [in model theory](https://www.lesswrong.com/posts/Z2CuyKtkCmWGQtAEh/proofs-implications-and-models). In math, a general type of step is 'valid' if it only produces semantically true statements from other semantically true statements, relative to a given model. If _x_ = _y_ in some set of variable assignments, then 2_x_ = 2_y_ in the same model. Maybe _x_ doesn't equal _y_, in some model, but even if it doesn't, the local step from \"_x_ = _y_\" to \"2_x_ = 2_y_\" is a locally valid step of argument. It won't introduce any _new_ problems.\n\nConversely, _xy_ = _xz_ does not imply _y_ = _z_. It happens to work when _x_ = 2, _y_ = 3, and _z_= 3, in which case the two statements say \"6 = 6\" and \"3 = 3\" respectively. But if _x_ = 0, _y_ = 4, _z_ = 17, then we have \"0 = 0\" on one side and \"4 = 17\" on the other. We can feed in a true statement and get a false statement out the other end. This argument is not _locally_ okay.\n\nYou can't get the concept of a \"mathematical proof\" unless on some level—though often an intuitive level rather than an explicit one—you understand the notion of a single step of argument that is locally okay or locally not okay, independent of whether you globally agreed with the final conclusion. There's a kind of approval you give to the pieces of the argument, rather than looking the whole thing over and deciding whether you like what came out the other end.\n\nOnce you've grasped that, it may even be possible to convince you of mathematical results that sound counterintuitive. When your understanding of the rules governing allowable argument steps has become stronger than your faith in your ability to judge whole intuitive conclusions, you may be convinced of truths you would not otherwise have grasped.\n\nii.\n---\n\nMore generally in life, even outside of mathematics, there are such things as bad arguments for good conclusions.\n\nThere are even such things as genuinely good arguments for false conclusions, though of course those are much rarer. By the Bayesian definition of evidence, \"strong evidence\" is exactly that kind of evidence which we very rarely expect to find supporting a false conclusion. Lord Kelvin's careful and multiply-supported lines of reasoning arguing that the Earth could not possibly be so much as a hundred million years old, all failed simultaneously in a surprising way because that era didn't know about nuclear reactions. But most of the time this does not happen.\n\nOn the other hand, bad arguments for true conclusions are extremely easy to come by, because there are tiny elves that whisper them to people. There isn't anything the least bit more difficult in making an argument terrible when it leads to a good conclusion, since the tiny elves own lawnmowers.\n\nOne of the marks of an intellectually strong mind is that they are able to take a curious interest in whether a particular argument is a good argument or a bad argument, independently of whether they agree with the conclusion of that argument.\n\nEven if they happen to start out believing that, say, the intelligence explosion thesis for Artificial General Intelligence is false, they are capable of frowning at the argument that the intelligence explosion is impossible because hypercomputation is impossible, or that there's really no such thing as intelligence [because of the no-free-lunch theorem](https://intelligence.org/2017/12/06/chollet/), and saying, \"Even if I agree with your conclusion, I think that's a terrible argument for it.\" Even if they agree with the mainstream scientific consensus on anthropogenic global warming, they still wince and perhaps even offer a correction when somebody offers as evidence favoring global warming that there was a really scorching day last summer.\n\nThere are weaker and stronger versions of this attribute. Some people will think to themselves, \"Well, it's important to use only valid arguments... but there was a sustained pattern of record highs worldwide over multiple years which does count as evidence, and that particular very hot day was a part of that pattern, so it's valid evidence for global warming.\" Other people will think to themselves, \"I'd roll my eyes at someone who offers a single very cold day as an argument that global warming is false. So it can't be okay to use a single very hot day to argue that global warming is true.\"\n\nI'd much rather buy a used car from the second person than the first person. I think I'd pay at least a 5% price premium.\n\nMetaphorically speaking, the first person will court-martial an allied argument if they must, but they will favor allied soldiers when they can. They still have a sense of motion toward the Right Final Answer as being progress, and motion away from the right final answer as anti-progress, and they dislike not making progress.\n\nThe second person has something more like the strict mindset of a mathematician when it comes to local validity. They are able to praise some proof steps as obeying the rules, irrespective of which side those steps are on, without a sense that they are thereby betraying their side.\n\niii.\n----\n\nThis essay has been bubbling in the back of my mind for a while, since I read that potential juror #70 for the Martin Shkreli trial was rejected during selection when, asked if they thought they could render impartial judgment, they replied, \"I can be fair to one side but not the other.\" And I thought maybe I should write something about why that was possibly a harbinger of the collapse of civilization. I've been musing recently about how a lot of the standard Code of the Light isn't really written down anywhere anyone can find.\n\nThe thought recurred during the recent #MeToo saga when some Democrats were debating whether it made sense to kick Al Franken out of the Senate. I don't want to derail into debating Franken's behavior and whether that degree of censure was warranted per se, and I'll delete any such comments. What brought on this essay was that I read some unusually frank concerns from people who did think that Franken's behavior was per se cause to not represent the Democratic Party in the Senate; but who worried that the Democrats would police themselves, the Republicans wouldn't, and so the Republicans would end up controlling the Senate.\n\nI've heard less of that since some upstanding Republican voters in Alabama stayed home on election night and put Doug Jones in the Senate.\n\nBut at the time, some people were replying, \"That seems horrifyingly cynical and realpolitik. Is the idea here that sexual line-crossing is only bad and worthy of punishment when Republicans do it? Are we deciding that explicitly now?\" And others were saying, \"Look, the end result of your way of doing things is to just hand over the Senate to the Republican Party.\"\n\nThis is a conceptual knot that, I'm guessing, results from not explicitly distinguishing game theory from goodness.\n\nThere is, I think, a certain intuitive idea that _ideally_ the Law is supposed to embody a subset of morality insofar as it is ever wise to enforce certain kinds of goodness. Murder is bad, and so there's a law against this bad behavior of murder. There's a lot of places where the law is in fact evil, like the laws criminalizing marijuana; that means the law is departing from its purpose, falling short of what it should be. Those who are not real-life straw authoritarians (who are sadly common) will cheerfully agree that there are some forms of goodness, even most forms of goodness, that it is not wise to try to legislate. But insofar as it _is_ ever wise to make law, there's an intuitive sense that law should reflect some particular subset of morally good behavior that we have decided it is wise to enforce with guns, such as \"Don't kill people.\"\n\nIt's from this perspective that \"As a matter of pragmatic realpolitik we are going to not enforce sexual line-crossing rules against Democratic senators\" seems like giving up, and maybe a harbinger of the fall of civilization if things have really gotten that bad.\n\nBut there's more than one function of legal codes, the way that money is both a store of value and a medium of exchange but these are different functions of money.\n\nYou can also look at laws as a kind of game theory played with people who might not share your morality at all. Some people take this perspective almost exclusively, at least in their verbal reports. They'll say, \"Well, yes, I'd like it if I could walk into your house and take all your stuff, but I would dislike it even more if you could walk into my house and take _my_ stuff, and that's why we have laws.\" I'm never quite sure how seriously to take the claim that they'd be happy walking into my house and taking my stuff. It seems to me that law enforcement and even social enforcement are simply not effective enough to count for the vast majority of human cooperation, and I have a sense that civilization is free-riding a whole lot on innate altruism... but game theory is certainly _a_ function served by law.\n\nThe same way that money is both medium of exchange and store of value, the law is both collective utility function fragment and game theory.\n\nIn its function as game theory, the law (ideally) enables people with different utility functions to move from bad Nash equilibria to better Nash equilibria, closer to the Pareto frontier. Instead of mutual defection getting a payoff of (2, 2), both sides pay 0.1 for law enforcement and move to enforced mutual cooperation at (2.9, 2.9).\n\nFrom this perspective, everything rests on notions like \"fairness\", \"impartiality\", \"equality before the law\", \"it doesn't matter whose ox is being gored\". If the so-called law punishes your defection but lets the other's defection pass, and this happens systematically enough and often enough, it is in your interest to blow up the current equilibrium if you have a chance.\n\nIt is coherent to say, \"Crossing this behavioral line is universally bad when anyone does it, and also we're not going to punish Democratic senators unless you also punish Republican senators.\" Though as the saga of Senator Doug Jones of Alabama also shows, you should be careful about preemptively assuming the other side won't cooperate; there are sad lost opportunities there.\n\niv.\n---\n\nThe way humans do law, it depends on the existence of what _feel like_ simple general rules that apply to all cases.\n\nThis is not a universal truth of decision theory, it's a consequence of our cognitive limitations. Two superintelligences could negotiate a compromise with complicated detailed boundaries going right up to the Pareto frontier. They could agree on mutually verified pieces of cognitive code designed to intelligently decide future events according to known principles.\n\nHumans use simpler laws than that.\n\nTo be clear, the kind of \"law\" I'm talking about here is not to be confused with the enormous modern morass of unreadable regulations. Think of, say, the written laws that actually got enforced in a small town in California in 1820. Or Democrats debating whether to enforce a sanction against Democratic senators if it's not being enforced against Republican senators. Or a small community's elders' star-chamber meeting to debate an accusation of sexual assault. Or the laws that cops will enforce even against other cops. These are the kinds of laws that must be simple in order to exist.\n\nThe reason that hunter-gatherer tribes don't have 100,000 pages of written legalism... is _not_ that they've wisely realized that lengthy rules are easier to fill with loopholes, and that complicated regulations favor large corporations with legal departments, and that laws often have unintended consequences which don't resemble their stated justifications, and that deadweight losses increase quadratically. It's _very clear_ that a supermajority of human beings are not that wise. Rather, hunter-gatherers just don't have enough time, energy, and paper to screw up that badly.\n\nWhen humans try to verbalize The Law that isn't to be confused with written law, the law that cops will enforce against other cops, it comes out in universally quantified short sentences like \"Anyone who defects in the Prisoner's Dilemma will be penalized TEN points even if that costs us fifteen\" or \"If you kill somebody who wasn't attacking you first, we'll exile you.\"\n\nAt one point somebody had the bright idea of trying to write down The Law. That way everyone could have common knowledge of what The Law was; and if you didn't break what was written, you could know you were safe from at least the official sanctions. Robert Heinlein called it the most important moment in political history, declaring that the law was above the politicians.\n\nI for one rather doubt the Code of Hammurabi was universally enforced. I expect that hunter-gatherer tribes long before writing had a sense of there being Laws that were above the decisions of individual elders. I suspect that even in the best of times most of the The Law was never written down, and that more than half of what was written down was never really The Law.\n\nBut unfortunately, once somebody had the bright idea of writing down The Law, somebody else had the bright idea of writing down more words on the same clay tablet.\n\nToday we live in a post-legalist era, when almost all of that which serves the true function of Law can no longer be written down. The government legalist system is too expensive in time and money and energy, too unreliable, and too slow, for any sane victim of sexual assault to appeal to the criminal justice system instead of the media justice system or the whispernet justice system. The civil legalist system outside of small claims court is a bludgeoning contest between entities that can afford lawyers, and the real law between corporations is enforced by merchant reputation and the threat of starting a bludgeoning contest. If you're in a lower-class neighborhood in the US, you can't get together and create order using your own town guards, because the police won't allow it. From your perspective, the function of the police is to prevent open gunfights and to not allow any more effective order than that to form.\n\nBut so it goes. We can't always keep the nice things we used to have, like written laws. The privilege was abused, and has been revoked.\n\nWhen remains of The Law must indeed be simple, because our written-law privileges have been revoked, and so The Law relies on everyone knowing The Law without it being written down. It isn't even recited in memorable verse, as once it was. The Law relies on the community agreeing on the application of The Law without there being professional judges or a precedent-based judiciary. If not universal agreement, it must at least seem that the choices of the elders are trying to appeal to The Law instead of just naked self-interest. To the extent a voluntary association can't agree on The Law in this sense, it will soon cease to be a voluntary association.\n\nThe Law also breaks down if people start believing that, when the simple rules say one thing, the deciders will instead look at whose ox got gored, evaluate their personal interest, and enforce a different conclusion instead.\n\nWhich is to say: human law ends up with what people at least _believe_ to be a set of simple rules that can be locally checked to test okay behavior. It's not actually algorithmically simple any more than walking is cheaply computable, but it feels simple the way that walking feels easy. Whatever doesn't feel like part of that small simple set won't be systematically enforced by the community, regardless of whether your civilization has reached the stage where police are seizing the cars of black people but not white people who use marijuana.\n\nv.\n--\n\nThe game-theoretic function of law can make following those simple rules feel like losing something, taking a step backward. You don't get to defect in the Prisoner's Dilemma, you don't get that delicious (5, 0) payoff instead of (3, 3). The law may punish one of your allies. You may be losing something according to your actual value function, which [feels like](https://www.lesswrong.com/posts/HFyWNBnDNEDsDNLrZ/the-true-prisoner-s-dilemma) the law having an objectively bad immoral result. You may coherently hold that the universe is a worse place for an instance of the enforcement of a good law, relative to its counterfactual state if that law could be lifted in just that instance without affecting any other instances. Though this does require seeing that law as having a game-theoretic function as well as a moral function.\n\nSo long as the rules are seen as moving from a bad global equilibrium to a global equilibrium seen as better, and so long as the rules are mostly-equally enforced on everyone, people are sometimes able to take a step backward and see that larger picture. Or, in a less abstract way, trade off the reified interest of The Law against their own desires and wishes.\n\nThis mental motion goes by names like \"justice\", \"fairness\", and \"impartiality\". It has ancient exemplars like a story I couldn't seem to Google, about a Chinese general who prohibited his troops from looting, and then his son appropriated a straw hat from a peasant; so the general sentenced his own son to death with tears running down his eyes.\n\nHere's a fragment of thought as it was before the Great Stagnation, as depicted in passing in H. Beam Piper's _Little Fuzzy_, one of the earliest books I read as a child. It's from 1962, when the [memetic collapse](https://www.facebook.com/yudkowsky/posts/10155616782514228) had started but not spread very far into science fiction. It stuck in my mind long ago and became one more tiny little piece of who I am now.\n\n> “Pendarvis is going to try the case himself,” Emmert said. “I always thought he was a reasonable man, but what’s he trying to do now? Cut the Company’s throat?”\n\n> “He isn’t anti-Company. He isn’t pro-Company either. He’s just pro-law. The law says that a planet with native sapient inhabitants is a Class-IV planet, and has to have a Class-IV colonial government. If Zarathustra is a Class-IV planet, he wants it established, and the proper laws applied. If it’s a Class-IV planet, the Zarathustra Company is illegally chartered. It’s his job to put a stop to illegality. Frederic Pendarvis’ religion is the law, and he is its priest. You never get anywhere by arguing religion with a priest.”\n\nThere is no suggestion in 1962 that the speakers are gullible, or that Pendarvis is a naif, or that Pendarvis is weird for thinking like this. Pendarvis isn't the defiant hero or even much of a side character. It's just a kind of judge you sometimes run into, part of a normal environment as projected from the author's mind that wrote the story.\n\nIf you don't have some people like Pendarvis, and you don't appreciate what they're trying to do even when they rule against you, sooner or later your tribe ends.\n\nI mean, I doubt the United States will literally fall into anarchy this way before the AGI timeline runs out. But the concept applies on a smaller scale than countries. It applies on a smaller scale than communities, to bargains between three people or two.\n\nThe notion that you can \"be fair to one side but not the other\", that what's called \"fairness\" is a kind of favor you do for people you like, says that even the _instinctive_ sense people had of law-as-game-theory is being lost in the modern memetic collapse. People are being exposed to so many social-media-viral depictions of the Other Side defecting, and viewpoints exclusively from Our Side without any leavening of any other viewpoint that might ask for a game-theoretic compromise, that they're losing the ability to appreciate the kind of anecdotes they used to tell in ancient China.\n\n(Or maybe it's hormonelike chemicals leached from plastic food containers. Let's not forget all the psychological explanations offered for a wave of violence that turned out to be lead poisoning.)\n\nvi.\n---\n\nAnd to take the point full circle:\n\nThe mental motion to evenhandedly apply The Rules irrespective of their conclusion is a kind of thinking that human beings appreciate intuitively, or at least they appreciated it in ancient China and mid-20th-century science fiction. In fact, we appreciate The Law more natively than we appreciate the notion of local syntactic rules capturing semantically valid steps in mathematical proofs, go figure.\n\nSo the legal metaphor is where a lot of people get started on epistemology: by seeing the local rules of valid argument as The Law, fallacies as crimes. The unusually healthy of mind will reject bad allied arguments with an emotional sense of practicing the way of an impartial judge.\n\nIt's ironic, in a way, because there is no game theory and no morality to the true way of the map that reflects the territory. A paperclip maximizer would also strive to debias its cognitive processes, alone in its sterile universe.\n\nBut I would venture a guess and hypothesis that you are better off buying a used car from a random mathematician than a random non-mathematician, even after controlling for IQ. The reasoning being that mathematicians are people whose sense of Law was strong enough to be appropriated for proofs, and that this will correlate, if imperfectly, with mathematicians abiding by what they see as The Law in other places as well. I could be wrong, and would be interested in seeing the results of any study like this if it were ever done. (But no studies on self-reports of criminal behavior, please. Unless there's some reason to believe that the self-report metric isn't measuring \"honesty times criminality\" rather than \"criminality\".)\n\nI have no grand agenda in having said all this. I've just sometimes thought of late that it would be nice if more of the extremely basic rules of thinking were written down."
    },
    "voteCount": 97,
    "forceInclude": true
  },
  {
    "title": "Belief in belief",
    "_id": "CqyJzDZWvGhhFJ7dY",
    "slug": "belief-in-belief",
    "forceInclude": true
  },
  {
    "title": "What do we mean by \"Rationality\"?",
    "_id": "RcZCwxFiZzE6X7nsv",
    "slug": "what-do-we-mean-by-rationality-1",
    "forceInclude": true
  },
  {
    "title": "What is evidence?",
    "_id": "6s3xABaXKPdFwA3FS",
    "slug": "what-is-evidence",
    "forceInclude": true
  },
  {
    "title": "How to convince me that 2+2=3",
    "_id": "6FmqiAgS8h4EJm86s",
    "slug": "how-to-convince-me-that-2-2-3",
    "forceInclude": true
  },
  {
    "title": "Occam's Razor",
    "_id": "f4txACqDWithRi7hs",
    "slug": "occam-s-razor",
    "forceInclude": true
  },
  {
    "title": "The lens that sees its flaws",
    "_id": "46qnWRSR7L2eyNbMA",
    "slug": "the-lens-that-sees-its-flaws",
    "forceInclude": true
  },
  {
    "title": "Making Beliefs Pay Rent (in Anticipated Experiences)",
    "_id": "a7n8GdKiAZRX86T5A",
    "slug": "making-beliefs-pay-rent-in-anticipated-experiences",
    "forceInclude": true
  },
  {
    "title": "The Virtue of Narrowness",
    "_id": "yDfxTj9TKYsYiWH5o",
    "slug": "the-virtue-of-narrowness",
    "forceInclude": true
  },
  {
    "title": "Hindsight Bias",
    "_id": "fkM9XsNvXdYH6PPAx",
    "slug": "hindsight-bias",
    "forceInclude": true
  },
  {
    "title": "Fake Explanations",
    "_id": "fysgqk4CjAwhBgNYT",
    "slug": "fake-explanations",
    "forceInclude": true
  },
  {
    "title": "Guessing the Teacher's Password",
    "_id": "NMoLJuDJEms7Ku9XS",
    "slug": "guessing-the-teacher-s-password",
    "forceInclude": true
  },
  {
    "title": "Fake Causality",
    "_id": "RgkqLqkg8vLhsYpfh",
    "slug": "fake-causality",
    "forceInclude": true
  },
  {
    "title": "Mysterious Answers to Mysterious Questions",
    "_id": "6i3zToomS86oj9bS6",
    "slug": "mysterious-answers-to-mysterious-questions",
    "forceInclude": true
  },
  {
    "title": "The Futility of Emergence",
    "_id": "8QzZKw9WHRxjR4948",
    "slug": "the-futility-of-emergence",
    "forceInclude": true
  },
  {
    "title": "Explain, Worship, Ignore",
    "_id": "yxvi9RitzZDpqn6Yh",
    "slug": "explain-worship-ignore",
    "forceInclude": true
  },
  {
    "title": "How an Algorithm Feels from Inside",
    "_id": "yA4gF5KrboK2m2Xu7",
    "slug": "how-an-algorithm-feels-from-inside",
    "forceInclude": true
  },
  {
    "title": "Feel the Meaning",
    "_id": "dMCFk2n2ur8n62hqB",
    "slug": "feel-the-meaning",
    "forceInclude": true
  },
  {
    "title": "Replace the Symbol with the Substance",
    "_id": "GKfPL6LQFgB49FEnv",
    "slug": "replace-the-symbol-with-the-substance",
    "forceInclude": true
  },
  {
    "title": "Dissolving the Question",
    "_id": "Mc6QcrsbH5NRXbCRX",
    "slug": "dissolving-the-question",
    "forceInclude": true
  },
  {
    "title": "Wrong Questions",
    "_id": "XzrqkhfwtiSDgKoAF",
    "slug": "wrong-questions",
    "forceInclude": true
  },
  {
    "title": "Righting a Wrong Question",
    "_id": "rQEwySCcLtdKHkrHp",
    "slug": "righting-a-wrong-question",
    "forceInclude": true
  },
  {
    "title": "Probability is in the Mind",
    "_id": "f6ZLxEWaankRZ2Crv",
    "slug": "probability-is-in-the-mind",
    "forceInclude": true
  },
  {
    "title": "Reductionism",
    "_id": "tPqQdLCuxanjhoaNs",
    "slug": "reductionism",
    "forceInclude": true
  },
  {
    "title": "Explaining vs. Explaining Away",
    "_id": "cphoF8naigLhRf3tu",
    "slug": "explaining-vs-explaining-away",
    "forceInclude": true
  },
  {
    "title": "Fake Reductionism",
    "_id": "mTf8MkpAigm3HP6x2",
    "slug": "fake-reductionism",
    "forceInclude": true
  },
  {
    "title": "Joy in the Merely Real",
    "_id": "x4dG4GhpZH2hgz59x",
    "slug": "joy-in-the-merely-real",
    "forceInclude": true
  },
  {
    "title": "Bind Yourself to Reality",
    "_id": "WjpA4PCjt5EkTGbLF",
    "slug": "bind-yourself-to-reality",
    "forceInclude": true
  },
  {
    "title": "If You Demand Magic, Magic Won't Help",
    "_id": "iiWiHgtQekWNnmE6Q",
    "slug": "if-you-demand-magic-magic-won-t-help",
    "forceInclude": true
  },
  {
    "title": "Mundane Magic",
    "_id": "SXK87NgEPszhWkvQm",
    "slug": "mundane-magic",
    "forceInclude": true
  },
  {
    "title": "The Beauty of Settled Science",
    "_id": "ndGYn7ZFiZyernp9f",
    "slug": "the-beauty-of-settled-science",
    "forceInclude": true
  },
  {
    "title": "To Spread Science, Keep It Secret",
    "_id": "3diLhMELXxM8rFHJj",
    "slug": "to-spread-science-keep-it-secret",
    "forceInclude": true
  },
  {
    "title": "Angry Atoms",
    "_id": "ddwk9veF8efn3Nzbu",
    "slug": "angry-atoms",
    "forceInclude": true
  },
  {
    "title": "Heat vs. Motion",
    "_id": "ne6Ra62FB9ACHGSuh",
    "slug": "heat-vs-motion",
    "forceInclude": true
  },
  {
    "title": "Brain Breakthrough! It's Made of Neurons!",
    "_id": "nzzNFcrSk7akQ9bwD",
    "slug": "brain-breakthrough-it-s-made-of-neurons",
    "forceInclude": true
  },
  {
    "title": "Reductive Reference",
    "_id": "gRa5cWWBsZqdFvmqu",
    "slug": "reductive-reference",
    "forceInclude": true
  },
  {
    "title": "Zombies! Zombies?",
    "_id": "fdEWWr8St59bXLbQr",
    "slug": "zombies-zombies",
    "forceInclude": true
  },
  {
    "title": "GAZP vs. GLUT",
    "_id": "k6EPphHiBH4WWYFCj",
    "slug": "gazp-vs-glut",
    "forceInclude": true
  },
  {
    "title": "Belief in the Implied Invisible",
    "_id": "3XMwPNMSbaPm2suGz",
    "slug": "belief-in-the-implied-invisible",
    "forceInclude": true
  },
  {
    "title": "Zombies— The Movie",
    "_id": "fsDz6HieZJBu54Yes",
    "slug": "zombies-the-movie",
    "forceInclude": true
  },
  {
    "title": "Excluding the Supernatural",
    "_id": "u6JzcFtPGiznFgDxP",
    "slug": "excluding-the-supernatural",
    "forceInclude": true
  },
  {
    "title": "What Do We Mean By \"Rationality\"",
    "_id": "RcZCwxFiZzE6X7nsv",
    "slug": "what-do-we-mean-by-rationality-1",
    "forceInclude": true
  },
  {
    "title": "Scientific Self-Help: The State of Our Knowledge",
    "_id": "33KewgYhNSxFpbpXg",
    "slug": "scientific-self-help-the-state-of-our-knowledge",
    "forceInclude": true
  },
  {
    "title": "Cached Selves",
    "_id": "BHYBdijDcAKQ6e45Z",
    "slug": "cached-selves",
    "forceInclude": true
  },
  {
    "title": "Efficient Charity: Do Unto Others...",
    "_id": "pC47ZTsPNAkjavkXs",
    "slug": "efficient-charity-do-unto-others",
    "forceInclude": true
  },
  {
    "title": "Making Beliefs Pay Rent (in Anticipated Experiences)",
    "_id": "a7n8GdKiAZRX86T5A",
    "slug": "making-beliefs-pay-rent-in-anticipated-experiences",
    "forceInclude": true
  },
  {
    "title": "About Less Wrong",
    "_id": "2om7AHEHtbogJmT5s",
    "slug": "about-less-wrong",
    "forceInclude": true
  },
  {
    "title": "Hindsight Devalues Science",
    "_id": "WnheMGAka4fL99eae",
    "slug": "hindsight-devalues-science",
    "forceInclude": true
  },
  {
    "title": "Tsuyoku Naritai! (I Want To Become Stronger)",
    "_id": "DoLQN5ryZ9XkZjq5h",
    "slug": "tsuyoku-naritai-i-want-to-become-stronger",
    "forceInclude": true
  },
  {
    "title": "Mysterious Answers to Mysterious Questions",
    "_id": "6i3zToomS86oj9bS6",
    "slug": "mysterious-answers-to-mysterious-questions",
    "forceInclude": true
  },
  {
    "title": "Twelve Virtues of Rationality",
    "_id": "7ZqGiPHTpiDMwqMN2",
    "slug": "twelve-virtues-of-rationality",
    "forceInclude": true
  },
  {
    "title": "Diseased thinking: dissolving questions about disease",
    "_id": "895quRDaK6gR2rM82",
    "slug": "diseased-thinking-dissolving-questions-about-disease",
    "forceInclude": true
  },
  {
    "title": "Humans are not automatically strategic",
    "_id": "PBRWb2Em5SNeWYwwB",
    "slug": "humans-are-not-automatically-strategic",
    "forceInclude": true
  },
  {
    "title": "Ugh Fields",
    "_id": "EFQ3F6kmt4WHXRqik",
    "slug": "ugh-fields",
    "forceInclude": true
  },
  {
    "title": "Probability is in the Mind",
    "_id": "f6ZLxEWaankRZ2Crv",
    "slug": "probability-is-in-the-mind",
    "forceInclude": true
  },
  {
    "title": "How An Algorithm Feels From Inside",
    "_id": "yA4gF5KrboK2m2Xu7",
    "slug": "how-an-algorithm-feels-from-inside",
    "forceInclude": true
  },
  {
    "title": "Knowing About Biases Can Hurt People",
    "_id": "AdYdLP2sRqPMoe8fb",
    "slug": "knowing-about-biases-can-hurt-people",
    "forceInclude": true
  },
  {
    "title": "A Fable of Science and Politics",
    "_id": "6hfGNLf4Hg5DXqJCF",
    "slug": "a-fable-of-science-and-politics",
    "forceInclude": true
  },
  {
    "title": "Taboo Your Words",
    "_id": "WBdvyyHLdxZSAMmoz",
    "slug": "taboo-your-words",
    "forceInclude": true
  },
  {
    "title": "The Fallacy of Gray",
    "_id": "dLJv2CoRCgeC2mPgj",
    "slug": "the-fallacy-of-gray",
    "forceInclude": true
  },
  {
    "title": "Mind Projection Fallacy",
    "_id": "ZTRiSNmeGQK8AkdN2",
    "slug": "mind-projection-fallacy",
    "forceInclude": true
  },
  {
    "title": "Reductionism",
    "_id": "tPqQdLCuxanjhoaNs",
    "slug": "reductionism",
    "forceInclude": true
  },
  {
    "title": "Privileging the Hypothesis",
    "_id": "X2AD2LgtKgkRNPj2a",
    "slug": "privileging-the-hypothesis",
    "forceInclude": true
  },
  {
    "title": "Conservation of Expected Evidence",
    "_id": "jiBFC7DcCrZjGmZnJ",
    "slug": "conservation-of-expected-evidence",
    "forceInclude": true
  },
  {
    "title": "The Apologist and the Revolutionary",
    "_id": "ZiQqsgGX6a42Sfpii",
    "slug": "the-apologist-and-the-revolutionary",
    "forceInclude": true
  },
  {
    "title": "Your Strength as a Rationalist",
    "_id": "5JDkW4MYXit2CquLs",
    "slug": "your-strength-as-a-rationalist",
    "forceInclude": true
  },
  {
    "title": "Practical Advice Backed By Deep Theories",
    "_id": "LqjKP255fPRY7aMzw",
    "slug": "practical-advice-backed-by-deep-theories",
    "forceInclude": true
  },
  {
    "title": "Scientific Evidence, Legal Evidence, Rational Evidence",
    "_id": "fhojYBGGiYAFcryHZ",
    "slug": "scientific-evidence-legal-evidence-rational-evidence",
    "forceInclude": true
  },
  {
    "title": "Guessing the Teacher's Password",
    "_id": "NMoLJuDJEms7Ku9XS",
    "slug": "guessing-the-teacher-s-password",
    "forceInclude": true
  },
  {
    "title": "The Sword of Good",
    "_id": "XuLG6M7sHuenYWbfC",
    "slug": "the-sword-of-good",
    "forceInclude": true
  },
  {
    "title": "Fake Explanations",
    "_id": "fysgqk4CjAwhBgNYT",
    "slug": "fake-explanations",
    "forceInclude": true
  },
  {
    "title": "The Proper Use of Humility",
    "_id": "GrDqnMjhqoxiqpQPw",
    "slug": "the-proper-use-of-humility",
    "forceInclude": true
  },
  {
    "title": "\"What Is Wrong With Our Thoughts\"",
    "_id": "EdyDGRLNFScEt5uDz",
    "slug": "what-is-wrong-with-our-thoughts",
    "forceInclude": true
  },
  {
    "title": "Doing your good deed for the day",
    "_id": "r8stxYL29NF9w53am",
    "slug": "doing-your-good-deed-for-the-day",
    "forceInclude": true
  },
  {
    "title": "Beware Trivial Inconveniences",
    "_id": "reitXJgJXFzKpdKyd",
    "slug": "beware-trivial-inconveniences",
    "forceInclude": true
  },
  {
    "title": "The Bias You Didn't Expect",
    "_id": "L8dB6yoMEWofoeDNt",
    "slug": "the-bias-you-didn-t-expect",
    "forceInclude": true
  },
  {
    "title": "Hold Off On Proposing Solutions",
    "_id": "uHYYA32CKgKT3FagE",
    "slug": "hold-off-on-proposing-solutions",
    "forceInclude": true
  },
  {
    "title": "Hindsight bias",
    "_id": "fkM9XsNvXdYH6PPAx",
    "slug": "hindsight-bias",
    "forceInclude": true
  },
  {
    "title": "The Trouble With \"Good\"",
    "_id": "M2LWXsJxKS626QNEA",
    "slug": "the-trouble-with-good",
    "forceInclude": true
  },
  {
    "title": "How to Not Lose an Argument",
    "_id": "6yTShbTdtATxKonY5",
    "slug": "how-to-not-lose-an-argument",
    "forceInclude": true
  },
  {
    "title": "You're Entitled to Arguments, But Not (That Particular) Proof",
    "_id": "vqbieD9PHG8RRJddu",
    "slug": "you-re-entitled-to-arguments-but-not-that-particular-proof",
    "forceInclude": true
  },
  {
    "title": "What's a Bias?",
    "_id": "jnZbHi873v9vcpGpZ",
    "slug": "what-s-a-bias",
    "forceInclude": true
  },
  {
    "title": "Applause Lights",
    "_id": "dLbkrPu5STNCBLRjr",
    "slug": "applause-lights",
    "forceInclude": true
  },
  {
    "title": "Explaining vs. Explaining Away",
    "_id": "cphoF8naigLhRf3tu",
    "slug": "explaining-vs-explaining-away",
    "forceInclude": true
  },
  {
    "title": "Talking Snakes: A Cautionary Tale",
    "_id": "atcJqdhCxTZiJSxo2",
    "slug": "talking-snakes-a-cautionary-tale",
    "forceInclude": true
  },
  {
    "title": "Reversed Stupidity Is Not Intelligence",
    "_id": "qNZM3EGoE5ZeMdCRt",
    "slug": "reversed-stupidity-is-not-intelligence",
    "forceInclude": true
  },
  {
    "title": "Politics is the Mind-Killer",
    "_id": "9weLK2AJ9JEt2Tt8f",
    "slug": "politics-is-the-mind-killer",
    "forceInclude": true
  },
  {
    "title": "Newcomb's Problem and Regret of Rationality",
    "_id": "6ddcsdA2c2XpNpE5x",
    "slug": "newcomb-s-problem-and-regret-of-rationality",
    "forceInclude": true
  },
  {
    "title": "Semantic Stopsigns",
    "_id": "FWMfQKG3RpZx6irjm",
    "slug": "semantic-stopsigns",
    "forceInclude": true
  },
  {
    "title": "Dissolving the Question",
    "_id": "Mc6QcrsbH5NRXbCRX",
    "slug": "dissolving-the-question",
    "forceInclude": true
  },
  {
    "title": "Learned Blankness",
    "_id": "puhPJimawPuNZ5wAR",
    "slug": "learned-blankness",
    "forceInclude": true
  },
  {
    "title": "Reason as memetic immune disorder",
    "_id": "aHaqgTNnFzD7NGLMx",
    "slug": "reason-as-memetic-immune-disorder",
    "forceInclude": true
  },
  {
    "title": "Making your explicit reasoning trustworthy",
    "_id": "m5AH78nscsGjMbBwv",
    "slug": "making-your-explicit-reasoning-trustworthy",
    "forceInclude": true
  },
  {
    "title": "Confidence levels inside and outside an argument",
    "_id": "GrtbTAPfkJa4D6jjH",
    "slug": "confidence-levels-inside-and-outside-an-argument",
    "forceInclude": true
  },
  {
    "title": "Truly Part Of You",
    "_id": "fg9fXrHpeaDD6pEPL",
    "slug": "truly-part-of-you",
    "forceInclude": true
  },
  {
    "title": "The 5-Second Level",
    "_id": "JcpzFpPBSmzuksmWM",
    "slug": "the-5-second-level",
    "forceInclude": true
  },
  {
    "title": "Ureshiku Naritai",
    "_id": "xnPFYBuaGhpq869mY",
    "slug": "ureshiku-naritai",
    "forceInclude": true
  },
  {
    "title": "Guilt: Another Gift Nobody Wants",
    "_id": "CZnBQtvDw33rmWpBD",
    "slug": "guilt-another-gift-nobody-wants",
    "forceInclude": true
  },
  {
    "title": "How to Beat Procrastination",
    "_id": "RWo4LwFzpHNQCTcYt",
    "slug": "how-to-beat-procrastination",
    "forceInclude": true
  },
  {
    "title": "Evaporative Cooling of Group Beliefs",
    "_id": "ZQG9cwKbct2LtmL3p",
    "slug": "evaporative-cooling-of-group-beliefs",
    "forceInclude": true
  },
  {
    "title": "Occam's Razor",
    "_id": "f4txACqDWithRi7hs",
    "slug": "occam-s-razor",
    "forceInclude": true
  },
  {
    "title": "Anchoring and Adjustment",
    "_id": "bMkCEZoBNhgRBtzoj",
    "slug": "anchoring-and-adjustment",
    "forceInclude": true
  },
  {
    "title": "The Least Convenient Possible World",
    "_id": "neQ7eXuaXpiYw7SBy",
    "slug": "the-least-convenient-possible-world",
    "forceInclude": true
  },
  {
    "title": "Scarcity",
    "_id": "MCYp8g9EMAiTCTawk",
    "slug": "scarcity",
    "forceInclude": true
  },
  {
    "title": "The Neglected Virtue of Scholarship",
    "_id": "64FdKLwmea8MCLWkE",
    "slug": "the-neglected-virtue-of-scholarship",
    "forceInclude": true
  },
  {
    "title": "Scholarship: How to Do It Efficiently",
    "_id": "37sHjeisS9uJufi4u",
    "slug": "scholarship-how-to-do-it-efficiently",
    "forceInclude": true
  },
  {
    "title": "Excluding the Supernatural",
    "_id": "u6JzcFtPGiznFgDxP",
    "slug": "excluding-the-supernatural",
    "forceInclude": true
  },
  {
    "title": "Joy in the Merely Real",
    "_id": "x4dG4GhpZH2hgz59x",
    "slug": "joy-in-the-merely-real",
    "forceInclude": true
  },
  {
    "title": "Affective Death Spirals",
    "_id": "XrzQW69HpidzvBxGr",
    "slug": "affective-death-spirals",
    "forceInclude": true
  },
  {
    "title": "The Lens That Sees Its Flaws",
    "_id": "46qnWRSR7L2eyNbMA",
    "slug": "the-lens-that-sees-its-flaws",
    "forceInclude": true
  },
  {
    "title": "The Virtue of Narrowness",
    "_id": "yDfxTj9TKYsYiWH5o",
    "slug": "the-virtue-of-narrowness",
    "forceInclude": true
  },
  {
    "title": "Generalizing From One Example",
    "_id": "baTWMegR42PAsH9qJ",
    "slug": "generalizing-from-one-example",
    "forceInclude": true
  },
  {
    "title": "Belief in Belief",
    "_id": "CqyJzDZWvGhhFJ7dY",
    "slug": "belief-in-belief",
    "forceInclude": true
  },
  {
    "title": "Beyond the Reach of God",
    "_id": "sYgv4eYH82JEsTD34",
    "slug": "beyond-the-reach-of-god",
    "forceInclude": true
  },
  {
    "title": "Experiential Pica",
    "_id": "9ZodFr54FtpLThHZh",
    "slug": "experiential-pica",
    "forceInclude": true
  },
  {
    "title": "Money: The Unit of Caring",
    "_id": "ZpDnRCeef2CLEFeKM",
    "slug": "money-the-unit-of-caring",
    "forceInclude": true
  },
  {
    "title": "The Importance of Goodhart's Law",
    "_id": "YtvZxRpZjcFNwJecS",
    "slug": "the-importance-of-goodhart-s-law",
    "forceInclude": true
  },
  {
    "title": "Policy Debates Should Not Appear One-Sided",
    "_id": "PeSzc9JTBxhaYRp9b",
    "slug": "policy-debates-should-not-appear-one-sided",
    "forceInclude": true
  },
  {
    "title": "The mathematical universe: the map that is the territory",
    "_id": "fZJRxYLtNNzpbWZAA",
    "slug": "the-mathematical-universe-the-map-that-is-the-territory",
    "forceInclude": true
  },
  {
    "title": "Beware of Other-Optimizing",
    "_id": "6NvbSwuSAooQxxf7f",
    "slug": "beware-of-other-optimizing",
    "forceInclude": true
  },
  {
    "title": "A Much Better Life?",
    "_id": "5Qvvi23WT2unNCoS9",
    "slug": "a-much-better-life",
    "forceInclude": true
  },
  {
    "title": "Less Wrong Rationality and Mainstream Philosophy",
    "_id": "oTX2LXHqXqYg2u4g6",
    "slug": "less-wrong-rationality-and-mainstream-philosophy",
    "forceInclude": true
  },
  {
    "title": "Philosophy: A Diseased Discipline",
    "_id": "FwiPfF8Woe5JrzqEu",
    "slug": "philosophy-a-diseased-discipline",
    "forceInclude": true
  },
  {
    "title": "Eight Short Studies On Excuses",
    "_id": "gFMH3Cqw4XxwL69iy",
    "slug": "eight-short-studies-on-excuses",
    "forceInclude": true
  },
  {
    "title": "Fake Causality",
    "_id": "RgkqLqkg8vLhsYpfh",
    "slug": "fake-causality",
    "forceInclude": true
  },
  {
    "title": "Chaotic Inversion",
    "_id": "NyFtHycJvkyNjXNsP",
    "slug": "chaotic-inversion",
    "forceInclude": true
  },
  {
    "title": "Measuring aversion and habit strength",
    "_id": "Fxv4o3LGEkgR2Qsz7",
    "slug": "measuring-aversion-and-habit-strength",
    "forceInclude": true
  },
  {
    "title": "Intellectual Hipsters and Meta-Contrarianism",
    "_id": "9kcTNWopvXFncXgPy",
    "slug": "intellectual-hipsters-and-meta-contrarianism",
    "forceInclude": true
  },
  {
    "title": "Too busy to think about life",
    "_id": "4psQW7vRwt7PE5Pnj",
    "slug": "too-busy-to-think-about-life",
    "forceInclude": true
  },
  {
    "title": "37 Ways That Words Can Be Wrong",
    "_id": "FaJaCgqBKphrDzDSj",
    "slug": "37-ways-that-words-can-be-wrong",
    "forceInclude": true
  },
  {
    "title": "How to Be Happy",
    "_id": "ZbgCx2ntD5eu8Cno9",
    "slug": "how-to-be-happy",
    "forceInclude": true
  },
  {
    "title": "Righting a Wrong Question",
    "_id": "rQEwySCcLtdKHkrHp",
    "slug": "righting-a-wrong-question",
    "forceInclude": true
  },
  {
    "title": "Self-fulfilling correlations",
    "_id": "XuyRMxky6G8gq7a69",
    "slug": "self-fulfilling-correlations",
    "forceInclude": true
  },
  {
    "title": "Why Truth?",
    "_id": "YshRbqZHYFoEMqFAu",
    "slug": "why-truth",
    "forceInclude": true
  },
  {
    "title": "You Only Live Twice",
    "_id": "yKXKcyoBzWtECzXrE",
    "slug": "you-only-live-twice",
    "forceInclude": true
  },
  {
    "title": "Less Wrong NYC: Case Study of a Successful Rationalist Chapter",
    "_id": "CsKboswS3z5iaiutC",
    "slug": "less-wrong-nyc-case-study-of-a-successful-rationalist",
    "forceInclude": true
  },
  {
    "title": "Outside the Laboratory",
    "_id": "N2pENnTPB75sfc9kb",
    "slug": "outside-the-laboratory",
    "forceInclude": true
  },
  {
    "title": "Raising the Sanity Waterline",
    "_id": "XqmjdBKa4ZaXJtNmf",
    "slug": "raising-the-sanity-waterline",
    "forceInclude": true
  },
  {
    "title": "Twelve Virtues of Rationality",
    "_id": "7ZqGiPHTpiDMwqMN2",
    "slug": "twelve-virtues-of-rationality",
    "forceInclude": true
  },
  {
    "title": "Diseased thinking: dissolving questions about disease",
    "_id": "895quRDaK6gR2rM82",
    "slug": "diseased-thinking-dissolving-questions-about-disease",
    "forceInclude": true
  },
  {
    "title": "Self-fulfilling correlations",
    "_id": "XuyRMxky6G8gq7a69",
    "slug": "self-fulfilling-correlations",
    "forceInclude": true
  },
  {
    "title": "Probability is in the Mind",
    "_id": "f6ZLxEWaankRZ2Crv",
    "slug": "probability-is-in-the-mind",
    "forceInclude": true
  },
  {
    "title": "You're Entitled to Arguments, But Not (That Particular) Proof",
    "_id": "vqbieD9PHG8RRJddu",
    "slug": "you-re-entitled-to-arguments-but-not-that-particular-proof",
    "forceInclude": true
  },
  {
    "title": "Efficient Charity: Do Unto Others...",
    "_id": "pC47ZTsPNAkjavkXs",
    "slug": "efficient-charity-do-unto-others",
    "forceInclude": true
  },
  {
    "title": "Newcomb's Problem and Regret of Rationality",
    "_id": "6ddcsdA2c2XpNpE5x",
    "slug": "newcomb-s-problem-and-regret-of-rationality",
    "forceInclude": true
  },
  {
    "title": "Occam's Razor",
    "_id": "f4txACqDWithRi7hs",
    "slug": "occam-s-razor",
    "forceInclude": true
  },
  {
    "title": "Confidence levels inside and outside an argument",
    "_id": "GrtbTAPfkJa4D6jjH",
    "slug": "confidence-levels-inside-and-outside-an-argument",
    "forceInclude": true
  },
  {
    "title": "The Apologist and the Revolutionary",
    "_id": "ZiQqsgGX6a42Sfpii",
    "slug": "the-apologist-and-the-revolutionary",
    "forceInclude": true
  },
  {
    "title": "Beyond the Reach of God",
    "_id": "sYgv4eYH82JEsTD34",
    "slug": "beyond-the-reach-of-god",
    "forceInclude": true
  },
  {
    "title": "The mathematical universe: the map that is the territory",
    "_id": "fZJRxYLtNNzpbWZAA",
    "slug": "the-mathematical-universe-the-map-that-is-the-territory",
    "forceInclude": true
  },
  {
    "title": "That Alien Message",
    "_id": "5wMcKNAwB6X4mp9og",
    "slug": "that-alien-message",
    "forceInclude": true
  },
  {
    "title": "A Much Better Life?",
    "_id": "5Qvvi23WT2unNCoS9",
    "slug": "a-much-better-life",
    "forceInclude": true
  }
]