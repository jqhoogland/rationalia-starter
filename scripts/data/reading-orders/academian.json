{
  "author": "academian",
  "chapters": [
    {
      "title": "Basic Concepts",
      "description": "What we mean by stuff",
      "children": [
        {
          "name": "Map and territory",
          "slug": "map-and-territory",
          "type": "tag"
        },
        {
          "name": "Rationality",
          "slug": "rationality",
          "type": "tag"
        },
        {
          "name": "Truth, Semantics, & Meaning",
          "slug": "truth-semantics-and-meaning",
          "type": "tag"
        },
        {
          "name": "Improper belief",
          "slug": "improper-belief",
          "type": "tag"
        },
        {
          "name": "Evidence",
          "slug": "evidence",
          "type": "tag"
        },
        {
          "name": "Bayes' theorem",
          "slug": "bayes-theorem",
          "type": "tag"
        },
        {
          "name": "Priors",
          "slug": "priors",
          "type": "tag"
        },
        {
          "name": "Belief in belief",
          "type": "post",
          "slug": "belief-in-belief",
          "_id": "CqyJzDZWvGhhFJ7dY",
          "url": null,
          "title": "Belief in Belief",
          "author": "Eliezer Yudkowsky",
          "question": false,
          "tags": [
            {
              "name": "Rationality"
            },
            {
              "name": "Self-Deception"
            },
            {
              "name": "Alief"
            },
            {
              "name": "Motivated Reasoning"
            },
            {
              "name": "Anticipated Experiences"
            },
            {
              "name": "Religion"
            }
          ],
          "tableOfContents": null,
          "contents": {
            "markdown": "Carl Sagan once told a [parable](http://www.godlessgeeks.com/LINKS/Dragon.htm) of someone who comes to us and claims: “There is a dragon in my garage.” Fascinating! We reply that we wish to see this dragon—let us set out at once for the garage! “But wait,” the claimant says to us, “it is an *invisible* dragon.”\n\nNow as Sagan points out, this doesn’t make the hypothesis unfalsifiable. Perhaps we go to the claimant’s garage, and although we see no dragon, we hear heavy breathing from no visible source; footprints mysteriously appear on the ground; and instruments show that something in the garage is consuming oxygen and breathing out carbon dioxide.\n\nBut now suppose that we say to the claimant, “Okay, we’ll visit the garage and see if we can hear heavy breathing,” and the claimant quickly says no, it’s an *inaudible* dragon. We propose to measure carbon dioxide in the air, and the claimant says the dragon does not breathe. We propose to toss a bag of flour into the air to see if it outlines an invisible dragon, and the claimant immediately says, “The dragon is permeable to flour.”\n\nCarl Sagan used this parable to illustrate the classic moral that poor hypotheses need to do fast footwork to avoid falsification. But I tell this parable to make a different point: The claimant must have an accurate model of the situation *somewhere* in their mind, because they can anticipate, in advance, *exactly which experimental results they’ll need to excuse.*\n\nSome philosophers have been much confused by such scenarios, asking, “Does the claimant *really* believe there’s a dragon present, or not?” As if the human brain only had enough disk space to represent one belief at a time! Real minds are more tangled than that. There are different types of belief; not all beliefs are direct anticipations. The claimant clearly does not *anticipate* seeing anything unusual upon opening the garage door. Otherwise they wouldn’t make advance excuses. It may also be that the claimant’s pool of propositional beliefs contains the free-floating statement *There is a dragon in my garage.* It may seem, to a rationalist, that these two beliefs should collide and conflict even though they are of different types. Yet it is a physical fact that you can write “The sky is green!” next to a picture of a blue sky without the paper bursting into flames.\n\nThe rationalist virtue of empiricism is supposed to prevent us from making this class of mistake. We’re supposed to constantly ask our beliefs which experiences they predict, make them pay rent in anticipation. But the dragon-claimant’s problem runs deeper, and cannot be cured with such simple advice. It’s not exactly *difficult* to connect belief in a dragon to anticipated experience of the garage. If you believe there’s a dragon in your garage, then you can expect to open up the door and see a dragon. If you don’t see a dragon, then that means there’s no dragon in your garage. This is pretty straightforward. You can even try it with your own garage.\n\nNo, this invisibility business is a symptom of something much worse.\n\nDepending on how your childhood went, you may remember a time period when you first began to doubt Santa Claus’s existence, but you still believed that you were *supposed* to believe in Santa Claus, so you tried to deny the doubts. As Daniel Dennett observes, where it is difficult to believe a thing, it is often much easier to believe that you *ought* to believe it. What does it mean to believe that the Ultimate Cosmic Sky is both perfectly blue and perfectly green? The statement is confusing; it’s not even clear what it would *mean* to believe it—what exactly would *be* believed, if you believed. You can much more easily believe that it is *proper*, that it is *good* and *virtuous* and *beneficial*, to believe that the Ultimate Cosmic Sky is both perfectly blue and perfectly green. Dennett calls this “belief in belief.”^1^\n\nAnd here things become complicated, as human minds are wont to do—I think even Dennett oversimplifies how this psychology works in practice. For one thing, if you believe in belief, you cannot admit to yourself that you merely believe in belief. What’s virtuous is to *believe*, not to believe in believing; and so if you only believe in belief, instead of believing, you are not virtuous. Nobody will *admit* to themselves, “I don’t believe the Ultimate Cosmic Sky is blue and green, but I believe I ought to believe it”—not unless they are unusually capable of acknowledging their own lack of virtue. People don’t believe in belief in belief, they just believe in belief.\n\n(Those who find this confusing may find it helpful to study mathematical logic, which trains one to make very sharp distinctions between the proposition P, a proof of P, and a proof that P is provable. There are similarly sharp distinctions between P, wanting P, believing P, wanting to believe P, and believing that you believe P.)\n\nThere are different kinds of belief in belief. You may believe in belief explicitly; you may recite in your deliberate stream of consciousness the verbal sentence “It is virtuous to believe that the Ultimate Cosmic Sky is perfectly blue and perfectly green.” (While also believing that you believe this, unless you are unusually capable of acknowledging your own lack of virtue.) But there are also less explicit forms of belief in belief. Maybe the dragon-claimant fears the public ridicule that they imagine will result if they publicly confess they were wrong.^2^ Maybe the dragon-claimant flinches away from the prospect of admitting to themselves that there is no dragon, because it conflicts with their self-image as the glorious discoverer of the dragon, who saw in their garage what all others had failed to see.\n\nIf all our thoughts were deliberate verbal sentences like philosophers manipulate, the human mind would be a great deal easier for humans to understand. Fleeting mental images, unspoken flinches, desires acted upon without acknowledgement—these account for as much of ourselves as words.\n\nWhile I disagree with Dennett on some details and complications, I still think that Dennett’s notion of *belief in belief* is the key insight necessary to understand the dragon-claimant. But we need a wider concept of *belief*, not limited to verbal sentences. “Belief” should include unspoken anticipation-controllers. “Belief in belief” should include unspoken cognitive-behavior-guiders. It is not psychologically realistic to say, “The dragon-claimant does not believe there is a dragon in their garage; they believe it is beneficial to believe there is a dragon in their garage.” But it is realistic to say the dragon-claimant *anticipates as if* there is no dragon in their garage, and *makes excuses as if* they believed in the belief.\n\nYou can possess an ordinary mental picture of your garage, with no dragons in it, which correctly predicts your experiences on opening the door, and never once think the verbal phrase *There is no dragon in my garage.* I even bet it’s happened to you—that when you open your garage door or bedroom door or whatever, and expect to see no dragons, no such verbal phrase runs through your mind.\n\nAnd to flinch away from giving up your belief in the dragon—or flinch away from giving up your *self-image* as a person who believes in the dragon—it is not necessary to explicitly think *I want to believe there’s a dragon in my garage.* It is only necessary to flinch away from the prospect of admitting you don’t believe.\n\nIf someone believes in their belief in the dragon, and also believes in the dragon, the problem is much less severe. They will be willing to stick their neck out on experimental predictions, and perhaps even agree to give up the belief if the experimental prediction is wrong.^3^ But when someone makes up excuses *in advance*, it would seem to require that belief and belief in belief have become unsynchronized.\n\n* * *\n\n^1^ Daniel C. Dennett, *Breaking the Spell: Religion as a Natural Phenomenon* (Penguin, 2006).\n\n^2^ Although, in fact, a rationalist would congratulate them, and others are more likely to ridicule the claimant if they go on claiming theres a dragon in their garage.\n\n^3^ Although belief in belief can still interfere with this, if the belief itself is not absolutely confident."
          },
          "voteCount": 132
        }
      ]
    },
    {
      "title": "Table of contents of [[How to Actually Change Your Mind]]",
      "description": "A preview of how ideas actually get applied",
      "children": []
    },
    {
      "title": "Selected posts from [[Map and Territory (sequence)]]",
      "description": "",
      "children": [
        {
          "name": "What do we mean by \"Rationality\"?",
          "type": "post",
          "slug": "what-do-we-mean-by-rationality-1",
          "_id": "RcZCwxFiZzE6X7nsv",
          "url": null,
          "title": "What Do We Mean By \"Rationality\"?",
          "author": "Eliezer Yudkowsky",
          "question": false,
          "tags": [
            {
              "name": "Rationality"
            },
            {
              "name": "Definitions"
            },
            {
              "name": "Distinctions"
            },
            {
              "name": "Motivational Intro Posts"
            }
          ],
          "tableOfContents": null,
          "contents": {
            "markdown": "I mean two things:\n\n1\\. **Epistemic rationality**: systematically improving the accuracy of your beliefs.\n\n2\\. **Instrumental rationality**: systematically achieving your values.\n\nThe first concept is simple enough. When you open your eyes and look at the room around you, you’ll locate your laptop in relation to the table, and you’ll locate a bookcase in relation to the wall. If something goes wrong with your eyes, or your brain, then your mental model might say there’s a bookcase where no bookcase exists, and when you go over to get a book, you’ll be disappointed.\n\nThis is what it’s like to have a false belief, a map of the world that doesn’t correspond to the territory. Epistemic rationality is about building accurate maps instead. This correspondence between belief and reality is commonly called “truth,” and I’m happy to call it that.^1^\n\nInstrumental rationality, on the other hand, is about *steering* reality—sending the future where you want it to go. It’s the art of choosing actions that lead to outcomes ranked higher in your preferences. I sometimes call this “winning.”\n\nSo rationality is about forming true beliefs and making decisions that help you win.\n\n(Where truth doesn't mean “certainty,” since we can do plenty to increase the *probability* that our beliefs are accurate even though we're uncertain; and winning doesn't mean “winning at others' expense,” since our values include *everything* we care about, including other people.)\n\nWhen people say “X is rational!” it’s usually just a more strident way of saying “I think X is true” or “I think X is good.” So why have an additional word for “rational” as well as “true” and “good”?\n\nAn analogous argument can be given against using “true.” There is no need to say “it is true that snow is white” when you could just say “snow is white.” What makes the idea of truth useful is that it allows us to talk about the general features of map-territory correspondence. “True models usually produce better experimental predictions than false models” is a useful generalization, and it’s not one you can make without using a concept like “true” or “accurate.”\n\nSimilarly, “Rational agents make decisions that maximize the probabilistic expectation of a coherent utility function” is the kind of thought that depends on a concept of (instrumental) rationality, whereas “It’s rational to eat vegetables” can probably be replaced with “It’s useful to eat vegetables” or “It’s in your interest to eat vegetables.” We need a concept like “rational” in order to note general facts about those ways of thinking that systematically produce truth or value—and the systematic ways in which we fall short of those standards.\n\nAs we’ve observed in the previous essays, experimental psychologists sometimes uncover human reasoning that seems very strange. For example, someone rates the probability “Bill plays jazz” as *less* than the probability “Bill is an accountant who plays jazz.” This seems like an odd judgment, since any particular jazz-playing accountant is obviously a jazz player. But to what higher vantage point do we appeal in saying that the judgment is *wrong* ?\n\nExperimental psychologists use two gold standards: *probability theory*, and *decision theory*.\n\nProbability theory is the set of laws underlying rational belief. The mathematics of probability applies equally to “figuring out where your bookcase is” and “estimating how many hairs were on Julius Caesars head,” even though our evidence for the claim “Julius Caesar was bald” is likely to be more complicated and indirect than our evidence for the claim “theres a bookcase in my room.” It’s all the same problem of how to process the evidence and observations to update one’s beliefs. Similarly, decision theory is the set of laws underlying rational action, and is equally applicable regardless of what one’s goals and available options are.\n\nLet “P(such-and-such)” stand for “the probability that such-and-such happens,” and “P(A,B)” for “the probability that both A and B happen.” Since it is a universal law of probability theory that P(A) ≥ P(A,B), the judgment that P(Bill plays jazz) is less than P(Bill plays jazz, Bill is an accountant) is labeled incorrect.\n\nTo keep it technical, you would say that this probability judgment is *non-Bayesian*. Beliefs that conform to a coherent probability distribution, and decisions that maximize the probabilistic expectation of a coherent utility function, are called “Bayesian.”\n\nI should emphasize that this *isn't *the notion of rationality thats common in popular culture. People may use the same string of sounds, “ra-tio-nal,” to refer to “acting like Mr. Spock of *Star Trek*” and “acting like a Bayesian”; but this doesn't mean that acting Spock-like helps one hair with epistemic or instrumental rationality.^2^\n\nAll of this does not quite exhaust the problem of what is meant in practice by “rationality,” for two major reasons:\n\nFirst, the Bayesian formalisms in their full form are computationally intractable on most real-world problems. No one can *actually *calculate and obey the math, any more than you can predict the stock market by calculating the movements of quarks.\n\nThis is why there is a whole site called “Less Wrong,” rather than a single page that simply states the formal axioms and calls it a day. There’s a whole further art to finding the truth and accomplishing value *from inside a human mind*: we have to learn our own flaws, overcome our biases, prevent ourselves from self-deceiving, get ourselves into good emotional shape to confront the truth and do what needs doing, et cetera, et cetera.\n\nSecond, sometimes the meaning of the math itself is called into question. The exact rules of probability theory are called into question by, e.g., [anthropic problems](http://www.anthropic-principle.com/?q=anthropic_principle/primer) in which the number of observers is uncertain. The exact rules of decision theory are called into question by, e.g., Newcomblike problems in which other agents may predict your decision before it happens.^3^\n\nIn cases where our best formalizations still come up short, we can return to simpler ideas like “truth” and “winning.” If you are a scientist just beginning to investigate fire, it might be a lot wiser to point to a campfire and say “Fire is that orangey-bright hot stuff over there,” rather than saying “I define fire as an alchemical transmutation of substances which releases phlogiston.” You certainly shouldn’t ignore something just because you can’t define it. I can't quote the equations of General Relativity from memory, but nonetheless if I walk off a cliff, I'll fall. And we can say the same of cognitive biases and other obstacles to truth—they won't hit any less hard if it turns out we can't define compactly what “irrationality” is.\n\nIn cases like these, it is futile to try to settle the problem by coming up with some new definition of the word “rational” and saying, “Therefore my preferred answer, *by definition*, is what is meant by the word ‘rational.’ ” This simply raises the question of why anyone should pay attention to your definition. I’m not interested in probability theory because it is the holy word handed down from Laplace. I’m interested in Bayesian-style belief-updating (with Occam priors) because I expect that this style of thinking gets us systematically closer to, you know, *accuracy*, the map that reflects the territory.\n\nAnd then there are questions of how to think that seem not quite answered by either probability theory or decision theory—like the question of how to feel about the truth once you have it. Here, again, trying to define “rationality” a particular way doesn’t support an answer, but merely presumes one.\n\nI am not here to argue the meaning of a word, not even if that word is “rationality.” The point of attaching sequences of letters to particular concepts is to let two people *communicate*—to help transport thoughts from one mind to another. You cannot change reality, or prove the thought, by manipulating which meanings go with which words.\n\nSo if you understand what concept I am *generally getting at *with this word “rationality,” and with the sub-terms “epistemic rationality” and “instrumental rationality,” we *have communicated*: we have accomplished everything there is to accomplish by talking about how to define “rationality.” What’s left to discuss is not *what meaning *to attach to the syllables “ra-tio-na-li-ty”; what’s left to discuss is *what is a good way to think*.\n\nIf you say, “It’s (epistemically) rational for me to believe X, but the truth is Y,” then you are probably using the word “rational” to mean something other than what I have in mind. (E.g., “rationality” should be *consistent under reflection*—“rationally” looking at the evidence, and “rationally” considering how your mind processes the evidence, shouldn’t lead to two different conclusions.)\n\nSimilarly, if you find yourself saying, “The (instrumentally) rational thing for me to do is X, but the right thing for me to do is Y,” then you are almost certainly using some other meaning for the word “rational” or the word “right.” I use the term “rationality” *normatively*, to pick out desirable patterns of thought.\n\nIn this case—or in any other case where people disagree about word meanings—you should substitute more specific language in place of “rational”: “The self-benefiting thing to do is to run away, but I hope I would at least try to drag the child off the railroad tracks,” or “Causal decision theory as usually formulated says you should two-box on Newcomb’s Problem, but I’d rather have a million dollars.”\n\nIn fact, I recommend reading back through this essay, replacing every instance of “rational” with “foozal,” and seeing if that changes the connotations of what I’m saying any. If so, I say: strive not for rationality, but for foozality.\n\nThe word “rational” has potential pitfalls, but there are plenty of *non*-borderline cases where “rational” works fine to communicate what I’m getting at. Likewise “irrational.” In these cases I’m not afraid to use it.\n\nYet one should be careful not to *overuse *that word. One receives no points merely for pronouncing it loudly. If you speak overmuch of the Way, you will not attain it.\n\n* * *\n\n^1^ For a longer discussion of truth, see “[The Simple Truth](https://www.lesswrong.com/rationality/the-simple-truth)” at the very end of this volume.\n\n^2^ The idea that rationality is about strictly privileging verbal reasoning over feelings is a case in point. Bayesian rationality applies to urges, hunches, perceptions, and wordless intuitions, not just to assertions.\n\nI gave the example of opening your eyes, looking around you, and building a mental model of a room containing a bookcase against the wall. The modern idea of rationality is general enough to include your eyes and your brains visual areas as things-that-map, and to include instincts and emotions in the belief-and-goal calculus.\n\n^3^ For an informal statement of Newcomb’s Problem, see Jim Holt, “Thinking Inside the Boxes,” *Slate*, 2002, [http://www.slate.com/articles/arts/egghead/2002/02/thinkinginside\\_the\\_boxes.single.html](http://www.slate.com/articles/arts/egghead/2002/02/thinkinginside_the_boxes.single.html)."
          },
          "voteCount": 263
        },
        {
          "name": "Bayes' rule: Guide",
          "type": "post",
          "comment": "The original post is deprecated in favor of this explanation.",
          "href": "https://arbital.com/p/bayes_rule/?l=1zq"
        },
        {
          "name": "What is evidence?",
          "type": "post",
          "slug": "what-is-evidence",
          "_id": "6s3xABaXKPdFwA3FS",
          "url": null,
          "title": "What is Evidence?",
          "author": "Eliezer Yudkowsky",
          "question": false,
          "tags": [
            {
              "name": "Epistemology"
            },
            {
              "name": "Causality"
            },
            {
              "name": "Anticipated Experiences"
            }
          ],
          "tableOfContents": null,
          "contents": {
            "markdown": "> The sentence “snow is white” is *true* if and only if snow is white.\n> \n> —Alfred Tarski\n\n> To say of what is, that it is, or of what is not, that it is not, is *true*.\n> \n> —Aristotle, *Metaphysics IV*\n\nWalking along the street, your shoelaces come untied. Shortly thereafter, for some odd reason, you start *believing* your shoelaces are untied. Light leaves the Sun and strikes your shoelaces and bounces off; some photons enter the pupils of your eyes and strike your retina; the energy of the photons triggers neural impulses; the neural impulses are transmitted to the visual-processing areas of the brain; and there the optical information is processed and reconstructed into a 3D model that is recognized as an untied shoelace. There is a sequence of events, a chain of cause and effect, within the world and your brain, by which you end up believing what you believe. The final outcome of the process is a state of *mind* which mirrors the state of your actual *shoelaces*.\n\nWhat is *evidence?* It is an event entangled, by links of cause and effect, with whatever you want to know about. If the target of your inquiry is your shoelaces, for example, then the light entering your pupils is evidence entangled with your shoelaces. This should not be confused with the technical sense of “entanglement” used in physics—here I’m just talking about “entanglement” in the sense of two things that end up in correlated states because of the links of cause and effect between them.\n\nNot every influence creates the kind of “entanglement” required for evidence. It’s no help to have a machine that beeps when you enter winning lottery numbers, if the machine *also* beeps when you enter *losing* lottery numbers. The light reflected from your shoes would not be useful evidence about your shoelaces, if the photons ended up in the same physical state whether your shoelaces were tied or untied.\n\nTo say it abstractly: For an event to be *evidence about* a target of inquiry, it has to happen *differently* in a way that’s entangled with the *different* possible states of the target. (To say it technically: There has to be Shannon mutual information between the evidential event and the target of inquiry, relative to your current state of uncertainty about both of them.)\n\nEntanglement can be contagious when processed correctly, which is why you need eyes and a brain. If photons reflect off your shoelaces and hit a rock, the rock won’t change much. The rock won’t reflect the shoelaces in any helpful way; it won’t be detectably different depending on whether your shoelaces were tied or untied. This is why rocks are not useful witnesses in court. A photographic film will contract shoelace-entanglement from the incoming photons, so that the photo can itself act as evidence. If your eyes and brain work correctly, *you* will become tangled up with your own shoelaces.\n\nThis is why rationalists put such a heavy premium on the paradoxical-seeming claim that a belief is only really worthwhile if you could, in principle, be persuaded to believe otherwise. If your retina ended up in the same state regardless of what light entered it, you would be blind. Some belief systems, in a rather obvious trick to reinforce themselves, say that certain beliefs are only really worthwhile if you believe them *unconditionally*—no matter what you see, no matter what you think. Your brain is supposed to end up in the same state regardless. Hence the phrase, “blind faith.” If what you believe doesn’t depend on what you see, you’ve been blinded as effectively as by poking out your eyeballs.\n\nIf your eyes and brain work correctly, your beliefs will end up entangled with the facts. *Rational thought produces beliefs which are themselves evidence.*\n\nIf your tongue speaks truly, your rational beliefs, which are themselves evidence, can act as evidence for someone else. Entanglement can be transmitted through chains of cause and effect—and if you speak, and another hears, that too is cause and effect. When you say “My shoelaces are untied” over a cellphone, you’re sharing your entanglement with your shoelaces with a friend.\n\nTherefore rational beliefs are contagious, among honest folk who believe each other to be honest. And it’s why a claim that your beliefs are *not* contagious—that you believe for private reasons which are not transmissible—is so suspicious. If your beliefs are entangled with reality, they *should* be contagious among honest folk.\n\nIf your model of reality suggests that the outputs of your thought processes should *not* be contagious to others, then your model says that your beliefs are not themselves evidence, meaning they are not entangled with reality. You should apply a reflective correction, and stop believing.\n\nIndeed, if you *feel*, on a *gut* level, what this all *means*, you will *automatically* stop believing. Because “my belief is not entangled with reality” *means* “my belief is not accurate.” As soon as you stop believing “ ‘snow is white’ is true,” you should (automatically!) stop believing “snow is white,” or something is very wrong.\n\nSo try to explain why the kind of thought processes you use systematically produce beliefs that mirror reality. Explain why you think you’re rational. Why you think that, using thought processes like the ones you use, minds will end up believing “snow is white” if and only if snow is white. If you *don’t* believe that the outputs of your thought processes are entangled with reality, why believe the outputs of your thought processes? It’s the same thing, or it should be."
          },
          "voteCount": 116
        },
        {
          "name": "How to convince me that 2+2=3",
          "type": "post",
          "slug": "how-to-convince-me-that-2-2-3",
          "_id": "6FmqiAgS8h4EJm86s",
          "url": null,
          "title": "How to Convince Me That 2 + 2 = 3",
          "author": "Eliezer_Yudkowsky",
          "question": false,
          "tags": [
            {
              "name": "Rationality"
            },
            {
              "name": "Epistemology"
            },
            {
              "name": "Logic & Mathematics "
            }
          ],
          "tableOfContents": null,
          "contents": {
            "markdown": "In “[What is Evidence?](https://www.lesswrong.com/rationality/what-is-evidence)” I wrote:^[1](#fn1x10)^\n\n> This is why rationalists put such a heavy premium on the paradoxical-seeming claim that a belief is only really _worthwhile_ if you could, in principle, be persuaded to believe otherwise. If your retina ended up in the same state regardless of what light entered it, you would be blind . . . Hence the phrase, “blind faith.” If what you believe doesn’t depend on what you see, you’ve been blinded as effectively as by poking out your eyeballs.\n\nCihan Baran replied:^[2](#fn2x10)^\n\n> I can not conceive of a situation that would make 2 + 2 = 4 false. Perhaps for that reason, my belief in 2 + 2 = 4 is unconditional.\n\nI admit, I cannot conceive of a “situation” that would _make_ 2 + 2 = 4 false. (There are redefinitions, but those are not “situations,” and then you’re no longer talking about 2, 4, =, or +.) But that doesn’t make my belief unconditional. I find it quite easy to imagine a situation which would _convince_ me that 2 + 2 = 3.\n\nSuppose I got up one morning, and took out two earplugs, and set them down next to two other earplugs on my nighttable, and noticed that there were now three earplugs, without any earplugs having appeared or disappeared—in contrast to my stored memory that 2 + 2 was supposed to equal 4. Moreover, when I visualized the process in my own mind, it seemed that making xx and xx come out to xxxx required an extra x to appear from nowhere, and was, moreover, inconsistent with other arithmetic I visualized, since subtracting xx from xxx left xx, but subtracting xx from xxxx left xxx. This would conflict with my stored memory that 3 - 2 = 1, but memory would be absurd in the face of physical and mental confirmation that xxx \\- xx = xx.\n\nI would also check a pocket calculator, Google, and perhaps my copy of 1984 where Winston writes that “Freedom is the freedom to say two plus two equals three.” All of these would naturally show that the rest of the world agreed with my current visualization, and disagreed with my memory, that 2 + 2 = 3.\n\nHow could I possibly have ever been so deluded as to believe that 2 + 2 = 4? Two explanations would come to mind: First, a neurological fault (possibly caused by a sneeze) had made all the additive sums in my stored memory go up by one. Second, someone was messing with me, by hypnosis or by my being a computer simulation. In the second case, I would think it more likely that they had messed with my arithmetic _recall_ than that 2 + 2 _actually_ equalled 4. Neither of these plausible-sounding explanations would prevent me from noticing that I was very, very, _very_ confused.^[3](#fn3x10)^\n\nWhat would convince me that 2 + 2 = 3, in other words, is exactly the same kind of evidence that currently convinces me that 2 + 2 = 4: The evidential crossfire of physical observation, mental visualization, and social agreement.\n\nThere was a time when I had no idea that 2 + 2 = 4. I did not arrive at this _new_ belief by random processes—then there would have been no particular reason for my brain to end up storing “2 + 2 = 4” instead of “2 + 2 = 7.” The fact that my brain stores an answer surprisingly similar to what happens when I lay down two earplugs alongside two earplugs, calls forth an explanation of what entanglement produces this strange mirroring of mind and reality.\n\nThere’s really only two possibilities, for a belief of fact—either the belief got there via a mind-reality entangling process, or not. If not, the belief can’t be correct except by coincidence. For beliefs with the slightest shred of internal complexity (requiring a computer program of more than 10 bits to simulate), the space of possibilities is large enough that coincidence vanishes.^[4](#fn4x10)^\n\nUnconditional facts are not the same as unconditional beliefs. If entangled evidence convinces me that a fact is unconditional, this doesn’t mean I always believed in the fact without need of entangled evidence.\n\nI believe that 2 + 2 = 4, and I find it quite easy to conceive of a situation which would convince me that 2 + 2 = 3. Namely, the same sort of situation that currently convinces me that 2 + 2 = 4. Thus I do not fear that I am a victim of blind faith.^[5](#fn5x10)^\n\n^[1](#fn1x10-bk)^See [_Map and Territory_](https://www.lesswrong.com/rationality/).\n\n^[2](#fn2x10-bk)^Comment: [http://lesswrong.com/lw/jl/what\\_is\\_evidence/f7h](http://lesswrong.com/lw/jl/what_is_evidence/f7h).\n\n^[3](#fn3x10-bk)^See “[Your Strength as a Rationalist](https://www.lesswrong.com/rationality/your-strength-as-a-rationalist)” in _Map and Territory_.\n\n^[4](#fn4x10-bk)^For more on belief formation and beliefs of fact, see “[Feeling Rational](https://www.lesswrong.com/rationality/feeling-rational)” and “[What Is Evidence?](https://www.lesswrong.com/rationality/what-is-evidence)” in _Map and Territory_. For more on belief complexity, see “[Occam’s Razor](https://www.lesswrong.com/rationality/occam-s-razor)” in the same volume.\n\n^[5](#fn5x10-bk)^If there are any Christians reading this who know Bayes’s Theorem, might I inquire of you what situation would convince you of the truth of Islam? Presumably it would be the same sort of situation causally responsible for producing your current belief in Christianity: We would push you screaming out of the uterus of a Muslim woman, and have you raised by Muslim parents who continually told you that it is good to believe unconditionally in Islam.\n\nOr is there more to it than that? If so, what situation would convince you of Islam, or at least, non-Christianity? And how confident are you that the general kinds of evidence and reasoning you appeal to would have been enough to dissuade you of your religion if you had been raised a Muslim?"
          },
          "voteCount": 118
        },
        {
          "name": "Occam's Razor",
          "type": "post",
          "slug": "occam-s-razor",
          "_id": "f4txACqDWithRi7hs",
          "url": null,
          "title": "Occam's Razor",
          "author": "Eliezer Yudkowsky",
          "question": false,
          "tags": [
            {
              "name": "Occam's Razor"
            },
            {
              "name": "Principles"
            }
          ],
          "tableOfContents": null,
          "contents": {
            "markdown": "The more complex an explanation is, the more evidence you need just to find it in belief-space. (In Traditional Rationality this is often phrased misleadingly, as “The more complex a proposition is, the more evidence is required to argue for it.”) How can we measure the complexity of an explanation? How can we determine how much evidence is required?\n\nOccam’s Razor is often phrased as “The simplest explanation that fits the facts.” Robert Heinlein replied that the simplest explanation is “The lady down the street is a witch; she did it.”\n\nOne observes that the length of an English sentence is not a good way to measure “complexity.” And “fitting” the facts by merely *failing to prohibit* them is insufficient.\n\nWhy, exactly, is the length of an English sentence a poor measure of complexity? Because when you speak a sentence aloud, you are using *labels* for concepts that the listener shares—the receiver has already stored the complexity in them. Suppose we abbreviated Heinlein’s whole sentence as “Tldtsiawsdi!” so that the entire explanation can be conveyed in one word; better yet, we’ll give it a short arbitrary label like “Fnord!” Does this reduce the complexity? No, because you have to tell the listener in advance that “Tldtsiawsdi!” stands for “The lady down the street is a witch; she did it.” “Witch,” itself, is a label for some extraordinary assertions—just because we all know what it means doesn’t mean the concept is simple.\n\nAn enormous bolt of electricity comes out of the sky and hits something, and the Norse tribesfolk say, “Maybe a really powerful agent was angry and threw a lightning bolt.” The human brain is the most complex artifact in the known universe. If *anger* seems simple, it’s because we don’t see all the neural circuitry that’s implementing the emotion. (Imagine trying to explain why *Saturday Night Live* is funny, to an alien species with no sense of humor. But don’t feel superior; you yourself have no sense of fnord.) The complexity of anger, and indeed the complexity of intelligence, was glossed over by the humans who hypothesized Thor the thunder-agent.\n\n*To a human,* Maxwell’s equations take much longer to explain than Thor. Humans don’t have a built-in vocabulary for calculus the way we have a built-in vocabulary for anger. You’ve got to explain your language, and the language behind the language, and the very concept of mathematics, before you can start on electricity.\n\nAnd yet it seems that there should be some sense in which Maxwell’s equations are *simpler* than a human brain, or Thor the thunder-agent.\n\nThere is. It’s *enormously* easier (as it turns out) to write a computer program that simulates Maxwell’s equations, compared to a computer program that simulates an intelligent emotional mind like Thor.\n\nThe formalism of Solomonoff induction measures the “complexity of a description” by the length of the shortest computer program which produces that description as an output. To talk about the “shortest computer program” that does something, you need to specify a space of computer programs, which requires a language and interpreter. Solomonoff induction uses Turing machines, or rather, bitstrings that specify Turing machines. What if you don’t like Turing machines? Then there’s only a constant complexity penalty to design your own universal Turing machine that interprets whatever code you give it in whatever programming language you like. Different inductive formalisms are penalized by a worst-case constant factor relative to each other, corresponding to the size of a universal interpreter for that formalism.\n\nIn the better (in my humble opinion) versions of Solomonoff induction, the computer program does not produce a deterministic prediction, but assigns probabilities to strings. For example, we could write a program to explain a fair coin by writing a program that assigns equal probabilities to all 2^N^ strings of length N. This is Solomonoff induction’s approach to *fitting* the observed data. The higher the probability a program assigns to the observed data, the better that program *fits* the data. And probabilities must sum to 1, so for a program to better “fit” one possibility, it must steal probability mass from some other possibility which will then “fit” much more poorly. There is no superfair coin that assigns 100% probability to heads and 100% probability to tails.\n\nHow do we trade off the fit to the data, against the complexity of the program? If you ignore complexity penalties, and think *only* about fit, then you will always prefer programs that claim to deterministically predict the data, assign it 100% probability. If the coin shows HTTHHT, then the program that claims that the coin was fixed to show HTTHHT fits the observed data 64 times better than the program which claims the coin is fair. Conversely, if you ignore fit, and consider *only* complexity, then the “fair coin” hypothesis will always seem simpler than any other hypothesis. Even if the coin turns up HTHHTHHHTHHHHTHHHHHT  . . .\n\nIndeed, the fair coin *is* simpler and it fits this data exactly as well as it fits any other string of 20 coinflips—no more, no less—but we see another hypothesis, seeming not too complicated, that fits the data much better.\n\nIf you let a program store one more binary bit of information, it will be able to cut down a space of possibilities by half, and hence assign twice as much probability to all the points in the remaining space. This suggests that one bit of program complexity should cost *at least* a “factor of two gain” in the fit. If you try to design a computer program that explicitly stores an outcome like HTTHHT, the six bits that you lose in complexity must destroy all plausibility gained by a 64-fold improvement in fit. Otherwise, you will sooner or later decide that all fair coins are fixed.\n\nUnless your program is being smart, and *compressing* the data, it should do no good just to move one bit from the data into the program description.\n\nThe way Solomonoff induction works to predict sequences is that you sum up over all allowed computer programs—if every program is allowed, Solomonoff induction becomes uncomputable—with each program having a prior probability of 1/2 to the power of its code length in bits, and each program is further weighted by its fit to all data observed so far. This gives you a weighted mixture of experts that can predict future bits.\n\nThe Minimum Message Length formalism is nearly equivalent to Solomonoff induction. You send a string describing a code, and then you send a string describing the data in that code. Whichever explanation leads to the shortest *total* message is the best. If you think of the set of allowable codes as a space of computer programs, and the code description language as a universal machine, then Minimum Message Length is nearly equivalent to Solomonoff induction.[^1^](#fn1x26)\n\nThis lets us see clearly the problem with using “The lady down the street is a witch; she did it” to explain the pattern in the sequence 0101010101. If you’re sending a message to a friend, trying to describe the sequence you observed, you would have to say: “The lady down the street is a witch; she made the sequence come out 0101010101.” Your accusation of witchcraft wouldn’t let you *shorten* the rest of the message; you would still have to describe, in full detail, the data which her witchery caused.\n\nWitchcraft may fit our observations in the sense of qualitatively *permitting* them; but this is because witchcraft permits *everything* , like saying “Phlogiston!” So, even after you say “witch,” you still have to describe all the observed data in full detail. You have not *compressed the total length of the message describing your observations* by transmitting the message about witchcraft; you have simply added a useless prologue, increasing the total length.\n\nThe real sneakiness was concealed in the word “it” of “A witch did it.” A witch did *what*?\n\nOf course, thanks to [hindsight bias](https://lesswrong.com/lw/il/hindsight_bias/) and [anchoring](https://www.lesswrong.com/rationality/anchoring-and-adjustment) and [fake explanations](https://www.lesswrong.com/rationality/fake-explanations) and [fake causality](https://www.lesswrong.com/rationality/fake-causality) and [positive bias](https://www.lesswrong.com/rationality/positive-bias-look-into-the-dark) and [motivated cognition](https://www.lesswrong.com/rationality/knowing-about-biases-can-hurt-people), it may seem all too obvious that if a woman is a witch, of *course* she would make the coin come up 0101010101. But I’ll get to that soon enough. . .\n\n* * *\n\n^1^ Nearly, because it chooses the *shortest* program, rather than summing up over all programs."
          },
          "voteCount": 79
        },
        {
          "name": "The lens that sees its flaws",
          "type": "post",
          "slug": "the-lens-that-sees-its-flaws",
          "_id": "46qnWRSR7L2eyNbMA",
          "url": null,
          "title": "The Lens That Sees Its Flaws",
          "author": "Eliezer Yudkowsky",
          "question": false,
          "tags": [
            {
              "name": "Rationality"
            },
            {
              "name": "Gears-Level"
            },
            {
              "name": "Epistemology"
            },
            {
              "name": "Map and Territory"
            }
          ],
          "tableOfContents": null,
          "contents": {
            "markdown": "Light leaves the Sun and strikes your shoelaces and bounces off; some photons enter the pupils of your eyes and strike your retina; the energy of the photons triggers neural impulses; the neural impulses are transmitted to the visual-processing areas of the brain; and there the optical information is processed and reconstructed into a 3D model that is recognized as an untied shoelace; and so you believe that your shoelaces are untied.\n\nHere is the secret of *deliberate rationality—*this whole process is not [magic](https://www.lesswrong.com/lw/iu/mysterious_answers_to_mysterious_questions/), and you can *understand* it. You can *understand* how you see your shoelaces. You can *think* about which sort of thinking processes will create beliefs which mirror reality, and which thinking processes will not.\n\nMice can see, but they can’t understand seeing. *You* can understand seeing, and because of that, you can do things that mice cannot do. Take a moment to [marvel](https://www.lesswrong.com/lw/j3/science_as_curiositystopper/) at this, for it is indeed marvelous.\n\nMice see, but they don’t know they have visual cortexes, so they can’t correct for optical illusions. A mouse lives in a mental world that includes cats, holes, cheese and mousetraps—but not mouse brains. Their camera does not take pictures of its own lens. But we, as humans, can look at a [seemingly bizarre image](http://www.richrock.com/gifs/optical-illusion-wheels-circles-rotating.png), and realize that part of what we’re seeing is the lens itself. You don’t always have to believe your own eyes, but you have to realize that you *have* eyes—you must have distinct mental buckets for the map and the territory, for the senses and reality. Lest you think this a trivial ability, remember how rare it is in the animal kingdom.\n\nThe whole idea of Science is, simply, reflective reasoning about a more reliable process for making the contents of your mind mirror the contents of the world. It is the sort of thing mice would never invent. Pondering this business of “performing replicable experiments to falsify theories,” we can see *why* it works. Science is not a [separate magisterium](https://www.lesswrong.com/lw/i8/religions_claim_to_be_nondisprovable/), far away from real life and the understanding of ordinary mortals. Science is not something that only applies to the [inside of laboratories](https://www.lesswrong.com/lw/gv/outside_the_laboratory/). Science, itself, is an understandable process-in-the-world that correlates brains with reality.\n\nScience *makes sense*, when you think about it. But mice can’t think about thinking, which is why they don’t have Science. One should not overlook the wonder of this—or the potential power it bestows on us as individuals, not just scientific societies.\n\nAdmittedly, understanding the engine of thought may be *a little more complicated* than understanding a steam engine—but it is not a *fundamentally* different task.\n\nOnce upon a time, I went to EFNet’s #philosophy chatroom to ask, “Do you believe a nuclear war will occur in the next 20 years? If no, why not?” One person who answered the question said he didn’t expect a nuclear war for 100 years, because “All of the players involved in decisions regarding nuclear war are not interested right now.” “But why extend that out for 100 years?” I asked. “Pure hope,” was his reply.\n\nReflecting on this whole thought process, we can see why the thought of nuclear war makes the person unhappy, and we can see how his brain therefore rejects the belief. But if you imagine a billion worlds—Everett branches, or Tegmark duplicates^1^—this thought process will not [systematically correlate](https://www.lesswrong.com/lw/jl/what_is_evidence/) optimists to branches in which no nuclear war occurs.^2^\n\nTo ask which beliefs make you happy is to turn inward, not outward—it tells you something about yourself, but it is not evidence entangled with the environment. I have nothing against happiness, but it should follow from your picture of the world, rather than tampering with the mental paintbrushes.\n\nIf you can see this—if you can see that hope is shifting your *first-order* thoughts by too large a degree—if you can understand your mind as a mapping engine that has flaws—then you can apply a reflective correction. The brain is a flawed lens through which to see reality. This is true of both mouse brains and human brains. But a human brain is a flawed lens that can understand its own flaws—its systematic errors, its biases—and apply second-order corrections to them. This, *in practice,* makes the lens far more powerful. Not perfect, but far more powerful.\n\n* * *\n\n^1^ Max Tegmark, “Parallel Universes,” in *Science and Ultimate Reality: Quantum Theory,* *Cosmology, and Complexity*, ed. John D. Barrow, Paul C. W. Davies, and Charles L. Harper Jr. (New York: Cambridge University Press, 2004), 459–491, [http://arxiv.org/abs/astro-ph/0302131](http://arxiv.org/abs/astro-ph/0302131).\n\n^2^ Some clever fellow is bound to say, “Ah, but since I have hope, I'll work a little harder at my job, pump up the global economy, and thus help to prevent countries from sliding into the angry and hopeless state where nuclear war is a possibility. So the two events are related after all.” At this point, we have to drag in Bayes’s Theorem and measure the relationship quantitatively. Your optimistic nature cannot have *that* large an effect on the world; it cannot, of itself, decrease the probability of nuclear war by 20%, or however much your optimistic nature shifted your beliefs. Shifting your beliefs by a large amount, due to an event that only slightly increases your chance of being right, will still mess up your mapping."
          },
          "voteCount": 146
        }
      ]
    },
    {
      "title": "Selected posts from [[Mysterious Answers to Mysterious Questions]]",
      "description": "",
      "children": [
        {
          "name": "Making Beliefs Pay Rent (in Anticipated Experiences)",
          "type": "post",
          "slug": "making-beliefs-pay-rent-in-anticipated-experiences",
          "_id": "a7n8GdKiAZRX86T5A",
          "url": null,
          "title": "Making Beliefs Pay Rent (in Anticipated Experiences)",
          "author": "Eliezer Yudkowsky",
          "question": false,
          "tags": [
            {
              "name": "Rationality"
            },
            {
              "name": "Anticipated Experiences"
            },
            {
              "name": "Epistemology"
            },
            {
              "name": "Empiricism"
            },
            {
              "name": "Principles"
            }
          ],
          "tableOfContents": null,
          "contents": {
            "markdown": "Thus begins the ancient parable:\n\n*If a tree falls in a forest and no one hears it, does it make a sound? One says, “Yes it does, for it makes vibrations in the air.” Another says, “No it does not, for there is no auditory processing in any brain.”*\n\nIf there’s a foundational skill in the martial art of rationality, a mental stance on which all other technique rests, it might be this one: the ability to spot, inside your own head, psychological signs that you have a mental map of something, and signs that you don’t.\n\nSuppose that, after a tree falls, the two arguers walk into the forest together. Will one expect to see the tree fallen to the right, and the other expect to see the tree fallen to the left? Suppose that before the tree falls, the two leave a sound recorder next to the tree. Would one, playing back the recorder, expect to hear something different from the other? Suppose they attach an electroencephalograph to any brain in the world; would one expect to see a different trace than the other?\n\nThough the two argue, one saying “No,” and the other saying “Yes,” they do not anticipate any different experiences. The two think they have different models of the world, but they have no difference with respect to what they expect will *happen to* them; their maps of the world do not diverge in any sensory detail.\n\nIt’s tempting to try to eliminate this mistake class by insisting that the only legitimate kind of belief is an anticipation of sensory experience. But the world does, in fact, contain much that is not sensed directly. We don’t see the atoms underlying the brick, but the atoms are in fact there. There is a floor beneath your feet, but you don’t *experience* the floor directly; you see the light *reflected* from the floor, or rather, you see what your retina and visual cortex have processed of that light. To infer the floor from seeing the floor is to step back into the unseen causes of experience. It may seem like a very short and direct step, but it is still a step.\n\nYou stand on top of a tall building, next to a grandfather clock with an hour, minute, and ticking second hand. In your hand is a bowling ball, and you drop it off the roof. On which tick of the clock will you hear the crash of the bowling ball hitting the ground?\n\nTo answer precisely, you must use beliefs like *Earth’s gravity is 9.8 meters per second per second,* and *This building is around 120 meters tall.* These beliefs are not wordless anticipations of a sensory experience; they are verbal-ish, propositional. It probably does not exaggerate much to describe these two beliefs as sentences made out of words. But these two beliefs have an inferential *consequence* that is a direct sensory anticipation—if the clock’s second hand is on the 12 numeral when you drop the ball, you anticipate seeing it on the 1 numeral when you hear the crash five seconds later. To anticipate sensory experiences as precisely as possible, we must process beliefs that are not anticipations of sensory experience.\n\nIt is a great strength of *Homo sapiens* that we can, better than any other species in the world, learn to model the unseen. It is also one of our great weak points. Humans often believe in things that are not only unseen but unreal.\n\nThe same brain that builds a network of inferred causes behind sensory experience can also build a network of causes that is not connected to sensory experience, or poorly connected. Alchemists believed that phlogiston caused fire—we could simplistically model their minds by drawing a little node labeled “Phlogiston,” and an arrow from this node to their sensory experience of a crackling campfire—but this belief yielded no advance predictions; the link from phlogiston to experience was always configured after the experience, rather than constraining the experience in advance.\n\nOr suppose your English professor teaches you that the famous writer Wulky Wilkinsen is actually a “retropositional author,” which you can tell because his books exhibit “alienated resublimation.” And perhaps your professor knows all this because their professor told them; but all they're able to say about resublimation is that it's characteristic of retropositional thought, and of retropositionality that it's marked by alienated resublimation. What does this mean you should expect from Wulky Wilkinsen’s books?\n\nNothing. The belief, if you can call it that, doesn’t connect to sensory experience at all. But you had better remember the propositional assertions that “Wulky Wilkinsen” has the “retropositionality” attribute and also the “alienated resublimation” attribute, so you can regurgitate them on the upcoming quiz. The two beliefs are connected to each other, though still not connected to any anticipated experience.\n\nWe can build up whole networks of beliefs that are connected only to each other—call these “floating” beliefs. It is a uniquely human flaw among animal species, a perversion of *Homo sapiens*’s ability to build more general and flexible belief networks.\n\nThe rationalist virtue of *empiricism* consists of constantly asking which experiences our beliefs predict—or better yet, prohibit. Do you believe that phlogiston is the cause of fire? Then what do you expect to see happen, because of that? Do you believe that Wulky Wilkinsen is a retropositional author? Then what do you expect to see because of that? No, not “alienated resublimation”; *what experience will happen to you?* Do you believe that if a tree falls in the forest, and no one hears it, it still makes a sound? Then what experience must therefore befall you?\n\nIt is even better to ask: what experience *must not* happen to you? Do you believe that *Élan vital* explains the mysterious aliveness of living beings? Then what does this belief *not* allow to happen—what would definitely falsify this belief? A null answer means that your belief does not *constrain* experience; it permits *anything* to happen to you. It floats.\n\nWhen you argue a seemingly factual question, always keep in mind which difference of anticipation you are arguing about. If you can’t find the difference of anticipation, you’re probably arguing about labels in your belief network—or even worse, floating beliefs, barnacles on your network. If you don’t know what experiences are implied by Wulky Wilkinsens writing being retropositional, you can go on arguing forever.\n\nAbove all, don’t ask what to believe—ask what to anticipate. Every question of belief should flow from a question of anticipation, and that question of anticipation should be the center of the inquiry. Every guess of belief should begin by flowing to a specific guess of anticipation, and should continue to pay rent in future anticipations. If a belief turns deadbeat, evict it."
          },
          "voteCount": 277
        },
        {
          "name": "The Virtue of Narrowness",
          "type": "post",
          "slug": "the-virtue-of-narrowness",
          "_id": "yDfxTj9TKYsYiWH5o",
          "url": null,
          "title": "The Virtue of Narrowness",
          "author": "Eliezer Yudkowsky",
          "question": false,
          "tags": [
            {
              "name": "Virtues"
            },
            {
              "name": "Reductionism"
            }
          ],
          "tableOfContents": null,
          "contents": {
            "markdown": "> What is true of one apple may not be true of another apple; thus more can be said about a single apple than about all the apples in the world.\n> \n> —“The Twelve Virtues of Rationality”\n\nWithin their own professions, people grasp the importance of narrowness; a car mechanic knows the difference between a carburetor and a radiator, and would not think of them both as “car parts.” A hunter-gatherer knows the difference between a lion and a panther. A janitor does not wipe the floor with window cleaner, even if the bottles look similar to one who has not mastered the art.\n\nOutside their own professions, people often commit the misstep of trying to broaden a word as widely as possible, to cover as much territory as possible. Is it not more glorious, more wise, more impressive, to talk about *all* the apples in the world? How much loftier it must be to *explain human thought in general*, without being distracted by smaller questions, such as how humans invent techniques for solving a Rubik’s Cube. Indeed, it scarcely seems necessary to consider *specific* questions at all; isn’t a general theory a worthy enough accomplishment on its own?\n\nIt is the way of the curious to lift up one pebble from among a million pebbles on the shore, and see something new about it, something interesting, something different. You call these pebbles “diamonds,” and ask what might be special about them—what inner qualities they might have in common, beyond the glitter you first noticed. And then someone else comes along and says: “Why not call *this* pebble a diamond too? And this one, and this one?” They are enthusiastic, and they mean well. For it seems undemocratic and exclusionary and elitist and unholistic to call some pebbles “diamonds,” and others not. It seems . . . *narrow-minded . . .* if you’ll pardon the phrase. Hardly *open*, hardly *embracing*, hardly *communal.*\n\nYou might think it poetic, to give one word many meanings, and thereby spread shades of connotation all around. But even poets, if they are good poets, must learn to see the world precisely. It is not enough to compare love to a flower. Hot jealous unconsummated love is not the same as the love of a couple married for decades. If you need a flower to symbolize jealous love, you must go into the garden, and look, and make subtle distinctions—find a flower with a heady scent, and a bright color, and thorns. Even if your intent is to shade meanings and cast connotations, you must keep precise track of exactly which meanings you shade and connote.\n\nIt is a necessary part of the rationalist’s art—or even the poet’s art!—to focus narrowly on unusual pebbles which possess some special quality. And look at the details which those pebbles—and those pebbles alone!—share among each other. This is not a sin.\n\nIt is perfectly all right for modern evolutionary biologists to explain *just* the patterns of living creatures, and not the “evolution” of stars or the “evolution” of technology. Alas, some unfortunate souls use the same word “evolution” to cover the naturally selected patterns of replicating life, *and* the strictly accidental structure of stars, *and* the intelligently configured structure of technology. And as we all know, if people use the same word, it must all be the same thing. These biologists must just be too dumb to see the connections.\n\nAnd what could be more virtuous than seeing connections? Surely the wisest of all human beings are the New Age gurus who say, “Everything is connected to everything else.” If you ever say this aloud, you should pause, so that everyone can absorb the sheer shock of this Deep Wisdom.\n\nThere is a trivial mapping between a graph and its complement. A fully connected graph, with an edge between every two vertices, conveys the same amount of information as a graph with no edges at all. The important graphs are the ones where some things are *not* connected to some other things.\n\nWhen the unenlightened ones try to be profound, they draw endless verbal comparisons between this topic, and that topic, which is like this, which is like that; until their graph is fully connected and also totally useless. The remedy is specific knowledge and in-depth study. When you understand things in detail, you can see how they are *not* alike, and start enthusiastically subtracting edges *off* your graph.\n\nLikewise, the important categories are the ones that do not contain everything in the universe. Good hypotheses can only explain some possible outcomes, and not others.\n\nIt was perfectly all right for Isaac Newton to explain *just* gravity, *just* the way things fall down—and how planets orbit the Sun, and how the Moon generates the tides—but *not* the role of money in human society or how the heart pumps blood. Sneering at narrowness is rather reminiscent of ancient Greeks who thought that going out and actually *looking* at things was manual labor, and manual labor was for slaves.\n\nAs Plato put it in *The Republic, Book VII*:\n\n> If anyone should throw back his head and learn something by staring at the varied patterns on a ceiling, apparently you would think that he was contemplating with his reason, when he was only staring with his eyes . . . I cannot but believe that no study makes the soul look on high except that which is concerned with real being and the unseen. Whether he gape and stare upwards, or shut his mouth and stare downwards, if it be things of the senses that he tries to learn something about, I declare he never could learn, for none of these things admit of knowledge: I say his soul is looking down, not up, even if he is floating on his back on land or on sea!\n\nMany today make a similar mistake, and think that narrow concepts are as lowly and unlofty and unphilosophical as, say, going out and looking at things—an endeavor only suited to the underclass. But rationalists—and also poets—need narrow words to express precise thoughts; they need categories that include only some things, and exclude others. There’s nothing wrong with focusing your mind, narrowing your categories, excluding possibilities, and sharpening your propositions. Really, there isn’t! If you make your words too broad, you end up with something that isn’t true and doesn’t even make good poetry.\n\n*And DON’T EVEN GET ME STARTED on people who think Wikipedia is an “Artificial Intelligence,” the invention of LSD was a “Singularity,” or that corporations are “superintelligent”!*"
          },
          "voteCount": 93
        },
        {
          "name": "Hindsight Bias",
          "type": "post",
          "slug": "hindsight-bias",
          "_id": "fkM9XsNvXdYH6PPAx",
          "url": null,
          "title": "Hindsight bias",
          "author": null,
          "question": false,
          "tags": [
            {
              "name": "Heuristics & Biases"
            },
            {
              "name": "Rationality"
            },
            {
              "name": "Hindsight Bias"
            }
          ],
          "tableOfContents": null,
          "contents": {
            "markdown": "_Hindsight bias_ is when people who know the answer vastly overestimate its _predictability_ or _obviousness,_ compared to the estimates of subjects who must guess without advance knowledge.  Hindsight bias is sometimes called the _I-knew-it-all-along effect_.\n\nFischhoff and Beyth (1975) presented students with historical accounts of unfamiliar incidents, such as a conflict between the Gurkhas and the British in 1814.  Given the account as background knowledge, five groups of students were asked what they would have predicted as the probability for each of four outcomes: British victory, Gurkha victory, stalemate with a peace settlement, or stalemate with no peace settlement.  Four experimental groups were respectively told that these four outcomes were the historical outcome.  The fifth, control group was not told any historical outcome.  In every case, a group told an outcome assigned substantially higher probability to that outcome, than did any other group or the control group.\n\nHindsight bias matters in legal cases, where a judge or jury must determine whether a defendant was legally negligent in failing to foresee a hazard (Sanchiro 2003). In an experiment based on an actual legal case, Kamin and Rachlinski (1995) asked two groups to estimate the probability of flood damage caused by blockage of a city-owned drawbridge. The control group was told only the background information known to the city when it decided not to hire a bridge watcher. The experimental group was given this information, plus the fact that a flood had actually occurred. Instructions stated the city was negligent if the foreseeable probability of flooding was greater than 10%. 76% of the control group concluded the flood was so unlikely that no precautions were necessary; 57% of the experimental group concluded the flood was so likely that failure to take precautions was legally negligent. A third experimental group was told the outcome andalso explicitly instructed to avoid hindsight bias, which made no difference: 56% concluded the city was legally negligent.\n\nViewing history through the lens of hindsight, we vastly underestimate the cost of effective safety precautions.  In 1986, the _Challenger_ exploded for reasons traced to an O-ring losing flexibility at low temperature.  There were warning signs of a problem with the O-rings.  But preventing the _Challenger_ disaster would have required, not attending to the problem with the O-rings, but attending to _every_ warning sign which seemed as severe as the O-ring problem, _without benefit of hindsight_.  It could have been done, but it would have required a _general policy_ much more expensive than just fixing the O-Rings.\n\nShortly after September 11th 2001, I thought to myself, _and now someone will turn up minor intelligence warnings of something-or-other, and then the hindsight will begin._  Yes, I'm sure they had some minor warnings of an al Qaeda plot, but they probably also had minor warnings of mafia activity, nuclear material for sale, and an invasion from Mars.\n\nBecause we don't see the cost of a general policy, we learn overly specific lessons.  After September 11th, the FAA prohibited box-cutters on airplanes—as if the problem had been the failure to take _this particular_ \"obvious\" precaution.  We don't learn the general lesson: _the cost of effective caution is very high because you must attend to problems that are not as obvious now as past problems seem in hindsight._\n\nThe test of a model is how much probability it assigns to the observed outcome.  Hindsight bias systematically distorts this test; we think our model assigned much more probability than it actually did.  Instructing the jury doesn't help.  You have to [write down your predictions in advance](http://www.overcomingbias.com/2007/08/conservation-of.html).  Or as Fischhoff (1982) put it:\n\n> When we attempt to understand past events, we implicitly test the hypotheses or rules we use both to interpret and to anticipate the world around us. If, in hindsight, we systematically underestimate the surprises that the past held and holds for us, we are subjecting those hypotheses to inordinately weak tests and, presumably, finding little reason to change them.\n\nPart of the sequence [_Mysterious Answers to Mysterious Questions_](http://wiki.lesswrong.com/wiki/Mysterious_Answers_to_Mysterious_Questions)\n\nNext post: \"[Hindsight Devalues Science](/lw/im/hindsight_devalues_science/)\"\n\nPrevious post: \"[Conservation of Expected Evidence](/lw/ii/conservation_of_expected_evidence/)\"\n\n* * *\n\nFischhoff, B. 1982. For those condemned to study the past: Heuristics and biases in hindsight. In Kahneman et. al. 1982: 332–351.\n\nFischhoff, B., and Beyth, R. 1975. I knew it would happen: Remembered probabilities of once-future things. Organizational Behavior and Human Performance, 13: 1-16.\n\nKamin, K. and Rachlinski, J. 1995. [Ex Post ≠ Ex Ante: Determining Liability in Hindsight](http://www.jstor.org/view/01477307/ap050075/05a00120/0). Law and Human Behavior, 19(1): 89-104.\n\nSanchiro, C. 2003. Finding Error. Mich. St. L. Rev. 1189."
          },
          "voteCount": 63
        },
        {
          "name": "Fake Explanations",
          "type": "post",
          "slug": "fake-explanations",
          "_id": "fysgqk4CjAwhBgNYT",
          "url": null,
          "title": "Fake Explanations",
          "author": "Eliezer Yudkowsky",
          "question": false,
          "tags": [
            {
              "name": "Anticipated Experiences"
            },
            {
              "name": "Rationality"
            }
          ],
          "tableOfContents": null,
          "contents": {
            "markdown": "Once upon a time, there was an instructor who taught physics students. One day the instructor called them into the classroom and showed them a wide, square plate of metal, next to a hot radiator. The students each put their hand on the plate and found the side next to the radiator cool, and the distant side warm. And the instructor said, *Why do you think this happens?* Some students guessed convection of air currents, and others guessed strange metals in the plate. They devised many creative explanations, none stooping so low as to say “I don’t know” or “This seems impossible.”\n\nAnd the answer was that before the students entered the room, the instructor turned the plate around.^1^\n\nConsider the student who frantically stammers, “Eh, maybe because of the heat conduction and so?” I ask: Is this answer a proper belief? The words are easily enough professed—said in a loud, emphatic voice. But do the words actually control anticipation?\n\nPonder that innocent little phrase, “because of,” which comes before “heat conduction.” Ponder some of the *other* things we could put after it. We could say, for example, “Because of phlogiston,” or “Because of magic.”\n\n“Magic!” you cry. “That’s not a *scientific* explanation!” Indeed, the phrases “because of heat conduction” and “because of magic” are readily recognized as belonging to different *literary genres.* “Heat conduction” is something that Spock might say on *Star Trek*, whereas “magic” would be said by Giles in *Buffy the Vampire Slayer*.\n\nHowever, as Bayesians, we take no notice of literary genres. For us, the substance of a model is the control it exerts on anticipation. If you say “heat conduction,” what experience does that lead you to *anticipate*? Under normal circumstances, it leads you to anticipate that, if you put your hand on the side of the plate near the radiator, that side will feel warmer than the opposite side. If “because of heat conduction” can also explain the radiator-adjacent side feeling *cooler*, then it can explain pretty much *anything.*\n\nAnd as we all know by this point (I do hope), if you are equally good at explaining any outcome, you have zero knowledge. “Because of heat conduction,” used in such fashion, is a disguised hypothesis of maximum entropy. It is anticipation-isomorphic to saying “magic.” It feels like an explanation, but it’s not.\n\nSuppose that instead of guessing, we measured the heat of the metal plate at various points and various times. Seeing a metal plate next to the radiator, we would ordinarily expect the point temperatures to satisfy an equilibrium of the diffusion equation with respect to the boundary conditions imposed by the environment. You might not know the exact temperature of the first point measured, but after measuring the first points—I’m not physicist enough to know how many would be required—you could take an excellent guess at the rest.\n\nA true master of the art of using numbers to constrain the anticipation of material phenomena—a “physicist”—would take some measurements and say, “This plate was in equilibrium with the environment two and a half minutes ago, turned around, and is now approaching equilibrium again.”\n\nThe deeper error of the students is not simply that they failed to constrain anticipation. Their deeper error is that they thought they were doing physics. They said the phrase “because of,” followed by the sort of words Spock might say on *Star Trek*, and thought they thereby entered the magisterium of science.\n\nNot so. They simply moved their magic from one literary genre to another.\n\n* * *\n\n^1^ Joachim Verhagen, *Science Jokes*, 2001, [http://web.archive.org/web/20060424082937/http://www.nvon.nl/scheik/best/diversen/scijokes/scijokes.txt](http://web.archive.org/web/20060424082937/http://www.nvon.nl/scheik/best/diversen/scijokes/scijokes.txt)"
          },
          "voteCount": 120
        },
        {
          "name": "Guessing the Teacher's Password",
          "type": "post",
          "slug": "guessing-the-teacher-s-password",
          "_id": "NMoLJuDJEms7Ku9XS",
          "url": null,
          "title": "Guessing the Teacher's Password",
          "author": "Eliezer Yudkowsky",
          "question": false,
          "tags": [
            {
              "name": "Problem-solving (skills and techniques)"
            },
            {
              "name": "Anticipated Experiences"
            },
            {
              "name": "Education"
            }
          ],
          "tableOfContents": null,
          "contents": {
            "markdown": "When I was young, I read popular physics books such as Richard Feynman’s *QED: The Strange Theory of Light and Matter*. I knew that light was waves, sound was waves, matter was waves. I took pride in my scientific literacy, when I was nine years old.\n\nWhen I was older, and I began to read the *Feynman Lectures on Physics*, I ran across a gem called “the wave equation.” I could follow the equation’s derivation, but, [looking back](http://www.math.utah.edu/~pa/math/polya.html), I couldn’t see its truth at a glance. So I thought about the wave equation for three days, on and off, until I saw that it was embarrassingly obvious. And when I finally understood, I realized that the whole time I had accepted the honest assurance of physicists that light was waves, sound was waves, matter was waves, I had not had the vaguest idea of what the word “wave” meant to a physicist.\n\nThere is an instinctive tendency to think that if a physicist says “light is made of waves,” and the teacher says “What is light made of?” and the student says “Waves!”, then the student has made a true statement. That’s only fair, right? We accept “waves” as a correct answer from the physicist; wouldn’t it be unfair to reject it from the student? Surely, the answer “Waves!” is either *true* or *false*, right?\n\nWhich is one more bad habit to [unlearn from school](http://lesswrong.com/lw/i2/two_more_things_to_unlearn_from_school/). Words do not have intrinsic definitions. If I hear the syllables “bea-ver” and think of a large rodent, that is a fact about my own state of mind, not a fact about the syllables “bea-ver.” The sequence of syllables “made of waves” (or “because of heat conduction”) is not a *hypothesis*; it is a pattern of vibrations traveling through the air, or ink on paper. It can *associate* to a hypothesis in someone’s mind, but it is not, of itself, right or wrong. But in school, the teacher hands you a gold star for *saying* “made of waves,” which must be the correct answer because the teacher heard a physicist emit the same sound-vibrations. Since verbal behavior (spoken or written) is what gets the gold star, students begin to think that verbal behavior has a truth-value. After all, either light is made of waves, or it isn’t, right?\n\nAnd this leads into an even worse habit. Suppose the teacher asks you why the far side of a metal plate feels warmer than the side next to the radiator. If you say “I don’t know,” you have *no* chance of getting a gold star—it won’t even count as class participation. But, during the current semester, this teacher has used the phrases “because of heat convection,” “because of heat conduction,” and “because of radiant heat.” One of these is probably what the teacher wants. You say, “Eh, maybe because of heat conduction?”\n\nThis is not a hypothesis *about* the metal plate. This is not even a proper belief. It is an attempt to *guess the teacher’s password.*\n\nEven visualizing the symbols of the diffusion equation (the math governing heat conduction) doesn’t mean you’ve formed a hypothesis *about* the metal plate. This is not school; we are not testing your memory to see if you can write down the diffusion equation. This is Bayescraft; we are scoring your anticipations of experience. If you *use* the diffusion equation, by measuring a few points with a thermometer and then trying to predict what the thermometer will say on the next measurement, then it is definitely connected to experience. Even if the student just visualizes something *flowing*, and therefore holds a match near the cooler side of the plate to try to measure where the heat goes, then this mental image of flowing-ness connects to experience; it controls anticipation.\n\nIf you aren’t *using* the diffusion equation—putting in numbers and getting out results that control your anticipation of particular experiences—then the connection between map and territory is severed as though by a knife. What remains is not a belief, but a verbal behavior.\n\nIn the school system, it’s all about verbal behavior, whether written on paper or spoken aloud. Verbal behavior gets you a gold star or a failing grade. Part of unlearning this bad habit is becoming consciously aware of the difference between an explanation and a password.\n\nDoes this seem too harsh? When you’re faced by a confusing metal plate, can’t “heat conduction?” be a first step toward finding the answer? Maybe, but only if you don’t fall into the trap of thinking that you are looking for a password. What if there is no teacher to tell you that you failed? Then you may think that “Light is wakalixes” is a good explanation, that “wakalixes” is the correct password. It happened to me when I was nine years old—not because I was stupid, but because this is what happens *by default.* This is how human beings think, unless they are trained *not* to fall into the trap. Humanity stayed stuck in holes like this for thousands of years.\n\nMaybe, if we drill students that *words don’t count, only anticipation-controllers,* the student will *not* get stuck on “Heat conduction? No? Maybe heat convection? That’s not it either?” Maybe *then*, thinking the phrase “heat conduction” will lead onto a genuinely helpful path, like:\n\n*   “Heat conduction?”\n*   But that’s only a phrase—what does it mean?\n*   The diffusion equation?\n*   But those are only symbols—how do I apply them?\n*   What does applying the diffusion equation lead me to anticipate?\n*   It sure doesn’t lead me to anticipate that the side of a metal plate farther away from a radiator would feel warmer.\n*   I notice that I am confused. Maybe the near side just *feels* cooler, because it’s made of more insulative material and transfers less heat to my hand? I’ll try measuring the temperature . . .\n*   Okay, that wasn’t it. Can I try to verify whether the diffusion equation holds true of this metal plate, at all? Is heat *flowing* the way it usually does, or is something else going on?\n*   I could hold a match to the plate and try to measure how heat spreads over time . . .\n\nIf we are *not* strict about “Eh, maybe because of heat conduction?” being a fake explanation, the student will very probably get stuck on some wakalixes-password. *This happens by default: it happened to the whole human species for thousands of years.*"
          },
          "voteCount": 178
        },
        {
          "name": "Fake Causality",
          "type": "post",
          "slug": "fake-causality",
          "_id": "RgkqLqkg8vLhsYpfh",
          "url": null,
          "title": "Fake Causality",
          "author": "Eliezer Yudkowsky",
          "question": false,
          "tags": [
            {
              "name": "Causality"
            }
          ],
          "tableOfContents": null,
          "contents": {
            "markdown": "Phlogiston was the eighteenth century’s answer to the Elemental Fire of the Greek alchemists. Ignite wood, and let it burn. What is the orangey-bright “fire” stuff? Why does the wood transform into ash? To both questions, the eighteenth-century chemists answered, “phlogiston.”\n\n. . . and that was it, you see, that was their answer: “Phlogiston.”\n\nPhlogiston escaped from burning substances as visible fire. As the phlogiston escaped, the burning substances lost phlogiston and so became ash, the “true material.” Flames in enclosed containers went out because the air became saturated with phlogiston, and so could not hold any more. Charcoal left little residue upon burning because it was nearly pure phlogiston.\n\nOf course, one didn’t use phlogiston theory to *predict* the outcome of a chemical transformation. You looked at the result first, then you used phlogiston theory to *explain* it. It’s not that phlogiston theorists predicted a flame would extinguish in a closed container; rather they lit a flame in a container, watched it go out, and then said, “The air must have become saturated with phlogiston.” You couldn’t even use phlogiston theory to say what you ought *not* to see; it could explain everything.\n\nThis was an earlier age of science. For a long time, no one realized there was a problem. Fake explanations don’t *feel* fake. That’s what makes them dangerous.\n\nModern research suggests that humans think about cause and effect using something like the directed acyclic graphs (DAGs) of Bayes nets. Because it rained, the sidewalk is wet; because the sidewalk is wet, it is slippery:\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/8f32ffc1c38cd1e27bb9704309d0b2bf863bb7270952b46c.png)\n\nFrom this we can infer—or, in a Bayes net, rigorously calculate in probabilities—that when the sidewalk is slippery, it probably rained; but if we already know that the sidewalk is wet, learning that the sidewalk is slippery tells us nothing more about whether it rained.\n\nWhy is fire hot and bright when it burns?\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/8ff736bb4279ee44938bc54a05d6849665b3433c7bc69ab2.png)\n\nIt *feels* like an explanation. It’s *represented* using the same cognitive data format. But the human mind does not automatically detect when a cause has an unconstraining arrow to its effect. Worse, thanks to hindsight bias, it may feel like the cause constrains the effect, when it was merely [fitted](https://www.lesswrong.com/rationality/conservation-of-expected-evidence) to the effect.\n\nInterestingly, our modern understanding of probabilistic reasoning about causality can describe precisely what the phlogiston theorists were doing wrong. One of the primary inspirations for Bayesian networks was noticing the problem of double-counting evidence if inference resonates between an effect and a cause. For example, let’s say that I get a bit of unreliable information that the sidewalk is wet. This should make me think it’s more likely to be raining. But, if it’s more likely to be raining, doesn’t that make it more likely that the sidewalk is wet? And wouldn’t *that* make it more likely that the sidewalk is slippery? But if the sidewalk is slippery, it’s probably wet; and then I should again raise my probability that it’s raining . . .\n\nJudea Pearl uses the metaphor of an algorithm for counting soldiers in a line. Suppose you’re in the line, and you see two soldiers next to you, one in front and one in back. That’s three soldiers, including you. So you ask the soldier behind you, “How many soldiers do *you* see?” They look around and say, “Three.” So that’s a total of six soldiers. This, obviously, is *not* how to do it.\n\nA smarter way is to ask the soldier in front of you, “How many soldiers forward of you?” and the soldier in back, “How many soldiers backward of you?” The question “How many soldiers forward?” can be passed on as a message without confusion. If I’m at the front of the line, I pass the message “1 soldier forward,” for myself. The person directly in back of me gets the message “1 soldier forward,” and passes on the message “2 soldiers forward” to the soldier behind them. At the same time, each soldier is also getting the message “N soldiers backward” from the soldier behind them, and passing it on as “N + 1 soldiers backward” to the soldier in front of them. How many soldiers in total? Add the two numbers you receive, plus one for yourself: that is the total number of soldiers in line.\n\nThe key idea is that every soldier must *separately* track the two messages, the forward-message and backward-message, and add them together only at the end. You never add any soldiers from the backward-message you receive to the forward-message you pass back. Indeed, the total number of soldiers is never passed as a message—no one ever says it aloud.\n\nAn analogous principle operates in rigorous probabilistic reasoning about causality. If you learn something about whether it’s raining, from some source *other* than observing the sidewalk to be wet, this will send a forward-message from \\[Rain\\] to \\[Sidewalk Wet\\] and raise our expectation of the sidewalk being wet. If you observe the sidewalk to be wet, this sends a backward-message to our belief that it is raining, and this message propagates from \\[Rain\\] to all neighboring nodes *except* the \\[Sidewalk Wet\\] node. We count each piece of evidence exactly once; no update message ever “bounces” back and forth. The exact algorithm may be found in Judea Pearl’s classic *Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference*.\n\nSo what went wrong in phlogiston theory? When we observe that fire is hot and bright, the \\[Fire Hot and Bright\\] node can send backward-evidence to the \\[Phlogiston\\] node, leading us to update our beliefs about phlogiston. But if so, we can’t count this as a successful forward-prediction of phlogiston theory. The message should go in only one direction, and not bounce back.\n\nAlas, human beings do not use a rigorous algorithm for updating belief networks. We learn about parent nodes from observing children, and predict child nodes from beliefs about parents. But we don’t keep rigorously separate books for the backward-message and forward-message. We just remember that phlogiston is hot, which *causes* fire to be hot. So it seems like phlogiston theory predicts the hotness of fire. Or, worse, it just feels like *phlogiston makes the fire hot.*\n\nUntil you notice that no *advance* predictions are being made, the non-constraining causal node is not labeled “fake.” It’s represented the same way as any other node in your belief network. It feels like a fact, like all the other facts you know: *Phlogiston makes the fire hot.*\n\nA properly designed AI would notice the problem instantly. This wouldn’t even require special-purpose code, just correct bookkeeping of the belief network. (Sadly, we humans can’t rewrite our own code, the way a properly designed AI could.)\n\nSpeaking of “hindsight bias” is just the nontechnical way of saying that humans do not rigorously separate forward and backward messages, allowing forward messages to be contaminated by backward ones.\n\nThose who long ago went down the path of phlogiston were not trying to be fools. No scientist deliberately wants to get stuck in a blind alley. Are there any fake explanations in *your* mind? If there are, I guarantee they’re not labeled “fake explanation,” so polling your thoughts for the “fake” keyword will not turn them up.\n\nThanks to hindsight bias, it’s also not enough to check how well your theory “predicts” facts you already know. You’ve got to predict for tomorrow, not yesterday. It’s the only way a messy human mind can be guaranteed of sending a pure forward message."
          },
          "voteCount": 89
        },
        {
          "name": "Mysterious Answers to Mysterious Questions",
          "type": "post",
          "slug": "mysterious-answers-to-mysterious-questions",
          "_id": "6i3zToomS86oj9bS6",
          "url": null,
          "title": "Mysterious Answers to Mysterious Questions",
          "author": "Eliezer Yudkowsky",
          "question": false,
          "tags": [
            {
              "name": "Reductionism"
            },
            {
              "name": "Mind Projection Fallacy"
            },
            {
              "name": "Rationality"
            },
            {
              "name": "Map and Territory"
            }
          ],
          "tableOfContents": null,
          "contents": {
            "markdown": "Imagine looking at your hand, and knowing nothing of cells, nothing of biochemistry, nothing of DNA. You’ve learned some anatomy from dissection, so you know your hand contains muscles; but you don’t know why muscles move instead of lying there like clay. Your hand is just . . . stuff . . . and for some reason it moves under your direction. Is this not magic?\n\n> It seemed to me then, and it still seems to me, most probable that the animal body does not act as a thermodynamic engine . . . The influence of animal or vegetable life on matter is infinitely beyond the range of any scientific inquiry hitherto entered on. Its power of directing the motions of moving particles, in the demonstrated daily miracle of our human free-will, and in the growth of generation after generation of plants from a single seed, are infinitely different from any possible result of the fortuitous concourse of atoms\\[.\\]^1^\n> \n> \\[C\\]onsciousness teaches every individual that they are, to some extent, subject to the direction of his will. It appears, therefore, that animated creatures have the power of immediately applying, to certain moving particles of matter within their bodies, forces by which the motions of these particles are directed to produce desired mechanical effects.^2^\n> \n> Modern biologists are coming once more to a firm acceptance of something beyond mere gravitational, chemical, and physical forces; and that unknown thing is a vital principle.^3^\n> \n> —Lord Kelvin\n\nThis was the theory of *vitalism* ; that the mysterious difference between living matter and non-living matter was explained by an *Élan vital* or *vis vitalis*. *Élan vital* infused living matter and caused it to move as consciously directed. *Élan vital* participated in chemical transformations which no mere non-living particles could undergo—Wöhler’s later synthesis of urea, a component of urine, was a major blow to the vitalistic theory because it showed that mere *chemistry* could duplicate a product of biology.\n\nCalling “Élan vital” an explanation, even a fake explanation like phlogiston, is probably giving it too much credit. It functioned primarily as a curiosity-stopper. You said “Why?” and the answer was “Élan vital!”\n\nWhen you say “Élan vital!” it *feels* like you know why your hand moves. You have a little causal diagram in your head that says:\n\n![](https://res.cloudinary.com/lesswrong-2-0/image/upload/v1586123558/MysteriousAnswersToMysteriousQuestions_diagram_1_dr5tbq.svg)\n\nBut actually you know nothing you didn’t know before. You don’t know, say, whether your hand will generate heat or absorb heat, unless you have observed the fact already; if not, you won’t be able to predict it in advance. Your curiosity feels sated, but it hasn’t been fed. Since you can say “Why? Élan vital!” to any possible observation, it is equally good at explaining all outcomes, a disguised hypothesis of maximum entropy, et cetera.\n\nBut the greater lesson lies in the vitalists’ reverence for the *Élan vital*, their eagerness to pronounce it a mystery beyond all science. Meeting the great dragon Unknown, the vitalists did not draw their swords to do battle, but bowed their necks in submission. They took pride in their ignorance, made biology into a *sacred* mystery, and thereby became loath to [relinquish their ignorance](https://www.lesswrong.com/rationality/twelve-virtues-of-rationality) when evidence came knocking.\n\nThe Secret of Life was *infinitely beyond the reach of science!* Not just a *little* beyond, mind you, but *infinitely* beyond! Lord Kelvin sure did get a tremendous emotional kick out of *not knowing something.*\n\nBut ignorance exists in the map, not in the territory. If I am ignorant about a phenomenon, that is a fact about my own state of mind, not a fact about the phenomenon itself. A phenomenon can *seem* mysterious to some particular person. There are no phenomena which are mysterious of themselves. To worship a phenomenon because it seems so wonderfully mysterious is to worship your own ignorance.\n\nVitalism shared with phlogiston the error of *encapsulating the mystery as a substance.* Fire was mysterious, and the phlogiston theory encapsulated the mystery in a mysterious substance called “phlogiston.” Life was a sacred mystery, and vitalism encapsulated the sacred mystery in a mysterious substance called “Élan vital.” Neither answer helped concentrate the model’s probability density—helped make some outcomes easier to explain than others. The “explanation” just wrapped up the question as a small, hard, opaque black ball.\n\nIn a comedy written by Molière, a physician explains the power of a soporific by saying that it contains a “dormitive potency.” Same principle. It is a failure of human psychology that, faced with a mysterious phenomenon, we more readily postulate mysterious inherent substances than complex underlying processes.\n\nBut the deeper failure is supposing that an *answer* can be mysterious. If a phenomenon feels mysterious, that is a fact about our state of knowledge, not a fact about the phenomenon itself. The vitalists saw a mysterious gap in their knowledge, and postulated a mysterious stuff that plugged the gap. In doing so, they mixed up the map with the territory. All confusion and bewilderment exist in the mind, not in encapsulated substances.\n\nThis is the ultimate and fully general explanation for why, again and again in humanity’s history, people are shocked to discover that an incredibly mysterious question has a non-mysterious answer. Mystery is a property of questions, not answers.\n\nTherefore I call theories such as vitalism *mysterious answers to mysterious questions*.\n\nThese are the signs of mysterious answers to mysterious questions:\n\n*   First, the explanation acts as a curiosity-stopper rather than an anticipation-controller.\n*   Second, the hypothesis has no moving parts—the model is not a specific complex mechanism, but a blankly solid substance or force. The mysterious substance or mysterious force may be said to be here or there, to cause this or that; but the reason why the mysterious force behaves thus is wrapped in a blank unity.\n*   Third, those who proffer the explanation cherish their ignorance; they speak proudly of how the phenomenon defeats ordinary science or is unlike merely mundane phenomena.\n*   Fourth, *even after the answer is given, the phenomenon is still a mystery* and possesses the same quality of wonderful inexplicability that it had at the start.\n\n* * *\n\n^1^ Lord Kelvin, “On the Dissipation of Energy: Geology and General Physics,” in *Popular* *Lectures and Addresses, vol. ii* (London: Macmillan, 1894).\n\n^2^ Lord Kelvin, “On the Mechanical action of Heat or Light: On the Power of Animated Creatures over Matter: On the Sources available to Man for the production of Mechanical Effect,” *Proceedings of the Royal Society of Edinburgh* 3, no. 1 (1852): 108–113.\n\n^3^ Silvanus Phillips Thompson, *The Life of Lord Kelvin* (American Mathematical Society, 2005)."
          },
          "voteCount": 140
        },
        {
          "name": "The Futility of Emergence",
          "type": "post",
          "slug": "the-futility-of-emergence",
          "_id": "8QzZKw9WHRxjR4948",
          "url": null,
          "title": "The Futility of Emergence",
          "author": "Eliezer Yudkowsky",
          "question": false,
          "tags": [
            {
              "name": "Anticipated Experiences"
            },
            {
              "name": "Philosophy of Language"
            },
            {
              "name": "Gears-Level"
            },
            {
              "name": "Emergent Behavior"
            }
          ],
          "tableOfContents": null,
          "contents": {
            "markdown": "The failures of phlogiston and vitalism are historical hindsight. Dare I step out on a limb, and name some *current *theory which I deem analogously flawed?\n\nI name *emergence *or *emergent phenomena*—usually defined as the study of systems whose high-level behaviors arise or “emerge” from the interaction of many low-level elements. ([Wikipedia](http://en.wikipedia.org/wiki/Emergence): “The way complex systems and patterns arise out of a multiplicity of relatively simple interactions.”)\n\nTaken literally, that description fits every phenomenon in our universe above the level of individual quarks, which is part of the problem. Imagine pointing to a market crash and saying “It’s not a quark!” Does that feel like an explanation? No? Then neither should saying “It’s an emergent phenomenon!”\n\nIt’s the noun “emergence” that I protest, rather than the verb “emerges from.” There’s nothing wrong with saying “X emerges from Y,” where Y is some specific, detailed model with internal moving parts. “Arises from” is another legitimate phrase that means exactly the same thing. Gravity arises from the curvature of spacetime, according to the specific mathematical model of General Relativity. Chemistry arises from interactions between atoms, according to the specific model of quantum electrodynamics.\n\nNow suppose I should say that gravity depends on “arisence” or that chemistry is an “arising phenomenon,” and claim that as my explanation.\n\nThe phrase “emerges from” is acceptable, just like “arises from” or “is caused by” are acceptable, if the phrase precedes some specific model to be judged on its own merits.\n\nHowever, this is *not *the way “emergence” is commonly used. “Emergence” is commonly used as an explanation in its own right.\n\nI have lost track of how many times I have heard people say, “Intelligence is an emergent phenomenon!” as if that explained intelligence. This usage fits all the checklist items for a mysterious answer to a mysterious question. What do you know, after you have said that intelligence is “emergent”? You can make no new predictions. You do not know anything about the behavior of real-world minds that you did not know before. It feels like you believe a new fact, but you don’t anticipate any different outcomes. Your curiosity feels sated, but it has not been fed. The hypothesis has no moving parts—there’s no detailed internal model to manipulate. Those who proffer the hypothesis of “emergence” confess their ignorance of the internals, and take pride in it; they contrast the science of “emergence” to other sciences merely mundane.\n\nAnd even after the answer of “Why? Emergence!” is given, *the phenomenon is* *still a mystery *and possesses the same sacred impenetrability it had at the start.\n\nA fun exercise is to eliminate the adjective “emergent” from any sentence in which it appears, and see if the sentence says anything different:\n\n*   *Before:* Human intelligence is an emergent product of neurons firing.\n*   *After:* Human intelligence is a product of neurons firing.\n*   *Before:* The behavior of the ant colony is the emergent outcome of the interactions of many individual ants.\n*   *After:* The behavior of the ant colony is the outcome of the interactions of many individual ants.\n*   *Even better:* A colony is made of ants. We can successfully predict some aspects of colony behavior using models that include only individual ants, without any global colony variables, showing that we understand how those colony behaviors arise from ant behaviors.\n\nAnother fun exercise is to replace the word “emergent” with the old word, the explanation that people had to use before emergence was invented:\n\n*   *Before: *Life is an emergent phenomenon.\n*   *After: *Life is a magical phenomenon.\n*   *Before: *Human intelligence is an emergent product of neurons firing.\n*   *After: *Human intelligence is a magical product of neurons firing.\n\nDoes not each statement convey exactly the same amount of knowledge about the phenomenon’s behavior? Does not each hypothesis [fit exactly the same set of outcomes](https://lesswrong.com/rationality/your-strength-as-a-rationalist)?\n\n“Emergence” has become very popular, just as saying “magic” used to be very popular. “Emergence” has the same deep appeal to human psychology, for the same reason. “Emergence” is such a wonderfully easy explanation, and it feels good to say it; it gives you a sacred mystery to worship. Emergence is popular *because *it is the junk food of curiosity. You can explain anything using emergence, and so people do just that; for it feels so wonderful to explain things.\n\nHumans are still humans, even if they’ve taken a few science classes in college. Once they find a way to escape the shackles of settled science, they get up to the same shenanigans as their ancestors—dressed up in the literary genre of “science,” but humans are still humans, and human psychology is still human psychology."
          },
          "voteCount": 99
        },
        {
          "name": "Explain, Worship, Ignore",
          "type": "post",
          "slug": "explain-worship-ignore",
          "_id": "yxvi9RitzZDpqn6Yh",
          "url": null,
          "title": "Explain/Worship/Ignore?",
          "author": "Eliezer_Yudkowsky",
          "question": false,
          "tags": [
            {
              "name": "Rationality"
            },
            {
              "name": "Distinctions"
            }
          ],
          "tableOfContents": null,
          "contents": {
            "markdown": "As our tribe wanders through the grasslands, searching for fruit trees and prey, it happens every now and then that water pours down from the sky.\n\n“Why does water sometimes fall from the sky?” I ask the bearded wise man of our tribe.\n\nHe thinks for a moment, this question having never occurred to him before, and then says, “From time to time, the sky spirits battle, and when they do, their blood drips from the sky.”\n\n“Where do the sky spirits come from?” I ask.\n\nHis voice drops to a whisper. “From the before time. From the long long ago.”\n\nWhen it rains, and you don’t know why, you have several options. First, you could simply not ask why—not follow up on the question, or never think of the question in the first place. This is the Ignore command, which the bearded wise man originally selected. Second, you could try to devise some sort of explanation, the Explain command, as the bearded man did in response to your first question. Third, you could enjoy the sensation of mysteriousness—the Worship command.\n\nNow, as you are bound to notice from this story, each time you select Explain, the best-case scenario is that you get an explanation, such as “sky spirits.” But then this explanation itself is subject to the same dilemma—Explain, Worship, or Ignore? Each time you hit Explain, science grinds for a while, returns an explanation, and then another dialog box pops up. As good rationalists, we feel duty-bound to keep hitting Explain, but it seems like a road that has no end.\n\nYou hit Explain for life, and get chemistry; you hit Explain for chemistry, and get atoms; you hit Explain for atoms, and get electrons and nuclei; you hit Explain for nuclei, and get quantum chromodynamics and quarks; you hit Explain for how the quarks got there, and get back the Big Bang . . .\n\nWe can hit Explain for the Big Bang, and wait while science grinds through its process, and maybe someday it will return a perfectly good explanation. But then that will just bring up another dialog box. So, if we continue long enough, we must come to a _special_ dialog box, a _new_ option, an Explanation That Needs No Explanation, a place where the chain ends—and this, maybe, is the only explanation worth knowing.\n\nThere—I just hit Worship.\n\nNever forget that there are many more ways to worship something than lighting candles around an altar.\n\nIf I’d said, “Huh, that does seem paradoxical. I wonder how the apparent paradox is resolved?” then I would have hit Explain, which does sometimes take a while to produce an answer.\n\nAnd if the whole issue seems to you unimportant, or irrelevant, or if you’d rather put off thinking about it until tomorrow, than you have hit Ignore.\n\nSelect your option wisely."
          },
          "voteCount": 74
        }
      ]
    },
    {
      "title": "Selected posts from [[A Human's Guide to Words]]",
      "description": "\"[Academian] strongly recommend[s] looking at the whole sequence\"",
      "children": [
        {
          "name": "How an Algorithm Feels from Inside",
          "type": "post",
          "slug": "how-an-algorithm-feels-from-inside",
          "_id": "yA4gF5KrboK2m2Xu7",
          "url": null,
          "title": "How An Algorithm Feels From Inside",
          "author": "Eliezer_Yudkowsky",
          "question": false,
          "tags": [
            {
              "name": "Rationality"
            },
            {
              "name": "Philosophy of Language"
            },
            {
              "name": "Cognitive Reduction"
            }
          ],
          "tableOfContents": null,
          "contents": {
            "markdown": "\"If a tree falls in the forest, and no one hears it, does it make a sound?\"  I remember seeing an actual argument get started on this subject—a fully naive argument that went nowhere near Berkeleyan subjectivism.  Just:\n\n> \"It makes a sound, just like any other falling tree!\"  \n> \"But how can there be a sound that no one hears?\"\n\nThe standard rationalist view would be that the first person is speaking as if \"sound\" means acoustic vibrations in the air; the second person is speaking as if \"sound\" means an auditory experience in a brain.  If you ask \"Are there acoustic vibrations?\" or \"Are there auditory experiences?\", the answer is at once obvious.  And so the argument is really about the definition of the word \"sound\".\n\nI think the standard analysis is essentially correct.  So let's accept that as a premise, and ask:  Why do people get into such an argument?  What's the underlying psychology?\n\nA key idea of the heuristics and biases program is that mistakes are often more revealing of cognition than correct answers.  Getting into a heated dispute about whether, if a tree falls in a deserted forest, it makes a sound, is traditionally considered a mistake.\n\nSo what kind of mind design corresponds to that error?\n\nIn [Disguised Queries](/lw/nm/disguised_queries/) I introduced the blegg/rube classification task, in which Susan the Senior Sorter explains that your job is to sort objects coming off a conveyor belt, putting the blue eggs or \"bleggs\" into one bin, and the red cubes or \"rubes\" into the rube bin.  This, it turns out, is because bleggs contain small nuggets of vanadium ore, and rubes contain small shreds of palladium, both of which are useful industrially.\n\nExcept that around 2% of blue egg-shaped objects contain palladium instead.  So if you find a blue egg-shaped thing that contains palladium, should you call it a \"rube\" instead?  You're going to put it in the rube bin—why not call it a \"rube\"?\n\nBut when you switch off the light, nearly all bleggs glow faintly in the dark.  And blue egg-shaped objects that contain palladium are just as likely to glow in the dark as any other blue egg-shaped object.\n\nSo if you find a blue egg-shaped object that contains palladium, and you ask \"Is it a blegg?\", the answer depends on what you have to do with the answer:  If you ask \"Which bin does the object go in?\", then you choose as if the object is a rube.  But if you ask \"If I turn off the light, will it glow?\", you predict as if the object is a blegg.  In one case, the question \"Is it a blegg?\" stands in for the [disguised query](/lw/nm/disguised_queries/), \"Which bin does it go in?\".  In the other case, the question \"Is it a blegg?\" stands in for the [disguised query](/lw/nm/disguised_queries/), \"Will it glow in the dark?\"\n\nNow suppose that you have an object that is blue and egg-shaped and contains palladium; and you have already observed that it is furred, flexible, opaque, and glows in the dark.\n\nThis answers _every_ query, observes every observable introduced.  There's nothing left for a disguised query to stand _for._\n\nSo why might someone feel an impulse to go on arguing whether the object is _really_ a blegg?\n\n[![Blegg3](/static/imported/2008/02/10/blegg3.png \"Blegg3\")](/static/imported/2008/02/10/blegg3.png)\n\nThis diagram from [Neural Categories](/lw/nn/neural_categories/) shows two different neural networks that might be used to answer questions about bleggs and rubes.  Network 1 has a number of disadvantages—such as potentially oscillating/chaotic behavior, or requiring O(N^2^) connections—but Network 1's structure does have one major advantage over Network 2:  Every unit in the network corresponds to a testable query.  If you observe every observable, clamping every value, there are no units in the network left over.\n\nNetwork 2, however, is a far better candidate for being something vaguely like how the human brain works:  It's fast, cheap, scalable—and has an extra dangling unit in the center, whose activation can still vary, even after we've observed every single one of the surrounding nodes.\n\nWhich is to say that even after you know whether an object is blue or red, egg or cube, furred or smooth, bright or dark, and whether it contains vanadium or palladium, it _feels_ like there's a leftover, unanswered question:  _But is it really a blegg?_\n\nUsually, in our daily experience, acoustic vibrations and auditory experience go together.  But a tree falling in a deserted forest unbundles this common association.  And even after you know that the falling tree creates acoustic vibrations but not auditory experience, it _feels_ like there's a leftover question:  _Did it make a sound?_  \n  \nWe know where Pluto is, and where it's going; we know Pluto's shape, and Pluto's mass—but is it a planet?\n\nNow remember:  When you look at Network 2, as I've laid it out here, you're seeing the algorithm from the outside.  People don't think to themselves, \"Should the central unit fire, or not?\" any more than you think \"Should neuron #12,234,320,242 in my visual cortex fire, or not?\"\n\nIt takes a deliberate effort to visualize your brain from the outside—and then you still don't see your actual brain; you imagine what you _think_ is there, hopefully based on science, but regardless, you don't have any direct access to neural network structures from introspection.  That's why the ancient Greeks didn't invent computational neuroscience.\n\nWhen you look at Network 2, you are seeing from the _outside;_ but the way that neural network structure feels from the _inside,_ if you yourself _are_ a brain running that algorithm, is that even after you know every characteristic of the object, you still find yourself wondering:  \"But is it a blegg, or not?\"\n\nThis is a great gap to cross, and I've seen it stop people in their tracks.  Because we don't instinctively see our intuitions as \"intuitions\", we just see them as the world.  When you look at a green cup, you don't think of yourself as seeing a picture reconstructed in your visual cortex—although that _is_ what you are seeing—you just see a green cup.  You think, \"Why, look, this cup is green,\" not, \"The picture in my visual cortex of this cup is green.\"\n\nAnd in the same way, when people argue over whether the falling tree makes a sound, or whether Pluto is a planet, they don't see themselves as arguing over whether a categorization should be active in their neural networks.  It seems like either the tree makes a sound, or not.\n\nWe know where Pluto is, and where it's going; we know Pluto's shape, and Pluto's mass—but is it a planet?  And yes, there were people who said this was a fight over definitions—but even that is a Network 2 sort of perspective, because you're arguing about how the central unit ought to be wired up.  If you were a mind constructed along the lines of Network 1, you wouldn't say \"It depends on how you define 'planet',\" you would just say, \"Given that we know Pluto's orbit and shape and mass, there is no question left to ask.\"  Or, rather, that's how it would _feel_—it would _feel_ like there was no question left—if you were a mind constructed along the lines of Network 1.\n\nBefore you can question your intuitions, you have to realize that what your mind's eye is looking at _is_ an intuition—some cognitive algorithm, as seen from the inside—rather than a direct perception of the Way Things Really Are.\n\nPeople [cling to their intuitions](/lw/n1/allais_malaise/), I think, not so much because they believe their cognitive algorithms are perfectly reliable, but because they can't see their intuitions _as the way their cognitive algorithms happen to look from the inside._\n\nAnd so everything you try to say about how the native cognitive algorithm goes astray, ends up being contrasted to their direct perception of the Way Things Really Are—and discarded as obviously wrong."
          },
          "voteCount": 179
        },
        {
          "name": "Feel the Meaning",
          "type": "post",
          "slug": "feel-the-meaning",
          "_id": "dMCFk2n2ur8n62hqB",
          "url": null,
          "title": "Feel the Meaning",
          "author": "Eliezer_Yudkowsky",
          "question": false,
          "tags": [
            {
              "name": "Philosophy of Language"
            }
          ],
          "tableOfContents": null,
          "contents": {
            "markdown": "When I hear someone say, \"Oh, look, a butterfly,\" the spoken phonemes \"butterfly\" enter my ear and vibrate on my ear drum, being transmitted to the cochlea, tickling auditory nerves that transmit activation spikes to the auditory cortex, where phoneme processing begins, along with recognition of words, and reconstruction of syntax (a by no means serial process), and all manner of other complications.\n\nBut at the end of the day, or rather, at the end of the second, I am primed to look where my friend is pointing and see a visual pattern that I will recognize as a butterfly; and I would be quite surprised to see a wolf instead.\n\nMy friend looks at a butterfly, his throat vibrates and lips move, the pressure waves travel invisibly through the air, my ear hears and my nerves transduce and my brain reconstructs, and lo and behold, I know what my friend is looking at.  Isn't that marvelous?  If we didn't know about the pressure waves in the air, it would be a tremendous discovery in all the newspapers:  Humans are telepathic!  Human brains can transfer thoughts to each other!\n\nWell, we _are_ telepathic, in fact; but [magic isn't exciting when it's merely _real_, and all your friends can do it too](/lw/j3/science_as_curiositystopper/).\n\nThink telepathy is simple?  Try building a computer that will be telepathic with you.  Telepathy, or \"language\", or whatever you want to call our partial thought transfer ability, is more complicated than it looks.\n\nBut it would be quite inconvenient to go around thinking, \"Now I shall partially transduce some features of my thoughts into a linear sequence of phonemes which will invoke similar thoughts in my conversational partner...\"\n\nSo the brain hides the complexity—or rather, never represents it in the first place—which leads people to think some peculiar thoughts about words.\n\nAs I remarked [earlier](/lw/ng/words_as_hidden_inferences/), when a large yellow striped object leaps at me, I think \"Yikes!  A tiger!\" not \"Hm... objects with the properties of largeness, yellowness, and stripedness have previously often possessed the properties 'hungry' and 'dangerous', and therefore, although it is not logically necessary, _auughhhh_ CRUNCH CRUNCH GULP.\"\n\nSimilarly, when someone shouts \"Yikes!  A tiger!\", natural selection would not favor an organism that thought, \"Hm... I have just heard the syllables 'Tie' and 'Grr' which my fellow tribe members associate with their internal analogues of my own _tiger_ concept, and which they are more likely to utter if they see an object they categorize as _aiiieeee_ CRUNCH CRUNCH _help it's got my arm_ CRUNCH GULP\".\n\n[![Blegg4_4](/static/imported/2008/02/12/blegg4_4.png \"Blegg4_4\")](/static/imported/2008/02/12/blegg4_4.png) Considering this as a design constraint on the human [cognitive architecture](/lw/no/how_an_algorithm_feels_from_inside/), you wouldn't want _any_ extra steps between when your auditory cortex recognizes the syllables \"tiger\", and when the tiger concept gets activated.\n\nGoing back to the [parable of bleggs and rubes](/lw/nm/disguised_queries/), and the [centralized network](/lw/nn/neural_categories/) that categorizes quickly and cheaply, you might visualize a direct connection running from the unit that recognizes the syllable \"blegg\", to the unit at the center of the blegg network.  The central unit, the blegg concept, gets activated almost as soon as you hear Susan the Senior Sorter say \"Blegg!\"\n\nOr, for purposes of talking—which also shouldn't take eons—as soon as you see a blue egg-shaped thing and the central blegg unit fires, you holler \"Blegg!\" to Susan.\n\nAnd what that algorithm [feels like from inside](/lw/no/how_an_algorithm_feels_from_inside/) is that the label, and the concept, are very nearly _identified;_ the meaning _feels like_ an intrinsic property of the word itself.\n\nThe cognoscenti will recognize this as yet another case of E. T. Jaynes's \"Mind Projection Fallacy\".  It feels like a word _has a_ meaning, as a property of the word itself; just like how redness is a property of a red apple, or [mysteriousness is a property of a mysterious phenomenon](/lw/iu/mysterious_answers_to_mysterious_questions/).\n\nIndeed, on most occasions, the brain will not distinguish at all between the word and the meaning—only bothering to separate the two while learning a new language, perhaps.  And even then, you'll see Susan pointing to a blue egg-shaped thing and saying \"Blegg!\", and you'll think, _I wonder what \"blegg\" means,_ and not, _I wonder what mental category Susan associates to the auditory label \"blegg\"._\n\nConsider, in this light, the part of the [Standard Dispute of Definitions](/lw/np/disputing_definitions/) where the two parties argue about what the word \"sound\" _really_ means—the same way they might argue whether a particular apple is _really_ red or green:\n\n> Albert: \"My computer's microphone can record a sound without anyone being around to hear it, store it as a file, and it's called a 'sound file'. And what's stored in the file is the pattern of vibrations in air, not the pattern of neural firings in anyone's brain.  'Sound' means a pattern of vibrations.\"\n> \n> Barry:  \"Oh, yeah?  Let's just see if the dictionary agrees with you.\"\n\nAlbert feels intuitively that the word \"sound\" _has a meaning_ and that the meaning _is_ acoustic vibrations.  Just as Albert feels that a tree falling in the forest _makes a sound_ (rather than causing an event that _matches the sound category_).\n\nBarry likewise _feels_ that:\n\n> sound.meaning == auditory experiences  \n> forest.sound == false\n\nRather than:\n\n> myBrain.FindConcept(\"sound\") == concept_AuditoryExperience  \n> concept_AuditoryExperience.match(forest) == false\n\nWhich is closer to what's _really_ going on; but humans have not evolved to know this, anymore than humans instinctively know the brain is made of neurons.\n\nAlbert and Barry's conflicting intuitions provide the fuel for continuing the argument in the phase of arguing over what the word \"sound\" means—which _feels_ like arguing over a fact like any other fact, like arguing over whether the sky is blue or green.\n\nYou may not even notice that anything has gone astray, until you try to perform the rationalist ritual of [stating a testable experiment](/lw/i3/making_beliefs_pay_rent_in_anticipated_experiences/) whose result depends on the facts you're so heatedly disputing..."
          },
          "voteCount": 43
        },
        {
          "name": "Replace the Symbol with the Substance",
          "type": "post",
          "slug": "replace-the-symbol-with-the-substance",
          "_id": "GKfPL6LQFgB49FEnv",
          "url": null,
          "title": "Replace the Symbol with the Substance",
          "author": "Eliezer_Yudkowsky",
          "question": false,
          "tags": [
            {
              "name": "Philosophy of Language"
            },
            {
              "name": "Map and Territory"
            },
            {
              "name": "Techniques"
            },
            {
              "name": "Rationalist Taboo"
            }
          ],
          "tableOfContents": null,
          "contents": {
            "markdown": "What does it take to—as in yesterday's example—see a \"baseball game\" as \"An artificial group conflict in which you use a long wooden cylinder to whack a thrown spheroid, and then run between four safe positions\"?  What does it take to play the rationalist version of Taboo, in which the goal is not to find a synonym that isn't on the card, but to find a way of describing without the standard concept-handle?\n\nYou have to visualize.  You have to make your mind's eye see the details, as though looking for the first time.  You have to perform an [Original Seeing](/lw/k7/original_seeing/).\n\nIs that a \"bat\"?  No, it's a long, round, tapering, wooden rod, narrowing at one end so that a human can grasp and swing it.\n\nIs that a \"ball\"?  No, it's a leather-covered spheroid with a symmetrical stitching pattern, hard but not metal-hard, which someone can grasp and throw, or strike with the wooden rod, or catch.\n\nAre those \"bases\"?  No, they're fixed positions on a game field, that players try to run to as quickly as possible because of their safety within the game's artificial rules.\n\nThe chief obstacle to performing an original seeing is that your mind already has a nice neat summary, a nice little easy-to-use concept handle.  Like the word \"baseball\", or \"bat\", or \"base\".  It takes an effort to stop your mind from sliding down the familiar path, the easy path, the path of least resistance, where the small featureless word rushes in and obliterates the details you're trying to see.  A word itself can have the destructive force of [cliche](/lw/jc/rationality_and_the_english_language/); a word itself can carry the poison of a [cached thought](/lw/k5/cached_thoughts/).\n\nPlaying the game of [Taboo](/lw/nu/taboo_your_words/)—being able to describe without using the standard pointer/label/handle—is one of the _fundamental_ rationalist capacities.  It occupies the same primordial level as the habit of constantly asking \"Why?\" or \"What does this belief make me anticipate?\"\n\nThe art is closely related to:\n\n*   Pragmatism, because seeing in this way often gives you a much closer connection to [anticipated experience](/lw/i3/making_beliefs_pay_rent_in_anticipated_experiences/), rather than [propositional belief](/lw/i4/belief_in_belief/);\n*   Reductionism, because seeing in this way often forces you to drop down to a lower level of organization, look at the parts instead of your eye skipping over the whole;\n*   [Hugging the query](/lw/ly/hug_the_query/), because words often distract you from the question you really want to ask;\n*   Avoiding [cached thoughts](/lw/k5/cached_thoughts/), which will rush in using standard words, so you can block them by tabooing standard words;\n*   The writer's rule of \"Show, don't tell!\", which has power among rationalists;\n*   And [not losing sight of your original purpose](/lw/le/lost_purposes/).\n\nHow could tabooing a word help you keep your purpose?\n\nFrom [Lost Purposes](/lw/le/lost_purposes/):\n\n> As you read this, some young man or woman is sitting at a desk in a university, earnestly studying material they have no intention of ever using, and no interest in knowing for its own sake.  They want a high-paying job, and the high-paying job requires a piece of paper, and the piece of paper requires a previous master's degree, and the master's degree requires a bachelor's degree, and the university that grants the bachelor's degree requires you to take a class in 12th-century knitting patterns to graduate.  So they diligently study, intending to forget it all the moment the final exam is administered, but still seriously working away, because they _want_ that piece of paper.\n\nWhy are you going to \"school\"?  To get an \"education\" ending in a \"degree\".  Blank out the forbidden words and all their obvious synonyms, visualize the actual details, and you're much more likely to notice that \"school\" currently seems to consist of sitting next to bored teenagers listening to material you already know, that a \"degree\" is a piece of paper with some writing on it, and that \"education\" is forgetting the material as soon as you're tested on it.\n\n[Leaky generalizations](/lw/lc/leaky_generalizations/) often manifest through categorizations:  People who actually learn in classrooms are categorized as \"getting an education\", so \"getting an education\" must be good; but then anyone who actually shows up at a college will also match against the concept \"getting an education\", whether or not they learn.\n\nStudents who understand math will do well on tests, but if you require schools to produce good test scores, they'll spend all their time teaching to the test.  A _mental category,_ that imperfectly matches your goal, can produce the same kind of incentive failure _internally._ You want to learn, so you need an \"education\"; and then as long as you're getting anything that matches against the category \"education\", you may not notice whether you're learning or not.  Or you'll notice, but you won't realize you've lost sight of your original purpose, because you're \"getting an education\" and that's how you mentally described your goal.\n\nTo categorize is to throw away information.  If you're told that a falling tree makes a \"sound\", you don't know what the actual sound is; you haven't actually heard the tree falling.  If a coin lands \"heads\", you don't know its radial orientation.  A blue egg-shaped thing may be a \"blegg\", but what if the exact egg shape varies, or the exact shade of blue?  You want to use categories to throw away irrelevant information, to sift gold from dust, but often the standard categorization ends up throwing out relevant information too.  And when you end up in that sort of mental trouble, the first and most obvious solution is to play Taboo.\n\nFor example:  \"Play Taboo\" is itself a leaky generalization.  Hasbro's version is not the rationalist version; they only list five additional banned words on the card, and that's not nearly enough coverage to exclude thinking in familiar old words.  What rationalists do would count as playing Taboo—it would match against the \"play Taboo\" concept—but not everything that counts as playing Taboo works to force original seeing.  If you just think \"play Taboo to force original seeing\", you'll start thinking that anything that counts as playing Taboo must count as original seeing.\n\nThe rationalist version isn't a game, which means that you can't win by trying to be clever and stretching the rules.  You have to play Taboo with a voluntary handicap:  Stop yourself from using synonyms that aren't on the card.  You also have to stop yourself from inventing a new simple word or phrase that functions as an equivalent mental handle to the old one.  You are trying to zoom in on your map, not rename the cities; dereference the pointer, not allocate a new pointer; see the events as they happen, not rewrite the cliche in a different wording.\n\nBy visualizing the problem in more detail, you can see the lost purpose:  Exactly what do you do when you \"play Taboo\"?   What purpose does each and every part serve?\n\nIf you see your activities and situation originally, you will be able to originally see your goals as well.  If you can look with fresh eyes, as though for the first time, you will see yourself doing things that you would never dream of doing if they were not habits.\n\nPurpose is lost whenever the substance (learning, knowledge, health) is displaced by the symbol (a degree, a test score, medical care).  To heal a lost purpose, or a lossy categorization, you must do the reverse:\n\nReplace the symbol with the substance; replace the signifier with the signified; replace the property with the membership test; replace the word with the meaning; replace the label with the concept; replace the summary with the details; replace the proxy question with the real question; dereference the pointer; drop into a lower level of organization; mentally simulate the process instead of naming it; zoom in on your map.\n\n\"[The Simple Truth](http://yudkowsky.net/bayes/truth.html)\" was generated by an exercise of this discipline to describe \"truth\" on a lower level of organization, without invoking terms like \"accurate\", \"correct\", \"represent\", \"reflect\", \"semantic\", \"believe\", \"knowledge\", \"map\", or \"real\".  (And remember that the goal is not _really_ to play Taboo—the word \"true\" appears in the text, but _not_ to define truth.  It would get a buzzer in Hasbro's game, but we're not _actually_ playing that game.  Ask yourself whether the document fulfilled its purpose, not whether it followed the rules.)\n\nBayes's Rule itself describes \"evidence\" in pure math, without using words like \"implies\", \"means\", \"supports\", \"proves\", or \"justifies\".  Set out to _define_ such philosophical terms, and you'll just go in circles.\n\nAnd then there's the most important word of all to Taboo.  I've [often](http://yudkowsky.net/virtues/) [warned](/lw/m4/two_cult_koans/) that you should be careful not to overuse it, or even [avoid the concept](/lw/nc/newcombs_problem_and_regret_of_rationality/) in certain cases.  Now you know the real reason why.  It's not a bad subject to think about.  But your true understanding is measured by your ability to describe what you're doing and why, _without_ using that word or any of its synonyms."
          },
          "voteCount": 61
        }
      ]
    },
    {
      "title": "Selected readings from [[Reductionism (Sequence)]] (first half)",
      "description": "",
      "children": [
        {
          "name": "Dissolving the Question",
          "type": "post",
          "slug": "dissolving-the-question",
          "_id": "Mc6QcrsbH5NRXbCRX",
          "url": null,
          "title": "Dissolving the Question",
          "author": "Eliezer_Yudkowsky",
          "question": false,
          "tags": [
            {
              "name": "Introspection"
            },
            {
              "name": "Philosophy of Language"
            },
            {
              "name": "Philosophy"
            },
            {
              "name": "Dissolving the Question"
            }
          ],
          "tableOfContents": null,
          "contents": {
            "markdown": "\"If [a tree falls in the forest](/lw/np/disputing_definitions/), but no one hears it, does it make a sound?\"\n\nI didn't _answer_ that question.  I didn't pick a position, \"Yes!\" or \"No!\", and defend it.  Instead I went off and [deconstructed](/lw/no/how_an_algorithm_feels_from_inside/) the human algorithm for processing words, even going so far as to sketch an [illustration](/lw/nn/neural_categories/) of a neural network.  At the end, I hope, there was no question left—not even the feeling of a question.\n\nMany philosophers—particularly amateur philosophers, and ancient philosophers—share a dangerous instinct:  If you give them a question, they try to answer it.\n\nLike, say, \"Do we have free will?\"\n\nThe dangerous instinct of philosophy is to marshal the arguments in favor, and marshal the arguments against, and weigh them up, and publish them in a prestigious journal of philosophy, and so finally conclude:  \"Yes, we must have free will,\" or \"No, we cannot possibly have free will.\"\n\nSome philosophers are wise enough to recall the warning that most philosophical disputes are really disputes over the meaning of a word, or confusions generated by [using different meanings for the same word in different places](/lw/oc/variable_question_fallacies/).  So they try to define very precisely what they mean by \"free will\", and then ask again, \"Do we have free will?  Yes or no?\"\n\nA philosopher wiser yet, may suspect that the confusion about \"free will\" shows the notion itself is flawed.  So they pursue the Traditional Rationalist course:  They argue that \"free will\" is inherently self-contradictory, or meaningless because it has no [testable consequences](/lw/i3/making_beliefs_pay_rent_in_anticipated_experiences/).  And then they publish these devastating observations in a prestigious philosophy journal.\n\nBut _proving that_ you are confused may not make you feel any _less_ confused.  Proving that a question is meaningless may not help you any more than answering it.\n\nThe philosopher's instinct is to find the most defensible position, publish it, and move on.  But the \"naive\" view, the instinctive view, is a fact about human psychology.  You can prove that free will is impossible until the Sun goes cold, but this leaves an unexplained fact of cognitive science:  If free will doesn't exist, what goes on inside the head of a human being who thinks it does?  This is not a rhetorical question!\n\nIt is a fact about human psychology that people think they have free will.  Finding a more defensible _philosophical position_ doesn't change, or explain, that _psychological fact._  Philosophy may lead you to _reject_ the concept, but rejecting a concept is not the same as understanding the cognitive algorithms behind it.\n\nYou could look at the [Standard Dispute](/lw/np/disputing_definitions/) over \"If a tree falls in the forest, and no one hears it, does it make a sound?\", and you could do the Traditional Rationalist thing:  Observe that the two don't disagree on any point of [anticipated experience](/lw/i3/making_beliefs_pay_rent_in_anticipated_experiences/), and triumphantly declare the argument pointless.  That happens to be correct in this particular case; but, as _a question of cognitive science,_ why did the arguers make that mistake in the first place?\n\nThe key idea of the heuristics and biases program is that the _mistakes_ we make, often reveal far more about our underlying cognitive algorithms than our correct answers.  So (I asked myself, once upon a time) [what kind of mind design](/lw/nq/feel_the_meaning/) corresponds to the mistake of [arguing](/lw/np/disputing_definitions/) about trees falling in deserted forests?\n\nThe cognitive algorithms we use, _are_ [the way the world feels](/lw/no/how_an_algorithm_feels_from_inside/).  And these cognitive algorithms may not have a one-to-one correspondence with reality—not even macroscopic reality, to say nothing of the true quarks.  There can be things in the mind that cut skew to the world.\n\nFor example, there can be a [dangling unit](/lw/no/how_an_algorithm_feels_from_inside/) in the center of a [neural network](/lw/nn/neural_categories/), which does not correspond to any real thing, or any real property of any real thing, existent anywhere in the real world.  This dangling unit is often useful as a [shortcut in computation](/lw/o8/conditional_independence_and_naive_bayes/), which is why we have them.  (Metaphorically speaking.  Human neurobiology is surely far more [complex](/lw/o8/conditional_independence_and_naive_bayes/).)\n\nThis dangling unit _feels like_ an unresolved question, even after every answerable [query](/lw/nm/disguised_queries/) is answered.  No matter how much anyone proves to you that no difference of anticipated experience depends on the question, you're left wondering:  \"But does the falling tree _really_ make a sound, or not?\"\n\nBut once you understand _in detail_ how your brain generates the _feeling_ of the question—once you realize that your feeling of an unanswered question, corresponds to an illusory central unit wanting to know whether it should fire, even after all the edge units are clamped at known values—or better yet, you understand the technical workings of [Naive Bayes](/lw/o8/conditional_independence_and_naive_bayes/)—_then_ you're done.  Then there's no lingering feeling of confusion, no vague sense of dissatisfaction.\n\nIf there is _any_ lingering feeling of a remaining unanswered question, or of having been fast-talked into something, then this is a sign that you have not dissolved the question.  A [vague dissatisfaction](/lw/if/your_strength_as_a_rationalist/) should be as much warning as a shout.  _Really_ dissolving the question doesn't leave anything behind.\n\nA triumphant thundering refutation of free will, an absolutely unarguable proof that free will cannot exist, feels very _satisfying_—a [grand cheer](/lw/i6/professing_and_cheering/) for the [home team](/lw/mg/the_twoparty_swindle/).    And so you may not notice that—as a point of cognitive science—you do not have a full and satisfactory descriptive explanation of how each intuitive sensation arises, point by point.\n\nYou may not even want to admit your ignorance, of this point of cognitive science, because that would feel like a score against Your Team.  In the midst of smashing all foolish beliefs of free will, it would seem like a concession to the opposing side to concede that you've left anything unexplained.\n\nAnd so, perhaps, you'll come up with a [just-so evolutionary-psychological](/lw/mk/a_failed_justso_story/) argument that hunter-gatherers who believed in free will, were more likely to take a positive outlook on life, and so outreproduce other hunter-gatherers—to give one example of a completely bogus explanation.  If you say this, you are _arguing that_ the brain generates an illusion of free will—but you are not _explaining how._  You are trying to dismiss the opposition by deconstructing its motives—but in the story you tell, the illusion of free will is a brute fact.  You have not taken the illusion apart to see the wheels and gears.\n\nImagine that in the Standard Dispute about a tree falling in a deserted forest, you first prove that no difference of anticipation exists, and then go on to hypothesize, \"But perhaps people who said that arguments were meaningless were viewed as having conceded, and so lost social status, so now we have an instinct to argue about the meanings of words.\"  That's _arguing that_ or _explaining why_ a confusion exists.  Now look at the neural network structure in [Feel the Meaning](/lw/nq/feel_the_meaning/).  That's _explaining how_, disassembling the confusion into smaller pieces which are not themselves confusing.  See the difference?\n\nComing up with good hypotheses about cognitive algorithms (or even hypotheses that hold together for half a second) is a good deal harder than just refuting a philosophical confusion.  Indeed, it is an entirely different art.  Bear this in mind, and you should feel less embarrassed to say, \"I know that what you say can't possibly be true, and I can prove it.  But I cannot write out a flowchart which shows how your brain makes the mistake, so I'm not done yet, and will continue investigating.\"\n\nI say all this, because it sometimes seems to me that at least 20% of the real-world effectiveness of a skilled rationalist comes from [not stopping too early](/lw/jz/the_meditation_on_curiosity/).  If you keep asking questions, you'll get to your destination eventually.  If you decide too early that you've found an answer, you won't.\n\nThe challenge, above all, is to notice when you are confused—even if it just feels like a little tiny bit of confusion—and even if there's someone standing across from you, _insisting_ that humans have free will, and _smirking_ at you, and the fact that you don't know _exactly_ how the cognitive algorithms work, has _nothing to do_ with the searing folly of their position...\n\nBut when you can lay out the cognitive algorithm in sufficient detail that you can walk through the thought process, step by step, and describe how each intuitive perception arises—decompose the confusion into smaller pieces not themselves confusing—_then_ you're done.\n\nSo be warned that you may _believe_ you're done, when all you have is a mere triumphant [refutation of a mistake](/lw/lw/reversed_stupidity_is_not_intelligence/).\n\nBut when you're _really_ done, you'll _know_ you're done.[ ](/lw/gr/the_modesty_argument/)  Dissolving the question is an unmistakable feeling—once you experience it, and, having experienced it, resolve not to be fooled again.  [Those who dream do not know they dream, but when you wake you know you are awake.](/lw/gr/the_modesty_argument/)\n\nWhich is to say:  When you're done, you'll know you're done, but unfortunately the reverse implication does not hold.\n\nSo here's your homework problem:  What kind of cognitive algorithm, as felt from the inside, would generate the observed debate about \"free will\"?\n\nYour assignment is not to argue about whether people have free will, or not.\n\nYour assignment is not to argue that free will is compatible with determinism, or not.\n\nYour assignment is not to argue that the question is ill-posed, or that the concept is self-contradictory, or that it has no testable consequences.\n\nYou are not asked to invent an evolutionary explanation of how people who believed in free will would have reproduced; nor an account of how the concept of free will seems suspiciously congruent with bias X.  Such are mere attempts to _explain why_ people believe in \"free will\", not _explain how._\n\nYour homework assignment is to write a stack trace of the internal algorithms of the human mind as they produce the intuitions that power the whole damn philosophical argument.\n\nThis is one of the first real challenges I tried as an aspiring rationalist, once upon a time.  One of the easier conundrums, relatively speaking.  May it serve you likewise."
          },
          "voteCount": 85
        },
        {
          "name": "Wrong Questions",
          "type": "post",
          "slug": "wrong-questions",
          "_id": "XzrqkhfwtiSDgKoAF",
          "url": null,
          "title": "Wrong Questions",
          "author": "Eliezer_Yudkowsky",
          "question": false,
          "tags": [
            {
              "name": "Philosophy of Language"
            },
            {
              "name": "Cognitive Reduction"
            }
          ],
          "tableOfContents": null,
          "contents": {
            "markdown": "Where the mind cuts against reality's grain, it generates _wrong questions_—questions that cannot possibly be answered _on their own terms,_ but only [dissolved](/lw/of/dissolving_the_question/) by understanding the cognitive algorithm that generates the _perception_ of a question.\n\nOne good cue that you're dealing with a \"wrong question\" is when you cannot even _imagine_ any concrete, specific state of how-the-world-is that would answer the question.  When it doesn't even seem _possible_ to answer the question.\n\nTake the [Standard Definitional Dispute](/lw/np/disputing_definitions/), for example, about the tree falling in a deserted forest.  Is there any way-the-world-could-be—any state of affairs—that corresponds to the word \"sound\" _really meaning_ only acoustic vibrations, or _really_ _meaning_ only auditory experiences?\n\n(\"Why, yes,\" says the one, \"it is the state of affairs where 'sound' means acoustic vibrations.\"  So [Taboo](/lw/nv/replace_the_symbol_with_the_substance/) the word 'means', and 'represents', and all similar synonyms, and describe again:  How can the world be, what state of affairs, would make one side right, and the other side wrong?)\n\nOr if that seems too easy, take free will:  What concrete state of affairs, whether in deterministic physics, or in physics with a dice-rolling random component, could ever correspond to having free will?\n\nAnd if _that_ seems too easy, then ask \"Why does anything exist at all?\", and then tell me what a satisfactory answer to that question would even _look like._\n\nAnd no, I don't know the answer to that last one.  But I _can_ guess one thing, based on my previous experience with unanswerable questions.  The answer will not consist of some grand triumphant First Cause.  The question will go away as a result of some insight into how my mental algorithms run skew to reality, after which I will understand how the question itself was wrong from the beginning—how the question itself assumed the fallacy, contained the skew.\n\nMystery exists in the mind, not in reality.  If I am ignorant about a phenomenon, that is a fact about my state of mind, not a fact about the phenomenon itself.  All the more so, if it seems like no possible answer can exist:  Confusion exists in the map, not in the territory.  _Unanswerable_ questions do not mark places where magic enters the universe.  They mark places where your mind runs skew to reality.\n\nSuch questions _must_ be dissolved.  Bad things happen when you try to answer them.  It inevitably generates the worst sort of [Mysterious Answer to a Mysterious Question](/lw/iu/mysterious_answers_to_mysterious_questions/):  The one where you come up with seemingly strong arguments for your Mysterious Answer, but the \"answer\" doesn't let you make any new predictions even in retrospect, and the phenomenon still possesses the same sacred inexplicability that it had at the start.\n\nI could guess, for example, that the answer to the puzzle of the First Cause is that nothing _does_ exist—that the whole concept of \"existence\" is bogus.  But if you sincerely believed that, would you be any less confused?  Me neither.\n\nBut the wonderful thing about _unanswerable_ questions is that they are _always_ solvable, at least in my experience.  What went through Queen Elizabeth I's mind, first thing in the morning, as she woke up on her fortieth birthday?  As I can easily _imagine_ answers to this question, I can readily see that I may never be able to _actually_ answer it, the true information having been lost in time.\n\nOn the other hand, \"Why does anything exist at all?\" seems _so_ absolutely impossible that I can infer that I am just confused, one way or another, and the truth probably isn't all that complicated in an absolute sense, and once the confusion goes away I'll be able to see it.\n\nThis may seem counterintuitive if you've never solved an unanswerable question, but I assure you that it _is_ how these things work.\n\nComing tomorrow:  A simple trick for handling \"wrong questions\"."
          },
          "voteCount": 58
        },
        {
          "name": "Righting a Wrong Question",
          "type": "post",
          "slug": "righting-a-wrong-question",
          "_id": "rQEwySCcLtdKHkrHp",
          "url": null,
          "title": "Righting a Wrong Question",
          "author": "Eliezer_Yudkowsky",
          "question": false,
          "tags": [
            {
              "name": "Introspection"
            },
            {
              "name": "Causality"
            },
            {
              "name": "Rationality"
            }
          ],
          "tableOfContents": null,
          "contents": {
            "markdown": "When you are faced with an _unanswerable_ question—a question to which it seems impossible to even _imagine_ an answer—there is a simple trick which can turn the question solvable.\n\nCompare:\n\n*   \"Why do I have free will?\"\n*   \"Why do I think I have free will?\"\n\nThe nice thing about the second question is that it is _guaranteed_ to have a real answer, _whether or not_ there is any such thing as free will.  Asking \"Why do I have free will?\" or \"Do I have free will?\" sends you off thinking about tiny details of the laws of physics, so distant from the macroscopic level that you couldn't begin to see them with the naked eye.  And you're asking \"Why is X the case?\" where X may not be _coherent,_ let alone the case.\n\n\"Why do I _think_ I have free will?\", in contrast, is guaranteed answerable.  You do, in fact, believe you have free will.  This belief seems far more solid and graspable than the ephemerality of free will.  And there is, _in fact,_ some nice solid chain of cognitive cause and effect leading up to this belief.\n\nIf you've already outgrown free will, choose one of these substitutes:\n\n*   \"Why does time move forward instead of backward?\" versus \"Why do I think time moves forward instead of backward?\"\n*   \"Why was I born as myself rather than someone else?\" versus \"Why do I think I was born as myself rather than someone else?\"\n*   \"Why am I conscious?\" versus \"Why do I think I'm conscious?\"\n*   \"Why does reality exist?\" versus \"Why do I think reality exists?\"\n\nThe beauty of this method is that it works _whether or not_ the question is confused.  As I type this, I am wearing socks.  I could ask \"Why am I wearing socks?\" or \"Why do I believe I'm wearing socks?\"  Let's say I ask the second question.  Tracing back the chain of causality, I find:\n\n*   I believe I'm wearing socks, because I can see socks on my feet.\n*   I see socks on my feet, because my retina is sending sock signals to my visual cortex.\n*   My retina is sending sock signals, because sock-shaped light is impinging on my retina.\n*   Sock-shaped light impinges on my retina, because it reflects from the socks I'm wearing.\n*   It reflects from the socks I'm wearing, because I'm wearing socks.\n*   I'm wearing socks because I put them on.\n*   I put socks on because I believed that otherwise my feet would get cold.\n*   &c.\n\nTracing back the chain of causality, step by step, I discover that my belief that I'm wearing socks is fully explained by the fact that I'm wearing socks.  This is right and proper, as [you cannot gain information about something without interacting with it](/lw/o6/perpetual_motion_beliefs/).\n\nOn the other hand, if I see a mirage of a lake in a desert, the correct causal explanation of my vision does not involve the fact of any actual lake in the desert.  In this case, my belief in the lake is not just _explained,_ but _explained away._\n\nBut _either way,_ the belief itself is a real phenomenon taking place in the real universe—psychological events are events—and its causal history can be traced back.\n\n\"Why is there a lake in the middle of the desert?\" may fail if there is no lake to be explained.  But \"Why do I _perceive_ a lake in the middle of the desert?\" always has a causal explanation, one way or the other.\n\nPerhaps someone will see an opportunity to be clever, and say:  \"Okay.  I believe in free will because I have free will.  There, I'm done.\"  Of course it's not that easy.\n\nMy perception of socks on my feet, is an event in the visual cortex.  The workings of the visual cortex can be investigated by cognitive science, should they be confusing.\n\nMy retina receiving light is not a mystical sensing procedure, a magical sock detector that lights in the presence of socks for no explicable reason; there are mechanisms that can be understood in terms of biology.  The photons entering the retina can be understood in terms of optics.  The shoe's surface reflectance can be understood in terms of electromagnetism and chemistry.  My feet getting cold can be understood in terms of thermodynamics.\n\nSo it's not as easy as saying, \"I believe I have free will because I have it—there, I'm done!\"  You have to be able to break the causal chain into smaller steps, and explain the steps in terms of elements not themselves confusing.\n\nThe mechanical interaction of my retina with my socks is quite clear, and can be described in terms of non-confusing components like photons and electrons.  Where's the free-will-sensor in your brain, and how does it detect the presence or absence of free will?  How does the sensor interact with the sensed event, and what are the mechanical details of the interaction?\n\nIf your belief does derive from valid observation of a real phenomenon, we will eventually reach that fact, if we start tracing the causal chain backward from your belief.\n\nIf what you are really seeing is your own confusion, tracing back the chain of causality will find an algorithm that [runs skew to reality](/lw/of/dissolving_the_question/).\n\nEither way, the question is guaranteed to have an answer.  You even have a nice, concrete place to begin tracing—your belief, sitting there solidly in your mind.\n\nCognitive science may not seem so lofty and glorious as metaphysics.  But at least questions of cognitive science are _solvable._  Finding an answer may not be _easy,_ but at least an answer _exists._\n\nOh, and also: the idea that cognitive science is not so lofty and glorious as metaphysics is simply wrong.  Some readers are beginning to notice this, I hope."
          },
          "voteCount": 90
        },
        {
          "name": "Probability is in the Mind",
          "type": "post",
          "slug": "probability-is-in-the-mind",
          "_id": "f6ZLxEWaankRZ2Crv",
          "url": null,
          "title": "Probability is in the Mind",
          "author": "Eliezer_Yudkowsky",
          "question": false,
          "tags": [
            {
              "name": "Rationality"
            },
            {
              "name": "Mind Projection Fallacy"
            },
            {
              "name": "Bayes' Theorem"
            },
            {
              "name": "Map and Territory"
            },
            {
              "name": "Bayesianism"
            }
          ],
          "tableOfContents": null,
          "contents": {
            "markdown": "[![Monsterwithgirl_2](/static/imported/2007/08/10/monsterwithgirl_2.jpg \"Monsterwithgirl_2\")](/static/imported/2007/08/10/monsterwithgirl_2.jpg)\n\nYesterday I spoke of the Mind Projection Fallacy, giving the example of the alien monster who carries off a girl in a torn dress for intended ravishing—a mistake which I imputed to the artist's tendency to think that a woman's sexiness is a property of the woman herself, woman.sexiness, rather than something that exists in the mind of an observer, and probably wouldn't exist in an alien mind.\n\nThe term \"Mind Projection Fallacy\" was coined by the late great Bayesian Master, E. T. Jaynes, as part of his long and hard-fought battle against the accursèd frequentists.  Jaynes was of the opinion that probabilities were in the mind, not in the environment—that probabilities express ignorance, states of partial information; and if I am ignorant of a phenomenon, that is a fact about my state of mind, not a fact about the phenomenon.\n\nI cannot do justice to this ancient war in a few words—but the classic example of the argument runs thus:\n\nYou have a coin.  \nThe coin is biased.  \nYou don't know which way it's biased or how much it's biased.  Someone just told you, \"The coin is biased\" and that's all they said.  \nThis is all the information you have, and the only information you have.\n\nYou draw the coin forth, flip it, and slap it down.\n\nNow—before you remove your hand and look at the result—are you willing to say that you assign a 0.5 probability to the coin having come up heads?\n\nThe frequentist says, \"No.  Saying 'probability 0.5' means that the coin has an inherent propensity to come up heads as often as tails, so that if we flipped the coin infinitely many times, the ratio of heads to tails would approach 1:1.  But we know that the coin is biased, so it can have any probability of coming up heads _except_ 0.5.\"\n\nThe Bayesian says, \"Uncertainty exists in the map, not in the territory.  In the real world, the coin has either come up heads, or come up tails.  Any talk of 'probability' must refer to the _information_ that I have about the coin—my state of partial ignorance and partial knowledge—not just the coin itself.  Furthermore, I have all sorts of theorems showing that if I don't treat my partial knowledge a [certain way](http://www.overcomingbias.com/2008/01/something-to-pr.html), I'll make stupid bets.  If I've got to plan, I'll plan for a 50/50 state of uncertainty, where I don't weigh outcomes conditional on heads any more heavily in my mind than outcomes conditional on tails.  You can call that number whatever you like, but it has to obey the probability laws on pain of stupidity.  So I don't have the slightest hesitation about calling my outcome-weighting a probability.\"\n\nI side with the Bayesians.  You may have noticed that about me.\n\nEven before a fair coin is tossed, the notion that it has an _inherent_ 50% probability of coming up heads may be just plain wrong.  Maybe you're holding the coin in such a way that it's just about guaranteed to come up heads, or tails, given the force at which you flip it, and the air currents around you.  But, if you don't know which way the coin is biased on this one occasion, so what?\n\nI believe there was a lawsuit where someone alleged that the draft lottery was unfair, because the slips with names on them were not being mixed thoroughly enough; and the judge replied, \"To whom is it unfair?\"\n\nTo make the coinflip experiment repeatable, as frequentists are wont to demand, we could build an automated coinflipper, and verify that the results were 50% heads and 50% tails.  But maybe a robot with extra-sensitive eyes and a good grasp of physics, watching the autoflipper prepare to flip, could predict the coin's fall in advance—not with certainty, but with 90% accuracy.  Then what would the _real_ probability be?\n\nThere is no \"real probability\".  The robot has one state of partial information.  You have a different state of partial information.  The coin itself has no mind, and doesn't assign a probability to anything; it just flips into the air, rotates a few times, bounces off some air molecules, and lands either heads or tails.\n\nSo that is the Bayesian view of things, and I would now like to point out a couple of classic brainteasers that derive their brain-_teasing_ ability from the tendency to think of probabilities as inherent properties of objects.\n\nLet's take the old classic:  You meet a mathematician on the street, and she happens to mention that she has given birth to two children on two separate occasions.  You ask:  \"Is at least one of your children a boy?\"  The mathematician says, \"Yes, he is.\"\n\nWhat is the probability that she has two boys?  If you assume that the prior probability of a child being a boy is 1/2, then the probability that she has two boys, on the information given, is 1/3.  The prior probabilities were:  1/4 two boys, 1/2 one boy one girl, 1/4 two girls.  The mathematician's \"Yes\" response has probability ~1 in the first two cases, and probability ~0 in the third.  Renormalizing leaves us with a 1/3 probability of two boys, and a 2/3 probability of one boy one girl.\n\nBut suppose that instead you had asked, \"Is your eldest child a boy?\" and the mathematician had answered \"Yes.\"  Then the probability of the mathematician having two boys would be 1/2.  Since the eldest child is a boy, and the younger child can be anything it pleases.\n\nLikewise if you'd asked \"Is your youngest child a boy?\"  The probability of their being both boys would, again, be 1/2.\n\nNow, if at least one child is a boy, it must be either the oldest child who is a boy, or the youngest child who is a boy.  So how can the answer in the first case be different from the answer in the latter two?\n\nOr here's a very similar problem:  Let's say I have four cards, the ace of hearts, the ace of spades, the two of hearts, and the two of spades.  I draw two cards at random.  You ask me, \"Are you holding at least one ace?\" and I reply \"Yes.\"  What is the probability that I am holding a pair of aces?  It is 1/5.  There are six possible combinations of two cards, with equal prior probability, and you have just eliminated the possibility that I am holding a pair of twos.  Of the five remaining combinations, only one combination is a pair of aces.  So 1/5.\n\nNow suppose that instead you asked me, \"Are you holding the ace of spades?\"  If I reply \"Yes\", the probability that the other card is the ace of hearts is 1/3.  (You know I'm holding the ace of spades, and there are three possibilities for the other card, only one of which is the ace of hearts.)  Likewise, if you ask me \"Are you holding the ace of hearts?\" and I reply \"Yes\", the probability I'm holding a pair of aces is 1/3.\n\nBut then how can it be that if you ask me, \"Are you holding at least one ace?\" and I say \"Yes\", the probability I have a pair is 1/5?  Either I must be holding the ace of spades or the ace of hearts, as you know; and either way, the probability that I'm holding a pair of aces is 1/3.\n\nHow can this be?  Have I miscalculated one or more of these probabilities?\n\nIf you want to figure it out for yourself, do so now, because I'm about to reveal...\n\nThat all stated calculations are correct.\n\nAs for the paradox, there isn't one.  The _appearance_ of paradox comes from thinking that the probabilities must be properties of the cards themselves.  The ace I'm holding has to be either hearts or spades; but that doesn't mean that your _knowledge about_ my cards must be the same as if you _knew_ I was holding hearts, or _knew_ I was holding spades.\n\nIt may help to think of Bayes's Theorem:\n\n> P(H|E) = P(E|H)P(H) / P(E)\n\nThat last term, where you divide by P(E), is the part where you throw out all the possibilities that have been eliminated, and renormalize your probabilities over what remains.\n\nNow let's say that you ask me, \"Are you holding at least one ace?\"  _Before_ I answer, your probability that I say \"Yes\" should be 5/6.\n\nBut if you ask me \"Are you holding the ace of spades?\", your prior probability that I say \"Yes\" is just 1/2.\n\nSo right away you can see that you're _learning_ something very different in the two cases.  You're going to be eliminating some different possibilities, and renormalizing using a different P(E).  If you learn two different items of evidence, you shouldn't be surprised at ending up in two different states of partial information.\n\nSimilarly, if I ask the mathematician, \"Is at least one of your two children a boy?\" I expect to hear \"Yes\" with probability 3/4, but if I ask \"Is your eldest child a boy?\" I expect to hear \"Yes\" with probability 1/2.  So it shouldn't be surprising that I end up in a different state of partial knowledge, depending on which of the two questions I ask.\n\nThe only reason for seeing a \"paradox\" is thinking as though the probability of holding a pair of aces is _a property of cards_ that have at least one ace, or a property _of cards_ that happen to contain the ace of spades.  In which case, it would be paradoxical for card-sets containing at least one ace to have an inherent pair-probability of 1/5, while card-sets containing the ace of spades had an inherent pair-probability of 1/3, and card-sets containing the ace of hearts had an inherent pair-probability of 1/3.\n\nSimilarly, if you think a 1/3 probability of being both boys is an _inherent property_ of child-sets that include at least one boy, then that is not consistent with child-sets of which the eldest is male having an _inherent_ probability of 1/2 of being both boys, and child-sets of which the youngest is male having an inherent 1/2 probability of being both boys.  It would be like saying, \"All green apples weigh a pound, and all red apples weigh a pound, and all apples that are green or red weigh half a pound.\"\n\nThat's what happens when you start thinking as if probabilities are _in_ things, rather than probabilities being states of partial information _about_ things.\n\nProbabilities express uncertainty, and it is only agents who can be uncertain.  A blank map does not correspond to a blank territory.  Ignorance is in the mind."
          },
          "voteCount": 95
        },
        {
          "name": "Reductionism",
          "type": "post",
          "slug": "reductionism",
          "_id": "tPqQdLCuxanjhoaNs",
          "url": null,
          "title": "Reductionism",
          "author": "Eliezer_Yudkowsky",
          "question": false,
          "tags": [
            {
              "name": "Truth, Semantics, & Meaning"
            },
            {
              "name": "Map and Territory"
            },
            {
              "name": "Reductionism"
            }
          ],
          "tableOfContents": null,
          "contents": {
            "markdown": "Almost one year ago, in April 2007, Matthew C submitted the following suggestion for an Overcoming Bias topic:\n\n> \"How and why the current reigning philosophical hegemon (reductionistic materialism) is obviously correct \\[...\\], while the reigning philosophical viewpoints of all past societies and civilizations are obviously suspect—\"\n\nI remember this, because I looked at the request and deemed it legitimate, but I knew I couldn't do that topic until I'd started on the [Mind Projection Fallacy](/lw/oi/mind_projection_fallacy/) sequence, which wouldn't be for a while...\n\nBut now it's time to begin addressing this question.  And while I haven't yet come to the \"materialism\" issue, we can now start on \"reductionism\".\n\nFirst, let it be said that I do indeed hold that \"reductionism\", according to the [meaning](/lw/nr/the_argument_from_common_usage/) I will give for that word, is obviously correct; and [to perdition with any past civilizations that disagreed](/lw/lz/guardians_of_the_truth/).\n\nThis seems like a strong statement, at least the first part of it.  General Relativity seems well-supported, yet who knows but that some future physicist may overturn it?\n\nOn the other hand, we are never going _back_ to Newtonian mechanics.  The ratchet of science turns, but it does not turn in reverse_._  There are cases in scientific history where a theory suffered a wound or two, and then bounced back; but when a theory takes as many arrows through the chest as Newtonian mechanics, it _stays dead._\n\n\"To hell with what past civilizations thought\" seems safe enough, when past civilizations believed in something that has been falsified to the trash heap of history.\n\nAnd reductionism is not so much a positive hypothesis, as the _absence_ of belief—in particular, disbelief in a form of the Mind Projection Fallacy.\n\nI once met a fellow who claimed that he had experience as a Navy gunner, and he said, \"When you fire artillery shells, you've got to compute the trajectories using Newtonian mechanics.  If you compute the trajectories using relativity, you'll get the wrong answer.\"\n\nAnd I, and another person who was present, said flatly, \"No.\"  I added, \"You might not be able to compute the trajectories fast enough to get the answers in time—maybe that's what you mean?  But the relativistic answer will always be more accurate than the Newtonian one.\"\n\n\"No,\" he said, \"I mean that relativity will give you the _wrong answer,_ because things moving at the speed of artillery shells are governed by Newtonian mechanics, not relativity.\"\n\n\"If that were really true,\" I replied, \"you could publish it in a physics journal and collect your Nobel Prize.\"\n\nStandard physics uses the same _fundamental_ theory to describe the flight of a Boeing 747 airplane, and collisions in the Relativistic Heavy Ion Collider.  Nuclei and airplanes alike, according to our understanding, are obeying special relativity, quantum mechanics, and chromodynamics.\n\nBut we use entirely different _models_ to understand the aerodynamics of a 747 and a collision between gold nuclei in the RHIC.  A computer modeling the aerodynamics of a 747 may not contain a single token, a single bit of RAM, that represents a quark.\n\nSo is the 747 made of something other than quarks?  No, you're just _modeling_ it with _representational elements_ that do not have a one-to-one correspondence with the quarks of the 747.  The map is not the territory.\n\nWhy _not_ model the 747 with a chromodynamic representation?  Because then it would take a gazillion years to get any answers out of the model.  Also we could not store the model on all the memory on all the computers in the world, as of 2008.\n\nAs the saying goes, \"The map is not the territory, but you can't fold up the territory and put it in your glove compartment.\"  Sometimes you need a smaller map to fit in a more cramped glove compartment—but this does not change the territory.  The scale of a map is not a fact about the territory, it's a fact about the map.\n\nIf it _were_ possible to build and run a chromodynamic model of the 747, it would yield accurate predictions.  Better predictions than the aerodynamic model, in fact.\n\nTo build a fully accurate model of the 747, it is not necessary, in principle, for the model to contain explicit descriptions of things like airflow and lift.  There does not have to be a single token, a single bit of RAM, that corresponds to the position of the wings.  It is possible, in principle, to build an accurate model of the 747 that makes no mention of anything _except_ elementary particle fields and fundamental forces.\n\n\"What?\" cries the antireductionist.  \"Are you telling me the 747 _doesn't really have wings?_  I can see the wings right there!\"\n\nThe notion here is a subtle one.  It's not _just_ the notion that an object can have different descriptions at different levels.\n\nIt's the notion that \"having different descriptions at different levels\" is _itself_ something you say that belongs in the realm of Talking About Maps, not the realm of Talking About Territory.\n\nIt's not that the _airplane itself,_ the _laws of physics themselves,_ use different descriptions at different levels—as yonder artillery gunner thought.  Rather _we,_ for our convenience, use different simplified models at different levels.\n\nIf you looked at the ultimate chromodynamic model, the one that contained only elementary particle fields and fundamental forces, that model would contain all the facts about airflow and lift and wing positions—but these facts would be implicit, rather than explicit.\n\nYou, looking _at_ the model, and thinking _about_ the model, would be able to figure out where the wings were.  Having figured it out, there would be an explicit representation in your mind of the wing position—an explicit computational object, there in your neural RAM.  _In your mind._\n\nYou might, indeed, deduce all sorts of explicit descriptions of the airplane, at various levels, and even explicit rules for how your models at different levels interacted with each other to produce combined predictions—\n\nAnd the way that [algorithm feels from inside](/lw/no/how_an_algorithm_feels_from_inside/), is that the airplane would _seem_ to be made up of many levels at once, interacting with each other.\n\nThe way a belief _feels from inside_, is that you seem to be looking straight at reality.  When it actually _seems_ that you're looking at a belief, as such, you are really [experiencing a belief about belief](/lw/om/qualitatively_confused/).\n\nSo when your mind simultaneously believes explicit descriptions of many different levels, and believes explicit rules for transiting between levels, as part of an efficient combined model, it _feels like_ you are seeing a system that is _made of_ different level descriptions and their rules for interaction.\n\nBut this is just the brain trying to be efficiently compress an object that it cannot remotely begin to model on a fundamental level.  The airplane is too large.  Even a hydrogen atom would be too large.  Quark-to-quark interactions are insanely intractable.  You can't handle the _truth._\n\nBut the way physics _really_ works, as far as we can tell, is that there is _only_ the most basic level—the elementary particle fields and fundamental forces.  You can't handle the raw truth, but reality can handle it without the slightest simplification.  (I wish I knew where Reality got its computing power.)\n\nThe laws of physics do not contain distinct additional causal entities that correspond to lift or airplane wings, the way that _the mind of an engineer_ contains distinct additional _cognitive_ entities that correspond to lift or airplane wings.\n\nThis, as I see it, is the thesis of reductionism.  Reductionism is not a positive belief, but rather, a disbelief that the higher levels of simplified multilevel models are out there in the territory.  Understanding this on a gut level [dissolves the question](/lw/of/dissolving_the_question/) of \"How can you say the airplane doesn't really have wings, when I can see the wings right there?\"  The critical words are _really_ and _see_."
          },
          "voteCount": 80
        },
        {
          "name": "Explaining vs. Explaining Away",
          "type": "post",
          "slug": "explaining-vs-explaining-away",
          "_id": "cphoF8naigLhRf3tu",
          "url": null,
          "title": "Explaining vs. Explaining Away",
          "author": "Eliezer_Yudkowsky",
          "question": false,
          "tags": [
            {
              "name": "Causality"
            },
            {
              "name": "Reductionism"
            },
            {
              "name": "Physics"
            },
            {
              "name": "Emotions"
            }
          ],
          "tableOfContents": null,
          "contents": {
            "markdown": "John Keats's [_Lamia_](http://en.wikisource.org/wiki/Lamia) (1819) surely deserves some kind of award for Most Famously Annoying Poetry:\n\n>                     ...Do not all charms fly  \n> At the mere touch of cold philosophy?  \n> There was an awful rainbow once in heaven:  \n> We know her woof, her texture; she is given  \n> In the dull catalogue of common things.  \n> Philosophy will clip an Angel's wings,  \n> Conquer all mysteries by rule and line,  \n> Empty the haunted air, and gnomed mine—  \n> Unweave a rainbow...\n\nMy usual reply ends with the phrase:  \"If we cannot learn to take joy in the merely real, our lives will be empty indeed.\"  I shall expand on that tomorrow.\n\nToday I have a different point in mind.  Let's just take the lines:\n\n> Empty the haunted air, and gnomed mine—  \n> Unweave a rainbow...\n\nApparently \"the mere touch of cold philosophy\", i.e., the truth, has destroyed:\n\n*   Haunts in the air\n*   Gnomes in the mine\n*   Rainbows\n\nWhich calls to mind a rather different bit of [verse](http://www.youtube.com/watch?v=Ect-kgxBb4M):\n\n> One of these things  \n> Is not like the others  \n> One of these things  \n> Doesn't belong\n\nThe air has been emptied of its haunts, and the mine de-gnomed—but the rainbow is still there!\n\nIn \"[Righting a Wrong Question](http://www.overcomingbias.com/2008/03/righting-a-wron.html)\", I wrote:\n\n> Tracing back the chain of causality, step by step, I discover that my belief that I'm wearing socks is fully explained by the fact that I'm wearing socks...  On the other hand, if I see a mirage of a lake in the desert, the correct causal explanation of my vision does not involve the fact of any actual lake in the desert.  In this case, my belief in the lake is not just _explained,_ but _explained away._\n\nThe rainbow was _explained._  The haunts in the air, and gnomes in the mine, were _explained away._\n\nI think this is the key distinction that anti-reductionists don't get about reductionism.\n\nYou can see this failure to get the distinction in the classic objection to reductionism:\n\n> If reductionism is correct, then even your belief in reductionism is just the mere result of the motion of molecules—why should I listen to anything you say?\n\nThe key word, in the above, is _mere;_ a word which implies that accepting reductionism would explain _away_ all the reasoning processes leading up to my acceptance of reductionism, the way that an optical illusion is explained _away_.\n\nBut you can explain how a cognitive process works without it being \"mere\"!  My belief that I'm wearing socks is a mere result of my visual cortex reconstructing nerve impulses sent from my retina which received photons reflected off my socks... which is to say, according to scientific reductionism, my belief that I'm wearing socks is a mere result of the fact that I'm wearing socks.\n\nWhat could be [going on in the anti-reductionists' minds](http://www.overcomingbias.com/2008/03/dissolving-the.html), such that they would put rainbows and belief-in-reductionism, in the same category as haunts and gnomes?\n\nSeveral things are going on simultaneously.  But for now let's focus on the basic idea introduced yesterday:  The [Mind Projection Fallacy](http://www.overcomingbias.com/2008/03/mind-projection.html) between a multi-level map and a mono-level territory.\n\n(I.e:  There's no way you can model a 747 quark-by-quark, so you've _got_ to use a multi-level map with explicit cognitive representations of wings, airflow, and so on.  This doesn't mean there's a multi-level territory.  The true laws of physics, to the best of our knowledge, are only over elementary particle fields.)\n\nI think that when physicists say \"There are no _fundamental_ rainbows,\" the anti-reductionists hear, \"There are no rainbows.\"\n\nIf you don't distinguish between the multi-level map and the mono-level territory, then when someone tries to explain to you that the rainbow is not a fundamental thing in physics, acceptance of this will _feel like_ erasing rainbows from your multi-level map, which _feels like_ erasing rainbows from the world.\n\nWhen Science says \"tigers are not _elementary_ particles, they are made of quarks\" the anti-reductionist hears this as the same sort of dismissal as \"we looked in your garage for a dragon, but there was just empty air\".\n\nWhat scientists did to rainbows, and what scientists did to gnomes, seemingly felt the same to Keats...\n\nIn support of this sub-thesis, I deliberately used several phrasings, in my discussion of Keats's poem, that were Mind Projection Fallacious.  If you didn't notice, this would seem to argue that such fallacies are customary enough to pass unremarked.\n\nFor example:\n\n> \"The air has been emptied of its haunts, and the mine de-gnomed—but the rainbow is still there!\"\n\nActually, Science emptied the _model of_ air of _belief in_ haunts, and emptied the _map of_ the mine of _representations of_ gnomes.  Science did not actually—as Keats's poem itself would have it—take real Angel's wings, and destroy them with a cold touch of truth.  In reality there _never were_ any haunts in the air, or gnomes in the mine.\n\nAnother example:\n\n> \"What scientists did to rainbows, and what scientists did to gnomes, seemingly felt the same to Keats.\"\n\nScientists didn't _do_ anything _to_ gnomes, only to \"gnomes\".  The quotation is not the referent.\n\nBut if you commit the Mind Projection Fallacy—and by default, our beliefs just feel like the way the world _is_—then at time T=0, the mines (apparently) contain gnomes; at time T=1 a scientist dances across the scene, and at time T=2 the mines (apparently) are empty.  Clearly, there used to be gnomes there, but the scientist killed them.\n\nBad scientist!  No poems for you, gnomekiller!\n\nWell, that's how it _feels,_ if you get emotionally attached to the gnomes, and then a scientist says there aren't any gnomes.  It takes a strong mind, a deep honesty, and a deliberate effort to say, at this point, \"That which can be destroyed by the truth should be,\" and \"The scientist hasn't taken the gnomes away, only taken my delusion away,\" and \"I never held just title to my belief in gnomes in the first place; I have not been deprived of anything I _rightfully_ owned,\" and \"If there are gnomes, I desire to believe there are gnomes; if there are no gnomes, I desire to believe there are no gnomes; let me not become attached to beliefs I may not want,\" and all the other things that rationalists are supposed to say on such occasions.\n\nBut with the rainbow it is not even necessary to go that far.  The rainbow is _still there!_"
          },
          "voteCount": 75
        },
        {
          "name": "Fake Reductionism",
          "type": "post",
          "slug": "fake-reductionism",
          "_id": "mTf8MkpAigm3HP6x2",
          "url": null,
          "title": "Fake Reductionism",
          "author": "Eliezer_Yudkowsky",
          "question": false,
          "tags": [
            {
              "name": "Reductionism"
            },
            {
              "name": "Emotions"
            },
            {
              "name": "Distinctions"
            }
          ],
          "tableOfContents": null,
          "contents": {
            "markdown": "> There was an awful rainbow once in heaven:  \n> We know her woof, her texture; she is given  \n> In the dull catalogue of common things.  \n>         —John Keats, _Lamia_  \n\nI am guessing—though it is only a guess—that Keats himself did _not_ know the woof and texture of the rainbow.  Not the way that Newton understood rainbows.  Perhaps not even at all.  Maybe Keats just read, somewhere, that Newton had explained the rainbow as \"light reflected from raindrops\"—\n\n—which was actually known in the 13th century.  Newton only added a refinement by showing that the light was decomposed into colored parts, rather than transformed in color.  But that put rainbows back in the news headlines.  And so Keats, with Charles Lamb and William Wordsworth and Benjamin Haydon, drank \"Confusion to the memory of Newton\" because \"he destroyed the poetry of the rainbow by reducing it to a prism.\" That's one reason to suspect Keats didn't understand the subject too deeply.\n\nI am guessing, though it is only a guess, that Keats could _not_ have sketched out on paper why rainbows only appear when the Sun is behind your head, or why the rainbow is an arc of a circle.\n\nIf so, Keats had a [Fake Explanation](/lw/ip/fake_explanations/).  In this case, a _fake reduction_.  He'd been _told that_ the rainbow had been reduced, but it had not actually _been reduced_ in his model of the world.\n\nThis is another of those distinctions that anti-reductionists fail to get—the difference between [professing](/lw/i4/belief_in_belief/) the flat fact that something is reducible, and _seeing_ it.\n\nIn this, the anti-reductionists are not too greatly to be blamed, for it is part of a general problem.\n\nI've written before on [seeming knowledge that is not knowledge](/lw/ip/fake_explanations/), and beliefs that are not _about_ their supposed objects but only [recordings to recite back in the classroom](/lw/iq/guessing_the_teachers_password/), and words that operate as [stop signs for curiosity](/lw/it/semantic_stopsigns/) rather than answers, and technobabble which only conveys membership in the [literary genre of \"science\"](/lw/ir/science_as_attire/)...\n\nThere is a very great distinction between being able to _see_ where the rainbow comes from, and playing around with prisms to confirm it, and maybe making a rainbow yourself by spraying water droplets—\n\n—versus some dour-faced philosopher just _telling_ you, \"No, there's nothing special about the rainbow.  Didn't you hear? Scientists have explained it away.  Just something to do with raindrops or whatever.  Nothing to be excited about.\"\n\nI think this distinction probably accounts for a hell of a lot of the deadly existential emptiness that supposedly accompanies scientific reductionism.\n\nYou have to interpret the anti-reductionists' experience of \"reductionism\", not in terms of their _actually seeing_ how rainbows work, not in terms of their having the critical \"Aha!\", but in terms of their being told that the [password](/lw/iq/guessing_the_teachers_password/) is \"Science\".  The effect is just to move rainbows to a different _literary genre_—a [literary genre](/lw/ir/science_as_attire/) they have been [taught](/lw/k5/cached_thoughts/) to regard as [boring](/lw/j3/science_as_curiositystopper/).\n\nFor them, the effect of hearing \"Science has explained rainbows!\" is to hang up a sign over rainbows saying, \"This phenomenon has been labeled BORING by order of the Council of Sophisticated Literary Critics.  Move along.\"\n\nAnd that's all the sign says: only that, and nothing more.\n\nSo the literary critics have their gnomes yanked out by force; not dissolved in insight, but removed by flat order of authority.  They are given no beauty to replace the hauntless air, no genuine understanding that could be interesting in its own right.  Just a label saying, \"Ha!  You thought rainbows were pretty?  You poor, unsophisticated fool.  This is part of the literary genre of science, of dry and solemn incomprehensible words.\"\n\nThat's how anti-reductionists experience \"reductionism\".\n\nWell, can't blame Keats, poor lad probably wasn't raised right.\n\nBut he dared to drink \"Confusion to the memory of Newton\"?\n\nI propose \"To the memory of Keats's confusion\" as a toast for rationalists.  Cheers."
          },
          "voteCount": 69
        }
      ]
    },
    {
      "title": "Selected posts from [[Joy in the Merely Real]]",
      "description": "",
      "children": [
        {
          "name": "Joy in the Merely Real",
          "type": "post",
          "slug": "joy-in-the-merely-real",
          "_id": "x4dG4GhpZH2hgz59x",
          "url": null,
          "title": "Joy in the Merely Real",
          "author": "Eliezer_Yudkowsky",
          "question": false,
          "tags": [
            {
              "name": "Reductionism"
            },
            {
              "name": "Emotions"
            },
            {
              "name": "Fun Theory"
            }
          ],
          "tableOfContents": null,
          "contents": {
            "markdown": ">                     ...Do not all charms fly  \n> At the mere touch of cold philosophy?  \n> There was an awful rainbow once in heaven:  \n> We know her woof, her texture; she is given  \n> In the dull catalogue of common things.  \n>         —John Keats, _Lamia_\n> \n> \"Nothing is 'mere'.\"  \n>         —Richard Feynman\n\nYou've got to admire that phrase, \"dull catalogue of common things\".  What is it, exactly, that goes in this catalogue?  Besides rainbows, that is?\n\nWhy, things that are mundane, of course.  Things that are normal; things that are unmagical; things that are known, or knowable; things that play by the rules (or that play by _any_ rules, which makes them boring); things that are part of the ordinary universe; things that are, in a word, _real._\n\nNow that's what I call setting yourself up for a fall.\n\nAt that rate, sooner or later you're going to be disappointed in _everything_—either it will turn out not to exist, or even worse, it will turn out to be real.\n\nIf we cannot take joy in things that are merely real, our lives will _always_ be empty.\n\nFor what sin are rainbows demoted to the dull catalogue of common things?  For the sin of having a scientific explanation.  \"We know her woof, her texture\", says Keats—an interesting use of the word \"we\", because I [suspect that Keats didn't know](/lw/op/fake_reductionism/) the explanation himself.  I suspect that just being [told that someone else knew](/lw/j3/science_as_curiositystopper/) was too much for him to take.  I suspect that just the notion of rainbows being scientifically explicable _in principle_ would have been too much to take.  And if Keats didn't think like that, well, I know plenty of people who do.\n\nI have already remarked that nothing is _inherently_ [mysterious](/lw/iu/mysterious_answers_to_mysterious_questions/)—nothing that actually exists, that is.  If I am [ignorant](/lw/oj/probability_is_in_the_mind/) about a phenomenon, that is [a fact about my state of mind](/lw/oi/mind_projection_fallacy/), not a fact about the phenomenon; to [worship](/lw/j2/explainworshipignore/) a phenomenon because it seems so wonderfully mysterious, is to worship your own ignorance; a blank map does not correspond to a blank territory, it is just somewhere we haven't visited yet, etc. etc...\n\nWhich is to say that _everything_—everything that _actually_ exists—is liable to end up in \"the dull catalogue of common things\", sooner or later.\n\nYour choice is either:\n\n*   Decide that things are allowed to be unmagical, knowable, scientifically explicable, in a word, _real,_ and yet still worth caring about;\n*   Or go about the rest of your life suffering from existential ennui that is _unresolvable._\n\n(Self-deception might be an option for others, but [not for you](/lw/je/doublethink_choosing_to_be_biased/).)\n\nThis puts quite a different complexion on the bizarre habit indulged by those strange folk called _scientists,_ wherein they suddenly become fascinated by pocket lint or bird droppings or rainbows, or some other ordinary thing which world-weary and sophisticated folk would never give a second glance.\n\nYou might say that scientists—at least _some_ scientists—are those folk who are _in principle_ capable of enjoying life in the real universe."
          },
          "voteCount": 108
        },
        {
          "name": "Bind Yourself to Reality",
          "type": "post",
          "slug": "bind-yourself-to-reality",
          "_id": "WjpA4PCjt5EkTGbLF",
          "url": null,
          "title": "Bind Yourself to Reality",
          "author": "Eliezer_Yudkowsky",
          "question": false,
          "tags": [
            {
              "name": "Contact with Reality"
            },
            {
              "name": "Rationality"
            }
          ],
          "tableOfContents": null,
          "contents": {
            "markdown": "So perhaps you're reading all this, and asking:  \"Yes, but what does this have to do with [reductionism](/lw/on/reductionism/)?\"\n\nPartially, it's a matter of [leaving a line of retreat](/lw/o4/leave_a_line_of_retreat/).  It's not easy to take something _important_ apart into components, when you're convinced that this removes magic from the world, unweaves the rainbow.  I do plan to take certain things apart, on this blog; and I prefer not to create pointless existential anguish.\n\nPartially, it's the crusade against Hollywood Rationality, the concept that understanding the rainbow subtracts its beauty.  The rainbow is still beautiful _plus_ you get the beauty of physics.\n\nBut even more deeply, it's one of these subtle [hidden-core-of-rationality](http://yudkowsky.net/virtues/) things.  You know, the sort of thing where I start talking about '[the Way](/lw/m7/zen_and_the_art_of_rationality/)'.  It's about _binding yourself to reality._\n\nIn one of Frank Herbert's _Dune_ books, IIRC, it is said that a Truthsayer gains their ability to detect lies in others by always speaking truth themselves, so that they form a relationship with the truth whose violation they can feel.  It wouldn't work, but I still think it's one of the more beautiful thoughts in fiction.  At the very least, to get close to the truth, you have to be willing to press yourself up against reality as tightly as possible, without flinching away, or sneering down.\n\nYou can see the bind-yourself-to-reality theme in \"[Lotteries:  A Waste of Hope](/lw/hl/lotteries_a_waste_of_hope/).\"  Understanding that lottery tickets have negative expected utility, does not mean that you give up the hope of being rich.  It means that you stop wasting that hope on lottery tickets.  You put the hope into your job, your school, your startup, your eBay sideline; and if you truly have nothing worth hoping for, then maybe it's time to start looking.\n\nIt's not dreams I object to, only _impossible_ dreams.  The lottery isn't impossible, but it is an un-actionable near-impossibility.  It's not that winning the lottery is extremely _difficult_—requires a desperate effort—but that _work_ isn't the issue.\n\nI say all this, to exemplify the idea of taking emotional energy that is flowing off to nowhere, and binding it into the realms of reality.\n\nThis doesn't mean setting goals that are low enough to be \"realistic\", i.e., easy and safe and parentally approved.  Maybe this is good advice in your personal case, I don't know, but I'm not the one to say it.\n\nWhat I mean is that you can invest emotional energy in rainbows even if they turn out _not_ to be magic.  [The future is always absurd](/lw/j6/why_is_the_future_so_absurd/) but it is never _unreal._\n\nThe Hollywood Rationality stereotype is that \"rational = emotionless\"; the more reasonable you are, the more of your emotions Reason inevitably destroys.  In \"[Feeling Rational](/lw/hp/feeling_rational/)\" I contrast this against _\"That which can be destroyed by the truth should be\"_ and _\"That which the truth nourishes should thrive\"._  When you have arrived at your best picture of the truth, there is nothing irrational about the emotions you feel as a result of that—the emotions cannot be destroyed by truth, so they must not be irrational.\n\nSo instead of _destroying_ emotional energies associated with bad explanations for rainbows, as the Hollywood Rationality stereotype would have it, let us _redirect_ these emotional energies into reality—bind them to beliefs that are as true as we can make them.\n\nWant to fly?  Don't give up on flight.  Give up on flying potions and build yourself an airplane.\n\nRemember the theme of \"[Think Like Reality](/lw/hs/think_like_reality/)\", where I talked about how when physics seems counterintuitive, you've got to accept that it's not _physics_ that's weird, it's _you_?\n\nWhat I'm talking about now is like that, only with [emotions](/lw/hp/feeling_rational/) instead of hypotheses—binding your feelings into the real world.  Not the \"realistic\" everyday world.  I would be a howling hypocrite if I told you to shut up and do your homework.  I mean the _real_ real world, the [lawful universe](/lw/hr/universal_law/), that includes [absurdities](/lw/j6/why_is_the_future_so_absurd/) like Moon landings and the evolution of human intelligence.  Just not any magic, anywhere, ever_._\n\nIt is a Hollywood Rationality meme that \"Science takes the fun out of life.\"\n\nScience puts the fun back _into_ life.\n\nRationality directs your emotional energies into the universe, rather than somewhere else."
          },
          "voteCount": 46
        },
        {
          "name": "If You Demand Magic, Magic Won't Help",
          "type": "post",
          "slug": "if-you-demand-magic-magic-won-t-help",
          "_id": "iiWiHgtQekWNnmE6Q",
          "url": null,
          "title": "If You Demand Magic, Magic Won't Help",
          "author": "Eliezer_Yudkowsky",
          "question": false,
          "tags": [
            {
              "name": "Fiction (Topic)"
            },
            {
              "name": "Emotions"
            },
            {
              "name": "Aesthetics"
            }
          ],
          "tableOfContents": null,
          "contents": {
            "markdown": "> Most witches don't believe in gods.  They know that the gods exist, of course.  They even deal with them occasionally.  But they don't believe in them.  They know them too well.  It would be like believing in the postman.  \n>         —Terry Pratchett, _Witches Abroad_\n\nOnce upon a time, I was pondering the philosophy of fantasy stories—\n\nAnd before anyone chides me for my \"failure to understand what fantasy is about\", let me say this:  I was raised in an SF&F household.  I have been reading fantasy stories since I was five years old.  I occasionally try to _write_ fantasy [stories](http://yudkowsky.net/other/fiction).  And I am _not_ the sort of person who tries to write for a genre without pondering its philosophy.  Where do you think story ideas come from?\n\nAnyway:\n\nI was pondering the philosophy of fantasy stories, and it occurred to me that if there were actually dragons in our world—if you could go down to the zoo, or even to a distant mountain, and meet a fire-breathing dragon—while nobody had ever actually seen a zebra, then our fantasy stories would contain zebras aplenty, while dragons would be unexciting.\n\nNow that's what I call painting yourself into a corner, wot?  The grass is always greener on the other side of unreality.\n\nIn one of the standard fantasy plots, a protagonist from our Earth, a sympathetic character with lousy grades or a crushing mortgage but still a good heart, [suddenly finds themselves in a world](/lw/hq/universal_fire/) where magic operates in place of science.  The protagonist often goes on to practice magic, and become in due course a (superpowerful) sorcerer.\n\nNow here's the question—and yes, it is a little unkind, but I think it needs to be asked:  Presumably most readers of these novels see themselves in the protagonist's shoes, fantasizing about their own acquisition of sorcery.  Wishing for magic.  And, barring improbable demographics, most readers of these novels are not scientists.\n\nBorn into a world of science, they did not become scientists.  What makes them think that, in a world of magic, they would act any differently?\n\nIf they don't have the scientific attitude, that [nothing is \"mere\"](/lw/or/joy_in_the_merely_real/)—the capacity to be interested in merely real things—how will magic help them?  If they actually _had_ magic, it would be merely _real,_ and lose the charm of unattainability.  They might be excited at first, but (like the lottery winners who, six months later, aren't nearly as happy as they expected to be), the excitement would soon wear off.  Probably as soon as they had to actually _study_ spells.\n\n_Unless_ they can find the capacity to take joy in things that are merely real.  To be just as excited by hang-gliding, as riding a dragon; to be as excited by making a light with electricity, as by making a light with magic... even if it takes a little study...\n\nDon't get me wrong.  I'm not dissing dragons.  Who knows, we might even create some, one of these days.\n\nBut if you don't have the capacity to enjoy hang-gliding even though it is _merely real_, then as soon as dragons _turn_ real, you're not going to be any more excited by dragons than you are by hang-gliding.\n\nDo you think you would prefer living in the Future, to living in the present?  That's a quite understandable preference.  Things do seem to be getting better over time.\n\nBut don't forget that _this is_ the Future, relative to the Dark Ages of a thousand years earlier.  You have opportunities undreamt-of even by kings.\n\nIf the trend continues, the Future might be a very fine place indeed in which to live.  But if you do make it to the Future, what you find, when you get there, will be another Now.  If you don't have the basic capacity to enjoy being in a Now—if your emotional energy can _only_ go into the Future, if you can _only_ hope for a better tomorrow—then no amount of passing time can help you.\n\n(Yes, in the Future there could be a pill that fixes the emotional problem of always looking to the Future.  I don't think this invalidates my basic point, which is about what sort of pills we should want to take.)\n\nMatthew C., [commenting here on LW](/lw/on/reductionism/irh), seems very excited about an informally specified \"theory\" by Rupert Sheldrake which \"[explains](/lw/is/fake_causality/)\" such non-explanation-demanding phenomena as protein folding and snowflake symmetry.  But why isn't Matthew C. just as excited about, say, Special Relativity?  Special Relativity is actually _known_ to be a law, so why isn't it even _more_ exciting?  The advantage of becoming excited about a law already known to be true, is that you know your excitement will not be wasted.\n\nIf Sheldrake's theory were accepted truth taught in elementary schools, Matthew C. wouldn't care about it.  Or why else is Matthew C. fascinated by that one particular law which he believes to be a law of physics, more than all the other laws?\n\nThe worst catastrophe you could visit upon the New Age community would be for their rituals to start working reliably, and for UFOs to actually appear in the skies.  What would be the point of believing in aliens, if they were just _there,_ and everyone else could see them too?  In a world where psychic powers were merely real, New Agers wouldn't _believe in_ psychic powers, any more than anyone cares enough about gravity to believe in it.  (Except for scientists, of course.)\n\nWhy am I so negative about magic?  Would it be _wrong_ for magic to exist?\n\nI'm not actually negative on magic.  Remember, I occasionally try to write fantasy stories.  But I'm annoyed with this psychology that, if it were born into a world where spells and potions did work, would pine away for a world where household goods were abundantly produced by assembly lines.\n\nPart of binding yourself to reality, on an emotional as well as intellectual level, is coming to terms with the fact that you _do live here._  Only then can you see this, your world, and whatever opportunities it holds out for you, without wishing your sight away.\n\nNot to put too fine a point on it, but _I've_ found no lack of dragons to fight, or magics to master, in this world of my birth.  If I were transported into one of those fantasy novels, I wouldn't be surprised to find myself studying the forbidden ultimate sorcery—\n\n—because why should being transported into a magical world change anything?  It's not _where_ you are, it's _who_ you are.\n\nSo remember the Litany Against Being Transported Into An Alternate Universe:\n\nIf I'm going to be happy anywhere,  \nOr achieve greatness anywhere,  \nOr learn true secrets anywhere,  \nOr save the world anywhere,  \nOr feel strongly anywhere,  \nOr help people anywhere,  \nI may as well do it in reality."
          },
          "voteCount": 114
        },
        {
          "name": "Mundane Magic",
          "type": "post",
          "slug": "mundane-magic",
          "_id": "SXK87NgEPszhWkvQm",
          "url": null,
          "title": "Mundane Magic",
          "author": "Eliezer_Yudkowsky",
          "question": false,
          "tags": [
            {
              "name": "Aesthetics"
            },
            {
              "name": "Reductionism"
            }
          ],
          "tableOfContents": null,
          "contents": {
            "markdown": "As you may recall from some months earlier, I think that part of the rationalist ethos is _binding yourself emotionally_ to an [absolutely lawful](/lw/hr/universal_law/) [reductionistic](/lw/on/reductionism/) universe—a universe containing [no ontologically basic mental things](/lw/tv/excluding_the_supernatural/) such as souls or [magic](/lw/ou/if_you_demand_magic_magic_wont_help/)—and pouring all your hope and all your care into that merely real universe and its possibilities, without disappointment.\n\nThere's an old trick for combating [dukkha](http://en.wikipedia.org/wiki/Dukkha) where you make a list of things you're grateful for, like a roof over your head.\n\nSo why not make a list of abilities you have that would be amazingly cool _if they were magic,_ or [if only a few chosen individuals had them](/lw/os/joy_in_discovery/)?\n\nFor example, suppose that instead of one eye, you possessed a magical _second_ eye embedded in your forehead.  And this second eye enabled you to _see into the third dimension_—so that you could somehow tell _how far away_ things were—where an ordinary eye would see only a two-dimensional shadow of the true world.  Only the possessors of this ability can accurately aim the legendary distance-weapons that kill at ranges far beyond a sword, or use to their fullest potential the shells of ultrafast machinery called \"cars\".\n\n\"Binocular vision\" would be [too light a term](http://chaitanya1.files.wordpress.com/2008/06/bigbang.gif) for this ability.  We'll only appreciate it once it has a properly impressive name, like Mystic Eyes of Depth Perception.\n\nSo here's a list of some of my favorite magical powers:\n\n*   _Vibratory Telepathy_.  By transmitting invisible vibrations through the very air itself, two users of this ability can _share thoughts_.  As a result, Vibratory Telepaths can form emotional bonds much deeper than those possible to other primates.\n*   _Psychometric Tracery._  By tracing small fine lines on a surface, the Psychometric Tracer can leave impressions of emotions, history, knowledge, even the structure of other spells.  This is a higher level than Vibratory Telepathy as a Psychometric Tracer can share the thoughts of long-dead Tracers who lived thousands of years earlier.  By reading one Tracery and inscribing another simultaneously, Tracers can duplicate Tracings; and these replicated Tracings can even contain the detailed pattern of other spells and magics.  Thus, the Tracers wield almost unimaginable power as magicians; but Tracers can get in trouble trying to use complicated Traceries that they could not have Traced themselves.\n*   _Multidimensional Kinesis._  With simple, almost unthinking acts of will, the Kinetics can cause extraordinarily complex forces to flow through small tentacles and into any physical object within touching range—not just pushes, but combinations of pushes at many points that can effectively apply torques and twists.  The Kinetic ability is far subtler than it first appears: they use it not only to wield existing objects with martial precision, but also to apply forces that sculpt objects into forms more suitable for Kinetic wielding.  They even create tools that extend the power of their Kinesis and enable them to sculpt ever-finer and ever-more-complicated tools, a positive feedback loop fully as impressive as it sounds.\n*   _The Eye._  The user of this ability can perceive infinitesimal traveling twists in the Force that binds matter—tiny vibrations, akin to the life-giving power of the Sun that falls on leaves, but far more subtle.  A bearer of the Eye can sense objects far beyond the range of touch using the tiny disturbances they make in the Force.  Mountains many days travel away can be known to them as if within arm's reach.  According to the bearers of the Eye, when night falls and sunlight fails, they can sense huge fusion fires burning at unthinkable distances—though no one else has any way of verifying this.  Possession of a single Eye is said to make the bearer equivalent to royalty.\n\nAnd finally,\n\n*   _The Ultimate Power._  The user of this ability contains a smaller, imperfect echo of the entire universe, enabling them to search out paths through probability to any desired future.  If this sounds like a ridiculously powerful ability, you're right—game balance goes right out the window with this one.  Extremely rare among life forms, it is the _sekai no ougi_ or \"hidden technique of the world\".\n    \n    Nothing can oppose the Ultimate Power except the Ultimate Power.  Any less-than-ultimate Power will simply be \"comprehended\" by the Ultimate and disrupted in some inconceivable fashion, or even absorbed into the Ultimates' own power base.  For this reason the Ultimate Power is sometimes called the \"master technique of techniques\" or the \"trump card that trumps all other trumps\".  The more powerful Ultimates can stretch their \"comprehension\" across galactic distances and aeons of time, and even perceive the bizarre laws of the hidden \"world beneath the world\".\n    \n    Ultimates have been killed by immense natural catastrophes, or by extremely swift surprise attacks that give them no chance to use their power.  But all such victories are ultimately a matter of luck—it does not confront the Ultimates on their own probability-bending level, and if they survive they will begin to bend Time to avoid future attacks.\n    \n    But the Ultimate Power itself is also dangerous, and many Ultimates have been destroyed by their own powers—falling into one of the flaws in their imperfect inner echo of the world.\n    \n    Stripped of weapons and armor and locked in a cell, an Ultimate is still one of the most dangerous life-forms on the planet.  A sword can be broken and a limb can be cut off, but the Ultimate Power is \"the power that cannot be removed without removing you\".\n    \n    Perhaps because this connection is so intimate, the Ultimates regard one who loses their Ultimate Power permanently—without hope of regaining it—as _schiavo,_ or \"dead while breathing\".  The Ultimates argue that the Ultimate Power is so important as to be a necessary part of what makes a creature an end in itself, rather than a means.  The Ultimates even insist that anyone who lacks the Ultimate Power cannot begin to truly comprehend the Ultimate Power, and hence, cannot understand why the Ultimate Power is morally important—a suspiciously self-serving argument.\n    \n    The users of this ability form an absolute aristocracy and treat all other life forms as their pawns."
          },
          "voteCount": 162
        },
        {
          "name": "The Beauty of Settled Science",
          "type": "post",
          "slug": "the-beauty-of-settled-science",
          "_id": "ndGYn7ZFiZyernp9f",
          "url": null,
          "title": "The Beauty of Settled Science",
          "author": "Eliezer_Yudkowsky",
          "question": false,
          "tags": [
            {
              "name": "Scholarship & Learning"
            },
            {
              "name": "Replication Crisis"
            }
          ],
          "tableOfContents": null,
          "contents": {
            "markdown": "Facts [do not need](/lw/or/joy_in_the_merely_real/) to be unexplainable, to be beautiful; truths do not become [less worth learning](/lw/os/joy_in_discovery/), if someone else knows them; beliefs do not become [less worthwhile](/lw/ou/if_you_demand_magic_magic_wont_help/), if many others share them…\n\n…and if you only care about scientific issues that are controversial, you will end up with a head stuffed full of garbage.\n\nThe media thinks that only the cutting edge of science is worth reporting on. How often do you see headlines like “General Relativity still governing planetary orbits” or “Phlogiston theory remains false”? So, by the time anything is solid science, it is no longer a breaking headline. “Newsworthy” science is often based on the thinnest of evidence and wrong half the time—if it were not on the uttermost fringes of the scientific frontier, it would not be breaking news.\n\nScientific _controversies_ are problems _so difficult_ that even people who’ve spent years mastering the field can still fool themselves. That’s what makes for the heated arguments that attract all the media attention.\n\nWorse, if you aren’t in the field and part of the game, controversies _aren’t even fun_.\n\nOh, sure, you can have the fun of picking a side in an argument. But you can get that in any [football game](/lw/mg/the_twoparty_swindle/). That’s not what the fun of science is about.\n\nReading a well-written textbook, you get: Carefully phrased explanations for incoming students, math derived step by step (where applicable), plenty of experiments cited as illustration (where applicable), test problems on which to display your new mastery, and a reasonably good guarantee that what you’re learning is actually true.\n\nReading press releases, you usually get: [Fake explanations](/lw/ip/fake_explanations/) that convey nothing except the [delusion of understanding](/lw/iq/guessing_the_teachers_password/) of a result that the press release author didn’t understand and that probably has a better-than-even chance of failing to replicate.\n\nModern science is built on discoveries, built on discoveries, built on discoveries, and so on, all the way back to people like Archimedes, who discovered facts like why boats float, that can make sense even if you don’t know about other discoveries. A good place to start traveling that road is at the beginning.\n\nDon’t be embarrassed to read _elementary_ science textbooks, either. If you want to pretend to be sophisticated, go find a play to sneer at. If you just want to have _fun,_ remember that simplicity is at the core of scientific beauty.\n\nAnd thinking you can jump right into the frontier, when you haven’t learned the settled science, is like…\n\n…like trying to climb only the _top_ half of Mount Everest (which is the only part that interests you) by standing at the base of the mountain, bending your knees, and jumping _really hard_ (so you can pass over the boring parts).\n\nNow I’m not saying that you should never pay attention to scientific controversies. If 40% of oncologists think that white socks cause cancer, and the other 60% violently disagree, this is an important fact to know.\n\nJust don’t go thinking that science _has_ to be controversial to be interesting.\n\nOr, for that matter, that science has to be recent to be interesting. A steady diet of science _news_ is bad for you: You are what you eat, and if you eat only science reporting on fluid situations, without a solid textbook now and then, your brain will turn to liquid."
          },
          "voteCount": 73
        },
        {
          "name": "To Spread Science, Keep It Secret",
          "type": "post",
          "slug": "to-spread-science-keep-it-secret",
          "_id": "3diLhMELXxM8rFHJj",
          "url": null,
          "title": "To Spread Science, Keep It Secret",
          "author": "Eliezer_Yudkowsky",
          "question": false,
          "tags": [
            {
              "name": "Practice & Philosophy of Science"
            },
            {
              "name": "World Optimization"
            }
          ],
          "tableOfContents": null,
          "contents": {
            "markdown": "Sometimes I wonder if the Pythagoreans had the right idea.\n\nYes, I've [written](/lw/in/scientific_evidence_legal_evidence_rational/) about how \"science\" is inherently public.  I've written that \"science\" is distinguished from merely rational knowledge by the in-principle ability to reproduce scientific experiments for yourself, to know without relying on authority.  I've said that \"science\" should be defined as the publicly accessible knowledge of humankind.  I've even suggested that future generations will regard all papers not published in an open-access journal as non-science, i.e., it can't be part of the public knowledge of humankind if you make people pay to read it.\n\nBut that's only one vision of the future.  In another vision, the knowledge we now call \"science\" is taken _out_ of the public domain—the books and journals hidden away, guarded by [mystic cults](/lw/md/cultish_countercultishness/) of [gurus](/lw/m2/the_litany_against_gurus/) wearing [robes](/lw/m4/two_cult_koans/), requiring fearsome initiation rituals for access—so that more people will _actually_ study it.\n\nI mean, right now, people _can_ study science but they _don't._\n\n\"[Scarcity](/lw/oz/scarcity/)\", it's called in [social psychology](http://www.google.com/search?q=scarcity+psychology).  What appears to be in limited supply, is more highly valued.  And this effect is _especially_ strong with information—we're much more likely to try to obtain information that we believe is secret, and to value it more when we do obtain it.\n\nWith science, I think, people assume that if the information is freely available, it must not be important.  So instead people join cults that have the sense to keep their Great Truths secret.  The Great Truth may actually be gibberish, but it's more satisfying than coherent science, because it's _secret._\n\nScience is the great Purloined Letter of our times, left out in the open and ignored.\n\nSure, scientific openness helps the scientific elite.  They've already _been_ through the initiation rituals.  But for the rest of the planet, science is kept secret a hundred times more effectively by making it freely available, than if its books were guarded in vaults and you had to walk over hot coals to get access.  (This being a fearsome trial indeed, since the great secrets of insulation are only available to Physicist-Initiates of the Third Level.)\n\nIf scientific knowledge were hidden in ancient vaults (rather than hidden in inconvenient pay-for-access journals), at least then people would _try_ to get into the vaults.  They'd be _desperate_ to learn science.  Especially when they saw the power that Eighth Level Physicists could wield, and were told that they _weren't allowed to know_ the explanation.\n\nAnd if you tried to start a cult around oh, say, Scientology, you'd get some degree of public interest, at first.  But people would very quickly start asking uncomfortable questions like \"Why haven't you given a public demonstration of your Eighth Level powers, like the Physicists?\" and \"How come none of the Master Mathematicians seem to want to join your cult?\" and \"Why should I follow your Founder when he isn't an Eighth Level anything outside his own cult?\" and \"Why should I study _your_ cult _first,_ when the Dentists of Doom can do things that are so much more impressive?\"\n\nWhen you look at it from that perspective, the escape of math from the Pythagorean cult starts to look like a major strategic blunder for humanity.\n\nNow, I know what you're going to say:  \"But science _is_ surrounded by fearsome initiation rituals!  Plus it's _inherently_ difficult to learn!  Why doesn't _that_ count?\"  Because the public _thinks_ that science is freely available, that's why.  If you're _allowed_ to learn, it must not be important enough _to_ learn.\n\nIt's an image problem, people taking their cues from others' attitudes.  Just _anyone_ can walk into the supermarket and buy a light bulb, and nobody looks at it with awe and reverence.  The physics supposedly aren't secret (even though [_you_ don't know](/lw/os/joy_in_discovery/)), and there's a one-paragraph [explanation](/lw/ip/fake_explanations/) in the newspaper that sounds vaguely authoritative and convincing—essentially, no one treats the lightbulb as a sacred mystery, so neither do you.\n\nEven the simplest little things, completely inert objects like crucifixes, can become magical if everyone _looks_ at them like they're magic.  But since you're theoretically _allowed_ to know why the light bulb works without climbing the mountain to find the remote Monastery of Electricians, there's no need to _actually_ bother to learn.\n\nNow, because science does in fact have initiation rituals both social and cognitive, scientists are not wholly dissatisfied with their science.  The problem is that, in the present world, very few people bother to study science in the first place.  Science cannot be the true Secret Knowledge, because just anyone is allowed to know it—_even though, in fact, they don't_.\n\nIf the Great Secret of Natural Selection, passed down from Darwin Who Is Not Forgotten, was only ever imparted to you after you paid $2000 and went through a ceremony involving torches and robes and masks and sacrificing an ox, _then_ when you were shown the fossils, and shown [the optic cable going through the retina](/lw/kr/an_alien_god/) under a microscope, and finally told the Truth, you would say \"That's the most brilliant thing ever!\" and _be satisfied._  After that, if some other cult tried to tell you it was actually a bearded man in the sky 6000 years ago, you'd laugh like hell.\n\nAnd you know, it might actually be more _fun_ to do things that way.  Especially if the initiation required you to put together some of the evidence for yourself—together, or with classmates—before you could tell your Science Sensei you were ready to advance to the next level.  It wouldn't be _efficient,_ sure, but it would be _fun._\n\nIf humanity had never made the mistake—never gone down the religious path, and never learned to fear anything that smacks of religion—then maybe the Ph.D. granting ceremony would involve litanies and chanting, because, hey, that's what people like.  Why take the fun out of everything?\n\nMaybe we're just doing it wrong.\n\nAnd no, I'm not _seriously_ proposing that we try to reverse the last five hundred years of openness and classify all the science secret.  At least, not at the moment.  Efficiency is important for now, especially in things like medical research.  I'm just explaining why it is that I won't tell anyone the Secret of [how the ineffable difference between blueness and redness arises from mere atoms](/lw/og/wrong_questions/) for less than $100,000—\n\nAhem!  I meant to say, I'm telling you about this vision of an alternate Earth, so that you give science equal treatment with cults.  So that you don't undervalue scientific truth when you learn it, _just_ because it doesn't seem to be protected appropriately to its value.  _Imagine_ the robes and masks.  Visualize yourself creeping into the vaults and stealing the Lost Knowledge of Newton.  And don't be fooled by any organization that _does_ use robes and masks, unless they also show you the data.\n\nPeople seem to have [holes in their minds](/lw/oy/is_humanism_a_religionsubstitute/) for Esoteric Knowledge, Deep Secrets, the Hidden Truth.  And I'm not even criticizing this psychology!  There _are_ deep secret esoteric hidden truths, like quantum mechanics or [Bayes-structure](/lw/o7/searching_for_bayesstructure/).  We've just gotten into the habit of presenting the Hidden Truth in a very _unsatisfying_ way, wrapped up in false mundanity.\n\nBut if the holes for secret knowledge are not filled by true beliefs, they will be filled by false beliefs.  There is _nothing but_ science to learn—the emotional energy must either be [invested in reality](/lw/ot/bind_yourself_to_reality/), or wasted in total nonsense, or destroyed.  For myself, I think it is better to invest the emotional energy; fun should not be needlessly cast away.\n\nRight now, we've got the worst of both worlds.  Science isn't _really_ free, because the courses are expensive and the textbooks are expensive.  But the public _thinks_ that anyone is allowed to know, so it must not be important.\n\nIdeally, you would want to arrange things the other way around."
          },
          "voteCount": 66
        }
      ]
    },
    {
      "title": "Selected posts from [[Reductionism (Sequence]] (second half)",
      "description": "",
      "children": [
        {
          "name": "Angry Atoms",
          "type": "post",
          "slug": "angry-atoms",
          "_id": "ddwk9veF8efn3Nzbu",
          "url": null,
          "title": "Angry Atoms",
          "author": "Eliezer_Yudkowsky",
          "question": false,
          "tags": [
            {
              "name": "Reductionism"
            },
            {
              "name": "World Modeling"
            }
          ],
          "tableOfContents": null,
          "contents": {
            "markdown": "Fundamental physics—quarks 'n stuff—is far removed from the levels we can [_see,_](/lw/p2/hand_vs_fingers/) like hands and fingers.  At best, you can know how to replicate the experiments which show that your hand (like everything else) is composed of quarks, and you may know how to derive a few equations for things like atoms and electron clouds and molecules.\n\nAt worst, the existence of quarks beneath your hand may just be something you were [told](/lw/iq/guessing_the_teachers_password/).  In which case it's questionable in one what sense you can be said to \"know\" it at all, even if you repeat back the same word \"quark\" that a physicist would use to convey knowledge to another physicist.\n\nEither way, you can't actually _see_ the identity between levels—no one has a brain large enough to _visualize_ avogadros of quarks and recognize a hand-pattern in them.\n\nBut we at least understand what hands _do._  Hands push on things, exert forces on them.  When we're told about atoms, we visualize little billiard balls bumping into each other.  This makes it seem obvious that \"atoms\" can push on things too, by bumping into them.\n\nNow this notion of atoms is not quite correct.  But so far as _human imagination_ goes, it's relatively easy to imagine our hand being made up of a little galaxy of swirling billiard balls, pushing on things when our \"fingers\" touch them.  Democritus imagined this 2400 years ago, and there was a time, roughly [1803-1922](http://en.wikipedia.org/wiki/Atom), when Science thought he was right.\n\nBut what about, say, anger?\n\nHow could little billiard balls be angry?  Tiny frowny faces on the billiard balls?\n\nPut yourself in the shoes of, say, a hunter-gatherer—someone who may not even have a notion of writing, let alone the notion of using base matter to perform computations—someone who has no idea that such a thing as neurons exist.  Then you can imagine the _functional_ gap that your ancestors might have perceived between billiard balls and \"Grrr!  Aaarg!\"\n\nForget about subjective experience for the moment, and consider the sheer _behavioral_ gap between anger and billiard balls.  The difference between what little billiard balls _do,_ and what anger makes people _do._ Anger can make people raise their fists and hit someone—or say snide things behind their backs—or plant scorpions in their tents at night.  Billiard balls just push on things.\n\nTry to put yourself in the shoes of the hunter-gatherer who's never had the \"Aha!\" of information-processing.  Try to avoid [hindsight bias](/lw/iz/failing_to_learn_from_history/) about things like neurons and computers.  Only then will you be able to see the uncrossable explanatory gap:\n\nHow can you explain angry behavior in terms of billiard balls?\n\nWell, the _obvious_ materialist conjecture is that the little billiard balls push on your arm and make you hit someone, or push on your tongue so that insults come out.\n\nBut how do the little billiard balls know how to do this—or how to guide your tongue and fingers through long-term plots—if they aren't angry themselves?\n\nAnd besides, if you're not seduced by—gasp!—scientism, you can see from a first-person perspective that this explanation is obviously false.  Atoms can push on your arm, but they can't make you _want_ anything.\n\nSomeone may point out that drinking wine can make you angry.  But who says that wine is made exclusively of little billiard balls?  Maybe wine just contains a potency of angerness.\n\nClearly, reductionism is just a flawed notion.\n\n(The novice goes astray and says \"The art failed me\"; the master goes astray and says \"I failed my art.\")\n\nWhat does it take to cross this gap?  It's not just the idea of \"neurons\" that \"process information\"—if you say only this and nothing more, it just inserts a magical, unexplained level-crossing rule into your model, where you go from billiards to thoughts.\n\nBut an Artificial Intelligence programmer who knows how to create a chess-playing program out of base matter, has taken a _genuine_ step toward crossing the gap.  If you understand concepts like [consequentialism](/lw/l4/terminal_values_and_instrumental_values/), backward chaining, utility functions, and [search trees](http://en.wikipedia.org/wiki/Minimax), you can make merely causal/mechanical systems compute plans.\n\nThe trick goes something like this:  For each possible chess move, compute the moves your opponent could make, then your responses to those moves, and so on; evaluate the furthest position you can see using some local algorithm (you might simply count up the material); then trace back using [minimax](http://en.wikipedia.org/wiki/Minimax) to find the best move on the current board; then make that move.\n\nMore generally:  If you have chains of causality inside the mind that have a kind of mapping—a mirror, an echo—to what goes on in the environment, then you can run a utility function over the end products of imagination, and find an action that achieves something which the utility function rates highly, and output that action.  It is not necessary for the chains of causality inside the mind, that are similar to the environment, to be made out of billiard balls that have little auras of intentionality.  Deep Blue's transistors do not need little chess pieces carved on them, in order to work.  See also [The Simple Truth](http://yudkowsky.net/bayes/truth.html).\n\nAll this is still tremendously oversimplified, but it should, at least, reduce the apparent length of the gap.  If you can understand all that, you can see how a planner built out of base matter can be influenced by alcohol to output more angry behaviors.  The billiard balls in the alcohol push on the billiard balls making up the utility function.\n\nBut even if you know how to write small AIs, you can't _visualize_ the level-crossing between transistors and chess.  There are too many transistors, and too many moves to check.\n\nLikewise, even if you knew all the facts of neurology, you would not be able to _visualize_ the level-crossing between neurons and anger—let alone the level-crossing between atoms and anger.  Not the way you can visualize a hand consisting of fingers, thumb, and palm.\n\nAnd suppose a cognitive scientist just [flatly tells](/lw/op/fake_reductionism/) you \"Anger is hormones\"?  Even if you repeat back the words, it doesn't mean you've crossed the gap.  You may [believe you believe it](/lw/i4/belief_in_belief/), but that's not the same as understanding what little billiard balls have to do with wanting to hit someone.\n\nSo you come up with interpretations like, \"Anger is _mere_ hormones, it's caused by little molecules, so it must not be justified in any moral sense—_that's_ why you should learn to control your anger.\"\n\nOr, \"There isn't really any such thing as anger—it's an illusion, a quotation with no referent, like a mirage of water in the desert, or looking in the garage for a dragon and not finding one.\"\n\nThese are both tough pills to swallow (not that you _should_ swallow them) and so it is a good easier to [profess](/lw/ir/science_as_attire/) them than to believe them.\n\nI think this is what non-reductionists/non-materialists think they are criticizing when they criticize reductive materialism.\n\nBut materialism isn't that easy.  It's not as cheap as saying, \"Anger is made out of atoms—there, now I'm done.\"  That wouldn't explain how to get from billiard balls to hitting.  You need the specific insights of computation, consequentialism, and search trees before you can start to close the explanatory gap.\n\nAll this was a relatively easy example _by modern standards,_ because I restricted myself to talking about angry _behaviors._  Talking about outputs doesn't require you to appreciate [how an algorithm feels from inside](/lw/no/how_an_algorithm_feels_from_inside/) (cross a first-person/third-person gap) or [dissolve a wrong question](/lw/og/wrong_questions/) (untangle places where the interior of your own mind runs skew to reality).\n\nGoing from material substances that bend and break, burn and fall, push and shove, to angry _behavior,_ is just a practice problem by the standards of modern philosophy.  But it is an _important_ practice problem.  It can only be fully appreciated, if you realize how _hard_ it would have been to solve before writing was invented.  There was once an explanatory gap here—though it may not seem that way in [hindsight](/lw/iz/failing_to_learn_from_history/), now that it's been bridged for generations.\n\nExplanatory gaps can be crossed, if you accept help from science, and don't trust the view from the interior of your own mind."
          },
          "voteCount": 46
        },
        {
          "name": "Heat vs. Motion",
          "type": "post",
          "slug": "heat-vs-motion",
          "_id": "ne6Ra62FB9ACHGSuh",
          "url": null,
          "title": "Heat vs. Motion",
          "author": "Eliezer_Yudkowsky",
          "question": false,
          "tags": [
            {
              "name": "Physics"
            },
            {
              "name": "Map and Territory"
            },
            {
              "name": "Reductionism"
            }
          ],
          "tableOfContents": null,
          "contents": {
            "markdown": "After yesterday's post, it occurred to me that there's a much simpler example of reductionism jumping a gap of apparent-difference-in-kind: the reduction of heat to motion.\n\nToday, the equivalence of heat and motion may seem [too obvious](http://www.overcomingbias.com/2007/08/failing-to-lear.html) in [hindsight](http://www.overcomingbias.com/2007/08/hindsight-deval.html)—[everyone says](http://www.overcomingbias.com/2007/10/cached-thoughts.html) that \"heat is motion\", therefore, it can't be a \"[weird](http://www.overcomingbias.com/2007/12/cultish-counter.html)\" belief.\n\nBut there was a time when the [kinetic theory of heat](http://en.wikipedia.org/wiki/Theory_of_heat) was a highly controversial scientific hypothesis, contrasting to belief in a [caloric fluid](http://en.wikipedia.org/wiki/Caloric) that flowed from hot objects to cold objects.  Still earlier, the main theory of heat was \"[Phlogiston!](http://www.overcomingbias.com/2007/08/fake-causality.html)\"\n\nSuppose you'd _separately_ studied kinetic theory and caloric theory.  You now know something about kinetics: collisions, elastic rebounds, momentum, kinetic energy, gravity, inertia, free trajectories.  Separately, you know something about heat:  Temperatures, pressures, combustion, heat flows, engines, melting, vaporization.\n\nNot only is this state of knowledge a plausible one, it is the state of knowledge possessed by e.g. Sadi Carnot, who, working strictly from within the caloric theory of heat, developed the principle of the Carnot cycle—a heat engine of maximum efficiency, whose existence implies the [second law of thermodynamics](http://www.overcomingbias.com/2008/02/second-law.html).  This in 1824, when kinetics was a highly developed science.\n\nSuppose, like Carnot, you know a great deal about kinetics, and a great deal about heat, as _separate_ entities.  Separate entities _of knowledge_, that is: your brain has separate filing baskets for beliefs about kinetics and beliefs about heat.  But [from the inside](http://www.overcomingbias.com/2008/02/algorithm-feels.html), this state of knowledge _feels_ like living in a world of moving things and hot things, a world where motion and heat are independent properties of matter.\n\nNow a Physicist From The Future comes along and tells you:  \"Where there is heat, there is motion, and vice versa.  That's why, for example, rubbing things together makes them hotter.\"\n\nThere are (at least) two possible interpretations you could attach to this statement, \"Where there is heat, there is motion, and vice versa.\"\n\nFirst, you could suppose that heat and motion exist separately—that the caloric theory is correct—but that among our universe's physical laws is a \"bridging law\" which states that, where objects are moving quickly, caloric will come into existence.  And conversely, another bridging law says that caloric can exert pressure on things and make them move, which is why a hotter gas exerts more pressure on its enclosure (thus a steam engine can use steam to drive a piston).\n\nSecond, you could suppose that heat and motion are, in some as-yet-mysterious sense, _the same thing_.\n\n\"Nonsense,\" says Thinker 1, \"the words 'heat' and 'motion' have two different meanings; that is why we have two different words.  We know how to determine when we will call an observed phenomenon 'heat'—heat can melt things, or make them burst into flame.  We know how to determine when we will say that an object is 'moving quickly'—it changes position; and when it crashes, it may deform, or shatter.  Heat is concerned with change of substance; motion, with change of position and shape.  To say that these two words have the same meaning is simply to confuse yourself.\"\n\n\"Impossible,\" says Thinker 2.  \"It may be that, in our world, heat and motion are associated by bridging laws, so that it is a law of physics that motion creates caloric, and vice versa.  But I can easily imagine a world where rubbing things together does _not_ make them hotter, and gases _don't_ exert more pressure at higher temperatures.  Since there are possible worlds where heat and motion are not associated, they must be different properties—this is true a priori.\"\n\nThinker 1 is [confusing the quotation and the referent](http://www.overcomingbias.com/2008/03/quote-not-refer.html).  2 + 2 = 4, but \"2 + 2\" ≠ \"4\".  The string \"2 + 2\" contains 5 characters (including whitespace) and the string \"4\" contains only 1 character.  If you type the two strings into a Python interpreter, they yield the same output,—> 4.  So you can't conclude, from looking at the strings \"2 + 2\" and \"4\", that just because the strings are different, they must have different \"meanings\" relative to the Python Interpreter.\n\nThe words \"heat\" and \"kinetic energy\" can be said to \"refer to\" the same thing, even before we _know_ how heat reduces to motion, in the sense that we don't know yet what the reference is, but the references are in fact the same.  You might imagine an Idealized Omniscient Science Interpreter that would give the same output when we typed in \"heat\" and \"kinetic energy\" on the command line.\n\nI talk about the Science Interpreter to emphasize that, to dereference the pointer, you've got to step outside cognition.  The end result of the dereference is something out there in reality, not in anyone's mind.  So you can _say_ \"real referent\" or \"actual referent\", but you can't _evaluate_ the words locally, from the inside of your own head.  You can't reason using the actual heat-referent—if you thought using _real heat_, thinking \"1 million Kelvin\" would vaporize your brain.  But, by forming a belief about your belief about heat, you can talk _about_ your belief about heat, and say things like \"It's possible that my belief about heat doesn't much resemble _real_ heat.\"  You can't actually perform that comparison right there in your own mind, but you can talk _about_ it.\n\nHence you can say, \"My beliefs about heat and motion are not the same beliefs, but it's possible that actual heat and actual motion are the same thing.\"  It's just like being able to acknowledge that \"the morning star\" and \"the evening star\" might be the same planet, while also understanding that you can't determine this just by examining your beliefs—you've got to haul out the telescope.\n\nThinker 2's mistake follows similarly.  A physicist told him, \"Where there is heat, there is motion\" and P2 mistook this for a statement of _physical law_:  The presence of caloric _causes_ the existence of motion.  What the physicist really means is more akin to an _inferential rule:_  Where you are told there is \"heat\", deduce the presence of \"motion\".\n\nFrom this basic projection of a multilevel model into a multilevel reality follows another, distinct error: the conflation of conceptual possibility with logical possibility.  To Sadi Carnot, it is _conceivable_ that there could be another world where heat and motion are not associated.  To Richard Feynman, armed with specific knowledge of how to derive equations about heat from equations about motion, this idea is not only inconceivable, but so wildly inconsistent as to make one's head explode. \n\nI should note, in fairness to philosophers, that there are philosophers who have said these things.  For example, Hilary Putnam, [writing](http://books.google.com/books?id=cEPNtxAoYPAC&pg=PA18&lpg=PA18&dq=h2o+logically+possible&source=web&ots=R9zyWvo3Uz&sig=FjyegdF0D-2I77l7dHQ-RRkSR88&hl=en) on the \"Twin Earth\" thought experiment:\n\n> Once we have discovered that water (in the actual world) is H20, _nothing counts as a possible world in which water isn't H20._  In particular, if a \"logically possible\" statement is one that holds in some \"logically possible world\", _it isn't logically possible that water isn't H20._\n> \n> On the other hand, we can perfectly well imagine having experiences that would convince us (and that would make it rational to believe that) water _isn't_ H20.  In that sense, it is conceivable that water isn't H20.  It is conceivable but it isn't logically possible!  Conceivability is no proof of logical possibility.\n\nIt appears to me that \"water\" is being used in two different senses in these two paragraphs—one in which the word \"water\" _refers_ to what we type into the Science Interpreter, and one in which \"water\" _refers_ to what we get out of the Science Interpreter when we type \"water\" into it.  In the first paragraph, Hilary seems to be saying that after we do some experiments and find out that water is H20, water becomes automatically redefined to _mean_ H20.  But you could coherently hold a different position about whether the word \"water\" now _means_ \"H20\" or \"whatever is _really_ in that bottle next to me\", so long as you use your terms consistently.\n\nI believe the above has already been said as well?  Anyway...\n\nIt is quite possible for there to be only _one_ thing out-there-in-the-world, but for it to take on sufficiently different forms, and for you yourself to be sufficiently ignorant of the reduction, that it feels like living in a world containing two entirely different things.  Knowledge concerning these two different phenomena may taught in two different classes, and studied by two different academic fields, located in two different buildings of your university.\n\nYou've got to put yourself quite a ways back, into a historically realistic frame of mind, to remember how _different_ heat and motion once seemed.  Though, depending on how much you know today, it may not be as hard as all that, if you can look past the pressure of conventionality (that is, \"heat is motion\" is an un-weird belief, \"heat is not motion\" is a weird belief).  I mean, suppose that tomorrow the physicists stepped forward and said, \"Our popularizations of science have always [contained one lie](http://www.overcomingbias.com/2008/02/my-favorite-lia.html).  Actually, heat has nothing to do with motion.\"  Could you _prove_ they were wrong?\n\nSaying \"Maybe heat and motion are the same thing!\" is easy.  The difficult part is explaining _how_.  It takes a great deal of detailed knowledge to get yourself to the point where you can no longer _conceive_ of a world in which the two phenomena go separate ways.  Reduction isn't cheap, and that's why it buys so much.\n\nOr maybe you could say:  \"Reductionism is easy, reduction is hard.\"  But it does kinda help to be a reductionist, I think, when it comes time to go looking for a reduction."
          },
          "voteCount": 32
        },
        {
          "name": "Brain Breakthrough! It's Made of Neurons!",
          "type": "post",
          "slug": "brain-breakthrough-it-s-made-of-neurons",
          "_id": "nzzNFcrSk7akQ9bwD",
          "url": null,
          "title": "Brain Breakthrough! It's Made of Neurons!",
          "author": "Eliezer_Yudkowsky",
          "question": false,
          "tags": [
            {
              "name": "Humor"
            },
            {
              "name": "April Fool's"
            }
          ],
          "tableOfContents": null,
          "contents": {
            "markdown": "In an [amazing breakthrough](http://www.overcomingbias.com/2008/03/amazing-breakth.html), a multinational team of scientists led by Nobel laureate Santiago Ramón y Cajal announced that the brain is composed of a _ridiculously_ complicated network of tiny cells connected to each other by infinitesimal threads and branches.\n\nThe multinational team—which also includes the famous technician Antonie van Leeuwenhoek, and possibly Imhotep, promoted to the Egyptian god of medicine—issued this statement:\n\n\"The present discovery culminates years of research indicating that the convoluted squishy thing inside our skulls is even more complicated than it looks.  Thanks to Cajal's application of a new staining technique invented by Camillo Golgi, we have learned that this structure is not a continuous network like the blood vessels of the body, but is actually composed of many tiny cells, or \"neurons\", connected to one another by even more tiny filaments.\n\n\"Other extensive evidence, beginning from Greek medical researcher Alcmaeon and continuing through Paul Broca's research on speech deficits, indicates that the brain is the seat of reason.\n\n\"Nemesius, the Bishop of Emesia, has previously argued that brain tissue is too earthy to act as an intermediary between the body and soul, and so the mental faculties are located in the ventricles of the brain.  However, if this is correct, there is no reason why this organ should turn out to have an immensely complicated internal composition.\n\n\"Charles Babbage has independently suggested that many small mechanical devices could be collected into an 'Analytical Engine', capable of performing activities, such as arithmetic, which are widely believed to require thought.  The work of Luigi Galvani and Hermann von Helmholtz suggests that the activities of neurons are electrochemical in nature, rather than mechanical pressures as previously believed.  Nonetheless, we think an analogy with Babbage's 'Analytical Engine' suggests that a vastly complicated network of neurons could similarly exhibit thoughtful properties.\n\n\"We have found an enormously complicated material system located where the mind should be.  The implications are shocking, and must be squarely faced.  We believe that the present research offers strong experimental evidence that Benedictus Spinoza was correct, and René Descartes wrong:  Mind and body are of one substance.\n\n\"In combination with the work of Charles Darwin showing how such a complicated organ could, in principle, have arisen as the result of processes not themselves intelligent, the bulk of scientific evidence now seems to indicate that intelligence is ontologically non-fundamental and has an extended origin in time.  This strongly weighs against theories which assign mental entities an ontologically fundamental or causally primal status, including all religions ever invented.\n\n\"Much work remains to be done on discovering the specific identities between electrochemical interactions between neurons, and thoughts.  Nonetheless, we believe our discovery offers the promise, though not yet the realization, of a full scientific account of thought.  The problem may now be declared, if not solved, then solvable.\"\n\nWe regret that Cajal and most of the other researchers involved on the Project are no longer available for comment."
          },
          "voteCount": 60
        },
        {
          "name": "Reductive Reference",
          "type": "post",
          "slug": "reductive-reference",
          "_id": "gRa5cWWBsZqdFvmqu",
          "url": null,
          "title": "Reductive Reference",
          "author": "Eliezer_Yudkowsky",
          "question": false,
          "tags": [
            {
              "name": "World Modeling"
            },
            {
              "name": "References (Language)"
            }
          ],
          "tableOfContents": null,
          "contents": {
            "markdown": "The reductionist thesis (as I formulate it) is that human minds, for reasons of efficiency, use a multi-level map in which we separately _think_ about things like \"atoms\" and \"quarks\", \"hands\" and \"fingers\", or \"heat\" and \"kinetic energy\".  Reality itself, on the other hand, is single-level in the sense that it does not seem to contain atoms as _separate, additional, causally efficacious_ entities _over and above_ quarks.\n\nSadi Carnot formulated the (precursor to) the second law of thermodynamics using the caloric theory of heat, in which heat was just a fluid that flowed from hot things to cold things, produced by fire, making gases expand—the effects of heat were studied separately from the science of kinetics, considerably before the reduction took place.  If you're trying to design a steam engine, the effects of all those tiny vibrations and collisions which we name \"heat\" can be summarized into a much simpler description than the full quantum mechanics of the quarks.  Humans compute efficiently, thinking of only significant effects on goal-relevant quantities.\n\nBut reality itself does seem to use the full quantum mechanics of the quarks.  I [once met a fellow](/lw/on/reductionism/) who thought that if you used General Relativity to compute a low-velocity problem, like an artillery shell, GR would give you the _wrong answer_—not just a slow answer, but an _experimentally wrong_ answer—because at low velocities, artillery shells are governed by Newtonian mechanics, not GR.  This is exactly how physics does _not_ work.  Reality just seems to go on crunching through General Relativity, even when it only makes a difference at the fourteenth decimal place, which a human would regard as a huge waste of computing power.  Physics does it with brute force.  No one has _ever_ caught physics simplifying its calculations—or if someone did catch it, the Matrix Lords erased the memory afterward.\n\nOur map, then, is very much unlike the territory; our maps are multi-level, the territory is single-level.  Since the representation is so incredibly unlike the referent, in what sense can a belief like \"I am wearing socks\" be called _true,_ when in reality itself, there are only quarks?\n\nIn case you've forgotten what the word \"true\" means, the classic definition was given by Alfred Tarski:\n\n> The statement \"snow is white\" is _true_ if and only if snow is white.\n\nIn case you've forgotten what the difference is between the statement \"I believe 'snow is white'\" and \"'Snow is white' is true\", see [here](/lw/om/qualitatively_confused/).  Truth can't be evaluated _just_ by looking inside your own head—if you want to know, for example, whether \"the morning star = the evening star\", you need a telescope; it's not enough just to look at the beliefs themselves.\n\nThis is the point missed by the postmodernist folks screaming, \"But how do you _know_ your beliefs are true?\"  When you do an experiment, you actually _are_ going outside your own head.  You're engaging in a complex interaction whose outcome is causally determined by the thing you're reasoning about, not just your beliefs about it.  I once [defined \"reality\" as follows](http://yudkowsky.net/bayes/truth.html):\n\n> Even when I have a simple hypothesis, strongly supported by all the evidence I know, sometimes I'm still surprised. So I need different names for the thingies that determine my predictions and the thingy that determines my experimental results. I call the former thingies 'belief', and the latter thingy 'reality'.\"\n\nThe interpretation of your experiment still depends on your prior beliefs.  I'm not going to talk, for the moment, about Where Priors Come From, because that is not the subject of this blog post.  My point is that truth refers to an _ideal_ comparison between a belief and reality.  Because we understand that planets are distinct from beliefs about planets, we can design an experiment to test whether the belief \"the morning star and the evening star are the same planet\" is _true._  This experiment will involve telescopes, not just introspection, because we understand that \"truth\" involves comparing an internal belief to an external fact; so we use an instrument, the telescope, whose perceived behavior we believe to depend on the external fact of the planet.\n\nBelieving that the telescope helps us evaluate the \"truth\" of \"morning star = evening star\", relies on our prior beliefs about the telescope interacting with the planet.  Again, I'm not going to address that in this particular blog post, except to quote one of my favorite Raymond Smullyan lines:  \"If the more sophisticated reader objects to this statement on the grounds of its being a mere tautology, then please at least give the statement credit for not being inconsistent.\"  Similarly, I don't see the use of a telescope as circular logic, but as reflective coherence; for every systematic way of arriving at truth, there ought to be a rational explanation for how it works.\n\nThe question on the table is what it _means_ for \"snow is white\" to be _true,_ when, in reality, there are just quarks.\n\nThere's a certain pattern of neural connections making up your beliefs about \"snow\" and \"whiteness\"—we believe this, but we do not know, and cannot concretely visualize, the actual neural connections.  Which are, themselves, embodied in a pattern of quarks even less known.  Out there in the world, there are water molecules whose temperature is low enough that they have arranged themselves in tiled repeating patterns; they look nothing like the tangles of neurons.  In what sense, comparing one (ever-fluctuating) pattern of quarks to the other, is the belief \"snow is white\" _true?_\n\nObviously, neither I nor anyone else can offer an Ideal Quark Comparer Function that accepts a quark-level description of a neurally embodied belief (including the surrounding brain) and a quark-level description of a snowflake (and the surrounding laws of optics), and outputs \"true\" or \"false\" over \"snow is white\".  And who says the fundamental level is _really_ about particle fields?\n\nOn the other hand, throwing out all beliefs because they aren't written as gigantic unmanageable specifications about quarks we can't even see... doesn't seem like a very prudent idea.  [Not the best way to optimize our goals.](/lw/nb/something_to_protect/) \n\nIt seems to me that a word like \"snow\" or \"white\" can be taken as a kind of promissory note—not a _known_ specification of exactly which physical quark configurations count as \"snow\", but, nonetheless, there are [things you call snow and things you don't call snow](/lw/nh/extensions_and_intensions/), and even if you got a few items wrong (like plastic snow), an Ideal Omniscient Science Interpreter would see [a tight cluster in the center](/lw/nl/the_cluster_structure_of_thingspace/) and [redraw the boundary](/lw/o0/where_to_draw_the_boundary/) to have a [simpler definition](/lw/o3/superexponential_conceptspace_and_simple_words/).\n\nIn a single-layer universe whose bottom layer is unknown, or uncertain, or just too large to talk about, the concepts in a multi-layer mind can be said to represent a kind of promissory note—we don't know _what_ they correspond to, out there.  But it seems to us that we can distinguish positive from negative cases, in a [predictively productive way](/lw/o2/mutual_information_and_density_in_thingspace/), so we think—perhaps in a fully general sense—that there is _some_ difference of quarks, _some_ difference of configurations at the fundamental level, which explains the differences that feed into our senses, and ultimately result in our saying \"snow\" or \"not snow\".\n\nI see this white stuff, and it is the same on several occasions, so I hypothesize a stable latent cause in the environment—I give it the name \"snow\"; \"snow\" is then a promissory note referring to a believed-in simple boundary that could be drawn around the unseen causes of my experience.\n\nHilary Putnam's \"Twin Earth\" thought experiment, where water is not H20 but some strange other substance denoted XYZ, otherwise behaving much like water, and the subsequent philosophical debate, helps to highlight this issue.  \"Snow\" doesn't have a logical definition known to us—it's more like an empirically determined pointer to a logical definition.  This is true even if you believe that snow is ice crystals is low-temperature tiled water molecules.  The water molecules are made of quarks.  What if quarks turn out to be made of something else?  What _is_ a snowflake, then?  You don't know—but it's still a snowflake, not a fire hydrant.\n\nAnd of course, these very paragraphs I have just written, are likewise far above the level of quarks.  \"Sensing white stuff, visually categorizing it, and thinking 'snow' or 'not snow'\"—this is also talking very far above the quarks.  So my meta-beliefs are also promissory notes, for things that an Ideal Omniscient Science Interpreter might know about which configurations of the quarks (or whatever) making up my brain, correspond to \"believing 'snow is white'\".\n\nBut then, the entire grasp that we have upon reality, is made up of promissory notes of this kind.  So, rather than calling it circular, I prefer to call it self-consistent.\n\nThis can be a bit unnerving—maintaining a precarious epistemic perch, in both object-level beliefs and reflection, far above a huge unknown underlying fundamental reality, and hoping one doesn't fall off.\n\nOn reflection, though, it's hard to see how things could be any other way.\n\nSo at the end of the day, the statement \"reality does not contain hands as fundamental, additional, separate causal entities, over and above quarks\" is not the same statement as \"hands do not exist\" or \"I don't have any hands\".  There are no _fundamental_ hands; hands are made of fingers, palm, and thumb, which in turn are made of muscle and bone, all the way down to elementary particle fields, which are the fundamental causal entities, so far as we currently know.\n\nThis is not the same as saying, \"there are no 'hands'.\"  It is not the same as saying, \"the word 'hands' is a promissory note that will never be paid, because there is no empirical cluster that corresponds to it\"; or \"the 'hands' note will never be paid, because it is logically impossible to reconcile its supposed characteristics\"; or \"the statement 'humans have hands' refers to a sensible state of affairs, but reality is not in that state\".\n\nJust:  There are patterns that exist _in_ reality where we see \"hands\", and these patterns have something in common, but they are not fundamental.\n\nIf I _really_ had no hands—if reality suddenly transitioned to be in a state that we would describe as \"Eliezer has no hands\"—reality would shortly thereafter correspond to a state we would describe as \"Eliezer screams as blood jets out of his wrist stumps\".\n\nAnd this is _true,_ even though the above paragraph hasn't specified any quark positions.\n\nThe previous sentence is likewise meta-true.\n\nThe map is multilevel, the territory is single-level.  This doesn't mean that the higher levels \"don't exist\", like looking in your garage for a dragon and finding nothing there, or like seeing a mirage in the desert and forming an expectation of drinkable water when there is nothing to drink.  The higher levels of your map are not _false,_ without referent_;_ they have referents _in_ the single level of physics.  It's not that the wings of an airplane unexist—then the airplane would drop out of the sky.  The \"wings of an airplane\" exist _explicitly_ in an engineer's multilevel model of an airplane, and the wings of an airplane exist _implicitly_ in the quantum physics of the real airplane.  Implicit existence is not the same as nonexistence.  The exact description of this implicitness is not known to us—is not explicitly represented in our map.  But this does not prevent our map from working, or even prevent it from being _true._\n\nThough it is a bit unnerving to contemplate that every single concept and belief in your brain, including these meta-concepts about how your brain works and why you can form accurate beliefs, are perched orders and orders of magnitude above reality..."
          },
          "voteCount": 40
        }
      ]
    },
    {
      "title": "Selected posts from [[Zombies]] & [[Supernaturality]]",
      "description": "",
      "children": [
        {
          "name": "Zombies! Zombies?",
          "type": "post",
          "slug": "zombies-zombies",
          "_id": "fdEWWr8St59bXLbQr",
          "url": null,
          "title": "Zombies! Zombies?",
          "author": "Eliezer Yudkowsky",
          "question": false,
          "tags": [
            {
              "name": "Consciousness"
            },
            {
              "name": "Zombies"
            }
          ],
          "tableOfContents": null,
          "contents": {
            "markdown": "[![Doviende38008649](/static/imported/2008/04/02/doviende38008649.jpg \"Doviende38008649\")](http://flickr.com/photos/doviende/38008649/)Your \"zombie\", in the philosophical usage of the term, is putatively a being that is exactly like you in _every_ respect—identical behavior, identical speech, identical brain; every atom and quark in _exactly_ the same position, moving according to the same causal laws of motion—_except_ that your zombie is not conscious.\n\nIt is furthermore claimed that if zombies are \"possible\" (a term over which battles are still being fought), then, purely from our knowledge of this \"possibility\", we can deduce a priori that consciousness is extra-physical, in a sense to be described below; the standard term for this position is \"epiphenomenalism\".\n\n(For those unfamiliar with zombies, I emphasize that _this is not a strawman._  See, for example, [the SEP entry on Zombies](http://plato.stanford.edu/entries/zombies/).  The \"possibility\" of zombies is accepted by a substantial fraction, possibly a majority, of academic philosophers of consciousness.)\n\nI once read somewhere, \"You are not the one who speaks your thoughts—you are the one who _hears_ your thoughts\".  In Hebrew, the word for the highest soul, that which God breathed into Adam, is N'Shama—\"the hearer\".\n\nIf you conceive of \"consciousness\" as a purely passive listening, then the notion of a zombie initially seems easy to imagine.  It's someone who lacks the N'Shama, the hearer.\n\n(Warning:  Long post ahead.  _Very_ long 6,600-word post involving David Chalmers ahead.  This may be taken as my demonstrative counterexample to Richard Chappell's [Arguing with Eliezer Part II](http://www.philosophyetc.net/2008/03/arguing-with-eliezer-part-ii.html), in which Richard accuses me of not engaging with the complex arguments of real philosophers. Edit December 2019: There now exists a shorter edited version of this post [here](https://www.lesswrong.com/posts/7DmA3yWwa6AT5jFXt/zombies-redacted))\n\nWhen you open a refrigerator and find that the orange juice is gone, you think \"Darn, I'm out of orange juice.\"  The sound of these words is probably represented in your auditory cortex, as though you'd heard someone else say it.  (Why do I think this?  Because native Chinese speakers can remember longer digit sequences than English-speakers.  Chinese digits are all single syllables, and so Chinese speakers can remember around ten digits, versus the famous \"seven plus or minus two\" for English speakers.  There appears to be a loop of repeating sounds back to yourself, a size limit on working memory in the auditory cortex, which is genuinely phoneme-based.)\n\nLet's suppose the above is correct; as a postulate, it should certainly present no problem for advocates of zombies.  Even if humans are not like this, it seems easy enough to imagine an AI constructed this way (and imaginability is what the zombie argument is all about).  It's not only conceivable in principle, but quite possible in the next couple of decades, that surgeons will lay a network of neural taps over someone's auditory cortex and read out their internal narrative.  (Researchers have already tapped the lateral geniculate nucleus of a cat and reconstructed recognizable visual inputs.)\n\nSo your zombie, being physically identical to you down to the last atom, will open the refrigerator and form auditory cortical patterns for the phonemes \"Darn, I'm out of orange juice\".  On this point, epiphenomalists would willingly agree.\n\nBut, says the epiphenomenalist, in the zombie there is no one inside to _hear;_ the inner listener is missing.  The internal narrative is spoken, but unheard.  You are not the one who speaks your thoughts, you are the one who hears them.\n\nIt seems a lot more straightforward (they would say) to make an AI that prints out some kind of internal narrative, than to show that an inner listener hears it.\n\nThe Zombie Argument is that if the Zombie World is _possible_—not necessarily physically possible in our universe, just \"possible in theory\", or \"imaginable\", or something along those lines—then consciousness must be extra-physical, something over and above mere atoms.  Why?  Because even if you somehow knew the positions of all the atoms in the universe, you would still have be told, as a separate and additional fact, that people were conscious—that they had inner listeners—that we were not in the Zombie World, as seems _possible._\n\nZombie-ism is not the same as dualism.  Descartes thought there was a body-substance and a wholly different kind of mind-substance, but Descartes also thought that the mind-substance was a _causally active_ principle, interacting with the body-substance, controlling our speech and behavior.  Subtracting out the mind-substance from the human would leave a _traditional_ zombie, of the lurching and groaning sort.\n\nAnd though the Hebrew word for the innermost soul is N'Shama, that-which-hears, I can't recall hearing a rabbi arguing for the possibility of zombies.  Most rabbis would probably be aghast at the idea that the divine part which God breathed into Adam _doesn't actually do anything._\n\nThe technical term for the belief that consciousness is there, but has no effect on the physical world, is _epiphenomenalism._\n\nThough there are other elements to the zombie argument (I'll deal with them below), I think that the intuition of the passive listener is what first seduces people to zombie-ism.  In particular, it's what seduces a lay audience to zombie-ism.  The core notion is simple and easy to access:  The lights are on but no one's home.\n\nPhilosophers are appealing to the intuition of the passive listener when they say \"Of course the zombie world is imaginable; you know exactly what it would be like.\"\n\nOne of the great battles in the Zombie Wars is over what, exactly, is meant by saying that zombies are \"possible\".  Early zombie-ist philosophers (the 1970s) just thought it was obvious that zombies were \"possible\", and didn't bother to define what sort of possibility was meant.\n\nBecause of my reading in mathematical logic, what instantly comes into my mind is logical possibility.  If you have a collection of statements like (A->B),(B->C),(C->~A) then the compound belief is _logically possible_ if it has a _model_—which, in the simple case above, reduces to finding a value assignment to A, B, C that makes all of the statements (A->B),(B->C), and (C->~A) true.  In this case, A=B=C=0 works, as does A=0, B=C=1 or A=B=0, C=1.\n\nSomething will _seem_ possible—will seem \"conceptually possible\" or \"imaginable\"—if you can consider the collection of statements without _seeing_ a contradiction.  But it is, in general, a very hard problem to see contradictions _or_ to find a full specific model!  If you limit yourself to simple Boolean propositions of the form ((A or B or C) and (B or ~C or D) and (D or ~A or ~C) ...), conjunctions of disjunctions of three variables, then this is a very famous problem called 3-SAT, which is one of the first problems ever to be proven NP-complete.\n\nSo just because you don't see a contradiction in the Zombie World at first glance, it doesn't mean that no contradiction is there.  It's like not seeing a contradiction in the Riemann Hypothesis at first glance.  From conceptual possibility (\"I don't see a problem\") to _logical possibility_ in the full technical sense, is a very great leap.  It's easy to make it an NP-complete leap, and with first-order theories you can make it arbitrarily hard to compute even for finite questions.  And it's _logical_ possibility of the Zombie World, not conceptual possibility, that is needed to suppose that a logically omniscient mind could know the positions of all the atoms in the universe, and yet need to be told as an _additional_ non-entailed fact that we have inner listeners.\n\nJust because you don't see a contradiction _yet,_ is no guarantee that you won't see a contradiction in another 30 seconds.  \"All odd numbers are prime.  Proof:  3 is prime, 5 is prime, 7 is prime...\"\n\nSo let us ponder the Zombie Argument _a little longer_:  Can we think of a counterexample to the assertion \"Consciousness has no third-party-detectable causal impact on the world\"?\n\nIf you close your eyes and concentrate on your inward awareness, you will begin to form thoughts, in your internal narrative, that go along the lines of \"I am aware\" and \"My awareness is separate from my thoughts\" and \"I am not the one who speaks my thoughts, but the one who hears them\" and \"My stream of consciousness is not my consciousness\" and \"It seems like there is a part of me which I can imagine being eliminated without changing my outward behavior.\"\n\nYou can even say these sentences out loud, as you meditate.  In principle, someone with a super-fMRI could probably read the phonemes out of your auditory cortex; but saying it out loud removes all doubt about whether you have entered the realms of testability and physical consequences.\n\nThis certainly seems like the inner listener is being _caught in the act of listening_ by whatever part of you writes the internal narrative and flaps your tongue.\n\nImagine that a mysterious race of aliens visit you, and leave you a mysterious black box as a gift.  You try poking and prodding the black box, but (as far as you can tell) you never succeed in eliciting a reaction.  You can't make the black box produce gold coins or answer questions.  So you conclude that the black box is causally inactive:  \"For all X, the black box doesn't do X.\"  The black box is an effect, but not a cause; epiphenomenal; without causal potency.  In your mind, you test this general hypothesis to see if it is true in some trial cases, and it seems to be true—\"Does the black box turn lead to gold?  No.  Does the black box boil water?  No.\"\n\nBut you can _see_ the black box; it absorbs light, and weighs heavy in your hand.  This, too, is part of the dance of causality.  If the black box were _wholly_ outside the causal universe, you couldn't see it; you would have no way to know it existed; you could not say, \"Thanks for the black box.\"  You didn't _think_ of this counterexample, when you formulated the general rule:  \"All X: Black box doesn't do X\".  But it was there all along.\n\n(Actually, the aliens left you _another_ black box, this one _purely_ epiphenomenal, and you haven't the slightest clue that it's there in your living room.  That was their joke.)\n\nIf you can close your eyes, and sense yourself sensing—if you can be aware of yourself being aware, and think \"I am aware that I am aware\"—and say out loud, \"I am aware that I am aware\"—then your consciousness is not without effect on your internal narrative, or your moving lips.  You can see yourself seeing, and your internal narrative reflects this, and so do your lips if you choose to say it out loud.\n\nI have not seen the above argument written out that particular way—\"the listener caught in the act of listening\"—though it may well have been said before.\n\nBut it is a [standard point](http://plato.stanford.edu/entries/zombies/)—which zombie-ist philosophers accept!—that the Zombie World's philosophers, being atom-by-atom identical to our own philosophers, write identical papers about the philosophy of consciousness.\n\nAt this point, the Zombie World stops being an intuitive consequence of the idea of a passive listener.\n\nPhilosophers writing papers about consciousness would _seem_ to be at least one effect of consciousness upon the world.  You can argue clever reasons why this is not so, but you have to be clever.\n\nYou would intuitively suppose that if your inward awareness went away, this would change the world, in that your internal narrative would no longer say things like \"There is a mysterious listener within me,\" because the mysterious listener would be gone.  It is usually right _after_ you focus your awareness on your awareness, that your internal narrative says \"I am aware of my awareness\", which suggests that if the first event never happened again, neither would the second.  You can argue clever reasons why this is not so, but you have to be clever.\n\nYou can form a propositional belief that \"Consciousness is without effect\", and not _see_ any contradiction at first, if you don't realize that talking about consciousness is an effect of being conscious.  But once you [see](http://www.overcomingbias.com/2008/03/hand-vs-fingers.html) the connection from the general rule that consciousness has no effect, to the specific implication that consciousness has no effect on how philosophers write papers about consciousness, zombie-ism stops being intuitive and starts requiring you to postulate strange things.\n\nOne strange thing you might postulate is that there's a Zombie Master, a god within the Zombie World who surreptitiously takes control of zombie philosophers and makes them talk and write about consciousness.\n\nA Zombie Master doesn't seem impossible.  Human beings often don't sound all that coherent when talking about consciousness.  It might not be that hard to fake their discourse, to the standards of, say, a human amateur talking in a bar.  Maybe you could take, as a corpus, one thousand human amateurs trying to discuss consciousness; feed them into a non-conscious but sophisticated AI, better than today's models but not self-modifying; and get back discourse about \"consciousness\" that sounded as sensible as most humans, which is to say, not very.\n\nBut this speech about \"consciousness\" would not be spontaneous.  It would not be produced _within_ the AI.  It would be a [recorded imitation](http://www.overcomingbias.com/2007/11/artificial-addi.html) of someone else talking.  That is just a holodeck, with a central AI writing the speech of the [non-player characters](http://yudkowsky.net/fiction/NPC.html).  This is _not_ what the Zombie World is about.\n\nBy supposition, the Zombie World is atom-by-atom identical to our own, except that the inhabitants lack consciousness.  Furthermore, the atoms in the Zombie World move under the same laws of physics as in our own world.  If there are \"bridging laws\" that govern _which configurations of atoms evoke consciousness,_ those bridging laws are absent.  But, by hypothesis, the difference is not experimentally detectable.  When it comes to saying whether a quark zigs or zags or exerts a force on nearby quarks—anything experimentally measurable—the same physical laws govern.\n\nThe Zombie World has no _room_ for a Zombie Master, because a Zombie Master has to control the zombie's lips, and that control is, in principle, experimentally detectable.  The Zombie Master moves lips, therefore it has observable consequences.  There would be a point where an electron zags, instead of zigging, because the Zombie Master says so.  (Unless the Zombie Master is actually _in_ the world, as a pattern of quarks—but then the Zombie World is not atom-by-atom identical to our own, unless you think _this_ world also contains a Zombie Master.)\n\nWhen a philosopher in our world types, \"I think the Zombie World is possible\", his fingers strike keys in sequence:  Z-O-M-B-I-E.  There is a chain of causality that can be traced back from these keystrokes: muscles contracting, nerves firing, commands sent down through the spinal cord, from the motor cortex—and then into less understood areas of the brain, where the philosopher's internal narrative first began talking about \"consciousness\".\n\nAnd the philosopher's zombie twin strikes the same keys, _for the same reason,_ causally speaking.  There is no cause within the chain of explanation for why the philosopher writes the way he does, which is not also present in the zombie twin.  The zombie twin also has an internal narrative about \"consciousness\", that a super-fMRI could read out of the auditory cortex.  And whatever other thoughts, or other causes of any kind, led to that internal narrative, they are exactly the same in our own universe and in the Zombie World.\n\nSo you can't say that the philosopher is writing about consciousness _because of_ consciousness, while the zombie twin is writing about consciousness _because of_ a Zombie Master or AI chatbot.  When you trace back the chain of causality behind the keyboard, to the internal narrative echoed in the auditory cortex, to the cause of the narrative, you must find the _same_ physical explanation in our world as in the zombie world.\n\nAs the most formidable advocate of zombie-ism, David Chalmers, [writes](http://books.google.com/books?id=0fZZQHOfdAAC&dq=the+conscious+mind&pg=PP1&ots=qjv14gGUOR&sig=98dNHve8xZp-yltM37iN8mgq2AQ&hl=en&prev=http://www.google.com/search?hl=en&q=the+conscious+mind&sa=X&oi=print&ct=title&cad=one-book-with-thumbnail#PPA180,M1):\n\n> Think of my zombie twin in the universe next door. He talks about conscious experience all the time—in fact, he seems obsessed by it. He spends ridiculous amounts of time hunched over a computer, writing chapter after chapter on the mysteries of consciousness. He often comments on the pleasure he gets from certain sensory qualia, professing a particular love for deep greens and purples. He frequently gets into arguments with zombie materialists, arguing that their position cannot do justice to the realities of conscious experience.\n> \n> And yet he has no conscious experience at all! In his universe, the materialists are right and he is wrong. Most of his claims about conscious experience are utterly false. But there is certainly a physical or functional explanation of why he makes the claims he makes. After all, his universe is fully law-governed, and no events therein are miraculous, so there must be some explanation of his claims.\n> \n> ...Any explanation of my twin’s behavior will equally count as an explanation of my behavior, as the processes inside his body are precisely mirrored by those inside mine. The explanation of his claims obviously does not depend on the existence of consciousness, as there is no consciousness in his world. It follows that the explanation of my claims is also independent of the existence of consciousness.\n\nChalmers is not arguing _against_ zombies; those are his actual beliefs!\n\n> This paradoxical situation is at once delightful and disturbing.  It is not obviously fatal to the nonreductive position, but it is at least something that we need to come to grips  \n> with...\n\nI would seriously nominate this as the largest bullet ever bitten in the history of time.  And that is a backhanded compliment to David Chalmers:  A lesser mortal would simply fail to see the implications, or refuse to face them, or rationalize a reason it wasn't so.\n\nWhy would anyone bite a bullet that large?  Why would anyone postulate unconscious zombies who write papers about consciousness for _exactly the same reason_ that our own genuinely conscious philosophers do?\n\nNot because of the first intuition I wrote about, the intuition of the passive listener.  That intuition may say that zombies can drive cars or do math or even fall in love, but it doesn't say that zombies write philosophy papers about their passive listeners.\n\nThe zombie argument does not rest _solely_ on the intuition of the passive listener.  If this was all there was to the zombie argument, it would be dead by now, I think.  The intuition that the \"listener\" can be eliminated without effect, would go away as soon as you realized that your internal narrative routinely _seems_ to catch the listener in the act of listening.\n\nNo, the drive to bite _this_ bullet comes from an entirely different intuition—the intuition that no matter how many atoms you add up, no matter how many masses and electrical charges interact with each other, they will never _necessarily_ produce a subjective sensation of the mysterious redness of red.  It may be a fact about our physical universe (Chalmers says) that putting such-and-such atoms into such-and-such a position, _evokes_ a sensation of redness; but if so, it is not a _necessary_ fact, it is something to be explained above and beyond the motion of the atoms.\n\nBut if you consider the second intuition on its own, without the intuition of the passive listener, it is hard to see why it implies zombie-ism.  Maybe there's just a _different kind of stuff,_ apart from and additional to atoms, that is _not_ causally passive—a soul that actually _does_ stuff, a soul that plays a real causal role in why we write about \"the mysterious redness of red\".  Take out the soul, and... well, assuming you just don't fall over in a coma, you certainly won't write any more papers about consciousness!\n\nThis is the position taken by Descartes and most other ancient thinkers:  The soul is of a different kind, but it _interacts_ with the body.  Descartes's position is technically known as _substance dualism_—there is a thought-stuff, a mind-stuff, and it is not like atoms; but it is causally potent, interactive, and leaves a visible mark on our universe.\n\nZombie-ists are _property dualists_—they don't believe in a _separate_ soul; they believe that matter in our universe has _additional properties_ beyond the physical.\n\n\"Beyond the physical\"?  What does that mean?  It means the extra properties are there, but they don't influence the motion of the atoms, like the properties of electrical charge or mass.  The extra properties are not experimentally detectable _by third parties;_ _you_ know you are conscious, from the _inside_ of your extra properties, but no scientist can ever directly detect this from outside.\n\nSo the additional properties are there, but not causally active.  The extra properties do not move atoms around, which is why they can't be detected by third parties.\n\nAnd that's why we can (allegedly) imagine a universe just like this one, with all the atoms in the same places, but the extra properties missing, so that everything goes on the same as before, but no one is conscious.\n\nThe Zombie World may not be _physically_ possible, say the zombie-ists—because it is a fact that all the matter in our universe has the extra properties, or obeys the bridging laws that evoke consciousness—but the Zombie World is _logically_ possible: the bridging laws could have been different.\n\nBut, once you realize that conceivability is not the same as logical possibility, and that the Zombie World isn't even all that intuitive, why say that the Zombie World is logically possible?\n\nWhy, oh why, say that the extra properties are epiphenomenal and indetectable?\n\nWe can put this dilemma very sharply:  Chalmers believes that there _is_ something called consciousness, and this consciousness embodies the true and indescribable substance of the mysterious redness of red.  It may be a property beyond mass and charge, but it's _there_, and it _is_ consciousness.  Now, having said the above, Chalmers furthermore specifies that this true stuff of consciousness is epiphenomenal, without causal potency—but _why say that?_\n\nWhy say that you could subtract this true stuff of consciousness, and leave all the atoms in the same place doing the same things?  If that's true, we need some _separate_ physical explanation for why Chalmers talks about \"the mysterious redness of red\".  That is, there exists both a mysterious redness of red, which is extra-physical, and _an entirely separate_ reason, _within_ physics, why Chalmers _talks_ about the \"mysterious redness of red\".\n\nChalmers does confess that these two things seem like they ought to be related, but really, why do you need both?  Why not just pick one or the other?\n\nOnce you've postulated that there is a mysterious redness of red, why not just say that it interacts with your internal narrative and makes you talk about the \"mysterious redness of red\"?\n\nIsn't Descartes taking the simpler approach, here?  The _strictly_ simpler approach?\n\nWhy postulate an extramaterial soul, _and then_ postulate that the soul has no effect on the physical world, _and then_ postulate a mysterious unknown _material_ process that causes your internal narrative to talk about conscious experience?\n\nWhy not postulate the true stuff of consciousness which no amount of mere mechanical atoms can add up to, _and then,_ having gone that far already, let this true stuff of consciousness have causal effects like making philosophers talk about consciousness?\n\nI am not endorsing Descartes's view.  But at least I can understand where Descartes is coming from.  Consciousness seems mysterious, so you postulate a [mysterious stuff of consciousness](http://www.overcomingbias.com/2007/08/mysterious-answ.html).  Fine.\n\nBut now the zombie-ists postulate that this mysterious stuff _doesn't do anything_, so you need a _whole new_ explanation for why you _say_ you're conscious.\n\nThat isn't vitalism.  That's something so bizarre that vitalists would spit out their coffee.  \"When fires burn, they release [phlogiston](http://www.overcomingbias.com/2007/08/fake-causality.html).  _But_ phlogiston doesn't have any experimentally detectable impact on our universe, so you'll have to go looking for a _separate_ explanation of why a fire can melt snow.\"  _What?_\n\nAre property dualists under the impression that if they postulate a new _active_ force, something that has a causal impact on observables, they will be sticking their necks out too far?\n\nMe, I'd say that if you postulate a mysterious, separate, additional, inherently mental property of consciousness, above and beyond positions and velocities, then, at that point, you have _already_ stuck your neck out as far as it can go.  To postulate this stuff of consciousness, and then further postulate that it _doesn't do anything_—for the love of cute kittens, _why?_\n\nThere isn't even an obvious career motive.  \"Hi, I'm a philosopher of consciousness.  My subject matter is the most important thing in the universe and I should get lots of funding?  Well, it's nice of you to say so, but actually the phenomenon I study doesn't do anything whatsoever.\"  (Argument from career impact is not valid, but I say it to [leave a line of retreat](http://www.overcomingbias.com/2008/02/leave-retreat.html).)\n\nChalmers critiques substance dualism on the grounds that it's hard to see what new theory of physics, what new substance that interacts with matter, could possibly explain consciousness.  But property dualism has exactly the same problem.  No matter what kind of dual property you talk about, how exactly does it explain consciousness?\n\nWhen Chalmers postulated an extra property that _is_ consciousness, he _took_ that leap across the unexplainable.  How does it help his theory to further specify that this extra property _has no effect_?  Why not just let it be causal?\n\nIf I were going to be unkind, this would be the time to drag in the dragon—to mention Carl Sagan's parable of the [dragon in the garage](http://www.overcomingbias.com/2007/07/belief-in-belie.html).  \"I have a dragon in my garage.\"  Great!  I want to see it, let's go!  \"You can't see it—it's an invisible dragon.\"  Oh, I'd like to hear it then.  \"Sorry, it's an inaudible dragon.\"  I'd like to measure its carbon dioxide output.  \"It doesn't breathe.\"  I'll toss a bag of flour into the air, to outline its form.  \"The dragon is permeable to flour.\"\n\nOne motive for trying to make your theory unfalsifiable, is that deep down you fear to put it to the test.  Sir Roger Penrose (physicist) and Stuart Hameroff (neurologist) are substance dualists; they think that there is something mysterious going on in quantum, that Everett is wrong and that the \"collapse of the wave-function\" is physically real, and that this is where consciousness lives and how it exerts causal effect upon your lips when you say aloud \"I think therefore I am.\"  Believing this, they predicted that neurons would protect themselves from decoherence long enough to maintain macroscopic quantum states.\n\nThis is in the process of being tested, and so far, prospects are not looking good for Penrose—\n\n—but Penrose's basic conduct is scientifically respectable.  [Not Bayesian, maybe](http://www.overcomingbias.com/2007/08/my-wild-and-rec.html), but still fundamentally healthy.  He came up with a wacky hypothesis.  He said how to test it.  He went out and tried to actually test it.\n\nAs I once said to Stuart Hameroff, \"I think the hypothesis you're testing is completely hopeless, and your experiments should _definitely_ be funded.  Even if you don't find exactly what you're looking for, you're looking in a place where no one else is looking, and you might find something interesting.\"\n\nSo a nasty dismissal of epiphenomenalism would be that zombie-ists are afraid to say the consciousness-stuff can have _effects,_ because then scientists could go _looking_ for the extra properties, and fail to find them.\n\nI don't think this is actually true of Chalmers, though.  If Chalmers lacked self-honesty, he could make things a _lot_ easier on himself.\n\n(But just in case Chalmers is reading this and does have falsification-fear, I'll point out that if epiphenomenalism is false, then there _is_ some otherexplanation for that-which-we-call consciousness, and it will eventually be found, leaving Chalmers's theory in ruins; so if Chalmers cares about his place in history, he has no motive to endorse epiphenomenalism unless he [really thinks it's true](http://www.overcomingbias.com/2007/10/curiosity.html).)\n\nChalmers is one of the most frustrating philosophers I know.  Sometimes I wonder if he's pulling an \"[Atheism Conquered](http://www.expatsingapore.com/forum/index.php?topic=16763.msg126809)\".  Chalmers does this really _sharp_ analysis... and then turns left at the last minute.  He lays out everything that's wrong with the Zombie World scenario, and then, having reduced the whole argument to smithereens, calmly accepts it.\n\nChalmers does the same thing when he lays out, in calm detail, the problem with saying that our own beliefs in consciousness are justified, when our zombie twins say exactly the same thing for exactly the same reasons and are wrong.\n\nOn Chalmers's theory, Chalmers saying that he believes in consciousness cannot be _causally_ justified; the belief is not [caused by the fact itself](http://www.overcomingbias.com/2008/03/explaining-away.html).  In the absence of consciousness, Chalmers would write the same papers for the same reasons.\n\nOn epiphenomenalism, Chalmers saying that he believes in consciousness cannot be justified as the product of a process that systematically outputs true beliefs, because the zombie twin writes the same papers using the same systematic process and is wrong.\n\nChalmers admits this.  Chalmers, in fact, explains the argument in great detail in his book.  Okay, so Chalmers has solidly proven that he is not justified in believing in epiphenomenal consciousness, right?  No.  Chalmers writes:\n\n> Conscious experience lies at the center of our epistemic universe; we have access to it _directly._  This raises the question: what is it that justifies our beliefs about our experiences, if it is not a causal link to those experiences, and if it is not the mechanisms by which the beliefs are formed?  I think the answer to this is clear: it is _having_ the experiences that justifies the beliefs. For example, the very fact that I have a red experience now provides justification for my belief that I am having a red experience...\n> \n> Because my zombie twin lacks experiences, he is in a very different epistemic situation from me, and his judgments lack the corresponding justification.  It may be tempting to object that if my belief lies in the physical realm, its justification must lie in the physical realm; but this is a _non sequitur._ From the fact that there is no justification in the physical realm, one might conclude that the _physical_ portion of me (my brain, say) is not justified in its belief. But the question is whether _I_ am justified in the belief, not whether my _brain_ is justified in the belief, and if property dualism is correct than there is more to me than my brain.\n\nSo—if I've got this thesis right—there's a core you, above and beyond your brain, that believes it is not a zombie, and directly experiences not being a zombie; and so its beliefs are justified.\n\nBut Chalmers just _wrote all that stuff down,_ in his very physical _book,_ and so did the zombie-Chalmers.\n\nThe zombie Chalmers can't have written the book _because_ of the zombie's core self above the brain; there must be some entirely different reason, within the laws of physics.\n\nIt follows that even if there _is_ a part of Chalmers hidden away that is conscious and believes in consciousness, directly and without mediation, there is also a _separable subspace_ of Chalmers—a causally closed cognitive subsystem that acts entirely _within_ physics—and this \"outer self\" is what speaks Chalmers's internal narrative, and writes papers on consciousness.\n\nI do not see any way to evade the charge that, on Chalmers's own theory, this separable outer Chalmers is deranged.  This is the part of Chalmers that is the same in this world, or the Zombie World; and in either world it writes philosophy papers on consciousness _for no valid reason._  Chalmers's philosophy papers are not output by that inner core of awareness and belief-in-awareness, they are output by the mere physics of the internal narrative that makes Chalmers's fingers strike the keys of his computer.\n\nAnd yet this deranged outer Chalmers is writing philosophy papers that [_just happen_ to be perfectly right](http://www.overcomingbias.com/2008/02/perpetual-motio.html), _by a separate and additional miracle._  Not a logically necessary miracle (then the Zombie World would not be logically possible).  A physically contingent miracle, that happens to be true in what we think is our universe, even though science can never distinguish our universe from the Zombie World.\n\nOr at least, that would seem to be the implication of what the self-confessedly deranged outer Chalmers is telling us.\n\nI think I speak for all reductionists when I say Huh?\n\nThat's not epicycles.  That's, \"Planetary motions follow these epicycles—but epicycles don't actually _do_ anything—there's something else that makes the planets move the same way the epicycles say they should, which I haven't been able to explain—and by the way, I would say this even if there weren't any epicycles.\"\n\nI have a [nonstandard perspective](http://www.overcomingbias.com/2007/10/how-to-seem-and.html) on philosophy because I look at everything with an eye to designing an AI; specifically, a self-improving Artificial General Intelligence with stable motivational structure.\n\nWhen I think about designing an AI, I ponder principles like [probability theory](http://www.overcomingbias.com/2008/01/beautiful-proba.html), the [Bayesian](http://yudowsky.net/bayes/bayes.html) notion of [evidence as differential diagnostic](http://www.overcomingbias.com/2007/09/what-is-evidenc.html), and above all, reflective coherence.  Any self-modifying AI that starts out in a reflectively inconsistent state [won't stay that way for long](http://www.overcomingbias.com/2008/01/newcombs-proble.html).\n\nIf a self-modifying AI looks at a part of itself that concludes \"B\" on condition A—a part of itself that writes \"B\" to memory whenever condition A is true—and the AI inspects this part, determines how it (causally) operates in the context of the larger universe, and the AI decides that this part systematically tends to write false data to memory, then the AI has found what appears to be a bug, and the AI will self-modify not to write \"B\" to the belief pool under condition A.\n\nAny epistemological theory that disregards reflective coherence is not a good theory to use in constructing self-improving AI.  This is a knockdown argument from my perspective, considering what I intend to actually use philosophy _for_.  So I have to invent a reflectively coherent theory anyway.  And when I do, by golly, reflective coherence turns out to [make intuitive sense](http://www.overcomingbias.com/2007/09/the-lens-that-s.html).\n\nSo that's the unusual way in which I tend to think about these things.  And now I look back at Chalmers:\n\nThe causally closed \"outer Chalmers\" (that is not influenced in any way by the \"inner Chalmers\" that has separate additional awareness and beliefs) must be carrying out some systematically unreliable, unwarranted operation which _in some unexplained fashion_ causes the internal narrative to produce beliefs about an \"inner Chalmers\" that are _correct for no logical reason_ in what happens to be our universe.\n\nBut there's no possible warrant for the outer Chalmers _or any reflectively coherent self-inspecting AI_ to believe in this mysterious correctness.  A good AI design should, I think, look like a reflectively coherent intelligence embodied in a causal system, with a _testable_ theory of how that selfsame causal system produces systematically [accurate](http://www.overcomingbias.com/2008/03/qualitatively-c.html) beliefs on the way to [achieving its goals](http://www.overcomingbias.com/2008/01/something-to-pr.html).\n\nSo the AI will scan Chalmers and see a closed causal cognitive system producing an internal narrative that is uttering nonsense.  Nonsense that seems to have a high impact on what Chalmers thinks _should be considered a morally valuable person._\n\nThis is not a _necessary_ problem for Friendly AI theorists.  It is _only_ a problem if you happen to be an epiphenomenalist.  If you believe either the reductionists (consciousness happens _within_ the atoms) or the substance dualists (consciousness is _causally potent_ immaterial stuff), people talking about consciousness are talking about something real, and a reflectively consistent Bayesian AI can see this by tracing back the chain of causality for what makes people say \"consciousness\".\n\nAccording to Chalmers, the causally closed cognitive system of Chalmers's internal narrative is (mysteriously) malfunctioning in a way that, not by necessity, but just in _our_ universe, miraculously happens to be correct.  Furthermore, the internal narrative asserts \"the internal narrative is mysteriously malfunctioning, but miraculously happens to be correctly echoing the justified thoughts of the epiphenomenal inner core\", and again, in _our_ universe, miraculously happens to be correct.\n\n_Oh, come on!_\n\nShouldn't there come a point where you just give up on an idea?  Where, on some raw intuitive level, you just go:  _What on Earth was I thinking?_\n\nHumanity has accumulated some broad experience with what correct theories of the world look like.  _This is not what a correct theory looks like._\n\n\"Argument from incredulity,\" you say.  Fine, you want it spelled out?  The said Chalmersian theory postulates multiple unexplained complex miracles.  This drives down its prior probability, by the [conjunction rule of probability](http://www.overcomingbias.com/2007/09/burdensome-deta.html) and [Occam's Razor](http://www.overcomingbias.com/2007/09/occams-razor.html).  It is therefore dominated by at least two theories which postulate fewer miracles, namely:\n\n*   Substance dualism:\n    *   There is a stuff of consciousness which is not yet understood, an extraordinary super-physical stuff that _visibly affects_ our world; and this stuff is what makes us talk about consciousness.\n*   [Not-quite-faith-based](http://www.overcomingbias.com/2008/01/gray-fallacy.html) reductionism:\n    *   [That-which-we-name](http://www.overcomingbias.com/2008/04/reductive-refer.html) \"consciousness\" happens _within_ physics, in a way not yet understood, just like what happened the last three thousand times humanity ran into something mysterious.\n    *   Your intuition that no material substance can possibly [add up](http://www.overcomingbias.com/2008/03/wrong-questions.html) to consciousness is incorrect.  If you _actually_ knew _exactly_ why you talk about consciousness, this would give you new insights, of a form you can't now anticipate; and afterward you would realize that your arguments about normal physics having no room for consciousness were [flawed](http://www.overcomingbias.com/2008/03/dissolving-the.html).\n\nCompare to:\n\n*   Epiphenomenal property dualism:\n    *   Matter has additional consciousness-properties which are not yet understood.  These properties are epiphenomenal with respect to ordinarily observable physics—they make no difference to the motion of particles.\n    *   _Separately_, there exists a not-yet-understood reason _within normal physics_ why philosophers talk about consciousness and invent theories of dual properties.\n    *   _Miraculously,_ when philosophers talk about consciousness, the bridging laws of _our_ world are exactly right to make this talk about consciousness correct, even though it arises from a malfunction (drawing of logically unwarranted conclusions) in the causally closed cognitive system that types philosophy papers.\n\nI know I'm speaking from limited experience, here.  But based on my limited experience, the Zombie Argument may be a candidate for _the most deranged idea in all of philosophy._\n\nThere are times when, as a rationalist, you have to believe things that [seem weird](http://www.overcomingbias.com/2007/05/think_like_real.html) to you.  Relativity seems weird, quantum mechanics seems weird, [natural selection](http://www.overcomingbias.com/2007/11/an-alien-god.html) seems weird.\n\nBut these weirdnesses are pinned down by massive evidence.  There's a difference between believing something weird because science has confirmed it overwhelmingly—\n\n—versus believing a proposition that seems downright _deranged,_ because of a great big complicated philosophical argument centered around unspecified miracles and giant blank spots not even claimed to be understood—\n\n—in a case where _even if you accept everything that has been told to you so far,_ afterward the phenomenon will still seem like a mystery and [still have the same quality of wondrous impenetrability that it had at the start](http://www.overcomingbias.com/2007/08/mysterious-answ.html).\n\nThe correct thing for a rationalist to say at this point, if all of David Chalmers's arguments seem individually plausible—which they don't seem to me—is:\n\n\"Okay... I don't know how consciousness works... I admit that... and maybe I'm approaching the whole problem wrong, or asking the wrong questions... but this zombie business _can't possibly be right._  The arguments aren't nailed down enough to make me believe this—especially when accepting it won't make me feel any less confused.  On a core gut level, this just _doesn't look_ like the way reality could _really really_ work.\"\n\nMind you, I am not saying this is a substitute for careful analytic refutation of Chalmers's thesis.  [System 1](http://www.overcomingbias.com/2006/11/why_truth_and.html) is not a substitute for System 2, though it can help point the way.  You still have to track down where the problems are _specifically._\n\nChalmers wrote a big book, not all of which is available through free Google preview.  I haven't duplicated the long chains of argument where Chalmers lays out the arguments against himself in calm detail.  I've just tried to tack on a final refutation of Chalmers's last presented defense, which Chalmers has not yet countered to my knowledge.  Hit the ball back into his court, as it were.\n\nBut, yes, on a core level, the _sane_ thing to do when you see the conclusion of the zombie argument, is to say \"That can't _possibly_ be right\" and start looking for a flaw."
          },
          "voteCount": 85
        },
        {
          "name": "GAZP vs. GLUT",
          "type": "post",
          "slug": "gazp-vs-glut",
          "_id": "k6EPphHiBH4WWYFCj",
          "url": null,
          "title": "GAZP vs. GLUT",
          "author": "Eliezer_Yudkowsky",
          "question": false,
          "tags": [
            {
              "name": "Zombies"
            },
            {
              "name": "Consciousness"
            }
          ],
          "tableOfContents": null,
          "contents": {
            "markdown": "In \"The Unimagined Preposterousness of Zombies\", Daniel Dennett says:\n\n> To date, several philosophers have told me that they plan to accept my challenge to offer a non-question-begging defense of zombies, but the only one I have seen so far involves postulating a \"logically possible\" but fantastic being — a descendent of Ned Block's Giant Lookup Table fantasy...\n\nA Giant Lookup Table, in programmer's parlance, is when you implement a function as a giant table of inputs and outputs, usually to save on runtime computation.  If my program needs to know the multiplicative product of two inputs between 1 and 100, I can write a multiplication algorithm that computes each time the function is called, or I can precompute a Giant Lookup Table with 10,000 entries and two indices.  There are times when you _do_ want to do this, though not for multiplication—times when you're going to reuse the function a lot and it doesn't have many possible inputs; or when clock cycles are cheap while you're initializing, but very expensive while executing.\n\nGiant Lookup Tables get very large, very fast.  A GLUT of all possible twenty-ply conversations with ten words per remark, using only 850-word Basic English, would require 7.6 * 10^585^ entries.\n\nReplacing a human brain with a Giant Lookup Table of all possible sense inputs and motor outputs (relative to some fine-grained digitization scheme) would require an _unreasonably large amount_ of memory storage.  But \"in principle\", as philosophers are fond of saying, it could be done.\n\nThe GLUT is not a zombie in the classic sense, because it is microphysically dissimilar to a human.  (In fact, a GLUT can't _really_ run on the same physics as a human; it's too large to fit in our universe.  For philosophical purposes, we shall ignore this and suppose a supply of unlimited memory storage.)\n\nBut is the GLUT a zombie at _all_?  That is, does it behave exactly like a human without being conscious?\n\nThe GLUT-ed body's tongue talks about consciousness.  Its fingers write philosophy papers.  In every way, so long as you don't peer inside the skull, the GLUT seems just like a human... which certainly seems like a valid example of a zombie: it behaves just like a human, but there's no one home.\n\nUnless the GLUT is conscious, in which case it wouldn't be a valid example.\n\nI can't recall ever seeing _anyone_ claim that a GLUT is conscious.  (Admittedly my reading in this area is not up to professional grade; feel free to correct me.)  Even people who are accused of being (gasp!) functionalists don't claim that GLUTs can be conscious.\n\nGLUTs are the _reductio ad absurdum_ to anyone who suggests that consciousness _is simply_ an input-output pattern, thereby disposing of all troublesome worries about what goes on inside.\n\nSo what does the [Generalized Anti-Zombie Principle](http://www.overcomingbias.com/2008/04/anti-zombie-pri.html) (GAZP) say about the Giant Lookup Table (GLUT)?\n\nAt first glance, it would seem that a GLUT is the very archetype of a Zombie Master—a distinct, additional, detectable, non-conscious system that animates a zombie and makes it talk about consciousness for _different_ reasons.\n\nIn the interior of the GLUT, there's merely a very simple computer program that looks up inputs and retrieves outputs.  Even talking about a \"simple computer program\" is overshooting the mark, in a case like this.  A GLUT is more like ROM than a CPU.  We could equally well talk about a series of switched tracks by which some balls roll out of a previously stored stack and into a trough—_period;_ that's _all_ the GLUT does.\n\nA spokesperson from People for the Ethical Treatment of Zombies replies:  \"Oh, that's what all the anti-mechanists say, isn't it?  That when you look in the brain, you just find a bunch of neurotransmitters opening ion channels?  If ion channels can be conscious, why not levers and balls rolling into bins?\"\n\n\"The problem isn't the levers,\" replies the functionalist, \"the problem is that a GLUT has the _wrong pattern_ of levers.  You need levers that implement things like, say, formation of beliefs about beliefs, or self-modeling...  Heck, you need the ability to write things to memory just so that time can pass for the computation.  Unless you think it's possible to program a conscious being in Haskell.\"\n\n\"I don't know about that,\" says the PETZ spokesperson, \"all I know is that this so-called zombie writes philosophical papers about consciousness.  Where do these philosophy papers come from, if not from consciousness?\"\n\nGood question!  Let us ponder it deeply.\n\nThere's a game in physics called Follow-The-Energy.  [Richard Feynman's father](http://www.textbookleague.org/103feyn.htm) played it with young Richard:\n\n>     It was the kind of thing my father would have talked about:  \"What makes it go?  Everything goes because the sun is shining.\"   And then we would have fun discussing it:  \n>     \"No, the toy goes because the spring is wound up,\" I would say.  \"How did the spring get wound up?\" he would ask.  \n>     \"I wound it up.\"  \n>     \"And how did you get moving?\"  \n>     \"From eating.\"  \n>     \"And food grows only because the sun is shining.   So it's because the sun is shining that all these things are moving.\"   That would get the concept across that motion is simply the _transformation_ of the sun's power.\n\nWhen you get a little older, you learn that energy is conserved, never created or destroyed, so the notion of _using up_ energy doesn't make much sense.  You can never change the total amount of energy, so in what sense are you _using_ it?\n\nSo when physicists grow up, they learn to play a new game called [Follow-The-Negentropy](http://www.overcomingbias.com/2008/02/second-law.html)—which is really the same game they were playing all along; only the rules are mathier, the game is more useful, and the principles are harder to wrap your mind around conceptually.\n\nRationalists learn a game called [Follow-The-Improbability](http://www.overcomingbias.com/2008/02/perpetual-motio.html), the grownup version of \"How Do You Know?\"  The rule of the rationalist's game is that every improbable-seeming belief needs an [equivalent amount of evidence to justify it](http://www.overcomingbias.com/2007/09/how-much-eviden.html).  (This game has _amazingly similar_ rules to Follow-The-Negentropy.)\n\nWhenever someone violates the rules of the rationalist's game, you can find a place in their argument where [a quantity of improbability appears from nowhere;](http://www.overcomingbias.com/2008/02/perpetual-motio.html) and this is as much a sign of a problem as, oh, say, an ingenious design of linked wheels and gears that keeps itself running forever.\n\nThe one comes to you and says:  \"I believe with firm and abiding faith that there's an object in the asteroid belt, one foot across and composed entirely of chocolate cake; you can't [prove](http://www.overcomingbias.com/2008/01/absolute-author.html) that this is impossible.\"  But, unless the one had access to some kind of evidence for this belief, it would be highly improbable for a correct belief to form _spontaneously_.  So either the one can point to evidence, or the belief won't turn out to be true.  \"But you can't prove it's _impossible_ for my mind to spontaneously generate a belief that happens to be correct!\"  No, but that kind of spontaneous generation is _highly improbable_, just like, oh, say, an egg unscrambling itself.\n\nIn [Follow-The-Improbability](http://www.overcomingbias.com/2008/02/perpetual-motio.html), it's highly suspicious to even _talk_ about a specific hypothesis without having had [enough evidence to narrow down the space of possible hypotheses](http://www.overcomingbias.com/2007/09/einsteins-arrog.html).  Why aren't you giving equal air time to a decillion other equally plausible hypotheses?  You need sufficient evidence to find the \"chocolate cake in the asteroid belt\" hypothesis in the hypothesis space—otherwise there's no reason to give it more air time than a trillion other candidates like \"There's a wooden dresser in the asteroid belt\" or \"The Flying Spaghetti Monster threw up on my sneakers.\"\n\nIn Follow-The-Improbability, you are not allowed to pull out big complicated specific hypotheses from thin air without _already_ having a corresponding amount of evidence; because it's not realistic to suppose that you could spontaneously start discussing the _true_ hypothesis by _pure coincidence._\n\nA philosopher says, \"This zombie's skull contains a Giant Lookup Table of all the inputs and outputs for some human's brain.\"  This is a very _large_ improbability.  So you ask, \"How did this improbable event occur?  Where did the GLUT come from?\"\n\nNow this is not standard philosophical procedure for thought experiments.  In standard philosophical procedure, you are allowed to postulate things like \"Suppose you were riding a beam of light...\" without worrying about physical possibility, let alone mere improbability.  But in this case, the origin of the GLUT matters; and that's why it's important to understand the motivating question, \"Where did the improbability come from?\"\n\nThe obvious answer is that you took a computational specification of a human brain, and used _that_ to precompute the Giant Lookup Table.  (Thereby creating uncounted googols of human beings, some of them in extreme pain, the supermajority gone quite mad in a universe of chaos where inputs bear no relation to outputs.  But damn the ethics, this is for _philosophy_.)\n\nIn this case, the GLUT _is_ writing papers about consciousness because of a conscious algorithm.  The GLUT is no more a zombie, than a cellphone is a zombie because it can talk about consciousness while being just a small consumer electronic device.  The cellphone is just transmitting philosophy speeches from whoever happens to be on the other end of the line.  A GLUT generated from an originally human brain-specification is doing the same thing.\n\n\"All right,\" says the philosopher, \"the GLUT was generated randomly, and _just happens_ to have the same input-output relations as some reference human.\"\n\nHow, exactly, did you randomly generate the GLUT?\n\n\"We used a true randomness source—a quantum device.\"\n\nBut a quantum device just implements the Branch Both Ways instruction; when you generate a bit from a quantum randomness source, the deterministic result is that one set of universe-branches (locally connected amplitude clouds) see 1, and another set of universes see 0.  Do it 4 times, create 16 (sets of) universes.\n\nSo, really, this is like saying that you got the GLUT by writing down all possible GLUT-sized sequences of 0s and 1s, in a really damn huge bin of lookup tables; and then reaching into the bin, and _somehow_ pulling out a GLUT that happened to correspond to a human brain-specification.  Where did the improbability come from?\n\nBecause if this _wasn't just a coincidence_—if you had some reach-into-the-bin function that pulled out a human-corresponding GLUT by _design,_ not just chance—then that reach-into-the-bin function is probably conscious, and so the GLUT is again a cellphone, not a zombie.  It's connected to a human at two removes, instead of one, but it's still a cellphone!  Nice try at concealing the source of the improbability there!\n\nNow behold where Follow-The-Improbability has taken us: where is the source of this body's tongue talking about an inner listener?  The consciousness isn't in the lookup table.  The consciousness isn't in the factory that manufactures lots of possible lookup tables.  The consciousness was in whatever _pointed to one particular already-manufactured lookup table,_ and said, \"Use _that_ one!\"\n\nYou can see why I introduced the game of Follow-The-Improbability.  Ordinarily, when we're talking to a person, we tend to think that whatever is inside the skull, must be \"where the consciousness is\".  It's only by playing Follow-The-Improbability that we can realize that the real source of the conversation we're having, is that-which-is-responsible-for the _improbability_ of the conversation—however distant in time or space, as the Sun moves a wind-up toy.\n\n\"No, no!\" says the philosopher.  \"In the thought experiment, they aren't randomly generating lots of GLUTs, and then using a conscious algorithm to pick out one GLUT that seems humanlike! I am _specifying_ that, in this thought experiment, they reach into the inconceivably vast GLUT bin, and _by pure chance_ pull out a GLUT that is identical to a human brain's inputs and outputs!  _There!_  I've got you cornered now!  You can't play Follow-The-Improbability any further!\"\n\nOh.  So your _specification_ is the source of the improbability here.\n\nWhen we play Follow-The-Improbability again, we end up _outside the thought experiment,_ looking at the _philosopher._\n\nThat which points to the one GLUT that talks about consciousness, out of all the vast space of possibilities, is now... the conscious person asking us to imagine this whole scenario.  And our own brains, which will fill in the blank when we imagine, \"What will this GLUT say in response to 'Talk about your inner listener'?\"\n\nThe moral of this story is that when you follow back discourse about \"consciousness\", you generally find consciousness.  It's not always right in front of you.  Sometimes it's very cleverly hidden.  But it's there.  Hence the Generalized Anti-Zombie Principle.\n\nIf there is a Zombie Master in the form of a chatbot that processes and remixes amateur human discourse about \"consciousness\", the humans who generated the original text corpus are conscious.\n\nIf someday you come to understand consciousness, and look back, and see that there's a program you can write which will output confused philosophical discourse that sounds an awful lot like humans without itself being conscious—then when I ask \"How did this program come to sound similar to humans?\" the answer is that _you_ wrote it to sound similar _to conscious humans,_ rather than choosing on the criterion of similarity to something else.  This doesn't mean your little Zombie Master is conscious—but it does mean I can find consciousness somewhere in the universe by tracing back the chain of causality, which means we're not entirely in the Zombie World.\n\nBut suppose someone actually _did_ reach into a GLUT-bin and by _genuinely pure chance_ pulled out a GLUT that wrote philosophy papers?\n\nWell, then it wouldn't be conscious.  IMHO.\n\nI mean, there's got to be more to it than inputs and outputs.\n\nOtherwise even a GLUT would be conscious, right?\n\n* * *\n\nOh, and for those of you wondering how this sort of thing relates to my day job...\n\nIn this line of business you meet an awful lot of people who think that an arbitrarily generated powerful AI will be \"moral\".  They can't agree among themselves on why, or what they mean by the word \"moral\"; but they all agree that doing Friendly AI theory is unnecessary.  And when you ask them how an arbitrarily generated AI ends up with moral outputs, they proffer [elaborate rationalizations aimed at AIs](http://www.overcomingbias.com/2007/11/fake-optimizati.html) of that which they deem \"moral\"; and there are [all sorts of problems with this](http://www.overcomingbias.com/2007/12/fake-fake-utili.html), but the number one problem is, \"Are you _sure_ the AI would follow the same line of thought you invented to argue human morals, when, unlike you, the AI doesn't start out knowing what _you_ want it to rationalize?\"  You could call the counter-principle Follow-The-Decision-Information, or something along those lines.  You can account for an AI that does improbably nice things by telling me how you chose the AI's design from a huge space of possibilities, but otherwise the improbability is being pulled out of nowhere—though more and more heavily disguised, as rationalized premises are rationalized in turn.\n\nSo I've already done a [whole series of posts](http://www.overcomingbias.com/2007/12/fake-fake-utili.html) which [I myself generated](http://www.overcomingbias.com/2007/11/truly-part-of-y.html) using Follow-The-Improbability.  But I didn't spell out the rules _explicitly_ at that time, because I hadn't done the [thermodynamic](http://www.overcomingbias.com/2008/02/perpetual-motio.html) posts yet...\n\nJust thought I'd mention that.  It's amazing how many of my Overcoming Bias posts would coincidentally turn out to include ideas surprisingly relevant to discussion of Friendly AI theory... if you believe in coincidence."
          },
          "voteCount": 58
        },
        {
          "name": "Belief in the Implied Invisible",
          "type": "post",
          "slug": "belief-in-the-implied-invisible",
          "_id": "3XMwPNMSbaPm2suGz",
          "url": null,
          "title": "Belief in the Implied Invisible",
          "author": "Eliezer_Yudkowsky",
          "question": false,
          "tags": [
            {
              "name": "Physics"
            },
            {
              "name": "Occam's Razor"
            },
            {
              "name": "Solomonoff Induction"
            },
            {
              "name": "Anticipated Experiences"
            }
          ],
          "tableOfContents": null,
          "contents": {
            "markdown": "One generalized lesson _not_ to learn from the Anti-Zombie Argument is, \"Anything you can't see doesn't exist.\"\n\nIt's tempting to conclude the general rule.  It would make the Anti-Zombie Argument much simpler, on future occasions, if we could take this as a premise.  But unfortunately that's just not Bayesian.\n\nSuppose I transmit a photon out toward infinity, not aimed at any stars, or any galaxies, pointing it toward one of the great voids between superclusters.  Based on standard physics, in other words, I don't expect this photon to intercept anything on its way out.  The photon is moving at light speed, so I can't chase after it and capture it again.\n\nIf the expansion of the universe is accelerating, as current cosmology holds, there will come a future point where I don't expect to be able to interact with the photon even in principle—a future time beyond which I don't expect the photon's future light cone to intercept my world-line.  Even if an alien species captured the photon and rushed back to tell us, they couldn't travel fast enough to make up for the accelerating expansion of the universe.\n\nShould I believe that, in the moment where I can no longer interact with it even in principle, the photon disappears?\n\nNo.\n\nIt would violate Conservation of Energy.  And the second law of thermodynamics.  And just about every other law of physics.  And probably the Three Laws of Robotics.  It would imply the photon knows I care about it and knows exactly when to disappear.\n\nIt's a _silly idea_.\n\nBut if you can believe in the continued existence of photons that have become experimentally undetectable to you, why doesn't this imply a general license to believe in the invisible?\n\n(If you want to think about this question on your own, do so before the jump...)\n\nThough I failed to Google a source, I remember reading that when it was first proposed that the Milky Way was our _galaxy_ —that the hazy river of light in the night sky was made up of millions (or even billions) of stars—that Occam's Razor was invoked against the new hypothesis.  Because, you see, the hypothesis vastly multiplied the number of \"entities\" in the believed universe.  Or maybe it was the suggestion that \"nebulae\"—those hazy patches seen through a telescope—might be galaxies full of stars, that got the invocation of Occam's Razor.\n\n_Lex parsimoniae:  Entia non sunt multiplicanda praeter necessitatem._\n\nThat was Occam's original formulation, the law of parsimony:  Entities should not be multiplied beyond necessity.\n\nIf you postulate billions of stars that no one has ever believed in before, you're multiplying entities, aren't you?\n\nNo.  There are [two Bayesian formalizations of Occam's Razor](/lw/jp/occams_razor/):  Solomonoff Induction, and Minimum Message Length.  Neither penalizes galaxies for being big.\n\nWhich they had better not do!  One of the lessons of history is that what-we-call-reality keeps turning out to be bigger and bigger and huger yet.  Remember when the Earth was at the center of the universe?  Remember when no one had invented Avogadro's number?  If Occam's Razor was weighing against the multiplication of entities every time, we'd have to start doubting Occam's Razor, because it would have consistently turned out to be wrong.\n\nIn Solomonoff induction, the complexity of your model is the amount of _code_ in the computer program you have to write to simulate your model.  The amount of _code,_ not the amount of RAM it uses, or the number of cycles it takes to compute.  A model of the universe that contains billions of galaxies containing billions of stars, each star made of a billion trillion decillion quarks, will take a lot of RAM to run—but the _code_ only has to describe the behavior of the quarks, and the stars and galaxies can be left to run themselves.  I am speaking semi-metaphorically here—there are things in the universe besides quarks—but the point is, postulating an extra billion galaxies doesn't count against the size of your code, if you've already described one galaxy.  It just takes a bit more RAM, and Occam's Razor doesn't care about RAM.\n\nWhy not?  The Minimum Message Length formalism, which is nearly equivalent to Solomonoff Induction, may make the principle clearer:  If you have to tell someone how your model of the universe works, you don't have to individually specify the location of each quark in each star in each galaxy.  You just have to write down some equations.  The amount of \"stuff\" that obeys the equation doesn't affect how long it takes to write the equation down.  If you encode the equation into a file, and the file is 100 bits long, then there are 2^100^ other models that would be around the same file size, and you'll need roughly 100 bits of supporting evidence.  You've got a limited amount of probability mass; and a priori, you've got to divide that mass up among all the messages you could send; and so postulating a model from within a model space of 2^100^ alternatives, means you've got to accept a 2^-100^ prior probability penalty—but having more galaxies doesn't add to this.\n\nPostulating billions of stars in billions of galaxies doesn't affect the length of your message describing the overall behavior of all those galaxies.  So you don't take a probability hit from having the _same_ equations describing more things.  (So long as your model's predictive successes aren't sensitive to the exact initial conditions.  If you've got to specify the exact positions of all the quarks for your model to predict as well as it does, the extra quarks do count as a hit.)\n\nIf you suppose that the photon disappears when you are no longer looking at it, this is an _additional law_ in your model of the universe.  It's the laws that are \"entities\", costly under the laws of parsimony.  Extra quarks are free.\n\nSo does it boil down to, \"I believe the photon goes on existing as it wings off to nowhere, because my priors say it's simpler for it to go on existing than to disappear\"?\n\nThis is what I thought at first, but on reflection, it's not quite right.  (And not just because it opens the door to obvious abuses.)\n\nI would boil it down to a distinction between belief in the _implied invisible,_ and belief in the _additional invisible._\n\nWhen you believe that the photon goes on existing as it wings out to infinity, you're not believing that as an _additional_ fact.\n\nWhat you believe (assign probability to) is a set of simple equations; you believe these equations describe the universe.  You believe these equations because they are the simplest equations you could find that describe the evidence.  These equations are _highly_ experimentally testable; they explain huge mounds of evidence visible in the past, and predict the results of many observations in the future.\n\nYou believe these equations, and it is a _logical implication_ of these equations that the photon goes on existing as it wings off to nowhere, so you believe that as well.\n\nYour priors, or even your probabilities, don't _directly_ talk about the photon.  What you assign probability to is not the photon, but the general laws.  When you assign probability to the laws of physics as we know them, you _automatically_ contribute that same probability to the photon continuing to exist on its way to nowhere—if you believe the logical implications of what you believe.\n\nIt's not that you believe in the invisible _as such,_ from reasoning about invisible things.  Rather the experimental evidence supports certain laws, and belief in those laws logically implies the existence of certain entities that you can't interact with.  This is belief in the _implied invisible._\n\nOn the other hand, if you believe that the photon is eaten out of existence by the Flying Spaghetti Monster—maybe on this just one occasion—or even if you believed without reason that the photon hit a dust speck on its way out—then you would be believing in a specific extra invisible event, on its own.  If you thought that this sort of thing happened in general, you would believe in a specific extra invisible law.  This is belief in the _additional invisible._\n\nThe whole matter would be a lot simpler, admittedly, if we could just rule out the existence of entities we can't interact with, once and for all—have the universe stop existing at the edge of our telescopes.  But this requires us to be very silly.\n\nSaying that you shouldn't ever need a separate and additional belief about invisible things—that you only believe invisibles that are _logical implications_ of general laws which are themselves testable, and even then, don't have any further beliefs about them that are not logical implications of visibly testable general rules—actually does seem to rule out all abuses of belief in the invisible, when applied correctly.\n\nPerhaps I should say, \"you should assign unaltered prior probability to additional invisibles\", rather than saying, \"do not believe in them.\"  But if you think of a _belief_ as something evidentially additional, something you bother to track, something where you bother to count up support for or against, then it's questionable whether we should ever have additional beliefs about additional invisibles.\n\nThere are exotic cases that break this in theory.  (E.g:  The epiphenomenal demons are watching you, and will torture [3^^^3](/lw/kd/pascals_mugging_tiny_probabilities_of_vast/) victims for a year, somewhere you can't ever verify the event, if you ever say the word \"Niblick\".)  But I can't think of a case where the principle fails in human practice.\n\n**Added:**  To make it clear why you would sometimes want to think about implied invisibles, suppose you're going to launch a spaceship, at nearly the speed of light, toward a faraway supercluster.  By the time the spaceship gets there and sets up a colony, the universe's expansion will have accelerated too much for them to ever send a message back.  Do you deem it worth the purely altruistic effort to set up this colony, for the sake of all the people who will live there and be happy?  Or do you think the spaceship blips out of existence before it gets there?  This could be a very real question at some point."
          },
          "voteCount": 46
        },
        {
          "name": "Zombies— The Movie",
          "type": "post",
          "slug": "zombies-the-movie",
          "_id": "fsDz6HieZJBu54Yes",
          "url": null,
          "title": "Zombies: The Movie",
          "author": "Eliezer_Yudkowsky",
          "question": false,
          "tags": [
            {
              "name": "Fiction"
            },
            {
              "name": "Zombies"
            },
            {
              "name": "Humor"
            },
            {
              "name": "Parables & Fables"
            },
            {
              "name": "Consciousness"
            },
            {
              "name": "The Hard Problem of Consciousness"
            }
          ],
          "tableOfContents": null,
          "contents": {
            "markdown": "FADE IN around a serious-looking group of uniformed military officers.  At the head of the table, a senior, heavy-set man, GENERAL FRED, speaks.\n\nGENERAL FRED:  The reports are confirmed.  New York has been overrun... by _zombies_.\n\nCOLONEL TODD:  Again?  But we just had a zombie invasion 28 days ago!\n\nGENERAL FRED:  These zombies... are different.  They're... _philosophical_ zombies.\n\nCAPTAIN MUDD:  Are they filled with rage, causing them to bite people?\n\nCOLONEL TODD:  Do they lose all capacity for reason?\n\nGENERAL FRED:  No.  They behave... _exactly_ like we do... except that they're not conscious.\n\n(_Silence grips the table._)\n\nCOLONEL TODD:  Dear God.\n\nGENERAL FRED moves over to a computerized display.\n\nGENERAL FRED:  This is New York City, two weeks ago.\n\nThe display shows crowds bustling through the streets, people eating in restaurants, a garbage truck hauling away trash.\n\nGENERAL FRED:  _This..._ is New York City... _now._\n\nThe display changes, showing a crowded subway train, a group of students laughing in a park, and a couple holding hands in the sunlight.\n\nCOLONEL TODD:  It's worse than I imagined.\n\nCAPTAIN MUDD:  How can you tell, exactly?\n\nCOLONEL TODD:  I've never seen anything so brutally ordinary.\n\nA lab-coated SCIENTIST stands up at the foot of the table.\n\nSCIENTIST:  The zombie disease eliminates consciousness without changing the brain in any way.  We've been trying to understand how the disease is transmitted.  Our conclusion is that, since the disease attacks dual properties of ordinary matter, it must, itself, operate outside our universe.  We're dealing with an _epiphenomenal virus_.\n\nGENERAL FRED:  Are you sure?\n\nSCIENTIST:  As sure as we can be in the total absence of evidence.\n\nGENERAL FRED:  All right.  Compile a report on every epiphenomenon ever observed.  What, where, and who.  I want a list of everything that hasn't happened in the last fifty years.\n\nCAPTAIN MUDD:  If the virus is epiphenomenal, how do we know it exists?\n\nSCIENTIST:  The same way we know _we're_ conscious.\n\nCAPTAIN MUDD:  Oh, okay.\n\nGENERAL FRED:  Have the doctors made any progress on finding an epiphenomenal cure?\n\nSCIENTIST:  They've tried every placebo in the book.  No dice.  Everything they do has an effect.\n\nGENERAL FRED:  Have you brought in a homeopath?\n\nSCIENTIST:  I tried, sir!  I couldn't find any!\n\nGENERAL FRED:  Excellent.  And the Taoists?\n\nSCIENTIST:  They refuse to do anything!\n\nGENERAL FRED:  Then we may yet be saved.\n\nCOLONEL TODD:  What about David Chalmers?  Shouldn't he be here?\n\nGENERAL FRED:  Chalmers... was one of the first victims.\n\nCOLONEL TODD:  Oh no.\n\n(_Cut_ to the INTERIOR of a cell, completely walled in by reinforced glass, where DAVID CHALMERS paces back and forth.)\n\nDOCTOR:  David!  David Chalmers!  Can you hear me?\n\nCHALMERS:  Yes.\n\nNURSE:  It's no use, doctor.\n\nCHALMERS:  I'm perfectly fine.  I've been introspecting on my consciousness, and I can't detect any difference.  I _know_ I would be expected to say that, but—\n\nThe DOCTOR turns away from the glass screen in horror.\n\nDOCTOR:  His words, they... they _don't mean anything._\n\nCHALMERS:  This is a grotesque distortion of my philosophical views.  This sort of thing can't actually happen!\n\nDOCTOR:  Why not?\n\nNURSE:  Yes, why not?\n\nCHALMERS:  Because—\n\n(_Cut_ to two POLICE OFFICERS, guarding a dirt road leading up to the imposing steel gate of a gigantic concrete complex.  On their uniforms, a badge reads \"BRIDGING LAW ENFORCEMENT AGENCY\".)\n\nOFFICER 1:  You've got to watch out for those clever bastards.  They look like humans.  They can talk like humans.  They're identical to humans on the atomic level.  But they're not human.\n\nOFFICER 2:  Scumbags.\n\nThe huge noise of a throbbing engine echoes over the hills.  Up rides the MAN on a white motorcycle.  The MAN is wearing black sunglasses and a black leather business suit with a black leather tie and silver metal boots.  His white beard flows in the wind.  He pulls to a halt in front of the gate.\n\nThe OFFICERS bustle up to the motorcycle.\n\nOFFICER 1:  State your business here.\n\nMAN:  Is this where you're keeping David Chalmers?\n\nOFFICER 2:  What's it to you?  You a friend of his?\n\nMAN:  Can't say I am.  But even zombies have rights.\n\nOFFICER 1:  All right, buddy, let's see your qualia.\n\nMAN:  I don't have any.\n\nOFFICER 2 suddenly pulls a gun, keeping it trained on the MAN.  OFFICER 2:  Aha!  A zombie!\n\nOFFICER 1:  No, zombies claim to have qualia.\n\nOFFICER 2:  So he's an ordinary human?\n\nOFFICER 1:  No, they also claim to have qualia.\n\nThe OFFICERS look at the MAN, who waits calmly.\n\nOFFICER 2:  Um...\n\nOFFICER 1:  Who _are_ you?\n\nMAN:  I'm Daniel Dennett, bitches.\n\nSeemingly from nowhere, DENNETT pulls a sword and slices OFFICER 2's gun in half with a steely noise.  OFFICER 1 begins to reach for his own gun, but DENNETT is suddenly standing behind OFFICER 1 and chops with a fist, striking the junction of OFFICER 1's shoulder and neck.  OFFICER 1 drops to the ground.\n\nOFFICER 2 steps back, horrified.\n\nOFFICER 2:  That's not possible!  How'd you do that?\n\nDENNETT:  I am one with my body.\n\nDENNETT drops OFFICER 2 with another blow, and strides toward the gate.  He looks up at the imposing concrete complex, and grips his sword tighter.\n\nDENNETT _(quietly to himself)_:  There is a spoon.\n\n(_Cut_ back to GENERAL FRED and the other military officials.)\n\nGENERAL FRED:  I've just received the reports.  We've lost Detroit.\n\nCAPTAIN MUDD:  I don't want to be the one to say \"Good riddance\", but—\n\nGENERAL FRED:  Australia has been... _reduced to atoms._\n\nCOLONEL TODD:  The epiphenomenal virus is spreading faster.  Civilization itself threatens to dissolve into total normality.  We could be looking at the middle of humanity.\n\nCAPTAIN MUDD:  Can we negotiate with the zombies?\n\nGENERAL FRED:  We've sent them messages.  They sent only a single reply.\n\nCAPTAIN MUDD:  Which was...?\n\nGENERAL FRED:  It's on its way now.\n\nAn orderly brings in an envelope, and hands it to GENERAL FRED.\n\nGENERAL FRED opens the envelope, takes out a single sheet of paper, and reads it.\n\nSilence envelops the room.\n\nCAPTAIN MUDD:  What's it say?\n\nGENERAL FRED:  It says... that _we're_ the ones with the virus.\n\n(A silence falls.)\n\nCOLONEL TODD raises his hands and stares at them.\n\nCOLONEL TODD:  My God, it's true.  It's true.  I...\n\n(A tear rolls down COLONEL TODD's cheek.)\n\nCOLONEL TODD:  I don't feel anything.\n\nThe screen goes black.\n\nThe sound goes silent.\n\nThe movie continues exactly as before.\n\n* * *\n\n[![Elizombies](/static/imported/2008/04/19/elizombies.jpg \"Elizombies\")](/static/imported/2008/04/19/elizombies.jpg) PS:  This is me being attacked by zombie nurses at Penguicon.\n\nOnly at a _combination_ science fiction and open-source convention would it be possible to attend a session on knife-throwing, cry \"In the name of Bayes, die!\", throw the knife, and then have a fellow holding a wooden shield say, \"Yes, but how do you determine the prior for where the knife hits?\""
          },
          "voteCount": 113
        },
        {
          "name": "Excluding the Supernatural",
          "type": "post",
          "slug": "excluding-the-supernatural",
          "_id": "u6JzcFtPGiznFgDxP",
          "url": null,
          "title": "Excluding the Supernatural",
          "author": "Eliezer_Yudkowsky",
          "question": false,
          "tags": [
            {
              "name": "Reductionism"
            },
            {
              "name": "Rationality"
            }
          ],
          "tableOfContents": null,
          "contents": {
            "markdown": "Occasionally, you hear someone claiming that creationism should not be taught in schools, especially not as a competing hypothesis to evolution, because creationism is _a priori and automatically_ excluded from scientific consideration, in that it invokes the \"supernatural\".\n\nSo... is the idea here, that creationism _could_ be true, but _even if it were true_, you wouldn't be _allowed_ to teach it in science class, because science is only about \"natural\" things?\n\nIt seems clear enough that this notion stems from the desire to [avoid a confrontation between science and religion](/lw/i8/religions_claim_to_be_nondisprovable/).  You don't want to come right out and say that science doesn't teach Religious Claim X because X has been [tested by the scientific method and found false](/lw/i8/religions_claim_to_be_nondisprovable/).  So instead, you can... um... claim that science is excluding hypothesis X _a priori_.  That way you don't have to discuss how experiment has falsified X _a posteriori._\n\nOf course this plays right into the creationist claim that Intelligent Design isn't getting a fair shake from science—that science has _prejudged_ the issue in favor of atheism, regardless of the evidence.  If science excluded Intelligent Design _a priori,_ this would be a justified complaint!\n\nBut let's back up a moment.  The one comes to you and says:  \"Intelligent Design is excluded from being science _a priori,_ because it is 'supernatural', and science only deals in 'natural' explanations.\"\n\nWhat exactly do they mean, \"supernatural\"?  Is any explanation invented by someone with the last name \"Cohen\" a supernatural one?  If we're going to summarily kick a set of hypotheses out of science, what is it that we're supposed to exclude?\n\nBy _far_ the best definition I've ever heard of the supernatural is [Richard Carrier's](http://richardcarrier.blogspot.com/2007/01/defining-supernatural.html):  A \"supernatural\" explanation appeals to _ontologically basic mental things,_ mental entities that cannot be reduced to nonmental entities.\n\nThis is the difference, for example, between saying that [water rolls downhill because it _wants_ to be lower](/lw/te/three_fallacies_of_teleology/), and setting forth differential equations that claim to describe only motions, not desires.  It's the difference between saying that a tree puts forth leaves because of a tree spirit, versus examining plant biochemistry.  Cognitive science [takes the fight against supernaturalism into the realm of the mind](/lw/tf/dreams_of_ai_design/).\n\nWhy is this an excellent definition of the supernatural?  I refer you to [Richard Carrier](http://richardcarrier.blogspot.com/2007/01/defining-supernatural.html) for the full argument.  But consider:  Suppose that you discover what seems to be a _spirit,_ inhabiting a tree: a dryad who can materialize outside or inside the tree, who speaks in English about the need to protect her tree, et cetera.  And then suppose that we turn a microscope on this tree spirit, and she turns out to be [made of parts](/lw/t5/when_anthropomorphism_became_stupid/)—not inherently spiritual and ineffable parts, like fabric of desireness and cloth of belief; but rather the same sort of parts as quarks and electrons, parts whose behavior is defined in motions rather than minds.  Wouldn't the dryad immediately be [demoted to the dull catalogue of common things](/lw/or/joy_in_the_merely_real/)?\n\nBut if we accept Richard Carrier's definition of the supernatural, then a dilemma arises: we _want_ to give religious claims a fair shake, but it seems that we have _very good_ grounds for excluding supernatural explanations _a priori._\n\nI mean, what _would_ the universe look like if reductionism were false?\n\nI previously [defined the reductionist thesis](/lw/on/reductionism/) as follows: human minds create multi-level _models_ of reality in which high-level patterns and low-level patterns are separately and explicitly _represented._  A physicist knows Newton's equation for gravity, Einstein's equation for gravity, and the derivation of the former as a low-speed approximation of the latter.  But these three separate mental representations, are only a convenience of human cognition.  It is not that _reality itself_ has an Einstein equation that governs at high speeds, a Newton equation that governs at low speeds, and a \"bridging law\" that smooths the interface.  Reality itself has only a single level, Einsteinian gravity.  It is only the [Mind Projection Fallacy](/lw/oi/mind_projection_fallacy/) that makes some people talk as if the higher levels could have a separate existence—different levels of organization can have separate representations in human maps, but the territory itself is a single unified low-level mathematical object.\n\nSuppose this were wrong.\n\nSuppose that the [Mind Projection Fallacy](/lw/oi/mind_projection_fallacy/) was not a fallacy, but simply true.\n\nSuppose that a 747 had a fundamental physical existence apart from the quarks making up the 747.\n\nWhat experimental observations would you expect to make, if you found yourself in such a universe?\n\nIf you can't come up with a good answer to that, it's not _observation_ that's ruling out \"non-reductionist\" beliefs, but _a priori_ logical incoherence.  If you can't say what predictions the \"non-reductionist\" model makes, how can you say that experimental evidence rules it out?\n\nMy thesis is that non-reductionism is a _confusion;_ and once you realize that an idea is a confusion, it becomes a tad difficult to envision what the universe would look like if the confusion were _true._ Maybe I've got some multi-level model of the world, and the multi-level model has a one-to-one direct correspondence with the causal elements of the physics?  But once all the rules are specified, why wouldn't the model just flatten out into yet another list of fundamental things and their interactions?  Does everything I can _see in_ the model, like a 747 or a human mind, have to become a separate real thing?  But what if I see a pattern in that new supersystem?\n\nSupernaturalism is a special case of non-reductionism, where it is not 747s that are irreducible, but just (some) mental things.  Religion is a special case of supernaturalism, where the irreducible mental things are God(s) and souls; and perhaps also sins, angels, karma, etc.\n\nIf I propose the existence of a powerful entity with the ability to survey and alter each element of our observed universe, but with the entity reducible to nonmental parts that interact with the elements of our universe in a lawful way; if I propose that this entity wants certain particular things, but \"wants\" using a brain composed of particles and fields; then this is not yet a religion, just a naturalistic hypothesis about a naturalistic Matrix.  If tomorrow the clouds parted and a vast glowing amorphous figure thundered forth the above description of reality, then this would not imply that the figure was necessarily honest; but I would show the movies in a science class, and I would try to derive testable predictions from the theory.\n\nConversely, [religions have ignored](/lw/kr/an_alien_god/) the discovery of that [ancient bodiless thing](/lw/kr/an_alien_god/): omnipresent in the working of Nature and immanent in every falling leaf: vast as a planet's surface and billions of years old: itself unmade and arising from the structure of physics: designing without brain to shape all life on Earth and the minds of humanity.  Natural selection, when Darwin proposed it, was not hailed as the long-awaited Creator:  It wasn't _fundamentally_ mental.\n\nBut now we get to the dilemma: if the staid conventional normal boring understanding of physics and the brain _is_ correct, there's no way _in principle_ that a human being can concretely envision, and derive testable experimental predictions about, an alternate universe in which things _are_ irreducibly mental.  Because, if the boring old normal model is correct, your brain is made of quarks, and so your brain will only be able to envision and concretely predict things that can predicted by quarks.  You will only ever be able to construct models made of interacting simple things.\n\nPeople who live in reductionist universes cannot concretely envision non-reductionist universes.  They can [pronounce the syllables](/lw/i6/professing_and_cheering/) \"non-reductionist\" but they can't _imagine_ it.\n\nThe basic error of anthropomorphism, and the reason why [supernatural explanations sound much simpler than they really are](/lw/jp/occams_razor/), is your brain using itself as an opaque black box to predict other things labeled \"mindful\".  Because you already have big, complicated webs of neural circuitry that implement your \"wanting\" things, it seems like you can easily describe water that \"wants\" to flow downhill—the one word \"want\" acts as a [lever](/lw/sp/detached_lever_fallacy/) to set your _own_ complicated wanting-machinery in motion.\n\nOr you imagine that God likes beautiful things, and therefore made the flowers.  Your own \"beauty\" circuitry determines what is \"beautiful\" and \"not beautiful\".  But you don't know the diagram of your own synapses.  You can't describe a _nonmental_ system that computes the same label for what is \"beautiful\" or \"not beautiful\"—can't write a computer program that predicts your own labelings.  But this is just a defect of knowledge on your part; it doesn't mean that the brain [has no explanation](/lw/iu/mysterious_answers_to_mysterious_questions/).\n\nIf the \"boring view\" of reality is correct, then you can _never_ predict anything irreducible because _you_ are reducible.  You can never get Bayesian confirmation for a hypothesis of irreducibility, because any _prediction you can make_ is, therefore, something that could also be predicted by a reducible thing, namely your brain.\n\nSome boxes you really _can't_ think outside.  If our universe _really is_ Turing computable, we will never be able to _concretely_ envision anything that isn't Turing-computable—no matter how many levels of halting oracle hierarchy our mathematicians can talk _about,_ we won't be able to predict what a halting oracle would actually _say,_ in such fashion as to experimentally discriminate it from merely computable reasoning.\n\nOf course, that's all assuming the \"boring view\" is correct.  _To the extent_ that you believe evolution is true, you should not expect to encounter strong evidence against evolution.  To the extent you believe reductionism is true, you should expect non-reductionist hypotheses to be _incoherent_ as well as wrong.  To the extent you believe supernaturalism is false, you should expect it to be _inconceivable_ as well.\n\nIf, on the other hand, a supernatural hypothesis turns out to be true, then presumably you will also discover that it is not inconceivable.\n\nSo let us bring this back full circle to the matter of Intelligent Design:\n\nShould ID be excluded _a priori_ from experimental falsification and science classrooms, because, by invoking the supernatural, it has placed itself outside of natural philosophy?\n\nI answer:  \"Of course not.\"  The _irreducibility_ of the intelligent designer is not an indispensable part of the ID hypothesis.  For every irreducible God that can be proposed by the IDers, there exists a corresponding reducible alien that behaves in accordance with the same predictions—since the IDers themselves are reducible; to the extent I believe reductionism is in fact correct, which is a rather strong extent, I must expect to discover reducible formulations of all supposedly supernatural predictive models.\n\nIf we're going over the archeological records to test the assertion that Jehovah parted the Red Sea out of an explicit desire to display its superhuman power, then it makes little difference whether Jehovah is ontologically basic, or an alien with nanotech, or a Dark Lord of the Matrix.  You do some archeology, find no skeletal remnants or armor at the Red Sea site, and indeed find records that Egypt ruled much of Canaan at the time.  So you stamp the historical record in the Bible \"disproven\" and carry on.  The hypothesis is coherent, falsifiable and wrong.\n\nLikewise with the evidence from biology that foxes are designed to chase rabbits, rabbits are designed to evade foxes, and [neither is designed \"to carry on their species\" or \"protect the harmony of Nature\"](/lw/l5/evolving_to_extinction/); likewise with the retina being designed backwards with the light-sensitive parts at the bottom; and so on through a thousand other items of evidence for splintered, immoral, [incompetent](/lw/kt/evolutions_are_stupid_but_work_anyway/) design.  The Jehovah model of our [alien god](/lw/kr/an_alien_god/) is coherent, falsifiable, and wrong—coherent, that is, so long as you don't care whether Jehovah is ontologically basic or just an alien.\n\nJust convert the supernatural hypothesis into the corresponding natural hypothesis.  Just make the same predictions the same way, without asserting any mental things to be ontologically basic.  Consult your brain's black box if necessary to make predictions—say, if you want to talk about an \"angry god\" without building a full-fledged angry AI to label behaviors as angry or not angry.  So you derive the predictions, or look up the predictions made by ancient theologians without advance knowledge of our experimental results.  If experiment conflicts with those predictions, then it is fair to speak of the religious claim having been scientifically refuted.  It was given its just chance at confirmation; it is being excluded _a posteriori,_ not _a priori._\n\nUltimately, reductionism is just disbelief in _fundamentally complicated_ things.  If \"fundamentally complicated\" sounds like an oxymoron... well, that's why I think that the doctrine of non-reductionism is a _confusion,_ rather than a way that things could be, but aren't.  You would be wise to be wary, if you find yourself supposing such things.\n\nBut the ultimate rule of science is to look and see.  If ever a God appeared to thunder upon the mountains, it would be something that people looked at and saw.\n\n_Corollary:_  Any supposed [designer](/lw/tf/dreams_of_ai_design/) of Artificial General Intelligence who [talks about religious beliefs in respectful tones](/lw/gv/outside_the_laboratory/), is clearly not an [expert on](http://www.overcomingbias.com/2007/04/expert_at_versu.html) reducing mental things to nonmental things; and indeed knows so very little of the uttermost basics, as for it to be scarcely plausible that they could be [expert at](http://www.overcomingbias.com/2007/04/expert_at_versu.html) the art; unless their _idiot savancy_ is complete.  Or, of course, if they're outright lying.  We're not talking about a subtle mistake."
          },
          "voteCount": 65
        }
      ]
    }
  ]
}